Artificial Intelligence 175 (2011) 79–119Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintApproximation of action theories and its applicationto conformant planningPhan Huy Tu a, Tran Cao Son b,∗, Michael Gelfond c, A. Ricardo Morales ca Microsoft Corporation, 1 Microsoft Way, Redmond, WA 98052, USAb Computer Science Department, New Mexico State University, Las Cruces, NM 88003, USAc Computer Science Department, Texas Tech University, Lubbock, TX 79409, USAa r t i c l ei n f oa b s t r a c tArticle history:Available online 3 April 2010Keywords:Reasoning about action and changeKnowledge representationPlanningIncomplete informationAnswer set programmingThis paper describes our methodology for building conformant planners, which is basedon recent advances in the theory of action and change and answer set programming. Thedevelopment of a planner for a given dynamic domain starts with encoding the knowledgeabout fluents and actions of the domain as an action theory D of some action language.Our choice in this paper is AL – an action language with dynamic and static causal lawsand executability conditions. An action theory D of AL defines a transition diagram T (D)(cid:4)(cid:5) belongs to T (D)containing all the possible trajectories of the domain. A transition (cid:3)s, a, s(cid:4)iff the execution of the action a in the state s may move the domain to the state s.The second step in the planner development consists in finding a deterministic transitiondiagram T lp(D) such that nodes of T lp(D) are partial states of D, its arcs are labeled byactions, and a path in T lp(D) from an initial partial state δ0 to a partial state satisfying thein T (D). The transition diagramgoal δ f corresponds to a conformant plan for δ0 and δ fT lp(D) is called an ‘approximation’ of T (D). We claim that a concise description of anapproximation of T (D) can often be given by a logic program π (D) under the answersets semantics. Moreover, complex initial situations and constraints on plans can be alsoexpressed by logic programming rules and included in π (D). If this is possible then theproblem of finding a parallel or sequential conformant plan can be reduced to computinganswer sets of π (D). This can be done by general purpose answer set solvers. If plans aresequential and long then this method can be too time consuming. In this case, π (D) isused as a specification for a procedural graph searching conformant planning algorithm.The paper illustrates this methodology by building several conformant planners whichwork for domains with complex relationship between the fluents. The efficiency of theplanners is experimentally evaluated on a number of new and old benchmarks. In additionwe show that for a subclass of action theories of AL our planners are complete, i.e., if inT lp(D) we cannot get from δ0 to a state satisfying the goal δ f then there is no conformantplan for δ0 and δ fin T (D).© 2010 Elsevier B.V. All rights reserved.1. IntroductionA conformant planner is a program that generates a sequence of actions, which achieves a goal from any possible initialstate of the world, given the information about the initial state and the possible effects of actions. Such sequences arenormally referred to as conformant plans. In this paper we describe our methodology for the design and implementation of* Corresponding author.E-mail addresses: tuphan@microsoft.com (P.H. Tu), tson@cs.nmsu.edu (T.C. Son), mgelfond@cs.ttu.edu (M. Gelfond), ricardo@cs.ttu.edu (A.R. Morales).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.00780P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119conformant planners. The methodology is rooted in the ideas of declarative programming [42] and utilizes recent advancesin answer set programming and the theory of action and change. This allows the designer to guarantee a substantiallyhigher degree of trust in the planners’ correctness, as well as a greater degree of elaboration tolerance [43].The design of a declarative solution of a problem P normally involves the selection of a logical language capable ofrepresenting knowledge relevant to P . We base our methodology on representing such knowledge in action languages– formal models of parts of natural language used for reasoning about actions and their effects. A theory in an actionlanguage (often called an action description) is used to succinctly describe the collection of all possible trajectories of agiven dynamic domain. Usually this is done by defining the transition diagram, T (D), of an action description D. The statesof T (D) correspond to possible physical states of the domain represented by D. Arcs of T (D) are labeled by actions.A transition (cid:3)s, a, s. In someaction languages, actions are elementary (or atomic). In some others, an action a is viewed as a finite non-empty collectionof elementary actions. Intuitively, execution of an action a = {e1, . . . , en}, where the ei ’s are elementary actions, correspondsto the simultaneous execution of every ei ∈ a.(cid:4)(cid:5) ∈ T (D) if the execution of the action a in the state s may move the domain to the state s(cid:4)There are by now a large number of action languages (see for instance [8,24,25,32,41,67]) capturing different aspectsof dynamic domains. Our choice in this paper is AL [8] – an action language with dynamic causal laws describing directeffects of actions, impossibility conditions stating the conditions under which an action cannot be executed, and static causallaws (a.k.a. state constraints) describing static relations between fluents. For example the statement “putting a block A on topof block B causes A to be on top of B” can be viewed as a dynamic causal law describing the direct effect of action put( A, B).The statement “a block A cannot be put on B if there is a block located on A or on B” represents an impossibility condition.The statement “block A is above block C if A is on C or it is on B and B is above C” is an example of a (recursive) static causallaw. Note that static causal laws can cause actions to have indirect effects. Consider for instance the effects of executing theaction put( A, B) in a state in which both A and B are clear and B is located above some block C . The direct effects ofthis action (described by the dynamic causal law above) is on( A, B). An indirect effect, above( A, C), is obtained from ourstatic causal law. The problem of determining such indirect effects, known as the ramification problem, remained open fora comparatively long time. In the last decade, several solutions to this problem have been proposed, for example [5,38,28,37,41,53,54,46,64]. One of these solutions [41] is incorporated in the semantics of AL. The ability to represent causal lawsmakes AL a powerful modeling language. It was successfully used for instance to model the reactive control system of thespace shuttle [4]. The system consists of fuel and oxidizer tanks, valves and other plumbing needed to provide propellantto the maneuvering jets of the shuttle. It also includes electronic circuitry; both to control the valves in the fuel lines andto prepare the jets to receive firing commands. Overall, the system is rather complex, in that it includes 12 tanks, 44 jets,66 valves, 33 switches, and around 160 computer commands (computer-generated signals). The use of static causal laws(including recursive ones) was crucial for modeling the system and for the development of industrial size planning anddiagnostic applications.While static causal laws have been intensively studied by researchers interested in knowledge representation, they haverarely been considered by the mainstream planning community. Although the original specification of the Planning DomainDescription Language (PDDL) – a language frequently used for the specification of planning problems by the planning com-munity – includes axioms1 (which correspond to non-recursive static causal laws in our terminology) [27], most of theplanning domains investigated by this community, including those used for planning competitions [1,17,40] do not includeaxioms. This is partly due to the fact that the semantics for PDDL with axioms is not clearly specified, and partly to the(somewhat mistaken but apparently widespread) belief that static causal laws can always be replaced by dynamic causallaws. There is fortunately also an opposing view. For instance, in [63], the authors argue that the use of axioms not onlyincreases the expressiveness and elegance of the problem representation but also improves the performance of planners. Itis known that the complexity of the conformant planning problem is much higher than classical planning in deterministicdomains (Σ P2 vs. NP-complete) [6,68], and hence the question of efficiency becomes even more important.An action description D of AL describing the corresponding dynamic domain can be used for multiple purposes in-cluding classical planning and diagnostics (see for instance [3,4,7,35]). One way to attack this problem is to replace thetransition diagram T (D) by a deterministic transition diagram T lp(D) such that nodes of T lp(D) are partial states of D, itsarcs are labeled by actions, and a path in T lp(D) from an initial partial state δ0 to a partial state satisfying δ f correspondsto a conformant plan for δ0 and δ f in T (D). The transition diagram T lp(D) is called an approximation of T (D). Even thoughT lp(D) normally has many more states than T (D) does, validating whether a given sequence of actions is a conformant planusing T lp(D) is much easier. As pointed out in [6] the use of an approximation can substantially help reduce the complexityof the planning problem. Indeed, an approximation in domains with incomplete information and static causal laws has beendeveloped and applied successfully in the context of conditional and conformant planning in [66]. Of course a drawback ofthis approach is the possible incompleteness of approximation based planners, i.e., existence of solvable planning problemsfor which such a planner might not find a solution.According to this methodology the second step in the development of a conformant planner consists in finding a suitableapproximation of T (D). We claim that a concise description of an approximation of T (D) can often be given by a logicprogram π (D) under the answer sets semantics [26,62]. Moreover, complex initial situations and constraints on plans can1 In our view, static causal laws can be used to represent relationships between fluents and thus could be considered as axioms in PDDL.P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–11981be also expressed by logic programming rules and included in π (D). If this is possible then the problem of finding aparallel or sequential conformant plan can be reduced to computing answer sets of π (D). This can be done by generalpurpose answer set solvers (e.g., [19,33,55]).If plans are sequential and long then this method can be too time consuming. In this case, the system designer may useπ (D) as a specification for a procedural graph searching conformant planning algorithm.This paper illustrates the proposed methodology by building several conformant planners which take as input an actiondescription of AL, an incomplete description of an initial situation, and a description of the goal. In summary, the maincontributions of this paper are• A new approach to defining and computing an approximation of the transition graph of action theories with incompleteinitial situation, static causal laws, and parallel actions;• A sufficient condition for the completeness of the reasoning and/or planning tasks which employ the approximatedtransition diagram instead of the possible world semantics; and• Different approximation-based planners that can generate sequential and/or parallel conformant plans. These includethe planner CPasp and CpA. The former is a logic programming based planner and can generate both minimal andparallel conformant plans while the latter is a heuristic forward search planner, implemented in C++, and can onlygenerate sequential plans.• The introduction of fairly simple planning problems with static causal laws that appear to be challenging for manycontemporary planners.In addition, we discuss how complex initial situations and/or constraints on a planning problem can be easily incorporatedinto our logic programming based planner.The paper is organized as follows. In the next section, we review the basics of the language AL including its syntax andsemantics, the logic programming representation of transition diagrams specified by AL action theories, and the problemof conformant planning. In Section 3, we introduce the notion of an approximation of AL action theories and define adeterministic approximation of an action theory D by means of a logic program π (D). In Section 4, we describe an imple-mentation, in the answer set programming paradigm, of a conformant planner based on this approximation and investigateits completeness in Section 5. Section 6 extends the results in the previous section to planning problems with disjunc-tive initial states. In Section 7, we describe a heuristic based sequential planner whose basic component is a module forcomputing the approximation. We provide a comparative study of the performance of our planners against state-of-the-artconformant planners in Section 8. We discuss some advantages of the use of logic programming in conformant planning inSection 9 and conclude in Section 10.2. BackgroundWe begin with a short review of the syntax and semantics of the language AL for domains with static causal laws from[8,67], and the notion of a planning problem and its solutions.2.1. SyntaxThe signature Σ of an action theory of AL consists of two disjoint, non-empty sets of symbols: the set F of fluents, andthe set A of elementary actions. By an action we mean a non-empty set a of elementary actions. Informally an execution ofan action a is interpreted as a simultaneous execution of its components. For simplicity we identify an elementary actione with the action {e}. A fluent literal (or literal for short) l is a fluent or its negation. By ¬l we denote the fluent literalcomplementary to l, i.e., ¬( f ) = ¬ f and ¬(¬ f ) = f . An AL action theory is a set of statements of the following forms:e causes l if ψl if ψimpossible a if ψ(1)(2)(3)where e is an elementary action, a is an action, l is a fluent literal, and ψ is a set of fluent literals from the signature Σ .The set of fluent literals ψ is referred to as the precondition of the corresponding statement. When the precondition ψ isempty, the if part of the statement can be omitted. Statement (1), called a dynamic causal law, says that if e is executedin a state satisfying ψ then l will hold in any resulting state. Statement (2), called a static causal law, says that any statesatisfying ψ must satisfy l. Statement (3), called an impossibility condition, says that action a cannot be executed in any statesatisfying ψ .To illustrate the syntax of AL, let us consider an instance of (a variant of) the Bomb in the toilet domain [45].Example 1. There are two packages p1 and p2 and two toilets t1 and t2. Each of the packages may contain a bomb whichcan be disarmed by dunking the package into a toilet. Dunking a package into a toilet also clogs the toilet. Flushing a toiletunclogs it. We are safe only if both packages are disarmed.82P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Meta variables:pi ’s stand for packages, i ∈ {1, 2}, p1 (cid:7)= p2t j ’s stand for toilets, j ∈ {1, 2}, t1 (cid:7)= t2Fluents:armed(pi ): package pi contains the bombclogged(t j ): toilet t j is cloggedsafe: all the bombs are disarmedActions:dunk(pi , t j ): dunk package pi into toilet t jflush(t j ): flush toilet t jAction theory:impossible {dunk(pi , t j ), flush(t j )}impossible {dunk(p1, t j ), dunk(p2, t j )}impossible {dunk(pi , t1), dunk(pi , t2)}impossible dunk(pi , t j ) if clogged(t j )dunk(pi , t j ) causes ¬armed(pi )dunk(pi , t j ) causes clogged(t j )flush(t j ) causes ¬clogged(t j )safe if ¬armed(1), ¬armed(2)¬safe if armed(1)¬safe if armed(2)Fig. 1. Dbomb, the bomb in the toilet theory.Fig. 1 shows an action theory of AL, denoted by Dbomb, that describes the domain.2 There are four impossibility state-ments in the action theory. The first one says that “it is impossible to dunk a package into a toilet that is being flushed”. Thesecond one states that “it is impossible to dunk two different packages into the same toilet at the same time”. The third onesays that “it is impossible to dunk a package into two different toilets at the same time”. Unlike the first three statementsthat specify physical impossibilities of concurrent actions, the last one specifies physical impossibility of an elementaryaction. It says that “it is impossible to dunk a package into a clogged toilet”.In addition to the impossibility statements, the action theory also includes statements to describe the effects of actionsdunk and flush and the relationship between the fluents safe and armed.2.2. SemanticsIntuitively, an AL action theory describes a transition diagram containing all possible trajectories of the correspondingdomain. Before providing the precise definition of such a transition diagram, let us introduce some terminology and notation.Given an action theory D, a set σ of fluent literals is consistent if it does not contain two complementary fluent literals.We say that σ is complete if for every fluent f , either f or ¬ f belongs to σ . A fluent literal l holds in σ if l belongs to σ ;l possibly holds in σ if ¬l does not belong to σ . A set γ of fluent literals holds (resp. possibly holds) in σ if every fluentliteral in γ holds (resp. possibly holds) in σ .A set of fluent literals σ is closed under a static causal law (2) if l holds in σ whenever ψ holds in σ . By ClD(σ ) wedenote the smallest set of fluent literals that contains σ and is closed under the static causal laws of D.A state s is a complete, consistent set of fluent literals closed under the static causal laws of D. An action b is said tobe prohibited in s if D contains an impossibility condition (3) such that ψ holds in s and a ⊆ b; otherwise, b is said to beexecutable in s. An action is executable in a set of states S if it is executable in every state s ∈ S.Given a state s and an action a that is executable in s, a fluent literal l is called a direct effect of a in s if there exists adynamic causal law (1) such that e ∈ a and ψ holds in s. By de(a, s) we denote the set of all direct effects of a in s.The action theory D describes a transition diagram T (D) whose nodes correspond to possible physical states of thedomain and whose arcs are labeled with actions. The transitions of the diagram are defined as follows.Definition 1. For an action a and two states s and sClD(de(a, s) ∪ (s ∩ s(cid:4))).(cid:4), a transition (cid:3)s, a, s(cid:4)(cid:5) ∈ T (D) iff a is executable in s and s(cid:4) =(cid:4)Intuitively (cid:3)s, a, sin state s. Such state scontext then we simply say that sconcurrent actions, the equation in Definition 1 is equivalent to the one proposed in [41].(cid:4)(cid:5) ∈ T (D) indicates that if the system is in state s then after the execution of a the system may moveis called a possible successor state of s as a result of the execution of a. If action a is clear from theis a possible successor state of s. It is worth to note that for action theories without(cid:4)(cid:4)Example 2. Consider the action theory Dbomb from Example 1. Let2 Note that in the description of an action theory, we often use typed variables. A statement with variables are understood as a shorthand for thecollection of its ground instances.P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–11983(cid:2)s0 =armed(1), armed(2), ¬clogged(1), ¬clogged(2), ¬safe(cid:3)andThen(cid:2)(cid:3)dunk(1, 1), dunk(2, 2)a =(cid:2)s1 =¬armed(1), ¬armed(2), clogged(1), clogged(2), safe(cid:3)is the unique successor state of s0, i.e., (cid:3)s0, a, s1(cid:5) ∈ T (Dbomb), because(cid:5)de(a, s0) ∪ (s0 ∩ s1)(cid:3)¬armed(1), ¬armed(2), clogged(1), clogged(2)(cid:4)(cid:2)(cid:4)ClDbomb(cid:5)∪ ∅= ClDbomb(cid:2)=¬armed(1), ¬armed(2), clogged(1), clogged(2), safe(cid:3)= s1Note that safe belongs to the closure of σ = {¬armed(1), ¬armed(2), clogged(1), clogged(2)} because Dbomb contains thestatic causal lawsafe if ¬armed(1), ¬armed(2)and both ¬armed(1) and ¬armed(2) hold in σ .Now let(cid:2)b =(cid:3)dunk(1, 1), flush(2)Then,(cid:2)s2 =¬armed(1), armed(2), clogged(1), ¬clogged(2), ¬safe(cid:3)is the unique successor state of s0, i.e., (cid:3)s0, b, s2(cid:5) ∈ T (Dbomb).We next define the notion of a consistent action theory.Definition 2 (Consistent action theory). An action theory D is consistent if for any state s and action a executable in s, thereexists at least one state ssuch that (cid:3)s, a, s(cid:4)(cid:5) ∈ T (D).(cid:4)Observe that action AL-theories could be non-deterministic and therefore determining whether or not a given actiontheory is consistent is not a simple task. Indeed, we can prove the following complexity result.Theorem 1. Deciding whether or not a given action theory is consistent is an NP-complete problem.The proof of this theorem is a straightforward reformulation of a similar result in [68] and is therefore not presentedhere.It is worth mentioning that the problem is a P-problem for action theories without static causal laws.Example 3 (Consistent and inconsistent action theories). Consider the following action theory:(cid:6)D0 =e causes f if ge causes ¬ f if h(cid:4)We have that e is executable in s = { f , g, h}. If sto sabove definition, this implies that D0 is inconsistent.. This is a contradiction because s(cid:4)(cid:4)However, if we add to D0 the following impossibility conditionis a successor state of s then it is easy to see that both f and ¬ f belongmust be consistent. Hence, there exists no successor state for s. According to theimpossible e if g, hthen the action theory will become consistent because e cannot be executed in any state in which both g and h holds.Hence, at most one of the above dynamic causal laws takes effect, which guarantees the consistency of the theory.The consistency of an action theory ensures that the execution of a legal action in a state yields at least one possiblesuccessor state. In this paper, we are interested in consistent action theories only. We will next define the notion of deterministicaction theory.84P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Definition 3 (Deterministic action theory). An action theory D is deterministic if for any state s and action a, there exists atmost one state ssuch that (cid:3)s, a, s(cid:4)(cid:5) ∈ T (D).(cid:4)It is easy to see that if an action theory D does not contain any static causal laws then it is deterministic. In the presenceof static causal laws, an action theory, however, may be non-deterministic.3 The following example shows such an actiontheory.Example 4 (Non-deterministic action theory). Consider the following action theory:(cid:7)D1 =e causes fg if f , ¬hh if f , ¬gLet s0 = {¬ f , ¬g, ¬h}. We can verify that (cid:3)s0, e, s1(cid:5) ∈ T (D1) and (cid:3)s0, e, s2(cid:5) ∈ T (D1) where s1 = { f , h, ¬g} and s2 ={ f , g, ¬h}. Hence, by definition, D1 is non-deterministic.For our later discussion, the following definition will be useful.Definition 4 (Entailment). Let D be an action theory and M be a path in T (D), i.e., M is an alternate sequence of states andactions (cid:3)s0, a0, s1, . . . , an−1, sn(cid:5) such that (cid:3)si, ai, si+1(cid:5) ∈ T (D) for 0 (cid:2) i < n. We say that M entails a set of fluent literals σ ,written as M |(cid:12) σ , if σ holds in sn.For a path M = (cid:3)s0, a0, s1, . . . , an−1, sn(cid:5) in T (D), s0 and sn are referred to as the initial state and final state, respectively,of M. The sequence of actions α = (cid:3)a0, . . . , an−1(cid:5) is referred to as a chain of events. We also say that M is a model of α andsometimes write (cid:3)s0, α, sn(cid:5) ∈ T (D) to denote that there exists a model of α whose initial state and final state are s0 and snrespectively.A chain of events α = (cid:3)a0, a1, . . . , an−1(cid:5) is executable in a state s if either (i) n = 0, i.e., α is an empty chain of events,(cid:4)(cid:5) ∈ T (D). A chain of events isor (ii) a0 is executable in s and (cid:3)a1, . . . , an−1(cid:5) is executable in every sexecutable in a set of states S if it is executable in every state s ∈ S.such that (cid:3)s, a, s(cid:4)2.3. A logic programming representation of T (D)We now describe a logic program, called lp(D), which can be used to compute the transitions in T (D). lp(D) con-sists of rules for reasoning about the effects of actions. Among these rules, the inertial rule encodes the solution to theframe problem, first discussed by John McCarthy and Pat Hayes in their landmark paper on reasoning about actions andchanges [44].The signature of lp(D) includes terms corresponding to fluent literals and actions of D, as well as non-negative integersused to represent time steps. We often write lp(D, n) to denote the restriction of the program lp(D) to time steps between0 and n. Atoms of lp(D) are formed by the following (sorted) predicate symbols:• fluent(F ) is true if F is a fluent;• literal(L) is true if L is a fluent literal;• h(L, T ) is true if the fluent literal L holds at time-step T ; and• o(E, T ) is true if the elementary action E occurs at time-step T .In our representation, letters T , F , L, A, and E (possibly indexed) (resp. t,f , l, a, and e) are used to represent variables(resp. constants) of sorts time step, fluent, fluent literal, action, and elementary action correspondingly. Moreover, we alsouse some shorthands: if a is an action then o(a, T ) = {o(e, T ) | e ∈ a}. For a set of fluent literals γ , h(γ , T ) = {h(l, T ) | l ∈ γ },noth(γ , T ) = {noth(l, T ) | l ∈ γ }, ¬γ = {¬l | l ∈ γ }, and lit(ψ) = {literal(l) | l ∈ ψ}. The set of rules of lp(D) is divided into thefollowing five subsets4:(1) Dynamic causal laws: for each statement of the form (1) in D, the rule:h(l, T ) ← o(e, T −1), h(ψ, T −1),T > 0(4)belongs to lp(D). This rule states that if the elementary action e occurs at time step T − 1 and the precondition ψholds at that time step then l holds afterward.3 This shows that there exist AL action theories with deterministic actions which cannot be represented in PDDL with deterministic actions and non-recursive axioms.4 For simplicity, we omit atoms of the form lit(l), lit(ψ), and step(T ) in the body of the rules.(2) Static causal laws: for each statement of the form (2) in D, lp(D) contains the rule:P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119h(l, T ) ← h(ψ, T )This rule states that if ψ holds at T then so does l.(3) Impossibility conditions: for each statement of the form (3) in D, lp(D) contains the following rule:← o(a, T ), not h(¬ψ, T )85(5)(6)This rule states that if the precondition ψ possibly holds at time step T then the action a cannot occur at that timestep.(4) Inertia: lp(D) contains the following rule which solves the frame problem [44]:h(L, T ) ← h(L, T −1), not h(¬L, T ),T > 0(7)This rule says that a fluent literal L holds at time step T if it holds at the previous time step and its negation does nothold at T .(5) Auxiliary rules: lp(D) also contains the following rules:← h(F , T ), h(¬F , T )literal(F ) ← fluent(F )literal(¬F ) ← fluent(F )(8)(9)(10)The first constraint impedes two complementary fluent literals from holding at the same time. The last two rules areused to define fluent literals.For an action a and a state s, letΦ(a, s) = lp(D, 1) ∪ h(s, 0) ∪ o(a, 0)(11)The next theorem states that the program lp(D) correctly implements T (D) (see [58,67]).Theorem 2. (See [58,67].) Let s be a state and a be an action. Then (cid:3)s, a, sthat s(cid:4) = {l | h(l, 1) ∈ A}.(cid:4)(cid:5) ∈ T (D) iff there exists an answer set A of Φ(a, s) such2.4. Conformant planningThe conformant planning problem, as investigated in this paper, has been discussed in [12–15,18,21,52,56] and in ourpapers [60–62]. Given an action theory D, a set of fluent literals δ is a partial state if it is a subset of some state s andis closed under the static causal laws. Intuitively, a partial state represents the knowledge of an agent associated with Dabout the current state of the world. For example, ∅ is a partial state of the action theory Dbomb because ∅ is closed underthe set of static causal laws of Dbomb and it is a subset of the state s0 in Example 2. Likewise, {armed(1), armed(2), ¬safe}is another partial state of Dbomb. However, {¬armed(1), ¬armed(2)} is not a partial state of Dbomb because it is not closedunder the laws in Dbomb.From now on, we will use symbols σ , s, and δ (possibly indexed) to denote a set of fluent literals, a state and a partialstate respectively. For a partial state δ, the completion of δ, denoted by comp(δ), is the set of all states s such that δ ⊆ s.A (conformant) planning problem is defined as follows.Definition 5. A planning problem P is a tuple (cid:3)D, δ0, δ f (cid:5) where D is an action theory, and δ0 and δ f are partial states of D.Observe that in this section we consider planning problems whose initial state description is a set of literals. Moregeneral description will be considered in Sections 6 and 9. The solutions of a planning problem are defined as follows.Definition 6. Let P = (cid:3)D, δ0, δ f (cid:5) be a planning problem. A chain of events α = (cid:3)a0, . . . , an−1(cid:5) is a solution of P if α isexecutable in comp(δ0) and for every model M of α with the initial state in comp(δ0), M |(cid:12) δ f .We often refer to such an α as a plan for δ f . If δ0 is a state and the action theory D is deterministic then α is calleda classical plan; otherwise it is a conformant plan. Furthermore, if each ai of α is an elementary action then α is calleda sequential plan; otherwise it is called a parallel plan. We next illustrate these definitions using the bomb-in-the-toiletexample.86P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Example 5. Consider the action theory Dbomb and let δ0 = ∅ and δ f = {safe}. Then, Pbomb = (cid:3)D, δ0, δ f (cid:5) is a planning problem.We can check that(cid:9)(cid:8)flush(1), dunk(1, 1), flush(1), dunk(2, 1)α1 =and(cid:8)(cid:2)(cid:3)flush(1), flush(2),(cid:2)α2 =dunk(1, 1), dunk(2, 2)(cid:3)(cid:9)are two solutions of Pbomb. The first one is a sequential plan, whereas the second one is a parallel plan.3. Approximations of AL action theoriesLet D be an action theory. In this section, we first define what we mean by an approximation of the transition diagramT (D) and discuss how approximations can be used to find a solution of a planning problem. Then we introduce a logicprogram for defining such an approximation.Let us begin with the definition of an approximation.Definition 7 (Approximation). A transition diagram T(cid:4)(D) is an approximation of T (D) if1. nodes of T2. if (cid:3)δ, a, δ(cid:4)(cid:5) ∈ T(cid:4)(D) are partial states of D and arcs of T(cid:4)(D) then for every s ∈ comp(δ),(cid:4)(D) are labeled with actions, and(a) a is executable in s, and(b) δ(cid:4) ⊆ sfor every s(cid:4)(cid:4)such that (cid:3)s, a, s(cid:4)(cid:5) ∈ T (D).Intuitively, the first condition describes that an approximation T(cid:4)(D) is a transition diagram between partial states and(cid:4)(D) to be sound with respect to T (D).the second condition requires TGiven an approximation Tin T(cid:4)(D), we will write (cid:3)δ, α, δ(cid:4)(cid:5) ∈ T(cid:4)(D) to denote that there exists a path corresponding to α(cid:4)(D) is deterministic if forfrom δ to δ(cid:4)each partial state δ and action a, there exists at most one δ(cid:4)(cid:4)(D). Even though approximations can benon-deterministic, in this paper we are interested in deterministic approximations only. The next observation shows howthe soundness of an approximation extends from transitions to paths.(cid:4)(D) for every partial state δ. We say that T(cid:4)(D) and by convention (cid:3)δ, (cid:3)(cid:5), δ(cid:5) ∈ Tsuch that (cid:3)δ, a, δ(cid:4)(cid:5) ∈ TObservation 3.1. Let Tevery s ∈ comp(δ),(cid:4)(D) be an approximation of T (D). Then, for every chain of events α if (cid:3)δ, α, δ(cid:4)(cid:5) ∈ T(cid:4)(D) then for(1) α is executable in s, and(2) δ(cid:4) ⊆ sfor every s(cid:4)(cid:4)such that (cid:3)s, α, s(cid:4)(cid:5) ∈ T (D).Observation 3.1 shows that given an approximation Tplanning problem (cid:3)D, δ, δ f (cid:5), where δ f ⊆ δ(cid:4). This gives rise to the following questions:(cid:4)(D), each path from δ to δ(cid:4)corresponds to a solution of the(1) How to find an approximation of T (D)?(2) How an approximation can be used to solve conformant planning problems?In the rest of this section, we define an approximation of T (D) called T lp . In the next section, we will use T lp toconstruct a conformant planner.In our approach, the transitions in T lp(D) are defined by a logic program π (D) called the cautious encoding of D.Following the lp-function theory from [22], π (D) is obtained from lp(D) (see Section 2.3) by adding to it some new rulesand modifying the inertial rule (7) to allow π (D) to deal with partial states.Let b be an action and δ be a partial state. We say that b is safe in δ if there exists no impossibility condition (3) suchthat a ⊆ b and ψ possibly holds in δ. A fluent literal l is a direct effect (resp. possible direct effect) of b in δ if there exists adynamic causal law (1) such that e ∈ b and ψ holds (resp. possibly holds) in δ. (Recall that a partial state is also a set offluent literals and thus the concepts of “holds” and “possibly holds” are already defined in Section 2.2.) Observe that if b issafe in δ then b is executable in every state s ⊇ δ. Furthermore, the direct effects of b in δ are also the direct effects of b ins which in turn are the possible direct effects of b in δ.The program π (D): The signature of π (D) is the same as the signature of lp(D). As before, we write π (D, n) to denotethe restriction of π (D) to time steps between 0 and n. Atoms of π (D) are atoms of lp(D) and those formed by thefollowing (sorted) predicate symbols:• de(l, T ) is true if the fluent literal l is a direct effect of an action that occurs at the previous time step; andP.H. Tu et al. / Artificial Intelligence 175 (2011) 79–11987• ph(l, T ) is true if the fluent literal l possibly holds at time step T .We still use letters T , F , L, E, and A (possibly indexed) to represent variables of sorts time step, fluent, fluent literal,elementary action, and action correspondingly. π (D) includes(1) the rules (4)–(6) and (8)–(10) from the program lp(D); and(2) additional rules defined as follows.(a) For each dynamic causal law (1) in D, π (D) contains the rulede(l, T ) ← o(e, T − 1), h(ψ, T − 1),T > 0(12)This rule encodes a direct effect of an elementary action e at the time step T . It says that if e occurs at time stepT − 1 and the precondition ψ holds then the fluent literal l is a direct effect of e.Since the agent’s knowledge about the state of the world at a time step might be incomplete, we add to π (D) therule to define what possibly holds after the execution of an action e at the time step T − 1:ph(l, T ) ← o(e, T − 1), not h(¬ψ, T − 1), not de(¬l, T ),T > 0(13)This rule says that a fluent literal l possibly holds after the execution of an elementary action e if (i) e occurs at theprevious time step; (ii) there exists a dynamic causal law (1) for e such that the precondition ψ possibly holds atthe previous step; and (iii) ¬l is not a direct effect of some action occurring in the previous time step.(b) For each static causal law (2) in D, π (D) contains the rule:ph(l, T ) ← ph(ψ, T )This rule states that if ψ possibly holds at T then so does l.(c) In addition, π (D) contains the following ruleph(L, T ) ← not h(¬L, T −1), not de(¬L, T ),T > 0(14)(15)This rule completes the definition of the predicate ph. It defines what possibly holds by inertia: a fluent literalpossibly holds if (i) it possibly holds at the previous time step; and (ii) its negation is not a direct effect of anaction occurring in the previous time step.(d) Finally, the inertial law is encoded in π (D) as follows:h(L, T ) ← not ph(¬L, T ),T > 0which says that L holds at the time moment T > 0 if its negation does not possibly hold at T .For an action a and a partial state δ, letΠ(a, δ) = π (D, 1) ∪ h(δ, 0) ∪ o(a, 0)Then, the program Π(a, δ) has the following property.(16)(17)Proposition 1. Let δ be a partial state and a be an action. If Π(a, δ) is consistent then it has a unique answer set B and δ(cid:4) = {l |h(l, 1) ∈ B} is a partial state.Proof. See Appendix A.1. (cid:2)We define a transition diagram, called T lp(D), based on the program π (D) as follows.Definition 8. Let T lp(D) be a transition diagram such that(1) nodes of T lp(D) are partial states of D and arcs of T lp(D) are labeled with actions of D, and(2) (cid:3)δ, a, δ(cid:4)(cid:5) ∈ T lp(D) iff Π(a, δ) is consistent and δ(cid:4) = {l | h(l, 1) ∈ B} where B is the answer set of Π(a, δ).The next theorem states that T lp(D) is indeed an approximation of T (D) and furthermore it is deterministic.Theorem 3. T lp(D) is a deterministic approximation of T (D).88P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Proof. See Appendix A.2. (cid:2)At this point, it is instructive to mention that an approximation for action theories with static causal laws has beenintroduced in [66]. This approximation is an extension of the 0-approximation in [57]. The approximation defined in thispaper is also an extension of the 0-approximation in [57]. Indeed, the following result holds (similar to Theorem 4.6 in[65]).Observation 3.2. Let D be action theory without static causal laws, δ be a partial state, and a be an action executable in δ.Then, (cid:3)δ, a, δ(cid:4)(cid:5) ∈ T lp(D) if and only if δ(cid:4) = Φ0(a, δ) where Φ0 denotes the 0-approximation in [57].While the approximations in [57,62,66] are proposed in the context of a given action language, the approximationproposed in this paper, first published in [61], is applicable for action languages whose semantics can be specified by atransition diagram. We note that the approximation proposed in this paper and the one in [66] deal with action theo-ries with static causal laws and the one in [57] does not. Furthermore, only the approximation proposed in this paper isapplicable to transition diagrams which allow parallel actions.The approximation in [66] is based on the idea of computing what can possibly change after an action is executed.A fluent literal l possibly changes its value after the execution of an action e if (a) it is a direct effect of e; or (b) thereexists some dynamic law [e causes l if ψ] such that ψ possibly changes; or there exists some static causal law [l if ψ] suchthat ψ possibly changes. Furthermore, the approximation in [66] does not consider action theories with parallel actions.As such, instead of computing what possibly holds (rules (13), (14), and (15)) and employing this result in the inertial law(rule (16)), the planner in [66] implements rules for computing what possible changes as follows5:pc(l, T ) ← o(e, T − 1), not h(¬ψ, T )pc(l, T ) ← not h(l, T − 1), pc(lh(l, T ) ← h(l, T − 1), not pc(¬l, T )(cid:4), T ), not de(¬ψ, T )The first rule is for a dynamic causal law of the form [e causes l if ψ] and the second rule is for a static causal law of(cid:4) ∈ ψ . The last rule encodes the inertial law. It is worth mentioning that the rule encoding thethe form [l if ψ] with linertial law (16) does not include the atom h(l, T − 1), i.e., it is applicable to fluents whose value is unknown (w.r.t. theapproximation) before the execution of an action. For this reason, we now favor this approximation over the one developedin [66].Observe that T lp(D) and the approximation in [66] are incomparable in the sense that T lp(D) sometimes entails someconclusions that could not be derived using the approximation in [66] and vice versa. For instance, for the theoryD2 = {a causes ¬h, ¬ f if g}we have that (cid:3){ f }, a, {¬h}(cid:5) ∈ T lp(D) (or {¬h} is the partial state resulting from the execution of a in { f } according to theapproximation T lp ) whereas { f , ¬h} is the partial state resulting from the execution of a in { f } according to [66]. On theother hand, for the theory(cid:6)(cid:10)D3 =a causes fg if f , ¬ha causes g if kk if fg if f , hp if g, qwe have that (cid:3){¬ f , ¬g, ¬p, ¬q}, a, { f , ¬p, ¬q, k}(cid:5) ∈ T lp(D3) while { f , k, ¬q} is the partial state resulting from the executionof a in {¬ f , ¬g, ¬p, ¬q} according to [66].4. An approximation based conformant plannerLet P = (cid:3)D, δ0, δ f (cid:5) be a planning problem. Observation 3.1 implies that for any (deterministic) approximation T(cid:4)(D) of, then α is a solution of P . Because T lp(D) is aT (D), if α is a chain of events such that (cid:3)δ0, α, δ(cid:4)(cid:5) ∈ Tdeterministic approximation of T (D), we consider the following decision problem.(cid:4)(D) and δ f ⊆ δ(cid:4)Conformant planning with respect to T lp(D): Given a planning problem P = (cid:3)D, δ0, δ f (cid:5), determine whether P has a solu-tion with respect to T lp(D).The following complexity result is similar to Theorem 1 in [66].Theorem 4. The conformant planning problem with respect to T lp(D) is NP-complete.5 We adapt the encoding style used in this paper in encoding the approximation in [66].P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–11989The fact that T lp(D) is deterministic also allows us to use the program π (D) to compute solutions of P . In this section,we describe how to construct a logic program from π (D) for this purpose. This logic program, denoted by π (P, n), has twoinput parameters: a planning problem P and an integer n. The answer sets of π (P, n) contain solutions of length n of P .Like π (D), the signature of π (P, n) includes terms corresponding to fluent literals and actions of D. Rules of π (P, n)include all the rules of π (D, n) and the following rules:(1) Initial partial state encoding: we add to π (P, n) the following facts to describe the initial partial state(cid:5)(cid:4)δ0, 0hNote that the above is a shorthand for the set of facts {h(l, 0) | l ∈ δ0}.(2) Goal encoding: for each l ∈ δ f , π (P, n) contains the constraint:← not h(l, n)This set of constraints makes sure that every fluent literal in δ f holds in the final state.(3) Action generation rule: π (P, n) contains the following rules for generating action occurrences:o(E, T ) ∨ ¬o(E, T ) ← T < n← not o(A, T ),T < n(18)(19)(20)(21)(Recall that A is the set of all actions.) These rules state that at any time step T < n at least one action occurs.6The following theorem shows that we can use π (P, n) to find solutions of P .Theorem 5. Let C be an answer set of π (P, n) and let ai = {e | o(e, i) ∈ C} (0 (cid:2) i < n). Then, α = (cid:3)a0, . . . , an−1(cid:5) is a solution of P .Proof. See Appendix B. (cid:2)It is worth noticing that the program π (P, n) is similar to the one presented in [58] in that each of its answer setscorresponds to a solution of the planning problem P . Nevertheless, there are two important differences between π (P, n)and the program in [58]. π (P, n) can deal with incomplete initial situation and also considers parallel actions. None of theseaspects were considered in [58]. The program π (P, n) is strongly related to the planner in [66]. They differ from each otherin that they implement different approximations and π (P, n) can deal with parallel actions and the one in [66] cannot.Theorem 5 implies that each answer set of π (P, n) corresponds to a solution of length n of P . To find minimal solutions,we run the program π (P, n) with n = 0, 1, . . . sequentially until it returns an answer set (i.e., the first n such that π (P, n)is consistent). The chain of events corresponding to this answer set is a minimal solution of P . This framework is hereafterreferred to as the planner CPasp.7As will be seen in Section 8, CPasp can solve a wide range of planning problems. However, it is incomplete, i.e. thereare some planning problems for which a solution exists but cannot be found by CPasp. A precise definition of the (in-)completeness of CPasp is given below.Definition 9 (Completeness and incompleteness of CPasp). Let P be a planning problem. We say that CPasp is complete withrespect to P if either (i) P does not have a solution; or (ii) P has a solution and there exists an integer n such that π (P, n)is consistent. Otherwise, we say that CPasp is incomplete with respect to P .Observe that Theorem 5 shows that if P does not have a solution then and π (P, n) is inconsistent for every n. One ofthe main reasons for the incompleteness of CPasp is the inability of the program π (P, n) to do reasoning by cases. Thefollowing example demonstrates this issue.6 An alternative for this set of rules is a choice rule(cid:2)o(E, T ): action(E)(cid:3)1← T < nwhich were introduced in [55].7 CPasp stands for Conformant Planning using Answer Set Programming.90P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Example 6. Consider the action theory D4 with two dynamic causal lawse causes f if ge causes f if ¬gLet P4 = (cid:3)D4, ∅, { f }(cid:5). Clearly (cid:3)e(cid:5) is a solution of P4 because either g or ¬g is true in any state belonging to comp(∅) andthus one of the above dynamic causal laws would take effect when e is performed. Yet, it is easy to verify that this solutioncannot be generated by CPasp due to the fact that for every n, π (P4, n) does not have any answer set (Constraint (19)cannot be satisfied).The next example shows that not only conditional effects but also static causal laws can cause CPasp to be incomplete.Example 7. Consider the action theory D5:e causes fg if f , hg if f , ¬hWe can check that (cid:3)e(cid:5) is a solution of the planning problem P5 = (cid:3)D5, {¬ f , ¬g}, {g}(cid:5) since e causes f to be true and thetwo static causal laws make g become true whenever fis true.Now suppose that the program π (P5, 1) has an answer set C which corresponds to the plan (cid:3)e(cid:5). This implies thato(e, 0) ∈ C . Then, because of the rule (12), we have that de( f , 1) ∈ C . Furthermore, any atom of the form de(. . . , . . .) belongsto C if and only if it is the head of a ground instance of the rule (12). Thus, the only atom of the form de(. . . , . . .) in C isde( f , 1) ∈ C .By rule (13), this implies that ph( f , 1) ∈ C . By rule (15), ph(¬g, 1), ph(h, 1), ph(¬h, 1) all belong to C . By rule (16),because ph(¬g, 1) ∈ C , h(g, 1) cannot belong to C . As a result, constraint (19) is not satisfied. This is a contradictionbecause C must satisfies all the constraints of π (P5, 1).Hence, π (P5, 1) does not have any answer set. In fact, we can verify that for any integer n, π (P5, n) does not have ananswer set. This implies that CPasp cannot find a solution of P5.The above examples raise a question about the applicability of CPasp: what kind of planning problems can CPasp solve?Observe that the action theories presented in Examples 6 and 7 are rather artificial and are not likely to come up in thespecification of real-world domains. In fact, the theories in Examples 6 and 7 can be simplified to D(cid:4)= {e causes f } and4D(cid:4)= {e causes f } ∪ {g if h} respectively. It is easy to see that CPasp is complete with respect to the simplified domains. In5[59], we developed a transformation that removes such artificial situations. It is worth to mention that CPasp can solve allbenchmark problems with non-disjunctive initial state that we have encountered so far.5. A sufficient condition for the completeness of CPASPIn this section, we present our initial study of the completeness of CPasp. Specifically, we introduce a class of planningproblems, called simple planning problems (Definition 14) with respect to which CPasp is complete. For convenience, givenan action theory D, for a set S of states and action a, by Res(a, S) we denote the set of possible successor states of statesin S after the execution of a, i.e.,Res(a, S) =(cid:2)(cid:4)s(cid:11)(cid:11) s ∈ S,(cid:8)(cid:9)(cid:4)(cid:3)∈ T (D)s, a, sFor a chain of events α, by Res(α, S) we denote the set of possible states reachable from some state in S after the executionof α, i.e.,Res(α, S) =(cid:2)(cid:4)s(cid:11)(cid:11) s ∈ S,(cid:8)(cid:9)(cid:4)(cid:3)∈ T (D)s, α, sDefinition 10 (Simple action theories). A static causal law is simple if its precondition contains at most one fluent literal. Anaction theory D is simple if each of its static causal laws is simple.By this definition, action theories without static causal laws are simple.8 Furthermore, we observe that many real worldstatic causal laws are simple in nature. For example, to represent the unique location of a robot at a time, we can use thefollowing collection of static causal laws:8 This implies that all benchmarks used in the international planning competitions involve only simple action theories as static causal laws are not usedin their representation.(cid:2)¬at(X) if at(Y )(cid:11)(cid:11) X (cid:7)= Y(cid:3)P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–11991Likewise, to represent the unique value of a multi-valued object obj, we can use:(cid:2)¬value(obj, X) if value(obj, Y )(cid:11)(cid:11) X (cid:7)= Y(cid:3)Observe that simple action theories can be translated into action theories without static causal laws by• computing S(l) = {h | there exists a sequence l0 = l, . . . , ln = h such that li if li−1 for 1 (cid:2) i (cid:2) n}, for each fluent literal l,and• replacing a dynamic law [a causes l if ϕ] with the set of dynamic laws(cid:2)a causes h if ϕ(cid:11)(cid:3)(cid:11) h ∈ S(l)A main disadvantage of this approach lies in that the number of dynamic laws might increase quadratically in the numberof static laws.To characterize situations in Examples 6–7, we define a notion of dependency between fluent literals.Definition 11 (Dependencies between literals). A fluent literal l depends on a fluent literal g, written as l (cid:16) g, if one of thefollowing conditions holds.(1) l = g.(2) There exists a (dynamic or static) causal law of D such that l is the head and g belongs to the precondition of the law.(3) There exists a fluent literal h such that l (cid:16) h and h (cid:16) g.(4) The complementary of l depends on the complementary of g, i.e., ¬l (cid:16) ¬g.Note that the dependency relationship between fluent literals is reflexive, transitive but not symmetric. The next defini-tion is about the dependence between actions and fluent literals.Definition 12 (Dependencies between actions and fluent literals). An action b depends on a fluent literal l, written as b (cid:16) l, if(1) there exists an impossibility condition (3) such that a ⊆ b and ¬l ∈ ψ , or(2) there exists a fluent literal g such that b (cid:16) g and g (cid:16) l.For a set of fluent literals σ and a fluent literal l, we write l (cid:16) σ to denote that l (cid:16) g for some g ∈ σ and l (cid:2) σ to denotethat there exists no g ∈ σ such that l (cid:16) g.The next definition characterizes when a set of states S representing the possible states of the world can be reduced toa single partial state δ.Definition 13 (Reducibility). Let S be a set of states, δ be a partial state, and σ be a set of fluent literals. We say that S isreducible to δ with respect to σ , denoted by S (cid:17)σ δ if(1) δ is a subset of every state s in S,(2) for any fluent literal l ∈ σ , there exists a state s ∈ S such that l (cid:2) (s \ δ), and(3) for any action a, there exists a state s ∈ S such that a (cid:2) (s \ δ).There are two interesting properties about the reducibility of a set of states. First, if S (cid:17)σ δ and S represents the set ofpossible states of the world then to reason about (formulae composed from) σ , it suffices to know δ.Proposition 2. Let D be a simple action theory, S be a set of states, δ be a partial state, and σ be a set of fluent literals such thatS (cid:17)σ δ. Then,(cid:12)s ∩ σ = δ ∩ σs∈SProof. See Appendix C.1. (cid:2)The reducibility of a set of states is preserved along the course of the execution of actions.Proposition 3. Let D be a simple action theory, S be a set of states, δ be a partial state, and σ be a set of fluent literals such thatS (cid:17)σ δ. For any action a, if a is executable in S then92P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119(1) a is safe in δ,(2) Res(a, S) (cid:17)σ δ(cid:4)where (cid:3)δ, a, δ(cid:4)(cid:5) ∈ T lp(D).Proof. See Appendix C.2. (cid:2)The second property can be extended to a chain of events α as follows.Proposition 4. Let D be a simple action theory, S be a set of states, δ be a partial state, and σ be a set of fluent literals such thatS (cid:17)σ δ. For any chain of events α, if α is safe in S then(1) α is safe in δ(2) Res(a, S) (cid:17)σ δ(cid:4)where (cid:3)δ, a, δ(cid:4)(cid:5) ∈ T lp(D).Proof. See Appendix C.3. (cid:2)We define a class of planning problems, called simple planning problems, as follows.Definition 14 (Simple planning problems). A planning problem (cid:3)D, δ0, δ f (cid:5) is simple if(1) D is simple, and(2) comp(δ0) (cid:17)δ f δ0.The following theorem states that for the class of simple planning problems the planner CPasp is complete.Theorem 6. Let P = (cid:3)D, δ0, δ f (cid:5) be a planning problem. If P is simple then CPasp is complete with respect to P .Proof. See Appendix C.4. (cid:2)We would like to conclude this section by relating our completeness condition for approximation based reasoning inaction theories with incomplete initial state to other works. The proposed condition is strongly related to the result in [60],in which a completeness condition was developed for action theories without static causal laws.9 Observe that the proofsof this result in [60] make use of different techniques while the proofs of the completeness result in this paper relies ontechniques developed by the logic programming community.In [49], Palacios and Geffner presented several translations that convert a planning problem P = (cid:3)D, δ0, δ f (cid:5) into a clas-sical planning problem P (cid:4) = (cid:3)D(cid:4), γ 0, γ f (cid:5) (γ 0 is complete) whose solutions can be computed using classical planners (e.g.FF). A translation might introduce new fluents and actions. Roughly, the completeness of a translation depends on the setof new fluents and actions. The authors of [49] identified a class of complete translations. They can be characterized by thewidth of the problem, which depends on a relevance relation between literals. This relation is similar to the dependencyrelation between literals in Definition 11. Similar to [60], the approach in [49] does not deal with action theories with staticcausal laws. Nor does it consider parallel actions.The determinicity of T lp(D) and the fact, that computing the result of the execution of an action in a given partial statecan be done in polynomial time (in the size of the theory), imply that computing the result of the execution of an actionsequence from a given state can be done in polynomial time. In other words, the temporal projection problem [30] in theclass of simple action theories is tractable. This result is similar to the result developed by Liu and Levesque in [39]. As withother works, the work [39] does not deal with static causal laws or parallel actions.6. A logic programming based planner for disjunctive initial stateBesides being sometimes incomplete, another weakness of CPasp is that it does not consider planning problems withdisjunctive information about the initial state. We call such problems disjunctive planning problems and formally define themas follows.Definition 15 (Disjunctive planning problems). A disjunctive planning problem P is a tuple (cid:3)D, Δ0, δ f (cid:5) where D is an actiontheory, Δ0 is a non-empty set of partial states and δ fis a partial state.9 This result is similar to Theorem 4 in [60]. Theorem 3 in [60] is incorrect but this does not invalidate Theorem 4. A corrected version of Theorem 3 canbe found in [65].P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–11993It is easy to see that a planning problem in Definition 5 is a special case of a disjunctive planning problem where Δ0 isa singleton set. Solutions of a disjunctive planning problem are defined as follows.Definition 16. A chain of events α = (cid:3)a0, . . . , an−1(cid:5) is a solution of a disjunctive planning problem P = (cid:3)D, Δ0, δ f (cid:5) if forevery δ0 ∈ Δ0, α is a solution of the planning problem (cid:3)D, δ0, δ f (cid:5).Example 8. Consider the domain(cid:6)D6 =e causes f if ge causes f if hand let Δ0 = {{g}, {h}} and δ f = { f }. Then, P6 = (cid:3)D6, Δ0, δ f (cid:5) is a disjunctive planning problem. By Definition 16, it is easyto see that α = (cid:3){e}(cid:5) is a solution of P6.It turns out that the framework in the previous section can be naturally extended to find solutions of a disjunctiveplanning problem P . First, we extend the program π (D) to deal with explicit disjunctive information about the initialstate; the newly extended program will be referred to as Γ (D). Then, we construct a program Γ (P, n) from Γ (D) so thatevery answer set of Γ (P, n), if exists, represents a solution of P .The basic idea in the development of Γ (D) is based on the approach in [66]. We add a second constant to Γ (D), calledworlds, to denote the number of initial partial states in Δ0, i.e., worlds = |Δ0|. With the exception of the predicate o( A, T )and the auxiliary predicates (e.g., literal(L), fluent(F ), etc.), every other predicate of π (D) is modified to accept a thirdparameter W to indicate the possible world that the reasoner may be in during the execution of the plan, in the followingway:• h(l, T , W ) is true if the fluent literal l holds at time-step T in the world W ;• de(l, T , W ) is true if the fluent literal l is a direct effect of an action that occurs at the previous time step in theworld W ; and• ph(l, T , W ) is true if fluent literal l possibly holds at time step T in the world W .The rules of Γ (D) are obtained from the rules of π (D) by replacing predicates h(l, T ), de(l, T ) and ph(l, T ) with thenew predicates h(l, T , W ), de(l, T , W ) and ph(l, T , W ) respectively. For example, the rule (4) will becomeh(l, T + 1, W ) ← o(e, T ), h(ψ, T , W )Similarly, the rule (13) becomesph(l, T + 1, W ) ← o(e, T ), not h(¬ψ, T , W ), not de(¬l, T + 1, W )As before, we use the notation Γ (D, n) to denote the restriction of Γ (D) to the time step to take value between 0 and n.Suppose Δ0 = {δ0, . . . , δk−1}. The program Γ (P, n) is constructed as follows:(1) The value of the constant worlds is k.(2) The set of rules of Γ (P, n) includes(a) the rules of Γ (D, n), and(b) for each δi ∈ Δ0, the ruleh(δi, 0, i)(c) for each l ∈ δ f and i ∈ {1, . . . , k}, the rule← not h(l, n, i)The following proposition establishes the relationship between Γ (P) and Π(P).Proposition 5. Let P = (cid:3)D, {δ0}, δ f (cid:5) and P (cid:4) = (cid:3)D, δ0, δ f (cid:5). A set of atoms A is an answer set of Γ (P, n) iff there exists an answer setB of π (P (cid:4), n) such that(1) for every fluent literal l, h(l, i, 0) ∈ A iff h(l, i) ∈ B, and(2) for every elementary action e, o(e, i, 0) ∈ A iff o(e, i) ∈ B.Theorem 7. Let P = (cid:3)D, Δ0, δ f (cid:5) be a disjunctive planning problem. If A is an answer set of Γ (P, n) then α = (cid:3)a0, . . . , an−1(cid:5), whereai = {e | o(e, i) ∈ A}, is a solution of P .94P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Res(D,a,δ)Input: An action theory D, an action a, and a partial state δOutput: successor partial state of δ1. Beginpde = ∅2. de = ∅3. for each dynamic causal law (1) in D doif ψ holds in δ then de = de ∪ {l}4.if ψ possibly holds in δ then pde = pde ∪ {l}5.6. ph = Closure(D, (pde ∪ (lit \ ¬δ)) \ ¬de)7. return Closure(D, de ∪ (lit \ ¬ph))8. Endlit = F ∪ ¬FFig. 2. An algorithm for computing the transition function of T lp .Closure(D,σ )Input: An action theory D and a set of fluent literal σOutput: ClD(σ )1. Begin2. σ1 = σ2 = σ3. repeat4.5.6.7.8.9. until fixpoint10. return σ111. Endfixpoint = truefor each static causal law (2) in D doif ψ holds in σ1 and l /∈ σ2 thenfixpoint = falseσ2 = σ2 ∪ {l}σ1 = σ2Fig. 3. Computing the closure of a set of fluent literals.Proposition 5 is intuitive and it is not difficult to prove its correctness. The proof of Theorem 7 is similar to the proof ofTheorem 5 in Appendix B. Hence, we omit the proofs of both of them here for brevity.7. Heuristic search through T lpIn the previous sections, we demonstrated the usefulness of approximations in planning in the logic programmingparadigm. The experiments (Section 8) show that CPasp is really good in planning with parallel actions and in domainsrich in static causal laws. However, it does not perform well in domains with large grounded representation. One of themain reasons for this problem is because the current answer set solvers do not scale up well to programs that require largegrounded representation. Various approaches have been proposed to attack this issue from different perspectives [9,20]. Tofurther investigate the usefulness of approximations, we implemented a C++ sequential planner, called CpA, based on thetransition diagram induced by T lp . CpA employs the best first search strategy with repeated state avoidance and the num-ber of fulfilled subgoals as the heuristic function. The initial state is described as a CNF formula and thus CpA allows fordisjunctive information as well.One of the main functions inside CpA is to compute the transition function specified by T lp . The algorithm for thisfunction is presented in Fig. 2. It takes as input an action theory D, an action a and a partial state δ and returns as outputthe successor partial state of δ. Note that because T lp is deterministic, such a successor partial state exists and is unique,provided that a is safe in δ. In the algorithm, variables de and pde denote the set of direct effects and the set of possibledirect effects respectively; ph is the set of fluent literals that possibly holds in the successor partial state; and lit is the setof all fluent literals. The only for loop is to compute the sets of direct effects and possible direct effects.The algorithm makes two calls to the Closure function (depicted in Fig. 3). The Closure function takes as input an actiontheory D and a set of fluent literals σ and returns as output ClD(σ ). This process is performed in the following steps. First,the closure is set to σ . Then the function loops over the static causal laws, adding to the closure the head of static causallaws whose bodies hold in the previous iteration. The loop terminates when no more fluent literals can be added to theclosure.The correctness of the algorithm is stated in the following theorem.Theorem 8. Let D be an action theory, a be an action, and δ be a partial state. If a is safe in δ then Res(D, a, δ) = δ(cid:4)T lp(D).iff (cid:3)δ, a, δ(cid:4)(cid:5) ∈Proof. See Appendix D. (cid:2)P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–11995We would like to note that CpA also deals with disjunctive information about the initial state. CpA employs an explicitDNF representation of the initial state. Given an CNF representation of the initial state,10 CpA converts it into its internalDNF representation. Naturally, this might be very time consuming and could affect the planner’s performance. Finding animplementation of CpA that does not require an DNF representation of the initial state is an interesting topic but is beyondthe scope of this paper and is considered as one of our goals in the future.8. ExperimentsIn this section, we will present our experimental evaluation of the performance of CPasp and CpA. The platform used fortesting CPasp is a 2.4 GHz CPU, 768 MB RAM machine, running Slackware 10.0 operating system. Experiments with CpA andsequential planners are conducted on a 3.2 GHz CPU, 2 GB RAM, running Suse Linux 9.0 operating system. Every experimenthad a time limit of 30 minutes.8.1. Evaluation of CPasp8.1.1. Planning systems usedWe compared CPasp with three other conformant planners: CMBP [15], DLVK [18], and C-Plan [14]; We selected theseplanners because they are designed in spirit similar to CPasp (that is, a planning problem is translated into an equivalentproblem in a more general setting which can be solved by an off-the-shelf software system) and can directly deal withstatic causal laws. The main difference between these planners and CPasp lies in the use of different types of reasoningin searching for plans. CMBP, DLVK, and C-Plan use the possible world semantics whereas CPasp uses the approximation.A brief overview of these planners is given below.• CMBP (Conformant Model Based Planner): CMBP is a conformant planner developed by Cimatti and Roveri [15]. CMBPemploys BDD (Binary Decision Diagram) techniques to represent planning domains and search for solutions. CMBPallows non-deterministic domains with uncertainty in both the initial state and action effects. However, it does nothave the capability of generating concurrent plans. The input language is the action language AR [28]. The versionused for testing was downloaded from http://www.cs.washington.edu/research/jair/contents/v13.html.• DLVK: DLVK is a declarative,logic-programming-based planning system built on top of the DLV system (http://www.dbai.tuwien.ac.at/proj/dlv/). Its input language K is a logic-based planning language described in [18]. The ver-sion used for testing was downloaded from http://www.dbai.tuwien.ac.at/proj/dlv/K/. DLVK is capable of generating bothconcurrent and conformant plans.• C-Plan: C-Plan is a SAT-based conformant planner. C-Plan works using a generate-and-test method. The input languageis the action language C [21,29]. C-Plan is primarily designed for generating concurrent plans.8.1.2. BenchmarksWe prepared two test suites. The first test suite contains sequential, conformant planning benchmarks and the secondcontains concurrent, conformant planning benchmarks. Many benchmarks are taken from the literature (e.g. [12,14,18])and some were developed for the international planning competition (e.g. [17]). To test the performance of our systems indomains with static causal laws, we developed two simple domains which are rich in static causal laws.Sequential benchmarks. The sequential benchmark test suite includes the following domains:• BT(m, n): This domain is a variant of the well-known Bomb-In-the-Toilet domain. In this domain, there are m packagesand n toilets. One of the packages contains a bomb. The bomb can be disarmed by dunking the package that containsit into a toilet. The goal is to disarm the bomb.• BTC(m, n): This domain is similar to BT. However, we assume that dunking a package into a toilet will clog the toilet;flushing the toilet will make it unclogged.• RING(n): The encoded version of this problem follows the encoding in the distribution of CFF. In this domain, the agentcan move (forward or backward) around a building with n-rooms arranged in a ring in a cyclic fashion. Each room hasa window which can be closed or open. Closed windows may be locked. Initially, neither the location of the agent northe states (open/locked) of the windows is known. The goal is to have all windows locked. A possible conformant planconsists of performing actions forward, close, lock repeatedly. Notice that the initial location of the agent needs to berepresented as a disjunction.Observe that the location of the agent satisfies the well-known static causal law stating that an agent cannot be in twoplaces at the same time. We therefore created a domain, called RING-C(n), by introducing this static causal law.We should remark here that this domain is used in different ways in our experiments. In testing CPasp, we assume thatthe location of the agent is known in the initial state. This is because CPasp does not deal with disjunctive information.On the other hand, we assume that the location of the agent is unknown in the initial state when testing CpA.10 In PDDL, this is specified by oneof- and or-clauses.96P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119• Domino(n): We have n dominoes standing on a line in such a way that if one of them falls then the domino on itsright side also falls. There is a ball hanging close to the leftmost one. Touching the ball causes the first domino to fall.Initially, the states of dominoes are unknown. The goal is to have the rightmost domino fall.• Gaspipe(n): The objective of this domain is to start a flame in a burner which is connected to a gas tank through apipe line. The gas tank is on the left-most end of the pipe line and the burner is on the right-most end. The pipe lineis made of sections connected with each other by valves. The pipe sections can be either pressured by the tank orun-pressured. Opening a valve causes the section on its right side to be pressured if the section to its left is pressured.Moreover, for safety reasons, a valve can be opened only if the next valve on the line is closed. Closing a valve causesthe pipe section on its right side to be un-pressured. Static causal laws are useful in the representation of this domain.One such law is that if a valve is open and the section on its left is pressured then the section on its right will bepressured. Otherwise (either the valve is closed or the section on the left is un-pressured), the pipe on the right side isun-pressured. The burner will start a flame if the pipe connecting to it is pressured. The gas tank is always pressured.The uncertainty in the initial situation is that the states of the valves are unknown. A possible conformant plan will beto close all valves except the first one (that is, the one that directly connects to the gas tank) from right to left andthen opening them from left to right.• Cleaner(n, p): This domain is a modified version of the Ring domain in which instead of locking windows, the goal ofthe agent is to clean multiple objects located in every room (there are p objects in each room). To contrast with theRing domain, we assume that initially, the agent is in the first room and does not know whether or not any of theobjects are cleaned.Concurrent benchmarks. There are four domains in this test suite, namely BTp , BTCp , Gaspipep and Cleanerp . The BTp andBTCp domains are modifications the BT and BTC domains respectively in which we allow to dunk different packages intodifferent toilets at the same time. The Gaspipep domain is a modification of the Gaspipe domain, which allows to closemultiple valves at the same time. In addition, it is possible to open a valve while closing other valves. However, it is notallowed to open and close the same valve or open two different valves at the same time. The final domain in the test suite,Cleanerp , is a modified version of the Cleaner domain where we allow the robot to concurrently clean multiple objects inthe same room. Modifications to the Ring and Domino domain are not considered as these problems only have sequentialsolutions.8.1.3. Experimental resultsWe ran CPasp using the answer set solvers smodels [55] and cmodels [33] and observed that cmodels yielded betterperformance in general. The running times of CPasp reported here were obtained using cmodels. The timing results forthe sequential benchmarks is shown in Tables 1 and 2, and the results for the concurrent benchmarks is shown in Table 3.We did not test C-Plan on the sequential planning benchmarks since it was developed only for concurrent planning. Inthese tables, times are shown in seconds; The “PL” column shows the length of the plan found by the planner. “−” denotesthat the planner did not return a solution within the time limit for some reasons: e.g., out of time or out of memory. “NA”denotes that the problem was not run with the planner. Since both DLVK and CPasp require as an input parameter thelength of a plan to search for, we ran them inside of a loop in which we incrementally increase the plan length to searchfor, starting from 1,11 until a plan is found. Notice that in this way CPasp is not only finding conformant plans but minimalconformant plans with respect to the defined approximation. For example, C-Plan and CPasp took 0.74 second and 2.72second, respectively, to find the solution for the BTC(6,2) problem. Nevertheless, CPasp’s solution is shorter.As it can be seen in Table 1, in the BT and BTC domains CMBP outperforms both DLVK and CPasp on most probleminstances. In general, CPasp has better performance than DLVK on these domains. As an example, DLVK took more thanthree minutes to solve BT(6,2), while it took only 0.77 seconds for CPasp to solve the same problem. In addition, within thetime limit, CPasp was able to solve more problems than DLVK. In the Ring domain, although outperformed by both CMBPand DLVK in some small instances, CPasp is the only planner that was able to solve Ring(8).CPasp works well with domains rich in static causal laws (Domino and Gaspipe). In the Domino domain, CPasp outper-forms all the other planners in most of instances. It took only 2.41 seconds to solve Domino(2000), while both DLVK andCMBP took more than one minute. In fact CPasp could scale up very well to larger instances, e.g., Domino(10000). In theGaspipe domain, CPasp also outperforms DLVK: it was able to solve all the problem instances while DLVK was able to solveonly the first three problem instances.12The Cleaner domain turns out to be hard for the planners: they could solve very small instances only. In this domain,CPasp is outperformed by CMBP. To solve the Cleaner(6, 2), CMBP took only 4.1 seconds while CPasp took more than 3minutes. However, CPasp performs better than DLVK in general: DLVK reported a timeout with the problem Cleaner(6, 2).We have seen that CPasp can be competitive with CMBP and DLVK on the sequential benchmarks. Let us move ourattention now to the concurrent benchmarks. As can be seen from Table 3, CPasp outperforms both DLVK and C-Plan onmost instances of the BTp , BTCp , and Gaspipep domains. Furthermore, CPasp is the only planner that was able to solve all11 We did not start from 0 because none of the benchmarks has a plan of length 0.12 We tried to test this domain with CMBP but had some problem with the encoding. We contacted the author of CMBP and are still waiting for response.P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–11997Table 1Sequential benchmarks: Bomb & Ring domains.ProblemCMBPBT(2, 2)BT(4, 2)BT(6, 2)BT(8, 4)BT(10, 4)BTC(2, 2)BTC(4, 2)BTC(6, 2)BTC(8, 4)BTC(10, 4)Ring(2)Ring(4)Ring(6)Ring(8)Ring(10)PL246810261051117Time0.030.170.210.631.50.160.260.74––0.060.100.48––Table 2Sequential benchmarks: Domino, Gaspipe & Cleaner domains.ProblemDomino(100)Domino(200)Domino(500)Domino(1000)Domino(2000)Domino(5000)Domino(10000)Gaspipe(3)Gaspipe(5)Gaspipe(7)Gaspipe(9)Gaspipe(11)Cleaner(2, 2)Cleaner(2, 5)Cleaner(2, 10)Cleaner(4, 2)Cleaner(4, 5)Cleaner(4, 10)Cleaner(6, 2)Cleaner(6, 5)Cleaner(6, 10)CMBPPL1111115111117Time0.261.797.9213.2066.60559.46–NANANANANA0.10.610.134.1–––––DLVKPL24626511DLVKPL11111591351111Time0.040.55216.55––0.1272.44––––––0.102.14Time0.10.352.4013.1062.42––0.130.4242.62––0.104214.69–14.82–––––CPaspPL2468102685111723CPaspPL1111111591317215111117Time0.200.410.776.73890.060.220.712.72––0.522.9844.431424.07–Time0.210.280.741.232.416.0712.5841.342.226.1839.32868.100.493.88–2.09––224.39––the instances in the test suite. In the Cleaner domain, C-Plan is the best. To solve the Cleanerp (6, 10) problem, C-Plan tookonly 0.35 seconds, whereas DLVK reported a timeout and CPasp needed 3.73 seconds. As CMBP does not produce concurrentplans, it is not included in this table.8.2. Evaluation of CpA8.2.1. Planning systems usedWe compared CpA with four conformant planners: CFF [12], KACMBP [16], t0 [49], and POND [13]. These planners were,at the time of our experiments, known as the fastest conformant planners on most of the conformant planning benchmarksin the literature.13 CFF [12] is superior to other state-of-the-art conformant planners like GPT [11], MBP [15]; KACMBP isreported [16] to outperform DLVK and C-Plan in many domains in the literature. t0 [49] is known to be better than CFFin many domains. We did not compare our planners with the PKS system [50] (improved in [51]) which employs a richerrepresentation language and the knowledge-based approach to reason about effects of actions in the presence of incomplete13 In a recent planning competition, an improved version of CpA won the first prize over t0 (see http://ippc-2008.loria.fr/wiki/index.php/Results). Theversion of CpA used in this paper, however, is the version used at the time of the original submission of this paper.98P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Table 3Concurrent benchmarks: Bomb, Gaspipe & Cleaner domains.ProblemBTp (2, 2)BTp (4, 2)BTp (6, 2)BTp (8, 4)BTp (10, 4)BTCp (2, 2)BTCp (4, 2)BTCp (6, 2)BTCp (8, 4)BTCp (10, 4)Gaspipep (3)Gaspipep (5)Gaspipep (7)Gaspipep (9)Gaspipep (11)Cleanerp (2, 2)Cleanerp (2, 5)Cleanerp (2, 10)Cleanerp (4, 2)Cleanerp (4, 5)Cleanerp (4, 10)Cleanerp (6, 2)Cleanerp (6, 5)Cleanerp (6, 10)C-PlanPL1232135333777111111Time0.070.051.814.32–0.050.077.51–––––––0.050.120.060.060.090.130.110.190.35DLVKPL1232135468103337771111Time0.070.093.0610.52–0.050.90333.27––0.080.170.4417.44–0.070.060.070.190.80237.634.47986.73–CPaspPL12323135354681012333777111111Time0.110.260.340.241.910.130.300.670.501192.450.400.751.223.178.830.260.300.300.770.931.161.982.943.73information as most planners used in the comparison are domain-independent planners. A brief overview of these plannersis given below.• KACMBP: KACMBP is an extension of CMBP. Similarly to CMBP, KACMBP uses techniques from symbolic model checkingto search in belief space. However, in KACMBP, the search is guided by a heuristic function which is derived based onknowledge associated with a belief state. KACMBP is designed for both sequential and concurrent settings. The inputlanguage of KACMBP is SMV. The system used in the experiments was downloaded from http://sra.itc.it/tools/mbp/AIJ04/.• Conformant-FF (CFF): CFF14 extends the classical FF planner [31] to deal with uncertainty in the initial state. The basicidea is to represent a belief state s just by the initial belief state (which is described as a CNF formula) together withthe action sequence that leads to s. In addition, the reasoning is done by checking the satisfiability of CNF formulae.The input language of CFF is a subset of the PDDL language with a minor change that allows the users to specify theinitial state as a CNF formula. CFF supports sequential conformant planning. However, it does not support concurrentand conditional planning.• POND: POND extends the planning graph algorithm [10] to deal with sensing actions. Conformant planning is alsosupported as a feature of POND. The input language is a subset of PDDL. The version for testing was obtained throughemail communication with Daniel Bryce, author of POND. This is the version 1.1.1. of POND.• t0 is the winner of the 2006 International Planning Competition, in the “Conformant Planning” category. t0 solves aplanning problem P by translating it into a classical planning problem P (cid:4). The earlier version of t0, called cf2cs(ff),[48] is incomplete but t0 [49] is complete. As with other planners (e.g., CFF, POND), t0 does not deal with static causallaws.8.2.2. BenchmarksWe evaluated the performance of the planning systems on the four domains tested with CPasp: Ring, Domino, Cleaner,and BTC. In addition, we use four other domains: Cube-Center (Cube-C), Logistic (Log), Sortnet, and UTS.The Ring, Domino, BTC and Cleaner domains have been described in the previous section. We remark that the initiallocation of the agent is unknown in the Ring domain for the experiments conducted in this section.The Cube-Center(2k + 1) domain is described in [16]. In this domain, an agent can move up, down, left, and right in athree-dimension grid of the size 2k + 1. A move results in the agent being in a corresponding cell (e.g., moving up, downchanges the y-coordinate of the agent) or being in the same cell (e.g., moving up when the y coordinate is 2k + 1 does notresult in any change). The goal of the agent is to be at the center of the Cube. Initially, the agent does not know its location.14 We would like to thank Jörg Hoffmann for providing us with an executable version of the system for testing.P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–11999Table 4Classical domains: Ring, Cube-Center, BTC, and Logistics.Prob.sizeRing(n)2345Cube-Center(n)357911BTC(p, t)10, 120, 150, 1100, 110, 520, 550, 5100, 510, 1020, 1050, 10100, 10Logistics(l, c, p)2, 2, 22, 3, 33, 2, 23, 3, 34, 3, 3KACMBPPLTime81522351425354555193999199153595195103090190143414400.000.010.020.030.040.160.160.320.360.010.040.352.520.060.180.963.940.170.552.9617.420.13161.130.120.19–CFFPL7152645633401939991991535951951030901901624203437Time0.010.122.1340.770.000.2822.62––0.010.062.1857.770.010.021.8253.490.000.031.5247.850.000.000.020.030.04PONDPL61316206152419399919915971951030941941630223837Time0.000.104.34247.450.5510.00162.85––0.000.020.192.050.01AB0.631.800.020.131.4517.970.071.490.4524.0734.32t0PL5813176152845481939991991535951951030901901624203436CpAPL581114624427381193999199153595195103090190104844350Time0.040.050.060.050.080.120.140.320.230.060.060.100.240.030.060.120.420.050.080.160.420.050.080.080.110.15Time0.000.050.402.050.109.82117.57797.841405.240.000.050.644.980.030.121.489.850.050.312.8816.840.117.433.33340.09–The Logistic(l, c, p) domain is described in [12] and distributed together with CFF. In this domain, we need to transportp packages between locations of c cities (each city has p locations) via trucks and airplanes. The uncertainty in the initialstate is that the exact locations of the packages are unknown in the beginning.The Sortnet(n) and UTS(n) are two domains from the 2006 International Planning Competition, downloaded from http://www.ldc.usb.ve/~bonet/. The Sortnet(n) domain is a synthesis of sorting networks which has disjunctive goals and a largenumber of possible initial states. The UTS(n) domain is the computation of universal transversal sequence for graphs. In thisdomain, the goal is to visited all the nodes. To do so, an agent must start traveling. Afterwards, he/she can visit neighbornodes. The uncertainty lies in the initial location of the agent.8.2.3. Experimental resultsThe experimental results are shown in Tables 4–7. Times are shown in seconds. “–” indicates that the planner did notreturn a solution within the time limit and “NA” indicates that the problem is not applicable. “AB” denotes that the programencountered some error and halted abnormally.As can be seen in Table 4, in the Ring domain, KACMBP performs the best. t0 is almost as good as KACMBP. CFF andPOND on the contrary did not perform well on this domain. As explained in [12], CFF does not perform well on this domainbecause of the lack of informativity of the heuristic function in the presence of non-unary effect conditions and the problemwith checking repeated states. Although CpA was able to solve all the instances, its performance is not as good as KACMBPor t0, e.g., to solve Ring(5), CpA needed more than two seconds, while KACBMP or t0 needed less than 0.05 seconds.A similar performance trend can be observed in the Cube-Center domain. t0 and KACMBP dominate in this domain andt0 yields better performance and also shorter plans than KACMBP in large instances. CpA outperforms POND but is slowerthan CFF in instances solvable by CFF. Again, CpA was able to solve all instances in this domain but its solutions are usuallylonger than the solutions returned by other planners.In the BTC domain, t0 is better in general. POND and KACMBP are comparable to each other. CpA is competitive withothers in many instances. It outperforms CFF, POND, and KACMBP in BTC(100, 10) but takes twice the time comparing toPOND and KACMBP in solving the BTC(100, 1) instance. It is interesting to observe that CFF and t0 seem to have no problemwhen the number of toilets increases, while there is a significant increase in the amount of time needed to find a solutionfor KACMBP, POND, and CpA. This increase is, however, less substantial for CpA than for KACMBPor POND. For example, if100P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Table 5Domains with static causal laws and high degree of uncertainty: Ring-C, Domino, an Cleaner.CFFPL1050214110120154104254Prob.sizeDomino(n)1050100200500100020005000Cleaner(r, o)2, 102, 202, 502, 1005, 105, 205, 505, 100KACMBPPL23163376852214110120156106256Time0.040.131.4516.42––––0.040.367.89113.990.574.75143.40–Ring with static causal laws, Ring-C(n)10152025852312700.499.03503.99–Time0.000.00NANANANANANA0.000.020.293.480.030.277.46NA––––PONDPL10501002005001000Time0.000.354.51128.26378.441258.17––2119.30–––––––––––t0PL1050100200500100021411012015410425450430456075Time0.050.070.211.9358.34745.95––0.050.080.100.230.060.100.230.791.016.7627.4479.58CpAPL1111111121411012015410425450430456075Time0.000.010.020.020.050.160.523.120.030.192.7622.710.261.7826.66214.270.835.5722.2065.32the number of packages is fixed to 100 and the number of toilets increases from 5 to 10, the amount of time needed by CFFeven decreases, while the time needed by KACMBP increases about 5 times, CpA’s time doubles, and POND’s time increasesmore than 10 times while t0’s remains the same.In the Logistic domain, both KACMBP and CpA had difficulty in finding plans. Although KACMBP performs better thanCpA, its performance is far from that of CFF or t0, which solved every problem instance in less than one second. POND cansolve all problems but the time increases very fast. We believe that the poor performance of CpA on this domain lies in thenot-so-good heuristic function used. This is reflected in the lengths of the plans found by CpA.Table 5 contains experimental results on the performance of the planners in domains that are different from domainsdescribed in Table 4 in the following way: these domains are either rich in static causal laws or have a high degree ofuncertainty in the initial state. The domains encoded for CpA use static causal laws. These static causal laws are compiledaway in the encodings used for other planners, following the procedure suggested in [63].In the Domino domain, with the exception of CpA, none of the other planners performed well. The reason being that thisdomain is rich in static causal laws – a feature that is not directly supported by KACMBP, CFF, t0, or POND. Thus, we hadto compile them away when encoding this domain for these planners, which requires the introduction of extra actions andfluents. As a result, the performance of these planners is hit by an extra overhead that CpA is not affected with. It is worthmentioning that the ‘NA’ in CFF’s column is due to some predefined constants of the system (e.g. maximal plan length isset to 500).In the Cleaner domain, CpA also obtained good performance and it is the only planner that could solve all instances.KACMBP behaves well only on small problems but does not scale up as well as CFF and CpA. CFF is very good at all instancesof the domain, except Cleaner(5, 100) where it reported the error message: “maximum length of plans is exceeded”. Webelieve that this can easily be fixed by increasing some constant for the maximum length of plans allowed. POND could notsolve only one instance in this set of problems. In this domain, the high degree of uncertainty in the initial state increaseswith the number of objects.It is interesting to observe how static causal laws affect the performance of CpA and how different reasoning methodcould be useful in other planners. As we have mentioned before, the only difference between the Ring-C domain andthe Ring domain is that we add the encoding of the static causal laws “an agent cannot be at two different locationsat the same time” to the problem specification. In the encoding used to run CpA, it is expressed as a static causal law.This information is expressed in the encoding for other planners by adding the additional (negative) effects to the for-ward/backward of the domain. For example, if we have three rooms 1, 2, and 3, the encoding for CpA will include the actionforward causes ini+1 if ini where i = 1, 2, 3 and in4 ≡ in1, and the static causal law ¬in j if ini for i (cid:7)= j; the encoding ofthis action for other planners will have conditional effects of the form (when(in3)(in1 ∧ ¬in2 ∧ ¬in3)), etc. CpA yields thebest result in this domain. Only CpA and t0 were able to solve all instances of this problem. Neither CFF nor POND returneda solution for the smallest instance of this domain (Ring-C(10)). The difference between t0 and CFF indicates that in thisdomain, the method employed by t0 is better than that of CFF. It is likely that the translation t0 introduces only a fewactions and fluents into the problem which ultimately results in better performance.P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119101Table 6Sortnet domains.ProblemKACMBPPL13610152128364555667891105120Time0.000.000.020.050.190.200.420.761.242.183.225.558.2420.6228.54Sortnet(1)Sortnet(2)Sortnet(3)Sortnet(4)Sortnet(5)Sortnet(6)Sortnet(7)Sortnet(8)Sortnet(9)Sortnet(10)Sortnet(11)Sortnet(12)Sortnet(13)Sortnet(14)Sortnet(15)Table 7UTS domains.CFFPLTimeNANANANANANANANANANANANANANANAPONDPL13591216192631384350556165ProblemKACMBPCFFPONDk01k02k03k04k05k06k07k08k09k10l01l02l03l04l05l06l07l08l09l10r03r04r05r06r07r08r09r10PL41125Time0.010.2012.11–––––––––––––––––––––––––PL410162228344046525841117232935414752581725333847486266Time0.000.000.030.080.331.042.575.9812.0923.640.010.000.010.010.330.080.210.450.861.600.020.041.479.651.604.763.8211.63PL4121926334047546168414233346647082881724324153546468Time0.000.000.000.000.010.000.010.01∗0.050.060.110.130.200.22Time0.000.010.060.240.671.653.557.2014.0224.760.000.020.080.321.11AB7.4016.9939.53100.000.080.230.651.734.148.9517.4436.59t0PLTimeNANANANANANANANANANANANANANANATime0.040.06AB0.150.250.420.731.201.912.940.050.040.070.080.140.220.350.702.304.120.070.100.200.300.591.031.462.12t0PL4112329354147535941323304762676490851926344044495767CpAPL1359152127354455CpAPL41220284047578281904143060538914121319418315060105107146128Time0.000.000.050.400.669.7635.51117.41371.581010.10–––––Time0.000.030.281.627.6918.8151.52122.22217.95400.790.000.020.362.249.3224.75106.37456.24–1147.510.231.819.9424.1598.41230.63480.28698.11We have seen that CpA performed reasonably well in several domains. Let us now focus on its performance in the twodomains Sortnet (Table 6) and UTS (Table 7). As we have mentioned earlier, the Sortnet domain has disjunctive goals. Thetwo systems CFF and t0 does not consider problems with disjunctive goals. CpA does not consider disjunctive goals directlyand we have to introduce additional fluents and static causal laws to encode the goal. For example, we replace a goal ofthe form f ∨ g by the fluents f g and add to the problem the set of static causal laws { f g if f , f g if g, ¬ f g if ¬ f ∧ ¬g}.We observe that this encoding is not applicable to the CFF and t0 systems. In this domain, POND is the best. It can solveevery instance in less than half a second with the exception of the instance Sortnet(9) where it halts abnormally. KACMBPcan solve all instances but it was much slower than POND. CpA, on the other hand, can only solve instances from 1 to 10(when n = 11, the number of partial states in the initial state is 211). This domain indicates that the explicit representationof the set of partial states used by CpA needs to be compensated in some ways for the planner to scale up.Finally, Table 7 indicates that the UTS domain is also a challenging problem for CpA and KACMBP. In this domain, thenumber of initial states is linear in the size of the problem. We believe that the heuristic used by CpA is not providing good102P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119guidance for the search in this domain. Even though CpA can solve all problems with the exception of the l09 problem, itrequires much more time than other planners in large instances and also returns significant longer plans. For example, inthe k10-problem, CpA needs 401 seconds while CFF and POND only require around 24 seconds and t0 needs less than 3second; CpA’s plan is also contains 20 or 30 actions more than those of POND or CFF and t0. On the other hand, CpA cansolve all problems but one while KACMBP can solve only three smaller problems.Observe that CpA relies on the approximation defined by T lp(D) in its search for a solution. Therefore, it is, theoretically,a sound but incomplete planner because T lp(D) is deterministic whereas D can be non-deterministic as discussed Section2.2. To make sure that our approach can cover a broader spectrum of planning problems, we also tested CpA with classicalplanning problems. The first domain considered was the Blocks World, and we performed tests with five problem instancesdescribed in [18]. We then tested with problems in the Rovers domain.15 We experimented with five problem instances,different from each other in the numbers of way points, rovers, cameras, rock and soil samples, and objectives. It turns outthat CpA can solve all those problems.9. Conformant planning – logic programming vs. heuristic search through approximationThe experimental results in the previous section indicate that the C++ planner CpA yields better performance than thelogic programming based planner CPasp in most of the sequential benchmarks. However, CPasp is competitive for the plan-ning problems with short solutions. Our experimental results also show that in problems with complex static causal laws,CPasp seems to do better than CpA and other state-of-the-art planners but it does not do well on large problems. One ofthe main reasons for this weakness of CPasp is its reliance on a general answer set solver in computing a solution. On theother hand, the use of logic programming brings a number of advantages. We will now discuss some of these advantages indetail.(1) Generating Parallel Plans: Logic programming based planners such as CPasp and DLVK can generate parallel plans. InCPasp, this is achieved by the rules (20) and (21). On the other hand, most of the state-of-the-art planners do notcompute parallel plans.(2) Incorporation of Control Knowledge and/or Preferences: Logic programming provides an ideal environment for adding con-trol knowledge and/or preferences in searching for plans satisfying some qualitative criteria. For example, in the bombin the toilet domain with one package, flushing the toilet followed by dunking the package ( f lush; dunk) is a planachieving the goal of having the package disarmed, even when we know that the toilet is unclogged in the initial sit-uation. Although it is a valid solution, this plan is hardly a preferable one. The problem is that the action theory doesnot include knowledge stipulating the planner not to consider such a plan when the plan with a single action dunkwould be sufficient. This type of knowledge cannot be represented by an impossibility condition for flush. As such, forplanners relying on a fixed representation language (e.g. PDDL), the incorporation of knowledge in planning requires alot of work (e.g. TPlan [2] develops separate modules to deal with temporal knowledge). In logic programming, we caneasily express the above knowledge by the constraint(cid:4)← occ(cid:5)(cid:4)flush(t), T, h¬clogged(t), T(cid:5)which disallows answer sets in which the action flush occurs when the toilet is unclogged. The paper [58] discusses indetail how different types of control knowledge can be easily incorporated in answer set planning.(3) Dealing with Complex Initial Situations: Section 6 discusses how CPasp can be easily adapted to handle disjunctive infor-mation about the initial situations. This type of knowledge generates multiple branches in the search tree and can bedealt with fairly efficiently by state-of-the-art sequential planners. Yet, the specification of the initial situation in CPaspand its extension described in Section 6 remains simple in the sense that the initial state can be expressed by a set offacts. While this is adequate in all benchmarks found in the literature, the following example, discussed in [26], showsthat allowing the specification of more complex initial situations is sometimes necessary and useful.Example 9 (Complex initial situation). Assume that the packages in the bomb in the toilet domain are coming from differ-ent sources belonging to one of two hierarchically structured organizations, called b (bad) and g (good). The hierarchiesare described in the usual way using relation link(D1, D2) which indicates that a department D1 is a subdivision of adepartment D2. Organization g for instance can be represented by a collection of atoms:link(d1, g)link(d2, g)link(d3, d1)link(d4, d1)15 http://planning.cis.strath.ac.uk/competition/.P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119103These atoms represent the fact that di (i = 1, 2, 3, 4) are departments in the good company. It is known that packagescoming from the organization g are safe and the bomb is sent by someone working for b. There are packages labeled bythe name of the department the sender works for, which can be recorded by the atom from(P , D) – package P came fromdepartment D. There are also some unlabeled packages. The initial situation of the modified bomb in the toilet problemwill be described by the program H consisting of the above atoms and the following rules which define the organizationthe package came from and our trust in the good guys from g.from(P , D2) ← from(P , D1), link(D1, D2)(cid:4)¬armed(P ), 0← from(P , g)h(cid:5)As usual, P ranges over packages and D’s range over the departments. It is easy to see that such a program has a uniqueanswer set and can be used to specify the initial situation.It is easy to see that for π (D) (as well as Γ (D)) to work with the planning problems with this type of initial situation,we only need to replace the rule (18) by the above program.10. Conclusion and future workWe define the notion of an approximation of action theories with static causal laws, concurrent actions, and incompleteinformation. The proposed approximation is deterministic and can be computed efficiently, using either logic programs oran imperative language (C++).We develop two conformant planners using the proposed approximation, a logic programming based planner (calledCPasp) and a heuristic search based planner (CpA). CPasp can generate parallel plans and CpA is a sequential planner. Bothcan handle disjunctive information about the initial state. Unlike many state-of-the-art conformant planners, these plannersdeal directly with static causal laws. Their performance is comparable with state-of-the-art conformant planners over severalbenchmark domains as well as over newly invented domains. Due to the simple heuristic used in the implementation ofCpA, we believe that the good performance of CpA lies in the use of the approximation. In a recent development, oneof the authors has worked to improve this aspect of CpA and entered an improved version of CpA into the InternationalPlanning Competition. The improved version of CpA won the best prize in the conformant planning category.16 This resultand the development of these planners demonstrates that a careful study in approximated reasoning may pay off well inthe development of practical planners.We also provide a sufficient condition for the completeness of the approximation. The condition is applicable to simpleaction theories whose static causal laws are of a special form. We do believe that this condition can be extended to cover abroader class of action theories. We leave this as one of our primary tasks in the near future. Furthermore, we would alsolike to investigate the use of more informative heuristics in improving the performance of CpA.AcknowledgementsWe gratefully acknowledge our debt to the pioneering work and influence of John McCarthy. The important ideas usedand investigated in this paper, including declarative programming, non-monotonic logic, reasoning about actions and theireffects, finding solutions to the frame and ramification problems, and elaboration tolerance all originated from John Mc-Carthy’s research [34]. This paper can be viewed as an attempt to apply some of these ideas to the narrow (but hopefullysufficiently important) topic of conformant planning.17The first two authors would like to acknowledge the partial support from the NSF grants CNS-0220590, HRD-0420407, and IIS-0812267. The third author would like to acknowledge the partial support from the NASA contract NASA-NEG05GP48G.Appendix A. Proofs of Proposition 1 and Theorems 3 and 5Suppose an action theory D is given. Let s be a state, δ be a partial state and a be an action. From Theorem 2, we havethe following result.Lemma 1. Let A be an answer set of Φ(a, s) and let s(cid:4) = {l | h(l, 1) ∈ A}. Then s(cid:4)is a state and (cid:3)s, a, s(cid:4)(cid:5) ∈ T (D).Since this appendix mainly deals with programs Φ(a, s) and Π(a, δ) (defined by (17)), to simplify the proofs, we removefrom the programs atoms and rules that are of no interest.First, let Φ0(a, s) (resp. Π0(a, δ)) denote the program obtained from Φ(a, s) (resp. Π(a, δ)) by removing its constraints.16 http://ippc-2008.loria.fr/wiki/index.php/Results.17 Some of these results have been reported in the proceedings of the 20th National Conference on Artificial Intelligence [62], the 8th InternationalConference on Logic Programming and Nonmonotonic Reasoning Conference [61], and the 20th International Joint Conference on Artificial Intelligence [47].104P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Lemma 2. Π0(a, δ) is a stratified program.Proof. We need to find a function λ which maps atoms in Π0(a, δ) into non-negative integers such that for every r inΠ0(a, δ) with the head y,• λ( y) (cid:3) λ(x) for every atom x such that x appears in the body of r; and• λ( y) > λ(x) for every atom x such that not x appears in the body of r.It is easy to check that the following function λ satisfies the above property.• λ(de(l, 1)) = 1;• λ(ph(l, 1)) = 2;• λ(h(l, 1)) = 3; and• λ(at) = 0 for any other atom at, e.g., λ(h(l, 0)) = 0 for every fluent literal l. (cid:2)Lemma 3. The program Π0(a, δ) is consistent and has a unique answer set.Proof. It follows from Lemma 2 and [23]. (cid:2)Let X be the set of atoms of the forms fluent(F ) and literal(L). Clearly X is a splitting set [36] of both Φ0(a, s) andΠ0(a, δ). It is easy to see that the bottom parts of Φ0(a, s) and Π0(a, δ) with respect to X are positive programs and haveonly one answer set(cid:2)U =fluent(F ), literal(F ), literal(cid:4)(cid:5) (cid:11)(cid:11) F ∈ Fneg(F )(cid:3)(A.1)Let Φ1(a, s) and Π1(a, δ) denote the evaluation of the top parts of Φ0(a, s) and Π0(a, δ) with respect to U . The rules ofthese programs are listed below (the condition for each rule follows the rule).• Φ1(a, s) contains the following rules:h(l, 1) ← o(e, 0), h(ψ, 0)(cid:4)[e causes l if ψ] ∈ D(cid:5)h(l, 0) ← h(ψ, 0)(cid:4)[l if ψ] ∈ D(cid:5)h(l, 1) ← h(ψ, 1)(cid:4)(cid:5)[l if ψ] ∈ Dh(L, 1) ← h(L, 0), not h(¬L, 1)h(s, 0) ←h(a, 0) ←• Π1(a, δ) contains the following rules:h(l, 1) ← o(e, 0), h(ψ, 0)[e causes l if ψ] ∈ Dde(l, 1) ← o(e, 0), h(ψ, 0)(cid:4)(cid:4)(cid:5)(cid:5)[e causes l if ψ] ∈ Dph(l, 1) ← o(e, 0), not h(¬ψ, 0), not de(¬l, 1)[e causes l if ψ] ∈ D(cid:5)(cid:4)ph(L, 1) ← not h(¬L, 0), not de(¬L, 1)h(l, 0) ← h(ψ, 0)(cid:4)(cid:5)[l if ψ] ∈ D(A.2)(A.3)(A.4)(A.5)(A.6)(A.7)(A.8)(A.9)(A.10)(A.11)(A.12)P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119(cid:5)h(l, 1) ← h(ψ, 1)(cid:4)[l if ψ] ∈ Dph(l, 1) ← ph(ψ, 1)(cid:4)[l if ψ] ∈ Dh(L, 1) ← not ph(¬L, 1)h(δ, 0) ←h(a, 0) ←(cid:5)105(A.13)(A.14)(A.15)(A.16)(A.17)Let Y be the set of atoms of the form o(e, 0), h(l, 0), or ph(l, 0). Then it is easy to see that Y is a splitting set of bothΦ1(a, s) and Π1(a, δ).Because s is a state, the bottom part of Φ1(a, s) with respect to Y has the unique answer setV = o(a, 0) ∪ h(s, 0)Hence, the evaluation of the top part of Φ1(a, s) with respect to V is the following set of rulesh(l, 1) ←(l is a direct effect of a in s)h(l, 1) ← h(ψ, 1)(cid:4)[l if ψ] ∈ Dh(L, 1) ← not h(¬L, 1)(cid:5)(L holds in s)Let Φ2(a, s) denote this program.Lemma 4. A is an answer set of Φ(a, s) iff there exists an answer set A1 of Φ2(a, s) such thatA = U ∪ V ∪ A1where U and V are defined by (A.1) and (A.18). Furthermore, we haveh(l, 1) ∈ A iff h(l, 1) ∈ A1Proof. By the splitting set theorem [36]. (cid:2)Similarly, because δ is a partial state, the bottom part of Π1(a, δ) has the unique answer setW = o(a, 0) ∪ h(δ, 0)The evaluation of the top part of Π1(a, δ) with respect to W is the following set of rulesh(l, 1) ←(l is a direct effect of a in δ)de(l, 1) ←(l is a direct effect of a in δ)ph(l, 1) ← not de(¬l, 1)(l is a possible direct effect of a in δ)ph(L, 1) ← not de(¬L, 1)(L possibly holds in δ)h(l, 1) ← h(ψ, 1)(cid:4)[l if ψ] ∈ D(cid:5)(A.18)(A.19)(A.20)(A.21)(A.22)(A.23)(A.24)(A.25)(A.26)(A.27)106P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119ph(l, 1) ← ph(ψ, 1)(cid:4)[l if ψ] ∈ Dh(L, 1) ← not ph(¬L, 1)(cid:5)Let us denote this program by Π2(a, δ).Lemma 5. B is an answer set of Π0(a, δ) iff there exists an answer set B0 of Π2(a, δ) such thatB = U ∪ W ∪ B0where U and W are defined by (A.1) and (A.22). Furthermore, we haveh(l, 1) ∈ B iff h(l, 1) ∈ B0Proof. It follows from the splitting set theorem. (cid:2)(A.28)(A.29)(A.30)(A.31)Let us further split the program Π2(a, δ) by using the splitting set consisting of atoms of the form de(l, 1). The bottompart of Π2(a, δ) contains only rules of the form (A.24) and thus it has the only answer set(cid:2)de(l, 1)(cid:11)(cid:11) l is a direct effect of a(cid:3)Hence, the evaluation of the top part of Π2(a, δ) with respect to this answer set, denoted by Π3(a, δ), contains the followingrules:h(l, 1) ←(l is a direct effect of a in δ)ph(l, 1) ←(l is a possible direct effect of a in δ, ¬l is not a direct effect of a)ph(L, 1) ←(L possibly holds in δ, ¬L is not a direct effect of a)h(l, 1) ← h(ψ, 1)(cid:4)[l if ψ] ∈ Dph(l, 1) ← ph(ψ, 1)(cid:4)(cid:5)[l if ψ] ∈ Dh(L, 1) ← not ph(¬L, 1)(cid:5)Lemma 6. B is an answer set of Π0(a, δ) iff there exists an answer set B1 of Π3(a, δ) such that(cid:11)(cid:11) l is a direct effect of a in δB = U ∪ W ∪de(l, 1)(cid:2)(cid:3)∪ B1Furthermore, we haveh(l, 1) ∈ B iff h(l, 1) ∈ B1Proof. It follows from Lemma 5 and the splitting set theorem. (cid:2)(A.32)(A.33)(A.34)(A.35)(A.36)(A.37)(A.38)(A.39)(A.40)For a set of atoms X , let Φ X2 (a, s) (resp. Π X3 (a, δ)) denote the reduct of Φ2(a, s) (resp. Π3(a, δ)) with respect to X . Thatis, Φ X2 (a, s) is the following set of rules:h(l, 1) ←(l is a direct effect of a in s)h(l, 1) ← h(ψ, 1)(cid:4)[l if ψ] ∈ D(cid:5)h(L, 1) ←(cid:4)L holds in s, h(¬L, 1) /∈ X(cid:5)(A.41)(A.42)(A.43)P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119107and Π X3 (a, δ) is the following set of rules:h(l, 1) ←(l is a direct effect of a in δ)ph(l, 1) ←(l is a possible direct effect of a in δ, ¬l is not a direct effect of a)ph(L, 1) ←(L possibly holds in δ, ¬L is not a direct effect of a)h(l, 1) ← h(ψ, 1)(cid:4)[l if ψ] ∈ Dph(l, 1) ← ph(ψ, 1)(cid:4)(cid:5)[l if ψ] ∈ D(cid:5)h(L, 1) ←(cid:4)ph(¬L, 1) /∈ X(cid:5)Let us prove the following lemma.(A.44)(A.45)(A.46)(A.47)(A.48)(A.49)Lemma 7. Let A1 be an answer set of Φ2(a, s) and B1 be an answer set of Π3(a, δ). If δ ⊆ s then for any fluent literal l, ph(l, 1) ∈ B 1implies that h(l, 1) ∈ A1.Proof. Suppose δ ⊆ s. Let P = Φ A1respectively, we have2 (a, s) and Q = Π B13 (a, δ). Because A1 and B1 are answer sets of Φ2(a, s) and Π3(a, δ)A1 =B1 =(cid:12)i(cid:12)iP (∅)T iQ (∅)T i(A.50)(A.51)(A.52)where T P and T Q are the immediate consequence operators of programs P and Q respectively.First of all, we show that for any integer i (cid:3) 0 the following result holdsh(l, 1) ∈ T iP (∅) ⇒ ph(l, 1) ∈ T iQ (∅)Let us prove by induction on i.(1) Base case: i = 0. (A.52) holds because T 0P (∅) = T 0(2) Inductive step: suppose (A.52) holds for i = k, we need to show that it also holds for i = k + 1.Q (∅) = ∅.Let l be a fluent literal such that h(l, 1) ∈ T k+1(a) h(l, 1) holds in T k+1P(∅) by rule (A.41). This means that l is a direct effect of a in s. Observe that a direct effect of a ins is always a possible direct effect of a in δ. Furthermore, because D is consistent, ¬l is not a direct effect of a in s.Hence, ¬l is not a direct effect of a in δ. As a result, rule (A.45) belongs to Q , which implies that ph(l, 1) ∈ T k+1Q (∅).PP(∅). By the definition of T k+1(∅), there are three cases(b) h(l, 1) holds in T k+1P(∅) by rule (A.42). This means that there exists a static causal law (2) such thath(ψ, 1) ⊆ T kP (∅)By the inductive hypothesis, we haveph(ψ, 1) ⊆ T kQ (∅)By rule (A.48), this implies that ph(l, 1) ∈ T k+1Q (∅).(c) h(l, 1) holds in T k+1P(∅) by rule (A.43). This means that l holds in s and h(¬l, 1) /∈ A1.Because l holds in s, l possibly holds in δ. On the other hand, h(¬l, 1) /∈ A1 implies that ¬l is not a direct effect of ain s (because of rule (A.41)). Hence, ¬l is not a direct effect of a in δ. Accordingly, Q contains the rule of the form(A.46) with L = l. Thus, it follows that ph(l, 1) ∈ T k+1Q (∅).As a result, (A.52) holds. The lemma directly follows from (A.52), (A.50), and (A.51). (cid:2)108P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Lemma 8. Let A be an answer set of Φ(a, s) and B be an answer set of Π0(a, δ). If δ ⊆ s then for any fluent literal l, h(l, 1) ∈ B impliesh(l, 1) ∈ A.Proof. By Lemmas 4 and 6, the programs Φ2(a, s) and Π3(a, s) have answer sets A1 and B1, respectively, such thatA = U ∪ V ∪ A1andB = U ∪ W ∪(cid:2)de(l, 1)(cid:11)(cid:11) l is a direct effect of a in δ(cid:3)∪ B1Let P and Q be programs defined in Lemma 7. First of all, we will show thath(l, 1) ∈ T iQ (∅) ⇒ h(l, 1) ∈ A1for i (cid:3) 0 by using induction on i.(A.53)(1) Base case: trivial because there exist no fluent literal l such that l ∈ T i(2) Inductive step: suppose (A.53) holds for i (cid:2) k.Let l be a fluent literal such that h(l, 1) ∈ T k+1(a) h(l, 1) ∈ T k+1Q (∅) = ∅.Q (∅). We need to show that h(l, 1) ∈ A1. Consider the following casesQ (∅) by rule (A.44). This means that l is a direct effect of a in δ. On the other hand, a direct effectof a in δ is also a direct effect of a in s. As a result, l is a direct effect of a in s. By rule (A.19), this implies thath(l, 1) ∈ A1.(b) h(l, 1) ∈ T k+1Q (∅). By the inductive hypothesis, we have h(ψ, 1) ⊆Q (∅) by rule (A.47). This implies that h(ψ, 1) ⊆ T kA1. As a result, by rule (A.20), it follows that h(l, 1) ∈ A1.(c) h(l, 1) ∈ T k+1Q (∅) by rule (A.49). This implies that ph(¬l, 1) /∈ B 1. It follows from Lemma 7 that h(¬l, 1) /∈ A1, i.e.,h(¬l, 1) /∈ A(cid:4) = {g | h(g, 1) ∈ A}. From (A.54), we have ¬l /∈ s(cid:4)(cid:4). Accordingly, we have l ∈ sLet sthat either l or ¬l belongs to sFrom (A.53) and (A.51), we have. On the other hand, by Lemma 1, sis a state, which implies. From this, it follows that h(l, 1) ∈ A and thus h(l, 1) ∈ A1.(cid:4)(cid:4)(A.54)h(l, 1) ∈ B1 ⇒ h(l, 1) ∈ A1On the other hand, by Lemmas 4 and 6, we haveh(l, 1) ∈ A1iff h(l, 1) ∈ Aandh(l, 1) ∈ B1iff h(l, 1) ∈ BConsequently, we can conclude that the lemma holds. (cid:2)Lemma 9. Π(a, δ) is consistent iff a is safe in δ.Proof. Suppose Π(a, δ) is consistent. Let B denote the answer set of Π(a, δ). According to the definition of an answer setof a program with constraints, B is an answer set of Π0(a, δ) and B does not violate constraints (6) and (8). This impliesthat there exists no impossibility conditionimpossible b if ψin D such that o(b, 0) ⊆ B and h(¬ψ, 0) ∩ B = ∅. Because o(a, 0) is the set of all atoms of the form o(e, 0), where e is anelementary action, contained in B (see Lemma 5), there exists no impossibility conditionimpossible b if ψsuch that b ⊆ a and ψ possibly holds in δ. By definition, this means a is safe in δ.Now suppose that a is safe in δ. By Lemma 3, Π0(a, δ) has a unique answer set B. We will show that B is also an answerset of Π(a, δ) by showing that it satisfies constraints (6) and (8):(1) Constraint (6): Trivial because a is safe in δ.(2) Constraint (8): Since δ is a partial state, there exists a state s such that δ ⊆ s. As a is safe in δ, it is executable in s.By Theorem 3, it follows that the program Φ(a, s) has an answer set A and this answer set satisfies constraint (8). ByLemma 8, it follows that B also satisfies constraint (8). (cid:2)P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119109Lemma 10. If Π(a, δ) is consistent then the only answer set of Π(a, δ) is the answer set of Π0(a, δ).Proof. The lemma follows from Lemma 3 and from the fact that Π(a, δ) differs from the program Π0(a, δ) in two constraints(6) and (8) only. (cid:2)A.1. Proof of Proposition 1Suppose Π(a, δ) is consistent. Lemma 10 implies that the only answer set of Π(a, δ) is the answer set of Π0(a, δ). Let(cid:2)l(cid:11)(cid:11) h(l, 1) ∈ B(cid:3)(cid:4) =δTo complete the proof, we need to show that δ(cid:4)laws of D because of constraint (5). So, we only need to show that there exists a state sis a partial state. First of all, observe that δ(cid:4)(cid:4)such that δ(cid:4) ⊆ s(cid:4).satisfies all the static causalSince δ is a partial state there exists a state s such that δ ⊆ s. Because Π(a, δ) is consistent, by Lemma 9, a is safe(cid:4)(cid:5) ∈ T (D). Bysuch that (cid:3)s, a, s(cid:4) = {l | h(l, 1) ∈ A}. By Lemma 8, it is easyin δ. Thus, it is executable in s. Since we assume D is consistent, there must be a state sTheorem 2, this implies that the program Φ(a, s) has an answer set A such that sto see that δ(cid:4) ⊆ s.(cid:4)(cid:4)A.2. Proof of Theorem 3Let (cid:3)δ, a, δ(cid:4)(cid:5) be a transition in T lp(D). It follows from Definition 8 that the program Π(a, δ) is consistent and has ananswer set B such that δ(cid:4) = {l | h(l, 1) ∈ B}. Note that by Lemma 10, such an answer set B is unique and it is also theanswer set of Π0(a, δ).First, let us show that T lp(D) is an approximation of T (D). Clearly, to prove that, it suffices to show that for everys ∈ comp(δ),(1) a is executable in s;(2) for every state s(cid:4)such that (cid:3)s, a, s(cid:4)(cid:5) ∈ T (D), δ(cid:4) ⊆ s(cid:4).Consider a state s ∈ comp(δ). By Lemma 9, a is safe in δ. Because δ ⊆ s, a is executable in s. Now let s(cid:4)(cid:5) ∈ T (D). By Theorem 2, this implies that there exists an answer set A of Φ(a, s) such that s(cid:3)s, a, sLemma 8, we have δ(cid:4) = {l | h(l, 1) ∈ B} ⊆ {l | h(l, 1) ∈ A} = s.(cid:4)(cid:4)be a state such that(cid:4) = {l | h(l, 1) ∈ A}. ByWe have showed that T lp(D) is an approximation of T (D). The determinism of T lp(D) follows directly from the fact thatB is unique.Appendix B. Proof of Theorem 5This appendix contains the proof of Theorem 5. We assume that a planning problem P = (cid:3)δ0, D, δ f (cid:5) is given. For thesake of simplicity of the proof, similarly to the previous section, we will begin with a simplification of the program π (P, n).Let π0(P, n) be the program obtained from π (P, n) by removing constraints (6) and (8). Let X be the set of atoms ofthe forms fluent(F ) and literal(L). Then, X is a splitting set of π0(P, n). The bottom part of π0(P, n) is a positive programand has a unique answer set U defined by (A.1). Let π1(P, n) denote the evaluation of the top part of π0(P, n) with respectto U .Lemma 11. A set of atoms C is an answer set of π0(P, n) iff C = C1 ∪ U where C1 is an answer set of π1(P, n).Proof. Follows from the splitting set theorem. (cid:2)For an integer 0 (cid:2) i (cid:2) n, let Xi denote the set of atoms whose time parameters are less than or equal to i. Then, it iseasy to see that the sequence (cid:3) Xi(cid:5)ni=0 is a splitting sequence [36] of π1(P, n).Lemma 12. A set of atoms C1 is an answer set of π1(P, n) iff there is a sequence of sets of atoms (cid:3)D i(cid:5)nconditions are satisfied.i=0 such that the following(1) D0 is an answer set ofμ0 = b X0(cid:4)(cid:5)π1(P, n)(2) For every 1 (cid:2) i (cid:2) n, D i is an answer set of(cid:4)(cid:13)(cid:4)μi = e Xib Xi(cid:5)π1(P, n)\ b Xi−1(cid:5)π1(P, n),(cid:12)(cid:14)D j1(cid:2) j(cid:2)i−1(B.1)(B.2)P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119110(3)C1 =n(cid:12)i=0D i(B.3)where b X (P ) denote the bottom part of a program P with respect to X and e X (Q , V ) denote the evaluation of a program Q relativeto V .Proof. Follows from the splitting sequence theorem [36]. (cid:2)Now suppose that C is an answer set of π (P, n). By definition C is also an answer set of π0(P, n) and C does not violateany constraint of π (P, n). By Lemma 11, the program π1(P, n) has an answer set C1 such that C = C1 ∪ U . By Lemma 12,i=0 that satisfies (B.1)–(B.3). Let δi = {l | h(l, i) ∈ D i} and letit follows that there exists a sequence of sets of atoms (cid:3)D i(cid:5)nai = {l | o(e, i) ∈ D i}. It is easy to see that μ0 is the following set of rules(cid:5)h(l, 0) ← h(ψ, 0)(cid:4)[l if ψ] ∈ D(cid:5)(cid:4)δ0, 0←ho(E, 0) ∨ ¬o(E, 0) ←and for i (cid:3) 1, μi is the following set of rulesh(l, i) ←(l is a direct effect of ai−1 in δi−1)de(l, i) ←(l is a direct effect of ai−1 in δi−1)ph(l, i) ← not de(¬l, i)(l is a possible direct effect of ai−1 in δi−1)ph(L, i) ← not de(¬L, i)(cid:4)h(¬L, i − 1) /∈ D i−1(cid:5)h(l, i) ← h(ψ, i)(cid:4)[l if ψ] ∈ D(cid:5)(cid:5)ph(l, i) ← ph(ψ, i)(cid:4)[l if ψ] ∈ Dh(L, i) ← not ph(¬L, i)o(E, 0) ∨ ¬o(E, 0) ←(B.4)(B.5)(B.6)(B.7)(B.8)(B.9)(B.10)(B.11)(B.12)(B.13)(B.14)Lemma 13. If δi−1 is a partial state then (cid:3)δi−1, ai−1, δi(cid:5) ∈ T lp(D).Proof. Suppose δi−1 is a partial state. To prove that (cid:3)δi−1, ai−1, δi(cid:5) ∈ T lp(D) we need to show that Π(ai−1, δi−1) is consistentand its only answer set B satisfies(cid:2)l(cid:11)(cid:11) h(l, 1) ∈ B(cid:3)= δiFirst, observe that because C is an answer set of π (P, n), its satisfies the constraint (6). As a result, ai−1 is safe in δi−1. ByLemma 9, this implies that Π(ai−1, δi−1) is consistent and thus, by Proposition 1, it has a unique answer set B. By Lemma 5,the program Π2(ai−1, δi−1) (rules (A.23)–(A.29) with a = ai−1 and δ = δi−1) has an answer set B0 such that(cid:3)(cid:2)l(cid:11)(cid:11) h(l, 1) ∈ B(cid:11)(cid:11) h(l, 1) ∈ B0Furthermore, such B0 is unique because Π2(ai−1, δi−1) is stratified.(cid:2)l=(cid:3)Observe that the program Π2(ai−1, δi−1) is the same as μi except that the time parameter of predicates in the former is1 while it is i in the latter. Hence, we haveP.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119111(cid:2)l(cid:11)(cid:11) h(l, 1) ∈ B0(cid:3)= δiThat is, the lemma holds. (cid:2)Let us go back to the proof of Theorem 5. It is easy to see that δ0 = δ0 and thus it is a partial state. By Lemma 13, iteasy to see that for all 1 (cid:2) i (cid:2) n we have (cid:3)δi−1, ai−1, δi(cid:5) ∈ T lp(D).Hence, we have (cid:3)δ0, α, δn(cid:5) ∈ T lp(D). On the other hand, because C satisfies constraint (19), we have δ f ⊆ δn. Accordingly,α is a solution of P . Thus, the theorem is proved.Appendix C. Proofs of Propositions 2–4 and Theorem 6This appendix contains the proofs of Propositions 2–4 and Theorem 6. We assume that a simple planning problem P isgiven. In addition, for simplicity, we assume that the body of each static causal law of D has exactly one fluent literal aswith some minor changes, the proofs in this appendix can be applied to simple action theories with arbitrary simple staticcausal laws, including those with an empty body. To make the proofs easy to follow, let us define some notions.Definition 17. Let a be an action and s be a state. A fluent literal l is called an effect of a in s if either(1) l is a direct effect of a in s; or(2) D contains a static causal lawl if gsuch that g is an effect of a in s.Definition 18. Let a be an action and δ be a state. A fluent literal l is called a possible effect of a in δ iff either(1) l is a possible direct effect of a in δ; or(2) l possibly holds by inertia, i.e., l possibly holds in δ and ¬l is not a direct effect of a in δ; or(3) D contains a static causal lawl if gsuch that g is a possible effect of a in δ.The proofs in this appendix will make use of programs Φ2(a, s) (rules (A.19)–(A.21)) and Π3(a, δ) (rules (A.32)–(A.38))and some results from Appendices A and B.Let s and sbe partial states and a be an action such that (cid:3)s, a, sTheorems 2 and 3, and Definition 8, the programs Φ(a, s) and Π(a, δ) have answer sets A and B, respectively, such that(cid:4)(cid:5) ∈ T (D) and (cid:3)δ, a, δ(cid:4)(cid:5) ∈ T lp(D). Bybe states, δ and δ(cid:4)(cid:4)(cid:2)l(cid:11)(cid:11) h(l, 1) ∈ A(cid:3)(cid:4) =sand(cid:2)l(cid:11)(cid:11) h(l, 1) ∈ B(cid:3)(cid:4) =δ(C.1)(C.2)By Lemmas 4 and 6, this implies that the programs Φ2(a, δ) and Π3(a, δ) have answer sets A1 and B1 respectively suchthatA = U ∪ V ∪ A1(cid:2)B = U ∪ W ∪de(l, 1)(cid:11)(cid:11) l is a direct effect of a in δ(cid:3)∪ B1where U , V , and W are defined by (A.1) and (A.18), and (A.22). Furthermore, we haveh(l, 1) ∈ A iff h(l, 1) ∈ A1h(l, 1) ∈ B iff h(l, 1) ∈ B1(C.3)(C.4)(C.5)(C.6)Let P and Q be the reducts of Φ2(a, s) and Π3(a, δ) with respect to A1 and B1 respectively. That is, P is the set of rules(A.41)–(A.43) where X = A1, and Q is the set of rules (A.44)–(A.49) where X = B 1.Lemma 14. If g /∈ s and h(g, 1) ∈ A1 then g is an effect of a in s.112P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Proof. Let i (cid:3) 0 be an arbitrary integer. Because A1 =h(g, 1) ∈ T iP (∅) then g is an effect of a in s. We prove this by induction on i.i T iP (∅), to prove the lemma, it suffices to show that if g /∈ s and(cid:15)(1) Base case: i = 0. Trivial because there exists no fluent literal g such that h(g, 1) ∈ T 0(2) Inductive step: Suppose the lemma holds for i (cid:2) k. We will show that it also holds for i = k + 1. Let g be a fluent literalP (∅) = ∅.such thatg /∈ s ∧ h(g, 1) ∈ T k+1P(∅)Because g does not hold in s, the rule (A.43) with L = g does not belong to P . As a result, there are two possibilitiesfor h(g, 1) ∈ T k+1(a) g is a direct effect of a in s. By Definition 17, g is an effect of a in s.(b) D contains a static causal law(∅):Pg if hsuch thath(h, 1) ∈ T kP (∅)It is easy to see thath /∈ s(C.7)(C.8)(C.9)because if otherwise, we would have g ∈ s (note that s is a state and thus it satisfies static causal law (C.7)).From (C.8) and (C.9) and by the inductive hypothesis, it follows that h is an effect of a in s. Hence, by Definition 17,g is also an effect of a in s. (cid:2)Lemma 15. If ph(g, 1) ∈ B1 then g is a possible effect of a in δ.Proof. Let i (cid:3) 0 be an arbitrary integer. Clearly, to prove the lemma, it suffices to show thatif ph(g, 1) ∈ T iQ (∅) then g is a possible effect of a in δLet us prove (C.10) by induction on i.(C.10)(1) Base case: i = 0. Trivial because there is no i such that ph(g, 1) ∈ T 0(2) Inductive step: Suppose (C.10) is true for i (cid:2) k. We will show that it is also true for i = k + 1.Q (∅) = ∅.Let g be a fluent literal such that ph(g, 1) ∈ T k+1X = B1. Hence, there are three possibilities for ph(g, 1) ∈ T k+1(a) g is a possible direct effect of a in δ and ¬g is not a direct effect of a in δ. By definition, g is also a possible effectQ (∅). Recall that Q is the set of rules of the form (A.44)–(A.49) whereQ (∅).of a in δ.(b) g possibly holds in δ and ¬g is not a direct effect of a in δ. By definition, in this case g is also a possible effect ofa in δ.(c) D contains a static causal lawg if hsuch that ph(h, 1) ∈ T kalso a possible effect of a in δ.Q (∅). By the inductive hypothesis, h is a possible effect of a in δ. Hence, by Definition 18, g isSo, (C.10) is true. The lemma follows directly from this result. (cid:2)We will also need the following proposition.Proposition 6. Let D be a simple action theory. Let (cid:3)s, a, sl ∈ s, we have l (cid:16) (s \ δ).(cid:4) \ δ(cid:4)(cid:4)(cid:5) ∈ T (D) and (cid:3)δ, a, δ(cid:4)(cid:5) ∈ T lp(D). If δ ⊆ s then for every fluent literalProof. Suppose δ ⊆ s and l be a fluent literal in sl (cid:16) g.(cid:4) \ δ(cid:4). We need to show that there exists a fluent literal g ∈ s \ δ such thatIf l ∈ s \ δ then the proposition is trivial because by definition, l depends on itself and thus we can take g = l ∈ s \ δ tohave l (cid:16) g. Now, consider the case that l /∈ s \ δ. There are two possibilitiesP.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119113(1) l /∈ s, or(2) l ∈ δ.Let us consider each possibility in turn.(1) l /∈ s. As l ∈ s(cid:4) \ δ(cid:4), we have l ∈ s(cid:4). This implies that h(l, 1) ∈ A1. According to Lemma 14, l is an effect of a in s. It followsfrom Definition 17, that one of the following two cases occurs(a) D contains a dynamic causal lawe causes l if ψsuch that e ∈ a and ψ holds in s.If ψ holds in δ then l is a direct effect of a in δ. By rule (A.44), we have h(l, 1) ∈ B 1, i.e., l ∈ δ(cid:4)l ∈ sBecause ψ holds in s and does not hold in δ, we have(cid:4) \ δ(cid:4).. This contradicts toψ ⊆ s and ψ (cid:3) δAs a result, there exists a fluent literal g ∈ ψ such that g ∈ s \ δ. By the definition of dependencies (Definition 11),we have l (cid:16) g.(b) D contains a dynamic causal law e causes l0 if ψ and a sequence of static causal laws [l1 if l0], [l2 if l1], . . . ,[ln if ln−1], [l if ln] such that e ∈ a and ψ holds in s.If ψ holds in δ then by rule (A.44), we have l0 ∈ δ(cid:4)laws of D, it follows that l ∈ δ(cid:4)So, we have ψ does not hold in δ. Similarly to previous case, this implies that there exists a fluent literal g ∈ ψsuch that g ∈ s \ δ. Hence, we have; this contradicts to the assumption l ∈ s; on the other hand, because δ(cid:4)is closed under the static causal(cid:4) \ δ(cid:4).l (cid:16) ln (cid:16) ln−1 (cid:16) · · · (cid:16) l0 (cid:16) g(2) l ∈ δ.First, we will show that ph(¬l, 1) ∈ B 1. Since l /∈ δ(cid:4)According to Lemma 15, ph(¬l, 1) ∈ B 1 implies that ¬l is a possible effect of a in δ. By the definition of a possible effect(Definition 18), we have the following three cases(a) ¬l is a possible direct effect of a in δ. That is, D contains a dynamic causal law, we have h(l, 1) /∈ B1. By rule (A.49), it follows that ph(¬l, 1) ∈ B 1.e causes ¬l if ψsuch that e ∈ a and ψ possibly holds in δ. This implies that¬ψ ∩ δ = ∅(cid:4)(cid:4)(C.11)As l ∈ sis a state, we have ¬l /∈ sdirect effect of a in s. Hence, ψ does not hold in s, i.e.,and s(cid:4). This means that h(¬l, 1) /∈ A. By rule (A.41), it follows that ¬l is not a¬ψ ∩ s (cid:7)= ∅(C.12)there exists a fluent literal g ∈ ¬ψ such that g ∈ s \ δ. BecauseFrom (C.11) and (C.12),[e causes ¬l if ψ] belongs to D, this implies that ¬l (cid:16) ¬g. By the definition of dependencies, it follows thatl (cid:16) g.it follows that(b) l does not hold in δ and l is not a direct effect of a. Because l ∈ δ, this case never happens.(c) D contains a sequence of static causal laws[l1 if l0], [l2 if l1], . . . , [ln if ln−1], [¬l if ln]such that l0 is a possible effect of a in δ or l0 possibly holds by inertia.(i) l0 is a possible direct effect of a. By definition, this means that D contains a dynamic causal lawe causes l0 if ψsuch that e ∈ a and ψ possibly holds in δ.As ψ possibly holds in δ, we have¬ψ ∩ δ = ∅On the other hand, it is easy to see that ψ does not hold in s as if otherwise, we would have ¬l ∈ scontradicts to the assumption l ∈ s. Therefore, we have(cid:4)(C.13)(cid:4), which114P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119¬ψ ∩ s (cid:7)= ∅(C.14)By (C.13) and (C.14), there exists a fluent literal g ∈ s \ δ such that ¬g ∈ ψ . It is easy to see that¬l (cid:16) ln (cid:16) ln−1 (cid:16) · · · (cid:16) l0 (cid:16) ¬gHence, we have l (cid:16) g.(ii) l0 possibly holds by inertia. This means that l0 possibly holds in δ and ¬l is not a direct effect of a in δ.(cid:4). Because s is a state, it follows that ¬l0 ∈ s.It is easy to see that l0 does not hold in s as if otherwise, we would have ¬l ∈ sl ∈ sAs l0 possibly holds in δ and ¬l0 ∈ s, we have ¬l0 ∈ s \ δ. On the other hand, by the definition of dependencies,we have l (cid:16) ¬l0. Accordingly, we can select g = ¬l0 ∈ s \ δ to have l (cid:16) g. (cid:2), which is impossible due to(cid:4)C.1. Proof of Proposition 2By the definition of (cid:17)σ (Definition 13), δ is a subset of every state s in S. Hence, the right-hand side of the equation ofthe proposition is a subset of the left-hand side. Therefore, to prove the lemma, it is sufficient to show that(cid:13) (cid:16)(cid:14)s∈Ss∩ σ ⊆ δ ∩ σSuppose otherwise, that is, there exists a fluent literal l such that l ∈ (s∈S s) ∩ σ but l /∈ δ ∩ σ . This implies that (i) l ∈ σ ,and (ii) l ∈ s \ δ for every s ∈ S. The latter implies that l (cid:16) (s \ δ) for every s ∈ S. By the definition of (cid:17), S (cid:7)(cid:17)σ δ. This is acontradiction.(cid:17)C.2. Proof of Proposition 3(1) Assume that a is not safe in δ. That means there exists an impossibility conditionimpossible b if ψsuch that b ⊆ a and ψ possibly holds in δ, i.e.,¬ψ ∩ δ = ∅(C.15)By the definition of (cid:17), S (cid:17)σ δ implies that there exists a state s ∈ S such that b (cid:2) (s \ δ). Because a is executable in s,ψ does not hold in s, i.e., ψ (cid:3) s. Because s is a complete set of fluent literals, it follows that¬ψ ∩ s (cid:7)= ∅(C.16)By (C.15) and (C.16), there exists a fluent literal l ∈ (s \ δ) such that l ∈ ¬ψ . By the definition of dependencies, we haveb (cid:16) l and this is a contradiction because b (cid:2) (s \ δ).(2) Let S(cid:4) = Res(a, S).Consider an arbitrary state s ∈ S. Because a is executable in S, it follows from the previous item that a is safe in δ. ByLemma 9, Proposition 1, and the definition of T lp(D), it follows that there exists a (unique) partial state δ(cid:4)such that(cid:3)δ, a, δ(cid:4)(cid:5) ∈ T lp(D). We need to show that SSuppose otherwise, that is, STheorem 3, δ(cid:4) ⊆ sfor every s(a) there exists a fluent literal l ∈ σ such that l (cid:3) (s. Then, there are two possible cases (note that because δ ⊆ s for every s ∈ S, by(cid:4) \ δ(cid:4)) for every s(cid:4) (cid:7)(cid:17)σ δ(cid:4)(cid:4) ∈ S):(cid:4) (cid:17)σ δ(cid:4)(cid:4) ∈ S..(cid:4)(cid:4)Let l be such a fluent literal. Consider an arbitrary state s ∈ S and let (cid:3)s, a, slet g ∈ ssuch that l (cid:16) g. By Proposition 6, because g ∈ sg (cid:16) h. Because of the transitivity of (cid:16), we have l (cid:16) h. This implies that(cid:4) \ δ(cid:4)(cid:4) \ δ(cid:4)(cid:4)(cid:5) be a transition in T (D). Furthermore,, there must be a fluent literal h ∈ s \ δ such thatl (cid:16) (s \ δ)(C.17)Because s can be any arbitrary state in S, (C.17) implies that S (cid:7)(cid:17)σ δ. This is a contradiction.(cid:4) \ δ(cid:4)) for every s(b) there exists an action b such that b (cid:16) (s(cid:4) ∈ S.(cid:4)Consider an arbitrary state s ∈ S and let (cid:3)s, b, sliteral such that b (cid:16) l. By Proposition 6, because l ∈ sdefinition of dependencies, it follows that b (cid:16) g ∈ (s \ δ). Hence, we have(cid:4) \ δ(cid:4)(cid:4)(cid:5) be a transition in T (D). Furthermore, let l ∈ sbe a fluent, there exists a fluent literal g in s \ δ such that l (cid:16) g. By the(cid:4) \ δ(cid:4)b (cid:16) (s \ δ)(C.18)Because s can be any arbitrary state in S, (C.18) implies that S (cid:7)(cid:17)σ δ. This is a contradiction.P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119115C.3. Proof of Proposition 4Let α = (cid:3)a0, a1, . . . , an−1(cid:5). For i (cid:3) 0, let α[i] denote the chain of the initial i events of α, i.e., α[i] = (cid:3)a0, a1, . . . , ai−1(cid:5). Wewill prove the proposition by induction on the length n of α.(1) Base case: n = 0.Item 1 is trivial. Item 2 is true because Res(α, S) = S (cid:17)σ δ, (cid:3)δ, (cid:3)(cid:5), δ(cid:5) ∈ T lp(D) and T lp(D) is deterministic.(2) Inductive Step: Suppose the proposition is true for n (cid:2) k. We need to show that it is true for n = k + 1.Let S i = Res(α[i], S) and let δi be the partial state such that (cid:3)δ, α[i], δi(cid:5) ∈ T lp(D). Clearly to prove the inductive step, weonly need to show that(a) ak is safe in δk, and(b) Sk+1 (cid:17)σ δk+1.By the inductive hypothesis, we have Sk (cid:17)σ δk. By Proposition 3, it follows that ak is safe in δk and Sk+1 (cid:17)σ δk+1.C.4. Proof of Theorem 6This theorem follows directly from Proposition 4 and the definition of a simple planning problem (Definition 14). If Phas no solution then it is trivial. Suppose that P has a solution, say, α = (cid:3)a0, . . . , an−1(cid:5). We will show that π (P, n) isconsistent.Because α is a solution of P , there exists a sequence of sets of states (cid:3)S(cid:5)ni=0 such that(1) S0 = comp(δ0);(2) S i = Res(ai−1, S i−1) for i (cid:3) 1;(3) δ f ⊆ s for every s ∈ Sn.According to Proposition 4, there exists a sequence of partial states (cid:3)δi(cid:5)ni=0 such that δ0 = δ0 and Sn (cid:17)δ f δn. By Proposi-tion 2, we have δ f ⊆ δn.Let us construct a sequence of sets of atoms D i as follows:(cid:5)(cid:4)δ0, 0D0 = hfor 1 (cid:2) i (cid:2) n − 1,∪ o(a0, 0) ∪ ¬o( A \ a0, 0)(cid:2)D i = h(l, δi) ∪ o(ai, i) ∪ ¬o( A \ ai, i)de(l, i)(cid:11)(cid:3)(cid:11) l ∈ de(ai−1, δi−1)∪(cid:2)ph(l, i)(cid:11)(cid:3)(cid:11) l ∈ ph(ai−1, δi−1)and(cid:2)h(l, n)(cid:3)(cid:11)(cid:11) l ∈ δn∪(cid:2)Dn =de(l, n)(cid:11)(cid:3)(cid:11) l ∈ de(an−1, δn−1)∪(cid:2)ph(l, n)(cid:11)(cid:3)(cid:11) l ∈ ph(an−1, δn−1)It is easy to see that for 0 (cid:2) i (cid:2) n, D i is an answer set of μi (defined previously in Appendix B). By Lemma 12, C1 =is an answer set of π1(P, n). As a result, by Lemma 11, C = C1 ∪ U is an answer set of π0(P, n).We will show that C is also an answer set of π (P, n). Because π (P, n) is the program π0(P, n) with additional con-straints (6), (8), (19), and (21), all we need to do is to show that C does not violate any of these constraints. For (6) and(8), it is trivial because δi is a partial state and(cid:3)δi−1, ai−1, δi(cid:5) ∈ T lp(D). Constraint (19) is satisfied by C because δ f ⊆ δn.Constraint (21) is satisfied because ai is an action, i.e., a non-empty set of elementary actions.Hence, C is an answer set of π (P, n). This means that the program π (P, n) is consistent. As a result, the theorem holds.(cid:15)ni=0 D iAppendix D. Proof of Theorem 8We begin with a lemma about the operator ClD that will be used in the proof.Given an action theory D, for a set of fluent literals σ , letΛ(σ ) = σ ∪(cid:2)l(cid:11)(cid:11) ∃[l if ψ] ∈ D such that ψ ⊆ σ(cid:3)(D.1)Let Λ0(σ ) = σ and Λi+1(σ ) = Λ(Λi(σ )) for i (cid:3) 0. Since, by the definition of Λ, for any set of fluent literals σ (cid:4)σ (cid:4) ⊆ Λ(σ (cid:4)), the sequence (cid:3)Λi(σ )(cid:5)∞bounded by the set of fluent literals. Thus, there exists σ limit such thati=0 is monotonic with respect to the set inclusion operation. In addition, (cid:3)Λi(σ )(cid:5)∞we havei=0 isσ limitD=∞(cid:12)Λi(σ )i=0Furthermore, σ limitDis unique and satisfies all static causal laws in D.(D.2)116P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119Lemma 16. For any set of fluent literals σ , we have σ limitD= ClD(σ ).Proof. By induction we can easily show that Λi(σ ) ⊆ ClD(σ ) for all i (cid:3) 0. Hence, we haveσ limitD⊆ ClD(σ )Furthermore, from the construction of Λi(σ ), it follows that σ limit satisfies all static causal laws in D. Because of theminimality property of ClD(σ ), we haveClD(σ ) ⊆ σ limitDAccordingly, we haveσ limitD= ClD(σ )(cid:2)Lemma 17. For every set of fluent literals σ , Closure(D,σ ) = ClD(σ ).Proof. It is easy to see that the function Closure(D,σ ) is a straightforward computation of σ limitHence, by Lemma 16, we have Closure(D,σ ) = ClD(σ ). (cid:2)D(Eqs. (D.2) and (D.1)).The following lemma shows a code fragment that correctly computes the closure of a set of fluent literals.Lemma 18. Let i (cid:3) 0 be an arbitrary integer, and x be a binary predicate symbol. For any set σ of fluent literals, the following programx(l, i) ← (l ∈ σ )x(l, i) ← x(ψ, i)(cid:4)(cid:5)[l if ψ] ∈ Dhas the unique answer set {x(l, i) | l ∈ ClD(σ )}.Proof. By the definition of an answer set of a positive program, it is easy to see that the above program has the uniqueanswer set {x(l, i) | l ∈ σ limit} = {x(l, i) | l ∈ ClD(σ )} (see Lemma 16). (cid:2)DLet a be an action and δ be a partial state such that Π(a, δ) is consistent. By Proposition 1, Π(a, δ) has a unique answerset, say B. Let lit denote the set of all fluent literals, i.e., lit = F ∪ ¬F. We definede(a, δ) = {l | l is a direct effect of a in δ}pde(a, δ) = {l | l is a possible direct effect of a in δ}ph(a, δ) = ClD(cid:5)pde(a, δ) ∪ (lit \ ¬δ)(cid:5)\ de(a, δ)(cid:4)(cid:4)(D.3)(D.4)(D.5)Then, we have the following lemma.Lemma 19. For any fluent literal l, h(l, 1) ∈ B iff l ∈ ClD(de(a, δ) ∪ (lit \ ¬ph(a, δ))).Proof. Because B is an answer set of Π(a, δ), it is also an answer set of Π0(a, δ) (recall that Π0(a, δ) is the programobtained from Π(a, δ) by removing constraints).According to Lemma 6, there exists an answer set B 1 of the program Π3(a, δ) (rules (A.32)–(A.38)) such that (A.39) and(A.40) hold.It is easy to see that X = {ph(l, 1) | l ∈ lit} is a splitting set of Π3(a, δ). The bottom part of Π3(a, δ) with respect to X isthe following set of rulesph(l, 1) ←(l is a possible direct effect of a in δ, ¬l is not a direct effect of a)ph(L, 1) ←(L possibly holds in δ, ¬L is not a direct effect of a)ph(l, 1) ← ph(ψ, 1)(cid:4)[l if ψ] ∈ D(cid:5)which can be rewritten to (see (D.3) and (D.4) for the definition of de(a, δ) and pde(a, δ))P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119117ph(l, 1) ←(cid:4)l ∈(cid:4)(cid:5)pde(a, δ) \ ¬de(a, δ)(cid:4)(cid:4)lit \ ¬∪(cid:5)(cid:5)(cid:5)δ ∪ de(a, δ)ph(l, 1) ← ph(ψ, 1)(cid:4)[l if ψ] ∈ D(cid:5)By Lemma 17, this program has a unique answer set (see (D.5) for the definition of ph(a, δ))(cid:2)M =ph(l, 1)(cid:11)(cid:3)(cid:11) l ∈ ph(a, δ)Hence, the evaluation of the top part of Π3(a, δ) with respect to M is the following set of rules:h(l, 1) ←(cid:4)(cid:5)l ∈ de(a, δ)h(l, 1) ← h(ψ, 1)(cid:4)[l if ψ] ∈ D(cid:5)h(L, 1) ←(cid:4)(cid:5)¬l /∈ ph(a, δ)Again, by Lemma 17, this program has a unique answer set (note that (¬l /∈ ph(a, δ)) ⇔ (l ∈ (lit \ ¬ph(a, δ)))):(cid:5)(cid:5)(cid:3)(cid:4)(cid:2)h(l, 1)(cid:11)(cid:11) l ∈ ClDN =de(a, δ) ∪(cid:4)lit \ ¬ph(a, δ)By the splitting set theorem, we have B 1 = M ∪ N. Hence by (A.40), we have(cid:4)h(l, 1) ∈ B(cid:5)⇔(cid:4)l ∈ ClD(cid:4)de(a, δ) ∪(cid:4)lit \ ¬ph(a, δ)(cid:5)(cid:5)(cid:5)Hence, the lemma holds. (cid:2)We now show that Theorem 8 holds. By Lemma 18, it is easy to see that Res(D, a, δ) = ClD(de(a, δ) ∪ (lit \ ¬ph(a, δ))).By Lemma 19, it follows that Theorem 8 holds.References[1] F. Bacchus, The AIPS’00 planning competition, AI Magazine 22 (3) (2001), http://www.cs.toronto.edu/aips2000/.[2] F. Bacchus, F. Kabanza, Using temporal logics to express search control knowledge for planning, Artificial Intelligence 116 (1,2) (2000) 123–191.[3] M. Balduccini, M. Gelfond, Diagnostic reasoning with A-Prolog, Theory and Practice of Logic Programming 3 (4,5) (2003) 425–461.[4] M. Balduccini, M. Gelfond, M. Nogueira, Answer set based design of knowledge systems, Annals of Mathematics and Artificial Intelligence (2006).[5] C. Baral, Reasoning about actions: Non-deterministic effects, constraints and qualification, in: Proceedings of the 14th International Joint Conferenceon Artificial Intelligence, Morgan Kaufmann Publishers, San Francisco, CA, 1995, pp. 2017–2023.[6] C. Baral, V. Kreinovich, R. Trejo, Computational complexity of planning and approximate planning in the presence of incompleteness, Artificial Intelli-gence 122 (2000) 241–267.[7] C. Baral, S. McIlraith, T.C. Son, Formulating diagnostic problem solving using an action language with narratives and sensing, in: Proceedings of theSeventh International Conference on Principles of Knowledge and Representation and Reasoning (KR’2000), 2000, pp. 311–322.[8] C. Baral, M. Gelfond, Reasoning agents in dynamic domains, in: J. Minker (Ed.), Logic-Based Artificial Intelligence, Kluwer Academic Publishers, 2000,pp. 257–279.[9] S. Baselice, P.A. Bonatti, M. Gelfond, Towards an integration of answer set and constraint solving, in: Maurizio Gabbrielli, Gopal Gupta (Eds.), LogicProgramming, 21st International Conference, ICLP 2005, Sitges, Spain, October 2–5, 2005, Proceedings, in: Lecture Notes in Computer Science, vol. 3668,2005, pp. 52–66.[10] A. Blum, M. Furst, Fast planning through planning graph analysis, in: Proceedings of the 14th International Joint Conference on Artificial Intelligence,Morgan Kaufmann Publishers, San Francisco, CA, 1995, pp. 1636–1642.[11] B. Bonet, H. Geffner, GPT: A tool for planning with uncertainty and partial information, in: A. Cimatti, H. Geffner, E. Giunchiglia, J. Rintanen, (Eds.),Proceedings of the IJCAI-01 Workshop on Planning with Uncertainty and Partial Information, Seattle, WA, 2001, pp. 82–87.[12] R. Brafman, J. Hoffmann, Conformant planning via heuristic forward search: A new approach, in: Sven Koenig, Shlomo Zilberstein, Jana Koehler (Eds.),Proceedings of the 14th International Conference on Automated Planning and Scheduling (ICAPS-04), Morgan Kaufmann, Whistler, Canada, 2004,pp. 355–364.[13] D. Bryce, S. Kambhampati, D. Smith, Planning graph heuristics for belief space search, Journal of Artificial Intelligence Research 26 (2006) 35–99.[14] C. Castellini, E. Giunchiglia, A. Tacchella, SAT-based planning in complex domains: Concurrency, constraints and nondeterminism, Artificial Intelli-gence 147 (July 2003) 85–117.[15] A. Cimatti, M. Roveri, Conformant planning via symbolic model checking, Journal of Artificial Intelligence Research 13 (2000) 305–338.[16] A. Cimatti, M. Roveri, P. Bertoli, Conformant planning via symbolic model checking and heuristic search, Artificial Intelligence Journal 159 (2004)127–206.[17] S. Edelkamp, J. Hoffmann, M. Littman, H. Younes, The IPC-2004 Planning Competition, http://ls5-www.cs.uni-dortmund.de/~edelkamp/ipc-4/, 2004.[18] T. Eiter, W. Faber, N. Leone, G. Pfeifer, A. Polleres, A logic programming approach to knowledge state planning, II: The DLVsystem, Artificial Intelli-Kgence 144 (1–2) (2003) 157–211.118P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119[19] T. Eiter, N. Leone, C. Mateis, G. Pfeifer, F. Scarcello, The KR system dlv: Progress report, comparisons, and benchmarks, in: International Conference onPrinciples of Knowledge Representation and Reasoning, 1998, pp. 406–417.[20] I. Elkabani, E. Pontelli, T.C. Son, Smodels with CLP and its applications: A simple and effective approach to aggregates in ASP, in: Bart Demoen, VladimirLifschitz (Eds.), Logic Programming, 20th International Conference, ICLP 2004, Saint-Malo, France, September 6–10, 2004, Proceedings, in: Lecture Notesin Computer Science, vol. 3132, Springer, 2004, pp. 73–89.[21] P. Ferraris, E. Giunchiglia, Planning as satisfiability in nondeterministic domains, in: Proceedings of the Seventeenth National Conference on ArtificialIntelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, July 30–August 3, 2000, AAAI Press/The MIT Press, Austin,Texas, USA, 2000, pp. 748–753.[22] M. Gelfond, A. Gabaldon, From functional specifications to logic programs, in: J. Maluszynski (Ed.), Proceedings of International Symposium on LogicProgramming, 1997, pp. 355–370.[23] M. Gelfond, V. Lifschitz, The stable model semantics for logic programming, in: R. Kowalski, K. Bowen (Eds.), Logic Programming: Proceedings of theFifth International Conference and Symposium, 1988, pp. 1070–1080.[24] M. Gelfond, V. Lifschitz, Representing actions and change by logic programs, Journal of Logic Programming 17 (2,3,4) (1993) 301–323.[25] M. Gelfond, V. Lifschitz, Action languages, ETAI 3 (6) (1998).[26] M. Gelfond, R. Morales, Encoding conformant planning in a-prolog, in: Proceedings of DRT’04, in: Lecture Notes in Computer Science, Springer, 2004.[27] M. Ghallab, A. Howe, C. Knoblock, D. McDermott, A. Ram, M. Veloso, D. Weld, D. Wilkins, PDDL – the Planning Domain Definition Language. Version 1.2,Technical Report CVC TR98003/DCS TR1165, Yale Center for Comp., Vis. and Ctrl., 1998.[28] E. Giunchiglia, G. Kartha, V. Lifschitz, Representing action: Indeterminacy and ramifications, Artificial Intelligence 95 (1997) 409–443.[29] E. Giunchiglia, V. Lifschitz, An action language based on causal explanation: preliminary report, in: Proceedings of AAAI 98, 1998, pp. 623–630.[30] S. Hanks, D.V. McDermott, Default reasoning, nonmonotonic logics, and the frame problem, in: Proceedings of the 5th National Conference on ArtificialIntelligence, Morgan Kaufmann, Philadelphia, PA, 1986, pp. 328–333.[31] J. Hoffmann, B. Nebel, The FF planning system: Fast plan generation through heuristic search, Journal of Artificial Intelligence Research 14 (2001)253–302.[32] G. Kartha, V. Lifschitz, Actions with indirect effects: Preliminary report, in: KR 94, 1994, pp. 341–350.[33] Y. Lierler, M. Maratea, Cmodels-2: SAT-based answer set solver enhanced to non-tight programs, in: Vladimir Lifschitz, Ilkka Niemelä (Eds.), Proceedingsof the 7th International Conference on Logic Programming and NonMonotonic Reasoning Conference (LPNMR’04), in: LNCS, vol. 2923, Springer-Verlag,2004, pp. 346–350.[34] V. Lifschitz (Ed.), Formalizing Common Sense: Papers by John McCarthy, Ablex Publishing Corporation, 1989.[35] V. Lifschitz, Answer set programming and plan generation, Artificial Intelligence 138 (1–2) (2002) 39–54.[36] V. Lifschitz, H. Turner, Splitting a logic program, in: Pascal Van Hentenryck (Ed.), Proceedings of the Eleventh International Conference on LogicProgramming, 1994, pp. 23–38.[37] V. Lifschitz, On the logic of causal explanation (research note), Artificial Intelligence 96 (2) (1997) 451–465.[38] F. Lin, Embracing causality in specifying the indirect effects of actions, in: Proceedings of the 14th International Joint Conference on Artificial Intelli-gence, Morgan Kaufmann Publishers, San Mateo, CA, 1995, pp. 1985–1993.[39] Y. Liu, H. Levesque, Tractable reasoning with incomplete first-order knowledge in dynamic systems with context-dependent actions, in: Proceedings ofthe 19th International Joint Conference on Artificial Intelligence, IJCAI, Edinburgh, Scotland, 2005.[40] D. Long, M. Fox, The 3rd international planning competition: Results and analysis, Journal of Artificial Intelligence Research (JAIR) 20 (2003) 1–59,http://planning.cis.strath.ac.uk/competition/.[41] N. McCain, H. Turner, A causal theory of ramifications and qualifications, in: Proceedings of the 14th International Joint Conference on ArtificialIntelligence, Morgan Kaufmann Publishers, San Mateo, CA, 1995, pp. 1978–1984.[42] J. McCarthy, Programs with common sense, in: Proceedings of the Teddington Conference on the Mechanization of Thought Processes, Her Majesty’sStationery Office, London, 1959, pp. 75–91.[43] J. McCarthy, Elaboration tolerance, in: Proceedings of the 98’s Common Sense Workshop, 1998, http://www-formal.stanford.edu/jmc/elaboration.html.[44] J. McCarthy, P. Hayes, Some philosophical problems from the standpoint of artificial intelligence, in: B. Meltzer, D. Michie (Eds.), Machine Intelligence,vol. 4, Edinburgh University Press, Edinburgh, 1969, pp. 463–502.[45] D. McDermott, A critique of pure reason, Computational Intelligence 3 (1987) 151–160.[46] S. McIlraith, Integrating actions and state constraints: A closed-form solution to the ramification problem (sometimes), Artificial Intelligence 116 (2000)87–121.[47] A.R. Morales, P.H. Tu, T.C. Son, An extension to conformant planning using logic programming, in: Manuela M. Veloso (Ed.), Proceedings of the 20thInternational Joint Conference on Artificial Intelligence, IJCAI 2007, Hyderabad, India, January 6–12, 2007, pp. 1991–1996.[48] H. Palacios, H. Geffner, Compiling uncertainty away: Solving conformant planning problems using a classical planner (sometimes), in: Proceedings ofthe Twenty-First National Conference on Artificial Intelligence, 2006.[49] H. Palacios, H. Geffner, From conformant into classical planning: Efficient translations that may be complete too, in: Proceedings of the 17th Interna-tional Conference on Planning and Scheduling, 2007.[50] R.P.A. Petrick, F. Bacchus, A knowledge-based approach to planning with incomplete information and sensing, in: Proceedings of the Sixth InternationalConference on Artificial Intelligence Planning Systems, Toulouse, France, April 23–27, 2002, AAAI, 2002, pp. 212–222.[51] R.P.A. Petrick, F. Bacchus, Extending the knowledge-based approach to planning with incomplete information and sensing, in: Proceedings of the SixthInternational Conference on Automated Planning and Scheduling, 2004, pp. 2–11.[52] J. Rintanen, Asymptotically optimal encodings of conformant planning in qbf, in: Proceedings of the Twenty-Second AAAI Conference on ArtificialIntelligence, Vancouver, British Columbia, Canada, July 22–26, 2007, AAAI Press, 2007, pp. 1045–1050.[53] E. Sandewall, Assessments of ramification methods that use static domain constraints, in: L.C. Aiello, J. Doyle, S. Shapiro (Eds.), Proceedings of KRR,Morgan Kaufmann, 1996, pp. 89–110.[54] M. Shanahan, The ramification problem in the event calculus, in: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,IJCAI 99, Stockholm, Sweden, July 31–August 6, 1999, 2 volumes, 1450 pages, 1999, pp. 140–146.[55] P. Simons, N. Niemelä, T. Soininen, Extending and implementing the stable model semantics, Artificial Intelligence 138 (1–2) (2002) 181–234.[56] D.E. Smith, D.S. Weld, Conformant graphplan, in: AAAI, 1998, pp. 889–896.[57] T.C. Son, C. Baral, Formalizing sensing actions – a transition function based approach, Artificial Intelligence 125 (1–2) (January 2001) 19–91.[58] T.C. Son, C. Baral, N. Tran, S. McIlraith, Domain-dependent knowledge in answer set planning, ACM Transactions on Computational Logic 7 (4) (2006)613–657.[59] T.C. Son, E. Pontelli, Some results on the completeness of approximation based reasoning, in: Proceedings of the Tenth Pacific Rim InternationalConference on Artificial Intelligence, 2008, pp. 358–369.[60] T.C. Son, P.H. Tu, On the completeness of approximation based reasoning and planning in action theories with incomplete information, in: Proceedingsof the 10th International Conference on Principles of Knowledge Representation and Reasoning, 2006, pp. 481–491.[61] T.C. Son, P.H. Tu, M. Gelfond, R. Morales, An approximation of action theories of AL and its application to conformant planning, in: Proceedings ofthe 7th International Conference on Logic Programming and NonMonotonic Reasoning, 2005, pp. 172–184.P.H. Tu et al. / Artificial Intelligence 175 (2011) 79–119119[62] T.C. Son, P.H. Tu, M. Gelfond, R. Morales, Conformant planning for domains with constraints – a new approach, in: Proceedings of the TwentiethNational Conference on Artificial Intelligence, 2005, pp. 1211–1216.[63] S. Thiebaux, J. Hoffmann, B. Nebel, In defense of PDDL axioms, in: Proceedings of the 18th International Joint Conference on Artificial Intelligence(IJCAI’03), 2003.[64] M. Thielscher, Ramification and causality, Artificial Intelligence 89 (1–2) (1997) 317–364.[65] P.H. Tu, Reasoning and planning with incomplete information in the presence of static causal laws, PhD thesis, New Mexico State University, 2007.[66] P.H. Tu, T.C. Son, C. Baral, Reasoning and planning with sensing actions, incomplete information, and static causal laws using logic programming, Theoryand Practice of Logic Programming 7 (2006) 1–74.[67] H. Turner, Representing actions in logic programs and default theories, Journal of Logic Programming 31 (1–3) (May 1997) 245–298.[68] H. Turner, Polynomial-length planning spans the polynomial hierarchy, in: Proceedings of Eighth European Conference on Logics in Artificial Intelligence(JELIA’02), 2002, pp. 111–124.