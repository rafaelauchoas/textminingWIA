Artificial Intelligence 175 (2011) 1498–1527Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintEfficient solutions to factored MDPs with imprecise transitionprobabilitiesKarina Valdivia Delgado a,∗, Scott Sanner b, Leliane Nunes de Barros a,ca EACH, Universidade de São Paulo, Av. Arlindo Béttio, 1000 – Ermelino Matarazzo São Paulo – SP, Brazilb NICTA and the Australian National University, Canberra, ACT 2601, Australiac IME, Universidade de São Paulo, Rua de Matão, 1010 – Cidade Universitária São Paulo – SP, Brazila r t i c l ei n f oa b s t r a c tArticle history:Received 3 December 2009Received in revised form 31 December 2010Accepted 31 December 2010Available online 4 January 2011Keywords:Probabilistic planningMarkov Decision ProcessRobust planningWhen modeling real-world decision-theoretic planning problems in the Markov DecisionProcess (MDP) framework, it is often impossible to obtain a completely accurate estimateoftransition probabilities. For example, natural uncertainty arises in the transitionspecification due to elicitation of MDP transition models from an expert or estimation fromdata, or non-stationary transition distributions arising from insufficient state knowledge. Inthe interest of obtaining the most robust policy under transition uncertainty, the MarkovDecision Process with Imprecise Transition Probabilities (MDP-IPs) has been introduced tomodel such scenarios. Unfortunately, while various solution algorithms exist for MDP-IPs,they often require external calls to optimization routines and thus can be extremely time-consuming in practice. To address this deficiency, we introduce the factored MDP-IP andpropose efficient dynamic programming methods to exploit its structure. Noting that thekey computational bottleneck in the solution of factored MDP-IPs is the need to repeatedlysolve nonlinear constrained optimization problems, we show how to target approximationtechniques to drastically reduce the computational overhead of the nonlinear solverwhile producing bounded, approximately optimal solutions. Our results show up to twoorders of magnitude speedup in comparison to traditional “flat” dynamic programmingapproaches and up to an order of magnitude speedup over the extension of factored MDPapproximate value iteration techniques to MDP-IPs while producing the lowest error of anyapproximation algorithm evaluated.© 2011 Elsevier B.V. All rights reserved.1. IntroductionMarkov Decision Processes (MDP) [1] have become the de facto standard model for decision-theoretic planning problemsand a great deal of research in recent years has aimed to exploit structure in order to compactly represent and efficientlysolve factored MDPs [2–5]. However, in many real-world problems, it is simply impossible to obtain a precise representationof the transition probabilities in an MDP. This may occur for many reasons, including (a) imprecise or conflicting elicitationsfrom experts, (b) insufficient data from which to estimate reliable precise transition models, or (c) non-stationary transitionprobabilities due to insufficient state information.For example, in an MDP for traffic light control, it is difficult to estimate the turn probabilities for each traffic lane thathas the option of going straight or turning. These lane-turning probabilities may change during the day or throughout theyear, as a function of traffic at other intersections, and based on holidays and special events; in general it is impossible to* Corresponding author. Tels.: +55 11 73326036, +55 11 30919878; fax: +55 11 30916134.E-mail addresses: kvd@usp.br (K.V. Delgado), ssanner@nicta.com.au (S. Sanner), leliane@ime.usp.br (L.N. de Barros).0004-3702/$ – see front matter © 2011 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2011.01.001K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271499accurately model all of these complex dependencies. In this case it would be ideal to have a traffic control policy optimizedover a range of turn probabilities in order to be robust to inherent non-stationarity in the turn probabilities.To accommodate optimal models of sequential decision-making in the presence of strict uncertainty over the transitionmodel, the MDP with imprecise transition probabilities (MDP-IP) was introduced [6,7]. While the MDP-IP poses a robustframework for the real-world application of decision-theoretic planning, its general solution requires the use of computa-tionally expensive optimization routines that are extremely time-consuming in practice.To address this computational deficiency, we extend the factored MDP model to MDP-IPs by proposing to replace theusual Dynamic Bayes Net (DBN) [8] used in factored MDPs with Dynamic Credal Nets (DCNs) [9] to support compact factoredstructure in the imprecise transition model of factored MDP-IPs. Then we propose efficient, scalable algorithms for solvingthese factored MDP-IPs. This leads to the following novel contributions in this work:• We introduce the parameterized ADD (PADD) with polynomial expressions at its leaves and explain how to extend ADDproperties and operations to PADDs.• We extend the decision-diagram based SPUDD and APRICODD algorithms for MDPs [3,4] to MDP-IP algorithms thatexploit DCN structure via PADDs.• As shown in our experimental evaluation, the generalization of SPUDD and APRICODD to MDP-IPs using PADDs isjust the first step in obtaining efficient solutions. Observing that the key computational bottleneck in the solution ofMDP-IPs is the need to repeatedly solve nonlinear constrained optimization problems, we show how to target ourapproximations to drastically reduce the computational overhead of the nonlinear solver while producing provablybounded, approximately optimal solutions.As our results will demonstrate, using the above contributions we can obtain up to two orders of magnitude speedupin comparison to traditional “flat” dynamic programming approaches [6]. In addition, our best approximate factored MDP-IP solver yields an order of magnitude speedup over a direct generalization of state-of-the-art approximate factored MDPsolvers [4] for factored MDP-IPs (also implemented in this work) and consistently produces the lowest error of all approxi-mate solution algorithms evaluated.2. Markov decision processesFormally, an MDP is defined by the tuple M = (cid:3)S, A, P , R, T , γ (cid:4), where [1,10]:(cid:5)|s, a) is the conditional probability of reaching state s• S is a finite set of fully observable states;• A is a finite set of actions;• P (s• R : S × A → R is a fixed reward function associated with every state and action;• T is the time horizon (number of decision stages remaining) for decision-making;• γ = [0, 1) is a discount factor (the reward obtained t stages into the future is discounted in the sense that it is multiplied(cid:5) ∈ S when action a ∈ A is taken from state s ∈ S;by γ t ).A stationary policy π : S → A indicates the action a = π (s) to take in each state s (regardless of stage). The value of astationary policy π is defined as the expected sum of discounted rewards over an infinite horizon (|T | = ∞) starting instate s0 at stage 0 and following πV π (s) = Eπ(cid:4)(cid:4) s0 = sγ t Rt(cid:5),(cid:2)∞(cid:3)t=0(1)where Rt (abbreviation of Rt (st, π (st)) is the reward obtained at stage t when the agent is in state st and takes action π (st ).(1) can be decomposed and rewritten recursively based on the values of the possible successor states s(cid:6)(cid:5) ∈ S as follows:(cid:3)(cid:6)(cid:7)(cid:6).(2)V π (s) = R(cid:7)s, π (s)+ γ(cid:7)(cid:5)|s, π (s)(cid:5)V πsPss(cid:5)∈SOur objective is to find an optimal policy π ∗that yields the maximal value in each state, i.e., ∀s, π (cid:5)V π ∗ (s) (cid:2) V π (cid:5) (s).A well-known algorithm to solve an MDP is value iteration [1]. For t > 0, it constructs a series of t-stage-to-go valuefunctions V t . Starting with arbitrary V 0, value iteration performs value updates for all states s, computing V t based onV t−1. The Q-value for state s and action a is:Q t(s, a) = R(s, a) + γ(cid:6)P(cid:5)|s, as(cid:7)V t−1(cid:7)(cid:6)(cid:5)s(cid:3)s(cid:5)∈Swhere the best value attainable at decision stage t and state s is(3)1500K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527Fig. 1. A credal set example represented by the gray region. The credal set is defined by the triplets {P (x1), P (x2), P (x3)} that belong to this region.V t(s) = maxa∈ AQ t(s, a).We define the greedy policy πV w.r.t. some V as follows:(cid:8)πV (s) = arg maxa∈ AR(s, a) + γ(cid:6)P(cid:5)|s, as(cid:7)(cid:6)V(cid:5)s(cid:9)(cid:7).(cid:3)s(cid:5)∈SAt the infinite horizon, the value function provably converges(cid:4)(cid:4)(cid:4)V t(s) − V t−1(s)(cid:4) = 0maxslimt→∞(4)(5)(6)leading to a stationary, deterministic optimal policy π ∗ = πV ∞ [1]. For practical MDP solutions, we are often only concernedwith (cid:4)-optimality. If we terminate the MDP when the following condition is met:(cid:4)(cid:4)(cid:4)V t(s) − V t−1(s)(cid:4) <maxs(cid:4)(1 − γ )2γthen we guarantee that the greedy policy πV tπ ∗[1].3. MDPs with imprecise transitionsloses no more than (cid:4) in value over an infinite horizon in comparison to(7)As described in our introductory traffic example, it is often necessary to work with imprecise probabilities in order torepresent incomplete, ambiguous or conflicting expert beliefs about transition probabilities. An MDP with imprecise transitionprobabilities (MDP-IP)1 is specifically designed for this setting and is simply an extension of the MDP where the transitionprobabilities can be imprecisely specified. That is, instead of a probability measure P (·|s, a) over the state space S, we havea set of probability measures. For example, let P ( X) be the probability density function for X = {x1, x2, x3} defined with thefollowing constraint set:(cid:10)C =P (x1) (cid:3) 2/3,P (x3) (cid:3) 2/3,2P (x1) (cid:2) P (x2),P (x1) + P (x2) + P (x3) = 1(cid:11).(8)The two-dimensional region of all probability measures that satisfy C is shown as the gray region in Fig. 1. This is referredto as a credal set, i.e., a set of probability measures (or a set of distributions for a random variable) [11]. We denote a credalset of distributions for variable X by K ( X).Next we slightly specialize the definition of credal set to specify uncertainty in MDP-IP transition probabilities:1 The term MDP-IP was proposed by White III and Eldeib [7], while Satia and Lave Jr. [6] adopt instead the term MDP with Uncertain Transition Probabilities.K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271501Definition 3.1 (Transition credal set). A credal set containing conditional distributions over the next state sand an action a, is referred to as a transition credal sets [11] and denoted by K (sdefine imprecisely specified transition probabilities., given a state s(cid:5)|s, a). Thus, we have P (·|s, a) ∈ K (·|s, a) to(cid:5)We assume that all credal sets are closed and convex, an assumption that is often used in the literature, and that(cid:5)|s, a); that(cid:5)|s, a) to be(cid:5)|s, a) may be selected from the corresponding credal sets in a time-dependentencompasses most practical applications [12]. We further assume stationarity for the transition credal sets K (s(cid:5)|s, a) is non-stationary, we note that this does not require P (sis, they do not depend on the stage t. While K (sstationary in an MDP-IP: distributions P (smanner [13].Formally, an MDP-IP is defined by MIP = (S, A, K , R, T , γ ). This definition is identical to the MDP M, except that thetransition distribution P is replaced with a transition credal set K . We will represent K implicitly as the set of transitionprobabilities consistent with a set of side linear inequality constraints C , like (8), over the probability parameters.There are several optimization criteria that can be used to define the value of a policy in an MDP-IP. In the context ofthe discounted infinite horizon setting that we focus on in this work, there is always a deterministic stationary policy that(cid:5)|s, a)is maximin optimal [6] (i.e., no other policy could achieve greater value under the assumption that Nature’s selects P (sadversarially to minimize value); moreover, given the assumption that A is finite and the credal set K is closed, this policyinduces an optimal value function that is the unique fixed-point solution of(cid:12)∗V(s) = maxa∈ AminP ∈KR(s, a) + γ(cid:6)P(cid:5)|s, as(cid:7)(cid:6)∗(cid:5)sV(cid:13)(cid:7).(cid:3)s(cid:5)∈S(9)There are various algorithms for solving flat (i.e., enumerated state) MDP-IPs based on dynamic programming [6,7]. In thiswork, we build on a flat value iteration solution to MDP-IPs [6]:(cid:12)V t(s) = maxa∈ AminP ∈KR(s, a) + γ(cid:6)P(cid:5)|s, as(cid:7)V t−1(cid:6)(cid:5)s(cid:13)(cid:7).(cid:3)s(cid:5)∈S(10)Value iteration for MDP-IPs is the same as that given in (3) and (4) for MDPs except that now for every state s, we optimizeour action choice a ∈ A w.r.t. the worst-case distribution P ∈ K that minimizes the future expected value. Thus we ensurethat the resulting value function and policy are robust to the worst outcome that Nature could choose in light of the futurevalue V t−1(s(cid:5)) that we expect to achieve.As we noted before, Nature’s true transition function P may be non-stationary; Nature can choose a different P ∈ K forevery action a and every state s and every decision stage t. As an example of such non-stationarity that may occur in practice,in the previously discussed traffic scenario, we observed that traffic turn probabilities may differ on holidays versus normalweekdays even though the embedded traffic controller may not be explicitly aware of the holiday in its state description.However, as long as such transition non-stationarity can be bounded by P ∈ K , convergence properties of MDP-IP valueiteration in (10) still hold [13].In [14,9] we have shown how MDP-IP solutions can be formulated as a bilevel or multilinear programming problem. Inthis paper we are interested in extending the dynamic programming solution for MDP-IPs [6,7] outlined above to efficientlysolve problems with a factored state description, which we discuss next.4. Factored MDP and MDP-IPs4.1. Factored MDPIn many MDPs, it is often natural to think of the state as an assignment to multiple state variables and a transitionfunction that compactly specifies the probabilistic dependence of variables in the next state on a subset of variables in thecurrent state. Such an approach naturally leads us to define a Factored MDP [2], where S = {(cid:10)x}. Here, (cid:10)x = (x1, . . . , xn) whereeach state variable xi ∈ {0, 1}.2The definition of actions a ∈ A is unchanged between MDPs and factored MDPs, so the reward can simply be specifiedas R((cid:10)x, a). The transition probabilities in a factored MDP are encoded using Dynamic Bayesian Networks (DBNs) [8]. A DBNis a directed acyclic graph (DAG) with two layers: one layer represents the variables in the current state and the other layer(cid:5)represents the next state (Fig. 2a). Nodes xi and xi refer to the respective current and next state variables. The connectionbetween these two layers defines the dependences between state variables w.r.t. the execution of an action a ∈ A. Directededges are allowed from nodes in the first layer into the second layer, and also between nodes in the second layer (these(cid:5)(cid:5)i) the parents of xlatter edges are termed synchronic arcs). We denote by paa(xin the graph for action a. The graphi(cid:5)encodes the standard Bayes net conditional independence assumption that a variable xi is conditionally independent of itsnondescendants given its parents, which incidentally for a DBN also encodes the Markov assumption (the current state is2 While our extensions are not necessarily restricted to binary state variables, we make this restriction here for simplicity of notation.1502K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527(cid:5)Fig. 2. a) A Dynamic Bayesian Network (DBN) for an action a; b) conditional probability table for x2(cid:5)= 1; c) conditional probability table for x2= 0.independent of the history given the previous state). The use of a DBN leads to the following factorization of transitionprobabilities:(cid:6)P(cid:5)|(cid:10)x, a(cid:10)x(cid:7)=n(cid:14)i=1(cid:6)P(cid:5)xi|paa(cid:6)(cid:5)xi(cid:7)(cid:7), a.Fig. 2(b) shows the conditional probability table (CPT) for P (xThe tables show all combinations of variable assignments for the parents of xrow in Fig. 2(b) and Fig. 2(c) must be 1, which can be easily verified.(cid:5)2(cid:5)(cid:5)= 1|paa(x2), a); Fig. 2(c) shows the same CPT for x2= 0.(cid:5)(cid:5)2); by definition, the sum of each2, i.e., pa(x(11)4.2. Factored MDP-IPAs our first major contribution, we extend the factored MDP representation [2] to compactly represent MDP-IPs. Thissimply requires modifying the DBN transition representation to account for uncertainty over the exact transition probabili-ties. Before we formally describe this transition function though, we first introduce one possible extension of the SysAdminfactored MDP to allow for imprecise transition probabilities, which we use from here out as a running example of a factoredMDP-IP.SysAdmin domain [5].In the SysAdmin domain we have n computers c1, . . . , cn connected via different directed graphtopologies: (a) unidirectional ring, (b) bidirectional ring and (c) independent bidirectional rings of pairs of computers (Fig. 3).Let state variable xi denote whether computer ci is up and running (xi = 1) or not (xi = 0). Let Conn(c j, ci) denote a⎧(i)connection from c j to ci . Formally, the CPTs for this domain have the following form:if a = reboot(ci): then 1if a (cid:11)= reboot(ci) ∧ xi = 1: thenpi1 · |{x j | j(cid:11)=i∧x j=1∧Conn(c j,ci )}|+1if a (cid:11)= reboot(ci) ∧ xi = 0: thenpi2 · |{x j | j(cid:11)=i∧x j=1∧Conn(c j,ci )}|+1|{x j | j(cid:11)=i∧Conn(c j,ci )}|+1⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎩= 1|(cid:10)x, a(iii)(ii)(cid:5)xi=P(cid:7)(cid:6)|{x j | j(cid:11)=i∧Conn(c j,ci )}|+1(12)and the constraints C on the probabilities variables areC = {0.85+ pi2 (cid:3) pi1 (cid:3) 0.95}.We have n + 1 actions: reboot(c1), . . . , reboot(cn) and notreboot, the latter of which indicates that no machine is rebooted.The intuition behind Eq. (12) is that if a computer is rebooted then its probability of running in the next time step is 1(situation i); if a computer is not rebooted and its current state is running (situation ii) or not running (situation iii), theprobability depends on the fraction of computers with incoming connections that are also currently running. The probabilityparameters pi1, pi2 and the constraint C over them define the credal sets K (·|(cid:10)x, a).The reward for SysAdmin is simply 1 if all computers are running at any time step otherwise the reward is 0, i.e.,(cid:19)ni=1 xi . An optimal policy in this problem will reboot the computer that has the most impact on the expectedR((cid:10)x) =future discounted reward given the network configuration.Like the previous definition of an enumerated state MDP-IP, the set of all legal transition distributions for a factoredMDP-IP is defined as a credal set K . The challenge then is to specify such transition credal sets in a factored manner that isK.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271503Fig. 3. Connection topologies for the SysAdmin example: a) unidirectional-ring, b) bidirectional ring and c) independent bidirectional rings of pairs ofcomputers [5].Fig. 4. a) Dynamic Credal Network for action notreboot for an unidirectional-ring topology of SysAdmin domain with 2 computers. b) Conditional prob-(cid:5)= 1 and the constraints related to the probabilities. c) The Parameterized ADD representation forability table for the state variables x1x|x1, x2, notreboot) that we call CPTnotreboot . A solid line indicates the true (1) branch of a variable test and a dashed line indicates the false (0) branch.(cid:5)= 1 and x2(cid:5)P (x1(cid:5)1itself compact. For this, we propose to use dynamic credal networks (DCNs), a special case of credal networks [11,15], as anappropriate language to express factored transition credal sets.Definition 4.1 (Factored transition credal set). A credal set containing conditional distributions over the values of a variable xi ,given the values of paa(xi) (the parents of xi in the graph for action a), is referred to as a factored transition credal set anddenoted by Ka(xi|paa(xi)).Definition 4.2 (Dynamic credal network). A Dynamic credal network (DCN) is a generalization of a DBN. Different from thedefinition of a DBN, in a DCN each variable xi is associated with factored transition credal sets Ka(xi|paa(xi)) for each valueof paa(xi). We assume that a DCN represents a joint credal set [15,11] over all of its variables consisting of all distributions(cid:5)|paa(xthat satisfy the factorization in Eq. (11), where each CPT distribution P (xi), a) is an element of the transition credal(cid:5)set Ka(xi(cid:5)|paa(xi)) associated with the DCN.(cid:5)iA DCN example is shown in Fig. 4(a). For each variable x(cid:5)i in a DCN, we have a conditional probability table (CPT) withimprecise probabilities. If we examine the CPTs in Fig. 4(b), we note that entries are specified by probability parameters pi j(cid:5)(cid:5)(i for variable xi ). Furthermore, we note that we have a set of side lineari and j for the jth parameter in the CPT for xconstraints on these pi j (shown in the boxes below the CPT, collectively call this constraint set C ). We use (cid:10)p to denote avector containing all parameter values that are free to vary within the given credal sets (i.e., that satisfy the probabilityconstraints C of the DCN).We note that the joint transition probability may be nonlinear in the probability parameters (cid:10)p. However, we explicitlyintroduce the following restriction to prevent exponents exceeding 1:(cid:5)i .Restriction 4.3 (DCN parameter restriction for factored MDP-IP CPTs). a parameter pi j may only appear in the CPT for x(cid:5)= 1, x2(cid:5)i are multiplied together to determineThis restriction prevents the multiplication of pi j by itself when CPTs for each xthe joint transition distribution in the DCN. This subset of nonlinear expressions, where the exponent of each pi j is either0 or 1, is referred to as a multilinear expression. To see the multilinearity of the transition probability in Fig. 4, we observe(cid:5)P (x1When combined with a set of constraints C on the pi j , there are efficient implementations that we can use in practiceto solve the resulting multilinear program. Interestingly, because there are no additional restrictions on the linear constraintsC defined over the pi j in a multilinear program, Restriction 4.3 actually turns out to be a minor limitation in practice as wedemonstrate in the experimental domains of Section 8.= 1|x1 = 1, x2 = 1, notreboot) = p11 p21.1504K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527Fig. 5. An example reward function R(x1, x2, x3) =(cid:20)3i=1 xi represented as an ADD.Even though we can qualitatively represent the conditional independence properties of a distribution using DCNs, thereare certain independences that we cannot represent with the Credal network structure, e.g., independences that hold forspecific contexts (assignments of values to certain variables) known as context-specific independence (CSI) [16]. In order tocompactly represent CSI and shared function structure in the CPTs for an MDP-IP, we propose a novel extension of algebraicdecision diagrams (ADDs) [17] called parameterized ADDs (PADDs) since the leaves are parameterized expressions as shownin Fig. 4(c). PADDs will not only allow us to compactly represent the CPTs for factored MDP-IPs, but they will also enableefficient computations for factored MDP-IP value iteration operations as we outline next.5. Parameterized algebraic decision diagramsAlgebraic decision diagrams (ADDs) [17] are a generalization of ordered binary decision diagrams (BDDs) that representboolean functions {0, 1}n → {0, 1} [18]. A BDD is a data structure that has decision nodes, each node labeled with a booleantest variable with two successor nodes: l (low) and h (high). The arc from a node to its successor l (h) represents anassignment 0(1) to the test variable. BDDs are DAGs whose variable tests on any path from root to leaf follow a fixed totalvariable ordering. BDDs are used to generate the value of a boolean function as follows: given assignments to the booleantest variables in a BDD, we follow branches l or h, until we get to a leaf, which is the boolean value returned by the function.The only difference between an ADD and a BDD is that terminal nodes in an ADD are real values, i.e., ADDs permit thecompact representation of functions {0, 1}n → R. BDDs and ADDs often provide an efficient representation of functions with(cid:20)context-specific independence [16] and shared function structure. For example, the reward function R(x1, x2, x3) =3i=1 xirepresented in Fig. 5 as an ADD exploits the redundant structure of sub-diagrams through its DAG representation.Operations on ADDs can be performed efficiently by exploiting their DAG structure and fixed variable ordering. Examplesof efficient ADD operations are unary operations such as min, max (return the minimum or maximum value in the leaves) that eliminates a variable xi of an ADD; binary operations such asof a given ADD), marginalization over variables (addition (⊕), subtraction ((cid:14)), multiplication (⊗), division ((cid:16)), and even min(·, ·) and max(·,·) (return an ADD with min/maxvalues in the leaves). We refer the reader to [17] for details.(cid:20)xiParameterized ADDs (PADDs) are an extension of ADDs that allow for a compact representation of functions from{0, 1}n → E, where E is the space of expressions parameterized by (cid:10)p (in our case, we further restrict this to the spaceof multilinear expressions of (cid:10)p). For example, the CPT in Fig. 6 represented as a PADD contains leaves consisting of singleparameters while Fig. 8(d) shows a PADD with a leaf containing a more complex parameterized expression.In the following, we formally define PADDs and their basic operations needed to construct efficient solutions for MDP-IPs. Because PADDs are introduced to solve MDP-IPs, we make the following restrictive assumptions: (a) we allow onlymultilinear expressions in the leaves; (b) we only define a subset of PADD operations that could be inherited from ADDs;and (c) we only show these operations are closed (i.e., yield a resulting PADD with multilinear leaves) for the operationsneeded in MDP-IPs. Finally, we contribute a new unary operation MinParameterOut (min(cid:10)p ) specific to PADDs.5.1. PADD: Formal definition, properties and operationsPADDs generalize the constant real-valued leaves of ADDs to polynomials (Poly) expressed in a sum-of-products canonicalform:d0 +(cid:3)(cid:14)dipi jij(13)where the di are constants and the pi j are parameters. Formally, we can define a PADD by the following BNF grammar3:3 We will adopt lowercase ( f ) to refer to a mathematical function, and uppercase (F ) to refer to the function represented structurally as a PADD.K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271505(cid:5)(cid:5)2 for action a1. b) The Parameterized ADD representation for P (xFig. 6. a) Conditional probability table for the state variable x2= 1|x1, x2, x3, x4, a1).F ::= Poly|if(F var) then Fh else Fl(cid:3)Poly ::= d0 +pi j.(cid:14)diijThis grammar is notationally overloaded, so we briefly explain: a PADD node F can either be a terminal node with anexpression of type Poly or a decision node with variable test F var (e.g., x1 or xn) and two branches Fh and Fl (both ofgrammar non-terminal type F ), where Fh is taken when F var = 1 and Fl is taken when F var = 0.The value returned by a function f represented as a PADD F containing (a subset of) the variables {x1, . . . , xn} withvariable assignment ρ ∈ {0, 1}n can be defined recursively by:⎧⎨V al(F , ρ) =⎩if F = Poly :if F (cid:11)= Poly ∧ ρ(F var) = true : Val(Fh, ρ)if F (cid:11)= Poly ∧ ρ(F var) = false : Val(Fl, ρ).PolyThis recursive definition of Val(F , ρ) reflects the structural evaluation of a PADD F by starting at its root node and followingthe branch at each decision node corresponding to the variable assignment in ρ — this continuing until a leaf node isreached, which is then returned as V al(F , ρ). As an example, for the PADD represented in Fig. 6, assigning ρ = {1, 0, 1, 0}for variables {x1, x2, x3, x4} yields Val(F , ρ) = p21.Like ADDs, for any function f (x1, . . . , xn) and a fixed variable ordering over x1, . . . , xn, a reduced PADD is defined as theminimally sized ordered decision diagram representation of a function f .Lemma 5.1. There exists a unique reduced PADD F (the canonical PADD representation of f ) satisfying the given variable orderingsuch that for all ρ ∈ {0, 1}n we have f (ρ) = Val(F , ρ).The proof of this lemma for BDDs was provided by [19] and can be trivially generalized to ADDs and PADDs. Since PADDsallow polynomial leaves, the only change for demonstrating this lemma is that we need to ensure that there exists a way toidentify when two leaf expressions are identical, which can be easily done by (a) sorting the parameters in each multilinearterm, (b) factoring out (grouping terms with the same ordered set of parameters) and summing constants in identicalmultilinear terms, and (c) sorting the list of terms according to the lowest variable index and number of parameters. Withsuch a unique leaf identification method, the proof of [19] generalizes to PADDs and shows that there is a unique canonicalPADD representation for every function from {0, 1}n to polynomials in the form of (13).In fact, not only does such a minimal, reduced PADD always exist for a function f that can be represented as a PADD,but there is a straightforward algorithm for computing it called ReducePADD, which we present in Section 5.2.1. Beforewe present formal PADD algorithms though, we first discuss extensions of the unary and binary operations from ADDs toPADDs. Fortunately, this only requires that operations on the leaves of ADDs are modified to accept and produce resultingpolynomials in the form of (13).5.1.1. Binary operations on PADDsThe binary operations ⊕ (sum) and (cid:14) (subtraction) as defined for ADDs [17] can be extended for PADDs and are alwaysclosed since these operations yield PADDs with leaves in the form of (13). However, the binary operation ⊗ (product) can1506K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527Fig. 7. An example application of Restrict operation and Marginalization operation on a PADD.only yield a PADD with leaves in the form of (13) if the set of parameters (cid:10)p in the leaves of each operand are disjoint.Fortunately, for factored MDP-IPs, we note that the only place ⊗ is used is to compute the product of the DCN CPTs;because of Restriction 4.3 on the usage of parameters pi j in these CPTs, we note that the condition for closed ⊗ operationson PADDs is always satisfied for the required factored MDP-IP operations.However, not all PADD binary operations have simple conditions under which they are closed. We note that PADDs arenot closed under (cid:16) (binary division), i.e., the resulting leaves could be a polynomial fraction and hence cannot be expressedas (13). Similarly, the binary min(·,·) and max(·,·) operations defined for ADDs [17] cannot generally be computed in closedform unless the actual assignment to the parameters (cid:10)p is known. Fortunately, (cid:16), min(·,·), and max(·,·) will not be neededin our proposed solution to factored MDP-IPs.5.1.2. Unary operations on PADDsThe two important classical unary operations for ADDs are restriction (F |xi ) and marginalization (extended to PADDs as follows:(cid:20)xi) and can be easily• Restriction of a variable xi to either true (F |xi =1) or false (F |xi =0) can be calculated by replacing all decision nodes forvariable xi with either the high or low branch, respectively. This operation can be used to do marginalization as weshow next. This operation does not affect the leaves of the decision diagram, so its extension from ADDs to PADDs isstraightforward.(cid:20)• The marginalization or sum_out operation (represented as) eliminates a variable xi from an ADD. It is computed asxithe sum of the true and false restricted functions, i.e., (F |xi =1 ⊕ F |xi =0). Since ⊕ is closed for PADDs, marginalization isalso closed for PADDs. An example is shown in Fig. 7.The classical unary min(·) and max(·) operations for ADDs cannot generally be computed for PADDs unless the actualassignment to the parameters (cid:10)p is known. However, we will not need this particular PADD operation for factored MDP-IPs,but rather a new unary operation for PADDs called MinParameterOut, which in our case will make the choices of Nature inEq. (9).Definition 5.2 (MinParameterOut operation). Represented as min(cid:10)p(F ), this operation takes as input (1) a PADD F and (2) aset C of global constraints over the PADD’s parameters, and returns an ADD. We note that an ADD is a special case of aPADD with constant expressions at its leaves, which implies that min(cid:10)p(F ) is closed for PADDs. This unary operation calls anonlinear solver for each leaf expression e in the form of (13) to compute c = min(cid:10)p(e) w.r.t. constraints C and replaces theleaf e with the constant c.Because the set of variable assignments that can reach each PADD leaf are disjoint, each leaf can be minimized indepen-dently of the others. This is precisely the operation we’ll need for factored MDP-IPs, since we note that Nature performs it’sminimization independently per state s in (9), and every path in the PADD will correspond to a different state assignment.An example of min(cid:10)p(F ) is shown in Fig. 12.5.2. PADD algorithmsPreviously we have discussed PADD algorithms conceptually, in this subsection, we discuss how to implement efficientoperations for PADDs. In the following algorithms we use four hash tables: ReduceCache, NodeCache, ApplyCache and Min-ParCache. We use the notation key → value to represent key/value pairs in the hash table. The table NodeCache stores aunique identification for each node (representing sub-diagrams by unique identifiers), the hash table ReduceCache storesthe reduced canonical nodes (the results of previous Reduce operations), the table ApplyCache stores the results of previousApply operations (so we can avoid redundant calculations) and the hash table MinParCache stores the results of previousMinParameterOut operations.K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271507Algorithm 1: ReducePADD(F)input : F (root node id for an arbitrary ordered decision diagram)output: F r (root node id for reduced PADD)begin//if terminal node, return canonical terminal nodeif F is terminal node thenreturn canonical terminal node for polynomial of F ;//use recursion to reduce sub diagramsif F → F r is not in ReduceCache thenFh = ReducePADD(Fh );Fl = ReducePADD(Fl );//get a canonical internal node idF r = GetNode(F var , Fh , Fl);insert F → F r in ReduceCache;return F r ;end12345678910111213Algorithm 2: GetNode((cid:3)var, Fh, Fl(cid:4))input : (cid:3)var, Fh, Fl(cid:4) (variable and true and false branches node ids for internal node)output: F r (canonical internal node id)123456789begin//redundant branchesif Fl = Fh thenreturn Fl ;//check if the node exists previouslyif (cid:3)var, Fh, Fl(cid:4) → id is not in NodeCache thenid = new unallocated id;insert (cid:3)var, Fh, Fl(cid:4) → id in NodeCache;return id;10end5.2.1. Reduce algorithm for PADDsWhile we know there exists a unique canonical form for every function expressible as a PADD (Lemma 5.1), the algo-rithm ReducePADD actually allows the efficient construction of this unique canonical PADD representation from an arbitraryordered decision diagram with polynomial leaves of type (13).Algorithm 1 recursively constructs such a reduced PADD from the bottom up. In this algorithm, an internal node is repre-sented as (cid:3)F var, Fh, Fl(cid:4), where F var is the variable name, and Fh and Fl are the true and false branch node ids, respectively.Additionally, the input F refers to an arbitrary node, while the returned value Fr refers to a canonical node id. Reducedcanonical nodes are stored in the hash table ReduceCache and the helper function GetNode (Algorithm 2) ensures that re-dundant decision tests at internal nodes are removed. The table NodeCache used in the function GetNode stores a uniqueidentification for each node.An example of the application of the ReducePADD algorithm is shown in Fig. 8. The hollow arrow points to the internalnode F that is being evaluated by ReducePADD after the two recursive calls to ReducePADD (lines 7 and 8) but before line10. Fig. 8(a) shows the input diagram for the algorithm where x3 is being evaluated by ReducePADD creating two canonicalterminal nodes for 0.3 + 5p12 and 0. Note that while evaluating node x3 (on the left), the execution of line 10 will result inthe insertion of (cid:3)x3, 0.3 + 5p12, 0(cid:4) in the NodeCache hash table. Fig. 8(b) shows the resulting evaluation of node x3 on theright, which returns the same previous canonical terminal nodes for 0.3 + 5p12 and 0. And again, after executing line 10,the GetNode algorithm will return the same id for (cid:3)x3, 0.3 + 5p12, 0(cid:4), previously inserted in the NodeCache. Fig. 8(c) showsthe evaluation of x2. Note that Fh and Fl are equal, thus after getNode is called, Fl is returned and as a consequence x2disappears. Finally, Fig. 8(d) shows the canonical PADD representation of the input. Note that ReducePADD (Fl) returned thesame canonical terminal node that exists previously for the node 0.The running time and space of ReducePADD are linear in the size of the input diagram since the use of the ReduceCacheguarantees that each node is visited only once and at most one unique reduced node is generated in the canonical diagramfor every node visited.5.2.2. Apply algorithm for binary operations for PADDsThe notation we will use in this paper for PADDs is shown in Fig. 9. Any operation with two PADDs, F 1 and F 2, resultsand two new sub-diagrams Fh and Fl. Note that F i,hin a new canonical PADD Fr , with eventually a new root node F varand F i,l represent sub-diagrams.rFor all binary operations, we use the generic function Apply(F 1, F 2, op) (Algorithm 3) and the result computation table inthe helper function ComputeResult (Table 1) that supports operations between arbitrary PADD nodes and polynomial leaves.1508K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527Fig. 8. A step-by-step illustration of the application of ReducePADD algorithm (Algorithm 1) where a) and d) are the input and output PADDs, respectively.Fig. 9. The notation used in the Apply and ChooseVarBranch algorithms.Fig. 10. An example of PADDs multiplication.Table 1 is implemented as a method named ComputeResult, which is simply a case structure for each line of Table 1. Noticethat lines 2–9 of Table 1 define the result of PADD operations in special cases that avoid unnecessary computation in Apply.The Apply algorithm (Algorithm 3) has as input two operands represented as canonical PADDs, F 1 and F 2, and a binaryoperator op ∈ {⊕, (cid:14), ⊗}; the output is the result of the function application represented as a canonical PADD F r . Apply(F 1, F 2, op) first checks if the result can be immediately computed by calling the method ComputeResult (line 3). If theresult is null, it then checks whether the result was previously computed by checking in the ApplyCache, which stores theresults of previous Apply operations (line 6). If there is not a cache hit, Apply chooses the earliest variable in the orderingto branch on by calling the auxiliary function ChooseVarBranch (Algorithm 4) and then branches on this variable with tworecursive Apply calls, one to compute Fl and other to compute Fh. After that, the results of these two operations are checkedfor redundancy elimination throughout GetNode function. An example of PADD multiplication via Apply algorithm is shownin Fig. 10.5.2.3. MinParameterOut algorithm for PADDsThe MinParameterOut algorithm (Algorithm 5) has as input a canonical PADD F and a set of constraints C over the PADD’sparameters; the output is the result of calling the nonlinear solver for each PADD leaf, represented as a canonical ADD F r .MinParameterOut first checks if F is a constant terminal node, in this case it is not necessary to call the nonlinear solverfor this leaf. If the terminal node is not a constant then we need to make a call to the nonlinear solver passing the leafexpression as an objective to minimize subject to C (line 7). If F is not a terminal node, Algorithm 5 recursively traversesthe PADD. Similar to ReducePADD, an internal node is represented as (cid:3)F var, Fh, Fl(cid:4) and previously computed canonical nodesare stored in the hash table MinParCache. The helper function GetNode (Algorithm 2) ensures again that redundant decisiontests at internal nodes are removed.With this last specification of MinParameterOut, we have formally described almost all of the PADD algorithms we willneed in our factored MDP-IP solution. We omit the restriction and marginalization algorithms for PADDs since they areidentical to the same operations for ADDs (i.e., these operations don’t modify the leaves, which is the only place that PADDsand ADDs differ).K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271509Algorithm 3: Apply(F 1, F 2, op)input : F 1 (root node id for operand 1),F 2 (root node id for operand 2),op (binary operator, op ∈ {⊕, (cid:14), ⊗})output: F r (root node id for the resulting reduced PADD)begin//check if the result can be immediately computedif ComputeResult(F 1, F 2, op) → F r (cid:11)= null thenreturn F r ;//check if we previously computed the same operationif (cid:3)F 1, F 2, op(cid:4) → F r is not in ApplyCache then//choose variable to branchvar = ChooseVarBranch(F 1, F 2);//set up nodes for recursionif F 1 is non-terminal ∧ var = F var1thenelseF v1lF v1h= F 1,l ;= F 1,h ;F v1l,h= F 1;F v2lF v2h= F 2,l ;= F 2,h ;if F 2 is non-terminal ∧var = F var2thenelseF v2l,h= F 2;l, F v2lh , F v2//use recursion to compute true and false branches for resulting PADDFl = Apply(F v1, op);Fh = Apply(F v1h , op);F r = GetNode(var, Fh, Fl);//save the result to reuse in the futureinsert (cid:3)F 1, F 2, op(cid:4) → F r into ApplyCache;return F r ;end123456789101112131415161718192021222324252627Algorithm 4: ChooseVarBranch(F 1, F 2)input : F 1 (root node id for operand 1),F 2 (root node id for operand 2)output: var (selected variable to branch)begin//select the variable to branch based on the order criterionif F 1 is a non-terminal node thenif F 2 is a non-terminal node thenthencomes before F var2if F var1var = F var1 ;elsevar = F var2 ;elsevar = F var1 ;elsevar = F var2 ;return var;end123456789101112131415166. Factored MDP-IP value iterationIn the two previous sections, we showed a compact representation for factored MDP-IPs based on dynamic credal net-works (DCNs) and parameterized ADDs (PADDs) and the respective algorithms needed to manipulate DCNs and PADDs. Inthis section, we will present our first exact value iteration solution that exploits both of these representations. This solutionis an extension of the SPUDD [3] algorithm. First, we give a mathematical description of the proposed solution and thenproceed to formally specify the algorithm that computes it.6.1. SPUDD-IP descriptionWe extend the SPUDD [3] algorithm for exploiting DBN and ADD structure in the solution of factored MDPs to a novelalgorithm SPUDD-IP for exploiting DCN and PADD structure in the solution of factored MDP-IPs. We begin by expressing1510K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527Table 1Input case and result for the method ComputeResult for binary operations ⊕, (cid:14) and ⊗ for PADDs.Case number123456789; F 2 = Poly2Case operationF 1 op F 2; F 1 = Poly1F 1 ⊕ F 2; F 2 = 0F 1 ⊕ F 2; F 1 = 0F 1 (cid:14) F 2; F 2 = 0F 1 ⊗ F 2; F 2 = 1F 1 ⊗ F 2; F 1 = 1F 1 ⊗ F 2; F 2 = 0F 1 ⊗ F 2; F 1 = 0otherReturnPoly1 op Poly2F 1F 2F 1F 1F 200nullAlgorithm 5: MinParameterOut(F, C )input : F (root node id for a PADD),C (set of constraints)output: F r (root node id for an ADD)begin//if terminal node, call the solver and return the valueif F is terminal node thennode = canonical terminal node for polynomial of F ;if node is a constant thenreturn node;c = CallNonLinearSolver(node,C );return canonical terminal node for the constant c;//use recursion to compute sub diagramsif F → F r is not in ReduceCacheMinPar thenFh = MinParameterOut(Fh );Fl = MinParameterOut(Fl );//get a canonical internal node idF r = GetNode(F var , Fh , Fl);insert F → F r in ReduceCacheMinPar;return F r ;end1234567891011121314151617MDP-IP value iteration from (10) in the following factored form using the transition representation of (11) and operationson decision diagrams4:⎧⎪⎨⎫⎪⎬D D ((cid:10)x) = maxV ta∈ AR D D ((cid:10)x, a) ⊕ γ min(cid:10)p⎪⎩(cid:3)n(cid:21)P D D(cid:10)x(cid:5)i=1(cid:6)(cid:6)(cid:7)(cid:7)(cid:5)xi|paa(cid:5)xi, aV t−1D D(cid:6)(cid:7)(cid:5)(cid:10)x.⎪⎭(14)(cid:5)|paa(xBecause the transition CPTs in the MDP-IP DCN contain parameters (cid:10)p, these CPTs must be represented in decision diagram(cid:5)i), a)). On the other hand, the reward R D D ((cid:10)x, a) can be represented as an ADD since itformat as PADDs ( P D D (xicontains only constants (for the purpose of operations, recall that ADDs are special cases of PADDs). Although it may appearD D ((cid:10)x) is a PADD, we note that the parameters (cid:10)p are “minimized”-out w.r.t. the side constraints on (cid:10)pthat the form of V t(cid:2) is the MinParameterOut operation on PADDs, that performs theduring the min(cid:10)pminimization over the parameters by calling a nonlinear solver for each leaf and returns an ADD). This is crucial, becauseD D ((cid:10)x)the maxa∈ A can only be performed on ADDs (recall that max is not a closed operation on PADDs). Thus the resulting V tcomputed from the maxa∈ A has constant leaves and can be expressed as the ADD special case of PADDs.(cid:2) operation in (14) (remember that min(cid:10)pTo explain the efficient evaluation of (14) in more detail, we can exploit the variable elimination algorithm [20] in the(cid:5)(cid:5)i for i (cid:11)= 1, we can “push” the1 is not dependent on any other x(cid:10)x(cid:5) . For example, if x(cid:20)marginalization over all next states(cid:5)1 inwards to obtain:sum over x⎧⎪⎪⎪⎨⎪⎪⎪⎩D D ((cid:10)x) = maxV ta∈ AR D D ((cid:10)x, a) ⊕ γ min(cid:10)p(cid:3)n(cid:21)(cid:6)(cid:5)xi|paa(cid:7)(cid:6)(cid:5)xi, aP D D(cid:7)(cid:3)(cid:5)i (i(cid:11)=1)xi=1(i(cid:11)=1)(cid:5)x1(cid:6)(cid:5)x1|paa(cid:6)(cid:7)(cid:7), a(cid:5)x1V t−1D D(cid:7)(cid:6)(cid:5)(cid:10)xP D D⎫⎪⎪⎪⎬⎪⎪⎪⎭.(15)We show this graphically in the example of Fig. 11. Here, we have the ADD representation for the first value function V 0(cid:5)R D D (Fig. 11(a)), which we multiply by P D D (x1=(cid:5)|paa(x1), a) (Fig. 11(b)) yielding the result (Fig. 11(c)) and sum this out overD D4 We use D D for the functions represented by ADDs or PADDs, since the first is a special case of the second.K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271511Fig. 11. a) We show V 0(cid:5)representation for P (x1which is a PADD.A D D|x1, x2, notreboot) (CPT= R(x1, x2) for the unidirectional-ring topology of SysAdmin domain with 2 computers represented as an ADD. b) The PADDx(cid:5)1,notreboot resulting in a PADD. d) The result of summing out over xnotreboot ). c) The multiplication V 0⊗ CPTA D D(cid:5)1(cid:5)1x(cid:5)(cid:5)|paa(x2), a),1 to obtain the final result (Fig. 11(d)). Then we can continue with xx(cid:5)(cid:5)(cid:5)i to compute (cid:2). After this (cid:2) does not contain anymore the variables xsumming out over x2, and repeating for all xi , butonly the variables xi .(cid:5)(cid:5)2, multiplying this result by the P D D (x2Representing the contents of (cid:2) as f ((cid:10)x, a, (cid:10)p), we obtain(cid:25)D D ((cid:10)x) = maxV ta∈ AR D D ((cid:10)x, a) ⊕ γ min(cid:10)pf ((cid:10)x, a, (cid:10)p)(cid:26).(16)Note that min(cid:10)p f ((cid:10)x, a, (cid:10)p) leads to a separate nonlinear expression minimization for every (cid:10)x and every a subject to the setC of side linear constraints on (cid:10)p (given with the DCN specification) since this follows from the definition of the MDP-IPBellman equation — every state gets its own minimizer and each PADD leaf corresponds to a set of states with exactlythe same minimization objective. This optimization problem may be represented as a simple multilinear program due to(cid:5)Restriction 4.3 that guarantees each pi j only appears in the DCN CPT for xi (this prevents multiplication of pi j by itself,thereby preventing exponents exceeding 1). This restriction is important to guarantee the existence of exact solutions andthe existence of efficient implementations that we can use in practice to solve multilinear programs. We note that this isonly a restriction on the factored MDP-IP models themselves.To demonstrate how the min(cid:10)p f ((cid:10)x, a, (cid:10)p) is performed on PADDs, we refer to Fig. 12. Here, each leaf expression inf ((cid:10)x, a, (cid:10)p) (Fig. 12(a)) given by the PADD corresponds to the function that Nature must minimize in each state. We cru-cially note that the PADD aggregates states with the same minimization objective, thus saving time-consuming calls to themultilinear solver. We will observe this time savings in our experiments. Now, we need to make a call to the multilinearsolver for each leaf, passing the leaf expression as the objective to minimize subject to the side constraints C of our DCNthat specify the legal (cid:10)p — after the minimization, we can replace this leaf with a constant for the optimal objective valuereturned by the multilinear solver (Fig. 12(b)). We can see that after the min(cid:10)p operation, all PADDs are simplified to thespecial case of ADDs with leaf nodes that are constants.To complete one step of factored MDP-IP value iteration, we take the ADD resulting from the min(cid:10)p operation, multiply itby the scalar γ , add in the reward R D D ((cid:10)x, a), and finally perform a sequence of binary ADD max(·,·) operations to computethe maxa, thus yielding the ADD V tD D ((cid:10)x) and completing one step of value iteration from (14).D D ((cid:10)x) from the ADD for V t−16.2. SPUDD-IP algorithmFactored MDP-IP value iteration is formally specified in the following two procedures:Solve (Algorithm 6) constructs a series of t-stage-to-go value functions V tD D that are represented as ADDs. First wecreate the PADD representation of all DCN CPTs in the MDP-IP and initialize the first value function to 0 (line 3). The loopis repeated until a maximum number of iterations or until a Bellman error BE termination condition (BE < tol) is met.1512K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527Fig. 12. The MinParameterOut operation example. a) The PADD before minimization and a multilinear program for the first leaf, the solution for this leaf isthe constant value c1. b) The resulting ADD after the minimization at all leaves.Fig. 13. a) The value function V twithin error of each other have been merged and averaged and the resulting ADD simplified.D D represented as an ADD. b) the result of ApproxADD applied to V tD D with approximation error = 1; note that the leavesWe note that setting the tolerance tol according to (7) guarantees (cid:4)-optimality for MDP-IPs since the same terminationconditions used for MDPs directly generalize to MDP-IPs in the discounted case (γ < 1). At each iteration the Regressalgorithm is called (line 13) and V tD D for each action a). Af-|V t((cid:10)x) − V t−1((cid:10)x)| is computed and tested for termination. We observe in Algorithm 6 the parameterster this, BE = max(cid:10)xδ, APRICODD, Objective, and Vmax play no role now; they are used for approximation as we explain in the next section (inparticular we use δ = 0 to obtain an exact solution by the SPUDD-IP).D D is updated with the max over all Q tD D (there is a Q tD D , i.e, it regresses V t−1Regress (Algorithm 7) computes Q tD D through action a that provides the values Q tD D that couldbe obtained if executing a and acting so as to obtain V t−1D D thereafter. During regression we “prime” the variables using(cid:5)i (since the V ithe function convertToPrimes that converts each xi to xD D is now part of the “next” state) and the CPTs for(cid:5)i ,action a are multiplied in and summed out (lines 4–6). We assume here there are no synchronic arcs among variables x(cid:5)j for i (cid:11)= j in the DCN. If synchronic arcs are present, the algorithm can be simply modified to multiply in all relevantxCPTs. After this, the MinParameterOut function is performed that calls the multilinear solver to find the minimizing (cid:10)p foreach leaf in the PADD w.r.t. the side linear constraints C on the DCN (line 11), resulting in an ADD. We note that if a leafis already a constant, then the multilinear solver call can be avoided altogether; this observation will prove important laterwhen we introduce objective pruning. Finally, the future value is discounted and the reward ADD is added in to completethe regression. Objective and error are used for approximate value iteration and will be discussed later.7. Factored MDP-IP approximate value iterationThe previous SPUDD-IP exact value iteration solution to factored MDP-IPs often yields an improvement over flat valueiteration as we will demonstrate in our experiments. But as the number of state variables in a problem grows larger, it oftenbecomes impossible to obtain an exact solution due to time and space limitations.Approximate value iteration (AVI) is one way to trade off time and space with error by approximating the value func-tion after each iteration. In this section, we propose two (bounded) AVI extensions of SPUDD-IP: the APRICODD-IP and theObjective-IP algorithms. Each method uses a different way to approximate the value, but both methods incur a maximumof δ · Vmax error per iteration where Vmax as computed in Solve represents the maximum possible value at each stepof value iteration (with 0 < δ (cid:3) 1). By making the approximation error sensitive to Vmax we prevent over-aggressive valueapproximation in the initial stages of AVI when values are relatively small as suggested in [4]. Even with this value ap-K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271513Algorithm 6: Solve (MDP-IP, tol, maxIter, δ, APRICODD, Objective)input : MDP-IP (given by (cid:3)S, A, R, K , γ (cid:4)),tol (tolerance that guarantees (cid:4)-optimality),maxIter (maximum number of iterations),//variables used for approximate value iterationδ (fraction of the maximum possible value, with 0 < δ (cid:2) 1),APRICODD (APRICODD = true to execute APRICODD-IP),Objective (Objective = true to execute Objective-IP)output: V tDD (t-state-to-go value function)123456789101112131415161718192021222324251234567891011121314DD until termination condition is metbeginADD= 0;(cid:5)|pa(xi ), a) for MDP-IP;(cid:5)Create PADD: P D D (xiV 0//Vmax is the maximum possible value at each iterationVmax = max(RDD);t = 0;//construct t-stage-to-go value functions V twhile i < maxIter dot = t + 1;= −∞;V t//update V tforeach a ∈ A doDDDD with the max over all Q tDDDD=Regress(V t−1Q tDD, Q tDD=max(V tV tDD , a, δ · Vmax, Objective);DD);//compute Bellman Error (BE) and check for terminationDD= V t(cid:14) V t−1DD ;Diff DDBE = max(max(Diff DD),− min(Diff DD));if BE < tol thenbreak;//approximate value iteration: APRICODD-IPif APRICODD pruning then=ApproxADD (V tDD, δ · Vmax);V tDDVmax = max(RDD) + γ Vmax;return V tDD ;endAlgorithm 7: Regress(V DD, a, error, Objective)input : V DD (value function),a (action),error (maximum error),Objective (Objective = true to execute Objective-IP)output: Q DD (the value function obtained if executing a and acting so as obtain V DD thereafter)begin(cid:5)Q DD = convertToPrimes(V DD); //convert variables xi to xi//CPTs are multiplied in and summed out(cid:5)i in Q DD dofor all x(cid:5)Q DD = Q DD ⊗ P D D (x(cid:20)iQ DD =(cid:5)|pa(xi ), a);Q DD;x(cid:5)i//approximate value iteration: Objective-IPif Objective pruning thenQ DD =approxPADDLeaves ( Q DD, error);//call the nonlinear solver for each PADD leaf — returns an ADDQ DD = MinParameterOut ( Q DD,C );Q DD = RDD ⊕ (γ ⊗ Q DD) ;return Q DD;endproximation at every iteration, satisfying the termination condition BE < tol for some tol still yields strict guarantees on theoverall approximation error given by (7) as discussed previously for SPUDD-IP.7.1. APRICODD-IP algorithmThe APRICODD algorithm [4] provides an efficient way of approximating the ADD value representation for a factored MDP,reducing its size and thus reducing computation time per iteration. This approach immediately generalizes to MDP-IPs sincethe value function V tD D is also an ADD. To execute APRICODD-IP AVI for MDP-IPs, we simply call Solve (Algorithm 6) withAPRICODD = true and set δ (0 < δ (cid:3) 1) to some fraction of the maximum possible value Vmax with which to approximatecalling the algorithm ApproxADD (line 22 Algorithm 6).1514K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527Algorithm 8: ApproxADD(valueiD D ,error)input : valueiD D (an ADD),error (maximum error)output: a new ADDbegin//collect all leaves of the ADDleavesold=collectLeavesADD (valuei//group the leaves that can be merged within maximum error{leavesold → leavesnew} = mergeLeaves (leavesold, error);//return a simplified ADDreturn createNewDD (valueiD D , {leavesold → leavesnew});D D );end12345678ApproxADD (Algorithm 8) has two inputs: (1) a value function represented as an ADD and (2) an approximation errorto merge the leaves. The output is a new ADD with the merged leaves. The algorithm first collects all leaves of the ADDand determines which can be merged to form new values without approximating more than error. The old values are thenreplaced with these new values creating a new (minimally reduced) ADD that represents the approximated value function.An illustrative example is shown in Fig. 13.collectLeavesADD compiles the leaves of the ADD and puts them in a set (leavesold). In the example in Fig. 13, theset of old leaves is {9, 0, 10, 1}.mergeLeaves groups together the leaves that can be merged within error and computes the average for each group,creating a new set of leaves (leavesnew). In the example of Fig. 13, the groups than can be merged within an error = 1 are{9, 10} and {0, 1}; and the new leaves are 9.5 and 0.5.createNewDD creates a simplified ADD, replacing the old leaves by the new ones. The result of this operation in theexample is shown in Fig. 13(b)).7.2. Objective-IP algorithmAPRICODD is an effective extension of SPUDD for factored MDPs (not MDP-IPs) because it reduces the size of the valuefunction ADDs, which largely dictate the time complexity of the SPUDD algorithm. However, in solving (factored) MDP-IPs,the time is dictated less by the size of the value function ADD and more by the number of calls to the multilinear optimizer(cid:2). SPUDD-IP started to attack this source of time complexity by aggregating states with the sameto compute the min(cid:10)p(cid:2). Our goal with the Objective-IP pruning algorithm will be to more closely target the source of timeobjective for the min(cid:10)pcomplexity in an AVI version of SPUDD-IP by approximating the objectives in an attempt to avoid calling the solver altogether.To execute Objective-IP for MDP-IPs, we simply call Solve (Algorithm 6) with APRICODD = false, Objective = true and setδ (0 < δ (cid:3) 1) to some fraction of the maximum possible value Vmax. Noting that each PADD leaf in Regress function isa multilinear objective, we simplify it by calling ApproxPADDLeaves (line 9 Algorithm 7) just prior to carrying out themultilinear optimization at the leaves of that PADD (line 11 Algorithm 7).ApproxPADDLeaves (Algorithm 9) is called for a PADD by Regress when Objective = true. It takes as input a PADDand the maximum error, and the output is a new PADD with approximated leaves using the upper and lower bounds of theparameters. The main loop of the algorithm attempts to approximate each leaf in a PADD (lines 3–17). To approximate themultilinear term i, Algorithm 9 first computes the average of their maximum and minimum values (line 8), this requiresknowing the absolute upper pUi j and lower bounds p Li j for any pi j , which can be easily precomputed once for the entire= min pi j subject to the side linear constraints C= max pi j and p LMDP-IP by calling the nonlinear solver to compute pUi ji jon all CPTs. After that, Algorithm 9 computes the error incurred by using these maximum and minimum values (line 10).If the actual accumulated error for the leaf (curError + termErrori ) is less than the maximum error (error), the term i isremoved (line 13) and replaced by the average (line 14). In some cases the complexity of the leaf expression may bereduced, in others, it may actually be reduced to a constant. Note that the leaves are each approximated independently, thiscan be done since each leaf corresponds to a different state (or set of states) and the system can only be in one state at atime. Furthermore, we can guarantee that no objective pruning at the leaves of the PADD incurs more than error after themultilinear optimization is performed:Theorem (ApproxPADDLeavesError Bound). Given an MDP-IP, its precomputed constants p Lmum approximation error, then whenever ApproxPADDLeaves (Algorithm 9) reduces a leaf d0 +expression, the approximation error in the objective minimization (min(cid:10)p) of that leaf is bounded by error.i j and pUi j for all pi j , and the maxi-(cid:20)#termsj pi j to a simpleri=1(cid:19)diProof. We begin by showing that the approximation error induced by removing a single term i from the objective isbounded by termErrori . To do this, we first find upper and lower bounds on term i (dij pi j ) based on the legal values ofi j). Thus for any possible legal values of (cid:10)p the term(cid:10)p. We know the maximal (minimal) possible value for each pi j is pUi must be bounded in the interval [Li, U i] with Li and U i defined as follows:i j (p L(cid:19)K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271515Algorithm 9: ApproxPADDLeaves(DD, error)12345678910111213141516171819input : DD (parameterized ADD),error (maximum error)output: DD (simplified parameterized ADD)begin(cid:19)//approximate each leaf independently(cid:20)foreach leaf : d0 +#termsi=1i = 1, curError = 0;//for all terms of the leaf, prune them if possiblewhile curError < error ∧ i (cid:2) #terms doj pi j ∈ DD dodi+(cid:19)(cid:19)j pUi j//compute the average of max and min values for term inewValue = dij p Li j ) ;2 (//compute the error of using max and min values for term itermErrori = | dij pUj p L2 (i j//if within error, prune term i from leafif curError + termErrori < error thenj pi j from leaf ;i j)| ;(cid:19)(cid:19)(cid:19)−remove term did0 = d0 + newValue;curError = curError + termErrori ;i = i + 1;return DD;end(cid:27)Li =di > 0 didi < 0 di(cid:19)(cid:19)j p Li jj pUi j,(cid:27)U i =di > 0 didi < 0 di(cid:19)(cid:19)j pUi jj p Li j.2Let g be a value for term i and (cid:28)g = Li +U i|) = | Li −U i(cid:28)g|, |U i − (cid:28)g|) = max(| Li −U i|, | U i −Li(cid:20)(cid:19)2Now, let OBJ1 = d0 +di2#termsi=1mal objective value using (cid:10)p = (cid:10)p1. Let OBJ2 = d0 + (cid:28)g +after replacing term 1 with L1+U 1We want to prove that −| L1−U 122, then maxg|g − (cid:28)g| occurs at g = Li or g = U i . So the max termErrori = max(|Li −| as computed in Algorithm 9.j pi j be the original non-approximated objective expression to minimize and v 1 the opti-j pi j be the approximated objective expression to minimize#termsi=2(cid:20)(cid:19)diand v 2 the optimal objective using (cid:10)p = (cid:10)p2.| < v 1 − v 2 < | L1−U 1(cid:5)1(cid:19)|. First we prove the second part of this inequality. Using (cid:10)p2 in OBJ1j p1 j, (cid:10)p2) + v 2 − d0 − (cid:28)g (where eval is a function to= d0 + eval(d12and the approximated objective expression we obtain vevaluate the term with the assigned values). Because v 1 is optimal v 1 (cid:3) v(cid:9)(cid:8)2(cid:5)1 then:v 1 − v 2 (cid:3) evald1p1 j, (cid:10)p2−(cid:28)g.(17)(cid:14)j|, i.e., −| L1−U 1Additionally for any possible legal values of (cid:10)p and for (cid:10)p2, |eval(d1j p1 j,(cid:10)p2) −(cid:28)g < | L1−U 1|. The proof of the first inequality follows by thesame reasoning, but this time substituting (cid:10)p1 into OBJ2 and using the non-approximated objective expression. Thus, we obtain(cid:5)v2|. From this equation and (17) we obtain v1 − v2 < | L1−U 1j p1 j, (cid:10)p1). Because v 2 is optimal v 2 (cid:3) v= d0 + (cid:28)g + v 1 − d0 − eval(d1j p1 j, (cid:10)p2) −(cid:28)g| < | L1−U 1| < eval(d1(cid:5)2 then:(cid:19)2222(cid:19)(cid:19)(cid:8)v 2 − v 1 (cid:3) (cid:28)g − evald1(cid:9)p1 j, (cid:10)p1.(cid:14)j(cid:19)| < eval(d1j p1 j, (cid:10)p1) − (cid:28)g < | L1−U 12|. From this equation(18)Additionally for any possible legal values of (cid:10)p and for (cid:10)p1, −| L1−U 1and (18) we obtain v1 − v2 > −| L1−U 1|.22This bounds the objective approximation error for one term approximation and by simple induction, we can additivelybound the accumulated error for multiple approximations as calculated using curError in Algorithm 9. (cid:2)8. Experimental resultsBefore we delve into experimental results involving the SPUDD-IP, APRICODD-IP, and Objective-IP algorithms contributedin the previous sections, we begin by describing the factored MDP-IP domains used in our experiments.1516K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15278.1. DomainsWe perform experiments with three factored MDP-IP domains: Factory [4], SysAdmin [5] and Traffic (a new domain).In the following, we review Factory and introduce the new Traffic domain; SysAdmin was already introduced in Section 4.8.1.1. Factory domainThe Factory domain [4] is based on a manufacturing problem in which connected, finished parts are produced. Theparts must be shaped, polished, painted and connected by bolting, welding or gluing them. In particular in Factory do-main the agent’s task is connect two objects A and B. The agent can choose between the following actions: shape(x),handPaint(x), polish(x), drill(x), weld(x, y), dip(x) (paint x by dipping it), bolt(x, y) (connect objects x and y by bolting them)and glue(x, y) (connect objects x and y by gluing them) and sprayPaint(x). sprayPaint(x) yields a lower quality of paintingthan handPaint(x).The main variables in this domain are:• connected and connectedWell, that represent if objects A and B are simply connected (e.g. by gluing) or are well con-nected (e.g. by welding them). The only reason for the objects well connected became not connected is when the agentshapes one of them.• apainted, bpainted, apaintedWell and bpaintedWell are variables to represent the painted state of the object respectively.Painted object remains painted if it is not shaped, polished or drilled.• ashaped bshaped represent object A shaped and B shaped respectively. Shaped part remains shaped if it is not drilled.• asmooth and bsmooth, an object becomes smoothed if the agent execute the action polish and it succeeds. Smoothedobject remains smoothed if it is not shaped or drilled.• adrilled and bdrilled, an object becomes drilled if the action drill is apply and it succeeds.There are other variables that describe the things that are available in the environment to be used by the agent such as:spraygun, glue, bolts, drill and clamps. Additionally, the variable skilledlab represents the existence of skilled labor.The quality required for the finished product is represented by the variable typeneeded and can be high-quality or low-quality. The process and the reward depend directly on the quality required. For example, when high-quality is required,hand-painted, drilled and bolted objects will have more reward while spray-painted and glued objects will obtain littlereward. Additional variables can be included in the problem to generate different instances.To obtain a factored MDP-IP, we introduce uncertainty in the bolt action for the variable connected as follows. The successprobability of the bolt action for two objects that are not connected before, when there are bolts, A is drilled and B is drilledis p1. In the case when the two objects are not drilled but there are bolts, the success probability is p2. These probabilitiesare constrained by 0.2 + p2 (cid:3) p1 (cid:3) 1 and 0.5 (cid:3) p2 (cid:3) 1. Note that p1 should always be an equal or higher probability thanp2 (since the process associated with p1 is more likely to succeed), hence the implied constraint p2 (cid:3) p1.8.1.2. A new domain: TrafficWe introduce Traffic, a factored MDP-IP domain motivated by a real traffic intersection control problem modeled usingcellular transition model dynamics [21]. While this is not meant to be an accurate large-scale traffic model over long stretchesof road, it should still approximately model local traffic propagation at busy intersections where speeds are necessarilylimited by queuing and traffic turn delays.A graphical representation with examples of state variables are given in Fig. 14. We encode our traffic state as (cid:10)x =(x1, . . . , xn) where (cid:10)x ∈ {O , U }n indicating that each traffic cell xi (1 (cid:3) i (cid:3) n) is either occupied O or unoccupied U .Our basic traffic model for intermediate road cells is that a car will move forward into the next cell as long as it isunoccupied, otherwise it stops in its current cell and waits.For each intersection road cell x j (i.e., leading into an intersection), we define a state variable t j ∈ {turn, no-turn} indicatingwhether a car in xi will intend to turn into oncoming traffic or not. The state variable t j is drawn randomly with probabilitypt that a car will turn when a new car arrives. When determining the update for x j , we note that it can always go straightor turn left on a green, but whether it can cross the opposing lane to make a right turn depends on the opposing trafficlight state and the opposing traffic cell states to and xo (two opposing right-turning cars may safely turn though and this isallowed by conditioning on to).We refer to a boundary traffic cell xk as a feeder road cell since new cars are introduced at these points. We assume thatwhen the cell is not occupied, new cars arrive on a time step with probability pa.Finally, we have state variables (cid:10)c encoding the current state of the light cycle. The action set is simply to remain in thesame state or advance in the sequence: A = {advance, no-change}. In Fig. 14, we have (cid:10)c = (c1, c2, c3, c4), where one mayinterpret each binary ci as indicating whether the intended light is green (or not). However, each ci need not be binary, itcould have an additional state for the period between green lights before advancing to the next cycle. We need not committo a particular state sequence here, rather we simply rely on a model-specific function next-state((cid:10)c) to generate the nextstate from the current when the lights advance.With this high-level description, we now proceed to define the DCN, reward, and specific Traffic instance configurationsused in this article.K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271517Fig. 14. Diagram showing a 4-way single-lane intersection with cells (dotted boxes) and various state variables used in our state description. Note thatwe do not model road cells that exit the intersection as we assume that cars freely exit the boundaries of the model once they have passed through theintersection.Traffic DCN transition model. Based on the above description, the transition model is provided in a compact factored formatas a dynamic credal network (DCN) [11,15], subdivided into different functional subcomponents as follows.Light cycle transition. Here we simply model the effect of a no-change or advance action on the light state:(cid:7)(cid:6)(cid:5)|(cid:10)c, a(cid:10)cP=⎧⎨⎩1.0 a = no-change ∧ (cid:10)c1.0 a = advance ∧ (cid:10)c0.0 otherwise.(cid:5) = (cid:10)c(cid:5) = next-state((cid:10)c)Lane turning indicator. Here we assume that the probability of a car at the head of the queue making a right turn is pt andthat while a car is waiting, its turn decision does not change:(cid:6)t(cid:5)jP= turn|t j, x j⎧⎨⎩(cid:7)=1.0 x j = O ∧ t j = turn0.0 x j = O ∧ t j = no-turnptx j = U .It is difficult in traffic models to obtain an accurate estimate of pt over all hours of the day, so using our DCN, we allow the(cid:3) 1 (to be defined for specificturn probability to fluctuate over time and thus model pminproblem instances).(cid:3) pt (cid:3) pmaxfor 0 (cid:3) pmin(cid:3) pmaxttttIntermediate road cell. The occupancy of a car in an intermediate road cell xi depends on whether an occupying car canmove forward into the next cell xi+1 and if so, whether there is a car in the previous cell xi−1 that can move forward totake its place:1518K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527(cid:6)(cid:5)xiP= O |xi−1, xi, xi+1⎧⎨⎩(cid:7)=1.0 xi = O ∧ xi+1 = O1.0 xi = U ∧ xi−1 = O0.0 xi = otherwise.Feeder road cell. A feeder road cell simply serves as an input to the traffic network with cars arriving at each unoccupiedfeeder cell with probability pa:(cid:6)(cid:5)xkP= O |xk(cid:12)(cid:7)=1.0 xk = O ∧ xk+1 = Ootherwise.paIt is difficult in traffic models to obtain an accurate estimate of arrival probabilities pa over all hours of the day, so using(cid:3) 1our DCN, we allow the arrival probability to fluctuate over time and thus model pmin(to be defined for specific problem instances).(cid:3) pa (cid:3) pmaxfor 0 (cid:3) pmin(cid:3) pmaxaaaaIntersection road cell. The intersection road cells are the most complex cells to model in a traffic network as traffic behaviordepends on the light state, the occupancy of all cells with green access to the intersection, and the state of turning traffic.Here we attempt to implement a basic model of traffic behavior taking into account all of these contingencies:(cid:6)(cid:5)xjP(cid:7)= O |x j, t j, xo, to, (cid:10)c=⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩0.0 x j = U ∧ x j−1 = U1.0 x j = U ∧ x j−1 = O0.0 x j = O ∧ t j = no-turn0.0 x j = O ∧ t j = turn ∧ xo = U0.0 x j = O ∧ t j = turn ∧ xo = O ∧ ¬green((cid:10)c, o)1.0 x j = O ∧ t j = turn ∧ xo = O ∧ green((cid:10)c, o) ∧ to = no-turn0.0 x j = O ∧ t j = turn ∧ xo = O ∧ green((cid:10)c, o) ∧ to = turn.Here we assume there are user-defined helper functions green((cid:10)c, j) that extract the part of the state (cid:10)c indicating whether(cid:5)the intersection cell j has a green light (or not). We also assume that when ¬green((cid:10)c, o) holds, then x= x j (thereby makingja simplifying assumption of no turns on red).Traffic reward model. Because our goal is to reduce traffic congestion in the intersection, our objective is to minimizethe count of occupied road cells around an intersection. Thus, an appropriate reward to maximize would be the count ofunoccupied cells5:5 We use I[·] as an indicator function taking the value 1 when its argument is true and 0 otherwise.K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271519Fig. 15. Time performance comparison for Traffic, SysAdmin and Factory problems using SPUDD-IP and Flat Value Iteration. The name includes the numberof variables in each problem, so the corresponding number of states is 2#variables.R((cid:10)x) =n(cid:3)i=1I[xi = U ].Here, we get +1 reward for every cell that is unoccupied.In this article we solve instances of Traffic domain with two opposing lanes. In these particularTraffic problem instances.= 1 and furthermore constrain the turninstances, we set the turn probability minimum as pminprobabilities p1 and p2 of the two different lanes to be highly correlated using the constraint |p1 − p2| (cid:3) 0.1. Additionally,= 0.4the probabilities p3 and p4 of a car arriving at either of the feeder cells for each lane use the probability bounds pminand pmax= 0.6 and are constrained by |p3 − p4| (cid:3) 0.1.= 0 and maximum as pmaxatta8.2. EvaluationIn this section, we empirically evaluate four algorithms: Flat Value Iteration from (10) and our three contributions fromthe previous section for solving factored MDP-IPs: (i) SPUDD-IP that offers an exact solution; (ii) APRICODD-IP and (iii)Objective-IP that offer bounded approximate solutions.As an additional point of comparison, we note that recent years have seen the emergence of very fast approximatefactored MDP solvers based on Approximate Linear Programming (ALP) [5]. Recently, such techniques have been extended tofactored MDP-IPs [9]. Thus, we also compare the approximate solutions APRICODD-IP and Objective-IP based on approximatevalue iteration with an Approximate Multilinear Programming (AMP) algorithm from [9]. AMP performs linear-value functionapproximation using a fixed set of basis functions and a compact constraint encoding for multilinear optimization problemsthat exploits structure in the DCN.For all algorithms, we set maxIter = 50 for SysAdmin and maxIter = 75 for the other domains with γ = 0.9. In the nextsubsections we present our main results.8.2.1. Flat value iteration vs. SPUDD-IPIn Fig. 15 we compare the running time of the two exact solution methods: SPUDD-IP and Flat Value Iteration which∗((cid:10)x).6 Solutions not completing in five hours are marked Did Not Finish (DNF). We note that SPUDD-IP did notcompute Voutperform Flat Value Iteration on the SysAdmin domains because the exact value function has little structure as an ADD.However, both Traffic and Factory had highly structured value functions and up to two orders magnitude time improvement isdemonstrated by SPUDD-IP, largely due to the ability of the PADDs to aggregate common nonlinear objectives, thus savinga substantial number of calls to the nonlinear solver and therefore time.8.2.2. APRICODD-IP vs. Objective-IPIn order to see the scalability of our approximate solutions, in Fig. 16 we compare the running time for APRICODD-IP andObjective-IP vs. the number of state variables using δ = 0.1 for Factory, Traffic, and the three configurations of SysAdmin.6 We note that to do this comparison, we need to slightly extend Flat Value Iteration algorithm from (10) to allow for multilinear expressions in thetransition probability table.1520K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527Fig. 16. Time performance of APRICODD-IP and Objective-IP for Traffic, SysAdmin and Factory problems for δ = 0.1.We note that Objective-IP runs faster than APRICODD-IP in all domains when running with a fixed bound on maximumerror per iteration (i.e., δ = 0.1).In order to evaluate the policy returned by our AVI solutions, we compute for each fixed value of δ (δ is the maximumerror per iteration w.r.t. V max), the True Approximation Error (TAE) given by:(cid:4)(cid:4)V∗(cid:4)(cid:4)((cid:10)x) − V approx((cid:10)x)max(cid:10)x(19)where V approx((cid:10)x) is the value returned by APRICODD-IP or Objective-IP and VIP.∗((cid:10)x) is the optimal value computed by SPUDD-In the following plots we ran Solve for a range of δ. In Figs. 17 and 18 we present a detailed comparison of the time,size (number of nodes in the ADD of the last iteration), and number of nonlinear solver calls required by APRICODD-IP andObjective-IP plotted vs. the TAE for traffic-10, respectively. We note little relationship between the space required by theADD value representation (number of nodes) and the running times of the two algorithms (space actually increases slightlyfor Objective-IP while running time decreases, see Fig. 18). But what is striking about these plots is that the running time ofeach algorithm is directly correlated with the number of nonlinear solver calls made by the algorithm (taking up to 100 msin some cases), reflecting our intuitions that the time complexity of solving MDP-IPs is governed by the computationaloverhead of nonlinear optimization.K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271521Fig. 17. Time, nonlinear solver calls and ADD size of APRICODD-IP pruning for the traffic problem with 10 variables. Results are plotted for δ ∈{0.0, 0.025, 0.05, 0.075, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.Fig. 18. Time, nonlinear solver calls and ADD size of Objective-IP pruning for the traffic problem with 10 variables. Results are plotted for δ ∈{0.0, 0.025, 0.05, 0.075, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.Fig. 19 shows the advantage of Objective-IP pruning that uses the upper and lower values to approximate the leavesin PADDs. For all problems, as the number of nodes reduced to a constant grows, we see that the True ApproximationError increases, but also the number of calls to the multilinear solver decreases. These figures also show cases where theObjective-IP approach to PADD reduction occurs with great success, since the original PADD sizes for the exact cases arevery large, but can be reduced by orders of magnitude in exchange for a reasonable amount of approximation error.In Figs. 20, 21, 22, 23 and 24 we show a comparison of the True Approximation Error (TAE) vs. running times forthree problems and three different sizes of each problem (varying δ). The results here echo one conclusion: Objective-IPconsistently takes less time than APRICODD-IP to achieve the same approximation error and up to one order of magnitude lesstime than APRICODD-IP. This time reduction can be explained by the decreased number of calls to the multilinear solver.8.2.3. Approximate value iteration vs. approximate multilinear programmingIn Figs. 20, 21, 22, 23 and 24 we compare the two approximate solution methods, APRICODD-IP and Objective-IP, withour implementation of approximate multilinear programming (AMP) [9] for MDP-IPs. We used simple basis functions (onefor each variable in the problem description) and pairwise basis functions (one for each pair of variables that have a commonchild variable in the DCN).When it does finish within a limit of ten hours, AMP takes only a few seconds to produce an approximate solution foreach problem (except for the Factory domain for which it did not return a solution). Comparing the algorithms in termsof their true approximation error, we observe that: (a) in the SysAdmin problem (Figs. 22, 23, 24), AMP with pair basisfunctions outperforms APRICODD-IP and obtains a solution 2–3× larger than the error of Objective-IP, but in significantlyless time; (b) for the Traffic problem (Fig. 21), AMP with the simple basis solution obtains a solution with 2–3× more error1522K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527Fig. 19. Number of nonlinear solver calls and number of nodes reduced to a constant vs. True Approximation Error for Objective-IP pruning for threedifferent problems. Results are plotted for δ ∈ {0.0, 0.025, 0.05, 0.075, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.Fig. 20. True Approximation Error vs. time required for APRICODD-IP, Objective-IP and MPA with simple basis functions (MPA pairwise did not finish in aten hour time limit and MPA with simple basis functions did not finish for two problems) for three Factory problems.K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271523Fig. 21. True Approximation Error vs. time required for APRICODD-IP, Objective-IP and MPA with simple basis and pairwise basis functions for Trafficproblem.Fig. 22. True Approximation Error vs. time required for APRICODD-IP, Objective-IP and MPA with simple basis and pairwise basis functions for SysAdminproblem with unidirectional-ring topology.1524K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527Fig. 23. True Approximation Error vs. time required for APRICODD-IP, Objective-IP and MPA with simple basis and pairwise basis functions for SysAdminproblem with bidirectional-ring topology.Fig. 24. True Approximation Error vs. time required for APRICODD-IP, Objective-IP and MPA with simple basis and pairwise basis functions for SysAdminproblem with independent bidirectional topology.K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271525than Objective-IP, still in significantly less time; and (c) in the case of the Factory problem (Fig. 20), AMP only can solveone instance, while Objective-IP can solve the rest within the time limit with much lower error. These results lead us toconclude that Objective-IP consistently gives an error at least 2–3× lower than AMP and sometimes runs as fast as the AMPsolution, while in other cases running slower.8.2.4. Results summaryOver all problems, given the unpredictable performance of AMP (which has no error guarantees and often does notfinish within the time limit) and the consistently worse performance of APRICODD-IP compared to Objective-IP, Objective-IPstands out as the more reliable option: it offers guaranteed error bounds and empirically it offers consistently lower errorrates (the lowest of any algorithm) with overall reasonable running times (if not the fastest).9. Related workThe Bounded-parameter Markov Decision Process (BMDP) [22] is a special case of an MDP-IP, where the probabilities andrewards are specified by constant intervals. Exploiting the specific structure available in a BMDP given by the intervals,the algorithm in [22] can directly derive the solution without requiring expensive optimization techniques. Recent solutionsto BMDPs include extensions of real-time dynamic programming (RTDP) [23] and LAO* [24,25] that search for the bestpolicy under the worst model. The Markov Decision Process with Set-valued Transitions (MDPSTs) [26] is another subclassof MDP-IPs where probability distributions are given over finite sets of states. Since BMDP and MDPST are special casesof MDP-IPs, we can represent both by “flat” MDP-IPs. Then the algorithms defined in this paper clearly apply to bothBMDPs and MDPSTs, however their solutions do not generalize to the factored MDP-IPs we examined in this paper, which allowa multilinear probability representation resulting from the use of a DCN. Furthermore, MDP-IPs allow for general linearconstraints between probabilities, which are prohibited in interval bounded probability settings like BMDPs. This use ofgeneral linear constraints is particularly useful when we do not know the probabilities, but only relative constraints betweenthem (e.g., two probabilities in the Traffic problem are unknown but highly correlated).Previous work on “flat” MDP-IPs [6,7,27] focused on credal sets (represented as polytopes) proposed algorithms basedon dynamic programming, but they only solved very small problems. It is important to notice that our factored MDP-IPmodel is more expressive than the simple “flat” MDP-IPs referred to in those papers; as we saw in Section 4, the joint DCNtransition probabilities in factored MDP-IPs may be nonlinear, while for flat MDP-IPs, the transition probability for any nextstate, given a previous state and action, can only be trivially linear (a single parameter pi ).As we have discussed in Section 8, it is interesting to note that if we allow only interval bounds on the parameters inthe CPTs of the DCN of the factored MDP-IP then the result is still a more expressive model than a “flat” MDP-IP or BMDP,i.e., the transition expression for any next state given a previous state and action can be a multilinear expression of (cid:10)p.Consequently, to define Flat Value Iteration for the comparative analysis from the previous section, we note that we alreadyneeded to slightly extend previous work to allow for multilinear expressions in the transition probability tables required tomatch the expressivity of factored MDP-IPs.A final piece of work that is related with MDP-IPs is a two-player zero-sum alternating Markov Game [28] (a.k.a. a StochasticGame [29]). This is a subset of “flat” MDP-IPs if intermediate state variables are introduced to represent opponent actionsand the parameters specify the distribution over opponent actions is allowed to vary in the full interval [0, 1]. However,it might be computationally wasteful to use a “flat” or factored MDP-IP algorithm to solve a Stochastic Game since aminimization over a finite set of opponent actions would likely be computationally cheaper than a (nonlinear) optimizationover the probability parameters required in an MDP-IP solution. Hence, it seems more computationally advantageous to usespecialized algorithms for the solution of finite action Stochastic Games to exploit the specific structure found there than toattempt to use any of the more general-purpose MDP-IP algorithms presented here.Finally, probability trees were also used to represent convex sets of probabilities associated to intervals to obtain posteriorintervals of probability [30]. Probability trees can compactly represent context-specific independence (CSI), but as we saw inSection 5, our parameterized ADDs are DAGs that not only exploit CSI but also shared function structure. Additionally, weused PADDs to represent general probability expressions (multilinear for the case of factored MDP-IPs), not just probabilityintervals.10. Concluding remarksMotivated by the real-world need to solve MDPs with uncertainty in the transition model, we made a number of novelcontributions to the literature in this article. In Section 4, we introduced the factored MDP-IP model based on DynamicCredal Networks (DCNs). In Section 5, we contributed the novel parameterized ADD (PADD) data structure containing leaveswith parameterized expressions; we showed how to efficiently obtain a minimal canonical representation of a PADD; andwe showed how to efficiently perform a variety of unary and binary operations on PADDs. In Section 6, we contributedthe exact factored MDP-IP solution algorithm SPUDD-IP and showed how to efficiently make use of the PADD in all stepsof this factored MDP-IP value iteration algorithm. The resulting SPUDD-IP algorithm yielded up to two orders of magnitudespeedup over existing value iteration techniques for MDP-IPs.1526K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–1527To further enhance the SPUDD-IP algorithm, in Section 7, we contributed two novel approximate value iteration ex-tensions: APRICODD-IP and Objective-IP. While APRICODD-IP is the obvious extension based on previous work, it did notspecifically target the main source of time complexity for solving MDP-IPs — calls to the nonlinear solver during MDP-IPvalue iteration. Based on this observation, we developed an alternate and novel approximation method that directly ap-proximated the objective of multilinear solver calls, proving the theoretical correctness of this innovative bounded errorapproximation approach and substantially reducing the number of nonlinear solver calls and thus running time of approx-imate value iteration. In Section 8, we performed comparisons of the above algorithms to a previously existing “flat” valueiteration algorithm as well as a state-of-the-art approximate multilinear programming (AMP) solver for MDP-IPs.Altogether these novel contributions — and particularly their culmination in the Objective-IP algorithm — enable the(bounded approximate) solution of factored MDP-IPs that can scale orders of magnitude beyond existing flat value iterationapproaches to MDP-IPs and yield substantially lower errors than other existing approximate MDP-IP solvers like approximatemultilinear programming (AMP) that have no a priori error guarantees and depend on appropriate basis function generationalgorithms.For future work, we note that PADDs represent the tip of the iceberg in the use of advanced decision diagram techniquesfor solving factored MDP-IPs. Following the success of the Affine extension of ADDs for solving factored MDPs [31] withadditive and multiplicative structure, it would be interesting to extend this technique to PADDs to exploit the same structurein MDP-IPs. Such advances would ideally reduce the running time of solutions for factored MDP-IP problems like Trafficthat contains significant additive structure in their reward definition and might be amenable to even further exploitation offactored MDP-IP problem structure.Finally, we note that the exploration of objectives other than maximin optimality for factored MDP-IPs would also beinteresting. Although the maximin criteria works fine in a domain with many imprecise parameters (like in the SysAdmindomain we have used in our experiments), we observe that for a problem with large imprecision in terms of very looseconstraints (e.g. 0.1 (cid:2) pi j (cid:2) 0.9), the maximin criterion may be too adversarial — it may reflect a worst-case that wouldbe extremely unlikely in practice. Hence, future work might also examine other methods of handling transition uncertainty,such as a Bayesian approach [32], and determine whether factored MDP-IPs and PADDs could enhance solution approachesfor those alternate criteria.AcknowledgementsThis work was performed while the first author was visiting NICTA. NICTA is funded by the Australian Government asrepresented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Councilthrough the ICT Centre of Excellence program. This work has also been supported by the Brazilian agencies FAPESP (undergrant 2008/03995-5) and CAPES.References[1] M.L. Puterman, Markov Decision Processes, Wiley Series in Probability and Mathematical Statistics, John Wiley and Sons, New York, 1994.[2] C. Boutilier, S. Hanks, T. Dean, Decision-theoretic planning: Structural assumptions and computational leverage, JAIR 11 (1999) 1–94.[3] J. Hoey, R. St-Aubin, A. Hu, C. Boutilier, SPUDD: Stochastic planning using decision diagrams, in: Proceedings UAI, Morgan Kaufmann, 1999, pp. 279–288.[4] R. St-Aubin, J. Hoey, C. Boutilier, APRICODD: Approximate policy construction using decision diagrams, in: Proceedings NIPS, MIT Press, 2000, pp. 1089–1095.[5] C. Guestrin, D. Koller, R. Parr, S. Venkataraman, Efficient solution algorithms for factored MDPs, JAIR 19 (2003) 399–468.[6] J.K. Satia, R.E. Lave Jr., Markovian decision processes with uncertain transition probabilities, Oper. Res. 21 (1970) 728–740.[7] C.C. White III, H.K. El-Deib, Markov decision processes with imprecise transition probabilities, Oper. Res. 42 (4) (1994) 739–749.[8] T. Dean, K. Kanazawa, A model for reasoning about persistence and causation, Comput. Intell. 5 (3) (1990) 142–150.[9] K.V. Delgado, L.N. de Barros, F.G. Cozman, R. Shirota, Representing and solving factored Markov decision processes with imprecise probabilities, in:Proceedings ISIPTA, Durham, United Kingdom, 2009.[10] D.P. Bertsekas, J.N. Tsitsiklis, An analysis of stochastic shortest path problems, Math. Oper. Res. 16 (3) (1991) 580–595.[11] F.G. Cozman, Credal networks, Artificial Intelligence 120 (2000) 199–233.[12] P. Walley, Statistical Reasoning with Imprecise Probabilities, Chapman and Hall, London, 1991.[13] A. Nilim, L. El Ghaoui, Robust control of Markov decision processes with uncertain transition matrices, Oper. Res. 53 (5) (2005) 780–798.[14] R. Shirota, F.G. Cozman, F.W. Trevizan, C.P. de Campos, L.N. de Barros, Multilinear and integer programming for Markov decision processes with impre-cise probabilities, in: Proceedings ISIPTA, Prague, Czech Republic, 2007, pp. 395–404.[15] F.G. Cozman, Graphical models for imprecise probabilities, Internat. J. Approx. Reason. 39 (2–3) (2005) 167–184.[16] C. Boutilier, N. Friedman, M. Goldszmidt, D. Koller, Context-specific independence in Bayesian networks, in: Proceedings UAI, 1996, pp. 115–123.[17] R.I. Bahar, E.A. Frohm, C.M. Gaona, G.D. Hachtel, E. Macii, A. Pardo, F. Somenzi, Algebraic decision diagrams and their applications, in: ProceedingsICCAD, IEEE Computer Society Press, Los Alamitos, CA, USA, 1993, pp. 188–191.[18] R.E. Bryant, Symbolic Boolean manipulation with ordered binary-decision diagrams, ACM Comput. Surv. 24 (3) (1992) 293–318.[19] R.E. Bryant, Graph-based algorithms for Boolean function manipulation, IEEE Trans. Comput. 35 (8) (1986) 677–691.[20] N.L. Zhang, D. Poole, A simple approach to Bayesian network computations, in: Proceedings of the Tenth Canadian Conference on Artificial Intelligence,1994, pp. 171–178.[21] C.F. Daganzo, The cell transmission model: a dynamic representation of highway traffic consistent with the hydrodynamic theory, Transport. Res.B 28 (4) (1994) 269–287.[22] R. Givan, S. Leach, T. Dean, Bounded-parameter Markov decision processes, Artificial Intelligence 122 (2000) 71–109, (39).[23] O. Buffet, D. Aberdeen, Robust planning with LRTDP, in: Proceedings IJCAI, 2005, pp. 1214–1219.[24] S. Cui, J. Sun, M. Yin, S. Lu, Solving uncertain Markov decision problems: an interval-based method, in: Proceedings ICNC (2), 2006, pp. 948–957.K.V. Delgado et al. / Artificial Intelligence 175 (2011) 1498–15271527[25] M. Yin, J. Wang, W. Gu, Solving planning under uncertainty: quantitative and qualitative approach, in: Proceedings IFSA (2), 2007, pp. 612–620.[26] F.W. Trevizan, F.G. Cozman, L.N. de Barros, Planning under risk and knightian uncertainty, in: Proceedings IJCAI, 2007, pp. 2023–2028.[27] J.A. Bagnell, A.Y. Ng, J.G. Schneider, Solving uncertain Markov decision processes, Tech. rep., Carnegie Mellon University, 2001.[28] M.L. Littman, Markov games as a framework for multi-agent reinforcement learning, in: Proceedings ICML, Morgan Kaufmann, 1994, pp. 157–163.[29] L.S. Shapley, Stochastic games, Proc. Natl. Acad. Sci. USA 39 (1953) 327–332.[30] A. Cano, S. Moral, Using probability trees to compute marginals with imprecise probabilities, Internat. J. Approx. Reason. 29 (1) (2002) 1–46.[31] S. Sanner, D. McAllester, Affine algebraic decision diagrams (AADDs) and their application to structured probabilistic inference, in: Proceedings IJCAI,2005, pp. 1384–1390.[32] M.O. Duff, Optimal learning: Computational procedures for Bayes-adaptive Markov decision processes, Ph.D. thesis, University of Massachusetts,Amherst, January 2002.