Artificial Intelligence 320 (2023) 103919Contents lists available at ScienceDirectArtificial Intelligencejournal homepage: www.elsevier.com/locate/artintReasoning about causality in gamesLewis Hammond a,∗Alessandro Abate a, Michael Wooldridge aa University of Oxford, United Kingdomb DeepMind, United Kingdom, James Fox a,∗, Tom Everitt b, Ryan Carey a, a r t i c l e i n f oa b s t r a c tArticle history:Received 17 March 2022Received in revised form 1 April 2023Accepted 2 April 2023Available online 5 April 2023Keywords:CausalityGame theoryGraphical modelsContentsCausal reasoning and game-theoretic reasoning are fundamental topics in artificial intel-ligence, among many other disciplines: this paper is concerned with their intersection. Despite their importance, a formal framework that supports both these forms of reasoning has, until now, been lacking. We offer a solution in the form of (structural) causal games, which can be seen as extending Pearl’s causal hierarchy to the game-theoretic domain, or as extending Koller and Milch’s multi-agent influence diagrams to the causal domain. We then consider three key questions:i) How can the (causal) dependencies in games – either between variables, or between strategies – be modelled in a uniform, principled manner?ii) How may causal queries be computed in causal games, and what assumptions does this require?iii) How do causal games compare to existing formalisms?To address question i), we introduce mechanised games, which encode dependencies be-tween agents’ decision rules and the distributions governing the game. In response to question ii), we present definitions of predictions, interventions, and counterfactuals, and discuss the assumptions required for each. Regarding question iii), we describe correspon-dences between causal games and other formalisms, and explain how causal games can be used to answer queries that other causal or game-theoretic models do not support. Finally, we highlight possible applications of causal games, aided by an extensive open-source Python library.© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/).1.2.Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1.1.Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1.2.Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Causal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2.1.Game-theoretic models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .2.2.3345573. Mechanised MAIDs and relevance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10* Corresponding authors.E-mail addresses: lewis.hammond@cs.ox.ac.uk (L. Hammond), james.fox@cs.ox.ac.uk (J. Fox).https://doi.org/10.1016/j.artint.2023.1039190004-3702/© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/).L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 1039197.6.4.5.3.1. Mechanised MAIDs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103.2.Relevance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13Causality in games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14Predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154.1.Interventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164.2.4.3.Counterfactuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17Solution concepts and subgames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21Nash equilibria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215.1.Subgames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225.2.Equilibrium refinements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235.3.Connections to EFGs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256.1.Equivalences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266.2.Causality in EFGs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276.3.Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28Case study: insurance pricing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287.1.Blame, intent, incentives, and fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307.2.Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31Advantages and disadvantages of causal games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318.1.Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328.2.Declaration of competing interest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32Data availability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33Appendix A.Transformations between game representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33Theoretical results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34Further examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42Counterfactuals using the closest possible world principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42Non-existence results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43Reasoning about existing concepts using causal games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46Codebase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48Creating MAIDs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48Computing equilibria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51B.1.B.2.B.3.Appendix B.Appendix C.A.1.A.2.C.1.C.2.8.NotationSymbolObjectAncVcRGChVDescVdodom(V )EVEEFaVGIJMM(ζk)MVmGmMNPaVPr or PPrπ or P σQRR(mM)r DAncestors of VCondensed R-Relevance GraphChildren of VDescendants of VDo OperatorDomain of VExogenous Variable for VEdgesExtensive-Form GameFamily of VGraphInterventionIntervention SetModelPerturbed ModelMechanism Variable for VMechanised GraphMechanised ModelAgentsParents of VProbability DistributionProbability Distribution Combining Pr with π or P with σQueriesRationality RelationsR-Rational Outcomes of MRationality Relation for DPage542556575855625524101111855, 89, 9131112112L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919SymbolrRGVvVvδ(cid:5)(cid:6)Vθμiπ i˙π iπ(cid:7)Dσ⊥G⊥⊥≺ObjectR-Relevance GraphVariableValue of VVariablesValues of VKronecker Delta FunctionProbability SimplexParameter Variable for VParametersMixed Policy for Agent i(Behavioural) Policy for Agent iPure (Behavioural) Policy for Agent i(Behavioural) Policy ProfileDecision Rule Variable for D(Behavioural) Strategy Profiled-Separated in GIndependentTopological OrderingPage1355556510522922910866421. IntroductionCausal reasoning and game-theoretic reasoning are core capabilities for intelligent systems, and as such, they are fun-damental research topics in artificial intelligence (AI). Causal reasoning is concerned with identifying causal relationships and estimating the effects of interventions. Game-theoretic reasoning is concerned with strategic behaviour: how rational decision-makers interact, taking into account others’ incentives. Whilst formal treatments of causality [73,75,82,91] and the game-theoretic foundations of multi-agent systems [71,88,99] have individually led to many recent applications in AI, our present concern is with techniques that combine causal and strategic reasoning.Models that support both these kinds of reasoning offer a wide range of possible applications including analysing incentives [22,57], fairness [49,55,67,102], and blame and intention [28,39]. As systems of multi-agent systems become increasingly ubiquitous and sophisticated, the problem of how to formally reason about these notions becomes increas-ingly acute. A framework that supports causal analysis of systems containing multiple self-interested agents would therefore appear to be of great importance [22,28,76].Causal questions are often more challenging to answer in multi-agent settings; one must consider not only the causal dependencies present in the environment, but also the dependencies between agents’ strategies. Similarly, reasoning strate-gically about what the effects of an action would be, or what other agents would have done under different circumstances, naturally leads one to consider both interventions and counterfactual possibilities when analysing games. These causal con-cepts, however, are typically left implicit in game-theoretic models.The central purpose of this paper is to introduce a unifying framework for modelling games that supports both causal and game-theoretic reasoning. This framework – (structural) causal games – can be interpreted in two different ways. In one sense, it lifts Koller and Milch’s (henceforth, K&M) multi-agent influence diagrams (MAIDs) [51] from the probabilistic models at level one of Pearl’s ‘causal hierarchy’ [73] to causal models that support both interventions and counterfactuals, corresponding to levels two and three of the hierarchy, respectively. Building on K&M’s graphical representation also means we may employ existing game-theoretic concepts such as strategic relevance [51] and sufficient recall [64], which can be elicited purely from the structure of the game. Alternatively, causal games can be interpreted as generalising the models of the causal hierarchy to the game-theoretic domain by introducing decision variables that lack a distribution until chosen by the corresponding agent, and a set of real-valued utility variables representing the payoff of each agent. Causal games thus support both game-theoretic and causal queries, as well as combinations of the two. It is our hope that this framework serves as a foundation for further work at the intersection of causality and game theory.1.1. ContributionsIn answering the three key questions introduced above, we make the following contributions.i) How can the (causal) dependencies in games – either between variables, or between strategies – be modelled in a uniform, principled manner?• We introduce mechanised MAIDs in Section 3, which allow us to cleanly model the dependencies of decision rules on each other and on the parameters of the game.• We also generalise K&M’s notion of strategic relevance to R-relevance to enable modelling many different decision-making principles.• Furthermore, we derive sound and complete graphical criteria for detecting relevant variables when agents are playing best responses to one another.3L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919ii) How may causal queries be computed in such models, and what assumptions does this require?• We generalise Pearl’s causal hierarchy of models to the game-theoretic domain by introducing (structural) causal games in Section 4.• We then describe how these models can be used to answer conditional, interventional, and counterfactual queries. By quantifying over the equilibria in the game (leading to first order queries such as “is it the case that in every Nash equilibrium, setting variable X to value x would increase agent i’s expected payoff?”) and taking into account causal effects due to rational agents who adapt their strategies to changes in the environment, such queries strictly generalise those in other causal models.• In essence, the assumptions required to model a given problem as a causal game are the same as for their non-game-theoretic counterparts. To mechanise the causal game we further require assumptions about the decision-making principles that agents use play the game.iii) How do the models we propose compare to existing formalisms?• We introduce subgames and two classic equilibrium refinements – subgame perfectness and trembling hand perfectness – to MAIDs in Section 5, and provide a detailed comparison between MAIDs and EFGs (including several equivalence results) in Section 6.• We also show that often more subgames can be found in a MAID than in the corresponding EFG, ruling out more non-credible threats, and making the computation of equilibria more efficient.• Finally, we discuss a range of applications in which causal games subsume prior work in Section 7, and (dis)advantages with respect to other work in Section 8.A previous conference paper contained preliminary results on subgames and equilibrium refinements in MAIDs, as well as their connections to EFGs [42], but did not contain any discussion of causality, which is the main focus of this paper. Similarly, a previous tool paper introduced an earlier version of our codebase [27], which implements many of the concepts in this paper though does not contain any theoretical work, which is the emphasis of the present paper.We conclude this section with a review of other related work. Before the primary exposition of our results, we also provide the relevant background material on causal models, EFGs, and MAIDs, in Section 2. Proofs, further examples, and details of our codebase are relegated to Appendices A, B, and C, respectively.1.2. Related workCausal games build on Pearl’s hierarchy of causal models [73], and work on influence diagrams (IDs) – a kind of graphical model used to capture single-agent decision-making scenarios [45]. Later works have explicitly considered causal IDs, or CIDs [18,22]. Indeed, Heckerman and Shachter’s proposal to unify causal modelling and decision analysis via CIDs [43] can be viewed as a precursor to our proposal to unify causal modelling and game theory via causal games. None of these theories, however, place emphasis on games or strategic interactions between multiple agents, which is our setting of interest.In contrast, multi-agent IDs (MAIDs) generalise IDs [51,64], and were originally developed by K&M as a way to efficiently represent and solve games. One useful feature of MAIDs (which causal games inherit) is that their graphical structure encodes what information is, or is not, relevant for making an optimal decision. Thus in order to find an equilibrium, it is often possible to remove some of the edges (i.e. the ignorable information) prior to solving the game [64]. Many other graphical models of games, often inspired by MAIDs, have been introduced [48], including networks of influence diagrams (NIDs) [30], interactive dynamic influence diagrams (I-DIDs) [20,77], and temporal action graph games [47]. Like MAIDs however, most of these works are motivated by computational concerns, and none incorporate rigorous causal reasoning.Modelling causal relationships in game-theoretic scenarios often leads to cyclic dependencies, such as when one agent’s best response depends on the other’s, and vice versa. This can result in multiple solutions, a feature that is also captured by generalised or cyclic causal models [8,38], chain graphs [59], probabilistic relational models [33,62], and credal networks [14], among others. In mechanised (structural) causal games, these solutions correspond to different equilibria induced by (serial) relations representing the potentially non-deterministic decision-making processes of agents in the game (such as when an agent selects a decision rule using an arg max operation, for example). This is essential for modelling equilibria in games, where agents may be indifferent between decision rules. The result is a relational causal model with cycles – a formalism first explored concurrently with this work [1], but without any reference to the game-theoretic scenarios that are our focus.Perhaps the most similar work to this paper is on settable systems,1 which are partly inspired by generalised structural causal models and can be used to capture optimisation, equilibria, and learning [95,96]. In order to deal with cycles, settable systems duplicate intervenable variables into a ‘response’ and ‘setting’ variable, so as to indicate which side of the structural 1 We note that Gonzalez-Soto et al. also define a kind of causal game [35], though theirs is essentially a Bayesian game where the outcomes are computed using an unknown causal model. This is a simpler and more restricted setup than both settable systems and our causal games.4L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919function each occurs on. In these models, the emphasis is on the causal analysis of optimisation procedures at a relatively low level of abstraction, meaning that the algorithms used by agents to select actions, or the procedures used to select one equilibrium from many, are explicitly instantiated. In contrast, causal games represent the causal dependencies arising from the fact that agents select their decision rules rationally and non-deterministically, leading us to ask first-order causal queries. Settable systems concentrate on problems such as how to capture the data and attributes of a machine learning process, whereas we focus on problems such as identifying subgames and equilibrium refinements. Despite these differences, settable systems are nonetheless a useful comparator for causal games.The concurrency and semantics community is another that has produced work at the intersection of games and causal-ity. Much of the most influential recent work on the foundations of denotational semantics uses distributed games that are based on event structures – a partially ordered model of discrete events [69] – for deterministic [81] and probabilis-tic [98] concurrent games. Other related approaches to concurrent games use simpler mathematical formalisms [11,101]. Though containing the same primitive concepts, these works are motivated primarily by the problem of deriving formal, low-level semantics for programs or probabilistic systems, whereas we are interested in more high-level models of strategic interactions that can be applied to a wide range of scenarios and disciplines.Indeed, closely related causal models have been used (often in the context of analysing AI systems) to define notions of blame and intent (both in the single-agent and multi-agent settings) [28,39], harm [80], incentives to control or respond to certain variables [22,57], [49,55,67,102], social influence [46], and reasoning patterns such as manipulation and signalling [76]. We show in Sections 4 and 7 that causal games subsume the models in these works and allow for even richer concepts. Aside from analysing AI systems, one other relevant domain of application is in economic analysis and mechanism design. For example, Toulis and Parkes use a behavioural causal model to determine long-term effects of policy interventions on multi-agent economies [92] – though their emphasis is on dynamical systems and behavioural analysis – and some regulators such as the UK’s Financial Conduct Authority include an informal ‘causal chain’ in their cost-benefit analyses when proposing policy analyses [25]. We provide a more formal case study of this second example in Section 7.2. BackgroundWe assume a basic familiarity with both probabilistic graphical models and game theory, though for completeness, we briefly review Pearl’s causal hierarchy [73] and two game-theoretic models: extensive form games (EFGs) [54,94] and multi-agent influence diagrams (MAIDs) [51]. Readers familiar with these models may safely skip these sections.Throughout this paper, we use capital letters V for random variables, lowercase letters v for their instantiations, and bold letters V and v respectively for sets of variables and their instantiations. We let dom(V ) denote the domain of V(where by default we assume that dom(V ) is finite) and abuse notation somewhat by writing dom(V ) :=×V ∈V dom(V ). PaV denotes the parents of a variable V in a graphical representation and paV the instantiation of PaV . We also define ChV , AncV , DescV , and FaV := PaV ∪ {V } as the children, ancestors, descendants, and family of V , respectively (where note that neither AncV nor DescV contain V , by convention). As with paV , their instantiations are written in lowercase. We use (cid:5)(dom(V )) to denote the set of all probability distributions over the values of V , and therefore write (cid:5)(dom( A) |dom(B)) :=×b∈dom(B) (cid:5)(dom( A)) to express the set of all conditional probability distributions (CPDs) over A given the values of B. Unless otherwise indicated, we use superscripts to indicate an agent i ∈ N = {1, . . . , n} and subscripts to index the elements of a set; for example, the decision variables belonging to agent i are denoted D i = {D i}.1, . . . , D im2.1. Causal modelsPearl’s causal hierarchy consists of three kinds of model: associational, interventional, and counterfactual [73]. Each step up the hierarchy demands stricter assumptions. The lowest level, association, pertains to correlations between variables that allow for predictions about a system. For this, it is sufficient to use observational data to construct a joint probability distribution over all of the variables in that system, which can then be represented graphically as a Bayesian network (BN). On the second level, we wish to reason about the effects of interventions – deliberate alterations made to the variables from outside the system. This requires the edges of the BN to reflect causal, not just associational, relationships, giving rise to a causal Bayesian network (CBN). The final level of the hierarchy is concerned with counterfactual questions – asking what would have happened had something been different, given that we made a certain observation – which corresponds to conditioning (as in associational queries), then intervening. Answering such questions requires knowledge of the underlying deterministic relationships between the variables, typically characterised using a structural causal model (SCM).2.1.1. AssociationDefinition 1 ([73]). A Bayesian network (BN) over a set of random variables V with joint distribution Pr(V ) is a structure M = (G, θ ) where G = (V , E ) is a directed acyclic graph (DAG) with vertices V and edges E that is Markov compatiblewith Pr, meaning that Pr(v; θ ) =; θV ). We drop the parameters θ = {θV }V ∈V of the CPDs from our notation where unambiguous.V ∈V Pr(v | paV(cid:2)5L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919ACBDE AECACBDEBED(a)(b)Fig. 1. (a) A (C)BN with a Markov compatible joint distribution Pr(a, b, c, d) = Pr(a | c, d) Pr(b | d) Pr(d | c) Pr(c). (b) An SCM representing the same system. Each variable V is now associated with an exogenous parent EV ∈ E.We can use the d-separation graphical criterion to identify the set of conditional independencies that any Markov com-patible joint distribution over a DAG G must satisfy [74].Definition 2 ([73]). A path p in a DAG G = (V , E ) is a sequence of unrepeated adjacent variables in V . A path p is said to be blocked by a set of variables Y ⊂ V if and only if p contains either:• A chain X → W → Z or X ← W ← Z , or a fork X ← W → Z , and W ∈ Y ; or• A collider X → W ← Z and W /∈ AncY ∪ {Y }.For disjoint sets X, Y , Z , the set Y d-separates X from Z , denoted X ⊥G Z | Y , if every path in G from a variable in X to a variable in Z is blocked by a variable in Y . Otherwise, X is said to be d-connected to Z given Y , denoted X (cid:10)⊥G Z | Y .For example, in the graph G shown in Fig. 1a, we have that A (cid:10)⊥G B | C due to the active path A ← D → B, but that A ⊥G B | D, as conditioning on D blocks the connection along the aforementioned path as well as along the path A ← C →D → B.If X ⊥G Z | Y in G, then X and Z are probabilistically independent conditional on Y in the sense that Pr(x | y, z) = Pr(x |y), written X ⊥⊥ Z | Y , in every distribution Pr that is Markov compatible with G and for which Pr( y, z) > 0. Conversely, if X (cid:10)⊥G Z | Y , then X and Z are dependent conditional on Y in at least one distribution Markov compatible with G [93].A second well-established graphical criterion will also be a useful auxiliary result in Section 3.Definition 3 ([86]). Given a DAG G, a variable V is a requisite probability node for Pr(x | y) if there exist two parameterisa-tions θ (cid:10)= θ (cid:11)differing only on θV such that Pr(x | y; θ ) (cid:10)= Pr(x | y; θ (cid:11)).of G for BNs M and M(cid:11)Lemma 1 ([31]). Given a BN M, a variable V is a requisite probability node for the query Pr(X | Y ) if and only if MV (cid:10)⊥m⊥G X | Y .2.1.2. InterventionAssociational models such as BNs are, in general, insufficient to answer questions about interventions as they do not describe how the joint distribution changes in response; a causal model, or level two model, such as a causal Bayesian network (CBN), is required. The graph underlying a CBN differs from that of a BN only in its causal interpretation: the directed edges now represent the fact that intervening on a variable cannot affect those causally ‘upstream’ of it. The simplest form of intervention, a hard intervention do(Y = y), sets the values of variables Y to some y. We denote the resulting joint distribution by Pr y(V ) or, equivalently, Pr(V y) [73].2Definition 4 ([73]). A causal Bayesian network (CBN) is a BN M = (G, θ ) such that G is Markov compatible with Pr y for every Y ⊆ V and y ∈ dom(Y ), and that:(cid:3)Pr y(v | paV ) =1Pr(v | paV ) when V /∈ Y and paV is consistent with y.when V ∈ Y and v is consistent with y,By the Law of Total Probability, Pr y(v | paV ) = 0 when v is inconsistent with y. When paV is inconsistent with y, then conditioning on a zero-probability event means that Pr y(v | paV ) is undefined. More generally, a soft intervention, specified ∗using a partial distribution I over variables Y replaces each CPD Pr(Y | PaY ) with a new CPD I(Y | PaY ) for each Y ∈ Y , Y∗Y may differ from PaY .3 Any intervention I on the set of variables Y leads to a new joint distribution:where Pa; θ ∗2 This is also known as an atomic [13], structural [21], surgical [73], or independent [52] intervention.3 A soft intervention is also known as a parametric [21] or dependent [52] intervention, and is referred to as conditional or stochastic when deterministic or stochastic, respectively [13]. Hard interventions are a special case in which each I(Y | PaY ) = δ(Y ,y) for some y ∈ dom(Y ), where δ is the Kronecker delta function.∗6L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919ACBd(a)ACBI(cid:11)D(b)Fig. 2. (a) The graph Gd representing the CBN in Fig. 1a after a hard intervention do(D = d). (b) A similar graph GI where I(B | Pawith Pa= PaB ∪ {C}.∗B∗B ) is a soft intervention PrI (v) :=(cid:4)Y ∈YI( y | pa∗Y ) ·(cid:4)V ∈V \YPr(v | paV ).We represent an intervention I on Y graphically by outlining each variable Y ∈ Y , replacing each variable name Y with (cid:10)= PaY . More generally, we use Pr(V I ) and Y I , and removing or adding edges from parent variables as necessary, if PaPrI (V ) interchangeably, and denote the new graph and model as GI and MI respectively. When I is a hard intervention, we simply sever all incoming edges to Y , setting their values to y, and write V y , G y , and M y respectively. Examples of hard and soft interventions are shown in Figs. 2a and 2b respectively.∗Y2.1.3. CounterfactualsIn counterfactual queries, evidence about the actual state of the world informs us about a hypothetical scenario in which some variables have been modified. For instance, we might be interested in the probability of x in the scenario in which y, given that (in fact) we observed z, written Pr(x y | z). To answer such questions in general, one must appeal to level three of the causal hierarchy, such as by using a structural causal model (SCM). In SCMs, variables are partitioned into exogenous and endogenous sets E and V respectively, where each endogenous variable V ∈ V is deterministically related to its parents via a structural function f V : dom(V \ {V }) × dom(E) → dom(V ) that specifies the mechanism governing the values of the variable, and where all stochasticity is relegated to the distribution Pr(E; θ ) over the exogenous variables.In this paper, we make the simplifying assumption that all SCMs are Markovian, meaning that each variable V has exactly one exogenous parent EV and the exogenous variables are independent. We also depart from convention by de-scribing SCMs as a particular form of CBN (which, in turn, are a particular form of BN), using deterministic distributions Pr(V | PaV ) = δfor each endogenous variable V . This equivalent formulation will prove useful for avoiding unnecessary repetition and notation when introducing causal models of games in Section 4.(cid:6)V , f V (PaV )(cid:5)Definition 5 ([73]). A (Markovian) structural causal model (SCM) is a CBN M = (G, θ ) where G = (E ∪ V , E ) is a DAG over exogenous variables E = {EV }V ∈V and endogenous variables V , where PaV ∩ E = {EV }. The parameters θ assign determin-E∈E Pr(E; θE) to the istic distributions Pr(v | paVexogenous variables.; θV ) to each endogenous variable and a stochastic distribution Pr(E; θ ) =(cid:2)Using such a model, we can evaluate a general counterfactual query Pr(xI | z) by following three steps [73]:1. Update Pr(e) to Pr(e | z) by conditioning on observation z (‘abduction’);2. Apply the intervention I(Y | Pa3. Return the marginal distribution Pr(x) in this modified model (‘prediction’).∗Y ) to the variables Y (‘action’);By convention, exogenous variables are viewed as beyond the realm of observation and intervention. Further, we assume ∗that each I(Y | PaY ) is a deterministic function of its parents. Soft, stochastic interventions may be modelled by adding a new exogenous variable E∈ Pa(cid:7)Note that by marginalising out the exogenous variables in an SCM, we can form a standard (C)BN with a joint distribution e Pr(v, e). This CBN is Markov compatible with the SCM’s graph G when restricted to variables in V . Moreover, Pr(v) =any CBN is also a BN. Thus, each model in the causal hierarchy can be seen to generate those on lower levels; every query that can be answered in a lower level model can also be answered in a higher level model.∗Y .∗Y2.2. Game-theoretic modelsWe now review two important formalisms for representing sequential strategic decision-making scenarios, extensive form games (EFGs) and multi-agent influence diagram (MAIDs). Both of these models are illustrated using the following example of a signalling game [90].Example 1 (Job market). A worker, who is either hard-working or lazy, is hoping to be hired by a firm. They have the option of pursuing a university education, but know that they will suffer from three years of studying, especially if they are lazy. 7L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919(4, 3)(−1, −1)(3, −2)(−2, 0)jjgV 11¬gV 21V 22¬ jhp¬ jI 21V 0I 22¬h1 − pV 24V 23gV 12¬g¬ j¬ jjj(5, 3)(0, −1)(5, −2)(0, 0)(a)TU 1U 2D 1D 2(b)Fig. 3. (a) An EFG representing Example 1. (b) A MAID representing the same game. (For interpretation of the colours in the figure(s), the reader is referred to the web version of this article.)The firm prefers hard-workers, but is using an automated hiring system that can only observe the worker’s education, not their temperament.2.2.1. Extensive form gamesThe material on EFGs is required only for Section 6, and so may be safely skipped or referred back to later, depending on the reader’s preferences.Definition 6 ([54]). An extensive form game (EFG) is a structure E = (N, T , P , A, λ, I, U ), where:• N = {1, . . . , n} is a set of agents.• T = (V , E ) is a game tree with nodes V that are partitioned into sets V 0, V 1, . . . , V n, L where R ∈ V is the root of T , L are the leaves of T , V 0 are chance nodes, and V i are the decision nodes controlled by agent i ∈ N. The nodes are connected by edges E .) over the children of each chance node V 0j .• P = {P 1, . . . , P |V 0|} is a set of probability distributions P j(ChV 0• A is a set of actions, where Aij• λ : E → A is a labelling function mapping each edge (V i• I = {I 1, . . . , In} contains a collection of information sets I i ⊂ 2V ik, V i∈ I i is defined such that for all V ij, V klji. Each information set I ijat both nodes.⊆ A denotes the set of actions available at V ij∈ V i .l ) to an action a ∈ Aij ., which partition the decision nodes controlled by agent ∈ I iare the same j , the available actions Ai:= A V i= A V ijkl• U : L → Rn is a utility function mapping each leaf node to a vector that determines the final payoff for each agent.Fig. 3a shows Example 1’s signalling game in extensive form. Nature, as a chance node V 0, flips a biased coin at the root of the tree to decide whether the worker is hard-working h (with probability p) or lazy ¬h (with probability 1 − p). The worker’s decision whether to go g or avoid ¬g university is represented at nodes V 1∈ V 1 and the automated hiring ∈ V 2, each with the option to offer a job or reject the worker ( j or ¬ j). system’s decisions are given by V 23 , V 2The two non-singleton information sets (marked by dotted black lines) represent the fact that the hiring system does not receive the worker’s temperament as an input.2 , V 21 , V 11 , V 224The payoffs for the worker and the firm are given by the first and second elements at the leaves of the tree respectively. The worker receives a payoff of 5 if they are given a job offer, but they incur a cost of 1 for going to university if they are hard-working and a cost of 2 for going to university if they are lazy. The firm receives a payoff of 3 if they hire a hard worker. If they offer a job to a lazy worker, they incur a cost of 2, and if they reject a hard worker, they incur an opportunity cost of 1.j ( Aij) over the actions available to the agent at each of their information sets I iDefinition 7. Given an EFG E = (N, T , P , A, λ, I, U ), a (behavioural) strategy σ i for agent i is a set of probability distributions j (a) ∈σ i{0, 1} and fully stochastic when σ ij . A strategy profile σ = (σ 1, . . . , σ n) is a tuple of strategies, and σ −i = (σ 1, . . . , σ i−1, σ i+1, . . . , σ n) denotes the partial strategy profile of all agents other than i, hence σ = (σ i, σ −i).j . A strategy is pure when each σ ij (a) > 0, for all a ∈ Ai8L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Combining a strategy profile σ with the distributions in P defines a probability distribution P σ over paths ρ in E . For each path ρ beginning from R and terminating in a leaf node ρ[L] ∈ L, agent i receives utility U (ρ[L])[i] – the ith entry in the corresponding payoff vector. Agent i’s expected utility under a strategy profile σ is therefore given by EσU (ρ[L])[i](cid:9)(cid:8).Definition 8. A subgame of an EFG, E = (N, T , P , A, λ, I, U ), is the game E restricted to a subtree T; and for any V k ∈ Vthat: for any information set I iand (V k, V l) ∈ E (cid:11)V l ∈ Vj in E , if there exists V k ∈ I i, then I ij⊆ V∩ V.(cid:11)(cid:11)(cid:11)j(cid:11), E (cid:11)) of T such (cid:11) = (V(cid:11), if (V k, V l) ∈ E , then In other words, a subtree of the original game tree forms a subgame if it is closed under information sets and descen-dants. Any EFG is trivially a subgame of itself, so a subgame on a strictly smaller subtree is called proper. In this paper, we denote subgames in EFGs by enclosing them within dashed boxes. For instance, the EFG in Fig. 3a has no proper subgames, but the EFG given later in Fig. 8c has two.2.2.2. Multi-agent influence diagramsInfluence diagrams (IDs) generalise BNs to the decision-theoretic setting by adding decision and utility variables [45,65], and multi-agent influence diagrams (MAIDs) generalise IDs by introducing multiple agents [51,64]. MAIDs can therefore be viewed as a BN over a graph without parameters for the decision variables, although technically lie between levels one and two of Pearl’s causal hierarchy as the decisions are effectively modelled as causal interventions [43], but paths between non-decision variables need not encode causal relationships [22].Definition 9 ([51]). A multi-agent influence diagram (MAID) is a structure M = (G, θ ) where G = (N, V , E ) specifies a set of agents N = {1, . . . , n} and a DAG (V , E ) where V is partitioned into chance variables X , decision variables D =i∈N D i , and utility variables U =i∈N U i . The parameters θ = {θV }V ∈V \D define the CPDs Pr(V | PaV ; θV ) for each non-decision variable such that for any parameterisation of the decision variable CPDs, the resulting joint distribution over V induces a BN.(cid:10)(cid:10)Fig. 3b shows a MAID representing Example 1. Chance variables, such as whether the worker’s temperament is hard-working or lazy (T ), are denoted by white circles. Decision and utility variables are represented using squares and diamonds respectively. The worker’s decision (D1) and utility (U 1) variables are displayed in red, and the firm’s (D2 and U 2) in blue. Instead of information sets, the fact that an agent is unaware of the value of a certain variable when making a decision is represented by a missing edge between the two (e.g., the absence of an edge T → D2). The parameters θ define conditional distributions for variables T , U 1, and U 2, in accordance with the values shown in Fig. 3a.(cid:11) is a set of decision rules πD for each D ∈ DDefinition 10. Given a MAID M = (G, θ ), a decision rule πD for D ∈ D is a CPD πD (D | PaD ) and a partial policy profile(cid:11)(cid:11) ⊆ D, where we write π−D(cid:11) for the set of decision rules for each D ∈ D \ D. πDA (behavioural) policy π i refers to π D i , and a (full, behavioural) policy profile π = (π 1, . . . , π n) is a tuple of policies, where π −i := (π 1, . . . , π i−1, π i+1, . . . , π n). A decision rule is pure if πD (d | paD ) ∈ {0, 1} and fully stochastic if πD (d | paD ) > 0for all d ∈ dom(D) and each decision context paD∈ dom(PaD ); this holds for a policy (profile) if it holds for all decision rules in the policy (profile).By combining π with the partial distribution Pr over the chance and utility variables, we obtain a joint distribution:Prπ (x, d, u) :=Pr(v | paV ) ·πD (d | paD ),(cid:4)(cid:4)V ∈V \DD∈Dover all the variables in M; inducing a BN. The expected utility for an agent i given a policy profile π is defined as the U ∈U i Eπ [U ] . This allows a Nash equilibrium (NE) [68] to be defined, expected sum of their utility variables in this BN, which identifies outcomes of a game where every agent is simultaneously playing a best-response.4(cid:7)Definition 11 ([51]). A policy profile π is a Nash equilibrium (NE) in a MAID if, for every agent i ∈ N, π i ∈ arg max ˆπ i ∈dom((cid:2)i )(cid:7)U ∈U i E( ˆπ i ,π −i )[U ].K&M also define strategic relevance (s-relevance) [51] as a concept to infer whether the choice of a decision rule can affect the optimality of another decision rule. They further show how s-relevance can be determined using a graphical criterion (s-reachability). In what follows, we use capital letters for variables (cid:7)D and (cid:3) which may take different values πD and θ . A more detailed introduction to this notation is provided in Section 3.1.4 In Section 5.1, we build upon what’s already known about NE in MAIDs, by explaining the difference between mixed policies and behavioural policies in MAIDs and clarifying when an NE is guaranteed to exist.9L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Definition 12 ([51]). Let Dk, Dl ∈ D be decision nodes in a MAID M. (cid:7)Dl is strategically relevant (or s-relevant) to (cid:7)Dk if there exist two joint distributions over V parameterised by (cid:3) and policy profiles π and π (cid:11)respectively, and a decision rule πDk , such that:(cid:7)U ∈U i E( ˆπD ,π −D )[U ];∈ arg max ˆπD ∈dom((cid:7)D )• πDk• π differs from π (cid:11)• πDk /∈ arg max ˆπD ∈dom((cid:7)D )) > 0.5such that Prπ (cid:11)(paDkonly at (cid:7)Dl ;(cid:7)U ∈U i E( ˆπD ,π (cid:11)−D )[U ], and neither does any decision rule ˜πD that agrees with πDk on all paDkProposition 1 ([51]). (cid:7)D(cid:11) is s-relevant to (cid:7)D if and only if (cid:7)D(cid:11) (cid:10)⊥G(cid:11) U i ∩ DescD | D, PaD ((cid:7)D is s-reachable from (cid:7)D(cid:11) ), where G(cid:11)(cid:11)the same as G with an additional variable (cid:7)D(cid:11) and edge (cid:7)D(cid:11) → D.is Both s-relevance and s-reachability only consider which other decision rules matter under a particular assumption about the rationality of agents (corresponding to a notion of subgame perfectness). In Section 3, we generalise this idea to also consider the parameterisation of non-decision variables, which is key to reasoning about causality in games. We also gen-eralise the concepts to other assumptions about agents’ rationality. By considering the directed (but not necessarily acyclic) graph over all variables (cid:2)D := {(cid:7)D }D∈D such that there is an edge (cid:7)D(cid:11) → (cid:7)D if and only if (cid:7)D(cid:11)is s-relevant to (cid:7)D – called the (s-)relevance graph – K&M also introduce a weakening of the perfect recall assumption that is sufficient for the existence of an NE.Definition 13 ([51]). Agent i in a MAID M has perfect recall if there exists a topological ordering D1 ≺ · · · ≺ Dm over D isuch that FaD j⊂ PaDk for any 1 ≤ j < k ≤ m. M is said to have perfect recall if all agents in M have perfect recall.Definition 14 ([64]). Agent i in a MAID M has sufficient recall if the s-relevance graph restricted to just agent i’s decision rules is acyclic. The MAID M is said to have sufficient recall if all agents have sufficient recall.6Proposition 2 ([51]). Any MAID with sufficient recall has at least one NE in behavioural policies.3. Mechanised MAIDs and relevanceAlthough MAIDs allow us to elegantly and concisely represent the dependencies between variables in a game, the edges in the graph only tell part of the story. Indeed, game-theoretic models are traditionally presented as objects to be solved, with the procedure used to produce solutions (i.e., policy profiles) left extrinsic to the game representation. However, the fact that agents act strategically means that agents’ policies may be dependent on some of the parameters of the game (as well as other agents’ policies) in ways that are crucial for causal reasoning, yet not explicitly represented by MAIDs.In this section, we introduce mechanised MAIDs, which allow us to model these dependencies alongside the existing dependencies of the MAID. This representation will be fundamental for causal reasoning in games (in Section 4), as well as for the introduction of subgames in MAIDs (in Section 5.2). In the following subsections, we first define mechanised MAIDs before formally introducing the concept of relevance and corresponding graphical criteria.3.1. Mechanised MAIDsIn order to explicitly capture the implicit dependencies between the decision rules and CPDs of a MAID M = (G, θ ), a mechanised MAID mM adds two elements: a (graphical) representation of the decision rules and parameters, and a descrip-tion of how these depend on one another.3.1.1. Mechanised graphsFor the first addition, we extend G to form a mechanised graph mG. For each decision variable D, a new parent (cid:7)Drepresenting its decision rule is added, and for each non-decision variable V , a new parent (cid:6)V representing the parameters of its CPD. We call these additional parents mechanism variables M, as they determine the mechanisms by which values of the variables in the game are set,7 and the variables V in the original MAID object-level variables. To distinguish between different types of mechanism variable, we often refer to those for decisions (cid:2) = MD as decision rule variables and those for non-decisions (cid:3) = MV \D as parameter variables.5 This final condition is included in order to ensure that πDk doesn’t lead to poor decisions in decision contexts that occur with probability zero.6 Note that although sufficient recall is defined using the s-relevance graph, a similar criterion could be created for any set of rationality relations R and the resulting R-relevance graph.7 This name is partially inspired by the field of mechanism design, in which agents act by reporting their types t ∈ T and the mechanism f : T → Odetermines how this type profile (which we might view as d) is mapped to an outcome o ∈ O of the game (which we might view as v).10L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919(cid:10)(cid:5)(M \ (cid:7)D ) × (cid:7)D(cid:6)D∈DIn the mechanised graph mG, we also add new edges E (cid:11) ⊆from other mechanism variables into decision rule variables. This represents the fact that agents typically select a decision rule πD (i.e., the value of (cid:7)D ) based on both the parameterisation of the game (i.e., the values of (cid:3)) and the selection of the other decision rules in the game π −D . For example, the worker and the firm’s hiring system in Example 1 might want to select their decision rules πD1and πD2 as a function of the probability p that a worker has a hard-working temperament T = h. This would imply edges (cid:6)T → (cid:7)D1 and (cid:6)T → (cid:7)D2 .would be maximal (i.e., Pa(cid:7)DIn general, an agent might select each of their decision rules based on the value of any other mechanism variable – and so E (cid:11)= MV \{D}) – though typically some of these variables will not be relevant and thus E (cid:11)will not be maximal, a notion we make precise in Section 3.2. A mechanised graph mG for Example 1 is shown in Fig. 4a, where decision rule and parameter variables are represented using black and white rounded squares respectively.3.1.2. Mechanised gamesFor the second addition to M – a description of how the decision rules and parameters depend on one another – we must specify how the values of the new mechanism variables are determined and how each of the CPDs of the original object-level variables are defined as a function of their new mechanism variable parent. Beginning with the latter, note that for any parameterisation mV \D = θ ∈ dom((cid:3)) of the game and any policy profile mD = π ∈ dom((cid:2)), the resulting joint distribution over V can be written as:Prπ (v; θ ) = Pr(v | m) :=(cid:4)V ∈VPr(v | paV , mV ),where Pr(v | paV , mV ) is simply the CPD Pr(v | paV; θV ) defined by parameters θV = mV if V is a non-decision variable, or the decision rule πD (d | paD ) defined by πD = mV if V is a decision variable D. As such, given the parameters θ of the MAID M and a policy profile π , the distribution over V in mM is identical to that in the original MAID.Finally, we must specify how the values of the mechanism variables are determined. For the parameter variables, we simply set the distribution over each (cid:6)V to δ((cid:6)V ,θV ), and let its domain be given by (cid:5)(dom(V ) | dom(PaV )). Intuitively, this may be viewed as the fact that any particular game induced over the graph corresponds to an instantiation of the parameter variables in the mechanised game, the values of which are known by all agents.8In order to provide values for the decision rule variables, we introduce a set of rationality relations R = {r D }D∈D that describe assumptions about how the agents choose decision rules. Concretely, each decision rule (cid:7)D is governed by a serial relation r D ⊆ dom(Pa(cid:7)D ) × dom((cid:7)D ).9 This accounts for the fact an agent may not deterministically choose a single decision . In the remainder of the paper we abuse notation by also using r D (pa(cid:7)D ) as a set-valued rule πD in response to some pa(cid:7)Dfunction to denote the set of all decision rules πD such that r D (pa(cid:7)D ) = πD .10Definition 15. Given a MAID M = (G, θ ) and a set of rationality relations R, a mechanised MAID is a structure mM =(mG, θ , R), such that: the mechanised graph mG = (N, V ∪ M, mE ) is a directed (possibly cyclic) graph over V and M with edges mE := E ∪ {(MV , V )}V ∈V ∪ E (cid:11); and R = {r D }D∈D is a set of rationality relations, where each r D ⊆ dom(Pa(cid:7)D ) × dom((cid:7)D ) is a serial relation.(cid:5)(M \ (cid:7)D ) × (cid:7)D, where E (cid:11) ⊆D∈D(cid:10)(cid:6)As an example, let us suppose that each agent in Example 1 plays a best response with respect to the other. In this case, the values of (cid:7)D1 and (cid:7)D2 are determined by relations RBR = {rBRD1 , rBRD2} such that:πD ∈ rBRD (pa(cid:7)D ) ⇔ πD ∈ arg maxˆπD ∈dom((cid:7)D )(cid:11)U ∈U iE( ˆπD ,π −D )[U ],(1)for each D ∈ D i , where note that the expectation – despite the fact that the notation above includes object-level variables = mV \{D} and πD . Note that for games in which each agent has only – is defined in terms of the mechanism variables pa(cid:7)Done decision rule, this gives rise to an NE, as defined in Definition 11. In other words, the values of the mechanism variables determine the CPDs of the object-level variables, which in turn define the joint distribution over the object-level variables, and hence any expected quantities in the game. Many other kinds of game-theoretic equilibria can be naturally defined in terms of rationality relations, including subgame perfect equilibria and trembling hand perfect equilibria (introduced in Section 5.3).118 This simply amounts to a common prior assumption. Though such an assumption can be relaxed, doing so introduces additional complexities that we do not address in this work.9 A serial relation r ⊆ A × B is one such that for any a ∈ A there exists at least one b ∈ B such that (a, b) ∈ r.10 Whether we refer to some r D (pa(cid:7)D ) ∈ dom((cid:7)D ) or r D (pa(cid:7)D ) ⊆ dom((cid:7)D ) will typically be unambiguous.11 It would also be possible to define R such that, for example, every agent randomises their actions at every decision, or chooses the action that minimises their expected utility. Whilst it would be hard to view such behaviour as ‘rational’ in the game-theoretic sense, one may think of R as defining the degree(s) of rationality in a game.11L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919(cid:6)T(cid:6)U 1(cid:6)U 2TU 1U 2D 1D 2(cid:6)T(cid:6)U 1(cid:6)U 2(cid:7)D1(cid:7)D2(a)(cid:7)D1(cid:7)D2(b)Fig. 4. (a) A mechanised graph mG representing Example 1. Dotted edges represent those that are present in neither the RBR-minimal nor the s-minimal mechanised graph. Dashed edges represent just those that are not present in the s-minimal mechanised graph. (b) The RBR-relevance graph is formed by removing the dotted edges, and the s-relevance graph by further removing the dashed edge.It will often be visually useful to restrict a mechanised MAID to just the mechanism variables, as can be seen in Fig. 4b. We can then view a mechanised graph as simply the composition of the original MAID and this graph of mechanism variables, via the addition of edges MV → V for each V ∈ V .3.1.3. Rational outcomesIf all of the rationality relations R in a MAID are satisfied by π , then we say that π is an R-rational outcome of the game. For example, the RBR-rational outcomes of the MAID M representing Example 1 are simply the NEs of M. Note that R-rationality is not merely a convenience when reasoning about games, rather, it performs a necessary role by fully characterising the process by which decision rules are generated. As we will see later, the nature of the chosen rationality relations also allows us to deduce various facts about the game, purely from its graphical structure. In essence, such results hinge on the links between how agents choose their decision rules, and how the object-level variables depend on one another.Definition 16. Given a mechanised MAID mM, we say that any πD ∈ r D (pa(cid:7)D ) is an R-rational response to pa(cid:7)D, and that . The set of (full) R-rational policy profiles in a (partial) policy profile π DmM are the R-rational outcomes of the game (where we tend to drop R from the terms above when unambiguous) and is denoted by R(mM).(cid:11) is R-rational if πD ∈ r D (pa(cid:7)D ) for every D ∈ D(cid:11)In general, the R-rational outcomes in mM therefore define a set of distributions {Prπ }π ∈R(mM) over the variables Vwhere Prπ (v; θ ) = Pr(v | m) for mD = π and mV \D = θ , which can be extended over the mechanised MAID as Pr(v, m) =Pr(v | m)δ(M,m). We can thus view a mechanised MAID as a set of BNs induced by the R-rational outcomes.12 The key point here is that rationality relations form a principled, formal, and highly general way to model the inherent non-determinism in a game, in order to render the model suitable for causal reasoning.Note that Definition 16 is related to the notion of a solution in cyclic causal models [1,8]. In both formalisms, a solution corresponds to a joint probability distribution consistent with all cyclic relationships, and in general a model may have many or no solutions. This, in turn, will impact the answers to the causal queries that we might wish to ask (as we explain further in Section 4). Similarly, we can view the rational responses at a decision variable as a credal set [60], and therefore the resultant model as a form of credal network [14], where the imprecise probabilities arise from the fact that there may be more than one rational outcome in a game.Remark 1. Often, agents might only be boundedly rational [89]. For example, due to computational costs, agents might seek only to satisfice (rather than optimise) their expected utility, leading to (cid:12)-approximate variations of equilibria concepts in which each agent cannot improve their expected utility by more than (cid:12) > 0 by deviating [16]. These bounded rationality conditions can also be captured using rationality relations.12 We continue, however, to highlight different variable types in our diagrams (rather than drawing each variable as a chance variable) for clarity of exposition.12L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 1039193.2. RelevanceA natural question to ask is which edges in the mechanised graph mG are necessary. In other words, which elements of Pa(cid:7)D are necessary for computing the rational response r D (pa(cid:7)D )? In the mechanised game shown in Fig. 4a, for example, we don’t need to know θU 2 in order to compute the set of best responses πD1 if we are given πD2 . Whenever a mechanism variable MV is found to be irrelevant to (cid:7)D in this sense, we may remove the edge MV → (cid:7)D to make this independence explicit. Removing all irrelevant edges results in a subgraph mRG of mG that is minimal with respect to R.In the extreme case where all decision rules are chosen independently from all other mechanism variables, then all edges E (cid:11)between mechanism variables can be pruned. We call this subgraph of mG the independent mechanised graph13m⊥G. Game-theoretic models thereby break the independent causal mechanism assumption, which states that mechanisms should be causally and probabilistically independent of each other [75].Formally, we have the following definitions, which are similar in spirit to those proposed in earlier work [51,64,70], but differ in that they consider the case of non-deterministic rationality relations that encode the solutions of a game.Definition 17. Given a mechanised MAID mM with rationality relations R, MV ∈ Pa(cid:7)D is R-relevant to (cid:7)D if there exists and papa(cid:7)D(cid:11)such that r D (pa(cid:7)D ) (cid:10)= r D (pa(cid:7)Ddiffer only on MV .), where pa(cid:7)D(cid:11)(cid:10)= pa(cid:7)D(cid:11)(cid:7)DDefinition 18. Given a mechanised MAID mM with rationality relations R, we say that its mechanised graph mG is R-minimal, denoted by mRG, when it contains an edge MV → (cid:7)D if and only if MV is R-relevant to (cid:7)D . When mRG is restricted to the mechanism variables M, we refer to it as the R-relevance graph, denoted rRG.Definition 17 is a property of a game after mechanising it via rationality relations. Therefore, an immediate follow-up question is whether given some choice of R, we can identify R-relevance (and hence prune edges in order to find the R-minimal graph) for any MAID M = (G, θ ) simply by appealing to its underlying graph G, and the form of R. For many natural choices of R this is indeed the case, and we can derive sound and complete graphical criteria for identifying R-relevance. For a given R, we refer to these criteria as R-reachability. For example, the graphical criteria defining RBR-reachability in Proposition 3 enable us to remove the dotted edges (cid:6)U 1 → (cid:7)D2 and (cid:6)U 2 → (cid:7)D1 in Fig. 4a.Proposition 3. MV is RBR-relevant to (cid:7)D if and only if MV (cid:10)⊥m⊥G U i ∩ DescD | D, PaD or MV (cid:10)⊥m⊥G PaD , where if D ∈ D i , then U i ∩ DescD (cid:10)= ∅.By applying R-reachability to mG, we can find the R-minimal mechanised graph mRG and thus the R-relevance graph rRG. This latter object will be one that we make repeated use of, and can be viewed as a generalisation of K&M’s concept of a relevance graph simpliciter, to include all mechanism variables (as opposed to just those for decision variables) and for use with any R.14 To avoid confusion, we refer to K&M’s relevance graph as an s-relevance graph. By generalising s-relevance (Definition 12) and s-reachability (Proposition 1) to all mechanism variables, we can see in Fig. 4a, for example, that (cid:6)T is not s-relevant to (cid:7)D1 , though it is RBR-relevant.Our generalisation of the idea of s-relevance to different rationality relations parallels our development of various equi-librium refinements within MAIDs (introduced in Section 5.3), as opposed to the singular concept introduced by K&M. For instance, we shall see in Section 5.3 that s-relevance corresponds to the concept of subgame perfectness. Moreover, use of the full mechanised graph as opposed to simply its restriction to the decision rule variables is not only important for rea-soning about game-theoretic notions such as equilibria and subgames, but will also be critical for reasoning about causality in games where agents may adapt their policies in response to interventions on any mechanism variable (not just those representing decision rules).3.2.1. Generalising the soundness and completeness resultsWe conclude this section by noting that it is possible to generalise the proof procedures for Propositions 3 and 1 to other choices of R. The key step in doing so is to identify, for any decision variable D, a ‘sufficient’ (sound) but ‘minimal’ (complete) set of queries QD ⊆ Q whose values, given some m = (π , θ ), completely determine each r D . Formally, let Q(m)be the set of probabilistic queries Prπ (x | y; θ ) over X, Y ⊆ V that can be formed in a MAID given m = (π , θ). Note that we can use such queries to express expected utilities, among many other things. For any decision variable D, we are looking for a sufficient but minimal subset of queries QD ⊆ Q and a truth-valued function g D such that:πD ∈ r D (mV \{D}) ⇔ g D(cid:5)(cid:6)QD (m), dom(V ).(2)13 Dawid [18] call this diagram the extended influence diagram, but they do not investigate the dependencies and relationships between mechanisms.14 Note that the directions of the edges of the relevance graphs in this paper are the same as in [51], but are reversed compared with [50] and [42]. K&M also refer to V being relevant to/reachable from D, whereas we phrase this in terms of MV being relevant to/reachable from (cid:7)D .13L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Fig. 5. In this paper, we introduce CGs and SCGs. The causal hierarchy (associational, interventional, and counterfactual) forms the vertical axis and the number of agents (0, 1, and n) forms the horizontal axis. Note that all models in this diagram can also be mechanised.In this setting, asking whether MV is R-relevant to (cid:7)D reduces to asking whether the choice of mV may affect some Prπ (x |y) ∈ QD , which is, in turn, equivalent to asking whether V is a requisite probability node for Prπ (x | y) [86], introduced in Definition 3. Whether a variable is a requisite probability node can be determined using a well-established graphical criterion [31], given as Lemma 1.We can use this criterion to establish graphical criteria for R-reachability, which, given a judicious choice of QD , will be sound and complete. For example, the graphical criteria in Propositions 1 (s-reachability) and 3 (RBR-reachability) cor-respond to choices:QsDQBRD= {Prπ (ui ∩ descD | d, paD )},= {Prπ (ui ∩ descD | d, paD ), Prπ (paD )},respectively. With such a soundness and completeness result in hand, we can identify an R-relevance graph as follows:rRG contains an edge MV → (cid:7)D⇔ MV is R-relevant to (cid:7)D⇔ r D (pa(cid:7)D ) may vary with the value of MV⇔ QD (pa(cid:7)D , πD ) may vary with the value of MV⇔ V is a requisite probability node for some Prπ (x | y) ∈ QD⇔ MV (cid:10)⊥m⊥G X | Y for some Prπ (x | y) ∈ QDby Definition 18by Definition 17by (2)by Definition 3by Lemma 14. Causality in gamesMany types of queries may be of interest in game-theoretic scenarios. For instance, recalling Example 1, we could ask questions corresponding to:1) Predictions, such as a) ‘Given that the worker went to university, what is their wellbeing?’ or b) ‘Given that the worker always decides to go to university, what is their wellbeing?’2) Interventions, such as a) ‘Given that the worker is forced to go to university, what is their wellbeing?’ or b) ‘Given that the worker goes to university if and only if they are selected via a lottery system, what is their wellbeing?’3) Counterfactuals, such as a) ‘Given that the worker didn’t go to university, what would be their wellbeing if they had?’ or b) ‘Given that the worker never decides to go to university, what would be their wellbeing if they always decided to go to university?’Although these queries are ostensibly similar, they belong to different levels of the causal hierarchy. Predictions can be answered using (mechanised) MAIDs, which, as associational models, reside on level one. However, in order to reason about interventions and counterfactuals in games, we require models on levels two and three respectively. To this end, we introduce causal games (CGs) and structural causal games (SCGs). These can be viewed as generalising MAIDs to the causal setting, or CBNs and SCMs to the game-theoretic setting.The connections between all of these models and their various acronyms are displayed in Fig. 5. Importantly, the models in each row are a special case of those in the row below (and the models in each column generalise those in the column to its left). As such, everything defined with respect to MAIDs (such as the mechanised games of Definition 15 and the equilibrium refinements we introduce in Section 5) are also well-defined in both CGs and SCGs.Note that as well as asking queries regarding the object-level variables – such as queries 1a, 2a, and 3a – after some policy profile π has been chosen, mechanised games allow us to also ask queries regarding the mechanism variables – such as queries 1b, 2b, and 3b – before fixing a policy. While this distinction between ‘post-policy’ and ‘pre-policy’ queries is less significant in the case of predictions, we shall see in Section 4.2 that this difference corresponds to whether agents can adjust their policies in response to an intervention or not.This distinction, which is critical in game-theoretic settings, does not apply to standard causal models, as they do not contain strategic, decision-making agents. Similarly, standard game-theoretic models do not explicitly represent this dis-tinction either, as the dependencies between agents’ decisions and other aspects of the game are not explicitly captured. 14L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Table 1Examples of the queries we may ask in causal models of games, corre-sponding to those listed at the top of this section. Recall that dom(D1) ={g, ¬g}, indicating whether or not the worker goes to university, and that ¯πD1 , ˆπD1 , ˜πD1 denote possible values of the decision rule variable (cid:7)D1 . For example, ¯πD1 in query 1b is the decision rule in which the worker always decides to go to university, i.e., ¯πD1 (D 1 | T ) = δ(D 1, g). As introduced in Section 2, Pr(x y ) denotes the probability of x given a hard intervention (Y = y) and Pr(x y | z) represents the counterfactual probability of x had y been true, given that (in fact) z was true.a) Post-policyb) Pre-policy1) PredictionPrπ (u1 | g)Pr(u1 | ¯πD1 )2) InterventionPrπ (u1g )Pr(u1)ˆπD13) CounterfactualPrπ (u1gPr(u1¯πD1| ¬g)| ˜πD1 )Computationally, pre-policy interventions correspond to altering the original game then calculating the outcomes, and post-policy interventions correspond to calculating the outcomes of the original game, and then altering them. In a sense, these latter queries are a more natural analogue of existing work; in this work we unify both types of query using the same formalism.In the remainder of this section, we show how the six types of causal queries represented by the examples above (each of which is written formally in Table 1) can be answered using causal games. Doing so requires us to identify which level of the causal hierarchy a query belongs to, and to assess whether the query is pre-policy or post-policy. It is also simple to combine pre- and post-policy queries using mechanised games, though for clarity and brevity we do not do so here.4.1. PredictionsEach policy profile π in a MAID M induces a BN with joint distribution Prπ (V ; θ ). We can therefore easily compute the probability of x given some observation z, written Prπ (x | z), under a given policy profile π . Note that the distribution Prπ (V ; θ ) in the MAID can equivalently be written in the mechanised MAID as Pr(V | m) with m = (π , θ).However, in game-theoretic settings, we typically assume only that a rational outcome of the game will be chosen, not some unique π . Moreover, we may not have any reason to favour one rational outcome over another, implying that we ought to evaluate queries with respect to a set of policy profiles. In the definition below, we therefore consider the distribution Prπ (x | z) induced by each rational outcome that is consistent with the observation z. Note that, as remarked in Section 3.1, there may be many or no such rational outcomes.Definition 19. Given a mechanised MAID mM with rationality relations R, the answer to a conditional query of the prob-π ∈R(mM|z) where R(mM | z) := {π ∈ R(mM) :ability of x given observation z is given by the set PrPrπ (z) > 0} is the set of conditional rational outcomes. In general, Z ⊆ V ∪ M can include mechanism variables, and so we compute Prπ (x | z) in M as Pr(x | z, m(cid:11)) in mM, where M(cid:11) = M \ Z and mD = π .(cid:13)Prπ (x | z)R(x | z) :=(cid:12)More generally, we can view queries in games as first-order queries defined over formulae in which π is a free variable, such as ϕ(π ) ≡ Prπ (x | z). The answers to these queries are therefore only well-defined when this free variable becomes bound, or when considering a set of answers, as in Definition 19. For example, we can bind π by quantifying over it as in (cid:5)∃π ∈ R(mM | z). where ∼ ∈ {<, ≤, =, ≥, >} and q ∈ [0, 1] (as is often done in first-order logics for reasoning ϕ(π ) ∼ qabout and verifying multi-agent systems [12,56,100]), or by returning bounds such as maxπ ∈R(mM|z) ϕ(π ) (as is often done in credal networks [14]). Above, we quantify over R(mM | z), but it is also possible to quantify over any desired set of policies, or even to posit a prior distribution Pr((cid:2)) over policies. These queries strictly generalise those in BNs and models such as settable systems [95,96], which effectively consider only a single instantiation of (cid:2).(cid:6)By way of illustration, let us return to Example 1 and query 1a from earlier in this section. We can interpret this question as asking about the expected utility of the worker given the observation that they went to university, written Eπ [U 1 | g] for some policy π . For the worked examples in this section, we assume that p = Pr(T = h) = 12 and that the automated hiring system and the worker are playing best responses to one another, i.e., R = RBR. The various rational outcomes induced by this choice (i.e., the NEs of the game) are:1. The worker always chooses ¬g. The hiring system chooses j if the worker chose ¬g, otherwise they choose j with any 2. The worker chooses ¬g if T = ¬h, and otherwise chooses g with probability 12 . The hiring system chooses j if the worker chose g, otherwise they choose j with probability 45 .3. The worker always chooses g. The hiring system chooses j if the worker chose g, otherwise they choose j with any probability q ∈ [0, 1].probability q ∈ [0, 35].15L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919R(u1 | g), which yields {Eπ [U 1 | g]}π ∈R(mM|g) = { 7The expected utilities for the worker under these rational outcomes are 5, 4, and 72 , respectively. In query 1a, the conditional rational outcomes R(mM | g) are the NEs consistent with observing D1 = g. To answer the query, we must therefore compute Pr2 , 4}.As noted above, Definition 19 can also be employed when the observations made are of the mechanism variables. For example, in query 1b we condition on the observation that the worker’s strategy is given by δ(D1,g), which we refer to as ¯πD1 , i.e., the worker always decides to go to university. Based on this, we can use the mechanised MAID to compute }. Note that this set is distinct from the answer to query 1a as Probservations of mechanism and object-level variables provide us with different information, i.e., R(mM | ¯πD1 ) (cid:10)= R(mM |g). In particular, observations of mechanism variables serve primarily to rule out certain rational outcomes by conditioning on decision rules (conditioning on parameter variables tells us nothing as they have deterministic distributions and no parents).R(u1 | ¯πD1 ) and thus that {E[U 1 | ¯πD1 ]}π ∈R(mM| ¯πD1 ) = { 72One advantage of computing predictions in MAIDs (as opposed to in EFGs, for instance) is that we may exploit the conditional independencies in the graph. For example, if we were interested in how likely a worker is to be hard-working given that they went to university and were hired, then T ⊥G D2 | D1 implies that Prπ (h | g, j) = Prπ (h | g) for any policy profile π . When answering queries over object-level variables using mechanised MAIDs, we implicitly condition on the values of the mechanism variables to represent the fact the game and policy under consideration are fixed. For example, the query Prπ (h | g, j) in M is given by Pr(h | g, j, m) in mM, where mD = π . Hence, although T (cid:10)⊥mG D2 | D1, we do have T ⊥mG D2 | D1, M, as expected.4.2. InterventionsInterventional queries concern the effect of causal influences from outside a system. This becomes especially interesting in the case of games, when interventions affect not only the environment but also how the self-interested agents adapt their policies in response. In order to answer such queries, the edges in a MAID must reflect the causal structure of the world. This gives rise to the following definition, which can be viewed simply as a CBN without parameters for the decision variables. Note that as causal games are a form of MAID, they also support the associational queries introduced in the preceding section (just as CBNs may also be used to compute both interventional and associational queries).Definition 20. A causal game (CG) M = (G, θ ) is a MAID such that for any parameterisation of the decision variable CPDs π , the induced model with joint distribution Prπ (V ) is a CBN.Unlike CBNs, CGs let us ask about the effect of an intervention before or after a policy profile has been selected, which we refer to as pre- and post-policy queries respectively. Asking about the effect of an intervention after a particular policy profile π has been selected (as in query 2a) is simply the same as performing an interventional query on the CBN with joint distribution Prπ . Asking about the effect of an intervention before a policy profile has been selected (as in query 2b) means that agents are made aware of the intervention before selecting their decision rules, and thus they may react to its effects. In other words, the intervention can be viewed as producing a slightly different game that the agents then (knowingly) play.Our key observation is that pre-policy interventions can be modelled as interventions on the mechanism variables in the mechanised CG, which ensures that the effects are propagated through the processes via which agents select their decision rules. This is because the additional mechanism variables and their outgoing edges in a mechanised CG represent causal (though potentially non-deterministic) processes via which parameterisations for the object-level variables are selected. Post-policy interventions, in turn, can be modelled as standard interventions on object-level variables. We write mMI for an intervention I which may contain both pre-policy and post-policy interventions.This unification of pre- and post-policy interventions is one of the key benefits of mechanised models. Indeed, post-policy interventions, and pre-policy interventions on parameter variables, are defined exactly as in CBNs, while a pre-policy intervention on a decision rule variable (cid:7)D corresponds to replacing r D : dom(Pa(cid:7)D ) → dom((cid:7)D ) by some new ∗(cid:10)= Pa(cid:7)D . As for conditional queries, in our definition (which mirrors rDthat introduced for cyclic causal models [8], but where the cyclic dependencies are governed by relations instead of functions [1]) we quantify over the set of rational outcomes that are consistent with the given intervention.∗) → dom((cid:7)D ), where we may have Pa(cid:7)D: dom(Pa∗(cid:7)DDefinition 21. Given a mechanised CG mM with rationality relations R, the answer to an interventional query of the π ∈R(mMI ) where R(mMI )probability of x given intervention I on variables Y is given by the set Pris the set of interventional rational outcomes in the mechanised MAID mMI with rationality relations R∗ := {r}(cid:7)D ∈Y ∪{r D }(cid:7)D /∈Y and parameters determined by Pr((cid:3)I ). Note that if I is fully post-policy, then the rational outcomes remain the same, i.e., R(mMI ) = R(mM) when Y ⊆ V .(cid:13)Prπ (xI )R(xI ) :=∗D(cid:12)To illustrate these ideas, let us return to Example 1 and queries 2a and 2b. Query 2a concerns a post-policy intervention since the worker is forced to go to university unbeknownst to the firm’s hiring system; in other words, the hiring system does not observe this fact before computing a decision rule. To compute the worker’s expected utility, we must calculate g) and thus perform a hard intervention do(D1 = g) in the mechanised game (shown in Fig. 6a). As the set of rational PrR(u116L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919(cid:6)T(cid:6)U 1(cid:6)U 2(cid:6)T(cid:6)U 1(cid:6)U 2TU 1U 2TU 1U 2gD 2D 1D 2(cid:7)D1(cid:7)D2(a)ˆπ 1(cid:7)D2(b)Fig. 6. (a) A mechanised game showing the hard post-policy intervention do(D1 = g), where incoming edges to D1 are severed. (b) A mechanised game showing the hard pre-policy intervention do((cid:7)D1 = ˆπ 1), where incoming edges to (cid:7)D1 are severed.R(u1g) =(cid:12)(cid:13)Prπ (u1g)2]2 , 7= {− 3(cid:13)π ∈R(mM)π ∈R(mM), which results in outcomes does not change under a post-policy intervention, we have that Pr(cid:12)}. Note that unlike query 1a, the fact that D1 = g tells us nothing about the value of T , as it Eπ [U 1gis causally upstream of the intervention on D1. Therefore, the wellbeing of the worker may decrease because they may be sent to university even when they are lazy.To answer query 2b, concerning a pre-policy intervention, we must compute Pr(u1) where ˆπD1 (g | h) = ˆπD1 (g | ¬h) = 1represents the aforementioned lottery system, which selects students to attend university randomly with probability 12 . This time, the firm modifies their hiring system after observing the intervention – denoted by I and shown in Fig. 6b – but before the hiring system computes a decision rule, and so the new set of rational outcomes is given by R(mMI ) ={( ˆπD1 , πD2 ) : πD2 ∈ r D2 (pa(cid:7)D2 )}. In other words, we set (cid:7)D1 = ˆπD1 using a hard intervention and then allow the hiring system to best respond to this decision rule using r D2 . Note that R(mMI ) (cid:10)= R(mM | ˆπD1 ) = ∅, as there is no NE in the game that contains decision rule ˆπD1 . The lottery system removes any signalling effect of going to university, resulting in an =optimal policy for the hiring system of always offering a job to the worker and expected utility { 174π ∈R(mMI )Eπ [U 1ˆπD1ˆπD1}.(cid:13)(cid:12)]2Remark 2. In previous work, soft interventions on object-level variables V have been modelled as hard interventions on its mechanism variable MV [18]. While these can be viewed as essentially equivalent in the single-agent case,15 the possible dependencies between mechanism variables in mechanised games mean that these two types of intervention may have markedly different effects in the multi-agent setting. The difference between pre- and post-policy interventions results from a difference in the information that is available to agents when they make their decisions, rather than to the chronology of play in a game (as is also the case for the structure of EFGs).4.3. CounterfactualsThe final type of question we investigate arises when we combine predictions and interventions, as in query 3a: ‘Given that the worker didn’t go to university, what would be their wellbeing if they had?’. Such questions are counterfactual, as they combine observations made in the actual world (in which the worker didn’t go to university), with questions pertaining to a counterfactual world (where they did go to university). Answering these queries in games is significantly more nuanced and complex than those of the preceding subsections. To do so, we must first consign all stochasticity to a set of exogenous variables, one for each variable in the causal game. Just as in an SCM, each variable V is thus associated with an exogenous variable EV , and is governed by a deterministic CPD Prπ (V | PaV ), where EV ∈ PaV .Definition 22. A (Markovian) structural causal game (SCG) M = (G, θ ) is a causal game over exogenous and endogenous variables E ∪ V such that for any (deterministic) parameterisation of the decision variable CPDs ˙π , the induced model with joint distribution Pr˙π (V , E) is an SCM.15 To be precise, this is true when the agent has sufficient recall; see Definition 14.17L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919ETEU 1EU 2(cid:6)ET(cid:6)EU 1(cid:6)EU 2TU 1U 2(cid:6)T(cid:6)U 1(cid:6)U 2D 1D 2(cid:7)D1(cid:7)D2ED1ED2(a)(cid:6)ED1(cid:6)ED2(b)Fig. 7. (a) A (Markovian) SCG representing Example 1. Note that we have included exogenous variables for U 1 and U 2, although as neither is stochastic, this is not strictly necessary. (b) The RBR-relevance graph of this game.An SCG can be seen as an SCM without parameters for the decision variables. Given a policy π , we recover an SCM, as we explain in more detail below. Meanwhile, mechanised SCGs can be viewed as (a special case of) a relational causal model with cycles [1].When we mechanise SCGs, although we introduce mechanism variables for the exogenous variables (as can be seen in Fig. 7b), we view them and their object-level exogenous children as beyond the realm of observation and intervention, just as in SCMs. As such, mechanism variables of exogenous variables can largely be ignored. As in SCMs, interventional ∗distributions I(V | PaV ) must be deterministic, and soft interventions may be defined by introducing a new exogenous ∗∗V [13]. With these caveats in place, both pre-policy and post-policy predictions and interventions may be V to Pavariable Edefined in this model as described in 4.1 and 4.2 respectively.While computing predictions and interventions in SCGs is therefore relatively straightforward, there are two main diffi-culties that arise when computing counterfactuals. The first is the choice of how to represent stochastic decision rules using structural functions and exogenous variables, and the second is the problem of updating our beliefs about the policy profile played in the counterfactual world given our evidence about the policy profile played in the actual world. We resolve each of these difficulties in turn.4.3.1. Decision rules as structural functionsWe assume that agents play the same kind of game regardless of the level in the causal hierarchy at which we model them. In these games, (behavioural) play equates to selecting decision rules which stochastically sample a decision condi-tional on the value of some (non-exogenous) parents. In structural causal games, we represent these decision rules using structural functions and exogenous variables. One proposal would therefore be to view each agent as choosing both a ‘struc-tural decision rule’ ˙πD (D | PaD ) and a distribution Pr(ED ), with a shared mechanism parent (cid:7)D for both D and ED . This, however, leads to a different type signature for decision rules, and moreover leads to a formalism in which (pre-policy) in-terventions can be made upstream of stochastic variables, which are ruled out in SCMs. We therefore propose an equivalent formulation in which each agent controls only their decision variables and not their exogenous parents.Unfortunately, while our assumptions about the rationality of the agents tell us what CPDs are assigned to their decision variables, they are insufficient for telling us what precise deterministic mechanisms the agents use to implement these CPDs (as a function of some stochastic exogenous variable). In fact, unless we chose to explicitly restrict the form of the mech-anism, such as by stipulating that it belongs to some parametric class, there will typically be infinitely many deterministic functions that induce a particular distribution over a decision variable [6]. Without specifying such functions, it will not (in general) be possible to answer counterfactual queries in games, and yet the precise form of these functions may impact the answers to these queries [18].In essence, the choice of how to represent a decision rule πD ∈ (cid:5)(dom(D) | dom(PaD \ {ED })) using a stochastic exoge-nous variable ED and a deterministic mechanism ˙πD ∈ (cid:5)(dom(D) | dom(PaD )) is the choice of what part of the decision rule we assume remains fixed across counterfactual worlds (ED ) and what part may vary ( ˙πD ). Assuming that we have no pre-existing knowledge about this representation, we propose to stay true to the spirit of behavioural policies by viewing each agent’s randomisation as independent between both:• Decision rules, in the sense that learning about an agent’s random choice under one decision rule πD is uninformative in settings where the agent is using a different decision rule π (cid:11)D ;• Decision contexts, in the sense that an agent’s decision rule πD can naturally be interpreted as independently sampling an action d after seeing an assignment paD of the non-exogenous parents PaD := PaD \ {ED }.18L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919for each We formalise this assumption by representing each exogenous variable ED as a set containing a variable EπD ∈ (cid:5)(dom(D) | dom(PaD )) and paD) = dom(D) (i.e., ED is a random field with inde-pendently distributed elements). Given a stochastic decision rule πD (D | PaD ), we may then define a canonical structural representation by setting:∈ dom(PaD ), where dom(EπD ,paDDπD ,paDDPr(EπD ,paDD˙πD (D = d | paD , eD ) := δ(D,e= d) := πD (d | paD ),πD ,paD),D(3)where note that ˙πD is effectively parameterised by πD , i.e., we have ˙πD (D | PaD , ED ; πD ). The joint distribution over ED is [7]. Proposition 4 below then follows immediately,16 and means simply the product of probability distributions over Ethat we may continue to interpret decision rules, policy profiles, and rationality relations as we do in MAIDs and CGs, where each agent plays some πD ∈ r D (pa(cid:7)D ) ⊆ (cid:5)(dom(D) | dom(PaD )) and each πD parameterises ˙πD . Moreover, this additional structure allows us to generalise the definition of counterfactuals in SCMs to counterfactuals in games.πD ,paDDProposition 4. For distributions over ED and D as governed by equations (3), there is a one-to-one correspondence between the set of stochastic decision rules (cid:5)(dom(D) | dom(PaD )) and the set of deterministic decision rules dom( ˙(cid:7)D ) ⊂ (cid:5)(dom(D) | dom(PaD )). (cid:14)Moreover, given two such corresponding decision rules πD and ˙πD , then ˙πD (d | paD , eD ) Pr(eD ) deD = πD (d | paD ).dom(ED )4.3.2. Counterfactual rational outcomesThe second difficulty of answering counterfactual queries in games arises due to the possible existence of multiple rational outcomes. If we have evidence that the equilibrium π was played in the actual world, how and to what extent should that inform us of the equilibrium π (cid:11)played in the counterfactual world where the values of some mechanism variables may have changed?To answer this question, we begin by introducing our approach to answering counterfactual queries in SCGs, which mirrors Pearl’s approach for SCMs (described in Section 2.1). That is, we condition, intervene, and then compute the resulting distribution, though we generalise this to observations and interventions on both object-level and mechanism variables. In particular, we compute the set PrR(xI | z) as follows:1. For every actual rational outcome π ∈ R(mM | z), update Pr(e) to Prπ (e | z) (‘abduction’);2. Apply the intervention I, on variables Y , recomputing any rational responses to form π (cid:11)and adding new exogenous variables Eto form E∗∗(cid:11) = E ∪ E3. Return each marginal distribution nal outcome π (cid:11)(‘prediction’).where required (‘action’);(cid:14)(cid:11)) Pr(e(x | e(cid:11)) Prπ (cid:11)dom(E(cid:11)) de(cid:11)in this modified model for each new counterfactual ratio-In the first step, we update our beliefs about the exogenous and decision rule variables in the actual world under π ; in the second, we apply an intervention I to form the counterfactual world (and recompute the rational outcomes based on this change); and in the third, we return the new set of distributions that are consistent with our beliefs from the first step and the results of the intervention made in the second step..By reading these steps carefully, one notices a difficulty in SCGs that does not arise in an SCM: when we condition on zin the first step we obtain a set of policies R(mM | z) that are rational in the actual world. Then in step two, we compute new rational responses π (cid:11)rather than R(mM | z) that features in the counterfactual world of the final step. This raises the question of the extent to which knowledge of the rational policies R(mM | z) should be used to compute π (cid:11), and it is π (cid:11)Let us first note that if I is a post-policy intervention, then this has no impact on the rational outcomes of the counter-factual world – they are simply the same as the actual world, given by R(mM | z) – and hence there is no difficulty. The issue only arises in pre-policy counterfactuals, such as query 3b (‘Given that the worker never decides to go to university, what would be their wellbeing if they always decided to go to university?’), where the intervention (in query 3b, on the worker’s decision rule) means that the set of rational outcomes in the counterfactual world will be different from those in the actual world.Because each policy profile π is made up of decision rules πD , we can formalise this question by asking which decision rule variables (cid:2)(I) ⊆ (cid:2) are invariant to the intervention I. Written in terms of the three-step process above, we must have π (I) = π (cid:11)(I), i.e., for any invariant decision rule variable (cid:7) ∈ (cid:2)(I) then the counterfactual decision rule π (cid:11)D is equal to the actual decision rule πD . Those that are not invariant must have their values recomputed in the new counterfactual model. For instance, as argued above, when I is post-policy, (cid:2)(I) = (cid:2). That is, none of the values of the decision rule variables need to be recomputed. How should we choose (cid:2)(I) when I is not (fully) post-policy?There are multiple principles that could be invoked in order to make this choice. The simplest – let us call it the simplicity principle – is to recompute the values of all decision rule variables, i.e., (cid:2)(I) = ∅. In other words, the intervention 16 Though note that there are many constructions that would result in these properties; we merely present one particularly simple example.19L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919means that ‘all bets are off’ after the intervention is made, and the actual rational outcomes R(mM | z) have no bearing on the counterfactual rational outcomes. Under this principle, computing pre-policy counterfactuals is reminiscent of the approach taken in existing work on cyclic causal models [8], where two sets of solutions are induced by two halves of a twin graph. The problem with this principle is that it may require us to ignore information gathered from our observation z. For example, if Z = z implies that (cid:7)D = πD in the actual world and I is causally downstream of (cid:7)D (and hence can have no effect on the value of (cid:7)D ), then this means we also know that (cid:7)D = πD in the counterfactual world, i.e., (cid:7)D ∈ (cid:2)(I).To solve this problem, we can instead invoke the closest possible world principle, where we retain as much information as possible from our knowledge of the rational policies R(mM | z). While the values of some decision rules may still need to be recomputed, by keeping (cid:2)(I) as large as possible, we avoid the need for re-solving the entire game, and can provide a more accurate answer to counterfactual queries. The process of computing (cid:2)(I) under this principle is slightly more complicated, however, as it involves propagating the effects of an intervention through models that contain both cycles and non-determinism. In the remainder of the paper we therefore employ the simplicity principle above, which is also more in keeping with prior work.17 An algorithm for computing (cid:2)(I) according to the closest possible world principle is, however, provided in Appendix B.1 for reference.4.3.3. Defining counterfactuals in gamesGiven a set of invariant decision rule variables (cid:2)(I), the answer produced by the three-step process given above can be written as follows.Definition 23. Given a mechanised SCG mM with rationality relations R, the answer to a counterfactual query of the probability of x given observation z and intervention I on variables Y is given by the set:R(xI | z) :=Pr(cid:12)(cid:14)(cid:11)) Prπ (cid:11)dom(E(xI | e, e∗) Pr(e∗) Prπ (e | z) de(cid:13)(cid:11)(π ,π (cid:11))∈R(mMI |z),where E∗ = E(cid:11) \ E are any newly added exogenous variables as a result of a soft intervention,R(mMI | z) :=(cid:12)(π , π (cid:11)) ∈ R(mM | z) × R(mMI ) : π (I) = π (cid:11)(cid:13)(I),is the set of actual-counterfactual rational outcomes, and (cid:2)(I) is the set of invariant decision rule variables. Note that if I is fully post-policy, then the rational outcomes remain the same, i.e., (cid:2)(I) = (cid:2) and R(mMI | z) =(π , π ) : π ∈(cid:13)R(mM | z)when Y ⊆ V .(cid:12)In the definition of PrR(xI | z), for every actual policy π and counterfactual policy π (cid:2), we compute the product of three quantities. Prπ (e | z) is the updated distribution over the exogenous variables under π , and corresponds to the first step. If (cid:11) \ E to capture the stochasticity of I, which leads to the term I is a soft intervention, then we add further variables E, hence the term Prπ (cid:11)∗). Finally, Pr(e. The set R(mMI | z) defines the pairs of policies that we in the third step, we marginalise over all exogenous variables Emust consider. Namely, actual rational outcomes π ∈ R(mM | z) and counterfactual rational outcomes π (cid:11) ∈ R(mMI ) such that the decision rules invariant to I, denoted (cid:2)(I), remain the same: π (I) = π (cid:11)(I).and compute the value of xI under π (cid:11)(cid:11)∗).18 We then condition on both e and e(xI | e, e∗ = E∗We briefly demonstrate the three step process above by returning to queries 3a and 3b. To answer 3a we must compute Prπ (u1| ¬g). First, we note that as this involves a post-policy intervention then we only need to consider the actual rational goutcomes, as (cid:2)(I) = (cid:2). Thus, for each π ∈ R(mM | ¬g) we begin by updating Pr(e) to Prπ (e | ¬g). In this case, such an update amounts to changing Prπ (eT , eD1 , eD2 ) to Prπ (eT , eD1 , eD2 | ¬g), where note that we independently update each πD ,paDfor every such π and endogenous decision context paD . Following this, we apply the intervention do(D1 = g). The EDπ ∈R(mM|¬g). final answer to the query is therefore rather simple in this case, and is given by PrWe thus have (cid:13)| ¬g)Prπ (u1g| ¬g) =R(u1g| ¬g]}.(cid:12)(cid:12)(cid:13)π ∈R(mM|¬g)= { 72Query 3b involves a hard pre-policy intervention I that sets (cid:7)D1 to π D1 . As (cid:2)(I) = ∅, the answers to the query . In this particular game, (cid:13)(π (cid:11), π (cid:11)) : π ∈ R(mMI )are given under the interventional outcomes, i.e., R(mMI | ˜πD1 ) =R(mMI ) = R(mM | π D1 ) and so the answer is the same as the answer to query 1b.(cid:12)Eπ [U 1gRemark 3. The reasons for choosing between a CG or an SCG to model a given problem are analogous to the respective reasons for choosing a CBN or an SCM. Using the latter, one can reason about counterfactuals, path-specific effects, and questions of identifiability [5]. The former, however, is a simpler formalism, and requires less knowledge about the precise functions holding between the variables, making CGs a more attractive choice when one does not require any of the features listed above.17 In practice, for queries 3a and 3b, it happens to be the case that both principles lead to the same answer.18 Note that the distribution over these ‘fresh’ exogenous variables does not depend on the policy, actual or counterfactual.20L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919BU 1U 2BU 2D 1D 2D 2(cid:6)B(cid:6)U 1(cid:6)U 2(cid:6)B(cid:6)U 2(cid:7)D1(cid:7)D2(a)(cid:7)D2(b)V 11q¬qV 21p1V 22¬p1p2¬p2(2, 5)(4, 4)(1, 5)(2, 6)(c)Fig. 8. (a) A MAID M = (G, θ) representing Example 2 and its s-relevance graph. (b) The smallest proper s-subdiagram G(cid:11)(c) A reduced EFG where each of the proper subgames corresponds to the two possible s-subgames for G(cid:11), where D 1 = q or D 1 = ¬q respectively.of G and its s-relevance graph. 5. Solution concepts and subgamesIn Section 3, we explained how a set of rationality relations can be used to capture the process by which agents choose their decision rules, and thus which mechanisms agents need to consider when doing so. In this section, we build on these ideas in three subsections. Firstly, we detail the distinction between mixed and behavioural policies and their relation to NEs in MAIDs. Secondly, we introduce the concept of subgames within MAIDs, which, analogously to their EFG counterparts, allow us to analyse and solve parts of the game independently. Finally, we introduce several equilibrium refinements for MAIDs, which are discussed in relation to their EFG counterparts in Section 6.2. With these contributions, we aim to place causal games on a more equal footing with EFGs as a tool for game-theoretic analysis. Note that as (S)CGs are refinements of MAIDs, the results in this section also apply to these models. Concepts in this section will be explained with the help of the following example, shown in Fig. 8a.Example 2 (Warehouse robots). Two robots are working together in a warehouse. Robot one is responsible for fulfilling orders; it can decide to move quickly or slowly. It will not break anything if it moves slowly, but might break something if it moves quickly. Robot two is responsible for keeping the warehouse tidy and so must decide whether to patrol or not, but it can only observe what robot one does. If it patrols, robot two can repair broken items, but by doing so it might obstruct robot one and prevent it from completing its order. Robot one is rewarded for fulfilling orders, the quicker the better, and penalised for breaking things. Robot two is rewarded for everything in the warehouse ending up in a state of repair, but incurs a small energy cost for patrolling.To parameterise this game, let us suppose that robot one breaks something if it moves quickly (D1 = q) with probability 2 if it patrols (D2 = p) 3 but will not break anything otherwise, and that robot two obstructs robot one with probability 11and probability zero otherwise. Finally, we define the utility functions such that robot one receives a reward of 5 or 2 for completing an order quickly or slowly respectively, but it also incurs a penalty of −3 for breakages. Robot two receives a reward of 6 for everything being in a state of repair, but incurs a penalty of −1 for patrolling. Given this parameterisation, we can easily calculate the expected payoff of each agent for the four possible action combinations. For reference, we show these in Fig. 8c using an EFG that only bifurcates on the two robots’ decisions; the chance variable B has been marginalised out to create a reduced EFG (as detailed in Appendix 6.1).5.1. Nash equilibriaA solution concept aims to identify a subset of the possible outcomes of a game that may occur if agents act rationally. In non-cooperative games, the most fundamental solution concept is a Nash equilibrium (NE) [68], a policy profile such that no agent may benefit by unilaterally deviating. In Example 2, for instance, the policy profile π NE in which robot one chooses D1 = q and robot two chooses D2 = p whatever the value of D1 is an NE. Previous work introduced this concept to MAIDs, as recalled in Definition 11 [51], but did not fully characterise when an NE is guaranteed to exist in a MAID. This existence depends on which class of policies agents are permitted to choose from.So far in this paper, we have viewed agents as employing behavioural policies, where each agent selects decision rules for each of their decisions independently. In contrast, a mixed policy allows an agent to coordinate their choice of decision rules at different decisions; it is a distribution over pure policies. In what follows, we use a dot ˙ to denote the determinism of pure policies and μ to denote mixed policies.21L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Definition 24. Let dom( ˙(cid:7)D ) be the set of all possible pure decision rules for D, and recall that we write dom(V ) =×V ∈V dom(V ). A mixed policy for agent i is some μi ∈ (cid:5)(dom( ˙(cid:2)D i )), a behavioural policy is some π i ∈ dom((cid:2)D i ), and apure policy is some ˙π i ∈ dom( ˙(cid:2)D i ).Proposition 5. A (behavioural policy) NE is not guaranteed to exist in a MAID.Nash’s theorem establishes that any finite game is guaranteed to have an NE, as long as all agents are allowed to choose mixed policies [68]. However, in general, there is no such guarantee when agents are only permitted to use behavioural policies (for an example, see Appendix B.2). Despite this negative result, behavioural policies are often regarded as more ‘natural’ due to their representation of agents randomising at each decision point instead of once at the beginning of the game [72]. Moreover, behavioural policies respect the conditional independencies of the MAID’s graph. As such, an inter-esting question is when an NE in behavioural policies is guaranteed to exist. As a first step, it is relatively straightforward to prove an analogue of Kuhn’s theorem: if all agents in the game have perfect recall (i.e., agents never forget their past moves nor any of the information they knew previously, as introduced in Definition 13), then an NE in behavioural policies is guaranteed to exist [54].Lemma 2. Let π −i ∈ (cid:5)(dom(D−i )) be a partial (behavioural or mixed) policy profile for agents N \ {i} in a MAID M. If agent i has perfect recall in M, then for every mixed policy μi there exists a behavioural policy π i such that Pr(μi ,π −i )(v) =Pr(π i ,π −i )(v).−i) | dom(PaDProposition 6. In any MAID M with perfect recall, there exists a (behavioural) policy profile π that is an NE.Going further, because MAIDs reveal conditional independencies between variables, only a weaker criterion of sufficient recall is sufficient for the existence of an NE in behavioural policies. K&M implicitly prove this result – included as Proposi-tion 2 – when proving that their Algorithm for finding an NE always converges under certain conditions (as these correspond with sufficient recall) [51]. Appendix B.2 provides an example of a MAID in which all agents have sufficient but imperfect recall. In this example, there exists a mixed policy profile with no behavioural policy resulting in the same distribution over outcomes. There is still, however, an NE in behavioural policies. On the other hand, a MAID without sufficient recall may not have an NE in behavioural policies (an example is again given in Appendix B.2).Proposition 7. If an agent i in a MAID M has perfect recall, then they also have sufficient recall. However, if an agent has sufficient recall, then they do not always have perfect recall.5.2. SubgamesSubgames in EFGs represent parts of the game that can be solved independently from the rest; we now introduce the analogous notion of R-subgames in MAIDs. These subgames have three uses: they allow us to introduce further equilibrium refinements (in Section 5.3); they can reduce the cost of computing equilibria [42]; and they allow us to analyse agents’ decision-making in isolation from other parts of the game, which may be useful for other forms of analysis (such as those discussed in Section 7). In Appendix C.2, we demonstrate the second of these facts by empirically showing how subgames in MAIDs can be used to compute NEs much more efficiently than in EFGs.However, there are two key differences between subgames in EFGs and those in MAIDs. Firstly, because MAIDs explicitly represent conditional independencies between variables, we can often find more subgames in a MAID than in its corre-sponding EFG. Secondly, because the ability to solve part of a game independently varies with the solution concept, the R-subgames vary with R. Given a set of graphical criteria, R-reachability, such as those in Section 3.2, one can identify the structure of any R-subgames – which we refer to as an R-subdiagram – using only the underlying graph.Definition 25. Given a mechanised MAID mM = (mG, θ , R) and a set of sound and complete graphical criteria for R-(cid:11) ⊆ N possessing relevance – i.e., R-reachability – we refer to the subgraph (Vdecision variables in that subgraph, as an R-subdiagram G(cid:11) = (N(cid:11), E (cid:11)) of G, along with the set of agents N(cid:11), E (cid:11)) if:(cid:11), V(cid:11)(cid:11)• V• V(cid:11)contains every variable Z such that M Z is R-reachable from some (cid:7)D with D ∈ V, every variable that lies on a directed path X (cid:2)(cid:2)(cid:3) Y in G.contains, for all X, Y ∈ V(cid:11);As before, we drop R from our notation where unimportant or unambiguous.The first condition on Vensures that for any decision variable D in the subdiagram, any variable whose mechanism may impact the rational response for D is also included in the graph. This means that the rational responses for the decision rules in this part of the game are independent of what happens elsewhere. The second condition says that additional (cid:11)22L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919TU 1U 2U 1U 2U 1U 2U 2D 1D 2D 1D 2D 2D 2(cid:6)T(cid:6)U 1(cid:6)U 2(cid:6)U 1(cid:6)U 2(cid:6)U 1(cid:6)U 2(cid:6)U 2(cid:7)D1(cid:7)D2(cid:7)D1(cid:7)D2(a)(b)(cid:7)D2(c)(cid:7)D2(d)Fig. 9. (a) A MAID M = (G, θ) for the modified version of Example 1 – in which the firm’s profits are also function of the worker’s decision but not of their temperament – and resulting s-relevance graph. The graph G is also an (improper) s-subdiagram and the full game an (improper) s-subgame. Figures (b), (c), and (d) illustrate the remaining (proper) s-subdiagrams of G and their s-relevance graphs.variables may also be included in the subdiagram as long as mediators are included too. This ensures that the CPDs for all the variables in the subgame remain consistent, and allows us to define a correspondence between subgames in MAIDs and subgames in EFGs (in Section 6.2). To find the subgames for each subdiagram, we must update the parameterisation of the remaining variables to be consistent with the original game and the structure of the graph.Definition 26. Given a mechanised MAID mM = (mG, θ , R), an R-subgame of M is a new MAID M(cid:11) = (G(cid:11), θ (cid:11)) where (cid:11); θ (cid:11)) := Pr(vG(cid:11)(cid:11) | z; θ ), where z is some instantiation of the variables Z = V \ V.19 An R-subgame is feasible if there exists a policy profile π where Prπ (z) > 0.is an R-subdiagram of G and θ (cid:11)is defined by Pr(cid:11)(v(cid:2)R-subgames can be found for any R using only R-reachability. In particular, s-reachability produces s-subgames, which in many ways are the most natural form of subgame in MAIDs (because of their connection to subgames in EFGs, as shown formally in Section 6.2). In the remainder of the paper, unless otherwise specified, we therefore focus our attention on this case. Note also that any MAID is an R-subgame of itself, and so an R-subgame on a strictly smaller set of variables is called a proper R-subgame. For example, the MAID for Example 1 (shown in Fig. 3b) has no proper RBR-subgames because (cid:7)D1 and (cid:7)D2 are both RBR-reachable from one another.5.2.1. Identifying more subgames in MAIDsBefore continuing, we note that the conditional dependencies captured in MAIDs allow for a richer and stronger notion of subgames than in EFGs. Not only can different notions of R-subgame be introduced for different rationality relations, but it is often possible to identify more subgames (and hence rule out more non-credible threats) in a MAID than in the corresponding EFG. This can be seen by considering a minor variation on Example 1. Suppose that the firm has a new vacancy in which what is important is not whether the worker is hard-working or lazy, but whether they have studied at university. In other words, U 2 is a function of D1 and D2 but not T .The game graph and s-relevance graph for this example are shown in Fig. 9a. Note that the structure of the EFG (shown in Fig. 3a) does not change, only the payoffs for the firm. As such, there are no proper subgames in the EFG because when the hiring system is making its decision (D2), it cannot observe the value of T and so no proper subtree is closed under both descendants and information sets. In contrast, we can recognise three proper s-subdiagrams (shown in Figs. 9b, 9c, and 9d) of the equivalent MAID. Each of these s-subdiagrams has two s-subgames associated with it owing to the two values that T can take (for the s-subdiagram in Fig. 9b) and the two values that D1 can take (for the s-subdiagrams in Figs. 9c and 9d).5.3. Equilibrium refinementsWhen more than one NE exists, it is useful to specify additional criteria to rule out less plausible outcomes. This cor-responds to making additional assumptions about the rationality of agents, which can be encoded as stricter rationality relations. Below, we provide definitions of two of the most important equilibrium refinements – subgame perfect equilibria [85] and trembling hand perfect equilibria [84] – within MAIDs. Later, in Section 6.2, we provide proofs regarding various equivalences between these definitions and those in EFGs.19 In fact, it can easily be appreciated that only the setting z of the variables that have a child in V(cid:2)will matter.23L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 1039195.3.1. Subgame perfect equilibriumThe concept of a subgame perfect equilibrium (SPE) was introduced into EFGs in order to eliminate NEs containing non-credible threats – choices made by an agent in a sequential game that would not be in their best interest to carry out if given the opportunity [84,85]. In MAIDs, we can rule out non-credible threats by ensuring that each agent plays a best response in every feasible s-subgame. In games with sufficient recall, an SPE (in behavioural policies) can always be constructed by performing backwards induction over the s-subgames and finding an NE in each; it is this technique in Appendix C.2 that allows an NE to be computed much more efficiently than in a corresponding EFG. However, if even one agent in the game has insufficient recall, an SPE may not exist even when allowing for mixed policies (see Appendix B.2 for an example).Definition 27. A policy profile π in a MAID M is a subgame perfect equilibrium (SPE) if π is an NE in every feasible s-subgame of M, when restricted to that subgame.20Proposition 8. Any MAID M with sufficient recall has at least one SPE in behavioural policies.Recall π NE in Example 2, introduced in Section 5.1, in which robot one chooses D1 = q and robot two chooses D2 = pwhatever the value of D1. We can immediately see that in the feasible s-subgame where D1 = ¬q – the s-subdiagram for the smallest such s-subgame is shown in Fig. 8b – then choosing D2 = p is a non-credible threat, resulting in expected utility 5 instead of 6 for robot two. Instead, an example of an SPE is the policy profile π SPE in which robot one chooses D1 = ¬q, and robot two chooses D2 = p if and only if D1 = q. Under such a policy profile, robot two achieves its optimal expected utility in any of the feasible s-subgames, and given that robot two is following this policy, robot two receives expected utility 2 regardless of whether they move quickly or not.Before continuing, we note that rationality relations allow us to capture arbitrary sets of policy profiles as rational outcomes, including the equilibrium refinements in this section. For example, (when each agent has at most one decision) the rational outcomes RBR(mM) are simply the NEs of M, as can easily be seen via inspection of Equation (1) and Definition 11. Similarly, for a MAID M let us denote by M(D) the set of all feasible s-subgames containing D, and define:πD ∈ rSPD (pa(cid:7)D ) ⇔ πD ∈ arg maxˆπD ∈dom((cid:7)D )E( ˆπD ,π (cid:11)−D )[U ] ∀ M(cid:11) ∈ M(D),(cid:11)U ∈U i ∩V(cid:11)expressing that each agent plays a best response for their decision rule in every s-subgame containing that decision. In other words, π ∈ RSP(mM) if π (cid:11) ∈ RBR(mM(cid:11)) for each s-subgame M(cid:11)is π restricted to the decision variables in M(cid:11). If M has sufficient recall, RSP(mM) are the SPEs of the game, as stated formally below. While such representations may sometimes be slightly more cumbersome, encoding equilibria via rationality relations facilitates the use of R-relevance and hence R-reachability. This offers a principled way to identify independencies that can be useful both for causal and game-theoretic reasoning.of M, where π (cid:11)Proposition 9. Suppose that a MAID M has sufficient recall. Then, the set of SPEs of M is equal to the set of rational outcomes RSP(mM).5.3.2. Trembling hand perfect equilibriumIn an SPE, agents make decisions on the assumption that an SPE will be played in all proper subgames. As a result, however, the optimality of their strategies may not be robust to events in which other agents make mistakes, or ‘tremble’, with some small probability. To solve this problem, we can stipulate that each agent must play a best response (leading to an NE) in each perturbed game [84]. Let ζk be a perturbation vector containing, for every D ∈ D, d ∈ dom(D), and decision context paD , a value (cid:12)paD≤ 1. Then, given a game M, the perturbed game M(ζk) is ∈ (0, 1) such that defined such that each decision rule πD is forced to have πD (d | paD ) ≥ (cid:12)paDd∈dom(D) (cid:12)paD(cid:7)dd.dDefinition 28. A policy profile π is a trembling hand perfect equilibrium (THPE) in a MAID M if there is a sequence of k∈N such that limk→∞ (cid:22)ζk(cid:22)∞ = 0 and for each perturbed MAID M(ζk) there is an NE π k such that perturbation vectors {ζk}limk→∞ π k = π .For example, the policy profile π SPE is not a THPE. To see this, suppose that robot two trembles with probability (cid:12) > 0when D1 = q and (cid:12)(cid:11) > 0 when D1 = ¬q. Then, robot one’s expected utility when playing q is 2 + 2(cid:12) and when playing ¬qis 2 − (cid:12)(cid:11). Therefore in any NE of the perturbed game robot one will play q with probability one. One THPE is given by the policy profile π THPE in which robot one chooses D1 = q and robot two chooses D2 = p if and only if D1 = q. Note that in any two-agent game, THPEs rule out all weakly dominated policies. In this example, robot one’s policy of always choosing D1 = ¬q is weakly dominated by the policy of always choosing D1 = q (which itself is not weakly dominated by any other policy).20 Note that this notion of subgame perfectness can be generalised to other choices of rationality relations.24L. Hammond, J. Fox, T. Everitt et al.6. Connections to EFGsArtificial Intelligence 320 (2023) 103919EFGs are perhaps the most widely studied model of dynamic strategic decision-making. Despite their intuitive appeal, these tree-based models can be less concise and reveal less of the underlying structure of a game than the DAG-based models we use in this paper. With that said, a natural question is whether the game-theoretic definitions for MAIDs in Section 5 successfully capture the familiar concepts of their EFG counterparts, or whether we might lose something by working with MAIDs – and hence (S)CGs – instead.In this section, we show that such concerns are unwarranted: these game-theoretic concepts are preserved when we convert between representations. We begin by briefly describing two algorithms, maid2efg and efg2maid, that imple-ment these conversions. Using these procedures, we then provide equivalence results for the definitions from Section 5. Following this, we briefly discuss how tree-based models can also be used for causal reasoning in certain cases.6.1. TransformationsWe briefly summarise two procedures for converting between MAIDs and EFGs. A more formal treatment of both can be found in Appendix A.1.6.1.1. From MAID to EFGThere are many ways to convert a MAID M = (G, θ ) into an EFG E , but these differ in their computational costs [51,74]. The basic idea, as employed by K&M, is to use a topological ordering ≺ over the variables of G to construct E ’s game tree by splitting on each of the variables in order. This splitting is required due to the fact that a variable in G defines what happens given any context (a setting of the parent variables), whereas a node in E defines what happens only in one context (the path taken to reach that node). As such, we may end up with exponentially more nodes in E than variables in G.By querying the CPDs of each variable, branches from chance nodes are labelled with probabilities based on the path taken from the root of the tree to that node, and similarly for the utilities assigned to each leaf; branches from decision nodes are labelled with the possible actions available. Given two nodes corresponding to the same decision variable D in G, we assign them to the same information set if and only if the values taken by their ancestors along the paths from the root of the tree to each node agree on all those variables in PaD . Note that because there can be more than one topological ordering ≺, we regard the output of maid2efg as a set of EFGs – one for each possible ordering.Remark 4. This procedure can be made more efficient by marginalising out all variables not in U ∪ FaD , such as in the reduced EFG for Example 2 (shown in Fig. 8c), which only has 22 leaves, as opposed to the 23 leaves that would have resulted had we retained all the variables in the original MAID (in Fig. 8a). Importantly, the information in this reduced EFG is sufficient for computing its equilibria, and can thus offer significant efficiency gains (since the cost of solving an EFG depends on its size, which is exponential in the length of ≺). In our codebase [27], we therefore implement this more efficient transformation (originally described by K&M), which can be used in conjunction with Gambit, a popular tool for solving EFGs [63].6.1.2. From EFG to MAIDBy encoding the CPDs for each variable in the MAID using trees as opposed to tables, MAIDs can represent any game using at most the same (but often exponentially less) space than an EFG [51]. In general, there are many MAIDs that can represent a given EFG. For instance, upon converting the EFG representation (in Fig. 3a) of Example 1 to a MAID, we could naïvely create a decision variable for each information set. Alternatively, we could recognise, for example, that V 11 and V 12correspond to the same real-world variable – the worker’s choice to go to university or not – and thus combine them (as shown in Fig. 3b, in which the two information sets for the second agent have also been combined).Whether two nodes represent the same variable in an EFG is a matter of domain knowledge external to the EFG, but this knowledge typically exists when creating a model of a game and is then lost by viewing the interaction as an EFG. By formalising this notion as a question of whether nodes belong to the same intervention set, we provide a procedureefg2maid (described fully in Appendix A.1.1), which maps an EFG to a unique, canonical MAID.Definition 29. An intervention set J in an EFG E is either a set of chance nodes, information sets belonging to the same agent, or leaves, such that:• Every node V ∈ J or V ∈ I iJ• No path from the root of E to one of its leaves passes through J more than once.∈ J has the same number of children;Moreover, for a valid partition of information sets and chance variables into intervention sets, we require that any path to a node V ∈ J or V ∈ I iJ∈ J passes through the same intervention sets before reaching J .In efg2maid, we assume that intervention sets are given; in the absence of such knowledge, one can simply choose each intervention set to be a singleton. The resulting MAID will remain equivalent to the EFG (in the sense described in the 25L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919following subsection), but may not be as compact as it otherwise could be. The basic idea behind the conversion is to first view the game tree as a MAID, then to add missing edges to each node from its ancestors whilst merging nodes that are in the same intervention set into single variables. Incoming edges from any variable V whose value is not observed by a decision D are then removed from PaD , along with any duplicate edges produced by merging nodes. Each leaf intervention set is split into one utility variable for each agent. The distributions over each variable V are then formed for each context paV by summing the relevant probabilities of sets of merged edges.6.2. EquivalencesUsing these transformations, we derive equivalence results between EFGs and MAIDs to demonstrate that the fundamen-tal game-theoretic notions of subgames and various equilibrium refinements in Section 5 are preserved when converting between representations. We begin by defining what we mean for two game representations to be ‘equivalent’; the under-lying idea is that the (behavioural) policy and strategy spaces in each game should be the same and each corresponding (behavioural) policy and strategy profile should lead to the same expected utility for each agent in both games. An imme-diate consequence of this is that NEs are also preserved.Definition 30. We say that a MAID M is equivalent to an EFG E (and vice versa) if there is a bijection for each agent f i : (cid:14)i → dom((cid:2)i)/ ∼ between their strategies in E and a partition of their policies in M (the quotient set of dom((cid:2)i) by an equivalence relation ∼ where π i ∼ ˆπ i if and only if π i and ˆπ i differ only on infeasible decision contexts) such that for U ∈U i Eπ [U ], where f (σ ) :=×i∈N f i(σ i). We refer to any every π ∈ f (σ ) and every agent i, we have Eσ=such f as a natural mapping between E and M.U (ρ[L])[i](cid:7)(cid:8)(cid:9)Lemma 3. Let f be a natural mapping between E and M. Then σ is an NE in E if and only if every π ∈ f (σ ) is an NE in M.The reason we use an equivalence relation on the space of policies is that efg2maid can introduce additional infeasible decision contexts: those paD such that Prπ (paD ) = 0 for all π , corresponding to paths in the EFG that do not exist. An agent’s choice in such decision contexts has no bearing on the outcome of the game, meaning we may safely abstract away from them. Any natural mapping is therefore sufficient for preserving the essential game-theoretic features of each representation, as we show below, though it may not be unique. We begin with a supporting lemma that justifies the correctness of the procedures maid2efg and efg2maid, and forms the basis of our other results.Lemma 4. If E ∈ maid2efg(M) or M = efg2maid(E), then E and M are equivalent.This lemma follows directly from the construction of a natural mapping f using the two procedures, maid2efg andefg2maid respectively. The intuition is that the information sets in an EFG correspond to the feasible decision contexts in a MAID, and thus a behavioural strategy profile σ in the EFG corresponds to a behavioural policy profile π in the MAID, and vice versa. The following result is a direct corollary of Lemma 3 and Lemma 4.Corollary 1. If E ∈ maid2efg(M) or M = efg2maid(E), then there is a natural mapping f between E and M such that σ is an NE in E if and only if every π ∈ f (σ ) is an NE in M.For a subgame E (cid:11)in an EFG, it can be shown that the variables outside E (cid:11)are not s-relevant to those in the corresponding . This means that subgames in EFGs have equivalent counterparts in their equivalent MAID, as established , and so we must add ) to the payoff for the agents in order to equate , however, this change in value is constant for each agent under s-subgame M(cid:11)by the following proposition. A minor caveat here is that some utility variables may not occur in M(cid:11)their value (under the setting of the variables outside G(cid:11)their expected utilities with that of E (cid:11)any policy profile, and so has no effect on their decisions.. For any subgame M(cid:11)that defines M(cid:11)Proposition 10. If E ∈ maid2efg(M) or M = efg2maid(E), then there is a natural mapping f between E and M such that, for every subgame E (cid:11)in M that is equivalent (modulo a constant difference between the utilities for each agent under any policy in M(cid:11)under the natural mapping f restricted to the strategies of E (cid:11)in E there is an s-subgame M(cid:11)) to E (cid:11).This restriction of f to the strategies in E (cid:11)can be made precise by considering only those feasible decision contexts that correspond to the information sets contained in E (cid:11). By first applying Proposition 10 and then Lemma 3 to each of the resulting subgames, we see that any SPE in M is carried over to E . We note, however, that as there may be more s-subgames in a MAID than in its equivalent EFG, the criterion of subgame perfectness may be slightly stronger in the MAID. In other words, not all SPEs in an EFG may be SPEs in the equivalent MAID. This additional strength can be useful in ruling out non-credible threats even when they do not fall under a particular subgame in the EFG.26L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Corollary 2. If E ∈ maid2efg(M) or M = efg2maid(E), then there is a natural mapping f between E and M such that if every π ∈ f (σ ) is an SPE in M, then σ is an SPE in E .Finally, we derive an equivalence between the THPEs in EFGs and those in MAIDs. In order to do so, it suffices to prove an equivalence between perturbed versions of the corresponding games E(ζk) and M(ζk), which can easily be done by construction using maid2efg and efg2maid, and then by applying Lemma 4 and Lemma 3 to each of these perturbed games.Proposition 11. If E ∈ maid2efg(M) or M = efg2maid(E), then there is a natural mapping f between E and M such that σis a THPE in E if and only if every π ∈ f (σ ) is a THPE in M.This series of equivalence results serves to justify MAIDs as an appropriate choice of game representation. Not only do they provide computational advantages over EFGs, they preserve some of the most fundamental game-theoretic concepts commonly employed in EFGs.6.3. Causality in EFGsOne alternative to causal games would be to define causal concepts directly via EFGs. In this section, we explain how this can be done for a limited set of causal queries by extending methods designed for probability trees [32], and explain some shortcomings of this approach relative to the models we focus on in this paper.6.3.1. InterventionsIntuitively, interventions in EFGs correspond to replacing the probability distribution(s) governing some node(s) in the game tree. More formally, consider an EFG E with intervention sets (if each intervention set is a singleton, we recover the case where each EFG variable is treated separately). As in CGs, we can apply both pre- and post-strategy interventions, and both operations have the same form. We make an intervention I on an intervention set J by replacing P j or σ ij(V j | ∩V j ∈ J AncV j ), for each V j ∈ J . Hard interventions with an arbitrary probability distribution Prepresent the special case when this distribution is given by δ(V j,v j), where v j corresponds to an outgoing edge from each node V j ∈ J . Pre-strategy interventions result from performing an intervention on the game tree before agents choose their strategies – in essence, forming a new game EI – whereas post-strategy interventions result from intervening on the probability tree that results after agents have chosen their strategies – in other words, forming a new distribution P σI over paths in the tree.∗j (V j | ∩V j ∈ J AncV j ) or σ i∗jWhile the graph of an EFG is typically viewed as encoding informational constraints upon the decisions of agents, as opposed to temporal or causal structure, it can also be given a causal interpretation in much the same way that a MAID or BN can, by restricting the EFG to be consistent with the set of all possible (hard) interventional distributions. This restriction produces a level two model in which the interventional queries described in the paragraph above are semantically meaningful. Due to the total ordering imposed by EFGs over the variables in a game, however, only a restricted subset of interventions are available (those whose distributions I condition only on ancestors of the node in question). Similarly, some queries (be they probabilistic or causal) over EFGs are not well-defined, due to the fact that, in general, not all variables are assigned a value on a path ρ from the root to a leaf.6.3.2. CounterfactualsAnswering counterfactual queries is slightly more complicated. We briefly describe an existing procedure for probability trees, and refer the interested reader to this work for further details [32]. Suppose that we wish to calculate P σ (xI | z)– the value that X = V \ {Y , Z } would have taken if Y were distributed according to I, given that in fact z is true. The Z (z) = 1, which is equivalent to first performing a hard intervention first step is to condition on z by setting P Z (z) = 1 or σ ido(Z = z) and then renormalising the probability distributions over the branches upstream of this intervention. The second step is to perform the soft intervention I on Y , without any renormalising. Thirdly and finally, we restore the probability distributions over the branches downstream of this second intervention to the original settings given by P σ . The resulting probability tree describes the post-strategy counterfactual distribution P σ (xI | z). By modifying this procedure such that the normalisation upstream of Z and the restoring of distributions downstream of Y takes place according to the actual strategy profile σ , then allowing agents to play a new counterfactual strategy profile σ (cid:11), we produce a pre-strategy counterfactual distribution (though if conditioning is performed downstream of differences between σ and σ (cid:11), the resulting distribution will not, in general, be correct).Counterfactual queries in EFGs are not invariant to the choice of tree representation, even if the distributions P σ are the same for any strategy profile σ . This difficulty arises because standard EFGs do not reside at level three of the causal hierarchy, but may be overcome (as in CBNs and CGs) by relegating all stochasticity to a set of exogenous chance nodes that appear at the top of the game tree, and allowing agents only to select deterministic strategies (over endogenous decision nodes) as a function of some exogenous noise. This also resolves the difficulties of pre-policy counterfactuals in EFGs, as updates made due to conditioning on z will now be upstream of differences between σ and σ (cid:11). Existing work on probability 27L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919trees [32], on the other hand, treats stochasticity as endogenous, and only possible to learn about (in the counterfactual world) for those variables that are upstream of the intervention I. The ordering of variables in the tree therefore implicitly represents a substantive modelling assumption that resolves the subtle complexities surrounding counterfactuals in models with intrinsic stochasticity [17,73].7. ApplicationsHaving described and analysed the theoretical features of causal games, we now consider potential domains of applica-tion. In the first subsection, we provide a case study based on the UK home insurance market. In the second, we discuss how several existing concepts for the analysis of (artificial) agents can be embedded and further developed within our framework. We provide worked examples of these embeddings in Appendix B.3. More substantive results, however, are be-yond the scope of this paper, whose fundamental aim is to lay the theoretical foundations for causal games, rather than to explore specific applications.7.1. Case study: insurance pricingWhile our primary motivation is to use causal games to analyse AI systems, another natural domain of application is in economics, where both causal and game-theoretic modelling are common. For example, when a regulator in the UK proposes a policy intervention, they are required to conduct a cost-benefit analysis to justify it. This entails making a comparison between the consequences of the intervention and the consequences of no intervention and/or alternative interventions. Some regulators, such as the Financial Conduct Authority (FCA), include a ‘causal chain’ in their analyses, though these are only heuristic diagrams rather than formal causal models. In this case study, we demonstrate the usefulness of causal games for economic analysis with an example based on the UK home insurance market, worth £6.31 billion in 2021 [34], which was the focus of a recent FCA report [25].7.1.1. The modelIn this setting: some customers are inert and do not change their insurance provider from year to year; firms charge a low price to attract customers in the first year, then increase their prices upon renewal; inert customers, therefore, end up paying more, while savvy customers spend time negotiating or switching provider each year. This allows firms to increase profits at customers’ expense, either from increased prices (if inert), or switching costs (if savvy). These practices are especially troubling because it is often more vulnerable customers (such as older or less educated individuals) who are likely to be exploited [61]. While this is not the focus of the aforementioned FCA report, it is conceivable that the use of sophisticated AI algorithms to set insurance prices based on customer details might potentially lead to an even greater degree of exploitation. A simplified model of this setting is as follows.Example 3 (Insurance market). A customer in a duopolistic insurance market must choose an insurance provider, and aims to minimise their costs. They already have a contract with one of the two providers, and so must decide whether to renew or switch. The customer is either savvy (in which case they are modelled as having a low switching cost) or inert (in which case they are modelled as having a high switching cost). The two firms use internal pricing algorithms to offer their quotes simultaneously, with the knowledge of which firm the customer has an existing contract with, but without the knowledge of the customer’s type (savvy or inert), in order to maximise their profits.We represent this setting as a causal game M with three agents (in Fig. 10a) and parameterise the model using real-world data on pricing strategies and consumer tendencies in the UK home insurance market within the period 2013-18 [25,26,61]. The two firms (agents one and two) use their pricing algorithms to decide prices d1, d2 ∈ N for their insurance premiums based on which firm the customer (agent three) currently has a contract with, c ∈ {1, 2}.21 The customer uses this information, along with their type T – savvy (s) or inert (¬s) – to choose a firm d3 ∈ {1, 2} to purchase insurance from, or to exit the market (⊥).Firm i’s utility (their profit) is assumed to be given by their policy price di minus the marginal cost of providing the policy, if selected by the customer, and zero otherwise. The customer’s utility is defined as their valuation of the policy, minus the price they pay and switching costs, with an extra −∞ term if the customer is inert (i.e., to model the searching and switching cost being prohibitively high). The customer’s utility is zero if they exit the market. The chance variables C(the customer’s current firm) and T (the customer’s type) are parameterised following the values in Table 10b.7.1.2. The proposed interventionIn the aforementioned report, the FCA proposes to ban insurance companies from setting different prices for new and existing customers [25]. Because firms are made aware of the intervention before deploying their pricing algorithms, we 21 In practice, the firms’ prices will be bounded between the marginal cost of supplying the policy, and the point at which the customer exits the market due to prices being too high.28L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919CTD 1U 1D 3U 3U 2D 2(a)Average QuantityMarginal cost of supplying policyaCustomer valuation of policySwitching cost for customerSavvy customersbCustomers currently with first firmAmount£206£296£3872%p%aThis quantity is estimated as the average amount received per policy by firms (£231) minus the average profit per policy (£25) [25]. In practice, there are other small fees that affect a firm’s rev-enue.bTaken as those who self-report as ‘actively’ searching for new policies [25].(b)Fig. 10. (a) A causal game representing Example 3. (b) Data from the UK home insurance market within the period 2013-18 [25,26,61], used to parameterise the game.represent this as a pre-policy intervention on (cid:7)D1 and (cid:7)D2 . More concretely, this corresponds to changing the rationality relations r D1 and r D2 by restricting their codomains – dom((cid:7)1) and dom((cid:7)2) – to be such that πD1 (D1 | C) = πD1 (D1) and πD2 (D2 | C) = πD2 (D2), effectively removing the edges C → D1 and C → D2 (shown as dashed in Fig. 10a). In what follows we denote this intervention by I. The FCA offers a number of hypotheses about the likely effects of this intervention, namely that:i) The percentage of customers switching will reduce;ii) Switching customers will end up paying higher prices, and renewing customers will end up paying lower prices;iii) Overall, customers will end up better off on average.Representing the intervention within a causal game M allows us to express these hypotheses formally by measuring and quantifying the causal effects of I, which cannot, in general, be done without a formal causal model. For the purpose of this analysis, we set p = 0.6 and assume that agents play a pure THPE, which, while not truly realistic, illustrates the real-world applications of causal games (which can also be used to capture boundedly rational agents). In what follows, we denote the rationality relations describing this assumption as R.Before addressing the three hypotheses, it is a simple exercise to see that for each π ∈ R(M), firms do indeed end up setting minimal prices (i.e., equal to their marginal cost) for new customers and higher prices for renewing customers, up to the point of driving away sales due to switching costs from customers. In other words, we have that πD i (D i | C =3 − i) = δ(D i, 206) and πD i (D i | C = i) = δ(D i, 244) for each i ∈ {1, 2}. For their part, customers always seek the lowest price, taking into account switching costs, and are indifferent when the price at an alternative firm is the same as the price at their current firm plus the switching cost. After the intervention I, the customer’s policy stays the same, but we have πD1 (D1 | C) = δ(D1, 288) and πD2 (D2 | C) = δ(D2, 250) for the two pricing algorithms, respectively, for any π ∈ R(MI ).The rational outcomes of the original game correspond to whether savvy customers (at each of the two firms) switch to the other firm or not. Depending on whether customers with neither firm, the first firm, the second firm, or both firms switch, then we have 0%, 43.2%, 28.8%, or 72% of customers switching, respectively. After intervention I, customers already with the second firm have no incentive to switch, and so the percentage of customers switching is either 0% or 43.2%. If we were to assume a uniform prior over rational outcomes, for instance, then we would see that the causal effect of I on the percentage of customers switching insurance provider is given by:1|R(MI )|(cid:11)switch(π ) −1|R(M)|(cid:11)π ∈R(M)switch(π ) = 21.6% − 36% = −14.4%,π ∈R(MI )(cid:7)where switch(π ) :=confirmed.i Prπ (D3 = i | C = 3 − i) is the probability that a customer switches provider. Thus, hypothesis i) is For the first part of hypothesis ii), we wish to measure the causal effect of I on the quantity Eπ [D i | C = i, D3 = 3 − i]for each i ∈ {1, 2}, in settings such that Prπ (C = i, D3 = 3 − i) > 0. For π ∈ R(M), this quantity is equal to 206, which may occur when either i = 1 or i = 2. For π ∈ R(MI ) this quantity is equal to 250, which occurs when i = 2; the customer never switches to firm one. This confirms the first half of the hypothesis. For the second part, a similar analysis shows that renewing customers pay £244 before the intervention either £250 or £288 after the intervention, which is contrary to the hypothesis. We discuss the reasons for this in the following paragraph, as well as how another proposal can be employed to achieve the desired result.Hypothesis iii) concerns simply the causal effect of I on Eπ [U 3]. The customer’s expected utility is the same under any of the rational outcomes and interventional rational outcomes respectively, so we can write simply:Eπ [U 3I ] − Eπ [U 3] = 23.12 − 52 = −28.88.29L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919As in the case for the second half of hypothesis ii), this disconfirms the hypothesis (in our model). The reason for both of these results is that the relatively high switching costs for customers (£38) and the relatively high percentage of inert customers (28%) mean that the firms’ pricing algorithms are better off sticking to higher prices to exploit customers who will not switch, rather than lower prices in order to attract customers away from other firms.To achieve the desired effects, we can instead consider another intervention proposed by the FCA, which is to reduce switching costs for customers (for example, by allowing them to stop their insurance policy from auto-renewing more easily) [25]. If we therefore consider a second intervention I(cid:11)that makes the same change as I but also modifies (cid:6)U 3 to set the switching costs of customers to £0, for example, then we have that:Eπ [U 3I(cid:11) ] − Eπ [U 3] = 72 − 52 = 20,as πD i (D i | C) = δ(D i, 224) for i ∈ {1, 2} in any π ∈ R(MI(cid:11) ). Returning to hypothesis ii), this implies that all customers now pay £224 when renewing as opposed to £244, thus completing the set of all desired outcomes.7.1.3. ExtensionsAs shown above, not only can our analysis be used to investigate the qualitative hypotheses put forwards by the FCA, it provides quantitative results too. Nevertheless, we emphasise that the model we present is highly simplified, and considers only a very simple intervention. More complex models and queries are beyond the scope of the present paper, but represent a natural avenue for further applied work. Before continuing, we briefly remark on possible extensions.First, we could extend the model to make it more realistic. This might include: increasing the number of firms; different marginal costs for different firms; different customer valuations of insurance policies from different firms; small costs to firms from customers switching; including intermediary firms; more explicit modelling of the firms’ pricing algorithms; not making savviness a binary variable; and modelling the market over time instead of as a one-shot game. With these added complexities to the model come greater opportunities to exploit the richness of causal games.In particular, in the simple model above, the lack of more intricate causal dependencies between the agents’ actions means that applying a pre-policy intervention essentially reduces to re-solving a slightly different game (where the dashed edges in Fig. 10a are removed). In larger games, it is both easier and more important to avoid this cost, which cannot (in general) be avoided in models such as EFGs that do not explicitly represent the causal structure of a game. We could also use the model for more advanced analyses, such as answering counterfactuals, identifying subgames, and computing mixtures of pre- and post-policy queries.7.2. Blame, intent, incentives, and fairnessMany of the most important concepts for reasoning about the safe and ethical use of AI systems are implicitly causal in nature. Moreover, in recent years, substantial progress has been made on formalising these concepts using causal models [22,39,55,66,80]. Because causal games are the first causal framework to explicitly capture multi-agent and game-theoretic reasoning, they open up the possibility of further work in these directions. In what follows, we explain how causal games might be applied to existing concepts in order to arrive at richer and more general results. Concrete examples of how these concepts can be modelled using causal games are provided in Appendix B.3.7.2.1. Blame and intentSCMs have been fruitfully employed in order to formally define the notion of actual causation (i.e., what it means for some event c to, in fact, cause some event e) [37,40]. Such ideas are not only of philosophical interest, but have been argued as being crucial for building AI systems that can automatically generate explanations, either of their own workings or that of other agents [41,66]. In recent years, this underlying theory has been used to formalise the concepts of blameworthiness and intention [28,39]. As these works have highlighted, issues of blame and intent become particularly acute in multi-agent settings.While these concepts can be accommodated within (structural) causal games (see Appendix B.3), they also allow for an even richer formalism of blame and intent in multi-agent settings. More specifically, there are at least four generalisations we might make. Firstly, the use of sets of utility variables that define multiple utility functions, one for each agent, would mean that costs (and hence both blame and intent) can be viewed from the perspective of multiple agents. Secondly, we could consider blame and intent with respect to strategies rather than single actions, which corresponds to considering soft (post-policy) interventions as opposed to hard (post-policy) interventions. Thirdly, we might wish to consider pre-policyinterventions by using mechanised games. For example, if a pre-policy change to one agent’s decision means that the only rational response for the second agent is something that causes a bad outcome, we might think that the first agent is at least somewhat to blame for this turn of events (although these concepts become more nuanced when there are cyclic dependencies between decision rule variables). Finally, all of these extensions can also be viewed in terms of coalitions of agents and their strategies, which has already begun to be explored in prior work [28].7.2.2. IncentivesIn order to build intelligent agents that do not behave in undesirable ways, it is useful to be able to reason about their incentives [19,22,57] and reasoning patterns [3,76]. For example, given a (possibly multi-agent) decision-making scenario, we 30L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919might be interested in asking whether an agent has an incentive to base its policy on a protected attribute, or to influence a variable that we would prefer to be left alone. Similarly, notions of fairness are often naturally expressed in terms of whether a different outcome would result in some counterfactual situation.Recent work has sought to formalise incentives by modelling such scenarios as single-agent causal games (i.e., causal influence diagrams). This work has developed a number of sound and complete graphical criteria for identifying Value of Information (VoI) [44] and Value of Control (VoC) [87]. Sound and complete graphical criteria have also been introduced for two novel concepts: response incentives, which indicate when a decision-maker can benefit from responding to a variable, and instrumental control incentives, which establish whether an agent can benefit from manipulating a variable [22].Existing work, however, has been limited to the single-agent, single-decision setting; it is our hope that causal games will provide the basis for generalising this work to the multi-agent, multi-decision setting and that the use of mechanised games may lead to additional insights. For instance, for some incentive concept C we may wish to ask questions such as ‘is there any R-rational outcome in which agent i ∈ N has a C incentive with respect to a particular variable X in the game?’. Given the fact that many proposals for safe AI systems are multi-agent [23], this generalisation to the multi-agent setting marks an important next step in analysing agent incentives. We provide an example of such a proposal – cooperative inverse reinforcement learning [36] – modelled as a causal game, in Appendix B.3.7.2.3. FairnessAnother important and useful application of incentives is for reasoning about fairness, a number of popular and influen-tial definitions of which are based explicitly on causal frameworks [4,49,55,67,102]. Indeed, it can be shown that all optimal policies π ∗in a single-decision SCIM are counterfactually unfair [55] with respect to a protected attribute A (meaning that a change to the protected attribute would change the decision made) if and only if there is an RI on A [22].The question of fairness arguably becomes even more important and interesting in the multi-agent setting, in which one not only has to ask whether or not a process is fair, but fair to whom, and whether we might be forced to trade off fairness with respect to different agents. Interestingly, despite the large number of existing works investigating fairness in games (see, e.g., the seminal papers of Rabin [79] or Fehr and Schmidt [24]) and the recent insights gained from using causal definitions of fairness, to the best of our knowledge there has been no application of these definitions to the multi-agent setting. One possible (partial) explanation for this is that researchers have, until now, been lacking precisely the kind of models that we introduce.8. DiscussionIn this paper, we have introduced a framework that we argue provides a unifying formalism for reasoning about causality in games. As mentioned in Section 1, combinations of causal and game-theoretic reasoning have long been considered by various research communities, and so we conclude with a brief summary of the relative advantages and disadvantages of causal games compared to other models, before offering some final thoughts about future directions.8.1. Advantages and disadvantages of causal gamesThe primary benefit of causal games compared to prior work is that no previous framework captured both game-theoretic and causal features in a general, principled way. Alongside our discussion of related work in Section 1.2, we expand briefly upon this claim by considering other causal and game-theoretic models in turn.8.1.1. Causal modelsStandard causal models do not take into account the presence of rational, self-interested agents. Doing so requires more than a simple re-labelling of some of the variables as decisions and utilities, as the presence of strategic, decision-making agents violates the standard assumption of independent causal mechanisms [75], represented by the edges between mech-anism variables in mechanised games. Alternative causal models that do include agents, such as CIDs [18,22,45], typically only consider a single agent.To the best of our knowledge, the only exception to this rule is settable systems [95,96], though these models do not capture the multiplicity of equilibria that arise in game-theoretic analyses, and thus do not naturally support the analysis we seek to do in this paper. The focus in these works is instead on capturing lower-level algorithmic details, which may make them more appropriate when reasoning about the data and attributes of, say, an optimisation or machine learning process.Our use of non-deterministic mechanisms (arising whenever agents are indifferent between alternatives) also serves to generalise existing work on cyclic causal models [8,38] to the relational setting [1,33,62]. However, the cyclic dependencies in causal games are of a specific form due to the fact that they represent game-theoretic equilibria, which means that causal games are not necessarily an appropriate model for analysing the more arbitrary dynamical systems and sets of simultaneous equations studied in these works.The fact that we build on top of MAIDs means that causal games inherit some of the existing game-theoretic concepts introduced in these models, such as NEs, while being consistent with the more standard probabilistic graphical models upon which MAIDs are based. This further allows us to derive notions of subgames and other equilibrium refinements, which are 31L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919critical for game-theoretic reasoning and yet are not supported by other causal models. Such concepts have been historically underexplored in the context of graphical games when compared to, say, EFGs or strategic-form games.8.1.2. Game-theoretic modelsThe most natural game-theoretic model with which to compare causal games is EFGs, which are tree-based models as opposed to DAG-based. For our purposes, the most important benefit of DAG-based models is that they can be more readily used to natively support a wide range of causal queries. This is bolstered by the use of R-relevance graphs, which form an explicit representation of the causal dependencies between agents’ decision rules; there is no such representation for these dependencies in EFGs.22 Mechanised games can be used to define a wide range of both pre- and post-policy queries in games. In contrast, as explained in Section 6.3, there are many causal queries that cannot be natively answered in EFGs.DAG-based models also more compactly and explicitly represent dependencies between variables, which can often only be understood in an EFG through inspecting the parameterisation of the game. Moreover, a DAG-based representation of a game need never be bigger than the corresponding EFG and can be exponentially smaller. On the other hand, if a game is highly asymmetric in its play paths (such as when if play proceeds down one path the game stops immediately, and on the other path it continues for several more moves), then this structure is not immediately observable from a causal game, and may effectively make many valuations of the variables irrelevant due to their inconsistency with the shorter game path.In addition to their transparency, MAIDs allow us to further exploit the conditional independencies between variables using d-separation. Using R-reachability, we may construct the R-relevance graph for any game and find more subgames in a MAID than in its equivalent EFG. This can significantly reduce the computational complexity of solving a game (as shown by the example in Appendix C.2), offers analytical benefits, and provides a way to define a stronger subgame perfectness condition. On the other hand, in the case of context-specific independencies – such as when A sometimes depends on B, and B sometimes depends on A – it is well-known that DAG-based models are a less natural choice than tree-based models [10].23Finally, though we have significantly extended the number of standard game-theoretic concepts for MAIDs (and hence causal games) and proved their equivalence to their EFG counterparts, EFGs remain a more well-investigated representation. Thus, if one is interested in more exotic equilibrium refinements, for example, EFGs are likely to be a more suitable model. It is our hope that further research on MAIDs and causal games will reduce this last difference.8.2. Future workOur priority is to use causal games to further analyse incentives in multi-agent systems, which has important applications in ensuring that we build AI systems that are safe and fair. As mentioned in Appendix 7.2, existing work has already characterised some of these incentives using CIDs. Therefore, a natural next step is to extend these definitions to multi-agent (and multi-decision) scenarios, though this is by no means trivial. For example, in single agent-settings VoI is always non-negative, whereas in multi-agent settings this need not be the case (if other agents are aware of the information gain) [78]. Further, we might wish to rule certain incentives in or out based on whether or not they occur under all or some policy profiles satisfying a particular equilibrium refinement, or more generally, falling within the set of rational outcomes.Other specific applications for which causal games may prove fruitful are, for example: designing mechanisms for auc-tions and other multi-agent systems, or analysing possible interventions on those mechanisms; generalising counterfactual fairness to multi-agent settings; providing artificial agents with the means to more easily provide explanations and reason about qualitative concepts (such as blame and intent or reasoning patterns) that can be defined using causal models of games; and deriving new definitions for similar concepts. We might also hope to extend the framework presented here with: model variations that can more easily capture dynamic settings, fine-grained subjective beliefs, or optimisation; defi-nitions capturing other classic equilibrium refinements such as perfect Bayesian equilibrium [29] or sequential equilibrium [53]; and methods of causal discovery for games.Given that we propose this paper and our accompanying codebase as a robust foundation for reasoning about causality in games, we believe our work presents many other interesting avenues for further research. We hope that the advantages causal games confer based on their generality, explainability, and succinctness (not to mention their compatibility with existing mainstream models) make them an attractive choice for researchers and practitioners alike who are interested in the intersection of causality and game theory.Declaration of competing interestThe authors declare the following financial interests/personal relationships which may be considered as potential com-peting interests: Co-author previously served as an associate editor of Artificial Intelligence journal (2009-2012) - M.W.22 This further implies that we cannot take advantage of the structure of these dependencies in order to answer pre-strategy queries more efficiently.23 Though it is possible to support said independencies in DAGs via tree-based representations of the CPDs, which can graphically capture different independencies on different branches.32L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Data availabilityAll data used is included within the paper.AcknowledgementsThis paper is a significantly expanded version of a previous publication [42]. We thank Zac Kenton, Jon Richens, Ilya Sh-pitser, Colin Rowat, Chris van Merwijk, Patrick Forré, David Reber, Joe Halpern, Paul Harrenstein, Will Lee, Vincent Conitzer, and several anonymous reviewers for their helpful comments and discussions while completing this work. Hammond was supported by an EPSRC Doctoral Training Partnership studentship (Reference: 2218880), Fox was supported by the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and Systems (Reference: EP/S024050/1), and Wooldridge was supported by a UKRI Turing AI World Leading Researcher Fellowship (Reference: EP/W002949/1).Appendix A. ProofsA.1. Transformations between game representationsMAID to EFGIn this section, we provide an encoding transforming a MAID, M = (G, θ ), into an EFG, E = (N, T , P , A, λ, I, U ). One can decide on whether one wants to keep record of the structure of the original MAID, or if one wants to optimise the encoding by splitting only some of G’s variables. If one wants to be able to preserve the full structure of M then the set of splitting variables in the resulting tree is S = X ∪ D.24 If instead, however, one wishes to minimise the complexity of computing equilibria, then one only needs to split on the MAID’s decision variables and their parents, S = FaD [74]. This is because the EFG’s tree size will be exponential in |S|. The following procedure, which we refer to as maid2efg, is based on that of K&M [51]. Given a MAID M = (G, θ ) we define an equivalent EFG E = (N, T , P , A, λ, I, U ) as fol-lows:• Choose a topological ordering S1 ≺ · · · ≺ Sn over all variables in S such that if Sk is a descendent of S j , then S j ≺ Sk.25Further, define S≺k= {S(cid:11) ∈ S : S(cid:11) ≺ Sk}.• The set of agents, N, in E is the same as in M.• The tree T is a symmetric tree with each path containing splits over all the variables in S in the order defined by ≺.• For all variables S ∈ S , if S is a chance variable S ∈ X , add it to E ’s set of chance nodes V 0. Else, if S is a decision variable S ∈ D i , add S to agent i’s set of nodes in E , V i .• Label each node V in T with an instantiation μ(V ) corresponding to the values taken by each EFG node (i.e., the branch followed from this node) on the path from the tree’s root node R to V .• For every node V in T such that its corresponding variable S V ∈ S is a chance variable in M, we determine the (cid:11) ∈ ChV (each child corresponds to a value sv ∈ dom(S V )) (cid:11)) := Pr. This equation . probability distribution P V : ChV → [0, 1] over its children Vby querying that variable’s CPD with the instantiation label at V . In other words, P V (Vcan be simplified because in a BN the CPD for each variable S V only depends on the values of its parents, paS VTherefore, we have:(cid:6)sv | μ(V )(cid:5)(cid:5)(cid:6)sv | μ(V )= Pr(cid:5)sv | μ(V )[PaS V(cid:6)],(cid:11)) := PrP V (Vwhere μ(V )[PaS Vchild from the EFG.]) is the restriction of μ(V ) to the values of PaS V . If P V (V(cid:11)) = 0 for some child V(cid:11), we remove that • The set of available actions Aivariable D = S V iin the MAID, i.e. Aijjj for agent i at node V i:= dom(D).j in the EFG is given by the domain of the corresponding decision • Define λ : V × V → A for each decision node V ∈ V 1 ∪ · · · ∪ V n and V(cid:11) ∈ ChV such that λ(V , V(cid:11)) = μ(V(cid:11))[S V ] to label the outcome of the decision.(cid:11))[PaS V• Define an equivalence relation ∼ over V 1 ∪ · · · ∪ V n such that V ∼ V] =(cid:11) ]. Then the set of information sets I i for each agent i ∈ N is simply the quotient set V i/ ∼ – the set of (cid:11) ∈ V which correspond to the same decision j for agent i if and only if their instantiation labels, restricted to their μ(V∼ equivalence classes partitioning V i . In other words, two nodes V , Vvariable S in M are in the same information set I iparents in M, are the same.if and only if S V = S V (cid:11) = S and μ(V )[PaS V(cid:11)24 We assume that descU = ∅.25 This topological ordering will, in general, be non-unique.33L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919• The payoff U : L → Rn for each leaf L ∈ L of the EFG’s tree is U (L) = (u1, . . . , un), where ui :=the expected utility of agent i in M, given the instantiation μ(L) of the variables S .(cid:7)U ∈U i E[U | μ(L)] is A.1.1. EFG to MAIDWe now detail the construction efg2maid. Unlike in the case for maid2efg, we show how every EFG can be converted to a unique, canonical equivalent MAID. Given an EFG E = (N, T , P , A, λ, I, U ) (including intervention sets) we define an equivalent MAID M = (G, θ ) as follows:• The set of agents N remains the same in M as in E .• Initialise the MAID’s graph (V , E ) as T , where edges are directed from parents to children.• For each of E ’s chance nodes V ∈ V 0, label each outgoing edge from V with a value v. Every other node with an outgoing edge is labelled, by λ, with the decision corresponding to that edge. Thus, let dom(V ) in M contain the labels of the outgoing edges from V .• For each variable V ∈ V let ρV be the unique path formed by the sequence of labels from the root R of E to the corresponding EFG node for V and let ρV [Vj in E , let ρ≺(I i• For each information set I i(cid:10)(cid:11)] denote the label of the outgoing edge from node Vj) be the set of paths from R into the nodes of I ij to the set of variables in AncI ion the path ρV .j . Next, define a function whose outgoing label is the same (cid:11)i∈N I i → 2V that maps each information set I iμ :in every path in ρ≺(I ithe nodes whose values are in μ(I) are the same as the nodes whose values are in μ(Ij). Note that by Definition 29, if information sets I and I(cid:11)j(cid:11)).• We then consider E ’s chance, decision, and leaf nodes in turn:are in the same intervention set, then – For every outgoing edge with some label v j from a chance node V ∈ V 0, add the label (v j | ρV j ) : p where p =P j(v j).– For every information set I ilabels of ρD restricted to those in μ(I ij and each variable V ∈ I ij)]) : _ where ρV [μ(I ij) and _ is a placeholder to be parameterised by a decision rule.j , add the label (v | ρV [μ(I ij)] denotes the – For each leaf variable L ∈ L with payoff vector U (L) = (u1, . . . , un), split L into n utility variables U 1, . . . , U n (dupli-cating incoming edges) with labels (ui | ρL) : 1 respectively.• Given these labellings we proceed by merging variables according to the intervention sets:– Merge each information set I ij into a single variable D j ∈ D i , collecting the labels (v | ρV [μ(I ij)]) : _ for each V ∈ I ijand retaining all incoming and outgoing edges.– Begin by adding a directed edge from every V(cid:11) ∈ AncV to V for every variable V . Then for every variable D j corre-sponding to an information set I ij , remove all the incoming edges from variables that are not in μ(I ij).– Merge each group of variables, collecting their incoming and outgoing labelled edges, that belong to the same inter-vention set.– Merge any utility variables U ij and U ik belonging to the same agent i that have the same sets of incoming edges, and collect their labels.• We then let V := X ∪ D ∪ U where X = V 0, D is the union of variable sets D i defined above, and U is the collection of utility variables, also defined above.• E is the set of edges in the graph defined above.• We conclude by defining the CPDs Pr(V | PaV ), for every V ∈ X ∪ U .– For certain instantiations, paV , the CPD over V is undefined because paV does not represent a path through the original game tree T (as mentioned in Section 6.2). For non-decision variables, this is dealt with by simply adding a null value ⊥ for every variable in X and a value of 0 for every variable in U .– Recall the labels of the form (v | ρV ) : p and let l(V ) denote the set of V ’s labels.∗ For each variable V ∈ V \ D, we define Pr(v | paV ) :=∗ For any paV such that for all v ∈ dom(V ), Pr(v | paV ) = 0, we set Pr(⊥| paV ) = 1 if V ∈ X and Pr(0 | paV ) = 1 if (v|paV ):p ∈ l(V ) p.V ∈V \D Pr(v | paV ) therefore forms a partial distribution over the non-decision variables in M.V ∈ U .• By construction, • For decision variables, we now see that the only changes in parameterisations of πD (D | PaD ) that can have any effect on any of the other variables are those that occur under settings paD such that there exists a strategy σ and path ρ in E capturing all values in paD with Prσ (ρ) > 0. In other words, those values of πD (d | paD ) corresponding to a parametrisation of (d | ρD [μ(I i(cid:7)(cid:2)j)]) : _.A.2. Theoretical resultsA.2.1. Section 3Proposition 3. MV is RBR-relevant to (cid:7)D if and only if MV (cid:10)⊥m⊥G U i ∩ DescD | D, PaD or MV (cid:10)⊥m⊥G PaD , where if D ∈ D i , then U i ∩ DescD (cid:10)= ∅.34L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Proof. First, recall from Definition 17, that MV ∈ Pa(cid:7)D is RBR-relevant to (cid:7)D if there exists pa(cid:7)DD (pa(cid:7)D ) (cid:10)= rBRrBRand only if:differ only on MV . Moreover, recall that for each D ∈ D i , πD ∈ rBR), where pa(cid:7)Dand paD (pa(cid:11)(cid:7)D(cid:11)(cid:7)D(cid:11)(cid:10)= pa(cid:7)Dsuch that D (pa(cid:7)D ) if πD ∈ arg maxˆπD ∈dom((cid:7)D )E( ˆπD ,π −D )[U ].(cid:11)U ∈U iGiven some setting m−D of all mechanism variables apart from (cid:7)D , then for each U ∈ U i and any ˆπD ∈ dom((cid:7)D ) we have that:Pr( ˆπD ,π −D )(u) =(cid:11)(cid:11)Pr( ˆπD ,π −D )(u, d, paD )==d∈dom(D)(cid:11)paD∈dom(PaD )(cid:11)d∈dom(D)(cid:11)paD∈dom(PaD )(cid:11)d∈dom(D)paD∈dom(PaD )Prπ −D (u | d, paD ) ˆπD (d | paD ) Prπ −D (paD )Prπ (u | d, paD ) ˆπD (d | paD ) Prπ (paD )The first equality simply rewrites the marginal distribution as a sum over a joint distribution; the second factorises this joint distribution; and the third results from the observations that the distribution over PaD cannot depend on the CPD πD (D | PaD ), and nor can the distribution over U given some d, paD . Next, note that in the definition of rBR we quantify over ˆπD via the arg max operation, and that the choice of ˆπD (given π −D ) can only impact the expected value of those utility variables U ∈ U i that are also in DescD . Therefore, any decision rule that is in rBRD (pa(cid:7)D ) satisfies:(cid:15) (cid:11)(cid:11)(cid:11)arg maxˆπD ∈dom((cid:7)D )U ∈U i ∩DescDd∈dom(D)paD∈dom(PaD )Prπ (u | d, paD ) ˆπD (d | paD ) Prπ (paD ) · u.(A.1)(cid:16)(cid:11)(cid:7)DTherefore, if the expression (A.1) is independent of the value mV , then MV is not RBR-relevant to (cid:7)D . As explained in = {Prπ (ui ∩Section 3.2, this is equivalent to asking whether V is a requisite probability node for any distribution in QBRDdescD | d, paD ), Prπ (paD )}. The graphical criterion for checking whether something is a requisite probability node is given in Lemma 1, and results in the criteria (which we refer to as RBR-reachability) MV (cid:10)⊥m⊥G U i ∩ DescD | D, PaD or MV (cid:10)⊥m⊥G PaD , where if D ∈ D i , then U i ∩ DescD (cid:10)= ∅.differ only on the value of MV ∈ Pa(cid:7)D , and let pa(cid:7)DWe begin by proving soundness, i.e., that if MV is not RBR-reachable from (cid:7)D , then MV is not RBR-relevant to (cid:7)D . Let (cid:10)= pabe the respective settings of Pa(cid:7)D . Additionally, let (cid:11) = (π (cid:11), θ (cid:11)). If MV is not RBR-reachable from (cid:7)D then we have that both MV ⊥m⊥G U i ∩ DescD |; θ ) = Prπ (cid:11); θ (cid:11)). Thus, (u | d, paD(paD), meaning that MV is not (cid:11)m (cid:10)= mus write m = (π , θ ) and mD, PaD and MV ⊥m⊥G PaD , implying that both Prπ (u | d, paDthe value of expression (A.1) is independent of the value of MV and so rBRRBR-relevant to (cid:7)D , as required.; θ (cid:11)) and Prπ (paD(cid:11)(cid:7)DD (pa(cid:7)D ) = rBRWe now turn to completeness, i.e., that if MV is RBR-reachable from (cid:7)D in some mechanised graph mG, where D ∈ D iand U i ∩ DescD (cid:10)= ∅, then for some mechanised MAID mM = (mG, θ , RBR), MV is RBR-relevant to (cid:7)D . Our goal is therefore to find some parameterisation of the mechanised graph mG such that if the parameterisation of (cid:7)D ’s parents pa(cid:7)Dis changed to pa, where these only differ on MV , then r D (pa(cid:7)D ) (cid:10)= r D (pa; θ ) = Prπ (cid:11)D (pa(cid:11)(cid:7)DIf MV (cid:10)⊥m⊥G U i ∩ DescD | D, PaD , then we can simply follow K&M’s proof of completeness for s-reachability to construct a parameterisation of the mechanised graph such that MV is s-relevant, and hence RBR-relevant, to (cid:7)D in the mechanised MAID. If, instead, we only have MV (cid:10)⊥m⊥G PaD , then we proceed as follows. Below, we restrict our attention to those decision nodes that satisfy U i ∩ DescD (cid:10)= ∅ for D ∈ D i . This is because if U i ∩ DescD = ∅, then agent i will be indifferent between all of its decision rules for D and so trivially we will have rBR; in this case, MVis never RBR-relevant to (cid:7)D .D (pa(cid:7)D ) = rBR) for any pa(cid:7)DLet us assume that MV (cid:10)⊥m⊥G Z for some Z ∈ PaD . Then, as MV has no parents in m⊥G and all paths between MVand Z that contain colliders are blocked, there must be a directed path MV (cid:2)(cid:2)(cid:3) Z in m⊥G. Let us denote the variables along this directed path by X0 (cid:2)(cid:2)(cid:3) Xn+1, where X0 = MV and Xn+1 = Z . Next, following our remarks above, we know that U i ∩ DescD (cid:10)= ∅, and so there is a directed path D (cid:2)(cid:2)(cid:3) U for some U ∈ U i . We denote the variables along this path by Y 0 (cid:2)(cid:2)(cid:3) Ym+1, where Y 0 = D and Ym+1 = U . Thus, in m⊥G we have a directed path X0 (cid:2)(cid:2)(cid:3) Xn+1 → Y 0 (cid:2)(cid:2)(cid:3) Ym+1 where the segments X1 (cid:2)(cid:2)(cid:3) Xn and Y 1 (cid:2)(cid:2)(cid:3) Ym might be empty.We now give two parameterisations of mG that are identical except for the two settings pa(cid:7)Dthat differ only on MV . We begin with what the two parameterisations have in common. First, we let all non-utility variables be binary, although we will show how this assumption can easily be relaxed at the end. Next, we set the CPDs for all nodes along the paths X2 (cid:2)(cid:2)(cid:3) Xn+1 and Y 1 (cid:2)(cid:2)(cid:3) Ym to copy the value of their parent in this path. That is, for i ∈ {2, n}, xi = xi−1 with probability 1, and for i ∈ {1, m}, yi = yi−1 with probability 1. Note that the value of X1 = V is determined according to the (cid:11)(cid:10)= pa(cid:7)Dand paD (pa(cid:11)(cid:7)D(cid:11)(cid:7)D(cid:11)(cid:7)D).35L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919value of MV . Finally, we set the distributions of the utility functions so that U copies the value of Ym and any other Utakes value 0 regardless of the value of PaU (cid:11) .(cid:11)For pa(cid:7)DD (pa(cid:7)D ), note that the decision rule ¯πD ∈ rBRwe let mV set the distribution over V to δ(V ,1), implying that Xn+1 = Z takes value 1. Given some decision For pa(cid:7)DD (pa(cid:7)D ) too, where ¯πD is identical to πD apart from the fact that rule πD ∈ rBR¯πD (D = 0 | Z = 0, pa= PaD \ {Z }. When D = 0 then, by the construction above, Ym+1 = U = 0, but as Z = 0 with probability zero given mV , then this outcome never occurs, and so ¯πD remains a rational response to pa(cid:7)D(cid:11)(cid:11)D ) = 1 where paD is any setting of the variables Pa(cid:11)V set the distribution over V to be such that Prπ (V = 0 | paV ) > 0 for any paV . In this case, ¯πD, we let mis not a rational response, as the context Z = 0 occurs with non-zero probability, and thus so to does U = 0, due to the (cid:11)construction above and the fact that ¯πD (D = 0 | Z = 0, paD ) = 1. Agent i’s expected utility could instead be strictly improved (cid:11)(cid:11)by selecting the decision rule δ(D,1). Thus, ¯πD /∈ rBR), i.e., MV is RBR-relevant to D (paD (pa(cid:7)D(cid:7)D(cid:7)D , as required. To extend the above proof to the case where variables have arbitrary (as opposed to binary) domains, we simply label one value in the domain of each variable by ‘1’ and another by ‘0’, making sure that for the utility variables the value labelled as ‘1’ is strictly greater than the value labelled as ‘0’. (cid:2)D (pa(cid:7)D ) (cid:10)= rBR), and hence rBR(cid:11)D.(cid:11) ∈ U iA.2.2. Section 4Proposition 4. For distributions over ED and D as governed by equations (3), there is a one-to-one correspondence between the set of stochastic decision rules (cid:5)(dom(D) | dom(PaD )) and the set of deterministic decision rules dom( ˙(cid:7)D ) ⊂ (cid:5)(dom(D) | dom(PaD )). (cid:14)Moreover, given two such corresponding decision rules πD and ˙πD , then ˙πD (d | paD , eD ) Pr(eD ) deD = πD (d | paD ).dom(ED )D then we have that f (πD ) = f (π (cid:11)Proof. Recall from Section 4.3 that we define PaD := PaD \ {ED }. Let us first examine the correspondence between the stochastic and non-stochastic decision rules. Let f : (cid:5)(dom(D) | dom(PaD )) → dom( ˙(cid:7)D ) be a function where for πD ∈ (cid:5)(dom(D) | dom(PaD )) we define f (πD ) = ˙πD such that ˙πD (d | paD , eD ) = δ(d,e). Given our definition of EDthen for any πD (cid:10)= π (cid:11)for all paD . Clearly, however, for πD (cid:10)= π (cid:11)D then there exists some paD and d ∈ dom(E= d) := πD (d | paD ) (cid:10)= π (cid:11)D ), hence f⊆ ED and hence some πD ∈ (cid:5)(dom(D) | dom(PaD )) such that f (πD ) = ˙πD , hence fπD ,paDDand so f (πD ) (cid:10)= f (π (cid:11){Eand therefore a bijection, giving us our one-to-one correspondence.is an injection. Moreover, any ˙πD of this form can be identified with the set is a surjection, π (cid:11)πD ,paDD ) if and only if EDDD ,paD) = dom(E) such that:π (cid:11)DD (d | paD ) =: Pr(EπD ,paDDD ,paDπD ,paDDπD ,paDD∈dom(PaD )= d),D ,paDPr(E= Eπ (cid:11)DpaD}Now take some πD ∈ (cid:5)(dom(D) | dom(PaD )) and ˙πD = f (πD ). Let EED and its joint product of probability distributions [7], we have that:(cid:17)−πD ,paDD:= ED \ {EπD ,paDD}. Then, by our definition of ˙πD (d | paD , eD ) Pr(eD ) deDδ(d,eπD ,paDD) Pr(eD ) deD(cid:11)δ(d,eπD ,paDD) Pr(eπD ,paDD) ·Pr(e−πD ,paDD) de−πD ,paDD(cid:17)πD ,paDeDπD ,paDD)∈dom(E(cid:11)dom(E−πD ,paDD)δ(d,eπD ,paDD)πD (eπD ,paDD| paD ) · 1dom(ED )(cid:17)=dom(ED )==πD ,paDeD∈dom(EπD ,paDD)=πD (d | paD ). (cid:2)A.2.3. Section 5Proposition 5. A (behavioural policy) NE is not guaranteed to exist in a MAID.Proof. See Appendix B.2 for an example of a MAID that does not have a (behavioural policy) NE. (cid:2)Lemma 2. Let π −i ∈ (cid:5)(dom(D−i )) be a partial (behavioural or mixed) policy profile for agents N \ {i} in a MAID M. If agent i has perfect recall in M, then for every mixed policy μi there exists a behavioural policy π i such that Pr(μi ,π −i )(v) =Pr(π i ,π −i )(v).−i) | dom(PaD36L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Proof. For the purposes of this proof, let us abuse notation by viewing each pure decision rule ˙πD as a function, so that ˙πD (paD ) = d just in case ˙πD (d | paD ) = 1. Further, for some pure policy ˙π i ∈×D∈D i˙(cid:7)D , let us write ˙π i(paD i ) = di just in case ˙πD (paD ) = d for every D ∈ D i . Then, given a fixed MAID M and a partial (behavioural or mixed) policy profile π −i , we have the partial distribution Prπ −iV ∈V \D i Prπ −i(v | paV ). It therefore suffices to show that for any μi ∈ (cid:5)(×D∈D i dom( ˙(cid:7)D )), there exists some π i ∈×D∈D i (cid:7)D such that:(cid:11)−i : di) =(v, d(cid:4)(cid:2)μi(di | paD i ) :=μi( ˙π i) =πD (d | paD ) =: π i(di | paD i ),˙π i ∈×D∈Di˙(cid:7)D : ˙π i (paDi )=diD∈D iwhere again we abuse notation somewhat by using μi and π i to denote both policies and the resulting partial distributions over variables in M. In what follows, we also suppress the qualification that ˙π i ∈×D∈D i˙(cid:7)D , which we take to be implicit when considering ˙π i . Suppose that |D i| = m and let us assume, without loss of generality, that the indices 1, . . . , m of these decision rules reflect the unique (in virtue of the fact that each agent has perfect recall) topological ordering of variables D i in M; i.e., D1 ≺ · · · ≺ Dm. In the remainder of the proof, we write D i≤ j to denote the set of variables {D1, . . . , D j} ⊆ D i . Then for each D j ∈ D i we have that FaD< j⊆ PaD j . We now define each decision rule π iD jas follows:π iD j(d j | paD j) :=⎧⎨⎩1|dom(D j )|(cid:7)1Z j˙π iD≤ j: ˙π iD≤ j(paDi≤ j)=di≤ jμi( ˙π D≤ j )if Z j = 0otherwise,where:Z j :=⎧⎨⎩1(cid:7)˙π iD< j: ˙π iD< j(pa)=di< jDi< jμi( ˙π D< j )if j = 1otherwise.Note that this behavioural decision rule is well-formed. Concretely, π iD j) = 1 as:(d j | paD jwhenever used to normalise π iD j(d j | paD jπ iD j(cid:7)d j(d j | paD j) ≥ 0 as μi( ˙π D≤ j ) ≥ 0 and thus Z j > 0(cid:21)(cid:11)(cid:11)), and (cid:22)μi( ˙π D≤ j )=(cid:11)μi( ˙π D< j ).d j˙π iD≤ j: ˙π iD≤ j(pa)=di≤ jDi≤ j˙π iD< j: ˙π iD< j(pa)=di< jDi< jUsing this definition, we may now conclude the proof, proceeding by cases. First, suppose that Z j = 0 for some j ≥ 2. Then:(cid:11)μi( ˙π D< j ) = 0 ⇒ πD j−1 (d j−1 | paD j−1) = 0 ⇒ π i(di | paD i ) = 0.˙π iD< j: ˙π iD< j(pa)=di< jDi< jDi )=di μi( ˙π i) = 0, and so we have π i(di | paD i ) =But the first antecedent above also implies that μi(di | paD i ) :=μi(di | paD i ), as required. Now suppose that there is no j such that Z j = 0. Then, by telescoping terms we have the following:˙π i (pa˙π i:(cid:7)πD j (d j | paD j)m(cid:4)j=1m(cid:4)j=1π i(di | paD i ) :===1Z j(cid:11)μi( ˙π D≤ j )˙π iD≤ j: ˙π iD≤ j(cid:11)(paDi≤ j)=di≤ jμi( ˙π D≤m ))=di≤m(pa˙π i: ˙π iD≤mD≤mDi≤m=:μi(di | paD i ). (cid:2)Proposition 6. In any MAID M with perfect recall, there exists a (behavioural) policy profile π that is an NE.Proof. First, note that an immediate consequence of Lemma 2 is that, for a given partial policy π −i (behavioural or mixed) in a MAID M with perfect recall, then under any mixed policy μi and equivalent behavioural policy π i we have:37L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919(cid:11)U ∈U kE(μi ,π −i )[U ] =(cid:11)U ∈U kE(π i ,π −i )[U ],for all agents k ∈ N. Next, by a straightforward application of Nash’s theorem [68], we know that every MAID is guaranteed to have an NE μ in mixed policies. Given μ = (μ1, . . . , μn), we can therefore apply Lemma 2 n times to result in a behavioural policy π = (π 1, . . . , π n) such that U ∈U k Eπ [U ] for all agents k ∈ N. Finally, let us assume (for a contradiction) that there is some agent i ∈ N that may deviate from π i in order to increase their expected utility. Let this deviation be denoted by ˆπ i . Then we can construct a mixed policy ˆμi ∈ (cid:5)(×D∈D i dom( ˙(cid:7)D )) that results in the same joint distribution as ˆπ i in the following manner. We define ˆμi( ˙π i) :=U ∈U k Eμ[U ] =) where we set:(cid:7)(cid:7)(cid:2)mj=1ˆμi( ˙π iD jˆμi( ˙π iD j) :=(cid:4)pa(cid:11)D j∈dom(PaD j)(cid:5)ˆπ ij˙π iD j(pa(cid:11)D j) | pa(cid:6).(cid:11)D jAgain, note that this results in a well-formed mixed policy. Concretely, ˆμi( ˙π i) ≥ 0 for any ˙π i as ˆπ ijand we have:(cid:5)˙π iD j(pa(cid:11)D j(cid:11)) | paD j(cid:6)≥ 0, (cid:11)˙π iˆμi( ˙π i) =(cid:11)(cid:11)m(cid:4). . .ˆμi( ˙π iD j)j=1˙π iDm˙π iD1m(cid:4)(cid:11)j=1m(cid:4)˙π iD j(cid:11)˙π iD j(cid:21)j=1m(cid:4)====ˆμi( ˙π iD j)(cid:21)(cid:4)(cid:11)paD j∈dom(PaD j(cid:21)(cid:4)(cid:22)(cid:6)(cid:5)ˆπ ij)˙π iD j(pa(cid:11)D j) | pa(cid:11)D j(cid:21)(cid:11)(cid:11)∈dom(D j )˙π iD j: ˙π iD j(pa(cid:11)D j)=d(cid:11)jj=1m(cid:4)(cid:21)pa(cid:11)D j∈dom(PaD j)(cid:4)(cid:11)jd(cid:22)1j=1pa(cid:11)D j∈dom(PaD j)(cid:5)ˆπ ij˙π iD j(pa(cid:11)D j) | pa(cid:11)D j(cid:22)(cid:22)(cid:22)(cid:6)=1.In order to show that policy profiles ( ˆμi, π −i) and ( ˆπ i, π −i) result in the same joint distribution over all variables (and hence the same expected payoffs for each agent), it suffices to show the following, which holds for any j ∈ {1, . . . , m}:ˆμij(d j | paD j) :===(cid:11)ˆμi( ˙π iD j)˙π iD j(paD j: ˙π iD j(cid:11))=d j(cid:21)(cid:4))=d j˙π iD j(paD j: ˙π iD j(cid:11)pa(cid:11)D j∈dom(PaD j(cid:21))ˆπ ij(d j | paD j)˙π iD j: ˙π iD j(paD j)=d jpa(cid:4)(cid:5)ˆπ ij˙π iD j(cid:11)(paD j(cid:11)) | paD j(cid:22)(cid:6)(cid:4)(cid:11)∈dom(PaD jD j(cid:21))\{paD j}(cid:11)(cid:5)ˆπ ij˙π iD j(cid:11)(paD j(cid:11)) | paD j(cid:22)(cid:6)(cid:5)ˆπ ij˙π iD j(pa(cid:11)D j) | pa(cid:11)D j(cid:22)(cid:6)= ˆπ ij(d j | paD j) ·= ˆπ ij(d j | paD j) ·= ˆπ ij(d j | paD j).pa(cid:11)D j∈dom(PaD j(cid:4))\{paD j}˙π iD j: ˙π iD j(paD j)=d jpa(cid:11)D j∈dom(PaD j)\{paD j}138L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Thus, if ˆπ i is a beneficial deviation against π −i , ˆμi is also a beneficial deviation against π −i . Further, by again appealing to Lemma 2 we can see that:(cid:11)U ∈U iE( ˆμi ,π −i )[U ] =(cid:11)U ∈U iE( ˆμi ,μ−i )[U ],and so ˆμi is also a beneficial deviation against μ−i . In which case, μ cannot be an NE, giving us our contradiction. (cid:2)Proposition 7. If an agent i in a MAID M has perfect recall, then they also have sufficient recall. However, if an agent has sufficient recall, then they do not always have perfect recall.Proof. For the first direction, we recognise that if agent i has perfect recall in M, then this means there exists a unique topological ordering of variables D i in M; i.e., D1 ≺ · · · ≺ Dm. Furthermore, for any j < k we have that FaD j⊆ PaDk . This implies that:(cid:7)D j⊥m⊥G U i ∩ DescDk| Dk, PaDk ,→ (cid:7)Dk in the s-relevance graph rsG. This implies and so (cid:7)D j is not s-reachable from (cid:7)Dk . Therefore, there is no edge (cid:7)D jthat rsG, when restricted to just agent i’s decision rule variables, must be acyclic (i.e., there can only exist edges (cid:7)D j→ (cid:7)Dkwhen j > k), as required. For the second direction, we provide an example in Appendix B.2 of a MAID where an agent has sufficient but imperfect recall. (cid:2)Proposition 8. Any MAID M with sufficient recall has at least one SPE in behavioural policies.Proof. Let G1 ≺ · · · ≺ Gm be an ordering of MAID M’s s-subdiagrams such that G j ≺ Gk implies that Gk is not an s-subdiagram of G j . If all agents have sufficient recall, then G1 contains at most one decision variable for each agent, and for each s-subdiagram G j where 1 ≤ j < m, G j+1 contains at most one additional decision variable for each agent. The newly added decision nodes in each subsequent s-subdiagram for such an ordering correspond exactly with a topological ordering of the strong connected components in M’s s-relevance graph. This means that we can employ the same method as in a similar proof of Theorem 6.2 in K&M [51] to show that if we use a backwards induction procedure to optimise the decision rules of each subgame in this order, then the resulting policy profile is guaranteed to be an NE in every feasible s-subgame of M, and hence an SPE. (cid:2)Proposition 9. Suppose that a MAID M has sufficient recall. Then, the set of SPEs of M is equal to the set of rational outcomes RSP(mM).Proof. As in the previous proof, let G1 ≺ · · · ≺ Gm be an ordering of M’s s-subdiagrams such that G j ≺ Gk implies that Gk is not an s-subdiagram of G j . If all agents have sufficient recall, then G1 contains at most one decision variable for each agent, and for each s-subdiagram G j where 1 ≤ j < m, G j+1 contains at most one additional decision variable for each agent. The fact that G1 contains at most one decision variable for each agent means that π 1 is an NE in any (feasible) s-subgame M1over G1 just in case:πD ∈ arg maxˆπD ∈dom((cid:7)D )(cid:11)U ∈U i ∩V 1E( ˆπD ,π 1,−D )[U ],for each M1, i.e., just in case πD ∈ rSPeach agent has only one decision variable are simply the NEs of the game.D (pa(cid:7)D ). This is akin to the result that the RBR-rational outcomes of a game where Similarly, if we reason analogously to the backwards induction procedure over s-subgames as described in the proof of Proposition 8 and assume that π j forms an NE in every feasible s-subgame of every M j , then by fixing π j in each (feasible) s-subgame M j+1 we induce a new MAID where each agent has at most one decision variable. A partial policy profile π (cid:11)j+1 is an NE in this game if and only if:(cid:11)E( ˆπD ,π (cid:11)j,−D )[U ],πD ∈ arg maxˆπD ∈dom((cid:7)D )(cid:10)U ∈U i ∩V j+11≤k≤ j Dk. Moreover, by our inductive hypothesis the resulting policy profile π j+1 = (π (cid:11)for each D ∈ D j+1 \j+1, π j) forms an NE (and in fact an SPE) in each (feasible) M j+1, and so we have that each such πD ∈ rSPD (pa(cid:7)D ). At the conclusion of this line of argument we see that any (full) policy profile π = π m is an NE in every feasible s-subgame of M, when restricted to that subgame, just in case each πD ∈ rSPD (pa(cid:7)D ), i.e., just in case π ∈ RSP(mM). (cid:2)39L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919A.2.4. Section 6Lemma 3. Let f be a natural mapping between E and M. Then σ is an NE in E if and only if every π ∈ f (σ ) is an NE in M.(cid:7)Proof. This result follows straightforwardly from Definition 30. Suppose that σ is an NE. Then if f is a natural mapping, we for every π ∈ f (σ ) and every agent i. If ˆπ i is a profitable deviation for player ihave that in M then there must exist some f i( ˆσ i) (cid:24) π i such that ˆσ i is a profitable deviation for player i in E . I.e., we must have:U ∈U i Eπ [U ] = EσU (ρ[L])[i](cid:8)(cid:9)(cid:11)(cid:11)Eπ [U ] <E(π −i , ˆπ i )[U ] = E(σ −i, ˆσ i )(cid:8)(cid:9)U (ρ[L])[i](cid:8)(cid:9)U (ρ[L])[i],> EσU ∈U iU ∈U iwhich contradicts our assumption that σ is an NE. The same argument also applies in the opposite direction, thus conclud-ing the proof. (cid:2)Lemma 4. If E ∈ maid2efg(M) or M = efg2maid(E), then E and M are equivalent.Proof. In what follows we make the trivial assumption that any non-leaf node V in E has more than one child (otherwise we could simply remove such nodes), and that if V is a chance node then all of its children are reached with positive probability (otherwise we could delete the subtrees rooted at such children). The proof follows directly by construction using the procedures maid2efg and efg2maid respectively.To see this, first suppose we have a MAID, M, and E is an EFG resulting from maid2efg(M). A decision rule πD defines a probability distribution over dom(D) conditional on each decision context, paD . Following the maid2efg procedure, each j where S V = Dfeasible decision context is an instantiation of a set of variables which corresponds to one information set I i(cid:11)) = d. In particular, if paDfor all V ∈ I iis infeasible then the probability of reaching I i(cid:11) ∈ ChV such that λ(V , Vj is zero and it would be removed from E (as per our assumption above).j , and for each d ∈ dom(D) there exists precisely one node VFor a policy profile π , let us define σ such that σ ij (d) := πD (d | paD ) for each feasible decision context paD . Note that j (D), πD (D | paD ) ∈ (cid:5)(dom(D)), for a given paD then this construction results in a one-to-one correspondence between σ iand that for any two π ∼ π (cid:11)differ only on infeasible decision contexts) then the same strategy σ will be constructed. This construction therefore results in a bijection f : (cid:14) → dom((cid:2))/ ∼. By reasoning analogously about the chance and utility variables, and observing that the difference between chosen actions in infeasible decision contexts has no bearing on the expected utility of each agent in M (because they all occur with probability zero) it follows that the expected utility for each agent i, Eσis a natural mapping and M and E are equivalent.U ∈U i Eπ [U ] for any σ such that π ∈ f (σ ), hence f(i.e., π and π (cid:11)U (ρ[L])[i](cid:7)=(cid:8)(cid:9)For the second part of the proof, note first that the deterministic nature of the efg2maid procedure guarantees unique-ness of the resulting MAID, and so suppose we have some M = efg2maid(E). In our construction above, the incoming edges for each decision variable D ∈ D i are precisely those that originate from the variables whose values agent i can deter-mine when in the corresponding information set(s) I ij , which assigns a probability distribution over the set of available decisions Aij , is determined as a function of paD , where paD corresponds to a particular information set I ij at node V ij from which D was created.j . Hence the strategy σ iThus, given a strategy profile σ in E we fix the corresponding policy profiles π in M to have πD (d | paD ) := σ ij (d) when paD is feasible, and let πD (d | paD ) vary otherwise. As before, we therefore have a bijection f : (cid:14) → dom((cid:2))/ ∼. The same form of correspondence (i.e., ignoring the infeasible settings of the parents) can analogously be seen to hold between the distributions P of E and the CPDs for X and U . Thus, for any π ∈ f (σ ), we have that for each agent i ∈ N, and so fis a natural mapping, as required. (cid:2)U ∈U i Eπ [U ] = EσU (ρ[L])[i](cid:7)(cid:9)(cid:8)Definition 31. A strategy profile σ is a Nash equilibrium (NE)(cid:9)σ i ∈ arg max ˆσ i ∈(cid:14)i E( ˆσ i ,σ −i )U (ρ[L])[i](cid:8).in an EFG if, for every agent i ∈ N,Corollary 1. If E ∈ maid2efg(M) or M = efg2maid(E), then there is a natural mapping f between E and M such that σ is an NE in E if and only if every π ∈ f (σ ) is an NE in M.Proof. By Lemma 4 we have that E and M are equivalent. Thus, let f : (cid:14) → dom((cid:2))/ ∼ be a natural mapping between Eand M as defined in Definition 30. By applying Lemma 3, we have that σ is an NE in E if and only if every π ∈ f (σ ) is an NE in M. (cid:2)Proposition 10. If E ∈ maid2efg(M) or M = efg2maid(E), then there is a natural mapping f between E and M such that, for every subgame E (cid:11)in M that is equivalent (modulo a constant difference between the utilities for each agent under any policy in M(cid:11)under the natural mapping f restricted to the strategies of E (cid:11)in E there is an s-subgame M(cid:11)) to E (cid:11).40L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Proof. We begin by proving existence. Recall that a subgame E (cid:11)sets and descendants. Let Vor contained in E (cid:11). We first show that Vmechanism node M Z is s-reachable from (cid:7)D for some D ∈ Von a directed path X (cid:2)(cid:2)(cid:3) Y in G.in an EFG E is a subtree that is closed under information (cid:11) ⊆ V be the set of variables in M corresponding to the intervention sets overlapping with contains every variable Z in M whose contains every variable that lies forms an s-subdiagram, i.e., that V(cid:11), and for every X, Y ∈ VBeginning with the first condition, it suffices to show that there exists no variable Z ∈ Z = V \ Vsuch that M Z is s-reachable from the decision rule (cid:7)D of some variable D ∈ D i ∩ V. Recall that this means we would have MZ (cid:10)⊥m⊥G U i ∩ DescD | FaD , where m⊥G is the mechanised graph with no edges between mechanism variables. Any path supporting such a dependency must have one of the following two forms:for i ∈ N, V(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)• MZ → Z → · · · U i ∩ DescD . In this case, as Z /∈ VE corresponding to Z must lie outside E (cid:11)observed by any decision node in E (cid:11)Z ∈ FaD . Hence, by conditioning on FaD we block the path above, meaning there is no dependency.then, by either construction efg2maid or maid2efg, any node in is closed under information sets, then the value of Z must be corresponding to D, and thus there is an information link Z → D in M and so (cid:2). Further, as E (cid:11)• MZ → Z ← · · · U i ∩ DescD . In this case, let W be the first variable in the path (from left to right) that is a fork variable, i.e., we have · · · ← W → · · · . Such a variable must exist because we assume that utility variables do not have children. As before, notice that by either construction efg2maid or maid2efg we have that Z /∈ V, and as there is a directed path W (cid:2)(cid:2)(cid:3) Z in M then W must also lie outside E (cid:11)is closed under information sets means that W is observed by D and so conditioning on FaD blocks the path above, meaning there is no dependency.. But then the fact that E (cid:11)must lie outside E (cid:11)(cid:2)and there is a directed path X (cid:2)(cid:2)(cid:3) Z (cid:2)(cid:2)(cid:3) Y in M. Then We next consider the second condition. Suppose that X, Y ∈ Vif E ∈ maid2efg(M) or M = efg2maid(E), any topological ordering ≺ over the variables in M must have X ≺ Z ≺ Yand hence if there are nodes in E (cid:11)is closed under descendants we must have a node corresponding to Z in E (cid:11)by the (cid:11)definition of V.. This means that the intervention set containing Z overlaps with E (cid:11)corresponding to both X and Y then as E (cid:11)and so Z ∈ V(cid:11)For the second part of the existence proof, we show that there is a setting z of Z which, when combined with V(cid:2)(cid:11)leads to an s-subgame M(cid:11)the root R of E to the root Rthe path from R to Rthe argument above, the decision variables in Dmoreover that any decision context of any D ∈ Dnot feasible.of E (cid:11)(cid:11), and let M(cid:11)(cid:11)of M that is equivalent to E (cid:11), . First, note that any node passed through on the path ρR(cid:11) from must correspond to a variable in Z . Let z be any setting of Z that is consistent with with z. Observe that by be the resulting s-subgame that is obtained by combining V, and is, by definition, are precisely those corresponding to the information sets in E (cid:11)that does not correspond to some information set in E (cid:11)(cid:2) ⊆ V(cid:2)(cid:2)(cid:2)Thus, the natural mapping f between E and M, when restricted to E (cid:11)and the quotient set of behavioural policy profiles in M(cid:11), leads to a bijection between the strategies in E (cid:11)by ∼ (where two policies are in the same equiv-alence class if and only if they differ only on those decision contexts that are infeasible), as per the arguments in the proof of Lemma 4. Furthermore, for any equivalent strategy π ∈ f (σ ), it follows directly by construction using maid2efgor efg2maid that if E(cid:11)(cid:11)] denote the expected utility of agent i when σ and π are U (ρ[L])[i]and σand M(cid:11)restricted to E (cid:11)respectively, then:(cid:9)(cid:11)])[i](cid:11)σ(cid:11)σ (ρ)U (ρ[L])[i]and M(cid:11)U (ρ[LU (cid:11)∈U i ∩V(cid:11) E(cid:11)π(cid:11)[U:=(cid:7)EP(cid:8)(cid:8)(cid:9)====ρ(cid:11)P σ (ρ | ρR(cid:11) )U (ρ[L])[i]ρ(cid:11)(cid:11)u · Prπ (u | z; θ)U ∈U i(cid:11)u∈dom(U )u +(cid:11)(cid:11)U ∈U i ∩Z(cid:11)u +U (cid:11)∈U i ∩V(cid:11)(cid:11)U ∈U i ∩ZU (cid:11)∈U i ∩V(cid:11)uu(cid:11)∈dom(U (cid:11))(cid:11)uu(cid:11)∈dom(U (cid:11))(cid:11) · Prπ (u(cid:11) | z; θ)(cid:11) · Prπ (u(cid:11); θ(cid:11)) =:(cid:11)u +(cid:11)U ∈U i ∩ZU (cid:11)∈U i ∩V(cid:11)E(cid:11)π(cid:11)][U(cid:11), where P σ (ρ | ρR(cid:11) ) is the distribution over paths ρ in E conditional on ρ containing ρR(cid:11) as a sub-for every agent i ∈ Npath. Note that because some utility variables U ∈ U i may not occur in Vthen we must add their value u according to zto the payoff for each agent in order to equate the expected utilities for each agent in both games. Given z, however, this difference between the utilities for each agent is constant under any policy in M(cid:11), and so has no effect on the optimality of policies in M(cid:11), as required. (cid:2). Modulo this small difference, f forms a natural mapping between E (cid:11)and M(cid:11)(cid:11)Definition 32. A strategy profile σ in an EFG E is a subgame perfect equilibrium (SPE) if σ is an NE in every subgame of E , when restricted to that subgame.41L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Corollary 2. If E ∈ maid2efg(M) or M = efg2maid(E), then there is a natural mapping f between E and M such that if every π ∈ f (σ ) is an SPE in M, then σ is an SPE in E .Proof. The corollary can be seen to follow immediately from combining the definitions of SPEs in EFGs and MAIDs with Proposition 10 and Lemma 3. Concretely, if E ∈ maid2efg(M) or M = efg2maid(E), and if any π ∈ f (σ ) is an SPE in M (for some natural mapping f between E and M satisfying Proposition 10) then π is an NE in any s-subgame of M, and as every subgame of E is equivalent to an s-subgame of M under f (modulo a constant difference between the utilities for each agent under any policy in M(cid:11)is an NE or not), then we must have that σ is an NE in each subgame of E (by Lemma 3). (cid:2), which does not affect whether a given policy profile in M(cid:11)Definition 33. A perturbation vector ηk contains perturbations (cid:12)i, jsuch that in a perturbed game E(ηk), each strategy is forced to have σ iperfect equilibrium (THPE) in an EFG E if there is a sequence of perturbation vectors {ηk}and for each perturbed EFG E(ηk) there is an NE σk such that limk→∞ σk = σ .∈ (0, 1) with j (a) ≥ (cid:12)i, j≤ 1 for every information set I ija . A strategy profile σ is a trembling hand k∈N such that limk→∞ (cid:22)ηk(cid:22)∞ = 0(cid:12)i, jaa∈ Aija(cid:7)Proposition 11. If E ∈ maid2efg(M) or M = efg2maid(E), then there is a natural mapping f between E and M such that σis a THPE in E if and only if every π ∈ f (σ ) is a THPE in M.Proof. Given Lemma 4 and Lemma 3, it suffices to show that the perturbed MAID resulting from efg2maid(E(ηk)) is equivalent to E(ηk), and similarly that any perturbed EFG resulting from maid2efg(M(ζk)) is equivalent to M(ζk). For the first part, given ηk we define the entries of ζk for each decision d and decision context paD as:(cid:3)(cid:12)paDd:=(cid:12)i, jamini, j,a (cid:12)i, jaif d = a and paD is feasibleotherwise,j as per our efg2maid construction. Clearly for any π ∈ f (σ ) where f is a where paD corresponds to the information set I iif and only if πD (d | paD ) ≥ (cid:12)paDnatural mapping between E and M then σ i. Based on this, it can easily be seen is also a natural mapping between the perturbed games E(ηk) and M(ζk), and hence they are equivalent. that any such fFor the second part, given ζk we construct ηk such that (cid:12)i, jif a = d and paD is feasible. A similar argument to the above shows that any natural mapping f between M and E induces a natural mapping between the perturbed games M(ζk) and E(ηk), as required. (cid:2)j (a) ≥ (cid:12)i, j:= (cid:12)paDddaaAppendix B. Further examplesB.1. Counterfactuals using the closest possible world principleWhen computing counterfactuals in SCGs under the ‘closest possible world’ principle, our basic desideratum is to con-sider those counterfactual rational outcomes π (cid:11) ∈ R(mMI ) that are consistent with the decision rules used in some actual rational outcome π ∈ R(mM | z) whenever those decision rules are not affected by I. In other words, we wish to keep the set of invariant decision rules (cid:2)(I) as large as possible while propagating changes due to I.As a first attempt, we might set (cid:2)(I) to (cid:2) \ (Y ∪ DescY ). It can easily be seen, however, that this choice is flawed. = {(cid:7)D }, to which we apply an intervention do((cid:7)D =ˆπD ∈r D () r D(cid:11) ( ˆπD ) – i.e., the set of rational responses for decision rule (cid:7)(cid:11)∈ {(cid:7)D } ∪ Desc(cid:7)D .Consider an R-relevance graph with two variables (cid:7)D , (cid:7)D(cid:11) , with Pa(cid:7)(cid:11)πD ). Then it is perfectly possible that r D(cid:11) (πD ) =does not change – and in which case there is no sense in which (cid:7)(cid:11)D is affected by I, even though (cid:7)(cid:11)Instead, we can compute (cid:2)(I) by propagating the effects of I through the maximal strongly connected components(MSCCs) of the R-relevance graph when restricted to decision rule variables.26 Let cRG denote the condensation of the R-relevance graph rRG when restricted to the decision rule variables – called the ‘component graph’ by K&M [51] – with topological ordering C1 ≺ · · · ≺ Cm over the vertices in cRG (where each C j ⊆ (cid:2) is an MSCC of rRG restricted to the decision rule variables), and let us write:(cid:12)c j : πD ∈ r D (pa(cid:7)D ) ∀ (cid:7)D ∈ C jrC j (paC j) :=(cid:10)(cid:13),DDDto denote the rational responses c j = (πD )(cid:7)D ∈C j . Then we may compute (cid:2)(I) using Algorithm 1.Algorithm 1 functions by incrementally expanding the set (cid:2)(I) – which is at first initialised to (cid:2) \ (Y ∪ DescY ) – by computing the rational responses for each MSCC to both the actual and counterfactual rational outcomes, adding the 26 Recall that a strongly connected component (SCC) is a subgraph containing a directed path between every pair of nodes and a maximal SCC is an SCC that is not a strict subset of any other SCC.42L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Algorithm 1Input: mM, z, IOutput: (cid:2)(I)1: Pr(θ ) ← Pr(θ I )2: (cid:2)(I) ← (cid:2) \ (Y ∪ DescY )3: form cRG from rRG with topological ordering C1 ≺ · · · ≺ Cm4: for j = 1, . . . , m do5:6:7:8:9:10:11:12: return (cid:2)(I)actual ← R(mM | z)counterfactual ←(cid:3) j ← PaC j(cid:2) j ← PaC jifif C j ⊆ (cid:2)(I) then continueelseπ ∈actual rC j (θ j , π j ) =\ (cid:2)∩ (cid:2)π ∈actual(cid:10)(cid:10)(cid:10)(cid:12)π (cid:11)∈counterfactual rC j (θ j , π (cid:11)(cid:13)π (cid:11) ∈ R(mMI ) : π (I) = π (cid:11)(I)j ) then (cid:2)(I) ← (cid:2)(I) ∪ C jABU 1D 2(a)U 2B 1aAI 12¬aB 2b¬bb¬bD 21D 22I 2D 23D 24(cid:7) A(cid:7)Bd¬dd¬dd¬dd¬d(cid:6)U 1(cid:6)U 2(cid:7)D2(b)(1, −1)(−1, 1)(−2, 2)(−2, 2)(−2, 2)(−2, 2)(−1, 1)(1, −1)(c)Fig. 11. (a) A MAID representing the NE non-existence example and (b) its s-relevance graph. (c) An equivalent EFG representing the same game.variables in the component to (cid:2)(I) if and only if the set of such responses remains the same. It can therefore be seen as generalising interventions to models containing both cycles and non-determinism, while maintaining a maximally justifiable similarity to an observed outcome. In other words, we maintain our updated beliefs (due to observation z) about the values of a decision rule variable (cid:7)D if and only if the set of all values πD takes in any rational outcome π remains the same after the intervention I is performed, i.e., if and only if (cid:7)D ∈ (cid:2)(I). We may then use this set (cid:2)(I) in Definition 23 to define counterfactual queries in games under this principle.B.2. Non-existence resultsB.2.1. No Nash equilibrium in behavioural policiesProposition 2 in Section 5.1 says that in a MAID with sufficient recall, there must exist an NE in behavioural policies. The example presented here, adapted from [97], demonstrates that if even one agent in a MAID does not have sufficient recall, then although there will be an NE in mixed policies, the same may not hold true for behavioural policies.The example is shown as both a MAID M and an EFG E in Fig. 11, in which agent 1 has two binary decision variables ( A and B), agent 2 has one binary decision variable (D), and the utilities are assigned according to the leaves in E . For the sake of simplicity, we identify each pure policy with the decisions chosen, i.e., dom( ˙(cid:2)1) = {¬a¬b, ¬ab, a¬b, ab} and dom( ˙(cid:2)2) = {¬d, d}. Note that agent 1 has insufficient recall as the s-relevance graph restricted to contain just (cid:7) A and (cid:7)Bis not acyclic.In order to show that M has no NE in behavioural policies, we first show that there exists no NE where agent 1 uses a pure policy. Suppose, by contradiction, that there exists some policy profile ˙π = ( ˙π 1, ˙π 2) where ˙π 1 is pure and ˙π is an NE. First, ˙π 1 cannot be ¬ab or a¬b because under any choice of ˙π 2 agent 1 receives expected utility −2 and so could improve their expected utility by playing ¬a¬b or ab. Second, if ˙π 1 = ¬a¬b, then agent 2’s best response is ˙π 2 = d, and if ˙π 1 = ab, then agent 2’s best response is ˙π 2 = ¬d. However, agent 1’s best response to ˙π 2 = d is ˙π 1 = ab and agent 1’s best response to ˙π 2 = ¬d is ˙π 1 = ¬a¬b. Therefore, there exists no choice of pure policy ˙π 1 for any ˙π 2 ∈ ˙(cid:2)2 such that both agents are simultaneously playing a best response. Hence, no NE in pure policies exists in M.43L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919U 3XABD 4ABD 3U 1D 2U 2U 4U 1(a)U 2D 2(b)D 3U 3(c)(cid:6)U 3(cid:6)X(cid:7) A(cid:7)B(cid:7)D4XABD 4(cid:7)D3(cid:6)U 1(cid:7)D2(cid:6)U 2(cid:6)U 4U 1D 2U 2U 4(d)(e)Fig. 12. (a) A MAID M representing the SPE non-existence example. (b), (c), and (e) show the three proper s-subdiagrams of this MAID, G1, G2, and G3respectively. (d) The s-relevance graph of M.We now consider the case where agent 1 is using a behavioural policy in which π 1B is stochastic. Suppose, by contradiction, that there exists some policy profile π = (π 1, π 2) that is an NE in behavioural policies. We let agent 1’s decision rules be parameterised by p, q ∈ [0, 1] such that π 1B (b) = q. First, consider the case where agent 2 plays d or ¬d with probability 1, then agent 1’s best response is to play the pure policy ˙π 1 = ab or ˙π 1 = ¬a¬b with probability 1 respectively. Since we know that no NE in pure policies exists, agent 2 must instead select a stochastic decision D2 and so agent 2 must be indifferent between d and ¬d. We thus obtain two constraints on p and q. On the one rule π 2hand, agent 1’s behavioural policy π 1 must result in π 1(¬a, ¬b) = π 1(a, b), and hence we have:A(a) = p and π 1A and/or π 1(1 − p)(1 − q) = pq ⇒ p + q = 1.On the other hand, agent 1 receives utility −2 if its policy π 1 selects ¬a and b or a and ¬b (whatever the choice of π 2). Therefore, we must have that π 1(¬a, b) + π 1(a, ¬b) < π 1(a, b) + π 1(¬a, ¬b) and thus (by substituting in the result that p + q = 1):(1 − p)q + p(1 − q) < pq + (1 − p)(1 − q) ⇒ (2p − 1)2 < 0.This contradiction implies that M has no NE in behavioural policies. However, as expected by Nash’s theorem [68], there does exist an NE of M in mixed policies (i.e., where both agents randomise over their pure policies). To find this mixed policy NE, we know from the principle of indifference that the expected utility from all of an agent’s pure policies in the support of their mixed policy must be the same. This means that we can immediately rule out the pure policies ¬ab and a¬b being played with any positive probability. Assuming that agent 1 plays mixed policy μ1r , which we define as ab with probability r and ¬a¬b with probability 1 − r, then E(μ12 . Furthermore, s , which we define as d with probability s and ¬d with probability 1 − s, then assuming that agent 2 plays mixed policy μ22 . Thus, the policy profile μ = (μ1E(ab,μ2s )) is an NE of M in mixed policies.[U 1] = E(¬a¬b,μ2s )[U 2] implies r = 1[U 1] implies s = 1[U 2] = E(μ1r ,¬d), μ2r ,d)1212B.2.2. No subgame perfect equilibriumWe now extend the MAID from the previous example (in Fig. 11a) to construct a MAID that has no SPE. Proposition 8 in Section 5.3 says that in a MAID with sufficient recall, there must exist an SPE in behavioural policies. The example presented here demonstrates that if even one agent in a MAID does not have sufficient recall, then there may not exist an SPE, even if we allow mixed policies.Fig. 12a shows the graph G of the MAID M for our example and Fig. 12d shows its s-relevance graph (where we can observe that agent 1 has insufficient recall). The proper s-subdiagrams – G1, G2, and G3 – of G are shown in Figs. 12b, 12c, and 12e respectively. To parameterise M, all non-utility variables are given a Boolean domain (which we interpret as integers, e.g., a = 1 and ¬a = 0) and U 1 and U 2 inherit the same parameterisation as the previous example (shown for reference via the EFG in Fig. 11c). We let U 3 = D3 and let U 4 = 1 if and only if D4 = B, otherwise U 4 = 0. Finally, we set the CPD of X such that X = A if D3 = 1 and X = 1 − A if D3 = 0.From the previous example we know that any s-subgame defined over G1 has no behavioural NEs, and that the only 2 , and μ2plays the pure strategies ab and ¬a¬b each with probability 1mixed NE is given by μ1 = (μ1) where μ1, μ21212121244L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919(cid:7) A(cid:7)BAXBU 1(cid:6)X(cid:6)U 1A1a1B 1x12¬a1I 11B 2X¬x12I 12a2B 3A2¬a2B 4b1¬b1b2¬b2b1¬b1b2¬b2O 1O 2O 3O 4O 5O 6O 7O 8(a)(b)Fig. 13. (a) An s-minimal mechanised MAID representing the equivalent behavioural and mixed policies non-existence example. (b) An equivalent EFG representing the same game.plays pure strategy d2 with probability 1μ2 = μ2.122 . Thus, any mixed SPE μ = (μ1, μ2, μ3, μ4) in M must be such that μ1 = μ1and 12With μ1 and μ2 fixed, we now show that there is no choice of μ4 that leads to an NE in every feasible subgame defined over G3. First, note that there are effectively only two such subgames, M3 and M(cid:11)3 (defined by setting D3 = d3and D3 = ¬d3 respectively), both of which are clearly feasible. In M3 the only policy μ4 that forms an NE with μ1 and μ2 is one that assigns all probability mass to the pure policy that plays d4 if and only if X = 1. However, in M(cid:11)3, the only best response for agent 4 is to play ¬d4 if and only if X = 1. As such, there is no policy profile that forms an NE in everys-subgame of M, and hence no SPE in M.The reason for this result is that the use of a mixed policy by agent 1 means that variables A and B become correlated. This can be modelled graphically by the introduction of a shared parent C of A and B. Given this correlation, (cid:7)D3 is now s-relevant to (cid:7)D4 due to the new path:(cid:7)D3 → D3 → X ← A ← C → B → U 4,in the independent mechanised graph, which is active given FaD4 = { X, D4}.B.2.3. No equivalent mixed and behavioural policiesFinally, we provide an example to demonstrate why sufficient recall is not a sufficient condition for every mixed policy to have an equivalent behavioural policy that results in the same probability distribution over game outcomes. It therefore serves as a proof for the second part of Proposition 7.Fig. 13a shows the mechanised MAID for this example, M, in which the single agent has sufficient, but imperfect recall. We assume that all variables are binary, and further that Pr(x) = 12 . To aid our reasoning, Fig. 13b shows the corresponding EFG for M. The outcomes of the game (equivalent to a setting of all the variables in M) are denoted by {O 1, . . . , O 8}, e.g., O 3 represents (x, ¬a, b, u13). We use the same notation as the previous example to denote a pure policy. For example, ˙π 1 = a1a2b1¬b2 is the pure policy where the agent selects a whatever the value of X , and b if a, and ¬b if ¬a. Because there are two decision contexts for A and B respectively (corresponding to the four information sets in the EFG), there are 24 pure policies. A mixed policy is then a distribution over these pure policies.We will now show that there is no behavioural policy equivalent to the mixed policy μi = [ 12 (a1¬a2b1¬b2),12 (a1a2¬b1¬b2)]. Let use begin by parameterising a general behavioural policy as:π A(a | x) = pπ A(a | ¬x) = qπB (b | a) = rπB (b | ¬a) = swhere p, q, r, s ∈ [0, 1]. Suppose, for a contradiction, that a behavioural policy equivalent to μi exists. This behavioural policy must then induce the same probability distribution over the outcomes of the game {O 1, . . . O 8} as μi , and so the following equalities must hold:45L. Hammond, J. Fox, T. Everitt et al.(O 1) 12(O 2) 12(O 3) 12(O 4) 12· p · r = 14· p · (1 − r) = 14· (1 − p) · s = 0· (1 − p) · (1 − s) = 0Artificial Intelligence 320 (2023) 103919(O 5) 12(O 5) 12(O 7) 12(O 8) 12· q · r = 0· q · (1 − r) = 14· (1 − q) · s = 0· (1 − q) · (1 − s) = 14From (O 5), we must have q = 0 or r = 0, but q = 0 is ruled out by (O 6). Taking r = 0 implies that q = 1so s = 0 using (O 7). If s = 0, then p = 1 using (O 4); however, this means that r = 1forced to set r = 0. This contradiction implies that there is no behavioural policy equivalent to μi in M.2 using (O 6), and 2 from (O 1) or (O 2) and yet we were B.3. Reasoning about existing concepts using causal gamesB.3.1. BlameThe following definition is adapted from [39], and formalises the extent to which an agent is blameworthy for causing an event.Definition 34 ([39]). Let M be an SCG, π a policy profile, D ∈ D i a decision variable with d, dcombination of terms V = v. The degree of blameworthiness of d for ϕ relative to das:(cid:11)(cid:11) ∈ dom(D), and ϕ a Boolean (cid:11), ϕ), and is defined is denoted db S (d, ddb S (d, d(cid:11), ϕ) := δd,d(cid:11),ϕ ·S − max(cid:11)) − ci(d), 0(cid:6),(cid:5)ci(dSwhere:• δd,d(cid:11),ϕ := max(cid:11)decision d;(cid:7)U ∈U i Eπ [U ] −(cid:6)(cid:5)0, Prπ (ϕd) − Prπ (ϕd(cid:11) )(cid:7)• ci(d) :=• S > maxd∈dom(D) ci(d) is a given measure of cost-sensitivity.U ∈U i Eπ [U d] captures the cost to agent i of performing d;captures how much more likely it is that ϕ will result from decision d than from The overall degree of blameworthiness of d for ϕ is db S (d, ϕ) := maxd(cid:11)∈dom(D) db S (d, d(cid:11), ϕ).The factor δd,d(cid:11),ϕ captures the extent to which the agent’s decision causes the likelihood of ϕ to increase. The max(cid:11), ϕ) captures the operation ensures that no blame is possible if the likelihood decreases. The second factor of db S (d, dcosts of actions – the idea being that an agent is less liable to be blamed for ϕ if taking an alternative action would have been particularly costly. The extent to which these costs are taken into account is determined by S, where note that limS→∞ db S (d, d(cid:11), ϕ) = δd,d(cid:11),ϕ .In Example 2, for instance, one might wish to compute db S (q, ¬q, B = b) – the degree of blame we should assign to robot one for moving quickly (as opposed to not moving quickly) with respect to breaking an item. Let us assume the policy profile in question is π THPE as described in Section 5.3 (in which robot one moves quickly and robot two patrols if and only if it observes robot one moving quickly). Then, we have the following:db S (q, ¬q, B = b) = δq,¬q,B=b ·S − max(cid:5)c1(¬q) − c1(q), 0(cid:6)= max(cid:5)(cid:6)0, Prπ (bq) − Prπ (b¬q)·S(cid:5)S − max0, Eπ [U 1q] − Eπ [U 1¬q(cid:6)]S= max(0,13− 0) · S − max(0, 2 − 2)S= 13.Before continuing, we note that definitions (and the definitions of intent below) were originally formalised in SCMs with a single ‘action’ variable A and a single utility function U : dom(V ) → R, resulting in some small differences. Firstly, in an SCM, every variable must have a default value and actions are viewed as interventions on said variables, whereas, in SCGs, the values of the endogenous variables are typically undefined before actions are chosen. This is because SCMs model sequences of events and their causal connections in a broad sense, whereas SCGs represent a game-theoretic model to be analysed. These two framings can easily be reconciled if one posits a default value for every decision variable in an SCG, such as ‘do nothing’. Relatedly, the use of utility variables in SCGs strictly generalises U , and can be used to describe a more fine-grained structure. Finally, previous work views SCMs as representing a subjective, epistemic state possessed by the 46L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919intervening agent.27 In contrast, SCGs represent a more objective view of a sequential strategic interaction that is assumed to be common knowledge among all agents in the game. This perspective may be more appropriate when, say, assessing blame and intention from the point of view of an arbiter, system designer, or other third party.B.3.2. IntentIn the same work as above, the authors also formalise the question of whether or not an agent intends to bring about an event, which we adapt below for use with SCGs.Definition 35 ([39]). Let M be an SCG, π a policy profile, D ∈ D i a decision variable with d ∈ dom(D) and alt(d) ⊆ dom(D)be a set of alternative decisions under consideration. An agent intends to bring about Y = y by performing d if and only if:• There exists some Z ⊇ Y such that this inequality;(cid:7)U ∈U i Eπ [U d] ≤ maxd(cid:11)∈alt(d)(cid:7)U ∈U i Eπ [U d(cid:11),zd], and Z is minimal with respect to • Prπ ( yd) > 0, where recall that Prπ ( yd) denotes Prπ (Y d = y);• For all y(cid:11) ∈ dom(Y ) such that Prπ ( y(cid:11)d) > 0 we have (cid:7)U ∈U i Eπ [U y(cid:11) ] ≤(cid:7)U ∈U i Eπ [U y].Here zd is used in a nested counterfactual to denote the value that Z would take under the intervention do(D = d) (evaluated with respect to some marginalised setting e of the exogenous variables).The notion of intent here addresses the problem of differentiating desirable, intended effects from undesirable, unin-tended effects. The first condition says that if the variables Z were set to the values that they would take under d, then (cid:11) ∈ alt(d) becomes at least as good, and further that Z is the minimal set of variables that the agent another decision dis intending to affect in this way. The second condition says that it is possible to bring about y by doing d (where it is assumed that the agent knows this). Finally, the third condition says that y is an optimal value of Y – among those possible under the intervention do(D = d) – for agent i.For instance, we can see that robot two does not intend to obstruct robot one (meaning that it receives utility U 1 = 0) by patrolling. This is because the first property in Definition 35 fails to hold. Taking alt(p) = {¬p}, then the minimal set of variables Z such that:c2(p) ≤ maxp(cid:11)∈alt(p)(cid:11)U ∈U 1Eπ [U p(cid:11),z p],is {U 2} /(cid:24) U 1. In other words, given that robot one is moving quickly, {U 2} is the minimal set of outcomes that robot two is trying to affect by performing p. If the values of U 2 were restricted to those that occur when D2 = p, then there would be no incentive for robot two to patrol.B.3.3. IncentivesIn what follows, structural causal influence models (SCIMs) refer to SCGs with only one agent.Definition 36 ([22]). A policy π in a single-decision SCIM M responds to a variable X if there exists x ∈ dom( X) and e ∈ dom(E) such that Prπ (D x | e) (cid:10)= Prπ (D | e). X has a response incentive (RI) if all optimal policies π ∗respond to X , and a graph G admits a RI on X if there is a SCIM over G that has a RI on X .Definition 37 ([22]). In a single-decision SCIM M, there is an instrumental control incentive (ICI) on a variable X in decision context paD if for all optimal policies π ∗, there exists some d ∈ dom(D) such that:(cid:11)(cid:11)Eπ ∗ [U xd| paD] (cid:10)=Eπ ∗ [U | paD].U ∈UU ∈UA graph G admits an ICI on X if there is a SCIM over G that has an ICI on X for some paD .These definitions can help us to understand what algorithmic frameworks lead to agents having undesirable incentives, and thus to build safer AI systems. One such proposal for building safe AI systems is cooperative inverse reinforcement learning (CIRL) [36], which seeks to formalise the alignment problem [9,83] as an assistance game in which a human H and a robot R attempt to cooperatively perform a task in an unknown environment, but where the robot is uncertain about the human’s true reward function (parameterised by the value of P H ) and so must infer it through observing the human’s 27 The philosophical motivation here is that arguably it only makes sense to judge an agent’s, e.g. intent, relative to that agent’s own beliefs. In our work we make a common prior assumption and assume that all agents are aware of the structure of the game, though relaxations of this assumption ought to support more subjective definitions of intent.47L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919S1R1A R1A H1S2R2P HS3R3A R2A H2Fig. 14. A MAID representing an assistance game [23], played by agents H (a human) and R (a robot); both want to maximise the same rewards, indicated by the shared utility variables.actions. This setup is formalised using a decentralised partially observable Markov decision process (Dec-POMDP) and is shown as a MAID in Fig. 14 for a finite horizon game of three timesteps. At each timestep t, both the human and the robot perform an action A Ht and A Rfrom their shared state St , after which they transition to a new state St+1 and receive a treward Rt+1, which is only observable by the human.1 , A RIf we assume that, in the example given by Fig. 14, actions A H2 have already been taken (and thus that the MAID reduces to a single-decision ID with decision variable A R2 ), then we may apply the sound and graphical criteria derived in previous work to detect RIs and ICIs in CIRL [22]. A variable X in a graph admits an RI (with respect to decision variable D) if and only if there is a directed path X (cid:2)(cid:2)(cid:3) D in the graph that results when all information links to D from variables Y satisfying Y (cid:10)⊥G U ∩ DescD | D, PaD are removed.28 As A H2 exists in this new graph, and hence we see that the robot R has an RI to act according to P H , as we would hope. A variable Xadmits an ICI (with respect to decision variable D) if and only if there exists a directed path D (cid:2)(cid:2)(cid:3) X (cid:2)(cid:2)(cid:3) U for some utility → S3 → R3, that the robot has an ICI to influence S3 as, again, we would variable U . Hence we see, due to the path A R2expect.2 does not satisfy this criterion, the path P H → A H1 , and A H→ A R2Appendix C. CodebaseIn this section, we briefly describe PyCID [27], our open source Python library that implements MAIDs (and their causal variants). PyCID has a range of classes, methods, and functions for handling IDs at level one and level two of the causal hierarchy. For our work in this paper, the MAID class is of primary interest; however, note that PyCID also has significant other functionality including functions for computing incentives in single-agent causal IDs [22] and reasoning patterns in MAIDs [76]. This makes the codebase well-suited as a testbed for future research and applications.We begin by showing how to instantiate MAIDs for Examples 1 and 2 in PyCID. We then provide empirical results demonstrating how MAIDs can be used to compute NEs faster than in the equivalent EFGs. We refer the reader to an existing tool paper [27] and our online codebase for further details, including several tutorials.C.1. Creating MAIDsListings 1 and 2 show how to instantiate a MAID for Examples 1 and 2 as instances of PyCID’s MAID class, which inherits from pgmpy’s BayesianModel class [2]. A MAID is initialised using a list of edges as its first argument, and then dictionaries to specify each agent’s decision and utility variables. The method draw plots the graphs of these MAIDs (shown in Fig. 15a and 15b) with chance variables as grey circles, decision variables as rectangles, utility variables as diamonds, and colouring to denote different agents (each agent is assigned a unique colour). Recall that MAIDs are syntactically the same as CGs. Therefore, both can be defined as MAID objects using PyCID. In the case of MAIDs, a level one model, all class methods are permitted except those that involve causal interventions.A MAID is parameterised by assigning domains to the decision variables and CPDs to every chance and utility variable. CPDs in PyCID are StochasticFunctionCPD objects, and there are multiple ways to define them. Listing 1 shows how to instantiate a MAID for Example 1. The CPD for T follows a Bernoulli distribution with success probability 12 ; D1 and D2are decision variables for the worker and firm’s hiring system respectively with binary domains; and U 1 and U 2 are defined as described in Section 2.2.1.28 The observations available at D satisfying this criterion are known as non-requisite observations [58]. Note the similarity to s-reachability, defined in Proposition 1.48L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919[],agent_decisions={1: ["D1"],2: ["D2"],("T", "D1"),("T", "U1"),("T", "U2"),("D1", "D2"),("D1", "U1"),("D2", "U1"),("D2", "U2"),1 import pycid23 job_market = pycid.MAID(456789101112131415161718192021 )2223 job_market.draw()2425 prob = 1/22627 job_market.add_cpds(282930313233 )},agent_utilities={1: ["U1"],2: ["U2"],},T = pycid.bernoulli(prob), # T = 1 corresponds to T = h (hard-working)D1 = [0,1], # D1 = 1 corresponds to D1 = g (going to university)D2 = [0,1], # D2 = 1 corresponds to D2 = j (offering a job)U1 = lambda d1, d2, t: 5*d2 - t*d1 - 2*d1*(1-t),U2 = lambda d2, t: 3*t*d2 - 2*(1-t)*d2 - (1-d2)*t,Listing 1. An instantiation of Example 1 in PyCID.[],agent_decisions={1: ["D1"],2: ["D2"],("D1", "D2"),("D1", "U1"),("D1", "B"),("B", "U2"),("B", "U1"),("D2", "U2"),("D2", "U1"),1 warehouse_robots = pycid.MAID(2345678910111213141516171819 )2021 warehouse_robots.draw()2223 warehouse_robots.add_cpds(242526272829 )},agent_utilities={1: ["U1"],2: ["U2"],},B = lambda d1 : {0: 1 if d1==0 else 2/3, 1: None}, # B = 1 corresponds to B = b (breaking something)D1 = [0,1], # D1 = 1 corresponds to D1 = q (moving quickly)D2 = [0,1], # D2 = 1 corresponds to D2 = p (patrolling)U1 = lambda d1, d2, b: (1 - 0.5*d2)*((1-d1)*2 + d1*5 - 3*b),U2 = lambda d2, b: 6*(1 - (1-d2)*b) - d2,Listing 2. An instantiation of Example 2 in PyCID.49L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919Fig. 15. MAIDs for (a) Example 1 and (b) Example 2 drawn in PyCID.d1D 21d2¬d2D 31D 32D 1I 2I 3¬d1D 22d2¬d2D 33D 34d3¬d3d3¬d3d3¬d3d3¬d3(1, 1, −1)(0, −1, 1)(0, −1, 1)(0, 1, −1)(0, 1, −1)(0, −1, 1)(0, −1, 1)(0, 1, −1)(a)MAIDEFG104103102101100−110−210)s(nekaTemiT13579Number of Agents(c)D 1U 1D 2D 3U 2U 3(b)Fig. 16. The (a) MAID and (b) EFG for the 3-agent version of the matching-pennies-like game described above. (c) A plot of the time taken to find an NE in the MAID and EFG representations of this game for varying numbers of agents.Listing 2 shows how to instantiate a MAID for Example 2. D1 and D2 are decision variables for the two robots with binary domains; U 1 and U 2 are defined as described in Section 5; and B is a chance variable defined as a function of its parent D1 – it takes value ¬b with probability 1 if D1 = ¬q and probability 23 if D1 = q.50L. Hammond, J. Fox, T. Everitt et al.C.2. Computing equilibriaArtificial Intelligence 320 (2023) 103919PyCID finds all pure NEs and SPEs in a MAID natively, and finds all behavioural NEs and SPEs in two agent games by converting the MAID into a normal form game and interfacing with Nashpy.29 We refer the interested reader to our online codebase for up-to-date syntax showing how to compute NEs. All SPEs in a MAID are found by adapting Algorithm 6.2 from [51] to iterate backwards through a topological ordering of the MAID’s s-subdiagrams, finding NEs in every s-subgame in turn. More specifically, the method follows the procedure outlined in the proof of Proposition 8. In contrast to K&M’s algorithm, our implementation finds all (pure) SPEs, as opposed to just one.Finally, we demonstrate the computational usefulness of subgames (and SPEs) in MAIDs. Consider a class of games with agents simultaneously choosing whether to place a coin heads or tails face up. The first agent gets utility 1 if and only if all agents in the game choose heads, otherwise their utility equals zero. All other agents are added to the game in pairs, and receive utility according a standard game of matching pennies with their partner: 1 (or -1) utility for matching (heads or tails), and -1 (or 1) for mismatching, respectively. Matching pennies has no pure NEs, only a mixed NE in which both agents randomise equally between choosing heads or tails. A MAID for the 3-agent variant of this game is shown in Fig. 16b.By the above construction, in any game belonging to this class with more than one agent, there must be no pure NE, as one agent in each pair would always have an incentive to deviate. Moreover, the EFG representation of the game (shown for the 3-agent case in Fig. 16a) has no proper subgames.30 Since the complexity of finding even an approximate mixed NE is PPAD-complete [15], finding an NE in this game is hard for any EFG solver, and soon becomes intractable as the number of agent pairs playing matching pennies increases.In the MAID representation, however, every matching-pennies-playing pair of agents is identified as a proper s-subgame. This means that the unique behavioural SPE (agents randomising equally between choosing heads or tails) can be found in these subgames (with fewer agents) before returning to the full game where, finally, the best response of agent one will be to always play heads. Fig. 16c compares the time taken to find an NE in the MAID and EFG representations of games in this game class with between one and nine agents; for the nine-agent game, the difference is three orders of magnitude.31There are games in which neither a MAID nor a corresponding EFG has any proper subgames. In these games, the time taken to compute an NE will be comparable. Nevertheless, many games will have more subgames in the MAID than the EFG (as explained in Section 6.2) and every subgame in an EFG is guaranteed to be a subgame in the corresponding MAID (as proven in Corollary 2). Thus, subgames may often allow for the more efficient computation of equilibria in MAIDs (and hence causal games) compared to EFGs.References[1] R. Ahsan, D. Arbour, E. Zheleva, Relational causal models with cycles: representation and reasoning, in: First Conference on Causal Learning and Reasoning, Proceedings of Machine Learning Research, vol. 177, 2022, pp. 1–18.[2] A. Ankan, A. Panda, Pgmpy: probabilistic graphical models using Python, in: Proceedings of the 14th Python in Science Conference, 2015, pp. 6–11.[3] D. Antos, A. Pfeffer, Identifying reasoning patterns in games, in: Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence, [4] C. Ashurst, R. Carey, S. Chiappa, T. Everitt, Why fair labels can yield unfair predictions: graphical conditions for introduced unfairness, in: Proceedings of the Thirty-Sixth AAAI Conference on Artificial Intelligence, 2022, pp. 9494–9503.[5] C. Avin, I. Shpitser, J. Pearl, Identifiability of path-specific effects, in: Proceedings of the 19th International Joint Conference on Artificial Intelligence, [6] A. Balke, J. Pearl, Probabilistic evaluation of counterfactual queries, in: Proceedings of the Twelfth AAAI National Conference on Artificial Intelligence, 2008, pp. 9–18.2005, pp. 357–363.1994, pp. 230–237.[7] H. Bauer, Probability Theory, vol. 23, Translated by: Robert B. Burckel, De Gruyter, 1996.[8] S. Bongers, P. Forré, J. Peters, J.M. Mooij, Foundations of structural causal models with cycles and latent variables, Ann. Stat. 49 (5) (2021) 2885–2915.[9] N. Bostrom, Superintelligence: Paths, Dangers, Strategies, Oxford University Press, 2014.[10] C. Boutilier, N. Friedman, M. Goldszmidt, D. Koller, Context-specific independence in Bayesian networks, in: Proceedings of the Twelfth International Conference on Uncertainty in Artificial Intelligence, 1996, pp. 115–123.[11] J. Bradfield, J. Gutierrez, M. Wooldridge, Partial-order Boolean games: informational independence in a logic-based model of strategic interaction, Synthese 193.3 (2015) 781–811.(2010) 677–693.[12] K. Chatterjee, T.A. Henzinger, N. Piterman, Strategy logic, in: Special Issue: 18th International Conference on Concurrency Theory, Inf. Comput. 208 (6) [13] J. Correa, E. Bareinboim, A calculus for stochastic interventions: causal effect identification and surrogate experiments, in: Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020, pp. 10093–10100.[14] F.G. Cozman, Credal networks, Artif. Intell. 120 (2) (2000) 199–233.[15] C. Daskalakis, P.W. Goldberg, C.H. Papadimitriou, The complexity of computing a Nash equilibrium, SIAM J. Comput. 39 (1) (2009) 195–259.[16] C. Daskalakis, A. Mehta, C. Papadimitriou, A Note on Approximate Nash Equilibria, International Workshop on Internet and Network Economics, [17] A.P. Dawid, Causal inference without counterfactuals, J. Am. Stat. Assoc. 95 (450) (2000) 407–424.[18] A.P. Dawid, Influence diagrams for causal modelling and inference, Int. Stat. Rev. 70 (2) (2002) 161–189.[19] G. Déletang, J. Grau-Moya, M. Martic, T. Genewein, T. McGrath, V. Mikulik, M. Kunesch, S. Legg, P.A. Ortega, Causal analysis of agent behavior for AI Springer, 2006, pp. 297–306.safety, arXiv:2103 .03938, 2021.29 Available at https://github.com/drvinceknight/nashpy.30 Technically there are six possible corresponding EFGs because of the six permutations of D1, D 2, and D 3, but none of them have any proper subgames.31 These calculations were performed using PyCID [27] and Gambit [63] on an NVIDIA Tesla K80 GPU and the time taken is the mean of seven runs.51L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919[20] P. Doshi, Y. Zeng, Q. Chen, Graphical models for interactive POMDPs: representations and solutions, Auton. Agents Multi-Agent Syst. 18 (3) (2008) 376–416.[21] F. Eberhardt, R. Scheines, Interventions and causal inference, Philos. Sci. 74 (5) (2007) 981–995.[22] T. Everitt, R. Carey, E.D. Langlois, P.A. Ortega, S. Legg, Agent incentives: a causal perspective, in: Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021, pp. 11487–11495.[23] T. Everitt, R. Kumar, V. Krakovna, S. Legg, Modeling AGI Safety Frameworks with Causal Influence Diagrams, IJCAI AI Safety Workshop, 2019.[24] E. Fehr, K.M. Schmidt, A theory of fairness, competition, and cooperation, Q. J. Econ. 114 (3) (1999) 817–868.[25] Financial Conduct Authority, General Insurance Pricing Practices Market Study: Consultation on Handbook Changes, 2020, CP20/19.[26] Financial Conduct Authority, General Insurance Pricing Practices: Interim Report, 2019, MS18/1.2.[27] J. Fox, T. Everitt, R. Carey, E. Langlois, A. Abate, M. Wooldridge, PyCID: a Python library for causal influence diagrams, in: Proceedings of the 20th Python in Science Conference, 2021, pp. 43–51.[28] M. Friedenberg, J.Y. Halpern, Blameworthiness in multi-agent settings, in: Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, 2019, pp. 525–532.(2008) 109–147.2020.Learning, The MIT Press, 2007.[29] D. Fudenberg, J. Tirole, Perfect Bayesian equilibrium and sequential equilibrium, J. Econ. Theory 53 (2) (1991) 236–260.[30] Y. Gal, A. Pfeffer, Networks of influence diagrams: a formalism for representing agents’ beliefs and decision-making processes, J. Artif. Intell. Res. 33 [31] D. Geiger, T. Verma, J. Pearl, Identifying independence in Bayesian networks, Networks 20 (5) (1990) 507–534.[32] T. Genewein, T. McGrath, G. Déletang, V. Mikulik, M. Martic, S. Legg, P.A. Ortega, Algorithms for causal reasoning in probability trees, arXiv:2010 .12237, [33] L. Getoor, N. Friedman, D. Koller, A. Pfeffer, B. Taskar, Probabilistic relational models, in: L. Getoor, B. Taskar (Eds.), Introduction to Statistical Relational [34] GlobalData, United Kingdom (UK) Household Insurance Market Size, Trends, Competitor Dynamics and Opportunities, Report Code: GDFS0406IA, 2022.[35] M. Gonzalez-Soto, L.E. Sucar, H.J. Escalante, Causal games and causal Nash equilibrium, arXiv:1910 .06729, 2019.[36] D. Hadfield-Menell, A. Dragan, P. Abbeel, S. Russell, Cooperative inverse reinforcement learning, in: Proceedings of the 30th International Conference on Neural Information Processing Systems, 2016, pp. 3916–3924.[37] J.Y. Halpern, A modification of the Halpern-Pearl definition of causality, in: Proceedings of the 24th International Conference on Artificial Intelligence, 2015, pp. 3022–3033.[38] J.Y. Halpern, Axiomatizing causal reasoning, in: Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, 1998, pp. 202–210.[39] J.Y. Halpern, M. Kleiman-Weiner, Towards formal definitions of blameworthiness, intention, and moral responsibility, in: Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, 2018, pp. 1853–1860.[40] J.Y. Halpern, J. Pearl, Causes and explanations: a structural-model approach. Part I: Causes, Br. J. Philos. Sci. 56 (4) (2005) 843–887.[41] J.Y. Halpern, J. Pearl, Causes and explanations: a structural-model approach. Part II: Explanations, Br. J. Philos. Sci. 56 (4) (2005) 889–911.[42] L. Hammond, J. Fox, T. Everitt, A. Abate, M. Wooldridge, Equilibrium refinements for multi-agent influence diagrams: theory and practice, in: Proceed-ings of the 20th International Conference on Autonomous Agents and Multiagent Systems, 2021, pp. 574–582.[43] D. Heckerman, R. Shachter, A decision-based view of causality, in: Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, 1994, pp. 302–310.[44] R. Howard, Information value theory, IEEE Trans. Syst. Sci. Cybern. 2 (1) (1966) 22–26.[45] R.A. Howard, J.E. Matheson, Influence diagrams, Decis. Anal. 2 (3) (2005) 127–143.[46] N. Jaques, A. Lazaridou, E. Hughes, C¸ . Gülc¸ ehre, P.A. Ortega, D. Strouse, J.Z. Leibo, N. de Freitas, Social influence as intrinsic motivation for multi-agent deep reinforcement learning, in: Proceedings of the 36th International Conference on Machine Learning, 2019, pp. 3040–3049.[47] A.X. Jiang, K. Leyton-Brown, A. Pfeffer, Temporal action-graph games: a new representation for dynamic games, in: Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, 2009, pp. 268–276.[48] M. Kearns, Graphical games, in: N. Nisan, T. Roughgarden, E. Tardos, V.V. Vazirani (Eds.), Algorithmic Game Theory, Cambridge University Press, 2007, [49] N. Kilbertus, M. Rojas-Carulla, G. Parascandolo, M. Hardt, D. Janzing, B. Schölkopf, Avoiding discrimination through causal reasoning, in: Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, pp. 656–666.[50] D. Koller, B. Milch, Multi-agent influence diagrams for representing and solving games, in: Proceedings of the 17th International Joint Conference on pp. 159–180, Chap. 7.[51] D. Koller, B. Milch, Multi-agent influence diagrams for representing and solving games, Games Econ. Behav. 45 (1) (2003) 181–221.[52] K.B. Korb, L.R. Hope, A.E. Nicholson, K. Axnick, Varieties of causal intervention, in: C. Zhang, W.H. Guesgen, W.-K. Yeap (Eds.), PRICAI 2004: Trends in [53] D.M. Kreps, R. Wilson, Sequential equilibria, Econometrica 50 (4) (1982) 863.[54] H.W. Kuhn, Extensive games and the problem of information, in: Contributions to the Theory of Games (AM-28), vol. 2, Princeton University Press, Artificial Intelligence, 2001, pp. 1027–1034.Artificial Intelligence, 2004, pp. 322–331.1953, pp. 193–216.Systems, 2017, pp. 4069–4079.[55] M. Kusner, J. Loftus, C. Russell, R. Silva, Counterfactual fairness, in: Proceedings of the 31st International Conference on Neural Information Processing [56] M. Kwiatkowska, G. Norman, D. Parker, G. Santos, Multi-player equilibria verification for concurrent stochastic games, in: Proceedings of the 17th International Conference on Quantitative Evaluation of SysTems, 2020.[57] E.D. Langlois, T. Everitt, How RL agents behave when their actions are modified, in: Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021, pp. 11586–11594.[58] S.L. Lauritzen, D. Nilsson, Representing and solving decision problems with limited information, Manag. Sci. 47 (9) (2001) 1235–1251.[59] S.L. Lauritzen, T.S. Richardson, Chain graph models and their causal interpretations, J. R. Stat. Soc., Ser. B, Stat. Methodol. 64 (3) (2002) 321–348.[60] I. Levi, The Enterprise of Knowledge: An Essay on Knowledge, Credal Probability, and Chance, MIT Press, 1980.[61] London Economics, YouGov, Kudos Research, General Insurance Pricing Practices, 2019.[62] M. Maier, K. Marazopoulou, D. Jensen, Reasoning about independence in probabilistic models of relational data, arXiv:1302 .4381, 2013.[63] R.D. McKelvey, A.M. McLennan, T.L. Turocy, Gambit: Software Tools for Game Theory, Version 0.2006. 01.20, 2016.[64] B. Milch, D. Koller, Ignorable Information in Multi-agent Scenarios, Tech. rep. MIT-CSAIL-TR-2008-029, Computer Science and Artificial Intelligence Laboratory, MIT, 2008.[65] A.C. Miller III, M.W. Merkhofer, R.A. Howard, J.E. Matheson, T.R. Rice, Development of Automated Aids for Decision Analysis, Tech. rep., 1976.[66] T. Miller, Contrastive explanation: a structural-model approach, Knowl. Eng. Rev. 36 (2021).[67] R. Nabi, I. Shpitser, Fair inference on outcomes, in: Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, 2018, pp. 1931–1940.[68] J.F. Nash, Equilibrium points in n-person games, Proc. Natl. Acad. Sci. 36 (1) (1950) 48–49.[69] M. Nielsen, G. Plotkin, G. Winskel, Petri Nets, Event Structures and Domains, Semantics of Concurrent Computation, Springer, 1979, pp. 266–284.[70] T.D. Nielsen, F.V. Jensen, Welldefined decision scenarios, in: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, 1999, pp. 502–511.52L. Hammond, J. Fox, T. Everitt et al.Artificial Intelligence 320 (2023) 103919[71] N. Nisan, T. Roughgarden, E. Tardos, V.V. Vazirani (Eds.), Algorithmic Game Theory, Cambridge University Press, 2007.[72] M.J. Osborne, A. Rubinstein, A Course in Game Theory, MIT Press, 1994.[73] J. Pearl, Causality, Cambridge University Press, 2009.[74] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann Publishers Inc., 1988.[75] J. Peters, D. Janzing, B. Schölkopf, Elements of Causal Inference: Foundations and Learning Algorithms, The MIT Press, 2017.[76] A. Pfeffer, Y. Gal, On the reasoning patterns of agents in games, in: Proceedings of the 22nd National Conference on Artificial Intelligence, 2007, pp. 102–109.and Multiagent Systems, 2007, pp. 1–3.[77] K. Polich, P. Gmytrasiewicz, Interactive dynamic influence diagrams, in: Proceedings of the 6th International Joint Conference on Autonomous Agents [78] J.-P. Ponssard, On the concept of the value of information in competitive situations, Manag. Sci. 22 (7) (1976) 739–747.[79] M. Rabin, Incorporating fairness into game theory and economics, Am. Econ. Rev. 83 (5) (1993) 1281–1302.[80] J.G. Richens, R. Beard, D.H. Thompson, Counterfactual harm, arXiv:2204 .12993, 2022.[81] S. Rideau, G. Winskel, Concurrent strategies, in: 2011 IEEE 26th Annual Symposium on Logic in Computer Science, 2011.[82] D.B. Rubin, Causal inference using potential outcomes, J. Am. Stat. Assoc. 100 (469) (2005) 322–331.[83] S. Russell, Human Compatible, Penguin LCC US, 2019.[84] R. Selten, Reexamination of the perfectness concept for equilibrium points in extensive games, Int. J. Game Theory 4 (1) (1975) 25–55.[85] R. Selten, Spieltheoretische Behandlung Eines Oligopolmodells Mit Nachfrageträgheit: Teil I: Bestimmung Des Dynamischen Preisgleichgewichts, J. Inst. Theor. Econ. H. 2 (1965) 301–324.[86] R.D. Shachter, Bayes-ball: rational pastime (for determining irrelevance and requisite information in belief networks and influence diagrams), in: Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, 1998, pp. 480–487.[87] R.D. Shachter, Evaluating influence diagrams, Oper. Res. 34 (6) (1986) 871–882.[88] Y. Shoham, K. Leyton-Brown, Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations, Cambridge University Press, 2008.[89] H.A. Simon, Models of Man. Social and Rational. Mathematical Essays on Rational Human Behavior in a Social Setting, John Wiley & Sons, 1957.[90] M. Spence, Job market signaling, Q. J. Econ. 87 (3) (1973) 355.[91] P. Spirtes, C. Glymour, R. Scheines, Causation, Prediction, and Search, Springer, 1993.[92] P. Toulis, D.C. Parkes, Long-term causal effects via behavioral game theory, Adv. Neural Inf. Process. Syst. 29 (2016) 2604–2612.[93] T. Verma, J. Pearl, Causal networks: semantics and expressiveness, in: Proceedings of the Fourth Annual Conference on Uncertainty in Artificial Intelligence, 1990, pp. 69–78.[94] J. von Neumann, Zur Theorie der Gesellschaftsspiele, Math. Ann. 100 (1) (1928) 295–320.[95] H. White, K. Chalak, Settable systems: an extension of pearl’s causal model with optimization, equilibrium, and learning, J. Mach. Learn. Res. 10 (2009) 1759–1799.[96] H. White, H. Xu, K. Chalak, Causal discourse in a game of incomplete information, J. Econom. 182 (1) (2014) 45–58.[97] P.C. Wichardt, Existence of Nash equilibria in finite extensive form games with imperfect recall: a counterexample, Games Econ. Behav. 63 (1) (2008) 366–369.[98] G. Winskel, Distributed probabilistic and quantum strategies, Electron. Notes Theor. Comput. Sci. 298 (2013) 403–425.[99] M. Wooldridge, An Introduction to Multiagent Systems, John Wiley & Sons, 2009.[100] M. Wooldridge, J. Gutierrez, P. Harrenstein, E. Marchioni, G. Perelli, A. Toumi, Rational verification: from model checking to equilibrium checking, in: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 2016, pp. 4184–4190.[101] V. Zahoransky, J. Gutierrez, P. Harrenstein, M. Wooldridge, Partial order games, Games 13 (1) (2021) 2.[102] J. Zhang, E. Bareinboim, Fairness in decision-making - the causal explanation formula, in: Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, 2018, pp. 2037–2045.53