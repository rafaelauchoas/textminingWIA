Modeling of Facial Aging and Kinship: A Survey1Markos Georgopoulos1, Yannis Panagakis1, 2, and Maja Pantic11Dept. of Computing, Imperial College London, UK2Dept. of Computer Science, Middlesex University London, UK(cid:70)8102ceD1]VC.sc[2v63640.2081:viXraAbstract—Computational facial models that capture properties of facialcues related to aging and kinship increasingly attract the attention of theresearch community, enabling the development of reliable methods forage progression, age estimation, age-invariant facial characterization,and kinship verification from visual data. In this paper, we review recentadvances in modeling of facial aging and kinship. In particular, weprovide an up-to date, complete list of available annotated datasets andan in-depth analysis of geometric, hand-crafted, and learned facial rep-resentations that are used for facial aging and kinship characterization.Moreover, evaluation protocols and metrics are reviewed and notableexperimental results for each surveyed task are analyzed. This surveyallows us to identify challenges and discuss future research directionsfor the development of robust facial models in real-world conditions.1 INTRODUCTIONHumans possess explicit, cue-based, and often culturallydetermined systems for perceiving the facial appearance oftheir peers [250]. Facial appearance is a primary source of in-formation regarding the person’s identity, gender, ethnicity,affective state, head pose, age and kinship relations. Hence,the perception of facial attributes governs person percep-tion, interpersonal attraction, and consequently prosocialand social behaviour [5], [170].Human face has been thoroughly studied from different,but complementary, perspectives across several disciplinessuch as neuroscience e.g., [70], psychology e.g., [23], [157],sociology e.g., [61], anthropometry e.g, [66], medicine e.g.,[51] and computer science. From a computational point ofview, in particular, advances in computational face mod-eling enabled the development of reliable methods forautomatic detection of faces [249], recognition of identity[259], [107], [2], gender [159], and ethnicity [72]; detectionof salient facial features [223], [58], estimation of head pose[50] and analysis of facial expressions [190], [169], [251] fromvisual data. Notably, recently proposed methods match oreven achieve better accuracy than humans in several taskse.g., [97]. This progress herald a surge of novel applica-tions in communication, entertainment, cosmetology, andbiometrics, to name a few, while facilitating basic researchin social sciences and medicine e.g., [158]. A thorough listof machine learning and computer vision methods solvingthe aforementioned face modeling and analysis tasks canbe found in the comprehensive survey papers [249], [259],[107], [159], [72], [223], [50], [190], [251].Research towards the development of more detailedcomputational facial models that capture properties of fa-cial cues related to aging and kinship increasingly attractsthe attention of the community. Indeed, by capitalizing onrecent advances in machine learning, computer vision, andthe massive collections of facial data available, significantprogress has been made towards addressing the followingproblems:i Age Progression: that is, the process of transforming afacial visual input, in order to model it across differ-ent ages. The change of the age can be bidirectional,so that the facial output can appear either youngeror older than the input.ii Age Estimation: refers to the process of labelling afacial signal with an age or age group. The input sig-nal can be 2D, 3D or image sequences. The problemsthat fall into this category can be divided furtherinto two subcategories, depending on the labels ofthe training data: (a) real age or (b) apparent ageestimation, which refers to the age that is inferredby humans based on the individual’s appearance.iii Age-Invariant Facial Characterization:involves theprocess of building a signal representation that isinvariant to the facial transformations and appear-ance changes caused by aging.iv Kinship Verification: is defined as the process of de-termining whether the individuals in a pair of facialvisual inputs are blood related.Early models for facial age progression and estimationdate back to 1994-95 [118], [43], while the problem of facerecognition across ages was first investigated in 2000 [121].More recently, since 2010, methods for kinship verificationhave emerged [65]. Since then, the development of 1) ro-bust and computationally efficient models (e.g., AAMs [40],CLMs [42] etc) and descriptors (e.g., SIFT [144], HoGs [44],LPPs [99], SURF [12], DAISY[215] etc) of facial appearance,2) effective machine learning methods such as Boosting [71]and Support Vector Machines [220] and 3) manually anno-tated facial datasets e.g., MORPH2 (2006), FG-NET (2004),FERET (1998), YGA (2008), Gallagher’s Images of Groups(2009), Ni’s Web-Collected Images (2009), have facilitatedthe deployment of reliable computational models for facialaging and kinship. Models, methods, and data for facialaging modeling which have been published before 2010   are thoroughly surveyed in [179], [73], while an overviewof research efforts for facial kinship modeling is currentlymissing.This paper aims to provide an up-to-date literature sur-vey of the work done 1) towards the development of facialaging models, complementing previous studies in [73], [179]in several ways and 2) in the emerging topic of facial kinshipmodeling. Concretely, the aims of this survey are organizedas follows:• A complete catalogue of publicly available datasetswith manual annotations for facial age and kinshipmodeling tasks is listed in Section 3. We put partic-ular emphasis on data collected in naturalistic, real-world (in the wild) conditions by providing reviewsfor 9 recently collected datasets for age modeling andall the available (i.e., 16) collections of facial imagesfor kinship verification.• A comprehensive review of recent as well as seminalmethodologies for age progression, age estimation,facial characterization, and kinshipage-invariantverification is provided in Sections 4-7. In particular,we provide an in-depth analysis of both geometric,hand-crafted and learned facial representations forthe aforementioned tasks and discuss the type ofinformation they encode as well as their advantagesand limitations. We further elaborate on methodsthat rely on deep discriminative (e.g., CNNs [125])and generative (e.g., GANs [85]) models and appearto be highly effective. Moreover, we review evalu-ation protocols and metrics, and analyze the mostnotable experimental results for each surveyed task.• The review of data, computational methods for facialaging and kinship reveal useful practices as well aschallenges that are yet to be solved. These along withdrawn conclusions are discussed in Section 9.2a lifelike image of a person. In some cases, forensics expertsface the need to change the age of a face. Such cases includeupdating archive images of wanted criminals as well asimages of lost children. Additionally, cases such as matchingorphaned or lost children and finding the kin of a victim, toname a few, demand the verification of kin relationship oftwo people. To that end, automatic genealogical research cansignificantly aid the work of law enforcement agencies.Medicine and Cosmetology: Being able to model aging andkinship and simulating the transformations on the face isvital for modern medicine and cosmetology. Medical homesystems that are used to monitor elderly people can aidmedical diagnosis by detecting premature aging. On theother hand, automatic rejuvenation of the face can serveas a guide for cosmetic surgery. Particularly in the case ofchildren, the parents’ craniofacial aging patterns can be usedto predict the child’s head growth, so that injury-relatedcosmetic surgery can have optimal long-term results.Commercial use: The ever-growing usage of social mediaand availability of personal photos have led to the rapidintegration of facial analysis by businesses. Automaticallyestimating the customers’ age can help with efficient cus-tomer profiling and age-oriented decision making, e.g., age-oriented advertisements. Likewise, targeted ads can be moreeffective when taking kinship into considerations, as peo-ple’s preferences can be affected by their relatives.Entertainment: Visual effects that age or rejuvenate theactors are already being used in the film making industry.These effects are not limited to movies but are also widelyapplied to photo editing. The imminent integration of suchtools into popular design software will make for morerealistic retouche of photos. Make-up artists that specializein transforming the face can leverage the construction ofperson, age and kin specific morphable models. Guided bythose models, the artists will transform the face of the actorfor roles that demand sibling-like similarity between actors.To begin with, a number of modern applications of com-putational models for facial aging and kinship are discussedin the following section.3 DATASETS2 APPLICATIONSIn this section we present the most significant applicationsof modeling facial aging as well as kinship in biometrics,forensics, medicine, cosmetology, business and entertain-ment.Biometrics: The physical, physiological or behaviouralcues based on which a person is recognized, e.g.,iris,fingerprint, face are referred to as biometrics [108]. Ageand kinship comprise soft biometrics [109], [45], [164] asthey can be used to boost the effectiveness of recognition.Besides improving face recognition accuracy there is a needfor robustness towards aging and kinship. Passport checksdemand age-invariance in case of large age gap betweenthe passport image and the person in question. Similarly,kinship invariance can potentially boost automatic facerecognition, in particular towards distinguishing betweenkin that look alike.Forensics: Forensics include a set of scientific techniquesthat are used for crime detection. Among these techniques,forensics art demonstrates the challenging task of producingThe availability of labelled datasets is the cornerstone ofthe development of facial models for aging and kinship.In this section, we review recently released datasets whichcontain annotations for age progression, age estimation, age-invariant facial characterization, and kinship verification.Particular emphasis is put on data capturing naturalistic,real-world conditions, often referred to as in the wild [105].A complete catalogue of the available datasets for facialaging modeling is listed in Table 1 while Table 2 containsthe available facial data for kinship verification.3.1 Datasets with age labelsThe vast majority of the available datasets for facial agingmodeling contain still images and apart from the FACES,IRIP, LHI, and YGA face datasets, they are not balanced withregards to the gender and age of the subjects, as shown inTable 1. Also, while containing an abundance of differentannotated faces, many of these datasets do not containa considerable number of images of the same person atdifferent ages, which is essential for training methods forage progression and age-invariant facial characterization.TABLE 1: Datasets with age labels3Databases#data#subjectsage-rangeprecisionIn the WildYear modalityAgeDB [156]IMDB-WIKI [185]AFAD [162]IRIP [245]OUI-Adience [57]CACD [32]UvA-Nemo [49]VADANA [204]LHI Face Dataset [1]HOIP [69]FACES [55]Web Image Db [160]Images of Groups [77]YGA [88]Iranian Face [10]Brown Sisters [163]Scherbaum’s Db [192]MORPH 2 [181]WIT-BD [217]AI&R [76]FRGC [173]Lifespan Database [154]FG-NET [119],[122]PIE [200]FERET [174]Caucasian Face Db [24]56820,28416,458523,051164,432 N/AN/A2,1002,28426,5802,000163,4464001,247432,2988,0008,000300306,6001711026219,892 N/A28,2318,0003,6001643855.13426,2223450,0005761.00241,63814,12614728,2311,600616443813.6185,5001756857682681,1991471-1010-10014-40+1-700-60+N/A8-760-789-8915-6419-801-800-66+0-932-8515-628-18 or adult16-773-8522-6618-7018-930-69N/AN/A20-62exact ageexact ageexact ageexact ageage groupexact ageexact ageage groupexact ageage groupexact ageexact ageage groupexact ageexact ageexact agebothexact ageage groupexact ageexact ageage groupexact ageexact ageexact ageexact ageyesyesyesyesyesyesnoyesnononoyesyesyesyesyesnonoyesnopartiallyyesyesnopartiallyno20172016201620162014201420142011201020102010200920092008200720072007200620062006200520042004200219981995imagesimagesimagesimagesimagesimagesvideosimagesimagesimagesimagesimagesimagesimagesimagesimages3Dimagesimagesimagesimages, 3DimagesimagesimagesimagesimagesTABLE 2: Datasets with kinship labelsDatabases#data#kin pairs#relationshipsIn the Wildsame photoYear modalityKFVW [242]WVU Kinship Db [115]Families In the Wild [184]TSKinface [176]Sibling-Face [96]Group-Face [96]HQfaces [221]LQfaces [221]KinFaceW-I [149]KinFaceW-II [149]Family 101 [64]IIITD-Kinship [114]VADANA [204]UvA-Nemo [49]UBKinface [236], [194]CornellKin [65]83690411,193787N/A1061841961,0662,00014,8165442,298515600300418113418,0601,015x272039592985331,000206272699540015047114373344477744yesyesyesyesyesyesnoyesyesyesyesyesyesnoyesyesN/AN/ApartiallyyesnoyesnonopartiallyyesnoN/Ayesnonopartially2017201720162015201420142014201420142014201320122012201220112010videosimagesimagesimagesimagesimagesimagesimagesimagesimagesimagesimagesimagesvideosimagesimagesFor all aging modeling tasks, the most widely usedbenchmarks, and their respective aging databases, are FG-NET and MORPH (alboum 2). The Face and Gesture recog-nition Network database (FG-NET) contains 1,002 images of82 subjects, which are mostly Caucasians. The age of thesubjects ranges from newborns to 69 years old, while thegap between the age of the same person in different imagesranges from 0 to 54 years. This dataset was collected frompersonal photo albums and contains coordinates of faciallandmarks for each image. The MORPH dataset was re-leased in 2006 and its second album contains 55,134 imagesof 13,618 people. The subjects are mostly African while thereare 4 images of each person on average.3.1.1 Data suitable for age estimationThe databases used for the task of age estimation varygreatly in terms of sample size, number of subjects, andage-range, as indicated in Table 1. Nevertheless, the recentsuccess of deep learning-based models in computer visionhas created a need for larger datasets with age annotations.Towards this end, the IMDB-WIKI, AFAD and OUI-Adiencedatasets have been collected.OUI-Adience [57]: The OUI-Adience dataset contains26,580 facialimages from the albums in the websiteFlickr.com. These photos were made publicly availablethrough the Creative Commons license. The photos werecollected from approximately 200 albums, while the Viola-Jones [222] detector was used to detect the faces. The age-range of the dataset is 0-60+, although the ages above 48years old are less represented.UvA-NEMO [49]: The UvA-NEMO dataset consists of1,240 videos of 400 people smiling. The smiles are bothposed and spontaneous and were captured under controlledconditions. The age of the faces varies from 8 to 76 years.AFAD [162]: The Asian Face Age Dataset (AFAD) wasintroduced by Niu et al. and contains 164,432 face images.These images were collected from the RenRen Social Net-work which is widely used by Asian students. Therefore,most subjects are Asian and under 30 years old. The date ofbirth of every subject was provided by the respective useraccount.IMDB-WIKI [185]: The largest age-annotated dataset isthe IMDB-WIKI dataset. The dataset consist of 523,051 facialimages that were crawled from the Wikipedia and IMDBwebsites. All metadata were also collected from the abovementioned web-sites. The average age of the dataset is about32 years old.The age annotations to all the datasets in Table 1 referto real age, and therefore cannot be used for the taskof apparent age estimation. In order to address the lackof such datasets Escalera et al. [59] built the Chalearn-AgeGuess dataset, which is the first dataset with apparentage annotations. This dataset contains 4,691 images and wasused for the Chalearn competition. The annotations wereacquired using the AgeGuess online voting platform. Usingthe same platform, as well as Mechanical Turk workers, theAPPA-REAL [3] dataset was introduced by Agustsson et al.This dataset contains 7,591 faces with real and apparent ageannotations in more than 7,000 images. The age range isbetween 0 and 95 years.3.1.2 Datasets for Age Progression and Age-Invariant Fa-cial CharacterizationThe tasks of age progression and age-invariant facial char-acterization require data containing the same person atdifferent ages. Therefore, the collection of such databases isa challenging procedure. Besides the FG-NET and MORPHdatasets, the recently created AgeDB, VADANA and CACDdatasets are also widely used for age progression and age-invariant facial characterization tasks. A brief overview ofthese datasets is presented below.AgeDB [156]: AgeDB contains 16,488 images of 568 sub-jects, manually collected from Google Images. Avoiding tocollect the images in a semi-automatic manner ensures thatthe labels are not noisy. Each subject in the dataset has29 photos on average, while the average age-range of thesubjects is 50.3 years.VADANA [204]: The VADANA dataset contains 2,293images from only 43 subjects (26 males and 17 females) thatare mostly South-Asian. It contains 3-300 images per subjectwith wide variation in pose, illumination and occlusions,while the maximum age gap between two images of thesame subject is 37 years. The annotations of this datasetinclude amongst other attributes, kinship and occlusions(e.g., facial hair and glasses).CACD [32]: The Cross-Age Celebrity Dataset (CACD)was published in 2014 and contains 163,496 images from2,000 celebrities. It was collected from Google Images usinginformation from IMDB. The dataset contains more than 80images per subject on average, but the maximum age gapbetween these images is 10 years. This is due to the fact thatthe collected images were captured between 2004 and 2013.The subjects are mostly Caucasian.Table 1 reveals that age modeling has been primarilystudied by employing datasets captured in the wild. Thefirst major benchmark in the wild is FG-NET and tracesback to 2004. Ever since, the majority of publicly availabledatasets are captured under unconstrained conditions, albeit4having a small number of images. Images from the FG-NET or Morph2 datasets exhibit limited pose and imagequality variation. These issues deteriorate the real-worldperformance of age modeling systems, especially whendealing with extreme poses or small and blurry images. Therecent advances in face detection technology [153], [254] hasresulted in much larger automatically collected datasets e.g.,IMDB-WIKI, CACD, OUI-Adience. These datasets contain agreater number of images and exhibit greater variation withregards to pose, expression and image quality. The reportedbaseline performance for the OUI-Adience is much lower(i.e., classification accuracy of 45.6%) than older datasets,which is characteristic of the challenging nature of thebenchmark.3.2 Datasets with kinship labelsKinship verification from faces is a relatively new task,aiming to verify whether the individuals in a pair of facialvisual inputs are blood related. There are eleven differentkinds of blood relations: (i) Father- Daughter, (ii) Mother-Daughter, (iii) Father- Son, (iv) Mother- Son, (v) Sister- Sister,(vi) Sister- Brother, (vii) Brother- Brother. (viii) Grandfather-Grandson, (ix) Grandmother- Grandson, (x) Grandfather-Granddaughter and (xi) Grandmother- Granddaughter. Acatalogue of dataset with kinship annotations is tabulated inTable 2. The vast majority of the available data is annotatedin terms of only 4 kin relations.CornellKin [65]: The first widely used dataset with kin-ship annotation is the CornellKin Database. The datasetconsists of 300 images and 150 kin pairs, gathered throughonline searches.UB KinFace [236], [194]: In 2011, Shao et al. publishedthe UB KinFace Ver 2.0 dataset, which was an extension toUB KinFace Ver 1.0. The UB KinFace consists of 600 web-collected images and 400 kin pairs, half of which are Asian.Every kin pair is represented by an image of the child, animage of the parent at a young age and an image of theparent at an older age.IIITD Kinship Database [114]: The IIITD Kinship Databasewas released in 2012 and contains 544 images of 272 kinpairs in unconstrained environment. The dataset also in-cludes 272 non-kin pairs. The faces are mainly Indian andAmerican, while other ethinicities include Asian and Afro-American. The dataset contains 7 kin relations.Family 101 [64]: The Family 101 dataset was publishedin 2013 and consists of 14,816 images of 607 people, col-lected from Amazon Turk workers. It is also the first oneto indicate the structure of the families. In particular, thedataset contains whole family trees, each containing 1 to 7nuclear families. There are 206 families, each having 3 to9 family members. The subjects depicted in the data aremostly Caucasian.KinFaceW I & II [149]: The most widely used kinship-annotated datasets are the KinFaceW I & II albums thatwere intoduced by Lu et al. in 2012. The KinFaceW I albumcontains 1,066 images and 533 kin pairs, while the KinFaceWII contains 2,000 images of 1,000 pairs. All the images arecollected from the web, while the difference between thetwo albums is that KinFaceW II contains kin-related facescropped from the same image.HQ & LQ Faces [221]: In order to investigate the abilityto generalize from high-quality images captured under con-trolled condition to low-quality images gathered from theInternet, Vieira et al. introduced the HQfaces and LQfacesdatasets. The HQfaces dataset contains frontal images of92 sibling pairs captured by a professional photographerin neutral expression. The LQfaces dataset consists of 98sibling pairs collected from the Internet with varying resolu-tion. The subjects in the two datasets are mainly Caucasian.Sibling-Face & Group-Face [96]: In 2014, motivated by thelack of large number of sibling pairs, Guo et al. introducedthe Sibling-Face dataset. The images were collected fromsites like Flickr, while the annotations were obtained fromthe tags and descriptions of the images. Along with thisdataset, the Group-Face dataset was also introduced, con-taining images of groups of kin-related people.TSKinface [176]: Qin et al.introduced the TSKinfacedataset, which consists of 787 images and 1015 familygroups. These groups contain the father, the mother andchild or children. The are 274 photos of Father-Mother-Daughter families, 285 Father-Mother-Son families and 228Father-Mother-Daughter-Son families. The majority of thesubjects included in this dataset are Asian.Families in the Wild [183]: One of the most recent effortsin kinship annotated datasets is the Families in the Wild(FiW) dataset, containing 11,193 images. The FiW datasetis the largest kinship annotated dataset to date, contain-ing 30,725 face-images of 10,676 individuals. The datasetincludes 11,193 family photos, while it is the first to have2,060 annotated pairs of grandparents and grandchildren.WVU Kinship Database [115]: The WVU Kinship Databasewas introduced in 2017 by Kohli et al. and contains 113 kin-related pairs. The dataset allows for intra-class variation, asit contains 4 images per person. The dataset is not balancedand consists of: 22 Brother-Brother pairs, 9 Brother- Sisterpairs, 12 Sister -Sister pairs, 12 Father- Daughter pairs, 34Father- Son pairs, 12 Mother- Daughter pairs and 8 Mother-Son pairs.Kinship Face Videos in the Wild Dataset [242]: The KinshipFace Videos in the Wild (KFVW) dataset contains 418 facevideos from TV shows. Each video is 100-500 frames longand presents great variation in pose, occlusions and ex-pressions. The baseline experiments on the dataset achievedpoor performance and thus, the authors note the challengesof advancing video-based kisnhip verification with thisdataset.Similar to age related tasks, the problem of kinship mod-eling has been attacked using mainly in the wild datasets.The nature of the labels makes the task of gathering andannotating such datasets very laborious. As a result most ofthe datasets in Table 2 contain a small number of images,which is not ideal for modeling large variations. What ismore, these images, more often than not, originate from thesame photo. Cropping the data from the same image cansignificantly bias the task of classification, by adding factorslike the environment, lighting, chrominance and the imagequality [143]. Therefore, datasets that contain such imagesare considered biased and can not be used for the task ofkinship verification. This is evident when comparing thestate-of-the-art performance between classic benchmarkslike KinFaceW-II (i.e., classification accuracy 96.2%) and5more recent ones like FIW (i.e., classification accuracy 71%).It is worth mentioning that, VADANA and UvA-NEMOSmile Databases contain both kinship and age annotation.In particular, for the Smile Dataset, the kinship annotationswere obtained based on the names of the subjects.In the following sections, both seminal and recent meth-ods for age progression, age estimation, age-invariant fa-cial characterization, and kinship verification are reviewed.Particular emphasis is put on the facial representationsemployed. That is, for each of the aforementioned tasks,the presented methods are grouped according to the typeof information captured by the employed facial representa-tions. Broadly, geometric, hand-crafted, and learned facialrepresentations are considered. Discussion on correspond-ing classification and regression methods is also provided.Furthermore, for each task evaluation protocols and metricsare reviewed, and the most notable experimental results areanalyzed.4 AGE ESTIMATIONA significant volume of research has been done in ageestimation from facial visual data. Since the labels of thevarious age-annotated datasets are either discrete ages orcorrespond to age-groups, the problem can be naturallycast as a multiclass classification problem, where the classesrepresent discrete ages or age-groups. However, neighbour-ing labels (ages, or age groups) might share importantinformation which is neglected by classification methods.This is addressed by regression methods which appearto perform better. Nevertheless, appearance changes morerapidly during youth and slower in adults. To alleviatethis, non-stationary kernels can be employed; yet learningwith kernels is prone to overfitting. A different approachto deal with this challenge is to adopt ranking methodse.g., [28],[29], that learn an individual classifier for eachage class. In the following subsections several methods forage estimation are reviewed. As already mentioned, theyare organized according to the type of facial representationemployed.4.1 Geometric facial representation-based methodsThe tasks of automatic age estimation from facial imagesintroduced in 1994 by Kwon and Lobo [118]. Inspired byanthropometric studies [5] that describe the growth of thehuman head from infancy to adulthood, six facial distanceratios are used to discriminate between infants and adults.The adult faces are further classified into young adults andolder adults by using snakelets (i.e., deformable curves)[110], which capture wrinkles on certain regions.The use of such anthropometric models, that representthe shape of the human head, introduces a number ofchallenges for age estimation. For instance, geometric de-scriptors are sensitive to pose variations and can thereforebe applied only to frontal images. Several methods dealwith this challenge by normalizing the faces via Procrustesanalysis [112]. Invariant to small changes in camera locationand head pose geometric facial representation has beenproposed in [230] and [212] by employing the properties ofGrassmann manifold [56]. Age estimation is obtained viaSupport Vector Machine (SVM)-based regression in [230]and Relevance Vector Machine (RVM) [214] in [212].Experimental results on the FG-NET dataset in [230]indicate that even though geometric information is sufficientfor age estimation in young ages, texture information isneeded for accurate age estimation in adults. To alleviatethis, fusion of geometric and texture information (repre-sented by Gabor phase patterns (HGPP)) is applied [252].4.2 Appearance-based methodsAppearance models allow to capture texture information ofthe face along with its shape. A wide variety of appearancemodels have been proposed in the literature. Below, wepresent those that have been extensively used in the contextof age estimation.Active Appearance Models: Active Appearance Model(AAM) is a generative facial model introduced in [40] byCootes et al. AAMs employ Principal Component Analysis(PCA) to learn a linear model for shape and appearancefrom images and a set of landmarks. This representationwas first used for age estimation in [123]. In order to capture95 per cent of the data variation, 50 parameters of AAMare used and age estimation is formulated as a regressionproblem in the parameter space. Besides age estimation, theeffectiveness of the model in simulating the aging process isevaluated on age progression as well as age-invariant facerecognition.Most of the datasets in Table 1 are imbalanced; that is,the labels of the data are sparse and not evenly distributed.To overcome this problem, the age labels are encoded ina cumulative manner in [37] using Cumulative attribute(CA) vectors. If the face is older than the ith age, the ithelement of its CA vector is 1 and 0 otherwise. In this way,data points with similar labels have a similar cumulativeattribute vector. The CA is obtained from the parametersof AAM and Support Vector Regression (SVR) is performedin the CA space. The experiments on FG-NET indicate thatshape features play a vital role in age estimation of youngfaces, while texture features become increasingly discrimi-native after the age of 20.In order to make use of the ordinal information of theage labels, Chang et al. [28], [29] introduced a rankingapproach for the task of age estimation in 2010 using AAMs.A ranking model gradually splits the feature space as abinary classification problem is solved for each age labelk. The splits are produced by conducting a query ’is thisface older than age k?’. The final estimation is then producedby a ranking rule based on the outputs of the classifiers.Nevertheless, learning a large number of classifiers can becomputationally intensive. More recent ranking approachesintroduce AAM-based methodologies to reduce the compu-tational cost, e.g., [138], [137]. In particular, a Partial LeastSquared-based Ranker, which learns all classifiers jointly, isintroduced in [138]. Also, a more time-efficient algorithm isproposed in [137]. A Linear Canonical Correlation Analysis-based Ranker is employed and competitive results are ob-tained with a lower number of parameters. The smallercomplexity and model size make such approaches ideal forreal-time applications.In the above papers the effectiveness of the texture andshape parameters of AAM as a facial representation for the6task of age estimation have been demonstrated. However,AAM uses PCA and can therefore only account for linearmodes of variation, which introduces a set of challengeswhen generalizing to unseen data. In order to capture non-linear variations such as expressions and poses, Duong etal. [54] introduced the Deep Appearance Model (DAM).This generative model is based on Deep Boltzman Machines[189], a hierarchical method that can capture variations inshape and texture that could be higher than second order.Experiments in the FG-NET dataset indicate the ability ofDAM to outperform AAM, while being more robust toadditive noise. Nonetheless, having more parameters, thismethod is more resource intensive in comparison to tradi-tional AAM. Deep hierarchical representations are analyzedfurther in subsection 4.4.Local Binary Pattern: The Local Binary Patterns His-togram (LBP) [165] is an effective texture descriptor whichhas been widely employed for texture classification andface recognition [4]. LBPs are binary codes computed in theneighbourhood of pixels and their histogram is obtained fordifferent image-patches. Some variations of the descriptor[229] compare the values of three (Three Patch-LBP) orfour (Four Patch-LBP) patches to produce a code for eachpixel. Yang et al. [247] employed this facial representationto classify faces into three age-classes. In particular, LBPHistograms are extracted from a large set of possible facepatches and AdaBoost [191] is applied next to choose themost discriminative ones. Several different regression meth-ods are applied to the extracted representations, exhibitingpromising results. In [57], fusion of LBP and FP-LBP [229] isexploited for age group and gender classification via SVMclassifier.Wavelet-based Features: Wavelet-based features have beenwidely used in facial analysis, due to their ability to robustlycapture texture information. Among the most widely usedtype of wavelet in age estimation is the Haar wavelet [151],yielding features that are robust to appearance variation.Haar-like features are employed for the task of age esti-mation in [260]. Age estimation is posed as a regressionproblem with a regularized L2 loss function using boosting.Gabor wavelet has also been widely applied in textureanalysis [132]. Importantly, this wavelet has a biologicalsignificance, since their kernels are similar to the receptivefield of the mammalian cortical simple cells. The magnitudeof the coefficients, which is obtained by convolving facialimages with Gabor wavelets across different scales andorientations, is used as facial representation in [78]. Age-group classification is performed using Fuzzy LDA. Exper-iments on a private dataset indicate improved performancecompared to that obtained by employing LBPs.To capture curvature information,like wrinkles, ex-tended curvature Gabor (ECG) filters are used in [113].After feature selection, Random Forest (RF) Regression isused for age estimation. Another interesting wavelet basedrepresentation is the scattering transform (ST) [150]. ST iscalculated by cascading wavelet modulus operators alongdifferent paths in a deep convolution network. A number ofrecent works use Gabor wavelets with ST for age estimation.A ranking approach using ST representations is introducedin [27]. The experiments reveal that AAMs performs betterthan ST in FG-NET dataset, while the opposite stands forthe MORPH2 dataset. This is attributed to the fact that, FG-NET has significantly more images per person and fewerpeople than MORPH2, capturing long-term person-specificage information. Hence, the AAM can efficiently modelthe intra-person variance, while the ST appears to modelperson-invariant age information better.Biologically Inspired Features: Besides the biologically rel-evant Gabor wavelets, the Biologically Inspired Features(BIF) [91] have also been proposed as suitable facial repre-sentation for age estimation. Inspired by the HMAX model[182], the BIF pipeline consists of simple (S1) and complex(C1) layers imitating the classical Hubel and Wiesel model[106] for the primary virtual cortex. Gabor filters are usedfor S1 units, while the C1 units pool over the S1 unitswith a non-linear maximum operator ’MAX’ and normalizeswith a standard deviation operator ”STD”. It is argued thatthe ”STD” operator reveals local variations capturing vitalaging information, like wrinkles and eyelid bags. Exper-iment are conducted on the FG-NET and YGA datasetsusing SVM classification and SVR regression. Experimentalresults indicate that classification accuracy drops on smallimbalanced datasets like the FG-NET.A variety of different regression methods have beenproposed using the BIF descriptor. In particular, a non-linearextension of Partial Least Squares (PLS) [79] is introducedin [90]. Kernel PLS (KPLS) regression is used on BIF featurespace for simultaneous dimensionality reduction and func-tion learning. Another BIF-based method, particularly anage estimator that is robust to pose variation, is introducedin [205]. The multi-view property of this method is obtainedby using video information in a semi-supervised manner.That is, a regression problem is solved with a regulariza-tion factor that enforces output consistency throughout anunlabelled video sequence.A fusion of classifiers and regressors is used in [97],where an automatic demographic estimator is introduced.A hierarchical fusion of SVM classifiers and SVR regressionis used for age estimation. That is, a face is passed througha series of binary classifiers, indicating different age groupsin a coarse-to-fine manner, before going through an SVRregressor. A comparative study between the automated sys-tem and human workers is also presented in [97], revealingthe inability of the humans to perform accurate real ageestimation, as well as the challenges of crowd-sourcing suchefforts.A set of regression methods that take into account thecorrelation between age classes is proposed in [81], [80].Based on the fact that aging is a slow and smooth process,the information extracted from a face does not only describethe exact age of the subject, but also the neighbouring ages.Therefore, based on that assumption the model assigns alabel distribution to the data p(y|x), instead of the exact label.For that purpose two label distribution learning algorithms,namely IIS-LLD [81] and CPNN [81], are proposed andtested in FG-NET and MORPH2 datasets, using AAMs andBIFs respectively.4.3 Subspace learning-based methodsThe aforementioned facial representations are extractedfrom each and every image individually. Nevertheless, the7images of the same person at different ages are correlatedand form a pattern to be recognized. To investigate thisconcept, the AGing pattErn Subspace (AGES) method isintroduced in [82]. In order to represent the transforma-tions caused by aging, aging patterns are employed. Thesepatterns are obtained as sequences of face images sortedin time order. Each individual in the training set has adifferent aging pattern, which is obtained from the trainingimages and by solving a missing value problem for the miss-ing ages. By applying PCA on the resulting aging patternvectors, the so-called Aging Pattern Subspace is obtained.Each point on this subspace represents an aging pattern.During testing, an unseen face is projected on the agingpattern subspace and an aging pattern is selected based onthe reconstruction error. That is, the input face is projectedon all the possible positions in each aging pattern andthe generated face is compared to the input. The selectedposition on the selected aging pattern is then returned asthe estimated age. Instead of pixel intensities, the methoduses low-level face representation extracted from a differentmodel, in particular the AAM.One of the shortcoming of the AGES method is that itrequires a large number of images of the same person atdifferent ages to obtain the aging patterns. In practice, mostdatasets are very sparse, containing only a few images perperson that do not span a large range of ages. To overcomethis challenge, Fu et al. [74] uses face images from all thepeople in the training set to obtain the aging pattern. Inthat way, the common underlying aging pattern of the datais captured. By representing each age label with multipleimages, a discriminative manifold [193] is obtained in a su-pervised manner. During testing, the input face is embeddedon this low dimensional subspace and a regression problemis solved. Several methods are applied to visualize thegeometry of the manifold, e.g., Locality Preserving Projec-tions (LPP) [99], Orthogonal Locality Preserving Projections(OLPP) [25] and Conformal Embedding Analysis (CEA)[75], with the results indicating a distinct pattern. This iscontrary to results produced by unsupervised methods, e.g.,PCA, that do not take into account the age information. Dif-ferent regression methods have been applied on the agingmanifold. Particularly, simple regression methods, i.e. linear,quadratic and cubic, are used in [74]. A more sophisticatedmethod is proposed in [88], where Locally Adjusted RobustRegression (LARR) is introduced.The aging manifold on the aforementioned methodsspans the image space. That is, pixel intensities are usedas local descriptors. This can deteriorate the discriminativeability of the model, since the image space represents allvariations and is not particularly age-sensitive. In order toincorporate the texture information in the aging manifold,LBP are used as low-level representations in [63] where theage is obtained using regression methods, e.g., a Neural Net-works (NN) and Quadratic Funtion (QF). Similarly, agingmanifold can be obtained from different descriptors, e.g.,Gabor [127] and BIF [126]. Particularly, an aging manifoldthat preserves the ordinal information as well as the geo-metric structure of the data is obtained in [127], [126]. Theresulting subspace is well-suited for ranking methods andtherefore the OHRank is employed for age estimation.Most manifold learning algorithms assume that the fea-ture space is locally Euclidean and therefore use a Euclideanmetric to determine neighbourhoods. In the aging manifod,this is not always the case, due to the non-linear natureof aging. Therefore, a distance metric adjustment to LPP isintroduced in [30]. The new metric is learned using Label-sensitive Relevant Component Analysis (RCA) [9]. Ordinalinformation is also incorporated in the aging manifold andk-Nearest Neighbours (kNN) and SVR are used for ageestimation on the embedded subspace. Similarly, a subspacewith ordinal information is captured in [39]. Pairwise ageranking is utilized to obtain a subspace that minimizes thedistance between images of the same label, under a set ofranking constraints.4.4 Deep learning-based methodsAmong the different sources of facial variation, the non-linear transformations, such as deformations due to expres-sions, pose and age, are the most challenging to model. Tomodel such non-linear variations, some of the aforemen-tioned methods extract simpler features at different levels ina hierarchical manner. Since deep architectures build non-linearities on top of each other, they can learn non-lineartransformations more efficiently than simple models andperform better in most tasks, albeit being more vulnerable tooverfitting. A classical deep architecture is the feed-forwardArtificial Neural Network (ANN). This model uses pixelintensities (PI) or low-level descriptors as inputs and buildsa subspace in a supervised manner using backpropagation.A variation of ANN, namely the Compositional PatternProducing Network (CPPN) [206], is used in [62] for thetask of age estimation. Contrary to typical ANNs, the four-layer ANN proposed in [62] has different transfer functionsfor each neuron.Similar to the ANNs, Convolutional Neural Networks(CNN) [125] are used to extract high-level features, or per-form regression or classification as an end-to-end model.Instead of using handcrafted features, a CNN applies filterson the raw images in a hierarchical supervised manner.Convolution and subsampling are applied iteratively tocreate each layer’s feature map. The filters are then opti-mized using backpropagation, while the architectures usedcan vary greatly. Deep CNNs have outperformed classicalmethods in most tasks, including age estimation. Yang et al.[246] is the first to use a 5-layer CNN for age estimation. Themodel has multiple outputs for gender, age and race, whilethe age estimation performance is worse than that obtainedby BIF. In order to improve the performance, Yi et al.[248] adopts methodologies from traditional facial analysisand used a fully connected ensemble of 23 CNNs. EachCNN is applied on different face patches and the methodsucceeds in improving the state-of-the-art performance onthe MORPH 2 dataset.Recently, the large availability of data and computationalpower have boosted the popularity of deeper and morecomplex architectures (e.g., ResNet [98], DenseNet [104]).A hybrid deep multi-task CNN is employed in [240] topredict age, race and gender. This architecture incorporatesthe gender and race predictions by training a different ageestimator for each group. Another deep CNN architecturethat leverages weakly labelled data is proposed in [103]. The8proposed model performs age estimation with assistanceof age difference information (AEAD). The age differenceinformation is obtained from image pairs of the same subjectwith known age gap.Deep ranking methods have found great success in ageestimation. A deep ordinal regressor is trained on the verylarge AFAD dataset in [162]. This ranking approach usesmultiple output CNNs (MO-CNN) and outperforms theclassical ranking algorithms on the MORPH2 dataset. Simi-larly, a Ranking-CNN approach, that utilizes multiple deepbinary classifiers, is proposed in [38]. This method employsan ensemble of CNNs that are fused with aggregation. Theinconsistency of the binary outputs is proven not to affectthe overall performance as the output error is bounded bythe maximum error of the binary classifiers [38]. Therefore,as long as the maximum error is decreased, the inconsistentlabels of the classifiers do not matter.A different age ranker for separate age-groups isadopted in [134] to learn a series of aging patterns. Adeep CNN architecture is employed to minimize the dis-tance of faces within the same age-group, while maximizingthe distance between faces from different groups. The ageprediction is then acquired using the OHRank algorithmfor each age-group. To effectively maximize the distancebetween age groups, label-sensitive deep metric learning(LSDML) is introduced [136]. This method optimizes theprocedure by jointly learning a discriminative metric andmining hard negative pairs. In order to mitigate the effectof sparse and imbalanced datasets, the method is extendedto multisource LSDML that maximizes the cross populationcorrelation between different datasets.A large number of deep methods are applied to apparentage estimation, mainly hosted by the Chalearn Lookingat People (LAP) challenge [59], [60]. In particular, a deepnetwork consisting of 10 convolutional layers, 5 poolinglayer and 1 fully connected layer [36] is used in [180]. For theaging function, a test image is classified into 3 age groupsand a 3-layer ANN regresses the apparent age. Each of theregressors is trained on a different dataset.The highest positions in the LAP challenge are popu-lated by variation of popular deep learning architectures.Such variations include ensembles of multiple networks andfusion of different models. The most widely adopted onesare the VGG-16 [202] and GoogLeNet [210] Deep CNN, dueto their success in the Imagenet object recognition challenge[186]. Modified versions of the original VGG-16 model areused for apparent age estimation in [185], [117], [7] and[218]. These models are trained and finetuned on severaldifferent datasets to achieve significant generalization, whilea variety of classification and regression techniques areemployed on the deep representations. Particularly, an SVMbased classifier is used in [218] and improves the resultsof the regression method used in [185]. Lastly, a fusion ofGoogLeNet-based classifier and regressor is used in [141]and hierarchical SVM age grouping followed by SVR andRF regression on the deep features is employed in [262].4.5 Other representationsBesides the above facial representation, several other ap-proaches have been proposed. Such methodologies eitherutilize multiple descriptors, e.g., [11], [33], [67]; that is,multiple features are fused to capture more discriminativeinformation, or introduce novel extensions to these repre-sentations, e.g., [148], [224]. In particular, the NeighborhoodCentroid Difference Vector (NCDV) from the LBP descriptoris used in [148] and the Cost-Sensitive Local Binary Features(CS-LBFL) are introduced. A multi-feature learning exten-sion is also proposed and the final prediction is obtainedusing the OHRank algorithm.Most descriptors in this section take into account age-sensitive information, e.g., wrinkles, smoothness, bags un-der eyes, facial hair, etc. Since different age groups sharethe same attributes, such information is not always accurateand can lead to misclassifications. Nevertheless the presenceof these attributes is different between the age groups. In[224], a Learning Using Privileged Information (LUPI) [219]approach is proposed, so that such inaccuracies are usedto boost the model’s generalization ability. That is, for eachattribute, the age groups are ordered based on the presenceof the specific attribute. The problem is solved using relativeattribute SVM+ (raSVM+).In order to obtain a discriminative mapping of thefeature space, a number of the above methods have in-troduced hierarchical features. The notion of hierarchicalrepresentations is not limited to deep learning methods, butis generally used in representations that accumulate infor-mation in multiple layers. A simple and efficient method toextract hierarchical features for age estimation is presentedin [116]. The method, namely MidFea, consists of k-means,convolution, max-pooling, vector quantization and randomprojection operations. The MidLevel features are then usedas input to a Neuron Selectivity layer (NS) [15] to obtain thefinal representation.All ofthe above representations originate from 2-Dimensional images; that is, they are either extracted frompixel intensities or facial landmarks. Nevertheless, othermodalities, e.g., 3D [234], video [48], context [77], containage information and a number of methodologies are pro-posed to capture it. In particular, Gallagher et al. [77] assigncontextual features that capture local pairwise informationand the global position of the person. A Gaussian MaximumLikelihood (GML) classifier is used to classify the faces into 7age groups. Interestingly, the contextual features achieve anaccuracy of more than double random chance, while genderis correctly estimated two-thirds of the time.An interesting combination of different modalitites isthat of appearance with facial dynamics. This is investigatedin [48], where temporal face dynamics are extracted fromsmile and disgust videos. The videos that are used in-clude posed and spontaneous expression and were recordedunder controlled conditions. The experiments in [48] re-veal that dynamic features are not as discriminative asappearance features for age estimation. Towards that end,several appearance estimators are used, i.e. dynamic, in-tensity based encoded features (IEF), gradient-based en-coded features (GEF), BIF and LBP, while feature selectionis performed using the Min-Redundancy Max-Relevance(mRMR) algorithm. The combination of the appearance anddynamic features achieves the best results, indicating thatdynamics can help significantly towards age estimation.94.6 Evaluation protocols, metrics, and results for ageestimationProtocols: A number of different evaluation protocols havebeen used to evaluate Age Estimation models. Classicalmethods like k-fold cross-validation (c-v) and 80-20 splithave been used in this context as well. Contrary to c-v, whenapplying an 80-20 split, a predefined 80% of the data is usedfor training and 20% is used for testing, without reshufflingthe data. Besides these widely adopted protocols, specificprotocols have been proposed for specific datasets.One of the most widely used protocols is the Leave-One-Person-Out (LOPO) protocol, proposed for the FGNETdataset. According to this protocol, experimental evaluationis performed using images of previously unseen individ-uals. Therefore, training is done using all subjects in thedatabase apart from the subject whose age we are estimat-ing.Other dataset-specific protocols include the Images ofGroups (Groups) protocol and the Chalearn apparent agedataset protocol (chalearn). The data split for the Groupsis a random selection of 3,500 training images and 1,050testing images. The age group classification accuracy (acc.)is calculated both for exact match (AEM) as well as forallowing error of one category (AEO). On the other hand,the Chalearn dataset is split into 2,476 images for training,1,136 images for validation and 1087 images for testing.Metrics: The large number of methods in age estimationhas exposed the need for a common evaluation protocol.The most widely adopted measures are the Mean AbsoluteError (MAE) and the Cumulative Score (CS). The MAE isdefined as the average of the distance between the predictedage labels and the ground truth,M AE =N(cid:88)=1|¯y − y|/N.(1)On the other hand, CS(j ) is the percentage of the samples,where the predicted age of model did not deviate from theground truth more than j years,CS(j) = Ne≤j/N × 100%.(2)Lastly, specifically for the apparent age estimation models[59], the adopted measure is the error calculated as follows:error = 1 − e− (y−yi)22σ2.(3)Results: Table 3 includes the methods that have reportedthe highest scores for each dataset in this subsection, usingcomparable and widely used protocols. A chronologicaloverview of the most important methods and result on AgeEstimation is tabulated in Table 4.4.7 Discussion on age estimationThere is a number of factors affecting the accuracy of ageestimation algorithms. The choice of facial representationplays a vital role in this, with the advantages of each ap-proach having been described above. Besides that, the typesof variation within each dataset determines the success ofthe facial representation and the algorithm. Such variationsinclude image quality, race, gender and facial expressions.TABLE 3: Best reported results on Age Estimation for eachdatasetDatasetmethod metricprotocolscore[135]FG-NET[135]MORPH2[127]Groups[135]Adience[162]AFADChalearn[7]UvA-NEMO [48]MAEMAEAEM/AEO acc.acc.MAE,CS(≤ 10)errorMAELOPO80-20groups5-fold c-v80-20chalearn10-fold c-v3.742.8948.5% , 88%60.2% ±5.33.34, ∼ 95%0.24114.33An experimental study of age estimation under changes inimage quality was conducted in [6]. Image quality affectstexture information that captures aging cues, like wrinkles.Therefore, the results confirmed the deterioration of theaccuracy with declining image quality.People of different gender do not follow the same agingpatterns. A similar assumptions can be made for peopleof different race, eg. Caucasian and Asian. Several populardatasets, including MORPH2, contain faces from people ofdifferent gender and race. Such non linear variations affectthe accuracy of the facial representations. The studies in [89], [95] and [14] indicate this problem and propose transferlearning and domain adaptation solutions to deal with it.A similar problem arises when performing age estimationacross different expressions. The quantitative study in [94]indicates the significant influence of facial expressions onage estimation, while a cross-expression age estimationframework is proposed.In total, different datasets have different modes of vari-ation, since they were collected under different circum-stances. That is, image quality, illumination, race, genderand age among the data can vary significantly. Similar tothe above, using the tools of transfer learning, Su et al.represented different datasets as different domains and across-database age estimation framework is proposed in[207].4.8 Challenges in the wildMost of the methods presented in this subsection are evalu-ated on in the wild datasets. Nevertheless, as mentioned insection 3, these datasets pose different levels of challenges.Table 3 indicates that datasets like Adience and Groups areindeed more challenging, having great variation regardingface size, image quality, make-up and occlusions. Particu-larly, the median face from the Groups dataset has 18.5pixels between the eye centers, while the Adience datasetdemonstrates extreme variation in image quality (Figure 1).On the other hand, FG-NET and MORPH2 contain familyphotos and mugshots respectively. While the image qual-ity in the first one varies greatly, neither of them containextreme poses. It is therefore expected, that the state-of-the-art results for these benchmarks are better compared to theAdience and Groups benchmarks. Lastly, the best score isachieved with the MORPH2 dataset, which can be justifiedby the fact that the faces in the mugshots are frontal.5 AGE PROGRESSIONFace synthesis of an individual at different age groups is atask that has been studied for several years across computer10graphics, anthropometry and computer vision. Althoughearlier methods used face models introduced in computergraphics, recent work has gradually transitioned to com-puter vision. One of the seminal works is by Burt et al.[43], which simulates the aging process by employing bothcolour and shape information. In this approach, the synthe-sized image is produced by adding the difference betweenthe average faces (i.e. the prototypes) of each age group tothe test image. Other early works include anthropometricgrowth [177], [178], modeling of wrinkles [232], [233], [231],[213], aging functions [122] and caricaturing 3D face models[168], [167]. Notably, not all of the aforementioned earliermethods produce photorealistic results. More recently pro-posed methodologies in age synthesis are presented in thefollowing paragraphs.5.1 Subspace Learning based MethodsThe tools provided by subspace and factor analysis havebeen used to model the transformations caused by aging.A prototype based approach that compensates for illumi-nation variation in the prototypes is introduced in [111]. Inthis Illumination Invariant Age Progression (IAAP) method,the prototype for each age cluster is obtained as a rank-4approximation using Singular Value Decomposition (SVD)and are relighted according to the input face. Then theinput face is rendered to a different age by applying thedifference in flow and texture between the source and targetprototypes. The results are evaluated in a large scale userstudy on Mechanical Turk, the result of which indicates theinability of the workers to recognize a face across large agedifferences.In prototype-based approaches (e.g., [43], [111]), the tex-ture transformation is modelled as the difference betweenthe prototypes of the source and target age groups. Thatis, the texture changes are the same for different inputs,as long as the source and target age is the same. Insteadof a specific prototype, a dictionary of aging patterns foreach age group is learned in [198]. To capture the differentfactors of variation, an input face is decomposed into anaging and a personalized layer. The changes in the aginglayer are modelled using the dictionaries of the target andthe source age groups. A more general approach that isrobust to different kinds of variation, namely Robust AgeProgression (RAP), is introduced in [187]. Each image ofa specified gender is expressed as a superposition of theage component and the common component. The commoncomponent captures facial variations such as identity, shape,pose, occlusions, illumination and expressions, while theage information is captured by the age component. Thus,by computing an orthonormal bases of the age and commoncomponents via SVD, an image can be progressed to anotherage group as a linear combinations of these bases at the kthage group.In a similar manner, Hidden Factor Analysis (HFA) isused in [245] to decompose the facial input. Thus, the face isdecomposed into a linear combination of the mean face andthe identity, age and noise factors. The age component at adifferent age is then sparsely represented by a dictionary ofage components of the same age group. The shape is pro-gressed by applying the difference between the mean shapesYear1994200220052007200720082008200920092009201020102011201120112011201120112012201220122013201320132013201420142015201520152015201511Paper RepresentationMethod[118]ratios,wrinkleshand crafted rule[123] AAM[260] Haar[247]LBP[82]AAMregressionregression, boostingRule-based, AdaboostLDA, AGES[74]PICEA, multilinear regression[88]AAMOLPP, LAR[77][78][91]contextualGaborBIFGMLFuzzy LDAPCA,SVM/SVR[28]AAM[256] AAMThreshold rankerMulti-task warped GP[29]AAMOHRank[90][205][161]BIFBIFBIF[62][246]AAMPI[230][212][127] Gabor , contextualgeometric, HGPPgeometric HGPPKernel PLSmultiview regressionRMIRCPPN, kNN5-layer CPNNSVM, PLSRVMPLO, OHRank[81]AAM , BIFMFA, CPNN/IIS-LLD[37]AAM[30][39]AAMAAMCA, SVRLPP, lsRCA, kNN, SVRranking SVR[57]LBP, FPLBPDropout-SVM[248]PICNNDataset(Score)private(100%)private(1.88)FG-NET(5.81)FERET(7.88%)PIE(12.5%)FG-NET(6.22)MORPH(8.07)YGA-fem(8)YGA-male(7.8)FG-NET(5.07)YGA-mal(5.30)YGA-fem(5.25)Groups(32.9%, 64.4%)private(91%)FG-NET(4.77)MORPH-fem(3.47)MORPH-mal(3.91)FG-NET(4.67, ∼ 68%)MORPH(6.49, sim50%)FG-NET(4.14)MORPH(4.07)FG-NET(4.48, 74.7%)MORPH (5.88, 56.5%)MORPH(4.18)Custom(6.94 ±0.07)FG-NET(8.37)MORPH(6.06)FG-NET(4.67)FG-NET(4.88)FG-NET(6.76)FG-NET(6.2)FG-NET(4.82)Groups(48.5%, 88%)FG-NET(4.76)MORPH(4.87 ±0.31)FG-NET(4.67, 74.5%)MORPH(5.88, 57.9%)FG-NET(4.38)FG-NET(4.56)MORPH1(5.41)MORPH2-Cau(4.42)Adience(45.1% ±2.6)Groups(66.6% ±0.7)MORPH(3.64)MetricaccuracyMAEMAEerror rateerror rateMAEMAEMAEMAEMAEMAEMAEAEM, AEOaccuracyMAEMAEMAEMAE,CS(5)MAE,CS(5)MAEMAEMAE,CS(5)MAE,CS(5)MAEMAEMAEMAEMAEMAEMAEMAEMAEAEM, AEOMAEMAEMAE,CS(5)MAE,CS(5)MAEMAEMAEMAEaccuracyaccuracyMAEMidFea-NS, SVMmRMR, SVM, SVRFG-NET(4.73)UvA-NEMO(4.33 ±4.06)MAEMAE[116][48][27]PIdynamic, IEF, GEF,BIF, LBPSTCSOHRank[54][126]PI, landmarksBIFDBMPLO, OHRank2015[97]BIFAdaboost, SVM,SVR201620162016201620172017[224] DSIFT, LUPIraSVM+, OHRank[162][185][7][38][136]PIPIPIPIPIMOCNNDEXensemble DEXRanking-CNNM-LSDML2017[103]PIAEADTABLE 4: Overview of Age Estimation methodsFG-NET(4.70, sim75%)MORPH(3.82, sim78%)FG-NET(5.28)FG-NET(1.306)Groups(0.864)FACES-best(5.16)FG-NET(3.8 ±4.2)MORPH(3.5 ±3.0)PCSO(4.1 ±3.3)FG-NET(4.07)MORPH(5.05±0.11)MORPH(3.27)AFAD(3.34)Chalearn(3.221, 0.2649)Chalearn(0.2411)MORPH(2.96, ∼ 85%)MORPH(2.89)Adience(60.2±5.3)FG-NET(3.74)FACES(3.11 - 5.01)Chalearn(0.315)MORPH(2.78)FG-NET(2.8)MAE,CS(5)MAE,CS(5)MAEMAEMAEMAEMAEMAEMAEMAEMAEMAEMAEMAE, errorerrorMAE,CS(5)MAEaccuracyMAEMAEerrorMAEMAE12Fig. 1: Samples from Adience and FG-NET datasetsof the target and source age clusters. In order to disentanglethe common and individual components, a robust extensionto Joint and Individual Variation Explained [142] is used in[188]. Thus, by modifying these components, an input facecan be reconstructed at a different age group. The methodis evaluated on the FG-NET and AgeDB datasets, whileexperiments are conducted on facial expression synthesisas well.To deal with the different modes of variation, instead ofa matrix, a tensor is used to represent the data in [227]. Thedifferent modalities of the data, namely the pixels intensi-ties, identities and ages of the face images, are includedalong the the dimensions of the tensor. This means thatalong each axis of the 3-dimensional structure, the variationof only one modality changes. Based on the fact that lowfrequency image components preserve the face identity,super-resolution in tensor space is used to map the textureof a downsampled input image to a different age.ods have been proposed, including Markov Process [209],[208] and Recurrent Neural Networks (RNN) [225]. In par-ticular, the transformation of different facial parts is mod-elled separately in [208]. Instead of using AAM, a regionbased AAM model (RB-AAM) is adopted. The relationshipsbetween the different sub-regions are modelled based onthe physical structure of the face. To approximate the short-term aging of each sub-region, an aging function approachis used. The aging model over a long period is formulated asa Markov Process by concatenating the short period agingfunctions under smoothness and consistency constraints. Onthe other hand, Wang et al. [225] use RNN [87] to model ageprogression between neighbouring age groups. The faces ofthe neighbouring groups are normalized jointly. An RNN isadopted to perform aging of a face in the shared eigenfacespace [216] of two neighbouring age groups. The output ofthe RNN is reconstructed, projected on the shared eigenfacespace of the next group and used as input for the next RNN.5.2 Sequence modeling5.3 3-Dimensional RepresentationsSince the effects of aging are temporally correlated, i.e. thetransformation is smooth and continuous, it is intuitive tomodel it as a sequence across the different age groups. Inorder to describe the evolution of the facial representationthrough the age groups, different sequence modeling meth-The aforementioned methods come with certain limitationssince they are based on 2-dimensional representations. Con-trary, 3D facial representations capture both shape and tex-ture information and can potentially obtain pose invariance.A 3-dimensional model is used to build an age progressionsystem for children faces [197]. Different facial components,such as mouth, eyes, nose and face shape, are extracted andprogressed individually. The basic assumption is that if twochildren look similar at a young age, they will continue tolook similar when they grow older. Therefore, each compo-nent is compared to a database of facial components and isprogressed according to the corresponding aging pattern.The selected components are then merged to synthesizethe progressed image. The experimental results on selectedimages of the Jackson family indicate the possibility ofsynthesizing age progressed faces of children given the facesof their relatives.A different approach that incorporates the whole face ina 3D aging model is proposed in [171]. The aging modelis approximated as a weighted average of all the textureand shape aging patterns in the training set. The modelis evaluated using a commercial face recognition systemas described in Section 6. Similarly, texture and shape aremodelled separately in [152]. The face shape of each agegroup is modelled using age specific 3D Point DistributionModel (3DPDM) [41], while the texture is modelled usingrecursive PCA.5.4 Deep learning-based representationsSynthesizing a photorealistic face image with arbitrarymodes of variation is a challenging task. Deep learningmethods are able to incorporate knowledge from multi-ple datasets and are therefore suited to deal with thiscomplicated problem. In particular, Generative AdversarialNetworks (GANs)[85] have proven capable of producingrealistic images and have been successfully applied to facesynthesis, e.g., [139], [258]. The original model consists oftwo networks, a Generator and a Discriminator, that aretrained simultaneously. The Generator tries to model thedata distribution and synthesizes images which the Discrim-inator classifies as ’real’ or ’fake’, that is whether they comefrom the actual data distribution. The image generationis conditioned on random input noise z, which follows apredefined distribution. The optimization procedure can beconsidered as a minmax game between the two models,as the parameters of each network are optimized alterna-tively. In some variations of the model, particularly in theConditional GAN (CGAN) [155], the generated images areconditioned on a specific input, rather than random noise.A Conditional Adversarial Autoencoder (CAAE) for ageprogression is introduced in [257]. Instead of randomlysampling z, it is obtained from an autoencoder, so that incor-porates the personality of the face. The model includes twodiscriminators, one to enforce p(z) to be uniform; that is, toforce z to evenly populate the latent space, and one to forcethe generator to produce realistic images. The generationof the image is conditioned on both the personality of theinput, as well as the desired age group. The method is evalu-ated based on survey results, that indicate the ground truth.A conditional GAN (Age-cGAN) is also employed in [8] toperform face aging. The model obtains the embeddings ofthe input and ouput of the GAN from a face recognitionneural network. In order to preserve the identity of theinput face, the Euclidean distance between the embeddingsis minimized.135.5 Evaluation protocols, metrics, and results for ageprogressionProtocols: There is a wide variety of different protocolsin the literature regarding the evaluation of age progres-sion models, since not all methods focus on the same agegroups, e.g., some focus on children faces, while othersfocus on adult faces. That being said, all the methods inthis subsection use the FG-NET dataset to test their methodsqualitatively or quantitatively.Metrics: The evaluation of age progression methods isnontrivial, since the task itself is ill-posed and the actualaging process is highly uncertain. A comparative study ofdifferent age progression evaluation techniques is presentedin [120], while a framework for evaluation is described in[124]. A short description of the most common evaluationmethods is presented as follows:a. Human-based evaluation: Directly comparing the syn-thesized images with the ground truth is not an effectivemeasure of accuracy, as the images were taken under dif-ferent sources of variation. A way to evaluate the results ofage progression algorithms is by conducting a study withhuman users. The results can then be evaluated based onthe users’ answers to questions comparing the synthesizedimage and the ground truth at the target age.b. Age estimation accuracy: Age progression models arealso evaluated based on their ability to generate faces withcharacteristics of the target age group. The accuracy of thesynthesis can be measured in an automated manner, usingan age estimator and the corresponding metrics as describedin Section 4.c. Preservation of face identity: An age progression methodis evaluated based on whether the synthesized imagescan still be recognized as images of the same person. Todetermine that, face recognition and verification systemshave been widely used. In the context of age progressionevaluation, common metrics include classification accuracy(rank n), the ROC curve and the area under it (AUC) as wellas Equal Error Rate (EER). A more detailed description ofAge-Invariant Face Recognition (AIFR) and Cross-Age FaceVerification (CAFV) methods and the corresponding metricsis included in Section 6.Results: While the methods in this subsection havebeen evaluated on different datasets, we will only use thecommon results on FG-NET for comparison. Based on theirquantitative results on FG-NET, the most significant resultsare presented in Table 5. The results for preservation ofidentity are reported before (score-b) age progression as wellas after (score-a) wherever available.TABLE 5: Best reported results on Age Progression based onpreservation of identity on FG-NETmethodtestmetricscore-bscore-a[198][187][227][225]CAFVCAFVAIFRCAFV14.89% 8.53%EERacc., AUC N/A0.55accuracy∼ 15% ∼ 10%EER0.709, 0.8060.7Results on other databases are scarce. Preservation ofidentity results on AgeDB, Morph, CACD and Browns aretabulated in Table 6. The results on AgeDB are reported forage difference of 10 years. A chronological overview of themethods in this subsection is tabulated in Table 7.TABLE 6: Best reported results on Age Progression based onpreservation of identity on other datasets, score-B and score-A correspond to face verification score before and after ageprogressiondatasetmeth.testmetricscore-Bscore-AAgeDBMORPHCACDBROWNS[188][171][187][171]CAFVCAFVCAFVCAFV0.591,0.624acc.,AUCacc.(r=5)68%acc.,AUC N/A45%acc.(r=5)0.621,0.65473%0.735, 0.79860%5.6 Discussion on age progression methodsSince age progression is a challenging task, age clusters arelabelled with whole age-groups and not exact ages. Thisformulation also helps with the highly incomplete datasetsthat are available. This issue is more dominant in faces ofchildren, where images are very hard to collect and oldfamily photos are often of bad quality.5.7 Challenges in the wildThe number of widely adopted benchmarks for age pro-gression is limited to 5 datasets, all of which are capturedin the wild. The use of the CAFV protocol does not allowfor quantitative comparison between the different datasets,since different face verification algorithms are used. There-fore, a comparison can only take place using qualitativestandards that are hard to evaluate. The challenging natureof the task has led research to focus mainly on datasets withsmall variation in pose and image quality. The qualitativedifferences between the results is largely attributed to themethods and so, conclusions can not be drawn regardingthe ’wildness’ of the datasets for the task of age progression.146.1 Age-invariant descriptorsThe methods in this subsection focus on obtaining featuresthat are invariant to the aging process of the face. The localdescriptors that are widely adopted in face recognition, e.g.,LBPs, have been used in an age-invariant context as well. Anexperimental evaluation of local descriptors in age-invariantface recognition is presented in [13]. Nevertheless, theseclassical descriptors cannot be used as stand-alone facialrepresentations, as they do not always capture the age-invariant information. Therefore, the following methods fo-cus on introducing new facial representations that are robustto age changes. The face representation used in [131] is ahierarchical combination of Gradient Orientations (GO) [34]of each color channel of the image at different scales, calledGradient Orientation Pyramid (GOP). The hierarchical in-formation captured by this descriptor is classified usingSVM and improves the verification results on adults. Onthe other hand, higher order information does not improvethe accuracy significantly for age changes in teenagers.Using high dimensional LBPs, an age-invariant represen-tation for cross-age face verification is described in [31], [32].After extracting the descriptors, a reference representationof every person is obtained at each age. The features are thenencoded into this reference space and max pooling is used tonormalize the representations of the same person at differentages. The resulting features are age-invariant, since theyhave a high response at the reference person at any year.The face pairs are then classified according to their cosinesimilarity. Lastly, a novel encoder that makes use of binarypatterns, similarly to LBPs, is introduced in [84]. Unlikeother encoders, this descriptor converts binary patterns toevenly distributed codes. The final representation is ob-tained by maximizing the entropy of the descriptor. Identitymatching is performed by decomposing the representationusing Identity Factor Analysis (IFA) and classifying thecosine similarity of the inputs. A similar feature extractor,called Local Pattern Selection (LPS), is used at multiplescales in [128]. The multiple scaling and dense samplingof the method result in a high-dimensional representation,which is refined using bagging [21] multiple classifiers.6 AGE-INVARIANT FACIAL CHARACTERIZATIONAge-invariant facial characterization involves two basictasks: age-invariant face recognition and cross-age face ver-ification. The goal of the methods for age-invariant facerecognition is to build a model that is able to recognize theidentity of a face across different ages from a database offaces. On the other hand, cross-age face verification aimsto determine whether two age-separated images are fromthe same person. These tasks can be approached eitherwith generative or discriminative methods. In generativeapproaches, an input face is transformed to the target agebefore performing face recognition, according to the meth-ods described in the section 5. On the other hand, in discrim-inative approaches [130], [16], age-invariant representationsare extracted and a classification problem is solved. Thegenerative methods are described in the previous section,while the most recent discriminative methods are presentedin the following.6.2 Age-invariant subspace learning-based methodsSimilar to the previous tasks, instead of focusing on age-invariant local descriptors, an age-invariant subspace can belearned from simple representations. To build this subspace,several subspace analysis methods have been employed. Inparticular, Multi-Feature Discriminant Analysis (MFDA) isemployed in [129]. The subspace is obtained from ScaleInvariant Feature Transform (SIFT) [144] and Multi-ScaleLBPs features. The classification problem is solved in thesubspace using bagging. The LBP feature space is also em-ployed in [20] to obtain a subspace that captures geometricfeatures of the data, such as shape. Nonlinear TopologicalComponent Analysis (NTCA) is introduced to obtain thelow-dimensional age-invariant subspace.Another subspace learning method that has been used inother age modeling tasks, e.g., age progression [245], is theHFA. Hidden Factor Analysis decomposes facial featuresinto a linear combination of the age component, the identitycomponent and the noise term. Contrary to age progression,2011[197], [196]3Dmetric learningYearPaperRepr. MethodDataset(score-B - score-A)prototype basedPCAwavelet, prototype basedprivateprivateprivate15TestN/AN/AN/AMetricN/AN/AN/A1995[43]1999[167]2001[213]2006[177]2008[178]2010[209]2010[171]2D3D2D2D2D2D3D20122012[227][208]2014[111]2015[198]20162016[245][187]2016[225]20172017[257][188]2017[8]2D2D2D2D2D2D2D2D2D2Dcraniofacial growthCustom(28% - 37%)Recogntionacc.(rank=1)craniofacial, wrinkle analysisCustom(38% - 51%)Recogntionacc.(rank=5)And-Or Graph, Markov Chainweighted averageLHIMORPHFG-NET(42% - 55%)MORPH(68% - 73%)BROWNS(45% - 60%)FG-NETJackson Familytensor space analysisRB-AAM, Markov ChainFG-NET(55% - 70%)CustomIAAPFG-NETN/AN/AVerificationVerificationVerificationN/AN/Aacc.(rank=5)acc.(rank=5)acc.(rank=5)N/AN/AN/AN/ARecogntionN/AAccuracyN/AN/AN/Adictonary learningFG-NET(14.89% - 8.53%)VerificationEERHFARAPRFACAAERJIVEAge-cGANFG-NET(∼ 52% − ∼ 54%)FG-NET(N/A - 0.709)FG-NET(N/A-0,806)CACD(N/A - 0.735)CACD(N/A-0,798)FG-NET(∼ 15% - ∼ 9%)CustomAgeDB(0.591- 0.621)AgeDB(0.624-0,654)IMDB-WIKI-cleaned( - 82.9%)RecognitionVerificationVerificationVerificationVerificationVerificationN/AVerificationVerificationRecognitionacc.(rank=1)AccuracyAUCAccuracyAUCEERN/AAccuracyAUCaccuracyTABLE 7: Overview of Age Progression Methods, score-B and score-A correspond to face verification score before and afterage progressionfor the task of face recognition, the identity componentis taken into account. The feature space is obtained fromHistograms of Oriented Gradients (HOGs) [44]. The finaloutput is obtained based on the cosine similarity of theidentity components.Lastly, a different method that coordinates cross-age faceverification and cross-face age verification is introducedin [52]. Motivated by the fact that, the first task needsage-invariant features while the latter needs age-sensitivefeatures, the method coordinates the two in a multi-tasklearning manner. To achieve this, both tasks share the samefeature pool and feature interaction is encouraged via anorthogonal regularization. That is, the final features areselected so that the age-sensitive ones are avoided.6.3 Deep learning-based methodsDeep Learning models have not found wide use for age-invariant facial characterization, possibly due to the lackof large aging datasets. Nevertheless, a latent factor guidedconvolutional network is presented in [228]. The fully con-nected layer of the CNN is designed using Latent IdentityAnalysis, separating the identity components from the restof the convolutional features. The experiments performedindicate that simply finetuning a deep CNN on an agingdataset improves the accuracy significantly. This revealsthat a baseline CNN needs further processing in order toefficiently learn age-invariant features.6.4 Evaluation protocols, metrics, and results for age-invariant facial characterizationProtocols: The most widely used datasets in this subsec-tion are the FG-NET, MORPH and CACD datasets, whilethe protocols involve some of the aforementioned oneslike k-fold cross-validation and LOPO. Particularly for theMORPH Album 2 dataset, a common protocol includessplitting the dataset into a training and a test subset, eachcontaining images from 10,000 subjects (10k-10k). Lastly, itshould be noted that since the CACD dataset consists ofsome noisy labels as well as duplicates, a verification subsetcalled CACD-VS is introduced in [32] and used for testing.Metrics : Different methods have been used to evaluateage-invariant facial recognition and cross-age face verifica-tion. Some methods [52], [131] use Equal Error Rate (EER)to evaluate the performance of the verification. This metricdescribes the rate at which both accept and reject errorsagree. Nevertheless, the most popular evaluation metric isverification accuracy or recognition accuracy. In most cases,rank-1 or rank-n accuracy is used, where the rank indicatesthe number of gallery images that have to be inspected inorder to achieve this performance.Results: The most accurate method among the onesincluded in this subsection is the deep learning modelin [228], which outperforms the rest in all datasets. Theresults are tabulated as follows, along with the second-bestperformance in each dataset. A chronological overview ofthe methods in this subsection is tabulated in Table 9.TABLE 8: Best reported results on age-invariant facial char-acterizationdatasetmethod metricprotocolscoreFG-NETFG-NETMORPHMORPHCACD-VSCACD-VS[228][84][228][128][228][32]rank-1 acc.rank-1 acc.rank-1 acc.rank-1 acc.rank-1 acc.rank-1 acc.LOPOLOPO10k-10k10k-10k10-fold c-v10-fold c-v88.1%76.2%97.51%94.87%98.5%87.6%6.5 Discussion on age-invariant characterization meth-odsSimilar to classical Face Verification and Recognition meth-ods, pose, illumination, occlusions and expressions are sig-nificant variation inducing factors. Therefore, the afore-mentioned methods should be able to obtain invariantrepresentations, not only to aging, but also these kind ofvariation. The issue of inadequate aging datasets, that wasmentioned in the previous subsection, stands for this taskas well. This is important, especially for images of children,where experiments [131] have shown that verification ismuch harder compared to adults. Lastly, face recognitionand verification depend prominently on the age differencebetween the probe image and the gallery image [92], wherethe accuracy of the algorithm usually decreases with largerage gaps.6.6 Challenges in the wildSimilar to age progression, age-invariant facial characteri-zation methods have been evaluated only on three in thewild datasets. Since all three benchmarks follow similarprotocols, they can be compared based on the the state-of-the-art results. The results on Table 8 validate the non-challenging nature of the MORPH2 benchmark, particularlyin comparison to the pose and image quality variation inFG-NET.7 KINSHIP VERIFICATIONKinship Verification has recently gained interest as a taskin machine learning and computer vision community. Thetask refers to bi-subject verification; that is, classifying a pairof input images as kin or non-kin. A smaller number ofmethods have explored tri-subject kinship verification [176],which compares a couple (Mother and Father) against achild, or whole family classification [64], as well as kinshiprecognition [96]. The seminal work on kinship verificationis by Fang et al. [65]. The first kinship annotated dataset isintroduced and facial parts (FP), such as mouth hair andnose, are extracted using a pictorial structure model [68].That is, the facial structures are represented as parts in adeformable configuration. The representation also includedcolor information, facial distances (FD) and gradient his-tograms (GH). The difference between the feature vectorsof the query couple are classified using methods like k-NNand SVM. The experiments performed in the paper indicatethat the most discriminant feature was the color of the eyes.Moreover, the highest accuracy was achieved for Father-Sonimage-pairs, while the algorithm outperformed the humanworkers. Such results suggest that kinship verification ofother people is a difficult task for humans. Similar to theabove, this section is organized based on the facial repre-sentation used for the task.167.1 Invariant descriptors to genetic variationsIntuitively, people perceive kinship based on local facialattributes. These attributes often correspond to facial parts,like nose, eyes and mouth, the heredity of which indicateskin-related people. Different descriptors have been appliedto capture facial part information for kinship verification.The eyes, nose and mouth are explored in [93], where theDAISY [215] descriptor is used to represent each part. Asimilarity score is then computed for each part of the twofaces to determine the familial traits. Similarly, 12 facialparts are explored in [64] using the dense SIFT (dSIFT)descriptor. In this approach, each part is reconstructed froma dictionary of facial parts. The dictionary contains facialparts from the family of the query face, as well as non-related faces. In order to determine the family of a test face,sparsity is enforced on the reconstruction coefficients, basedon the assumption that the query face only inherits featuresfrom its relatives. The final family prediction is obtainedbased on the three most dominant facial attributes.A set of overlapping facial patches is used for featureextraction in [237]. The face is segmented in 5 layers fromcoarse to fine; that is, the first layer contains the whole faceand the other layers contain increasingly smaller parts. Bi-nary attributes that indicate the presence or absence of a trait(e.g., mustache), as well as relative attributes (e.g., biggernose) are taken into consideration. The LIFT algorithm [255]is employed to extract the binary attributes, while a rankingfunction is learned for the relative ones. Finally, the outputis obtained from an SVM classifier.Instead of modeling different facial parts, several meth-ods employ texture descriptors that are widely adoptedin face verification. In particular, Local Phase Quantization(LPQ) [166], Three and Four-Patch LBPs and Weber LocalDescriptors (WLD) [35] are employed to represent the facein [19]. The fusion of multiple features results in improvedclassification accuracy. Feature selection is employed to ob-tain the final representation and the classification problemis solved using SVM. Similarly, the Weber-normalized [35]faces are obtained in [114] to alleviate the illumination vari-ation in the data. The features are detected around salientpoints, that are extracted using Difference of Gaussian. Asimilarity vector is then computed around each pair ofsalient points by employing the Self-Similarity Descriptor(SSD) [195]. The distance measure is then classified by anSVM classifier.On the other hand, in order to perform tri-subject kin-ship verification, each image is partitioned into overlappingpatches and SIFT features are extracted in [176]. Featureselection is then performed and Symmetric Bilinear Model(SBM) is used to learn the similarity between the parentsand the child, by learning the pairwise similarities first.The verification function is modelled as the linear com-bination of the two similarities. Calculating the tri-subjectsimilarity in two steps can induce noise to the model, sincethe tri-person inheritance is not considered. Based on that,Zhang et al. [253] introduced a model to compute tri-subjectYearPaperRepresentationMethodDataset(score)Metric172007[130]GOPSVMPrivate I(5.1%)Private II(10.8%)2008[16]2010[131]SIFTGOPFeature DriftFG-NET(N/A)SVMFG-NET(≥ 18)(24.1%)FG-NET(8-18)(30.5%)FG-NET(≤ 8)(38.6%)Private I(5.1%)Private II(10.8%)FG-NET(47.5%)MORPH(83.9%)FG-NET(69%)MORPH(91.14%)2011[129]SIFT, MLBPMFDA2013[83]HOGPCA, LDA, IFA2014[31], [32]CARCPCA,LDA, SVM MORPH(92.8%)2015[84]MEFD, MLBP, SIFTPCA, LDA, IFA2015[20]LBPNTCA2015[52]SIFT, LBP, GOP, BIF AGCD20162016[128][228]LPSPILFDA, HFALF-CNNCACD(87.6%)FG-NET(76.2%)MORPH(94.59%)FG-NET(48.96%)MORPH(83.8%)FG-NET(19.4%)MORPH(5.5%)MORPH(94.87%)FG-NET(88.1%)MORPH(97.51%)CACD(98.5%)EEREERROCEEREEREEREEREERacc.(rank=1)acc.(rank=1)acc.(rank=1)acc.(rank=1)acc.(rank=1)acc.(rank=1)acc.(rank=1)acc.(rank=1)acc.(rank=1)acc.(rank=1)EEREERacc.(rank=1)acc.(rank=1)acc.(rank=1)acc.(rank=1)TABLE 9: Overview of age-invariant facial characterization methodsdissimilarity. An over-complete set of features from HighDimensional LBP Histograms (HDLBPH) is employed asfacial representation. The experimental results indicate theeffectiveness of modeling tri-subject kinship jointly insteadof using two bi-subject models.7.2 Subspace learning-based methodsThe methods in this subsection focus on learning a kinship-invariant subspace by making use of tools like factor anal-ysis and transfer learning. The aforementioned descriptorscan be used as local descriptors to build the subspace. Inparticular, LBP, LPQ and SIFT descriptors are used in [53].Based on the hypothesis that, symmetry information cancapture non kinship variations like pose and illumination,the symmetry feature is employed as the kinship unrelatedpart of an image. The kinship unrelated part is then sub-tracted from the original image and a classifier performsverification. The experimental results indicate that LPQperform better than the other descriptors on the KinFaceW-II dataset, while LBP perform better on the KinFaceW-Idataset.Midlevel features are employed in [244], where LBPand SIFT descriptors are used as low-level image repre-sentations. The final subspace is obtained using PrototypeDiscriminative Feature Learning (PDFL). For this method,two datasets are used, one with labelled kinship and onewithout. To obtain the subspace, the objective functionminimizes the difference between kin-related samples andmaximizes the difference between neighbouring non-kinsamples. An SVM classifier is used for verification, whilethe experimental results indicate improved performancecompared to the state-of-the-art. Nevertheless, the modelfails to outperform the human observer for some kinds ofkinship.Based on the assumption that parents resemble theirchildren more closely when they are younger compared towhen they are older, transfer subspace tools are used in[238] and [194] to determine kinship in photos. In particular,the divergence between the distributions of children andold parents is minimized, using the young parents as anintermediate set. Gabor features and distance ratios from theanthropometic model [177] are used as descriptors and theresulting subspace maximizes the child-young parent andchild-old parent similarities. Experiments on context-awarekinship verification are performed and are presented in thesubsection 7.7.7.3 Metric learning-based methodsMetric Learning methods [239] aim at learning a distancemetric that gets similar samples closer than dissimilar ones.A number of methods have successfully used the tools ofmetric learning to perform kinship verification using differ-ent facial features. In particular, Ensemble Metric Learningwith sample and feature selection is employed in [203] tocreate a mapping that reduces the distance between imagesof related people. The local representations include Spatialhistogram of SIFT (SH-SIFT), as well as a vector containingintensity, Pyramid Histogram of Gradients (PHOG) [17]and Gabor wavelet at four scales and six orientations. Thefinal decision is obtained using SVR. The experiments re-veal the implications of age difference and gender on theperformance of the kinship verification model, which arediscussed in the next subsection.In order to learn a metric that not only projects kin-related faces close but also pulls unrelated ones out of theirneighbourhood, Neighbourhood Repulsed Metric Learning(NRML) is introduced in [149]. Further to that, MultiviewNRML is proposed to obtain metrics for multiple featurerepresentations and to deal with multiview data. Four dif-ferent descriptors are used, namely LBP, SIFT, TPLBP andLE [26]. The last one outperforms the other descriptors inthe experiments, due to the fact that it is directly learnedfrom the data. The method is extended to handle peri-oculal images in [172]. The block-based NRML (BNRML)uses the histograms from the block pairs to learn multipledistance metrics. The method uses LTP [211] features andoutperforms the original NRML method on the KinFaceWbenchmarks.Multiple distance metrics for different sets of featuresare learned in [243] and Discriminative Multi-Metric Learn-ing (DMML) is introduced. Three different descriptors areemployed, namely LBP, SIFT and Spatial Pyramid LEarning(SPLE) [261] and the method learns a metric for each ofthem simultaneously. This method [243] performs better formultiple feature metric learning, as well as single metriclearning, while the SPLE appears to be the best descriptorfor the task. In both methods, kinship is decided using anSVM classifier.Multiple feature representations are also employed in[102] and [101]. Instead of learning a distance metric withconcatenated feature vectors, Large Margin Multi-MetricLearning (LM3L) [102], [101] is introduced and multipledistance metrics are learned jointly. In that way, more dis-criminative and complementary information is exploited, asthe correlation of the different representations is maximized.Furthermore, to better exploit the local manifold structure ofimages, Local Large Margin Multi-Metric Learning (L2M3L)[101] is introduced to incorporate Local Metric Learningwith LM3L.Learning a Mahalanobis distance metric is equivalent tofinding a linear transformation that projects the samples to asubspace, where the Euclidean distance of the similar sam-ples is smaller than the dissimilar ones. Similarly, a lineartransformation that minimizes the correlation, instead of theeuclidean distance, is learned in [241]. In order to learn a setof non-linear transformations, Discriminative Deep MetricLearning (DDML) is introduced in [146]. This method usesneural networks to project the faces to a discriminativesubspace and is also extended to Descriminative Deep MultiMetric Learning (DDMML) for multiple features. The ex-perimental results show that deep architectures that employmultiple features perform better for the tasks of face andkinship verification. Subsection 7.4 features more on deephierarchical representations for kinship verification.7.4 Deep learning-based representationsSimilar to the aforementioned tasks in this survey, DeepLearning methodologies have shown some of the mostpromising results for kinship verification. The widely usedarchitectures, e.g., VGG16, have been used for this task [183]to exploit large datasets like the Families In the Wild dataset.More sophisticated deep learning methods introduced forkinship verification are described as follows.A deep autoencoder is used in [46] to encode the re-lationship between the faces in two images. Instead ofencoding each image separately, a relational model that usestwo images as input is introduced. The gated autoencodermodel outperforms most metric learning techniques, as it18learns the features and the metrics jointly. The output is astatistic about relatedness and resemblance, which is usedto perform kinship verification. The experimental results in-dicate the genetic inheritance of facial features in the Family101 dataset. In particular, sons appear to resemble more theirfathers, while the opposite stands for the daughters.One of the latest works in kinship verification is thefiltered contractive Deep Belief Network (fcDBN) proposedin [115]. A DBN is a deep learning method that consistsof multiple stacked Restricted Boltzman Machines (RBM)[100]. The deep representations are obtained by applyingfilters to capture the inherent structure of the images. Inorder to train the fcDBN model a large number of imagesfrom multiple datasets are used, a strategy that had notbeen previously employed for this task before. Kinshipverification is performed by a 3-layer Neural Network andthe method succeeds in outperforming the state-of-the-artmethods in multiple datasets.7.5 Other representationsAdditionally to the above, other approaches have beenintroduced for the task of Kinship verification. Inheritablegenetic transformation is modelled in [140] and [175]. Inparticular, Fisher Vector [201] in Opponent Colour Space areused in [175] as local representations. The inheritable infor-mation is captured by learning a common transformation onthe representations of child and parent. On the other hand,the SIFT flow algorithm [133] is employed in [175] and theinheritable transformation is similarly learned so that thekin-related representations are as close as possible.Instead of only applying texture descriptors, Geometricinformation is also incorporated in [226]. Facial landmarksare used and the representation is obtained using the meth-ods of the Grassmann manifold. In addition, appearancefeatures are extracted by using LBP on a Gaussian im-age pyramid and are fused with the geometric features.A Gaussian Mixture Model (GMM) is used for similarityfeature extraction from the appearance features. Verificationis achieved using an SVM and the experiments indicatethat even though geometric information is not effective onits own for kinship verification, it significantly boosts theperformance when fused with appearance information.Among other modalities, contextual and dynamic fea-tures have been studied for kinship verification. In par-ticular, kinship verification in a photo using prior contextknowledge is studied in [238]. The contextual informationincluded captures gender relation, age difference, relativedistance and kinship score. The experiments show thatcontextual information is useful for kinship verification.Lastly, dynamic features from smiles are employed studiedin [47]. The dynamic features and are used along withCompleted LBP from Three Orthogonal Patterns (CLBP-TOP). The experimental results indicate that smile dynamicsare not sufficiently discriminative when used individually,but like geometric features, they can increase the accuracyof kinship verification in combination with spatio-temporalfeatures. Interestingly, the performance of the modelishigher for spontaneous smiles rather than posed ones.7.6 Evaluation protocols, metrics, and results for kin-ship verificationProtocols: The most widely adopted Evaluation Protocolfor Kinship Verification is k-fold cross-validation, partic-ularly 5-fold cross-validation (5-fold c-v). This protocol isused for most of the datasets in this subsection. Specificallyfor the Vadana dataset, a different protocol is proposed.The dataset is split into 6 subsets, 3 including parent-child(VADANA-PC) and 3 including sibling (VADANA-S). Foreach relationship, two subsets included only adult faces andone both adult and child faces. Lastly, the aforementionedleave-one-out protocol protocol is used for UvA-NEMOdataset.Metrics: The common evaluation metric for the task ofkinship verification is the classification accuracy (acc.). Theresults are usually obtained for every subset, indicating adifferent kinship relation, as well as on the whole dataset.In cases where the accuracy is obtained on more than oneset, instead of the average, we include the best and theworst scores reported. Aforementioned metrics, like EER,are also applied to kinship verification. The metric used inthe VADANA protocol is the accuracy at EER (AEER).Results: The best reported results for each dataset, alongwith the evaluation protocols are tabulated in Table 10. Themethods in this subsection are presented in chronologicalorder in Table 11.TABLE 10: Best reported results on Kinship Verification (theresults in [96] are for Kinship Recognition)datasetmethod metricprotocolscoreTSKinFaceCornellKinFIWWVU KInshipVADANAKinFaceW-IKinFaceW-IIFamily 101UvA-NEMOSibling-FaceGroup-FaceUBKinFaceIIITD-Kinship[253][115][183][115][84][115][115][226][47][96][96][115][114]acc.acc.acc.acc.AEERacc.acc.acc.acc.acc.acc.acc.acc.5-fold c-v5-fold c-v5-fold c-v5-fold c-vvadana5-fold c-v5-fold c-v5-fold c-vleave-one-outN/AN/A5-fold c-v5-fold c-v89.8%89.5%71% ±2.390.8%60.43-80.18%96.1%96.2%92.03%67.11%52.48%69.25%91.8%75.2%7.7 Discussion on kinship verificationThe performance of kinship verification models can beaffected by a number of factors. Some of the factors thatcan deteriorate the performance include pose, image quality[18], lighting conditions [176], as well as gender and agedifference between kin [203]. Experiments are conducted inthe cited studies to define the effect of each factor.In order to determine the effectiveness of the machinein kinship verification, a study on kinship verification byhumans is performed in [149]. The results indicate thathumans perform slightly worse than machines, while con-textual information such as hair, colour and backgroundcan improve their accuracy. Moreover, a study of the sev-eral factors affecting human accuracy at verifying kin [115]indicates that women outperform men, while older peoplecan distinguish kin better. The kin relationships containingfemales appear also to be detected more accurately, which19may be attributed occlusions like beard or mustache. Thestudy also explains how much kin-related information iscaptured in different face patches.7.8 Challenges in the wildWith the oldest benchmark dating back to 2010, kinshipverification is a field in its infancy. As mentioned in Section3, the majority of methods in this subsection is evaluated onin the wild data. Nevertheless, Table 10 indicates significantdifferences between the state-of-the-art performance on eachdataset (e.g., 96.2% on KinFaceW II and 71% on FIW). Thesedifferences can be interpreted by the bias in experimentalset ups inflicted by gathering images from the same photo[143]. Additionally, the laborious nature of gathering theseimages has resulted in small datasets that do not containextreme variation with regards to pose, occlusions or imagequality. On the other hand, the much larger FIW datasetdemonstrates larger variation in pose, image quality as wellas age of the subjects. The problem of kinship verificationin the wild has been tackled in two competitions [147],[145].The second is based on the KinFaceW datasets while a newdataset was collected for the first one. The difference in theresults indicates that the KinFaceW datasets are indeed lesschallenging.8 AGING AND KINSHIPThe fact that both aging and kinship are genetically encoded[22] indicates an inherent synergy between the two. Thissynergy is used in [199] to perform kinship guided ageprogression. The parent face serves as a prior to predictthe aging of the child. The kinship information is incor-porated by morphing the age-progressed face to the oneof the parent. On the other hand, old and young parentface images are used in [235]. The correlation between theaging of the parent face and their children is leveraged by atransfer learning model from the young parent-child to theold parent-child domain.Besides the possible synergy between them, aging andkinship modeling face similar issues when it comes todealing with faces in the wild. Variations like illumina-tion, pose, expression and image quality deteriorate theaccuracy of such systems. The methods in the previoussections apply classical face analysis descriptors, e.g., LBP,SIFT, HOG, as well as state-of-the-art deep CNN to attackthese problems. Furthermore, aging and kinship modelingshare a number of protocols. In particular, cross-age faceverification and kinship verification systems aim to learn asimilarity metric and similar methods can be applied. Thisapproach is adopted in [149], where the task of cross-ageface verification is tackled as ’self-kinship verification’ usingmetric learning.9 CONCLUSIONSMotivated by the increasing interest and plethora of realworld applications, the state-of-the-art methods in age andkinship modeling have been surveyed. The main challengesand results of the methods are described for each taskindividually. The results tabulated in Tables 4, 7, 9 and11 indicate the superiority of deep learning methods in20YearPaperRepresentationMethod2010[65]FP, FD, Colour, GHkNN, SVM2011[194]Gabor20122012201220122012[93][237][114][238][203]DAISYbinary, relative attributesDoG salient pointsGaborPHOG, Gabor, SH-SIFTCMC,TSL, kNNBayes decisionSVMSSD, SVMCMC, TSL, kNNMeric Learning, SVR20132013[64][47]dSIFTdynamic, CLBP-TOPSparse Group LassomRMR, SVM2014[149]LBP, SIFT, TPLBP and LEMNRLM, SVM2014[243]LBP, SIFT, SPLEDMML, SVM2014201420142014[102][46][226][96]LE, LBP, TPLBP, SIFTPILMMMLgated autoencoderLBP, Grassman ManifoldLBP,BIF, relative ageGMM, geodesic distance, SVMlogistic regression, graph classifier2015[19]LPQ, TPLBP, FPLBP, WLD mRMR, SFFS, SVM20152015[176][53]SIFTLPQRSBMPCA, feature substraction2015[244]LBP, SIFTPDFL, SVMDataset(score)CornellKin(70.67%)UBKinFace(56.5%)private(75%)UBKinFace(82.5%)IITD(75.2%)UBKinFace(56.5%)VADANA-PC(80.18%)VADANA-S(75.64%)Family101(32%)UvA-NEMO(67.11%)KinFaceW-I(69.9%)KinFaceW-II(76.5%)KinFaceW-I(69.5-75.5%)KinFaceW-II(76.5-79.5%)CornellKin(70.5-77.5%)UBKinFace(70-74.5%)KinFaceW-II(78.7%)KinFaceW-I(74.5%)KinFaceW-II(82.5%)Family101(92.03%)Custom(52.48%)GroupFace(69.25%)KinFaceW-I(86.3%)KinFaceW-II(83.1%)TSKinFace(85.4%)KinFaceW-I(70.9%)KinFaceW-II(77.1%)KinFaceW-I(70.1%)KinFaceW-II(77%)CornellKin(71.9%)UBKinFace(67.3%)2016[253]HDLBPdissimilarity vector, mRMRTSKinFace(89.7%)20172017[101][146]LE, LBP, TPLBP, SIFTLBP,DSIFT, HOG, LPQLLMMMLDDMML2017[241]LE2017[172]LTP20172017[184][115]PIPINRCMLBNRMLVGGfcDBN,3-l NNKinFaceW-II(80%)KinFaceW-I(83.5%)KinFaceW-II(84.3%)TSKinFace(82.5-88.5%)KinFaceW-I(66.3%)KinFaceW-II(78.8%)KinFaceW-I(78.7%)KinFaceW-II(80.55%)FIW(71%±2.3)KinFaceW-I(96.1%)KinFaceW-II(96.2%)CornellKin(89.5%)UBKinFace(91.8%)WVU(90.8%)Metricacc.acc.acc.acc.acc.acc.AEERAEERacc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.acc.TABLE 11: Overview of Kinship Verification Methodsmodeling the non-linear transformations of aging and kin-ship. Specifically, the discriminative power of deep learningrepresentations, in addition to the availability of in the wildimage datasets, have produced results that surpass humanabilities in the tasks of age estimation, age-invariant facialcharacterization and kinship verification. On the other hand,the application of deep generative models (e.g., GANs) forthe task of age progression has produced photorealistic age-progressed images with impressive high-frequency details.Nevertheless, deep learning (DL) methods demonstratea number of limitations that hinder the effective use ofsuch methods for modeling complex transformations likeaging and kinship. To approximate such functions DL meth-ods use hierarchical models that need sufficiently densesampling from the data distribution, that is, they are datahungry. The resulting models are highly non-linear andcomplex, creating systems that are not mathematically trans-parent, while being difficult to train with no guarantee ofconvergence. This deems DL methods unsuitable for criticaltasks due to their lack of interpretability and sensitivity toadversarial examples [86].To further advance the fields of aging and kinshipmodeling, the community should focus on diversifying themodalities of the data, since the vast majority of researchfocuses on 2-Dimensional data. Although some papers in-dicate the possibility of using dynamic representation forthese tasks (e.g., [47], [48]), the lack of labelled videoscaptured in the wild delays progress in this problem. Lastly,the lack of large in the wild datasets with multiple annota-tions, i.e. both age and kinship, does not allow for researchon the correlation between age and kinship. Since bothkinship and aging transformations are genetically encoded,the inheritance of aging patterns can be investigated. Suchdatasets will also reveal the possibility of incorporating ageor kinship as soft-biometrics to boost the accuracy of facerecognition systems (e.g., in [115]) as well as investigate thesynergies between the two.REFERENCES[1][2][3][4][5][6][7][8][9]LHI Image Database. 2010.A. F. Abate, M. Nappi, D. Riccio, and G. Sabatino. 2d and 3d facerecognition: A survey. Pattern recognition letters, 28(14):1885–1906,2007.E. Agustsson, R. Timofte, S. Escalera, X. Baro, I. Guyon, andR. Rothe. Apparent and real age estimation in still imagesIn IEEEwith deep residual regressors on appa-real database.International Conference on Automatic Face & Gesture Recognition(FG), 2017.T. Ahonen, A. Hadid, and M. Pietikainen. Face description withlocal binary patterns: Application to face recognition. IEEE trans-actions on pattern analysis and machine intelligence, 28(12):2037–2041, 2006.R. A. Alley. Social and Applied Aspects of Perceiving Faces. Erlbaum,Hillsdale, NJ, 1988.F. Alnajar, T. Gevers, and S. Karaoglu. Age estimation underIn IEEEchanges in image quality: An experimental study.International Conference on Image Processing (ICIP), pages 3987–3991, Sept 2015.G. Antipov, M. Baccouche, S.-A. Berrani, and J.-L. Dugelay. Ap-parent age estimation from face images combining general andIn IEEE Conferencechildren-specialized deep learning models.on Computer Vision and Pattern Recognition Workshops (CVPR-W),2016.G. Antipov, M. Baccouche, and J.-L. Dugelay. Face aging withIEEE Internationalconditional generative adversarial networks.Conference on Image Proccessing (ICIP), 2017.A. Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall. Learningdistance functions using equivalence relations. In Proceedings ofthe 20th International Conference on Machine Learning (ICML), pages11–18, 2003.[10] A. Bastanfard, M. A. Nik, and M. M. Dehshibi.Iranian facedatabase with age, pose and expression. In Machine Vision, 2007.ICMV 2007. International Conference on, pages 50–55. IEEE, 2007.[11] C. Bauckhage, A. Jahanbekam, and C. Thurau. Age recognition inthe wild. In International Conference on Pattern Recognition (ICPR),pages 392–395, 2010.[12] H. Bay, T. Tuytelaars, and L. Van Gool. Surf: Speeded up robustfeatures. European Conference onComputer vision (ECCV), pages404–417, 2006.[13] M. Bereta, P. Karczmarek, W. Pedrycz, and M. Reformat. Localdescriptors in application to the aging problem in face recogni-tion. Pattern Recognition, 46(10):2634–2646, 2013.[14] B. Bhattarai, G. Sharma, A. Lechervy, and F. Jurie. A joint learningapproach for cross domain age estimation. In IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP),pages 1901–1905, March 2016.[15] E. L. Bienenstock, L. N. Cooper, and P. W. Munro. Theory forthe development of neuron selectivity: orientation specificity andbinocular interaction in visual cortex. Technical report, DTICDocument, 1981.S. Biswas, G. Aggarwal, N. Ramanathan, and R. Chellappa. Anon-generative approach for face recognition across aging.InIEEE International Conference on Biometrics: Theory, Applications andSystems (BTAS), pages 1–6. IEEE, 2008.[16][17] A. Bosch, A. Zisserman, and X. Munoz. Representing shapeIn Proceedings of the 6th ACMwith a spatial pyramid kernel.international conference on Image and video retrieval, pages 401–408.ACM, 2007.[18] A. G. Bottino, M. De Simone, A. Laurentini, and T. Vieira. A newproblem in face image analysis: finding kinship clues for siblingspairs. 2012.[19] A. Bottinok, I. U. Islam, and T. F. Vieira. A multi-perspectiveholistic approach to kinship verification in the wild. In IEEE In-ternational Conference and Workshops on Automatic Face and GestureRecognition (FG), volume 2, pages 1–6. IEEE, 2015.[20] D. Bouchaffra. Nonlinear topological component analysis: Ap-plication to age-invariant face recognition. IEEE Transactions onNeural Networks and Learning Systems, 26(7):1375–1387, July 2015.[21] L. Breiman. Bagging predictors. Machine learning, 24(2):123–140,1996.21[22] W. S. Browner, A. J. Kahn, E. Ziv, A. P. Reiner, J. Oshima, R. M.Cawthon, W.-C. Hsueh, and S. R. Cummings. The genetics ofhuman longevity. The American journal of medicine, 117(11):851–860, 2004.[23] V. Bruce and A. Young. In the eye of the beholder: the science of faceperception. Oxford University Press, 1998.[24] D. M. Burt and D. I. Perrett. Perception of age in adult caucasianmale faces: Computer graphic manipulation of shape and colourinformation. Proceedings of the Royal Society of London B: BiologicalSciences, 259(1355):137–143, 1995.[25] D. Cai, X. He, J. Han, and H.-J. Zhang. Orthogonal laplacian-faces for face recognition. IEEE transactions on image processing,15(11):3608–3614, 2006.[26] Z. Cao, Q. Yin, X. Tang, and J. Sun. Face recognition withlearning-based descriptor. In IEEE Conference on Computer Visionand Pattern Recognition (CVPR), pages 2707–2714. IEEE, 2010.[27] K. Y. Chang and C. S. Chen. A learning framework for age rankestimation based on face images with scattering transform. IEEETransactions on Image Processing, 24(3):785–798, March 2015.[28] K. Y. Chang, C. S. Chen, and Y. P. Hung. A ranking approachfor human ages estimation based on face images. In InternationalConference on Pattern Recognition (ICPR), pages 3396–3399, 2010.[29] K. Y. Chang, C. S. Chen, and Y. P. Hung. Ordinal hyperplanesIn IEEE Con-ranker with cost sensitivities for age estimation.ference on Computer Vision and Pattern Recognition (CVPR), pages585–592, 2011.[30] W.-L. Chao, J.-Z. Liu, and J.-J. Ding. Facial age estimation basedon label-sensitive learning and age-oriented regression. PatternRecognition, 46(3):628–641, 2013.[31] B.-C. Chen, C.-S. Chen, and W. H. Hsu. Cross-age reference cod-ing for age-invariant face recognition and retrieval. In EuropeanConference on Computer Vision (ECCV), 2014.[32] B. C. Chen, C. S. Chen, and W. H. Hsu. Face recognition andretrieval using cross-age reference coding with cross-age celebritydataset. IEEE Transactions on Multimedia, 17(6):804–815, June 2015.[33] C. Chen, W. Yang, Y. Wang, K. Ricanek, and K. Luu. Facial featureIn IEEE Inter-fusion and model selection for age estimation.national Conference and Workshops on Automatic Face and GestureRecognition (FG), pages 200–205, March 2011.[35][34] H. F. Chen, P. N. Belhumeur, and D. W. Jacobs. In search of illu-mination invariants. In Computer Vision and Pattern Recognition,2000. Proceedings. IEEE Conference on, volume 1, pages 254–261.IEEE, 2000.J. Chen, S. Shan, C. He, G. Zhao, M. Pietikainen, X. Chen, andW. Gao. Wld: A robust local image descriptor. IEEE transactionson pattern analysis and machine intelligence, 32(9):1705–1720, 2010.J.-C. Chen, V. M. Patel, and R. Chellappa. Unconstrained faceverification using deep cnn features. In IEEE Winter Conference onApplications of Computer Vision (WACV), pages 1–9. IEEE, 2016.[36][37] K. Chen, S. Gong, T. Xiang, and C. C. Loy. Cumulative attributespace for age and crowd density estimation. In IEEE Conferenceon Computer Vision and Pattern Recognition, pages 2467–2474, June2013.S. Chen, C. Zhang, M. Dong, J. Le, and M. Rao. Using ranking-cnn for age estimation.[38][39] Y. L. Chen and C. T. Hsu. Subspace learning for facial age esti-mation via pairwise age ranking. IEEE Transactions on InformationForensics and Security, 8(12):2164–2176, Dec 2013.[40] T. F. Cootes, G. J. Edwards, and C. J. Taylor. Active appearancemodels. IEEE Transactions on pattern analysis and machine intelli-gence, 23(6):681–685, 2001.[41] T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham. Activeshape models-their training and application. Computer vision andimage understanding, 61(1):38–59, 1995.[42] D. Cristinacce and T. F. Cootes. Feature detection and trackingwith constrained local models. In BMVC, volume 1, page 3, 2006.[43] D. I. P. D. Michael Burt. Perception of age in adult caucasianmale faces: Computer graphic manipulation of shape and colourinformation. Proceedings: Biological Sciences, 259(1355):137–143,1995.[44] N. Dalal and B. Triggs. Histograms of oriented gradients forhuman detection. In Computer Vision and Pattern Recognition, 2005.CVPR 2005. IEEE Computer Society Conference on, volume 1, pages886–893. IEEE, 2005.[45] A. Dantcheva, P. Elia, and A. Ross. What else does your biometricIEEE Transactions ondata reveal? a survey on soft biometrics.Information Forensics and Security, 11(3):441–467, 2016.[46] A. Dehghan, E. G. Ortiz, R. Villegas, and M. Shah. Who do ilook like? determining parent-offspring resemblance via gatedautoencoders. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition (CVPR), pages 1757–1764, 2014.[47] H. Dibeklioglu, A. Ali Salah, and T. Gevers. Like father, likeson: Facial expression dynamics for kinship verification. In IEEEInternational Conference on Computer Vision (ICCV), pages 1497–1504, 2013.[48] H. Dibeklio ˘glu, F. Alnajar, A. A. Salah, and T. Gevers. Combin-IEEEing facial dynamics with appearance for age estimation.Transactions on Image Processing, 24(6):1928–1943, 2015.[49] H. Dibeklio ˘glu, A. Salah, and T. Gevers. Are you really smilingat me? spontaneous versus posed enjoyment smiles. EuropeanConference on Computer Vision (ECCV), pages 525–538, 2012.[50] C. Ding and D. Tao. A comprehensive survey on pose-invariantface recognition. ACM Transactions on intelligent systems andtechnology (TIST), 7(3):37, 2016.[51] R. O. Dingman and P. Natvig. Surgery of facial fractures. Saunders,1964.[52] L. Du and H. Ling. Cross-age face verification by coordinatingwith cross-face age verification. In IEEE Conference on ComputerVision and Pattern Recognition (CVPR), pages 2329–2338, June2015.[53] X. Duan and Z.-H. Tan. A feature subtraction method for imagebased kinship verification under uncontrolled environments. InIEEE International Conference on Image Processing (ICIP), pages1573–1577. IEEE, 2015.[54] C. N. Duong, K. Luu, K. G. Quach, and T. D. Bui. Beyond prin-cipal components: Deep boltzmann machines for face modeling.In 2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 4786–4794, June 2015.[55] N. C. Ebner, M. Riediger, and U. Lindenberger.FACES–Adatabase of facial expressions in young, middle-aged, and olderwomen and men: Development and validation. Behavior researchmethods, 42(1):351–362, 2010.[56] A. Edelman, T. A. Arias, and S. T. Smith. The geometry of al-gorithms with orthogonality constraints. SIAM journal on MatrixAnalysis and Applications, 20(2):303–353, 1998.[57] E. Eidinger, R. Enbar, and T. Hassner. Age and gender estimationof unfiltered faces. IEEE Transactions on Information Forensics andSecurity, 9(12):2170–2179, Dec 2014.[58] P. Ekman and W. V. Friesen. Facial action coding system. 1977.[59]S. Escalera, J. Fabian, P. Pardo, X. Bar, J. Gonzlez, H. J. Escalante,D. Misevic, U. Steiner, and I. Guyon. Chalearn looking at peo-ple 2015: Apparent age and cultural event recognition datasetsIn IEEE International Conference on Computer Visionand results.Workshop (ICCV- W), pages 243–251, Dec 2015.S. Escalera, M. Torres Torres, B. Martinez, X. Baro, H. Jair Es-calante, I. Guyon, G. Tzimiropoulos, C. Corneou, M. Oliu,M. Ali Bagheri, and M. Valstar. Chalearn looking at people andfaces of the world: Face analysis workshop and challenge 2016.In The IEEE Conference on Computer Vision and Pattern Recognition(CVPR) Workshops, June 2016.[60][61] R. V. Exline. Explorations in the process of person perception:Visual interaction in relation to competition, sex, and need foraffiliation. Journal of personality, 31(1):1–20, 1963.[62] N. Fan. Learning nonlinear distance functions using neuralnetwork for regression with application to robust human ageIn IEEE International Conference on Computer Visionestimation.(ICCV), pages 249–254, Nov 2011.[63] H. Fang, P. Grant, and M. Chen. Discriminant feature manifoldfor facial aging estimation. In International Conference on PatternRecognition (ICPR), pages 593–596, Aug 2010.[64] R. Fang, A. C. Gallagher, T. Chen, and A. Loui. Kinship classifi-cation by modeling facial feature heredity. In IEEE InternationalConference on Image Processing (ICIP), pages 2983–2987. IEEE,2013.[65] R. Fang, K. D. Tang, N. Snavely, and T. Chen. Towards com-In IEEE Internationalputational models of kinship verification.Conference on Image Processing (ICIP), pages 1577–1580. IEEE,2010.[66] L. G. Farkas. Anthropometry of the Head and Face. Raven Pr, 1994.[67] E. Fazl-Ersi, M. E. Mousa-Pasandi, R. Laganire, and M. Awad.Age and gender recognition using informative features of varioustypes. In IEEE International Conference on Image Processing (ICIP),pages 5891–5895, Oct 2014.22[68] P. F. Felzenszwalb and D. P. Huttenlocher. Pictorial structuresInternational journal of computer vision,for object recognition.61(1):55–79, 2005.S. J. Foundation. Human and object interaction processing (hoip)face database. 2014.[69][70] W. A. Freiwald, D. Y. Tsao, and M. S. Livingstone. A facefeature space in the macaque temporal lobe. Nature neuroscience,12(9):1187–1196, 2009.[71] Y. Freund and R. E. Schapire. A desicion-theoretic generalizationof on-line learning and an application to boosting. In Europeanconference on computational learning theory, pages 23–37. Springer,1995.S. Fu, H. He, and Z.-G. Hou. Learning race from face: A sur-vey. IEEE transactions on pattern analysis and machine intelligence,36(12):2483–2509, 2014.[72][73] Y. Fu, G. Guo, and T. S. Huang. Age synthesis and estimation viafaces: A survey. IEEE Transactions on Pattern Analysis and MachineIntelligence, 32(11):1955–1976, Nov 2010.[74] Y. Fu and T. S. Huang. Human age estimation with regression ondiscriminative aging manifold. IEEE Transactions on Multimedia,10(4):578–584, June 2008.[75] Y. Fu, M. Liu, and T. S. Huang. Conformal embedding analysisIn IEEEwith local graph modeling on the unit hypersphere.Conference on Computer Vision and Pattern Recognition (CVPR),pages 1–6. IEEE, 2007.[76] Y. Fu and N. Zheng. M-face: An appearance-based photorealisticmodel for multiple facial attributes rendering. IEEE Transactionson Circuits and Systems for Video technology, 16(7):830–842, 2006.[77] A. C. Gallagher and T. Chen. Understanding images of groupsIn IEEE Conference on Computer Vision and Patternof people.Recognition, pages 256–263. IEEE, 2009.[78] F. Gao and H. Ai. Face age classification on consumer imageswith gabor feature and fuzzy lda method. In International Confer-ence on Biometrics (ICB), pages 132–141. Springer, 2009.[79] P. Geladi and B. R. Kowalski. Partial least-squares regression: atutorial. Analytica chimica acta, 185:1–17, 1986.[80] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptivelabel distribution learning. In International Conference on PatternRecognition (ICPR), pages 4465–4470, Aug 2014.[81] X. Geng, C. Yin, and Z. H. Zhou. Facial age estimation by learningfrom label distributions. IEEE Transactions on Pattern Analysis andMachine Intelligence, 35(10):2401–2412, Oct 2013.[82] X. Geng, Z. H. Zhou, and K. Smith-Miles. Automatic ageIEEE Transactions onestimation based on facial aging patterns.Pattern Analysis and Machine Intelligence, 29(12):2234–2240, Dec2007.[83] D. Gong, Z. Li, D. Lin, J. Liu, and X. Tang. Hidden factor analysisfor age invariant face recognition. In IEEE International Conferenceon Computer Vision (ICCV), pages 2872–2879, Dec 2013.[85][84] D. Gong, Z. Li, D. Tao, J. Liu, and X. Li. A maximum entropyfeature descriptor for age invariant face recognition. In 2015 IEEEConference on Computer Vision and Pattern Recognition (CVPR),pages 5289–5297, June 2015.I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adver-In Advances in neural information processing systemssarial nets.(NIPS), pages 2672–2680, 2014.I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harness-ing adversarial examples. 2015.[86][87] A. Graves. Supervised sequence labelling. In Supervised SequenceLabelling with Recurrent Neural Networks, pages 5–13. Springer,2012.[88] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based humanage estimation by manifold learning and locally adjusted robustregression. IEEE Transactions on Image Processing, 17(7):1178–1188,2008.[89] G. Guo and G. Mu. Human age estimation: What is the influenceacross race and gender? In IEEE Conference on Computer Visionand Pattern Recognition Workshop (CVPR- W), pages 71–78, June2010.[90] G. Guo and G. Mu. Simultaneous dimensionality reduction andhuman age estimation via kernel partial least squares regression.In IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 657–664, June 2011.[91] G. Guo, G. Mu, Y. Fu, and T. S. Huang. Human age estimationusing bio-inspired features. In IEEE Conference on Computer Visionand Pattern Recognition (CVPR), pages 112–119, June 2009.[92] G. Guo, G. Mu, and K. Ricanek. Cross-age face recognition ona very large database: The performance versus age intervalsIn Internationaland improvement using soft biometric traits.Conference on Pattern Recognition (ICPR), pages 3392–3395, Aug2010.[93] G. Guo and X. Wang. Kinship measurement on salient facialIEEE Transactions on Instrumentation and Measurement,features.61(8):2322–2325, 2012.[94] G. Guo and X. Wang. A study on human age estimation underfacial expression changes. In IEEE Conference on Computer Visionand Pattern Recognition (CVPR), pages 2547–2553, June 2012.[95] G. Guo and C. Zhang. A study on cross-population age estima-tion. In IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 4257–4263, June 2014.[96] Y. Guo, H. Dibeklioglu, and L. van der Maaten. Graph-based kin-ship recognition. In International Conference on Pattern Recognition(ICPR), pages 4287–4292. Citeseer, 2014.[97] H. Han, C. Otto, X. Liu, and A. K. Jain. Demographic estimationfrom face images: Human vs. machine performance. IEEE Trans-actions on Pattern Analysis and Machine Intelligence, 37(6):1148–1161, June 2015.[98] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning forIn IEEE Conference on Computer Vision andimage recognition.Pattern Recognition (CVPR), pages 770–778, 2016.[99] X. He, S. Yan, Y. Hu, P. Niyogi, and H.-J. Zhang. Face recognitionIEEE transactions on pattern analysis andusing laplacianfaces.machine intelligence, 27(3):328–340, 2005.[100] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learningalgorithm for deep belief nets. Neural computation, 18(7):1527–1554, 2006.[101] J. Hu, J. Lu, Y.-P. Tan, J. Yuan, and J. Zhou. Local large-marginIEEEmulti-metric learning for face and kinship verification.Transactions on Circuits and Systems for Video Technology, 2017.[102] J. Hu, J. Lu, J. Yuan, and Y.-P. Tan. Large margin multi-metricIn Asianlearning for face and kinship verification in the wild.Conference on Computer Vision, pages 252–267. Springer, 2014.[103] Z. Hu, Y. Wen, J. Wang, M. Wang, R. Hong, and S. Yan. FacialIEEE Transactions on Imageage estimation with age difference.Processing, 26(7):3087–3097, 2017.[104] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten.In IEEE ConferenceDensely connected convolutional networks.on Computer Vision and Pattern Recognition (CVPR), 2017.[105] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeledfaces in the wild: A database for studying face recognition inunconstrained environments. Technical report.[106] D. H. Hubel and T. N. Wiesel. Receptive fields, binocularinteraction and functional architecture in the cat’s visual cortex.The Journal of physiology, 160(1):106–154, 1962.[107] R. Jafri and H. R. Arabnia. A survey of face recognition tech-niques. 2009.[108] A. Jain, P. Flynn, and A. A. Ross. Handbook of Biometrics. SpringerScience & Business Media, 2007.[109] A. K. Jain, S. C. Dass, and K. Nandakumar. Soft biometric traitsIn Biometric Authentication,for personal recognition systems.pages 731–738. Springer, 2004.[110] M. Kass, A. Witkin, and D. Terzopoulos. Snakes: Active contourmodels. International journal of computer vision, 1(4):321–331, 1988.[111] I. Kemelmacher-Shlizerman, S. Suwajanakorn, and S. M. Seitz.Illumination-aware age progression. In IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), pages 3334–3341, June2014.[112] D. G. Kendall. Shape manifolds, procrustean metrics, and com-plex projective spaces. Bulletin of the London Mathematical Society,16(2):81–121, 1984.[113] J. Kim, D. Han, S. Sohn, and J. Kim. Facial age estimation viaextended curvature gabor filter. In IEEE International Conferenceon Image Processing (ICIP), pages 1165–1169, Sept 2015.[114] N. Kohli, R. Singh, and M. Vatsa. Self-similarity representationIn IEEE Internationalof weber faces for kinship classification.Conference on Biometrics: Theory, Applications and Systems (BTAS),pages 245–250. IEEE, 2012.[115] N. Kohli, M. Vatsa, R. Singh, A. Noore, and A. Majumdar. Hi-erarchical representation learning for kinship verification. IEEETransactions on Image Processing, 26(1):289–302, Jan 2017.[116] S. Kong, Z. Jiang, and Q. Yang. Modeling neuron selectivityIEEEover simple midlevel features for image classification.Transactions on Image Processing, 24(8):2404–2414, Aug 2015.23[117] Z. Kuang, C. Huang, and W. Zhang. Deeply learned rich codingIn IEEE Internationalfor cross-dataset facial age estimation.Conference on Computer Vision Workshop (ICCV-W), pages 338–343,Dec 2015.[118] Y. H. Kwon et al. Age classification from facial images. In IEEEConference on Computer Vision and Pattern Recognition (CVPR),pages 762–767. IEEE, 1994.[119] A. Lanitis. FG-NET Aging Database. 2002.[120] A. Lanitis. Comparative evaluation of automatic age-progressionmethodologies. EURASIP Journal on Advances in Signal Processing,2008:101, 2008.[121] A. Lanitis and C. J. Taylor. Towards automatic face identificationIn IEEE International Conference androbust to ageing variation.Workshops on Automatic Face and Gesture Recognition (FG), pages391–396. IEEE, 2000.[122] A. Lanitis, C. J. Taylor, and T. F. Cootes. Toward automaticsimulation of aging effects on face images. IEEE Transactions onPattern Analysis and Machine Intelligence, 24(4):442–455, 2002.[123] A. Lanitis, C. J. Taylor, and T. F. Cootes. Toward automaticsimulation of aging effects on face images. IEEE Transactions onPattern Analysis and Machine Intelligence, 24(4):442–455, 2002.[124] A. Lanitis, N. Tsapatsoulis, K. Soteriou, D. Kuwahara, and S. Mor-ishima. Fg2015 age progression evaluation. In IEEE InternationalConference and Workshops on Automatic Face and Gesture Recognition(FG), volume 1, pages 1–6, May 2015.[125] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-basedlearning applied to document recognition. Proceedings of the IEEE,86(11):2278–2324, 1998.[126] C. Li, Q. Liu, W. Dong, X. Zhu, J. Liu, and H. Lu. Humanage estimation based on locality and ordinal information. IEEETransactions on Cybernetics, 45(11):2522–2534, Nov 2015.[127] C. Li, Q. Liu, J. Liu, and H. Lu. Learning ordinal discriminativefeatures for age estimation. In IEEE Conference on Computer Visionand Pattern Recognition (CVPR), pages 2570–2577, June 2012.[128] Z. Li, D. Gong, X. Li, and D. Tao. Aging face recognition: Ahierarchical learning model based on local patterns selection.IEEE Transactions on Image Processing, 25(5):2146–2154, May 2016.[129] Z. Li, U. Park, and A. K. Jain. A discriminative model forIEEE transactions on informationage invariant face recognition.forensics and security, 6(3):1028–1037, 2011.[130] H. Ling, S. Soatto, N. Ramanathan, and D. W. Jacobs. A study offace recognition as people age. In IEEE International Conference onComputer Vision (ICCV), pages 1–8. IEEE, 2007.[131] H. Ling, S. Soatto, N. Ramanathan, and D. W. Jacobs. Faceverification across age progression using discriminative methods.IEEE Transactions on Information Forensics and Security, 5(1):82–91,March 2010.[132] C. Liu and H. Wechsler. Gabor feature based classificationusing the enhanced fisher linear discriminant model for facerecognition. IEEE Transactions on Image processing, 11(4):467–476,2002.[133] C. Liu, J. Yuen, and A. Torralba. Sift flow: Dense correspondenceacross scenes and its applications. In Dense Image Correspondencesfor Computer Vision, pages 15–49. Springer, 2016.[134] H. Liu, J. Lu, J. Feng, and J. Zhou. Group-aware deep featurelearning for facial age estimation. Pattern Recognition, 66:82–94,2017.[135] H. Liu, J. Lu, J. Feng, and J. Zhou. Label-sensitive deep metriclearning for facial age estimation. IEEE Transactions on InformationForensics and Security, 2017.[136] H. Liu, J. Lu, J. Feng, and J. Zhou. Label-sensitive deep metriclearning for facial age estimation. IEEE Transactions on InformationForensics and Security, 13(2):292–305, 2018.[137] H. Liu and X. Sun. Linear canonical correlation analysis basedranking approach for facial age estimation. In IEEE InternationalConference on Image Processing (ICIP), pages 3249–3253, Sept 2016.[138] H. Liu and X. Sun. A partial least squares based ranker for fastand accurate age estimation. In IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pages 2792–2796.IEEE, 2016.[139] M.-Y. Liu and O. Tuzel. Coupled generative adversarial net-works. In Advances in neural information processing systems, pages469–477, 2016.[140] Q. Liu, A. Puthenputhussery, and C. Liu. Inheritable fisher vectorfeature for kinship verification. In IEEE International Conferenceon Biometrics: Theory, Applications and Systems (BTAS), pages 1–6.IEEE, 2015.[141] X. Liu, S. Li, M. Kan, J. Zhang, S. Wu, W. Liu, H. Han, S. Shan,and X. Chen. Agenet: Deeply learned regressor and classifier forrobust apparent age estimation. In IEEE International Conferenceon Computer Vision Workshop (ICCV- W), pages 258–266, Dec 2015.Jointand individual variation explained (jive) for integrated analysisof multiple data types. The annals of applied statistics, 7(1):523,2013.[142] E. F. Lock, K. A. Hoadley, J. S. Marron, and A. B. Nobel.[143] M. B. Lopez, E. Boutellaa, and A. Hadid. Comments on theIEEE Transactions on Patternkinship face in the wild data sets.Analysis and Machine Intelligence, 38(11):2342–2344, Nov 2016.[144] D. G. Lowe. Distinctive image features from scale-invariantInternational journal of computer vision, 60(2):91–110,keypoints.2004.[145] J. Lu, J. Hu, V. E. Liong, X. Zhou, A. Bottino, I. U. Islam, T. F.Vieira, X. Qin, X. Tan, S. Chen, et al. The fg 2015 kinship veri-In IEEE International Conferencefication in the wild evaluation.and Workshops on Automatic Face and Gesture Recognition (FG),volume 1, pages 1–7. IEEE, 2015.[146] J. Lu, J. Hu, and Y.-P. Tan. Discriminative deep metric learningIEEE Transactions on Imagefor face and kinship verification.Processing, 2017.[147] J. Lu, J. Hu, X. Zhou, J. Zhou, M. Castrill ´on-Santana, J. Lorenzo-Navarro, L. Kou, Y. Shang, A. Bottino, and T. F. Vieira. Kinshipverification in the wild: The first kinship verification competition.In Biometrics (IJCB), 2014 IEEE International Joint Conference on,pages 1–6. IEEE, 2014.[148] J. Lu, V. E. Liong, and J. Zhou. Cost-sensitive local binary featureIEEE Transactions on Imagelearning for facial age estimation.Processing, 24(12):5356–5368, Dec 2015.[149] J. Lu, X. Zhou, Y.-P. Tan, Y. Shang, and J. Zhou. Neighborhood re-pulsed metric learning for kinship verification. IEEE transactionson pattern analysis and machine intelligence, 36(2):331–345, 2014.[150] S. Mallat. Group invariant scattering. Communications on Pure andApplied Mathematics, 65(10):1331–1398, 2012.[151] S. G. Mallat. A theory for multiresolution signal decomposition:the wavelet representation. IEEE transactions on pattern analysisand machine intelligence, 11(7):674–693, 1989.[152] A. Maronidis and A. Lanitis. Facial age simulation using age-specific 3d models and recursive pca. In VISAPP (1), pages 663–668, 2013.[153] M. Mathias, R. Benenson, M. Pedersoli, and L. Van Gool. Facedetection without bells and whistles. In European Conference onComputer Vision (ECCV), pages 720–735. Springer, 2014.[154] M. Minear and D. C. Park. A lifespan database of adult facialstimuli. Behavior Research Methods, 36(4):630–633, 2004.[155] M. Mirza and S. Osindero. Conditional generative adversarialnets. arXiv preprint arXiv:1411.1784, 2014.[156] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia,and S. Zafeiriou. Agedb: the first manually collected, in-the-wildage database. In IEEE Conference on Computer Vision and PatternRecognition Workshop (CVPR- W), 2017.[157] E. Moyse. Age estimation from faces and voices: a review.Psychologica Belgica, 54(3), 2014.[158] L. Nanni, A. Lumini, and S. Brahnam. Local binary patterns vari-ants as texture descriptors for medical image analysis. Artificialintelligence in medicine, 49(2):117–125, 2010.[159] C. B. Ng, Y. H. Tay, and B. M. Goi. Vision-based human genderrecognition: A survey. 2012.[160] B. Ni, Z. Song, and S. Yan. Web image mining towards universalage estimator. In Proceedings of the 17th ACM international confer-ence on Multimedia, pages 85–94. ACM, 2009.[161] B. Ni, Z. Song, and S. Yan. Web image and video miningIEEE Transactionstowards universal and robust age estimator.on Multimedia, 13(6):1217–1229, Dec 2011.[162] Z. Niu, M. Zhou, L. Wang, X. Gao, and G. Hua. Ordinal regres-In The IEEEsion with multiple output cnn for age estimation.Conference on Computer Vision and Pattern Recognition (CVPR),June 2016.[163] G. Nixon and P. Galassi. The Brown Sisters: Thirty-Three Years.Museum of Modern Art, 2007.[164] M. S. Nixon, P. L. Correia, K. Nasrollahi, T. B. Moeslund, A. Ha-did, and M. Tistarelli. On soft biometrics. Pattern RecognitionLetters, 68:218–230, 2015.24nary patterns.intelligence, 24(7):971–987, 2002.IEEE Transactions on pattern analysis and machine[166] V. Ojansivu and J. Heikkil¨a. Blur insensitive texture classificationusing local phase quantization. In International conference on imageand signal processing, pages 236–243. Springer, 2008.[167] A. J. O’Toole, T. Price, T. Vetter, J. Bartlett, and V. Blanz. 3d shapeand 2d surface textures of human faces: the role of averages inImage and Vision Computing, 18(1):9–19,attractiveness and age.1999.[168] A. J. O’toole, T. Vetter, H. Volz, and E. M. Salter.Three-dimensional caricatures of human heads: distinctiveness and theperception of facial age. Perception, 26(6):719–732, 1997.[169] M. Pantic and L. J. M. Rothkrantz. Automatic analysis of facialIEEE Transactions on patternexpressions: The state of the art.analysis and machine intelligence, 22(12):1424–1445, 2000.[170] J. H. Park, M. Schaller, and M. Van Vugt. Psychology of humankin recognition: Heuristic cues, erroneous inferences, and theirimplications. Review of General Psychology, 12(3):215, 2008.[171] U. Park, Y. Tong, and A. K. Jain. Age-invariant face recognition.IEEE Trans. Pattern Anal. Mach. Intell., 32(5):947–954, May 2010.[172] B. Patel, R. Maheshwari, and B. Raman. Evaluation of periocularfeatures for kinship verification in the wild. Computer Vision andImage Understanding, 2017.[173] P. J. Phillips, P. J. Flynn, T. Scruggs, K. W. Bowyer, J. Chang,K. Hoffman, J. Marques, J. Min, and W. Worek. Overview of theface recognition grand challenge. In IEEE Conference on ComputerVision and Pattern Recognition, volume 1, pages 947–954. IEEE,2005.[174] P. J. Phillips, H. Wechsler, J. Huang, and P. J. Rauss. The feretdatabase and evaluation procedure for face-recognition algo-rithms. Image and vision computing, 16(5):295–306, 1998.[175] A. Puthenputhussery, Q. Liu, and C. Liu. Sift flow based geneticfisher vector feature for kinship verification. In IEEE InternationalConference on Image Processing (ICIP), pages 2921–2925, Sept 2016.[176] X. Qin, X. Tan, and S. Chen. Tri-subject kinship verification: Un-derstanding the core of a family. IEEE Transactions on Multimedia,17(10):1855–1867, 2015.[177] N. Ramanathan and R. Chellappa. Modeling age progression inIn IEEE Computer Society Conference on Computeryoung faces.Vision and Pattern Recognition (CVPR), volume 1, pages 387–394,June 2006.[178] N. Ramanathan and R. Chellappa. Modeling shape and texturalIn IEEE International Conference andvariations in aging faces.Workshops on Automatic Face and Gesture Recognition (FG), pages1–8. IEEE, 2008.[179] N. Ramanathan, R. Chellappa, and S. Biswas. ComputationalJournal of Visualmethods for modeling facial aging: A survey.Languages & Computing, 20(3):131 – 144, 2009.[180] R. Ranjan, S. Zhou, J. C. Chen, A. Kumar, A. Alavi, V. M. Patel,and R. Chellappa. Unconstrained age estimation with deepconvolutional neural networks. In IEEE International Conferenceon Computer Vision Workshop (ICCV- W), pages 351–359, Dec 2015.imagedatabase of normal adult age-progression. In IEEE InternationalConference on Automatic Face and Gesture Recognition (FG), pages341–345. IEEE, 2006.[181] K. Ricanek and T. Tesafaye. Morph: A longitudinal[182] M. Riesenhuber and T. Poggio. Hierarchical models of objectrecognition in cortex. Nature neuroscience, 2(11):1019–1025, 1999.[183] J. P. Robinson, M. Shao, Y. Wu, and Y. Fu. Family in the wild(fiw): A large-scale kinship recognition database.[184] J. P. Robinson, M. Shao, Y. Wu, and Y. Fu. Families in the wild(fiw): Large-scale kinship image database and benchmarks.InProceedings of the 2016 ACM on Multimedia Conference, MM ’16,pages 242–246, New York, NY, USA, 2016. ACM.[185] R. Rothe, R. Timofte, and L. V. Gool. Deep expectation of realand apparent age from a single image without facial landmarks.International Journal of Computer Vision, July 2016.[186] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. ImagenetInternational Journal oflarge scale visual recognition challenge.Computer Vision, 115(3):211–252, 2015.[187] C. Sagonas, Y. Panagakis, S. Arunkumar, N. Ratha, andS. Zafeiriou. Back to the future: A fully automatic method forIn International Conference on Patternrobust age progression.Recognition (ICPR), pages 4226–4231, Dec 2016.[165] T. Ojala, M. Pietikainen, and T. Maenpaa. Multiresolution gray-scale and rotation invariant texture classification with local bi-[188] C. Sagonas, Y. Panagakis, A. Leidinger, S. Zafeiriou, et al. Robustjoint and individual variance explained. In IEEE Computer Society25Conference on Computer Vision and Pattern Recognition (CVPR),2017.Acoustics, Speech and Signal Processing (ICASSP), pages 1529–1532.IEEE, 2012.[189] R. Salakhutdinov and G. Hinton. Deep boltzmann machines. InArtificial Intelligence and Statistics, pages 448–455, 2009.[190] E. Sariyanidi, H. Gunes, and A. Cavallaro. Automatic analysisof facial affect: A survey of registration, representation, andIEEE transactions on pattern analysis and machinerecognition.intelligence, 37(6):1113–1133, 2015.[191] R. E. Schapire and Y. Singer.Improved boosting algorithmsusing confidence-rated predictions. Machine learning, 37(3):297–336, 1999.[192] K. Scherbaum, M. Sunkel, H.-P. Seidel, and V. Blanz. Predictionof individual non-linear aging trajectories of faces. In ComputerGraphics Forum, volume 26, pages 285–294. Wiley Online Library,2007.[193] H. S. Seung and D. D. Lee. The manifold ways of perception.science, 290(5500):2268–2269, 2000.[194] M. Shao, S. Xia, and Y. Fu. Genealogical face recognition basedon ub kinface database. In IEEE Conference on Computer Vision andPattern Recognition Workshop (CVPR- W), pages 60–65. IEEE, 2011.[195] E. Shechtman and M. Irani. Matching local self-similarities acrossIn IEEE Conference on Computer Vision andimages and videos.Pattern Recognition (CVPR). IEEE, 2007.[196] C.-T. Shen, F. Huang, W.-H. Lu, S.-W. Shih, and H.-Y. M. Liao.3d age progression prediction in children’s faces with a smallexemplar-image set. J. Inf. Sci. Eng., 30(4):1131–1148, 2014.[197] C.-T. Shen, W.-H. Lu, S.-W. Shih, and H.-Y. M. Liao. Exemplar-based age progression prediction in children faces. In IEEE In-ternational Symposium on Multimedia (ISM), pages 123–128. IEEE,2011.[198] X. Shu, J. Tang, H. Lai, L. Liu, and S. Yan. Personalized age pro-gression with aging dictionary. In IEEE International Conference onComputer Vision (ICCV), pages 3970–3978, Dec 2015.[199] X. Shu, J. Tang, H. Lai, Z. Niu, and S. Yan. Kinship-guided ageprogression. Pattern Recognition, 59:156–167, 2016.[200] T. Sim, S. Baker, and M. Bsat. The cmu pose, illumination, andIn IEEE International Conference onexpression (pie) database.Automatic Face and Gesture Recognition (FG), pages 53–58. IEEE,2002.[201] K. Simonyan, O. M. Parkhi, A. Vedaldi, and A. Zisserman. Fishervector faces in the wild.[202] K. Simonyan and A. Zisserman. Very deep convolutionalarXiv preprintnetworks for large-scale image recognition.arXiv:1409.1556, 2014.[203] G. Somanath and C. Kambhamettu. Can faces verify blood-relations? In IEEE International Conference on Biometrics: Theory,Applications and Systems (BTAS), pages 105–112. IEEE, 2012.[204] G. Somanath, M. Rohith, and C. Kambhamettu. Vadana: A densedataset for facial image analysis. In IEEE International Conferenceon Computer Vision Workshops (ICCV- W), pages 2175–2182. IEEE,2011.[205] Z. Song, B. Ni, D. Guo, T. Sim, and S. Yan. Learning universalmulti-view age estimator using video context. In IEEE Interna-tional Conference on Computer Vision (ICCV), pages 241–248, Nov2011.[206] K. O. Stanley. Compositional pattern producing networks: Anovel abstraction of development. Genetic programming and evolv-able machines, 8(2):131–162, 2007.[207] Y. Su, Y. Fu, Q. Tian, and X. Gao. Cross-database age estimationIn IEEE International Conference onbased on transfer learning.Acoustics, Speech and Signal Processing (ICASSP), pages 1270–1273,March 2010.[208] J. Suo, X. Chen, S. Shan, W. Gao, and Q. Dai. A concatenationalgraph evolution aging model. IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 34(11):2083–2096, Nov 2012.[209] J. Suo, S. C. Zhu, S. Shan, and X. Chen. A compositional andIEEE Transactions on Patterndynamic model for face aging.Analysis and Machine Intelligence, 32(3):385–401, March 2010.[210] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper withconvolutions. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), pages 1–9, 2015.[211] X. Tan and B. Triggs. Enhanced local texture feature sets for facerecognition under difficult lighting conditions. IEEE transactionson image processing, 19(6):1635–1650, 2010.[212] P. Thukral, K. Mitra, and R. Chellappa. A hierarchical approachfor human age estimation. In 2012 IEEE International Conference on[213] B. Tiddeman, M. Burt, and D. Perrett. Prototyping and transform-ing facial textures for perception research. IEEE computer graphicsand applications, 21(5):42–50, 2001.[214] M. E. Tipping. Sparse bayesian learning and the relevance vectormachine. Journal of machine learning research, 1(Jun):211–244, 2001.[215] E. Tola, V. Lepetit, and P. Fua. Daisy: An efficient dense descriptorIEEE transactions on patternapplied to wide-baseline stereo.analysis and machine intelligence, 32(5):815–830, 2010.[216] M. A. Turk and A. P. Pentland. Face recognition using eigenfaces.In IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 586–591. IEEE, 1991.[217] K. Ueki, T. Hayashida, and T. Kobayashi. Subspace-based age-group classification using facial images under various lightingconditions. In Automatic Face and Gesture Recognition, 2006. FGR2006. 7th International Conference on, pages 6–pp. IEEE, 2006.[218] M. Uricar, R. Timofte, R. Rothe, J. Matas, and L. Van Gool.Structured output svm prediction of apparent age, gender andIn The IEEE Conference on Computersmile from deep features.Vision and Pattern Recognition (CVPR) Workshops, June 2016.[219] V. Vapnik and A. Vashist. A new learning paradigm: Learningusing privileged information. Neural networks, 22(5):544–557,2009.[220] V. N. Vapnik and V. Vapnik. Statistical learning theory, volume 1.Wiley New York, 1998.[221] T. F. Vieira, A. Bottino, A. Laurentini, and M. De Simone. Detect-ing siblings in image pairs. The Visual Computer, 30(12):1333–1345,2014.[222] P. Viola and M. J. Jones. Robust real-time face detection. Interna-tional journal of computer vision, 57(2):137–154, 2004.[223] N. Wang, X. Gao, D. Tao, H. Yang, and X. Li. Facial feature pointdetection: A comprehensive survey. Neurocomputing, 2017.[224] S. Wang, D. Tao, and J. Yang. Relative attribute svm+ for learningfor age estimation. IEEE Transactions on Cybernetics, 46(3):827–839,March 2016.[225] W. Wang, Z. Cui, Y. Yan, J. Feng, S. Yan, X. Shu, and N. Sebe.Recurrent face aging. In The IEEE Conference on Computer Visionand Pattern Recognition (CVPR), June 2016.[226] X. Wang and C. Kambhamettu. Leveraging appearance andgeometry for kinship verification. In IEEE International Conferenceon Image Processing (ICIP), pages 5017–5021. IEEE, 2014.[227] Y. Wang, Z. Zhang, W. Li, and F. Jiang. Combining tensorspace analysis and active appearance models for aging effectIEEE Transactions on Systems, Man,simulation on face images.and Cybernetics, Part B (Cybernetics), 42(4):1107–1118, Aug 2012.[228] Y. Wen, Z. Li, and Y. Qiao. Latent factor guided convolutionalneural networks for age-invariant face recognition. In The IEEEConference on Computer Vision and Pattern Recognition (CVPR),June 2016.[229] L. Wolf, T. Hassner, and Y. Taigman. Descriptor based methodsin the wild. In European Conference on Computer Vision Workshop(ECCV- W), 2008.[230] T. Wu, P. Turaga, and R. Chellappa. Age estimation and faceverification across aging using landmarks. IEEE Transactions onInformation Forensics and Security, 7(6):1780–1788, Dec 2012.[231] Y. Wu, P. Kalra, L. Moccozet, and N. Magnenat-Thalmann. Simu-lating wrinkles and skin aging. The visual computer, 15(4):183–198,1999.[232] Y. Wu, N. M. Thalmann, and D. Thalmann. A plastic-visco-elasticmodel for wrinkles in facial animation and skin aging.[233] Y. Wu, N. M. Thalmann, and D. Thalmann. A dynamic wrinklemodel in facial animation and skin ageing. Computer Animationand Virtual Worlds, 6(4):195–205, 1995.[234] B. Xia, B. B. Amor, M. Daoudi, and H. Drira. Can 3d shape of theface reveal your age? In 2014 International Conference on ComputerVision Theory and Applications (VISAPP), volume 2, pages 5–13,Jan 2014.[235] S. Xia, M. Shao, and Y. Fu. Kinship verification through transferlearning.[236] S. Xia, M. Shao, and Y. Fu. Kinship verification through transferjoint conference onIn IJCAI Proceedings-internationallearning.artificial intelligence, volume 22, page 2539, 2011.[237] S. Xia, M. Shao, and Y. Fu. Toward kinship verification usingvisual attributes. In International Conference on Pattern Recognition(ICPR), pages 549–552. IEEE, 2012.of the 19th ACM international conference on Multimedia, pages 953–956. ACM, 2011.[262] Y. Zhu, Y. Li, G. Mu, and G. Guo. A study on apparent ageIn IEEE International Conference on Computer Visionestimation.Workshop (ICCV-W), pages 267–273, Dec 2015.26[238] S. Xia, M. Shao, J. Luo, and Y. Fu. Understanding kin relation-IEEE Transactions on Multimedia, 14(4):1046–ships in a photo.1056, 2012.[239] E. P. Xing, M. I. Jordan, S. J. Russell, and A. Y. Ng. Distance metriclearning with application to clustering with side-information. InAdvances in neural information processing systems, pages 521–528,2003.[240] J. Xing, K. Li, W. Hu, C. Yuan, and H. Ling. Diagnosing deeplearning models for high accuracy age estimation from a singleimage. Pattern Recognition, 66:106–116, 2017.[241] H. Yan. Kinship verification using neighborhood repulsed cor-Image and Vision Computing, 60:91–97,relation metric learning.2017.[242] H. Yan and J. Hu. Video-based kinship verification using distancemetric learning. Pattern Recognition, 2017.[243] H. Yan, J. Lu, W. Deng, and X. Zhou. Discriminative multimetriclearning for kinship verification. IEEE Transactions on Informationforensics and security, 9(7):1169–1178, 2014.[244] H. Yan, J. Lu, and X. Zhou. Prototype-based discriminativeIEEE transactions onfeature learning for kinship verification.cybernetics, 45(11):2535–2545, 2015.[245] H. Yang, D. Huang, Y. Wang, H. Wang, and Y. Tang. Faceaging effect simulation using hidden factor analysis joint sparserepresentation. IEEE Transactions on Image Processing, 25(6):2493–2507, 2016.[246] M. Yang, S. Zhu, F. Lv, and K. Yu. Correspondence drivenadaptation for human profile recognition. In IEEE Conference onComputer Vision and Pattern Recognition (CVPR), pages 505–512,June 2011.[247] Z. Yang and H. Ai. Demographic classification with local binarypatterns. Advances in Biometrics, pages 464–473, 2007.[248] D. Yi, Z. Lei, and S. Z. Li. Age estimation by multi-scaleIn Asian Conference on Computer Visionconvolutional network.(ACCV), pages 144–158. Springer, 2014.[249] S. Zafeiriou, C. Zhang, and Z. Zhang. A survey on face detectionin the wild: past, present and future. Computer Vision and ImageUnderstanding, 138:1–24, 2015.[250] L. A. Zebrowitz and J. M. Montepare.Social psychologicalface perception: Why appearance matters. Social and PersonalityPsychology Compass, 2(3):1497–1517, 2008.[251] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. A surveyof affect recognition methods: Audio, visual, and spontaneousIEEE transactions on pattern analysis and machineexpressions.intelligence, 31(1):39–58, 2009.[252] B. Zhang, S. Shan, X. Chen, and W. Gao. Histogram of gaborphase patterns (hgpp): A novel object representation approachIEEE Transactions on Image Processing,for face recognition.16(1):57–68, 2007.[253] J. Zhang, S. Xia, H. Pan, and A. Qin. A genetics-motivatedunsupervised model for tri-subject kinship verification. In IEEEInternational Conference on Image Processing (ICIP), pages 2916–2920. IEEE, 2016.[254] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao.Joint face detectionand alignment using multitask cascaded convolutional networks.IEEE Signal Processing Letters, 23(10):1499–1503, 2016.[255] M.-L. Zhang and L. Wu. Lift: Multi-label learning with label-specific features. IEEE transactions on pattern analysis and machineintelligence, 37(1):107–120, 2015.[256] Y. Zhang and D. Y. Yeung. Multi-task warped gaussian processfor personalized age estimation. In 2010 IEEE Computer SocietyConference on Computer Vision and Pattern Recognition, pages 2622–2629, June 2010.[257] Z. Zhang, Y. Song, and H. Qi. Age progression/regression byconditional adversarial autoencoder. In CVPR, 2017.[258] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generativeIn International Conference on Learningadversarial networks.Representations (ICLR), 2017.[259] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld. Facerecognition: A literature survey. ACM computing surveys (CSUR),35(4):399–458, 2003.[260] S. K. Zhou, B. Georgescu, X. S. Zhou, and D. Comaniciu. ImageIn IEEE Internationalbased regression using boosting method.Conference on Computer Vision (ICCV), volume 1, pages 541–548.IEEE, 2005.[261] X. Zhou, J. Hu, J. Lu, Y. Shang, and Y. Guan. Kinship verificationfrom facial images under uncontrolled conditions. In Proceedings