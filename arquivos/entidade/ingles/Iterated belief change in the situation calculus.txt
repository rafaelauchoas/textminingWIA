Artificial Intelligence 175 (2011) 165–192Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintIterated belief change in the situation calculus ✩Steven Shapiro a,∗, Maurice Pagnucco b, Yves Lespérance c, Hector J. Levesque aa Department of Computer Science, University of Toronto, Toronto, ON M5S 3G4, Canadab ARC Centre of Excellence for Autonomous Systems and National ICT Australia, School of Computer Science and Engineering, The University of New South Wales,Sydney, NSW 2052, Australiac Department of Computer Science and Engineering, York University, Toronto, ON M3J 1P3, Canadaa r t i c l ei n f oa b s t r a c tArticle history:Available online 3 April 2010Keywords:Knowledge representation and reasoningReasoning about action and changeSituation calculusBelief changeintelligenceJohn McCarthy’s situation calculus has left an enduring mark on artificialresearch. This simple yet elegant formalism for modelling and reasoning about dynamicsystems is still in common use more than forty years since it was first proposed. The abilityto reason about action and change has long been considered a necessary component forany intelligent system. The situation calculus and its numerous extensions as well as themany competing proposals that it has inspired deal with this problem to some extent. Inthis paper, we offer a new approach to belief change associated with performing actionsthat addresses some of the shortcomings of these approaches. In particular, our approach isbased on a well-developed theory of action in the situation calculus extended to deal withbelief. Moreover, by augmenting this approach with a notion of plausibility over situations,our account handles nested belief, belief introspection, mistaken belief, and handles beliefrevision and belief update together with iterated belief change.© 2010 Elsevier B.V. All rights reserved.The work of John McCarthy has had a profound and lasting effect on artificial intelligence research. One of his moreenduring contributions has been the introduction of the situation calculus [2,3]. This simple yet elegant formalism for mod-elling and reasoning about dynamic systems is still in common use more than forty years since it was first proposed. Theability to reason about action and change has long been considered a necessary component for any intelligent system. Anagent acting in its environment must be capable of reasoning about the state of its environment and keeping track of anychanges to the environment as actions are performed. Various theories have been developed to give an account of howthis can be achieved. Foremost among these are theories of belief change and theories for reasoning about action. Whileoriginating from different motivations, the two are united in their aim to have agents maintain a model of the environmentthat matches the actual environment as closely as possible given the available information. An important consideration isthe ability to deal with a succession of changes; known as the problem of iterated belief change.In this paper, we consider a new approach for modelling iterated belief change using the language of the situationcalculus [2,3]. While our approach is in some ways limited in its applicability, we feel that it is conceptually very simpleand offers a number of useful features not found in other approaches:• It is completely integrated with a well-developed theory of action in the situation calculus [4] and its extension tohandle knowledge expansion [5,6]. Specifically, the manner in which beliefs change in our account is simply a special✩An earlier version of this paper appeared in Shapiro et al. (2000) [1].* Corresponding author.E-mail addresses: steven@cs.toronto.edu (S. Shapiro), morri@cse.unsw.edu.au (M. Pagnucco), lesperan@cse.yorku.ca (Y. Lespérance), hector@ai.toronto.edu(H.J. Levesque).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.003166S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192case of how other fluents change as the result of actions, and thus among other things, we inherit a solution to theframe problem.• Like Scherl and Levesque [5,6], our theory accommodates both belief update and belief expansion. The former concernsbeliefs that change as the result of the realization that the world has changed; the latter concerns beliefs that changeas the result of newly acquired information.• Unlike Scherl and Levesque, however, our theory is not limited to belief expansion; rather it deals with the more generalcase of belief revision. It will be possible in our model for an agent to believe some formula φ, acquire information thatcauses it to change its mind and believe ¬φ (without believing the world has changed), and later go back to believing φagain. In Scherl and Levesque and in other approaches based on this work such as [7,8], new information that contradictsprevious beliefs cannot be consistently accommodated.• Because belief change in our model is always the result of action, our account naturally supports iterated belief change.This is simply the result of a sequence of actions. Moreover, each individual action can potentially cause both an update(by changing the world) and a revision (by providing sensing information) in a seamless way.• Like Scherl and Levesque and unlike many previous approaches to belief change, e.g., [9,10], our approach supportsbelief introspection: an agent will know what it believes and does not believe. Furthermore, it has information aboutthe past, and so will also know what it used to believe and not believe. Finally, an agent will be able to predict what itwill believe in the future after it acquires information through sensing.• Unlike Scherl and Levesque, our agents will be able to introspectively tell the difference between an update and arevision as they move from believing φ to believing ¬φ. In the former case, the agent will believe that it believed φ inthe past, and that it was correct to do so; in the latter case, it will believe that it believed φ in the past but that it wasmistaken.• One important lesson learned is that not only does our method for iterated belief change in the situation calculuspossess interesting properties but attempting to use more sophisticated schemes that involve modifying plausibilities ofpossible worlds, leads to unintuitive introspection properties when applied to situations.The rest of the paper is organized as follows: in the next section, we briefly review the situation calculus including theScherl and Levesque [5,6] model of belief expansion, and we review the most popular accounts of belief revision, beliefupdate, and iterated belief change; in Section 3, we motivate and define a new belief operator as a modification to the oneused by Scherl and Levesque; in Section 4, we prove some properties of this operator, justifying the points made above;in Section 5, we show the operator in action on a simple example, and how an agent can change its mind repeatedly;in Section 6, we analyze the extent to which our framework satisfies revision, update, and iterated revision postulates; inSection 7, we compare our framework to some of the existing approaches to belief change; and in the final section, we drawsome conclusions and discuss future work.1. BackgroundThe basis of our framework for belief change is an action theory [4] based on the situation calculus [2,3], and extendedto include a belief operator [5,6]. In this section, we begin with a brief overview of the situation calculus and follow it witha short review of belief change in sufficient detail to understand the contributions made in this paper.1.1. The situation calculusThe situation calculus is a predicate calculus language for representing dynamically changing domains. A situation repre-sents a snapshot of the domain. There is a set of initial situations corresponding to the ways the agent1 believes the domainmight be initially. The actual initial state of the domain is represented by the distinguished initial situation constant, S 0,which may or may not be among the set of initial situations believed possible by the agent. The term do(a, s) denotes theunique situation that results from the agent performing action a in situation s. Thus, the situations can be structured into aset of trees, where the root of each tree is an initial situation and the arcs are actions.Predicates and functions whose value may change from situation to situation (and whose last argument is a situation)are called fluents. For instance, we use the fluent InR1(s) to represent that the agent is in room R1 in situation s. The effectsof actions on fluents are defined using successor state axioms [4], which provide a succinct representation for both effectaxioms and frame axioms [2,3]. For example, assume that there are only two rooms, R 1 and R2, and that the action leavetakes the agent from the current room to the other room. Then, the successor state axiom for InR1 is2:(cid:2)InR1(s) ∧ a (cid:6)= leaveThis axiom asserts that the agent will be in R1 after doing some action if and only if either the agent is in R2 (¬InR1(s))and leaves it or the agent is currently in R1 and the action is anything other than leaving it.(cid:3)¬InR1(s) ∧ a = leave(cid:3)do(a, s)InR1(cid:3)(cid:3)(cid:2)(cid:2)≡∨(cid:2).1 The situation calculus can accommodate multiple agents, but for the purposes of this paper we assume that there is a single agent, and all actions areperformed by that agent.2 We adopt the convention that unbound variables are universally quantified in the widest scope.S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192167Moore [11] defined a possible-worlds semantics for a modal logic of knowledge in the situation calculus by treatingsituations as possible worlds. Scherl and Levesque [5,6] adapted the semantics to the action theories of Reiter [4]. The ideais to have an accessibility relation on situations, B(sis considered possibleby the agent. Note that the order of the arguments is reversed from the usual convention in modal logic.(cid:7), s), which holds if in situation s, the situation s(cid:7)Levesque [8] introduced a predicate, SF(a, s), to describe the result of performing the binary-valued sensing action a.SF(a, s) holds if and only if the sensor associated with a returns the sensing value 1 in situation s. Each sensing actionsenses some property of the domain. The property sensed by an action is associated with the action using a guarded sensedfluent axiom [12]. For example, suppose that there are lights in R1 and R2 and that Light1(s) (Light2(s), respectively) holdsif the light in R1 (R2, respectively) is on. Then:InR1(s) ⊃¬InR1(s) ⊃(cid:2)(cid:3)SF(senseLight, s) ≡ Light1(s)(cid:2)(cid:3)SF(senseLight, s) ≡ Light2(s)can be used to specify that the senseLight action senses whether the light is on in the room where the agent is currentlylocated.Scherl and Levesque [5,6] defined a successor state axiom for B that shows how actions, including sensing actions, affectthe beliefs of the agent. We use the same axiom (with some notational variation) here:Axiom 1 (Successor state axiom for B).(cid:2)Bs(cid:7)(cid:7)(cid:3), do(a, s)(cid:4)(cid:7)(cid:2)(cid:3)(cid:2)(cid:7)(cid:7) = doa, s(cid:7)(cid:3)∧ s(cid:7)s, sB∧(cid:2)(cid:3)(cid:2)a, s(cid:7)SF≡ ∃s≡ SF(a, s)(cid:3)(cid:5).The situations s(cid:7)(cid:7)the sensor associated with action a has the same value in sthe explanatory text that follows. We will see in Section 2.2 how a belief operator can be defined in terms of this fluent.that are B-related to do(a, s) are the ones that result from doing action a in a situation s, such thatas it does in s. This axiom is further illustrated in Fig. 2 and(cid:7)(cid:7)There are various ways of axiomatizing dynamic applications in the situation calculus. Here we adopt a simple formof the guarded action theories described by De Giacomo and Levesque [12] consisting of: (1) successor state axioms3 foreach fluent (including B and pl introduced below), and guarded sensed fluent axioms for each action, as discussed above;(2) unique names axioms for the actions, and domain-independent foundational axioms (given below); and (3) initial stateaxioms, which describe the initial state of the domain and the initial beliefs of the agent.4 For simplicity, we assume herethat all actions are always executable and omit the action precondition axioms and references to a Poss predicate that arenormally included in situation calculus action theories. These do not add any significant complexity to our approach butomitting them here allows us to focus on the key elements of our framework.In what follows, we will use Σ to refer to a guarded action theory of this form. By a domain-dependent fluent, we meana fluent other than B or pl, and a domain-dependent formula is one that only mentions domain-dependent fluents.As part of every guarded action theory, we have unique names axioms for actions and foundational axioms. The uniquenames axioms for the actions state that distinct action function symbols correspond to different action functions. For everypair of distinct action functions, a1 and a2, we need an axiom of the following form:Axiom 2.a1((cid:10)x) (cid:6)= a2((cid:10)y).Also, for an action function, a, we need an axiom of the following form:Axiom 3.a((cid:10)x) = a((cid:10)y) ⊃ (cid:10)x = (cid:10)y.This means that an action function applied to distinct arguments is mapped to different actions, i.e., all action functionsare injective. If we have n action functions, we need O (n2) unique names axioms [13]. However, it would not be difficult tohave them automatically generated from a list of the action names and arities.We want the situations to be the smallest set generated by sequences of actions starting in an initial situation. Weaxiomatize the structure of the situations with foundational axioms based on the ones listed in Levesque et al. [14] and Pirri3 We could use the more general guarded successor state axioms of De Giacomo and Levesque [12], but regular successor state axioms suffice for thesimple domain we consider here and for illustrating our approach.4 These are axioms that only describe initial situations. Reiter [4] has adopted S0 as the only initial situation, but to formalize belief, we need additionalinitial situations representing the alternative scenarios consistent with the agent’s initial beliefs.168S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192and Reiter [15] for the language of the “epistemic situation calculus”. We first define the initial situations to be those thathave no predecessors:(cid:2)(cid:3)Inits(cid:7)def= ¬∃a, s.s(cid:7) = do(a, s).We declare S0 to be an initial situation.Axiom 4.Init(S0).We also need an axiom stating that do is injective.Axiom 5.do(a1, s1) = do(a2, s2) ⊃ (a1 = a2 ∧ s1 = s2).The induction axiom for situations says that if a property P holds of all initial situations, and P holds for all successorsof situation s if it holds for s, then P holds for all situations.Axiom 6.∀P .(cid:4)(cid:2)(cid:3)∀s.Init(s) ⊃ P (s)∧(cid:2)∀a, s.P (s) ⊃ P(cid:2)(cid:3)(cid:3)(cid:5)do(a, s)⊃ ∀s P (s).We now define precedence for situations. We say that s strictly precedes sof actions that take s to s.(cid:7)(cid:7)if and only if there is a (non-empty) sequenceAxiom 7.∀s1, s2.s1 ≺ s2 ≡(cid:2)(cid:3)∃a, s.s2 = do(a, s) ∧ (s1 (cid:13) s),where s1 (cid:13) s2def= s1 = s2 ∨ s1 ≺ s2 denotes that s1 precedes s2.1.2. Belief changeBefore formally defining a belief operator in this language, we briefly review the notion of belief change as it exists in theliterature. Belief change, simply put, aims to study the manner in which an agent’s epistemic (belief) state should changewhen the agent is confronted by new information. In the literature,5 there is often a clear distinction between two formsof belief change: revision and update. Both forms can be characterized by an axiomatic approach (in terms of rationalitypostulates) or through various constructions (e.g., epistemic entrenchment, possible worlds, etc.). The AGM theory [9] is theprototypical example of belief revision while the KM framework [10] is often identified with belief update.Intuitively speaking, belief revision is appropriate for modelling static environments about which the agent has onlypartial and possibly incorrect information. New information is used to fill in gaps and correct errors, but the environmentitself does not undergo change. Belief update, on the other hand, is intended for situations in which the environment itselfis changing due to the performing of actions.For completeness and later comparison, we list here the AGM postulates [16,9] for belief revision. By K ∗ φ we mean therevision of belief state K by new information φ.6∗∗∗∗∗∗∗∗(K(K(K(K(K(K(K(K1) K ∗ φ is deductively closed2) φ ∈ K ∗ φ3) K ∗ φ ⊆ K + φ4) If ¬φ /∈ K , then K + φ ⊆ K ∗ φ5) K ∗ φ = L iff |(cid:16) ¬φ6) If |(cid:16) φ ≡ ψ , then K ∗ φ = K ∗ ψ7) K ∗ (φ ∧ ψ) ⊆ (K ∗ φ) + ψ8) If ¬ψ /∈ K ∗ φ, then (K ∗ φ) + ψ ⊆ K ∗ (φ ∧ ψ)5 We shall restrict our attention to approaches in the AGM vein [16,9,10] although there are many others.6 In the AGM theory, K is a set of formulae and φ is a formula taken from an object language L containing the standard Boolean connectives andthe logical constant ⊥ (falsum). Furthermore, K is a set of formulae (from L) closed under the deductive consequence operator Cn associated with theunderlying logic. The operation K + φ denotes the belief expansion of K by φ and is defined as K + φ = Cn(K ∪ {φ}). [K ] denotes the set of all consistentcomplete theories of L containing K .S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192169Katsuno and Mendelzon [10] provide the following postulates for belief update, where K (cid:19) φ denotes the update of beliefstate K by formula φ.7(K (cid:19) 1) K (cid:19) φ is deductively closed(K (cid:19) 2) φ ∈ K (cid:19) φ(K (cid:19) 3) If φ ∈ K , then K (cid:19) φ = K(K (cid:19) 4) K (cid:19) φ = L iff K |(cid:16) ⊥ or φ |(cid:16) ⊥(K (cid:19) 5) If |(cid:16) φ ≡ ψ , then K (cid:19) φ = K (cid:19) ψ(K (cid:19) 6) K (cid:19) (φ ∧ ψ) ⊆ (K (cid:19) φ) + ψ(K (cid:19) 7) If K is complete and ¬ψ /∈ K (cid:19) φ, then (K (cid:19) φ) + ψ ⊆ K (cid:19) (φ ∧ ψ)(cid:6)(K (cid:19) 8) If [K ] (cid:6)= ∅, then K (cid:19) φ =w∈[K ] w (cid:19) φOne of the major issues in this area is that of iterated belief change, i.e., modelling how the agent’s beliefs change aftera succession of belief revisions or updates occur. Two of the main developments in this area are the work of Darwicheand Pearl [18] and Boutilier [19]. Darwiche and Pearl put forward the following postulates as a way of extending the AGMrevision postulates to handle iterated revision.8(DP1) If ψ |(cid:16) φ, then (K ∗ φ) ∗ ψ = K ∗ ψ(DP2) If ψ |(cid:16) ¬φ, then (K ∗ φ) ∗ ψ = K ∗ ψ(DP3) If φ ∈ K ∗ ψ , then φ ∈ (K ∗ φ) ∗ ψ(DP4) If ¬φ /∈ K ∗ ψ , then ¬φ /∈ (K ∗ φ) ∗ ψIn Section 5, we return to consider the extent to which our framework satisfies these postulates.2. Our account of belief change2.1. Belief change and introspectionScherl and Levesque provide an elegant framework for incorporating knowledge change into the situation calculus. How-ever, in many applications, there is a need to represent information that could turn out to be wrong, i.e., we need to beable to represent beliefs and how they change due to actions. In order to incorporate belief change into our framework,we decided to adapt ideas from Spohn [20] and Darwiche and Pearl [18]. Our first attempt was to add an extra argumentto the accessibility relation. This extra argument was a natural number corresponding to the plausibility of the accessiblesituation.9 B(swas possible with κ -ranking (plausibility) n.10 The lowerκ -ranking, the more plausible the situation would be considered by the agent, and the beliefs of the agent in s would bedetermined by the situations accessible from s with κ -ranking 0, i.e.,(cid:7), n, s) would denote that in s, the agent thinks s(cid:7)Bel(φ, s) def= ∀s(cid:7).B(cid:2)(cid:7)s, 0, s(cid:3)(cid:4)(cid:5).(cid:7)s⊃ φThe successor state axiom for B would adjust the plausibilities of the B-related situations depending on the resultsof sensing using a scheme similar to Darwiche and Pearl’s. Unlike most other approaches to belief revision, we wantedto handle positive and negative introspection of beliefs. However, we realized that this desideratum was in conflict withany reasonable scheme for updating plausibilities of accessible situations. Any reasonable scheme would have the followingproperty: if the accessible situation agrees with the actual situation on the result of a sensing action, the plausibility of the situation(cid:7), n, s) andshould increase (i.e., its κ -ranking should decrease), otherwise the plausibility should decrease. In other words, if B(s(cid:7)) ≡ ¬SF(a, s) holds,SF(a, sthen m should be greater than or equal to n. On the other hand, to ensure positive and negative introspection of beliefs,we combined and generalized the constraints that B be transitive and Euclidean to obtain the following requirement on B(which we call (TE) for transitive and Euclidean):(cid:7)), m, do(a, s)) should hold for some m (cid:2) n. Similarly, if SF(a, s(cid:7)) ≡ SF(a, s) hold, then B(do(a, s(cid:7)∀s, s∃n.B(cid:7)s, n, s(cid:2).(cid:2)(cid:3)(cid:3)(cid:4)∀s⊃(cid:2)(cid:3)(cid:2)(cid:7)(cid:7), m.B(cid:7)(cid:7)s, m, s(cid:7)≡ B(cid:7)(cid:7)s, m, s(cid:3)(cid:5).This requirement ensures that any situation shas the sameaccessible situations with the same plausibilities as s. This ensures that the agent has positive and negative introspection ofaccessible from s has the same belief structure as s, i.e., s(cid:7)(TE)(cid:7)7 To facilitate comparison with the AGM postulates, we have reformulated the original postulates of Katsuno and Mendelzon into an equivalent set usingAGM-style terminology [17]. For renderings of these postulates and the AGM postulates above in the KM-style, refer to Katsuno and Mendelzon [10].8 Again, for consistency of presentation, we have translated the Darwiche and Pearl postulates into AGM-style terminology rather than KM-style termi-nology used in the original paper.9 In fact, the actual numbers assigned to the situations are not relevant. All that is important is the ordering of the situations by plausibility. We couldhave used any total preorder on situations for this purpose, but using (cid:2) on natural numbers simplifies the presentation of our framework.10 We adopt Spohn’s [20] terminology here.170S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192Fig. 1. Belief introspection and plausibility update clash.its entire epistemic state, i.e., both its beliefs and conditional beliefs. For example, in Fig. 3, all the situations within eachoval are mutually accessible (and only these situations are accessible) with the plausibility indicated inside the oval.To see why this requirement conflicts with any reasonable plausibility update scheme for perfectly accurate sensors,consider the following example illustrated in Fig. 1. In the figure, there are three situations, S, S 1, and S2. S1 and S2 areaccessible from S, and S1 has κ -ranking n (the κ -ranking of S2 is irrelevant to the example). In order to satisfy (TE), S1must also be accessible from S2 with κ -ranking n. However, after the agent senses φ (i.e., after it performs the actionsenseφ ), S1 will have to become less plausible relative to S, but more plausible relative to S 2, since S1 and S disagree onthe value of φ, whereas S1 and S2 agree. Therefore, (TE) will in general be violated after the agent senses φ.A possible solution to this problem is to consider only what the agent believes in the actual situation, i.e., in S 0 and itssuccessors, and set the plausibilities of all accessible situations according to whether they agree with the actual situation onthe value of the property being sensed. In our example, if we take S to be S 0, S1 would then become less plausible relativeto both S and S2. Unfortunately, this solution can also lead to subtle undesirable introspective properties. For instance, inSection 6 we show how one can construct an example where the following holds: Bel(¬φ ∧ Bel(φ, do(senseφ, now)), S), i.e.,in S, the agent believes ¬φ, and also believes that after sensing φ, it will believe φ, which is counterintuitive. If the agentbelieves ¬φ, then it should also believe that it will continue to believe ¬φ after sensing φ.Our resolution to this problem, which is discussed at length in the following sections, was to revert back to a binaryaccessibility relation and to use Scherl and Levesque’s successor state axiom for B. Instead of assigning plausibilities relativeto a situation, each situation is assigned an absolute plausibility using a functional fluent pl(s), which maps a situation toa natural number corresponding to the κ -ranking of s (again, the lower the κ -ranking, the more plausible the situation).The plausibilities of successor situations are constrained to be the same as their predecessors, i.e., the plausibility of asituation is unaffected by actions. The beliefs of the agent in a situation s are those formulae true in the most plausiblesituations accessible from s, but these situations are no longer required to have κ -rank 0. When sensing occurs, accessiblesituations will be dropped, therefore the set of the most plausible accessible situations will change, and the agent’s beliefswill change. Since Scherl and Levesque’s successor state axiom for B preserves (TE), positive and negative introspection willbe maintained, if it holds initially.2.2. Definition of the belief operatorIn this section, we define what it means for an agent to believe a formula φ in a situation s, i.e., Bel(φ, s). Since φ willusually contain fluents, we introduce a special symbol now as a placeholder for the situation argument of these fluents, e.g.,Bel(InR1(now), s). φ[s] denotes the formula that results from substituting s for now in φ. To make the formulae easier toread, we will often suppress the situation argument of fluents in the scope of a belief operator, e.g., Bel(InR1, s).Scherl and Levesque [5,6], define a modal operator for belief in terms of the accessibility relation on situations, B(sFor Scherl and Levesque, the believed formulae are the ones true in all accessible situations:Definition 8.BelSL(φ, s) def= ∀s(cid:2)(cid:2)(cid:7)B(cid:7)s, s(cid:3)(cid:5)(cid:3)(cid:4)(cid:7)s.⊃ φ(cid:7), s).To understand how belief change works, both in Scherl and Levesque and here, consider the example illustrated in Fig. 2.In this example, we have three initial situations S, S1, and S2 (across the bottom of the diagram). S1 and S2 are B-relatedto S (i.e., B(S1, S) and B(S2, S)), as indicated by the arrows labelled B. (Ignore the circles around certain situations fornow.) In all three situations, the agent is not in the room R1. In S and S2 the light in R1 is on, and in S1 the light is off. Soat S, the agent believes it is not in R1 (i.e., that it is in R2), but it has no beliefs about the status of the light in R1. We firstconsider the action of leaving R2, which will lead to a belief update. By the successor state axiom for B, both do(leave, S 1)and do(leave, S2) are B-related to do(leave, S). In the figure, these three situations are called S, respectively.The successor state axiom for InR1 causes InR1 to hold in these situations. Therefore, the agent believes InR1 in S. By thesuccessor state axiom for Light1, which we state below, the truth value of Light1 would not change as the result of leave.This is an example of belief update: the agent’s beliefs are modified as a result of reasoning about actions performed in theenvironment.(cid:7)2 and S(cid:7)1, S(cid:7)(cid:7)Now the agent performs the sensing action senseLight. According to the sensed fluent axioms for senseLight,∗SF(senseLight, S.(cid:7)1. So, SF holds for senseLight in the former two situations butIn the figure, the light in R1 is on in Snot in the latter. The successor state axiom for B ensures that after doing a sensing action A, any situation that disagreesif and only if the light is on in the room in which the agent is located in S∗) holds for situation S(cid:7)(cid:7)2, but not in Sand S∗S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192171Fig. 2. An example of belief update and revision.with the actual situation on the value of SF for A is dropped from the B relation in the successor state. In the figure, S(cid:7)(cid:7)is the actual situation. Since S1 inthe figure) is not B-related to do(senseLight, Sagree on the value of SF for(cid:7)(cid:7)senseLight, so do(senseLight, S2 in the figure) is B-related to S. The result is that the agent believes the lightis on in S. This is an example of belief expansion because the belief that the light is on was simply added to the beliefstate of the agent. Belief revision works using the same principles.on the value of SF for senseLight, do(senseLight, S(cid:7)1 disagrees with S(cid:7)1) (labelled S). On the other hand, S(cid:7)2) (labelled S(cid:7)) (labelled S(cid:7)2 and S(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)Our definition of Bel is similar to the one in Scherl and Levesque, but we are going to generalize their account in orderto be able to talk about how plausible the agent considers a situation to be. Plausibility is assigned to situations using afunction pl(s), whose range is the natural numbers, where lower values indicate higher plausibility. The pl function only hasto be specified over initial situations, using an initial state axiom. Successor situations have the same plausibility as theirpredecessors, as stipulated by the following successor state axiom:Axiom 9 (Successor state axiom for pl).(cid:3)do(a, s)= pl(s).pl(cid:2)We say that the agent believes a proposition φ in situation s, if φ holds in the most plausible B-related situations.A situation is most plausible in situation s, if it is at least as plausible as all alternate situations to s:Definition 10.(cid:2)(cid:3)def= ∀s(cid:7)(cid:7).B(cid:2)(cid:3)(cid:3)(cid:2)⊃ pl(cid:7)s(cid:7)(cid:7)s, s(cid:2)(cid:3).(cid:7)(cid:7)s(cid:2) pl(cid:7)s, sMPWe use MPB(s(cid:7), s) to denote the situations s(cid:7)that are most plausible and B-related to s:Definition 11.(cid:2)(cid:3)(cid:2)(cid:3)(cid:7)s, sdef= B(cid:2)(cid:3).(cid:7)s, s∧ MP(cid:7)s, sMPBFinally, we define the belief operator as follows:Definition 12.Bel(φ, s) def= ∀s(cid:7).MPB(cid:2)(cid:3)(cid:4)(cid:5).(cid:7)s⊃ φ(cid:7)s, sThat is, φ is believed at s when it holds at all the most plausible situations B-related to s. Note that unlike Spohn [20]and Darwiche and Pearl [18], we do not require some situations to have plausibility 0.We now return to the initial situations in Fig. 2, and add a plausibility structure to the belief state of the agent bysupposing that S1 is more plausible than S2 (indicated by the circle surrounding S1). For example, suppose that pl(S1) = 0and pl(S2) = 1. Now, the beliefs of the agent are initially determined only by S1. Therefore, the agent now believes that(cid:7)(cid:7)the light R1 is off in S. After leaving R2, the agent continues to believe that the light is off. After doing senseLight, S1 is172S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192dropped from B as before, so now Sof the agent. Since the light is on in Sto believing it is on, this is a case of belief revision.(cid:7)(cid:7)2 is the most plausible accessible situation, which means that it determines the beliefs(cid:7)(cid:7)2, the agent believes it is on in S. Since the agent goes from believing the light is off(cid:7)(cid:7)In order to ensure positive and negative introspection of beliefs, we assert the (TE) constraint over initial situations usingan initial state axiom:Axiom 13.Init(s) ∧ B(cid:2)(cid:7)s, s(cid:3)(cid:2)⊃(cid:7)(cid:7)∀s.B(cid:2)(cid:3)(cid:7)(cid:2)(cid:3)(cid:3).(cid:7)(cid:7)s, s≡ B(cid:7)(cid:7)s, sThe successor state axiom for B preserves this constraint over all situations.Theorem 14.{Axioms 1, 6, and 13} |(cid:16) ∀s, s(cid:2)(cid:7).B(cid:7)s, s(cid:3)⊃(cid:2)(cid:7)(cid:7)∀s.B(cid:2)(cid:3)(cid:7)(cid:2)(cid:3)(cid:3).(cid:7)(cid:7)s, s≡ B(cid:7)(cid:7)s, sIn order to clarify how this constraint ensures that introspection is handled properly, we will show that in the exampleillustrated in Fig. 2, the agent introspects its past beliefs. First, we need some notation that allows us to talk about the past.We use Prev(φ, s) to denote that φ held in the situation immediately before s:Definition 15.Prev(φ, s) def= ∃a, s(cid:7)(cid:2).s = doa, s(cid:7)(cid:3)(cid:4)(cid:5).(cid:7)s∧ φRecall that in the example, the agent believed that the light in R1 was off in S(cid:7)(cid:7)) holds, i.e., in Sthat Bel(Prev(Bel(¬Light1)), S∗in R1 was off. Consider a situation S(cid:7)(cid:7)2. We need to show that Prev(Bel(¬Light1), Sis only one such situation, namely, S(cid:7)(cid:7)2 is B-related to the same situations as SBy Theorem 14, Srequire that ¬Light1(S(cid:7)). We want to show, the agent believes that in the previous situation it believed that the light(cid:7)(cid:7). In this example, there(cid:7)(cid:7)(cid:7)2) holds, i.e., that Bel(¬Light1, S2) holds.(cid:7)(cid:7)1 is more plausible than S2, we only(cid:7)1) holds. Since this is true, it follows that Bel(Prev(Bel(¬Light1)), Sthat is among the most plausible B-related situations to S, i.e., Bel(¬Light1, S(cid:7)(cid:7)) is also true.(cid:7)2. Since S(cid:7)1 and S, i.e., S(cid:7)(cid:7)The specification of pl and B over the initial situations is the responsibility of the axiomatizer of the domain in question.This specification need not be complete. Of course, a more complete specification will yield more interesting propertiesabout the agent’s current and future belief states.(cid:7)We have another constraint on the specification of B over the initial situations: the situations B-related to an initialsituation are themselves initial, i.e., the agent believes that initially nothing has happened. We assert this constraint as aninitial state axiom:Axiom 16.Init(s) ∧ B(cid:2)(cid:3)(cid:7)s, s⊃ Init(cid:2)(cid:3).(cid:7)s3. PropertiesIn this section, we highlight some of the more interesting properties of our framework. In order to clarify our explana-tions and facilitate a comparison with previous approaches to belief change, it will be important for us to attach a specificmeaning to the use of the terms revision and update, which we will do here. Let Σ denote the set of axioms of the previoussections (i.e., Axioms 1–16).3.1. Belief revisionRecall from Section 2 that belief revision is suited to the acquisition of information about static environments for whichthe agent may have mistaken or partial information. In our framework, this can only be achieved through the use of sensingactions since they do not act to modify the environment but rather to tell us something about it. We suppose that for eachformula φ by which we might want to revise, there is a corresponding sensing action capable of determining the truth valueof φ. Moreover, we assume that this sensing action has no effect on the environment; the only fluent it changes is B.11Definition 17 (Uniform formula). We call a formula uniform if the only situation term it contains is the situation constantnow and it contains no unbound variables.11 This is not an overly strict imposition for we can capture sensing actions that modify the domain by “decomposing” the action into a sequence ofnon-sensing actions and sensing actions.S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192173We now define a revision action as follows:Definition 18 (Revision action for φ). A revision action A for a uniform formula φ with respect to action theory Σ is a sensingaction that satisfies the following condition for every domain-dependent fluent F :(cid:5)(cid:4)∀s.SF( A, s) ≡ φ[s]∧(cid:4)Σ |(cid:16)∀s∀(cid:10)x.F ((cid:10)x, s) ≡ F(cid:2)(cid:10)x, do( A, s)(cid:3)(cid:5).In other words, A is a sensing action for the formula φ, and it does not change any physical fluents. Since we assumethere is a revision action A for each formula φ that we might want to revise by, we assume that Σ also contains theappropriate sensed fluent axioms and successor state axioms to satisfy this definition.Definition 19 (Domain-dependent formula). We refer to a formula as domain-dependent if all the fluents mentioned in it aredomain-dependent.It is easy to see from Definition 18 that if A is a revision action and φ∗value of φ∗.is domain-dependent, then A does not affect theLemma 20. Let φ∗Σ |(cid:16) ∀s.φbe a domain-dependent formula, and A be a revision action for some formula φ. Then:∗[s] ≡ φ(cid:4)(cid:5)do( A, s)∗.We now show that belief revision is handled appropriately in our system in the sense that if the sensor indicates thatφ holds, then the agent will indeed believe φ after performing A. Similarly, if the sensor indicates that φ is false, then theagent will believe ¬φ after doing A.Theorem 21. Let φ be a domain-dependent, uniform formula, and A be a revision action for φ with respect to Σ . It follows that:(cid:2)(cid:3)(cid:5)(cid:2)(cid:4)∀s.φ[s] ⊃ BelΣ |(cid:16)φ, do( A, s)(cid:4)∀s.¬φ[s] ⊃ Bel∧¬φ, do( A, s)(cid:3)(cid:5).If the agent is indifferent towards φ before doing the action, i.e., does not believe φ or ¬φ, this is a case of beliefexpansion. If, before sensing, the agent believes the opposite of what the sensor indicates, then we have belief revision.Note that this theorem also follows from Scherl and Levesque’s theory. However, for Scherl and Levesque, if the agentbelieves ¬φ in S and the sensor indicates that φ is true, then in do( A, S), the agent’s belief state will be inconsistent. Theagent will then believe all propositions, including φ. In our theory, the agent’s belief state will be consistent in this case, aslong as there is some situation Saccessible from S that agrees with S on the value of the sensor associated with A.(cid:7)Theorem 22. Let A be a revision action for a domain-dependent, uniform formula φ with respect to Σ . The following set of sentences(which we denote by Γ ) is satisfiable:(cid:7)Σ ∪Bel(¬φ, S0), Bel(cid:2)(cid:3)φ, do( A, S0), ¬Bel(cid:2)FALSE, do( A, S0).(cid:3)(cid:8)3.2. Belief updateBelief update refers to the belief change that takes place due to a change in the environment. In analogy to revision, weintroduce the notion of an update action.Definition 23 (Update action for φ). An update action A for a uniform formula φ with respect to action theory Σ is anon-sensing action that always makes φ true in the environment. That is, Σ |(cid:16) ∀s.φ[do( A, s)] ∧ SF( A, s).As with Scherl and Levesque’s theory, the agent’s beliefs are updated appropriately when an update action A for φoccurs, i.e., the agent will believe φ after A is performed.Theorem 24. Let A be an update action for φ. Then:Σ |(cid:16) ∀s.Bel(cid:2)(cid:3)φ, do( A, s).In our framework, we can represent actions that do not fall under the category of update actions. Of particular interestare ones whose effects depend on what is true in the current situation, i.e., conditional effects. We can prove an analogoustheorem for such actions. Suppose that A is a non-sensing action, i.e., Σ |(cid:16) ∀s.SF( A, s), and that A is an action that causesφ(cid:7)to hold, whenever φ holds beforehand. Further suppose that the agent believes φ in S. Then, after performing A in S,the agent ought to believe that φ(cid:7)holds.174S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192Theorem 25. Let A be a ground action term, and φ, φ(cid:7)(cid:5)(cid:2)(cid:2)(cid:3)(cid:4)Σ |(cid:16) ∀s.Bel(φ, s) ∧ ∀sSFA, s(cid:7)(cid:7)∧(cid:7)∀s.φs(cid:7)be uniform formulae. Then:(cid:7)(cid:2)(cid:4)do⊃ φA, s(cid:3)(cid:5)(cid:3)(cid:7)(cid:2)(cid:7)(cid:3), do( A, s).φ⊃ BelIt is very important to note that in our framework, there are no actions that correspond directly to the actions “reviseby φ” or “update by φ”. We only have physical actions and sensing actions. It is, therefore, the properties associated withthese actions by the successor state and sensed fluent axioms that determine how (and whether) the agent’s beliefs getrevised or updated.3.3. IntrospectionSince we constrained the accessibility relation to be transitive and Euclidean, our agents are guaranteed to be introspec-tive.Theorem 26.(cid:4)Bel(φ, s) ⊃ BelΣ |(cid:16)3.4. Awareness of mistakes(cid:2)Bel(φ), s(cid:3)(cid:5)∧(cid:4)¬Bel(φ, s) ⊃ Bel(cid:2)¬Bel(φ), s(cid:3)(cid:5).In Section 2.2, we claimed that the agent can also introspect its past beliefs. Suppose that the agent believes ¬φ in S,and after performing a revision action A for φ in S, the agent believes φ. In do( A, S), the agent should also believe that inthe previous situation φ was true, but it believed φ was false. In other words, the agent should believe that it was mistakenabout φ. We now prove a theorem that states that the agent will indeed believe that it was mistaken about φ.Theorem 27. Let A be a revision action for a domain-dependent, uniform formula φ with respect to Σ . Then:(cid:2)(cid:2)(cid:2)(cid:3)φ, do( A, s)⊃ BelPrev(cid:3)φ ∧ Bel(¬φ)(cid:3), do( A, s).Σ |(cid:16) ∀s.Bel(¬φ, s) ∧ BelThe properties presented in this section demonstrate the elegance and the power of our framework. While the frameworkitself is not overly complex, it provides a powerful system in which to reason about the beliefs of an agent in a dynamicenvironment with the capability to perform actions and to sense the environment.4. ExampleWe now present an example to illustrate how this theory of belief change can be applied. We model a world in whichthere are two rooms, R1 and R2. The agent can move between the rooms. Each room contains a light that can be on oroff. The agent has two binary sensors. One sensor detects whether or not the light is on in the room in which the agent iscurrently located. The other sensor detects whether or not the agent is in R 1.We have three fluents: Light1(s) (Light2(s), respectively), which holds if and only if there is light in R1 (R2, respectively)in situation s, and InR1(s), which holds if the agent is in R1 in s. If the agent is not in R1, then it is assumed to be in R2.There are three actions: the agent leaves the room it is in and enters the other room (leave), the agent senses whether itis in R1 (senseInR1), and the agent senses whether the light is on in the room in which it is currently located (senseLight).The successor state axioms and guarded sensed fluent axioms for our example, which we will call E, are as follows:Light1(do(a, s)) ≡ Light1(s)Light2(do(a, s)) ≡ Light2(s)InR1(do(a, s)) ≡ ((¬InR1(s) ∧ a = leave) ∨ (InR1(s) ∧ a (cid:6)= leave))TRUE ⊃ (SF(leave, s) ≡ TRUE)InR1(s) ⊃ (SF(senseLight, s) ≡ Light1(s))¬InR1(s) ⊃ (SF(senseLight, s) ≡ Light2(s))TRUE ⊃ (SF(senseInR1, s) ≡ InR1(s))Next we must specify the initial state. This includes both the physical state of the domain and the belief state of theagent. First we describe the initial physical state of the domain, by saying which domain-dependent fluents hold in theactual initial situation, S0. Initially, the lights in both rooms are on and the agent is in R2 (this is illustrated on the left-hand side of Fig. 3):Light1(S0) ∧ ¬InR1(S0) ∧ Light2(S0).S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192175Fig. 3. The initial state of the example domain.The initial belief state of the agent is illustrated in Fig. 3. It shows that in the most plausible situations B-related to S 0(the ones with plausibility 0 in the figure), ¬Light1 and InR1 hold. In the next most plausible situations B-related to S0(the ones with plausibility 1), Light1 and InR1 hold. In the third most plausible (the ones with plausibility 2) B-relatedsituations to S0, Light2 and ¬InR1 hold. There is also at least one situation in the latter group in which Light1 holdsand one in which ¬Light1 holds. Specifying this belief state directly can be cumbersome. For example, the axiom for thesituations with plausibility 1 is:(cid:2)∃s.Init(s) ∧ B(s, S0) ∧ pl(s) = 1(cid:2)(cid:3)∀s.Init(s) ∧ pl(s) = 1 ⊃ Light1(s) ∧ InR1(s)∧(cid:3).For now, we will not enumerate the set of axioms that specify the belief state shown in Fig. 3. But we assume that wehave such a set which, together with the axioms for the initial physical state, we refer to as I . After we have discussed theexample, we will show that there is a more elegant way to specify the initial belief state of the agent. So for this example,we add E, and I to Σ , and obtain the following:Proposition 28. The following formulae are entailed by Σ ∪ E ∪ I :1. Bel(¬Light1 ∧ InR1, S0)2. Bel(Light1 ∧ InR1, do(senseLight, S0))3. Bel(¬InR1, do(senseInR1, do(senseLight, S0)))4. Bel(Prev(¬InR1 ∧ Bel(InR1)), do(senseInR1, do(senseLight, S0)))5. ¬Bel(Light1, do(senseInR1, do(senseLight, S0))) ∧¬Bel(¬Light1, do(senseInR1, do(senseLight, S0)))6. Bel(InR1, do(leave, do(senseInR1, do(senseLight, S0))))7. Bel(Light1,do(senseLight,do(leave, do(senseInR1,do(senseLight, S0))))).We shall now give a short, informal explanation of why each part of the previous theorem holds.1. In the most plausible situations B-related to S0, ¬Light1 ∧ InR1 holds.2. Even though the agent believes that it is in R1 initially, it is actually in R2. Therefore, its light sensor is measuringwhether there is light in R2, even though the agent thinks that it is measuring whether there is light in R 1. It turnsout that there is light in R2 in S0, so the sensor returns 1. Since the agent believes that the light sensor is measuringwhether there is light in R1 and in all the situations with plausibility 0, there is no light in R1, those situations aredropped from the B relation. In the situations with plausibility 1, the light is on in R 1, so those situations are retained.In those situations Light1 ∧ InR1 holds and those fluents are not affected by the senseLight action, so the agent believesLight1 ∧ InR1 after doing senseLight.3. Now the agent senses whether it is in R1. Again the agent’s most plausible situations conflict with what is actually thecase, so they are dropped from the B relation. The situations with plausibility 2 become the most plausible situations,176S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192Fig. 4. Example domain. Only the important details are displayed.so the agent believes it is not in R1. By using plausibilities we allow for “fallback” situations and thus deal with settingsthat Scherl and Levesque [5,6] cannot handle. Their formalism would have descended into inconsistency in this caseas there would be no situations in which ¬InR1 is possible. In fact, this would already have happened after the firstsensing action senseLight. Scherl and Levesque can only reason about situations that are consistent with the agent’scurrent beliefs and therefore cannot deal with sensing results that conflict with these beliefs where an account of beliefrevision, such as the one offered here, is required.4. By Theorem 27, the agent realizes that it was mistaken about being in R 1.5. Among the situations with plausibility 2, there is one in which the light is on in R 1 and one in which it is not on.Therefore, the agent is unsure as to whether the light is on.6. Now the agent leaves R2 and enters R1. This happens in all the B-related situations as well. Therefore, the agentbelieves that it is in R1. This is an example of an update.7. The light in R1 was on initially, and since no action was performed that changed the state of the light, the light remainson. After checking its light sensor, the agent believes that the light is on in R 1.A more complete illustration of this example is given in Fig. 4.This example shows that the agent’s beliefs change appropriately after both revision actions and update actions. Theexample also demonstrates that our formalism can accommodate iterated belief change. The agent goes from believing thatthe light is not on, to believing that it is on, to not believing one way or the other, and then back to believing that it is on.To facilitate the specification of the initial belief state of the agent, we find it convenient to define another belief operator⇒, in the spirit of the conditional logic connective [21]:Definition 29.S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192177φ ⇒s ψ def=(cid:7)(cid:7)(cid:2)∀s.Bs, s(cid:3)(cid:2)(cid:5)(cid:4)(cid:7)s∧∧ φ(cid:7)(cid:7)∀s.B(cid:2)(cid:3)(cid:4)(cid:5)(cid:3)(cid:2)⊃ pl(cid:7)s(cid:7)(cid:7)s∧ φ(cid:7)(cid:7)s, s(cid:3)(cid:3)(cid:2)(cid:7)(cid:7)s(cid:2) pl(cid:4)(cid:5).(cid:7)s⊃ ψφ ⇒s ψ holds if in the most plausible situations B-related to s where φ holds, ψ also holds. Note that for any situation S,Bel(φ, S) is equivalent to (TRUE ⇒S φ).We can use this operator to specify the initial belief state of the agent without having to explicitly mention the plausi-bility of situations. To obtain the results of Proposition 28, it suffices to let I be the following set of axioms:Light1(S0) ∧ ¬InR1(S0) ∧ Light2(S0)TRUE ⇒S0¬Light1 ∧ InR1Light1 ⇒S0¬(Light2 ∧ ¬InR1 ⇒S0¬(Light2 ∧ ¬InR1 ⇒S0Light1)¬Light1)InR1It is easy to see that the belief state depicted in Fig. 3 satisfies these axioms. In the most plausible worlds, (¬Light1 ∧InR1) holds. In the most plausible worlds where the light in R1 is on, the agent is in R1. Finally, the last two axioms statethat among the most plausible worlds where the light is on in R 2 and the agent is in R2, there is one where the light is offin R1 and one in which the light is on (respectively).5. Postulate soundnessIn this section, we consider the extent to which our framework satisfies the AGM postulates for belief revision, the KMpostulates for belief update, and the DP postulates for iterated belief revision. In order to do so we first need to establisha common footing. The first notion to establish is what is meant by the belief state of the agent. We define a belief state(relative to a given situation) to consist of those formulae believed true at a particular situation. We limit our attentionto uniform, domain-dependent formulae, since these frameworks only consider objective formulae (i.e., formulae withoutbelief operators), and they do not have an explicit representation of state, but rather they implicitly refer to the currentstate, and so there is no need to consider beliefs regarding more than one situation. Therefore, the language we use here,Lnow, is a set of domain-dependent uniform formulae. We assume that Lnow is propositional and finite.12 φ, ψ , and γ willbe used to denote domain-dependent uniform formulae. Also, t and u, possibly decorated, will be used to denote groundsituation terms.In the previous section, we used a theory to specify the beliefs of the agent. We said that the agent believed (did notbelieve, respectively) a formula, if the theory entailed that the formula was believed (not believed, respectively). Unless thetheory is complete with respect to the beliefs of the agent, we will not be able to determine whether some formulae arebelieved or not. In other words, there could be more than one model of the theory. This is not the case for the semanticframeworks for belief change, e.g., the AGM framework. There the belief state of the agent is also determined by a set ofsentences, however, there is also an implicit closed world assumption. The sentences in the set are believed by the agent,and if a sentence is not in the set, then it is not believed by the agent. There is no uncertainty about what is believed bythe agent, i.e., the belief state of the agent can be represented by a single model. To bring our framework in line with theAGM framework, we assume that we have a model M of Σ , which will be used to fix the belief state of the agent. We canencode an AGM belief state K in our setting using M and a ground situation term t such that M |(cid:16) Bel(φ, t) if and only ifφ ∈ K .We need to define the three operators used in the postulates: belief expansion, revision and update. The operators inthe postulates map belief states to new belief states. Our framework is based on situations and actions rather than beliefsets and their operators, so we will use a different representation and then translate the postulates appropriately. Ourrevision and update operators are actions, i.e., they map situations into situations. Sensing actions lead to belief revision,and physical actions yield belief update. Iterated revisions and updates are handled using sequences of actions. However,there is no action type that corresponds to belief expansion. Therefore, we define the expansion operator directly as a beliefstate as discussed below. As a consequence, expansions cannot be iterated, but the postulates do not require that we iteratethem.We first define a function that maps a situation t into the belief state of the agent at t.12 We make this assumption to accord with the AGM and KM frameworks, however this assumption is only used in the proof for the soundness of (K (cid:19) 8)(Lemma 57).178S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192Definition 30 (K (t)). We denote the belief state at t (in M) by K (t) and define it as follows:K (t) =(cid:7)(cid:8)ψ : M |(cid:16) Bel(ψ, t).It is easily verified that K (t) is closed under deduction.Recall that the expansion of a belief state K by a formula φ is defined to be the belief state that results from simplyadding φ to K . If φ is inconsistent with K , then the belief state that results from adding φ to K will be inconsistent. Wecan encode this in our framework as an operator that maps a situation t and a formula φ into a belief state, but this timeit will be the belief set that results from adding φ to K (t).Definition 31 (t + φ). We denote the expansion of t by φ (in M) by t + φ and define it as follows:t + φ =(cid:7)(cid:8)ψ : M |(cid:16) Bel(φ ⊃ ψ, t).So, the belief state that results from expanding t with φ is the set of formulae that are believed to be implied by φ in t.Since our expansion operator does not return a situation, it can only be applied last in a sequence of operations. This is thecase for all of the postulates, so this definition suffices for our purposes.Next, we define the revision of t by φ, t ∗ φ, as the situation that results from performing a revision action for φ in t. Aswe said earlier, we assume that for any φ under consideration, there is a revision action Aφ for φ with respect to Σ . Whenφ is clear from the context, we will drop the subscript. In the AGM setting, a revision K ∗ φ is interpreted as the revisionof beliefs K after learning φ. In our case, we do not know whether φ will be true until after performing the revision action.Accordingly, we define a revision of t by φ only in the case that φ happens to be true in situation t (i.e., M |(cid:16) φ[t]).Definition 32 (t ∗ φ). We denote the revision of t by φ (in M) as t ∗ φ and define it as follows:t ∗ φ = do( Aφ, t),whenever M |(cid:16) φ[t]. If M |(cid:16) ¬φ[t], then t ∗ φ is undefined.5.1. AGM postulatesHere are the AGM postulates translated into our notation.∗∗∗∗∗∗∗∗(K(K(K(K(K(K(K(K1) K (t ∗ φ) is deductively closed2) φ ∈ K (t ∗ φ)3) K (t ∗ φ) ⊆ t + φ4) If ¬φ /∈ K (t), then t + φ ⊆ K (t ∗ φ)5) K (t ∗ φ) (cid:6)= Lnow6) If |(cid:16) φ ≡ ψ , then K (t ∗ φ) = K (t ∗ ψ)7) K (t ∗ φ ∧ ψ) ⊆ (t ∗ φ) + ψ8) If ¬ψ /∈ K (t ∗ φ), then (t ∗ φ) + ψ ⊆ K (t ∗ φ ∧ ψ)∗∗Note that (K5) is somewhat different from the corresponding standard AGM postulate. This is due to the fact that it isnot possible in our framework to revise with an identically false formula, since t ∗ φ is only defined if M |(cid:16) φ[t]. To obtain5) in our framework, we require a further assumption, i.e., that initially the agent does not think that the actual situation(Kis completely implausible, i.e., B is reflexive.13 This does not mean that we get knowledge instead of belief, since the actualsituation does not have to be most plausible, but it must not be completely implausible. We use an initial state axiom tostate this assumption:Axiom 33.Init(s) ⊃ B(s, s).We add this axiom to Σ , and it follows that B is everywhere reflexive:Theorem 34.Σ |(cid:16) ∀sB(s, s).13 Note that strictly speaking, we only need the initial situation that precedes t to be self-accessible, however the theorem is easier to state if we assumethat all initial situations are self-accessible.S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192179As an immediate consequence, we have that the agent’s beliefs never become inconsistent.Corollary 35.Σ |(cid:16) ∀s¬Bel(FALSE, s).Finally, we are now able to show that our translations of the AGM postulates are satisfied.Theorem 36. For any model M of Σ , the AGM postulates (Ksituation t and domain-dependent uniform formulae φ and ψ .∗∗1)–(K8) are satisfied, when ∗ is defined as in Definition 32 for any5.2. KM postulatesWe now turn to defining belief update in our framework, which is defined analogously to revision but using an updateaction instead of a revision action. The update of a situation t with a consistent formula φ is the situation that results fromperforming an update action for φ in t. We can only handle consistent formulae because according to the definition of anupdate action A, Σ |(cid:16) ∀s.φ[do( A, s)], which must be false if φ is inconsistent. We will assume that for any consistent φunder consideration, there is at least one update action for φ with respect to Σ , and that we have a function ua whichmaps a consistent formula φ into an update action for φ such that for any ψ , if |(cid:16) φ ≡ ψ then ua(φ) = ua(ψ) (we onlyneed this condition for postulate (K (cid:19) 5)).Definition 37. We define the update of t by φ to be:(cid:3)(cid:2)t (cid:19) φ = doua(φ), t.In order to translate postulate (K (cid:19) 8) into our framework, we need to definew∈[K ] w (cid:19) φ. As we saw above, [K ] isthe set of complete, consistent (cc) theories that contain K , and a cc theory can be thought of as a possible world. We usesituations as possible worlds, so given a belief set at a situation t, we need a set of situations that corresponds to the set ofall cc extensions of K (t). We can easily map situations into cc theories.(cid:6)Definition 38 (Tr(t)). We define the truths at t (in M) to be:(cid:8)ψ : M |(cid:16) ψ[t]Tr(t) =(cid:7).Since Lnow is propositional, the minimal accessible situations from t coincide with the cc extensions of K (t). Let MPB(t)denote {t(cid:7): MPB(t(cid:7), t)}.Lemma 39. w ∈ [K (t)] if and only if there exists t(cid:7) ∈ MPB(t) such that w = Tr(t(cid:7)).Now, we can translate the equation in the postulate as follows:(cid:3)(cid:9)K (t (cid:19) φ) =Trt(cid:7)∈MPB(t)(cid:2)t(cid:7) (cid:19) φ.In other words, the belief set that results from updating t by φ is the same as the intersection of the truths that result fromupdating each member of MPB(t) by φ.Here is a translation of the KM postulates into our notation14:(K (cid:19) 1) K (t (cid:19) φ) is deductively closed(K (cid:19) 2) φ ∈ K (t (cid:19) φ)(K (cid:19) 3) If φ ∈ K (t), then K (t (cid:19) φ) = K (t)(K (cid:19) 4) K (t (cid:19) φ) = Lnow iff K (t) |(cid:16) FALSE(K (cid:19) 5) If |(cid:16) φ ≡ ψ , then K (t (cid:19) φ) = K (t (cid:19) ψ)(K (cid:19) 6) K (t (cid:19) (φ ∧ ψ)) ⊆ (t (cid:19) φ) + ψ(K (cid:19) 7) If K (t) is complete and ¬ψ /∈ K (t (cid:19) φ), then (t (cid:19) φ) + ψ ⊆ K (t (cid:19) (φ ∧ ψ))(cid:6)(K (cid:19) 8) If MPB(t) (cid:6)= ∅ then K (t (cid:19) φ) =(cid:7) (cid:19) φ)t(cid:7)∈MPB(t) Tr(tTheorem 40. K (t (cid:19) φ) satisfies KM postulates (K (cid:19) 1), (K (cid:19) 2), (K (cid:19) 4), (K (cid:19) 5), (K (cid:19) 8) when (cid:19) is defined as in Definition 37 for anysituation t and consistent domain-dependent uniform formulae φ and ψ .14 For (K (cid:19) 4), note that if φ |(cid:16) FALSE, then t (cid:19) φ is not defined, since φ has to be consistent.180S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192Notice that postulates (K (cid:19) 3), (K (cid:19) 6), and (K (cid:19) 7) are not satisfied because while an update action for φ is guaranteed tomake φ true, it may have other effects. In fact, it is not possible in our framework to define an update action that makes anarbitrary φ true and no other changes because our action theories do not handle disjunctive effects of actions. If φ is of theform ψ1 ∨ ψ2, then we could have an action that only makes ψ1 true and one that only makes ψ2 true, but not one thatonly makes the disjunction true.Boutilier [19] has a problem with (K (cid:19) 3) ((U2) in the KM rendering) for similar reasons. In his framework, (update)actions have plausibilities, and the most plausible action explaining the new information is assumed to have taken place.It could be that this action has other effects. To satisfy this postulate, he introduces a null event and considers a model inwhich this is the most plausible event at any world.5.3. DP postulatesIn our framework, iterated revision corresponds to the performing of at least two consecutive revision actions. We nowshow that there is some correspondence with the Darwiche and Pearl [18] account of iterated belief revision. Here are theDP postulates translated into our notation.(DP1) If ψ |(cid:16) φ, then K ((t ∗ φ) ∗ ψ) = K (t ∗ ψ)(DP2) If ψ |(cid:16) ¬φ, then K ((t ∗ φ) ∗ ψ) = K (t ∗ ψ)(DP3) If φ ∈ K (t ∗ ψ), then φ ∈ K ((t ∗ φ) ∗ ψ)(DP4) If ¬φ /∈ K (t ∗ ψ), then ¬φ /∈ K ((t ∗ φ) ∗ ψ)Theorem 41. Postulates (DP1), (DP3) and (DP4) are satisfied when ∗ is defined as in Definition 32 for any situation t and domain-dependent uniform formulae φ and ψ .Interestingly, changes of the type described by (DP2) are not defined according to our view of belief revision. In the casewhere ψ |(cid:16) ¬φ, either t ∗ φ or t ∗ ψ is undefined, therefore K ((t ∗ φ) ∗ ψ) is undefined.6. DiscussionThere are various aspects of our framework that deserve further consideration. We address what we consider to be someof the more important issues here.This work continues the tradition begun by John McCarthy [2,3,22] exploring the use of symbolic representations forrepresenting and reasoning about dynamic systems. The legacy of the situation calculus has proved an influential and lastingcontribution to research on reasoning about action and change. It provides the foundations upon which our framework isbuilt. McCarthy’s early work [2,3] supplied the basic theory underlying the situation calculus and it is from here that wetake our departure point. His later work [22] identifies the frame problem and other issues that need to be addressed whenusing the situation calculus. While we have not utilized McCarthy’s [23] circumscription to solve the frame problem butrather followed Reiter’s [4] successor state axiom approach, Reiter’s approach is certainly influenced by the development ofcircumscription and its use to solve the frame problem. Our approach is also built on the insights of Moore [11] who reifiesthe accessibility relation used in the semantics for modal logics. This approach to modality is supported by McCarthy [24].Our plausibility function is based on ordinal conditional functions [18,20] (particularly Spohn’s κ -rankings). However, ourassignment of plausibilities to situations is fixed, whereas in most frameworks based on assigning plausibilities to worlds,the plausibility assigned to a world can change when revisions occur. The dynamics of belief in our framework derivesfrom the dynamics of the B-relation, rather than that of the plausibility assignment. Note that Friedman and Halpern[25] make similar assumptions to ours. In Darwiche and Pearl’s framework [18], the κ -ranking of a world that does notsatisfy the formula in a revision increases by 1. However, if the world satisfies the revision formula in future revisions, theworld’s κ -ranking decreases, and if it decreases to 0, the world will take part in determining the beliefs of the agent. In ourframework, when a sensing action occurs, any situation Sthat disagrees with the actual value of the sensor is removed fromwill never be readmitted to B, so they will neverthe B relation (actually, its successor is removed). The successors of Shelp determine the beliefs of the agent. This amounts to saying that the information that the agent learns from sensingis knowledge, i.e., the agent will never get new information that contradicts previous sensing information. However, this isquite reasonable since our framework assumes exact sensing and that there are no exogenous actions, so the agent shouldexpect that its sensory information will not be contradicted. For a generalization of our framework to the noisy sensing casethat also allows plausibility update, please see [26]. Our framework was generalized to handle exogenous actions in [27].(cid:7)(cid:7)One may think that having a fixed plausibility assignment limits the applicability of our approach. Consider an example15where, most plausibly, a cat is asleep at home, but where after phoning home, most plausibly, the cat is awake. (Nothing iscertain in either case.) This might seem to require adjustment of the plausibility assignment to situations.15 We are indebted to Jim Delgrande for this example.S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192181To handle this example, we need first to observe that in the action theory we are using, actions are taken to be deter-ministic, with effects described by successor state axioms, quite apart from properties of belief and plausibility. If in somesituations a phone action wakes the cat, and in others not, then there has to be some property M such that we can write asuccessor state axiom of the following form:(cid:3)do(a, s)(cid:2)Awake≡(cid:3)(cid:2)a = phone ∧ M(s)[. . . other actions that can wake cats . . .] ∨(cid:3)(cid:2)Awake(s) ∧ [a is not some put-to-sleep action]∨.For example, M could represent that “the phone’s ringer is loud enough to wake the cat”. With this model, we can thenarrange the B relation in the initial situation so that there are four groups of situations sB-related to S0, where the(cid:7)),(cid:7)), (3) ¬M(sfollowing hold (in order of decreasing plausibility): (1) M(sand (4) ¬M(s(cid:7)(cid:7)) ∧ Awake(s(cid:7)). Then we obtain that(cid:7)) ∧ ¬Awake(s(cid:7)) ∧ ¬Awake(s(cid:7)), (2) M(s(cid:7)) ∧ Awake(sBel(¬Awake, S0),holds since the most plausible situations ssituations B-related to do(phone, S0) are those situations do(phone, sdoes Awake(do(phone, s(cid:7))) by the successor state axiom for Awake above. Therefore,that are B-related to S0 satisfy M(s(cid:7)) where s(cid:7)(cid:7)(cid:7)) ∧ ¬Awake(s(cid:7)). However, the most plausible(cid:7)) holds, sois from group (1). Since M(s(cid:2)(cid:3)Awake, do(phone, S0),Belholds, exactly as desired.16 Of course, in this formalization, we also get that:Bel(M, S0),but this is to be expected: why would we believe it most likely that the cat would be awake after the phone rings, if wedid not also believe it most likely that the ringer was loud enough to waken it? In sum, we can account for changing ourminds about the plausibility of the cat being awake without needing to change the plausibility ordering over situations.We can also handle a variant of this example where we change our mind about whether phoning home wakes the cat.For example, imagine a sensing action examineRinger that informs us that M is false initially (e.g., the ringer on the phoneis set to low). Then, we get that¬Awake, dophone, do(examineRinger, S0)Bel(cid:3)(cid:3)(cid:2)(cid:2)holds, since the most plausible situations will now be descendants of the s¬M(s(cid:7)) holds. This is exactly as desired, and again without needing to change the plausibility assignment.that are B-related to S0 in group (3), where(cid:7)) ∧ ¬Awake(s(cid:7)In the process of developing the approach described in this paper, we experimented with various schemes where theplausibility assigned to situations could be updated. But as discussed in Section 2.1 we found that this led to problemsfor introspection. Consider a scheme where we combine the plausibility assignment with the belief accessibility relation byadding an extra argument to the B relation, i.e., where B(sis plausibleto degree n. In order to ensure that beliefs are properly introspected, the relation would have to satisfy the constraint (TE)discussed in Section 2.1, which is similar to the one given in Theorem 14, but taking plausibilities into account. That is tosay, all the B-related situations to a situation s must have the same belief structure as s, i.e., they should be B-related tothe same situations with the same plausibilities as s. Unfortunately, this conflicts with some of our intuitions about how tochange plausibilities to accommodate new information.(cid:7), n, s) means that in situation s the agent thinks s(cid:7)Consider an example where we have two situations S0 and S1, and where initially the agent considers situation S1more plausible than S0, i.e., B(S1, 0, S0), B(S0, 1, S0), B(S1, 0, S1), B(S0, 1, S1). Notice that S0 and S1 have the same beliefstructure. Suppose that Light1(S0) ∧ SF(senseLight, S0) holds as does ¬Light1(S1) ∧ ¬SF(senseLight, S1). The natural wayto update the plausibilities after sensing would be to make the most plausible situations from a situation do(senseLight, s)(cid:7)(cid:7)0 denote do(senseLight, S0) and Sbe the ones that agree with s on the value of SF(senseLight). So, if we let S1 denote(cid:7)(cid:7)(cid:7)do(senseLight, S1), then in S0. But1, S1 and in Sthis would violate the constraint that B-related situations have the same belief structure, and cause introspection to fail.(cid:7)1 should be more plausible than S(cid:7)0 should be more plausible than S(cid:7)0, SOne way to avoid this problem would be to update the plausibilities of all situations based on what holds in the ‘actual’situations, i.e., S0 and its successors (this focuses attention on beliefs that hold in actual situations, which is what we(cid:7)0 and adjust thenormally do anyway). For the example above, we would look at how the plausibilities should change in S(cid:7)(cid:7)0 is more1) in the same way. We would then have that Splausibilities in the situations B-related to S(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)1, i.e., B(S1). Notice that Splausible than S1 have the1 in both S0 and Ssame belief structure, so the constraint violation mentioned above is resolved.(cid:7)0 (in this case just S(cid:7)(cid:7)(cid:7)0), B(S0), B(S0, 0, S(cid:7)0 and S(cid:7)1), B(S(cid:7)1, 1, S(cid:7)1, 1, S(cid:7)0, 0, SUnfortunately, under this new scheme we have a problem with beliefs about future beliefs. If we were to redefineBel in the obvious way to accommodate the extra argument in B, our example would entail the very counterintuitive16 We can also handle a variant where nothing is believed about the cat sleeping initially by making the groups (1) and (2) the most plausible.182S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192Bel(¬Light1 ∧ Bel(Light1, do(senseLight, Now)), S0), i.e., in S0, the agent believes that the light is not on but thinks thatafter sensing he will believe that it is on. Our approach—which uses a fixed plausibility ordering on situations and simplydrops situations that conflict with sensing results from the B relation—avoids both of these problems.Hunter [28, pp. 67–69] claims that revision operators ∗ as defined by Definition 32 are, strictly speaking, not functions(cid:7) ∗ φ. However, bysince given two different situations t, tDefinition 32, t ∗ φ is defined as do( Aφ, t) and so each revision function ∗ can be considered defined relative to a particularsituation. We could, as it were, write this more precisely as ∗t however our interest here is to examine how closely ourframework complies with the AGM framework and not to use it to define AGM-like revision operators.17(cid:7)) it is not always the case that t ∗ φ = twhere K (t) = K (t(cid:7)7. Comparison to other frameworksOne proposal that is related to ours is that of Demolombe and Pozos Parra [29]. Rather than reifying the accessibilityrelation in the style of Moore [11] and Scherl and Levesque [5,6] as we do here, they introduce belief modalities B i andsuccessor state axioms for these modal operators. For each modal operator B i and fluent F two successor state axioms arerequired:(cid:2)(cid:3)(cid:3)(cid:2)(cid:2)do(a, s)(cid:2)do(a, s)B iF(cid:2)B i¬F≡ Γ(cid:3)(cid:3)+i1,F (a, s) ∨ B ii2,F (a, s) ∨ B i+≡ Γ(cid:3)F (s)(cid:2)∧ ¬Γ(cid:3)¬F (s)−i1,F (a, s),−i2,F (a, s).∧ ¬ΓFurthermore, these modal operators are assumed to obey axioms for the modal logic KD. Each modal operator can be used torepresent the beliefs of a different agent. In our framework this would be achieved through the introduction of accessibilityrelations B i (as noted in [29]). One issue that Demolombe and Pozos Parra point to is that our approach assumes that everyagent agrees on the same set of effects (i.e., successor state axioms) for each fluent. More precisely, our approach is directedtowards representing and reasoning about the beliefs of a single agent. This agent can reason about actions performed byother agents as these are treated as exogenous actions. To deal with multiple agents, the axioms for each (precondition,successor state, etc.) would be grouped into separate action theories and reasoned about separately. This seems appropriatesince each agent reasons about its own beliefs using its own conception of the world. This does not preclude that agentscan also reason about other agents’ beliefs. Demolombe and Pozos Parra [29] also consider that their approach might bebetter suited to dealing with noisy sensors.In [30] Demolombe and Pozos Para propose another solution which adopts an accessibility relation along the lines ofMoore [11] and Scherl and Levesque [5,6] however, in order to deal with multiple agents, they include a term for agents.More specifically, their accessibility relation is of the form K (i, sis compatible with agent i’sbeliefs in situation s. Their notion of belief B(i, φ(s(cid:7), s) meaning that situation s(cid:7), s) is defined as follows:(cid:7)(cid:2)Bi, φ(cid:2)(cid:3)(cid:7)s, s(cid:7), s, s(cid:3)def= ∀s(cid:7)(cid:2)(cid:2)Ki, s(cid:7), s(cid:3)(cid:2)(cid:7)s, s⊃ φ(cid:7), s), s(cid:3)(cid:3)Their aim is to deal with belief change in the situation calculus without recourse to plausibilities. They introduce the notionof real and imaginary situations and the actions whose occurrence can be witnessed by an agent. Real situations correspondto the actual situations in which the agent is placed18 while imaginary situations are simply those alternative situationswhich are compatible with the agent’s beliefs. Each agent can have different successor state axioms for fluents and also forreal and imaginary situations. The successor state axioms take the form(cid:2)(cid:7)∀a∀(cid:10)x∀s∀s(cid:2)+i,p(cid:2)¬Γi, (cid:10)x, a, s(cid:2)−i,pΓ(cid:7)(cid:2)(cid:2)(cid:2)(cid:3)(cid:2)K⊃, si, s(cid:2)(cid:10)x, doreal(s) ⊃a, s(cid:2)(cid:3)(cid:3)(cid:7)a = sensep(i) ∧ p((cid:10)x, s)∨ p(cid:3)(cid:7)(cid:2)a = sense p(i) ∧ ¬p((cid:10)x, s)i, (cid:10)x, a, s(cid:3)(cid:3)∨∨p(cid:2)(cid:7)≡(cid:3)∧(cid:7)(cid:10)x, s(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)where sensep(i) is a sensing action informing i about the truth of p((cid:10)x, s). The distinction between real and imaginarysituations allows the agent to maintain K -related situations even when they do not accord with the agent’s observations.Note however that as we have indicated in Section 4, the actual plausibility values themselves are not important and weprovide a way of applying our framework without having to supply these values explicitly.Another proposal for handling belief revision in the situation calculus is that of del Val and Shoham [31]. Their approachmodels beliefs through formulae of the form holds(bel(φ), s) and a causal axiom:(cid:2)∀s∀μ(cid:7)∃s(cid:2).holdsbel(μ), s(cid:7)(cid:3)(cid:3)(cid:2)⊃ holds(cid:2)(cid:3)(cid:3)bel(μ), resultlearn(μ), s.They use a circumscription policy to reason about the effects of performing actions and to reason about beliefs. Both ourapproaches are characterized axiomatically, however theirs does contain an assumption that they did not axiomatize, namely17 Note that Darwiche and Pearl’s [18] revision operators are defined similarly.18 As such, the successor of a real situation is also a real situation.S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192183that every possible valuation of the fluents is witnessed by a situation. The advantages of our approach over theirs are thatthey do not handle belief introspection and our formal apparatus is much simpler than theirs.Friedman and Halpern’s [25] approach is to begin with a very general framework that combines state dynamics withbelief, and to see what further constraints need to be placed in order to capture the standard approaches to belief revisionand update. It is interesting to note that there are several points in common between our belief revision frameworks. Weboth generalized an existing framework for representing knowledge using possible worlds by adding a plausibility structureto the worlds. Both of our possible worlds contain the history of all actions or events until the current time. We bothmake the assumption that the set of accessible worlds at a given time is a subset of the accessible worlds at earlier times.Friedman and Halpern’s framework, like ours, contains the assumption that the agent does not revise its assessment ofthe relative plausibility of situations. Rather, the agent is assumed to have a prior assessment of the relative plausibility ofsituations and the dynamics of the agent’s beliefs arises from dropping possible worlds that conflict with new information.As a consequence, their agents also cannot recover from inconsistency. However, they consider this to be a problem withthe postulates rather than with their framework.Friedman and Halpern have a similar constraint to (TE). They require that accessible situations have the same plausibilitystructure. Both of our frameworks are synchronous in that accessible situations have the same “current time”, and both ouragents have perfect recall for past actions. One point of difference between our frameworks is that they need separate setsof constraints to obtain revision and update, so they cannot intersperse revisions and updates, whereas we can.We have already noted above that the plausibility ordering remains fixed in our framework, yet this is sufficient to yieldsome rather desirable properties. However, several proposals for modifying plausibility orderings have been put forward inthe literature (many stemming from the work of Spohn [20]). Boutilier [32] and Williams [33] propose a scheme whereby,upon receiving new information φ, the most plausible φ-worlds become the most plausible worlds while all other worldsretain their relative levels of plausibility. Spohn [20] (and Darwiche and Pearl [18]) adopt a method where φ and ¬φ-worldsretain their relative levels of plausibility amongst themselves, but the two groups are “shifted” relative to each other, mak-ing the most plausible φ-worlds the most plausible worlds. However, these approaches do not consider belief introspection.While plausibilities themselves do not change in our framework, it must be kept in mind that the B-relation is also im-portant in terms of determining belief and it may certainly change as a result of performing sensing actions. Furthermore,changes in belief can also be brought about by non-sensing actions which have the capacity to alter the environment.The main aspect that distinguishes our work from previous approaches is the ability to represent belief introspectionproperties within the object language together with the facility to achieve iterated revision and update in a unified frame-work.Belief change in the situation calculus has already been dealt with by Scherl and Levesque [6]. However, as noted previ-ously, while they can handle belief update, they are limited to belief expansion. Del Val and Shoham [31] also address theissue of belief change in the situation calculus, and their theory deals with both revision and update. However, they cannotrepresent nested belief and consequently cannot deal with the issues of belief introspection and mistaken belief.There are a variety of frameworks that accommodate both belief revision and belief update. As noted, this is one strengthof the proposal by del Val and Shoham [31]. In a more traditional belief change setting, Boutilier [34] also provides a generalframework that allows for both these forms of change. However, this framework cannot deal with introspection in theobject language. One approach that supports both belief revision and update and also handles introspection is Friedman andHalpern [25]. Their approach to revision and update is fairly standard, but set within a very general modal logic frameworkthat combines operators for knowledge, belief (interpreted using a plausibility ordering), and time. But they do not discussinteractions between revision and update and introspection. The work of Demolombe and Pozos Parra [29,30] can alsohandle belief introspection.Another avenue of related work is that of modal logic accounts of belief change. Our account, taking some of its heritagefrom Moore [11], reifies the accessibility relation central to modal semantics. In an early work, Segerberg [35] developed adynamic doxastic logic in which action modalities [+Bφ], [−Bφ] and [∗Bφ] denote expansion, contraction and revision ofthe agent’s belief state by Bφ respectively. This allows for formulas like [∗Bφ]Bψ with the meaning that revising the agent’sbelief state by belief in φ will result in belief in ψ . In Segerberg’s framework, expansion, contraction, and revision are treatedas actions working with belief formulas like Bφ. This contrasts with our framework where there are only physical actions(leading to belief update) and sensing actions (leading to belief revision). More recent developments in this area includeHerzig and Longin [36] who introduce a modal logic approach to this problem. Their language is based on propositionaldynamic logic and introduces a modal belief operator Bel and the underlying logic is KD45. They introduce two successorstate axioms:(cid:2)perc(a, b) ∧ ¬Afteraperc(a, b) ∧ ¬Aftera⊃ (FeasibleaBel A ≡ BelAfterb A),⊃ (FeasibleaBel A ≡ BelAfterenablebTheir framework is capable of dealing with non-deterministic actions and misperception. Information is acquired when anobservation action observe(φ) is performed. observe(φ) has φ as precondition. A “test action” testIf (φ) (similar to a sensingaction) is treated as a non-deterministic choice between observe(φ) and observe(¬φ). Thus in their framework, the needfor belief revision arises when an action that is believed not to be executable is nonetheless perceived. As we can see inthe second axiom above, this is handled by performing a special type of action enablea whose effect is to make action a(cid:3)⊥⊥ ∧¬BelAfterb(cid:3)⊥⊥ ∧BelAfterbAfterb A).(cid:2)184S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192executable, i.e., make a’s preconditions true. This simple approach avoids the need for a plausibility ordering on epistemicalternatives. But only beliefs in the observed facts (and more generally the action’s preconditions) are revised. These beliefsmay have arisen due to other incorrect assumptions, but the latter would not be revised, unlike in a plausibility-basedapproach. It is not clear whether introspection about belief change is handled properly.There has been a lot of work recently on this type of modal logic of knowledge/belief and action, for instance thework of van Ditmarsch et al. [37], where the paradigm of dynamic epistemic logic, a family of epistemic/doxastic logicswith announcements and assignments/updates is developed. In van Ditmarsch et al. [38], an optimal regression method isdeveloped for reasoning about knowledge and action within this type of propositional logic. However, most of this work doesnot deal with belief revision (one exception is Chapter 3 of [37], which is based on [35]). Van Benthem [39] also developsthe correspondence between AGM-style belief change and dynamic epistemic logic. In particular, he shows how update rulescan be used to modify the plausibility relation between possible worlds. Van Linder et al. [40] show how one can extenda propositional modal logic of multiagent knowledge, belief, and action to accommodate belief revision/contraction actions,following a similar approach to Segerberg’s [35]. They also formalize agents’ ability to perform actions (including havingthe required information) and how this applies to belief revision actions. However, they do not discuss how sensing actionsmight lead to belief revision. Thielscher [41] introduces a framework for knowledge in the fluent calculus [42]. He introducesa predicate Knows(F , s) and provides knowledge update axioms to specify the effects of actions on the knowledge of theagent.8. Conclusions and future workWe have proposed an account of iterated belief change that integrates into a well-developed theory of action in thesituation calculus [4]. This has some advantages, in that previous work on the underlying theory can be exploited for dealingwith issues such as solving the frame problem, performing automated reasoning about the effects of actions, specifying andreasoning about complex actions, etc. Our framework supports the introspection of beliefs and ensures that the agent isaware of when it was mistaken about its beliefs. Our account of iterated belief change differs from previous accounts inthat, for us, the plausibility assignment to situations remains fixed over time. The dynamics of belief derives from thedynamics of the B modality and of the domain-dependent fluents. We showed that our theory satisfies all of the AGM, andthe majority of the KM and DP postulates.Our approach does have some limitations. In this paper, we have only looked at cases of belief change where the sensorsare accurate, so that the agent only revises its beliefs by sentences that are actually true. It is the case that our successorstate axiom for B ensures that the agent believes the output of its sensor after sensing. Also, our guarded sensed fluentaxioms allow only hard (but context-dependent) constraints to be specified between the output of the sensor and theassociated fluent; one cannot state that the sensor is only correct with a certain probability. However, we can also usebeliefs to correlate sensor values to the associated fluents instead of guarded sensed fluent axioms. Thus, we could specifythat the agent prefers histories where the sensors agree with the associated fluents more often to histories where theyagree less often. Some of these issues are addressed in [26].The fact that we never update the plausibility assignment, may suggest that our account has limited expressiveness. Butwe maintain that this is not the case. The example of Section 4 shows that we can handle some cases where a plausibilityassignment update seems to be required.We could extend the framework by having multiple agents that act independently and impart information to each other.Instead of beliefs changing only through sensing, they would also change as a result of inform actions. Shapiro et al. [43]provide a framework for belief expansion resulting from the occurrence of inform actions in the situation calculus, whichwe would like to generalize to handle belief revision.Lakemeyer and Levesque [7] incorporate the logic of only knowing into the Scherl and Levesque framework of beliefupdate and expansion. The traditional belief (and knowledge) operator specifies formulae that are believed (or known)by the agent, but there could be others. The ‘only knows’ operator is used to describe all that the agent knows, i.e., aformula that corresponds exactly to the knowledge state of the agent. In future work, we would like to define an analogous‘only believes’ operator that could be used to describe exactly what the agent believes in a framework that supports beliefrevision as well as belief expansion.AcknowledgementsThe authors would like to gratefully acknowledge Craig Boutilier and Jim Delgrande for valuable comments on an earlierversion of this paper. Financial support for the Canadian authors was gratefully received from the Natural Sciences andEngineering Research Council of Canada, and the Institute for Robotics and Intelligent Systems of Canada. The second authorwould like to acknowledge support from the Australian Research Council.Appendix A. ProofsTheorem 14.{Axioms 1, 6, and 13} |(cid:16) ∀s, s(cid:2)(cid:7).B(cid:7)s, s(cid:3)⊃(cid:2)(cid:7)(cid:7)∀s.B(cid:2)(cid:3)(cid:7)(cid:2)(cid:3)(cid:3).(cid:7)(cid:7)s, s≡ B(cid:7)(cid:7)s, sS. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192185Proof. Suppose M |(cid:16) {Axioms 1, 6, and 13}. The proof is by induction on s. The base case follows directly from Axiom 13.Now suppose that for some situation S:(cid:3)(cid:2)(cid:2)(cid:2)(cid:3)(cid:2)M |(cid:16) ∀s(cid:7)(cid:7).Bs, S⊃(cid:7)(cid:7)∀s.Bs(cid:7)(cid:7)(cid:7), s≡ B(cid:7)(cid:7)s, S(cid:3)(cid:3).(A.1)(cid:2)We need to show that for any action A:(cid:7)M |(cid:16) ∀s.B(cid:7)(cid:7)1 be a situation such that M |(cid:16) B(S(cid:3), do( A, S)∀s.B⊃(cid:2)s(cid:7)(cid:7)(cid:7)Let Ssuch that:(cid:2)(cid:3)(cid:7)(cid:2)(cid:7)(cid:7)s≡ B(cid:7)(cid:7)s, s(cid:3)(cid:3), do( A, S).(cid:7)(cid:7)1, do( A, S)). Then, by the successor state axiom for B (Axiom 1), there exists an S(cid:7)1S(cid:2)(cid:3)∧ S(cid:7)1, SM |(cid:16) B(cid:2)= doWe need to show that M |(cid:16) ∀sthat for some situation Sthat:(cid:7)(cid:7)1(cid:3)(cid:2)(cid:2)(cid:3)(cid:3)≡ SF( A, S)∧(cid:7)A, SSF1(cid:7)(cid:7)(cid:7)(cid:7), S(cid:7)(cid:7).B(s1) ≡ B(s(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)2, M |(cid:16) B(S1). By the successor state axiom for B and (A.2), there exists a situation S2, SA, S(A.2)(cid:7)(cid:7), do( A, S)). We will prove the ⊃ direction; the other case is similar. Suppose(cid:7)2, such(cid:7)1.(cid:2)(cid:3)(cid:2)(cid:3)(cid:2)(cid:2)(cid:3)(cid:2)(cid:3)(cid:3)S(cid:7)2, SM |(cid:16) B(cid:7)(cid:7)(cid:7)22(cid:7)It follows from (A.1), (A.2), and (A.3) that M |(cid:16) B(S2, S). From this, (A.2) and (A.3), and the successor state axiom for B, itfollows that M |(cid:16) B(S= do≡ SF(A.3)∧ SA, SA, SA, SSF∧(cid:7)2(cid:7)1(cid:7)1.(cid:7)(cid:7)2, do( A, S)). (cid:2)Lemma 20. Let φ∗Σ |(cid:16) ∀s.φbe domain-dependent formula, and A be a revision action for some formula φ. Then,∗[s] ≡ φ(cid:4)(cid:5)do( A, s)∗.Proof. By induction on φ∗. (cid:2)Theorem 21. Let φ be a domain-dependent, uniform formula, and A be a revision action for φ wrt Σ . It follows that:(cid:4)∀s.φ[s] ⊃ Bel(cid:2)Σ |(cid:16)φ, do( A, s)(cid:3)(cid:5)(cid:4)∀s.¬φ[s] ⊃ Bel(cid:2)∧¬φ, do( A, s)(cid:3)(cid:5).Proof. We will prove the first conjunct; the proof of the second is similar. Let M |(cid:16) Σ and suppose that S and Ssituations such that M |(cid:16) φ[S] ∧ MPB(S(cid:7)) ∧ (SF( A, S(cid:7), S) ∧ SM |(cid:16) B(SBy Lemma 20, M |(cid:16) φ[S(cid:7)(cid:7), do( A, S)). By the successor state axiom for B, there is a situation S(cid:7)) ≡ SF( A, S)). Therefore, since M |(cid:16) φ[S] and A is a revision action for φ, M |(cid:16) φ[Saresuch that(cid:7)].(cid:7)(cid:7) = do( A, S(cid:7)(cid:7)]. (cid:2)(cid:7)(cid:7)(cid:7)Theorem 22. Let A be a revision action for a domain-dependent, uniform formula φ wrt Σ . The following set of sentences (which wedenote by Γ ) is satisfiable:(cid:7)(cid:3)(cid:8)(cid:2)(cid:2)Σ ∪Bel(¬φ, S0), Bel(cid:3)φ, do( A, S0), ¬BelFALSE, do( A, S0).Proof. Let S1 and S2 be situation constants. Since Σ does not contain initial state axioms, we can construct a model M ofΣ such that:(cid:2)∀sM |(cid:16)∧ ¬φ[S1] ∧ φ[S2] ∧ pl(S1) < pl(S2).(cid:7) = S1 ∨ s(cid:7) = S2, S0.B(cid:3)(cid:3)≡(cid:2)(cid:3)(cid:2)ss(cid:7)(cid:7)It is easy to verify that M |(cid:16) Γ . (cid:2)Theorem 24. Let A be an update action for φ. Then:Σ |(cid:16) ∀s.Bel(cid:2)(cid:3)φ, do( A, s).(cid:7)(cid:7)be situations such that M |(cid:16) MPB(SProof. Let M be a model of Σ , and S, S(cid:7)(cid:7) = do( A, Ssuch that M |(cid:16) SB, there is a situation S(cid:7)). Since A is an update action for φ, M |(cid:16) φ[S(cid:7)(cid:7), do( A, S)). By the successor state axiom for(cid:7)(cid:7)]. (cid:2)(cid:7)Theorem 25. Let A be a ground action term, and φ, φ(cid:7)(cid:5)(cid:7)(cid:2)(cid:2)(cid:3)(cid:4)(cid:7)(cid:7)(cid:7)Σ |(cid:16) ∀s.Bel(φ, s) ∧ ∀sSF.φsA, s∀s∧be uniform formulae. Then:(cid:7)(cid:7)⊃ φ(cid:2)(cid:4)do⊃ BelA, s(cid:3)(cid:5)(cid:3)φ(cid:2)(cid:7)(cid:3), do( A, s).Proof. Let M be a model of Σ and S be a situation such that:M |(cid:16) Bel(φ, S) ∧ ∀s(cid:2)(cid:7)SF(cid:2)(cid:3)(cid:7)∧(cid:7)∀s.φ(cid:5)(cid:4)(cid:7)s(cid:7)(cid:2)(cid:4)do⊃ φA, s(cid:3)(cid:5)(cid:3)(cid:7).A, sSuppose that for some situation S(cid:7)(cid:7):(A.4)186S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192(cid:2)(cid:7)(cid:7)(cid:3), do( A, S)SM |(cid:16) MPB.We need to show that M |(cid:16) φ(cid:7)[S(cid:2)(cid:7)(cid:7) = doM |(cid:16) B∧ S, S(cid:2)(cid:3)S(cid:7)(cid:3)(cid:7).A, S(cid:7)(cid:7)]. By the successor state axiom for B, there is a situation S(cid:7)such that:(A.5)(A.6)(cid:7), S), then the theorem would follow because we could infer from (A.4) that M |(cid:16)Now, if we could show that M |(cid:16) MP(S(cid:7)φ[S1 is a situation such that:(cid:7)(cid:7)]. Suppose S(cid:7)] and also M |(cid:16) φ(cid:7)[S(cid:2)(cid:3)M |(cid:16) B(cid:7)1, SS.(A.7)We need to show that M |(cid:16) pl(S(cid:7)for B that M |(cid:16) B(do( A, S1), do( A, S)). We can infer from this, (A.5), and (A.6) that M |(cid:16) pl(do( A, Sand the successor state axiom for pl imply M |(cid:16) pl(S(cid:7)1). It follows from the second conjunct of (A.4), (A.7), and the successor state axiom(cid:7)1)). This(cid:7))) (cid:2) pl(do( A, S(cid:7)) (cid:2) pl(S(cid:7)) (cid:2) pl(S(cid:7)1), as desired. (cid:2)Theorem 26.(cid:4)Bel(φ, s) ⊃ BelΣ |(cid:16)(cid:2)Bel(φ), s(cid:3)(cid:5)∧(cid:4)¬Bel(φ, s) ⊃ Bel(cid:2)¬Bel(φ), s(cid:3)(cid:5).Proof. This follows directly from Theorem 14. (cid:2)Theorem 27. Let A be a revision action for a domain-dependent, uniform formula φ wrt Σ . Then:Σ |(cid:16) ∀s.Bel(¬φ, s) ∧ Bel(cid:2)(cid:3)φ, do( A, s)(cid:2)⊃ BelPrev(cid:2)(cid:3)φ ∧ Bel(¬φ)(cid:3), do( A, s).Proof. Let M be a model of Σ and S be a situation such that:M |(cid:16) Bel(¬φ, S) ∧ Bel(cid:2)(cid:3)φ, do( A, S).Suppose for some situation SM |(cid:16) MPB(cid:2)(cid:7)(cid:7)S(cid:7)(cid:7):(cid:3), do( A, S).By the successor state axiom for B, there is a situation S(cid:7)such that:M |(cid:16) B(cid:2)(cid:3)(cid:2)(cid:7)(cid:7) = do∧ S(cid:7)S, S(cid:3)(cid:7).A, S(A.8)(A.9)(A.10)(cid:7)(cid:7)], and from this, (A.10), andWe need to show that M |(cid:16) (φ ∧ Bel(¬φ))[SLemma 20 that M |(cid:16) φ[S(cid:3)(cid:7)]. Now, let S(cid:2)(cid:7)1 be a situation such that:M |(cid:16) MPB(cid:7)(cid:7)1, SS(cid:7)]. It follows from (A.8) and (A.9) that M |(cid:16) φ[S.(A.11)It follows from this, (A.10), and Theorem 14 that M |(cid:16) B(Swould follow since we could infer from (A.8) that M |(cid:16) ¬φ[Sfrom (A.10) and Theorem 14 that M |(cid:16) B(S(cid:7)1, S). If we could show that M |(cid:16) MP(S(cid:7)1, S) then the theorem(cid:7)(cid:7)2 be a situation such that M |(cid:16) B(S2, S). It follows(cid:7)2). (cid:2)(cid:7)1) (cid:2) pl(S]. Let S(cid:7)). We can now infer from (A.11) that M |(cid:16) pl(S(cid:7)2, S(cid:7)1Theorem 34.Σ |(cid:16) ∀sB(s, s).Proof. By induction on s. (cid:2)Theorem 36. For any model M of Σ , the AGM postulates (Ksituation t and domain-dependent uniform formulae φ and ψ .∗∗1)–(K8) are satisfied, when ∗ is defined as in Definition 32 for anyWe prove this theorem by proving each postulate as a separate lemma.Lemma 42 (K∗1). Under the assumptions of Theorem 36, K (t ∗ φ) is deductively closed.Proof. This lemma follows from the fact that the Bel operator is closed over logical entailment. (cid:2)Lemma 43 (K∗2). Under the assumptions of Theorem 36, φ ∈ K (t ∗ φ).S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192187Proof. This follows directly from Theorem 21. (cid:2)In the following lemma, we show conditions under which a most plausible, accessible situation remains so after arevision action for φ is performed.Lemma 44. Let A be a revision action for φ. Then, Σ |(cid:16) ∀s, s(cid:7).φ[s] ∧ φ[s(cid:7)] ∧ MPB(s(cid:7), s) ⊃ MPB(do( A, s(cid:7)), do( A, s)).Proof. Let M |(cid:16) Σ and t, tSF( A, t) ∧ SF( A, tmost plausible, let t(cid:7)(cid:7)(cid:7)1 such that M |(cid:16) tt1pl(t(cid:7)(cid:7)1). (cid:2)(cid:7)(cid:7)). Then, M |(cid:16) B(do( A, tbe situations such that M |(cid:16) φ[t] ∧ φ[t(cid:7), t). Since A is a revision action for φ, M |(cid:16)(cid:7)) is(cid:7)(cid:7)(cid:7)(cid:7)1 be a situation such that M |(cid:16) B(t1, do( A, t)). By the successor state axiom for B, there is a situation(cid:7)(cid:7))) (cid:2)(cid:7)) (cid:2) pl(t1). By assumption, M |(cid:16) pl(t= do( A, t(cid:7)), do( A, t)) follows from the successor state axiom for B. To see that do( A, t(cid:7)1). By the successor state axiom for pl, M |(cid:16) pl(do( A, t(cid:7)] ∧ MPB(tLemma 45 (K∗3). Under the assumptions of Theorem 36, K (t ∗ φ) ⊆ t + φ.Proof. Suppose ψ ∈ K (t ∗ φ),ation such that M |(cid:16) MPB(tM |(cid:16) MPB(do( A, tthat M |(cid:16) ψ[t(cid:7)]. (cid:2)i.e., M |(cid:16) Bel(ψ, do( A, t)). We need to show that M |(cid:16) Bel(φ ⊃ ψ, t). Let t(cid:7)(cid:7), t) ∧ φ[t(cid:7)]. Since t ∗ φ is defined, M |(cid:16) φ[t], therefore,(cid:7)), do( A, t)). This, together with the hypothesis, imply that M |(cid:16) ψ[do( A, tbe a situ-it follows from Lemma 44 that(cid:7))]. It follows from Lemma 20The following lemma identifies conditions under which the predecessor (under a revision action for φ) of a most plausi-ble and accessible situation is also most plausible and accessible.Lemma 46. Let A be a revision action for φ. Then,Σ |(cid:16) ∀s.φ[s] ∧ ¬Bel(¬φ, s) ⊃(cid:4)(cid:7)(cid:7)∀s.MPB(cid:2)(cid:3), do( A, s)(cid:7)(cid:7)s(cid:7)⊃ ∃s.MPB(cid:2)(cid:3)(cid:2)(cid:7)(cid:7) = do∧ s(cid:7)s, s(cid:3)(cid:7)(cid:4)(cid:5)(cid:5).(cid:7)s∧ φA, s, M |(cid:16) MPB(t(cid:7)(cid:7) = do( A, t(cid:7)(cid:7)]. It remains to show that M |(cid:16) MP(tProof. Suppose for some situation t, M |(cid:16) Σ ∧ φ[t] and M (cid:6)|(cid:16) Bel(¬φ, t). Then, for some situation u, M |(cid:16) MPB(u, t) ∧ φ[u].(cid:7)(cid:7), do( A, t)). Since M satisfies the successor state axiom for B, there(cid:7)(cid:7)Further suppose that for some situation t(cid:7)) ≡ SF( A, t). Since A is a revision action for φ, it follows(cid:7), t) ∧ tis a situation tthat M |(cid:16) φ[tsuch that(cid:7)).M |(cid:16) B(t(cid:7)(cid:7)). It follows from the assumptions and Lemma 44 that M |(cid:16)By the successor state axiom for pl, M |(cid:16) pl(do( A, u)) < pl(tB(do( A, u), do( A, t)), which contradicts M |(cid:16) MPB(t(cid:7)) ∧ SF( A, t(cid:7), t). Suppose to the contrary that there is a situation t(cid:7)). Since M |(cid:16) MPB(u, t), it follows that M |(cid:16) pl(u) (cid:2) pl(t∗), and therefore M |(cid:16) pl(u) < pl(tsuch that M |(cid:16) B(t(cid:7)(cid:7), do( A, t)). (cid:2)∗, t) ∧ pl(t∗) < pl(t∗Lemma 47 (K∗4). Under the assumptions of Theorem 36, if ¬φ /∈ K (t) then t + φ ⊆ K (t ∗ φ).Proof. Suppose ¬φ /∈ K (t), i.e., M |(cid:16) ¬Bel(¬φ, t), and ψ ∈ t + φ, i.e.,M |(cid:16) Bel(φ ⊃ ψ, t).We need to show that M |(cid:16) Bel(ψ, do( A, t)). Suppose that for some situation tM |(cid:16) φ[t]. Therefore, by Lemma 46, M |(cid:16) ∃s(cid:7)(cid:7) = do( A, tt(cid:7), t) ∧ t(cid:7)]. It follows from (A.12) that M |(cid:16) ψ[t(cid:7)(cid:7) = do( A, s(cid:7)) ∧ φ[s(cid:7)(cid:7)]. By Lemma 20, M |(cid:16) ψ[t(cid:7).MPB(s(cid:7)) ∧ φ[t(cid:7)(cid:7)(cid:7)]. Let t, M |(cid:16) MPB(t(A.12)(cid:7)(cid:7), do( A, t)). Since t ∗ φ is defined,(cid:7), t) ∧be a situation such that M |(cid:16) MPB(t(cid:7)(cid:7)]. (cid:2)Lemma 48 (K∗5). Under the assumptions of Theorem 36, K (t ∗ φ) (cid:6)= Lnow.Proof. This follows directly from Corollary 35. (cid:2)Lemma 49 (K∗6). Under the assumptions of Theorem 36, if |(cid:16) φ ≡ ψ , then K (t ∗ φ) = K (t ∗ ψ).Proof. This follows from the fact that the Bel operator preserves logical equivalence. (cid:2)Lemma 50 (K∗7). Under the assumptions of Theorem 36, K (t ∗ (φ ∧ ψ)) ⊆ (t ∗ φ) + ψ .Proof. Suppose γ ∈ K (t ∗ (φ ∧ ψ)), i.e.,(cid:2)M |(cid:16) Bel(cid:3)γ , do( Aφ∧ψ , t).(A.13)(cid:7)(cid:7)We need to show that γ ∈ (t ∗ φ) + ψ , i.e., M |(cid:16) Bel(ψ ⊃ γ , do( Aφ, t)). Suppose to the contrary that there is a situation tsuch that:188S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192M |(cid:16) MPB(cid:2)t(cid:7)(cid:7)(cid:3), do( Aφ, t)(cid:4)∧ (ψ ∧ ¬γ )t(cid:5)(cid:7)(cid:7).(A.14)Since t ∗ φ is defined, M |(cid:16) φ[t]. Therefore, it follows from the successor state axiom for B and (A.14) that there is a situation(cid:7)tsuch that:(A.15)(A.16)M |(cid:16) B(cid:3)(cid:2)t(cid:7), t(cid:2)(cid:7)(cid:7) = do∧ t(cid:3)(cid:7)(cid:4)t(cid:7)(cid:5).∧ φAφ, tWe can infer from Lemma 20, (A.14), and (A.15) that:(cid:4)M |(cid:16) (ψ ∧ ¬γ )tM |(cid:16) ¬γ(cid:2)(cid:4)do(cid:7)(cid:5),and thus(cid:3)(cid:5).(cid:7)M |(cid:16) B(cid:2)t∗∗(cid:7)), do( Aφ∧ψ , t)). Suppose t(cid:3), do( Aφ∧ψ , t).We need to show that M |(cid:16) pl(do( Aφ∧ψ , tand (A.18) that there is a situation t(cid:3)∗(cid:3)(cid:2)t∗, t(cid:2)∗∗ = do∧ t∗Aφ∧ψ , tM |(cid:16) B(cid:7))) (cid:2) pl(tsuch that:(cid:4)∧ (φ ∧ ψ)t∗(cid:5).Aφ∧ψ , t(A.17)(cid:7)), do( Aφ∧ψ , t)), since this along with (A.17) contradicts (A.13). SinceNow, it remains to show that M |(cid:16) MPB(do( Aφ∧ψ , tt ∗ (φ ∧ ψ) is defined, M |(cid:16) (φ ∧ ψ)[t]. Therefore, the successor state axiom for B together with (A.15) and (A.16) imply thatM |(cid:16) B(do( Aφ∧ψ , tis such that:∗∗(A.18)∗∗). Since M |(cid:16) (φ ∧ ψ)[t], it follows from the successor state axiom for BSimilarly, it follows from the successor state axiom for B and (A.19) that M |(cid:16) B(do( Aφ, twe can infer that M |(cid:16) pl(tinfer that M |(cid:16) pl(do( Aφ∧ψ , t(cid:7)(cid:7)) (cid:2) pl(do( Aφ, t(cid:7))) (cid:2) pl(t∗∗) as required. (cid:2)(A.19)∗), do( Aφ, t)). From this and (A.14),∗)). We can now use the successor state axiom for pl with (A.15) and (A.19) toLemma 51 (K∗8). Under the assumptions of Theorem 36, if ¬ψ /∈ K (t ∗ φ), then (t ∗ φ) + ψ ⊆ K (t ∗ φ ∧ ψ).Proof. Suppose that ¬ψ /∈ K (t ∗ φ), i.e.:M |(cid:16) ¬Bel(cid:2)(cid:3)¬ψ, do( Aφ, t),andfor some formula γ , γ ∈ (t ∗ φ) + ψ , i.e.:M |(cid:16) Bel(cid:2)(cid:3)ψ ⊃ γ , do( Aφ, t).We need to show that γ ∈ K (t ∗ φ ∧ ψ), i.e.:M |(cid:16) Bel(cid:2)(cid:3)γ , do( Aφ∧ψ , t).(A.20)(A.21)(A.22)(cid:7)(cid:7):M |(cid:16) MPBSuppose that for some situation t(cid:2)(cid:3), do( Aφ∧ψ , t)tWe need to show that M |(cid:16) γ [tsuccessor state axiom for B and (A.23) that there is a situation t(cid:7)(cid:7).(cid:7)such that:(A.23)(cid:7)(cid:7)]. Since Aφ∧ψ is a revision action for φ ∧ ψ and t ∗ (φ ∧ ψ) is defined, it follows from theM |(cid:16) B(cid:3)(cid:2)t(cid:7), t(cid:2)(cid:7)(cid:7) = do∧ t(cid:3)(cid:7)(cid:4)∧ (φ ∧ ψ)t(cid:5)(cid:7).Aφ∧ψ , t(A.24)We can infer from Lemma 20 that:(cid:2)(cid:4)M |(cid:16) (φ ∧ ψ)do(cid:7)(cid:3)(cid:5).Aφ, tIf we can show that M |(cid:16) MPB(do( Aφ, tfrom Lemma 20 and (A.24), i.e., M |(cid:16) γ [tsuccessor state axiom for B that M |(cid:16) B(do( Aφ, t(cid:7)), do( Aφ, t)), then by (A.21) and (A.25), M |(cid:16) γ [do( Aφ, t(A.25)(cid:7))], and the theorem follows(cid:7)(cid:7)]. Since Aφ is a revision action for φ, it follows from (A.24) and thebe a situation such that:(cid:7)), do( Aφ, t)). Now let t(cid:7)] ∧ γ [t∗∗M |(cid:16) B(cid:2)t∗∗(cid:3), do( Aφ, t).It remains to show that pl(do( Aφ, t(cid:4)(cid:2)(cid:3), do( Aφ, t)(cid:7)(cid:7)u∧ ψM |(cid:16) B(cid:7))) (cid:2) pl(t(cid:2)∧ pl(cid:5)(cid:7)(cid:7)u∗∗). From (A.20) and (A.26), it follows that there is a situation u(cid:7)(cid:7)such that:(A.26)(cid:3)(cid:7)(cid:7)(cid:2)(cid:2) plt∗∗(cid:3).(A.27)uSince t ∗ φ is defined, M |(cid:16) φ[t]. Therefore, since Aφ is a revision action for φ, it follows from (A.27), the successor stateaxiom for B, and Lemma 20 that there is a situation usuch that:(cid:7)M |(cid:16) B(cid:2)(cid:3)(cid:2)(cid:7)(cid:7) = do∧ u(cid:7)u, t(cid:3)(cid:7)(cid:4)∧ (φ ∧ ψ)u(cid:7)(cid:5).Aφ, uS. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192189(A.28)Since t ∗ (φ ∧ ψ) is defined, M |(cid:16) φ ∧ ψ[t]. Therefore, since Aφ∧ψ is a revision action for φ ∧ ψ , we can apply the successorstate axiom for B again to yield:(cid:2)do(cid:3), do( Aφ∧ψ , t)Aφ∧ψ , uM |(cid:16) B(cid:2)(cid:3).(cid:7)This together with (A.23) implies that M |(cid:16) pl(t(A.24) and (A.28), we obtain M |(cid:16) pl(do( Aφ, tdesired. (cid:2)(cid:7)(cid:7)) (cid:2) pl(do( Aφ∧ψ , u(cid:7)(cid:7)). This, together with (A.27), yields M |(cid:16) pl(do( Aφ, t(cid:7))). Using the successor state axiom for pl along with∗∗), as(cid:7))) (cid:2) pl(t(cid:7))) (cid:2) pl(uLemma 39. w ∈ [K (t)] iff there exists t(cid:7) ∈ MPB(t) such that w = Tr(t(cid:7)).Proof. The “if” part is obvious. For the “only if” part, we fix w ∈ [K (t)]. Since Lnow is propositional and finite, let F be the(cid:7)). Then it isconjunction of literals in w. Suppose, towards a contradiction, that there is no teasy to see that M |(cid:16) Bel(¬F , t). Therefore, ¬F ∈ K (t), which implies w /∈ [K (t)]. Contradiction. (cid:2)(cid:7) ∈ MPB(t) such that w = Tr(tTheorem 40. K (t (cid:19) φ) satisfies KM postulates (K (cid:19) 1), (K (cid:19) 2), (K (cid:19) 4), (K (cid:19) 5), (K (cid:19) 8) when (cid:19) is defined as in Definition 37 for anysituation t and consistent domain-dependent uniform formulae φ and ψ .We prove this theorem by proving each postulate as a separate lemma.Lemma 52 (K (cid:19) 1). K (t (cid:19) φ) is closed.Proof. This lemma follows from the fact that the Bel operator is closed over logical entailment. (cid:2)Lemma 53 (K (cid:19) 2). φ ∈ K (t (cid:19) φ).Proof. This lemma follows directly from Theorem 24. (cid:2)After an update action is performed, the accessible situations are simply projected forward. In particular, as the followinglemma shows, the most plausible accessible situations are preserved.Lemma 54. Let A be an update action for φ and t be a ground situation term. Then,(cid:2)Σ |(cid:16) ∀s.MPB(s, t) ≡ MPB(cid:3)do( A, s), do( A, t).Proof. This follows from the definition of an update action and the successor state axioms for B and pl. (cid:2)Lemma 55 (K (cid:19) 4). K (t (cid:19) φ) = Lnow iff K (t) |(cid:16) FALSE.Proof. This follows from Lemma 54. (cid:2)Lemma 56 (K (cid:19) 5). If |(cid:16) φ ≡ ψ , then K (t (cid:19) φ) = K (t (cid:19) ψ).Proof. This follows from the definition of ua. (cid:2)Lemma 57 (K (cid:19) 8). If MPB(t) (cid:6)= ∅ then K (t (cid:19) φ) =Proof. Let A denote ua(φ). Note that K (t (cid:19) φ) =(cid:2)t(cid:2)do(cid:9)(cid:9)A, t=TrTr(cid:2)(cid:3)(cid:7)(cid:7)(cid:7)(cid:3)(cid:3).(cid:6)t(cid:7)∈MPB(t) Tr(t(cid:6)(cid:7) (cid:19) φ).t(cid:7)∈MPB(do( A,t)) Tr(t(cid:7)) and t(cid:7) (cid:19) φ = do( A, t(cid:7)), therefore we need to show that:t(cid:7)(cid:7)∈MPB(do( A,t))t(cid:7)∈MPB(t)This follows from Lemma 54. (cid:2)Theorem 41. Postulates (DP1), (DP3) and (DP4) are satisfied when ∗ is defined as in Definition 32 for any situation t and domain-dependent uniform formulae φ and ψ .We prove this theorem as a series of lemmas.M |(cid:16) MPB(cid:2)t(cid:7)(cid:7)(cid:3), do( Aψ , t).We need to show that M |(cid:16) γ [tthere is a situation tsuch that(cid:2)(cid:7)(cid:7) = doM |(cid:16) B(cid:7)190S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192Lemma 58 (DP1). Under the conditions of Theorem 41, if ψ |(cid:16) φ, then K ((t ∗ φ) ∗ ψ) = K (t ∗ ψ).Proof. Suppose γ ∈ K ((t ∗ φ) ∗ ψ), i.e.:(cid:3)(cid:3)(cid:2)M |(cid:16) Bel(cid:2)γ , doAψ , do( Aφ, t).(A.29)We need to show that γ ∈ t ∗ ψ , i.e., M |(cid:16) Bel(γ , do( Aψ , t)). Let t(cid:7)(cid:7)be a situation such that(A.30)(cid:7)(cid:7)]. Since Aψ is a revision action for ψ and t ∗ ψ is defined, by the successor axiom for B,(cid:7)(cid:7)(cid:7)(cid:3)(cid:3), t(cid:4)t(cid:2)t∧ t∧ ψAψ , tAlso, since ψ |(cid:16) φ, it follows that M |(cid:16) φ[tsuccessor state axiom for B twice to obtain M |(cid:16) B(do( Aψ , do( Aφ, tthat M |(cid:16) MP(do( Aψ , do( Aφ, tM |(cid:16) γ [do( Aψ , do( Aφ, t∗∗∗there is a situation t(cid:5).(cid:7)]. By Lemma 20 and (A.31), M |(cid:16) ψ[do( Aφ, t(A.31)(cid:7))]. Therefore, we can apply the(cid:7))), do( Aψ , do( Aφ, t))). Now, if it were also the case(cid:7))), do( Aψ , do( Aφ, t))), then the theorem would follow since we could infer from (A.29) that(cid:7)(cid:7)] would follow from this, Lemma 20, and (A.31). Suppose to the contrary that(cid:2)(cid:2)M |(cid:16) BdoAψ , doAφ, tAψ , do( Aφ, t)(cid:3)(cid:3)(cid:2)∧ plt∗∗∗(cid:3)(cid:2)< pl(cid:2)do(cid:2)Aψ , doAφ, t(cid:3)(cid:3)(cid:3)(cid:7).(A.32)(cid:7)))], and M |(cid:16) γ [tsuch that:(cid:3)(cid:3)(cid:2)(cid:7)(cid:2), do∗∗(cid:2)(cid:2)M |(cid:16) BSince Aψ is a revision action for ψ , and (t ∗ φ) ∗ ψ is defined, it follows from the successor state axiom for B that there isa situation tsuch that:∗∗(cid:3)(cid:3), do( Aφ, t)(cid:2)∧ t∗∗∗ = do(cid:3)∗∗(cid:5)∗∗(cid:4)t∧ ψdoAφ, tWe can infer from this and the successor state axiom for B that there is a situation t∗). It follows from this, (A.33), and Lemma 20 that M |(cid:16) ψ[tdo( Aφ, twe can use the successor state axiom for B again to obtain: M |(cid:16) B(do( Aψ , tthat M |(cid:16) pl(tsuccessor state axiom for pl that: M |(cid:16) pl(do( Aψ , t(A.33)∗∗ =∗]. Also, since t ∗ ψ is defined, M |(cid:16) ψ[t]. Therefore,∗), do( Aψ , t)). We can now see from (A.30)∗)). However, it follows from (A.32), the situation equations, and repeated application of the(cid:7)(cid:7)). Contradiction. (cid:2)such that: M |(cid:16) B(t(cid:7)(cid:7)) (cid:2) pl(do( Aψ , t∗)) < pl(t∗, t) ∧ t∗Aψ , t.Lemma 59. Under the conditions of Theorem 41, φ ∈ K ((t ∗ φ) ∗ ψ).Proof. We need to show that M |(cid:16) Bel(φ, do( Aψ , do( Aφ, t))). Let t(cid:7)(cid:7)(cid:7)be a situation such that:M |(cid:16) MPB(cid:2)t(cid:7)(cid:7)(cid:7)(cid:2)(cid:3)(cid:3), doAψ , do( Aφ, t).(A.34)Since Aφ is a revision action for φ and t ∗ φ is defined, it follows from two applications of the successor state axiom for Bthat there is a situation tsuch that:(cid:7)M |(cid:16) B(cid:3)(cid:2)t(cid:7), t(cid:2)(cid:7)(cid:7)(cid:7) = do∧ t(cid:2)Aψ , do(cid:3)(cid:3)(cid:7)(cid:4)t(cid:7)(cid:5).∧ φAφ, t(cid:7)(cid:7)(cid:7)]. (cid:2)It follows from Lemma 20 that M |(cid:16) φ[t(DP3) follows as a corollary.Corollary 60 (DP3). Under the conditions of Theorem 41, if φ ∈ K (t ∗ ψ), then φ ∈ K ((t ∗ φ) ∗ ψ).Lemma 61 (DP4). Under the conditions of Theorem 41, if ¬φ /∈ K (t ∗ ψ), then ¬φ /∈ K ((t ∗ φ) ∗ ψ).Proof. Suppose that there is a situation t(cid:5).(cid:3), do( Aψ , t)M |(cid:16) MPB∧ φ(cid:2)t(cid:4)t∗∗∗∗∗∗such that:(A.35)We need to show that there is a situation t∧ φM |(cid:16) MPBAψ , do( Aφ, t), do(cid:2)t(cid:3)(cid:3)+(cid:2)such that:+(cid:4)t+(cid:5).Since Aψ is a revision action for ψ and t ∗ ψ is defined, it follows from the successor state axiom for B, Lemma 20, and(A.35), there is a tsuch that:∗M |(cid:16) B(cid:3)(cid:2)t∗, t(cid:2)∗∗ = do∧ t(cid:3)∗(cid:4)∧ (φ ∧ ψ)t∗(cid:5).Aψ , t(A.36)S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192191Since t ∗ φ and (t ∗ φ) ∗ ψ are defined, it follows from two applications of the successor state axiom for B that M |(cid:16)∗)), do( Aψ , do( Aφ, t))), then theB(do( Aψ , do( Aφ, ttheorem would follow because Lemma 20 implies that:∗)), do( Aψ , do( Aφ, t))). Now, if we can show that M |(cid:16) MP(do( Aψ , do( Aφ, t∗(cid:5)≡ φM |(cid:16) φ(cid:2)(cid:4)do(cid:4)tand M |(cid:16) φ[do( Aψ , do( Aφ, t(cid:2)tM |(cid:16) B, do(cid:7)(cid:7)(cid:7)(cid:2)(cid:3)(cid:3)Aψ , do( Aφ, t).(cid:2)∗(cid:3)(cid:3)(cid:5),Aφ, tAψ , do∗))] follows from this and (A.36). Let t(cid:7)(cid:7)(cid:7)be a situation such that:We need to show that M |(cid:16) pl(do( Aψ , do( Aφ, t(A.37) that there is a situation t(cid:2)(cid:7)(cid:7)(cid:7) = dosuch that:(cid:2)M |(cid:16) B∧ t(cid:2)tAψ , doAφ, t, t(cid:3)(cid:3)(cid:3)(cid:7)(cid:7)(cid:7)(A.37)(cid:7)(cid:7)(cid:7)). It follows from the successor state axiom for B, Lemma 20, and∗))) (cid:2) pl(t(cid:4)t(cid:7)(cid:5).∧ ψ(A.38)We can use the successor state axiom for B again to yield:M |(cid:16) B(cid:2)(cid:2)do(cid:7)Aψ , t(cid:3)(cid:3), do( Aψ , t).From this and (A.35), it follows that M |(cid:16) pl(twe can infer that M |(cid:16) pl(do( Aψ , do( Aφ, t∗))) (cid:2) t(cid:7)(cid:7)(cid:7), as desired. (cid:2)∗∗) (cid:2) pl(do( Aψ , t(cid:7))). Using (A.36), (A.38), and the successor state axiom for pl,References[1] S. Shapiro, M. Pagnucco, Y. Lespérance, H.J. Levesque, Iterated belief change in the situation calculus, in: A.G. Cohn, F. Giunchiglia, B. Selman (Eds.),Principles of Knowledge Representation and Reasoning: Proceedings of the Seventh International Conference (KR2000), Morgan Kaufmann Publishers,San Mateo, CA, 2000, pp. 527–538.[2] J. McCarthy, Situations, Actions and Causal Laws, Stanford University Artificial Intelligence Project Memo 2, Stanford University, Stanford, CA, 1963.[3] J. McCarthy, P.J. Hayes, Some philosophical problems from the standpoint of artificial intelligence, in: B. Meltzer, D. Michie (Eds.), Machine Intelligence,vol. 4, Edinburgh University Press, Edinburgh, 1969, pp. 463–502.[4] R. Reiter, The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression, in: V. Lifschitz(Ed.), Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy, Academic Press, 1991, pp. 359–380.[5] R.B. Scherl, H.J. Levesque, The frame problem and knowledge-producing actions, in: Proceedings of the Eleventh National Conference on ArtificialIntelligence, AAAI Press/The MIT Press, 1993, pp. 689–695.[6] R.B. Scherl, H.J. Levesque, Knowledge, action, and the frame problem, Artificial Intelligence 144 (1–2) (2003) 1–39.[7] G. Lakemeyer, H.J. Levesque, AOL: A logic of acting, sensing, knowing, and only knowing, in: Principles of Knowledge Representation and Reasoning:Proceedings of the Sixth International Conference (KR-98), 1998, pp. 316–327.[8] H.J. Levesque, What is planning in the presence of sensing?, in: Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96),Portland, OR, 1996, pp. 1139–1146.[9] P. Gärdenfors, Knowledge in Flux: Modeling the Dynamics of Epistemic States, MIT Press/Bradford Books, Cambridge, MA, 1988.[10] H. Katsuno, A.O. Mendelzon, Propositional knowledge base revision and minimal change, Artificial Intelligence 52 (1991) 263–294.[11] R.C. Moore, Semantical considerations on nonmonotonic logic, Artificial Intelligence 25 (1985) 75–94.[12] G. De Giacomo, H.J. Levesque, Progression using regression and sensors, in: Proceedings of the Sixteenth International Joint Conference on ArtificialIntelligence (IJCAI-99), 1999, pp. 160–165.[13] R. Reiter, Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems, MIT Press, Cambridge, MA, 2001.[14] H.J. Levesque, F. Pirri, R. Reiter, Foundations for a calculus of situations, Electronic Transactions on Artificial Intelligence 2 (3–4) (1998) 159–178.[15] F. Pirri, R. Reiter, Some contributions to the metatheory of the situation calculus, Journal of the ACM 46 (3) (1999) 325–361.[16] C.E. Alchourrón, P. Gärdenfors, D. Makinson, On the logic of theory change: Partial meet contraction and revision functions, Journal of Symbolic Logic 50(1985) 510–533.[17] P. Peppas, A.C. Nayak, M. Pagnucco, N.Y. Foo, R. Kwok, M. Prokopenko, Revision vs. update: Taking a closer look, in: W. Wahlster (Ed.), Proceedings ofthe Twelfth European Conference on Artificial Intelligence (ECAI-96), 1996, pp. 95–99.[18] A. Darwiche, J. Pearl, On the logic of iterated belief revision, Artificial Intelligence 89 (1–2) (1997) 1–29.[19] C. Boutilier, Iterated revision and minimal change of conditional beliefs, Artificial Intelligence 25 (3) (1996) 262–305.[20] W. Spohn, Ordinal conditional functions: A dynamic theory of epistemic states, in: W. Harper, B. Skyrms (Eds.), Causation in Decision, Belief Change,and Statistics, Kluwer Academic Publishers, 1988, pp. 105–134.[21] D.K. Lewis, Counterfactuals, Harvard University Press, 1973.[22] J. McCarthy, Epistemological problems in artificial intelligence, in: Proceedings of the Fifth International Joint Conference on Artificial Intelligence,Morgan Kaufmann Publishers, Los Altos, CA, 1977, pp. 1038–1044.[23] J. McCarthy, Circumscription: A form of non-monotonic reasoning, Artificial Intelligence 13 (1–2) (1980) 27–39.[24] J. McCarthy, Modality, si! modal logic, no!, Studia Logica 59 (1) (1997) 29–32.[25] N. Friedman, J.Y. Halpern, A knowledge-based framework for belief change. Part II: Revision and update, in: J. Doyle, E. Sandewall, P. Torasso (Eds.),Proceedings of Knowledge Representation and Reasoning (KR-94), 1994, pp. 190–201.[26] S. Shapiro, Belief change with noisy sensing and introspection, in: L. Morgenstern, M. Pagnucco (Eds.), Working Notes of the IJCAI-05 Workshop onNonmonotonic Reasoning, Action, and Change (NRAC 2005), 2005, pp. 84–89.[27] S. Shapiro, M. Pagnucco, Iterated belief change and exogenous actions in the situation calculus, in: R. López de Mántaras, L. Saitta (Eds.), Proceedingsof the Sixteenth European Conference on Artificial Intelligence (ECAI-04), IOS Press, Amsterdam, 2004, pp. 878–882.[28] A. Hunter, Belief change in the presence of actions and observations: A transition system approach, Ph.D. thesis, Simon Fraser University, Vancouver,BC, July 26, 2006.[29] R. Demolombe, M.P. Pozos Parra, A simple and tractable extension of situation calculus to epistemic logic, in: Z.W. Ras, S. Ohsuga (Eds.), Proceedingsof the Twelfth International Symposium on Intelligent Systems, in: LNAI, vol. 1932, 2000, pp. 515–524.[30] R. Demolombe, M.P. Pozos Parra, Belief change in the situation calculus: A new proposal without plausibility levels, in: Proceedings of the Workshopon Belief Revision and Dynamic Logic at the Seventeenth European Summer School on Logic, Language and Information, 2005.[31] A. del Val, Y. Shoham, A unified view of belief revision and update, Journal of Logic and Computation 4 (1994) 797–810.192S. Shapiro et al. / Artificial Intelligence 175 (2011) 165–192[32] C. Boutilier, Revision sequences and nested conditionals, in: Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence(IJCAI-93), 1993, pp. 519–525.[33] M.-A. Williams, Transmutations of knowledge systems, in: Proceedings of the Fourth International Conference on Principles of Knowledge Representa-tion and Reasoning (KR-94), 1994, pp. 619–629.[34] C. Boutilier, Generalized update: Belief change in dynamic settings, in: Proceedings of the Fourteenth International Joint Conference on ArtificialIntelligence (IJCAI-95), 1995, pp. 1550–1556.[35] K. Segerberg, Belief revision from the point of view of doxastic logic, Bulletin of the IGPL 3 (1995) 535–553.[36] A. Herzig, D. Longin, Sensing and revision in a modal logic of belief and action, in: F. can Harmelen (Ed.), Proceedings of the Fifteenth EuropeanConference on Artificial Intelligence, IOS Press, 2002, pp. 307–311.[37] H. van Ditmarsch, W. van der Hoek, B. Kooi, Dynamic Epistemic Logic, Synthese Library, vol. 337, Springer, 2007.[38] H.P. van Ditmarsch, A. Herzig, T.D. Lima, Optimal regression for reasoning about knowledge and actions, in: Proc. of the Twenty-Second AAAI Conferenceon Artificial Intelligence, Vancouver, British Columbia, Canada, July 22–26, 2007, pp. 1070–1076.[39] J.V. Benthem, Dynamic logic for belief revision, Journal of Applied Non-Classical Logics 17 (2) (2007) 129–155.[40] B. van Linder, W. van der Hoek, J.-J.C. Meyer, Actions that make you change your mind, in: A. Laux, H. Wansing (Eds.), Knowledge and Belief inPhilosophy and Artificial Intelligence, Akademie Verlag, Berlin, 1995, pp. 103–146.[41] M. Thielscher, Representing the knowledge of a robot, in: A. Cohn, F. Giunchiglia, B. Selman (Eds.), Principles of Knowledge Representation and Rea-soning: Proceedings of the Sixteenth Conference, Morgan Kaufmann, 2000, pp. 109–120.[42] M. Thielscher, Introduction to the fluent calculus, Electronic Transactions on Artificial Intelligence 2 (3–4) (1998) 179–192.[43] S. Shapiro, Y. Lespérance, H.J. Levesque, Specifying communicative multi-agent systems, in: W. Wobcke, M. Pagnucco, C. Zhang (Eds.), Agents andMulti-Agent Systems—Formalisms, Methodologies, and Applications, in: LNAI, vol. 1441, Springer-Verlag, Berlin, 1998, pp. 1–14.