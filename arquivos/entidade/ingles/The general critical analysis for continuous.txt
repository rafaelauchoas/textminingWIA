AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptHHS Public AccessAuthor manuscriptNeurocomputing. Author manuscript; available in PMC 2017 January 29.Published in final edited form as:Neurocomputing. 2016 January 29; 175(Pt A): 40–46. doi:10.1016/j.neucom.2015.09.103.The general critical analysis for continuous-time UPPAM recurrent neural networks*Chen Qiao†, Wen-Feng Jing‡, Jian Fang§, and Yu-Ping Wang¶Chen Qiao: qiaochen@mail.xjtu.edu.cn, cqiao@tulane.edu; Wen-Feng Jing: wfjing@mail.xjtu.edu.cn; Jian Fang: jfang3@tulane.edu; Yu-Ping Wang: wyp@tulane.edu†School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, 710049, P.R. China and with the Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA‡School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, 710049, P.R. China§School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, 710049, P.R. China and with the Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA¶Department of Biomedical Engineering, Tulane University, New Orleans, LA, 70118, USA and the Center of Genomics and Bioinformatics, Tulane University, New Orleans, LA, 70112, USAAbstractThe uniformly pseudo-projection-anti-monotone (UPPAM) neural network model, which can be considered as the unified continuous-time neural networks (CNNs), includes almost all of the known CNNs individuals. Recently, studies on the critical dynamics behaviors of CNNs have drawn special attentions due to its importance in both theory and applications. In this paper, we will present the analysis of the UPPAM network under the general critical conditions. It is shown that the UPPAM network possesses the global convergence and asymptotical stability under the general critical conditions if the network satisfies one quasi-symmetric requirement on the connective matrices, which is easy to be verified and applied. The general critical dynamics have rarely been studied before, and this work is an attempt to gain an meaningful assurance of general critical convergence and stability of CNNs. Since UPPAM network is the unified model for CNNs, the results obtained here can generalize and extend the existing critical conclusions for CNNs individuals, let alone those non-critical cases. Moreover, the easily verified conditions for general critical convergence and stability can further promote the applications of CNNs.KeywordsContinuous-time recurrent neural network; uniformly pseudo-projection-anti-monotone network; general critical condition; dynamical analysis*This research was supported by NSFC No. 11101327, No. 11471006 and No. 11171270, and was partially supported by NIH R01 GM109068 and R01 MH104680.Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.1 IntroductionPage 2The two basic elements of a recurrent neural network (RNN) are: the synaptic connections among the neurons and the nonlinear activation functions deduced from the input-output properties of the involved neurons. For applications such as associative memory, synaptic connections among the neurons are designed to encode the memories we hope to recover. The activation functions are assumed to capture the complex, nonlinear response of neurons of the brain. For different purpose of simulations and applications, both of them are preassigned before use. So understanding their properties are very important, and especially exploring the characteristics of the activation functions are quite crucial to determine the performance of the RNNs. For the commonly used RNN individuals, the activation functions are monotonically nondecreasing and saturated. To study and apply RNNs only based on such two features is far from enough. To overcome the non-thorough descriptions of activation functions, many special cases of activation functions have been brought forward, resulting in many different RNNs individuals. Furthermore, in order to obtain more useful results of RNNs, e.g., the convergence and stability of those individuals, additional strict requirements are unavoidable to impose on the networks for the lack of in-depth descriptions on the activation functions. Obviously, since those individuals are studied separately, it’s inevitable that there exist large numbers of redundancy of analysis for those individual models. In order to reduce the superabundance, establishing a harmonization methodology is a challenging work.In [16], Xu and Qiao put forward two novel concepts: uniformly anti-monotone as well as the pseudo-projection properties of the activation functions, which discover more essential characteristics other than the nondecreasing and bounded properties of the commonly used activation functions. It is shown that the proposed uniformly pseudo-projection anti-monotone (UPPAM) operator can embody most of activation operators (the precise definition of uniformly pseudo-projection-anti-monotone operator will be given in Section II), e.g., nearest-point projection, linear saturating operator, signum operator, symmetric multi-valued step operator, multi-threshold operator, winner-take-all operator, etc. Thus, the UPPAM operator can be considered as a framework of formalizing most of the activation operators of RNNs.In this paper, we use the concept of UPPAM operators to establish a unified model for continuous-time RNNs. Let’s consider the following continuous-time UPPAM RNNs mdoel:(1)where x(t) = (x1(t), x2(t), ···, xN(t))T is the neural network state, G = (g1, g2, ···, gN)T is the nonlinear activation operator deduced from all the activation functions gi, and G owns the uniformly pseudo-projection-anti-monotone property. Both A and W are the connective weight matrices, b, q are two fixed external bias vectors and τ is the state feedback coefficient. The form of model (1) includes two basic kinds of continuous-time RNNs [17], i.e., the static RNNs and the local field RNNs. Furthermore, as proved in [16], most Neurocomputing. Author manuscript; available in PMC 2017 January 29.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 3activation operators are special cases of the UPPAM operator. So, model (1) can be considered as a unified model of continuous-time RNNs and can include almost all of the existing continuous-time RNNs specials [4], e.g., Hopfield-type neural networks, Brain-State-in-a-Box neural networks, Recurrent Back-propagation neural networks, Mean-field neural networks, Bound-constraints Optimization Solvers, Convex Optimization Solvers, Recurrent Correlation Associative Memories neural networks, Cellular neural networks, etc. In addition, since model (1) owns the essential characteristics of the activation functions, i.e., the uniformly anti-monotone as well as the pseudo-projection properties, it can be expected that the analysis of model (1), especially the dynamics analysis can give more in-depth results and provide the unified and concise characterization of the continuous-time RNNs models. The main purpose of this paper will focus on discovering some essential global convergence and stability for the unified model (1), i.e., the critical convergence and stability.For RNNs, one difficult problem of dynamics analysis lies in the critical analysis. Define a discriminant matrixwhere Γ is a positive definite diagonal matrix, P is a diagonal matrix defined by the network, and W and A are the weight matrices. If there exists a positive definite diagonal matrix Γ, such that S(Γ, 2Λ − B) > 0 (i.e., S(Γ, 2Λ − B) is positive definite), where Λ and B are the anti-monotone and pseudo-projection constant matrices of the network (the definitions of them are given in Section II), then RNNs have exponential stability [4]. Many stability results have been achieved for RNNs individuals under various specifications of S(Γ, 2Λ − B) > 0 (typically, when S(Γ, 2Λ − B) > 0 is an M-matrix), and they are called as the non-critical dynamical analysis [1]. On the other hand, if there exists a positive definite diagonal matrix Γ such that S(Γ, V) is negative definite, here V = diag{r1, r2, ···, rN} with each ri > 0 being the maximum inversely Lipschitz constant of gi (i.e., for all s, t ∈ ℛN, |gi(t) − gi(s)| ≥ ri|t − s|), then RNNs are globally exponentially unstable [1, 7]. Since S(Γ, 2Λ − B) > 0 is the sufficient condition on the globally exponential stability of RNNs, and S(Γ, V) ≥ 0 is the necessary condition for RNNs to be globally stable, it is quite natural to explore the gap between S(Γ, 2Λ − B) ≤ 0 (i.e., S(Γ, 2Λ − B) is negative semi-definite) and S(Γ, V) ≥ 0 (i.e., S(Γ, V) is positive definite). Such a gap is called the general critical condition, and the dynamics analysis of RNNs under such condition is referred as the general critical dynamics analysis.For any application and practical design of RNNs, such as pattern recognition, associative memories, or as optimization solvers, the convergence and stability of RNNs are both prerequisite. For instance, when an RNN is used in associative memory or pattern recognition, any pattern we hope to store has to be an equilibrium point of the RNN. In addition, to ensure that each stored pattern can be retrieved even with noises, each equilibrium point must possess the stability. When the RNN is employed as an optimization solver, the possible optimal solutions correspond to the equilibrium of the RNN, and the Neurocomputing. Author manuscript; available in PMC 2017 January 29.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 4convergence of the RNN is a guarantee of finding the optimal solutions. Since the general critical conditions can be considered essentially as the distinct region of stability and non-stability of RNNs, studying the general critical dynamics behaviors of an RNN can find broad applications.Recently, due to the difficulty in the dynamical analysis of RNNs for general critical conditions, most of the studies on critical analysis have been focused on the special critical conditions, i.e., considering the asymptotic behaviors of RNNs under the condition that S(Γ, 2Λ − B) ≥ 0 (this is because S(Γ, 2Λ − B) > 0 is already known to be globally exponential stable and S(Γ, 2Λ − B) = 0 is a special case of the general critical condition). Even for this special critical condition, there only exists a few results since the analysis is much more difficult than the dynamics analysis under the non-critical condition that S(Λ, L) > 0. In [15], the globally exponential stability of a static neural network with projection operator (a special kind of UPPAM operator) has been proven under the condition that I − W is nonnegative (which is a special case of S(Γ, 2Λ − B) ≥ 0). The special critical convergence of a static neural network model with nearest point projection activation operator (special case of projection operator) on a region defined by the network has been achieved in [1] when W is quasi-symmetric. Some general critical stability conclusions for the static and the local field continuous-time RNNs with projection activation operators have been achieved in [2], but they require the network to satisfy one bounded matrix norm. In [4], for the presented unified continuous-time RNNs, namely, UPPAM RNNs, the special critical global convergence is obtained with some bound requirements on the defined nonlinear norm, but such requirements can not be verified easily in applications. In [5], some improvements on dynamics analysis of the UPPAM networks have been obtained, while they are still under the special critical conditions.In the present paper, we give some solutions on how to assure the convergence and stability under the general critical conditions. By applying the energy function method and Lasalle invariance principle to the unified continuous-time RNNs model (1), we obtain the global convergence and asymptotical stability under some general critical conditions, that is, S(Γ, 2Λ − B)+ Ψ is positive definite for one diagonal matrix Ψ. The results only require the network to satisfy some quasi-symmetric conditions on the connection matrices. Since the conclusions obtained here are for the unified RNNs model under the general critical conditions, they can sharpen and generalize, to a large extent, the latest critical results given by [1, 2, 4, 5, 15], and they can further be extended to those non-critical conclusions (see, e.g., [6–14,18–25] and the references quoted there). Furthermore, they can can be applied directly to many individual RNN models mentioned above. They can be widely applied to solve the linear variational inequality and many other optimization problems, etc. Therefore, the study here provides an insight on the unified continuous-time RNNs model with critical analysis.2 PreliminariesFor the activation operator G, the domain, range and fixed-point set of G are respectively defined by D(G), R(G) and F(G), and D(G) = R(G) ⊆ ℝN. Assume that ℝN is embedded with Euclidean norm ||·|| and inner product 〈 ·, · 〉.Neurocomputing. Author manuscript; available in PMC 2017 January 29.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 5For any x = (x1, x2, ···, xN)T ∈ D(G), writeG is said to be diagonal if gi(x) = gi(xi) holds for each i = 1, 2, ···, N.Definition 2.1[16] (i) An operator G is said to be a pseudo-projection if there exists a positive definite diagonal matrix B = diag{β1, β2, ···, βN}, such that BR(G) ⊆ D(G) and G = GBG (i.e., G(x)=G(BG(x)), ∀x ∈ D(G)). In this case, we say that G is a B-projection.(ii) An operator G is said to be λ-uniformly anti-monotone (λ-UAM) if there is a positive constant λ such that for any x ∈ D(G) and y ∈ BR(G),(iii) An operator G is uniformly pseudo-projection-anti-monotone (UPPAM) if it is pseudo-projection and uniformly anti-monotone; specially, we say it is (B, λ)-UPPAM whenever it is B-projection and λ-UAM.(2)Definition 2.2Let Λ = diag{λ1, λ2, ···, λN} and B = diag{β1, β2, ···, βN}. G is said to be diagonally (B, Λ)-UPPAM if each component gi of G is a βi-projection and λi-UAM.In [16], it is shown that most of the activation operators of RNNs in the literature are special cases of UPPAM operators. Thus, the RNNs with UPPAM operators, i.e., the uniformly pseudo-projection-anti-monotone neural networks provide an appropriate and unified framework, within which most of the known RNN models can be embedded and uniformly studied.Throughout the paper, the identity matrix is denoted by I. For a positive semi-definite diagonal matrix Δ = diag{δ1, δ2, …, δN}, let .3 General Critical dynamics ResultsIn this section, under the general critical conditions, results on global convergence and asymptotic stability for the unified continuous-time RNN model are established, which are quite easy to be verified in applications. In the following, we denote the equilibrium state set of (1) by Throughout this paper, we suppose that Θ is bounded, closed and convex., and the range of nonlinear activation operator, i.e., R(G), by Θ. Lemma 3.1For any x0 ∈ A(Θ) + b, x(t, x0), the solution of (1), satisfies x(t, x0) ∈ A(Θ) + b(t ≥ 0).Neurocomputing. Author manuscript; available in PMC 2017 January 29.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 6Proof—By the differential equation theory, we have(3)where . Since , and A(Θ)+ b is a bounded, closed and convex subset, then sum Further, by (3), we know x(t, x0) ∈ A(Θ) + b when x0 ∈ A(Θ) + b., should satisfy , the limit of the . For any v ∈ Θ, we define T(v) = AG(Wv + q) + b. Since that Θ is bounded, closed and convex, then by Brouwer’s fixed point theorem, T has at least one fixed point v*, so namely, is not empty.Theorem 3.1Assume that G is diagonally (B, Λ)-UPPAM with Θ being a bounded, closed and convex subset of ℝN, and A is a nonzero diagonal matrix. If there exists a positive definite diagonal matrix Γ and a diagonal matrix Ψ, such that (2Λ − B) Γ − ΓAW + Ψ is positive definite, QAW is symmetric (here Q = ((2Λ − B) Γ + Ψ)Λ−1) and Q is a positive definite diagonal matrix, then RNN model (1) is globally convergent on A(Θ) + b when disconnected. Moreover, when x* is the unique equilibrium point of (1), then x* is globally asymptotically stable on A(Θ) + b. is Proof—Denote A = diag{a1, a2, ···, aN}, Γ = diag{ξ1, ξ2, …, ξN}, D = diag{d1, d2, ·· ·, dN} and Ψ = diag{ϕ1, ϕ2, ···, ϕN}. For any trajectory x(t) of (1) starting from x0 ∈ A(Θ) + b, it follows from Lemma 3.1 that x(t) ∈ A(Θ) + b. Let y0 = Wx0 + q, y(t) = Wx(t) + q, z(t) = AG(y(t)) + b and u(t) = z(t) − x(t).LetSince x(t) ∈ A(Θ) + b, there exists p(t) ∈ Θ, such that x(t) = Ap(t) + b. Then, Ap(t) = x(t) − b. Meanwhile, noting that QB, QAW and ΓAW are all symmetric, and thus WT(ΓA) = (ΓAW)T = ΓAW for the case that both A and Γ are diagonal. Then, a direct calculation showsNeurocomputing. Author manuscript; available in PMC 2017 January 29.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 7(4)Since G is a B-projection and p(t) ∈ Θ, it is clear that G(Bp(t)) = p(t). Denote the diagonal matrix Q = diag{q1, q2, ···, qN}. By the diagonal nonlinear property of G, one can get thatFor each component gi of G being a βi-projection and λi-UAM, we haveFurther, since both Q and are Λ positive definite diagonal matrices, we haveThen from (4), we directly have(5)(6)By S = (2Λ − B) Γ − ΓAW + Ψ being positive definite, we get that λmin(S) > 0. From (6), we knowObviously, it can be deduced that , and the equal sign holds if and only if z(t) = (7)x(t), i.e., disconnected, then by LaSalle invariance principle [26], we know that RNN model (1) is . Moreover, since x(t) ∈ A(Θ) + b is bounded and is Neurocomputing. Author manuscript; available in PMC 2017 January 29.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 8globally convergent on Θ. Furthermore, when both attractive and stable on A(Θ) + b since A(Θ) + b is bounded, i.e., x* is globally asymptotically stable on A(Θ) + b. Thus, Theorem 3.1 is proved., it is easy to deduce that x* is Corollary 3.1Remark 3.1Assume that G is diagonally (B, Λ)-UPPAM with Θ being a bounded, closed and convex subset of ℝN, and A is a nonzero diagonal matrix. If there exists a positive definite diagonal matrix Γ such that Γ (2Λ − AW) is positive definite, then RNN model (1) is globally convergent on A(Θ) + b when equilibrium point of (1), then x* is globally asymptotically stable on A(Θ) + b. is disconnected. Moreover, when x* is the unique Proof—Let Ψ = BΓ in Theorem 3.1, then we have (2Λ − B) Γ − ΓAW + Ψ = Γ(2Λ − AW). Since both Λ and Γ are diagonal and positive definite matrices, we have Q = 2ΛΓΛ−1 = 2Γ is positive definite diagonal matrix, and QAW = 2ΓAW is obviously symmetric by the positive definite requirement of Γ(2Λ − AW). So the corollary follows directly from Theorem 3.1.Studying the dynamic behaviors of unified model (1) can provide uniform results for RNNs and thus can deduce the numerous redundancy existing in the RNNs individuals. Further, since model (1) owns the pseudo projection as well as the anti-monotone property, then by utilizing these two properties, we can obtain some meaningful conclusions.Recently, the dynamic studies of RNNs have attracted great interest in the critical analysis. It should be pointed out that due to the difficulty in analysis, most of them are based on the special critical conditions, i.e., discriminant matrix S(Γ, 2Λ − B) is positive semi-definite. In addition, in order to assure the stability, some other restrictions are required on the networks [1,2,4,5,15]. Obviously, just studying the special critical dynamics is far from enough in both theory and applications, and additional requirements on the networks are quite hard for applications.Theorem 3.1 and Corollary 3.1 exploit new methods to assure the global asymptotical stability and global convergence for the unified continuous-time RNN model (1). The results obtained here are under the general critical conditions, and do not need difficultly verified requirements. For Theorem 3.1, in addition to the general critical conditions, it only requires the UPPAM network to meet a quasi-symmetric conditions. That is because, in the sense of positive definite, one can easily choose a diagonal matrix Ψ in Theorem 3.1 satisfying Ψ > (B Ψ 2Λ)Γ, where B, Λ and Γ all are positive definite matrices. Thus, Q is positive definite. Then by Theorem 3.1, in order to assure the global stability and convergence, we only need to verify that QAW is symmetric, where both A and W are connection matrices of the network. Corollary 3.1 shows that existing critical results are the special cases of the results obtained in this paper. The critical condition that Γ((2Λ − B) − AW) ≥0 in [1, 4, 5, 15] is a special case of (2Λ − B) Γ − ΓAW + Ψ ≥ 0. Further, since both Γ and B are positive definite diagonal matrices, thus Γ((2Λ − B) − AW) ≥ 0 is a particular case of Γ (2Λ − AW) > 0. The latter is just the only requirement in Corollary 3.1 to guarantee the global stability and convergence for model (1). The critical dynamic Neurocomputing. Author manuscript; available in PMC 2017 January 29.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 9conclusions of Theorem 3.1 and Corollary 3.1 not only summarize, but also deepen to a large extent most of the existing results for the RNNs individuals.4 Illustrative ExamplesIn this section, we provide two illustrative examples to demonstrate the validity of the critical convergence and stability results formulated in the previous section. It should be noticed that the known stability and convergence results developed in literature can not be applied here.Example 4.1Consider the following UPPAM RNN:here each gi (i = 1, 2, ···, 6) is defined as follows:(8)and A = diag{−1, 2, −3, 4, −5, 6}, q = [−1.2290, −0.9133, 0.1524, 2.8258, −1.5383, 0.9961]T, b = [−1.782, 1.4427, −0.1067, 1.9619, 3.0046, −2.7749]T.In addition, Λ = B = diag{1, 1/2, 1/3, 1/4, 1/5, 1/6}, and the equilibrium state set, a single state set, which contains only one state (−1, −2.5, −3, 4, 5, 0)T., is For this UPPAM network, almost all of the existing stability conclusions can not be used here. That is because for any positive definite diagonal matrix Γ, (2Λ − B)Γ − ΓAW is neither positive definite nor positive semi-definite, so all the non-critical and the special critical dynamical results (see, the reference mentioned in the Introduction section) are not suitable here. In addition, since this example is for the unified RNNs model, it is totally hard to calculate the nonlinear norm of the network, and the conclusions in [2] can not be used here.In what follows, we will show that Theorem 3.1 established in this paper can be successfully and Ψ = ΛΓ, we applied here. By setting have (2Λ − B) Γ − ΓAW + Ψ ≥ 0, Q = ((2Λ − B)Γ + Ψ)−1) is a positive definite diagonal matrix and QAW is symmetric. Then by Theorem 3.1, it is quite easy to achieve the global Neurocomputing. Author manuscript; available in PMC 2017 January 29.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 10convergence of network (8) on A(Θ) + b with Θ = [−1, 1]6. FIG. 1 depicts the time responses of neural state variables of the system starting randomly from A(Θ) + b.Example 4.2Consider another UPPAM RNN:where each gi(i = 1, 2, ···, N) is defined as follows:W = (Wij)N×N with each Wij is(9)(10)and A = diag{a1, a2, ···, aN} with each ai = (−1)i−1·i. q and b are two N-dim vectors, and bi = (−1)ii.In this case, Λ = B = I. For any positive diagonal matrix Γ, Γ (2Λ − B − AW) is not positive semi-definite, i.e., the latest critical results in [5] and other recent results for the special critical analysis, e.g., [2–4] all can not be used for this example. When Γ = diag{1, 1/2, ···, 1/N}, it is easy to verify that Γ(2Λ − AW) is positive definite. Then by Corollary 3.1, network (9) is global convergent to the unique equilibrium, i.e., the origin. The following FIG. 2 depicts the time responses of neural state variables of network (9) with N = 3 starting randomly from A(Θ) + b, where Θ = [−1, 1]3.5 ConclusionIn the present paper, based on the unified RNN model, i.e., the uniformly pseudo-projection-anti-monotone RNNs model, the corresponding global convergence and the global asymptotic stability under the general critical conditions are given. In addition to the general critical conditions, our conclusions only require that the synaptic connective matrices defined by the network are quasi-symmetric. Comparing to the existing dynamical analysis results for RNNs, the conclusions obtained here demonstrate several advantages. Firstly, they are given under the general critical conditions, which have scarcely been studied before. Secondly, they are for the unified RNN model, so they can be applied to almost all of the existing individuals of RNNs and can be used directly in applications. Finally, with our results there is no need to verify additional intricate requirements on the network, so they can be easily used. In summary, the results achieved here are a significant step towards establishing a unified theory for the dynamics of recurrent neural networks.Neurocomputing. Author manuscript; available in PMC 2017 January 29.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.ReferencesPage 111. Peng J, Xu ZB, Qiao H. A critical analysis on global convergence of Hopfield-type neural networks. IEEE Trans Circuits Syst. 2005; 52:804–814.2. Qiao C, Xu ZB. On the P-critical dynamics analysis of projection recurrent neural networks. Neurocomputing. 2010; 73:2783–2788.3. Qiao C, Xu ZB. Critical dynamics study on recurrent neural networks: Globally exponential stability. Neurocomputing. 2012; 77:205–211.4. Qiao C, Jing WF, Xu ZB. The UPPAM continuous-timeRNN model and its critical dynamics. Neurocomputing. 2013; 77:158–166.5. Qiao C, Chen HB, Jing WF, Sun KF. Towards establishing a meaningful and practical dynamics results for the unified RNN model. Neurocomputing. 2015; 157:315–322.6. Liu XW, Chen TP. A new result on the global convergence of Hopfield neural networks. IEEE Trans Circuits Syst. 2002; 49:1514–1516.7. Qiao H, Peng J, Xu ZB. A Reference Model Approach to Stability Analysis of Neural Networks. IEEE Trans Syst, Man Cybern. 2003; 33:925–936.8. Xia YS, Wang J. On the stability of globally projected dynamical systems. J Optim Theory Appl. 2000; 106:129–150.9. Xu ZB, Qiao H, Peng J. A comparative study of two modeling approaches in neural networks. Neural Netw. 2004; 17:73–85. [PubMed: 14690709] 10. Slavova, A. Cellular Neural Networks: Dynamics and Modelling. Kluwer Academic Publishers Pub; 2003. 11. Guan ZH, Chen G, Qin Y. On equilibria, stability and instability of Hopfield neural networks. IEEE Trans Neural Netw. 2000; 11:534–540. [PubMed: 18249783] 12. Liang XB, Si J. Global exponential stability of neural networks with globally lipschitz continuous activation and its application to linear variational inequality problem. IEEE Trans Neural Netw. 2001; 12:349–359. [PubMed: 18244389] 13. Hu XL, Wang J. Design of General Projection Neural Networks for Solving Monotone Linear Variational Inequalities and Linear and Quadratic Optimization Problems. IEEE Trans Syst, Man Cybern. 2007; 37:1414–1421.14. Qiao H, Peng J, Xu ZB. Nonlinear measures: A new approach to exponential stability analysis for Hopfield-type neural networks. IEEE Trans Neural Netw. 2001; 12:360–370. [PubMed: 18244390] 15. Yang YQ, Cao J. Solving quadratic programming problems by delayed projection neural network. IEEE Trans Neural Netw. 2006; 17:1630–1634. [PubMed: 17131675] 16. Xu ZB, Qiao C. Towards a unified feedback neural network theory: The uniformly pseudo-projection-anti-monotone net. Acta Mathematica Sinica-English Series. 2011; 27:377–396.17. Xu ZB, Qiao H, Peng J. A comparative study of two modeling approaches in neural networks. Neural Netw. 2004; 17:73–85. [PubMed: 14690709] 18. Zhang HG, Liu ZW, Huang GB, Wang ZS. Novel weighting-delay-based stability criteria for recurrent neural networks with time-varying delay. IEEE Trans Neural Netw. 2010; 21:91–106. [PubMed: 19963697] 19. Xia YS, Feng G, Wang J. A novel neural network for solving nonlinear optimization problems with inequality constraints. IEEE Trans Neural Netw. 2008; 19:1340–1353. [PubMed: 18701366] 20. Liu SB, Wang J. A simplified dual neural network for quadratic programming with its KWTA application. IEEE Trans Neural Netw. 2006; 17:1500–1510. [PubMed: 17131664] 21. Cheng L, Hou ZG, Tan M. A neutral-type delayed projection neural network for solving nonlinear variational inequalities. IEEE Trans Circuits Syst II. 2008; 55:806–810.22. Cheng L, Hou ZG, Tan M. A delayed projection neural network for solving linear variational inequalities. IEEE Trans Neural Netw. 2009; 20:915–925. [PubMed: 19423439] 23. Forti M, Nistri P. Global Convergence of Neural Networks with Discontinuous Neuron Activations. IEEE Trans Circuits Syst I. 2003; 50:1421–1435.Neurocomputing. Author manuscript; available in PMC 2017 January 29.  AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptQiao et al.Page 1224. Zeng ZG, Wang J. Improved conditions for global exponential stability of recurrent neural networks with time-varying delays. IEEE Trans Neural Netw. 2006; 17:623–635. [PubMed: 16722168] 25. Liu QS, Wang J. Finite-time convergent recurrent neural network with a hard-limiting activation function for constrained optimization with piecewise-linear objective functions. IEEE Trans Neural Netw. 2011; 22:601–613. [PubMed: 21402513] 26. LaSalle, JP. The Stability of Dynamical Systems. Philadelphia, PA: SIAM; 1987. Neurocomputing. Author manuscript; available in PMC 2017 January 29.  Qiao et al.Page 13AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptFIG. 1. Transient behaviors of RNN in system (8) with random initial points x0 ∈ R(G)Neurocomputing. Author manuscript; available in PMC 2017 January 29.  Qiao et al.Page 14AuthorManuscriptAuthorManuscriptAuthorManuscriptAuthorManuscriptFIG. 2. Transient behaviors of RNN in system (9) with random initial points x0 ∈ R(G)Neurocomputing. Author manuscript; available in PMC 2017 January 29.  