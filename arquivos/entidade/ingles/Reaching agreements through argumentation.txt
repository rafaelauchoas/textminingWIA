Artificial Intelligence 103 ( 199X) l-60 Artificial Intelligence Reaching agreements through argumentation: a logical model and implementation * Sarit Kraus a,b-*, Katia Sycara ‘,‘, Amir Evenchik d,2 a Depurtment of Mathematics h Institute for Advanced Computer Studies. Uni~wsi~ of Mut&md, cmd Computer Scirncu, Bar-Ilun l/niversity, 52-900 Rumat-Gun, Israel Colle,qe Park, MD 20742, USA ’ School of Computer Science. Carnegie Mellon University, Pitt,shurgh. PA 15213. USA * Motorola Israel Ltd., 3 Krumenet.\ki Street. E,l-A\aiv 67899. Israel Received I9 September 1996; received in revised fhnn 7 June I998 Abstract agents self-motivated try to pursue In a multi-agent environment, where cannot be taken for granted. Cooperation must be planned for and achieved their own goals, through cooperation and negotiation. We present a logical model of the mental states of the agents based communication as an on a representation of their beliefs, desires, from exchanges among agents to persuade each other and bring about iterative process emerging and a change agreements. Using categories identified from human multi-agent negotiation, we demonstrate how the logic can be used to specify argument formulation and evaluation. We also illustrate how the developed intentions, and goals. We present argument&on in intentions. We look at argumentation logic can be used to describe different for achieving cooperation as a mechanism types of agents. Furthermore, we present a general Automated Negotiation Agent which we implemented. based on the logical model. Using this system, a user can analyze and explore different methods to negotiate for coordination exists. and argue in a noncooperative The development of negotiating agents is illustrated with an example where the agents plan, act, and resolve conflicts via negotiation in a Blocks World environment. 0 1998 Elsevier Science B.V. All rights reserved. in the framework of the Automated Negotiation Agent environment where no centralized mechanism K~ywrd~s: Automated negotiation: Argumentation: BDI model This material is based upon work supported in part by NSF Grant No. IRI-9423967. NSF grant No. IRI 9724937. NSF Grant No. IRI-9612 13 I, NSF Grant No. IRI-97 12607, ONR Grant No. N00014-96- I - 1722 and Army Research Lab under contract number DAALO197K013.5. We would like to thank Madhura Nirkhe for her contribution to the development of the formal model and to Ariel Stollman for the help in the implementation. author. Email: sarit@cs.bin.ac.il. * Corresponding ’ Email: katia@cs.cmu.edu. ’ Email: evenchik@cig.mot.com. 000+3702/98/S PII: SOOO4-3702(98)00078-2 - >ee front matter 0 1998 Elsevier Science B.V. All rights re\ervcd. 1. Introduction In a multi-agent environment, where self-motivated and negotiation. Negotiation often involves argumentution cannot be taken for granted. Cooperation must be planned cooperation through communication form of an exchange of messages or a dialogue. Arguments to change of negotiating self-interested cooperative. There are different arguments the intentions of another. Irrespective of what argument evaluate the argument and decide whether or not to change its intentions and actions. agents try to pursue their own goals, for and achieved in the are utterances whose aim is the context could make agents more to change is used, the recipient agent must (and consequently agents, the actions) of the listener. Within that could be used by one agent this change of intentions the intentions imagine For example, two mobile robots on Mars, each built to maximize its own utility. RI requests R2 to dig for a certain mineral. R:! refuses. RI responds with a threat: “if you do not dig for me, I will break your antenna”. RI is faced with the task of evaluating this threat. Several considerations must be taken into account, such as whether or not the threat is bounded, what RI’S credibility intact. so on and so forth. R1 may take a different approach if R2 refuses to dig, and respond with for a reward: “if you dig for me today, I will help you move your equipment a promise tomorrow”. Here, R2 needs to evaluate the promise of future reward. it is for R2 to have its antenna is, how important to bringing is essential about agreement Argumentation In about each other or the environment. when agents have such situations, to each other via the exchanged messages. Argumentation may also be called for when agents either do not have the ability or the time to make inferences. This is the case when agents have bounded systems which either may not be complete or may not be closed under inferences knowledge information incomplete impart in noncooperative situations inference [7 l,lOSl. agents In order to negotiate effectively, an agent needs the ability to (a) represent and maintain a model of its own beliefs, desires, goals, and intentions. (b) reason with other agents’ beliefs, desires, goals, and intentions, and (c) influence other agents’ beliefs, intentions, and is an iterative behavior. When agents are noncollaborative, exchange of proposals of the individual goals of the agents. towards reducing conflict and promoting the process of argumentation the achievement Arguments are used by a persuader as a means to dynamically change the preferences, to increase and actions of a persuadee, to intentions, cooperate. Over repeated encounters, agents may analyze each other’s patterns of behavior and reputation. This may to establish an analogue to the human notions of credibility influence such as the “threats” described the sending agent can update and correct its model of the recipient agent, thus refining the evaluation of arguments, later. By observing as we will see in scenarios and argumentation of the persuadee to the arguments, the willingness the reactions its planning knowledge. In this paper we develop a formal logic that forms a basis for the development of a system for argumentation. We offer a logical model of the mental formal axiomatization of their beliefs, desires, intentions and goals. states of the agents based on a representation We present argumentation as an iterative process of exchanges among agents to persuade each other and bring about a change in intentions. Our work on the formal mental model overlaps with the work of others who have developed formal models for communicative S. Kraus et al. /Artijcial Intelligence 104 (1998) i-69 3 agents (e.g., [16,19,64,69,86,115,129,133,138,164]) and for mental models of agents (e.g., [75,152,162]). We will discuss related work in Section 5 and point out the differences of from previous work is that our work with respect to that of others. The main difference point of view. We present a from the argumentation we have developed our formalization set of axioms that allows the agents to automatically in a multi-agent generate and evaluate arguments environment. in a simulated multi-agent a general Automated environ- Based on our formalization, we have developed and implemented Negotiation Agent (ANA) which acts and negotiates ment. In the simulation these agents is assigned an initial set of mental states and inference in every step and decision request evaluation, using arguments, and so on). Once created, if needed. Both the mental states and the different system, several ANA agents can be defined and created. Each of rules which guide it that it takes (goal seeking, argument generation and selection, its desires. the agent will try to accomplish inference its mental states according rules are based on our formal model. Each of the agents changes at the time of the change. The ability well as to define different system to test different argument agent’s negotiation most appropriate argument at any stage of the negotiation. These capabilities are illustrated through an extensive example, where agents negotiate logic to a rule which applies to define mental states for each of the agents. as allows the user of the types and to assess their impact on the effectiveness of the the capability. This also allows the user to evaluate ways of selecting in a Blocks World environment. rules for argument generation, inference The paper is organized the various argument as follows. Section 2 presents formalism and the various agent types which might be engaged describes argument appropriateness. (ANA) and its capabilities argumentation axiomatization. Section 6 presents concluding Section 4 discusses for argument generation types we have identified and how an agent can evaluate the general Automated Negotiation Agent and evaluation, based on the logical literature. logical argumentation the in argumentation. Section 3 Section 5 situates our work within the related remarks. 2. The mental model intentions, cooperative, with We have a set of agents, not necessarily messages. Their mental states are characterized desires, activities are motivated by the will to fulfill selects a consistent ascribes different degrees of importance higher importance. The set of goals motivate to exchange the notions of beliefs, goals, by using and local preferences. Each agent has a set of desires. The agent’s these desires. At any given time, an agent subset of its desires. This serves as its set of current goals. An agent to fulfill goals of to different goals. It prefers the agent’s planning process. the ability The planning process may generate several intentions. Some of these are in what we category and refer to actions that are within the would like to classify as the “intend-to-do” direct control of the agent. Others are among These are propositions not directly within the “intend-that” category the agent’s realm of control, [13,56,57,158]. that it must rely to modify the intention structure of another agent. the persuader wants it to do. While an agent tries to influence on other agents for satisfying. 3 Often, there is room for argumentation when intend-that is the means by which an agent, the persuader, actions are part of a plan. Argumentation to include attempts the intentions the actions of other agents, other agents may try to convince persuadee is not fixed, but dynamically a negotiation process, each agent may update message from another agent. If the argumentation must revise its arguments. its plan in question. it as well. The role of persuader and assumed during the agent interactions. Thus, during a and goals after receiving to fail, the agent which sent it the portion of its plans, and/or seek other sources of satisfying its intentions happens the persuadee, An agent’s belief set includes beliefs concerning the world and beliefs concerning mental states of other agents. An agent may be mistaken in both kinds of beliefs. It may update from other agents. Each the world and after receiving messages its beliefs by observing agent’s actions are based upon its mental model of other agents. The types of arguments (see Section 3) that a persuader generates depend on its knowledge of a persuadee’s mental model. An important piece of knowledge is a persuader’s assessment of the relative effective if it threatens an important persuadee goal. Incomplete that is contrived by a deceitful agent may result in a discrepancy between portrayed mental models. Argumentation can be used to establish a common platform of agreement despite these differences. for argument importance of a persuadee’s goals. For example, a threat the actual and since it information or information is especially crucial in these situations, selection is 2.1. The formal model We will use minimal structures [ I.51 style semantics desires, goals, and intentions. The modal operators have certain desired properties the point of view of our axiomatization. We assume that the agent may not be omniscient (may not have as beliefs all consequences Its set of beliefs may not be consistent. and it may not be aware of the inconsistency. As we discuss later. omniscience and argumentation where agents usually (or the lack of it) is very important transfer facts and their conclusions. in the context of negotiation of its “primitive” beliefs [105.157]). for each of the notions of beliefs, from The set of an agent’s desires may not always be consistent either. For example, an today, but also to go on a vacation, and the two desires among subset of the set of desires. (see also [ 1521). Usually, an agent has preferences agent may desire to earn money may lead to a contradiction The set of goals is a consistent its contradicting desires.’ Similarly. we have some implicit properties category. When an action serves to contribute agent may have the intention of a desire, or indirectly, [ 12,161 that does not contribute through another to act. The intention may contribute directly to the fulfillment intention. The action may have a side-effect to any of the agent’s desires. In such a case, the agent does in mind for actions to one or more of the agent’s desires, in the “intend-to-do” the 3 The proposition may include 3 negation. When fulfillment of the proposition is beyond the control of the agent. it can be achieved by convincing another agent to abandon a relevant intention, or by convincing it to take an a&on that will make the proposition true. ’ The issue of how an agent forms it\ original set of desires is not in the scope of this paper. However. this set may change ober time. a\ the agent patherh more information and updates its knowledge. S. Kraus et al. /Art$cial lntelligencv 104 (1998) 149 5 not intend the side-effect. Thus we require that the intentions be consistent but not confined to side-effects. ’ Briefly, we have a set of time lines, each of which extends infinitely far from the past into the future (see [ 1521). We use time lines instead of more usual worlds because they provide time into our system. With each time point, time line a simple, useful way of incorporating and predicate, we associate a set of sequences of elements the sequence of elements that have the property of the predicate, at the time point of the time line). (intuitively, A notion of satisfaction interpretation, sentence is defined in the language of a sentence line of a structure, given an (denoted by M, 1, V + I,?, see Section 2.3). The intension of a is the set of time lines in which the sentence I/J in a time is satisfied, i.e., is a belief at a given time point at a given time line if its intension A sentence accessible. According the agent may even believe in contradictions. We will later define different in accordance with different properties of their beliefs. is belief- the agent’s beliefs are not closed under inferences; types of agents, to this definition, Similarly, we assume that accessibility relations associated with desires, intentions, and time lines and time points, and sets of time lines [ 1571. An agent intends is a member of goals are between (respectively, desires, has goal) @ at time t, if the intension of I/J (Il$ll) the set of sets of time lines that are intention-accessible goals-accessible) that are intention-accessible An agent intends implies cp. An agent prefers $J over q at a given time t, if the agent prefers over lly~ll at time t. at time t. We further impose restrictions on the set of sets of time lines to an agent. that $I ll@ll at time t to q if it intends @, intends cp and intends (respectively, desires-accessible, (respectively, desires-accessible, I/I in order to contribute goals-accessible) A message may be one of the following types: a request, response, or a declaration. a message are produced using special argumentation A response can be an acceptance or a rejection. A message may carry an argument as axioms. An agent a justification. Arguments that can send and receive messages. Unlike Werner’s approach receiving an in itself changes informative message does not change the that it should add it to its beliefs. ’ Evaluating message and decides a received message and may even be that agents are untrustworthy, is useful especially since we assume untruthful. Only an evaluation process the internal following state of the agent. [164], we do not assume the mental state of the agent. Even receiving the agent evaluates the agent’s beliefs, unless an argument may change ’ While the issue of how to model the concept of intentions is a very involved focus of our work, we devote some effort to tailoring our semantics of the intention and desire operators these desired properties. Our main concern remains argumentation. the process of change identifying topic removed from the main to reflect in these modalities during ’ Note that if two sentences have the same intensions ’ The new information may be inconsistent with the agent’s current beliefs. We leave this for future discussion. ll$II = /IcJ~~, then they are semantically equivalent. See. for example, [6,22.26,27.53,66,96,103,125.166]. 6 S. Kraus et al. /Artijicial lntrlligrnce 104 (1998) 149 2.2. Syntax for desires, Goali for goals and Znti for intentions. We denote by Agents the set of agents. We assume that there are four modal operators for agent i : Beli for beliefs, Desirei, It may be the case that the agent is motivated by the need to satisfy its own goals or desires, or that it is convinced to perform an action following an argument. * In addition, we assume that there is another modal operator, Prefi which is associated with the agent’s preferences [ 1521, the basis among goals, desires, and intentions. As we mentioned for our formalism Informally, we have a set of time lines in the modal logic). The set of time lines is infinite. We (which play the role of “worlds” also have a set of time points. At every time in each time line, some propositions are true (and the rest are false). 9 is a simple temporal above, following language. Our variables and constants are sorted. We have a set TC of time point constants, (t, t^, tl , t2, . .), a set AC of agent constants, .), a set PC of preference values constants set n/ of time point variables (i, j, . .), a set AcC of action constants and a set AcV of action variables agent variables and a set PV of preference values (o, b, two special 2-ary variables (including AV, predicates Do and Capable. We denote by Variables the set of all variables AcV, TV and PV), by Constants (including AC, AcC, TC and PC), and by Terms the set of variables and constants. We also use the symbol nil. We first define the set of the well-formed . .), and a set Pred of predicate symbols the set of all constants (wff) of our language. (p, pt , ~2, including formulas a a set AV of (1) (2) (3) (4) (5) . . .,x,)1 is true at time t). ,x, are terms, and t E TC U TV, then . .,x,) If tl , t2 E TC U TV, then tl < t2 is a wff. If xl, x2 E Terms, then x1 = x2 is a wff. is a k-ary predicate, XI, If P E Pred is a wff (read as: P(xl, [r, P(x), If cp is a wff and @ is a wff, then so are cp & $J and lcp. If cp is a wff and x E Variables, then Vxq is a wff. 3, V, -+ have their usual meanings. If cp and $ are wffs, t E TC U TV, i. j E AC U AV and p E PC U PV, then the following expressions are wffs: (a) [t, Beljq] (b) (c) [t, Goalicp] (i has a goal q~ at time t), (d) [t, Intip)] (e) [t, Prefi((p, I/Y)] (i prefers q over $ ut time t), (f) [t,Agent(lC,. i)] (i is the agent of@). (i intends q), [t. Inti (cp, $)] (i intends q at time t to contribute [t, Desirei (cp, p)] (i desires p at time t with preference p), (i believes q at time t), to $I), (6) If (a and 1c, are wffs, then the following expressions are messages: (a) Request($. q) ($ is requested with the argument cp), (b) Reject( $. cp) ( $J is rejected with the argument cp), * Our intention model is closer to Shoham [ 1381’s Dee and Thomas et al. [ 1521’s Comit than to Cohen’s and Levesque [ 161’s “Intend’. y We have extended explicitly, over others where threats and arguments both evolve in time. We use an extension of first-order of propositional ]I521 to deal with the FOL case. We prefer this approach, where time can be expressed [ 16]), since than an extension logic since it is useful in the formalism of argumentation. time periods cannot be expressed in the language (for example, logic, rather S. Kraus rf al. /Artificial lntelli~encr 104 (I 998) 149 (c) Accept(Q, ~0) ($I is accepted with the argument cp). (d) Decl(@) (@ is declared), (e) Accept(+), Request(@), Reject($) rejected) with no argument)). (1c, ‘. ts ucc.epted (respectively, requested or (7) If m is a message. t E TC U TV and i, j E AC U AL', then [t, Receiveij m] (i receives m ,from ,j at time t) and [t, Send;;m] (i sends m to j ut time t) are wffs. We assume that there are two, 2%~ predicates is read as (;Y is done by i at time [t. Do(i, a)] time t agent i is able to perform a. We will sometimes use the abbreviation for [t. cp] & [t, $1 and will freely abbreviations in Pred, Do and Capable, where [t. Cupuble(i, w)] is read as at [t, q~ & $1 [t. -cp] and -[t,cp]. We will use similar for v and -+. interchange t and Requests and responses may include arguments. For example, an agent a may send its opponent, agent b. a message at time period t, requesting b to let him use its printer at time tl ; with the threat that otherwise a will break it at time t2. Formally, it can be expressed as ]t. Request([tl, Do(b, let.use.printer)], --[tl , Do(b. let.use.printer)] + [tz. Do(a. breuk.printer)])]. As can be seen from the example. arguments and was added to the language will carry out an action. the operator Do is useful to be able to explicitly in requests, specify responses, and the agent who 2.3. Semantics We start with the semantics of the various sentences of our language. This will be followed by the semantics for our modal operators. is a pair (T, +), where T is a set of time points and -C is a total order on T Time (unbounded in both directions). A BDIG model M is a structure (3, L, Agents, A, B. G, D. It. P. RECEIVE, SEND, @. v, M), where ( 1) E is a set of elements (2) L is a set of time lines. (3) Agents is a set of agents. in the agent’s environment, and M is a set of messages. (4) B : L x T x Agents + 2 2L is the belief-accessibility relation. (5) G : L x T x Agents + 22L is the goals-accessibility (6) It : L x T x Agents + 2’L t IS the intention-accessibility relation. relation. (7) D : L x T x Agents + 2 2L is the desire-accessibility relation. (8) P : L x T x Agents x 2”’ -+ Iw indicates associates with different propositions at a given time. for each agent the value (preference) it (9) @ interprets predicates and u interprets constants. x (10) : L x T x Agents x Agents -+ M RECEIVE the agents: SEND : L x T x Agents x Agents + M by the agents. in indicates the messages indicates received by the messages sent (11) A : L x T x 2’l. + Agents U (nil) associates an agent or nil with each proposition for any given time period. The domain of quantification is 0 = 3 U T U Agents U M U R. Given this, @ : Predk x L x T + 8”. V is the extension of v to all Variables. If for any extension U’ of u M. 1, U’ + @, we say that M, I satisfy @ (M, 1 + Q). Given a structure M, and a wff @, we denote by II$I[ the set (I 1 1 E L, M. 1. 17 + 3). The definition of satisfaction is as follows: (1) Iftt,t2ETCUTV,then M, 1, ti /= tl -c t2 iff ti(rt) < C(tz). (2) If xl, x2 E Terms, then M, 1, V ~XI =x2 iff 17(x1) = U(Q). (3) If P E Pred is a k-ary predicate, xi. , x, are terms, and r E TC U TV, then M.1. ti + [t. P(Xl,. .,Xk)] iff @(Xl), . ., 6(Q)) E @[P, 1, i(t)]. (4) If cp is a wff, $I is a wff and x E Variables, then: (a) M,l,ti~-cpiffM,1,17~q1; (b) M.l.u~cp&1CIiffM,I,u~cpandM,l,u~~; (c) M, 1, V + Vx q~ iff for every 5’ which agrees with V everywhere, except possibly on x M, 1. V’ + cp. (5) If cp and $J are wffs, t E TC U TV, i, j E AC U AV and p E PC U PV, then: (a) M, 1, V + [t. Belicp] iff ]]q]] E B(1. c(t), U(i)); (b) M, 1, U k [t. Desirei(cp, p)] iff ]]q]] E D(1. 6(t), U(i)) and P(1, 6(t). c(i), IIqII) = V(p); iff (/VI/ E G(1, i(t), 5(i)); (c) M, 1, V /= [t, G&iv] (d) M. 1, V /= [t, Inticpl iff ]]v]] E It(1. v(t), V(i)); (e) M.1, V I= [t. Prefi(q. @)I iff P(1. c(t), V(i), (f) M, 1. V /= [t,Agent(y, i)] iff A(/, 6(t). llqll) = V(i). t E TC U 7’V and i. ,j E AC U AV, then: (6) If m is a message, IIqII) > P(1, v(t), V(i). III/II); (a) M, 1. V b [t, Receive;jm] (b) M, 1. V + [t, Sendijm] the semantics We extend iff v(m) E SEND(l, c(t). c(i). V(j)). r,k to contribute intending to deal with iff ii(m) E RECEIVE(1, v(t), V(i). c(j)); M, 1, 17 + [t, Inti ($, cp)] iff M, 1, V + [t, &($)I, [t, Znti ($ + q)]. Note that in our model the domain of quantification (that play the role of possible worlds) is the same [81]. We also assume symbols are rigid designators. and M. 1, U + [t, h;(v)] to cp as follows: and M, 1, V + for al1 the time lines that the constant A BDIG model M = (E. L, Agents, A, B, G, D, I, P, RECEIVE, SEND, @, v, M) is rp if for every 1 E L, M. 1 + cp. A formula q is valid if it is said to validate a formula validated by any BDIG model. in our general The agents their attitudes do not have any appropriate properties, as stated in the next proposition. First, power, and have very framework reasoning little ‘(‘We note that if’ for all I t L. f E T and i, ,j E A,qmt.s, RECEIVE(/. t. i. ,j) = SEND(1. t, j. i). then the communication is reliable. See also [40]. S. Kraus et al. /Artificial Intelligence 104 (1998) l-69 9 rules of first-order all tautologies and inference by virtue of a model that is based on intensions of formulas, between following that if an agent believes cp, and cp is equivalent semantically inference equivalent rule (Rl), intentions beliefs, logic are valid in BDIG models. Second, to distinguish it is difficult desires and goals. Therefore, is always sound in a minimal model structures. to $, then the agent believes I/J. ” the It indicates Proposition 1. The following models: ,formal system is sound and complete ,for validi in BDIG (AO) All tautologies ofjirst-order logic. (MP) From cp and cp + I/I infer Q. (GR) From (p infer Vxcp. (RI) From cp * $ infer [t, Beli cpl ++ [t, Be4 $1, [t. Giqj t, [t, G;$], [t, [fit; cp] ++ [t, Intj *I. It. D;(~p,x)l ++ [t, Di(@,x)]. (1) (2) (3) technique The proof Even though the canonical model the domain of quantification is done using [47,106,157]. Note that since in our model lines is the same, and, similarly, the constant and variable assignment the time lines, we do not run into the classical problems of “quantifying (e.g., [15]) and ideas from for all the time is the same for all in”. this model can be the basis for to the attitudes, given specific features that the designer of introducing different properties an agent would like to impart to the agent. Furthermore, different types of agents can satisfy different axioms, which can be characterized in a precise way by appropriate conditions with respect to the accessibility following sections. relations of the models. We will discuss in our model are so simple. this issue in the the agents 2.3. I. Properties of the modalities In all the following axiom schemas, we will assume that the unbounded variables are in all the universally axiom schemas, we assume that i E ACUAV, t E TCU TV and that $I and q can be replaced by any wff in the language. as follows: Vl E L. a E Agents, 5, r’ E T. In addition, quantified Let us start with the semantics we would like the intentions constraints on the intention-accessibility relation It: for the intention operator (It). Following Bratman to be consistent. This can be achieved by introducing (CINTl) fl $Zt(l, t, a). (CINT2) If U E Zt(l, T. a) and V E Zt(l. 5. a) then U n V # fl. [ 121, two (4) (5) ’ ’ The introduction of the agent’s language G to the system as was suggested in [ IO61 will reduce the effect of this rule; the agent will believe, desire, intend, or have a goal e only if e is in its language. The following axiom (schema) and inference rule are sound with respect to the above conditions. I2 Proposition 2. A BDIG model that satis$es conditions dutes the axiom (CINTl:4) and (CINT2:5) vuli- (INTl) [t. -Znt; false] und validutes the inference rule (INT2) From cp -+ -I,L!J infer [t, Inti p] + -[t, lnt; $1. (6) (7) The consistency of the set of intentions at any given time is a basic premise system. For example, suppose an agent wants its opponent to its intentions argumentation (;Y which contributes intentions of its opponent. Due to the consistency its opponent place for (Y. As we will see later, the consistency argument generation process. (using argumentation) and goals. This intention requirement, to give up its original contradictory requirement for the to intend-to do (a) may contradict other the agent must convince intentions to make can guide the persuader’s for example when and vice versa, i.e.. is (see [73]). If, for example, an agent promised another agent to do al, and There are several other properties of intentions an agent intends q and intends @, whether it intends if the agent intends cp & +, whether it intends each of them separately. The first property more acceptable it intends that the agent intends to do it, then it is reasonable to do it and also promised it to do ~2, and it intends that are controversial, the conjunction, to do both actions. In our system this is captured by the following property: (CINT3) If U E Zt(l, t, u) and V E Zt(/, 5. a) then I/ f? V E Zt(l, t, a). The following axiom (schema) is sound with respect to the above condition. Proposition 3. A BDIG model thut satisjes condition (CINT3:8) validutes the axiom (INT3) [t, Inti $1 & [t, In& cp] -+ [t, lnt; I,!/ & ~1. (8) (9) Adopting the axiom in the reverse direction, [t, Znt; cp] & [t , Inti $1 seems less reasonable. Suppose an agent promised to move block A to location s and to put block B on block A. It is not clear that it intends to do each one of the actions separately. In many cases, there is no benefit in performing each action separately, but only doing them together is beneficial [t, Znti (cp & I/J)] + (see also [ 1271). i.e., adopting If one would not adopt the axiom cannot require the agent’s intentions axiom is appropriate in a specific application, that requires splitting a conjunctive intention, one to be closed under consequences. However, if such an the following axiom may also be sound: (INT4) [t. Int; $1 & [t, Int; pb + cp] + [t. Int; cp]. (10) ” When referring to a con&on or an axiom we use both its name and the equation aerial number. For example. in (CINTI :4), 4 specifies the equation number of the condition labeled with (CINTI). S. Kraus et al. /Artificial lntelligencr IO4 (1998) lb69 II Closure under consequence warrants, in addition to (CINT3:8), another restriction on It. Proposition 4. A BDIG model that satisfies condition (CINTS) ZfU E Zt(l. r. a) and U 5 V, then V E Zt(l, t. a) validates the a.riom (INTS) [t,Znti(p&&)] + [t,Inticp] & [t,Znti $1. (11) (12) It is clear from Propositions 3 and 4 that closure under consequence of intention (lNT4: 10) is valid in models that satisfy conditions (INTS: 12), is a special case of the following (CINT3:8) and (CINTS: 11). inference rule which is also valid in models that satisfy condition (CINTS: 11): (RINTS) From cp -+ @ infer that [t. Znt; cp] -+ [t, Znti $1. (13) since the agent needs to act and plan according We will make similar restrictions on G and obtain similar properties for goals. Intentions to its intentions and goals are consistent, and goals. l3 However, desires may be inconsistent, desire to dig on Mars, but also to conserve to a contradiction Usually, an agent has some preferences among its contradicting desires. above. An agent may its battery power, and the two desires may lead (see also [I 521). However, we do not want the agent to desire falsely. I4 as mentioned We impose the following restrictions on the desires (0) operator: (CDl) B 4 D(1, r, a). (CD2) If U E D(l, 5, a) and U 2 V, then V E D(1, r, a). l5 These restrictions yield axiom schemas similar replaced by Desirei. to (INT1:6) and (INT4: lo), where Int; is We take a different approach concerning preferences and desires than Wellman [ 1621. that the agent’s preferences We assume [162] preferences higher value to the intension of cp (llqlj) models, different restrictions may be put on P. are over the sets of time lines, while Wellman’s are over single models. An agent prefers p over I/J if it associates a In different than to the intension of I,+ (ll$ll). The agent’s desires are not derived from its preferences (see also [69]), but we make the following restriction on the model: (CPD) VU, U’ E 2L. If U E D(1, a, 5) and P(1, t. a. U) < P(1, 5, a, U’) (14) then U’ E D(1. 5. u). Hence, in our model the following axiom is sound. ” Another good property for agents is that their intentions and goals be consistent with respect to their beliefs. We discuss this in Section 2.6. ” Note that there is a difference between [t. Desire; (cp, pj)] & [t. Desirei (-9. pz)] and [t, Desire; (cp & -cp, p)]. We allow the first case, but not the second one. ” (CDI) is similar to (CINT1.4) and (CD2) is similar to (CINTS: 11). 12 S. Kraus et al. /Artijicial Intelligence 104 (1998) Id9 Proposition 5. A BDIG model that satisjies the condition (CPD: 14) vulidutes the,following axiom : (PD) [t. Desirei(@. p)] & [t, PreJ’;(cp. +)I + @P’)((P 3 PI 8~ [t, Desire;((o, ~‘11). (15) The property that desires may not be consistent plays a role in argumentation. its opponent to perform an action it may contribute In some that contradicts to one of the opponent’s an agent tries to convince situations, the opponent’s current set of goals. However, desires. We discuss this case in Section 3.7. 2.4. Agent types Within the general subsections, we define framework defined above, it is possible the additional to define various types of agents. In the following conditions on the models that these agents must satisfy in order to have a particular character. In addition, we define properties of the model associated with changes over time, as well as agent types that arise from different assumptions In Section 3.8, we discuss how agent types may be guiding factors in the selection of argument categories and the generation of arguments. among modalities of the model. as to interrelations 2.4.1. Properties associated with reasoning power The minimal properties we would like an agent to have with respect to its beliefs is that it will not believe in “false”. However, as was discussed above, an agent whose beliefs are in a contradiction, without being aware of it. not closed under consequences may believe This leads us to the definition of the following simple agent. Bounded agent We would like a bounded agent to have the following axiom: (Bl) [t, -Beli false]. (16) So that an agent does not believe in “false”, we need to impose additional restrictions on its belief-accessibility relation. Proposition 6. A BDIG model that satisfies the condition (CBl) fl$ B(l, 5, a) validates axiom (B 1: 16). (17) We further assume that all the other types of agents do not believe The beliefs of a bounded agent are not closed under consequences, believe that cp and that p + $, but it may not believe in +. However, as we discussed Section 2.3, it cannot distinguish between semantically equivalent formulas. in “false”, either. i.e., an agent may in An omniscient agent An agent whose beliefs are closed under inferences is said to be omniscient. For omniscience we impose relation. These render its model equivalent the following to a Kripke structure. additional conditions on the belief accessibility (CB2) L E B(1, r, a). (CB3) If ci E B(I, r, a) and U C V then, V E B(I. r. ~1). (CB4) If I/ E B(I, T, a) and V E (I, T. a) then, U n V E B(I. t, a). (18) (19) (20) that satisfies conditions A BDIG model structure, and every Kripke structure corresponds (CB2: 18)-(CB4:20) logic K (e.g., [ 151). [157], and thus such BDIG models validate (CB2:18)-(CB4:20) to a Kripke corresponds to a BDIG model that satisfies conditions the axioms of the modal that satisfy: conditions (CB2: 18)-(CB4:20) validate the Proposition 7. BDIG models ,following axioms: (B2) [t. Bel; true]: (B3) [r. Be/i + & cp] + [t. Bel; $1 & Lt. Bel; cpl; (B4) [t. Beli @] & [t, Be/i up] + [t, Be/; I/J & ~1. (21) (22) (2.3) There may be other types of agents that may have only a partial set of the axioms of (without the omniscient agent, for example, an agent that does not believe axiom (B2:21)), but can reason using axioms (B3:22) and (B4:23). in tautologies While we assume that there are agents whose beliefs are not closed under consequences, we do assume that all agents’ intentions and goals are closed under consequences. This is than the set of beliefs. The agent is justifiable, is much smaller aware of its intentions, its since it needs to search for plans to achieve to assume that the agent can compute intentions their closure under consequence. are under its scope, and it is reasonable since the set of intentions them. Therefore, A knowledgeable agent An agent is knowledgeable if its beliefs are correct. The corresponding axiom schema is: (B5) ]I 1 (Beli CP) + ~1. (24) The related condition, which makes this axiom sound. is specified in the following proposition: Proposition 8. BDIG models that satisfy the condition (CB5) if U E B(1. r. a). then 1 E U (25) vulidutes axiom (B5:24). 2.5. Properties associated with chunge in modalities over time So far, the agent typology, namely omniscient and knowledgeable to the agent at a particular time interval. intentions can change over time. Change agents, has considered In an open world in intentions that an agent keeps that an agent obtains by observing agent from other agents. The following track of as time changes, or or into intent- its environment, takes typology how the passage of time interacts with an agent’s beliefs and local however, agents’ only properties environment, can be the result of knowledge new knowledge through communications consideration ions. and/or beliefs An unforgetful agent An agent who does not forget anything follows the following axiom: (BUF) Vt, t’((t’ 3 t) + [t. Be& lo] + [I’. Beli cp]). (26) An agent who does not forget anything can be characterized according to the following proposition: Proposition 9. BDIG models thut sati& condition (CBUF) $s < r’. then B(I. r. a) C B(1, r’, a) (27) validates axiom (BUF:26). A memoryless agent We would like to characterize agents about past events. An agent does not have a memory under if I/ E B(1, r. a) and 1’ E CJ then for every T’ x r (i) for every P E Pred, @[P, 1, r’] = ti; (ii) B(I’, r’. a) = !A; (iii) G(l’, r’u) = k3; (iv) Ir(l’, r’a) = VI; (v) D(1’. r’a) = M; (vi) for every al. CI? E Agents. RECEIVE(1’. r’. al, a?) = OI and SEND(l’, r’. al, ~2) = ti. that do not have memory and cannot reason the following condition: ” A non-observer In some situations, of message evaluation; information We make the following is communication with other agents. it is useful to assume that an agent’s beliefs change only as a result things, and its only source of i.e., the agent does not observe restriction on the model of such an agent: if an agent does not it will keep then if it believes in something, receive any message at a given time period, believing If Vh E Agents. RECEIVE(I, in it during the next time period, and the agent will not adopt new beliefs. r. a. h) = ti, then B(1, r, a) = B(1, r - 1, a). ” Other reasonable restrictions can he chosen for characterizing a memoryless agent. Cooperative agents A group A of agents A C Agents is cooperative I7 in a BDIG model M, a time line 1 a time point t and an interpretation V, if it shares common goals. This imposes the following condition: n G(I, t. (2)) #(II. 0 E A Furthermore, we require that the goals are common belief. ” That is, let A be the set of common goals, i.e., A= 9 I II@111 E (-) G(1,t.a) 1 UEA . 1 and A” C AC are the “names” of the agents of A according to V, then M, 1. 17 b A\jeAL Goalj @I. It is easy to see that the set of common goals of cooperative Ctq c/&d agents is consistent. Cooperative agents may have contradictory goals, e.g., Lt. G&l & [t. Gj-Q]. to the set of common goals. Our definition of the cooperativeness These goals do not belong of agents may be time-dependent. A set of agents that are cooperative at a given time period may become adversaries at a later time period, when their common goals do not exist any- than the notion of Shared Plans more. Our notion of cooperative agents is less restrictive of 1561, the notions of Cohen, Levesque and Nunes [85] of joint persistent goals, and joint in the example pre- intentions or collaborative the negotiation. sented in Section 4.7, even this weak notion of cooperation may enhance activity of [ 1431. However, as demonstrated 2.6. Inter-relations among modalities So far we have presented axioms and semantics conditions modality. Now we will move to investigate First, every goal is also a desire. I9 inter-relations to define properties of each among the different modalities. (GD) It, Goal;( -+ [t. (3p)Desire;(cp. p)]. (28) The correspondence described in the following proposition: restriction on the goal and on the desire-accessibility relations is Proposition 10. A BDIG model that sutisjes the condition (CGD) ifU E G(I. r. LE), then U E D(1. T, a) (29) dilutes axiom (GD:28). ” Among cooperative required. However, agents. as among noncooperative agent\. conflicts may occur, and negotiation may be the argumentation is of a different nature, and we shall not dwell on that here. ‘* We denote by EI,!T the property in A believes knowledge [60]. I), everyone believes that all the agents of A believe @. @ is common belief at time t if everyone that $ i\ common that everyone believes I/I and so on 1751. C$ denotes I’) In Cohen and Levesque’s framework [16]. all the agent‘s beliefs are also Its goals. We do not ha\e such a property. An agent may believe p. but may not desire p and may not adopt it as one of its goals. 16 5. Kruus et al. /Arr$cial Inrelligencr 104 (1998) l-h9 An agent adopts all its goals as intentions: (GINT) [t, Goals q~] --+ [t. Znti p]. (30) The correspondence similar to (CGD:29). restriction on the goal and on the intention-accessibility relations is Proposition 11. A BDIG model that satisfies the condition (CGINT) if Cl E G(1. 5, a), then lJ E It(1, 5, u) (31) validates axiom (GINT:30). there may be intentions to a threat or promise for a reward. During However, in response come to have an intention it to give a reward, which only indirectly contributes is aware of its intentions and, moreover, that are not goals. An agent may hold an intention the agent may to prevent the opponent from carrying out a threat, or to convince to one of the agent’s goals. An agent its beliefs about its own intentions are correct: the argumentation, (INTBl) [t, Znt; cp] * [t, Beli[t, Znt; (p]]. (32) The correspondence restriction on the structure is as stated in the following proposition: Proposition 12. A BDIG model that satis$es the condition (CINTBl) CJ E It(1, T. a) ifl{l’ / U E It(l’, r, a)} E B(/, T. a) (33) vulidutes axiom (INTB 1:32). We assume that an agent’s intention does not contradict its beliefs: (INTB2) [f, Anti up] + [t. -Beli 1~1. The corresponding restriction on the It and B relations is as follows: Proposition 13. A BDIG model that satisfies the condition. (CINTB2) (f U E It(1, 5. u), then L \ U 4 B(1, 5. a) validates axiom (INTB2:34). (34) (35) In order to understand that if an agent has a P-GOAL of intention with Cohen and Levesque’s and Levesque assume agent believes in the future. The agent will drop a persistent goal p only p is true or that p is impossible. proposition; some specific the intuition behind axiom (INTB2:34), we compare our notion [ 16,181 persistence goal (P-GOAL). ” Cohen the is not true now, but that it will be true at some time that to believe in the appear that will be true at is true now, if it comes In their logic, time does not explicitly time in the future or consider situations where a proposition thus, they cannot express P-GOAL toward a proposition, that this proposition toward propositions then *(I Cohen and Levesque’s concept of intention is based on their notion of P-GOAL 5 Kraus et al. /Artificial Intdligencr IO4 (1998) I49 17 in addition this intention false. Since time is explicit t -c tl & [t.Znt;[rl. On(A, B)]] intuitively means that at time r, agent to make but which the agent believes will become false later and therefore has a P-GOAL in our logic, we can express it true again after it becomes to expressing Cohen and Levesque’s attitude P-GOAL. For such intentions, i intends example, there is no that block A will be on block B at some later time tt. In our framework, t, that On(A. B) relation between and whether agent is not true at time t. Furthermore, we may express toward propositions with the same predicates, different e.g., t < tl < tz & [t, Znti[tl, On(A. B)] & -[tz, On(A. f?)]], or different predicates, e.g., t < tl < tl & [t,lnt;[tl. On(A. B)] & [tI, On(C. B)]]. In both cases, [t. Beli[t, On(A. B)]] that at time t may be true or may be false. However, we require, the agent will believe that at time rt block A will be on block B, i.e., -[t. Be/j-[tl, Otz(A, B)]]. i believes, at time intentions time points. In such intentions, the propositions may include in axiom (lNTB2:34), that it is possible This expressibility types of agents. to their beliefs about their intentions. We may characterize an agent as confident of our system enables us to characterize different that it will succeed in carrying out its intended actions. according if it believes (Conf) [f. Beli([t. Inti cp &Agent(cp. i)] --f cp)]. The correspondence restriction on the model is as follows: Proposition 14. A BDIG model thut satisfies the condition (CConf) VU C L. (1’ / I/ +Z It(l’. r. a) or A(/‘. r. I/) #u or I’ E U} E B(I, t. a) v&dates axiom (CONF:36). (36) (37) For example, a confident agent may believe that if it intends to move block A on top of block B, then it will really do so, i.e., [t. Be&It, Inti([tl, Do(i, move(A. B))]) -+ [tl. Do(i. move(A, B))]]]. agent which believes A confident (INTB1:32) consider knows whether it is the agent of a proposition or not. in its intended is omniscient the agent’s beliefs about the agent of a given action. We assume actions. Before presenting this proposition, we that an agent (B 1: 16)-(B4:23) and aware of its intentions (AGBl) [t.Agent(cp. i)] ++ [t. Bel;[t,Agent(cp. i)]]. (38) The correspondence restriction on the structure is as stated in the following proposition: Proposition 15. A BDIG model that satisjies the conditiotl (CAGBl) A(1. T. I/) = a ifl{l’ 1 A(/‘. r. U) = a} E B(I. T, a) (39) v&dates axiom (AGBl:38). We are now ready to present the proposition on confident agents. Proposition 16. BDZG models (B 1: 16)~(B4:23) also validate that validate (INTB1:32). (Conf:36), (AGBl:38) und (ConfB) [t. hzt; cp] & [t. Agent(cp. i)] + [t. Bel; cp]. (40) A confident agent that is not omniscient (i.e., does not satisfy axioms (B2:21)-(B4:23)) may not believe agents by imposing an additional restriction on It and B. that its intended action will succeed. However, we can characterize such Proposition 17. A BDIG model that satisfies the condition (CConfB) fA(I. T, U) = a arid I/ E Zt(1, 5. a), then CJ E B(1, T, U) (41) v&dates axiom (ConfB:40). An agent that is sure that it will be able to satisfy all its intentions, including the ones that are not under its direct control, can be said to be oveucoqfident. (OverCod) [t. Bel;([t. Anti cp] + cp)]. (42) The correspondence restriction on the model is specified in the following proposition. Proposition 18. A BDIG model that scrtisfies the conditiorl (Cover Conf) (/’ ) lJ 6 Ml’. T. N) OY 1’ E Cl} E B(1. T. a) (43) ~~alidates axiom (OverConf :42). For example, an overconfident that block A will be on block B, then block A will actually be on block B, even if the agent is not capable of moving block A and does not intend to move it, i.e., agent may believe that if it intends [t,Bel;([t.Znti[tl. On(A, B)]] + [tl. On(A, B)])]. A proposition that is similar to Proposition 16 is true for an overconfident agent. Proposition 19. BDIG models (B4:23) also validute that validate (INTB 1:32), (OverConf :42), and (B 1: 16)- (OverConfB) [t. ht; cp] --f [t. Bel; cp]. (44) An overconfident agent that is not omniscient may not believe in its intentions. However, (OverConfB:44) is sound in structures that have additional relations between It and B. Proposition 20. A BDIG model that satisfies the condition (COverConfB) $’ U E Ml. 5. a), then U E B(1. T. a) (45) validates (OverConfB:44). An omniscient agent which according to its beliefs. That is, its beliefs about is overconfident will not adopt that conflict intentions the side-effects of its intentions will S. Kmu.t et al. /Artijicial lntelli~encr 104 (199X) l-69 I’) its intentions. influence of some side-effects effects (e.g.,[t, -lnt; If [t. Inti ~1, then i believes that cp. Therefore, if the agent is aware (e.g., [t. Be& (cp -+ I/T)]), then it cannot intend the negation of the side- -@I). ” Proposition 21. BDZG models thut validate (INTB 1:32), (OverConf also validate :42), (B 1: 16)-(B4:23) (INTB3) ([t. hi cp] & [t, Beli(cp --, $11) + -[r. Int, -$I. (46) Proposition 21 is not true for nonomniscient agents. That is, the intentions that are not omniscient may be contradictory of non- agents Intuitively, to their beliefs. this is because and they do not realize that they believe confident agents or (over)confident according consequences do not check their intentions carefully, humans. For example, a researcher may intend and believes before the deadline. Nevertheless, To prevent such behavior, we may wish to restrict our attention axiom (lNTB3:46), restrictions on the belief- and intention-accessibility their beliefs are not closed under they in a contradiction, in light of their beliefs. This is quite common among to finish a paper by a specific deadline, the paper will prevent her from attending meetings on the day to go to a faculty meeting on that day. that satisfy regardless of their reasoning power. This requires adding additional she still intends that finishing or because to agents relations. Proposition 22. A BDIG model that satisfies the corldition then UI n U2 n U3 # Cn vulidutes axiom (INTB3:46). The above axioms and propositions put some constraints on when an agent can abandon if an agent starts believing its intentions. that one of its intentions It is clear that according is not possible, to axiom (lNTB2:34), it must abandon this intention, i.e.. [t, Into cpl 8~ [t + 1, Bel; -cpl --, -[t + 1. Znt; cp]. it considers Similarly Cohen and Levesque toward a proposition that an agent will forgo its intention when it believes the intended proposition goal). Their requirement it is a maintenance there is no explicit time associated with their intended proposition However, the intended proposition overconfident forgo this intention. For example, suppose [ 161 also require that an agent will give up a persistent goal impossible. However, Cohen and Levesque also assume is true (unless is based on the attribute of their system that (as we explained above). that as a result of its own intended actions (that is associated with a specific time) is true. In particular, an are true. In such a case, it cannot in our case an agent may believe agent always believes that its intentions ” Similarly, a confident agent will not adopt intentions that it believes contradict its own actions that if it intends that block A will be on block B at which intuitively means that, at time t: agent i intends to move A on top of B in order to achieve this intention. Suppose time ~1. and it intends that agent i also believes it will really do it, e.g., if it to move block A, it will really move it, and that moving block A on top of B wiil intends i have the result that at the next time point O&A. B) will be true. In such a case, agent and beliefs, On(A, B) will be true at time tl However. believes its belief is based on its intended action to move A which is motivated by its intention that On(A, B) will be true at time 11. Thus, the agent must keep its intentiol~ 111, Qn(A. B)] even though it believes [tl . Opl(A. B)] to be true. that, given its intentions to do something, it believes it is impossible. it is true or believes In OUI‘ system an agent may abandon Cohen and Levesque also require that an agent does not abandon a persistent goal unless that it is impossible. We do not require that an agent keep it believes its invention until an intention due to other reasons. such as a request received from another agent or because of an observation. The ability for an agent to revise its intentions in making argumentation effective. An agent may drop an intention because of a direct request from another agent, or because a request from another agent may conflict with the intention. Also, an agent may drop an intention The and commitment such a study is beyond the scope of this paper (see [ 681). However, different types of agents may be characterized of an agent to its goals and intentions may greatly affect its performance as a result of a change in the environment. in our framework. is very important For example. an agent that does not change its intentions, unless if Vh E Stgmts, RECEIVE{!. it received a message t, LI. b) from another agent may be characterized = M, then Ir(i. 7. (I) = Ztll, 1, a). r - as follows: It is also easy with its previous ones, axiom (lNTl33:46): to see that if an agent adopts a new intention that it believes conflicts intentions. This is a simple corollary of it will drop the original Corollary 2.1. BDlG nwdels thut validate axiom (INTB3:46) also validate t!ze,followin,g fzimt (INTB4) [r. hti +I &L It + 1. Znti qJ & [t -t 1. Belitcp -+ -911 -+ (48) -[t + I. 1ntj $1. Similarly, believe that its intention from axiom (INTB1:32). is not possible, it is easy to conclude it will drop this intention. that if an agent comes to S. Kruus et trl. /Art@ial Intelligencr 104 (1998) id9 21 3. Axioms for argumentation and for argument evaluation The formal model can be used in two ways. One use is as a specification 1561. In this role, the model constrains certain planning and negotiation processes. also be used to check the agents’ behavior. Another use of the model themselves. can be used by a designer of an agent for the specification of the arguments the next section we will describe an automated agent which uses our logic. In this section we demonstrate how the logic presented for agent design It can is by the agents in the previous section it will use. In Indeed, the expressiveness typology of arguments. Arguments serve either to add an intention to present such an authoritative classification, to the persuadee’s set or to retract an intention or to change the preferences of the persuadee. Below we present a list of several argument of our logic, and (b) in the types which we use (a) to demonstrate development of an automated negotiator. These argument types are not meant to constitute it has been pointed out [ 1561 that it is not an exhaustive possible since arguments must be interpreted and are effective within a particular context and domain. The six argument present are ones that are commonly [65,110,124]. Argumentations which were shown to be successful may be also successful agents to be able to negotiate with humans. and therefore understand human argumentation. Moreover, negotiation of the agents, present are: Furthermore, we want our they need to be able to at least the types we force in human negotiations in human negotiation, the designers of the agents can follow to human negotiation. The argument thought to have persuasive agents’ negotiations. if it is similar types that we in automated on the part of the persuadee. ( 1) Threats to produce goal adoption or goal abandonment (2) Enticing (3) Appeal to past reward. (4) Appeal to precedents as counterexamples the persuadee with a promise of a future reward. to convey to the persuadee a contradiction between what she/he says and past actions. (5) Appeal to “prevailing practice” to convey to the persuadee that the proposed action will further his/her goals since it has furthered others’ goals in the past. to convince a persuadee to self-interest that taking (6) Appeal this action will enable achievement of a high-importance goal. Threats and promises are the most common arguments used in human negotiations [ 111. it was found that presenting example than presenting instances statistical summaries is the most common argument used in the legal system. (prevailing practice cases) is [6 1,70,107,15 11. An “appeal theory [ 1 lo] that assumes that a consistency of his/her cognition, and in repeated their credibility. The other two arguments, is also important is supported by the cognitive dissonance the internal psychological to keep his/her promises. This argument An appeal to prevailing practice Furthermore, much more persuasive to past promise” person seeks to maximize thus will be willing interactions “an appeal to self-interest” persuade bounded since agents prefer rational agents which have limited inferential resources. to maintain and “a counterexample” are examples of arguments useful to Each of the above arguments will be discussed type we present examples examples of automated agents’ also be presented. Examples of argumentation that are borrowed interactions. Axioms in the following from human argumentation, subsections. For each as well as for creation of such arguments will among automated agents presented below are based on the following scenario. Agents with different spheres of expertise may need to negotiate with each other for the sake of requesting each others’ services. Their expertise is also their b~g~ning power. As an example, consider a robot R, who has a better “eye” (has a powerful camera) while Rh has a better “hand” {has skilled functions with dextrous fingers enabling it to isolate mineral chunks). Yet another agent R, has specialized maps and terrain knowledge three self-motivated robots with goals to obtain samples from Mars. R,, Rh and R, are looking for different mineral samples. We can imagine these three agents facing the need to argue with each other. 22 and is adroit at navigation. Imagine these 3.1. Arguments involving threats that agent Suppose agent j intends j assumes If there is an action j3 that j can perform, that contradicts own beliefs, or intentions. another goal of i, and this last goal is preferred by i (again according first one, j threatens in several different forms. For example, suppose agent j intends fy, at time r, and i insists to maintain it will do #I if i will clo IX. i should do to at time ? and i refuses. Based on its that i refused to do Q probably since o! contradicts one of i’s goals (as per j’s beliefs) to j beliefs) over the i that it will do /3 if i will not do a. This type of argument may appear that agent i should not do i that its intention of doing cr. Here, agent j threatens to withdraw insists on a wage increase, The management its request. The management to compensate (i.e., whether A labor union and asks the union this increase, it will have to lay off employees cost that the increase will entail. The outcome threat or not) depends on the union’s preferences. impo~ant that the management will carry out the threat). If a wage increase then the union will not accept the argument and insist on a wage increase is irrelevant or not it believes decision.) If preserving the union will accept the argument the management will carry out its threat than wage increases, says it cannot afford it, threatens that, if it grants for the higher operational the union succumbs employment (assuming to the is more it believes is more important, (here, whether to the union’s One of the questions related to generating a threat is: how does j choose B? If j wants the threat to be effective, carrying out B should be painful for i and conflict with one of its (as we stated above). However, the threat should be credible according goals or intentions the evaluation of threats below). First of all, to i’s beliefs (see our discussion concerning usually, doing @ should be within carrying out a threat may contradict some of j’s intentions or goals. These intentions and to (again, at least in i’s goals should be less preferred by j than the goal that cx contributes view). the power of j (at least in i’s view). Furthermore, There may be several such ,9s that ,j may choose from. The fi that is chosen depends on whether the persuader, a preferred goal or intention will contradict a less preferred goal of i) and, if i refuses j, wants to inflict a very strong threat (i.e., a ,B which contradicts threat (i.e., one which it, to escalate with sponger of i), or to start with a weaker 2 Similar needs for argumentation nuclear accident, or at the site of a forest fire. can be envisaged when automated agents seek different goals at the site of a S. Kraus et cd. /Artificial Intt~lligmcr 104 (19%‘) f--69 23 i down). Argument evaluation is an important aspect of argumentation as threats (wearing discussed in the next section. Here is an axiom scheme specifying the generation of a threat ~gument in the logic presented in Section 2. 23 & iti. .Sen~jiRrqZlest(li. Ek>(d. a)])] & [tz, RereiVe,jiReject(li, DO(i. CC)])] --f [fj. SmdjjRequest([t, Do(i. a)], -[i, Do(i. a)] -+ [t4, Do(j, j3)])]}. Cred is a meta-predicate which stands for an axiom that j will use for estimating whether is a meta-predicate which stands for axioms threat for i ta do a. Appr~priute B is a credible that will specify how to choose /3, when several such gs exist. In the example of the robots on Mars agent Rh must explore for its mineral. Some help from R, in scanning digging camera would greatly contribute from R, the use of its camera. R, refuses, since the time spent in fu~hering Rh’s goals will interfere with its own goal-to that it will smash R,‘s camera lens if R, does not accede to this request. dig for its own mineral. Rh then threatens this goal. Rf, requests lit area while the area with a high resolution in a dimly towards 3.2. Evuluation of threats In this section we demonstrate that agents are honest, not assume whether affect the agent that threatens the threatening factors affecting the evaluation of a threat. Since we do is how to decide agent will carry out its threat. Usually, executing a threat will to carry it out, and this has a bearing on the evaluation. the main problem in the evaluation Suppose j had requested i to do (Y at a given time point t, and it had threatened i that if it does not do LX, ,j would do /3. Now, i should consider several issues. First of all, how bad is the threat? If CY contradicts one of i’s goals and @ contradicts another goal g, which goal does i prefer? But then again, i should evaluate whether j will carry out its threat at all. We may assume that /l has negative side-effects on ,j as well. The question is whether j prefers from the side-effects of b in case it the benefit for itself from a! over the losses resulting its carries out the threat. Another credibility and re~utati~~n. Another issue for i to consider issue relevant here is how important is it for j to preserve is whether the threat is a ~~~.~iz~~~~ 3 fZll the axioms listed in this section are only suggested possibilities We demonstrate tile expressibility of the logic in formalizing such axiom% for producing the relevant argumentations. i default. Usually, threat is always credible since i is aware of prior arrangements made by to i in a threat. A bounded j to execute the threat should prior exchange. If i believes that j may carry out its threat and decides that it is wo~hwhile for it to do (Y, it still needs to update its goals and intentions. Here i will intend cx in order ,6 which contradicts R. Note that here i intends to contribute i a, without since any goal is also an Intention the goal that cz contradicts, as well as the related intentions. should abandon j will convey this information /I being a goal. Furthermore, j from doing to preventing (GINT:30), (WI. t2, t3, t, a, j3, i, j)(2l < t2 < f < t-, & i # j In this axiom we have listed one way to evaluate whether a threat is credible. Here, i i’s goal f$\) as well as some that if j carries out threat 6, this will contradict believes possible goals of j (sj‘). If i believes that 61 is one of j’s goals, and if it believes that j prefers the goal that cz contributes A similar axiom can be specified which considers cases where j asks i not to do c~. to over gi then it will believe that B is a credible threat. Agent j entices agent i to do action 01 (alternately, avoid doing a) at time t by offering to to the desires /!I to cont~but~ do an action p at a future time, as a reward. Agent j believes of i . An example is a sales agent trying to persuade a customer to buy a VCR by offering a free servicing plan and a set of blank cassettes. (vtl,t2.t~,t4.t,i,j,CY,p)Itl <t2<t<t<t~&i#,j & [tl, SendjjRequest([t, Do(i, a)])] & [TV. RcTx+V~j~Rejc?Cf([t, DO(i, a)])] & [t3, Belj (It33 Goah If. gll& Goah IQ* ~211)l 6% [t3, Bekj ([t3, Pd(lfT R21, [t4, RI 111)l & [Q, Bdj[f, Do(i, CX) -+ -gll& If43 Wj, B) --, x211 8~ [tx, Rel,;[tg, Cred(B, a. i, )I1 & Et+, Beljlts, Aw-opriate(B, a, i)ll + [Q. ~~~~~~~Req~{~.st([~, Du(i, a)), [t. Do(i. a)] + IQ, Do(j, ,B)])]}. S. Kraus et al. /Artificial Intelligence 104 (1998) 149 25 Consider the scenario described in the threat example above involving robots. Instead of responding with a threat, Rh could offer to contribute it to isolate its samples from the debris better by using its sorting skills by means of skilled that R, now plans to collect, and fingers. This would reduce the weight of the samples greatly increase towards R,‘s goal by helping the ratio of mineral to debris for R,. 3.4. Appeal to past promise In this case agent j had requested If i j reminds him of the past promise. For example, a child has promised her parent them to buy her something. When she is later asked refuses, to clean the house, in order to convince by the parents to clean the house and refuses, the parents may remind her of the promise. i to do an action c~ based on a past promise. (Vru. tl, t2. tj, t& i, i, j, cx. #l){to c tl c t2 c t3 c i < t4 & i # j & [tl, SendjiRequest([i, Do(i. a)])] & [tz, Receivej;Reject([i. Do(i, a)])] & [tj. Belj [to, ReceivedjiRequest([th, Do(j. /3)]. [t& Do(j. p)] -+ [i, Do(i, cr)])]] + [tg. SendjiRequest([i. Do(i. a)], [to, Sed;jRequest([th, Do(j. /?)I, ItA, Do(j. /VI--+ [t Do(L ~>l)l)ll. For example, if as in the previous case, Rh offered to contribute to R,‘s goal by helping later, when the time for sample isolation arrives, R, may use this for a request from & to help it in isolating samples. it to isolate its samples argument as an argument 3.5. Countere.xample that i do (II at time i, requested it from i, but i refused. j had intended that the reason i refused is that u contradicts one of its goals or intentions. that in the past, i had done another action B that also contradicted Here, agent Now j believes However, j believes, same goal or similar intention, and brings it up as a counterexample. As an example, consider a parent trying to persuade a teenager to stay up until midnight to study for an exam. The teenager refuses on the grounds that she may suffer bad health from staying up late. The parent points out that the teenager had stayed up until 2 a.m. for a party it up as a counterexample the previous week, without suffering any ill-effects, to the argument. and brings the Wt’, tl, t2. t3, i, i, j, (11. B)(t’ < tl < t2 < tg < i&i # j & [tl. SendjiRequest([i, Do(i. w)])] 8~ [tz. ReceivejiReject([i. Do(i, w)], [f, Do(i, a) + -g])] 8~ [tg. Belj([t’, Do(i, /?) & Do(i, p) -+ -g])] + [tx, SendjiRequest([i, Do(i. w)]. [t’, Do(i, /I) & Do(i, 8) -+ -g])]}. The following is an example from the robots on Mars. Suppose, RI, requests R,,, to survey the terrain using its navigation skills. R,,, ‘s temperature sensors indicate that in some areas there may be high temperature pockets and these may harm its electronic circuitry. R,n refuses on these grounds. & points out that in the past two days, R,, has withstood used in the digging process, much higher the explosions without any evidence of harm to its circuitry. Rh brings to convince R, this up as a counterexample created during temperatures to undertake the survey. 3.6. Appeal to “prevailing practice ” In this case, j receives a refusal from i to do o on the grounds that another agent h had done the same u and it did not contradict that it contradicts goal g of i. If j believes the same goal g held by h at the time, it uses it as an argument. For example, a teacher intends in baseball should stay after school for extra curricular activity. This that a student talented will contribute team at school. He asks the student to do so, but the student refuses on the grounds that this will adversely affect his academic performance. The teacher points out that last year the star baseball player of the class was also an ‘A” student, and that several good players are also good students, and encourages to the teacher’s desire to build a good baseball the student to take up the activity. (Vt’. t. tl, t2, t3.1. i, .j, 12, a)( tf<tl <t~<t~<t&i#,j#h & [tl, Sendj;Request([T, Do(i, a)])] & [t2, Receivej;Reject([l, Do(i, a)], [F, Do(i, a) + -g])] & [tg, Belj([t’, Do(h. (Y) & -(Do(h, (Y) + -g)])] -+ [t3, SendjiRequest([f, Do(i, a)], [t’. Do(h, a) & -(Do(h, a) + -s)])]}. With the robots on Mars, consider the following mention of prevailing practice argument. As in the counterexample its navigation high temperature pockets and these may bring harm to its electronic circuitry. R, on these grounds. Rh points out that both R, and itself were exposed temperatures in an scenario, &, requests R,n to survey the terrain using that in some areas there may be refuses to much higher two days ago, and had withstood skills. R,n's temperature them quite well. indicate sensors 3.7. Appeal to .se(f-interest In this case j believes that cz implies one of i’s desires and uses it as an argument. This is a useful argument when j believes that i is not aware of the implication. For example, an employee has a goal to study Japanese, but wants to save money as well. She intends for refuses. her company The employee points out that having an employee with knowledge of Japanese is a great asset to the company, especially in the coming years when the company will face stiff competition lessons and asks the company. The company to pay for the Japanese from the Japanese. S. Kraus et al. /Art$ciul Intelligence 104 (1998) I-69 27 In another setting, agent j requests agent i to give up its intention to do (11 by pointing out that either u or its side-effect /3 contradicts one of i’s desires. (Vtl,t2,Q,t,i,j){tl <t:!<t3<i&i#j &[t] ,SOldjiReqZMSt([i, Do(i,cr)])] & [tz. ReceiVejiReject([t. Do(i, a)])] & [tj. Belj ([f, Desirei (dl , p)])] & [tx, Belj([i. Do(i, a) + dl])] + [tj. SendjiRequest([t. Do(i, a)], [i, Do(i, a) + dl & DeGrei(dl, p)])]). For example, suppose R, and & both plan to dig at site X on Tuesday. If they both dig at the same site, clearly, there will be destructive of the procedures of either. R, makes a proposal that R, digs in the morning while & digs in the evening. Rh refuses, since obviously, this proposal it will result the time will further Rh’s self-interest much better, since getting half the work done is better than getting no work its product. However, R, points out, that if &, refuses, in near zero product to a malfunctioning for Rh and R,. Instead, sharing to Rh to divide their digging interference, reduces time so leading done. 3.8. Selecting arguments by an agent’s type A persuading agent, faced with a particular use. In order for an argument persuadee. Therefore, for argument generation. One type of belief pertaining the persuadee to be. This can influence to be effective, is believed the beliefs of an agent about the persuadee can be used as guidelines is the type of agent to a persuadee the argumentation. to situation, must decide which argument it must address beliefs and intentions of the that /I is expensive For example, nonbounded agent k to do a! and threatens threats or future rewards are not applicable to j. If k will not do (Y, there is no benefit if an agent is it with memoryless. 24 Suppose agent j asks a memoryless to j /I. Let us assume its to carry out the threat (i.e., do B). The only reason there is no credibility. However, notion of credibility. j carried out its past threats or not. But then again, if it is clear that j will not carry out its threat (or will to begin with. for future reward), keep its promise It seems that in case of memoryless threats or rewards are applic- able. if agent k does not have a memory of past encounters, In future encounters, there is no sense in making agents only bounded that j may do B is to maintain k will not remember whether threats On the other hand, the counterexample argument is appropriate memoryless the purpose of the argument agent. In this case the agent does not remember the counterexample, is to bring it to its notice. Of course, a “memoryless” in the case of a and agent 24 Note, that this discussion memory of the past. is specific to automated agents. Human negotiators always have at least some may evaluate a counterexample memoryless. type argument as noncredible exactly because the agent is Counterexamples may also be useful as an argument This agent may not have realized the nonomniscient implication, agent may respond with a counter-argument it would not have taken the action in the past either. in the past that its action contradicted for an agent that is not omniscient. its goal. However, the that had it realized Appeal to self-interest is more appropriate in cases when the agent’s beliefs are incomplete. aware of its self-interest, and such an argument may change its intentions. It seems that the arguments used among cooperative in cases where the agent is not omniscient or the agent may not be In both situations self-motivated in noncooperative that are appropriate from those used among noncooperative different environments. concentrated mainly on arguments In a cooperative All the arguments consider goals and desires of self-motivated additional arguments may be made, such as “appeal to a universal principle” environment [ 1481. There is no sense in talking about fairness (e.g., fairness) or “appeal with a “self-motivated” that were built by the same designer, or that belong to the same organization, may be influenced by such an argument. If the agents On the other hand, threats seem inappropriate have common goals, standing to both agents. in the way of another agent may cause damage agent. However, cooperative in cooperative environments. to authority” agents. agents agents would to be agents. Here, we have tend 3.9. An example: labor union versus management negotiation We will consider the labor union example we presented insists on a wage increase. The management to withdraw to lay off employees entail. its request. The management to compensate in Section 3.1. The union says it cannot afford it, and asks the union it will have cost that the increase will claims that if it grants this increase, for the higher operational We will simplify the issues and describe only part of the negotiation process here. Let in wages wants to save money (suve). We the union and the management on us suppose that it is October 2nd, the union (u) has the goals to have an increase (wage.increuse) become effective December 1, and also to prevent causing unemployment (- unemployment). We assume that the management(m) that took place between also assume that in the exchange the previous day, i.e., on October 1, the union had received a message from the management it to call off asking for a December 1 st wage increase (ask. wage. increase), with requesting if there a threat that, otherwise, I, then there will not be a wage will not be a request for a wage increase on December increase and the management will save money. On the other hand, laying off employees will cause unemployment. We also assume that the union believes that the management has a goal to save money on December 1, but it does not have the goal to preserve employment. The union prefers a wage increase. From these that the union assumptions, one can conclude, using the evaluation of the threats axioms, will intend not to ask for a wage increase. The union will revise its goals, and will send the management to hold the an appropriate message. Note, that the union may still continue desire for a wage increase. the management will lay off employees to preserve employment (luy.o#). Evidently, over obtaining S. Kraus et al. /Art$cial Intelligence 104 (1998) I-ci9 29 Prior to Ott 1 (in the recent past): union requests a wage increase. Ott 1 (yesterday): [Octl, Receive,,Request([Decl, -Do(u. ask.wage.increase)], [Decl, Do(u, ask.wage.increase)] + [Drcl. Do(m, lay.qf)])] Ott 2 (today): at start of argument: [Oct2, Goal,[Decl, wageincrease] & Goal,[Decl, -unemployment]] [Oct2, Goal,[Decl, save]] [Oct2, Bel,[Oct2, Goal,[Decl, save] & -Goal, [Decl . -unemployment]]] [Oct2, Pref,,([Decl, -unemployment], [Decl, wage.increase])] [Oct2, Bel, [Decl. -Do(u, ask.wage.increase) -+ -wage.increase & save]] [Oct2, Bel,[Decl, Do(u. ask.wuge.increase) -+ wage.increase & -save]] [Oct2, Bel,[Decl, Do(m, lay.off) + unemployment]] On evaluation of argument: [Oct2,Znt,[[Decl, -Do(u, ask.wage.increase)], [Decl, -Do(m, luy.of)]] -[Oct2, Goal,[Decl, wage.increase]]. 3.10. Contract net example Assume a contract net kind of situation [ 1421 where agents make their bids indepen- in case two agents a and b opt to do dently, but also communicate task. An argument from one agent to another (or of an arbitrator agent to one of a particular the contestants) could be: agent a says to agent b, if you insist on doing the task (tusk.b), then the overall system goal will suffer (e.g., the overall task will not be on time (ontime)). Formally, the following message will be sent, using the axiom “appeal to self-interest”: to resolve any conflicts, [t, Send,b(Request([t’, -Do(b, task.b)], [t’, tusk.b + -ontime]))]. We note that there is a difference between this example and the previous one in that agent the overall task, on account of being to agent b, the deleterious a is not threatening upset over b doing consequences lack of knowledge or inferential power). This is a case of appealing to do an action such as delaying the task. Rather, agent a simply explains agent b could not deduce itself (due to of agent b’s action that presumably to self-interest. There is also a similarity in that, if agent b prefers his goal of the “overall task being on time” than doing the particular subtask itself (which could bring him some reward), then agent b will accept agent a’s argument to the previous examples (assuming b trusts a). 4. Automated Negotiation Agent (ANA) In the previous section we demonstrated specification for agent design. In particular, we demonstrated how axioms specifying how the formal model can be used as a the 30 S. Kraus et al. /Art$cial Intelligence 104 (1998) 169 (ANA) that acts in a simulated i)“, it would try to perform o. Similarly, this aspect of our formal model, we implemented system complies with the definition of an Agent Oriented Programming generation of arguments can be expressed. Another use of the formal model, which we present in this section, is that the agents themselves use the formal model and the axioms. if agent i derives “Do(a, For example, the agents would use axioms and rules to evaluate messages, to send arguments and to update their knowledge bases. In order to demonstrate The an Automated Negotiation Agent simulation (AOP) system [138]. This term denotes several ideas: (i) The agent is represented using notions of mental states. (ii) The agent’s actions depend on these mental states. (iii) The agent’s rules. mental state may change over time. (iv) Mental state changes are driven by inference layer of design flexibility. The system infrastructure gives the as its mental state the user can control the agents’ mental state behavior. In the agents and evaluate the properties for user the ability engine for each agent. Therefore, addition, the effectiveness of various arguments of the system controlling agents” [7.62]. in a Blocks World environment. The ability state behavior could be useful the user can test different methods of negotiation between to define the agents and to set a different mechanism to provide for constructing in the negotiation. We demonstrate ANA gives an additional agent’s mental environment.25 infrastructure “believable 4.1. The structure of an agent and its life cycle The general structure of an agent consists of the following main parts (see Fig. 1): l Mental state (beliefs, desires, goals, intentions). l Characteristics l Inference rules (mental state update, argument generation, argument selection, request (agent type, capabilities, belief verification capabilities). evaluation). The designer of a specific agent can influence following input at agent creation time: its general behavior by providing the (1) Mental state update rules: belief verification rules, goal selection rules and intention generation mechanism (the planner of actions). (2) Argument generation and selection mechanism. rules. (3) Request evaluation In addition, when the user creates an agent, he/she assigns it initial beliefs and desires. The the general inference agent behavior. Once an agent is created, it operates in a loop. We call this loop the agent l$z cycle. rules and the specific beliefs and desires are the driving force behind The agent life cycle includes the following steps: intentions for selected goals. (1) Generating goals for the current time interval. (2) Generating intentions-to. (3) Performing all possible (4) Generating requests (with or without arguments) (5) Selecting one best request from those generated. (6) Sending that request. for an intention-that. 25 ANA was implemented in LPA Prolog for Windows. See [37,144] for a user guide S. Kruus et al. /Art$cial Intelligencr 104 (1998) I-69 AGENT 1 MENTAL STATE: beliefs. desires: goals. intentions. INFERENCE RULES: IR for updating mental state: goals selection, belief verification planner. IR for argument generation IR for argument selection IR for argument evaluation. CHARACTERISTICS: agent type. capabilities. AGENTN MENTAL STATE: beliefs. desires, goals, intentions. Fig. I ANA structure. incoming messages. incoming messages. (7) Reading (8) Evaluating (9) Updating beliefs, goals and intentions. (10) Responding (11) Continuing As we said above, when to incoming messages. from step (3). it initial beliefs and desires. The agent associates a utility with each of its desires and uses its ranked time desires and its beliefs time interval interval. From these goals the agent generates to select its goals. These are the initial goals for the current its intentions the user creates an agent, he/she assigns for the current 32 S. Kraus et (11. /At@cial Intelligence 104 (1998) 149 using a planning mechanism. Each of these intentions intention-to, according actions defined in the intentions. or to the capabilities or lack of capabilities of the agent to perform the is categorized as intention-that as a bargaining If there are intentions The generation of an agent’s intentions completes to be performed cannot be executed by the agent (it is an intention-that), the first stage in the agent’s life cycle. The agent will try to fulfill each and every one of its intentions. that should be performed and can be performed by the agent, then it will execute them, unless these actions can be used by the agent in future negotiations card. If the next intention then the agent will generate a request message to one of the other agents that, according to the agent’s beliefs, can execute it. The agent will now wait for a response. Note that the agent will remember that it issued the request. Later on, the agent will be able to use this information. 26 According or will the agent will continue handling other intentions the two have to generate another request, this time using some type of reasoning. Actually, the agent will agents will start negotiation over this request. It will then select one generate all kinds of arguments to be most suitable, and will send it in a message of these arguments, which it determines is positive, to that other agent. Again, the agent will wait for its execution and continue the agent will try to generate a more persuasive argument and use it in a new request. This cycle will to be used, in which case, the agent will have continue until there are no more arguments to revise its intentions and possibly the agent will wait for a response. that can be used in the negotiation. from that point on. Otherwise, to the response, If argumentation If the response is needed, its goals. its goals from scratch. Assume This is a typical scenario for an agent life cycle. However, there might be cases in which the agent will generate that a second agent issues a request for help which contradicts one of the agent’s goals. If as a consequence of some argument, e.g., a threat, the first agent agrees to help the second agent, then the agent should select a new set of goals from its desires. This time, the selected set of goals will not contradict that second agent’s request. As we discussed in the theoretical section of this paper, time increments play a major role in the process of negotiation. As time passes, agents become more aware of their we environment and can reason about each other’s credibility. is not known divide the time line into several intervals. The number of time intervals to to argumentation are more relevant the agents. We believe than the length of each negotiation phase. We make this restriction, under the assumption that the agents have sufficient time interval. However, during negotiation, future time intervals. This is based on the agents’ ability to remember past events. the agents can take into consideration in the same past and that the stages of negotiation time to negotiate efficiently In the implementation, and take action Thus, assume that during the negotiation phase of the first time interval, the time interval for the next time interval expires. The agent will have to generate is not the same and start all over again. As mentioned above, starting a new time interval as starting the whole scenario from scratch. This time the agent has more information and knowledge about other agents and about the world. Moreover, the agent usually remembers its goals and intentions 26 Given the general framework described in this paper, more sophisticated agents can be constructed Here we describe the simplest agent which we developed. (see [144]). S. Kraus et al. /Artificial fntelligencr 104 (lYY8) l-69 33 threats, and actions. Now, it will be better able to evaluate previous promises, in the new time interval. Promises credibility a promise, negotiation requests the other agents’ in the eyes of the agent. The next time the agent will have to consider such it will not be that naive. The idea is to be able to conduct a more efficient as time progresses. that were not fulfilled will damage to respond We have described to any of the messages as it adapts itself to the situation the negotiation. The logical negotiation protocol between fits into its current plan before sending another request. In our implementation in conducting the life cycle of the agents and their capabilities on the agents’ negotiation with each other. Note that our model places no restrictions the agents has behavior during in which the agents are operating. a loose definition, it receives. On the other An agent has no obligation the model hand, an agent can send as many requests as it wishes for one action. Although does not impose the restriction that an agent has to wait for a response before sending any new requests, an agent presumably will want to wait for and evaluate how the reply to a response we allow the agent to relax this It is possible restriction, but it will require of rules for deciding when to send more and how to evaluate than one argument, which arguments messages which consist of more than one argument. However, sending one argument at a time is considered a good negotiation policy about (e.g., the need for keeping promises). Also, in our the sender and may lead to commitments In particular, communications environment finding a good argument the opponent argument, which can be sent in a new message. is complex, and thus. it is worth trying one argument and only if is not convinced, more time should be spent on finding and selecting a better less time than computations. to send in such situations, to send one argument reveal information in each message. [ 1651: arguments take significantly the development 4.2. Inference rules for mental state changes and updating As described earlier, every agent has inference its mental states according changing syntax of these rules can be found subsections, we present form of these rules and the general rules we implemented which specify the behavior agents exhibit. We also demonstrate environment for inputs. The details of the the the of theses rules in the Blocks World rules, provided by its designer, to various in Section 4.6 and Section 4.7. in [37]. In the following the instantiation 4.2. I. Belief veri$cation rules The agent is assigned an initial set of beliefs by the user of the system. These beliefs the agent receives from the environment change over time, according or from other agents. The agent uses its belief verification according to existing beliefs. The syntax of these rules is as follows: to new information rules to conclude a new belief agent_believes(AgentName, Belief, TimeInterval, TruthValue):-Condition. where AgentName statement time interval that is being generated by the inference that is being generated is the name of the agent. Belief is any data, information (or evaluated) by the inference rule. Timelntervul for which the Belief’s TruthValue is relevant. TruthValue or a fact is the is the truth value rule rule. Condition is the body of the inference (Prolog clause). Once the Cunditio?l to be correct or incorrect at time TimeInterval, according the Condirion. is found to be true, then the agent considers the belief to the TruthValue generated by In implementation Currently, we have defined and implemented two types of negotiating agents, ~~aso~~ble of these agents we the [84]. The agent’s explicit in its knowledge base. The implicit beliefs of a it can conclude using that are omniscient. the agent’s explicit and implicit beliefs and knowledgeable, distinguish between beliefs are the ones that are specified reasonable or knowledgeable rules. That is, the reasonable Section 2.4 and the beliefs of a knowledgeable agent’s beliefs are changing over time. However true, it will not change this truth value unless it receives some new relevant agent are the beliefs agent’s implicit beliefs satisfy axioms (B 1: 16)-(B4:23) of agent also satisfy axiom (B5:24). The is if the agent believes that a sentence information. its inference In addition, at any given to be true at a given time point, is true. However, when considering time point, we make the closed world assumption, i.e., an implicitly believes a sentence or its negation, but this implicit belief may agent always agent needs to check whether change over time. 27 When a reasonable or knowledgeable it will try to infer this sentence it believes a sentence is false t.hat this sentence with respect to the given time point. If it fails, it will conclude it may time period, and that its negation reach a different conclusion. Fu~hermore, the agent can revise its past and future beliefs after observations or after evaluating messages received from another agent. For example, a reasonable agent needs to find out whether its opponent suppose, during the negotiation, desires, at time tl , that block A will be on block B at time 12. It will check its knowledge- is not explicitly stated and it cannot be inferred from base and if this desire of the opponent that its opponent does not desire that A will be on B other explicit beliefs, it will conclude later its opponent asks the agent to put A on at the relevant B, the agent will revise its beliefs. time. However. if, for example, a different 4.2.2. Goal selecrion An agent’s desires may conflict with one another (see Section 2.3.1.) When a new time is reached (or just after the. agent has been created), the agent will look for a subset interval time interval. In order to of its desires do so, the agent may use the preference values to each and every desire. The selected desires (called goals) should not conflict with each other. The syntax for goal ge~~erating rules is as follows: in order to set these as its goals for the specific that is assigned generate_goals(AgentName, ListQfGoals, Tin~eltzterval~AgentNamefGonLState~GoalsTimefnten~al) :-Condition. is the name of the agent; ListQfGoals where AgentName which is a subset of its desires; TimeInterval a different environment is the list of the agent’s goals, is time interval; GoalState depends on the world example as it is used by the user. This will later be used to generate the agent’s of the selected goals. This representation is the current representation ?’ The appropriate semantic restriction on the structure of Section 2.3 is the following: VI E L, R E Agents. z E T. ii & L. ftl E B(/. T. u) or L \ U E &i. T. a>). S. Kraus et al. /Artificial Intelligence 104 (1998) 149 35 intentions. GoalsTimeInterval in which the goals being selected should be achieved. Condition is the body of the rule which defines the way to generate the agent’s ListOfGoals. is the time interval In our case example of the Blocks World environment, we used the simplest way to the agent’s goals. We generated all possible subsets of the agent’s nonconflicting generate desires. Then we selected the subset which has the maximum sum of preference value for all of the agent’s desires, regardless of the number of desires in that subset. 4.2.3. Intention generation-planning A set of rules should be assigned intentions the agent’s selected goals. Performance of the intended actions will lead to satisfying agent’s goal. to an agent to be used to generate for the search(AgentName, GoalState, Path, Intentions) :-Condition. from Intentions is the output rule that was previously is the name of the agent. GoalState the goal executed by the agent. Path is a set of steps that is a list of actions that are to be executed. Each action is In where AgentName generation lead to the intentions. accompanied by its generating cause. The following structure is formed: Action/Source. addition, each intention We added this ID for easier reference that that should be verified before is associated with Precondition which is a statement intention is found to be correct will the agent try to fulfill that intention. Here again, Condition is the body of the rule which defines the way to generate the agent’s Intentions. Once created, the system infrastructure will check each of the intentions to the agent’s capabilities, is assigned an ZntentionZD which is a numeric ID of the intention. intention. Finally, each intention to verify whether the agent is capable of carrying out that Action. According is classified as intention-to or intention-that. is carried out. Only if this precondition to the associated each intention 4.3. Argument production and evaluation In order that an agent be able to negotiate with other agents, it must be provided with rules; (2) argument selection rules; (3) request three sets of rules: (1) argument generation evaluation rules. We discuss them below. 4.3.1. Argument generation rules for its usage. Only if all of these preconditions Each agent in the system is assigned a list of possible argument types. Each argument are met, will the type defines preconditions agent be allowed the agent’s mental state. An agent cannot be certain of another agent’s goals. It simply holds beliefs about the other agent’s goals. Such information can be accumulated by the agent during negotiation. The rules require that the agent be able to use historical data and to evaluate the relationship between actions. type. These preconditions to use that argument are verified against The argument generation rule is used when an agent identifies an intention that it cannot execute (it is an intention-that). The syntax for argument generation rules is as follows: generute_one_argumentger_intention(AgentName,Argument, IntentionID, CurrentTimeInterval, Precondition, Action, GoalTime, Source) :-Condition. 36 S. Kraus et al. /Ar@cial Intelligence 104 (1998) l-69 is the generated argument to the intention and Precondition is the name of the agent; Argument resulting ID of the intention the search for an argument; Action is the intended activity where AgentName from use of the rule (as defined above); ZntentionZD is the numeric which motivates performed according satisfied before the action is executed; GoalTime is the time interval in which the intention is to be achieved and Source is the source cause for that intention; CurrentTimeIntervul is the current the way to generate types of arguments by more complex mechanisms of ANA. We will demonstrate World environment is the body of the rule which defines specific simple rules for the six rules can be replaced easily the main them here to demonstrate their use in a specific scenario of the Blocks the Argument. We have developed identified that should be state that should be time interval. As before, Condition rules (see [ 1441) but we present in Section 3. These simple in Section 4.7. is a condition As was discussed An uppeal topastpromise. in Section 3.4 in this case, the agent expects the opponent agent to perform an action based on a past promise. This type of argument should not be used with a memory-less that before generating assume execute the intended action. The main steps in finding such an argument are as follows: agent since it cannot remember past promises. We that the opponent can the agent has checked an argument ( 1) Check whether the opponent is a “memoryless” agent according to your beliefs. 28 If so, then this kind of argument cannot be used against it, since it does not remember past events. (2) Check that you received a request from the opponent in the past, which included a If that reward was the intended action right now, then this future reward argument. argument type can be used. As was discussed in Section 3.7, in this case, the agent believes ,411 Liypeul to self-interest. that the requested action will serve one of its opponent’s desires. This is a useful argument when the agent believes that the opponent agent is not aware of the implications. Therefore, this should not be used with a knowledgeable such agents are closed under consequences. or reasonable agent, since the inferences of The main steps in finding such arguments are as follows: (I) (2) (3) (4) agent (according If so. then this kind of argument cannot be used against it, since it Check whether the other agent is a ‘reasonable’ or ‘knowledgeable’ to your beliefs). is already aware of its own interests. Find one desire that you believe the opponent has. Generate the list of actions, the plan, which will lead from the current world state to a state which satisfies the opponent agent’s selected desire. This can be done using your own planning procedure. 29 Check whether the action (which is to be executed as a result of the argument which in the previous step. If SO, is being generated here), appears in the plan generated 2x Currently. implementation and gueu whether the agent does not to it. Future could be that an agent, while conducting a long-range negotiation with another agent, will learn type, but uses beliefs given its opponent’s learn about that other agent is a “memoryless” agent or not. “) In the Blocks World scenario. we use a STRIPS-like planner (see Section 4.6). S. Kraus et al. /Art$cial Intelligence 104 (1998) 149 31 use the selected desire in your argument, since the opponent, once carrying out the requested action, will get closer to fulfilling its own desires. This is how the self- interest condition is being checked in this rule. that the opponent agent An appeal to prevailing practice. refuses to perform the requested action since it contradicts one of its own goals. However, the agent gives a counterexample it will serve as a convincing method. The main stages are: from a third agent’s actions, hoping In this case, the agent believes (1) Find a third agent which you believe has executed the same action in the past. (2) Make sure that, according to your beliefs, this third agent had the same goals as the current persuadee. If so, use this third agent as an example. A counterexample. the counterexample assumed the past, or somehow has access to the persuadee’s past history. This argument is similar to “appeal to prevailing practice”; however, is taken from the opponent agent’s own history of activities. Here, it is in that the agent has had a chance to observe the actions of the current persuadee A promise of afuture reward. as a condition not be used with a “memoryless” steps to generate In this case, the agent promises its opponent a future reward for the opponent agent to help it execute the requested action. This should agent, since it cannot remember any promises. The basic this type of argument are: (1) Find one desire of the other agent for a future first joint desires, i.e., a desire of both agents. Also, try to find a desire which can be satisfied through actions that you can perform while your opponent cannot. time interval. Consider (2) Perform step (3) as in the production of arguments of the type, “an appeal to self- interest.” (3) This step results in a set of actions. Out of this set select an action which you can perform but your opponent cannot and that will cause you minimal cost. This action will be offered to the other agent as a reward in the future time interval, if it executes the action needed right now. In this case, the agent threatens A threat. opponent’s plans, if the opponent agent will not help the agent in executing action. The basic steps to generate a threat are: to execute an action which will conflict with its the requested (1) Find one desire of the opponent agent for a future time interval. Consider first desires in your that you can perform with the highest preference value for the opponent which are not included desires set. Also, try to find a desire which involves actions while your opponent cannot. (2) Find a contradicting action to the desire. 3o than it was desired. So, for example, action in the Blocks World, is an action which results in a block being placed to the agent’s desire BlockA should be if according then putting BlockA in 10~2 is a contradicting action and putting Bloc!& in lot 1 is also a contradicting 3o For example, a contradicting in a different position in LxI, action. 38 S. Kraus et al. /Artificial Intelligence 104 (1998) Id9 (3) If you are not able to find a contradicting (4 (b) Cc) to be desired by the opponent. action: Perform step (3) as in the production of arguments of the type, “an appeal to self-interest.” This results in a list of actions that can lead from the current state to a state believed Select one action from the many actions step. Choose a threatening action is one that undoes the effects of actions ” state believed in step (2) or step (3) above) will be presented chosen action action with respect to the selected action. A threatening that would bring about a world to be desired by the opponent. (either that were generated in the previous time interval, if the opponent refuses to the to The opponent execute the requested action right now. agent as a threat for a future 4.4. Argument selection rules The agent may generate several arguments for any specific situation. Only one of these that an agent be should be used for each step of the negotiation. Therefore, supplied with a means for choosing one of the arguments. The syntax for such rules is as follows: it is required select_and_send_one_urgument(AgentName, ArgumentList) :-Condition. is the name of the agent; ArgumentList that where AgentName were generated using (the previously is to be selected; Condition is the body of the rule which defines the way to select and send one argument is a list of all arguments rules and from which one argument from the list of ArgumentList. introduced) Currently, we implemented only one rule of selecting an argument. We order all types by their severity. That is, we order all of the argument argument from the weaker ones to the most aggressive ones, following of [ 1481. Our mechanism of choosing an argument the weakest argument and if it does not succeed, (see [52]). The order is set as follows: types on a continuum the argument severity ordering is simple. The agent will first try to use it will follow with stronger arguments (1) An appeal to prevailing practice. (2) A counterexample. (3) An appeal to past promise. (4) An appeal to self-interest. (5) A promise of a future reward. (6) A threat. We choose Once a rejection take into account this progression is received, two things: since a negotiation usually begins with a simple request. then an argument should be used. Such an argument should term efficiency. the immediate and the long efficiency in the Blocks World, suppose to be in location 1~x2. 3’ For example, loci An action that can bring about the opponent’s desired state is “move Suppose, BlockA from loci to 10~2”. A threatening action then could be any one of the following actions: (i) move BlockA away from 10~2. (ii) move another block to loc2, or (iii) move another block on top of BlockA. the opponent’s desired state is for Block4 that now BlockA is in location S. Kraus et al. /Artificial lntvlligencr 104 (1998) l-69 39 effectiveness. However, a threat has the best immediate Certainly it may be costly for the agent to carry it out. If threats are not carried out, in the long run, they lose their effectiveness. On the other hand, appeals to an agent’s common sense (i.e., an appeal to past promise, an appeal to prevailing practice, and a counterexample) these kind of arguments are not costly and effectiveness can be used regularly. Among since them, appeal to past promise refusing may reduce the opponent’s credibility. The agent first tries a promise for future that rewards reward and only in case it fails, it threatens contribute (both for short and long run) and that, in general, all parties gain from cooperation to cooperation ] I 651. (these are too naive). However, have the least immediate is the most convincing, since we believe its opponent, 4.5. Request evaluation rules Upon receipt of a request (or a counter-request), it. That is, the agent should be able to analyze a request and decide whether to accept or reject it. Moreover, the agent should be able to update its mental state resulting from new requests received and subsequent actions taken. an agent should be able to evaluate The syntax of such rules is as follows: evaluate_message(AgentName, Message) :-Condition. where AgentNume other agent; Condition upon receiving the Message. is the name of the agent; Message is the request message sent from the is the body of the rule which defines the way to evaluate and react In our implemented the request messages. agent, we defined a set of rules for evaluating to its capabilities. 32 First, the agent checks if the requested action can be done according rejection message. Next, it checks if the requested action If not. it generates an appropriate to its beliefs about the world state and the domain. If not, it generates can be done according an appropriate the request and its argument. Since taking any action has a cost, an agent is not likely to agree to every request. For this reason, we have defined three parameters that assist in deciding whether to accept or reject a request as follows: rejection message. If the action can be done, the agent should evaluate A Collision_Flag indicating whether the results of the requested action conflict with the is of this value agent’s current goals. Possible values: TRUE or FALSE (calculation obvious). A Convincing_Factor if at all, is the argument given for the requested action. Possible values: any integer value (positive or negative). The way to calculate this value will be shown later on. indicating how convincing, An Acceptance_Value the overall preference of the results of the requested action as opposed to all the other desires of the agent. Possible values: any integer value (positive or negative). The way to calculate this value will be shown later on. indicating ” An important Gde-effect requested action. is that the agent learns that the requesting agent is not capable of performing the 40 S. Kraus et al. /Artijiciul Intelligence 104 (1998) I-69 Table 1 Request evaluation criteria Collision_Flag = TRUE Collision_Flag = FALSE Convincing_F < 1 Convincing_F > 1 Cunvincing_F d I Convincing_F > I Request is rejected If Acceptance_Value > Performance~ThresholcI, Request is accepted. with the explanation request is accepted. “contradicts a current Add the requested action as an intention. god.” Add the requested action as an intention Else, request rejected with the explanation “not worth performing.” the third parameter When the agent needs to decide on the response to make a decision using the first two parameters. Computing consuming which conditions In general, response of the requested action and the agent’s current goals, and the argument (first column of Table 1). In such a case the request there is no conflict and the argument the other cases, the agent will use the third parameter-the to the other agent’s request, it will first try is time and will be used only when the first two are not helpful. Table 1 specifies under a request is to be accepted by an agent and when it should be rejected. the the results is not convincing is refused. The second case is when (last column of Table 1). However, for in two extreme cases. The first case is when there is a conflict between and the Convincing_Factor Acceptance_VaZue. the Collision_Flug is convincing to determine are enough the Convincing_Factor The Convincing_Fuctor should increase and vice versa. A “do not care” argument or a missing argument will be given the score of 0. In the implementation we used the following simple heuristics is calculated as follows. A more convincing the Convincing_Factor. to determine argument then the agent checks the past events. If is that the then the value given is 0, meaning calculated then the agent checks whether it is indeed for it to carry out the requested action. If so, a value of 1 is given to the is an appeal to self-interest, is an appeal to past promise, If the argument a reward was indeed promised by the agent then the Convincing_Factor equal to I. 33 If there was no past promise, agent is not convinced. If the request beneficial Convincing_Factor, Similar checks for verifying prevailing practice argument and a counter example argument. the argument For the future reward argument and for threats, the Acceptunce_Ikzlue used. Thus, Convincing_Factor than 1 when the Collision_Flug is set at 1 when the Collision_Flag is false. is valid, it assigns 1 to the Convincing_Factor, otherwise zero is given. the truthfulness of the argument are done in the case of If the agent believes and otherwise 0. factor is always is true and to less Calculating Collision_Flag the Acceptance_Kzlue is needed when there is no clear cut decision using the and the Convincing_Factor. Accepting or rejecting a request can influence 33 Meaning, conflicts with its goals, it will need the Accrptance_Value to make a final decision. that the agent will accept the request if it does not conflict with its current goals, and if the request S. Kraus et al. /Artijicial Intelligence 104 (1998) 1-69 41 two things. First, the total preference value that the agent will gain. Second, of actions that should be performed by the agent (referred to below as the intention the number list length). Accepting a request means doing another action in addition to the planned ones (as was determined using the rule of Section 4.2.3). It also means regenerating a completely new, and probably longer intentions list, which is time consuming. Such a list is also likely to achieve a new subset of goals, with a smaller preference value than the original. Rejecting a request seems to save the agent future unnecessary actions but it may cause the loss of a reward action offered by the other agent or lead to punishment by the other agent’s threat execution. In both cases, the agent may have to go a longer route towards its goals, in terms of time and number of intentions. This, too, might conclude reaching that the with a smaller preference value than the original. Here is a list of parameters the Acceptance_Value. Not all the parameters apply to all agent considers when computing argument types: (I) LX (doing length): the number of total intentions needed if the agent will accept the request. (2) NDL (not doing length): (3) DTL (doing that length): accept the request. the current number of total intentions needed. the number of “intention-that” needed if the agent will (4) NDTL (not doing that length): (5) PL (punishment the current number of “intention-that” needed. length): the number of total intentions needed if the agent will reject the request and therefore, the other agent will execute its threat. (6) PTL (punish that length): the number of “intention-that” needed if the agent will reject the request and the other agent will carry out its threat. (7) DP (doing preference): the preference value that the agent will gain, if it accepts the request, i.e., the sum of the preference values of the goals which will be achieved. (8) NDP (not doing preference): not accept the request. the preference value that the agent will gain, if it does (9) PP (punishment preference): the preference value that the agent will gain, if it does not accept the request and the opponent will carry out its threat. The idea is to add ratios of parameters pairs such as NDLIDL which should equal to one when there is no effect and increase when it pays to yield to the request. There are some characteristic that are also taken into consideration: agent parameters l RL: the agent’s reliability. l ORL: the other agent’s reliability for keeping promises. l OTE: the other agent’s percentage of threat executing. The RL parameter is given to the agent by its designer when it is created. ORL and OTE are computed by the agent based on its beliefs about provide the formulas computing the acceptance value for several argument its opponent. Below we types. The acceptance value must always be more than a pre-defined limit, which is defined as the Pq%rmance_Threshold This parameter belongs he/she creates the agent. in order for the agent to accept the request. agent’s parameter, to the characteristics which are determined by the designer when 42 S. Kmus et ~1. /Artificial Intelligence 104 (/99X) I-69 4.5.1. The basic formula for Acceptance-Value The basic formula of this section of all the argument is used as the basis for calculating types. In cases discussed below, additional the Accep- factors are tance-value added to the following basic formula. Basic_Acceptance_Value = NDLI-1 ---+2+2 DL+ 1 the request over performing it. The addition of accepting or rejecting a than the original. When it will eventually have to perform, the agent should and “intention-to”. The reason for this is that another the situation. “Intention- complicate The agent adds ratios of not performing 1 in the formulas prevents dividing by zero. As was mentioned, request may generate a new set of intentions with more intentions considering the number of intentions differentiate between “intention-that” “intention-that”-instead that” means task takes time and does not always succeed. Therefore, importance over NDP in order to consider result by Basic_Acceptance_Value. of an “intention-to”-will that the agent has to persuade another agent ratio. 34 It then multiplies the preference value gained to the “intention-that” to perform that action. This the agent gives twice as much the result by the ratio of DP the in each way. We denote In the case of an appeal into consideration, is taken Acceptance-Value. to past promise, and it is added the reliability parameter of the agent (RL) to get the to the Basic_Acceptance_Value 4.5.2. A promise of a future reward The other agent’s reliability the number of intentions request, depends on the other agent’s reward-keeping should be taken into consideration in this case. In particular, that the agent will have, if the agent accepts the opposing agent’s promises. If the other agent fulfills from its overall set of intentions, because its promise of reward, our agent can then subtract (at least) one intention that intention will be performed by the other agent. Since the agent cannot be certain that the opposing agent will perform the reward action, the agent subtracts one intention multiplied by the opposing agent’s reliability parameter (URL). Acceptance-Value NDL+l-ORL.RD+l DL+l = ( DP+l ‘NDP+ 1’ +2 NDTL + 1 DTL+ 1 -ORL.RDTA+ I 1 In the formula, RD is a flag which is equal to 1, if the action proposed by the opponent the reward is toward satisfying to be a reward list, or establishes a for one of the agent’s intentions or has the same result as one of the actions is really a reward, and 0 if not. RDTA indicates whether the intention-that, which is preferred by the agent. An action is considered (i.e., RD is set at 1) if the action is in the agent’s original precondition intentions j4 The exact form of the formulas were determined by a trial and error process. 5’. Kraus et al. /Artijicial Intelligence 104 (1998) Id9 43 intentions list. It is considered RDTA if the agent is not capable of in the agent’s original performing that intention. For example, consider a situation where BlockA is in loci and BlockB is in 10~2. Suppose that Agent 1 desires to have BlockB at loci and is capable of moving BlockB, but is not capable of moving BlockA. Its intentions list may include moving BlockA to 10~3 and moving BlockB to 10~1. A reward could be moving BlockA to 10~3, but can also be moving BlockA to another location, say 10~4. Moving BlockB to loc3 would not be considered as a reward. An agent’s (AgentY) reliability in the following way: AgentX its promises as is recorded in keeping promises (ORL) is calculated by another agent looks for the percentage of AgentY’s history of in AgentX’s knowledge-base. ORL is set to be this (AgentX) keeping percentage. 4.5.3. Acceptance-Value of a threat If the opponent will carry out its threat, the number of actions that the agent will need increase and its preference value will decrease, given that the to perform will possibly opponent will carry out its threat. Acceptance-Value = ( NDL + (PL - NDL)OTE + 1 DL+l DP+l +2 NDTL+ (PTL - NDTL)OTE+ DTL+ 1 1 > x NDP - OTE(NDP - PP) + 1. The agent adds to the “not doing” portion of the ratio the number of intentions if the other agent carries out its threat. This addition of the threat execution ratio and (PTL - NDTL)OTE will be added the probability “intention-to” the preference of rejecting will carry out the threat and the probability (OTE). That is, (PL - NDL)OTE in the “intention-that” into account of the threat execution that by is added in the ratio. We calculate the loss if the opponent i.e., NDP - the request by taking is multiplied (OTE), OTE(NDP - PP). The other agent’s percentage of threat execution the calculation of ORL, but with respect to threats. 4.6. The blocks world environment (OTE) is calculated in a way similar to Consider a table with unlimited The Blocks World (see, for example, [35,104,170]) has been selected as an example for size and a set of blocks. All blocks our implementation. have the same size. A world state is one of all possible combinations of blocks placed on the table or on each other. A block must be placed on the table or on another block. A block immediately placed back on cannot be taken from the table or from another block unless by any of the table or on another block. No other action can be performed simultaneously the agents. Some blocks are initially placed on the table. We will use the notation to specify a block position. We will omit the blockname when it is clear from the context. A state of (horizontalposition)l(verticalposition) (blockname), 44 S. Kraus et al. /Art&id Intelligence 104 (1998) l-69 the world will be a sequence of block positions between squared brackets. We will use the predicate world_stute(s) that the world is in state s. For example, to indicate world_stute([BlockA/5/1, BlockB/6/1]) that Block4 indicates is in horizontal position 5 and is on the table, and that BlockB is also on the table, near BlockA in location 6. When clear from the context, we will drop the predicate name and write [BlockA/S/ 1, BlockB/6/ I]. to be conflicting Each agent is given a set of desires. Each desire represents a subset of all possible world states which the agent will try to reach. In the Blocks World environment, we define two if one of the two following cases is desires in the same time interval, to the two valid: (a) if two different blocks are to be placed in the same position according to the two desires, or (b) if a block should be placed in two different positions according desires. In these cases the desires are considered to be conflicting and cannot be achieved simultaneously. An intention, in the Blocks World case, is equal to an atomic change to the world state, its position and thereby creates a new world state. 35 In our in which one block changes i.e., the generation of the list of example of the Blocks World environment, intentions in [43]). We supply the algorithm with three input parameters. The first is the current world state of blocks (see Fig. 3). The second is the desired world state that was generated by the goal generating procedure. Last, we supply the algorithm with actions that change the step by step, world state. The algorithm will generate a list of intentions. These intentions, change to the selected goals. In our implementation, the STRIPS-like planning algorithm uses two very simple rules when needed: (1) Pick a block which is not placed the state of the world from its current state to the desired one according for simplicity, If it can be moved to its desired position, and this action will not conflict with some other block’s position, then change it. If not, place that block in any position on the table which is not being used and should not be used by any other block in the desired world state. is performed using a simple depth-first STRIPS-like in its desired position. (described algorithm planning, (2) Pick a block which blocks the movement of another block which is not in its place and move it to a neutral position, as described above. Each of the generated intention-that. This is done according as described above. intentions is now categorized into one of two values: intention-to or to the ability of the agent to perform this intention, Consider a situation of four blocks as presented that Agent 1 has the following goals: It would like BlockA to be in 3/l, BlockB in 2/l, BlockC in 4/l and BlockD in l/ 1. Agent 1 is capable of moving BlockB and BlockC but cannot move Block4 or BlockD. in Fig. 2. Suppose the following Agent 1 generates (1) move BlockB from l/2 to 2/l (2) move BlockA from l/ 1 to 5/l (3) move BlockD from 3/2 to l/l intentions: (intention-to), (intention-that), (intention-that), 35 In [ 1441 we allow more abstract intentions, similar to the ones in our general model. S. Kraus et nl. /Artijicial Intelligence 104 (1998) 149 45 Initial configuration Fig. 2. The initial world state for the example in Section 4.7. Agent l’s desires of interval I 3 4 5 6 Fig. 3. The desired world state for Agent 1 interval preferences. I. The numbers above the blocks indicate the desires’ to 3/l (intention-that). loop while trying (4) move BlockC from 3/ 1 to 4/ 1 (intention-to), (5) move BlockA from 5/l The most well-known weakness of the STRIPS algorithm to achieve to get into an two goals at the same time. This occurs when the endless the action rules two goals do not conflict, while the way to achieve them does. However, in our system ensure that each new state in the world which is provided by the algorithm is different from any of the previous steps and that no one step generated by the algorithm will be generated twice. This ensures that the STRIPS search will not enter into an infinite loop. In the next section ANA’s behavior will be demonstrated using a specific scenario of the Blocks World environment. is its tendency 4.7. Simulation of a Blocks World scenario As an example, suppose there are two agents in the Blocks World environment, each with different capabilities. We assume that the agents hold beliefs about each other’s desires and that each agent has desires for the next two time intervals. In this scenario state is described there are four blocks: BlockA, BlockB, BlockC and BlockD. The initial in Fig. 2, where BlockA and BlockC are on the table in locations 1 and 3, 46 S. Kruus et al. /Artificial Intelligence 104 (1998) I-69 Agent 2’s desires of interval I Fig. 4. The desired world state for Agent 2, interval I Agent l’s desires of interval I1 (25) D (25) c (25) B A 112 3 456 Fig. 5. The desired world state for Agent I. interval 11. respectively, BlockB is on BlockA and BlockD is on BlockC. Agent 1 is capable of moving BlockB and BlockC and Agent 2 is capable of moving BlockA and BlockD. For time interval I, Agent 1 has desires, as shown in Fig. 3, and Agent 2 has desires, as shown in Fig, 4. For time interval II, Agent 1 has desires, as shown in Fig. 5 and Agent 2 has several desires, as shown in Fig. 6. As presented in Fig. 3, Agent l’s desires for the first time interval are that Block4 will be in location 3, BlockB in location 2, BlockC in location 4, and BlockD in location 1, all for its desires concerning BlockA, BlockC, and BlockD on the table. Agent is 15, and its preference for BlockB being in location 2 is only 5. Since this set of desires is consistent, Agent 1 adopts all of them as its goals. These goals require the movement of all the blocks; however Agent 1 is capable of moving only BlockB and BlockC. Therefore, it cannot fulfill its goals without the help of the other agent. l’s preferences Agent 2 also cannot fulfill its goals without the help of the other agent. It would like as Agent 1 would like (i.e., BlockA, BlockC and BlockD to be in location 6 (Fig. 4). locations 3, 4 and 1, respectively), As in Agent thus Agent 2 adopts them as its goals. Since Agent 2 is capable of moving only BlockA and BlockD, it must get help in moving BlockB and BlockC. Also, the agents will need to reach an agreement where to put BlockB. l’s case, all these desires are consistent, to be in the same locations like BlockB but would S. Kraus et al. /Arr@ictl Intelligence 104 (I 998) 1-69 41 Agent, 2’s desires of interval II 20 rl A 1 2 3 4 5 6 B 5 1 2 3 4 (15) B 2 1 3 4 5 (16) A 6 A 6 Fig. 6. The desired world states for Agent 2, interval II The negotiation in the first time interval is affected by the agents’ desires in the second time interval. Agent 1 would like BlockA, BlockB and BlockD to be in locations 6, 5 and 1, respectively, on the table and BlockC to be on BlockB (Fig. 5). The preferences of all these desires is 25. Again, this is a consistent set of desires, but agent 1 may need help in moving BlockA and BlockD. Agent 2’s desires for the second time period is more complicated (Fig. 6.) They concern only BlockA and BlockB, but conflict with each other. Agent 2 has a desire that BlockA will be in location 2 on the table with preference 20. Its desire to have BlockA at location 6 and BlockB at location 5 (on the table) has preference 16. Another desire is that BlockA will be at location 6 and BlockB at location 2, and its preference is 15. The set of desires conflicts, and thus Agent 2 will need to choose only one of them. At first, it chose the first desire (BlockA in 2/l) with the higher preference l’s goals. Only the second desire of Agent 2 is compatible with Agent to be its goal, which conflicts with Agent l’s goals. In the beginning of the first time interval, both agents searched for plans to satisfy their chosen goals. Agent 1 found the following plan: (1) move BlockB from l/2 (2) move BlockA from l/l (3) move BlockD from 3/2 to l/ 1, (4) move BlockC from 3/ 1 to 4/ 1, (5) move BlockA from 5/ 1 to 3/ 1. to 2/ 1, to 5/l (temporarily), As we mentioned therefore, intention-that. above, Agent I was not capable of moving BlockA and BlockD, and the intentions with respect to steps (2), (3) and (5) of its plan were of the type Agent 2 found the following plan and adopted the relevant (1) move BlockB from l/2 to 6/l, (2) move BlockA from I/ 1 to 2/ 1 (temporarily), (3) move BlockD from 3/2 to I/ 1, (4) move BlockC from 3/ 1 to 4/ 1, (5) move BlockA from 2/ 1 to 3/ 1. intentions: Since Agent 2 could not move BlockB and BlockC, the intentions plan were intention-that. in steps ( 1) and (4) of its Figs. 7 and 8 describe the main steps of the negotiations and activities of the first time interval. The steps of the negotiation, which are indexed by the numbers in the first column, are only to note the order in which the agents sent their messages and took their actions. If, on the same step, both agents have an entry, that means that Agent 1 sent a message and, from a step, that means just after, Agent 2 sent its message. that this agent did not send a message while the other agent sent two messages If one of the agents is missing in a row. there is a change its plans and intentions We highlight some interesting points of these steps. After step 2 Agent 2 revised its plan, in the world state which the agent did not expect, then for the current time interval. Therefore, after and changes to move BlockA to position 5/l and not to 2/l, as in its original is not empty any more). Also, Agent 2 intends-that BlockB be (and also requested at since whenever the agent re-considers Agent 1 at step 2 moves BlockB to 2/l, Agent 2 re-generates its plan, so that it intends plan (since location 2/l moved from location 2/ 1 and not from l/2, as it originally planned step 1). its intentions In step 3, Agent 1 asked Agent 2 to move BlockA to its temporary location 5/l so that BlockD will be moved to BlockA’s old position at 1 / 1 (see Agent 1 ‘s plan above). In step 6, Agent 2 moved BlockD to 5/l before it read the request sent to it by Agent 1 and sent a request to Agent 1 in the same step. Thus, after it has finished (step 7) it read its message concerning BlockD, and, of course, sent an acceptance message (step 8). the movement, Until step 13, no argumentation was needed, since the requests were exactly according the to the plans of the active agent. One request was rejected, at step 4, which concerned conflict block, i.e., BlockB. Agent 1 moved BlockB to its desired location 2/l and rejected Agent 2’s request to move it to 6/ 1. In step 13, Agent 2 requests again to move BlockB (now from its new location 2/l) to 6/l. Agent 1 rejects the request again. Therefore, Agent 2 generates a promise of future reward. Note that for keeping this promise, Agent 2 will need to give up a desire of preference 20 in the next time interval, and instead will be able to fulfill a desire of preference 16. In return, it fulfills a desire of preference 5 in the first time interval. Agent 1 evaluates this argument and is convinced fulfill a desire of preference 25 in the next time interval, the current time interval of preference 15. Agent promise. Thus in step 16, Agent 1 moves BlockB to 6/ 1. that getting help from Agent 2 to is worth not fulfilling a desire of that Agent 2 will keep its I also believes S. Krms Agent 1 Activities c 1 rt al. /Artificial Intrlligmcr 104 (199X) I-69 39 Agent 2 Activities Request_; Agent 1 to move BlockB from position l/2 to position 6/l. 2 Moves BlockB from posit,ion l/2 to position 2/l. 3 Requests Agent 2 to move BlockA from position l/l to position 5/l. 4 Rejects Agent 2 request to move BlockB, Accepts Agent, l’s request t,o move since it conflicts with its desire of locating BlockA. BlockB to position 2/l. r, Moves BlockA from position l/l 6 Requests Agent 2 to move BlockD htoves BlockD from position from position 3/2 to position ~_. ~~. l/l. 3/2 to position l/l. to position 5/l. Requests Agent 1 to move BlockC from position 3/l to position 4/l. Arcepts Agent l’s request) to move BlockD from position 3/2 to to position l/l (since it has already m( lved BlockD ro l/l). Accepts dgent 2’s request, to move BlockC from position 3/l to position 4/l. 7 8 + L 9 I 10 Moves BlockC from position 3/l to position 411. 11 Requests iigent 2 to move BlockA Accepts Agent l’s request to move Blo from position 5/l to position 3/l. from position 5/l to position 3/i. 12 RIoves Block-4 from position 5/l 7 ckA to position 3 /I. Fig. 7. The main steps of the negotiation in interval I The first time period ends after step 16. The state of the world after step 16 is as desired their l’s desires are by Agent 2 (Fig. 4). At the beginning of the second time interval, both agents consider desires, generate goals, and generate plans for the new time interval. Agent consistent; thus it chooses them all as its goals and generates the following plan: Agent 1 Activities Agent 2 Activities BlockB since it conflicts with its desire 13 14 1 Rejects Agent 2’s request to move of locating BlockB at position Z/l. 15 Accepts Agent 2’s request to move BlockB from position 2/l to position 6/l, since it Requests Agent 1 to move BlockB from posii,ion 2/l to posit,ion 6/l. Requests Agent 1 to move BlockB from position 2/l to position 6/l. The request is justified with a promise for future reward in the next time interval, of moving BlockA from position 3/l to position 6/l evaluates the argument and finds it convincing. 16 Moves BlockB from position 2/l to position 6/l. Fig 8. The main steps of the negotiation in interval 1 (cont.). ( 1) move BlockB from 6/ 1 to .5/ 1, to 6/ I, (2) move BlockA from 3/I (3) move BlockC from 4/ 1 to 5/2. Since Agent 1 cannot move BlockA, the second move turns into an intention-that. Agent 1 must get some help from Agent 2. Agent 2’s plan is as follows: (1) move BlockA from 3/ I to 2/ 1, (2) move BlockB from 6/ 1 to S/ 1. The main steps of the negotiations are listed in Fig. 9. In step 18, Agent 2 moves BlockA to position 5/l. to position 2/ 1 before reading After reading the request, since it conflicts with its goal that BlockA be in 2/ 1. At step 21, Agent 1 sends the request again, with an argument the argument, step. After receiving in the previous reminding Agent 2 of its promise Agent 2 decides to keep its promise and accept the request. It re-generates its goals for the time interval and selects the second desire (see Fig. 6). It re-generates a plan for achieving the new goal which consists of moving BlockA from 2/ 1 to 6/ 1. It performs step 24. the request of Agent 1 to move BlockA this message. Agent 2 rejects this action at We will demonstrate the use of the argumentation the above scenario. An argument of an appeal to past promise An appeal to self-interest Agent 2 move BlockD from 3/2 to 1 / 1. It could have used an argument the interest of Agent 2, since it follows from its desires (Fig. 4). in Section 4.3 using is used in step 21 above. could be used in step 6 above. In this step, Agent 1 requests that that this act is in rules described An appeal to prevailing practice could have been used if the scenario had been expanded time interval. Suppose that Agent 3 has a desire to have BlockA at location to include additional agents Agent 3 and Agent 4 and an additional this expansion is done and suppose S. Kraus et al. /Artijicial Intelligence 104 (1998) I49 51 Agent 1 Activities Agent 2 Activities 17 Moves Blocks from position 6/l to position 5/l. 18 Requests Agent 2 to move Blockh Moves BlockA from position 3/l to from position 3/l to position 6/l. position 2/l. 19 Requests Agent 2 to move BlockA from position 2/l to position 6/l. 20 Rejects Agent l’s request to move BlockA? from position 6/l to position 5/l. since it conflicts with its preferred desire of having BlockA in position 2/l. 21 Requests Agent 2 to move BlockA Evaluates the request and from position 2/l to position 6/l. accept Agent. l’s request to move The request is justified by the BlockA from position 2/l to position 6/l. promise made by Agent 2 to Agent 1 in the previous time interval. 23 Moves BlockA from position 2/l i to position 6/l. Fig. 9. The main steps of time interval II. 2/ 1, and Agent 4 has a desire to have BlockA at 6/l. Furthermore, Agent 3 is the only agent that is capable of moving BlockA, where the initial world state is: BlockA at l/ 1. A similar situation occurred in the first interval. In the first interval, Agent 1 was convinced by Agent 2 to move BlockA to 6/ 1, even though it conflicted with its desires. Agent 4 can now use the activities of Interval to convince Agent 3 to place BlockA on 6/ 1 instead of 2/l. I, as prevailing practice argument could be used in an example similar A counterexample where Agent 2 (rather Agent 4 has a desire to have BlockA at 6/l. Agent 4 can use the activities of Interval as a counterexample. to the previous one, and I to have BZockA at location 2/l than Agent 3) has a desire A promise for future reward is used in step 14 above. A threat could be used by Agent 2 in the scenario above. Agent 2 could threaten Agent 1 that in the next time interval it will l/ 1 to position 3/ 1. This threat is credible, since it will not move BlockD from position conflict with any of Agent 2’s desires in the second time interval, but will prevent Agent 1, who cannot move BlockD and wishes it to stay at position from satisfying one of its desires. l/l, 52 S. Kraus et al. /Artificial Intelligence 104 (1998) l-69 5. Related work Our work on negotiation and argumentation combines a formal model, an implemented testbed for developing general negotiation in agents, and an instantiation the Blocks World environment. Therefore, the work shares common issues with four main Distributed Artificial Intelligence (DAI) areas: formal models of mental state of an agent, agent oriented research. Other related areas of research are defeasible dialectics, negotiation In the following models subsections, we present related work in these areas and situate our research in the relevant literatures. Reader interested in detailed surveys of these areas may consult papers such as [10,14.167]. languages, multi-agent planning, and negotiation in game theory, and social psychology reasoning and computational research on persuasion. of the model research 5. I. Mental state intelligence Numerous works in artificial research try to formalize a logical axiomatiza- tion for rational agents (see [ 1671 for a good survey). This is accomplished by formalizing a model for agent behavior BDI (belief, desire, intention) in the research community systems (see [128]). Many similar definitions are presented in terms of beliefs, desires, goals etc. These works are known as for BDI systems. existing research is noted when comparing The first difference which is the varying [16,18] use only two attitudes, these attitudes such as intentions, [ 126,128] use a wider definition of attitudes: beliefs, goals and [ 1531 use the same set of mental states: beliefs, In [ 1.521, they consider an extended set which also includes for our needs of describing for addressing usage of attitudes and pro-attitudes. Cohen and Levesque beliefs and goals, and define other attitudes, only. Rao and Georgeff intentions. Both Shoham [ 1381 and Thomas commitments and knowledge. desires. In all of these cases, the definitions the issues of a more complex behavior of the agents which arguments. We use four basic attitudes: beliefs, desires, goals producing and evaluating and intentions. Desires, which are originally given to the agent, may be inconsistent. The goal set is a consistent like to satisfy. The intentions are formed to make the goals true. subset of its desires, which the agent would are not suitable is required using that we discussed Most of the BDI research adopts Kripke possible worlds semantics This yields the known problem of logical omniscience side-effect problem for intentions are not omniscient use arguments, order to allow considering structures believes p, and 9 is logically equivalent agent may believe p but not 9, even if p -+ q. (e.g., [16,59,75]). for belief and knowledge and the agents that agents may agents. In a wide variety of agents, we adopted the approach of minimal in our logic, that if an agent to it, then the agent also believes q. However, the [ 151 for all our modalities. We still have the problem, in the context of negotiation. Nonomniscient that are not useful for omniscient such as appeal to self interest, in Section 2.1. Considering is important Other approaches that attempt clude [33,38,39,71,84]. Works for appropriate semantics and [73] which also take the minimal is that we deal with all the modalities in- include [28,69,1261 structure approach. The advantage of our approach in the same manner, which addresses both issues to solve the problems associated with omniscience for intentions S. Kraus et al. /Artificial Intelligence 104 (1998) 1-69 53 and side-effects for intentions), and also allows for contradicting desires, a (omniscience condition that is critical for negotiating agents. In addition, since time plays an important role in negotiation, we do not use simple structures as in [73], but rather our possible worlds are time lines [152]. This toward past and future events and their change (but the belief-desire-intention accessible time lines that also allow uncertainty minimal enables us to consider agents’ attitudes over time. Georgeff and Rao [ 126,129] consider complex possible worlds semantics not minimal In their formalism, time structures. We preferred worlds are branching about the past, not only about the future, as in their model. structure models). In our framework, intentions, an agent can reason about other agents’ mental states in terms of (for example, see and goals. Not all systems allow this behavior (for example, systems intentional its beliefs, [16,18,58]). However, most of the latest systems do allow such reasoning see [ 128,138,153]). This is the exact difference between (where agents reason only about systems (where agents reason about other agents’ mental states, as well) as stated in [25]. in order to address the needs In our system, reasoning of argument generation. in our logic, every goal of an agent is also one of its desires. This is a unique relationship between the two attitudes, since in most other systems agents do not hold both mental states. their own mental states) and second-order about other agents is a necessity In addition, intentional first-order there may be some intentions In our logic, every goal is also an intention. Additional are steps to satisfy goals. However, that are not motivated by an agent’s own goal, but rather by a request from another agent. Researchers who did not consider multi- agent environments with negotiation is also a goal. (e.g., [ 1261) assumed that every intention intentions Another difference concerns then the agent believes that this proposition the relations between [ 16,181 assume in Section 2.6, Cohen and Levesque toward a proposition, be true some time in the future. According the proposition. possible (axiom (lNTB2:34) of Section 2.6). Since time is expressed explicitly we are able to present different characterize the appropriate and Rao [ 1261 also consider histories and its intentions. and beliefs. As discussed intentions that if an agent holds an intention is not true, but that it will to their logic, time does not explicitly appear in are in our logic, and relations. Georgeff relations between an agent’s beliefs about possible semantic constraints on their accessible interesting In our logic, we only require that the agent believes types of agents with different levels of self-confidence that its intentions As described in Section 4.1, in ANA we distinguish two types of intentions (intention- In Cohen and Levesque’s this distinction. the agent will look for ways to achieve it by itself. Moreover, such as that of Cohen and Levesque, do to and intention-that). Many other systems, system, once an agent adopts an not make the agent believes intention, in our case, the agent, once creating an that the intention intention, the it on its own or not. However, agent is uncertain whether there is another agent which is able to successfully execute that intention. Here, we follow the logic as presented it is capable of executing can be achieved. knows whether In contrast, in [55-571. Since our agents are self-interested, we do not try to provide formal specification agents working together on a joint goal [SS], team activity [ 17.1431, or SharedPlan for [55,56]. 54 S. Ktzus rt al. /Artijcial Intelligence 104 (199X) l-69 We concentrate on agents that have their own desires and negotiate resolve conflicts in order to maximize their own utility. to obtain help or to 5.2. Agent oriented language,s The idea that agents are modeled in terms of their mental states led to Shoham’s It enables modules paradigm. Shoham the Agent0 system. definition of an Agent Oriented Programming AOP language called beliefs about others and about to each of them a set of a program must allow its user to define agents and to assign capabilities, rules (to update their mental states over time). A few years later Thomas presented a second language which is the ability of its is the descendant of AgentO, called PLACA [ 1531. Its main contribution agents to plan their activities. first implemented to process knowledge initial mental states, and some mental-change the world. In order to be considered initial beliefs, [ 1381 an and an AOP language, Our system ANA can be viewed as an AOP language as well. Our main concern when set of rules for negotiation, building ANA was to allow its user to define the environment, and inference on different stages of the negotiation. As in the case of PLACA where a is to enable a user planning mechanism language, our contribution to define a planner with the ability of conducting a negotiation between the agents. Our aim is to allow argumentation in order to fulfill each of the agents’ desires and resolve conflicts during planning is added to the Agent0 system, In the Agent0 is the negotiation and argumentation to their commitment rule contains a message condition, a mental condition, are met. More specifically, rules. Each and an action. The commitment agent acts only when these two conditions the agent acts only when it receives a message from other agents. This is not the case in ANA. Our agents are motivated by their desires, and their activities are almost solely caused by their own wishes and desires. The more obvious difference between ANA and the two systems Agent0 and PLACA ability, and the concept of agent life cycle being supported by our system. The Concurrent Metatem language by Fisher [44] presents a system in which multiple agents can work simultaneously. Each agent is assigned a unique behavior specification and to these behavior rules. ANA is similar to the Concurrent Metatem system it acts according there are two major conceptual differences with respect between ANA and Concurrent Metatem. The first is that Concurrent Metatem allows and supports grouping of several agents together. Agents can be grouped together and form a to (from) an existing group, and messages can be group, agents can be added (removed) sent to all members of a specific group. We do not support such a feature in our system single agents that handle negotiation on their own. since we are interested The second difference rules in Concurrent Metatem are based on general condition-result Wavish [161] presents system which is a two- the Agent Behavior LanguagE-ABLE level agent behavior system that supports user definition of agents. The first level defines behavior for a group of agents. These rules of behavior are called licenses, schemas, and functions, and rules. In ANA such rules can correspond to different kinds of forward chaining production rules for an agent, while the second level can be used to define behavior to these two features. However, rules and not on a BDI logic. is the fact that the behavior in examining so that successful plans can be found. the agents act according S. Kruus et ~1. /Art$cial Intrlliyw 104 (1998) Id9 5.5 in negotiation, we present that do not exist. Since we are interested relate to negotiation only. Another difference between is the fact that the ABLE system can execute several rules for the same agent in parallel. These rules can be nested and can also be limited in time. These features and capabilities are missing from our system. but are not needed at this stage of the research. As in the case of the Concurrent Metatem system, the ABLE system is not a BDI system. rules for behavior the two systems, and Weerasooriya In its environment, Rao, Ramamohanarao [ 1301 present a distributed autonomous system, in families, offering services called AgentSpeak. to other agents, as well as data sharing functionality. This language supports concurrent object oriented languages and well developed communication capabilities. Although called AOP, strictly speaking it is not, since it does not support well defined BDI features for its agents. agents are organized language the two systems a simple planner and implementing and communication, [98] is a multi-agent In order to demonstrate that the aim of the planning the usage of ANA, we implemented The Agent PRocess Interaction Language-April is to construct a means for facilitating pure multi-tasking to pattern matching and symbolic processing capabilities. Trying in specifying purpose parallel these issues produces difficulties must use only basic April primitives. ANA uses the Prolog programming order to get the underlying was developed on top of the logic infrastructure. This enables the agents in a more intuitive way. Yet another difference between ability of an ANA agent to plan its actions in advance. an ability lacking system. Its main in to cover all of agents, since the user in logic part of the system working, while the multi-agent part the user of ANA to define is the in the April agents. for the Blocks world environment. This planner can be easily replaced by other planners. It is to remember important in ANA is to seek ways for reaching their goals. An agent only tries to satisfy its desires by generating is different from many other DA1 systems indicates planning for multiple agents’ activities, in order to reach some the agents (see Section 5.3 below). Such kind of agreement or to schedule tasks can be seen in ANA as one of the goals of the argumentation process, but the planning mechanism list of steps which the agent will use to satisfy its own goals. [ 1281 also consider in their system. In their case an agent was given a set of predefined plans which can be used to satisfy several goals at the same time. Their agent chooses one of the plans and in that way assigns itself a set of actions to be performed. The selection of the plan, its desires, and intentions. Their planning activity fulfills a purpose similar to if all ours. However, they also impose on the agent the belief that its goals are achievable agents act in appropriate ways. This is a very strong assumption that do not hold in our system. and process which is carried out by each of the agents is meant to generate a a list of actions which will lead it to its goals. This definition of planning in which the term planning several agents’ actions is based on the agent’s beliefs about the side-effect activity executed by the agents the problem of planning Rao and Georgeff that is, planning tasks between (or limitation) As mentioned above we implemented from its goals. This is accomplished intentions (as described usage is presented in [43]). The algorithm in [l]. Since planning a very simple mechanism for planning an agent’s algorithm is adjusted for our needs and world example. Similar task of our system, we will not the STRIPS-like is not a main by executing 56 S. Kruus et al. /Artt@inl Intelliptce 104 (1998) 1-69 present a thorough overview on the subject. For a well presented survey on planning, reader is advised to see Allen, Hendler and Tate [2]. the 5.3. Multi-agentplanning Multi-agent planning in DA1 research has evolved along a number focuses on agents dimensions. One dimension a given problem and integrate concerns planning centralized planning execution multiple agents’ mental attitudes for coordinating for task allocation, for multi-agent time action conflicts the results execution that cooperate [20,21,24,30,36,114]. so that effective execution will result [67,113], or centralized planning [50,51]. Another area of research has concentrated to solve subproblems of different of Other lines of inquiry [9.5,141], to avoid on [ l6,.55,150,154J. their activities has focused on multiple agents each of which Another area of investigation is self- i.e., has its own goals, which could be in conflict with the goals and/or actions motivated, of others [ 134,136]. This literature has not concentrated on deriving the plan steps; instead, it assumes that the agents are capable of deriving plans, and only concentrates on resolving the conflicts through forms of coordinated negotiation in goals and utilities [20,17 11. Our work combines plan generation with negotiation and argumentation during planning as an explicit mechanism for plan adjustment and conflict resolution during planning. Argumentation has been used as a method to represent in a multi-agent plan context. In particular, Ferguson and Allen’s development process [42] in a mixed-initiative work [42] aims to define plans as arguments, in the sense of [90], so the agents can reason defeasibly whether a certain course of action under certain explicit conditions will achieve certain goals. In our work instead, we use arguments the intentions of other agents so that effective plans can be produced. as a mechanism to influence interactions 5.4. Automated negotiation is usually Negotiation process used and arguments (see discussion and incrementally to as a communication in [147]). Two main approaches referred and resolve conflicts this line. The agents simulated their intentions to achieve are coordination introduced agents in previous research. The first approach suggests that by communicating, can influence each other’s goals and intentions. This influence should lead to resolution of goal and plan conflicts and better cooperation counterproposals work follows using argumentation to the other negotiating party. The PERSUADER similar system which involves a multi-agent program negotiation. its union, and a mediator. The negotiation model of the PERSUADER has also been applied to the domain of concurrent engineering [ 1491. Our paper tries to create a more general solution limiting inherited many of the the number of agents which can perform negotiation. Our system ideas presented the agent’s beliefs and behavior, several types of arguments, setting levels of strength for each argument (see [148]). The agents exchange proposals, reach an agreed upon solution. Our in our system try to convince each other and perform actions which are beneficial system by Sycara (see [ 146-1481) is a that operates in the domain of labor Some of these are the use of utilities type, and the argumentation for any domain, without three agents: a company, in PERSUADER. to change It contains to change strategies. S. Kraus et al. /Ar@cial Intelligence 104 (1998) l-69 51 to the negotiation they may continue performing The second approach suggests that incremental is introduced which is used by the agents. Using this mechanism, suggestions performed by the agents can find a plan that will satisfy all agents (refer to [74,78,1361). Here one of the main issues the negotiation. As time passes, some agents may benefit is time and how it influences their tasks. However, other agents may since, for example, lose, since each of their goals is aimed for a certain each agent will have a different approach process. mechanism conduct negotiation shown and even determines delaying agreements consider for automatization time to perform Therefore we do not analyze time. Therefore, In [78], a distributed negotiation the agents it is time, affects is that in the negotiation. Our current paper does not in creating a general system that there is enough and that each agent’s utility does not change over time. and can reach efficient agreements without delays. Moreover, of the negotiation process. In ANA, we assume the final agreement causes that is reached. The main conclusion the impact of time on the negotiation. approach of each agent towards time, since we are interested the overall negotiation that the individual the negotiation the negotiation inefficiency The Diplomat system by Kraus and Lehmann [76] presents a general model for a agent and handles the issues of who to negotiate with, how to generate the negotiating suggestions, how to evaluate counter suggestions, players. Yet this system was implemented Our system, though intended such as coalition building. We focus on allowing that will perform his/her own kind of desired negotiation and argumentation. game Diplomacy. to be a more general system, did not address difficult issues the user to define his/her own set of rules and how to form coalitions among for one specific domain-the information for various is to combine exchanged between to the global solution. in distributed problem for the entire group. The information In other research (e.g., 120,83,89]) negotiation Research by Lesser and Durfee and colleagues [23,3 1,821, addresses the issue of agents’ solving systems. Here, the main purpose of the communication from several agents (usually sensor data) in order to research the agents reach a conclusion levels of the problem. Later these will is usually based on partial solutions is used as contribute to try agents that use different search operators a metaphor In other negotiation models (e.g., [ 163]), agents to arrive at a global conflict-free that arise. Our approach differs negotiate through an arbitrator who resolves that the agents are designed as part from this line of work. In these works it is assumed towards a global system wide goal. In our work, each of a global system and are working agent is trying its own goals. We do not assume any common knowledge or goal. nor any immediate will to cooperate. On the contrary, we assume that the agents are self-motivated and argumentation. for a group of heterogeneous solution. should be pursued and achieved via negotiation and that cooperation the conflicts to achieve and Koperczak [97] is a decision Negoplan by Matwin, Szpakowicz support system its for conducting negotiation. Through simulating both parts of the negotiation process, Its main model presents main task is to give one party of the negotiation the current situation of the negotiation and by using a Goal Representation Tree (GRT) it is able to suggest paths for the negotiation process in which a better outcome could be achieved its main task is It does not represent different to support its user to negotiation for the party using the tool. Although strategies nor use argumentation. In contrast, our system allows this is a general system. and to suggest actions. the user in negotiation an advantage. simulate resulting several agents in an analysis of the negotiation and argumentation together, each of which uses different kinds of argumentation. process. Liu and Sycara [88] have modeled negotiation the agents are self-interested that gives them highest utility, but are also cooperative lower utility to facilitate reaching an agreement. The model does not use argumentation mental-state in the sense that they would relaxation process where like to achieve an agreement in the sense that they would accept for as a constraint revision. Parsons and Jennings [ 1121 have followed our formalism described in [77] to construct in negotiation. A recipient agent it. Their notions of for and against to evaluate proposals and counterporposals a proposal by constructing arguments evaluates argument defeat are based on the work of (34,45,79,90]. the social aspects of action arguments Gasser the agents. for a domain between [48] discusses In his the social mechanisms language between the agents, and forming different communities of agents. changing or no is most effective when the agents’ structure in the conflicts between view, different communication This approach structure exists at all. In our system, environments the agents), we do assume a formal means for interfacing between in which no pre-defined protocol exists (for solving this is not the case. Although we are interested in multi-agent resulting systems. in changing can dynamically is continuously emerge, lay interactions the ground in which their research (UNP) so it can be used in different and Rosenschein They describe [ 134,170-1721 a way of classifying theory of Zlotkin agents. This negotiation. and classification helps designers of agents to choose appropriate negotiation mechanisms the product of the agents’ utilities strategies. They use the Nash solution which maximizes types of and call it the Unified Negotiation protocol encounters. The main domain is the Blocks World environment, which we also used as an example domain. However, there is a significant the two works. We base the negotiation part of our work on a formal difference between to analyze logic model, based on the agents’ mental state. We did so, since we wanted the conflicts scenarios for solving In addition. between the agents. This is not the case in the work of Zlotkin and Rosenschein. of a sequence of plan steps but assume they do not deal explicitly with the construction them to that the agents have somehow constructed satisfy conflicting notions of utility. In contrast, we show how explicit communication of arguments and the change process. in mental states of the agents are part of the plan construction in which there is no pre-defined protocol or mechanism their plans and are choosing among is concentrated interest Recently, there has been increasing (e.g., [32,11 1,123]). Zeng and Sycara into the negotiation process [ I68,169] have developed an economic bargaining negotiation model, where the agents are self-interested. The model emphasizes the learning aspects. The agents keep histories of their interactions and update their beliefs, using Bayesian updating, after observing and the behavior of the other negotiating agents. their environment in integrating learning 5.5. Dgfeasible reasoning and compututinnul dialectics Many argumentation logical systems have been proposed in defeasible main purpose of these logics is to construct “defeasible proofs”, called arguments reasoning. The that can S. Kraus et al. /Artijicial Intelligence 104 (1998) l-69 59 proofs that may make use of assertions in conclusive that one sentence force. Arguments be partially ordered by relations expressing differences are primafacie is a (defeasible) reason for another. They indicate support for a proposition, but do not establish warrant [ 1.561 once and for all; it matters what other counterarguments there may be. Arguments may have structure [49,122]. sentences the “strength of arguments” In general, and criteria for determining so that undefeated difference arguments can be found and presented. This body of work assumes is logically undefeated is the most persuasive. syntactic grounds with no explicit links and representation preferences that is viewed on only of agents’ objectives, actions or [90,116], or may just be collections of supporting this body of work has focused on formalizing in strengths among arguments In other words persuasion that an argument (utilities) [94]. A large number of formal argumentation systems exist. Pollock [ 116-1211 developed arguments. Nute is system OSCAR that can reason with suppositional the LDR system where adjudication the argumentation [ 108,109] developed performed via so called top-rules. An argument defeats another if and only if the antecedent than the antecedent of the of the top-rule of the first argument top-rule of the second. Loui [90-931 presented a system of argumentation where defeat among arguments specificity, directness and [ 1391. In addition, Loui developed an evidence and introduced a new defeasible operator implementation for this rule system to compute defeat among arguments among competing arguments is strictly more specific in terms of interference, is defined recursively are trees. Lin and Shoham [63] presented a theory of mixed inheritance argumentation, where arguments Horty and Thomason proof nets that resembles Paths are one-dimensional systems where arguments system that captures some well-known nonmonotonic McDermott and Doyle’s nonmonotonic logical hierarchy among arguments. So, it is not possible undefeated. Konolige [72] proposed a solution important of which he discussed on situation calculus, where properties are attached mathematical attacks all attackers of that argument. symbolic lines of reasoning and are therefore simpler than argumentation an argument logic. they do not have is in the context is based to situations. Dung [29] presented a is accepted by S if and only if S issues in defeasible argumentation. His formalism logic, etc.). In their system, to determine which argument to the Yale Shooting Problem logics (e.g., Reiter’s default theory where an argument [87] developed argumentation [92]. in nonmonotonic are called paths. Vreeswijk [ 1601 presents a thoughtful critique of existing argumentation systems. We in some cases, (e.g., [ 1161) fallacious arguments that defeat lines of reasoning; or (e.g., [90]) cyclic sets of arguments can In general, most systems have determined which argument such as credulous [90,109,12 l] and others. Almost all systems to construct arguments present some of his critique. For example, are produced be constructed where each argument defeats its successor. it cannot be unambiguously in situations where difficulty should win. Various attempts to fix this have given rise to approaches reasoning include detailed specification that are syntactically undefeated argument [ 1601 presents an abstract argumentation how argumentation an is correct for one situation but wrong for the other [ 1591. Vreeswijk to prescribe should be performed, what arguments are in force, or how defeasible system where it is not attempted it is possible semantic [96,99,13 11, skeptical situation descriptions, of defeat. However, but, for different isomorphic reasoning 60 S. Kraus et al. /Art$%d Intelligence 104 (1998) I-h9 information notions of argumentation should be manipulated, but rather present a general framework in which basic (e.g., defeat) take on well-defined meaning. Our work differs from this body of work in that in our system we do not focus on in legal reasoning. Rather, to can guide negotiation by supplying or on which arguments a mechanism to present the beliefs and actions of others and achieve coordination in situations In this respect, our research also touches upon research the logical defeasibility concentrate on how argumentation agents to influence where agents are self-interested. on game theory and multi-agent planning. itself with legal disputation, AI work in legal reasoning has concerned this work is concerned with criteria for argument their refutations, based on rules or cases. Much of this work has been case-based 132,140]. In particular, on factors of a case, and when and how to combine [94] whose Additional work in this vein argument change to prevailing practice” and is the most frequently used argumentation method reasoning. types and providing a formal account of how they type of “appeal argument in legal arguments and [3,4, strength based rules and cases to support claims. by representing legal disputation. Case-based is exactly our argument includes rationale rationales, defining is reasoning focus 5.6. Game theory’s models of negotiation Game theory work [46,101,155] concerns itself mainly with determining In particular, solutions conditions in game [102] is heavily is the players the possibility of forms a Nash equilibrium if each player’s strategy under which the outcomes of a game can be predicted. theory consist of equilibrium relied upon. A profile of strategies response an optimal take their opponents’ influencing to the other players’ strategies. strategies as given and therefore do not consider In a Nash equilibrium, strategies. The notion of Nash equilibrium them. in which a player chooses actions after observing In games this conjecture that do not involve noncredible [135]. However, it was limiting leads to some absurd Nash equilibria some of his opponents’ [155]. Selten [46] proposed the subgame perfect equilibrium. The basic idea of actions, concept, a more restrictive equilibrium subgame perfect equilibrium is to select Nash equilibria threats, i.e., a threat that would not be carried out if the player were put in the position to do so, since the threat move would give the player lower payoff than he would get by not doing the threatened action. In many games of complete this notion turns out to be very powerful In games of incomplete information. The inference process takes the form of Bayesian updating player’s supposed equilibrium [ 1371 introduced depend on what one party believes However, for information along the equilibrium path, a move by a player can be designed beliefs, but a move off the equilibrium a zero-probability cannot be chosen. However, Selten’s equilibrium notion could not adequately of-equilibrium for games of complete a player who observes another’s move can extract from the second strategy and the observed action. For such games, Selten the notion of perfect Bayesian equilibrium. The outcomes of such games to actions by the first. sets that are not reached, Bayes formula does not hold. Thus, to influence his opponent’s event, so it restrict out- for “too many” the second will do in response to establish credibility path is considered thus enabling information, information. the players information beliefs, S. Kraus et al. /Artificial Intelligence 104 (1998) I49 61 threats. In other words, equilibrium incredible beliefs with communication (e.g., signaling games). [54]. The unrestricted proliferation of equilibria also occurs could be supported for threatened behavior through in games equilibria for each player’s To be able to pose restrictions (e.g., [41,54,100]). Grossman defines information the player will follow. A metastrategy could be on beliefs so that unreasonable game theorists more closely examined notions of credibility of threats and of beliefs eliminated, consistency which prescribes set, the set of actions when his beliefs are given by a probability distribution by the other player. An updating sets as a function of his belief in the past. Based on these notions, Grossman information defines credible updating these notions, he generalizes the idea of a game node to include a belief as well as a history. Using his mathematical definitions of these notions, he is able to show that he can restrict the set of resulting equilibria. the notion of metastrategy set and his beliefs over the information specifies a player’s action and he has observed a message sent the belief that a player has at each of his rule and consistent belief. Using rule specifies research theoretical The main and but equilibrium) represented. under which is on defining [80] sequential and exogenously solutions, but rather concentrate they can be obtained. Communication largely on Aumann’s derives and his payoff-relevant statements. Such analysis focus of game the conditions is not explicitly [ 1001 put it well: “the theory of noncooperative (based representing and evaluating communications to an agents’ mental attitudes. Arguments are exogenous in of ignore his increase and cannot provide a way to select among equilibria. To escape statements have the to define appropriate notions of is In our framework, we on a logic (arguments) where to the games with signaling and [5] correlated equilibrium and Kreps and the meaning of all statements and signals in which they are used. In fact if the mere act of saying something then there is always a “babbling” equilibria considered do not focus on the presence of equilibria framework for explicitly arguments are connected game. Myerson communication Wilson’s from the equilibrium does not directly affect any payoffs, which every player randomizes over the set of his possible statements his information meaningless the set of equilibria, from this conclusion, we must introduce literal meanings that are exogenously “reliability” notion of the credibility of negotiation and “plausibility”. These are defined precisely and mathematically. He defines negotiation statements that describes how the negotiator plans to choose his future actions and messages. promise that the negotiator may want for the other players and a request that describes strategies between our own work on them to use thereafter. We see a deep connection or expect them as mechanisms arguments in belief and in viewing especially framework. Our logical framework provides a language behavior of players and Myerson’s and inference mechanism for modeling and deploying a broader range of argumentation strategies, however we do not explicitly addition, we integrate agent interactions. conditions In framework within a process oriented model of of issues are not actions, and in which all other players that communication suggests then proceeds in terms of “tenability”, defined”. Myerson, statements to note that Myerson’s mathematical that describes a negotiator’s private equilibrium independently for credibility of negotiation in terms of an allegation (although computational provide a solution the argumentation effecting change It is interesting that negotiation the assumption formulation. information. formulation statements can only concept a 62 S. Kraus rt al. /Artijicicrl lntelligrncr 104 (IY98) i-69 addressed convincing power of an argument and its evaluation by the receiver agent. in his work) could be integrated into our framework and used to determine the 5.7. Social psychology A large part of the literature in social psychology and in particular is mediated by judgmental theory deals with persuasion. The basic tenet of social judgment change step process changes attitudes through controlled affect receiver’s judgement experimentation with human subjects and cognitive attitude change. [ 1 lo]. This literature presents the receiver assesses in which the position advocated processes and effects. Persuasion theoretical hypotheses to investigate theory in social judgement is that attitude is seen as a two- in the message, and that are then tested that the factors The research most relevant to our paper includes a number of investigations. Experi- this con- ments have found that threats (“fear appeal”) are very effective the fear appeal content of the message, whether, for text, some researchers have examined example, that such it contains gruesome pictures etc. But there is no conclusive evidence messages are persuasive. Another set of findings relates to presenting particular examples findings show that presenting examples versus statistical summaries. Robust experimental is much more persuasive type of “appeal to prevailing practice”. [61,70.107,15 11. This relates to our argument [ 11,14.5]. Within Other issues that social judgment credibility of the source of the argument of an argument. This finding concerning argumentation experimental present the weakest argument best [52]. This is the strategy we adopted for our model. theory examines are the notion of whether increased increases force research has been performed strategies (presentation of sequences of arguments). Though the it seems that the climax strategy (i.e., stronger arguments) works (the persuader) [8,9]. In addition, first and follow with increasingly is not absolutely conclusive, the persuasive is robust evidence 6. Conclusions In a multi-agent environment, where self-motivated for planning how to influence agents’ intentions and reduce disparities and conflicts. agents try to pursue their own goals, has been advocated as cooperation cannot be taken for granted. Persuasive argumentation in order to increase a general mechanism In this paper we have presented their cooperativeness in which the user can a formal framework and develop and test various algorithms and mechanisms of and negotiation between self-motivated reasoning joint plans where the agents may have different goals and where plan steps may give agents differing amounts of utility. this explicit representation in the process of constructing and a simulation environment has been interleaved about argumentation agents. In addition, for argumentation for establishing communication A formal mental model of the agents based on minimal-structure lines) has been developed using modal operators for beliefs, desires, set of properties. Under different assumptions (time and goals having an appropriate properties and conditions on the agent models, different of possible worlds intentions of agent types of agents have been S. Krnus rt al. /Art$cial Intelligence 104 f 199X) 149 63 scheme has been constructed for argument generation defined. A formal axiomatization and evaluation based on argument types identified addition, suitability of the various types of arguments described. A persuading to generate persuasive particular persuadee that are suitable arguments type. agent uses its model of another agent, the persuadee, from human negotiation patterns. In for the different agent types has been in order situation and the for the negotiation the need for persuasive argumentation As distributed systems become more widespread, and as humans become part of human- agent systems, formalisms will become apparent, especially when agents constrained by incomplete knowledge and bounded rationality will be forced to interact. One obvious and immediate application of such formalisms is agents that negotiate to provide services on the Internet-based environment. In summary, l Formalizing the main contributions of this paper are: to some extent the “semantics” of argumentation and linking to mental attitudes of the agents. l Unlike game theory, we explicitly anisms for influencing agents (in situations of incomplete information). others’ beliefs and behavior represent and reason about arguments as mech- in interactions of self-interested l The reasoning about argumentation system where it is used to reconcile conflicting goals and plan steps and guide plan adjustment and adjudication. in a multi-agent is integrated planning One aspect of our future work includes a more detailed between different modalities. Another modalities over time in the course of the argumentation external events and observations reputation of adversaries based on repeated encounters the argumentation investigation future focus will be investigation of the relations in the of change process, and as the result of from the environment. An analysis of the credibility and into is currently being incorporated process. Most importantly, different negotiation and verifying for selecting one best argument negotiation. future research will comparatively settings and for different in types of agents. Creating new arguments their effectiveness under different conditions may lead to effective criteria thereby getting the most out of the evaluate various arguments in a specific situation, References [ I] J.F. Allen, Planning as temporal reasoning, in: Principles of Knowledge Representation and Reasoning. Proceeding of the Second International Conference pp. 3-14. (KR-9 1). Morgan Kaufmann, San Mateo. CA. 199 I. 121 J.F. Allen, J. Hendler, A. Tate, Readings 131 K. Ashley, Defining salience in case-based in Planning, Morgan Kaufmann. San Mateo, CA, 1990. arguments. in: Proceedings IJCAI-89, Detroit. MI, 1989, pp. 537-542. 141 K. Ashley, V. Aleven. Toward an intelligent tutoring system for teaching law students to argue with cases, in: Proceedings 199 1 ACM International Conf. on AI and Law, 199 1. [S] R.J. Aumann, Correlated equilibria as an expression of bayesian 161 C. Baral, S. Kraus. J. Minker, V.S. Subrahmanian. Combining knowledge bases consisting of lust-order rationality, Econometrica 55 (1987) l-1 8. theories, Computational Intelligence 8 (1) (1992) 457 1, [71 J. Bates, A. Bryan Loyall, W. Scott Reilly, An architecture for action. emotion, and social behaviour. in: C. Castelfranchi, E. Werner (Eds.). Artificial Social Systems. Springer. Berlin. 1994, pp. 55-68. 64 S. Kraus et (11. /Artijiciul Intelligence 104 (1998) I-69 [8] M.J. Beatty, R.R. Behnke, Teacher credibility Communication Quarterly 28 (1) (1980) 55-59. as a function of verbal content and paralinguistic cues, [9] R.A. Bell, C.J. Zahn, R. Hopper, Disclaiming: a test of two competing views, Communication Quarterly 32 (1984) 28-36. [lo] A.H. Bond, L. Gasser. An analysis of problems and research in DAI, in: A.H. Bond, L. Gasser (Ed%). Readings in Distributed Artificial Intelligence, Morgan Kaufmann. San Mateo, CA, 1988, pp. 3-35. [l 11 EJ. Baster, P. Mongeau, Fear-arousing 1984. Yearbook 8, SAGE Publications, persuasive messages. in: R.N. Bostrom (Ed.), Communication 1121 M.E. Bratman, What intention? Communication, MIT Press, Cambridge, MA, 1990, pp. 15-3 1. in: P.R. Cohen, is J.Morgan, M.E. Pollack (Eds.), Intentions in [ 131 M.E. Bratman, Shared cooperative activity, Philos. Review 101 ( 1992) 327-34 1, [ 141 B. Chaib-draa, Distributed Artificial Intelligence: an overview, AI Review 6 (1) (1992) 35-66. [ 151 B.F. Chellas, Modal Logic: An Introduction, Cambridge University Press, Cambridge, UK, 1980. [ 161 P. Cohen, H. Levesque, Intention [ 171 P. Cohen, H. Levesque, Teamwork, Not&, 1991. pp. 4X7-5 12. [ 181 P.R. Cohen, H. Levesque, Rational M.E. Pollack (Eds.), Intentions in Communication, MIT Press, Cambridge, MA, 1990, pp. 221-256. interaction as the basis for communication, is choice with commitment, Artificial Intelligence 42 (I 990) 263-3 10. in: P.R. Cohen, J.L. Morgan, [ 191 P.R. Cohen, J. Morgan, M.E. Pollack (Eds.), Intentions in Communication, MIT Press, Cambridge, MA, 1990. [20] SE. Corny, K. Kuwabara. V.R. Lesser, R.A. Meyer, Multistage negotiation satisfaction, IEEE Trans. Systems Man Cybemet. 21 (6) (1991) for distributed Special Issue on Distributed Artificial 1462-1477. Intelligence, [21] D. Corkill, Hierarchical planning in a distributed environment. in: Proceedings 6th International Joint Conference on Artificial Intelligence (IJCAI-79) Tokyo, 1979. pp. 168-175. [22] M. Dalal, Investigations into a theory of knowledge base revision: preliminary report, in: Proceedings AAAI-98, 1998, pp. 475479. 1231 K. Decker. V. Lesser, A one-shot dynamic coordination algorithm for distributed sensor networks, in: Proceedings AAAI-93. Washington, DC, 1993, pp. 210-216. 1241 K. Decker, V. Lesser, Designing a family of coordination Conference on Multi-Agent Systems (ICMAS-95), San Francisco, CA, June 1995, pp. 73380. algorithms, in: Proceedings 1 st International 1251 D.C. Dennett, The Intentional Stance, MIT Press, Cambridge, MA, 1987. 1261 J. Doyle, Some theories of reasoned assumptions: an essay in rational psychology, Technical Report 83- 125, Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, 1983. [27J J. Doyle, Rational belief revision, [28] J. Doyle, Y. Shoham, M. Wellman, A logic of relative desire, in: Proceedings 6th International Symposium in: Proceedings KR-91. Cambridge, MA, 1991, pp. 163-l 74. on Methodologies for Intelligent Systems, 199 1. [29] P. Dung, On the acceptability of arguments and its fundamental programming, and human’s social and economical affairs. Artificial role in nonmonotonic reasoning, Intelligence 77 (1995) 321-357. logic (301 E.H. Durfee, V. Lesser, Negotiating task decomposition and allocation using partial global planning, in: L. Gasser, M. Huhns (Eds.). Distributed Artihcial Kaufmann, San Mateo, CA, 1989. pp. 229-244. Intelligence, Vol. Il. Pitman Publishing, London/Morgan [31] E.H. Durfee, V.R. Lesser, Partial global planning: a coordination framework for distributed hypothesis formation, IEEE Trans. Systems Man Cybernet. 2 1 (5) (199 1) 1167-l 183. 1321 G. Dworman, S.O. Kimbrough. J.D. Laing, Bargaining by artificial agents in two coalition games: a study in genetic programming for electronic commerce, in: Proceedings AAAI Genetic Programming Conference, 1996. [33] J. Elgot-Drapkin. D. Perlis, Reasoning Intelligence 2 (1) (1990) 75-98. situated in time 1: basic concepts. J. Experiment. Theoret. Artificial [34] M. Elvang-Goransson, P. Krause, J. Fox, Dialectic reasoning with inconsistent information, in: Proceedings 9th Conference on Uncertainty in Artificial Intelligence, 1993. [35] E. Ephrati, J.S. Rosenschein. The Clarke tax as a consensus mechanism among automated agents, in: Proceedings AAAI-91, Anaheim, CA, 1991, pp. 173-178. [36] E. Ephrati. J.S. Rosenschein, Distributed consensus mechanisms in: Proceedings Netherlands. May 1993. International Conference on Intelligent and Cooperative for self-interested heterogeneous agents, Information Systems, Rotterdam, S. Kraus et al. /Arfijicial Intelligence 104 (1998) 1-69 65 [371 A. Evenchik, Inference system for argumentation in negotiation between automatic agents, MSc Thesis, Department of Mathematics and Computer Science, Bar-Ban University, Ramat-Gan. Israel. 1995. [38] R. Fagin, J. Halpern, Belief, awareness, and limited reasoning, Artificial Intelligence 34 (1988) 39-76. [39] R. Fagin, J. Halpem. M.Y. Vardi, A nonstandard to the logical omniscience problem, Artificial approach Intelligence 79 (2) (1995) 203-240. [40] R. Fagin, M. Vardi, Knowledge and implicit knowledge in distributed environment: preliminary report, in: J.Y. Halpern Monterey, CA, Morgan Kaufmann, San Mateo, CA, 1986, pp. 187-206. (Ed.), Proceedings 1st Conference on Theoretical Aspects of Reasoning about Knowledge. [41] J. Farrell, Meaning and credibility in cheap-talk games, in: M. Dempster (Ed.). Mathematical Models in Economics, Oxford University Press, Oxford, 1988. 142) G. Ferguson, planning, J.F. Allen, Arguing about plans: plan representation and reasoning for mixed-initiative in: Proceedings 2nd International Conference on AI Planning Systems. 1994, pp. 4348. [43] R.E. Fikes, N.J. Nilsson, STRIPS: a new approach to the application of theorem proving to problem solving, Artificial Intelligence 2 ( I97 1) 189-208. and executing [44] M. Fisher, Representing agent-based systems, in: Intelligent Agents, Lecture Notes in Artificial Intelligence, Vol. 890. Springer, Berlin, 1995, pp. 307.-323. [45] J. Fox, P. Krause, S. Ambler, Arguments, European Conference on Artificial Intelligence contradictions reasoning, and practical (ECAI-92), Vienna, Austria. 1992. in: Proceedings 10th [46] D. Fudenberg, [471 J.W. Garson. Quantification J. Tirole, Game Theory, MIT Press, Cambridge, MA, 1991. in modal logic, in: D. Gabbay, F. Guenthner (Eds.), Handbook of Philosophical Logic II, D. Reidel, Dordrecht. Netherlands, 1984, pp. 249-307 [4X] L. Gasser, Social concepts of knowledge and action: DA1 foundations and open systems semantics, Artificial Intelligence 47 (l-3) [491 H. Geffner, J. Pearl, A framework (Eds.), Knowledge Representation 24.5-260. (1991) 107-138. for reasoning with defaults, and Defeasible Reasoning. Kluwer. Dordrecht, Netherlands, Jr., R. Loui, G. Carlaon 1990. pp. in: H.E. Kybug [50] M.A. Georgeff. Communication and interaction in multi-agent planning, in: Proceedings IJCAI-83, Karlsruhe, Germany, 1983, pp. 125-129. [Sll M.A. Georgeff. Theory of action for multi-agent planning, in: Proceedings AAAI-84, Austin, TX, 1984, pp. 121-125. 1521 H. Gilkinson, S.F. Paulson, D.E. Sikkink, Effects of order and authority in an argumentative speech, Quarterly J. Speech 40 (1954) 183-192. 1531 M.L. Ginsberg. Counterfactuals. Artificial Intelligence 30 (I ) (1986) 35-79. [541 S. Grossman, M. Perry. Perfect sequential equilibrium, 1551 8. Grosr. S Kraus, Collaborative plans for group activities, in: Proceedings J. Economic Theory 39 (1986) 97-119. IJCAI-93, Chambery, France. 1993, pp. 367-373. [5hl B.J. Grosz. S. Kraus, Collaborative plans for complex group activities. Artificial Intelligence 86 (2) (1996) 269-357. [571 B.J. Grosz. S. Kraus, The evolution of SharedPlans, in: A. Rao. M. Wooldridge (Eds.). Foundations and Theories of Rational Agency, Kluwer, Dordrecht, 1998. for agent-oriented [5X1 S. Hagg, F. Ygge, An architecture programming with a programmable model of interaction, in: Proceeding AICS-94, Dublin. Ireland. 1994. [591 J.Y. Halpem, Y. Moses, A guide to completeness and complexity for modal logics of knowledge and belief, Artificial Intelligence 54 (3) (1992) 319-379. 1601 J.Y. Halpern, Y. Moses, Knowledge and common knowledge in a distributed environment, J. ACM 37 (3) (1990) 549.-587. 161 I R. Hamill, T.D. Wilson, R.E. Nisbett, Insensitivity to sample bias: generalizing from atypical cases, 3. Personality and Social Psychology 39 (1980) 579-589. [621 B. Hayes-Roth. Agents on stage: advancing Quebec, August 1995, pp. 967-97 I. the state of the art in AI. in: Proceedings IJCAI-95. Montreal, [631 F. Horty. R. Thomason, Mixing strict and defeasible inheritance. in: Proceedings AAAI-88, St. Paul, MN, 1988, pp. 427432. [641 A.J.I. Jones, Toward a formal Pollack (Edh.). Intentions theory of communication and speech acts. in: P.R. Cohen, J. Morgan, M.E. in Communication, MIT Press, Cambridge. MA. 1990, pp. 161-185. 66 S. Krcrus et al. /Art$ciul lntrlligrrtcr 104 (IWK) l-69 1651 M. Karlins, H.I. Abelson, Persuasion: How Opinions and Attitudes are Changed, 2nd ed., Springer, Berlin, 1970. L661 H. Katsuno, A. Mendelzon, Knowledge base revision and minimal change. Artificial Intelligence 52 ( I 99 I ) 263-294. [67] M.J. Katz, J.S. Rosenschein. Verifying plans for multiple agents, J. Experiment, Theoret. Artihcial Intelligence 5 (1993) 39956. 16X1 D. Kinny. M. Georgeff. Commitment and effectiveness of situated agents, in: Proceedings 12th International Joint Conference on Artificial Intelligence 1691 Cl. Kiss, H. Reichgelt, Towards a semantics of desires, (IJCAI-9 I). Sydney, Australia, 199 I, pp. 82-8X. (Eds.), Decentralized in: E. Werner. Y. Demazeau Artificial Intelligence, Vol. 3, Elsevier. Amsterdam, 1992, pp. 1 IS-1 28. [70] R.R.Jr. Koballa, Persuading to reexamine the effect of anecdotal versus data-summary teachers innovative the communications. yesterday: 23 (1986) 437-449. elementary J. Research science programs of in Science Teaching [71] K. Konolige, A Deduction Model of Belief, Pitman, London, 1986 1721 K. Konolige, Hierarchic autoepistemic AAAI, St. Paul, MN, 1988. pp. 42-59. theories for nonmonotonic reasoning. in: Proceedings AAAI-X8. [731 K. Konolige, M.E. Pollack, A representationalist theory of intention. in: Proceedings IJCAI-93, Chambery, France, 1993. pp. 390-395. [741 S. Kraus, Beliefs. time and incomplete agents, Ann. Math. Artificial information Intelligence 20 (l-4) in multiple encounter negotiations among autonomous (1997) 11 I-159. 1751 S. Kraus. D. Lehmann. Knowledge. belief and time, Theoret. Comput. Sci. 58 (1988) 155-174. [76] S. Kraus. D. Lehmann, Designing and building a negotiating automated agent, Computational Intelligence 1 I (I) (1995) 132-171. 1771 S. Kraua. M. Nirkhe, K. Sycara, Reaching agreements through argumentation: a logic model (preliminary report), in: Proceeding\ Workshop on Distributed Artificial Intelligence. 1993. [78] S. Kraus. J. Wilkenfeld. G. Zlotkin. Multiagent negotiation under time constraints, Artificial Intelligence 75 (2) (1995) 297-345. 1791 P. Krause. S. Ambler, M. Elvdng-Coransson, J. Fox, A logic of argumentation for reasoning under uncertainty, Computational Intelligence 11 ( 1995) 113-l 3 I. [X0] D. Kreps, R. Wilson, Sequential equilibria, Econometrica SO ( 1982) 863-894. 18 I] S. Kripke. Semantical considerations [X2] B. Laasri, H. Laasri, V. Lesser, Negotiation on modal logic, Acta Philoa. Fenn. 16 (1963) 83394. and its role in cooperative distributed problem solving, in: Proceedings 10th International Workshop on DAI, 1990. Chapter 9. [83] S. Lander, V. Lesser. Understanding the role of negotiation in distributed search among heterogeneous agents, in: Proceedings 12th International Workshop on DAI, Hidden Valley, PA, 1993. (841 H. Levesquc, A logic of implicit and explicit belief. in: Proceedings AAAI-84, Austin. TX. 1984, pp. 19X-202. 1851 H. Levesque, 94-99. I? Cohen. J. Nunes, On acting together, in: Proceedings AAAI-90. Boston, MA. 1990, pp. [X61 M. Lewis, K. Sycara, Reaching informed agreement in multi-specialist cooperation, Group Decision and Negotiation 2 (3) (1993) 279-300. [X7] E Lin. Y. Shoham, Argument systems: a uniform basis for nonmonotonic reasoning, in: Proceedings 1st International Conf. on Knowledge Representation and Reasoning. Toronto, Ont., 1989, pp. 245-255. 1881 J.S. Liu, K.P. Sycara, Distributed meeting scheduling, Cognitive Science Society. Atlanta, GA, August 1994. in: Proceedings 16th Annual Conference of the 1X9] J.S. Liu, K. Sycara, Coordination of multiple agents for production management, Ann. Oper. Res. 75 C 1997) 235-289. 1901 R. Loui, Defeat among arguments: a system of defeasible inference, Computational Intelligence 3 (I 987) 1 OO- I 06. 1911 R. Loui, Defeat among arguments II, Technical Report WUCS-X9-06, Department of Computer Science, Washington University. St. Louis, MO, 1989. 1921 R. Loui. A design for reasoning with policies, precedents. and rationales. in: Proceedings 4th International Conf. on AI and Law, 1993. pp. 202-21 I. S. Kraus et al. /Artificial Intelligence 104 (1998J I-69 61 1931 R. Loui, Argument and arbitration games. working notes of the workshop on computational dialectics, in: Proceedings AAAI-94, Seattle, WA, 1994, pp. 72-83. [94] R. Loui, J. Norman, Rationales and argument moves. Artificial Intelligence and Law 3 (3) (1995) 15Y-189. and markets. Management Science 33 (1987) 13 17- [Y5] T.W. Malone, Modeling coordination in organization\ 1332. [96] J. Martins, S. Shapiro, A model for belief revision. Artificial Intelligence 35 (198X) 25-79. [97] S. Matwin, S. Szpakowicz, Z. Koperczak, G.E. Kersten, W. Michalowski, Negoplan: an expert system shell for negotiation support, IEEE Expert 4 (4) (1989) 50-62. [Y8J F. McCabe. K. Clark, April: Agent Process Interaction Language. in: Intelligent Agents, Lecture Notes in Artificial Intelligence, Vol. 890, Springer, Berlin, 1995, pp. 324-340. [9Y] J.-J.C. Meyer, Modal logics in Knowledge Engineering, for knowledge Instruments 1992, pp. 25 l-275. representation. in: Proceedings Workshop on Linguistic 1 1001 R.B. Myerson, Credible negotiation statements and coherent plans. J. Economic Theory 48 (1989) 263- 303. [lOI] R.B. Myerson, Game Theory: Analysis of Confict. Harvard University Press. Cambridge. MA, 19Yl. [ 1021 J.F. Nash. Noncooperative [ 1031 B. Nebel. A knowledge games, Ann. Math. 54 (1951) 289-295. level analysis of belief revision, 1st lntemational Conference on and Reasoning. Toronto, Ont., Morgan Kaufmann, San Mateo. in: Proceedings Principles of Knowledge Representation CA, 1989, pp. 301-311. [IO41 N.J. Nilsson. Principles of Artificial Intelligence, Morgan Kaufmann. San Mateo. CA, 1980. 11051 M. Nirkhe, S. Kraus, D. Perlis, Situated reasoning within and realistic tight deadlines on Logical Formalizations space and of Commonsense computation Reasoning, 1993. bounds, in: Proceedings 2nd Symposium [IO61 M. Nirkhe, S. Kraus, D. Perlis. Thinking take\ time: a modal active-logic for reasoning in time, in: Proceedings BISFAI-95, 1995. [ 1071 R.E. Nisbett. E. Borgida, R. Cram&all, H. Reed. Popular (Eds.). Cognition in: J.S. Carroll, J.W. Payne induction: is not necessarily and Social Behavior. Lawrence Erlbaum, information informative, Hillsdale, NJ, 1976. [ 1081 D. Nute, A nonmonotonic logic based on conditional logic, Technical Report ACMC 01-0007. Advanced Computational Methods Center, University of Georgia, Athens, GA. 1986. [IOS] D. Nute, Defeasible [ 1 lo] D.J. O’Keefe. Persuasion: Theory and Research, SAGE Publications, [ 1111 J. Oliver, An automated negotiation 1990. reasoning and decision support systems. Decision Support Systems 4 (1988) 97-l 10. and electronic commerce. PhD Thesis. University of Pennsylvania. Philadelphia, PA, 1996. 11 121 S. Parsons, N.R. Jennings, Negotiation through argumentation--a preliminary report, in: Proceedings 2nd International Conference on Multi-Agent Systems, 1996, pp. 267-274. 11131 E. Pednault. Formulating multi-agent dynamic world problems about Actions & Plans-Proceedings in: 1986 Workshop. Morgan Kaufmann. San Mateo. CA, in the classical planning paradigm. Reasoning 1986, pp. 47-82. [ 114) M.E. Pollack. The uses of plans. Artificial Intelligence 57 (I ) (1992) 43-68. [ 1151 M.E. Pollack, Plans as complex mental attitudes, in: P.R. Cohen. J. Morgan, M.E. Pollack (Eds.), Intentions in Communication, MIT Press, Cambridge, MA, 1990. pp. 77-103 J. Automated Reasoning 6 (1990) 419-461. Internat. J. Intelligent Systems 6 (1991) 33-54. reasoning, Cognitive Science 11 (4) (1987) 48 l-5 I X. [ 1161 J.L. Pollock. Defeasible [ 1171 J.L. Pollock, Interest driven suppositional [ 1181 J.L. Pollock. Self-defeating [119] J.L. Pollock, A theory of defeasible [ 1201 J.L. Pollock. How to reason defeasibly, Artificial Intelligence 57 (1992) l-42. 1121 I J.L. Pollock. Justification and defeat, Artificial Intelligence 67 t 1994) 377407. [ 122 I D. Poole, On the comparison of theories: preferring arguments, Minds Mach. 1 (199 I) .367-392. reasoning, reasoning, the most specilic explanation, 85. Los Angeles, CA, 198.5, pp. 144147. [123] M.V.N. Prasad. V.R. Lesser. S.E. Lander. Learning organizational agent system, Special Human-Computer Studies (IJHCS). to appear, issue on Evolution and Learning in Multiagent Systems. [ 1241 D.G. Pruitt. Negotiation Behavior. Academic Press, New York. I98 1. in: Proceedings IJCAl- roles for negotiated search International in a multi- Journal of 68 S. Kruus et al. /Artijiciul Intelligence 104 (1998) lb69 112.51 A. Rao, N.Y. Foo. Forma1 theories of belief revision, 1st International Conference on and Reasoning, Toronto, Ont., Morgan Kaufmann, San Mateo. in: Proceedings Principles of Knowledge Representation CA. 1989. pp. 369-380. I 1261 A. Rao. M. Georgeff, Modeling rational agents within BDI architecture. in: Proceedings 2nd International Conference of Knowledge Representation, Cambridge, MA, Morgan Kaufmann, San Mateo, CA, 199 I, pp. 473484. I1271 A. Rao, M. Georgeff, A model-theoretic approach to the veritication of situated reasoning systems, in: Proceedings IJCAI-93, Chambery, France, 1993, pp. 3 18-324. [ 1281 A. Rao, M. Georgeff, BDI agents: from theory to practice, in: Proceedings 1st International Conference on Multi-Agent Systems, 1995, pp. 3 12-3 19. 11291 A. Rao, M.P. Georgelf, Asymmetry thesis and side-effect problems in linear-time and branching-time intention logics, in: Proceedings I I301 A. Rao. K. Ramamohanarao, IJCAI-91, Sydney, Australia, 199 1, pp. 498-504. D. Weerasooriya. Design of a concurrent agent-oriented language, in: Intelligent Agents. Lecture Notes in Artificial Intelligence, Vol. 890. Springer, Berlin, 1995, pp. 386401. 113 I ] R. Reiter, A logic for default reasoning, Artificial Intelligence 13 (1980) 81-132. [ 1321 E. Rissland, K. Ashley, A case-based Conf. on AI and Law, 1987. system for trade secrets law. in: Proceedings 1987 ACM International [ 1331 J.S. Rosenschein, Synchronization of multi-agent plans, in: A.H. Bond. L. Gasser (Eds.), Readings in Distributed Artificial Intelligence, Morgan Kaufmann, San Mateo, CA. 1988, pp. 187-191, [ 13-41 J.S. Rosenschein. G. Zlotkin, Rules of Encounter: Designing Conventions for Automated Negotiation Among Computers, MIT Press, Boston, MA, 1994. ( 1351 A. Rubinstein, Perfect equilibrium 11361 R. Schwartz, S. Kraus, Negotiation on data allocation in a bargaining model, Econometrica SO ( 1982) 97-109. in multi-agent environments, in: Proceedings AAAI- 97, Providence, RI, 1997. pp. 29-35. 1 I.171 R. Selten, Reexamination of the perfectness concept for equilibrium points in extensive games, Intemat. J. Game Theory 4 (1975) 25-5.5. ( 13X] Y. Shoham, Agent oriented programing, Artificial Intelligence 60 (1) (1993) 5 l-92. 1391 G. Simari, R. Loui. A mathematical Intelligence 53 (1992) 12.5-157. treatment of defeasible reasoning and its implementation, Artificial l-h)] D. Skalak, E. Rissland. Argument moves in a rule-guided domain. in: Proceedings ACM International Conf. on AI and Law, 19Y 1. 141 1 R.G. Smith, The Contract net: a formalism for the control of distributed problem solving, in: Proceedings 5th International Joint Conference on Artificial Intelligence 1421 R.G. Smith. R. Davis. Negotiation as a metaphor (lY83) 633109. (IJCAI-77). Cambridge, MA, 1977. for distributed problem solving, Artificial Intelligence 20 1431 E. Sonenberg, G. Tidhar, E. Werner, D. Kinny, M. Ljungberg. A. Rao, Planned Intelligence Institute, Australia, 1992. Report 26, Australian Artifcial 1 I441 A. Stollman, Negotiation Methods for Automatic Agents, Master’s Thesis. Bar-Ban University, Ramat- team activity. Technical Gan. Israel, 1997. / 1351 S.R. Sutton, Feat-arousing communications: a critical examination of theory and research, in: J.R. Eiser (Ed.). Social Psychology and Behavioral Medicine, Wiley, New York, 1982. 1 !46l K.P. Sycara. Resolving adversarial conflicts: an approach I’hD Thesis. School of Information and Computer Science, Georgia lYX7. to integrating case-based and analytic methods, Institute of Technology, Atlanta, GA, / 1371 K.P. Sycara, Resolving goal conficts via negotiation in: Proceedings AAAI-88, St. Paul, 1988, pp. 245- 250. [ I48 j K.P. Sycara, Persuasive argumentation / 1491 K.P. Sycara, CM. Lewis, Modeling group decision and negotiation in negotiation, Theory and Decision 28 (1990) 203-242. in concurrent product design, Systems Automation: Research and Applications I (3) (1991). [ ISO] M. Tambe, Towards flexible teamwork, J. Artihcial II511 S.E. Taylor, S.C. Thompsons. Staking Intelligence Res. 7 (1997) 83-124. the elusive vividness effect. Psychological Review 89 (1982) 1_55- 181. [ I.521 B. Thomas, Y. Shoham. A. Schwartz, S. Kraus, Preliminary Internat. J. Intelligent Systems 6 (5) (1991) 497-508. thoughts on an agent description language, S. Kraus et al. /Artijicial Intdigence 104 (1998) 149 65, [ I.531 S.R. Thomas, PLACA, An agent oriented programming PhD Thesis, Computer Science Department, Stanford University, Stanford, CA, 1993. Also available as Technical Report No. STAN-CS- 93-1487. September 1993. language, 11541 G. Tidhar, A. Rao, E. Sonenberg, Guided team selection, in: Proceedings 2nd International Conference on Multi-Agent Systems, 1996, pp. 369-376. [155] J. Tirole, The Theory of Industrial Organization, MIT Press, Cambridge, MA, 1988. [I561 S. Toulmin, R. Rieke, A. Janik. An Introduction [ 1571 M. Vardi. On the complexity of epistemic to Reasoning, Macmillan, 1979. reasoning. in: Proceedings 4th Annual Symposium on Logic in Computer Science. 1989. [ 1581 B. Vermazen, Objects of intention, Philosophical Studies 71 (1993) 223-265. [ 1591 G.A.W. Vreeswijk, Abstract report, in: Proceedings Conference on the Fundamentals of Artificial Intelligence, Angkor, Paris, 1991, pp. 501-5 10. systems: preliminary argumentation 1st World [160] G.A.W. Vreeswijk, Abstract argumentation [I611 emergent behavior Intelligence, Vol. 3, Elsevier, Amsterdam, 1992, pp. 297-310. systems, Artificial Intelligence 90 (l-2) systems, in multi-agent P. Wavish, Exploiting Decentralized Artificial (1997) 225-279. in: E. Werner, Y. Demazeau (Eds.). [ 1621 M. Wellman. J. Doyle, Preferential semantics for goals, in: Proceedings AAAI-91, Anaheim, CA, 199 I. pp. 698-703. [ 1631 K. Werkman, Multiple agent cooperative design evaluation using negotiation, in: Proceedings 2nd International Conference on AI in Design, Pittsburgh, PA, June 1992. [ 1641 E. Werner, Toward a theory of communication and cooperation for multiagent planning, in: Proceedings 2nd Conference on Theoretical Aspects of Reasoning about Knowledge, Pacific Grove, CA, March 1988, pp. 129-143. 11651 G.R. Williams, Style and effectiveness in negotiation, in: L. Hall (Ed.), Negotiation: Strategies for Mutual Gain: the Basic Seminar of the Harvard Program on Negotiation, Sage, Newbury Park, CA, 1993. [ 1661 M. Winslett, Is belief revision harder than you thought? in: Proceedings AAAI-86. Philadelphia, PA. 1986. pp. 421427 11671 M.J. Wooldridge, N.R. Jennings, Agent a survey, Agents, Lecture Notes in Artificial Intelligence, Vol. 890, Springer, Berlin, 1995, pp. l-39. theories, architectures and languages: in: Intelligent [ 1681 D. Zeng, K.P. Sycara, Bayesiam in negotiation, in: Proceedings AAAI Stanford Spring Symposium on Adaptation, Co-evolution in Multi-Agent Systems, 1996. 1169J D. Zeng. K. Sycara, Bayesian in negotiation, Internal. J. Human-Computer Studies 48 (199~) 125-141. L 1701 G. Zlotkin, J S. Rosenschein, Cooperation and conflict resolution via negotiation among autonomous agents IEEE Trans. Systems Man domains, Special Issue on Distributed Artificial Intelligence, in noncooperative Cybemet. 21 (6) (1991) 1317-1324. 11711 G. Zlotkin, J.S. Rosenschein, Mechanism design for automated negotiation, and its application to task oriented domains. Artificial Intelligence 86 (2) (1996) 195-244. [ 1721 G. Zlotkin. J.S. Rosenschein, Mechanisms Intelligence Res. 5 (1996) 163-238. for automated negotiation in state oriented domains, J. Artificial learning and Learning learning 