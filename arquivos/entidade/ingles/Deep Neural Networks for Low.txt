Available online at www.sciencedirect.comAvailable online at www.sciencedirect.comScienceDirectScienceDirectProcedia Computer Science 00 (2020) 000–000Procedia Computer Science 00 (2020) 000–000Procedia Computer Science 176 (2020) 685–694www.elsevier.com/locate/procediawww.elsevier.com/locate/procedia24th International Conference on Knowledge-Based and Intelligent Information & Engineering24th International Conference on Knowledge-Based and Intelligent Information & EngineeringSystems Systems Deep Neural Networks for Low-Cost Eye TrackingDeep Neural Networks for Low-Cost Eye TrackingIldar Rakhmatulina, Andrew T. Duchowskib*Ildar Rakhmatulina, Andrew T. Duchowskib*aSouth Ural State University, Department of Power Plants Networks and Systems, Chelyabinsk, 454080, Russia bClemson University, 100 McAdams Hall, Clemson, SC 29634, USAaSouth Ural State University, Department of Power Plants Networks and Systems, Chelyabinsk, 454080, Russia bClemson University, 100 McAdams Hall, Clemson, SC 29634, USAAbstractAbstractThe paper presents a detailed analysis of modern techniques that can be used to track gaze with a webcam. We present a practicalimplementation of the most popular methods for tracking gaze. Various models of deep neural networks that can be involved inThe paper presents a detailed analysis of modern techniques that can be used to track gaze with a webcam. We present a practicalthe process of online gaze monitoring are reviewed. We introduce a new eye-tracking approach where the effectiveness of using aimplementation of the most popular methods for tracking gaze. Various models of deep neural networks that can be involved indeep learning method is significantly increased. Implementation is in Python where its application is demonstrated by controllingthe process of online gaze monitoring are reviewed. We introduce a new eye-tracking approach where the effectiveness of using ainteraction with the computer. Specifically, a dual coordinate system is given for controlling the computer with the help of adeep learning method is significantly increased. Implementation is in Python where its application is demonstrated by controllinggaze. The first set of coordinates–the position of the face relative to the computer, is implemented by detecting color from theinteraction with the computer. Specifically, a dual coordinate system is given for controlling the computer with the help of ainfrared LED via the OpenCV library. The second set of coordinates–giving gaze position–is obtained via the YOLO (v3)gaze. The first set of coordinates–the position of the face relative to the computer, is implemented by detecting color from thepackage. A method of labeling the eyes is given, in which 3 objects are used to track gaze (to the left, to the right, and in theinfrared LED via the OpenCV library. The second set of coordinates–giving gaze position–is obtained via the YOLO (v3)center).package. A method of labeling the eyes is given, in which 3 objects are used to track gaze (to the left, to the right, and in thecenter).© 2019 The Author(s). Published by Elsevier B.V. © 2020 The Authors. Published by Elsevier B.V.© 2019 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)Peer-review under responsibility of KES International.Peer-review under responsibility of the scientific committee of the KES International.Peer-review under responsibility of KES International.Keywords: eye tracking; deep learning in eye tracking; yolov3 in eye tracking; deep learning in gaze tracking Keywords: eye tracking; deep learning in eye tracking; yolov3 in eye tracking; deep learning in gaze tracking 1. Introduction 1. Introduction Widespread  adoption  of  eye-tracking  technology  has  expanded  its  application.  For  example,  eye-trackingWidespread  adoption  of  eye-tracking  technology  has  expanded  its  application.  For  example,  eye-trackingsystems are now used to detect disorders associated with multiple psychological illnesses [13]. Maurage et al. [18]systems are now used to detect disorders associated with multiple psychological illnesses [13]. Maurage et al. [18]gave a detailed account of using an eye-tracking system to determine alcoholism. Bueno et al. [2] submitted agave a detailed account of using an eye-tracking system to determine alcoholism. Bueno et al. [2] submitted adataset on the methods of eye tracking in neurodegenerative conditions and its potential clinical effect on cognitivedataset on the methods of eye tracking in neurodegenerative conditions and its potential clinical effect on cognitiveassessment. Robertson et al. [23] showed that eye tracking reveals subtle problems in understanding speech inassessment. Robertson et al. [23] showed that eye tracking reveals subtle problems in understanding speech inchildren with dyslexia. Eye tracking is also widely used to improve learning. Sun et al. [24] used an eye-trackingchildren with dyslexia. Eye tracking is also widely used to improve learning. Sun et al. [24] used an eye-trackingdevice to show the effectiveness of teaching a programming course in the C language. An empirical analysis baseddevice to show the effectiveness of teaching a programming course in the C language. An empirical analysis basedon eye tracking and subjective perception of students was described by Molina et al. [19]. Recently, using eyeon eye tracking and subjective perception of students was described by Molina et al. [19]. Recently, using eyetracking has become popular in determining the physical condition of a person. Li et al. [16] used an eye-trackingtracking has become popular in determining the physical condition of a person. Li et al. [16] used an eye-tracking* E-mail address: ildar.o2010@yandex.ru (Ildar Rakhmatulin), duchowski@clemson.edu (Andrew T. Duchowski)* E-mail address: ildar.o2010@yandex.ru (Ildar Rakhmatulin), duchowski@clemson.edu (Andrew T. Duchowski)**1877-0509 © 2019 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)1877-0509 © 2019 The Author(s). Published by Elsevier B.V. Peer-review under responsibility of KES International. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)Peer-review under responsibility of KES International. 1877-0509 © 2020 The Authors. Published by Elsevier B.V.This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)Peer-review under responsibility of the scientific committee of the KES International.10.1016/j.procs.2020.09.04110.1016/j.procs.2020.09.0411877-0509ScienceDirectAvailable online at www.sciencedirect.com686 Ildar Rakhmatulin et al. / Procedia Computer Science 176 (2020) 685–6942system for the identification and classification of mental fatigue of construction equipment operators.Ildar Rakhmatulin, Andrew T. Duchowski/ Procedia Computer Science 00 (2020) 000–000These examples show that despite the variety of areas in which eye-tracking systems are used, often low-costtools with low tracking accuracy are used to implement the technology. This suggests a need for low-cost eye-tracking devices comparable in quality to laboratory equipment, which would allow scientists from various fields toobtain results with high accuracy. It is also important that the source code be open, allowing customization of thesoftware. The main limitation of modern eye-tracking systems is their price. Equipment with an accuracy of about1−2 ◦ costs several thousand dollars. Here, we develop a low-cost eye-tracking system based on deep learningmethods.1.1. Overview of Modern Eye-Tracking MethodsToday, a particularly popular type of eye tracker is of the remote variety. With these trackers, the eyes areilluminated by direct invisible Infra-Red (IR) light, which leads to the appearance of its reflection on the cornea,which the camera tracks. The physiology of this process is described in detail by Hari [5]. In this paper, we use asimilar method, where an infrared LED is used to determine the position of the eye relative to the computer monitor.Huang et al. [6] used a recurrence module to refine the orientation of the eyes using the initial shape of the eyes. Theproposed algorithm can effectively supplement training data. The implementation of heat maps presented in thispaper was used in this study to verify the operation of a developed tracking device. This method does not give anaccurate assessment of the performance of the eye-tracking device and can only be used only for demonstrationpurposes. A new gaze detection model with a combination of superpixel segmentation and eye-tracking data wasproposed by Fen et al. [17]. This study used a high-resolution image of the eyes. Such an image cannot be obtainedfrom a standard webcam. Fen et al. did not describe how to focus the camera on the eyes. Ozcelik et al. [20]explained that the effect of color coding affords more accurate eye movement. In our work we use color-coding forthe  labeling  process  (while  shooting,  the  eye  looks  at  colored  objects).  Kerr  et  al.  [11]  presented  a  real-timecorrection method for eye tracking. This method implements the analysis of the collected calibration data from theuser using the nearest neighbor calibration points to calculate the predicted drift at the current point of focus. Kerr etal.  provide  empirical  data  from  participants  suffering  from  Amblyopia.  Larsson  et  al.  [14]  presented  thedevelopment of a method that reliably detects events in signals recorded using a mobile eye-tracking device. Theproposed method compensates for head movements recorded using an inertial measuring unit. In our research aninfrared LED showed greater accuracy in compensating for head movements. Huang et al. [7] proposed a two-phaseCNN learning strategy for combining head postures and viewing angles. The CNN architecture can reduce refitwhile training eye-tracking models directly with a head pose. In our research, the pose of the head and its effect onthe eyes did not increase the tracking accuracy of the developed eye-tracking system. Jagtap et al. [8] implemented amethod for monitoring driver fatigue to prevent traffic accidents. To determine the facial features of the driver, theViola-Jones Classifier was used. In our work, due to the low tracking efficiency of small objects, the Viola-JonesClassifier was used only to determine the position of the head. Pavlas et al. [21] provided practical guidance on thecreation of low-cost tracking. They used EyeWriter and ITU GazeTracker software in their system. The price of thedevice was about 100 dollars with an accuracy of 2 degrees. Although our device has good characteristics, it doesnot have the same versatility and is not quite as convenient to use. Lee et al. [15] proposed a method for assessingthe position of three-dimensional gaze, based on illumination reflections (Purkinje images) on the surface of thecornea and lens, taking into account the three-dimensional optical structure of a model of the human eye. 3Dmodeling of  the  eye  is a  promising direction,  but  this  requires  high-quality images.  This  approach  presents  apromising direction for eye tracking because they use the 3D plane for analyzing an eye position. To resolvePurkinje images on the eye requires a high-resolution camera. For eye-tracking, we use a web camera, where 3Dmodeling is difficult to implement. Several researchers described the use of slow deep neural networks for imageprocessing, such as Fast R-CNN, Faster R-CNN. For medical purposes, image processing can occur offline, but forreal-time use, as in this work, stream mode is required. Many eye-tracking systems carry certain limitations, such ashead position, lighting in the room. It is worth considering work in which the task was to develop a low-cost eyetracker. Javier et al. [9] presented a low-cost gaze tracking system that is based on a webcam mounted close to theuser’s eye, but accuracy of tracking was low. Frank et al. [4] introduced a system of stroboscopic catadioptric eyetracking (SCET). The new approach for mobile eye tracking, based on cameras with roller shutters and stroboscopicstructured infrared lighting was described. This method has high accuracy but requires a complicated setup and caneasily  go  astray  if  the  computer  tracks  glare.  Swarts  et  al.  [25]  presented  an  ultra-low-cost  eye  gaze  trackerIldar Rakhmatulin et al. / Procedia Computer Science 176 (2020) 685–694 687Ildar Rakhmatulin, Andrew T. Duchowski/ Procedia Computer Science 00 (2020) 000–000 3specifically aimed at studying visual attention in 3D virtual environments. The authors do not provide conclusiveevidence regarding accuracy of the developed device. Davin et al. [3] submitted information on how to build a low-cost eye-tracking system, work particularly interesting from the perspective of image processing. The closest priorart to our work is that of Krafka et al. [12], who presented software GazeCapture. GazeCapture, which contains datafrom  more  than  1,450  people,  consists of  almost  2.5  million  pictures  of  people.  In  our work,  we  use  similartechnology for processing the face: finding the area with the eyes and using the convolutional neural network totrack the eyes in this limited area. The difference in our work is a new labeling technique (using the eye and theinner eye gap for training the neural network) and the use of an infrared LED.2. Image PreprocessingThe first step towards building a model for the tracking task is image preprocessing. Image pre-processingprovides  visualization  necessary  to  identify  and  evaluate  the  usefulness  of  the  procedure.  Fig.  1  shows  theimplementation of the most popular pre-processing  functions of the OpenCV library  on the eye image with aresolution of 416×416 pixels. a             b              c              d              e f             g             h                i               j k                l               m              n               p                            q               r              sFig. 1. Image pre-processing: a – source image, b – edge detection (cv2.Canny), c – threshold operation(cv2.threshold), d – threshold binary (cv2.threshold), e – Gaussian filter (cv2.GaussianBlur, f – morphology filter(cv2.morphologyEx), g – frame difference (cv2.absdiff), h – mask for detect color (cv2.inRange), i,j – for findcolorLower, colorUpper characteristic (cv2.inRange), k – image segmentation, l,m – (cv2.HoughCircles), n – K-Means Clustering for Image Segmentation (cv2.kmeans), p – Image Segmentation with Watershed Algorithm(cv2.watershed), q – circles (cv2.HoughCircle), r – tracker (cv2.TrackerCSRT), s – flow(cv2.calcOpticalFlowFarneback)The methods that process the color of the image are the most informative. Color, however, is not constant,therefore, these methods are not highly efficient.3. Methods for Finding Eyes in the Video Stream The use of Haar cascades and the dlib library directly to monitor the gaze position showed extremely low andunstable results. But at the same time, these libraries can be used as auxiliary methods for finding the eye region.         688 Ildar Rakhmatulin et al. / Procedia Computer Science 176 (2020) 685–6944Fig. 2 shows the Haar cascade and dlib implementations.Ildar Rakhmatulin, Andrew T. Duchowski/ Procedia Computer Science 00 (2020) 000–000 a               b              c               d  Fig.2. Eye position search: a – haar cascade, b,c,d – various dlib library implementationThe  Haar cascade is a machine-learning method for  detecting objects in an  image,  the idea  of which wasproposed by Viola and Jones [26]. A trained Haar cascade, taking an image as input, determines whether it containsthe desired object, i.e., performs the classification task, dividing the input into two classes. The landmark detectionalgorithm proposed by Dlib is an implementation of the Regression Tree Ensemble (ERT), introduced by Kazemiand Sullivan [10]. This method uses a simple and quick function to directly estimate the location of a landmark.These estimated positions are subsequently refined using an iterative process performed by a cascade of regressors.Regressors make a new estimate from the previous one, trying to reduce the error of alignment of the estimatedpoints at each iteration. In our work, we decided to use the dlib library to determine the outline of the eye. At thefirst stage, the dlib.get frontal face detector() function determines the face contour. Next, using the dlib.shapepredictor command ("shape predictor 68 face landmarks.dat") we define facial features. For finding the eye shape,we used the model trained for 68 landmarks. We only use points 36–41 and add 20 millimeters to each point toexpand the range, and then use an OpenCV ROI to limit the area with the eye in the video stream. See Fig. 3, whichshows images from video with the eye limited by a rectangle with resolution 416×416.               a                           b                               c  Fig.3. Detect eye in video stream. a – face detection, b – new video stream with points 36 to 41, c – expand videostream with points 36 to 414. Deep Learning for Gaze Detection and DatasetsAlthough there are a large number of different neural networks, for gaze tracking tasks where speed and highaccuracy are required, only a few neural networks are suitable. Here, we considered YOLOv3, SSD, Mask RCNN,Freeznet. The main problem of deep neural networks is the need to use a large number of images to train thenetwork. Several datasets can be used for non-commercial use to train neural networks:-Columbia Gaze Data Set: This data set consists of 5,880 images of 56 people over varying gaze directions andhead  poses.  For  each  subject,  there  are  5  head  poses  and  21  gaze  directions  per  head  pose(http://www.cs.columbia.edu/CAVE/databases/columbiagaze/); Rajeev et al. [22] used this dataset to train neural networks - Openeds facebook dataset;Aayushy et al. [1] used a semantic segmentation data set collected with 152 participants of 12,759 images withannotations at a resolution of 400×640. Although the challenge participation deadline was September 15, 2019,the dataset is still available upon request (https://research.fb.com/programs/openeds-challenge/);--     Ildar Rakhmatulin et al. / Procedia Computer Science 176 (2020) 685–694 689 5- MPIIGaze dataset: This data set consists of images taken in everyday conditions using the laptop’s built-inIldar Rakhmatulin, Andrew T. Duchowski/ Procedia Computer Science 00 (2020) 000–000--webcams, in which 15 people participate. The MPIIGaze dataset contains 213,659 images [27];Kaggle dataset, competition: Competitions are held periodically, participation in which open access to thedataset (https://www.kaggle.com/c/gl-eye-tracking); Image.net: A massive database of annotated images designed to test and test methods of pattern recognitionand machine vision. Unfortunately, we cannot use the known datasets, because our approach develops low-cost eye-tracking withdeep learning functionality. In the described datasets the images at very high resolution can only be used formedical purposes. In our work, we used a custom dataset of 3000 images from 3 people. From previously describedmodels for eyetracking we selected YOLOv3. This model on well-known datasets (Cifar, imagenet) showed goodperformance and tracking accuracy. In YOLOv3 objects are defined by a rectangular frame. Image marking wascarried out using the YOLO-Annotation-Tool-master program (open source), an example of marking on the imageis shown in Fig. 4.Fig. 4. Labeling with YOLO-Annotation-Tool-master programIn this research, we used the non-standard method of tracking gaze. Three models of the object for the only onegaze at once were used (Fig. 5). The computer accepts the coordinates of the object that has the highest probabilityof correct recognition. This method shows a more vivid effect when working with small data sets.                       a                b               c               d Fig. 5. Labeling image for training model: a - the eye on the left, b - the eye in the middle, c - the eye on the, d - theinner and outer corners of the palpebral fissure.The entire dataset from 3 people: 3000 images, 1000 images; the eye on the left, 1000 images; the eye on theright, 1000 images; the eye in the middle, 3000 images for the inner and outer corners of the palpebral fissure. Theinner and outer corners of the palpebral fissure was used as a point of origin. Ultimately, in the quality of the pointsfor the origin, only the inner palpebral fissure was chosen, since its tracking accuracy is higher, in Fig. 6, point 1.  690 6Ildar Rakhmatulin et al. / Procedia Computer Science 176 (2020) 685–694Ildar Rakhmatulin, Andrew T. Duchowski/ Procedia Computer Science 00 (2020) 000–000Fig. 6. The calculation of gaze position. Inner palpebral fissure, used as the origin point x, y: (0, 0): 1 - innerpalpebral fissure, 2 – iris, 3 – pupilYOLOv3  does  not  predict  the  absolute  coordinates  of  the  center  of  the  bounding  box.  Rather,  it  predictsdisplacements, relative to the upper left corner of the grid cell that predicts the object. If the forecast for the center is(1, 1, 2, 3), then this means that the center is at the point (7, 1, 9, 3) on the 13×13 characteristics map. The boundingbox dimensions are predicted by applying the log output space transform to the output, and then multiplication withreference. The resulting forecasts, bounding box width, and bounding box length are normalized in height and widthof the image. Thus, if the length and width of the forecast for the field containing the eye are (0.1, 0.2), then theactual width and height on the characteristics map is (13×0.1, 13×0.2). An object’s assessment is the probability thatthe  object  is  contained  within  a  bounding  box.  Class  trusts  represent  the  probabilities  of  a  discovered  objectbelonging to a class. Before version 3, YOLO used softmax to evaluate the class, but in the YOLOv3 - softmaxYOLOv3 predicts 3 different scales. The detection layer is used to detect characteristics of three different sizes onthe cards with steps 32, 16, 8, respectively. This means that when entering 416×416, we do detection at 13×13,26×26 and 52×52 scales. The network reduces the sampling frequency of the input image to the first detection layer,where detection is performed using object maps of the layer in increments of 32. Also, the layers are sampled by afactor of 2 and combined with object maps of previous layers that have the same size object map. Now anotherdetection is performed on the layer from step 16. The same upsampling procedure is repeated, and the final detectionis performed on the layer of step 8. At each scale, each cell predicts 3 bounding boxes using 3 anchors, with theresult that the total number of anchors used is 9. In this work, the anchors using k-means were calculated. As initialweights, we used 237 MB weights presented by the developers of the YoloV3 library. In the model, we froze the last3 output layers and trained the last layers with our dataset. The authors of YoloV3 implemented a neural network onthe Darknet framework, but we did it on Keras. In the final, we obtained 256 MB of weight. To test the implementedneural network model, we used a real-time situation. We achieved this result using a relatively small dataset, whichis a good result for eye-tracking tasks.5. Empirical resultsIn this work, the algorithm for eye-tracking was used as shown in Fig.7.StartFace searchSearch for facialfeatures Ildar Rakhmatulin et al. / Procedia Computer Science 176 (2020) 685–694 Ildar Rakhmatulin, Andrew T. Duchowski/ Procedia Computer Science 00 (2020) 000–000691 7Region of interest for eyeGaze detectionInfrared point search in eyeTransfer eye coordinates into computer coordinatesFig. 7. The algorithm for eye-trackingWe developed the eye-tracking algorithm with two coordinate systems. The first coordinate system, due toOpenCV and the inrange function, determines the position of the eye relative to the infrared LED which is in frontof the monitor. This system coordinates to control the position of a head relative to the monitor. The secondcoordinate system calculates the position of the eye due to YOLOv3 relative to the inner and outer corners of thepalpebral fissure. The second coordinate system of the coordinates translates its coordinates into the first, therebycontrolling gaze even when the head changes its position. Fig. 8 shows schematically the process of transferringgaze coordinates at the computer.Fig. 8. Schematic of the process of transferring gaze coordinates at a computerFig. 9 shows the experimental device for eye tracking. Using a servomotor to control the position of the cameraallows  us  to  significantly  increase  the  area  and  autofocus  of  the  camera  work  range  from  600×800  mm  to1000×1200 mm. The servo motor is controlled by a Raspberry PI3, which receives the coordinates of the eye usingthe TCP-IP protocol from the computer. The focus of the camera is controlled by a simple neural network forclassification. 692 8Ildar Rakhmatulin et al. / Procedia Computer Science 176 (2020) 685–694Ildar Rakhmatulin, Andrew T. Duchowski/ Procedia Computer Science 00 (2020) 000–000Fig. 9. Experimental setup: 1 - camera with a resolution of 1080, 5Mp (2560×1920), 2 - servomotorWe have 4 stages of classification quality (see Fig. 10).                   1                2                3                4Fig. 10. Examples of images used for a neural network to classify focus - from 1: good - to 4: bad focusRaspberry PI3 receives classification result from the computer by TCP/IP every 0.5 sec and, depending on theresult, controls  the  focus position. For  the  classification of  quality of  the  image focus,  a convolutional neuralnetwork (CNN) was created. The CNN consists of 8 layers of Convolution2D and MaxPooling2D layers after thesecond and fourth convolution. On all layers, the ReLU activation function is used. To regularize our model, aftereach subsample layer and the first fully connected layer, the Dropout layer was used. The neural network has fouroutputs that characterize the quality of focusing. In our approach, the YOLOv3 network is subsequently used, whichon our dataset of 1,500 images showed a great effect in the process of tracking one object in comparison with othermodels. The device was tested for the condition when the Haar cascades and the dlib library determined the positionof the face and eyes with 100% accuracies. The camera focus has been adjusted visually. Correspondingly, the entireerror that was present during the tracking process is the error of the implemented neural network for tracking theeyes. Calibration of the device was carried out on 20 bright color squares, an area of 7.8 sm 2, located on the monitorscreen in the form of a desktop screen saver. The subject concentrated attention for 3 seconds alternately on brightcolored  squares  on  the  computer  screen  (see  Fig.  11),  the  coordinates  of  each  square  were  recorded  in  thecomputer’s memoryFig. 11. Computer desktop screen saver to calibrate and verify the eye-tracking device.Verification of the developed tracking device was carried out in comparing the coordinates obtained during the                Ildar Rakhmatulin et al. / Procedia Computer Science 176 (2020) 685–694 693Ildar Rakhmatulin, Andrew T. Duchowski/ Procedia Computer Science 00 (2020) 000–000 9tracking process with the coordinates that were obtained during the calibration process. The monitor size is 22inches with a screen resolution of 1366×768 pixels. The subject was located 500 mm from the monitor in a naturalposition for work. The error of concentration of the subject’s gaze on the monitor by one square gives an error of 2degrees. The results are presented in Table 1. For each point, the average error for 20 measurements is given. Thetotal error is 2.25 degrees. Frequency of the developed device - 30 Hz.Table 1. Error in degrees for various locations on a computer monitorPosition123412.053.110.850.95Error in degrees22.61.683.32.8533.22.22.82.242.951.92.13.1551.952.351.751.05The training of the model took place in different lighting conditions, with and without light. Moreover, if a newuser will work under different lighting conditions from the conditions presented here, it is only necessary to includeimages from this state in the dataset. In this work, we used a method where, due to a combination of variousmachine vision mechanisms and a nonstandard approach to marking, we managed to develop a budget device in theprice category of no more than 30 dollars, table 2.Table 2. Error in degrees for various locations on a computer monitorNameYOLOv3YOLOv3 characteristicFrame size, mmAccuracy in degreesFrecuency,hzWeight, gCost, dollars1000×120023030025Different lighting, reflections on the eye, camera resolution, and camera position can affect this accuracy of thedeveloped device. To avoid these disadvantages, in the next research, we want to prepare new labeling images withdifferent eye images (with a lot of subjects). As a result, the neural network will be trained to find the eye not somuch in the color characteristics of the pupil, but in the color contrast between pupil and sclera. External factorshave a passive effect on this feature6. Conclusion and Discussion Despite the more than 100-year history of studying the movement behind gaze, one of the main drawbacks ofresearch in this area is the lack of any standards in the development of these devices. Part of the study considers thecriterion of the accuracy of loss in degrees as a success criterion, where the accuracy of 1-2 degrees is considerednearly optimal. Other works describe calibration strategies, the invariable posture of the head and methods forassessing  gaze.  In  future  research,  it  is  necessary  to  focus  on  standardization  of  research  in  the  field  of  eyemovement, which will allow more competent comparison of various research and give a more complete picture ofthe situation in the field of development of these technologies and thereby more intelligently present to researcherstheir developments. Moreover, despite the explosive growth of interest in the use of neural networks in imageprocessing, in the described studies, as a rule, the authors use a standard convolutional network for deep learning oruse the OpenCV library tools. The disadvantage of this tracking method is the need for labeling. At the same time,the YOLOv3 network is constructed in such a way that labeling is made only by a rectangle. For comparison, weobtained a higher tracking accuracy when using Mask-RCNN. The main disadvantage of the Mask-RCNN is thelack of speed, as a result of which it cannot work on the streaming video. The further task is to train the neuralnetwork from scratch without external weights trained on various images. To train the network need uses only forthe eyes image in the extension 416 to 416 and leave only the last layers without freezing. This will allow a user to 694 Ildar Rakhmatulin et al. / Procedia Computer Science 176 (2020) 685–69410ultimately use a small number of images for completely train the network.Ildar Rakhmatulin, Andrew T. Duchowski/ Procedia Computer Science 00 (2020) 000–000References[1] Aayushy, C., Rakshit, K., 2019. Ritnet: Real-time semantic segmentation of the eye for gaze tracking, 1–7[2]  Bueno,  A.,  Sato,  J.,  Hornberger,  M.,  2018.  Eye  tracking  –  the  overlooked  method  to  measure  cognition  in  neurodegeneration?Neuropsychologia, 133, 107–109. doi:10.1016/j.imavis.2018.05.004[3]  Davin,  P.,  Heather,  L.,  2013.  How  to  build  a  low-cost  eye-tracking  system.  Ergonomics  in  Design  The  Quarterly  of  Human  FactorsApplications, 20, 1300–1305. doi:10.1177/1064804611428928[4] Frank, H., Carlos, H., Kim, J., Whang, M., 2019. Towards a low cost and high speed mobile eye tracker. Proceedings of the 11th ACMSymposium on Eye Tracking Research Application, 16, 1–9. doi:10.1145/3314111.3319841[5] Hari, S., 2012. Human eye tracking and related issues: A review. International Journal of Scientific and Research Publications, 2, 1–9[6] Huang, B., Chen, R., Zhou, Q., 2020. Applying eye tracking in information security. Pattern Recognition, 98, 145–157. doi:10.1016/j.patcog.2019.107076. [7] Huang, H., Xu, Y., Hua, X., Yan, W., 2019. A crowdsourced system for robust eye tracking. Journal of Visual Communication and ImageRepresentation, 60, 28–32. doi:10.1016/j.jvcir.2019.01.007[8] Jagtap, N., Kolap, A., Adgokar, M., 2015. Real time driver’s eye tracking design proposal for detection of fatigue drowsine. InternationalJournal of Innovative Research in Computerand Communication Engineering, 3, 3758–3758. doi:10.15680/ijircce.2015.0305003 [9] Javier, S., Skovsgaard, H., 2010. Evaluation of a low-cost open-source gaze tracker, 22–24doi:10.1145/1743666.1743685[10] Kazemi, V., Sullivan, J., 2014. One Millisecond Face Alignment with an Ensemble of Regression Trees, in: Proceedings of the 2014 IEEEComputer Society Conference on Computer Vision and Pattern Recognition, 1867–1874[11] Kerr, R., Marwan, M., Fuad, M., 2019. A real-time lazy eye correction method for low cost webcams. Procedia Computer Science, 159,281–290. doi:10.1016/j.procs.2019.09.183[12] Krafka, K., Khosla, A., Bhandarkar, S., 2016. Eye tracking for everyone. Journal of Neuroscience Methods, 274, 13–26. [13] Larrazabal, A., Cena, G., Mart´ınez, C., 2019. Video-oculography eye tracking towards clinical applications: A review. computers in biology´ and medicine. Computers in Biology and Medicine, 108, 57–66. doi:http://dx.doi.org/10.1016/j.patcog.2014.01.005. [14] Larsson, L., Schwaller, A., Nystrom, M., Stridh, M., 2016. Head movement compensation and multi-modal event detection in eye-tracking ¨data for unconstrained head movements. Journal of Neuroscience Methods, 274, 13–26. doi:10.1016/j.jneumeth.2016.09.005. [15] Lee, J., Cho, C., Shin, K., Lee, E., 2012. 3d gaze tracking method using purkinje images on eye optical model and pupil. Optics and Lasersin Engineering, 50, 736–751. doi:0.1016/j.optlaseng.2011.12.001. [16] Li, J., Li, H., Ume, W., Wang, H., 2020. Identification and classification of construction equipment operators’ mental fatigue using wearableeye-tracking technology. Automation in Construction, 109, 103–113. doi:10.1016/j.autcon.2019.103000. [17] and Liangchan, P., Lei, F., Xieping, G., 2018. Salient object detection based on eye tracking data. Signal Processing, 144, 392–397. doi:10.1016/j.sigpro.2017.10.019. [18] Maurage, P., Masson, N., Bollen, Z., Hondt, F., 2020. Eye tracking correlates of acute alcohol consumption: A systematic and criticalreview. ´ Neuroscience Biobehavioral Reviews, 108, 400–422. doi:https://doi.org/10.1016/j.neubiorev.2019.10.001. [19] Molina, A., Redondo, M., Lacave, C., Ortega, M., 2014. Assessing the effectiveness of new devices for accessing learning materials: Anempirical analysis based on eye tracking and learner subjective perception. Computers in Human Behavior, 31, 475–490. [20] Ozcelik, E., Karakus, T., Kursun, E., Cagiltay, K., 2009. An eye-tracking study of how color coding affects multimedia learning. ComputersEducation, 53, 445–453. doi:10.1016/j.compedu.2009.03.002. [21] Pavlas, D., Lum, H., Salas, E., 2012. Low to build a low-cost eye-tracking system. ergonomics in design the quarterly of human factorsapplications. Ergonomics in design, 3, 18–21. doi:10.1177/1064804611428928. [22] Rajeev, R., Shalini, D., 2018. Light-weight head pose invariant gaze tracking, 1–9. [23] Robertson, E., Gallant, J., 2019. Eye tracking reveals subtle spoken sentence comprehension problems in children with dyslexia. AppliedPsycholinguistics, 228, 102–105. doi:10.1017/S0142716409990208. [24] Sun, J., Hsu, K., 2019. A smart eye-tracking feedback scaffolding approach to improving students’ learning self-efficacy and performance ina c programming course. Computers in Human Behavior, 95, 66–72. [25] Swarts, M., Noh, J., 2013. Ultra low cost eye gaze tracking for virtual environments. VAMR 2013. Lecture Notes in Computer Science 21,1300–1305. doi:10.1007/978-3-642-39405-8_12. [26] Viola, P., Jones, M., 2001. Rapid Object Detection using a Boosted Cascade of Simple Features, in: Proceedings of the 2001 IEEE ComputerSociety Conference on Computer Vision and Pattern Recognition, pp. I–I. doi:10.1109/CVPR.2001.990517. [27] Xucong, Z., Yusuke, S., Mario, F., 2015. Appearance-based gaze estimation in the wild. Computer Vision and Pattern Recognition, 23, 456–459. doi:10.1016/j.procs.2016.07.013.