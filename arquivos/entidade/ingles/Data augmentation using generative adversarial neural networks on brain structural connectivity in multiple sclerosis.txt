Data Augmentation using Generative Adversarial Neural Networks onBrain Structural Connectivity in Multiple SclerosisBerardino Barilea, Aldo Marzullob, Claudio Stamilec, Fran√ßoise Durand-Dubiefa,d andDominique Sappey-Mariniera,eaCREATIS (UMR 5220 CNRS & U1206 INSERM), Universit√© Claude Bernard Lyon 1, Universit√© de Lyon, Villeurbanne, FrancebDepartment of Mathematics and Computer Science, University of Calabria, Rende, ItalycR&D Department CGnal, Milan, ItalydH√¥pital Neurologique, Service de Neurologie A, H√¥pital Civils de Lyon, Bron, FranceeCERMEP - Imagerie du Vivant, Universit√© de Lyon, Bron, FranceA R T I C L E I N F OA B S T R A C TKeywords:Brain ConnectivityMultiple SclerosisData AugmentationGenerative Adversarial NetworksBackground and objective: Machine learning frameworks have demonstrated their potentials indealing with complex data structures, achieving remarkable results in many areas, including brainimaging. However, a large collection of data is needed to train these models. This is particularlychallenging in the biomedical domain since, due to acquisition accessibility, costs and pathology re-lated variability, available datasets are limited and usually imbalanced. To overcome this challenge,generative models can be used to generate new data.Methods: In this study, a framework based on generative adversarial network is proposed to createsynthetic structural brain networks in Multiple Sclerosis (MS). The dataset consists of 29 relapsing-remitting and 19 secondary-progressive MS patients. T1 and diffusion tensor imaging (DTI) acqui-sitions were used to obtain the structural brain network for each subject. Evaluation of the qualityof newly generated brain networks is performed by (i) analysing their structural properties and (ii)studying their impact on classification performance.Results: We demonstrate that advanced generative models could be directly applied to the structuralbrain networks. We quantitatively and qualitatively show that newly generated data do not presentsignificant differences compared to the real ones. In addition, augmenting the existing dataset withgenerated samples leads to an improvement of the classification performance (ùêπ 1ùë†ùëêùëúùëüùëí 81%) with re-spect to the baseline approach (ùêπ 1ùë†ùëêùëúùëüùëí 66%).Conclusions: Our approach defines a new tool for biomedical application when connectome-baseddata augmentation is needed, providing a valid alternative to usual image-based data augmentationtechniques.1. IntroductionArtificial intelligence has revolutionized many areas ofresearch, from economics and law to health-care. However,a large collection of data is essential for statistical evalua-tion and machine learning applications, particularly in thefield of deep learning (DL). Indeed, DL frameworks haveachieved remarkable results in many fields, such as patternrecognition, natural language processing, image processing,among others [1, 2]. The main advantage of using DL ap-plications lies in their great ability to recognize hidden pat-terns in the data, thanks to the multiple nonlinear transfor-mations produced by the sequential stacks of multiple lay-ers [3, 4] However, huge amount of data are required fortraining this kind of models while in the context of biomed-ical domain, and particularly in medical imaging, extensivedatasets are challenging to obtain due to systems availabil-ity, costs constraints, acquisition methodology, and pathol-ogy related variability [5, 6], resulting in small and imbal-anced dataset. Notwithstanding, when dealing with imageberardino.barile@creatis.insa-lyon.fr (B.Barile); marzullo@mat.unical.it (A. Marzullo);cstamile@cgnal.com (C. Stamile);francoise.durand-dubief@chu-lyon.fr (F. Durand-Dubief);sappey-marinier@univ-lyon1.fr (D. Sappey-Marinier)ORCID(s):data, different solutions have been proposed to overcomethese limitations [7]. A general and widely accepted solu-tion is to impose meaningless perturbations to the originaldata [8] or to apply more advanced techniques, like rota-tion, reflection, scaling among others. These approachesoffer straightforward alternatives for augmenting the train-ing set, allowing DL models to reach better performanceand/or more stable training [9]. Recently, with the rise ofDL, interesting alternatives have appeared and new gener-ative DL-based models were proposed to obtain syntheticdata with characteristics spanning the original data manifold[10]. Therefore, in this study we refer to generative modelsas a subclass of DL frameworks able to generate complexdata data structure, including the recent modeling approachused to characterize brain networks by means of graph the-ory [11, 12, 13]. Given the great capability of graphs to rep-resent complex relations among different areas of the brain,such relational data structure started to be widely employedin many contexts, including social behavioral studies. Addi-tionally, advances in brain image acquisition and computerassisted methods have begun to provide meaningful resultsin support of clinicians, leading to a steadily growing use inthe neuroscience community, particularly in brain imaging[14]. Using magnetic resonance imaging (MRI), functionalor structural brain connectivity can be obtained by analyz-Berardino Barile: Preprint submitted to ElsevierPage 1 of 12Data Augmentation using Generative Adversarial Neural Networksing temporal correlations of gray matter (GM) activity withresting-state functional MRI (fMRI) or reconstructing whitematter (WM) fiber-bundles with diffusion tensor imaging(DTI), respectively. Such network-like structure of the hu-man connectome consists of nodes, defined by parcellisationof the brain grey matter (GM), and edges, correspondingto functional or structural links between the network nodes.These new approaches paved the way for a better character-ization of brain networks, particularly in brain diseases suchas Multiple Sclerosis (MS).MS is a demyelinating, inflammatory, chronic disease ofthe central nervous system [15]. While its etiology remainsunknown, MS is the most frequent disabling neurologicaldisease in young adults. Disease onset is characterized inabout 85% of cases [15], by a first acute episode calledclinically isolated syndrome (CIS) or a relapsing-remittingcourse (RRMS) followed by a secondary-progressive course(SPMS), while the remaining 15% of MS patients evolve di-rectly into a primary-progressive course (PPMS). The courseof the disease and the risk for developing permanent dis-ability are very different from one patient to another. Thus,the neurologist‚Äôs challenge is to predict the disease evolu-tion based on early clinical, biological and imaging mark-ers available from disease onset. However, the complexitybrought by conectome data is more cumbersome with re-spect to the grid-like pixel-by-pixel representation found inimages.In fact, due to the multiple interconnections be-tween different nodes, connectome data represent a chal-lenge for synthetic data generation for which simple opera-tions, like edge swapping, would end up changing the entirestructure of the graph network, jeopardizing the informationthey convey. [16].In this study, a generative adversarial network framework isproposed, namely Generative Adversarial Neural NetworkAutoEncoder (AAE). The framework is able to automati-cally generate synthetic structural brain connectivity data ofMS patients. To achieve this, a prior is imposed to the latentspace of the autoencoder network by means of an adversar-ial model. Moreover, a consistency loss is also introduced inorder to increase the stability of the training process. Newsamples of brain connectivity data are generated by drawingfrom the parametrized latent space. An overfitting analy-sis over generated graphs, by exploiting graph properties,is proposed for model evaluation. The synthetic generateddata can be used to augment the MS brain networks datasetto improve classification performances of classical machinelearning methods like the Random Forest Classifier.The paper is structured as follows. In Section 2, we illustratethe related literature, and in section 3, we provide a detaileddescription of our methodological approach. In Section 4,we describe our experimental results and finally, in Section5, we draw our conclusions.2. Related WorkDue to their ability to generate new data, generative mod-els have gained a lot of interest in the computer vision andmedical imaging research communities. The Generative Ad-versarial Network framework (GAN) has been previouslyused for generating realistic training images that synthet-ically augment datasets. Radford et al. [17] introduced aclass of generative model called deep convolutional gener-ative adversarial networks (DCGAN) to generate 2D brainMR images followed by an AE neural network for imagedenoising. Makhzani et al. [18] proposed a new methodfor regularizing AutoEncoders (AE) by imposing an arbi-trary prior on the latent representation. Calimeri et al. [19]proposed a GAN for the automatic generation of artificialMR images of the human brain. They demonstrated thatthe power of adversarial training could be exploited for thegeneration of brain networks data, which are more complexthan usual images.GAN frameworks have also shown to improve accuracy ofimage classification via generation of new synthetic trainingimages. Frid-Adar et al. [20], for instance, used syntheticmedical image augmentation with GAN for the classifica-tion of liver lesions. Similarly, Salehinejad et al. [21] usedthis framework to simulate pathology across five classes ofchest X-rays in order to augment the original imbalanceddataset and improve the performance of a convolutional modelin chest pathology classification. In the context of MS, Shui-Hua W. et al. [22] proposed a new transfer-learning-basedapproach to identify MS patients with higher accuracy, com-paring three different types of neural networks (DenseNet-121, DenseNet-169, and DenseNet-201), which make useof composite learning factors to different layers. Yu-DongZ. et al. [23] exploited the AlexNet model to classify MSpatients and studied the best transfer-learning settings (i.e.number of layers transferred and replaced) obtaining highlevel of performance.Interestingly, other applications of adversarial variationaltraining frameworks have been reported. For example, Zhanget al. [24] proposed a semi-supervised learning Adversar-ial Variational Embedding for leveraging both the power ofGAN as a high quality generative model and Variational Au-toEncoder (VAE) as a posterior distribution learner. Theydemonstrated that the combination of VAE and GAN pro-vided significant improvements of semisupervised classifi-cation. Imran et al. [25] used a network architecture thatincorporates an ensemble of discriminators in a VAE-GANnetwork using datasets from the computer vision and med-ical imaging domains in order to generate new realistic im-ages of medical data. They showed that the combinationof this two generative models can lead to superior perfor-mances against state-of-the-art semi-supervised models bothin image generation and classification tasks. However, thegeneration process become more cumbersome in the caseof highly structured graph data.In order to address thischallenge, many approaches have been reported. Chawlaet al.[26] proposed a GraphVAE method for generatingsmall graphs using a Variational approach. This model iscomposed by a simple linearized decoder output, which pro-duces a probabilistic fully-connected graph. Pan et al. [27]proposed a new architecture for which an adversarial train-Berardino Barile: Preprint submitted to ElsevierPage 2 of 12Data Augmentation using Generative Adversarial Neural Networksing is combined with a graph autoencoder structure (ARAE).The framework encodes the topological structure and nodecontent in a graph to a compact representation, on which adecoder is trained to reconstruct the graph structure.Freund et al. [28] proposed an approach based on adver-sarial regularization of the latent space for generating graphstructured data. They could demonstrate the ability of themodel to embed graph-based data coherently, and at thesame time, generate meaningful samples. Thus, Graph AEand VAE constitute today the best approach for embeddingnodes and learn a low dimensional vector representation withapplications to link prediction, node clustering and matrixcompletion. However, much less attention has been spent ongenerating the entire structure of graphs. Khoshgoftaar et al.[29] proposed a simple graph AE structure which does notuse the graph convolutional network. They demonstratedthat a straightforward linear models with adjacency matri-ces as inputs performed equally well in benchmark datasetslike Cora, Citeseer and Pubmed citation networks.3. Materials and Methods3.1. Dataset Description and PreprocessingForty-eight MS patients distributed across the two mostfrequent clinical courses, namely the RRMS course, whichis followed, between 10 to 20 years later, by the SPMScourse [1]. If these two clinical forms are distinguished bythe status of the patient, mainly expressed by its ExpandedDisability Status Scale (EDSS), they can also be differen-tiated by their biological and imaging markers reflectingtwo underlying pathological processes, such as inflamma-tion, and neurodegeneration. Each patient underwent multi-ple brain MRI examinations over different periods, rangingfrom 2.5 to 6 years. The minimum number of scans perpatient is 3 while the maximum is 10. The gap betweentwo consecutive scans is either 6 or 12 months. The to-tal number of MRI scans in the dataset is 270. This studywas approved by the local ethics committee (CPP Sud-EstIV) and the French national agency for medicine and healthproducts safety (ANSM). Written informed consent was ob-tained from all patients and the control subject prior to studyinitiation.For each subject, the brain structural connectivity graphsare generated by combining brain GM parcellation extractedfrom T1-weighted MRI and the white matter (WM) fiber-tracking obtained from DTI acquisition. An undirected graphùê∫ = (ùëâ , ùê∏) representing the WM fiber-tracks of the brainis created, where ùëâ defines the set of nodes (GM regions)and ùê∏ represents the set of connections (WM fiber-tracks)between these nodes. Each graph ùê∫ is represented by anadjacency matrix. More in detail, the data processing in-cludes an atlas parcellation of cortical and sub-cortical GMregions, performed after segmentation of the T1-weightedMRI in four classes [WM, cortical GM, sub-cortical GM,cerebro-spinal fluid (CSF)], as described in Kocevar et al.[30]. Meanwhile, a pre-processing of the diffusion images isperformed by applying the Eddy-current distortions correc-Table 1Population description by clinical profilesPatients (M\F)RRMS29 (20\80)SPMS19 (61\39)Age at first scan (years)35.1 (7.4)42.3 (4.4)Disease duration (years)6.75 (4.81)13.12 (5.84)EDSS median (range)2.0 (0-4.5)5.0 (3-7)Total number of scans18288tion [31] and a probabilistic streamline tractography algo-rithm is applied to generate WM fiber-tracks that combinedwith the T1-image parcellation leads to a symmetrical con-nectivity matrix ùê¥ ‚àà ‚Ñïùëû√óùëû+ where ùëû = 84 for each subject.Finally, due to the symmetry of the matrix, only the uppertriangular part was considered in order to reduce the dimen-sionality of the problem. This implies that a single matrixcan be represented as a vector ùë• ‚àà ‚Ñù(1,ùëë) where ùëë = 3486excluding the diagonal which is imposed to zero values.3.2. MRI Data AcquisitionMS patients underwent a MR examination on a 1.5TSiemens Sonata system (Siemens Medical Solution, Erlan-gen, Germany) using an 8-channel head-coil. The MR pro-tocol consisted in the acquisition of a sagittal 3D-T1 se-quence (1 √ó 1 √ó 1 ùëöùëö3, ùëá ùê∏‚àïùëá ùëÖ = 4‚àï2000 ùëöùë†) and an axial2D-spin-echo DTI sequence (ùëá ùê∏‚àïùëá ùëÖ = 86‚àï6900 ùëöùë†; 2√ó24directions of gradient diffusion; ùëè = 1000 ùë†.ùëöùëö‚àí2, spatialresolution of 2.5 √ó 2.5 √ó 2.5 ùëöùëö3) oriented in the AC-PCplane.3.3. Generative Adversarial Neural NetworkGAN is a generative model approach based on differ-entiable neural networks where two actors are involved: aGenerator [ùê∫ùëûùúÉ(ùëß|ùë•)(ùë•)] and a Discriminator [ùê∑(ùë£)] [8]. Theformer is a neural network mapping the input x to the out-put z by training a network with structure q and parametersùúÉ. In most applications, brand new data are generated bydefining a prior on input noise variables. The latter is a net-work which takes as input ùë£ and outputs the probability thatthe input is coming from the true data distribution insteadof being synthetically generated [19]. Formally, the adver-sarial game can be defined as a min-max problem followingEq. (1).minùê∫maxùê∑= ùîºùë£‚àºùëù(ùë£)[ùëôùëúùëîùê∑(ùë£)]+ùîºùëß‚àºùëù(ùëß)[1 ‚àí ùëôùëúùëîùê∑(ùê∫ùëûùúÉ(ùëß|ùë•)(ùë•))](1)Here, the first term represents the discriminator network‚Äôsprobability that true instances ùë£ from distribution ùëù(ùë£) arerightly classified. The second term in the summation, identi-fies the generator network‚Äôs ability to fool the discriminatorby producing data with probability distribution ùëù(ùëß) indis-tinguishable from that of the true data.Berardino Barile: Preprint submitted to ElsevierPage 3 of 12Data Augmentation using Generative Adversarial Neural NetworksFigure 1: Schematic representation of the proposed AAE model. Starting from the brain connectome data representation(adjacency matrix), conditional probability distributions were calculated, from which new batches of connectome data weresampled. From the vectorized representation of the sampled adjacency matrix, the encoder network compresses the input intoa latent lower dimensional representation, while the decoder reconstructs the input from its compressed latent representation.The combination of the two networks defines the autoencoder generator of the adversarial framework. Conversely, thediscriminator network takes as input the latent representation and a random noise vector and tries to discriminate betweenthe two, effectively imposing a constraint on the latent distribution of the autoencoder. Finally, from the latent space, anadditional classifier discriminates between RRMS and SPMS patients.3.4. Generative Adversarial Neural NetworkAutoencoderIn this study, the adversarial training is used to trainthe proposed AAE model at generating synthetic structuralbrain networks. Fig. 1, illustrates the adversarial process.The structure of the AAE model is defined by two adver-sarial neural networks: the generator and the discriminator.The former is an autoencoder composed by 13 layers forwhich fully connected and batch normalization layers alter-nate between one another except for the output layer. Theinput layer of the encoder is the number of upper triangu-lar nodes in the graph (ùëë = 3486). Subsequent fully con-nected layers have a number of neurons of 512, 256, 128,100. This last encoder layer (ùëûùúÉ(ùëß|ùë•)) maps the input vectorùë• ‚àà ‚Ñù(1,ùëë) to a lower dimensional space ùëß ‚àà ‚Ñù(1,ùëê) withùëê = 100. The decoder ùëùùúô(ùë•|ùëß) is defined as a mirror repre-sentation of the encoder with the aim of reconstructing theoriginal input. Furthermore, the encoder is provided withan additional branch, a fully connected layer with a singleneuron, used as regularization with respect to the clinicalform.On the other hand, a second neural network is intro-duced, which takes two inputs. The first is a random stan-dard gaussian vector ùë£ ‚àà ‚Ñù(1,ùëê) with ùëê = 100 where:ùë£ ‚àº Óà∫ (ùúá, ùúé2) =ùëí‚àí ùëß221‚àö2ùúã(2)with ùúá = 0 and ùúé = 1. The second is ùëß ‚àà ‚Ñù(1,ùëê) obtained asthe output of the encoder ùëûùúÉ(ùëß|ùë•). The second model (dis-criminator) produces a probability score, which defines thelikelihood that the two input vectors are coming from thesame underlying data distribution. Its architecture is com-posed by 6 layers in which fully connected and dropout lay-ers alternates between one another. The LeakyReLU activa-tion function with an alpha parameter of 0.2 is used for all ofthe middle layers in both the generator and the discriminatorwhile for the output layers a sigmoid activation function isemployed. Only for the generator, batch normalisation withmomentum 0.8 is added after each feedforward layer exceptfor the output layer. For the discriminator, dropout with pa-rameter 0.2 is used between each layer excluding the outputlayer. Finally, the encoder model ùëûùúÉ(ùëß|ùë•) is connected to asecond mirrored model ùëùùúô(ùë•|ùëß), the decoder, whose objec-tive is to learn the inverse mapping function of the encoder.In order to maximize the reconstruction quality of the inputdata, an additional penalty is imposed to final loss function(3).ùëÄùëÜùê∏ =1ùëòùëõùëõ‚àëùëò‚àëùëñ=1ùëó=1( ùë•ùëñùëó ‚àí ùëùùúô(ùë•ùëñùëó|ùëßùëñùëó)ùúéùëñùëó)2(3)where ùëò defines the total number of possible connectionsin the upper triangular of the connectivity matrix while ùëõdefines the size of the batch used for training the network.In other words, we penalized the loss function each time thereconstructed matrix is far from the original data matrix interms of mean squared error (MSE). This constraint ensuresthat while the hidden space ùëß is forced to follow a standardnormal distribution, the output of the model will produceresults that span the entire input space. Moreover, in orderto improve the classification performance we imposed anadditional form of regularization which bind the latent spaceto be ‚Äúcoherent‚Äù [32] between the encoder and the decoderBerardino Barile: Preprint submitted to ElsevierPage 4 of 12Data Augmentation using Generative Adversarial Neural Networksnetwork. Mathematically, this translates in Eq. (4):3.5. AAE Adversarial Training PipelineùëÄùëÜùê∏ùê∂ùëú‚Ñéùëíùëüùëíùëõùëêùëí =1ùëêùëõùëõ‚àëùëê‚àëùëñ=1ùëó=1( ùëûùúÉ(ùëß|ùë•) ‚àí Œ®ùúÉùëñùëóùêºùëÑùëÖ)2(4)with ùêºùëÑùëÖ = ùëÑ3 ‚àí ùëÑ1 where ùëÑ1 and ùëÑ3 are respectivelythe first and the third quartile of the distribution given byùëûùúÉ(ùëß|ùë•) ‚àí Œ®ùúÉ where Œ®ùúÉùëñùëó= ùëûùúÉ(ùëßùëñùëó|ùëùùúô(ùë•ùëñùëó|ùëûùúÉ(ùëß|ùë•ùëñùëó)).As long as the adversarial loss is concerned, let ùê∑(ùë£)be the discriminator network, where ùë£ ‚àº Óà∫ (ùúá = 0, ùúé2 =1) = ùëù(ùë£), is a standard normal distribution. The relatedloss function will thus be defined as ùîºùë£‚àºùëù(ùë£)[ùëôùëúùëîùê∑(ùë£)] forpositive cases and ùîºùëß‚àºùëù(ùëß)[1‚àíùëôùëúùëîùê∑(ùê∫ùëûùúÉ(ùëß|ùë•)(ùë•))] for negativecases. In this last case, the generation of the latent spaceùëß is defined as ùê∫ùëûùúÉ(ùëß|ùë•)(ùë•) with ùëß ‚àº ùëûùúÉ(ùëß|ùë•) = ùëù(ùëß). Wewould like that ùëù(ùëß) ‚âà ùëù(ùë£), which implies that the latentspace is distributed as a standard gaussian. On the contrary,we define with ùê∫(ùë•) the final generator (composed of anencoder and a decoder) and its respective loss function asùîºùëß‚àºùëûùúÉ(ùëß|ùë•)[ùëôùëúùëî ùëùùúô(ùë•|ùëß)]. Henceforth, the adversarial loss willbe defined as shown in Eq. (5).ùêø(ùúÉ, ùê∑(ùë£), ùê∫ùëûùúÉ(ùëß|ùë•)(ùë•)) = ùîºùëß‚àºùëûùúÉ(ùëß|ùë•)[ùëôùëúùëî ùëùùúô(ùë•|ùëß)]+ùîºùë£‚àºùëù(ùë£)[ùëôùëúùëîùê∑(ùë£)] + ùîºùëß‚àºùëù(ùëß)[1 ‚àí ùëôùëúùëîùê∑(ùê∫ùëûùúÉ(ùëß|ùë•)(ùë•))](5)Roughly speaking, the Kullback-Leibler, usually employedin a VAE framework [33], is now substituted with the ad-versarial loss. This model allows us to provide probabilisticdescriptions of observations in latent space, which translatesin the ability of the model to store latent attributes as proba-bility distributions. In order to take into account the clinicalform for each graph, an additional constraint has been im-posed and defined as follows:ùêøùê∂ùëüùëúùë†ùë†ùê∏ùëõùë°ùëüùëúùëùùë¶ = ‚àíùê∂‚àëùëó=1ùë¶ùëñ,ùëó log()ùëíùë•ùëù(ùëéùëñ)ùëó ùëíùë•ùëù(ùëéùëó)‚àë(6)where C is the number of the clinical forms.The final loss function to optimize is thus obtained bysumming up all the losses as defined in Eq. (7).ùêøùëì ùëñùëõùëéùëô = ùêø(ùúÉ, ùê∑(ùë£), ùê∫ùëûùúÉ(ùëß|ùë•)(ùë•)) + ùëÄùëÜùê∏ùëÄùëúùëëùëíùëô++ ùëÄùëÜùê∏ùê∂ùëú‚Ñéùëíùëüùëíùëõùëêùëí + ùêøùê∂ùëüùëúùë†ùë†ùê∏ùëõùë°ùëüùëúùëùùë¶(7)As long as the parameters used for training the AAE areconcerned, 500 iterations with a batch size of 64 are used fortraining the generator and the discriminator in an alternat-ing fashion. The process terminates when the capability ofthe discriminator to distinguish synthetic samples from truesamples remains stable approximately around 50%. TheAdam optimizer is used for both models while the learn-ing rate (lr), imposed for the discriminator, was chosen tobe 10 times smaller than the generator (ùëôùëü = 0.001). Thesesettings, have been empirically observed to lead to a morestable training of the adversarial network, providing betterresults.The whole pipeline for training the GAN framework andgenerating synthetic structural brain networks is summa-rized in Fig. 2. Generally speaking we can divide the entireworkflow in three main phases: i) Training the AAE modelii) Using the AAE model for generating synthetic MS struc-tural brain networks iii) Data augmentation for MS clinicalform classification.3.5.1. Training the AAE modelIn order to properly train the proposed AAE model, ana√Øve data augmentation procedure is needed. The originaldataset is split in training and test set by a leave-one-subject-out cross validation strategy (step 1). The training set wasthen exploited to calculate the conditional probability distri-bution defined in Eq. (8):ùëÉ (ùëã = ùë£|ùëå = ùë¶, ùëÑ = ùëû) =()ùëõùë£ùëñùë¶ùëû(1 ‚àí ùëùùëñùë¶ùëû)ùëõ‚àíùë£ùëùùë£(8)Here, ùëùùëñùë¶ùëû defines the probability of an edge ùëñ to be present inthe vectorized representation of the upper triangular matrixùë• ‚àà {0, 1}(1,ùëë) with dimensionality ùëë and ùëñ ‚àà [0, ùëë] givena class label ùë¶ and a degree quantile ùëû. The letter ùë£ definesthe number of times the edge ùëñ is present and ùëõ the numberof trials (number of subjects drawn).It is worth noting that outlier probabilities can be present.In fact, given that our training set is only the realization ofa stochastic process, the fact that two regions are always(ùëùùëñùë¶ùëû = 1) or never (ùëùùëñùë¶ùëû = 0) connected might not be true ingeneral. For example, it can be due to lack of data or biasesin the collected dataset. To overcome this issue Eq. (9) isapplied:ùëÉ (ùëã|ùëå , ùëÑ) ={0.95,0.05,ùëñùëì ùëùùëñùë¶ùëû > 0.95ùëñùëì ùëùùëñùë¶ùëû < 0.05‚àÄ ùëñ, ùë¶, ùëû(9)It is important to notice that the described anomalies rep-resents a tiny fraction of the total number of connections(‚â™ 1%) and do not mine the final result of the work. Yet,the operation is useful for a better generalisation capabilityand avoid overfitting.The calculated probability density function (pdf) is thenused as a ‚Äústamp" from which sampling new batches of dataat each iteration (step 2 and 3). More in detail, given aclass label ùë¶ and a degree quantile ùëû, a new vectorized rep-resentation of an upper triangular matrix can be obtainedùë• ‚àà {0, 1}(1,ùëë), where ùë•ùëñ is assigned 1 with probability ùëùùëñùë¶ùëûor 0 with probability 1 ‚àí ùëùùëñùë¶ùëû. Here, ùë•ùëñ denotes the pres-ence or the absence of a connection in the ùëñ-th edge of theùë¶-th MS class in the ùëû-th degree quantile. Finally, a contin-uous transformation of the connectivity matrices is appliedas following (Eq. (10)):{ùë•ùëñ =ùë•ùëñ + ùëà (0.01, 0.05),ùë•ùëñ ‚àó ùëà (0.95, 0.99),if ùë•ùëñ = 0if ùë•ùëñ = 1(10)Berardino Barile: Preprint submitted to ElsevierPage 5 of 12Data Augmentation using Generative Adversarial Neural Networkswhere ùëà is the continuous uniform distribution.In otherwords, a noise component is introduced to the new gener-ated connectivity matrix. This strategy is usually imple-mented in the context of an adversarial model and it hasbeen shown to provide more stable training [34].This na√Øve procedure for extracting new instances from theunderlining likelihood distribution provides a double advan-tage: first, several samples can be generated, thus addressingthe problem of limited training size. Second, the obtainedinstances resemble, at each iteration, the percentile distri-bution of the original dataset. Henceforth, this second ad-vantage is important to overcome the problem of mode col-lapse, which constitute a usual challenge when it comes totrain an adversarial network [12]. It is worth noting that thisdata augmentation technique is only used to train the AAEmodel but cannot be implemented for actual MS connectiv-ity data augmentation. Indeed, this na√Øve method is not ableto produce enough qualitative results, due to the hypothe-sis of conditional independence imposed between pairs ofnodes inside the graph. However, the resampling strategyguarantees that the dataset effectively used to train the ad-versarial framework is perfectly balanced with respect to a-priori information of the clinical profile and graph densitythat we are interested in generating.3.5.2. Using the AAE model for generating syntheticMS structural brain networksOnce the model is trained, we are ready for the secondphase in which the synthetic graphs are generated by sam-pling new instances from a standard gaussian distribution(Figure 2 step 4), with shape ‚Ñù(1,ùëê), and then used to feedthe already trained decoder, obtaining new realistic connec-tome data.3.5.3. MS clinical form classificationFinally, in the third phase, the synthetic dataset was usedto augment the original training set (step 5) and fed to a clas-sifier (step 6) in order to enhance the classification perfor-mance of MS clinical profiles (step 7). The predicted labelsobtained from the classifier were compared with the left-outsamples for performance evaluation. It is important to noticethat neither the training set nor the test set were ever used bythe adversarial model to generate synthetic data. The pro-posed pre-processing approach thus reduces the overfittingtendency of the generative model.3.6. Experimental ProtocolIn this section, the results obtained by evaluating boththe structural property of the connectivity matrices and theirrespective graph-derived metrics are reported. In order toperform the evaluation, a sample of data from a random nor-mal distribution ùë£ ‚àº Óà∫ (ùúá = 0, ùúé2 = 1) were drawn, whereùë£ ‚àà ‚Ñù(1,ùëê) and ùëê = 100. ùëÅ defines the number of samplesto be generated and passed to the decoder ùëùùúô(ùë•|ùëß) to obtaina new sample of synthetic data. Finally, we demonstratethe usefulness of our approach by improving the classifi-cation performance of MS clinical forms, even in presenceof strong imbalance between classes. Classical approaches,Table 2Performances of MS Clinical Forms Classification using dif-ferent data augmentation methodsStrategyùêπ 1ùë†ùëêùëúùëüùëíPrecisionRecallTrue Data65.65 ¬± 12.3477.49 ¬± 12.861.68 ¬± 12.63ROSSMOTEARAE65.84 ¬± 11.9777.49 ¬± 12.7661.76 ¬± 12.0872.32 ¬± 11.1883.04 ¬± 11.2868.45 ¬± 11.3970.0 ¬± 11.5881.1 ¬± 12.1464.8 ¬± 11.83AAE (ours) 81.0 ¬± 10.37 86.25 ¬± 10.36 79.65 ¬± 10.51Average classification performance (with standard errors) based on a leave-one-subject-out cross validation strategy on the original dataset (True Data)and after data augmentation of the training set using the Random Over Sam-pling (ROS) technique, Synthetic Minority Oversampling Technique (SMOTE),Adversarially Regularized Graph Autoencoder (ARAE) and our approach(AAE)like ROS and the more efficient SMOTE [26] are used forcomparisons as well as the more recent adversarial modelARAE [27]. The performance metrics used for the evalua-tion are ùêπ 1ùë†ùëêùëúùëüùëí, Precision and Accuracy which are definedin Eq. (11, 12, 13) respectively.ùêπ 1ùë†ùëêùëúùëüùëí =2ùëá ùëÉ2(ùëá ùëÉ + ùêπ ùëÉ + ùêπ ùëÅ)ùëÉ ùëüùëíùëêùëñùë†ùëñùëúùëõ =ùëÉ ùëüùëíùëêùëñùë†ùëñùëúùëõ =ùëá ùëÉùëá ùëÉ + ùêπ ùëÉùëá ùëÉùëá ùëÉ + ùêπ ùëÅ(11)(12)(13)where the abbreviations TP, TN, FP, FN represent theTrue Positive, True Negative, False Positive and False Neg-ative of instances respectively.From now on, we refer to the connectivity graphs gener-ated through the adversarial network as synthetic data, whilethe original dataset will be labelled as true data.4. Results4.1. Comparison of Data Augmentation Methodsfor MS ClassificationThe classification task was performed with a RandomForest Classifier (RF) with 100 trees, due to its robustnessto overfitting and unbalanced dataset. Table 2 reports theaverage classification performances (with standard errors)between our method and the three oversampling techniquespreviously introduced. Fig. 3 shows the corresponding con-fusion matrices. Compared to the true unbalanced date usedas reference, our method obtained higher performance, reach-ing an ùêπ 1ùë†ùëêùëúùëüùëí of 81% instead of 65.7%. The ROS andSMOTE methods provided a score of 65.8% and 72.3% re-spectively, while the ARAE model reports a value of 70%showing a marginal improvement over the unbalanced base-line.Berardino Barile: Preprint submitted to ElsevierPage 6 of 12Data Augmentation using Generative Adversarial Neural NetworksFigure 2: Schematic Representation of the entire workflow. (1) The original dataset is split in training and test set by means ofa leave-one-subject-out cross validation strategy. Considering only the training set, conditional probabilities were calculatedand mini-batch random samples were drawn (2), at each cycle, for training the AAE model (3). Once the training processwas completed, a batch of random instances were sampled from a standard gaussian distribution (4) and fed to the traineddecoder to produce synthetic graphs, used to augment the original training dataset (5). The resulting augmented datasetwas used to train a classifier (6) to predict MS clinical profiles (7). The entire process was repeated for each patient and thepredicted MS class was compared with the actual class from the left-out subjects by means of F1-score.producing synthetic graphs that span the entire range dis-tribution of the true data sample. In order to evaluate theproperties of synthetic graphs, an equal number of data weregenerated with respect to the true data samples obtaining aperfectly balanced dataset. For both true and synthetic data,the global assortativity degree metric was calculated alongwith a percentile distribution of 1% width. In other words,the distributions of true and synthetic data were computedwith the highest degree of precision following the idea thatlarger bandwidth will produce less precise comparisons bysmoothing the distributions and providing a too optimisticresult.In order to measure the distance between the twodistributions, the mean absolute point-wise deviation wascalculated (77.82 ¬± 46.07). The overlap proportion (OP) be-tween the two distributions (true vs synthetic) is calculatedby employing Eq. (14).ùëÇùëÉ =‚àëùëõùëó=1 ùëöùëñùëõ(ùêºùëó, ùëÄùëó)ùëó=1 ùëÄùëó‚àëùëõ(14)Here, ùëõ represents the number of comparison while ùêº andùëÄ represent the synthetic and true data distribution respec-tively. A value of 96.26% is obtained. A graphical anal-ysis of the true and synthetic data was also performed byFigure 3: Confusion matrices for the classification of MSclinical profiles4.2. Evaluation of Synthetic Data Based onGraphs MatricesWe want to evaluate both the coherence and the differ-Ideally, we aim atence between true and synthetic data.Berardino Barile: Preprint submitted to ElsevierPage 7 of 12Data Augmentation using Generative Adversarial Neural NetworksFigure 4: Embedded t-SNE representation of structuralgraphs: True vs Synthetic dataFigure 5: Graph2Vec embedding comparison of structuralgraphs: True vs Synthetic datameans of the t-distributed Stochastic Neighbour Embedding(t-SNE) model [35, 36]. This algorithm applies a non-lineartransformation of the original multidimensional data. It per-forms an embedding of data, mapping them in a lower di-mensional space. Specifically, the algorithm ensures that,each high-dimensional element is mapped to a lower dimen-sional space in such a way that similar objects are modelledby nearby points and dissimilar objects are modelled by dis-tant points with high probability [37, 38].Fig. 4 illustrates the embedded representation of the syn-thetic and true data. From the image, it can be observedthat the two groups are fairly similar.It is worth to notethat in the t-SNE procedures, the perplexity parameter dic-tates the shape of the mapping function. For this reason,multiple evaluations of this parameter has been performedusing value from 10 to 60 at 10 units increment.In Fig.4 the t-SNE results are illustrated for a perplexity parame-ter of 30. To be noticed that the author pointed out that asfar as the perplexity parameter remains in the usual range(5, 50) the model is quite robust [36]. In addition to the t-SNE representation, the embedding of the true and syntheticgraphs was also performed using the Graph2Vec algorithm[39], which is optimised for working with graphs (Fig. 5).It is a transductive neural embedding framework used tolearn from data-driven distribution representations of arbi-trary sized graphs. This framework ensures that structurallysimilar graphs are represented close to one another, whiledissimilar graph are depicted far apart. In other words, themodel is able to preserve the first and second order prox-imity. The former is the local pairwise similarity betweennodes linked by edges, while the latter indicates the sim-ilarity of the nodes neighbourhood structures. In order tonumerically compare the true and synthetic data, the ùêπ 1ùë†ùëêùëúùëüùëímetric was computed by linearizing the upper triangular partof the binary adjacency matrices (ùê¥ùë°, ùê¥ùë† ‚àà ‚Ñù(1,ùëë) respec-tively with ùëë = 3486). Each true adjacency matrix is com-pared with every synthetic vector. A minimum ùêπ 1ùë†ùëêùëúùëüùëí valueof 63% and a maximum value of 82% was obtained with anaverage score of 76%.4.3. Evaluation of Synthetic Data Based onGraphs FeaturesAnalyzing the metrics of synthetic graphs is importantto capture meaningful features characterizing the structuralbrain connectome. For this purpose, six graph-based globalmetrics are considered: Transitivity, Global Efficiency, Mod-ularity, Density, Betweenness Centrality and Assortativity.Such metrics are indeed widely used to characterize brainconnectivity [40], and could provide a reliable measure ofthe quality of generated data.In Fig. 6, the boxplot distribution between true and syn-thetic data is presented for each metric. Comparable valuesof median and interquartile range are observed for the twodistributions.As in the previous section, the t-SNE analysis has been re-peated varying the perplexity parameter in the range be-tween 10 and 60 at steps of 10. In Fig. 7, the t-SNE em-bedded representation shows similar distributions betweentrue and synthetic data without obvious discrepancies.In order to assess the similarity between the two groups, anadditional evaluation based on the Kernel Density Estima-tion (KDE) function was performed. The distributions oftrue and synthetic datasets were estimated by means of theKDE function and their likelihoods were compared. Thisapproach was originally introduced by Breuleux et al. [41]and applied in the context of generative adversarial networksin two reports [19][42]. The method estimates the prob-ability of the synthetic data, by fitting a Gaussian Parzenwindow to the generated samples and reports the likelihoodunder this distribution. The bandwidth of the Gaussian win-dow is obtained by cross-validating the training data. Af-terword, the similarity between the two datasets has beencomputed by estimating the pdf of the KDE estimation, sothat similar datasets could be represented by similar distri-butions. Value of 2773.66 and 2657.13 were obtained re-spectively by comparing the log-likelihoods for the true andBerardino Barile: Preprint submitted to ElsevierPage 8 of 12Data Augmentation using Generative Adversarial Neural NetworksFigure 6: Boxplot distributions of graphs metrics: True vs Synthetic dataFigure 7: Embedded t-SNE representation of graph metrics:True and Synthetic dataFigure 8: KDE cumulative density function estimation ofstructural graphs: True and Synthetic datasynthetic data. This result suggests, once more, that the twogroups of data are similar. Indeed, the two cumulative func-tions (true in blue and synthetic in orange) are rather close toone another (Fig. 8). Furthermore, they both follow approx-imately a straight monotonic-increasing path, which meansthat the probability mass is evenly distributed across all datasamples in both groups.As an additional test, the bandwidth value from 0.1 to 1 atsteps of 0.1 has been increased in order to evaluate the ro-bustness of our results. An increase in performances forevery value greater than 0.1 (best cross-validation) has beenobserved. Finally, in order to offer a sample visualization ofthe true and synthetic connectivity matrices, Fig. 9 providestwo visual examples from the RR and SP clinical forms. Itis possible to notice that the true and synthetic data are verysimilar.4.4. Evaluation of MSE CoherenceIn order to evaluate the stability of the training process,Figure (10) depicts the generator (orange) and discriminator(blue) training loss. Panel (A) and panel (B) report the re-sults obtained when the coherence loss in Eq. (4) was addedor excluded from the final objective function, respectively.Less spikes and noise are noticeable in the former case com-pared to the latter, implying an higher degree of stability ofthe adversarial training.Berardino Barile: Preprint submitted to ElsevierPage 9 of 12Data Augmentation using Generative Adversarial Neural NetworksFigure 9: Structural Graph Comparison: True and Syntheticdata5. DiscussionIn this work, an approach for generating new structuralconnectivity matrices of MS patients was presented. In acontext of imbalanced data, the proposed framework wasable to up-sample the minority class producing a much higherùêπ 1ùë†ùëêùëúùëüùëí (81%) with respect to the baseline unbalanced clas-sification (66%). Furthermore, comparing to other classicaloversampling techniques (ROS and SMOTE) and a graph-based adversarial network (ARAE), our method increasesthe classification performance by approximately +10%. Theimprovement can be related to the capability of our methodto generate more biologically plausible connectomes thatcan better represent the different clinical forms. One of thepossible explanations can be related to the additional classi-fier branch used as regularized factors. Indeed, it can helpto preserve meaningful structural information which char-acterizes the different clinical forms.Our method was evaluated by comparing true and syntheticdata by means of visual and analytical techniques. In fact,the generated data should meet two requirements in order tobe valid: first, they should preserve similar structural char-acteristics as the ones which can be observed in true MSbrain networks and second, the new generated brain net-works should not be simply copies of the original dataset(overfitting).In other words, while new synthetic graphswith enough diversity in terms of structure and propertiesneed to be generated, they still have to be plausible and lieinside the manifold of the true data. In this work, we showedthat both distributions (true and synthetic) were observed tobe different but very similar to one another. Indeed, the av-erage ùêπ 1ùë†ùëêùëúùëüùëí obtained by comparing the binary adjacencymatrix representation, is 76% (Range 63% to 82%). ThisFigure 10: Training loss comparison with (A) and without (B)coherence lossmeans that a significant portion of the generated graphs iscompletely different with respect to the distribution of thetrue data samples. Notwithstanding, the boxplot compari-son shown in Fig. 6 confirmed our visual observation ofFig. 7. The mean distributions of true and synthetic graphmetrics are very close. However, one can notice a substan-tial variability in the synthetic group. From all this evidencethe likelihood of overfitting the training set, by simply gen-erating duplicates, is negligible. Instead, experiments high-lighted the diversity of the synthetic samples, which lies in-side the manifold of the true data, demonstrating that com-pletely new instances have been generated. Moreover, theactual training set was never seen by the adversarial frame-work which was trained sampling from the conditional dis-tribution proposed in Section 3.5, thus reducing the chanceof overfitting even more.Once completed, the model will allow to increment the sizeof the available dataset and thus perform a much robusttraining of machine learning algorithms even with limitedamount and unbalanced data, which constitutes a commonscenario in the medical field. In addition, by learning thecomplete underline data distribution, it is possible to per-form meaningful bayesian statistical testing of hypothesisas well as generating graphs with desired characteristics.Finally, the combination with other embedding methods,which learn a meaningful representation of single nodes, isBerardino Barile: Preprint submitted to ElsevierPage 10 of 12Data Augmentation using Generative Adversarial Neural Networksof great interest. This study exploits the advantage of theAAE framework in the context of brain graphs data gener-ation and can be easily expanded for the analysis of otherbrain diseases or other related domains.Some limitations should be also mentioned. First, our datasetrepresents structural connectivity matrices in which connec-tions are binarized, discarding the valuable information con-veyed by weighted graphs. Second, due to its simple ar-chitecture, our method will not be efficient for very largegraphs. Moreover, in order to sample random batches fromEq. (8), in this work the clinical class and the degree per-centile were used for conditioning. In fact, the limited amountof data did not allow to add other covariates like age andgender which are worth exploring if one has a large enoughsample size. Finally, the computational time needed to per-form the leave-one-subject-out cross-validation is not negli-gible since for each subject the AAE model has to be trained.However, in the context of brain network analysis, we rarelydeal with much larger networks as the actual MRI data arelimited to a maximum of a few hundred nodes, justifyingthe simplification proposed in this study. Additionally, ourmethod performs well even with a limited in number andstrongly imbalanced data, in agreement with a previous re-port [29].6. ConclusionIn this study, a new data augmentation approach for con-nectome dataset was presented. Given the capability of graphsto represent complex brain networks, our approach providesa new tool for biomedical application, a domain in which thedata availability is scarse and poorly behaving. Therefore,our connectome-based data augmentation approach repre-sents a promising alternative to usual image-based techniques.Furthermore, the proposed data augmentation approach wascapable to improve the MS classification performance evenin cases of unbalanced data scenario. As future work, weaim to improve our approach by generating all clinical MSprofiles and exploit weighted connectivity matrices in placeof binary structural graphs. In addition, we plan to exploreconditional adversarial neural network methods to performmeaningful bayesian statistical testing of hypothesis as wellas generating graphs with desired characteristics. Finallythe combination with other embedding methods, which learna meaningful representation of single nodes, should be alsoexplored.AcknowledgementThis study is funded by the following projects: Euro-pean Research Council within the grant 813120-2018 of MarieSk≈Çodowska-Curie Innovative Training Networks (ITN) ofthe Horizon 2020 through the INSPiRE-MED project. FrenchNational Research Agency (ANR) within the national pro-gram ‚ÄúInvestissements d‚ÄôAvenir‚Äù through the OFSEP project(ANR-10-COHO-002).Declaration of Competing InterestThe authors declare that they have no conflict of interest.References[1] Young T., Hazarika D., Poria S., Cambria E. Recent trends in deeplearning based natural language processing. arXiv, 2017.[2] Milletari F., Navab N., Ahmadi S.A. V-net: Fully convolutional neu-ral networks for volumetric medical image segmentation. Fourth In-ternational Conference on 3D Vision (3DV), pages 565‚Äì571, 2016.[3] Yun K., Huyen A., Lu T. Deep neural networks for pattern recogni-tion. arXiv, 2018.[4] Schwenker F., Abbas H.M., El Gayar N., Trentin E. Artificial NeuralNetworks in Pattern Recognition. 2016.[5] Adibuzzaman M. , DeLaurentis P., Hill J., Benneyworth B.D. Bigdata in healthcare. AMIA Annu Symp Proc., pages 384‚Äì392, 2018.[6] Floca R. Challenges of open data in medical research. SpringerInternational Publishing, pages 297‚Äì307, 2014.[7] Perez L., Wang J. The effectiveness of data augmentation in imageclassification using deep learning. arXiv, 2017.[8] Goodfellow I., Bengio Y., Courville A. Deep learning Book. MITPress, 2016.[9] Shorten C. and Khoshgoftaar, T. M. A survey on image data augmen-tation for deep learning. Journal of Big Data, 6(1):60, 2019.[10] Shorten C., Khoshgoftaar T.M. A survey on image data augmentationfor deep learning. Journal of Big Data, 6, 2019.[11] Guo Y., Nejati H., Cheung N.M. Deep neural networks on graphsignals for brain imaging analysis. IEEE International Conferenceon Image Processing (ICIP), pages 3295‚Äì3299, 2017.[12] Baldassarre A., Ramsey, L. E.; Siegel, J. S., Shulman, G. L., Cor-betta, M. Brain connectivity and neurological disorders after stroke.Current Opinion in Neurology, 29(6):706‚Äì713, 2016.[13] Rezazadeh M., I.; Frohlich, J., Loo S. K.; Jeste S. Brain connec-tivity in autism spectrum disorder. Current Opinion in Neurology,29(2):137‚Äì147, 2016.[14] Rafael G., Jaime R.S., Dante B., Jaime A. How artificial intelligenceis supporting neuroscience research: A discussion about foundations,methods and applications. arXiv, pages 63‚Äì77, 2017.[15] Ghasemi N., Razavi S. and Nikzad E. Multiple sclerosis: Patho-genesis, symptoms, diagnoses and cell-based therapy. Cell Journal,19(1):1‚Äì10, 2016.[16] Verma V., Qu M., Lamb A., Bengio Y., Kannala J., and Tang J.Graphmix: Regularized training of graph neural networks for semi-supervised learning. ICRL 2020 Conference, 2020.[17] Radford A. , Metz L., Chintala S. Unsupervised representation learn-ing with deep convolutional generative adversarial networks. Pre-ceeding at ICLR, 1, 2016.[18] Makhzani A., Shlens J., Jaitly N., Goodfellow I., Frey B. Adversarialautoencoders. International Conference on Learning RepresentationsICLR, 2016.[19] Calimeri F. , Marzullo A. , Stamile C., Terracina G. . Biomedicaldata augmentation using generative adversarial neural networks. Ar-tificial Neural Networks and Machine Learning ICANN, Springer,10614:626‚Äì634, 2017.[20] Frid-Adar M., Diamant I. , Klang E. , Amitai M. , Goldberge J. ,Greenspan H. . Gan-based synthetic medical image augmentation forincreased cnn performance in liver lesion classification. Neurocom-puting, 321, 2018.[21] Salehinejad H., Valaee S., Dowdell T., Colak E., Barfett J. Gener-alization of deep neural networks for chest pathology classificationin x-rays using generative adversarial networks. IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP),page 990‚Äì994, 2018.[22] Shui-Hua W., Yu-Dong Z. Densenet-201-based deep neural networkwith composite learning factor and precomputation for multiple scle-rosis classification. Association for Computing Machinery, 16:1551‚Äì6857, 2020.Berardino Barile: Preprint submitted to ElsevierPage 11 of 12Data Augmentation using Generative Adversarial Neural Networks[23] Yu-Dong Z.. Vishnuvarthanan G., Chaosheng T. High performancemultiple sclerosis classification by data augmentation and alexnettransfer learning model. Journal of Medical Imaging and Health In-formatics, 9:1‚Äì10, 2019.[24] Zhang X., Yao L., Yuan F. Adversarial variational embedding forrobust semi-supervised learning. Research Track in KDD, 2019.[25] Imran A., Terzopoulos D. Multi-adversarial variational autoencodernetworks. CoRR, 2019.[26] Chawla N.V., Bowyer K.W., Hall L.O., Kegelmeyer W.P. Smote:Synthetic minority over-sampling technique. Journal of Artificial In-telligence Research, 6:321‚Äì357, 2002.[27] Pan S., Hu R., Long G., Jiang J., Yao L. and Zhang C. Adversariallyregularized graph autoencoder for graph embedding. IJCAI, 2018.[28] Feng F., He X., Tang J., Chua T.S. Graph adversarial training: Dy-namically regularizing based on graph structure. arXiv, 2019.[29] Khoshgoftaar T.M., Golawala M., Hulse J.V. An empirical study oflearning from imbalanced data using random forest. 19th IEEE In-ternational Conference on Tools with Artificial Intelligence (ICTAI),2:310‚Äì317, 2007.[30] Kocevar G., Stamile C., Hannoun S., Cotton F., Vukusic S., Durand-Dubief F., Sappey-Marinier D. Graph theory-based brain connectiv-ity for automatic classification of multiple sclerosis clinical courses.Frontiers in Neuroscience, 13, 2019.[31] Smith S.M., Jenkinson M., Woolrich M.W., Beckmann C.F. et al. Ad-vances in functional and structural mr image analysis and implemen-tation as fsl. Neuroimage, 62, 2019.[32] Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, AlexeiA. Unpaired image-to-image translation using cycle-consistent ad-versarial networks. Computer Vision (ICCV), 2017.[33] Doersch C. Tutorial on variational autoencoders. arXiv, 2016.[34] S√∏nderby C. K., Caballero J., Theis L., Shi W., Husz√°r F. AmortisedMAP inference for image super-resolution. arXiv, 2016.[35] Van der Maaten L.J.P. Learning a parametric embedding by preserv-ing local structure. Twelfth International Conference on Artificial In-telligence & Statistics (AI-STATS), JMLR W&CP, 5:384‚Äì391, 2009.[36] Van der Maaten L.J.P., Hinton G.E. Visualizing high-dimensionaldata using t-SNE. Journal of Machine Learning Research, 9:2579‚Äì2605, 2008.[37] Van der Maaten L.J.P. Accelerating t-SNE using tree-based algo-rithms. Journal of Machine Learning Research, 15:3221‚Äì3245, 2014.[38] Van der Maaten L.J.P., Hinton G.E. Visualizing non-metric similari-ties in multiple maps. Machine Learning, 87:33‚Äì55, 2012.[39] Narayanan A., Chandramohan M., Venkatesan R., Chen L., Liu Y.,Jaiswal S. graph2vec: Learning distributed representations of graphs.arXiv, 2017.[40] Beauchene C, Roy S, Moran R, Leonessa A, Abaid N. Comparingbrain connectivity metrics: a didactic tutorial with a toy model andexperimental data. Neuroimage, 15(5), 2018.[41] Breuleux O., Bengio Y., Vincent P. Quickly generating representa-tive samples from an RBM-derived process. Neural Computation,23(8):2053‚Äì2073, 2011.[42] Goodfellow I., Pouget-Abadie J., Mirza M., Xu B. et al. Generativeadversarial nets. Advances in Neural Information Processing Sys-tems, 23(8):2053‚Äì2073, 2011.Berardino Barile: Preprint submitted to ElsevierPage 12 of 12