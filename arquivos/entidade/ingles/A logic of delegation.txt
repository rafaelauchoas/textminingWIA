Artificial Intelligence 174 (2010) 51–71Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA logic of delegationTimothy J. Norman a,∗, Chris Reed ba Department of Computing Science, University of Aberdeen, Aberdeen AB24 3UE, Scotland, UKb School of Computing, University of Dundee, Dundee DD1 4HN, Scotland, UKa r t i c l ei n f oa b s t r a c tArticle history:Received 29 October 2008Received in revised form 1 October 2009Accepted 4 October 2009Available online 13 October 2009Keywords:DelegationGroupsImperativesResponsibilityAgent communication1. IntroductionDelegation is a foundational concept for understanding and engineering systems thatinteract and execute tasks autonomously. By extending recent work on tensed actionlogic, it becomes possible to pin down a specific interpretation of responsibility witha well specified semantics and a convenient and intuitive logic for expression. Oncedescriptions of direct agent responsibility can be formed, there is a foundation uponwhich to characterise the dynamics of how responsibility can be acquired, transferred anddischarged and, in particular, how delegation can be effected. The resulting logic, designedspecifically to cater for responsibility and delegation, can then be employed to offer anaxiological and semantic exploration of the related concepts of forbearance, imperativesand group communication.© 2009 Published by Elsevier B.V.Delegation is a concept that pervades agent-based computing — tasks such as the purchase of goods within an electronicinstitution may be delegated to a software agent acting on behalf of a user [14,16], a goal may be delegated from oneagent to another each representing different commercial organisations via, for example, the Contract Net protocol [49], oran agent representing a manager within one organisation may, by virtue of their organisational position, delegate a taskto an agent representing a subordinate [33]. Understanding the nature and complexities of delegation, therefore, has thepotential to impact upon a wide audience within AI. In this paper we address the problem of understanding delegationdirectly by presenting a responsibility-based semantics of delegation (Section 2), that provides for an axiological account ofhow responsibility is transferred during delegation. With the basic mechanics of delegation in place, several tricky cases arethen examined in detail, including:(1) the interaction between delegation and time (and specifically, how axioms of delegation might be extended in a tensedlogic) (Section 3.1);(2) the relationship between responsibility and the concept of forbearance (and in particular, whether the Refref conjec-ture [2], by which activity is claimed to be equivalent to refraining from refraining, is a defensible axiom) (Section 3.2);and(3) the way in which group-addressed communication can effect delegative transfer of responsibility (and specifically, howdistributive and collective responsibility [43] is composed from the responsibilities of the individuals in a group) (Sec-tion 3.3).* Corresponding author.E-mail addresses: t.j.norman@abdn.ac.uk (T.J. Norman), chris@computing.dundee.ac.uk (C. Reed).0004-3702/$ – see front matter © 2009 Published by Elsevier B.V.doi:10.1016/j.artint.2009.10.00152T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71Finally, with a rich account of delegation in place, we explore how imperative communication can be used to executedelegation, how responsibility is acquired as a result, and how it can be discharged through appropriate action meeting theconstraints of whole-hearted satisfaction [23] (Section 4).2. FoundationThe first step in characterising delegation is to construct a model of agent responsibility, so that the former can bedefined as a special case of the latter in which particular actions (often communicative actions) lead to a transfer of re-sponsibility. Such a model needs to tie together agent intentions, actions, and states of the world. Elsewhere [42] we haveargued that to handle this richness competently, it is necessary to adopt an approach that represents both states and eventsas first class objects. The argument there, and subsequent exploration of the system that results, leans heavily on founda-tional work carried out by Hamblin [23] in his investigation of imperatives. In this section we summarise a logic designed tocapture the nature of the imperative based upon Hamblin’s Action-State Semantics. The logic captures, at both the semanticand syntactic levels, the important ontological distinction between ‘responsibility for the achievement of a state of affairs’(captured by the modality S) and ‘responsibility for the execution of an action’ (captured by the modality T). This is one ofthe key distinguishing features of the language, which we refer to as ST.2.1. SyntaxIn presenting the syntax of our language, ST, we begin by defining the set of well-formed formulae, then briefly discussaxioms and rules of inference of the modalities S and T and of a standard Peircean tense logic which is used to enable theexpression of tensed responsibility formulae. This, along with the Hamblinian semantics of ST summarised in Section 2.2,lays the groundwork for the detailed analysis of the nature of delegation presented in Section 3.Basic atoms of the language ST include states of affairs, referred to using upper case Roman letters ( A, B, C , . . . ), actions,which are referred to using lower case Greek letters (α, β, γ , . . . ), and agents, for which we use x, y, . . . . We denote that aspecific agent, x, executes action α in the following manner: αx. Where the agent of an action is not specified, it is assumedthat the action is carried out by some agent but it is not important which one.The modalities S and T are relativised to specific agents and refer to state formulae and event formulae respectively. Inthis way, Sx A refers to agent x being responsible for the achievement of the state of affairs A, and Txα refers to x beingresponsible for the execution of action α. Note that these modal statements do not specify any particular action for agent x.In satisfying Txα, for example, agent x may order some other agent, y, to carry out α.Any sentence in the language may be tensed through the use of the modalities G (always true in all futures) and H(always true in all pasts), and their respective duals, F (true at some point in a possible future) and P (true at some point ina possible past). Tensed sentences are S-formulae; to say that something will be true, or that some action has been done,etc. is a state of affairs. It is, however, entirely reasonable to permit tense operators to range over both states and events:states of affairs may have held in the past, and events may happen in the future, etc.We may now define the well-formed formulae of our language ST. The basic atoms of our language are divided intotwo classes: (i) event formulae — those that consist entirely of propositional expressions of action (bound or unbound), and(ii) state formulae — all others. All such basic atoms are wffs. By conventional Propositional Logic (PL), for any two wffs, φand ψ , that are event formulae, φ ∨ ψ , φ ∧ ψ , φ → ψ and ¬φ are also wffs that are event formulae. Similarly for any twowffs that are state formulae, any PL combination of them is also a wff that is a state formula. For the action modalities,any wffs that are event formulae can be used to form a further wff with the T modality: Txα, Txαx, Tx(α y ∨ β), etc., whichare themselves state formulae. Any wff that is a state formula can be used to form a further wff with the S modality Sx A,SxT yαz, etc. that are state formulae. Finally, for any ψ that is either an event formula or a state formula, Gψ , Hψ , Fψ andPψ are also wffs that are state formulae.The logic of modality Sx is that of a regular modal system of type RT [10, p. 237]. This is the smallest system containingall axioms of Propositional Logic and closed under the rule of inference RE:REA ↔ BSx A ↔ Sx Bwith the additional axiom T, which is characteristic of logic of successful actionTSx A → Aand, of course, the distribution axiom K, which is minimally true of all modal logicsKSx( A → B) → (Sx A → Sx B)Unlike other models of agentive action [3,11,26] however, we include neither the rule of necessitation ( A/Sx A) nor that ofanti-necessitation (¬ A/¬Sx A). Consequently the logic of Sx is non-normal. A key advantage of including neither of theseaxioms is that we may include the equivalence R without introducing inconsistency. Axiom R captures the intuition that ifan agent is responsible for achieving A and responsible for achieving B then it is responsible for achieving the conjunction ofT.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–7153Gφ → ψ → (Gφ → Gψ)Hφ → ψ → (Hφ → Hψ)φ →(cid:3)Hφ → (Gφ → GHφ)(cid:2)Gφ → FφHφ → PφGφ → GGφHφ → HHφFig. 1. Axioms of a Peircean tense logic.these states, and that if an agent is responsible for achieving the conjunction of A and B then it is responsible for achievingeach conjunct. In the logic of action specified by Jones and Sergot [26], for example, including axiom Sx( A ∧ B) → Sx A ∧ Sx Balong with the rule of anti-necessitation will lead to a contradiction. Axiom R for modality Sx is:RSx( A ∧ B) ↔ Sx A ∧ Sx BFinally, we include axiom D in the logic of Sx, which captures the intuition that an agent cannot be responsible for theachievement of contradictory states of affairsDSx A → ¬Sx¬ AThe characterisation of modality Tx is identical to that of Sx; both being regular modal systems of type RT. It should benoted, however, that these modalities operate over different worlds in their interpretation (see Section 2.2).To enable us to explore the interpretation of responsibility over time, we require the use of a logic of time. For ourpurposes we adopt a simple Peircean tense logic. In this logic of time, there are two basic modalities: G and H. Their dualswith respect to some sentence φ, are defined as follows: Fφ def= ¬G¬φ and Pφ def= ¬H¬φ. The axioms for this tense logic aresummarised in Fig. 1 (the standard Peircean model).The well-formed formulae, rules of inference, definitions and axioms summarised in this section form the syntactic basisfor our language ST. Before exploring the nature of delegation and the scope of an agent’s responsibility we outline thesemantics of our language. Many readers may happily skip the following section, having an intuition of the meaning of theformulae of our language and wishing to ‘cut to the chase’. Those expecting a more thorough exploration of the semanticsare referred to [42].2.2. SemanticsThe first step is to preserve the distinction between states and events simply by dividing them into separate sets ofpossible worlds. That is, some worlds contain state descriptions, and other worlds contain event descriptions. Then we definean accessibility relation that holds between these worlds. Rather than adopting a conventional binary relation, Hamblin’ssemantics demands a ternary relation that links a world of state descriptions, with a world of event descriptions andanother world of state descriptions.1 In the following, we present the formal semantics for ST built upon a Hamblinianframe of reference, FH.Definition 1 (FH). FH = (cid:7)W, RH(cid:8) such that(i) W is a non-empty set that collects together our ‘state worlds’ and ‘event worlds’; and(ii) RH is a ternary relation, where (cid:7)u, v, w(cid:8) ∈ RH and u, v, w ∈ W . Each (cid:7)u, v, w(cid:8) ∈ RH should be understood as ‘statew is accessible from state u by way of v’.Note that it is not necessary to enforce types upon the worlds explicitly in the semantics; this can be handled implicitlyby RH. There are two conditions, however, on RH that are required to accurately capture the Hamblinian picture and theintuitions associated with it. The first is that each world in W is either a world containing state descriptions (in this caseit is a member of the set Wstates) or a world containing event descriptions (in which it is a member of the set Wevents asdefined above), but not both; i.e. Wstates ∩ Wevents = ∅. The second constraint on our relation RH is that the structure ofinterconnected ‘state worlds’ (via ‘event worlds’) is a directed acyclic graph. Beyond these two conditions, the relation RHdoes not portray any of the more typical characteristics of many binary accessibility relations used by classical modal logics:it is neither symmetric nor reflexive, neither transitive nor Euclidean.Definition 2 (Wstates). Each element of RH contains two state worlds (those appearing in the first and third places in thetuple).(cid:4)Wstates =w | (cid:7)w, x, y(cid:8) ∈ RH(cid:5)(cid:4)∪w | (cid:7)x, y, w(cid:8) ∈ RH(cid:5)1 It could equally well be linking two event description worlds with one state description worlds; ultimately nothing of importance hangs upon thisdecision.54T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71Definition 3 (Wevents). Each element of RH contains a single event world (appearing in the second place in the tuple).(cid:4)Wevents =w | (cid:7)x, w, y(cid:8) ∈ RH(cid:5)The semantic structure defined by RH forms the lower layer in our model, and, on the basis of this layer, it is possible todefine accessibility relations (and later, necessitation functions). First, for the temporal component we define an accessibilityrelation R F , expressing the earlier-later relation.Definition 4 (R F ). An event world v occurs later than the state world u immediately preceding it (i), consecutive stateworlds are related (ii), and, in (iii), transitivity is built into the relation.(cid:7)u, v(cid:8) ∈ R Fiff∃w ∈ W s.t. (cid:7)u, v, w(cid:8) ∈ RH or∃w ∈ W s.t. (cid:7)u, w, v(cid:8) ∈ RH or∃w, t ∈ W s.t. ((cid:7)u, w, t(cid:8) ∈ RH and (cid:7)t, v(cid:8) ∈ R F )(i)(ii)(iii)To provide a foundation for temporal statements in our language we cannot simply express a later-earlier relation interms on R F (the earlier-later relation). This is due to our use of the ternary relation RH and that all tensed sentences(Hψ , Fψ , etc.) are state formulae. Our later-earlier relation, R P , is, therefore, defined separately, and, together, R F and R Pform a conventional temporal frame for a traditional, transitive tense logic [40].Definition 5 (R P ). The definition of R P is a direct analog of R F .(cid:7)u, v(cid:8) ∈ R Piff∃w ∈ W s.t. (cid:7)w, v, u(cid:8) ∈ RH or∃w ∈ W s.t. (cid:7)v, w, u(cid:8) ∈ RH or∃w, t ∈ W s.t. ((cid:7)t, w, u(cid:8) ∈ RH and (cid:7)t, v(cid:8) ∈ R P )(i)(ii)(iii)The action component is characterised in a slightly different way. Given that the logic of S and T is non-normal, itdemands a minimal model, defined upon necessitation functions. Those necessitation functions must act upon a differentsubstrate: for the S modality, the substrate is state descriptions, for T, event descriptions. The necessitation functions arerelativised to individual agents in the usual way (that is, the way in which one agent’s behaviour is described is independentof how other agents’ behaviour is described). Thus S x is the necessitation function for the modality S, relativised to someagent, x. The functions map from worlds to sets of worlds. So, S x : Wstates → ℘ (℘ (Wstates)), as usual (thereby picking outworlds by which necessity is defined). The T modality is different in that T x : Wstates → ℘ (℘ (Wevents)); T x is, therefore,picking out particular events that are, loosely, “actionable” by x from a state world w. Furthermore, the Wevents worlds arenot simply propositional. To accurately model Hamblin’s conception of “deed-agent assignments”, these worlds are filled(exclusively) with statements of the form agent x performs action α, that we represent with the typographic shorthand αx,and wffs constructed from such statements using PL.In this way, the model as a whole is defined as (cid:7)W, X , I, RH, S x, T x(cid:8) for a set of possible worlds W , a set of agents X ,an interpretation function, I, the ternary Hamblinian accessibility relation, RH, and the relativised necessitation functionsfor the modalities S and T, S x and T x (for each x ∈ X ), respectively.The necessitation functions, in combination with the accessibility relations then offer a straightforward way of charac-terising the semantics of the logic as a whole:(cid:2)(cid:3)αx, ω= (cid:15)(cid:2)|(cid:14)M|(cid:14)M|(cid:14)M|(cid:14)M|(cid:14)Mω A iff I( A, ω) = (cid:15)iff Iω αxω α iff ∃x ∈ X s.t. Iαx, ωω Sx A iff (cid:16) A(cid:16)M ∈ S x(ω)ω Txα iff (cid:16)α(cid:16)M ∈ T x(ω)The truth set is constructed normally:(cid:3)= (cid:15)(cid:4)(cid:16)ϕ(cid:16) =ω ||(cid:14)Mω ϕ(cid:5)The truth set is constructed in the same manner for both states and events; this symmetry is a result of the typing ofpossible worlds, so that increased complexity in the model structure yields increased simplicity in the connection betweenthat structure and the syntactic surface.Now we are in a position to characterise the tense component of our language. Here, we are simply defining sentenceswith our tense modality with accessible worlds in which the object of that modality are true. For example, we can say inT.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–7155TSSTSTTFSTPSTFTTPTSxS y A → S y ASxT yα → T yαFSx A → F APSx A → P AFTxα → FαPTxα → PαQSQTQFSQPSQFTQPTSxS y A → Sx ASxT yα → TxαSxFS y A → SxF ASxPS y A → SxP ASxFT yα → SxFαSxPT yα → SxPαFig. 2. Theorems and axioms of delegation and responsibility.some world, ω, given some model, M, that φ holds in some point in a possible future, Fφ, if and only if there is someother world, ω(cid:17)that is accessible from ω via our future-directed relation, R F , in which φ holds.(cid:2)(cid:3)|(cid:14)M|(cid:14)M|(cid:14)M|(cid:14)Mω Fφ iff ω ∈ Wstates ∧ ∃ω(cid:17) ∈ W s.t. R Fω Gφ iff ω ∈ Wstates ∧ ∀ω(cid:17) ∈ W s.t. R Fω Pφ iff ω ∈ Wstates ∧ ∃ω(cid:17) ∈ W s.t. R Pω Hφ iff ω ∈ Wstates ∧ ∀ω(cid:17) ∈ W s.t. R P(cid:3)ω, ω(cid:17)(cid:2)ω, ω(cid:17)(cid:3)(cid:2)ω, ω(cid:17)(cid:2)ω, ω(cid:17)(cid:3)and |(cid:14)Mand |(cid:14)Mand |(cid:14)Mand |(cid:14)Mω(cid:17) φω(cid:17) φω(cid:17) φω(cid:17) φ3. Delegation and responsibilityWith this formal characterisation of the modalities S and T in place, we now explore the nature of delegation and, indoing so, develop a detailed model of the scope of an agent’s responsibility. Delimiting the scope of agentive responsibilityin this way provides a foundation for some of the key questions in the design, development and operation of multi-agentsystems. It forms an essential component for performance monitoring, checking compliance with protocols, and identifyingthe contributors to a failure which may impact upon the reputations of agents within a society.Fundamental to our notion of responsibility, of course, is the semantics of the modalities S and T, which captures ourconception that an agent be held responsible for its own volition. In broader terms, however, we must determine to whatextent an agent can be held responsible with respect to delegated activities. We start this discussion with a summary of thebasic axioms of delegation for our logic containing the action modalities S and T and tense modalities F and P (and hencetheir duals G and H respectively), which were first outlined in [42]. These lay the foundation for an extended analysis inwhich we explore the notion of refraining and group-directed imperatives. In the subsequent three sections, therefore, wefocus our attention on the following questions:(1) If an agent acts through another (i.e. if an agent delegates a task to another), should it be deemed to have acted inperson? (Section 3.1.)(2) Is doing the same as refraining from refraining? (Section 3.2.)(3) What does it mean for an imperative to be issued to a group of agents? (Section 3.3.)3.1. Responsibility for delegated tasksIn common with the model of agentive action proposed by Chellas [11], but contrary to Von Wright’s [52] characterisa-tion and that of Belnap et al. [4], the theory presented here offers scope for nesting the two modalities in building a richnotion of responsibility. Thus, in this section we discuss the theorems and axioms of delegation summarised in Fig. 2.An agent may use many means to delegate tasks; e.g. the issuing of a command within the context of a military or-ganisation, or asking a colleague to cover for a lecture. As a result, the agent having delegated a task will have someresponsibility for the task delegated, but what is the nature of this responsibility and how can it be expressed? To answerthis question, we must explore the meaning of formulae in which the S and T modalities are nested. For example, if theactivity concerned is the performance of action α, the delegator is agent x and the agent to whom responsibility for theact is delegated is y, this can be captured by the sentence SxT yα. Similarly, if the activity concerned is the achievement ofsome state of affairs, A, this can be captured by the sentence SxS y A. But, what do these sentences mean and where liesthe responsibility for, respectively, α and A?The first thing to notice is that specialisations of axiom T give us part of the answer. In Fig. 2, theorems TSS and TSTare presented. Taking TST for instance, if it can be said that agent x is responsible for agent y’s responsibility for theperformance of action α, then we can conclude that y is indeed responsible for the performance of α. But what of x’sresponsibilities?Chellas [11] provides us with an answer in the form of the legal principle Qui facit per alium facit per se (roughly, hewho acts through another has acted himself). This principle led to Chellas’s introduction of an axiom Q; equivalents of thisconcept are presented in Fig. 2 as QS and QT. Axiom QS expresses the idea that if it is the case that agent x is responsiblefor agent y’s responsibility for the achievement of A, then it can also be said that x is responsible for the achievement ofA. It is interesting to note that these axioms cannot be accepted by Belnap and Perloff [3]: Chellas [11, p. 506] shows that56T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71the something happens condition (for all possible combinations of choices of all agents there is at least one history) leads to(using our notation) SxS y A being false whenever x (cid:19)= y.Axioms QS and QT capture the essence of our answer to the first question posed in the introduction to this section,but this requires some refinement when the issue of tense is considered. In order to fully explore other possible theoremsand axioms, we must consider all 2- and 3-modality well-formed formulae with alternative tense and action modalities.Specifically, we must consider the meaning of all formulae that can be composed from the following schemes (a), (b), (c)and (d):(a) SxFPAα(b)FPSx ATxα(c) SxFPS y AT yα(d)FPSxFPAαThese summarise a total of 20 possible combinations of 2- and 3-modality wffs where tense and action modalitiesalternate. In these schemes the horizontal lines indicate that one of the two modalities above or below the line may beincluded to form a wff. Using scheme (c), for example, we can construct formulae commencing with the modality Sx,followed by either F (true at some point in a possible future) or P (true at some point in a possible past), and then followedby either S y A or T yα. The formula SxFS y A can, in this way, be constructed from this scheme, which expresses the notionthat agent x is responsible for it being the case at some point in a possible future that y is responsible for the achievementof A. Similarly, SxFT yα and SxPS y A can be constructed from this scheme.The entailments derived from formulae that can be created from scheme (a) are relatively straightforward. The T axiomwill, in each case, allow the derivation of a tensed formula; e.g. from SxF A we may derive F A and from SxPα we mayderive Pα. There are no further axioms required; we would certainly not wish to derive Sx A or FSx A from SxF A for example.Next, let us consider those wffs that can be created from scheme (b). In order for us to derive appropriate conclusionsfrom these wffs we do require further axioms. In fact, we need analogues of the T axiom to retain the notion of a model ofsuccessful action. We introduce the four further axioms, TFS, TPS, TFT and TPT (Fig. 2), for this purpose. These analoguesof the T axiom ensure that the tense (loosely) associated with an action is carried over to the successful completion of thataction. Thus from FSx A (it is true at some point in a possible future that agent x is responsible for the achievement of A),we want to conclude F A ( A is true at some point in a possible future), from PTxα we want to conclude Pα, and so on.Schemes (c) and (d) provide wffs that include three alternating tense and action modalities. In the case of the formulaethat may be created from scheme (d), the existing axioms are sufficient (including the tensed axioms of successful action in-troduced when discussing scheme (b)). For example, from FSxF A we can, through TFS, derive FF A and hence F A. Therefore,all the deductions one would wish to be able to draw are already catered for.Formulae that may be created from scheme (c), however, require further axioms to fully characterise delegation ofresponsibility over time. Thus SxFS y A yields not only FS y A via axiom T, and thence F A via axiom TFS, but in addition, wewant to capture the fact that SxFS y A also has a more intimate connection (i.e. x’s responsibility for) the future occurrenceof A. Specifically, by analogy to the atemporal Q axioms, we would want to be able to derive SxF A (axiom QFS in Fig. 2).In this way, we may construct four new analogs of the Q axiom (QFS, QPS, QFT and QPT, Fig. 2) that carry over tensemodalities, in just the same way as we have done for analogs of the T axiom above. To illustrate the role of these axioms,let us consider just one of them and summarise its meaning. Axiom QFT captures the idea that if agent x is responsible for,at some point in a possible future, agent y ensuring that α is done, then x is responsible for, at some point in a possiblefuture, α being done.The meanings of axioms QPS and QPT are, possibly, less intuitively clear. It may be a little difficult to accept QPS interms of our intuitions about time (and causality), but it should be noted that what we are capturing is the notion ofagentive responsibility in a temporal context. Thus, QPT may be better understood using the following characterisation: ifagent x is responsible for a state of affairs in which, at some point in a possible past, agent y ensures that α is done, thenx is responsible for, at some point in a possible past, the doing of α.An alternative to axiom QFS is defensible: we might want to allow FSx A to be derived from SxFS y A; i.e. that by x beingresponsible for it being the case that it is possible in the future for agent y to be responsible for the achievement of A, itis possible in the future for x to be responsible for A. We label this plausible, but stronger, alternative QFS↑:↑QFSSxFS y A → FSx A↑(SxPT yα → PTxα) are candidates for capturing(SxFT yα → FTxα), and QPTSimilarly, axioms QPSa notion of responsibility in defining a social system. The question is why are these stronger delegation axioms, and whatare the consequences of adopting these alternatives?(SxPS y A → PSx A), QFT↑↑↑To illustrate why axiom QFSintroduces a stronger notion of responsibility (and, by analogy, the other ‘’ axioms givenabove), consider the example of a mental health consultant making the decision to release a patient from a confinedpsychiatric ward. Now, suppose that the patient returns home and murders his family. Where lies the responsibility forthis act? Formalising this scenario, we can say that the consultant is responsible for it being the case that it is possible inthe future that the patient is at home (due to the consultant having released the patient); line 1 in Fig. 3. Let us supposethat the patient murdering his family is (future) possible if and only if it is (future) possible that the patient is at home;line 2 in Fig. 3. By rule RE, we can conclude line 3; i.e. that the consultant is responsible for it being a future possibilitythat patient is responsible for killing his family. Now, lines 4 and 5 offer two possible conclusions that may be drawn from↑T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71571.2.3.4.5.SconsultantFpatient_at_homeFpatient_at_home ↔ FSpatientfamily_murderedSconsultantFSpatientfamily_murderedSconsultantFfamily_murderedFSconsultantfamily_murdered(patient released)(by QFS)(by QFS↑)Fig. 3. Consultant responsibility example.line 3; leading to two possible axiomatic formulations: one with the conclusion given on line 4 of Fig. 3 (through QFS inFig. 2), and one with the conclusion given on line 5 of Fig. 3 (through the alternative axiom QFS).↑This emotive example generates strong intuitions. We may take the view that the mental patient is only partly re-sponsible for his actions, and that the consultant must shoulder some of the responsibility. One might expect a charge ofprofessional negligence to be levelled. But, intuitively, it is unlikely that the consultant would be seen to be guilty of (con-spiracy to) murder or manslaughter. This is the choice offered by QFS and QFS. Line 4 of Fig. 3 describes responsibilityof a possibility, in which culpability is only established through, for example, some assessment of fore-knowledge of thebiconditional in line 2. Responsibility for possible disaster is a common feature in assessing negligence. Line 5, in contrast,is quite different, as it describes the possibility of direct responsibility; this being, in example, sufficient for graver chargesas an axiom ofbeing laid. Cases such as this seem to suggest strongly that it may not be reasonable to include QFSdelegation in many social systems, and we would argue that it is not generally considered valid in legal systems.↑↑3.2. ForbearanceBelnap et al. [4], spend some time discussing and laying out the formal properties of what Belnap [2] describes as theRefref conjecture. Belnap describes Refref in such a clear and engaging way that we include here an extended quote:The background idea is that one good way of approaching agency is via the modal locution [α stit : Q ], to be read as“α sees to it that Q ”, where α is an agent and where according to the thesis that the complement of stit should beunrestricted, Q may take the place of any sentence whatsoever.Given the locution [α stit : Q ], it is easy to see that refraining when complemented by a non-agentive has just theform [α stit : ¬Q ], for example Autumn Jane refrains from becoming muddy comes to Autumn Jane sees to it that she doesnot become muddy. Accordingly, refraining from an action has the form [α stit : ¬[α stit : Q ]]. For example, Autumn Janerefrains from seeing to it that she becomes muddy comes to Autumn Jane sees to it that it is false that she sees to it she becomesmuddy. The two forms are easy for the ear to confuse, but the reflective eye can see that the advice to refrain fromseeing to it that one becomes muddy is much easier to follow than advice to refrain from becoming muddy. Parents andchildren alike doubtless rely on the ear’s confusion when they hash out the matter with each other after the dress issplattered by a passing truck.Give a modal logician a little nesting and more is wanted. The form [α stit : ¬[α stit : ¬[α stit : Q ]]], which maybe read as α refrains from refraining from seeing to it that Q , illustrates the nesting of refraining within itself. In thislanguage, the sample question noted above can be expressed as the Refref conjecture. [α stit : ¬[α stit : ¬[α stit : Q ]]] isequivalent to [α stit : Q ]. If the Refref conjecture is true, then the only way that Autumn Jane can refrain from refrainingfrom seeing to it that she becomes muddy is to see to it that she becomes muddy.The conjecture is perhaps not so exciting in itself, though I confess to a certain mud-pie fondness for it.In [4] the Refref conjecture is explored and shown to hold for both a-stit and d-stit (though the complexity of demon-strating the former far exceeds doing so for the latter). It might be expected from a purely formal point of view that therewould be an analog in the logic of S thus:[2, pp. 138–139].Sx¬Sx¬Sx A ≡ Sx AThe logic of T might be expected to be a little different (nested T statements are, after all, not even wffs), but brief inspectionyields the following candidate:Sx¬Sx¬Txα ≡ TxαDespite initial intuitions to the contrary, neither of these capture Refref. The problem arises from a superficial reading ofBelnap [2] (and Pörn) [39], and in particular, from mis-associating the rightmost stit in Belnap’s formulation with an Sx orTx expression. One indicator that something is awry is in Belnap’s description of “becoming muddy” as non-agentive: thoughit may be non-agentive, it is certainly action-oriented, even though the agent of the action is unspecified. (One cannot have“becoming muddy” as anything other than an action: even from a purely grammatical point of view, “becoming” cannotbe acting as a gerund after “refrains”, so it must be the participle, i.e., a statement referring to activity.) The contrast hedraws between that and the second formula is thus overstating the case: both [αstit : ¬Q ] and [αstit : ¬[αstit : Q ]] are58T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71Table 1Types of expressions of responsibility.Personalaction αxAction αState APositive responsibilityResponsibility for x doing αPerformance of αTxαxResponsibility for αAssurance of αTxαResponsibility for AEstablishment of ASx ANegative responsibilityResponsibility for x not doing αForbearance from αTx¬αxResponsibility for not αInterdiction of αTx¬αResponsibility for not AProhibition of ASx¬ Aconcerned with refraining from action, it is just that the first concerns an action in which α is a hapless victim (such asthe passing truck that splatters the dress), whereas the second concerns an action specifically of α. Translation of Refrefinto the logics of S and T is thus not simply a matter of translating αstit into Sα . The closest we might come to suchtranslation is to compare [αstit : Q ] with an action description such as Q α : the action of α seeing to it that Q (in Belnap’sterms) is analogous to the deed of Q being performed by α (in Hamblin’s terms). (We are not arguing for a translationthat maps perfectly either syntactically or semantically. The aim is only to use the comparison as a stepping stone toexpressing Refref). Unfortunately, though, for a simplistic attempt at persisting in this vein, an action description such asQ α cannot be further nested. The problem we are facing is that in Belnap’s account, responsibility for an action is exactlycoextensive with an agent’s performance of that action. In the logic of S and T, performance of an action is quite distinctfrom the responsibility for the execution of that action. Such a distinction is, of course, useful in capturing subtle issuesin the law (such as the concept of “diminished responsibility”) and is invaluable in accounting for delegation. But it makesunderstanding forbearance difficult: are we talking about forbearing from carrying out an action, or forbearing from takingon the responsibility?There are strong linguistic clues as to what is going on. One talks of “forbearing to do something” or “refraining fromsome activity”. That is, both forbearing (in this sense) and refraining predicate action. The logic of S and T, of course, isequipped to express and distinguish action and responsibility for action. One agent, x, simply happening to not performsome action α themselves is expressible as ¬αx. But as Belnap and Pörn have pointed out, this is not enough to capture theresponsibility involved in forbearing. For that, we need Tx¬αx: x’s (T–) responsibility for x not doing α. The non-agentive(or, more precisely, the implicitly universally quantified agentive) responsibility in Tx¬α is much stronger, capturing x’sresponsibility for the non-execution of α by any agent. This is capturing the notion of interdicting. Responsibility for agiven state of affairs not pertaining — or what we might loosely call prohibiting — can be straightforwardly captured bya third case: Sx¬ A. States of affairs are intrinsically non-agentive, and so the catalogue ends at this point, except to notethat for each of these expressions of "negative" responsibility, there is a dual that omits the negation (each of which hasbeen encountered in Section 2). The six types are laid out in Table 1. (In describing negative responsibility, the relationshipbetween the first and third cases could be emphasised terminologically by referring to Sx¬ A as forfending; similarly, thatbetween the second and third cases could be emphasised by having Tx¬α as proscribing; the terms in the table avoid undueemphasis on either pairing.)In this way, we can usefully break down the concept of responsibility into six distinct types: performance, assurance andestablishment, and their duals, forbearance, interdiction and prohibition. These terms are not ideal. Quite apart from theslight mismatches between their everyday meanings and the needs of the taxonomy here, there is a further terminologicalproblem in that the six all have their origins in verbs. Forbearing, interdicting and so on are all events, whereas thesedescriptions of responsibility are all states; for Belnap, forbearing is, of necessity, an action description, since his stit operatorranges only over action descriptions. Here, forbearance and its colleagues are treated strictly as species of responsibility, i.e.as state descriptions: the table’s more long-winded expressions of “Responsibility for . . . ” are more accurate if less concise.Where then does that leave the Refref conjecture in the logic of S and T? There are now three possible interpretations,depending upon how exactly one interprets Belnap’s concept of “refraining”. Given that both S and T expressions are them-selves state expressions, the first (i.e. outer) Ref of Refref must be negative responsibility with respect to a state, or whatwe have called prohibition in the table above. For the second (i.e. inner) Ref, each of the three types of negative respon-sibility are defensible candidates, giving rise to the three possible interpretations. The first is that Refref is prohibition offorbearance, i.e. Sx¬Tx¬αx, in which case our candidate for the conjecture isSx¬Tx¬αx ≡ TxαxIf we take Autumn Jane jumps in the muddy puddle and Autumn Jane avoids the muddy puddle as our αx and ¬αx, respectively,we can form a cumbersome accurate gloss thus: Autumn Jane is responsible for the state in which she is not responsiblefor herself avoiding the muddy puddle. More loosely, and more informatively, we have, Autumn Jane prohibits herself fromforbearing to jump in the puddle. Culpability in this case strikes the ear quite clearly.T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–7159The second interpretation is that Refref is prohibition of interdiction. This offersSx¬Tx¬α ≡ TxαIf this time we take the action α to be the throwing of a mud pie at Autumn Jane, we have an accurate gloss thus: AutumnJane is responsible for the state in which she is not responsible for the action of someone throwing a mud pie at her. Moreloosely, a gloss reads, Autumn Jane prohibits herself interdicting mud pie throwing. Her culpability seems here much harderto demonstrate. Can she be said to be responsible for the action of the mud pie throwing merely through her deciding notto interdict it? Intuition says not (assuming no other encouragement from Autumn Jane).The third interpretation is that Refref is prohibition of prohibition. This yields the equivalenceSx¬Sx¬ A ≡ Sx AWith the state A as Autumn Jane being muddy, the gloss is that Autumn Jane is responsible for the state in which she isnot responsible for being mud-free. More perspicuously, Autumn Jane is responsible for not prohibiting her muddiness. Thiscase is closely analogous to the previous. In both cases, Autumn Jane is working to avoid responsibility for staying clean(i.e. prohibiting her responsibility for cleanliness). In so doing though, she cannot be said to have taken responsibility forany subsequent muddiness. A further source of evidence that indicates that our intuitions hold up in the axiomatisationarises from T, by which Sx A yields A (i.e. S and T comprise a logic of successful action). Our intuitions suggest that neitherSx¬Tx¬α nor Sx¬Sx A ≡ Sx A require α or A to hold — merely that they might, and if they do it has nothing to do withAutumn Jane. In contrast, if she prohibits herself from forbearing to leap in the muddle, as in the first case, then we certainlyexpect her to do it — and to suffer whatever sanctions may then loom.The terms, “prohibition” and “interdiction”, in particular, have strong normative and legalistic connotations, so it is impor-tant to emphasise that the intended reading — and the logical forms — are entirely based on the actuality of the Hambliniansemantic frame, rather than upon a deontic ideality built upon it. That is not to say that there are not strong resonances:Lindahl’s [30] one-agent types and Hart’s [24] protective perimeter of rights provide exactly the right level of detail forexpressing whether or not an agent is permitted to forbear, and how other agents might impinge upon an agent that isprohibiting interdiction. The logic of S and T is designed with such normative accounts clearly in mind, but an explorationof their interrelations is beyond the scope of this paper.The Refref conjecture is somewhat reminiscent of the equivalence ¬¬P ≡ P in propositional logic; perhaps a critiquemight be expected to have an analogue in intuitionistic logic in which the equivalence is rejected. The reason for rejecting¬¬P ≡ P in intuitionistic logic is a consequence of the constructivist view of the Law of Excluded Middle: “the intuitionistreject[s] the platonistic notion of mathematical truth as obtaining independently of our capacity to give a proof”. [18, p. 18]The platonistic inevitability of A ∨ ¬ A is far less compelling when the disjuncts are ‘x does α’ and ‘x refrains from do-ing α’, so perhaps it is not surprising that a more fine-grained analysis is required for a logic that encompasses action andresponsibility.In conclusion, Belnap sees the Refref conjecture as a question: is it the case that the only way to refrain from refrainingfrom something is to do that something? If Refref holds, Belnap claims, the answer is yes. But on the current account, thereis more to be said: Refref is about getting one’s hands dirty. Mere responsibility, as captured by S and T is not enoughfor Refref to go through. The only way to prohibit one’s own forbearance is to execute the action oneself. But in the moregeneral case, it is possible to avoid responsibility of responsibility avoidance without having to commit the action oneself —it may, instead, come about through delegation, knowledge of the world, or mere chance. One may wish to go on to buildan account of, for example, legal culpability based upon causal knowledge, intent, malice aforethought, or other arbitrarilycomplex and nested representations of agents and their interactions with their environment. But equipped with nothingmore than a logic of action, it is possible to distinguish between the intuitively compelling direct involvement demandedby Refref for personal action, which forms a part of the logic of S and T, and is the motivation for accepting Refref in [4],and the much more suspect Refref for interdiction and prohibition which is rejected here, but cannot be disentangled andexcluded in the account of [4].3.3. Group delegation and joint responsibilityHere, we extend our theoretical model of responsibility to groups rather than individuals, and specifically, to the case ofan imperative being issued to a group. In other words, what does it mean for an agent to delegate (issue an imperative) toa group of agents, and what are the consequent responsibilities of each of the parties involved?In analysing group-directed imperatives, and hence approaching answers to these questions, let us start with a fewsimple examples:(i) “All of you stand up!”(ii) “Someone shut the door!”(iii) “Form a circle!”(iv) “Flip the switch!”60T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71The first two of these examples illustrate a key distinction between the group being addressed distributively and asa collective (the first reference to this distinction being, to our knowledge, in the insightful monologue by Rescher [43]).Rescher uses the terms “distributive” and “collective” groups; terms that are similarly adopted in this research [36]. Inexample (i), the group is being addressed distributively — each student should stand up — and in example (ii) the groupis being addressed as a collective — at least one student (but possibly all) should close the door. The third and fourthexamples are, at first glance, more troublesome. Intuitively, the request to form a circle may indicate all those addressed,but not necessarily; the imperative may very well be satisfied by five or six members of the group addressed forminga circle — the wording of the example is ambiguous — but it would be difficult to argue that a single agent would besufficient.2 The request to flip the switch is similarly challenging; given the assumption that flipping a switch changes thestate of an environment variable from one to another, the number of times that the switch is flipped is important in thesatisfaction of the imperative. This particular example may be criticised on the basis of formulation — why not request thatthe light (or whatever environmental variable is being referred to) be on/off — but it may very well be the case that animperative is issued to the effect that one and only one member of a group act in such and such a way. Or, for that matter,exactly two, or exactly three members of the group to whom the imperative is issued.It is important to note here that we make no assumptions within the model of any social or organisational context orstructure within the group to which an imperative is issued. In our view, the consideration of such structure (e.g. hierar-chies/teams that may exist within the group addressed) is outside a theory of delegation and responsibility, simply becausean imperative, issued to a (collective or distributive) group of agents, is issued to every member of that group regardless ofany additional organisational structure. This does not mean that organisational roles and relationships do not give weightto the delegation of an activity; this is an essential part of the context in which activities are delegated (see Section 4).Consider, for example, the CEO of a company issuing a directive regarding the company policy on (self-)certification of ill-ness. This is directed to all employees of the company regardless of their position. It may be that this imperative is issuedthrough the distribution of a memo — the mode of delivery is not important — but it applies to all those to whom it isdirected. Suppose, in contrast, that the CEO issues the imperative to her heads of department that they each reduce thecosts of their department by 10% over the following financial year. This is directed only to the heads of department. Theinstruction may provide weight to (give some justification for) any subsequent imperative issued by a head of departmentregarding the reduction of costs within their department. This subsequent imperative is, however motivated by the first, notthe simple transmission of the CEO’s imperative.In this analysis, let us start with the definition of S ˆX A, thus:Definition 6 (S ˆX A). All agents (in the set X ⊆ X ) are responsible for the achievement of the state of affairs A.def=S ˆX A(cid:6)Sx Ax∈ XThe question that we will consider here is whether this is sufficient to capture our intuitions about imperatives withrespect to the achievement of a state of affairs directed to a distributive group.Suppose that X is {Alice, John}; by definition for that group to be responsible for the achievement of A, both Alice andJohn must see to it that A. Suppose Alice and John are requested to make sure that their timesheets for the month arecompleted for tomorrow, the state R. In accordance with the above definition, this is equivalent to SAlice R ∧ SJohn R, and, ifuttered in an appropriate context, Alice and John would be suitably motivated to whole-heartedly satisfy this imperative,and take necessary action.Suppose that John fails to submit his timesheet. If Alice does submit her timesheet on schedule, can it be said thatthe original group-directed imperative has been whole-heartedly satisfied? Clearly not: SAlice R is satisfied, SJohn R is not.Thus, this definition does account for the idea that for the group-directed imperative to be satisfied, all those addressedmust whole-heartedly satisfy the individual imperatives that flow from it. This is, however, a rather weak notion of groupresponsibility; Alice is not responsible for John doing his part (and hence cannot be considered, in part, culpable if he doesnot), and, similarly, John has no directive to ensure that Alice contributes. Although it captures some aspects of what itmeans for agents to satisfy a group activity, it does not require that they act as a team.Consider an alternative, stronger, interpretation of the issuing of an imperative to a distributive group that emphasisesgroup responsibility.Definition 7 (S[X] A). Each member of the group X is responsible for each member being responsible for achieving the stateof affairs A. (An alternative formulation of this definition in terms of S ˆX is given in square brackets.)S[ X] Adef=(cid:6)x∈ X(cid:7) (cid:6)(cid:8)SxS y Ay∈ X(cid:9)or S[ X] A(cid:10)def= S ˆXS ˆX A2 A single wagon riding in a defensive circle is used as a gag in the 1974 Mel Brooks film Blazing Saddles, and is perhaps the exception that proves therule here!T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–7161Now, using the same example as above, the imperative S[{Alice,John}] R expands (reducing SxSx A to Sx A by T throughout)to: SAlice R ∧ SAliceSJohn R ∧ SJohn R ∧ SJohnSAlice R. This introduces a flavour of group responsibility for the satisfaction ofthe state of affairs concerned — as well as Alice and John being individually responsible for the submission of their owntimesheets, Alice is responsible for seeing to it that John submits his and vice versa. This distinction is exactly what isrequired in a model of group-directed imperatives, and this formulation of the imperative directed toward a distributivegroup illustrates the essence of our view of joint responsibility.3We now move to explore other forms of group-directed imperatives, starting with the variant of the S modality thatrefers to a group of agents as a collective. For an imperative directed to a collective group to be satisfied, at least one ofthe group members (possibly all of them) must be responsible for its achievement. A first attempt at a definition of thisgroup-directed imperative may, mirroring Definition 6, be to equate it with the disjunction of individual imperatives foreach member of the group, thus:Definition 8 (S ˇX A). At least one agent (in the set X being addressed), be responsible for the achievement of the state ofaffairs A.def=S ˇX A(cid:11)Sx Ax∈ XSuppose that Alice and John are instructed that at least one of them should prepare a presentation on next year’sfinancial plan for a meeting tomorrow. (In this example, although not in general, it would be sensible for only one of themto see to it that the presentation is prepared.) Following the initial definition above, this imperative, directed to a collectivegroup, would expand (where P indicates the state of affairs in which the presentation is prepared) to SAlice P ∨ SJohn P . Now,if John does not contribute to the presentation, can he be held to account for not satisfying the imperative as issued? Unlikethe imperative S ˆX A, we cannot say that John has failed to whole-heartedly satisfy the imperative. If Alice prepares thepresentation, then the imperative is satisfied and if she does not, it is not satisfied; in the former case John has done allthat he needs, and in the latter he has not. The fact that there is no notion of joint responsibility in this definition meansthat it is of little utility in capturing the concept of an imperative directed to a collective group.Mirroring the definition of S[X] A above, consider the following, stronger, interpretation of the issuing of an imperative toa collective group that emphasises group responsibility. (An alternative formulation of this definition in terms of S ˆX and S ˇXis given in square brackets.)Definition 9 (S(cid:7) X(cid:8) A). Everyone in the group X is responsible for at least one member of the group achieving A.S(cid:7) X(cid:8) Adef=(cid:6)x∈ X(cid:7) (cid:11)(cid:8)SxS y Ay∈ X(cid:9)or S(cid:7) X(cid:8) A(cid:10)def= S ˆXS ˇX AReturning to our example, the imperative instructing Alice and John to have a presentation prepared for the meetingtomorrow (S(cid:7){Alice,John}(cid:8) P ) will expand to the following: SAlice(SAlice P ∨ SJohn P ) ∧ SJohn(SAlice P ∨ SJohn P ). Alice is responsiblefor either herself or John (or both) ensuring that the presentation is prepared and John is similarly responsible. For eitherAlice or John to whole-heartedly satisfy this group-directed imperative, they must take into account the activity of the other.This, therefore, enforces cooperation in the satisfaction of the imperative issued.It is worth noting that this interpretation of an imperative issued to a collective group concurs with that defined byRescher [43, p. 59]. Using the example of a group of students being instructed to close the door, Rescher considers thefollowing alternative formulation: “do not let it occur that no one in the group [ . . .] closes the door” addressed distributivelyto the group. Consider this alternative formulation of the example considered here: “do not let it occur that neither Alicenor John are responsible for the presentation being prepared”. This can be expressed as the following re-writing of theexpansion of S(cid:7){Alice,John}(cid:8) P : SAlice¬(¬SAlice P ∧ ¬SJohn P ) ∧ SJohn¬(¬SAlice P ∧ ¬SJohn P ). In the following, however, we willpropose further refinement on the types of group-directed imperatives that may be expressed so that we can capturevarious constraints that the issuer of an imperative may place on the group so addressed. Before doing this we define theaction-oriented analogues of the state-oriented group-directed imperatives defined so far.The group-directed imperatives T ˆX α, T[X]α and T(cid:7) X(cid:8)α are similar to their equivalents for the S modality, and are definedas follows:Definition 10 (T ˆX α). All agents (in the set X ⊆ X ) are responsible for the performance of action α.T ˆX α def=(cid:6)Txαx∈ X3 Once the imperative has been issued and the responsibility established, or the responsibility established in some other way, the means by which agentscoordinate to live up to that responsibility is an important and complex topic [5,25], but not one that is within the scope of this paper.62T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71Definition 11 (T[X]α). Each member of the group X is responsible for each member being responsible for the performanceof action α. (An alternative formulation of this definition in terms of S ˆX and T ˆX is given in square brackets.)T[ X] Adef=(cid:6)x∈ XSx(cid:7) (cid:6)(cid:8)T yαy∈ X(cid:9)or T[ X] Adef= S ˆXT ˆX A(cid:10)Definition 12 (T(cid:7) X(cid:8)α). Everyone in the group X is responsible for at least one member of the group ensuring that α is done.T(cid:7) X(cid:8) Adef=(cid:6)x∈ XSx(cid:7) (cid:11)(cid:8)T yαy∈ XWe are now in a position to provide a generalised definition of group-directed imperatival utterances with respect toresponsibility for the achievement of states of affairs and with respect to the performance of acts. In doing so, we furtherrefine the notion by introducing the concept of a “minimal acceptable team” with respect to the group activity. By this wemean that minimum team size constraints can be placed on the group activity. For example, we can express the imperativeissued to a group of n individuals that they form a circle comprising of at least 5 individuals.Definition 13 (S( X,n) A). Everyone in the group X is responsible for there being established a team comprising of at least nindividuals, where 0 < n (cid:2) | X|, such that team is responsible for the achievement of A.S( X,n) Adef=(cid:6)x∈ XSx(cid:7) (cid:11)(cid:7) (cid:6)(cid:8)(cid:8)S y AY ∈2 X , |Y |=ny∈Ywhere 0 < n (cid:2) | X|The following special cases then follow from Definition 13:• S( X,| X|) A ≡ S[X] A. In this case, the only permissible team is that which consists of all those addressed.• S( X,1) A ≡ S(cid:7) X(cid:8) A. In this case, the team may contain one or more individual; i.e. the imperative is satisfied if all thoseaddressed ensure that at least one individual in the group (possibly all of them) is responsible for A.• S({x},1) A ≡ SxSx A and by TS, SxSx A → Sx A.Thus, this definition provides a general definition of individual- and group-directed imperatives with the added advantagethat the issuer of the imperative can place minimum cardinality constraints upon the group required to achieve the goalindicated.An equivalent definition for individual- and group-directed imperatives with respect to an action to be performed maybe constructed.Definition 14 (T( X,n)α). Everyone in the group X is responsible for there being established a team comprising of at least nindividuals, where 0 < n (cid:2) | X|, such that team is responsible for the achievement of α.T( X,n)α def=(cid:6)x∈ XSx(cid:7) (cid:11)(cid:7) (cid:6)(cid:8)(cid:8)T yαY ∈2 X , |Y |=ny∈Ywhere 0 < n (cid:2) | X|It is worth illustrating these definitions with a concrete example. Suppose that two or more members of a group of(cid:2)Sxthree, {x, y, z}, should attend a meeting (action μ). This can be expressed as T({x, y,z},2)μ, and expands to:(cid:3)(Txμ ∧ T yμ) ∨ (Txμ ∧ Tzμ) ∨ (T yμ ∧ Tzμ)(cid:2)(cid:3)(Txμ ∧ T yμ) ∨ (Txμ ∧ Tzμ) ∨ (T yμ ∧ Tzμ)(cid:3)(Txμ ∧ T yμ) ∨ (Txμ ∧ Tzμ) ∨ (T yμ ∧ Tzμ)S y(cid:2)Sz∧∧Each line (i.e. each outer conjunct) expresses, loosely, each agent’s joint responsibility to the minimum acceptable team.The different configurations of that minimum acceptable team are then expressed by the disjuncts. Of course, all three mayattend the meeting, which simply leaves multiple configurations met.We might imagine a situation in which x and z agree to attend the meeting. If z falls sick in the interim, and cannotattend, the second and third disjuncts of each conjunct are assuredly false, and x and y might reason normatively that thefirst disjunct must be fulfilled, i.e. that they are the two that must attend. This brief and informal scenario offers a firstexample of how an agent might acquire responsibility: an issue to which the next section turns in more detail.T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71634. Acquiring responsibility4.1. Acquiring responsibility through actionThe simplest way for an agent to be responsible for something is through direct action. That is, if an agent carries out anaction then, ceteris paribus, that agent is responsible for the execution of that action. Perhaps the simplest way to capturethis is to construct an axiomatic representation that links successful action to responsibility for that action:Definition 15 (RNR).αxTxαxAt first glance, this appears to be re-introducing necessitation, RN, by the back-door, after having worked so hard toexclude it in the development of the syntax and semantics, in order to preserve Hamblin’s intuitions. But the resemblanceis superficial. RN captures the relationship between a proposition, unrelativised, and relativised responsibility. What Jonesand Sergot found so galling — and what Chellas found so intuitive — was that logical truths should be the responsibility ofany agent whatsoever. What (RNR) is capturing, however, is a much more limited notion of what Jones and Sergot referredto as building a “logic of successful action” — namely, that a specific agent’s action should imply responsibility for that actionby that agent. Although it has formal similarity to RN, the relativised version RNR is a much weaker notion.Even this, however, is a little too strong. There are cases where agents of direct actions may, arguably, be absolved ofresponsibility for their actions. This is perhaps clearest in the legal notion of diminished responsibility. There are examplesin some legal systems of cases in which defendants have argued that their actions were not voluntary, or were carriedout when not of sound mind, and that therefore culpability is reduced or eliminated. This caveat suggests that the ruleexpressed by RNR needs to be weakened. A defeasible or default logic would be a good candidate for capturing the intuitionthat usually, committing an act incurs responsibility — but that exceptions may be conceivable. This might be representedin the usual default style as,Definition 16 (DefRNR).αx : TxαxTxαxIt is perhaps tempting to try to push further in this direction, and in some way associate execution of actions withresponsibility for their consequences. Under certain conditions, one might want to try and enforce thatTxαx → F A(i.e. that responsibility for the execution of a given action α has some effect A that holds at some point later), and thencethatif (cid:23)M Txαx → F A and (cid:23)M Txαxthen (cid:23)M ¬Sx¬F ASuch a relationship appears to be close to Hamblin’s notion of an agent’s partial i-strategy for ensuring that the contentof some imperative is not prohibited. The attempt, however, is wrong-headed. What these relationships are defining is alogical relationship between actions and their consequences. Though there is obviously some relationship, Hamblin is atpains to point out that there is an important gap between, as he puts it, logically possible worlds and physically possibleworlds. Enshrining the link between actions and their effects in a logical, axiomatic relationship is, for Hamblin, too strong.It also risks conflating the very distinction he constructed at the centre of the theory. As a result, we here leave the linkbetween S-formulae and T-formulae undefined, leaving it to a practical reasoning component to implement an appropriatemodel of the link (i.e. to implement an appropriate model of causality). One important advantage of this approach is inconsidering the ethics of delegation; an issue to which we return in Section 5. Given the simplicity of the S and T languageit is relatively straightforward to map from a number of traditional approaches to the sort of reasoning required. At oneend, there are purely logical approaches that seek to construct a model of epistemically embedded rationality, in whichintentions drive the generation of goals that in turn drive (or prune) action selection (such as [48]). At the other, areengineering-oriented approaches that aim to construct and modify plans based on causal reasoning (such as POCL plannersthat trace their ancestry back to UCPOP [38]). Both these classes of approach have clear representation of actions or states(or, in a limited sense, both), and assume that the “raw materials” of descriptions of those states or actions are available. Itis those raw materials that are described here.It may be that the richness of Hamblin’s underlying model — and possibly its formalisation in [42] that is employedhere — are sufficient to support a detailed analysis of the interactions between actions, their effects, and foreknowledge ofthose effects. Hamblin’s description of W phys and its position with respect to logical and temporal possibility suggests thatthis may be so. But such an account is beyond the scope of the current paper and is not a necessary prerequisite for theconstruction of a logic of delegation.64T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–714.2. Acquiring responsibility through delegationFig. 4. The imperative-normative-action cycle.Hamblin’s original motivation for developing action state semantics was to be able to explain imperatives and constructa logic that was designed to cope with them. We are now at a point to return to that motivation. The aim here, broadly, isto explain the five-step process (Fig. 4) from (i) the issuing of an imperative, to (ii) a deontic state in which requirements ofresponsibility are assigned, then (iii) an agent’s whole-hearted satisfaction of that imperative, which (iv) leads to the agent’sresponsibility for the state or action thereby (v) discharging of the deontic demand.Broadly, an agent acting within a normative environment follows the pattern shown in Fig. 4. The action cycle comprisesan agent recognising and selecting one or more norms to meet, and then, in Hamblinian terms, maintaining partial strategiesappropriate to the satisfaction of those norms. Selection of deeds is then constrained by those strategies. Communicationis one specific type of action, and uttering an imperative is one specific type of communication. Issuing an imperative canupdate the normative state to introduce some new norm, which is often one that affects some other agent. The selectionof deeds can be compared to some standard and conformance testing applied, resulting in sanctions being applied if con-formance testing fails. Here, we are interested in the central path from an agent’s strategies to the deeds it selects that areconsistent with those strategies, and in particular, communicative deeds involving imperatives, which in turn update norms.A key notion in understanding this path is Hamblin’s concept of whole-hearted satisfaction.4.2.1. Whole-hearted satisfactionWholehearted satisfaction is based upon the notion of a strategy. A strategy for a particular agent is the assignment of adeed to each time point. A partial i-strategy is then a set of incompletely specified strategies, all of which involve worlds inwhich i is extensionally satisfied. The wholehearted satisfaction of an imperative i by an agent x is then defined as beingx’s adoption of a partial strategy and the execution of a deed from that strategy at every time point after the imperative isissued.A Hamblinian world w ∈ W is defined such that for every time point in T there is:(1) a state from the set of states S,(2) a member of the set H of ‘big happenings’ (each of which collects together all happenings from one state to the next),and(3) a deed (in D) for every agent (in X ), i.e. an element from D X .The set W of worlds is, therefore, defined as (S × H × D X )T . The states, happenings and deed-agent assignments of agiven world w are given by S(w), H(w) and D(w).The next step is to let jt be a history of a world up to time t, including all states, deeds and happenings of the world upJ t is then the set of allto t. Thus jt is equivalent to a set of worlds which have a common history up to (at least) time t.possible histories up to t; i.e. all the ways in which the universe could have evolved up to time t. A strategy qt is then anallocation of a deed to each jt(cid:17) ∈ J t(cid:17) for every t(cid:17) (cid:3) t.4 Q t then denotes the set of all possible strategies at time t.Let the possible worlds in which the deeds of agent x are those specified by strategy qt be W strat(x, qt), and the worlds inwhich an imperative, i, is extensionally satisfied be W i . A strategy for the satisfaction of an imperative i (i.e. an i-strategy)can, therefore, be defined as follows: A strategy qt ∈ Q t is an i-strategy for agent x if and only if the worlds in which x doesthe deeds specified by qt are also worlds in which i is extensionally satisfied: W strat(x, qt) ⊆ W i .4 This notion of a strategy has an intensional component, since it prescribes over a set of possible w, rather than picking out, at this stage, the actualworld.T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–7165In practice, however, it is not feasible for an agent to select a particular strategy in Q t at time t that specifies everyafter t. For this reason, an agent will adopt a partial i-strategy. A partial i-strategy is a disjunction of(cid:17)⊆ Q t , and the set of worlds in which x adopts this partial i-strategy is W strat(x, Q(cid:17)t ).deed for every time ti-strategies, Q(cid:17)tWith this grounding, the wholehearted satisfaction of an imperative, i, can now be defined. An agent x may be said towholeheartedly satisfy an imperative i issued at t if and only if for every t(1) x has a partial i-strategy, Q(2) x does a deed from the set of deeds specified by that Q(cid:17)t(cid:17) ; and(cid:17)t(cid:17) .(cid:17) (cid:3) t:For further details, the reader is referred, of course, to Hamblin’s original monograph [23], and also to [53], whichprovides more detail on the role of such a model in the wider context of dialogue and, in its appendix, a more completeset-theoretic précis of Hamblin’s model.4.2.2. Responsibility from whole-heartedly satisfyingThere are at least two ways in which an agent might be said to be responsible for something. The first, as we have seenin Section 4.1, is by direct execution. If an agent, x, is the direct executor of some action, α (i.e. αx ∈ D), then, other thingsbeing equal, that agent can be said to be responsible for that action. Responsibility, however, is a broader notion that is wellmatched by Hamblin’s construction of whole-hearted satisfaction. The same sense of necessary involvement is pivotal, or,as Hamblin puts it, “[Something cannot count] as wholeheartedly satisfied if it is possible to say of it, He wouldn’t have doneit if it hadn’t been for so-and-so, or, It only came about by accident, or It would have come about anyway, what he did was irrelevantto it (or impeded it). Conversely, even when extensional satisfaction is lacking we sometimes want to say, Yes, but it wasn’t hisfault, or He did everything he could.” [23, p. 155].The whole-hearted satisfaction of some normative state of affairs (that was brought about by an imperative, or possi-bly some other societal or contextual process) thus represents a more general picture of agentive responsibility, of whichDefRNR is a specific instance. One way for an agent to whole-heartedly satisfy some expression is by building and main-taining a strategy in which, at an appropriate moment, the agent is a direct executor of an action that corresponds to theexpression. Even such postponed involvement may be unnecessary, however. An agent may be able to be responsible forsomething simply by establishing an appropriate normative state — specifically, by issuing an imperative. In the contextof an imperative, the first link between the ST-notion of responsibility and Hamblin’s wholehearted satisfaction starts toemerge. The T-formula, for example, that captures the statement of responsibility of a recipient of an imperative can be saidto constitute wholehearted satisfaction of that imperative. Having statements in the logic ST constitute statements of whole-hearted satisfaction in this way is a crucial step in understanding how delegation can effect the transfer of responsibilityvia imperatival utterance. It is to a more formal understanding of this relationship that we now turn.4.2.3. Delegation by imperatival utteranceGiven an appropriate social context, one agent may alter the normative state in some way for another agent. One mech-anism for effecting such a change is through an act that constitutes imperatival communication. To demonstrate how suchcommunication might itself constitute whole-hearted satisfaction of some other norm, we use a small example.Consider an agent x that is obliged to submit a report, action α, as a requirement of the position it holds in the organi-sation. As a consequence of this requirement, x forms an intention that α should be carried out. Let us further assume thatx’s reasoning mechanisms determine that proactivity is appropriate; i.e., that it should be responsible for α being carriedout, Txα. Let us imagine that x has no resources available for performing α itself, but that there is another agent, y, overwhom it has authority. In transforming its intention into a partial strategy, agent x might issue to y an imperative i express-ing that y should ensure that α is done. We use an abstract form for this imperative, where the predicate ! is instantiatedby a request or a command or some indirect speech act, to convey responsibility for α; we continue to use the superscriptconvention to express agentive execution, so that !x is an imperative issued by x. The addressee of the imperative can bemarked as a subscript, !xy , but to keep things syntactically simple, we adopt the convention that if not marked explicitly, theaddressee is taken to be the agent (or group) to which the S or T modality is relativised. Thus the imperative comes out asi = T yα!x. Given the appropriate context (the existence and mutual knowledge of the authority relation, for example), thisutterance creates a new normative expression such as SCOMM(x, y, T yα). (We use the notation of [8] because it is simpleand intuitive, but any appropriate language might be substituted.) This normative state in turn influences agent y whichdevelops (or, more accurately, is obliged to develop) a strategy for fulfilling i, namely, a partial i-strategy.There may be any number of ways for y to whole-heartedly satisfy i, i.e. there may be a diverse set of partial i-strategiespredicating different actions for y. The most straightforward is a strategy that requires y to perform α directly. At thatpoint (i.e. in that world), α y becomes true, and, by DefRNR, thence T yα y . Clearly, α y provides extensional satisfaction ofthe imperative i, but it is T yα y that constitutes the whole-hearted satisfaction of i — we might mark this syntactically aswhs( y, i), or, more explicitly, whs( y, T yα!x).What we need in order to close the loop is a relationship between whole-hearted satisfaction and delegated responsibil-ity, which we are now in a position to define axiologically, with the rule of inference for satisfaction, RS:66T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71Definition 17 (RS).whs( y, T yα!x)SxT yαIn this way, RS delivers a statement of responsibility that corresponds to the whole-hearted satisfaction of an imperative.The result of this inference then yields two conclusions. The first, unsurprisingly, is the responsibility of the subordinateagent, y, because by T, SxT yα gives T yα. Of course, in the example we are looking at here, T yα is inferable from T yα yby generalisation [42]. But in the general case in which whole-hearted satisfaction has been achieved by means other thandirect execution, T yα may not hold — so it is encouraging, therefore, that it is inferable by other means.More importantly, SxT yα also gives, by our axiom of delegation QT, that Txα. In other words, x’s responsibility fory’s responsibility for α being executed implies x’s responsibility for α simpliciter. This is vital, because it provides themechanism by which x can reason (for example, by planning or backchaining) that issuing the imperative T yα!x will serveits intention of being responsible for α (i.e. of Txα).The route from x’s intention Txα, generating the imperative T yα!x, causing the social commitment SCOMM(x, y, T yα),which in turn obliges a combination of the extensional satisfaction of α and a partial i-strategy (such as the one bywhich α y , and therefore T yα y ) that delivers whs( y, T yα!x), which in turn implies SxT yα yielding fulfilment of the orig-inal intention is the model of delegation supported by the logic of ST, and is summarised in Fig. 5 (and the path throughthe partial i-strategy labelled (1)).4.3. Multi-step delegationSection 4.2.3 described one example of a simple partial i-strategy that an agent might adopt in meeting the demandsof whole-hearted satisfaction. There are, of course, any number of such strategies that an agent might adopt in such asituation. One interesting alternative is where the agent decides to delegate the task further. So, in our earlier example,y may decide that submitting the report is best done by a subordinate, agent z, to whom y must delegate the task.Thus, the social commitment SCOMM(x, y, T yα) in Fig. 5 can be seen in the second partiali-strategy to generatethe intention in y that T yα. That intention in turn generates the imperative Tzα! y , causing the social commitmentSCOMM( y, z, Tzα), which in turn obliges a combination of the extensional satisfaction of α and a partial i-strategy for z,such as the one by which z performs α, i.e. α z and therefore Tzαz) that delivers whs(z, Tzα! y), which in turn implies S y Tzαyielding fulfilment of y’s original intention. In other words, the entirety of Fig. 5 can be embedded as the partial i-strategyfor y (with z substituting for y, and y for x).This embedding of one delegative step within another is exactly what one would hope for, since multi-step delegationof this kind is a very natural activity. There are, however, cases in which delegation of this sort is undesirable. Let usmodify the example from Section 4.2.3 slightly, and imagine that agent x delegates the work on the report to y becausey is the best report writer in the team. In such a situation, x explicitly does not want y to delegate the task further: howcan x effect such delegation? The answer lies in considering a “guard” condition that is added to the imperative: not onlymust the addressee be responsible for the action, but, furthermore, they must not be responsible for further delegation.There are strong echoes of Lindahl [30] and Hart [24] here in constructing the perimeter of rights using guards in thisway. The challenge is that, as we have seen, there is no identifiable “delegate” action that can be prohibited (and norwould we want there to be such an action; it seems self-evident to us that it is important to allow delegation to beachieved through many if not all of the primitives or mechanisms of an existing communication language). Nor can wecircumvent the absence of a delegate action by trying to identify a unique part of the state that can be identified withthe postcondition of delegation (apart from anything else, this would conflate the action/state distinction introduced byHamblin and preserved in the language of ST). A final complexity lies in the fact that just because one agent x is prohibitedfrom delegating an action α to some other agent y, there is no reason to assume that y might not already have receivedimperatives, or otherwise be socially committed to various actions — including, perhaps, α. The prohibition on x’s freedomcannot, therefore, be expressed solely in terms of the (potential) commitments of the agents to whom it might (potentially)delegate.The solution turns (again) upon Hamblin’s notion of whole-hearted satisfaction, and its link to responsibility. By ex-pressing the guard as itself an expression of the addressee’s responsibility, whole-hearted satisfaction does not precludeextensional satisfaction by other means. Thus, for our running example, if x wishes to delegate α to y in such a way thaty does not delegate further, the appropriate imperative is (T yα ∧ ∀z¬S y Tzα)!x. In this way, y is directed to take on re-sponsibility for α, but in addition (in the guard) is required not to be responsible for the state in which any other agentis responsible for α. Of course some other agent may, in fact, wind up with such responsibility (or may have it already),but so long as that responsibility has not itself been brought about by y (i.e. is not y’s responsibility), then the compoundimperative can still be whole-heartedly satisfied.Finally, the reverse situation is also possible: x may wish to direct y as to execution of α, for instance by demandingthat it be delegated further. Examples of explicit multi-step delegation imperatives typically involve some jump in a chainof command, from superior to inferior — e.g., “No, I’m sorry, this won’t do. I need you to find someone who can do theplastering properly”. Again, no delegate action is required in order to capture this, merely an expression such as ∃zS y Tzα!xT.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–7167Fig. 5. A logical account of delegation.(where T yα is inferable from S y Tzα by QT). Of course, such interference with the addressee’s fulfilment of the imperativeis not limited to demands for further delegation: the speaker could indicate means, timescale, or groups, amongst guardconditions; the speaker could also, using the language of Section 3.2 demand not just performance or assurance of actionsor establishment of states, but also forbearance, interdiction or prohibition. We emphasise the delegate/no-delegate guardsin particular to lay a foundation upon which future work can build imperative-based characterisations of Lindahl- andHart-style contractual relationships.This concludes our presentation of a logic of delegation that answers many of the most challenging questions in buildinga comprehensive model of the concept. The issues surrounding the locus of responsibility for delegated activities, the Refrefconjecture and the complexities of group-directed imperatives have been addressed, characterisations of these concepts havebeen offered, and mechanisms whereby responsibility may be acquired have been presented. In the following section wediscuss some important related research, attempting to capture the breadth of work in this important area. Our conclusionsfollow in Section 6.5. Related workComparison with Belnap’s conception of Refref has been explored in detail in Section 3.2, and differences between theunderlying language and that of the stits in [42]. Belnap et al.’s [4] rich and detailed theory also explores imperatives andthe links between stit, communication and deontic states (although does not explore delegation explicitly). One interestingobservation by way of comparison with part of this work is their discussion [4, Ch. 12] of Marcus’s [32] “unpretentious”example, Parking on highways ought to be forbidden. For Belnap et al. the interpretation of this example is:Oblg:[Γ dstit: ∃β[(β ∈ Γ ) & [β dstit: Sett: Will-always:∀α∀x Frbn-if-can-do:[α dstit: P αx]]]]The first part of this identifies an individual actor, β, from a particular group, Γ (“the authorities” or “they”), and the nextpart the temporally and spatially quantified prohibition (all agents α forbidden from parking P on a highway x). The theory68T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71presented here has little or nothing to say about the deontic aspect of this example. It does, however, allow us to describethe appropriate states, i.e. to focus not upon the ideality but instead upon the actuality (of the authorities’ responsibility foreveryone’s responsibility for not parking on highways). By making use of both collective and distributive groups, and thedistinction between state and event-oriented responsibility, we have: S(cid:7)Γ (cid:8)G∀xT[ A]¬P x (where the set A is taken to be thedomain of α, which is left implicit in Belnap et al.’s interpretation).The work of Governatori and colleagues is also worthy of mention here in relation to the logical underpinnings of ourmodel of delegation. Governatori and Rotolo [21], for example, elegantly fix Elgesem’s [19] account of action which in itsSuccess and Non-accidence conditions is similar to the approach presented here. In Avoidability, however, Elgesem, andGovernatori and Rotolo diverge from the RT-type logic that gives our account of delegation its rich notion of responsibility.We have discussed elsewhere [42] how the underlying formal model compares with that of Singh [44,45] whose pro-gramme of research represents one of the most significant, sustained investigations of the area in AI. Although Singh [46]also takes Hamblin as inspiration and his starting point, he does not address delegation directly in the context of his workwith WSAT. Delegation does appear explicitly in his more engineering-oriented work [47], where the types of delegation inwhich we are interested appear explicitly as structural patterns. To suit the target audience for that work, they are not tiedto underlying formal explication and are highly simplified (being reduced to atomic actions). Finally, Singh also comments(personal communication) that he does not like the term “imperative” because it is based on natural language syntax ratherthan semantics, and thereby conflates the diverse semantics of directives, permissives and prohibitives. For the work herewe are keen to disentangle the semantics of a given communicative utterance from the subsequent normative state, so thatvery diversity of semantics aids our purpose. (It is also convenient to be able to retain the term for its intuitive simplicityand to retain the explicit link to Hamblin’s original work.)Further discussion of research related to our underlying logic is given in [42], here, however, we focus more on relatedwork within multi-agent systems.Pacheco and Santos [37] formulate the issue of delegation in the context of role-based organisations, and, like us, relyupon a deontic characterisation of state to contribute to an understanding of how delegation is achieved. Unlike the modelpresented here, however, Pacheco and Santos place heavy restrictions on delegation, including, for example, requiring thatthe delegator “also transfers [. . . ] all the resources required”, presupposing both that those resources exist, that the delega-tor is aware of them, and has the power to require the delegates to employ them. This, of course, arises ultimately fromconflation of delegation of state (which can leave methods unspecified) and delegation of action (which does not). Pachecoand Santos also model joint agency only in respect of the delegator (which is achieved through a simple conjunction overcontributing agents): the more complex delegation to multiple agents would, for Pacheco and Santos, require formation ofa institutional agent and some separate system for distributing responsibility in that institution. We argue, and have shown,that the process of delegation is intimately tied to individual and joint responsibility, and that a definition of either isincomplete without reference to the other. Finally, Pacheco and Santos, reasonably enough, limit themselves to a deonticcharacterisation of delegation in isolation from features such as motivation, reasoning and communication. We have demon-strated that by using as a basis a rich account such as Hamblin’s that includes communicative and social aspects, we canconstruct a model of the entire cycle to explore what happens, for example, in cases of multiple-step delegation.Also within the context of agent reasoning mechanisms, Boella and van der Torre [6] describe the “social delegation cycle”which can be seen as a generalised version of the imperative-normative-action cycle presented in Fig. 4, and though both [6]and [7] are rather limited in their treatment of delegation as transfer of responsibility, they provide a rich multi-modal logicthat provides a context in which much of the work in the current paper could be set. Van der Hoek and Wooldridge [50]offer a mechanistic account of delegation that uses dynamic logic to describe how the control of propositional variablescan be passed between agents. That control is equated with power (i.e. power to alter the Boolean value of the variable),and transfer of control with delegation. Although this approach provides an interesting abstract model and may be a wayto implement some kinds of simulations of delegation (through work such as the Logic Programming-oriented [29]), itfails to handle a number of issues that are central to the model presented here, viz., delegation to groups, delegation withunspecified means, responsibility across delegation, the interactions between communication and delegation, and so on.Kumar et al. [28] make the distinction between “a group doing an action as an entity (or meta-agent)” and “everybodyin a list of individuals performing an action”. This distinction is possibly due to the formulation of their generalised requestaction. In specifying the generalised request action, Kumar et al. focus on the distinction between agents to whom a requestis directed and those who simply overhear it. This permits the definition of a request using Cohen and Levesque’s mentalisticlogic [12] such that the issuer does not know who the intended actor is within the group that the request is directedtowards. A request to a group treated as a “meta-agent” can then be defined, but the model is also able to capture requestsdirected to individuals as before. There are a number of limitations of this approach including the underlying logic, whichrelies on pseudo-states that express that specific acts have been done (see Reed and Norman [42] for more discussion onthis issue). With respect to capturing the concept of group-directed imperatives, however, Kumar et al. [28] go little furtherthan Rescher [43] who presents a thorough discussion of the importance of distinguishing between imperatives directedtowards a collective or distributive group and a practical model of these concepts. Kumar et al. do, however, point towardshow agent communication language specifications grounded on belief-desire-intention logics can be extended to refer togroups of agents. A more significant contribution in this regard, although only capturing the distributive case, is the modelof establishing collective intention through dialogue proposed by Dignum et al. [17]. This model represents one of the firstattempts to tie together a form of dialogue (in their case a form of persuasion dialogue based on RPD [53]) to resultantT.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–7169mental states of the participants (in their case an intention that is common to all agents within the group engaging inthe dialogue). Davis and Morgenstern [15] tackle the link between communication and planning in a multi-agent settingin a formal but pragmatic manner. Rather than addressing the issue of how one agent persuades another to adopt anintention, they assume agents are cooperative and pre-allocate time slots to others in anticipation of requests for plans tobe executed. David and Morgenstern’s contribution is in formally capturing some of the more simple forms of delegation,including group-directed delegation, considered in this paper and in the links with a model of knowledge. One of theexamples involves an agent issuing a request to a group of agents, one of which accedes to the request. The question of thegroup collectively deciding which agent accedes to the request is, however, not considered; in the example used by Davisand Morgenstern there is a unique resource that is requested, and hence the only agent to respond is the one that has thatunique resource; i.e. the question of how a group coordinates to respond to a request is not considered. By bridging thegap between requests and planning, however, Davis and Morgenstern [15] do offer a possible realisation of the notion ofwhole-hearted satisfaction: the responsible agent accedes to requests by adding actions in their plans (during one or moreof the time slots pre-allocated to the requesting agent) and ensuring that no other action interferes with the satisfaction ofthis request.One issue that Rescher [43] points out, is that not only is it possible that the recipient of an imperative (or any othercommunicative act) be a group, but the issuer (or source) of that act may be a group (collective or distributive). Rescher [43]uses a number of examples to illustrate the various possibilities. These include: “Group (Collective) to Group (Collective) Courtorder to a corporation to divest itself of certain holdings (in violation of antitrust statutes)” [43, p. 13]. Jones and Sergot [26]use similar examples to illustrate their “counts as” connective; for example, “x’s uttering the words ‘I pronounce you manand wife’ counts (in [society] s) as a means of guaranteeing that s sees to it that [the recipients of the declaration] aremarried”. Jones and Sergot do not, however, confine their theory to communicative acts, but present a general theory ofagents acting on behalf of a group. An analysis of the utterance of an imperative by an individual on behalf of a group may,therefore, be related to Jones and Sergot’s [26] notion of “counting as in a society”. This is a necessary element of a completetheory of delegation, and is an important avenue for future research, some first steps in which have been presented in [51].On commands in dialogue and the context of the issuing of a command, Rescher [43] states that, “[g]enerally speaking,the source should have some entitlement or authority for giving a command to its recipient”. This means that a command(or any imperative) could be questioned by its recipient regarding the authority of the source and the grounds for it beingissued. Understanding how the issuing of an imperative fits into the wider structure of inter-agent dialogue may influencethe design of flexible agent communication protocols. Recently this issue has been addressed by Atkinson et al. [1]. Buildingupon the earlier work by Girle [20], Atkinson et al. present argumentation schemes [54] for the issuing of commands withindialogues and demonstrate how these may be employed within a broader command dialogue protocol (CDP) and in thecontext of organisational relationships between dialogue participants. This represents an important first step to building acomplete, pragmatic theory of imperatives in dialogue, which complements the model presented in this paper.Castelfranchi and Falcone [9] pose a number of interesting questions that a theory of delegation must answer, focussingon the social (or organisational) context within which delegation takes place. These questions serve to clarify what, intheir view, is required for a comprehensive theory. They address the nature of the object of delegation, the nature ofthe relationship between the parties involved, the autonomy of the agent to whom a task is delegated, what is meantby “on behalf of”, and the issue of trust between the parties. For Castelfranchi and Falcone, the object of delegation is atask, goal pair, capturing their intuition that it is only meaningful for delegation to refer to a specific act along with theoutcome that this act is required to produce. In contrast, our model allows agents to delegate specific acts (captured by theT modality) without requiring reference to the intended outcome, or to a specific outcome (captured by the S modality)without specifying the means by which this outcome is to be achieved (an approach also advocated by Morgenstern [34]).In this respect, our model is more flexible; it is possible to tie tasks and goals intimately together by constraining the modelin this way, but this is neither necessary nor essential.Separating the object of delegation from its consequences, or from its side-effects, is also important when issues of ethicsare a concern [27]. Consider, for example, a failure in an automated aircraft control system leading to loss of life in a crash(the cause of the failure having been determined by an inquiry). The plane manufacturers and maintenance engineers (as acollective group) were responsible for providing an effective and safe aircraft. Suppose that the conclusion of the inquiry wasthat the plane manufacturers have failed to fulfill their responsibility. To draw this conclusion, however, a domain-specificmodel of causality is required. It might be argued that responsibility lies with the aircraft designers if the design was atfault, but this does not take into account testing and other phases of development. Responsibility for the side-effects ofactions (given a specification of the actions within a domain) has been explored by Grossi et al. [22]. In this complementaryresearch an agent can be said to be responsible for causing some state of affairs φ to hold if it just performed some actionα and φ would not have necessarily been the case if α was not performed by that agent. Such causal inference is essentialfor a model of reasoning about agentive responsibility, in the sense of what an agent can be held responsible for. It is, however,not something that should be a component of a logic of delegation and agentive responsibility, in the sense of what an agentis responsible for, following the issuing of an imperative.The model proposed by Castelfranchi and Falcone also fails to address the important issue of delegation to groups ofagents, something that is essential for a comprehensive theory, as argued in this paper and by others. One of the importantemphases of the theory developed by Castelfranchi and Falcone is the link between delegation and the mental states ofthe participants that are required (for a decision to delegate to be made) and consequent (i.e. resultant). As noted by Lorini70T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–71et al. [31], this link is not something that has featured strongly in our model until now; a limitation that we have partiallyresolved in Section 4. There is further research to be conducted in this area, however. The link between motivations fordelegating tasks and assessments of the competencies of agents to whom a task is delegated, for example, has not beenfully explored in this paper (although there is a substantial relevant literature in the area of trust [41]). To give a concreteexample, it may be reasonable for a teacher to delegate a task to a group of students that the teacher does not believecompetent to complete. Within the research reported here, however, we abstract away from these issues to focus on themeaning of responsibility and delegation. Concepts of belief, trust, monitoring, etc. will necessarily be at the core of acomplete model of actual responsibility [35], but this does not diminish the fact that, in principle, there is a transferof responsibility albeit defeasible. Conte and Paolucci [13] further explore the social context of delegation, and explorethe associated concepts of “counting upon” others for activities and of the accountability of agents. In this complementaryresearch the authors address the concepts of shared and collective responsibility, where collective responsibility correspondsto that of the collective group and shared responsibility corresponds to that of the distributive group as used by Rescher [43]and in this paper. Conte and Paolucci provide a detailed analysis of the social context underpinning these and other relatednotions including power, provide some useful examples that serve to ground this analysis and relate these concepts to otherrelated research including that of Jones and Sergot [26] discussed above.6. ConclusionsWe set out to develop a logic of delegation. The foundation of the approach is the formal characterisation of the modal-ities S and T developed in full in [42]. Using this starting point, the model presented here ties together the axiological andsemantic aspects of delegation both individually and to groups, both singly and in series, both with positive and negativeresponsibility. Our aim in the development of this model has been to bridge the gap between philosophically well-groundedconceptions of responsibility and practical, implementable logical systems that can support delegation. We have shown howit can both contribute to a philosophical understanding of, for example, forbearance, whilst simultaneously providing apractical account of group-oriented communication and the acquisition of responsibility.The paper provides, for the first time, a detailed analysis of the anatomy of delegation in terms of the logical, normativeand inferential aspects of an agent’s world. This analysis can serve as a starting point for building models of agents that useresponsibility and the transfer of responsibility as key components in their solo and social reasoning. Though the picturepresented here makes a number of simplifying assumptions, it benefits from the richness and detail of Hamblin’s accountof imperatives, and provides those benefits in full to designers of heterogeneous agent systems.AcknowledgementsWe are indebted to our reviewers for detailed, insightful and useful comments on an earlier draft.References[1] K. Atkinson, R. Girle, P. McBurney, S. Parsons, Command dialogues, in: I. Rahwan, P. Moraitis (Eds.), Proceedings of the Fifth International Workshop onArgumentation in Multi-Agent Systems, Springer-Verlag, 2008, pp. 9–23.[2] N. Belnap, Backwards and forwards in the modal logic of agency, Philosophy and Phenomenological Research 51 (1991) 777–807.[3] N. Belnap, M. Perloff, The way of the agent, Studia Logica 51 (3/4) (1992) 463–484.[4] N. Belnap, M. Perloff, M. Xu, Facing the Future, Oxford University Press, 2001.[5] K.E. Biggers, T.R. Ioerger, Automatic generation of communication and teamwork within multi-agent teams, Applied Artificial Intelligence 15 (10) (2001)875–916.[6] G. Boella, L. van der Torre, (cid:11): The social delegation cycle, in: Proceedings of the 7th International Workshop on Deontic Logic in Computer Science,in: Lecture Notes in Computer Science, vol. 3065, Springer-Verlag, 2004, pp. 29–42.[7] G. Boella, L. van der Torre, Delegation of power in normative multiagent systems, in: Proceedings of the 8th International Workshop on Deontic Logicin Computer Science, in: Lecture Notes in Computer Science, vol. 4048, Springer-Verlag, 2006, pp. 36–52.[8] C. Castelfranchi, Commitments: From individual intentions to groups and organisations, in: Proceedings of the First International Conference on Multi-Agent Systems, 1995, pp. 41–48.[9] C. Castelfranchi, R. Falcone, Towards a theory of delegation for agent-based systems, Robotics and Autonomous Systems 24 (1998) 141–157.[10] B.F. Chellas, Modal Logic: An Introduction, Cambridge University Press, 1980.[11] B.F. Chellas, Time and modality in the logic of agency, Studia Logica 51 (3/4) (1992) 485–517.[12] P.R. Cohen, H.J. Levesque, Communicative actions for artificial agents, in: Proceedings of the First International Conference on Multi-Agent Systems,1995, pp. 65–72.[13] R. Conte, M. Paolucci, Responsibility for societies of agents, Journal of Artificial Societies and Social Simulation 7 (4) (2004), http://jasss.soc.surrey.ac.uk/7/4/3.html.[14] R.K. Dash, D.C. Parkes, N.R. Jennings, Computational mechanism design: A call to arms, IEEE Intelligent Systems 18 (6) (2003) 40–47.[15] E. Davis, L. Morgenstern, A first-order theory of communication and multi-agent plans, Journal of Logic and Computation 15 (5) (2005) 701–749.[16] C. Dellarocas, M. Klein, Contractual agent societies: Negotiated shared context and social control in open multi-agent systems, in: C. Dellarocas, R.Conte (Eds.), Workshop on Norms and Institutions in MultiAgent Systems, ACM/AAAI, 2000, pp. 1–11.[17] F. Dignum, B. Dunin-K ¸eplicz, R. Verbrugge, Creating collective intention through dialogue, Logic Journal of the IGPL 9 (2) (2001) 289–304.[18] M. Dummett, Elements of Intuitionism, Clarendon Press, Oxford, UK, 1977.[19] D. Elgesem, The modal logic of agency, Nordic Journal of Philosophical Logic 2 (2) (1997) 1–46.[20] R. Girle, Commands in dialogue logic, in: D.M. Gabbay, H.J. Ohlbach (Eds.), Practical Reasoning: Proceedings of the First International Conference onFormal and Applied Practical Reasoning, in: Lecture Notes in Artificial Intelligence, vol. 1455, Springer-Verlag, 1996, pp. 246–260.T.J. Norman, C. Reed / Artificial Intelligence 174 (2010) 51–7171[21] G. Governatori, A. Rotolo, On the axiomatization of Elgesem’s logic of agency and ability, Journal of Philosophical Logic 34 (4) (2005) 403–431.[22] D. Grossi, L. Royakkers, F. Dignum, Organizational structure and responsibility: An analysis in a dynamic logic of organized collective agency, ArtificialIntelligence and Law 15 (2007) 223–249.[23] C.L. Hamblin, Imperatives, Basil Blackwell, 1987.[24] H.L.A. Hart, Bentham on legal rights, in: A.W.B. Simpson (Ed.), Oxford Essays in Jurisprudence, Second Series, Oxford, 1973, pp. 71–201.[25] N.R. Jennings, Coordination techniques for distributed artificial intelligence, in: G.M.P. O’Hare, N.R. Jennings (Eds.), Foundations of Distributed ArtificialIntelligence, John Wiley and Sons, 1996, pp. 187–210 (Chapter 6).[26] A.I.J. Jones, M.J. Sergot, A formal characterisation of institutionalised power, Logic Journal of the IGPL 4 (3) (1996) 429–445.[27] A. Kuflik, Computers in control: Rational transfer of authority or irresponsible abdication of autonomy, Ethics and Information Technology 1 (3) (1999)173–184.[28] S. Kumar, M.J. Huber, D.R. McGee, P.R. Cohen, H.J. Levesque, Semantics of agent communication languages for group interaction, in: Proceedings of theSeventeenth National Conference on Artificial Intelligence, 2000, pp. 42–47.[29] N. Li, B. Grosof, J. Feigenbaum, A practically implementable and tractable delegation logic, in: Proceedings of the Twenty-First IEEE Symposium onSecurity and Privacy, IEEE Computer Society Press, 2000, pp. 27–42.[30] L. Lindahl, Position and Change: A Study in Law and Logic, D. Reidel Publishing Company, Dordrecht, 1977.[31] E. Lorini, N. Troquard, A. Hertzig, C. Castelfranchi, Delegation and mental states, in: Proceedings of the Sixth International Joint Conference on Au-tonomous Agents and Multi-Agent Systems, 2007, pp. 622–624.[32] R.B. Marcus, Iterated deontic modalities, Mind 75 (1966) 580–582.[33] M. McCallum, W.W. Vasconcelos, T.J. Norman, Organisational change through influence, Autonomous Agents and Multi-Agent Systems 17 (2) (2008)157–189.[34] L. Morgenstern, Knowledge preconditions for actions and plans, in: Proceedings of the Tenth International Joint Conference on Artificial Intelligence,1987, pp. 867–874.[35] O. Morgenstern, Prolegomena to a theory of organization, ASTIA Document Number ATI 210734, 1951 (unpublished manuscript).[36] T.J. Norman, C. Reed, Group delegation and responsibility, in: Proceedings of the First International Joint Conference on Autonomous Agents andMulti-Agent Systems, 2002, pp. 491–498.[37] O. Pacheco, F. Santos, Delegation in a role-based organization, in: A. Lomuscio, D. Nute (Eds.), 7th International Workshop on Deontic Logic in ComputerScience, in: Lecture Notes in Computer Science, vol. 3065, Springer-Verlag, 2004, pp. 209–227.[38] J.S. Penberthy, D. Weld, UCPOP: A sound, complete, partial-order planner for ADL, in: Proceedings of the Third International Conference on KnowledgeRepresentation and Reasoning, 1992, pp. 103–114.[39] I. Pörn, The Logic of Power, Basil Blackwell, 1970.[40] A. Prior, Past, Present and Future, Clarendon Press, Oxford, 1967.[41] S.D. Ramchurn, D. Huynh, N.R. Jennings, Trust in multi-agent systems, The Knowledge Engineering Review 19 (1) (2004) 1–25.[42] C. Reed, T.J. Norman, A formal characterisation of Hamblin’s action-state semantics, Journal of Philosophical Logic 36 (2007) 415–448.[43] N. Rescher, The Logic of Commands, Routledge & Kegan Paul Ltd, 1966.[44] M.P. Singh, Towards a formal theory of communication for multi-agent systems, in: Proceedings of the Twelfth International Joint Conference onArtificial Intelligence, 1991, pp. 69–74.[45] M.P. Singh, A semantics for speech acts, Annals of Mathematics and Artificial Intelligence 8 (1993) 47–71.[46] M.P. Singh, Multiagent Systems: A Theoretical Framework for Intentions, Know-How, and Communications, Springer-Verlag, Secaucus, NJ, USA, 1994.[47] M.P. Singh, A.K. Chopra, N. Desai, Commitment-based SOA, http://people.engr.ncsu.edu/mpsingh/papers/drafts/Commitment-Based-SOA-TR-2007-19.pdf,2008.[48] I.A. Smith, P.R. Cohen, J.M. Bradshaw, M. Greaves, H. Holmback, Designing conversation policies using joint intention theory, in: Proceedings of theThird International Conference on Multi-Agent Systems, 1998, pp. 269–276.[49] R.G. Smith, R. Davis, Frameworks for cooperation in distributed problem solving, IEEE Transactions on Systems, Man and Cybernetics 11 (1) (1981)61–70.[50] W. van der Hoek, M. Wooldridge, On the dynamics of delegation, cooperation and control: A logical account, in: Proceedings of the Fourth InternationalJoint Conference on Autonomous Agents and Multi-Agent Systems, 2005, pp. 701–708.[51] W.W. Vasconcelos, M.J. Kollingbaum, T.J. Norman, Normative conflict resolution in multi-agent systems, Journal of Autonomous Agents and Multi-AgentSystems 19 (2) (2009) 124–152.[52] G.H. von Wright, An Essay in Deontic Logic and the General Theory of Action, Acta Philosophica Fennica, vol. 21, North-Holland, Amsterdam, 1968.[53] D.N. Walton, E.C.W. Krabbe, Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning, SUNY, New York, 1995.[54] D.N. Walton, C. Reed, F. Macagno, Argumentation Schemes, Cambridge University Press, 2008.