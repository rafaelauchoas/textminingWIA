Artificial Intelligence 174 (2010) 984–1006Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintUnderstanding the scalability of Bayesian network inference using cliquetree growth curvesOle J. MengshoelCarnegie Mellon University, NASA Ames Research Center, Mail Stop 269-3, Moffett Field, CA 94035, United Statesa r t i c l ei n f oa b s t r a c tArticle history:Received 16 January 2009Received in revised form 18 May 2010Accepted 18 May 2010Available online 25 May 2010Keywords:Probabilistic reasoningBayesian networksClique tree clusteringClique tree growthC/V -ratioContinuous approximationGompertz growth curvesControlled experimentsRegressionOne of the main approaches to performing computation in Bayesian networks (BNs) isclique tree clustering and propagation. The clique tree approach consists of propagationin a clique tree compiled from a BN, and while it was introduced in the 1980s, there isstill a lack of understanding of how clique tree computation time depends on variationsin BN size and structure. In this article, we improve this understanding by developingan approach to characterizing clique tree growth as a function of parameters that canbe computed in polynomial time from BNs, specifically: (i) the ratio of the number of aBN’s non-root nodes to the number of root nodes, and (ii) the expected number of moraledges in their moral graphs. Analytically, we partition the set of cliques in a clique treeinto different sets, and introduce a growth curve for the total size of each set. For thespecial case of bipartite BNs, there are two sets and two growth curves, a mixed cliquegrowth curve and a root clique growth curve. In experiments, where random bipartiteBNs generated using the BPART algorithm are studied, we systematically increase theout-degree of the root nodes in bipartite Bayesian networks, by increasing the numberof leaf nodes. Surprisingly, root clique growth is well-approximated by Gompertz growthcurves, an S-shaped family of curves that has previously been used to describe growthprocesses in biology, medicine, and neuroscience. We believe that this research improvesthe understanding of the scaling behavior of clique tree clustering for a certain class ofBayesian networks; presents an aid for trade-off studies of clique tree clustering usinggrowth curves; and ultimately provides a foundation for benchmarking and developingimproved BN inference and machine learning algorithms.© 2010 Elsevier B.V. All rights reserved.1. IntroductionBayesian networks (BNs) play a central role in a wide range of automated reasoning applications, including in diagnosis,sensor validation, probabilistic risk analysis, information fusion, and decoding of error-correcting codes [64,6,59,38,37,60,43,58]. A crucial issue in reasoning using BNs, as well as in other forms of model-based reasoning, is that of computationalscalability. Most BN inference problems are computationally hard in the general case [10,63,61,1], thus there may be rea-son to be concerned about scalability. One can make progress on the scalability question by studying classes of probleminstances analytically and experimentally. Such problem instances may come from applications or they may be randomlygenerated. In the area of application BNs, both encouraging and discouraging scalability results have been reported. Forexample, a prominent bipartite BN for medical diagnosis is known to be intractable using current technology [64]. Decodingof error-correcting codes, which can be understood as BN inference, is also not tractable but has empirically been found tobe solvable with high reliability using inexact BN inference [20,37]. On the other hand, it is well-known that BNs that areE-mail address: Ole.Mengshoel@sv.cmu.edu.0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.05.007O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006985tree-structured, including the so-called naive Bayes model, are solvable in polynomial time using exact inference algorithms.There are also encouraging empirical results for application BNs that are “close” to being tree-structured or more generallyfor application BNs that are not highly connected [26,43].Clique tree clustering, where inference takes the form of propagation in a clique tree compiled from a BN, is currentlyamong the most prominent BN inference algorithms [33,2,62]. The performance of tree clustering algorithms depends on aBN’s treewidth or the optimal maximal clique size of a BN’s induced clique tree [16,11,15]. The performance of other exactBN inference algorithms also depends on treewidth. A key research question is, then, how the size of a clique tree generatedfrom a BN (and consequently, inference time) depends on structural measures of BNs. One way to investigate this is throughthe use of random generation from distributions of problem instances [66,5,11,52,23]. Taking this approach, and increasingthe ratio C/V between the number of leaf nodes C and the number of root nodes V in bipartite BNs, an easy-hard-harderpattern along with approximately exponential growth have previously been observed for clique tree clustering for a certainclass of BNs, namely BPART BNs [45].In this article, we develop a more precise understanding of this easy-hard-harder pattern. This is done by formulatingmacroscopic and approximate models of clique tree growth by means of restricted growth curves, which we illustrate byusing bipartite BNs created by the BPARTalgorithm [45]. For the sake of this work, we assume that a clique tree propagationalgorithm, operating on a clique tree compiled from a BN, is executed in order to answer probabilistic queries of interest.We introduce a random variable for total clique tree size. This random variable is, for the case of bipartite BNs, the sum oftwo random variables, one for the size of root cliques and one for the size of mixed cliques. Reflecting the random variablefor total clique tree size, we introduce a continuous growth curve for total clique tree size which is the sum of growthcurves for the size of root cliques and mixed cliques. Of particular interest is the growth curve for root clique size, whereGompertz curves of the form g(∞)e, where g(∞), ζ , and γ are parameters, turn out to be useful. A key findingis that Gompertz growth curves are justified on theoretical grounds and also fit very well to experimental data generatedusing the BPART algorithm [45]. While we emphasize bipartite BNs in this article, we also discuss how to generalize toarbitrary BNs, by using multiple growth curves or translating arbitrary BNs to bipartite BNs via factor graphs [32,70].−ζ e−γ xFor experimentation, we sampled bipartite BNs using an implementation of the BPART algorithm. For the number ofroot nodes, V , we used V = 20 and V = 30. The number of leaf nodes was also varied, thereby creating BNs of varyinghardness; 100 BNs per C/V -level were randomly generated. A clique tree inference system, employing the minimum fill-inweight heuristic, was used to generate clique trees for the sampled BNs. Let W be a random variable representing thenumber of moral edges in moral graphs induced by random BNs. In addition to x = C/V , we consider x = E(W ) as anindependent variable. In experiments, we compared different growth curves and investigated x = C/V versus x = E(W ) asindependent variables for Gompertz growth curves. Linear regression was used to obtain values for the parameters ζ and γbased on a linear form of the Gompertz growth curve; values for g(∞) were obtained by analysis. Gompertz growth curvesare common in biological, medical, and neuroscience research [4,35,17], but have not previously been used to characterizeclique tree growth (except for in our earlier conference paper [41] which this article extends). We provide improved resultscompared to previous research, where an easy-hard-harder pattern and approximately exponential growth of upper boundson optimal maximal clique size as a function of C/V -ratio were established [45].We believe this research is significant for the following reasons. First, analytical growth curves improve the understand-ing of clique tree clustering’s performance for a certain class of BNs, namely BPART BNs. Consider Kepler’s three laws ofplanetary motion, developed using Brahe’s observational data of planetary movement. There is a need to develop similarlaws for clique tree clustering’s performance, and in this article we obtain such laws in the form of Gompertz growth curvesfor BPART BNs [45]. While they admittedly have a strong empirical basis, these Gompertz growth curves give significantlybetter fit to the raw data than alternative curves. Consequently, they provide better insight into the underlying mechanismsof the clique tree clustering algorithm and may be used to approximately predict the performance of the algorithm. Sincethe performance of other exact BN inference algorithms — including conditioning [55,11] and elimination algorithms [34,71,14] — also depends on optimal maximal clique size, our results may have significance for these algorithms as well. A sec-ond benefit of growth curves is that they can be used to summarize performance of different BN inference algorithms ordifferent implementations of the same algorithm on benchmark sets of problem instances, and thereby aid in evaluations.1Suppose that the growth curves g1(x) and g2(x) were obtained by benchmarking slightly different clique tree algorithms.Compared to looking at and evaluating potentially large amounts of raw data, it may be easier to understand the perfor-mance difference between the two algorithms by studying their curves g1(x) and g2(x) or by comparing their respectiveGompertz curve parameter values ζ1 and γ1 versus ζ2 and γ2. A third benefit is that growth curves provide estimates ofresource consumption in terms of clique tree size, estimates that can easily be translated into requirements on memorysize and inference time. Hence, this approach enables trade-off studies of resource consumption (or requirements) versusresource bounds, which is important in resource-bounded reasoners [48,40], and may also be of use if one wants to takeinto account, during BN structure learning, the computational resources needed for reasoning.The rest of this article is organized as follows. After introducing notation and background concepts related to Bayesiannetworks and clique tree clustering in Section 2, we study, in the context of the BPART algorithm, the development and1 Such evaluations have been performed, for example, at recent UAI conferences, see http://ssli.ee.washington.edu/~bilmes/uai06InferenceEvaluation/ andhttp://graphmod.ics.uci.edu/uai08/ for details. Application BNs for benchmarking can be found at http://genie.sis.pitt.edu/networks.html and http://www.cs.huji.ac.il/labs/compbio/Repository/.986O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006growth of BNs, causing corresponding clique tree growth. Specifically, in Section 3 we study the issue of independentvariables for growth curves, in particular the C/V -ratio and the expected number of moral edges E(W ), and discuss howgrowth curves can provide a macroscopic model of how clique trees grow. In Section 4, we discuss the connection betweenrandom graphs and our BPART model. In Section 5, we present experiments with BNs generated using the BPART algorithm,varying the number of root and leaf nodes. We compare different mathematical models of growth, and find that Gompertzgrowth curves give the best fit to sample data. We conclude and indicate future research directions in Section 6.2. BackgroundGraphs, and in particular directed acyclic graphs as introduced in the following definition, play a key role in Bayesiannetworks.Definition 1 (Directed acyclic graph (DAG)). Let G = (X, E) be a non-empty directed acyclic graph (DAG) with nodes X ={ X1, . . . , Xn} for n (cid:2) 1 and edges E = {E1, . . . , Em} for m (cid:2) 0. An ordered tuple E i = (Y , X), where 0 (cid:3) i (cid:3) m and X , Y ∈ X ,represents a directed edge from Y to X . Π X denotes the parents of X : Π X = {Y | (Y , X) ∈ E}. Similarly, Ψ X denotes thechildren of X : Ψ X = {Z | ( X, Z ) ∈ E}, where Z ∈ X . The out-degree and in-degree of a node X are defined as o( X) = |Ψ X |and i( X) = |Π X | respectively.In the rest of this article we assume for simplicity that DAGs and BNs are non-empty, even when not explicitly stated asin Definition 1. The following classification of nodes in DAGs, including in BNs, turns out to be useful when we discuss theperformance of BN inference algorithms.Definition 2. Let G = (X, E) be a non-empty DAG with X ∈ X . If i( X) = 0 then X is a root node. If i( X) > 0 then X isa non-root node. If i( X) > 0 and o( X) = 0 then X is a leaf node. If o( X) > 0 then X is a non-leaf node. If o( X) > 0 andi( X) > 0 then X is a trunk (non-leaf and non-root) node.With the concepts from Definition 2 in hand, we classify the nodes in a DAG as follows.Definition 3. Let G = (X, E) be a DAG. We identify the following subsets of X : V = { X ∈ X | i( X) = 0} (the root nodes);C = { X ∈ X | i( X) > 0 and o( X) = 0} (the leaf nodes); T = { X ∈ X | i( X) > 0 and o( X) > 0} (the trunk nodes); ¯V = { X ∈ X |i( X) > 0} (the non-root nodes); and ¯C = { X ∈ X | o( X) > 0} (the non-leaf nodes).A Bayesian network (BN) is a DAG with an associated set of conditional probability distributions [56]. In the following⊆ x isdefinition, we let n = |X| and let π Xi represent a complete instantiation of the parents Π Xi of Xi , in other words π Xia projection of a complete assignment x = { X1 = x1, . . . , Xn = xn}.Definition 4 (Bayesian network). A Bayesian network is a tuple β = (X, E, P ), where (X, E) is a DAG augmented withconditional probability distributions P = {Pr( X1 | Π X1 ), . . . , Pr( Xn | Π Xn )}. Here, Pr( Xi | Π Xi ) is the conditional probabilitydistribution for Xi ∈ X . The independence assumptions encoded in (X , E) imply the joint probability distributionPr(x) = Pr(x1, . . . , xn) = Pr(X1 = x1, . . . , Xn = xn) =n(cid:2)i=1Pr(xi | π Xi ).(1)In this article we will restrict ourselves to discrete random variables, and “BN node” will thus mean “discrete BN node”.Let a BN node X ∈ X have states {x1, . . . , xm}. We use the notation Ω X = Ω( X) = {x1, . . . , xm} to represent the state spaceof X . In our setting, a conditional probability distribution Pr( Xi | Π Xi ) is also denoted a conditional probability table (CPT).A BN is provided input or evidence by clamping zero or more of its nodes to their observed states. Given evidence,answers to different probabilistic queries can be computed by means of a BN. In this context, the concept of an instantiationor explanation over all non-clamped nodes is key, and is formally defined as follows.Definition 5 (Explanation). Consider a BN β = (X, E, P ) with X = { X1, . . . , Xn} and evidence e = { X1 = x1, . . . , Xm = xm}where 0 (cid:3) m < n. An explanation x is defined as x = {xm+1, . . . , xn} = { Xm+1 = xm+1, . . . , Xn = xn}.One is often interested in computing answers to queries of the form Pr(x | e), and in particular in finding a most probable∗ | e) (cid:2) Pr(x | e) for any other explanation x. In addition toexplanation (MPE). An MPE is an explanation xMPE, the computation of posterior marginals, or Pr( X | e) for X ∈ X , is of great interest. To compute answers to thesequeries, both complete and incomplete algorithms for Bayesian network computation may be used. Complete algorithmsinclude clique tree propagation [33,2,25,62], conditioning [55,11], variable elimination [34,71,14], and arithmetic circuitsuch that Pr(x∗O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006987evaluation [12,9,8]. Incomplete algorithms, and in particular stochastic local search algorithms, have also been used for MPE[27,39,21,42,44,46] as well as maximum a posteriori hypothesis (MAP) [52,53] computation.An important distinction exists between algorithms that rely on an off-line compilation step — for example clique treepropagation and arithmetic circuit evaluation — and those that do not — for example variable elimination and stochasticlocal search. Compilation has several benefits when it comes to integration into resource-bounded systems including hardreal-time systems [48,40]. Our main emphasis in this article is on compilation and in particular the Hugin clique treeclustering approach [33,25], in which clique trees play a central role.Definition 6 (Clique tree). A clique tree β(cid:6)(cid:6)(cid:6) = (Γ, Φ) for a BN β = (X, E, P ) consists of nodes (or cliques) Γ and edges Φ.Here, (Γ, Φ) is an undirected graph that is a tree, and needs to adhere to the following conditions: (i) for each cliqueγ ∈ Γ , γ ⊆ X ; (ii) each family in (X, E) must appear in some clique γ ∈ Γ ; and (iii) if a node X ∈ X is such that X ∈ γiand X ∈ γ j , then X ∈ γk for any γk ∈ Γ , where γk is on the path between γi and γ j in β(cid:6)(cid:6)(cid:6).The Hugin approach is interesting in its own right, and in addition there is a well-established relationship to arithmeticcircuits [54]. A clique tree2 β(cid:6)(cid:6)(cid:6), which is used for on-line computation, is constructed from a BN β = (X, E, P ) in thefollowing way by the Hugin algorithm [33,2]: A moral graph β(cid:6)is first constructed by making an undirected copy of β andthen augmenting it with moral edges as follows. For each node X ∈ X , Hugin adds, in its moralization step, to β(cid:6)a moral. Second, Hugin creates a triangulated graph β(cid:6)(cid:6)edge between each pair of nodes in Π X if no such edge already exists in β(cid:6)by heuristically adding fill-in edges to β(cid:6)such that no chordless cycle of length greater than three exists. Third, a cliquetree β(cid:6)(cid:6)(cid:6). A clique tree is constructed sequentially, such that for any two nodesγi and γ j in the clique tree, all nodes between them contain γi ∩ γ j . This is known as the running intersection property,which (informally) enforces global consistency through local consistency, thereby enabling the computation of marginalsand MPEs. Each CPT Pr( X | Π X ) ∈ P is assigned to a clique containing { X} ∪ Π X .is created from the triangulated graph β(cid:6)(cid:6), Hugin can compute marginals [33] or MPEs [13]. These computations rely on the following well-known result.Using β(cid:6)(cid:6)(cid:6)Theorem 7. Let Pr(X) be a probability distribution induced by a BN β, and let β(cid:6)(cid:6)(cid:6) = (Γ, Φ) be a corresponding clique tree. Then thedistribution can be expressed as:Pr(X) =(cid:2)(cid:3) (cid:2)Pr(γi)γi ∈Γ{γi ,γ j}∈ΦPr(γi ∩ γ j).The size of a clique tree β(cid:6)(cid:6)(cid:6)essentially determines the compilation and propagation times, be it for computation ofMPEs or marginals. The following parameters are useful in characterizing clique trees, and thereby also computation timesfor algorithms that use clique trees.Definition 8 (Clique tree parameters). Let Γ = {γ1, . . . , γη} be the set of cliques in a clique tree β(cid:6)(cid:6)(cid:6) = (Γ, Φ). The largestclique in Γ (in terms of number of BN nodes) is defined as(cid:12)(Γ ) = arg maxγ ∈Γ(cid:4)(cid:5),|γ |(2)with the cardinality of the largest clique (in terms of number of nodes) is defined as c(Γ ) = |(cid:12)(Γ )|. The state space size sof a clique γ ∈ Γ is defined as(cid:2)s(γ ) = |Ωγ | =|Ω X |,X∈γwhere X ∈ X is a node in β = (X, E, P ). The maximal clique in Γ (in terms of state space size) is defined as(cid:4)m(Γ ) = arg maxγ ∈Γ(cid:5),|Ωγ |with maximal clique size m(cid:6)∗(Γ ) = s(m(Γ )). The total clique tree size of β(cid:6)(cid:6)(cid:6)(cid:6)is defined ask(Γ ) =s(γ ) =|Ωγ |.γ ∈Γγ ∈ΓThe width w(Γ ) of a clique tree Γ is defined as w(Γ ) = c(Γ ) − 1, and thus size of a clique tree’s largest clique andwidth are closely related. In general, there is more than one clique tree for a BN, and it is interesting to consider optimalclique trees, which may be defined as follows.2 For simplicity, and even though they are used to denote slightly different concepts by some authors, we generally do not distinguish between junctiontrees and clique trees in this article.(3)(4)(5)988O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006Definition 9 (Clique tree optimization). Let Γ = {Γ1, Γ2, . . .} be the set of all (clique tree) cliques for clique trees {β(cid:6)(cid:6)(cid:6)2 , . . .}for a BN β. The clique tree with the optimal (minimal) largest clique (in terms of number of nodes) is defined, using (2), as1 , β(cid:6)(cid:6)(cid:6)L(Γ ) = arg minΓ ∈Γ(cid:4)(cid:5)(cid:12)(Γ ),(6)with optimal largest clique size (cid:12)∗(Γ ) = |(cid:12)(L(Γ ))|. The optimal clique tree (in terms of minimizing total size) is defined,using (5), asK (Γ ) = arg minΓ ∈Γ(cid:4)(cid:5)k(Γ ),and with optimal (minimal) total clique tree size defined as k∗(Γ ) = k(K (Γ )).(7)It should be noted that some of the quantities, and specifically (2) and (6) in Definition 9 and Definition 8 are strictlygraph-theoretic. Other quantities, specifically (7) and (4), also take into account the size of the state spaces of BN nodes,and are consequently of particular interest to researchers interested in the scalability of computation using clique treeclustering.The performance of many complete BN inference algorithms has been found to depend on treewidth w, which is de-∗(Γ ) = (cid:12)∗(Γ ) − 1 [33,15]. When the BN β and its clique trees {(Γ1, Φ1), (Γ2, Φ2), . . .} are obvious from thefined as wcontext, we simply write wis the minimal largest clique across all clique trees for a BN. Treewidthcomputation is NP-complete [3], and greedy triangulation heuristics that compute upper bounds on treewidth (or opti-mal maximal clique size) are typically used in practice [31]. A key research question, which we investigate in this article,is how clique tree size relates to parameters that can be computed for a BN in polynomial time, such as the followingparameters:∗ = (cid:12)∗ − 1, where (cid:12)∗∗• V = |V |, the number of root nodes in a BN, with V (cid:2) 1.• T = |T |, the number of trunk nodes in a BN, with T (cid:2) 0.• C = |C |, the number of leaf nodes in a BN, with C (cid:2) 0, so the total number of BN nodes is n = C + V + T .• P avg, the average number of parents for all non-root nodes ¯V ⊂ X in a BN, with 1 (cid:3) P avg (cid:3) N − 1.• Savg, the average number of states for all BN nodes X , with Savg (cid:2) 1.Using the parameters above, we study bipartite BNs in detail in this article. In a bipartite BN β = (X , E , P ), the nodesin X are partitioned into root nodes V and leaf nodes C according to the following definition.Definition 10 (Bipartite DAG). Let G = (X, E) be a DAG. If X can be split into partite sets V = { X ∈ X | i( X) = 0} (the rootnodes) and C = { X ∈ X | i( X) > 0} (the leaf nodes) such that any (V , C) ∈ E is such that V ∈ V and C ∈ C , then G is abipartite DAG.While our approach is general, as discussed in Section 3.3, we also note that important classes of application BNs arebipartite or have significant induced subgraphs that are bipartite. Naïve Bayes classifiers are, for example, a special caseof bipartite BNs with only one root node. Application areas where bipartite BNs can be found include gas path diagnosisfor turbofan jet engines [60], sensor validation and diagnosis of rocket engines [6], diagnosis in computer networks [59],medical diagnosis [64], and decoding of error-correcting codes [37]. A well-known bipartite BN for medical diagnosis isQMR-DT; in it diseases are root nodes and symptoms are leaf nodes [64]. QMR-DT may be used to compute the most likelyinstantiation of the disease nodes (i.e., the most probable explanation), given known symptoms [64,24,50]. In research ondecoding of error-correcting codes, a close relationship has been established to Bayesian network computation [38,37]. Itturns out that the subgraph induced by nodes corresponding to the hidden information and codeword bits in a decodingBN forms a bipartite BN [38, Fig. 7].Bipartite BNs also generalize satisfiability (SAT) instances: root nodes correspond to propositional logic variables and leafnodes correspond to propositional logic clauses [63,61,45]. Special inference algorithms have been designed for bipartiteBNs; see for example the study of approximate inference algorithms for bipartite BNs by Ng and Jordan [50]. Finally, generalBNs often have non-trivial bipartite components, and bipartite BNs therefore form a stepping stone for these more general,multi-partite BNs.For the purpose of this article, our emphasis is on randomly generated BNs, as this approach admits a very systematicinvestigation of BN inference algorithms [66,23,45]. Bipartite BNs are generated randomly using the BPART algorithm [45],which is a generalization of an algorithm that randomly generates hard and easy problem instances for satisfiability [47].O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006989Fig. 1. Two classes, Class A and Class B, of bipartite graphs and Bayesian networks (BNs). In both Class A and Class B BNs, all leaf nodes have the samenumber of parents P (here, P = 2). In Class A BNs, all root nodes have k or k + 1 children, with k (cid:2) 0. In Class B BNs, the number of children may varybetween the root nodes.Fig. 2. Compilation of BPART BN β (top) to clique tree β(cid:6)(cid:6)(cid:6)in triangulated graph β(cid:6)(cid:6), which again leads to cliques {V 4, V 1, V 2} and {V 4, V 2, V 3} in the clique tree β(cid:6)(cid:6)(cid:6).(bottom). There is a loop (V 1, V 2, V 3, V 4) in the moral graph β(cid:6), leading to a fill-in edge (V 2, V 4)The BPART algorithm, for which we use the signature BPART(V , C , P , S, R), operates as follows.3 First, V = |V | root nodesand C = |C | leaf nodes, all with S states, are created. The value of the binary input parameter R determines whether regularClass A (R = true) or irregular Class B (R = false) BNs are generated (see Fig. 1). In Class B BNs, P parent nodes { X1, . . . , X P }are, for each leaf node, picked uniformly at random without replacement among the V root nodes. In Class A BNs, whichform a strict subset of Class B BNs [45], parents are picked such that all root nodes have exactly k or k + 1 children forsome k (cid:2) 0. Conditional probability tables (CPTs) of all nodes are also constructed by BPART; however in this article wefocus on the impact of the structural parameters V , C , P = P avg, and S = Savg on clique tree size. As defaults, parametervalues S = 2 and R = false are employed, and we use BPART(V , C , P ) as an abbreviation for BPART(V , C , P , 2, false). Anadditional default of P = 2 is also used, giving BPART(V , C ) as an abbreviation for BPART(V , C , 2). Since, in a given contextwe always fix P , the total number of edges in a BPART(V , C , P ) BN is clearly E = C × P .Here is an example of using clique tree clustering on a small BPART BN.Example 11 (BPART BN). Fig. 2 shows how a BPART BN may be compiled into a clique tree. For each BN leaf node C ∈{C1, C2, C3, C4, C5, C6}, a clique is created. In addition, there are two cliques containing BN root nodes only, namely thecliques {V 1, V 2, V 4} and {V 2, V 3, V 4}.3 The more extensive signature BPART( Q , F , V , C , S, R, P ) was previously used [45]. Here, the Q and F parameters are used to control the conditionalprobability table (CPT) types of BN root and non-root nodes respectively. The parameter R is used to control the regularity in the number of children ofroot nodes. Since our emphasis in this article is on the impact of the parameters V , C , S, and P , we typically use the default values for Q , F , and R, andgenerally simplify the signature to BPART(V , C , P , S).990O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006Table 1Three different upper bounds on total clique tree size for seven example bipartite Bayesian networks.BNβ0β12β73β89β92β6β80Maximalcliquebound 2¯(cid:12)∗16,38416,38416,38416,38416,384262,144262,144Tree-widthbound ¯w∗13131313131717Clique treesize bound –trivial ¯k∗T1,474,5601,474,5601,474,5601,474,5601,474,56023,592,96023,592,960Clique treesize bound –easy ¯k∗E1,261,5681,261,5681,261,5681,261,5681,261,56819,136,51219,136,512Clique treesize bound –sum ¯k∗S61,05675,84081,20099,13853,344439,776284,192Relativesize –100¯k∗S /¯k∗T4.14%5.14%5.51%6.72%3.62%1.86%1.20%Relativesize –100¯k∗S /¯k∗E4.84%6.01%6.44%7.86%4.23%2.30%1.49%Note that clique tree clustering’s moralization step, which creates a moral graph β(cid:6)from a BPART BN β, ensures thatthere are edges between all P root nodes that share a leaf node. To keep the discussion succinct, we often say that BPARTcreates moral edges without explicitly mentioning the moralization step, even though moralization actually creates theedges when a BPART BN is compiled. The compilation of the bipartite BN in Fig. 2 illustrates the crucial formation of cyclesin a BN’s moral graph and the resulting generation of fill-in edges.In the bipartite case, all non-root nodes are leaf nodes and n = C + V . It has been shown analytically and empirically thatthe ratio of C to V , the C/V -ratio, is a key parameter for BN inference hardness [45]. (We consider only non-empty BNsand so V (cid:2) 1 and the C/V -ratio is always well-defined.) Specifically, the C/V -ratio can be used to predict upper and lowerbounds on the optimal maximal clique size (or treewidth) of the induced clique tree for BNs randomly generated usingthe BPART algorithm. Taking this approach, upper bounds on optimal maximal clique sizes as well as inference times havebeen computed. Using regression analysis, the mean number of nodes in the maximal clique was found to be approximatelylinear in the C/V -ratio. This linear growth translates into an approximately exponential growth in maximal clique size —and consequently in clique tree clustering computation time — as a function of the C/V -ratio. This has been found to betrue for both Class A and Class B BPART BNs [45].are hard to compute, one often computes upper bounds ¯wE , and ¯ktics. We now focus on three upper bounds ¯kY = arg max X∈X (|ΩX |) and d = |ΩY |, is well-known; an easy extension4 is ¯kis obtained by summing the sizes of cliques, or ¯kvations can be made in regard to these and related upper bounds. First, they may assume that the treewidth wBut it is NP-hard to compute treewidth w∗∗ (cid:2) wquently, we use an upper bound ¯wusing heuristics algorithms such as Hugin.using various heuris-∗+1), where= n × d(w∗+1). Another upper bound∗S (β) = k(Γ ), where Γ is computed from β using Hugin [28]. A few obser-is known.[3], and therefore bounds involving treewidth are not directly useful. Conse-when forming an upper bound ¯kcan be computed, and ¯k. The trivial upper bound ¯k∗T= (n − ¯w. Such upper bounds ¯w∗S for k∗) × d( ¯wBecause wT , ¯k, and k, ¯(cid:12)∗, (cid:12)∗∗E∗∗∗∗∗∗∗∗∗∗∗∗To empirically investigate these upper bounds, we consider a few existing BNs, generated using the signature BPART(V ,C , P ) = BPART(30, 60, 3) [45, Table 6]. With reference to the above bounds ¯k∗E , we thus have n = 30 + 60 = 90 andas well as ¯k∗d = S = 2. Here, ¯wS are computed by an implementation of the Hugin algorithm. Results for these BNs arepresented in Table 1, which shows that the bounds ¯kE are not very good compared to ¯k∗S . Specifically, we see inthe relative size columns of Table 1 the following. The size of the computed total clique tree size ¯k∗S relative to the trivialT ranges from 1.20% (worst case) to 6.72% (best case), and the easy upper bound ¯kupper bound ¯k∗E is only slightly better.Previously, a difference of an order of magnitude or more between the two bounds ¯k∗S has been found across abroad range of benchmark instances [51]. These empirical results illustrate why it is useful to consider ¯k∗S , as computed byclique tree clustering algorithms, instead of more obvious bounds such as ¯k∗E , and our growth curves in the rest ofthis article are based on ¯kT and ¯kT and ¯kT and ¯kT and ¯k∗∗∗∗∗∗∗S .In larger BNs, it is important but also very difficult to understand and predict clique tree clustering’s cycle-generation andfill-in processes, which again determine maximal clique size and total clique tree size. A main contribution of this article,further discussed in Sections 3, 4, and 5, is how we improve the understanding of the growth of total clique tree size ¯k∗Sas a function of BPART BN growth. Whether one computes MPEs or marginals, the structure and size of the clique tree is∗ | e) versus addition for marginalthe same, and only the numerical operations (maximization for MPE computation Pr(xcomputation Pr( X | e) for X ∈ X ) differ. The clique tree growth curves discussed in this article apply to both cases.3. Developing model-based reasoners using Bayesian networksThe development of model-based reasoners, including those that use Bayesian networks, typically involves an iterative orspiral process. One starts with a simple model, which is refined and extended as further information, experimental results,or additional requirements become available. In other words, an iterative development process often manifests itself as4 The extension was introduced by an anonymous reviewer.O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006991Fig. 3. An example of how a bipartite BN with V = 4 root nodes and C = 6 leaf nodes (top right) may be developed or grown from a bipartite BN withV = 2 root nodes and C = 4 leaf nodes (bottom left).model growth, for example Bayesian network growth. More specifically, if we consider bipartite Bayesian networks used fordiagnosis [64,6,59,60], we may identify at least two forms of growth:• Growth in the number of root nodes V , to capture additional faults that may occur in the system being modeled. Ina gas path diagnosis BN, these root nodes represent health parameters for a turbofan engine [60], and by increasingthe number of health parameters a more comprehensive diagnosis can be computed. In a BN for medical diagnosis,additional root nodes may be introduced because one wants to consider more diseases [64].• Growth in the number of leaf nodes C , to represent additional evidence that can be used to distinguish betweenthe underlying faults by computing marginals, MPE, or MAP. In a BN for gas path diagnosis, these leaf nodes canrepresent additional measurements made on the turbofan engine [60]. In a BN for medical diagnosis, these leaf nodesmay represent additional symptoms or tests [64].A hypothetical BN development process, where small BNs are used for the purpose of illustration, is provided in Fig. 3.The figure shows two different BN growth paths leading from a BN with V = 2 and C = 4 (lower left corner of Fig. 3) to alarger BN with V = 4 and C = 6 (upper right corner of Fig. 3).Even though we place emphasis on growth or increase here, it is really the concept of change that is important. Ourresults apply to change in general, both increases and decreases in BN size, however the increase or growth perspective ismore prevalent. For example, both in knowledge engineering and BN structure learning one typically proceeds by growinga BN iteratively. In addition, we want to emphasize the connection between BN growth and biological and medical growthprocesses [4,35,17] as well as growth of random graphs (see Section 4). Similar to these areas of research, this articlerepresents a shift away from a particular BN β to families or sequences (β(1), β(2), β(3), β(4), . . .) of BNs and the processesby which BNs are developed or grown. BN growth processes might be automatic, as in machine learning or data mining;manual, as in knowledge engineering by direct manipulation of a BN; or semi-automatic, as when editing a high-levellanguage from which BNs are auto-generated [43].An illustration of the connection between BN growth and clique tree growth is provided in Fig. 4. This figure illustrateswhy it is important to vary a cause (say, the number of leaf nodes in BNs or the density of edges in the moral graphsof BNs) such that a wide range of effects (different clique tree sizes) can be studied. At the highest level, we want tocommunicate two main ideas in this article. The first idea is the use of a macroscopic growth curve gT (x) for total cliquetree size, where x is an independent parameter. As an illustration, gT (x) for bipartite BNs is emphasized in Section 3.2, butthe approach clearly generalizes beyond bipartite BNs as discussed in Section 3.3. As a second idea, discussed in Section 3.1,we investigate different independent parameters x in gT (x). The use of x = C/V , where C is number of leaf nodes and V isnumber of root nodes, is well-known. A novel aspect of this work is the investigation of an alternative to C/V .The research on the BPART algorithm and its generalization, the MPART algorithm, extends existing research on gener-ating hard instances for the satisfiability problem [47] as well as existing research on randomly generating BNs [66,5,27,11,52,22,23,45]. Our work on BPART in this article is different from previous research [45] in several ways including thefollowing: The emphasis in this article is on total clique tree size instead of size of largest clique, and in particular we formtotal clique tree size by carefully partitioning the cliques Γ in the clique tree.992O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006Fig. 4. How clique tree size (bottom) varies when the number of BN leaf nodes is varied (top). Horizontally, this figure illustrates how a BN may grow byhaving leaf nodes added. Vertically, this figure shows how BNs are compiled into clique trees. The growth of a BPART(4, 2) BN (top left) into a BPART(4, 4)BN (top middle) and finally a BPART(4, 6) BN (top right) is depicted in the top row. Clique trees compiled from these BNs are shown in the bottom row.3.1. Independent parameters for Bayesian networks and moral graphsLet W be the random number of moral edges in the moral graph β(cid:6)of a randomly generated BN β. Then E(W ) is theexpected number of moral edges. It turns out to be fruitful to use x = E(W ) as the independent parameter in the growthcurve gT (x). In the rest of this section we discuss this issue in more detail.3.1.1. Balls and binsThe balls and bins model, where balls are placed uniformly at random into bins, turns out to be useful in our analysisof clique tree clustering’s moralization step. Following the balls and bins model, we let m denote the number of balls and ndenote the number of bins. Further, we let X and Y be random variables representing the number of empty and occupiedbins respectively. The expected number of empty bins X isE(X) = n(1 − 1/n)m.The expected number of occupied bins Y isE(Y ) = n1 − (1 − 1/n)m.(cid:5)(cid:4)(cid:4)It is well-established that the expected number of empty bins X can be approximated asE(X) ≈ ne−m/n,while the expected number of occupied bins Y is approximated byE(Y ) ≈ n1 − e−m/n(cid:5).(8)(9)(10)(11)How does the balls and bins model apply to the moral graph created, using clique tree clustering, from a bipartite BN?We restrict5 our attention to the subgraph of β(cid:6)induced by V , abbreviated β(cid:6)[V ]. The bins are all possible edges in themoral subgraph β(cid:6)[V ], and BN leaf nodes induce actual edges (corresponding to balls) in the moral graph. For clarity, wesay edge-bin instead of bin and edge-ball instead of ball. The formal definition that makes the connection between the ballsand bins model and a bipartite Bayesian network is as follows.Definition 12 (Edge-balls and edge-bins). Consider the root nodes V ⊆ X in a bipartite BN β = (X, E, P ). An edge-bin is apossible edge in the BN’s moral subgraph β(cid:6)[V ], namely an edge between two root nodes {V i, V j}, where V i, V j ∈ V andi (cid:11)= j. The set of all edge-bins is {{V i, V j} | V i, V j ∈ V and i (cid:11)= j}, and an edge-ball is placed into an edge-bin by picking oneedge-bin uniformly at random with probability p = 1/(cid:4) |V |(cid:5).2We use, as will be seen shortly, a balls and bins approach to obtain the expected number of moral edges in the moralgraphs induced by distributions of BNs, specifically BPART(V , C , P ) BNs. We now consider bipartite BNs where leaf nodeshave exactly two parents (Section 3.1.2) or an arbitrary number of parents (Section 3.1.3).5 The reason for this restriction is clarified in Section 3.2, where we discuss growth curves g R (x) and gM (x). The growth of total clique tree size due tochanges in β(cid:6)[V ], for example induced by an increasing x = C/V , is captured by g R (x).O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006993Fig. 5. As part of compilation to a clique tree, a bipartite Bayesian network (left) is transformed into a moral graph with moral edges (middle). We focus onthe root nodes {V 1, V 2, V 3, V 4} and in particular the moral edges in the moral graph’s subgraph induced by the root nodes (right). Both the moral edgesactually created (edge-bins filled with edge-balls as shown using solid lines) as well as the potential moral edges not created (edge-bins not filled withedge-balls as shown using dashed lines) are shown to the right above.3.1.2. Balls and bins: Two parentsIn the BPART(V , C, 2) model, all edge-bins are uniformly and repeatedly eligible for placing edge-balls into. In otherwords, we have sampling with replacement. Here is an example of applying our balls and bins model in the BPART setting.Example 13. Fig. 5 shows a BN sampled from the BPART(V , C , P ) distribution with V = 4, C = 6, and P = 2. For thisparticular BN, 4 of the 6 possible edge-bins contain edge-balls as can be seen in the subgraph induced by the root nodes{V 1, V 2, V 3, V 4} to the right in Fig. 5.Intuitively, as the C/V -ratio increases, it gets more and more likely that a given moral edge gets picked two or moretimes, or in other words that an edge-bin contains two or more edge-balls. This intuitive argument is formalized in thefollowing result, where we shorten the expectation E(W ; C, V ) to E(W ) when C and V are obvious.Theorem 14 (Moral edges, exact). Let the number of moral edges created using BPART(V , C , P ) be a random variable W . The expectednumber of moral edges E(W ; C, V ) is, for P = 2, given by:(cid:8)(cid:7)(cid:7)(cid:7)(cid:7)E(W ; C, V ) =1 −1 − 1.(12)V2(cid:8)(cid:8)C (cid:8)(cid:9)V2Proof. We use the balls and bins model. Here, the edge-balls correspond to leaf nodes, of which there are m = C . Theedge-bins are all possible moral edges, of which there are n =in a bipartite graph with V root nodes. Plugging m andn into (9) gives the desired result (12). (cid:2)V2(cid:5)(cid:4)It is sometimes convenient to use the following approximation for E(W ; C, V ) in (12).Theorem 15 (Moral edges, approximate). Let the number of moral edges created using BPART(V , C , P ) be a random variable W . Theexpected number of moral edges E(W ; C, V ) is, for P = 2, approximated as follows:E(W ; C, V ) ≈1 − exp−C(cid:7)(cid:8)(cid:7)V2(cid:7)(cid:7)(cid:8)(cid:8)(cid:8)(cid:9)V2.(13)Proof. We use the balls and bins model. Here, the edge-balls correspond to leaf nodes, of which there are m = C . Theedge-bins are all possible moral edges, of which there are n =in a bipartite graph with V root nodes. Plugging m andn into (11) gives the desired result (13). (cid:2)V2(cid:4)(cid:5)Given (12) and (13), we can make a few remarks. In contrast to the C/V -ratio or the E/V -ratio, the expectation E(W )takes into account the effect of picking parents among pairs of BN root nodes with replacement. For low values of C/V orE/V one would not expect the effect of replacement to be great, but for large C/V - or E/V -ratios the difference may besubstantial as illustrated in the following examples.Example 16 (C = 30 leaf nodes). Let V = 30, C = 30, and P = 2. The expected number of moral edges is E(W ) = 28.99 using(12) and E(W ) ≈ 29.02 using (13).Example 17 (C = 300 leaf nodes). Let V = 30, C = 300, and P = 2. The expected number of moral edges is E(W ) = 216. 91using (12) and E(W ) ≈ 216. 74 using (13).In Example 16, where E(W ) ≈ C , it is relatively unlikely that there are edge-bins with two or more edge-balls. InExample 17, on the other hand, it is very likely that there are edge-bins with two or more edge-balls, and E(W ) < C . In994O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006other words, adding the last leaf node has on average a smaller net effect on the number of moral edges in Example 17than in Example 16, and this is captured in E(W ) but not in C/V or E/V . This is important because the essential difference,as far as cycle (and thus clique) formation in clique tree clustering is concerned, is between (i) no edge-ball and (ii) one ormore edge-balls.3.1.3. Balls and bins: Arbitrary number of parentsWe now turn to BPART instances in which P is an arbitrary positive integer. The fundamental complication, as far asthe expected number of moral edges E(W ) is concerned, is this. For P > 2, BPART uses a combination of sampling withreplacement and sampling without replacement: Picking the parents of a given leaf node Ci amounts to sampling withoutreplacement, while picking parents for Ci when parents of C j are already known (for i > j) amounts to sampling withreplacement.We now introduce, for the purpose of approximation, a variant BPART(cid:6)which works exactly as BPART except that themoral edges induced by the P parent nodes are, for a given leaf node Ci , picked independently and with replacement fromthe moral graph β(cid:6). This means that we in Theorem 18 need to consider sampling with replacement only.Theorem 18 (Moral edges, exact). Consider BPART(cid:6)expected number of moral edges is:(V , C , P ) and let the number of moral edges created be a random variable Z . TheE(Z ; C, V , P ) =(cid:7)V2(cid:8)(cid:7)(cid:7)1 −1 − 1(cid:8)(cid:8)C(P2)(cid:8).(cid:7)(cid:9)V2Proof. We use the balls and bins model, and again the number of edge-bins is n =nodes. Since BPART(cid:6)into (9) gives the desired result (14). (cid:2)employs sampling with replacement, the number of edge-balls is m = C ×V2(cid:4)(cid:5)P2(14)(cid:5)in a bipartite graph with V root(cid:4). Plugging m and nWe note that Theorem 18 is a generalization of Theorem 14, and abbreviate E(Z ; C, V , P ) as E(Z ) when C , V , and Pare clear from the context. Further, we note that E(Z ) in (14) can be approximated in a way similar to the approximationof (12) by (13).We now consider two areas where BPART(cid:6)works differently than BPART. First, as mentioned above, there is the issueof picking parents with replacement versus without replacement. For BPART(V , C , P ), selecting P parents of a leaf nodeCi creates exactly(V , C , P ), onedges in the moral graph, since the parents are distinct and |ΠCi(cid:4)(cid:5)P2the other hand, we end up withgraph. A second issue is how edge-balls are placed by BPART(cid:6); specifically, the edge-bins picked might not form a clique.In summary, we note that E(Z ) is an approximation for E(W ) for BPART(V , C , P ) for P > 2, justified in part by the well-known fact that sampling without replacement can be approximated using sampling with replacement as the number ofobjects sampled from (here, the V root nodes) grows.edge-balls placed into edge-bins, and consequently at mostedges in the moral| = P . For BPART(cid:6)(cid:4)P2(cid:5)(cid:4)(cid:5)P2Why are the above balls and bins models of BN moralization interesting? The reason is that we are concerned with thepossible causes, at a macroscopic level, that influence clique tree size. The expected number of moral edges is one suchcause or independent parameter x. In the context of random BNs generated by BPART, we indirectly control the placementof moral edges, since we place constraints on the structure of these BNs through BPART’s input parameters. When it comesto the effect, namely tree clustering performance, it is natural to optimize (minimize) the size of the maximal clique. Sincethis is hard [3], current algorithms including Hugin use heuristics that upper bound optimal maximal clique size (cid:12)∗andclique tree size krespectively. Such upper bounds on clique tree size are just referred to as clique treesizes in the following, and we seek in Section 3.2 a closed form expression y = g(x) for the dependent parameter cliquetree size as a function of the independent parameter x.using ¯(cid:12)∗and ¯k∗∗3.2. Growth curves for bipartite Bayesian networksHere, we develop models of restricted clique tree growth that extend exponential growth curves [45] used to modelunrestricted growth. Even though Bayesian networks and clique trees are discrete structures, we approximate their growthby using continuous growth curves (or growth functions) in order to simplify analysis. We discuss bipartite BNs in thissection and generalize to arbitrary BNs in Section 3.3.For bipartite BNs, including BPART BNs, there are two types of nodes in the clique tree as reflected in the followingdefinition.Definition 19 (Root clique, mixed clique). Consider a clique tree β(cid:6)(cid:6)(cid:6)with cliques Γ constructed from a bipartite BN β. A cliqueγ ∈ Γ is denoted a root clique if all the BN nodes in γ are root nodes in β. A clique γ ∈ Γ is denoted a mixed clique if γcontains at least one root node and at least one leaf node.O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006995Fig. 6. The clique tree for a bipartite Bayesian network, with the two partitions of cliques indicated. Here, V 4 V 1 V 2 and V 4 V 2 V 3 are root cliques (withgrowth curve g R ) while the remaining six cliques are mixed cliques (with growth curve gM ).It is easy to see that root and mixed cliques are the only clique types induced by bipartite BNs, and that root cliquesare interior nodes in the clique tree while mixed cliques are leaf nodes. An illustration of Definition 19 is provided by theclique tree in Fig. 6. The BN from which this clique tree is compiled is depicted in Fig. 2 and in Fig. 4.We now consider clique trees generated from random BNs. Random variables K T , K R , and K M are used to represent thetotal clique tree size, the size of all root cliques, and the size of all mixed cliques respectively:K T = K R + K M .(15)S , with corresponding random variables ¯K∗S ) and= K T in (15). Total clique tree size is the sum of the clique sizes of both types, as is appropriate for clique tree clusteringOf particular interest are upper bounds ¯k¯Kalgorithms including Hugin. We use (15) and linearity of expectation to obtainS , for which we have Pr( ¯K= ¯k∗S∗S∗∗E(K T ) = E(K R ) + E(K M ),μT = μR + μM .(16)In the experimental part of this article, μR will be estimated using its sample mean ˆμR . Collections of such sample means,or the raw data sets themselves, can then be used to construct growth curves by means of regression.Let X be the predictor (or independent) random variable, and Y the response (or dependent) random variable. In aregression setting, one is interested in the conditional expectation(cid:10)μ(x) = E(Y | X = x) =y f ( y | x) dy,which along with (16) gives μT (x) = μR (x) + μM (x), which are deterministic functions of x. Here, x represents variation inone or more of BPART’s input parameters. For instance, C may be varied while V , P , and S are kept constant; see Section 3.1for details.The discussion above is intended as a background for understanding the benefit of quantitative growth curves, which wenow introduce.Definition 20 (Clique tree growth curve ). Let g R : R → R and gM : R → R. Further, let g R (x) be the growth curve for all rootcliques and gM (x) the growth curve for all mixed cliques. The (total) clique tree growth curve for a bipartite BN is defined asgT (x) = g R (x) + gM (x).Given Definition 20, we provide a qualitative discussion of the growth of BPART BNs in terms of the C/V -ratio, and putx = C/V . This discussion is supported by previous (see [45,41]) and current (see Section 5) experiments, as well as theconnection between BPART and random graphs (see Section 4), and motivates our introduction of growth curves. In orderto keep our discussion relatively simple, we identify three broad stages of clique tree growth, reflecting the growth of rootcliques: The initial growth stage, the rapid growth stage, and the saturated growth stage. The initial growth stage, wherethe C/V -ratio is “low”, is characterized by “few” leaf nodes relative to the number of root nodes. There is consequently arelatively low contribution by root cliques to the clique tree. In terms of gT (x), this stage is dominated by mixed cliques andgM (x). Indeed, as C/V → 0 there are no root cliques with more than one root node. During the rapid growth stage, wherethe C/V -ratio is “medium”, root cliques of non-trivial size start emerging, due to formation of cycles where fill-in edges arerequired in order to triangulate the moral graph. An example of the emergence of such a cycle can be seen in Fig. 2 andFig. 4. In this stage, and due to the addition of fill-in edges, the root cliques and g R (x) gradually overtake the mixed cliquesin terms of their contribution to total clique tree size gT (x). The saturated growth stage, where the C/V -ratio is “high”, ischaracterized by a “large” number of leaf nodes relative to the number of root nodes. As C/V approaches infinity, one rootclique with V BN root nodes (and size S V in the BPART model) emerges. In this stage, the mixed cliques eventually start todominate again, since there is one root clique which has reached its maximal size and cannot grow further. However, sincethe root clique size is exponential in the number of root nodes, it typically takes a long time before the mixed cliques startdominating again. For large V this effect can be disregarded, and main focus should generally be on the rapid growth stage,996O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006Fig. 7. Left: Gompertz curves g1(x) = 220e(cid:6)1(x), gRight: Growth rates g(cid:6)2(x) and g(cid:6)3(x) for the Gompertz growth curves.−5e−0.3x(green dotted curve), g2(x) = 220e−15e−0.3x(black solid curve), and g3(x) = 220e−5e−0.2x(blue boxed curve).and in particular the early part of it, in the transition between the initial and rapid growth stages, where g R (x) is becomingthe dominating factor in gT (x).In the following, we will often discuss g R (x) and gM (x) independently. In fact, as reflected in the informal discussionabove, the growth curve for mixed cliques gM (x) is generally less dramatic than the growth curve for root cliques g R (x).Therefore, we will place more emphasis on g R (x) in this article, and investigate restricted growth curves suitable for repre-senting this function.A number of sigmoidal growth curves (“S-curves”) have been used to model restricted growth, including the logistic,Gompertz, Complementary Gompertz, and Richards growth curves [4,35,17]. For restricted growth curves, limx→∞ g(x) al-ways exists and we define the restricting asymptote asg(∞) := limx→∞g(x).(17)For unrestricted growth curves, including the exponential growth curve, limx→∞ g(x) does not exist and there is no asymp-tote g(∞) as in (17).It turns out that restricted Gompertz growth curves give very good approximations of root clique growth g R (x) forBPART(V , C ), see Section 5, and we now study this family of curves in more detail.Definition 21 (Gompertz growth curve). Let ζ , γ ∈ R with ζ > 0 and γ > 0. A Gompertz growth curve is defined asg(x) = g(∞)e−ζ e−γ x.(18)The derivative g(x) = ddxg(cid:6)We now discuss some general properties due to the form of g(x) in (18). For x = 0, clearly e−γ x = 1, giving g(0) =−ζ in (18). In other words, the intersection of g(x) with the y-axis is determined by the parameters g(∞) and ζ in−γ x tends to 0, meaning that e−ζ . On the other hand, as x → ∞ in (18), etends to 1 and thus limx→∞ g(x) =−ζ e−γ xg(∞)eg(∞)eg(∞). The greater γ is, the faster e−γ x tends to zero, leading to faster convergence to the asymptote g(∞).(cid:6)(x) of the Gompertz growth curve isg(x) = g(∞)ζ γ e−γ xe−ζ e−γ x,(19)and expresses the growth rate of g(x); clearly g(cid:6)(x) > 0 given our assumptions in Definition 21.In Fig. 7 we investigate graphically a few examples of how the parameters g(∞) , ζ , and γ impact the shapes of Gom-pertz curves. The parameter g(∞) = 220 is obtained, for example, by considering bipartite BNs with V = 20 binary (S = 2)(cid:6)(x) changes when the parameters ζ and γ are varied. Let us first vary ζroot nodes. Fig. 7 also shows how the growth rate gas shown in Fig. 7. By increasing ζ from ζ = 5 to ζ = 15 while keeping γ = 0.3 constant, the x-location of maximal growth(cid:6)(x) at its maximum does not change. Let us next vary γ as is alsorate g(cid:6)(x)illustrated in Fig. 7. As γ decreases from γ = 0.3 to γ = 0.2, while ζ = 5 is kept constant, the x-location of maximal g(cid:6)(x) decreases with γ decreasing, and generally growth gets more gradual asincreases. In addition, the maximal value of gγ decreases.(cid:6)(x) is increased as well. However, the value of gIn the context of BNs, the independent variable x for the growth curve g(x) may be parametrized using x = C , x = C/V ,x = E/V = C P /V , or x = E(W ), depending on the data available and the purpose of the model. We now introduce, forBPART, a total growth curve model that includes a Gompertz growth curve.Theorem 22 (BPART Gompertz growth curve). The total growth curve gT (x) for BPART(V , C , P , S), assuming Gompertz growth forroot cliques and where x = C is the independent variable, is−γ x + xS P +1.gT (x) = S V e(20)−ζ eO.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006997Proof. Since BPART BNs are bipartite, the growth curve has the form gT (x) = g R (x) + gM (x), where g R (x) = g R (∞)e−γ xbecause we have the Gompertz growth curve. For BPART(V , C , P , S) we have g R (∞) = S V , and therefore g R (x) = S V efor appropriate choices of ζ and γ . Total mixed clique size is C × S P +1 [45], and hence gM (x) = xS P +1. By forming g R (x) +gM (x) we obtain the desired result (20). (cid:2)−γ x−ζ e−ζ eAnalytical growth models or growth curves have been used to model growth of organisms and tissue in biology andmedicine, growth of technology use or penetration, and growth of organizations or societies including the Web [4,35,17].However, our use of growth curves to model how clique tree size grows with x = C , x = C/V , or x = E(W ) is, to ourknowledge, novel.The Gompertz growth curve can be derived by solving the differential equation dg(x)/dx = ag(x), where a is a growthcoefficient [4]. Here, a is not constant but exponentially decreasing, formally da/dx = −ka for k > 0. These two equations canbe solved to obtain (18); see [4]. While a detailed study is beyond the scope of this article, it appears plausible that thesedifferential equations reflect, at a macroscopic level, clique tree clustering’s formation of cycles in a moral graph β(cid:6)alongwith the generation of fill-in edges. Once one cycle appears in β(cid:6), there may be many cycles appearing, all needing fill-inedges. Thus, once cycle formation starts in β(cid:6), a faster than exponential growth in root clique tree size g R (x) is realistic andindeed supported by previous experimental results [45]. This hyper-exponential growth is in this article captured by usingGompertz growth curves.We emphasize that Gompertz curves do not always provide accurate models of clique tree growth. For example, the(cid:6)R (x) > 0 does not reflect reality for very small x = C . Consider the first few BN leaf nodes added by BPART. Whenproperty gthere is no leaf node and x = 0, clearly μR (0) = V and μM (0) = 0. When there is one leaf node with P parents and x = 1,μR (1) = V − P and μM (1) = S P +1. Since μR (0) > μR (1), the contribution of the root cliques to the total clique tree size in(cid:6)fact decreases from x = 0 to x = 1, and clearly this is not consistent with gR (x) > 0 as follows for example from (19). Thesituation is similar for other small values of x, see Fig. 4 for x = 2. However, this early stage of growth is not very interestingsince the total clique tree size is extremely small and typically not a concern in applications. Consequently, we consider thisissue not to be an important limitation of our approach, and we use C/V (cid:2) 1/2 in our experiments below.Finally, we note that the Gompertz growth curve has a linear form, defined as follow [35].Definition 23 (Gompertz linear form). The Gompertz linear form is(cid:7)ln− ln(cid:8)g(x)g(∞)= ln(ζ ) − γ x.(21)Using (21), the Gompertz curve parameters ζ and γ in (18) can be estimated from data using linear regression, as wewill see in Section 5. Other growth curves, including logistic and Complementary Gompertz, have forms similar to (18) thatare also useful for parameter estimation by means of linear regression [35].3.3. Growth curves for general Bayesian networksWe now briefly discuss the generalization from bipartite BNs, considered earlier in this section, to arbitrary BNs. Thereare at least two ways of going beyond bipartite BNs:• Generalization of our analytical approach, which is tailored to bipartite BNs, to arbitrary BNs. There are several ways ofdoing this. First, one can maintain the use of two growth curves, but re-consider how cliques are assigned to them, inorder to handle arbitrary BNs. Second, one can potentially go beyond two growth curves, and base analysis on a finiteset of growth curves {g1(x), g2(x), . . . , gk(x)} where k (cid:2) 1. We discuss this approach in Section 3.3.1.• Translation of arbitrary BNs, perhaps via some intermediate form, into bipartite BNs. Such a translation can, for example,be based on the connection between Bayesian networks and factor graphs (which are bipartite graphs) [32,19]. Theresulting bipartite BNs can then be handled using the techniques discussed elsewhere in this article. We discuss thisapproach in Section 3.3.2.3.3.1. Generalization using growth functionsWe now discuss two approaches that generalize our growth curve analysis discussed earlier in Section 3. The first ap-proach continues to use two growth curves, but generalizes their meaning. Specifically, we introduce growth curves g1(x)and g2(x). Here, g1(x) represents cliques that contain leaf nodes and their parents (similar to gM (x)), while g2(x) repre-sents the remaining cliques (similar to g R (x)). Consequently, g2(x) represents the growth of not only cliques containing rootnodes only, but also cliques containing trunk nodes only, as well as cliques containing both trunk and root nodes. Clearly,this is a rather straightforward generalization approach, which may be too simple in some contexts, leading us to introducethe following alternative.The second approach amounts to introducing an arbitrary number of growth curves. We consider a clique tree Γ gen-erated from an arbitrary BN by clique tree clustering. One way to formalize the partitioning of cliques in a clique treeΓ = {γ1, . . . , γη} is by means of coloring the nodes in a graph (for us, a BN or a clique tree) as follows.998O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006Definition 24 (Graph coloring). Let G = (V , E) be a graph, let Φ = {1, . . . , φ} be a set of colors, and let h : V → Φ be a map(or coloring) from nodes to colors. Then (G , Φ, h) forms a graph coloring.The coloring defines a partitioning of a graph’s nodes into φ partitions. Definition 24 applies to both directed graphs(including DAGs) and undirected graphs (including clique trees). For BNs we will abuse notation slightly by saying that (β,Φ, h) is a graph coloring when β = (X , E , P ) is a BN; strictly speaking the coloring is in this case only for the DAG part(X , E) of the BN.The following definition of a coloring h partitions nodes into root nodes and non-root nodes.Definition 25. Let G = (V , E) be a DAG and let Φ = {1, 2} be a set of colors. The coloring h : V → Φ reflects the root versusnon-root status for any V ∈ V , and is defined as(cid:11)h(V ) =1 if i(V ) = 0,2 if i(V ) > 0.Similar to Definition 25, one can define a coloring that partitions nodes into leaf nodes and non-leaf nodes.How does graph coloring apply to BNs and their clique trees? A clique in a clique tree of a BN β consists of one or moreBN nodes, and these nodes may or may not have different colors as induced by a graph coloring (β, Φ, h). To reflect this,we introduce the concept of a color combination for a coloring, and have the following obvious result.Proposition 26. Let (G, Φ, h) be a graph coloring and let φ = |Φ|. The number of (non-empty) color combinations isκ(φ) = 2φ − 1.Similar to Definition 19, we partition the cliques, now according to color combinations. Formally, this amounts to formingsubsets of cliques Γi for 1 (cid:3) i (cid:3) κ(φ) such that Γ = Γ1 ∪ · · · ∪ Γκ(φ) and Γi ∩ Γ j = ∅ for i (cid:11)= j. In the bipartite special casediscussed in Section 3.2, κ(φ) = 2 and Γ = Γ1 ∪ Γ2, where Γ1 are the root cliques and Γ2 the mixed cliques. Assuming thatBNs are randomly distributed, we let K i be the random size of the cliques having the i-th color combination. By summing,we obtain a random variable K T representing the total clique tree size:K T =κ(φ)(cid:6)i=1K i.(22)This sum is a generalization of (15), which applies to the bipartite case.For each possible color combination, and reflecting the growth of the individual random variables K i in (22), we intro-duce a separate growth curve gi with parameters θ i as follows.Definition 27. Let (G, Φ, h) be a graph coloring with φ = |Φ| and let gi : R → R be a map with parameters θ i . Theng(x; θ ) =κ(φ)(cid:6)i=1gi(x; θ i),where g : R → R and θ = (θ 1, . . . , θ κ(φ)), is a total growth curve.(23)In words, Definition 27 adds up the growth curves for each color combination. A color combination corresponds to atype of clique. In this manner, we decompose the problem of estimating growth curves for complete clique trees into sub-problems of estimating growth curves for smaller pieces of clique trees. In the bipartite case, we have one color combinationfor mixed cliques and another color combination for root cliques, see Definition 20, resulting in two growth curves g R (x)and gM (x) on the right hand side of (23). We place no restrictions on the partitioning in Definition 24 and Definition 27,but for our purposes it typically makes sense to (i) let the coloring reflect the structure of a graph and (ii) only introduceas many colors as is needed.3.3.2. Generalization using factor graphsWhile our main emphasis in this article is on Bayesian networks and clique trees, many alternative approaches to rep-resenting multi-variate probability distributions by means of probabilistic graphical models exist. These alternatives includefactor graphs [32,70], Tanner graphs, Markov random fields [68,69], and arithmetic circuits [12].For the purpose of generalizing our growth curve approach to arbitrary BNs, factor graphs [32,70] turn out to be ofparticular interest. Informally, a factor graph (FG) is a bipartite graph in which root nodes are variables, leaf nodes arefactors (or functions), and a directed edge expresses an “is an argument of” relationship between a variable and a function.More formally, we have the following definition.Definition 28. Suppose that the function h( X1, . . . , Xn) admits the factorizationO.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006h(X1, . . . , Xn) =m(cid:2)i=1f i(S i),999(24)where S i ⊆ { X1, . . . , Xn} for 1 (cid:3) i (cid:3) m. Then the factor graph of h is defined as a (directed) bipartite graph (X, F , E) inwhich variables X = { X1, . . . , Xn} are root nodes; F = { f 1, . . . , fm} are leaf nodes; and E are edges between X and F suchthat ( X j, f i) ∈ E if and only if X j ∈ S i .Intuitively, the global function h in (24) is decomposed into products of local functions F , and each local functionf ∈ F only depends on a (hopefully small) subset S ⊆ { X1, . . . , Xn}. We are interested in probabilistic inference, whereh( X1, . . . , Xn) represents a joint probability distribution over discrete random variables. Summary propagation algorithms,which are iterative algorithms that utilize “summaries” or “messages” [36], have been introduced that exploit the factorgraph representation of h. Summary propagation algorithms come in two flavors, sum-product algorithms and max-productalgorithms. Using sum-product algorithms, one can use factor graphs to compute marginal distributions over Xi for 1 (cid:3)i (cid:3) m. The Viterbi algorithm [67,57], generalized to arbitrary tree-structured graphical models, is a max-product algorithmcalled max-product belief propagation [32].Having briefly introduced factor graphs, we now discuss how they relate to our use of bipartite BNs elsewhere in thisarticle. Factor graphs have been extended to unify and generalize directed graphical models (Bayesian networks) and undi-rected graphical models (Markov networks) [19]. The studies of Bayesian networks, Markov networks, and factor graphs aretherefore closely related. We now specifically exploit the close connection between factor graphs and BNs, and consider atwo-step translation process (see [70] for details). First, we translate an arbitrary BN β1 into a factor graph φ. Second, wetranslate φ into a bipartite BN β2, which is related to but different from β1. A factor graph factor from φ becomes a leafnode in the bipartite BN β2; a factor graph variable from φ becomes a root node in β2.We now make a few observations related to this translation process: There are no topological restrictions on β1; β2 isguaranteed to be bipartite; and the size of β2 is modest relative to β1. This translation approach means that we can translatean arbitrary BN β1 into a bipartite BN β2, and then apply to β2 the analytical and experimental machinery discussedelsewhere in this article.4. Random graphs and random Bayesian networksSimilar to BNs, random graphs are founded on graph theory. Random graphs were explored in the 1950s and early1960s by Solomonoff and Rapoport [65] as well as Erd ˝os and Rényi [18]. Compared to previous graph theory research, thecontribution of research on random graphs was and is its emphasis on graphs as probabilistic objects, which is similar toour perspective on random BNs including BPART BNs. Random graphs continue to be studied as part of graph theory inpure mathematics [7], but interesting connections have also been made to applied areas including social networks; spreadof disease; spread of information; and information search in the World Wide Web [49]. While a comprehensive discussionof random graphs is beyond the scope of this article, we now briefly discuss the connection between random graphs andrandom BNs, and in particular random BNs as generated by the BPART algorithm.concerned with undirected graphs. In G(n, p), each of theTwo prominent random graph models over n vertices are denoted G(n, p) and G(n, m) respectively. Both models are(cid:5)nedges is included with a probability p. In G(n, m), exactly(cid:5)2n2edges, without replacement. The sets of graphs defined bym edges are added uniformly at random, from among allG(n, p) and G(n, m) form probability spaces.(cid:4)(cid:4)It turns out that many random graph models, including G(n, p) and G(n, m), are in several respects similar. The numberof edges in G(n, p) clearly follows a Binomial distribution, and so the expectation is pthen G(n, p) andG(n, m) behave, in many respects, the same [7]. In particular, there is often an emergence of certain global properties —including graph structures in the form of trees, cycles, and cliques — as local connectivity parameters such as p (for theG(n, p) model) or m (for the G(n, m) model) increase for a fixed or varying n. How is a property likely to emerge? Erd ˝osand Rényi studied p(n) as n → ∞ and found that emergence is often fast. In other words, many properties quickly go fromvery unlikely to very likely; there is a phase transition at a certain probability p(n).. If m ≈ pn2n2(cid:4)(cid:4)(cid:5)(cid:5)Random graphs can be studied from an evolutionary perspective as well. In this perspective, edges are added one by one,edges as p → 1. (Clearly, asstarting with zero edges in the random graph and approaching a fully connected graph withwill be further discussed below, there is a similarity between increasing p in the G(n, p) model and increasing the C/V -ratioin the BPART model.) Early on in random graph evolution, edges are isolated, and isolated components form. There are nocycles, because edges are initially likely to merge components rather than create cycles. Gradually, some unicycles and treesshow up, however the largest components are only log n in size. Then, all of a sudden, comes the so-called double-jump[18] where two things happen. First, the number of cycles increases dramatically. Second, the size of the largest componentgrows, and the growth depends on whether c < 1 or c > 1. With p = cn and c < 1, the random graph consists of smallcomponents, the largest of size Θ(log n). For c > 1, on the other hand, many of these small components have clustered intoa “giant” component of size Θ(n). In other words, the giant component emerges when the average node degree is np = 1.n2(cid:5)(cid:4)1000O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006Which, if any, of the random graph models G(n, p) and G(n, m) is most relevant to the work discussed in this article?BPART(V , C , P ) is, under certain conditions which we discuss shortly, very similar to G(n, m). The conditions are as follows:1. For BPART(V , C , P ), the moral edges between the root nodes (induced by leaf nodes), or in other words the undirectedmoral graph induced by the root nodes, is what is important for our purposes, and we put n = V .2. In BPART(V , C , P ), we consider P = 2, since this yields independence between moral edges, similar to how edges arepicked independently under both G(n, p) and G(n, m).To make the connection between the G(n, m) model and BPART more explicit, we set n = V and P = 2, as stated in thetwo conditions above, and we may say BPART(n, C ) instead of BPART(V , C , P ).Now, the only difference between G(n, m) and BPART(n, C ) is that the former picks edges without replacement, while the(cid:13) m, thenlatter picks edges with replacement. However, this difference may not be that important in some situations. Ifthis difference can often, for approximation purposes, simply be ignored and we set m = C . Alternatively, if the differencecannot be ignored, one may proceed as follows. Without loss of generality, we assume that m and n are fixed in G(n, m),and consider E(W ; C, V ) as given by (12) or (13) after substituting V = n. In the case of (12), we can then obtain theinteger-valued lower bound C(cid:12) and upper bound Cu, where E(W ; C(cid:12), V ) (cid:3) m (cid:3) E(W ; Cu, V ) and C(cid:12) = Cu − 1. We then useC(cid:12) and Cu as input parameter to BPART, and use BPART(n, C(cid:12)) and BPART(n, Cu ) to bound G(n, m). If instead of (12) we use(13), one can put E(W ) = m and then solve for C in (13). Obviously, the solution for C will not in general be integer-valued,so one can use C(cid:12) = (cid:14)C(cid:15) and C(cid:12) = (cid:16)C(cid:17) as inputs to BPART in a manner similar to above.n2(cid:5)(cid:4)The issue of the treewidth of random graphs has, to our knowledge, not been extensively researched. However, there aresome results, which we now briefly review [30]. The following two analytical results apply to the early evolution of randomgraphs, before the giant component emerges.Lemma 29. Let 0 < c < 1 and suppose that p = c/n. Then almost every graph G(n, p) is such that every connected component is atree or a unicycle graph.Corollary 30. If m < n2 , then almost every graph G(n, m) has treewidth at most two.With respect to the above lemma’s application to BPART, we note that triangulation of trees and unicycle graphs is quitesimple. Trees do not need any fill-in edges, of course, while triangulation of a unicycle graph with k nodes amounts toadding (cid:16) k(cid:17) fill-in edges. A consequence of Corollary 30 for BPART is that C < V /2, or C/V < 1/2, is not very interesting2from a treewidth perspective.We now state a result that applies for a broader range of random graphs [30], including after the emergence of the giantcomponent.Theorem 31. Let δ (cid:2) 1.18. Then almost every graph G(n, m) with m (cid:2) δn has treewidth Θ(n).Comparing G(n, m) and BPART(V , C ), m corresponds to C (the number of leaf nodes) while n corresponds to V (thenumber of root nodes). The condition m (cid:2) δn in Theorem 31 thus corresponds to C (cid:2) δ V , or C/V (cid:2) δ. It is clear fromCorollary 30 and Theorem 31 that C/V ≈ 1 is an interesting region in the setting of clique tree clustering for BNs, assumingthe BPART(V , C ) model.To summarize, it is clear that several fruitful connections can be made between random graphs and random BNs, anda few have been made above. In particular, we hypothesize that there is a connection between the onset of the rapidgrowth phase observed for BPART BNs and δ ≈ 1.18 in Theorem 31, and that this behavior can be modeled using large(cid:6)(x) of a restricted growth curve g(x) such as a Gompertz growth curve. At the same time, there are manyvalues for gcaveats concerning the use of analytical results for random graphs in the analysis of random BNs; here are some of them.We start by identifying two structure-related issues. First, even if we ignore the small difference between G(n, m) andBPART(V , C , 2) identified above for a moment, it is clear that G(n, m) makes independence assumptions that are not madeby BPART(V , C , P ) for P > 2. Second, while there has been some work on treewidth of random graphs, as discussed brieflyabove, optimal treewidth has not been a central topic in random graphs. And even if it were, and if we take a strictlygraph-theoretic perspective, the issue of optimizing the maximal clique of random graphs is not the same as optimizingtotal clique tree size, even though they clearly are related. Total clique tree size has previously been emphasized for BNs[28,29]. Third, random graphs are typically considered in the limit n → ∞, while this is generally not the situation forrandom BNs and BNs in general. Fourth, random graphs are strictly graph-theoretic, and one does not consider state spaces,which can exhibit important variation in BNs. To make further progress on understanding clique tree growth for the BPARTmodel despite these limitations in applying results from the theory of random graphs, we now turn to our experimentalresults.5. ExperimentsO.J. Mengshoel / Artificial Intelligence 174 (2010) 984–10061001In the experiments we address the following questions in the context of bipartite BNs sampled using BPART: How welldo Gompertz growth curves match sample data in the form of clique trees generated using tree clustering, when the inde-pendent parameter as well as the nature of the sample data points are varied? How well do Gompertz growth curves fitsample data compared to alternative growth curve models? In answering these questions, we extend and complement pre-vious experimental results [45] by: (i) introducing restricted growth curves, including Gompertz growth curves, in additionto sample means and unrestricted exponential growth curves; (ii) using a greater range of values for C/V ; (iii) consideringboth V = 20 and V = 30; (iv) investigating x = E(W ) in addition to x = C/V as the independent parameter; and (v) usingas the dependent parameter the total clique tree size ¯kS . Clique treeswere generated, for sample BNs generated using BPART as indicated below, using an implementation of the Hugin cliquetree clustering algorithm. Clique trees were optimized heuristically, using the minimum fill-in weight triangulation heuristic,as treewidth computation is NP-complete.S rather than the size of the optimal maximal clique ¯(cid:12)∗∗In the rest of this experimental section, we discuss in Section 5.1 clique tree growth in the case of BPART BNs withV = 30 root nodes, using x = E(W ). In Section 5.2 we investigate BPART BNs with V = 20 root nodes and consider bothx = E(W ) and x = C/V . In Section 5.3 we investigate the growth of individual BPART BNs.5.1. Comparison between growth models for multiple BNsThe purpose of the first set of experiments was to compare the Gompertz growth model with a few alternatives: Ex-ponential, logistic, and complementary Gompertz. Here, we report on Bayesian networks generated using the signatureBPART(30, C , 2, 2) with varying values for C , specifically 15 (cid:3) C (cid:3) 600 or 1/2 (cid:3) C/V (cid:3) 20. For each C/V -level considered,100 BNs {β1, . . . , β100} were sampled using BPART, and clique trees with respective sizes {¯k∗S (β100)} computedusing the Hugin system.S (β1), . . . , ¯kWe now present the results of the Hugin experiments. In the top panel of Fig. 8, sample means ˆμR (x) along with cor-responding points from different analytical growth curves g R (x) as a function of x = E(W ) are presented. Sample meansˆμR (x) are obtained by averaging, for a particular x, over {¯k∗S (β100)} and then deducting gM (x). Here, the Gom-pertz, logistic, complementary Gompertz, and exponential functions are considered for g R (x). The bottom panel of Fig. 8shows how the growth curves in the top panel were obtained using linear forms such as (21). The following Gompertzgrowth curve was obtained(cid:4)S (β1), . . . , ¯k∗∗g R (x) = 230 × exp(cid:5)−19.14 × exp(−0.005874x),where x = E(W ). The parameters ζ and γ were for the other growth curves computed in a similar manner. Clearly, theGompertz curve fits the data much better than the alternative growth curves analyzed, with R 2 = 0.9995 (for Gompertz)versus R2 = 0.9413 (for logistic) and R2 = 0.9407 (for Complementary Gompertz). The excellent fit can also easily beconfirmed visually by considering the sample means along with the corresponding data points for the Gompertz curve inthe top panel of Fig. 8.5.2. Gompertz growth model details for multiple BNsIn a second set of experiments, Bayesian networks were generated using BPART(V , C , 2, 2) with V = 20 and varyingvalues for C , specifically 10 (cid:3) C (cid:3) 1400 or 1/2 (cid:3) C/V (cid:3) 70. Similar to above, for each C/V -level, 100 BNs were sampledusing BPART, and Hugin was used to compute clique trees with respective sizes {¯k∗S (β100)}. Using this relativelylow value for V allowed us to generate BNs for which the generated clique trees did not exhaust the computer’s memoryeven for very large C , thus supporting a comprehensive analysis using Gompertz growth curves with both x = C/V andx = E(W ) as independent parameters.S (β1), . . . , ¯k∗Fig. 9 illustrates the results of these experiments. Here, the left column of Fig. 9 presents Gompertz growth curvesg R (x), while the right column illustrates how these growth curves were obtained using (21) similar to above. In the toprow of Fig. 9, sample means as well as corresponding points from a Gompertz growth curve as a function of C/V -ratioare presented. As a baseline, an exponential interpolation curve for the sample means is also provided. Empirically, theGompertz growth curve was found to beg R (x) = 220 × exp(cid:4)(cid:5)−9.906 × exp(−0.1118x),where x = C/V and with R2 = 0.993477. The parameter values of ζ = e2.293 = 9.906 and γ = 0.1118 were obtained fromthe Gompertz linear form as illustrated to the top right in Fig. 9, based on sample means for the clique tree root cliquesand the linear regression result ln(ζ ) − γ x = −0.1118x + 2.293.In the bottom row of Fig. 9, we plot the expected number of moral edges E(W ) along the x-axis. Note that the right-most sample average in the bottom row of Fig. 9, at x = E(W ) ≈ 123, corresponds to the sample average at C/V = 10 inthe top row of Fig. 9. In other words, the use of x = C/V allows us to illustrate a broader range of behavior, through the1002O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006Fig. 8. Experimental results for bipartite BNs with V = 30 root nodes and varying number of leaf nodes C . Top: Comparison of Gompertz and other growthcurves with the sample means. The superior fit of the Gompertz curve is reflected in its better R2 value, namely R2 = 0.99948. Bottom: Linear formsshowing how the growth curves above were obtained.inclusion of BNs with a larger number of leaf nodes, compared to when x = E(W ) is used. We present sample means alongwith the corresponding points from a Gompertz growth curve as a function of E(W ); an exponential regression curve ispresented as a baseline. Here, the Gompertz growth curve was empirically determined to beg R (x) = 220 × exp(cid:4)(cid:5)−12.43 × exp(−0.01187x),where x = E(W ) and with R2 = 0.999215. The parameters ζ and γ were computed in a similar manner to above and assummarized to the bottom right in Fig. 9.We now revisit the three broad growth stages discussed in Section 3 and Section 4 in terms of Fig. 9. The sample meansshow an easy-hard-harder pattern, or monotonically increasing clique tree sizes, along these stages. The initial growth stage,where the C/V -ratio is “low” (for P = 2, up to approximately C/V ≈ 1), is characterized by “few” leaf nodes relative to thenumber of root nodes. The initial growth stage is in fact difficult to see in Fig. 9, since there are only a few sample pointsfor this stage and they are very close to each other. In the rapid growth stage, the C/V -ratio is “medium” (for P = 2, fromapproximately C/V ≈ 1 to say C/V ≈ 20) and non-trivial root cliques appear. As can be seen from the sample means to theleft in Fig. 9, growth is initially faster than indicated by the exponential regression curve and then slows down. Clearly, theGompertz growth curves give much better fits than the respective exponential curves for both C/V and E(W ). The saturatedgrowth stage, where the C/V -ratio is “high”, is characterized by slow or no growth due to saturation. At saturation, there isone root clique γ with |Ωγ | = 220, hence there is no room for further growth. In Fig. 9, we may say that saturation startsat C/V ≈ 20.Fig. 9 clearly shows the improved fit provided by Gompertz curves compared to exponential curves. Further, x = E(W )provides a better fit than x = C/V but for a narrower domain.O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–10061003Fig. 9. Empirical results for bipartite Bayesian networks generated with V = 20 root nodes and a varying number of leaf nodes C . Top left: Gompertz growthcurve as a function of the C/V -ratio. Top right: Gompertz growth curve’s linear form as a function of the C/V -ratio; used to create the Gompertz growthcurve to the left. Bottom left: Gompertz growth curve as a function of E(W ). Bottom right: Gompertz growth curve’s linear form as a function of E(W ); usedto create the Gompertz growth curve to the left.5.3. Comparison between growth models for individual BNsThe experimental results so far in this section have been based on constructing growth curves g R (x) using sample meansˆμR (x) of clique tree sizes for 100 BNs per C/V -value. What happens when individual BNs, instead of multiple BNs, are usedto construct growth curves g R (x)? To investigate this question, we considered in a third set of experiments BNs generatedusing the signature BPART(20, C ), with C varying from C = 100 to C = 1200. The following protocol was followed in orderto create a sequence of closely related BNs. Starting with a sampled BPART(20, 1200) BN, 100 leaf nodes were deletedat a time, giving a sequence of BNs consisting of a BPART(20, 1100) BN, a BPART(20, 1000) BN, and so forth, down to aBPART(20, 100) BN. Obviously, in a real development setting the sequence of BNs might be quite different than what weused here, and in particular a machine learning algorithm or a knowledge engineer might start with a small BN and growit, rather than the other way around. The manner in which the sequence of BNs is created for our experimental purposesdoes not matter as long as they are all BPART BNs, which they clearly are here.Experimental results for two sequences of clique trees generated from the two sequences of BNs, generated accordingto the above protocol, are presented in Fig. 10. For the β0 sequence (top of Fig. 10), the regression results for ¯k∗S are:Gompertz curve g R (x) = −0.1125x+ 2.2873 and R2 = 0.9571; Logistic curve g R (x) = 0.2676x− 6.3575 and R2 = 0.9265; andComplementary Gompertz curve g R (x) = 0.2411x − 6.1417 and R2 = 0.8962. For the β1 sequence (bottom of Fig. 10), theregression results for ¯k∗S are: Gompertz curve g R (x) = −0.0816 + 1.9406 and R2 = 0.971; Logistic curve g R (x) = 0.2205x −5.7281 and R2 = 0.8361; and Complementary Gompertz curve g R (x) = 0.2063x − 5.635 and R2 = 0.8074.This figure clearly shows the better fit provided by Gompertz curves compared to a few alternatives. The better fit isreflected in the higher R2 values for the Gompertz curves for both sequences. We note that the R 2 values found here,for the Gompertz curves, are smaller than the R2 values for the Gompertz curves found in Section 5.1 and Section 5.2.A key point in this regard is that each data point here represents the clique tree size of a single BN, while each data point1004O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006Fig. 10. Experimental results for sequences of individual bipartite BNs with V = 20 root nodes and a varying number of leaf nodes. Comparison of Gompertzand other growth curves, as a function of C/V , is shown for two sequences of BNs. Top: The superior fit of the Gompertz curve for one sequence of BNs isreflected in a higher R2 value, namely R2 = 0.9571. Bottom: The superior fit of the Gompertz curve for another sequence of BNs is reflected in a higher R2value, namely R2 = 0.971.in Section 5.1 and Section 5.2 represents the sample mean clique tree size for 100 BNs. The poorer fit reported here istherefore not surprising.6. Conclusion and future workSubstantial progress has recently been made, both in the area of Bayesian network (BN) reasoning algorithms and inthe area of applications of BNs. Based on experience from applications, it is clear that Bayesian networks are useful andpowerful but some care is needed when constructing them. In particular, due to the inherent computational complexity ofmost interesting BN queries [10,63,61,1], one may want to carefully consider the issue of scalability when generating BNs forresource-bounded systems including real-time and embedded systems [48,40]. BN generation may be performed manually,by means of knowledge-based model construction, or using machine learning methods. In resource-bounded systems, BNcompilation approaches including clique tree propagation [33,2,25,62] and arithmetic circuit propagation [12,9,8] are ofparticular interest [43]. In the clique tree approach, which we emphasize in this article, BN inference consists of propagationin a clique tree that is compiled from a Bayesian network. Total clique tree size is important because it determines theinference time. Unfortunately, a precise understanding of how varying structural parameters in BNs causes variation in thesizes of the induced clique tree sizes has been lagging. To attack this problem, we have in this article investigated theclique tree clustering approach, using bipartite BNs sampled by means of the BPART algorithm, by employing restricted andunrestricted growth curves. We have characterized the growth of clique tree size as a function of (i) the expected numberof moral edges or (ii) the C/V -ratio, where C is the number of leaf nodes and V is the number of non-leaf nodes. In thisarticle, we varied both (i) and (ii) by increasing the number of leaf nodes in our bipartite BNs, and also discussed how theapproach applies to arbitrary BNs. Gompertz growth curves have, for the bipartite BNs investigated, been shown to giveexcellent fit to empirical clique tree data and they appear theoretically plausible as well.O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–10061005The growth curve approach presented in this article and in an earlier paper [41] is novel and extends previous work[45]. We consider the expected number of moral edges E(W ) as well as the C/V -ratio, and a wide range of C/V -ratiovalues. We focus on the total clique tree size as opposed to size of the largest clique in the clique tree. We believe that theresearch reported here helps to fill a gap that appears to exist between theoretical complexity results and empirical resultsfor specific algorithms and application BNs. To fill this gap, we have here presented an approach that combines probabilisticanalysis, restricted growth curves, and experimentation. Analytically and experimentally, we have shown that the restrictedgrowth curves induce three stages for growing Bayesian networks: The initial growth stage, the rapid growth stage, and thesaturated growth stage. These stages are similar to what has been found for the evolution of random graphs. Our growth-curve results provide more detail compared to pure complexity-theoretic results; however they admittedly gloss over detailsavailable in the raw experimental data.Areas for future work include the following. First, this type of approach may be utilized in trade-off studies for thedesign of vehicle health management systems including diagnostic reasoners [40], in the analysis of knowledge-based modelconstruction algorithms, and perhaps even in the study of machine learning. In all these cases there is uncertainty regardingthe impact of different BN structures on clique tree size (and consequently computation time). In knowledge-based modelconstruction, BNs are constructed dynamically, while during the early design of health management systems there may belittle information available concerning the vehicle being developed. In machine learning, especially in structure learning, onecould during model selection score BNs according to their estimated computational feasibility in addition to their statisticalfit. Second, these analytical growth curves can also used to perform forecasts and derive requirements for very large-scaleBNs, which may have clique trees larger than what current software or hardware are capable of supporting. Third, it wouldbe natural to develop more fine-grained analytical models, including more accurate models for arbitrary number of parents,perhaps by improving our analytical growth models based on more extensive experimentation. Finally, further explorationof the connection between random BNs, random graphs, and BNs from applications would also be interesting.AcknowledgementsThis material is based upon work supported by NASA under awards NCC2-1426, NNA07BB97C, and NNA08205346R aswell as NSF awards CCF0937044 and ECCS0931978. Comments from the anonymous reviewers, which helped improve thearticle, are also acknowledged.References[1] A.M. Abdelbar, S.M. Hedetnieme, Approximating MAPs for belief networks is NP-hard and other theorems, Artificial Intelligence 102 (1998) 21–38.[2] S.K. Andersen, K.G. Olesen, F.V. Jensen, F. Jensen, HUGIN — a shell for building Bayesian belief universes for expert systems, in: Proceedings of theEleventh International Joint Conference on Artificial Intelligence, vol. 2, Detroit, MI, August 1989, pp. 1080–1085.[3] S. Arnborg, D.G. Corneil, A. Proskurowski, Complexity of finding embeddings in a k-tree, SIAM Journal of Algebraic and Discrete Methods 8 (1987)277–284.[4] R.B. Banks, Growth and Diffusion Phenomena, Springer, New York, 1994.[5] A. Becker, D. Geiger, Approximation algorithms for the loop cutset problem, in: Proceedings of the Tenth Annual Conference on Uncertainty in ArtificialIntelligence (UAI-94), San Francisco, CA, 1994, pp. 60–68.[6] T.W. Bickmore, A probabilistic approach to sensor data validation, in: AIAA, SAE, ASME, and ASEE 28th Joint Propulsion Conference and Exhibit,Nashville, TN, 1992.[7] B. Bollobas, Random Graphs, Cambridge University Press, 2001.[8] M. Chavira, Beyond treewidth in probabilistic inference, Ph.D. thesis, University of California, Los Angeles, 2007.[9] M. Chavira, A. Darwiche, Compiling Bayesian networks using variable elimination, in: Proceedings of the Twentieth International Joint Conference onArtificial Intelligence (IJCAI-07), Hyderabad, India, 2007, pp. 2443–2449.[10] F.G. Cooper, The computational complexity of probabilistic inference using Bayesian belief networks, Artificial Intelligence 42 (1990) 393–405.[11] A. Darwiche, Recursive conditioning, Artificial Intelligence 126 (1–2) (2001) 5–41.[12] A. Darwiche, A differential approach to inference in Bayesian networks, Journal of the ACM 50 (3) (2003) 280–305.[13] A.P. Dawid, Applications of a general propagation algorithm for probabilistic expert systems, Statistics and Computing 2 (1992) 25–36.[14] R. Dechter, Bucket elimination: A unifying framework for reasoning, Artificial Intelligence 113 (1–2) (1999) 41–85.[15] R. Dechter, Y. El Fattah, Topological parameters for time–space tradeoff, Artificial Intelligence 125 (1–2) (2001) 93–118.[16] R. Dechter, J. Pearl, Network-based heuristics for constraint satisfaction problems, Artificial Intelligence 34 (1) (1987) 1–38.[17] D.M. Easton, Gompertzian growth and decay: A powerful descriptive tool for neuroscience, Physiology & Behavior 86 (3) (2005) 407–414.[18] P. Erd ˝os, A. Rényi, On the evolution of random graphs, Publ. Math. Inst. Hung. Acad. Sci 5 (1960) 17–61.[19] B.J. Frey, Extending factor graphs so as to unify directed and undirected graphical models, in: Proc. of the 19th Conference in Uncertainty in ArtificialIntelligence (UAI-03), 2003, pp. 257–264.[20] R.G. Gallager, Low density parity check codes, IRE Transactions on Information Theory 8 (Jan 1962) 21–28.[21] F. Hutter, H.H. Hoos, T. Stützle, Efficient stochastic local search for MPE solving, in: Proceedings of the Nineteenth International Joint Conference onArtificial Intelligence (IJCAI-05), Edinburgh, Scotland, 2005, pp. 169–174.[22] J.S. Ide, F.G. Cozman, Generating random Bayesian networks, in: Proceedings on 16th Brazilian Symposium on Artificial Intelligence, Porto de Galinhas,Brazil, November 2002, pp. 366–375.[23] J.S. Ide, F.G. Cozman, F.T. Ramos, Generating random Bayesian networks with constraints on induced width, in: Proceedings of the 16th EuropeanConference on Artificial Intelligence, 2004, pp. 323–327.[24] T.S. Jaakkola, M.I. Jordan, Variational probabilistic inference and the QMR-DT database, Journal of Artificial Intelligence Research 10 (1999) 291–322.[25] F.V. Jensen, S.L. Lauritzen, K.G. Olesen, Bayesian updating in causal probabilistic networks by local computations, SIAM Journal on Computing 4 (1990)269–282.[26] P. Jones, C. Hayes, D. Wilkins, R. Bargar, J. Sniezek, P. Asaro, O.J. Mengshoel, D. Kessler, M. Lucenti, I. Choi, N. Tu, J. Schlabach, CoRAVEN: Modelingand design of a multimedia intelligent infrastructure for collaborative intelligence analysis, in: Proceedings of the International Conference on Systems,Man, and Cybernetics, San Diego, CA, October 1998, pp. 914–919.1006O.J. Mengshoel / Artificial Intelligence 174 (2010) 984–1006[27] K. Kask, R. Dechter, Stochastic local search for Bayesian networks, in: Proceedings Seventh International Workshop on Artificial Intelligence and Statis-tics, Morgan Kaufmann, Fort Lauderdale, FL, Jan 1999.[28] U. Kjaerulff, Triangulation of graphs: Algorithms giving small total state space, Technical Report R-90-09, Department of Mathematics and ComputerScience, Aalborg University, 1990.[29] U. Kjaerulff, Approximation of Bayesian networks through edge removals, Technical Report IR-93-2007, Department of Mathematics and ComputerScience, Aalborg University, 1993.[30] T. Kloks, Treewidth: Computations and Approximations, Springer-Verlag, 1994.[31] A.M.C.A. Koster, H.L. Bodlaender, S.P.M. van Hoesel, Treewidth: Computational experiments, in: H. Broersma, U. Faigle, J. Hurink, S. Pickl (Eds.), Elec-tronic Notes in Discrete Mathematics, vol. 8, Elsevier Science Publishers, 2001.[32] F.R. Kschischang, B.J. Frey, H.-A. Loeliger, Factor graphs and the sum-product algorithm, IEEE Transactions on Information Theory 47 (2) (2001) 498–519.[33] S. Lauritzen, D.J. Spiegelhalter, Local computations with probabilities on graphical structures and their application to expert systems (with discussion),Journal of the Royal Statistical Society Series B 50 (2) (1988) 157–224.[34] Z. Li, B. D’Ambrosio, Efficient inference in Bayes nets as a combinatorial optimization problem, International Journal of Approximate Reasoning 11 (1)(1994) 55–81.[35] J.K. Lindsey, Statistical Analysis of Stochastic Processes in Time, Cambridge University Press, Cambridge, 2004.[36] H.-A. Loeliger, An introduction to factor graphs, IEEE Signal Processing Magazine 21 (1) (2004) 28–41.[37] D.J.C. MacKay, Information Theory, Inference and Learning Algorithms, Cambridge University Press, Cambridge, UK, 2002.[38] R.J. McEliece, D.J.C. Mackay, J.-F. Cheng, Turbo decoding as an instance of Pearl’s belief propagation algorithm, IEEE Journal on Selected Areas inCommunications 16 (2) (1998) 140–152.[39] O.J. Mengshoel, Efficient Bayesian network inference: Genetic algorithms, stochastic local search, and abstraction, Ph.D. thesis, Department of ComputerScience, University of Illinois at Urbana-Champaign, Urbana, IL, April 1999.[40] O.J. Mengshoel, Designing resource-bounded reasoners using Bayesian networks: System health monitoring and diagnosis, in: Proceedings of the 18thInternational Workshop on Principles of Diagnosis (DX-07), Nashville, TN, 2007, pp. 330–337.[41] O.J. Mengshoel, Macroscopic models of clique tree growth for Bayesian networks, in: Proceedings of the Twenty-Second National Conference on Artifi-cial Intelligence (AAAI-07), Vancouver, British Columbia, 2007, pp. 1256–1262.[42] O.J. Mengshoel, Understanding the role of noise in stochastic local search: Analysis and experiments, Artificial Intelligence 172 (8–9) (2008) 955–990.[43] O.J. Mengshoel, A. Darwiche, K. Cascio, M. Chavira, S. Poll, S. Uckun, Diagnosing faults in electrical power systems of spacecraft and aircraft, in:Proceedings of the Twentieth Innovative Applications of Artificial Intelligence Conference (IAAI-08), Chicago, IL, 2008, pp. 1699–1705.[44] O.J. Mengshoel, D. Roth, D.C. Wilkins, Portfolios in stochastic local search: Efficiently computing most probable explanations in Bayesian networks,Journal of Automated Reasoning (2010), in press.[45] O.J. Mengshoel, D.C. Wilkins, D. Roth, Controlled generation of hard and easy Bayesian networks: Impact on maximal clique size in tree clustering,Artificial Intelligence 170 (16–17) (2006) 1137–1174.[46] O.J. Mengshoel, D.C. Wilkins, D. Roth, Initialization and restart in stochastic local search: Computing a most probable explanation in Bayesian networks,IEEE Transactions on Knowledge and Data Engineering (2010), in press.[47] D. Mitchell, B. Selman, H.J. Levesque, Hard and easy distributions of SAT problems, in: Proceedings of the Tenth National Conference on ArtificialIntelligence (AAAI-92), San Jose, CA, 1992, pp. 459–465.[48] D. Musliner, J. Hendler, A.K. Agrawala, E. Durfee, J.K. Strosnider, C.J. Paul, The challenges of real-time AI, IEEE Computer 28 (January 1995) 58–66.[49] M. Newman, A.L. Barabási, D.J. Watts (Eds.), The Structure and Dynamics of Networks, Princeton University Press, 2006.[50] A.Y. Ng, M.I. Jordan, Approximate inference algorithms for two-layer Bayesian networks, in: Advances in Neural Information Processing Systems, vol. 12(NIPS-99), MIT Press, 2000.[51] L. Otten, R. Dechter, Bounding search space size via (hyper)tree decompositions, in: Proc. of the 24th Conference on Uncertainty in Artificial Intelligence(UAI-08), 2008, pp. 452–459.[52] J.D. Park, A. Darwiche, Approximating MAP using local search, in: Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence(UAI-01), Seattle, WA, 2001, pp. 403–410.[53] J.D. Park, A. Darwiche, Complexity results and approximation strategies for MAP explanations, Journal of Artificial Intelligence Research (JAIR) 21 (2004)101–133.[54] J.D. Park, A. Darwiche, A differential semantics for jointree algorithms, Artificial Intelligence 156 (2) (2004) 197–216.[55] J. Pearl, A constraint-propagation approach to probabilistic reasoning, in: L.N. Kanal, J.F. Lemmer (Eds.), Uncertainty in Artificial Intelligence, Elsevier,Amsterdam, Netherlands, 1986, pp. 357–369.[56] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, San Mateo, CA, 1988.[57] L.R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proceedings of the IEEE 77 (1989) 257–286.[58] B.W. Ricks, O.J. Mengshoel, The diagnostic challenge competition: Probabilistic techniques for fault diagnosis in electrical power systems, in: Proc. ofthe 20th International Workshop on Principles of Diagnosis (DX-09), Stockholm, Sweden, 2009.[59] I. Rish, M. Brodie, S. Ma, Accuracy vs. efficiency trade-offs in probabilistic diagnosis, in: Eighteenth National Conference on Artificial Intelligence (AAAI-02), Edmonton, Canada, 2002, pp. 560–566.[60] C. Romessis, K. Mathioudakis, Bayesian network approach for gas path fault diagnosis, Journal of Engineering for Gas Turbines and Power 128 (1)(2006) 64–72.[61] D. Roth, On the hardness of approximate reasoning, Artificial Intelligence 82 (1996) 273–302.[62] P.P. Shenoy, A valuation-based language for expert systems, International Journal of Approximate Reasoning 5 (3) (1989) 383–411.[63] E. Shimony, Finding MAPs for belief networks is NP-hard, Artificial Intelligence 68 (1994) 399–410.[64] M.A. Shwe, B. Middleton, D.E. Heckerman, M. Henrion, E.J. Horvitz, H.P. Lehmann, G.F. Cooper, Probabilistic diagnosis using a reformulation of theINTERNIST-1/QMR knowledge base: I. The probabilistic model and inference algorithms, Methods of Information in Medicine 30 (4) (1991) 241–255.[65] R. Solomonoff, A. Rapoport, Connectivity of random nets, Bulletin of Mathematical Biology 13 (2) (June 1951) 107–117.[66] H.J. Suermondt, G.F. Cooper, Probabilistic inference in multiply connected belief networks using loop cutsets, International Journal of ApproximateReasoning 4 (1990) 283–306.[67] A.J. Viterbi, Error bounds for convolutional codes and an asymptotically optimal decoding algorithm, IEEE Transactions on Information Theory 13 (1967)260–269.[68] M. Wainwright, T. Jaakkola, A. Willsky, MAP estimation via agreement on (hyper)trees: Message-passing and linear programming approaches, IEEETransactions on Information Theory 51 (2002) 3697–3717.[69] M.J. Wainwright, T.S. Jaakkola, A.S. Willsky, Tree-based reparameterization framework for analysis of sum-product and related algorithms, IEEE Trans-actions on Information Theory 49 (2003) 2003.[70] J.S. Yedidia, W.T. Freeman, Y. Weiss, Understanding belief propagation and its generalizations, in: Exploring Artificial Intelligence in the New Millennium,Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2003, pp. 239–269.[71] N.L. Zhang, D. Poole, Exploiting causal independence in Bayesian network inference, Journal of Artificial Intelligence Research 5 (1996) 301–328.