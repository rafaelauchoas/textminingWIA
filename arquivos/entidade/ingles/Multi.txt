Artificial Intelligence 176 (2012) 2291–2320Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintMulti-instance multi-label learningZhi-Hua Zhou∗, Min-Ling Zhang, Sheng-Jun Huang, Yu-Feng LiNational Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210046, Chinaa r t i c l ei n f oa b s t r a c tArticle history:Received 29 April 2010Received in revised form 25 September2011Accepted 13 October 2011Available online 18 October 2011Keywords:Machine learningMulti-instance multi-label learningMIMLMulti-label learningMulti-instance learningIn this paper, we propose the MIML (Multi-Instance Multi-Label learning) framework wherean example is described by multiple instances and associated with multiple class labels.Compared to traditional learning frameworks, the MIML framework is more convenientand natural for representing complicated objects which have multiple semantic meanings.To learn from MIML examples, we propose the MimlBoost and MimlSvm algorithmsbased on a simple degeneration strategy, and experiments show that solving problemsinvolving complicated objects with multiple semantic meanings in the MIML frameworkcan lead to good performance. Considering that the degeneration process may loseinformation, we propose the D-MimlSvm algorithm which tackles MIML problems directlyin a regularization framework. Moreover, we show that even when we do not haveaccess to the real objects and thus cannot capture more information from real objectsby using the MIML representation, MIML is still useful. We propose the InsDif and SubCodalgorithms. InsDif works by transforming single-instances into the MIML representationfor learning, while SubCod works by transforming single-label examples into the MIMLrepresentation for learning. Experiments show that in some tasks they are able to achievebetter performance than learning the single-instances or single-label examples directly.© 2011 Elsevier B.V. All rights reserved.1. IntroductionIn traditional supervised learning, an object is represented by an instance, i.e., a feature vector, and associated with a classlabel. Formally, let X denote the instance space (or feature space) and Y the set of class labels. The task is to learn afunction f : X → Y from a given data set {(x1, y1), (x2, y2), . . . , (xm, ym)}, where xi ∈ X is an instance and yi ∈ Y is theknown label of xi . Although this formalization is prevailing and successful, there are many real-world problems which donot fit in this framework well. In particular, each object in this framework belongs to only one concept and therefore thecorresponding instance is associated with a single class label. However, many real-world objects are complicated, whichmay belong to multiple concepts simultaneously. For example, an image can belong to several classes simultaneously, e.g.,grasslands, lions, Africa, etc.; a text document can be classified to several categories if it is viewed from different aspects,e.g., scientific novel, Jules Verne’s writing or even books on traveling; a web page can be recognized as news page, sports page,soccer page, etc. In a specific real task, maybe only one of the multiple concepts is the right semantic meaning. For example,in image retrieval when a user is interested in an image with lions, s/he may be only interested in the concept lions insteadof the other concepts grasslands and Africa associated with that image. The difficulty here is caused by those objects thatinvolve multiple concepts. To choose the right semantic meaning for such objects for a specific scenario is the fundamentaldifficulty of many tasks. In contrast to starting from a large universe of all possible concepts involved in the task, it maybe helpful to get the subset of concepts associated with the concerned object at first, and then make a choice in the* Corresponding author.E-mail address: zhouzh@lamda.nju.edu.cn (Z.-H. Zhou).0004-3702/$ – see front matter © 2011 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2011.10.0022292Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320small subset later. However, getting the subset of concepts, that is, assigning proper class labels to such objects, is still achallenging task.We notice that as an alternative to representing an object by a single instance, in many cases it is possible to representa complicated object using a set of instances. For example, multiple patches can be extracted from an image where eachpatch is described by an instance, and thus the image can be represented by a set of instances; multiple sections can beextracted from a document where each section is described by an instance, and thus the document can be represented by aset of instances; multiple links can be extracted from a web page where each link is described by an instance, and thus theweb page can be represented by a set of instances. Using multiple instances to represent those complicated objects may behelpful because some inherent patterns which are closely related to some labels may become explicit and clearer. In thispaper, we propose the MIML (Multi-Instance Multi-Label learning) framework, where an example is described by multipleinstances and associated with multiple class labels.Compared to traditional learning frameworks, the MIML framework is more convenient and natural for representingcomplicated objects. To exploit the advantages of the MIML representation, new learning algorithms are needed. We proposethe MimlBoost algorithm and the MimlSvm algorithm based on a simple degeneration strategy, and experiments show thatsolving problems involving complicated objects with multiple semantic meanings under the MIML framework can lead togood performance. Considering that the degeneration process may lose information, we also propose the D-MimlSvm (i.e.,Direct MimlSvm) algorithm which tackles MIML problems directly in a regularization framework. Experiments show thatthis “direct” algorithm outperforms the “indirect” MimlSvm algorithm.In some practical tasks we do not have access to the real objects themselves such as the real images and the real webpages; instead, we are given observational data where each real object has already been represented by a single instance.Thus, in such cases we cannot capture more information from the real objects using the MIML representation. Even in thissituation, however, MIML is still useful. We propose the InsDif (i.e., INStance DIFferentiation) algorithm which transformssingle-instances into MIML examples for learning. This algorithm is able to achieve a better performance than learning thesingle-instances directly in some tasks. This is not strange because for an object associated with multiple class labels, ifit is described by only a single instance, the information corresponding to these labels are mixed and thus difficult forlearning; if we can transform the single-instance into a set of instances in some proper ways, the mixed information mightbe detached to some extent and thus less difficult for learning.MIML can also be helpful for learning single-label objects. We propose the SubCod (i.e., SUB-COncept Discovery) algo-rithm which works by discovering sub-concepts of the target concept at first and then transforming the data into MIMLexamples for learning. This algorithm is able to achieve a better performance than learning the single-label examples di-rectly in some tasks. This is also not strange because for a label corresponding to a high-level complicated concept, it maybe quite difficult to learn this concept directly since many different lower-level concepts are mixed; if we can transform thesingle-label into a set of labels corresponding to some sub-concepts, which are relatively clearer and easier for learning, wecan learn these labels at first and then derive the high-level complicated label based on them with a less difficulty.The rest of this paper is organized as follows. In Section 2, we review some related work. In Section 3, we propose theMIML framework. In Section 4 we propose the MimlBoost and MimlSvm algorithms, and apply them to tasks where theobjects are represented as MIML examples. In Section 5 we present the D-MimlSvm algorithm and compare it with the“indirect” MimlSvm algorithm. In Sections 6 and 7, we study the usefulness of MIML when we do not have access to realobjects. Concretely, in Section 6, we propose the InsDif algorithm and show that using MIML can be better than learningsingle-instances directly; in Section 7 we propose the SubCod algorithm and show that using MIML can be better thanlearning single-label examples directly. Finally, we conclude the paper in Section 8.2. Related workMuch work has been devoted to the learning of multi-label examples under the umbrella of multi-label learning. Notethat multi-label learning studies the problem where a real-world object described by one instance is associated with anumber of class labels,1 which is different from multi-class learning or multi-task learning [28]. In multi-class learning eachobject is only associated with a single label; while in multi-task learning different tasks may involve different domains anddifferent data sets. Actually, traditional two-class and multi-class problems can both be cast into multi-label problems byrestricting that each instance has only one label. The generality of multi-label problems, however, inevitably makes it moredifficult to address.One famous approach to solving multi-label problems is Schapire and Singer’s AdaBoost.MH [56], which is an extensionof AdaBoost and is the core of a successful multi-label learning system BoosTexter [56]. This approach maintains a set ofweights over both training examples and their labels in the training phase, where training examples and their correspondinglabels that are hard (easy) to predict get incrementally higher (lower) weights. Later, De Comité et al. [22] used alternatingdecision trees [30] which are more powerful than decision stumps used in BoosTexter to handle multi-label data andthus obtained the AdtBoost.MH algorithm. Probabilistic generative models have been found useful in multi-label learning.1 Most work on multi-label learning assumes that an instance can be associated with multiple valid labels, but there is also some work assuming thatonly one of the labels among those associated with an instance is correct [35].Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202293McCallum [47] proposed a Bayesian approach for multi-label document classification, where a mixture probabilistic model(one mixture component per category) is assumed to generate each document and an EM algorithm is employed to learn themixture weights and the word distributions in each mixture component. Ueda and Saito [65] presented another generativeapproach, which assumes that the multi-label text has a mixture of characteristic words appearing in single-label textbelonging to each of the multi-labels. It is noteworthy that the generative models used in [47] and [65] are both based onlearning text frequencies in documents, and are thus specific to text applications.Many other multi-label learning algorithms have been developed, such as decision trees, neural networks, k-nearestneighbor classifiers, support vector machines, etc. Clare and King [21] developed a multi-label version of C4.5 decision treesthrough modifying the definition of entropy. Zhang and Zhou [79] presented multi-label neural network Bp-Mll, which isderived from the Backpropagation algorithm by employing an error function to capture the fact that the labels belongingto an instance should be ranked higher than those not belonging to that instance. Zhang and Zhou [80] also proposed theMl-knn algorithm, which identifies the k nearest neighbors of the concerned instance and then assigns labels according tothe maximum a posteriori principle. Elisseeff and Weston [27] proposed the RankSvm algorithm for multi-label learning bydefining a specific cost function and the corresponding margin for multi-label models. Other kinds of multi-label Svms havebeen developed by Boutell et al. [11] and Godbole and Sarawagi [33]. In particular, by hierarchically approximating the Bayesoptimal classifier for the H-loss, Cesa-Bianchi et al. [15] proposed an algorithm which outperforms simple hierarchical Svms.Recently, non-negative matrix factorization has also been applied to multi-label learning [43], and multi-label dimensionalityreduction methods have been developed [74,85].Roughly speaking, earlier approaches to multi-label learning attempt to divide multi-label learning to a number of two-class classification problems [36,72] or transform it into a label ranking problem [27,56], while some later approaches tryto exploit the correlation between the labels [43,65,85].Most studies on multi-label learning focus on text categorization [22,33,39,47,56,65,74], and several studies aim to im-prove the performance of text categorization systems by exploiting additional information given by the hierarchical structureof classes [14,15,53] or unlabeled data [43]. In addition to text categorization, multi-label learning has also been found use-ful in many other tasks such as scene classification [11], image and video annotation [38,48], bioinformatics [7,12,13,21,27],and even association rule mining [50,63].There is a lot of research on multi-instance learning, which studies the problem where a real-world object described by anumber of instances is associated with a single class label. Here the training set is composed of many bags each containingmultiple instances; a bag is labeled positively if it contains at least one positive instance and negatively otherwise. The goalis to label unseen bags correctly. Note that although the training bags are labeled, the labels of their instances are unknown.This learning framework was formalized by Dietterich et al. [24] when they were investigating drug activity prediction.Long and Tan [44] studied the Pac-learnability of multi-instance learning and showed that if the instances in the bagsare independently drawn from product distribution, the Apr (Axis-Parallel Rectangle) proposed by Dietterich et al. [24] isPac-learnable. Auer et al. [5] showed that if the instances in the bags are not independent then Apr learning under themulti-instance learning framework is NP-hard. Moreover, they presented a theoretical algorithm that does not require prod-uct distribution, which was transformed into a practical algorithm named Multinst [4]. Blum and Kalai [10] described areduction from Pac-learning under the multi-instance learning framework to Pac-learning with one-sided random classifi-cation noise. They also presented an algorithm with smaller sample complexity than that of the algorithm of Auer et al. [5].Many multi-instance learning algorithms have been developed during the past decade. To name a few, Diverse Density[45] and Em-dd [83], k-nearest neighbor algorithms Citation-knn and Bayesian-knn [67], decision tree algorithms Relic [54]and Miti [9], neural network algorithms Bp-mip and extensions [77,90] and Rbf-mip [78], rule learning algorithm Ripper-mi [20], support vector machines and kernel methods mi-Svm and Mi-Svm [3], Dd-Svm [18], MissSvm [88], Mi-Kernel[32], Bag-Instance Kernel [19], Marginalized Mi-Kernel [42] and convex-hull method Ch-Fd [31], ensemble algorithmsMi-Ensemble [91], MiBoosting [70] and MilBoosting [6], logistic regression algorithm Mi-lr [51], etc. Actually almost allpopular machine learning algorithms have their multi-instance versions. Most algorithms attempt to adapt single-instancesupervised learning algorithms to the multi-instance representation, by shifting their focus from discrimination on instancesto discrimination on bags [91]. Recently there is some proposal on adapting the multi-instance representation to single-instance algorithms by representation transformation [93].It is worth mentioning that standard multi-instance learning [24] assumes that if a bag contains a positive instance thenthe bag is positive; this implies that there exists a key instance in a positive bag. Many algorithms were designed basedon this assumption. For example, the point with maximal diverse density identified by the Diverse Density algorithm [45]actually corresponds to a key instance; many Svm algorithms defined the margin of a positive bag by the margin of itsmost positive instance [3,19]. As the research of multi-instance learning goes on, however, some other assumptions havebeen introduced [29]. For example, in contrast to assuming that there is a key instance, some work has assumed that thereis no key instance and every instance contributes to the bag label [17,70]. There is also an argument that the instancesin the bags should not be treated independently [88]. All those assumptions have been put under the umbrella of multi-instance learning, and generally, in tackling real tasks it is difficult to know which assumption is the fittest. In other words,in different tasks multi-instance learning algorithms based on different assumptions may have different superiorities.In the early years of the research of multi-instance learning, most work considered multi-instance classification withdiscrete-valued outputs. Later, multi-instance regression with real-valued outputs was studied [2,52], and different versionsof generalized multi-instance learning have been defined [58,68]. The main difference between standard multi-instance2294Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320Fig. 1. Four different learning frameworks.learning and generalized multi-instance learning is that in standard multi-instance learning there is a single concept, anda bag is positive if it has an instance satisfying this concept; while in generalized multi-instance learning [58,68] thereare multiple concepts, and a bag is positive only when all concepts are satisfied (i.e., the bag contains instances fromevery concept). Recently, research on multi-instance clustering [82], multi-instance semi-supervised learning [49] and multi-instance active learning [60] have also been reported.Multi-instance learning has also attracted the attention of the Ilp community. It has been suggested that multi-instanceproblems could be regarded as a bias on inductive logic programming, and the multi-instance paradigm could be the keybetween the propositional and relational representations, being more expressive than the former, and much easier to learnthan the latter [23]. Alphonse and Matwin [1] approximated a relational learning problem by a multi-instance problem,fed the resulting data to feature selection techniques adapted from propositional representations, and then transformed thefiltered data back to relational representation for a relational learner. Thus, the expressive power of relational representationand the ease of feature selection on propositional representation are gracefully combined. This work confirms that multi-instance learning can really act as a bridge between propositional and relational learning.Multi-instance learning techniques have already been applied to diverse applications including image categorization [17,18], image retrieval [71,84], text categorization [3,60], web mining [86], spam detection [37], computer security [54], facedetection [66,76], computer-aided medical diagnosis [31], etc.3. The MIML frameworkLet X denote the instance space and Y the set of class labels. Then, formally, the MIML task is defined as:• MIML (multi-instance multi-label learning): To learn a function f : 2. . . , ( Xm, Ym)}, where Xi ⊆ X is a set of instances {xi1, xi2, . . . , xi,nilabels { yi1, yi2, . . . , yi,lilabels in Y i .X → 2Yfrom a given data set {( X1, Y 1), ( X2, Y 2),}, xi j ∈ X ( j = 1, 2, . . . , ni), and Y i ⊆ Y is a set of}, yik ∈ Y (k = 1, 2, . . . , li). Here ni denotes the number of instances in Xi and li the number ofIt is interesting to compare MIML with the existing frameworks of traditional supervised learning, multi-instance learning,and multi-label learning.• Traditional supervised learning (single-instance single-label learning): To learn a function f : X → Y from a given dataset {(x1, y1), (x2, y2), . . . , (xm, ym)}, where xi ∈ X is an instance and yi ∈ Y is the known label of xi .• Multi-instance learning (multi-instance single-label learning): To learn a function f : 2{( X1, y1), ( X2, y2), . . . , ( Xm, ym)}, where Xi ⊆ X is a set of instances {xi1, xi2, . . . , xi,niyi ∈ Y is the label of Xi .2 Here ni denotes the number of instances in Xi .• Multi-label learning (single-instance multi-label learning): To learn a function f : X → 2YX → Y from a given data set}, xi j ∈ X ( j = 1, 2, . . . , ni), and{(x1, Y 1), (x2, Y 2), . . . , (xm, Ym)}, where xi ∈ X is an instance and Y i ⊆ Y is a set of labels { yi1, yi2, . . . , yi,li(k = 1, 2, . . . , li). Here li denotes the number of labels in Y i .from a given data set}, yik ∈ Y2 According to notions used in multi-instance learning, ( Xi , yi ) is a labeled bag while Xi an unlabeled bag.Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202295Fig. 2. MIML can be helpful in learning single-label examples involving complicated high-level concepts.From Fig. 1 we can see the differences among these learning frameworks. In fact, the multi-learning frameworks areresulted from the ambiguities in representing real-world objects. Multi-instance learning studies the ambiguity in the inputspace (or instance space), where an object has many alternative input descriptions, i.e., instances; multi-label learningstudies the ambiguity in the output space (or label space), where an object has many alternative output descriptions, i.e.,labels; while MIML considers the ambiguities in both the input and output spaces simultaneously. In solving real-worldproblems, having a good representation is often more important than having a strong learning algorithm, because a goodrepresentation may capture more meaningful information and make the learning task easier to tackle. Since many realobjects are inherited with input ambiguity as well as output ambiguity, MIML is more natural and convenient for tasksinvolving such objects.It is worth mentioning that MIML is more reasonable than (single-instance) multi-label learning in many cases. Suppose amulti-label object is described by one instance but associated with l number of class labels, namely label1, label2, . . . , labell.If we represent the multi-label object using a set of n instances, namely instance1, instance2, . . . , instancen, the underlyinginformation in a single instance may become easier to exploit, and for each label the number of training instances can besignificantly increased. So, transforming multi-label examples to MIML examples for learning may be beneficial in sometasks, which will be shown in Section 6. Moreover, when representing the multi-label object using a set of instances,the relation between the input patterns and the semantic meanings may become more easily discoverable. Note that insome cases, understanding why a particular object has a certain class label is even more important than simply makingan accurate prediction, while MIML offers a possibility for this purpose. For example, under the MIML representation, wemay discover that one object has label1 because it contains instancen; it has labell because it contains instancei ; while theoccurrence of both instance1 and instancei triggers label j .MIML can also be helpful for learning single-label examples involving complicated high-level concepts. For example, asFig. 2(a) shows, the concept Africa has a broad connotation and the images belonging to Africa have great variance, thus it isnot easy to classify the top-left image in Fig. 2(a) into the Africa class correctly. However, if we can exploit some low-levelsub-concepts that are less ambiguous and easier to learn, such as tree, lions, elephant and grassland shown in Fig. 2(b), it ispossible to induce the concept Africa much easier than learning the concept Africa directly. The usefulness of MIML in thisprocess will be shown in Section 7.2296Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320Fig. 3. The two general degeneration solutions.4. Solving MIML problems by degenerationIt is evident that traditional supervised learning is a degenerated version of multi-instance learning as well as a degener-ated version of multi-label learning, while traditional supervised learning, multi-instance learning and multi-label learningare all degenerated versions of MIML. So, a simple idea to tackle MIML is to identify its equivalence in the traditionalsupervised learning framework, using multi-instance learning or multi-label learning as the bridge, as shown in Fig. 3.X × Y → {−1, +1}. For any y ∈ Y ,• Solution A: Using multi-instance learning as the bridge:The MIML learning task, i.e., to learn a function f : 2, can be transformed into a multi-instance learning task,f MIL( Xi, y) = +1 if y ∈ Y i and −1 otherwise. Thei.e., to learn a function f MIL : 2∗, y)] = +1}. This multi-proper labels for a new example Xinstance learning task can be further transformed into a traditional supervised learning task, i.e., to learn a functionf SISL : X × Y → {−1, +1}, under a constraint specifying how to derive f MIL( Xi, y) from f SISL(xi j, y) ( j = 1, 2, . . . , ni). Forniany y ∈ Y , f SISL(xi j, y) = +1 if y ∈ Y i and −1 otherwise. Here the constraint can be f MIL( Xi, y) = sign[j=1 f SISL(xi j, y)]which has been used by Xu and Frank [70] in transforming multi-instance learning tasks into traditional supervisedlearning tasks. Note that other kinds of constraint can also be used here.can be determined according to Y∗ = { y | sign[ f MIL( XYX → 2(cid:2)∗• Solution B: Using multi-label learning as the bridge:The MIML learning task, i.e., to learn a function f : 2, can be transformed into a multi-label learning task, i.e.,X → Z . The proper labelsYto learn a function f MLL : Z → 2∗)). This multi-label learning task can be furtherfor a new example Xtransformed into a traditional supervised learning task, i.e., to learn a function f SISL : Z × Y → {−1, +1}. For any y ∈ Y ,f SISL(zi, y) = +1 if y ∈ Y i and −1 otherwise. That is,f MLL(zi) = { y | f SISL(zi, y) = +1}. Here the mapping φ can beimplemented with constructive clustering which was proposed by Zhou and Zhang [93] in transforming multi-instancebags into traditional single-instances. Note that other kinds of mappings can also be used here.X → 2Yf MLL(zi) = f MIML( Xi) if zi = φ( Xi), φ : 2can be determined according to Y. For any zi ∈ Z ,∗ = f MLL(φ( X∗In the rest of this section we will propose two MIML algorithms, MimlBoost and MimlSvm. MimlBoost is an illustrationof Solution A, which uses category-wise decomposition for the A1 step in Fig. 3 and MiBoosting for A2; MimlSvm is anillustration of Solution B, which uses clustering-based representation transformation for the B1 step and MlSvm for B2. OtherMIML algorithms can be developed by taking alternative options. Both MimlBoost and MimlSvm are quite simple. We willsee that for dealing with complicated objects with multiple semantic meanings, good performance can be obtained underthe MIML framework even by using such simple algorithms. This demonstrates that the MIML framework is very promising,and we expect better performance can be achieved in the future if researchers put forward more powerful MIML algorithms.4.1. MimlBoostNow we propose the MimlBoost algorithm according to the first solution mentioned above, that is, identifying theequivalence in the traditional supervised learning framework using multi-instance learning as the bridge. Note that thisstrategy can also be used to derive other kinds of MIML algorithms.Given any set Ω , let |Ω| denote its size, i.e., the number of elements in Ω ; given any predicate π , let (cid:2)π (cid:3) be 1 if πholds and 0 otherwise; given ( Xi, Y i), for any y ∈ Y , let Ψ ( Xi, y) = +1 if y ∈ Y i and −1 otherwise, where Ψ is a functionX × Y → {−1, +1} which judges whether a label y is a proper label of Xi or not. The basic assumption of MimlBoostΨ : 2is that the labels are independent so that the MIML task can be decomposed into a series of multi-instance learning tasksto solve, by treating each label as a task. The pseudo-code of MimlBoost is summarized in Appendix A (Table A.1).|Y|In the first step of MimlBoost, each MIML example ( Xu, Y u) (u = 1, 2, . . . , m) is transformed into a set ofnumber of multi-instance bags, i.e., {[( Xu, y1), Ψ ( Xu, y1)], [( Xu, y2), Ψ ( Xu, y2)], . . . , [( Xu, y|Y|), Ψ ( Xu, y|Y|)]}. Note that[( Xu, y v ), Ψ ( Xu, y v )] (v = 1, 2, . . . , |Y|) is a labeled multi-instance bag where ( Xu, y v ) is a bag containing nu number ofinstances, i.e., {(xu1, y v ), (xu2, y v ), . . . , (xu,nu , y v )}, and Ψ ( Xu, y v ) ∈ {−1, +1} is the label of this bag.Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202297Thus, the original MIML data set is transformed into a multi-instance data set containing m × |Y| number of bags. Weorder them as [( X1, y1), Ψ ( X1, y1)], . . . , [( X1, y|Y|), Ψ ( X1, y|Y|)], [( X2, y1), Ψ ( X2, y1)], . . . , [( Xm, y|Y|), Ψ ( Xm, y|Y|)], andlet [( X (i), y(i)), Ψ ( X (i), y(i))] denote the i-th of these m × |Y| number of bags which contains ni number of instances.Then, from the data set a multi-instance learning function f MIL can be learned, which can accomplish the desired MIML∗, y)] = +1}. In this paper, the MiBoosting algorithm [70] is used to im-function because f MIML( Xplement f MIL. Note that by using MiBoosting, the MimlBoost algorithm assumes that all instances in a bag contributeindependently in an equal way to the label of that bag.∗) = { y | sign[ f MIL( XFor convenience, let (B, g) denote the bag [( X, y), Ψ ( X, y)], B ∈ B, g ∈ G, and E denotes the expectation. Then, herethe goal is to learn a function F (B) minimizing the bag-level exponential loss EB EG|B[exp(−gF (B))], which ultimatelyestimates the bag-level log-odds function 1Pr(g=−1|B) on the training set. In each boosting round, the aim is to expandF (B) into F (B) + c f (B), i.e., adding a new weak classifier, so that the exponential loss is minimized. Assuming that allj h(b j) can be derived, whereinstances in a bag contribute equally and independently to the bag’s label,h(b j) ∈ {−1, +1} is the prediction of the instance-level classifier h(·) for the j-th instance of the bag B, and nB is thenumber of instances in B.2 log Pr(g=1|B)f (B) = 1nB(cid:2)(cid:2)[ 1ninij=1W (i) g(i)h(bIt has been shown by [70] that the best(cid:2)f (B) to be added can be achieved by seeking h(·) which maximizes(i)j )], given the bag-level weights W = exp(−gF (B)). By assigning each instance the label of itsbag and the corresponding weight W (i)/ni , h(·) can be learned by minimizing the weighted instance-level classificationerror. This actually corresponds to the Step 3a of MimlBoost. When f (B) is found, the best multiplier c > 0 can be got bydirectly optimizing the exponential loss:i(cid:4)(cid:3)expEB EG|B−gF(B) + c(cid:4)−g f (B)(cid:5)(cid:5)(cid:6)==(cid:7)i(cid:7)i(cid:8)(cid:9)W (i) expc−g(i)(cid:2)W (i) exp(cid:3)(cid:4)2e(i) − 1(cid:5)c(cid:10)(cid:11)(i)j )(1)j h(bni(cid:6),(cid:2)j(cid:2)(h(b(i)j ) (cid:6)= g(i))(cid:3) (computed in Step 3b). Minimization of this expectation actually corresponds to Step 3d,where e(i) = 1niwhere numeric optimization techniques such as quasi-Newton method can be used. Note that in Step 3c if e(i) (cid:2) 0.5, theBoosting process will stop [89]. Finally, the bag-level weights are updated in Step 3f according to the additive structure ofF (B).4.2. MimlSvmNow we propose the MimlSvm algorithm according to the second solution mentioned before, that is, identifying theequivalence in the traditional supervised learning framework using multi-label learning as the bridge. Note that this strategycan also be used to derive other kinds of MIML algorithms.Again, given any set Ω , let |Ω| denote its size, i.e., the number of elements in Ω ; given ( Xi, Y i) and zi = φ( Xi) whereX → Z , for any y ∈ Y , let Φ(zi, y) = +1 if y ∈ Y i and −1 otherwise, where Φ is a function Φ : Z × Y → {−1, +1}.φ : 2The basic assumption of MimlSvm is that the spatial distribution of the bags carries relevant information, and informationhelpful for label discrimination can be discovered by measuring the closeness between each bag and the representative bagsidentified through clustering. The pseudo-code of MimlSvm is summarized in Appendix A (Table A.2).In the first step of MimlSvm, the Xu of each MIML example ( Xu, Y u) (u = 1, 2, . . . , m) is collected and put into a dataset Γ . Then, in the second step, k-medoids clustering is performed on Γ . Since each data item in Γ , i.e. Xu , is an unlabeledmulti-instance bag instead of a single instance, Hausdorff distance [26] is employed to measure the distance. The Hausdorffdistance is a famous metric for measuring the distance between two bags of points, which has often been used in computervision tasks; other techniques that can measure the distance between bags of points, such as the set kernel [32], can also be}, the Hausdorff distance between A andused here. In detail, given two bags A = {a1, a2, . . . , an AB is defined as} and B = {b1, b2, . . . , bnB(cid:12)dH ( A, B) = maxmaxa∈ Aminb∈B(cid:7)a − b(cid:7), maxb∈Bmina∈ A(cid:13)(cid:7)b − a(cid:7),(2)where (cid:7)a − b(cid:7) measures the distance between the instances a and b, which takes the form of Euclidean distance here.After the clustering process, the data set Γ is divided into k partitions, whose medoids are Mt (t = 1, 2, . . . , k), re-spectively. With the help of these medoids, the original multi-instance example Xu is transformed into a k-dimensionalnumerical vector zu , where the i-th (i = 1, 2, . . . , k) component of zu is the distance between Xu and Mi , that is, dH ( Xu, Mi).In other words, zui encodes some structure information of the data, that is, the relationship between Xu and the i-thpartition of Γ . This process reassembles the constructive clustering process used by Zhou and Zhang [93] in transformingmulti-instance examples into single-instance examples except that in [93] the clustering is executed at the instance levelwhile here it is executed at the bag level. Thus, the original MIML examples ( Xu, Y u) (u = 1, 2, . . . , m) have been trans-formed into multi-label examples (zu, Y u) (u = 1, 2, . . . , m), which corresponds to the Step 3 of MimlSvm.2298Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320∗) = f MLL(zThen, from the data set a multi-label learning function f MLL can be learned, which can accomplish the desired MIML∗). In this paper, the MlSvm algorithm [11] is used to implement f MLL. Concretely,function because f MIML( XMlSvm decomposes the multi-label learning problem into multiple independent binary classification problems (one perclass), where each example associated with the label set Y is regarded as a positive example when building Svm for anyclass y ∈ Y , while regarded as a negative example when building Svm for any class y /∈ Y , as shown in the Step 4 ofMimlSvm. In making predictions, the T-Criterion [11] is used, which actually corresponds to the Step 5 of the MimlSvmalgorithm. That is, the test example is labeled by all the class labels with positive Svm scores, except that when all the Svmscores are negative, the test example is labeled by the class label which is with the top (least negative) score.4.3. Experiments4.3.1. Multi-label evaluation criteriaIn traditional supervised learning where each object has only one class label, accuracy is often used as the performanceevaluation criterion. Typically, accuracy is defined as the percentage of test examples that are correctly classified. Whenlearning with complicated objects associated with multiple labels simultaneously, however, accuracy becomes less mean-ingful. For example, if approach A missed one proper label while approach B missed four proper labels for a test examplehaving five labels, it is obvious that A is better than B, but the accuracy of A and B may be identical because both of themincorrectly classified the test example.Five criteria are often used for evaluating the performance of learning with multi-label examples [56,92]; they are ham-ming loss, one-error, coverage, ranking loss and average precision. Using the same denotation as that in Sections 3 and 4, givena test set S = {( X1, Y 1), ( X2, Y 2), . . . , ( X p, Y p)}, these five criteria are defined as below. Here, h( Xi) returns a set of properlabels of Xi ; h( Xi, y) returns a real-value indicating the confidence for y to be a proper label of Xi ; rankh( Xi, y) returns therank of y derived from h( Xi, y).(cid:2)pi=1(cid:2)pi=1• hlossS (h) = 1p|Y| |h( Xi)(cid:8)Y i|, where (cid:8) stands for the symmetric difference between two sets. The hamming loss1evaluates how many times an object-label pair is misclassified, i.e., a proper label is missed or a wrong label is pre-dicted. The performance is perfect when hlossS (h) = 0; the smaller the value of hlossS (h), the better the performanceof h.(cid:2)• one-errorS (h) = 1p(cid:2)[arg max y∈Y h( Xi, y)] /∈ Y i(cid:3). The one-error evaluates how many times the top-ranked label isnot a proper label of the object. The performance is perfect when one-error S (h) = 0; the smaller the value of one-errorS (h), the better the performance of h.pi=1 max y∈Y i rankh( Xi, y) − 1. The coverage evaluates how far it is needed, on the average, to go downthe list of labels in order to cover all the proper labels of the object. It is loosely related to precision at the level ofperfect recall. The smaller the value of coverageS (h), the better the performance of h.• coverageS (h) = 1• rlossS (h) = 1p|{( y1, y2)|h( Xi, y1) (cid:3) h( Xi, y2), ( y1, y2) ∈ Y i × Y i}|, where Y i denotes the complementaryset of Y i in Y . The ranking loss evaluates the average fraction of label pairs that are misordered for the object. Theperformance is perfect when rlossS (h) = 0; the smaller the value of rlossS (h), the better the performance of h.1|Y i ||Y i |pi=1(cid:2)p• avgprecS (h) = 1. The average precision evaluates the average fraction ofproper labels ranked above a particular label y ∈ Y i . The performance is perfect when avgprecS (h) = 1; the larger thevalue of avgprecS (h), the better the performance of h.y∈Y ip|{ y(cid:8)|rankh( Xi , y(cid:8))(cid:2)rankh( Xi , y), yrankh( Xi , y)(cid:8)∈Y i }|(cid:2)(cid:2)pi=11|Y i |In addition to the above criteria, we design two new multi-label criteria, average recall and average F1, as below.(cid:2)• avgreclS (h) = 1pi=1|{ y|rankh( Xi , y)(cid:2)|h( Xi )|, y∈Y i }||Y i |. The average recall evaluates the average fraction of proper labels thathave been predicted. The performance is perfect when avgreclS (h) = 1; the larger the value of avgreclS (h), the betterthe performance of h.p• avgF1S (h) = 2×avgprecS (h)×avgreclS (h)avgprecS (h)+avgreclS (h). The average F1 expresses a tradeoff between the average precision and the averagerecall. The performance is perfect when avgF1S (h) = 1; the larger the value of avgF1S (h), the better the performanceof h.Note that since the above criteria measure the performance from different aspects, it is difficult for one algorithm tooutperform another on every one of these criteria.In the following we study the performance of MIML algorithms on two tasks involving complicated objects with multiplesemantic meanings. We will show that for such tasks, MIML is a good choice, and good performance can be achieved evenby using simple MIML algorithms such as MimlBoost and MimlSvm.4.3.2. Scene classificationThe scene classification data set consists of 2000 natural scene images belonging to the classes desert, mountains, sea,sunset and trees. Over 22% of the images belong to multiple classes simultaneously. Each image has already been representedZ.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202299Table 1Results (mean±std.) on scene classification data set (‘↓’ indicates ‘the smaller the better’; ‘↑’ indicates ‘the larger the better’).ComparedalgorithmsMimlBoostMimlSvmMimlSvmmiMimlNnAdtBoost.MHRankSvmMlSvmMl-knnEvaluation criteria↓hloss.193±.007189±.009.195±.008.185±.008.211±.006.210±.024.232±.004.191±.006↓one-error.347±.019.354±.022.317±.018.351±.026.436±.019.395±.075.447±.023.370±.017↓coverage.984±.0491.087±.0471.068±.0521.057±.0541.223±.0501.161±.1541.217±.0541.085±.048↓rloss.178±.011.201±.011.197±.011.196±.013N/A.221±.040.233±.012.203±.010↑aveprec.779±.012.765±.013.783±.011.771±.015.718±.012.746±.044.712±.013.759±.011↑averecl.433±.027.556±.020.587±.019.509±.022N/A.529±.068.073±.010.407±.026↑aveF1.556±.023.644±.018.671±.015.613±.020N/A.620±.059.132±.017.529±.023as a bag of nine instances generated by the Sbn method [46], which uses a Gaussian filter to smooth the image and thensubsamples the image to an 8 × 8 matrix of color blobs where each blob is a 2 × 2 set of pixels within the matrix. Aninstance corresponds to the combination of a single blob with its four neighboring blobs (up, down, left, right), which isdescribed with 15 features. The first three features represent the mean R, G, B values of the central blob and the remainingtwelve features express the differences in mean color values between the central blob and other four neighboring blobsrespectively.3We evaluate the performance of the MIML algorithms MimlBoost and MimlSvm. Note that MimlBoost and MimlSvmare merely proposed to illustrate the two general degeneration solutions to MIML problems shown in Fig. 3. We do notclaim that they are the best algorithms that can be developed through the degeneration paths. There may exist otherprocesses for transforming MIML examples into multi-instance single-label (MISL) examples or single-instance multi-label(SIML) examples. Even by using the same degeneration process as that used in MimlBoost and MimlSvm, there are alsomany alternatives to realize the second step. For example, by using mi-Svm [3] to replace the MiBoosting used in Miml-Boost and by using the two-layer neural network structure [81] to replace the MlSvm used in MimlSvm, we get MimlSvmmiand MimlNn respectively. Their performance is also evaluated in our experiments.We compare the MIML algorithms with several state-of-the-art algorithms for learning with multi-label examples, in-cluding AdtBoost.MH [22], RankSvm [27], MlSvm [11] and Ml-knn [80]; these algorithms have been introduced briefly inSection 2. Note that these are single-instance algorithms that regard each image as a 135-dimensional feature vector, whichis obtained by concatenating the nine instances in the direction from upper-left to right-bottom.The parameter configurations of RankSvm, MlSvm and Ml-knn are set by considering the strategies adopted in [11,27]and [80] respectively. For RankSvm, polynomial kernel is used where polynomial degrees of 2 to 9 are considered as in[27] and chosen by hold-out tests on training sets. For MlSvm, Gaussian kernel is used. For Ml-knn, the number of nearestneighbors considered is set to 10.The boosting rounds of AdtBoost.MH and MimlBoost are set to 25 and 50, respectively; The performance of the twoalgorithms at different boosting rounds is shown in Appendix B (Fig. B.1), it can be observed that at those rounds theperformance of the algorithms have become stable. Gaussian kernel Libsvm [16] is used for the Step 3a of MimlBoost. TheMimlSvm and MimlSvmmi are also realized with Gaussian kernels. The parameter k of MimlSvm is set to be 20% of thenumber of training images; The performance of this algorithm with different k values is shown in Appendix B (Fig. B.2), itcan be observed that the setting of k does not significantly affect the performance of MimlSvm. Note that in Appendix B(Figs. B.1 and B.2) we plot 1 – average precision, 1 – average recall and 1 – average F1 such that in all the figures, the lowerthe curve, the better the performance.Here in the experiments, 1500 images are used as training examples while the remaining 500 images are used for testing.Experiments are repeated for thirty runs by using random training/test partitions, and the average and standard deviationare summarized in Table 1,4 where the best performance on each criterion has been highlighted in boldface.Pairwise t-tests with 95% significance level disclose that all the MIML algorithms are significantly better than Adt-Boost.MH and MlSvm on all the seven evaluation criteria. This is impressive since as mentioned before, these evaluationcriteria measure the learning performance from different aspects and one algorithm rarely outperforms another algorithmon all criteria. MimlSvm and MimlSvmmi are both significantly better than RankSvm on all the evaluation criteria, whileMimlBoost and MimlNn are both significantly better than RankSvm on the first five criteria. MimlNn is significantly betterthan Ml-knn on all the evaluation criteria. Both MimlBoost and MimlSvmmi are significantly better than Ml-knn on allcriteria except hamming loss. MimlSvm is significantly better than Ml-knn on one-error, average precision, average recall andaverage F1, while there are ties on the other criteria. Moreover, note that the best performance on all evaluation criteria arealways attained by MIML algorithms. Overall, comparison on the scene classification task shows that the MIML algorithmscan be significantly better than the non-MIML algorithms; this validates the powerfulness of the MIML framework.3 The data set is available at http://lamda.nju.edu.cn/data_MIMLimage.ashx.4 For the shared implementation of AdtBoost.MH (http://www.grappa.univ-lille3.fr/grappa/en_index.php3?info=software), ranking loss, average recall andaverage F1 are not available in the program’s outputs.2300Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320Table 2Results (mean±std.) on text categorization data set (‘↓’ indicates ‘the smaller the better’; ‘↑’ indicates ‘the larger the better’).ComparedalgorithmsMimlBoostMimlSvmMimlSvmmiMimlNnAdtBoost.MHRankSvmMlSvmMl-knnEvaluation criteria↓hloss.053±.004.033±.003.041±.004.038±.002.055±.005.120±.013.050±.003.049±.003↓one-error.094±.014.066±.011.055±.009.080±.010.120±.017.196±.126.081±.011.126±.012↓coverage.387±.037.313±.035.284±.030.320±.030.409±.047.695±.466.329±.029.440±.035↓rloss.035±.005.023±.004.020±.003.025±.003N/A.085±.077.026±.003.045±.004↑aveprec.937±.008.956±.006.965±.005.950±.006.926±.011.868±.092.949±.006.920±.007↑averecl.792±.010.925±.010.921±.012.834±.011N/A.411±.059.777±.016.821±.021↑aveF1.858±.008.940±.008.942±.007.888±.008N/A.556±.068.854±.011.867±.0134.3.3. Text categorizationThe Reuters-21578 data set is used in this experiment. The seven most frequent categories are considered. After re-moving documents that do not have labels or main texts, and randomly removing some documents that have only onelabel, a data set containing 2000 documents is obtained, where over 14.9% documents have multiple labels. Each documentis represented as a bag of instances according to the method used in [3]. Briefly, the instances are obtained by splittingeach document into passages using overlapping windows of maximal 50 words each. As a result, there are 2000 bags andthe number of instances in each bag varies from 2 to 26 (3.6 on average). The instances are represented based on termfrequency. The words with high frequencies are considered, excluding “function words” that have been removed from thevocabulary using the Smart stop-list [55]. It has been found that based on document frequency, the dimensionality of thedata set can be reduced to 1–10% without loss of effectiveness [73]. Thus, we use the top 2% frequent words, and thereforeeach instance is a 243-dimensional feature vector.5The parameter configurations of RankSvm, MlSvm and Ml-knn are set in the same way as in Section 4.3.2. The boostingrounds of AdtBoost.MH and MimlBoost are set to 25 and 50, respectively. Linear kernels are used. The parameter k ofMimlSvm is set to be 20% of the number of training images. The single-instance algorithms regard each document as a243-dimensional feature vector which is obtained by aggregating all the instances in the same bag; this is equivalent torepresent the document using a sole term frequency feature vector.Here in the experiments, 1500 documents are used as training examples while the remaining 500 documents are usedfor testing. Experiments are repeated for thirty runs by using random training/test partitions, and the average and standarddeviation are summarized in Table 2, where the best performance on each criterion has been highlighted in boldface.Pairwise t-tests with 95% significance level disclose that, impressively, both MimlSvm and MimlSvmmi are significantlybetter than all the non-MIML algorithms. MimlNn is significantly better than AdtBoost.MH, RankSvm, and Ml-knn onall the evaluation criteria; significantly better than MlSvm on hamming loss, average recall and average F1 while there areties on the other criteria. MimlBoost is significantly better than AdtBoost.MH on all criteria except that there is a tie onhamming loss; significantly better than RankSvm on all criteria; significantly better than MlSvm on average recall and thereis a tie on average F1; significantly better than Ml-knn on one-error, coverage, ranking loss and average precision. Moreover,note that the best performance on all evaluation criteria are always attained by MIML algorithms. Overall, comparison onthe text categorization task shows that the MIML algorithms are better than the non-MIML algorithms; this validates thepowerfulness of the MIML framework.5. Solving MIML problems by regularizationThe degeneration methods presented in Section 4 may lose information during the degeneration process, and thus a“direct” MIML algorithm is desirable. In this section we propose a regularization method for MIML. In contrast to MimlSvmand MimlSvmmi , this method is developed from the regularization framework directly and so we call it D-MimlSvm. Thebasic assumption of D-MimlSvm is that the labels associated to the same example have some relatedness, and the per-formance of classifying the bags depends on the loss between the labels and the predictions on the bags as well as onthe constituent instances. Moreover, considering that for any class label the number of positive examples is smaller thanthat of negative examples, this method incorporates a mechanism to deal with class imbalance. We employ the constrainedconcave-convex procedure (Cccp) which has well-studied convergence properties [62] to solve the resultant non-convexoptimization problem. We also present a cutting plane algorithm that finds the solution efficiently.5.1. The loss functionGiven a set of MIML training examples {( X1, Y 1), ( X2, Y 2), . . . , ( Xm, Ym)}, the goal of D-MimlSvm is to learn a map-where the proper label set for each bag X ⊆ X corresponds to f ( X) ⊆ Y . Specifically, D-MimlSvmYX → 2ping f : 25 The data set is available at http://lamda.nju.edu.cn/data_MIMLtext.ashx.Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202301f = ( f 1, f 2, . . . , f T ), where T is the number of labels in the label spacechooses to instantiate f with T functions, i.e.X → R determines the belongingness of lt for X , i.e.Y = {l1, l2, . . . , lT }. Here, the t-th function ft : 2f ( X) = {lt | ft( X) >0, 1 (cid:3) t (cid:3) T }. In addition, each single instance x ∈ X in a bag X can be viewed as a bag {x} containing only one instance,f ({x}) and ft({x}) aresuch thatsimplified as f (x) and ft(x) in the rest of this section.f ({x}) = ( f 1({x}), f 2({x}), . . . , f T ({x})) is also a well-defined function. For convenience,To train the component functions ft (1 (cid:3) t (cid:3) T ) in f , D-MimlSvm employs the following empirical loss function Vinvolving two terms (balanced by λ):(cid:4)(cid:5)(cid:4)V{ Xi}mi=1, {Y i}mi=1, f= V 1{ Xi}mi=1, {Y i}mi=1, f(cid:5)(cid:4)+ λ · V 2{ Xi}mi=1, f(cid:5).(3)Here, the first term V 1 considers the loss between the ground-truth label set of each training bag Xi , i.e. Y i , to its predictedlabel set, i.e. f ( Xi). Let yit = 1 if lt ∈ Y i holds (1 (cid:3) i (cid:3) m, 1 (cid:3) t (cid:3) T ). Otherwise, yit = −1. Furthermore, let (z)+ = max(0, z)denote the hinge loss function. Accordingly, the first loss term V 1 is defined as:(cid:4)V 1{ Xi}mi=1, {Y i}mi=1, f(cid:5)= 1mTm(cid:7)T(cid:7)i=1t=1(cid:4)(cid:5)1 − yit ft(Xi)+.(4)The second term V 2 considers the loss between f ( Xi) and the predictions of Xi ’s constituent instances, i.e. { f (xi j) | 1 (cid:3) j (cid:3)}. Here, the common assumptionni}, which reflects the relationships between the bag Xi and its instances {xi1, xi2, . . . , xi,niin multi-instance learning is that the strength for Xi to hold a label is equal to the maximum strength for its instances tohold the label, i.e. ft( Xi) = max j=1,...,ni ft(xi j).6 Accordingly, the second loss term V 2 is defined as:(cid:4)V 2{ Xi}mi=1, f(cid:5)= 1mTm(cid:7)T(cid:7)(cid:4)li=1t=1ft(Xi), maxj=1,...,ni(cid:5)ft(xi j).(5)Here, l(v 1, v 2) can be defined in various ways and is set to be the l1 loss in this paper, i.e. l(v 1, v 2) = |v 1 − v 2|. By combiningEq. (4) and Eq. (5), the empirical loss function V in Eq. (3) is then specified as:(cid:4)V{ Xi}mi=1, {Y i}mi=1, f(cid:5)= 1mTm(cid:7)T(cid:7)(cid:4)(cid:5)1 − yit ft(Xi)++ λmT5.2. Representer theorem for MIMLi=1t=1m(cid:7)T(cid:7)i=1t=1(cid:4)lft(Xi), maxj=1,...,ni(cid:5)ft(xi j).(6)For simplicity, we assume that each function ft is a linear model, i.e.,ft(x) = (cid:11)wt, φ(x)(cid:12) where φ is the feature mapinduced by a kernel function k and (cid:11)·,·(cid:12) denotes the standard inner product in the Reproducing Kernel Hilbert Space (RKHS)H induced by the kernel k. We recall that an instance can be regarded as a bag containing only one instance, so the kernelk can be any kernel defined on a set of instances, such as the set kernel [32]. In the case of classification, objects (bags orinstances) are classified according to the sign of ft .D-MimlSvm assumes that the labels associated with a bag should have some relatedness; otherwise they should not beassociated with the bag simultaneously. To reflect this basic assumption, D-MimlSvm regularizes the empirical loss functionin Eq. (6) with an additional term Ω( f ):(cid:5)(cid:4)Ω( f ) + γ · V{ Xi}mi=1, {Y i}mi=1, f.(7)Here, γ is a regularization parameter balancing the model complexity Ω( f ) and the empirical risk V . Inspired by [28], weassume that the relatedness among the labels can be measured by the mean function w 0,w 0 = 1TT(cid:7)t=1wt.The original idea in [28] is to minimize(cid:2)Tt=1(cid:7)wt − w 0(cid:7)2 and meanwhile minimize (cid:7)w 0(cid:7)2, i.e. to set the regularizer as:(8)Ω( f ) = 1TT(cid:7)t=1(cid:7)wt − w 0(cid:7)2 + η(cid:7)w 0(cid:7)2.(9)6 Note that this assumption may be restrictive to some extent. There are many cases where the label of the bag does not rely on the instancewith the maximum predictions, as discussed in Section 2. In addition, in classification only the sign of prediction is important [19], i.e. sign( ft ( Xi )) =sign(max j=1,...,ni ft (xi j )). However, in this paper the above common assumption is still adopted due to its popularity and simplicity.(10)(11)2302Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320According to Eq. (8), the first term in the RHS of Eq. (9) can be rewritten as:1TT(cid:7)t=1(cid:7)wt − w 0(cid:7)2 = 1TT(cid:7)t=1(cid:7)wt(cid:7)2 − (cid:7)w 0(cid:7)2.Therefore, by substituting Eq. (10) into Eq. (9), the regularizer can be simplified as:Ω( f ) = 1TT(cid:7)t=1(cid:7)wt(cid:7)2 + μ(cid:7)w 0(cid:7)2.Further note that (cid:7)wt(cid:7)2 = (cid:7) ft(cid:7)2framework of D-MimlSvm as follows:H and (cid:7)w 0(cid:7)2 = (cid:7)(cid:2)Tt=1 ftT(cid:7)2H, by substituting Eq. (11) into Eq. (7), we have the regularizationminf ∈H1TT(cid:7)t=1(cid:14)(cid:14)(cid:14)H + μ(cid:14)(cid:7) ft(cid:7)2(cid:2)Tt=1 ftT(cid:14)(cid:14)(cid:14)(cid:14)2H(cid:4)+ γ · V{ Xi}mi=1, {Y i}mi=1, f(cid:5).(12)Here, μ is a parameter to trade off the discrepancy and commonness among the labels, that is, how similar or dissimilar theH + μ(cid:7)wt ’s are. Refer to Eq. (10), we have Ω( f ) = 1TIntuitively, when μ + 1 (or μ) is large, minimization of Eq. (12) will force (cid:7)among the labels becomes more important; when μ + 1 (or μ) is small, minimization of Eq. (12) will force (cid:7) ft −to tend to be zero and the commonness among the labels becomes more important [28].Tt=1(cid:7)2H to tend to be zero and the discrepancy(cid:7)2HH = 1(cid:7)2T(cid:2)Tt=1 ftTH + (μ + 1)(cid:7)(cid:7)2Tt=1 ftTTt=1 ftTTt=1 ftTTt=1 ftT(cid:7) ft −(cid:7) ft(cid:7)2(cid:7)2H.Tt=1(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)Given the above setup, we can prove the following representer theorem.Theorem 1. The minimizer of the optimization problem (12) admits an expansionft(x) =αt,i0k(x, Xi) +αt,i jk(x, xi j)(cid:15)m(cid:7)i=1where all αt,i0, αt,i j ∈ R.ni(cid:7)j=1(cid:16)(cid:10)Proof. Analogous to [28], we first introduce a combined feature map(cid:9)φ(x)√rΨ (x, t) =, 0, . . . , 0(cid:17) (cid:18)(cid:19) (cid:20)t−1and its decision function, i.e., ˆf (x, t) = (cid:11) ˆw, Ψ (x, t)(cid:12) where, φ(x), 0, . . . , 0(cid:17) (cid:18)(cid:19) (cid:20)T −t√ˆw = (r w 0, w 1 − w 0, . . . , w T − w 0).Here r = μT + T . Let ˆk denote the kernel function induced by Ψ and ˆH is its corresponding RKHS. We have Eqs. (13) and(14).(cid:21)ˆf (x, t) =(cid:22)ˆw, Ψ (x, t)T(cid:7)(cid:7) ˆf (cid:7)2ˆH= (cid:7) ˆw(cid:7)2 =i=1(cid:22)(cid:21)(w 0 + wt − w 0), φ(x)=(cid:21)(cid:22)wt, φ(x)== ft(x),(cid:7)wt − w 0(cid:7)2 + r(cid:7)w 0(cid:7)2 =T(cid:7)i=1(cid:7)wt(cid:7)2 + μT (cid:7)w 0(cid:7)2.Therefore, loss function in Eq. (6) can be represented by ˆV ({ Xi}mi=1, {Y i}mi=1, ˆf ), i.e.,(cid:4)ˆV{ Xi}mi=1, {Y i}mi=1, ˆf(cid:5)= 1mTm(cid:7)T(cid:7)(cid:4)1 − yit(cid:5)ˆf (Xi, t)+i=1t=1m(cid:7)T(cid:7)i=1t=1(cid:4)lˆf (Xi, t), maxj=1,...,ni(cid:5)ˆf (xi j, t).+ λmTThus, Eq. (12) is equivalent to(cid:4)(cid:7) ˆf (cid:7)2ˆH+ γ ˆV1Tminˆf ∈ ˆH{ Xi}mi=1, {Y i}mi=1, ˆf(cid:5).(13)(14)(15)(16)Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202303Note that (cid:7) ˆf (cid:7)2ˆHin [57]), each minimizer ˆf of the functional risk in Eq. (16) admits a representation of the form: [0, ∞) → R is a strictly monotonically increasing function. According to representer theorem (Theorem 4.2ˆf (x, t) =(cid:15)m(cid:7)T(cid:7)t=1i=1(cid:4)(cid:5)(Xi, t), (x, t)ˆk+βt,i0(cid:4)(cid:5)(xi j, t), (x, t)ˆkβt,i j(cid:16),ni(cid:7)j=1where βt,i j ∈ R and the corresponding weight vector ˆw is represented asT(cid:7)ˆw =(cid:15)m(cid:7)βt,i0Ψ (Xi, t) +t=1i=1(cid:16)βt,i jΨ (xi j, t).ni(cid:7)j=1Finally, with Eqs. (13) and (18), we have(cid:22)w, Ψ (x, t)ni(cid:7)(cid:22)wt, φ(x)(cid:15)m(cid:7)ft(x) ==(cid:21)(cid:21)(cid:16)=αt,i0k(x, Xi) +αt,i jk(x, xi j)(17)(18)(19)i=1j=1where αt,i j = 1√r(cid:2)(t βt,i j) + βt,i j/r. (cid:2)Note that x in Eq. (19) can be regarded not only as a bag Xi but also an instance xi j . In other words, both ft( Xi) andft(xi j) can be obtained by Eq. (19).5.3. OptimizationConsidering the use of l1 loss for l(v 1, v 2), Eq. (12) can be re-written as(cid:14)(cid:14)(cid:14)H + μ(cid:14)(cid:7) ft(cid:7)2(cid:2)Tt=1 ftT(cid:14)(cid:14)(cid:14)(cid:14)2H1TT(cid:7)t=1minf ∈H,ξ ,δ+ γmTξ (cid:8)1 + γ λmTδ(cid:8)1s.t.yit ft(Xi) (cid:2) 1 − ξit,ξ (cid:2) 0,−δit (cid:3) ft(Xi) − maxj=1,...,niwhere ξ = [ξ11, ξ12, . . . , ξit, . . . , ξmT ](cid:8)δit, . . . , δmT ](cid:8)ft(xi j) (cid:3) δit ∀i = 1, . . . , m, t = 1, . . . , T(20)are slack variables for the errors on the training bags for each label, δ = [δ11, δ12, . . . ,, and 0 and 1 are all-zero and all-one vector, respectively.Without loss of generality, assume that the bags and instances are ordered as ( X1, . . . , Xm, x11, . . . , x1,n1 , . . . , xm,1, . . . ,xm,nm ). Thus, each object (bag or instance) in the training set can then be indexed by the following function I, i.e.,(cid:23)I(Xi) = i,I(xi j) = m +(cid:2)i−1l=1 nl + jfor j = 1, . . . , ni and i = 1, . . . , m. With this ordering, we can obtain the (m + n) × (m + n) kernel matrix K defined onall objects in the training set, where n =mi=1 ni . Denote the i-th column of K by ki . According to Theorem 1, we haveft( Xi) = kI(xi j )αt + bt . Here, the bias bt for each label is included.According to definition of ft in Eq. (19), Eq. (20) can be cast as the optimization problemI( Xi )αt + bt and ft(xi j) = k(cid:2)(cid:8)(cid:8)(cid:8)1(cid:8)AK A1 + γmTξ (cid:8)1 + γ λmTδ(cid:8)1T(cid:7)1minA,ξ ,δ,bt K αt + μα(cid:8)T 2(cid:5)s.t.(cid:2) 1 − ξit,t=1(cid:8)I( Xi )αt + bt2T(cid:4)kyitξ (cid:2) 0,(cid:8)I(xi j)αt − δit (cid:3) kI( Xi )αt − maxj=1,...,niwhere A = [α1, α2, . . . , α T ] and b = [b1, b2, . . . , bT ](cid:8)(cid:8)I( Xi )αt,(cid:8)kI(xi j )αt (cid:3) δit,kk(cid:8).(21)2304Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320The above optimization problem is a non-convex optimization problem since the last constraint is non-convex. Note thatthis non-convex constraint is a difference between two convex functions, and thus the optimization problem can be solvedby Cccp [19,62], which is one of the most standard techniques to solve such kind of non-convex optimization problems.Cccp is guaranteed to converge to a local minimum [75], and in many cases it can even converge to a global solution [25].Here, for solving the optimization problem (21), Cccp works by solving a sequential convex quadratic problems. Con-(cid:8)I(xi j )αt , we solve the following convex quadratic(cid:8)I(xi j )αt of max j=1,...,ni kcretely, given the initial subgradientnij=1 ρi jt k(cid:2)optimization (QP) problem(cid:8)1(cid:8)AK A1 + γmTξ (cid:8)1 + γ λmTδ(cid:8)1minA,ξ ,δ,bs.t.T(cid:7)1t K αt + μα(cid:8)T 2(cid:5)t=1(cid:8)I( Xi )αt + bt2T(cid:4)yitkξ (cid:2) 0,(cid:8)I(xi j)αt − δit (cid:3) kk(cid:8)I( Xi )αt,(cid:2) 1 − ξit,k(cid:8)I( Xi )αt −ni(cid:7)j=1ρi jtk(cid:8)I(xi j )αt (cid:3) δit.(22)Then, in the next iteration we update ρi jk according toI(xi j )αt (cid:6)= maxk=1,...,ni (k(cid:23)(cid:8)(cid:8)I(xik)αt),ρi jt == 0,if k= 1/nd, otherwise,where nd is the number of active xi j ’s. It holdsguaranteed to converge to a local minimum.5.4. Handling class-imbalance(cid:2)nij=1 ρi jt = 1 for any t’s. The iteration continues and this procedure isThe above solution may be improved further if we explicitly take into account the instance-level class-imbalance, that is,for any class label the number of positive instances is smaller than the number of negative instances in MIML problems.We can roughly estimate the imbalance rate, which is the ratio of the number of positive instances to that of negativeinstances, for each class label using the strategy adopted by [41]. In detail, for a specific label y ∈ Y , we can divide thetraining bags {( X1, Y 1), ( X2, Y 2), . . . , ( Xm, Ym)} into two subsets, A1 = {( Xi, Y i) | y ∈ Y i} and A2 = {( Xi, Y i) | y /∈ Y i}. It isobvious that all the instances in A2 are negative to y. Then, for every ( Xi, Y i) in A1, assuming that the instances of different|Y i | where |Y i| returnslabels is roughly equally distributed, the number of positive instances of y in ( Xi, Y i) is roughly ni × 1the number of labels in Y i . Thus, the imbalance rate of y is:ibr( y) =m(cid:7)i=1y∈Y i×ni|Y i|1(cid:2)mi=1 ni=m(cid:7)i=1y∈Y inin × |Y i|.There are many class-imbalance learning methods [69]. One of the most popular and effective methods is rescaling [87],which can be incorporated into our framework easily. In short, after obtaining the estimated imbalance rate for every classlabel, we can use these rates to modulate the loss caused by different misclassifications.In detail, ξ in Eq. (22) is directly related to the hinge loss (1 − yit ft( Xi))+. According to the rescaling method [87],without loss of generality, we can rewrite the loss function into Eq. (23):(cid:9)yit + 12(cid:10)− yit × ibr( yit)(cid:4)(cid:5)1 − yit ft(Xi).(23)Let τ = [τ11, τ12, . . . , τit, . . . , τmT ], where τit = ( yit +1− yit × ibr( yit)). Then, to minimize the loss defined in Eq. (23),Eq. (22) becomes Eq. (24). Here ξ (cid:8)τ indicates the weighted loss after considering the instance-level class-imbalance. It isevident that the problem in Eq. (24) is still a standard QP problem.2T(cid:7)1t K αt + μα(cid:8)T 2(cid:5)t=1(cid:8)I( Xi )αt + btminA,ξ ,δ,bs.t.2T(cid:4)yitkξ (cid:2) 0,(cid:2) 1 − ξit,(cid:8)1(cid:8)AK A1 + γmTξ (cid:8)τ + γ λmTδ(cid:8)1Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320k(cid:8)I(xi j)αt − δit (cid:3) k(cid:8)I( Xi )αt,k(cid:8)I( Xi )αt −ni(cid:7)j=1ρi jtk(cid:8)I(xi j )αt (cid:3) δit.5.5. Efficient algorithm2305(24)Eq. (24) is a large-scale quadratic programming problem that involves many constraints and variables. To make ittractable and scalable, and observing that most of the constraints in Eq. (24) are redundant, we present an efficient al-gorithm which constructs a nested sequence of tighter relaxations of the original problem using the cutting plane method[40].Similar to its use with structured prediction [64], we add a constraint (or a cut) that is most violated by the currentsolution, and then find the solution in the updated feasible region. Such a procedure will converge to an optimal (orε-suboptimal) solution of the original problem. Moreover, Eq. (24) supports a natural problem decomposition since itsconstraint matrix is a block diagonal matrix, i.e., each block corresponds to one label.The pseudo-code of the algorithm is summarized in Appendix A (Table A.3). We first initialize the working sets St ’s asempty sets and the solutions as all zeros (line 1). Then, instead of testing all the constraints, which is rather expensive whenthere are lots of constraints, we use the speedup heuristic as described in [61], i.e., we use p constraints to approximatethe whole constraints (line 4). Smola and Schölkopf [61] have shown that when p is larger than 59, the selected violatedconstraint is with probability 0.95 among the 5% most violated constraints among all constraints. The Lossi (line 5) isx − d} where u and d are the linear coefficients and bias of the i-th linear constraint, respectively. Ifcalculated as max{0, u−4 in our experiments), no update willthe maximal Loss is lower than the given stopping criteria ε (we simply set ε as 10be taken for the working set St ; otherwise the constraint with the maximal Loss will be added into St (lines 8 and 9). Oncea new constraint is added, the solution will be re-computed with respect to St via solving a smaller quadratic programproblem (line 10). The algorithm stops when there is no update for all St ’s.(cid:8)5.6. ExperimentsThe previous experiments in Section 4.3 have shown that different MIML algorithms have different advantages on differ-ent performance measures. In this section we propose the D-MimlSvm algorithm. We do not claim that D-MimlSvm is thebest MIML algorithm. What we want to show is that, in contrast to heuristically solving the MIML problem by degeneration,developing algorithms from a regularization framework directly offers a better choice. So the most meaningful comparisonis between the D-MimlSvm, MimlSvm and MimlSvmmi algorithms, the latter two not being derived from the regularizationframework directly.To study the behavior of D-MimlSvm, MimlSvm and MimlSvmmi under different amounts of multi-label data, we derivefive data sets from the scene data used in Section 4.3.2. By randomly removing some single-label images, we obtain a dataset where 30% (or 40%, or 50%) images belonging to multiple classes simultaneously; by randomly removing some multi-label images, we obtain a data set where 10% (or 20%) images belong to multiple classes simultaneously. A similar process isapplied to the text data used in Section 4.3.3 to derive five data sets. On the derived data sets we use 25% data for trainingand the remaining 75% data for testing, and experiments are repeated for thirty runs with random training/test partitions.The parameters of D-MimlSvm, MimlSvm and MimlSvmmi are all set by hold-out tests on training sets. Since D-MimlSvmneeds to solve a large optimization problem, although we have incorporated advanced mechanisms such as cutting-planealgorithm, the current D-MimlSvm can only deal with moderate training set sizes.The seven criteria introduced in Section 4.3.1 are used to evaluate the performance. The average and standard deviationare plotted in Figs. 4 and 5. Note that in the figures we plot 1 – average precision, 1 – average recall and 1 – average F1 suchthat in all the figures, the lower the curve, the better the performance.As shown in Figs. 4 and 5, the performance of D-MimlSvm is better than those of MimlSvm and MimlSvmmi in mostcases. Specifically, pairwise t-tests with 95% significance level disclose that: a) On the scene classification task, among allthe 35 configurations (7 evaluation criteria × 5 percentages of multi-label bags), the performance of D-MimlSvm is superiorto MimlSvm and MimlSvmmi in 88% and 80% cases, comparable to them in 6% and 20% cases, and inferior to them in only6% and none cases; b) On the text categorization task, among all the 35 configurations, the performance of D-MimlSvm issuperior to MimlSvm and MimlSvmmi in 82% and 82% cases, comparable to them in 9% and 18% cases, and inferior to themin only 9% and none cases. The results suggest that D-MimlSvm is a good choice for learning with moderate number ofMIML examples.5.7. DiscussionThe regularization framework presented in this section has an important assumption, that is, all the class labels sharesome commonness, i.e., the w 0 in Eq. (8). This assumption makes the regularization easier to realize, however, it over-simplifies the real scenario. In fact, in real applications it is rare that all class labels share some commonness; it is more2306Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320Fig. 4. Results on the scene classification data set with different percentage of multi-label data. The lower the curve, the better the performance.Fig. 5. Results on the text categorization data set with different percentage of multi-label data. The lower the curve, the better the performance.typical that some class labels share some commonness, but the commonness shared by different labels may be different.For example, class label y1 may share something with class label y2, and y2 may share something with y3, but maybe y1shares nothing with y3. So, a more reasonable assumption is that different pairs of labels share different things (or evennothing). By considering this assumption, a more powerful method may be developed.Actually, it is not difficult to modify the framework of Eq. (12) by replacing the role of w 0 by W whose element W i jexpresses the relatedness between the i-th and j-th class labels, that is,min12T 2(cid:7)i, j(cid:7)w i − W i j(cid:7)2 + 1T 2(cid:7)i, jμi j(cid:7)W i j(cid:7)2 + γ V .Note that W is a tensor and W i j is a vector.(25)Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202307To minimize Eq. (25), taking derivative to W i j , we have−(w i − W i j) − (w j − W ji) + 2μi j W i j + 2μ ji W ji = 0.Considering W i j = W ji and μi j = μ ji , we have−(w i − W i j) − (w j − W i j) + 4μi j W i j = 0,and so,W i j = w i + w j4μ ji + 2.Put Eq. (26) into Eq. (25), we have(cid:14)(cid:14)(cid:14)(cid:14)(4μi j + 1)w i − w j4μi j + 212T 2min(cid:7)i, j2(cid:14)(cid:14)(cid:14)(cid:14)+ 1T 2(cid:14)(cid:14)(cid:14)(cid:14)μi j(cid:7)i, jw i + w j4μi j + 2After simplification, Eq. (25) becomesmin18T 2− 14T 2(cid:9)(cid:7)i, j(cid:7)i, j+ 10μi j + 116μ2i j(2μi j + 1)2(cid:7)w i(cid:7)2 + 2μi j + 1(2μi j + 1)2(cid:7)w j(cid:7)22μi j + 1(2μi j + 1)2(cid:11)w i, w j(cid:12) + γ V .+ γ V .2(cid:14)(cid:14)(cid:14)(cid:14)(cid:10)So, the new optimization task becomesminA,ξ ,δ,b18T 2T(cid:7)(cid:9)T(cid:7)+ 10μi j + 116μ2i j(2μi j + 1)2i K αi + 2μi j + 1α(cid:8)(2μi j + 1)2α(cid:8)j K α j(cid:10)i K α j + γα(cid:8)mTξ (cid:8)1 + γ λmTδ(cid:8)12μi j + 1(2μi j + 1)2(cid:5)(cid:2) 1 − ξit,s.t.j=1i=1T(cid:7)T(cid:7)i=1j=1(cid:8)I( Xi )αt + bt− 14T 2(cid:4)yitkξ (cid:2) 0,(cid:8)I(xi j)αt − δit (cid:3) kI( Xi )αt − maxj=1,...,nikk(cid:8)(cid:8)I( Xi )αt,(cid:8)kI(xi j )αt (cid:3) δit.(26)(27)(28)By solving Eq. (28) we can get not only an MIML learner, but also some understanding on the relatedness betweenpairs of labels from W i j , and some understanding on the different importance of the W i j ’s in determining the concernedclass label from μi j ’s; this may be very helpful for understanding the complicated concepts underlying the task. Eq. (28),however, is difficult to solve since it involves too many variables. Thus, how to exploit/understand the pairwise relatednessbetween different pairs of labels remains an open problem.6. Solving single-instance multi-label problems through MIML transformationThe previous sections show that when we have access to the real objects and are able to represent complicated objectsas MIML examples, using the MIML framework is beneficial. However, in many practical tasks we are given observationaldata where each object has already been represented by a single instance, and we do not have access to the real objects. Insuch case, we cannot capture more information from the real objects using the MIML representation. Even in this situation,however, MIML is still useful. Here we propose the InsDif (i.e., INStance DIFferentiation) algorithm which transforms single-instance multi-label examples into MIML examples to exploit the power of MIML.6.1. InsDifFor an object associated with multiple class labels, if it is described by only a single instance, the information corre-sponding to these labels are mixed and thus difficult to learn. The basic assumption of InsDif is that the spatial distributionof instances with different labels encodes information helpful for discriminating these labels, and such information willbecome more explicit by breaking the single-instances into a number of instances each corresponds to one label.2308Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320InsDif is a two-stage algorithm, which is based on instance differentiation. In the first stage, InsDif transforms eachexample into a bag of instances, by deriving one instance for each class label, in order to explicitly express the ambiguityof the example in the input space; in the second stage, an MIML learner is utilized to learn from the transformed data set.For the consistency with our previous description of the algorithm [81], in the current version of InsDif we use a two-levelclassification strategy, but note that other MIML algorithms such as D-MimlSvm can also be applied.Using the same denotation as that in Sections 3 and 4, that is, given data set S = {(x1, Y 1), (x2, Y 2), . . . , (xm, Ym)}, where}, yik ∈ Y (k = 1, 2, . . . , li). Here li denotes the number ofxi ∈ X is an instance and Y i ⊆ Y a set of labels { yi1, yi2, . . . , yi,lilabels in Y i .In the first stage, InsDif derives a prototype vector vl for each class label l ∈ Y by averaging all the training instancesbelonging to l, i.e.,vl = 1|Sl|(cid:9) (cid:7)(cid:10)xi,xi ∈Slwhere(cid:24)Sl =xi | {xi, Y i} ∈ S, l ∈ Y i(cid:25),l ∈ Y.(29)Here vl can be approximately regarded as a profile-style vector describing common characteristics of the class l. Actually,this kind of prototype vectors have already shown their usefulness in solving text categorization problems. For example,the Rocchio method [34,59] forms a prototype vector for each class by averaging all the documents (represented by weightvectors) of this class, and then classifies the test document by calculating the dot-products between the weight vector rep-resenting the document and each of the prototype vectors. Here we use such prototype vectors to facilitate bag generation.After obtaining the prototype vectors, each example xi is re-represented by a bag of instances B i , where each instance in B iexpresses the difference between xi and a prototype vector according to Eq. (30). In this way, each example is transformedinto a bag whose size equals to the number of class labels.B i = {xi − vl | l ∈ Y}.(30)In fact, such a process attempts to exploit the spatial distribution since xi − vl in Eq. (30) is a kind of distance betweenxi and vl. The transformation can also be realized in other ways. For example, other than referring to the prototype vectorof each class, one could also consider the following approach. For each possible class l, identify the k-nearest neighbors ofxi among training instances that have l as a proper label. Then, the mean vector of these neighbors can be regarded as aninstance in the bag. Note that the transformation of a single instance into a bag of instances can be realized as a generalpre-processing method which can be plugged into many learning systems.In the second stage, InsDif learns from the transformed training set S∗ = {(B1, Y 1), (B2, Y 2), . . . , (Bm, Ym)}. This task canbe realized by any MIML learning algorithm. By default we use the MimlNn algorithm introduced in Section 4.3.2. The useof other MIML algorithms for this stage will also be studied in the next section.The pseudo-code of InsDif is summarized in Appendix A (Table A.4). In the first stage (Steps 1 to 2), InsDif transformseach example into a bag of instances by querying the class prototype vectors. In the second stage (Step 3), an MIMLalgorithm is used to learn from the transformed data set. A test example xis then transformed into the corresponding bagrepresentation Band then fed to the learned MIML model.∗∗6.2. ExperimentsWe compare InsDif with several state-of-the-art multi-label learning algorithms, including AdtBoost.MH [22], RankSvm[27], MlSvm [11], Ml-knn [80] and Cnmf [43]; these algorithms have been introduced briefly in Section 2. In addition, byusing MimlBoost, MimlSvm and MimlSvmmi respectively to replace MimlNn for realizing the second stage of InsDif, weget three variants of InsDif, i.e., InsDifMIMLBOOST, InsDifMIMLSVM and InsDifMIMLSVMmi . These variants are also evaluated forcomparison.Note that the experiments here are very different from that in Sections 4.3 and 5.6. In Sections 4.3 and 5.6, it is assumedthat the data are MIML examples; while in this section, it is assumed that we are given observational data where eachreal object has already been represented as a single instance. In other words, in this section we are trying to learn fromsingle-instance multi-label examples, and therefore the experimental data sets are different from those used in Sections 4.3and 5.6.6.2.1. Yeast gene functional analysisThe task here is to predict the gene functional classes of the Yeast Saccharomyces cerevisiae, which is one of thebest studied organisms. Specifically, the Yeast data set investigated in [27,80] is studied. Each gene is represented by aZ.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202309Table 3Results (mean±std.) on Yeast gene data set (‘↓’ indicates ‘the smaller the better’; ‘↑’ indicates ‘the larger the better’).ComparedalgorithmsInsDifInsDifMIMLSVMInsDifMIMLSVMmiAdtBoost.MHRankSvmMlSvmMl-knnCnmfEvaluation criteria↓hloss.189±.010.189±.009.196±.011.212±.008.207±.013.199±.009.194±.010N/A↓one-error.214±.030.232±.040.238±.043.247±.029.243±.039.227±.032.230±.030.354±.184↓coverage6.288±0.2406.625±0.2616.396±0.2066.385±0.1517.090±0.5027.220±0.3386.275±0.2407.930±1.089↓rloss.163±.017.179±.015.172±.012N/A.195±.021.201±.019.167±.016.268±.062↑aveprec.774±.019.763±.021.765±.019.739±.022.750±.026.749±.021.765±.021.668±.093↑avgrecl.602±.026.591±.023.655±.024N/A.500±.047.572±.023.574±.022N/A↑avgF1.677±.023.666±.022.706±.017N/A.600±.041.649±.022.656±.021N/A103-dimensional feature vector generated by concatenating a gene expression vector and the corresponding phylogeneticprofile. Each 79-element gene expression vector reflects the expression levels of a particular gene under two different ex-perimental conditions, while the phylogenetic profile is a Boolean string, each bit indicating whether the concerned genehas a close homolog in the corresponding genome. Each gene is associated with a set of functional classes whose maximumsize can be potentially more than 190. Elisseeff and Weston [27] have pre-processed the data set where only the knownstructure of the functional classes are used. In fact, the whole set of functional classes is structured into hierarchies up to 4levels deep.7 Illustrations on the first level of the hierarchy used to generate the Yeast data can be found in [27,79,80]. Theresulting multi-label data set contains 2417 genes, fourteen possible class labels and the average number of labels for eachgene is 4.24 ± 1.57.For InsDif, the parameter M is set to be 20% of the size of training set; The performance of this algorithm with differentM settings is shown in Appendix B (Fig. B.3), it can be found that its performance is not sensitive to the setting of M. Theboosting rounds of AdtBoost.MH are set to 25; The performance of this algorithm at different boosting rounds is shownin Appendix B (Fig. B.4), it can be observed that after this round its performance has become stable. (Similar observationsare also found in Section 6.2.2.) For RankSvm, polynomial kernel with degree 8 is used as suggested in [27]. For MlSvm, a# features ). For Cnmf, a normalized Gaussian kernelGaussian kernel is used with default Libsvm setting for kernel width (i.e.as recommended in [43] is used to compute the pairwise class similarity. For Ml-knn, the number of nearest neighborsconsidered is set to 10. The criteria introduced in Section 4.3.1 are used to evaluate the learning performance. Ten-foldcross-validation is conducted on this data set and the results are summarized in Table 3,8 where the best performance oneach criterion has been highlighted in boldface.1Table 3 shows that InsDif and its variants achieve good performance on the Yeast gene functional data set. Pairwiset-tests with 95% significance level disclose that: a) InsDif is significantly better than all the compared multi-label learningalgorithms (i.e., the second part of Table 3) on all criteria, except that on coverage it is worse than Ml-knn but the differenceis not statistically significant9; b) InsDifMIMLSVM is significantly better than the compared multi-label learning algorithmsfor more than 68% cases, and is significantly inferior to them for less than 11% cases; c) InsDifMIMLSVMmiis significantlybetter than the compared multi-label learning algorithms for more than 65% cases, and is never significantly inferior tothem. Specifically, InsDifMIMLSVMmi outperforms all the compared algorithms in terms of average recall and average F1. It isnoteworthy that Cnmf performs quite poorly compared to other algorithms although it has used test set information. Thereason may be that the key assumption of Cnmf, i.e., two examples with high similarity in the input space tend to havelarge overlap in the output space, does not hold on this gene data since there are some genes whose functions are quitedifferent but the physical appearances are similar.Overall, results on the Yeast gene functional analysis task suggest that MIML can be useful when we are given observa-tional data where each complicated object has already been represented by a single instance.6.2.2. Web page categorizationThe web page categorization task has been studied in [39,65,80]. The web pages were collected from the “yahoo.com”domain and then divided into 11 data sets based on Yahoo’s top-level categories.10 After that, each page is classified intoa number of Yahoo’s second-level subcategories. Each data set contains 2000 training documents and 3000 test docu-ments. The simple term selection method based on document frequency (the number of documents containing a specific7 See http://mips.gsf.de/proj/yeast/catalogues/funcat/ for more details.8 Hamming loss, average recall and average F1 are not available for Cnmf; ranking loss, average recall and average F1 are not available for AdtBoost.MH. Theperformance of InsDifMIMLBOOST is not reported since this algorithm did not terminate within reasonable time on this data.9 Note that our implementation of RankSvm was obtained with the help of the authors of [27], yet our results are somewhat worse than the bestresults reported in [27]. We think that the performance gap may be caused by the minor implementation differences and the different experimental datapartitions. Nevertheless, it is worth mentioning that the results of InsDif are better than the best results of RankSvm in [27] in terms of hamming loss,one-error and average precision, and as same as the best results of RankSvm in [27] in terms of ranking loss.10 Data set available at http://www.kecl.ntt.co.jp/as/members/ueda/yahoo.tar.gz.2310Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320Table 4Results (mean±std.) on eleven web page categorization data sets (‘↓’ indicates ‘the smaller the better’; ‘↑’ indicates ‘the larger the better’).ComparedalgorithmsInsDifInsDifMIMLSVMAdtBoost.MHRankSvmMlSvmMl-knnCnmfEvaluation criteria↓hloss.039±.013.043±.015.044±.014.043±.014.042±.015.043±.015N/A↓one-error.381±.118.395±.119.477±.144.424±.135.375±.119.471±.157.509±.142↓coverage4.545±1.2856.823±1.6234.177±1.2617.228±2.4426.919±1.7674.097±1.2366.717±1.588↓rloss.102±.037.166±.045N/A.182±.057.168±.047.102±.045.171±.058↑aveprec.686±.091.653±.093.621±.108.621±.108.660±.093.625±.116.561±.114↑avgrecl.377±.163.501±.105N/A.252±.172.378±.167.292±.189N/A↑aveF1.479±.154.566±.102N/A.345±.177.472±.156.381±.196N/Aterm) was applied to each data set to reduce the dimensionality. Actually, only 2% words with the highest documentfrequency were retained in the final vocabulary. Other term selection methods such as information gain and mutual in-formation can also be adopted. After term selection, each document in the data set is described as a feature vector usingthe “Bag-of-Words” representation, i.e., each feature expresses the number of times a vocabulary word appearing in thedocument.Characteristics of the web page data sets are summarized in Appendix C (Table C.1). Compared to the Yeast datain Section 6.2.1, here the instances are represented by much higher-dimensional feature vectors and a large portion ofthem (about 20–45%) are multi-labeled. Moreover, here the number of categories (21–40) are much larger and manyof them are rare categories (about 20–55%). So, the web page data sets are more difficult than the Yeast data tolearn.The parameter settings are similar as those in Section 6.2.1. That is, for InsDif, the parameter M is set to be 20% ofthe size of training set; the boosting rounds of AdtBoost.MH are set to 25; for RankSvm, polynomial kernel is used wherepolynomial degrees of 2 to 9 are considered as in [27] and chosen by hold-out tests on training sets; for MlSvm andCnmf, linear and Gaussian kernel are used respectively; for Ml-knn, the number of nearest neighbors considered is setto 10.Results of the eleven data sets are shown in Appendix C (Fig. C.1), and the average results are summarized in Table 4where the best performance on each criterion has been highlighted in boldface.11Table 4 shows that InsDif and InsDifMIMLSVM perform well on the Yahoo data. Pairwise t-tests with 95% significancelevel disclose that: a) InsDif is only inferior to AdtBoost.MH and Ml-knn in terms of coverage, inferior to MlSvm interms of one-error, comparable to Ml-knn in terms of ranking loss, comparable to MlSvm in terms of average recall andaverage F1. Under all the other circumstances (more than 79% cases), the performance of InsDif is significantly betterthan the compared multi-label learning algorithms (i.e., the second part of Table 4); b) InsDifMIMLSVM is significantlybetter than the compared multi-label learning algorithms for more than 44% cases, and is significantly inferior to themfor less than 18% cases. Specifically, InsDifMIMLSVM achieves the best performance in terms of average recall and averageF1; on one-error, it is only inferior to MlSvm but significantly superior the other compared multi-label learning algo-rithms.Overall, results on the web page categorization task suggest that MIML can be useful when we are given observationaldata where each complicated object has already been represented by a single instance.7. Solving multi-instance single-label problems through MIML transformationIn many tasks we are given observational data where each object has already been represented as a multi-instancesingle-label example, and we do not have access to the real objects. In such case, we cannot capture more information fromthe real objects using the MIML representation. Even in this situation, however, MIML is still useful. Here we propose theSubCod (i.e., SUB-COncept Discovery) algorithm which transforms multi-instance single-label examples into MIML examplesto exploit the power of MIML.7.1. SubCodFor an object that has been described by multi-instances, if it is associated with a label corresponding to a high-levelcomplicated concept such as Africa in Fig. 2(a), it may be quite difficult to learn this concept directly. The basic assumptionof SubCod is that high-level complicated concepts can be derived by a number of lower-level sub-concepts which arerelatively clearer and easier for learning, so that we can transform the single-label into a set of labels each corresponds to11 The performance of InsDifMIMLBOOST and InsDifMIMLSVMmi are not reported since these algorithms did not terminate within reasonable time on thisdata. Note that though the significant differences between some numbers in the table might be subtle at the first glance (e.g., InsDif vs. RankSvm in termsof one-error), statistical tests based on detailed information (in online supplementary file) justify the significance.Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202311one sub-concept. Therefore, we can learn these labels at first and then derive the high-level complicated label based onthem, as illustrated in Fig. 2(b).SubCod is a two-stage algorithm, which is based on sub-concept discovery. In the first stage, SubCod transforms eachsingle-label example into a multi-label example by discovering and exploiting sub-concepts involved by the original label;this is realized by constructing multiple labels through unsupervised clustering all instances and then treating each clusteras a set of instances of a separate sub-concept. In the second stage, the outputs learned from the transformed data setare used to derive the original labels that are to be predicted; this is realized by using a supervised learning algorithm topredict the original labels from the sub-concepts predicted by an MIML learner.Using the same denotation as that in Sections 3 and 4, that is, given data set {( X1, y1), ( X2, y2), . . . , ( Xm, ym)}, where}, xi j ∈ X ( j = 1, 2, . . . , ni), and yi ∈ Y is the label of Xi . Here ni denotes theXi ⊆ X is a set of instances {xi1, xi2, . . . , xi,ninumber of instances in Xi .In the first stage, SubCod collects all instances from all the bags to compose a data setD = {x11, . . . , x1,n1 , x21, . . . , x2,n2 , . . . , xm1, . . . , xm,nm}.(cid:2)For the ease of discussion, let N =i=1 ni and re-index the instances in D as {x1, x2, . . . , xN }. A Gaussian mixture modelwith M mixture components is to be learned from D by the EM algorithm, and the mixture components are regardedas sub-concepts. The parameters of the mixture components, i.e., the means μk, covariances Σk and mixing coefficientsπk (k = 1, 2, . . . , M), are randomly initialized and the initial value of the log-likelihood is evaluated. In the E-step, theresponsibilities are measured according tomγik =(cid:2)πkN (xi|μk, Σk)j=1 π jN (xi|μ j, Σ j)M(i = 1, 2, . . . , N).In the M-step, the parameters are re-estimated according toμnewk=Σ newk=π newk=(cid:2)(cid:2),Ni=1 γikxi(cid:2)Ni=1 γiki=1 γik(xi − μnewNkNi=1 γik(cid:2)(cid:2)Ni=1 γikN,)(xi − μnewk)T,and the log-likelihood is evaluated according toln p(D|μ, Σ, π ) =N(cid:7)(cid:9) M(cid:7)lni=1k=1(cid:4)π newkNxi | μnewk, Σ newk(cid:10)(cid:5).(31)(32)(33)(34)(35)After the convergence of the EM process (or after a pre-specified number of iterations), we can estimate the associatedsub-concept for every instance xi ∈ D (i = 1, 2, . . . , N) bysc(xi) = arg maxγik(k = 1, 2, . . . , M).k(36)Then, we can derive the multi-label for each Xi (i = 1, 2, . . . , m) by considering the sub-concept belongingness. Let cidenote an M-dimensional binary vector where each element is either +1 or −1. For j = 1, 2, . . . , M, ci j = +1 means thatthe sub-concept corresponding to the j-th Gaussian mixture component appears in Xi , while ci j = −1 means that this sub-concept does not appear in Xi . Here the value of ci j can be determined according to a simple rule that ci j = +1 if Xi hasat least one instance which takes the j-th sub-concept (i.e., satisfying Eq. (36)); otherwise ci j = −1. Note that for exampleswith identical single-label, the derived multi-labels for them may be different.The above process works in an unsupervised way which does not consider the original labels of the bags Xi ’s. Thus,the derived multi-labels ci need to be polished by incorporating the relation between the sub-concepts and the originallabel of Xi . Here the maximum margin criterion is used. In detail, consider a vector zi with elements zi j ∈ [−1.0, +1.0]( j = 1, 2, . . . , M); zi j = +1 means that the label ci j should not be modified while zi j = −1 means that the label ci j should= ci (cid:16) zi as that for j = 1, 2, . . . , M, qi j = ci j zi j . Let θ denote the smallest number of labels thatbe inverted. Denote qicannot be inverted. SubCod attempts to optimize the objective12(cid:4)minw,b,ξ , Zs.t.(cid:7)w(cid:7)22+ Cm(cid:7)ξii=1w T(ci (cid:16) zi) + byiξ (cid:2) 0, −1 (cid:3) zi j (cid:3) 1(cid:5)(cid:2) 1 − ξi,2312Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320Table 5Predictive accuracy on Musk1, Musk2, Elephant, Tiger and Fox data sets.ComparedalgorithmsSubCodSubCodMIMLNNSubCodMIMLSVMmiDiverse DensityEm-ddmi-SvmMi-SvmCh-FdData setsMusk10.850±0.0350.859±0.0250.870±0.0230.8800.8480.8740.7790.888(cid:7)i, jzi j (cid:2) 2θ − mM,where Z = [z1, z2, . . . , zm].Musk20.921±0.0140.888±0.0220.869±0.0200.8400.8490.8360.8430.857Elephant0.836±0.0100.815±0.0230.805±0.017N/A0.7830.8200.8140.824Tiger0.808±0.0130.795±0.0180.787±0.016N/A0.7210.7890.8400.822Fox0.616±0.0200.599±0.0320.590±0.015N/A0.5610.5820.5940.604(37)By solving Eq. (37) we will get the vector zi which maximizes the margin of the prediction of the proper labelsof Xi . Here we solve Eq. (37) iteratively. We initialize Z with all 1’s. First, we fix Z to get the optimal w and b;this is a standard QP problem. Then, we fix w and b to get the optimal Z ; this is a standard LP problem. These twosteps are iterated till convergence. Finally, we set the multi-label vector’s elements which correspond to positive ci j zi j ’s(i = 1, 2, . . . , m; j = 1, 2, . . . , M) to +1, and set the remaining ones to −1. Thus, we get all the polished multi-label vec-tors ˜ci for the bags Xi . Thus, the original data set {( X1, y1), ( X2, y2), . . . , ( Xm, ym)} is transformed to an MIML data set{( X1, ˜c1), ( X2, ˜c2), . . . , ( Xm, ˜cm)}, and any MIML algorithms can be applied.To map the multi-labels predicted by the MIML classifier for a test example to the original single-labels y ∈ Y , in thesecond stage of SubCod, a traditional classifier f : {+1, −1}M → Y is generated from the data set {(˜c1, y1), (˜c2, y2), . . . ,(˜cm, ym)}. This is relatively simple and traditional supervised learning algorithms can be applied.The pseudo-code of SubCod is summarized in Appendix A (Table A.5). In the first stage (Steps 1 to 3), SubCod derivesmulti-labels via sub-concept discovery and transforms single-label examples into MIML examples, from which an MIMLlearner is generated. In the second stage (Step 4), a traditional classifier is trained to map the derived multi-labels to theis fed to the MIML learner to get its multi-labels, and the multi-labels are then fedoriginal single-labels. Test example Xto the supervised classifier to get the label ypredicted for X∗∗∗.7.2. ExperimentsWe compare SubCod with several state-of-the-art multi-instance learning algorithms, including Diverse Density [45], Em-dd [83], mi-Svm and Mi-Svm [3], and Ch-Fd [31]; these algorithms have been introduced briefly in Section 2. For SubCod,the MIML learner in Step 3 is realized by MimlSvm and the classifier in Step 4 is realized by Smo with default parameters.In addition, by using MimlNn and MimlSvmmi respectively to replace MimlSvm for realizing Step 3 of SubCod, we get twovariants of SubCod, i.e., SubCodMIMLNN and SubCodMIMLSVMmi . They are also evaluated for comparison.12Note that the experiments here are very different from that in Sections 4.3, 5.6 and 6.2. Both Sections 4.3 and 5.6deal with learning from MIML examples, Section 6.2 deals with learning from single-instance multi-label examples, whilethis section deals with learning from multi-instance single-label examples, and therefore the experimental data sets in thissection are different from those used in Sections 4.3, 5.6 and 6.2.Five benchmark multi-instance learning data sets are used, including Musk1, Musk2, Elephant, Tiger and Fox. Both Musk1and Musk2 are drug activity prediction data sets, publicly available at the UCI machine learning repository [8]. Here ev-ery bag corresponds to a molecule, while every instance corresponds to a low-energy shape of the molecule [24]. Musk1contains 47 positive bags and 45 negative bags, and the number of instances contained in each bag ranges from 2 to40. Musk2 contains 39 positive bags and 63 negative bags, and the number of instances contained in each bag rangesfrom 1 to 1044. Each instance is a 166-dimensional feature vector. Elephant, Tiger and Fox are three image annotationdata sets generated by [3] for multi-instance learning. Here every bag is an image, while every instance corresponds toa segmented region in the image [3]. Each data set contains 100 positive and 100 negative bags, and each instance is a230-dimensional feature vector. These data sets are popularly used in evaluating the performance of multi-instance learningalgorithms.Parameters of SubCod are determined by hold-out tests on training sets. Specifically, candidate values of M (the numberof Gaussian mixture components) range between [10, 70], while candidate values of θ (the smallest number of labels that12 We have also evaluated the variant SubCodMIMLBOOST which is obtained by employing MimlBoost to replace MimlSvm, however, it did not terminatewithin reasonable time and so its performance is not reported in this section.Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202313cannot be inverted) range between [mM × 10%, mM × 70%]. Ten runs of ten-fold cross validation are performed and theresults are summarized in Table 5, where the best performance on each data set has been highlighted in boldface. Notethat the results of the compared algorithms (second part of Table 5) are the best performance reported in literatures [3,31].13Table 5 shows that SubCod and its variants are very competitive to state-of-the-art multi-instance learning algorithms.In particular, on Musk2 their performance are much better than other algorithms. This is expectable because Musk2 is acomplicated data set which has the largest number of instances, while on such data set the sub-concept discovery processof SubCod may be more effective.Overall, the experimental results suggest that MIML could be useful when we are given observational data where eachobject has already been represented as a multi-instance single-label example.8. ConclusionThis paper extends our preliminary work [81,92] to formalize the MIML Multi-Instance Multi-Label learning framework,where an example is described by multiple instances and associated with multiple class labels. It was inspired by therecognition that when solving real-world problems, having a good representation is often more important than having astrong learning algorithm because a good representation may capture more meaningful information and make the learningtask easier to tackle. Since many real objects are inherited with input ambiguity as well as output ambiguity, MIML is morenatural and convenient for tasks involving such objects.To exploit the advantages of the MIML representation, we propose the MimlBoost algorithm and the MimlSvm algorithmbased on a simple degeneration strategy. Experiments on scene classification and text categorization show that solvingproblems involving complicated objects with multiple semantic meanings under the MIML framework can lead to goodperformance. Considering that the degeneration process may lose information, we also propose the D-MimlSvm algorithmwhich tackles MIML problems directly in a regularization framework. Experiments show that this “direct” Svm algorithmoutperforms the “indirect” MimlSvm algorithm.In some practical tasks we are given observational data where each complicated object has already been represented bya single instance, and we do not have access to the real objects such that we cannot capture more information from thereal objects using the MIML representation. For such scenario, we propose the InsDif algorithm which transforms single-instances into MIML examples to learn. Experiments on Yeast gene functional analysis and web page categorization showthat such algorithm is able to achieve a better performance than learning the single-instances directly. This is not difficultto understand. Actually, by representing the multi-label object using multi-instances, the structure information collapsed intraditional single-instance representation may become easier to exploit, and for each label the number of training instancescan be significantly increased. So, transforming multi-label examples to MIML examples for learning may be beneficial insome tasks.MIML can also be helpful for learning single-label examples involving complicated high-level concepts. Usually it maybe quite difficult to learn such concepts directly since many different lower-level concepts are mixed together. If we cantransform the single-label into a set of labels corresponding to some sub-concepts, which are relatively clearer and easierto learn, we can learn these labels at first and then derive the high-level complicated label based on them. Inspired by thisrecognition, we propose the SubCod algorithm which works by discovering sub-concepts of the target concept at first andthen transforming the data into MIML examples to learn. Experiments show that this algorithm is able to achieve betterperformance than learning the single-label examples directly in some tasks.We believe that semantics exist in the connections between atomic input patterns and atomic output patterns; while aprominent usefulness of MIML, which has not been realized in this paper, is the possibility of identifying such connection.As stated in Section 3, in the MIML framework it is possible to understand why a concerned object has a certain classlabel; this may be more important than simply making an accurate prediction, because the results could be helpful forunderstanding the source of ambiguous semantics.AcknowledgementsThe authors want to thank the anonymous reviewers for helpful comments and suggestions. The development ofMimlSvmmi , InsDifMIMLSVM, InsDifMIMLSVMmi , SubCodMIMLNN and SubCodMIMLSVMmi owes to reviewers’ suggestions. The au-thors also want to thank De-Chuan Zhan and James Kwok for help on D-MimlSvm, Yang Yu for help on SubCod, and AndréElisseeff and Jason Weston for providing the Yeast data and the implementation details of RankSvm. A preliminary Chineseversion has been presented at the Chinese Workshop on Machine Learning and Applications 2009. This research was sup-ported by the National Fundamental Research Program of China (2010CB327903) and the National Science Foundation ofChina (61073097, 61021062).13 The tradition of the multi-instance learning community is to compare with the best performance reported in literature. Since the detailed results arenot available [3,17,18,31,32,45,67,83], we do not perform statistical significance tests at here.2314Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320Appendix A. Pseudo-codes of the learning algorithmsTable A.1The MimlBoost algorithm.1 Transform each MIML example ( Xu, Y u ) (u = 1, 2, . . . , m) into |Y| number of multi-instance bags {[( Xu , y1), Ψ ( Xu, y1)], . . . , [( Xu , y|Y|), Ψ ( Xu, y|Y|)]}.Thus, the original data set is transformed into a multi-instance data set containing m × |Y| number of multi-instance bags, denoted by{[( X (i), y(i)), Ψ ( X (i), y(i))]} (i = 1, 2, . . . , m × |Y|).2 Initialize weight of each bag to W (i) = 13 Repeat for t = 1, 2, . . . , T iterations:m×|Y| (i = 1, 2, . . . , m × |Y|).3a Assign the bag’s label Ψ ( X (i), y(i)) to each of its instances (x(i)j , y(i)) (i = 1, 2, . . . , m × |Y|; j = 1, 2, . . . , ni ), set the weight of the j-th instance ofthe i-th bag W(i)j= W (i)/ni , and build an instance-level predictor ht [(x(i)j , y(i))] ∈ {−1, +1}.3b For the i-th bag, compute the error rate e(i) ∈ [0, 1] by counting the number of misclassified instances within the bag, i.e.(cid:2)nie(i) =(cid:2)ht [(x(i)j , y(i))](cid:6)=Ψ ( X (i), y(i))(cid:3)j=1.ni3c If e(i) < 0.5 for all i ∈ {1, 2, . . . , m × |Y|}, go to Step 4.(cid:2)m×|Y|3d Compute ct = arg mincti=1 W (i) exp[(2e(i) − 1)ct ].3e If ct (cid:2) 0, go to Step 4.3f Set W (i) = W (i) exp[(2e(i) − 1)ct ] (i = 1, 2, . . . , m × |Y|) and re-normalize such that 0 (cid:2) W (i) (cid:2) 1 and∗j is X∗j , y)]) = +1} (x∗ = { y | sign(’s j-th instance).t ct ht [(x(cid:2)(cid:2)∗j4 Return Y(cid:2)m×|Y|i=1 W (i) = 1.Table A.2The MimlSvm algorithm.1 For MIML examples ( Xu, Y u ) (u = 1, 2, . . . , m), Γ = { Xu |u = 1, 2, . . . , m}.2 Randomly select k elements from Γ to initialize the medoids Mt (t = 1, 2, . . . , k), repeat until all Mt do not change:2a Γt = {Mt } (t = 1, 2, . . . , k).2b Repeat for each Xu ∈ (Γ − {Mt |t = 1, 2, . . . , k}):index = argmint∈{1,...,k} dH ( Xu, Mt ), Γindex = Γindex ∪ { Xu}.dH ( A, B) (t = 1, 2, . . . , k).(cid:2)2c Mt = arg min A∈ΓtB∈Γt3 Transform ( Xu , Y u ) into a multi-label example (zu , Y u ) (u = 1, 2, . . . , m), where zu = (zu1, zu2, . . . , zuk) = (dH ( Xu , M1), dH ( Xu, M2), . . . , dH ( Xu , Mk)).4 For each y ∈ Y, derive a data set D y = {(zu , Φ(zu , y))|u = 1, 2, . . . , m}, and then train an Svm h y = SVMTrain(D y ).∗, Mk)).∗) (cid:3) 0, y ∈ Y}, where z5 Return Y∗ = {arg max y∈Y h y (z∗, M2), . . . , dH ( X∗)} ∪ { y|h y (z∗, M1), dH ( X∗ = (dH ( XTable A.3Efficient algorithm for Eq. (24).t , ξ t1, . . . , ξ tm, δt1, . . . , δtm, bt ) = 0Pick p indexes of constraints that are not in St randomly, denoted by I;Compute Lossi for every constraint in I;% find out the cutting planeq = arg maxi∈I LossiIf Lossq > εFor t = 1, . . . , TInput: K , λ, μ, γ , ε, { Xi , Y i }mi=11 ∀t, St = ∅, vt = (α T2 Repeat345678910111213 Until no St changesEnd IfEnd ForSt = St ∪ {q};vt ← optimized over St ;Table A.4The InsDif algorithm.1 For single-instance multi-label examples (xu , Y u ) (u = 1, 2, . . . , m), compute the prototype vectors vl (l ∈ Y) using Eq. (29).2 Derive the new training set S3 Learning from S∗ = {(B 1, Y 1), (B 2, Y 2), . . . , (Bm, Ym)} by using an MIML algorithm.by transforming each xi into a bag of instances B i using Eq. (30).∗Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202315Table A.5The SubCod algorithm.1 For multi-instance single-label examples ( Xu, yu ) (u = 1, 2, . . . , m), collect all the instances x ∈ Xu together and identify the Gaussian mixture com-ponents through the EM process detailed in Eqs. (31) to (35).2 Determine the sub-concept for every instance x ∈ Xu according to Eq. (36), and then derive the label vector cu for Xu .3 Make corrections to cu by optimizing Eq. (37), which results in ˜cu for Xu , and then train an MIML learner ht ( X) on {( Xu, ˜cu )} (u = 1, 2, . . . , m).4 Train a classifier h y (˜c) on {(˜cu , yu )} (u = 1, 2, . . . , m), which maps the derived multi-labels to the original single-labels.5 Return y∗)).∗ = h y (ht ( XAppendix B. Parameter settings of the learning algorithmsFig. B.1. Performance of MimlBoost and AdtBoost.MH at different rounds on scene classification data set.Fig. B.2. Performance of MimlSvm with different k values on scene classification data set.2316Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320Fig. B.3. Performance of InsDif with different M settings on Yeast gene data set.Fig. B.4. Performance of AdtBoost.MH at different rounds on Yeast gene data set.Appendix C. Web page data setsTable C.1Characteristics of the web page data sets (after term selection). PMC denotes the percentage of documents belonging to more than one category; ANLdenotes the average number of labels for each document; PRC denotes the percentage of rare categories, i.e., the kind of category where only less than 1%instances in the data set belong to it.Data SetArts&HumanitiesBusiness&EconomyComputers&InternetEducationEntertainmentHealthRecreation&SportsReferenceScienceSocial&ScienceSociety&CultureNumber ofcategoriesVocabularysize26303333213222334039274624386815506406126067937431047636Training setPMC44.50%42.20%29.60%33.50%29.30%48.05%30.20%13.75%34.85%20.95%41.90%ANL1.6271.5901.4871.4651.4261.6671.4141.1591.4891.2741.705PRC19.23%50.00%39.39%57.58%28.57%53.13%18.18%51.52%35.00%56.41%25.93%Test setPMC43.63%41.93%31.27%33.73%28.20%47.20%31.20%14.60%30.57%22.83%39.97%ANL1.6421.5861.5221.4581.4171.6591.4291.1771.4251.2901.684PRC19.23%43.33%36.36%57.58%33.33%53.13%18.18%54.55%40.00%58.97%22.22%Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202317Fig. C.1. Results on the eleven Yahoo data sets.Supplementary materialThe online version of this article contains additional supplementary material.Please visit doi:10.1016/j.artint.2011.10.002.2318Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320References[1] É. Alphonse, S. Matwin, Filtering multi-instance problems to reduce dimensionality in relational learning, Journal of Intelligent Information Sys-tems 22 (1) (2004) 23–40.[2] R.A. Amar, D.R. Dooly, S.A. Goldman, Q. Zhang, Multiple-instance learning of real-valued data, in: Proceedings of the 18th International Conference onMachine Learning, Williamstown, MA, 2001, pp. 3–10.[3] S. Andrews, I. Tsochantaridis, T. Hofmann, Support vector machines for multiple-instance learning, in: S. Becker, S. Thrun, K. Obermayer (Eds.), Advancesin Neural Information Processing Systems, vol. 15, MIT Press, Cambridge, MA, 2003, pp. 561–568.[4] P. Auer, On learning from multi-instance examples: Empirical evaluation of a theoretical approach, in: Proceedings of the 14th International Conferenceon Machine Learning, Nashville, TN, 1997, pp. 21–29.[5] P. Auer, P.M. Long, A. Srinivasan, Approximating hyper-rectangles: Learning and pseudo-random sets, Journal of Computer and System Sciences 57 (3)(1998) 376–388.[6] P. Auer, R. Ortner, A boosting approach to multiple instance learning, in: Proceedings of the 15th European Conference on Machine Learning, Pisa, Italy,2004, pp. 63–74.[7] Z. Barutcuoglu, R.E. Schapire, O.G. Troyanskaya, Hierarchical multi-label prediction of gene function, Bioinformatics 22 (7) (2006) 830–836.[8] C. Blake, E. Keogh, C.J. Merz, UCI repository of machine learning databases, Department of Information and Computer Science, University of California,Irvine, CA, 1998, http://www.ics.uci.edu/~mlearn/MLRepository.html.[9] H. Blockeel, D. Page, A. Srinivasan, Multi-instance tree learning, in: Proceedings of the 22nd International Conference on Machine Learning, Bonn,Germany, 2005, pp. 57–64.[10] A. Blum, A. Kalai, A note on learning from multiple-instance examples, Machine Learning 30 (1) (1998) 23–29.[11] M.R. Boutell, J. Luo, X. Shen, C.M. Brown, Learning multi-label scene classification, Pattern Recognition 37 (9) (2004) 1757–1771.[12] K. Brinker, J. Fürnkranz, E. Hüllermeier, A unified model for multilabel classification and ranking, in: Proceedings of the 17th European Conference onArtificial Intelligence, Riva del Garda, Italy, 2006, pp. 489–493.[13] K. Brinker, E. Hüllermeier, Case-based multilabel ranking, in: Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hydrabad,India, 2007, pp. 702–707.[14] L. Cai, T. Hofmann, Hierarchical document categorization with support vector machines, in: Proceedings of the 13th ACM International Conference onInformation and Knowledge Management, Washington, DC, 2004, pp. 78–87.[15] N. Cesa-Bianchi, C. Gentile, L. Zaniboni, Hierarchical classification: Combining Bayes with SVM, in: Proceedings of the 23rd International Conference onMachine Learning, Pittsburgh, PA, 2006, pp. 177–184.[16] C.-C. Chang, C.-J. Lin, LIBSVM: A library for support vector machines, Technical report, Department of Computer Science and Information Engineering,National Taiwan University, Taipei, 2001.[17] Y. Chen, J. Bi, J.Z. Wang, MILES: Multiple-instance learning via embedded instance selection, IEEE Transactions on Pattern Analysis and Machine Intel-ligence 28 (12) (2006) 1931–1947.[18] Y. Chen, J.Z. Wang, Image categorization by learning and reasoning with regions, Journal of Machine Learning Research 5 (2004) 913–939.[19] P.-M. Cheung, J.T. Kwok, A regularization framework for multiple-instance learning, in: Proceedings of the 23rd International Conference on MachineLearning, Pittsburgh, PA, 2006, pp. 193–200.[20] Y. Chevaleyre, J.-D. Zucker, A framework for learning rules from multiple instance data, in: Proceedings of the 12th European Conference on MachineLearning, Freiburg, Germany, 2001, pp. 49–60.[21] A. Clare, R.D. King, Knowledge discovery in multi-label phenotype data, in: Proceedings of the 5th European Conference on Principles of Data Miningand Knowledge Discovery, Freiburg, Germany, 2001, pp. 42–53.[22] F. De Comité, R. Gilleron, M. Tommasi, Learning multi-label altenating decision tree from texts and data, in: Proceedings of the 3rd InternationalConference on Machine Learning and Data Mining in Pattern Recognition, Leipzig, Germany, 2003, pp. 35–49.[23] L. De Raedt, Attribute-value learning versus inductive logic programming: The missing links, in: Proceedings of the 8th International Workshop onInductive Logic Programming, Madison, WI, 1998, pp. 1–8.[24] T.G. Dietterich, R.H. Lathrop, T. Lozano-Pérez, Solving the multiple-instance problem with axis-parallel rectangles, Artificial Intelligence 89 (1–2) (1997)31–71.[25] T. Pham Dinh, H.A. Le Thi, A D.C. optimization algorithm for solving the trust-region subproblem, SIAM Journal on Optimization 8 (2) (1998) 476–505.[26] G.A. Edgar, Measure, Topology, and Fractal Geometry, Springer, Berlin, 1990.[27] A. Elisseeff, J. Weston, A kernel method for multi-labelled classification, in: T.G. Dietterich, S. Becker, Z. Ghahramani (Eds.), Advances in Neural Infor-mation Processing Systems, vol. 14, MIT Press, Cambridge, MA, 2002, pp. 681–687.[28] T. Evgeniou, C.A. Micchelli, M. Pontil, Learning multiple tasks with kernel methods, Journal of Machine Learning Research 6 (2005) 615–637.[29] J. Foulds, E. Frank, A review of multi-instance learning assumptions, Knowledge Engineering Review 25 (1) (2010) 1–25.[30] Y. Freund, L. Mason, The alternating decision tree learning algorithm, in: Proceedings of the 16th International Conference on Machine Learning, Bled,Slovenia, 1999, pp. 124–133.[31] G. Fung, M. Dundar, B. Krishnappuram, R.B. Rao, Multiple instance learning for computer aided diagnosis, in: B. Schölkopf, J. Platt, T. Hofmann (Eds.),Advances in Neural Information Processing Systems, vol. 19, MIT Press, Cambridge, MA, 2007, pp. 425–432.[32] T. Gärtner, P.A. Flach, A. Kowalczyk, A.J. Smola. Multi-instance kernels, in: Proceedings of the 19th International Conference on Machine Learning,Sydney, Australia, 2002, pp. 179–186.[33] S. Godbole, S. Sarawagi, Discriminative methods for multi-labeled classification, in: Proceedings of the 8th Pacific–Asia Conference on KnowledgeDiscovery and Data Mining, Sydney, Australia, 2004, pp. 22–30.[34] D.J. Ittner, D.D. Lewis, D.D. Ahn, Text categorization of low quality images, in: Proceedings of the 4th Annual Symposium on Document Analysis andInformation Retrieval, Las Vegas, NV, 1995, pp. 301–315.[35] R. Jin, Z. Ghahramani, Learning with multiple labels, in: S. Becker, S. Thrun, K. Obermayer (Eds.), Advances in Neural Information Processing Systems,vol. 15, MIT Press, Cambridge, MA, 2003, pp. 897–904.[36] T. Joachims, Text categorization with support vector machines: Learning with many relevant features, in: Proceedings of the 10th European Conferenceon Machine Learning, Chemnitz, Germany, 1998, pp. 137–142.[37] Z. Jorgensen, Y. Zhou, M. Inge, A multiple instance learning strategy for combating good word attacks on spam filters, Journal of Machine LearningResearch 8 (2008) 993–1019.[38] F. Kang, R. Jin, R. Sukthankar, Correlated label propagation with application to multi-label learning, in: Proceedings of the IEEE Computer SocietyConference on Computer Vision and Pattern Recognition, New York, NY, 2006, pp. 1719–1726.[39] H. Kazawa, T. Izumitani, H. Taira, E. Maeda, Maximal margin labeling for multi-topic text categorization, in: L.K. Saul, Y. Weiss, L. Bottou (Eds.), Advancesin Neural Information Processing Systems, vol. 17, MIT Press, Cambridge, MA, 2005, pp. 649–656.[40] J.E. Kelley, The cutting-plane method for solving convex programs, Journal of the Society for Industrial and Applied Mathematics 8 (4) (1960) 703–712.[41] H. Kück, N. de Freitas, Learning about individuals from group statistics, in: Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence,Edinburgh, Scotland, 2005, pp. 332–339.Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–23202319[42] J.T. Kwok, P.-M. Cheung, Marginalized multi-instance kernels, in: Proceedings of the 20th International Joint Conference on Artificial Intelligence,Hydrabad, India, 2007, pp. 901–906.[43] Y. Liu, R. Jin, L. Yang, Semi-supervised multi-label learning by constrained non-negative matrix factorization, in: Proceedings of the 21st NationalConference on Artificial Intelligence, Boston, MA, 2006, pp. 421–426.[44] P.M. Long, L. Tan, PAC learning axis-aligned rectangles with respect to product distributions from multiple-instance examples, Machine Learning 30 (1)(1998) 7–21.[45] O. Maron, T. Lozano-Pérez, A framework for multiple-instance learning, in: M.I. Jordan, M.J. Kearns, S.A. Solla (Eds.), Advances in Neural InformationProcessing Systems, vol. 10, MIT Press, Cambridge, MA, 1998, pp. 570–576.[46] O. Maron, A.L. Ratan, Multiple-instance learning for natural scene classification, in: Proceedings of the 15th International Conference on MachineLearning, Madison, MI, 1998, pp. 341–349.[47] A. McCallum, Multi-label text classification with a mixture model trained by EM, in: Working Notes of the AAAI’99 Workshop on Text Learning,Orlando, FL, 1999.[48] G.-J. Qi, X.-S. Hua, Y. Rui, J. Tang, T. Mei, H.-J. Zhang. Correlative multi-label video annotation, in: Proceedings of the 15th ACM International Conferenceon Multimedia, Augsburg, Germany, 2007, pp. 17–26.[49] R. Rahmani, S.A. Goldman, MISSL, Multiple-instance semi-supervised learning, in: Proceedings of the 23rd International Conference on Machine Learn-ing, Pittsburgh, PA, 2006, pp. 705–712.[50] R. Rak, L. Kurgan, M. Reformat, Multi-label associative classification of medical documents from medline, in: Proceedings of the 4th InternationalConference on Machine Learning and Applications, Los Angeles, CA, 2005, pp. 177–186.[51] S. Ray, M. Craven, Supervised versus multiple instance learning: An empirical comparison, in: Proceedings of the 22nd International Conference onMachine Learning, Bonn, Germany, 2005, pp. 697–704.[52] S. Ray, D. Page, Multiple instance regression, in: Proceedings of the 18th International Conference on Machine Learning, Williamstown, MA, 2001,pp. 425–432.[53] J. Rousu, C. Saunders, S. Szedmak, J. Shawe-Taylor, Learning hierarchical multi-category text classification models, in: Proceedings of the 22nd Interna-tional Conference on Machine Learning, Bonn, Germany, 2005, pp. 774–751.[54] G. Ruffo, Learning single and multiple instance decision trees for computer security applications, PhD thesis, Department of Computer Science, Univer-sity of Turin, Torino, Italy, 2000.[55] G. Salton, Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer, Addison-Wesley, Reading, MA, 1989.[56] R.E. Schapire, Y. Singer, BoosTexter: A boosting-based system for text categorization, Machine Learning 39 (2–3) (2000) 135–168.[57] B. Schölkopf, A.J. Smola, Learning With Kernels: Support Vector Machines, Regularization, Optimization, and Beyond, MIT Press, Cambridge, MA, 2002.[58] S.D. Scott, J. Zhang, J. Brown, On generalized multiple-instance learning, Technical Report UNL-CSE-2003-5, Department of Computer Science, Universityof Nebraska, Lincoln, NE, 2003.[59] F. Sebastiani, Machine learning in automated text categorization, ACM Computing Surveys 34 (1) (2002) 1–47.[60] B. Settles, M. Craven, S. Ray, Multiple-instance active learning, in: J.C. Platt, D. Koller, Y. Singer, S. Roweis (Eds.), Advances in Neural InformationProcessing Systems, vol. 20, MIT Press, Cambridge, MA, 2008, pp. 1289–1296.[61] A.J. Smola, B. Schölkopf, Sparse greedy matrix approximation for machine learning, in: Proceedings of the 17th International Conference on MachineLearning, San Francisco, CA, 2000, pp. 911–918.[62] A.J. Smola, S.V.N. Vishwanathan, T. Hofmann, Kernel methods for missing variables, in: Proceedings of the 10th International Workshop on ArtificialIntelligence and Statistics, Savannah Hotel, Barbados, 2005, pp. 325–332.[63] F.A. Thabtah, P.I. Cowling, Y. Peng. MMAC, A new multi-class, multi-label associative classification approach, in: Proceedings of the 4th IEEE Interna-tional Conference on Data Mining, Brighton, UK, 2004, pp. 217–224.[64] I. Tsochantaridis, T. Joachims, T. Hofmann, Y. Altun, Large margin methods for structured and interdependent output variables, Journal of MachineLearning Research 6 (2005) 1453–1484.[65] N. Ueda, K. Saito, Parametric mixture models for multi-labeled text, in: S. Becker, S. Thrun, K. Obermayer (Eds.), Advances in Neural InformationProcessing Systems, vol. 15, MIT Press, Cambridge, MA, 2003, pp. 721–728.[66] P. Viola, J. Platt, C. Zhang, Multiple instance boosting for object detection, in: Y. Weiss, B. Schölkopf, J. Platt (Eds.), Advances in Neural InformationProcessing Systems, vol. 18, MIT Press, Cambridge, MA, 2006, pp. 1419–1426.[67] J. Wang, J.-D. Zucker, Solving the multi-instance problem: A lazy learning approach, in: Proceedings of the 17th International Conference on MachineLearning, San Francisco, CA, 2000, pp. 1119–1125.[68] N. Weidmann, E. Frank, B. Pfahringer, A two-level learning method for generalized multi-instance problem, in: Proceedings of the 14th EuropeanConference on Machine Learning, Cavtat-Dubrovnik, Croatia, 2003, pp. 468–479.[69] G.M. Weiss, Mining with rarity – problems and solutions: A unifying framework, SIGKDD Explorations 6 (1) (2004) 7–19.[70] X. Xu, E. Frank, Logistic regression and boosting for labeled bags of instances, in: Proceedings of the 8th Pacific–Asia Conference on KnowledgeDiscovery and Data Mining, Sydney, Australia, 2004, pp. 272–281.[71] C. Yang, T. Lozano-Pérez, Image database retrieval with multiple-instance learning techniques, in: Proceedings of the 16th International Conference onData Engineering, San Diego, CA, 2000, pp. 233–243.[72] Y. Yang, An evaluation of statistical approaches to text categorization, Information Retrieval 1 (1–2) (1999) 67–88.[73] Y. Yang, J.O. Pedersen, A comparative study on feature selection in text categorization, in: Proceedings of the 14th International Conference on MachineLearning, Nashville, TN, 1997, pp. 412–420.[74] K. Yu, S. Yu, V. Tresp. Multi-label informed latent semantic indexing, in: Proceedings of the 28th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval, Salvador, Brazil, 2005, pp. 258–265.[75] A.L. Yuille, A. Rangarajan, The concave-convex procedure, Neural Computation 15 (4) (2003) 915–936.[76] C. Zhang, P. Viola, Multiple-instance pruning for learning efficient cascade detectors, in: J.C. Platt, D. Koller, Y. Singer, S. Roweis (Eds.), Advances inNeural Information Processing Systems, vol. 20, MIT Press, Cambridge, MA, 2008, pp. 1681–1688.[77] M.-L. Zhang, Z.-H. Zhou, Improve multi-instance neural networks through feature selection, Neural Processing Letters 19 (1) (2004) 1–10.[78] M.-L. Zhang, Z.-H. Zhou, Adapting RBF neural networks to multi-instance learning, Neural Processing Letters 23 (1) (2006) 1–26.[79] M.-L. Zhang, Z.-H. Zhou, Multilabel neural networks with applications to functional genomics and text categorization, IEEE Transactions on Knowledgeand Data Engineering 18 (10) (2006) 1338–1351.[80] M.-L. Zhang, Z.-H. Zhou, ML-kNN: A lazy learning approach to multi-label learning, Pattern Recognition 40 (7) (2007) 2038–2048.[81] M.-L. Zhang, Z.-H. Zhou, Multi-label learning by instance differentiation, in: Proceedings of the 22nd AAAI Conference on Artificial Intelligence, Van-couver, Canada, 2007, pp. 669–674.[82] M.-L. Zhang, Z.-H. Zhou, Multi-instance clustering with applications to multi-instance prediction, Applied Intelligence 31 (1) (2009) 47–68.[83] Q. Zhang, S.A. Goldman, EM-DD: An improved multi-instance learning technique, in: T.G. Dietterich, S. Becker, Z. Ghahramani (Eds.), Advances in NeuralInformation Processing Systems, vol. 14, MIT Press, Cambridge, MA, 2002, pp. 1073–1080.[84] Q. Zhang, W. Yu, S.A. Goldman, J.E. Fritts, Content-based image retrieval using multiple-instance learning, in: Proceedings of the 19th InternationalConference on Machine Learning, Sydney, Australia, 2002, pp. 682–689.2320Z.-H. Zhou et al. / Artificial Intelligence 176 (2012) 2291–2320[85] Y. Zhang, Z.-H. Zhou, Multi-label dimensionality reduction via dependency maximization, ACM Transactions on Knowledge Discovery from Data 4 (3)(2010), Article 14.[86] Z.-H. Zhou, K. Jiang, M. Li, Multi-instance learning based web mining, Applied Intelligence 22 (2) (2005) 135–147.[87] Z.-H. Zhou, X.-Y. Liu, On multi-class cost-sensitive learning, in: Proceeding of the 21st National Conference on Artificial Intelligence, Boston, WA, 2006,pp. 567–572.[88] Z.-H. Zhou, J.-M. Xu, On the relation between multi-instance learning and semi-supervised learning, in: Proceeding of the 24th International Conferenceon Machine Learning, Corvallis, OR, 2007, pp. 1167–1174.[89] Z.-H. Zhou, Y.Yu. AdaBoost, in: X. Wu, V. Kumar (Eds.), The Top Ten Algorithms in Data Mining, Chapman & Hall, Boca Raton, FL, 2009, pp. 127–149.[90] Z.-H. Zhou, M.-L. Zhang, Neural networks for multi-instance learning, Technical report, AI Lab, Department of Computer Science and Technology,Nanjing University, Nanjing, China, August 2002.[91] Z.-H. Zhou, M.-L. Zhang, Ensembles of multi-instance learners, in: Proceeding of the 14th European Conference on Machine Learning, Cavtat-Dubrovnik,Croatia, 2003, pp. 492–502.[92] Z.-H. Zhou, M.-L. Zhang, Multi-instance multi-label learning with application to scene classification, in: B. Schölkopf, J. Platt, T. Hofmann (Eds.), Ad-vances in Neural Information Processing Systems, vol. 19, MIT Press, Cambridge, MA, 2007, pp. 1609–1616.[93] Z.-H. Zhou, M.-L. Zhang, Solving multi-instance problems with classifier ensemble based on constructive clustering, Knowledge and Information Sys-tems 11 (2) (2007) 155–170.