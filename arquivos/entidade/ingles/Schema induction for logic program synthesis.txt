Artificial Intelligence 98 (1998) I-47 Artificial Intelligence Schema induction for logic program synthesis Nancy Lynn Tinkham ’ Computer Science Department, Rowan University, 201 Mullica Hill Road, Glassboro, NJ 08028, USA Received February 1993; revised August 1997 Abstract Prolog program synthesis can be made more efficient by using schemata which capture similar- ities in previously-seen programs. Such schemata narrow the search involved in the synthesis of a new program. We define a generalization operator for forming schemata from programs and a downward refinement operator for constructing programs from schemata. These operators define schema-hierarchy graphs which can be used to aid in the synthesis of new programs. Algorithms are presented for efficiently obtaining least generalizations of schemata, for adding new schemata to a schema-hierarchy graph, and for using schemata to construct new programs. @ 1998 Elsevier Science KV. Keywords: Inductive logic programming; Inductive inference; Automatic programming; Learning 1. Introduction When writing computer programs, people often find it useful to draw on the knowledge of other ‘programs that have been written before. An experienced programmer may, when presented with a new problem, recall solving a similar problem on an earlier occasion; a novice programmer may use examples from the classroom or textbook to guide problem- solving. This observation motivates the following hypothesis: One way for machines to syn- thesize programs is ( 1) to see examples of programs, (2) to form generalizations which capture -information about the forms of programs, and then (3) to use these general- izations in writing future programs. This paper will describe a language for expressing generalizations of programs, an algorithm for deriving generalizations, and an algorithm for synthesizing a program from a generalization. ’ Email: nlt@rowau.edu. 0004-3702/98/$19.00 @ 1998 Elsevier Science B.V. All rights reserved. HI SOOO4-3702(97)00055-6 2 N.L. Tinkhatn/Art@cial Intelligence 98 (1998) l-47 an As illustration, the sujix(Su.List, List), which succeeds amples of the intended behavior of the su#ix predicate: task if SufJixist consider of synthesizing predicate the Prolog is a suffix of List, given some ex- positive examples: wm([cl7 [GhCl), &w[Y~ql~ LLY,41) negative examples: sufJix( [a, b] , [a, b, c] ) , sufJix( [z I, [w, x] ) Our task becomes easier if we have knowledge of the similar predicates pre@( PrejixList, List), ITW. Pwx[ pre$.d[XIYl,[XIZl) :-pyWI:Z). which succeeds if PrefixList is a prefix of List, and member(Element, List), member( Y [ VI W] ) . member( X, [ YlZ] ) :- member( X, Z). which succeeds which is a generalization if Element is a member of List. The first step is to derive a schema of prejix and member. One such schema is the following: Q(K [XIYI) :- QtZ Y>. This schema expresses as the base of the recursion, Further, we have the recursive call involves information the tail of that list. the structure of a recursive clause together with a clause serving in Prolog programs. the that one of the arguments a structure which is very common is a list, and that Taking this schema as a starting point, we search for a program which is a special- ization of the schema and which succeeds on all of the positive examples but none of the negative examples; eventually, the search finds wfJix(w v. su&(X, [YIZ]) :- su&(X, Z). The process, a generalization about each of these programs; information, examples. a program which then, has of a set of programs and captures some of the structural two major components: (1) to derive a schema which information (2) to derive from a schema, making use of this structural is consistent with a given set of positive and negative is I. I. Background Inductive inference, tions, including grammatical the process of learning inference, from examples, covers a range of applica- structures inference of logic formulas, learning N.L. i%kham/Art@cial Intelligence 98 (1998) 1-47 3 encoded in semantic nets, hypothesizing mathematical theorems, and automatic pro- gramming. . Gold 1: 161 introduced identification in the limit as a model for the inference of a langua,ge from examples. Learning, in Gold’s paper, is performed by enumerative is, algorithms that in some systematic way consider all machines in a algorithms-that given class until a machine for the target language is found. The main advantage of an enumerauve algorithm is its thoroughness, which often makes it possible to prove which classes of languages can and cannot be identified by the algorithm. This thoroughness is also the main disadvantage of an enumerative approach, in that the vast number of possibilities examined makes the search extremely slow. One o-F the improvements to enumerative algorithms is the introduction of refinement operators, which prune the search without sacrificing theoretical power. Refinement op- [ 30-321, who used them for refining discarded erators were introduced by Shapiro hypotheses. The mathematics of refinement operators in themselves were studied by Laird [ 19,201, who described both “downward” refinement (of which Shapiro’s op- erators were examples) and “upward” refinement. We will return to refinement op- erators in Section 2.2, defining some operators for generalizing and specializing pro- grams. Concept learning has also been studied by Valiant [ 401, Angluin and Laird [ 11, Mitchell [ 251, Michalski [ 241, and Winston [41]. Early work on finding least gener- alization,s of literals and clauses was done by Popplestone [2S], Reynolds [29], and Plotkin [26,27]. A survey of inductive inference systems is given by Angluin and Smith [ 21, and work in grammatical inference is surveyed by Biermann and Feldman [61. Automatic programming systems have been designed to work with LISP, Prolog, and other languages, and the input to these systems variously includes input/output example:s (as in [ 371)) input/output specifications [ 4,7,23,34], and computation traces (e.g., [ 51). The use of transformation rules to construct or improve programs has been studied by Burstall and Darlington [ 81 and Dershowitz [ 11,121. Logic program [ 131, Gilbert and Hogger [ 151, synthesis has been studied by Flener and Deville Sterling and Kirschenbaum [ 351, Lau and Prestwich [ 211, Bergadano and Gunetti [ 31, Grobelnik [ 171, Johansson [ 181, and others. 1.2. Prcject overview This paper describes a project which applies the idea of refinement operators to the problem of using known programs to aid in the synthesis of a new program to fit a set of positive and negative examples. It defines an upward refinement operator and an algorithm for using this operator efficiently to find generalizations of programs, called schemat.a. The paper then shows how these schemata can be used to narrow the search involved in program synthesis. These ideas have been implemented as a Prolog system which repeatedly adds new schemata to its knowledge base so that the system becomes increasingly more efficient at synthesizing new programs. This implemented system will be described at the end of the paper. 4 N.L. linkham/Artijcial Intelligence 98 (1998) l-47 2. Language and operator definitions Prolog has been chosen in this paper as the basis for the language in which to express programs and schemata. Programs will be represented as multisets of Prolog-like clauses; the clauses will differ from the standard Prolog form in that the right hand side of a clause will be regarded as a multiset * rather than a sequence of literals. We will restrict our attention to programs defining only a single predicate. Hence, one example of a program is: Jrafl4[RISl, [m) .@~W[[WlIW9X> :- {atom(R),\==(R, [ ]),jLmw(S,T)}, :- Cfratten([U~Vl,Y),Jlatten(U!Z),uppend(I:Z,X)}) Observe that this program would continue to be a correct definition of J&ten even if the sequence of literals or clauses were different; we are specifically choosing to study order-independent programs. This enables us to view programs more directly as logic expressions, without involving the extra-logical concept of order of computation. It is also more in keeping with a philosophy of Prolog programming which favors writing, where possible, programs which do not depend on the order of execution for correctness. A schema will have a representation like that of a program, except that a schema may contain predicate variables and may contain the symbol Cl (empty clause). The special symbols 0 and {Cl} will be used to represent the most specific and most general schemata, respectively. We will use “program” as the special case of “schema” in which no predicate variables or empty clauses occur; hence, a program is a schema, but a schema may or may not be a program. For an overview of the Prolog programming language, see [ 101 and [ 361. 2.1. Language deJinitions terminology must be introduced here for describing programs and schemata. Some A term is an individual-variable, an individual-constant, or a function symbol with its arguments. In the flatten program, X, [ 1, and [ RIS] are all terms. A literal is a predicate symbol with its arguments; atom(R) is an example of a literal. A clause is either 0 (representing the empty clause), a single literal, or an expression of the form where Ai, . . . , A,, are literals, and {AZ, . . . , An} is a multiset of liter&. The literal in a single-literal clause and the literal on the left-hand side of a multi-literal clause (Ai ) are positive literals; the literals on the right-hand side of a clause (AZ, . . . , A,) are negative literals. As an example, one of the clauses in thehtten program is * A multiset is a collection of objects in which repetition is significant, but, as in a set, order is not significant. The operations U (union), n (intersection), C (subset), c (proper subset), + (sum), and - (difference) on literals within clauses and on clauses within schemata will be multiset operations. For a definition of these multiset operators, see Appendix A and [22]. N.L. Tinkham/Artijcial Intelligence 98 (1998) 1-47 5 Jlattfm([[UlVlIWl,X) :- Cflatten([UlVl,Y),~tten(~Z),uppend(ZZ,X)} literal, jZutfen( [ [ UlV] (WI, X), It contains a positive literals, Jlatren( [ UlV] , Y) , jkuten( W Z) , and append( x Z, X) . A schema is a multiset of clauses which contains only one predicate symbol in the clauses’ positive literals. Thus, the flatten program above is a schema, as is and three negative {P(X) :- {R(XJN P(W) :- {P(Z))) However, {P(X) :- {NXJ)), q(.Z) :- {P(Z))) is not a schema, since it contains both p and q in its positive literals. A schema which contains no predicate-variables and does not contain Cl is called a program. The definition of @#en above, for example, is a program. letters (a, b, . . .), function symbols as lower-case The following notation conventions will be used for constant and variable symbols: Individual-variables will be written as upper-case letters (X, x. . .) , individual-constants as lower-case letters (f, g, . . .), predicate-constants as lower-case letters (p, q, . , . ), and predicate-variables as upper- case letters (P, Q, . . .). When needed for clarity, the arity of a function or predicate will be indicated by a superscript: f', p4. Mnemonic names for constants and functions (such as append and numerals) will also be used. As in Prolog, when working with the list-forming functor “.“, we will usually use list notation, rather than explicitly nested functions, to represent the list. For example, .(a, .( b, [ ] )) will be written [a, b], and .(u,.(b,X)) will be written [u,blX]. For any particular application, we will define schemata in terms of a finite set of function symbols and predicate-constants; this models a setting in which a finite set of Prolog predicates is “known”, having been previously defined, and we are defining a single new predicate. A finite set K of function symbols, individual-constants, and predicate-constants will be called a constant set. If K is a constant set and A is the set of all integers a such that there is a predicate-constant in K of arity a, then L is defined language over K if L is the set of all schemata u such that to be the schema-de@ition every function symbol, individual-constant, and predicate-constant occurring in c is an element of K, and every predicate-variable P occurring in (+ has an arity up such that up E A. (Observe that, regardless of the choice of K, L will contain the elements 0 and {Cl]..) Example. Let K = {p’, 42, f’, b}, and let L be the schema-definition language over K. Some examples of schemata in L are: (1) {P(X) :- {4(1:Z),P(f(Y)),P(Z)}, P(b)) (2) {R(X) :- {P(X))) 6 N.L. 7hkham/ArtiJicial Intelligence 98 (1998) I-47 Example of an expression which ( 1) is also a program, because is not a schema it contains no predicate-variables. in L is: An example (3) {P(X) :- {r(X))* p(f(Z)) :- {P(Z))> because it contains the predicate-constant r, which is not a member of K. The choice to use multisets rather than ordinary in representing initially) is viewed as a computation, hand side of a Prolog predicate, in that sense, putation; literal is removed. Second, and degree of recursion, used. schemata was made for several reasons. First, that if two identical observe this duplication will the program is not the same as one in which the properties studied are much more well-behaved when a multiset sets (as might be more intuitive if the program literals appear on the right indeed cause a repeated com- the duplicated in Section 3, degree of branching representation is that, from or of form P"(Xl language over a constant Three final language-related definitions will make the discussion of operators . . , X,, are individual-variables the definition of a schema-definition next section easier. Let L be the schema-definition most-general positive literal in a schema u E L is a positive where P” is a predicate-variable, is a predicate-variable and XI,. (Recall in a. integer such that there is a predicate-constant most-general negative literal in (+ E L is similarly defined as a negative of form PO, or of form P” ( XI, X2, . . . , X,), where XI, . . . , X, are individual-variables occurring exactly once in cr, with the additional constraint must occur only once literals because of the requirement literals of a schema must be identical. in the set K. A literal either of form PO, , X2, . . , , X,), where P”, n > 0, exactly once rr must be an in K.) A literal either P” for positive in the positive occurring language, the constants that the predicate symbols appearing that the predicate-variable in (T. This constraint from the definition of arity n among is omitted A most-general term in cr is a term which is either an term of form f”(Xt,Xz,... individual-variables occurring exactly once in ff. , X,), where f” is a function individual-constant or a symbol and Xt , . . . ,X,, are 2.2. Operator de$nitions an ordering This section introduces operators. The next section will describe that follows, operators. V by t will be written as {V\t}. In the presentation relation on schemata and a family of refinement and some of the properties of the relation of replacing the substitution all occurrences An interpretation is a set I of ground atoms. A goal is a single ground atom. A schema u is said to cover goal g in I if ( 1) one of the clauses (2) in g is the symbol 0; or (T contains 8 such that CUB = g and either Ai0 is in I or u covers AiB, for 1 < i < n. a clause K of the form a :- {At, . . . , A,}, and there is a substitution The set of goals covered by a schema (+ in an interpretation I will be denoted by C,(U) . N.L. 7inkham/Artificinl Intelligence 98 (1998) 1-47 1 Let 1 be and u be {q(u),q(b),r(u)}, Ex=wle. (+ covers the goal p(a), because 0 contains the clause p(a), and p(a) {} = p(u). covers the goal p(f(u) ), because u contains the clause p(f(X)) q(X)}: if we apply the substitution {X\u} to the literals of this clause, obtaining p (f( a) ) {p(u),q(u)}, is in I. a- does not cover p(b) or p(f(b)). :- we see that p(f(u)) matches our goal, p(u) is covered by U, and q(u) :- (p(X),q(X)}}. {p(u),p(f(X)) :- {p(X), (+ also Definition. Define an equivalence relation z on schemata: For schemata (+I and uz, (~1 M (~2 exactly if (+I and uz are identical except for, possibly, the naming of variables and the order of listing negative literals within a clause and clauses within a schema. ExampIes. renamed Y. {P(X) {P(X), p(4) = (Q(Y), Q(d). since P can be renamed Q and X can be :- {q(X),r(X)},P(a)} M {P(u),P(X) :- {r(X),q(X)}},sincetheydiffer only in order. For simplicity of presentation in the remainder of the paper, two schemata that are equivalent in the sense of M (that is, two schemata that differ only in variable names and in order of literals and clauses) will be considered to be the same schema. Definitioa. Define a partial order on schemata 5, as follows: If ~1 = (~1 :- S1 and K2 = (Y2 :- $2 are clauses, where Si and S2 are (possibly empty) multisets of literals, then ~1 ;6 K:! exactly if (1) K2isCl,or (2) there is a substitution B such that a# = (~1 and ,726 & Si (where & is the multiset subset relation). If gi and ~3 are schemata, then [TI 5 (~2 exactly if ( 1) ~1 and (+2 contain only 0 clauses, and ~1 contains at least as many clauses as ~2; or (2) a’i contains at least one non-Cl clause, and there is a one-to-one mapping 4 from clauses in (+I to clauses in uz and a substitution 6 such that if ~1 E (~1, ~2 E c2, and ~2 = @( ~1) , then KI 5 ~2 With substitution 8. Example. If (~1 is {P(U) :- {Q(a,V),r(b)}, p(u)) and ~72 is {P(Z)* P(W) :- {S(X, Y)}, P(C) :- {P(d))), then u1 5 ~2, with 8 = {Z\U, w\v, S\Q, X\U, r\v}. 8 N.L. 7Fnkham/Art$cial Intelligence 98 (1998) l-47 The ordering 5 is easily seen to be reflexive (let 0 be the empty substitution) and transitive (since we can compose substitutions). Tinkham [ 381 shows that 5 is antisymmetric and thus that 5 is a partial order. Given an ordering on expressions such as 5, Laird [20] defines an upward re- jinement y to be a recursively enumerable relation on expressions such that y* is 5, and a downward re$nement p to be a recursively enumerable relation on expressions such that p* is 5-l. When viewed computationally, y and p are referred to as (up- denotes the set of all ward and downward) refinement operators. The notation r(a) is the set of expressions which can be produced by applying y once to cr; y”(a) all expressions which can be produced by n applications of y to u; and y*(a) is the set of all expressions which can be produced by 0 or more applications of y to u. We introduce two refinement operators, one for upward refinement (generalization) and one for downward refinement (specialization). In order to make properties of the operators clearer to describe and study, each of the operators has been divided into two parts; hence, we will define generalization operators yi and y2 and specialization operators pr and ~2. Definition of yl. Let K be a set of function symbols and predicate-constants. Let L be the schema-definition language over K, and let err and (+z be schemata in L. Then ~72 E ye (al) exactly if one of the following holds: ( 1) Deleting negative literal: ~2 is derived from ~1 by deleting a most-general negative literal A from some clause K in ur. (2) Separating individual-variables: X is an individual-variable occurring more than once in (+I, and 1+2 is derived from (~1 by replacing one or more, but not all, of the occurrences of X by an individual-variable Y not occurring in (+I. (3) Separating predicate-variables: P is a predicate-variable occurring more than once in (+I, and ~72 is derived from ~1 by replacing one or more, but not all, of the occurrences of P by a predicate-variable Q not occurring in (+I. This rule is, a set of clauses may only be applied when the result will be a schema--that with only one predicate symbol in the positive literals. (4) Generalizing predicate: p is a predicate-constant occurring in a negative literal in ~1, P is a predicate-variable not occurring in ui, and u2 is derived from ut by replacing one or more occurrences of p in negative literals by P. (5) Generalizing predicate: p is a predicate-constant occurring in a positive literal in ur , P is a predicate-variable not occurring in ur , and u2 is derived from ur by replacing all occurrences of p in positive literals and, optionally, one or more occurrences of p in negative literals, by P. (6) Generalizing term: 172 is derived from ur by replacing one or more occur- rences of a most-general term t in ur by an individual-variable X not occurring in ~1. Definition of ~2. Let K be a set of function symbols and predicate-constants. Let L be the schema-definition language over K, and let UI and u2 be schemata in L. Then u2 E yz (ur ) exactly if one of the following holds: N.L. Tinkhum/Artifcial Intelligence 98 (1998) 1-47 9 (1) * E Yl(Ul). (2) Adding clause: I.TI and 02 do not contain Cl, and ~72 is derived from ui by adding one clause K to the set of clauses in (+I. (3) Replacing most-general positive literal by 0: Clause K in CT~ is a set containing a single most-general positive literal and no negative literals, and (~2 is derived from (+I by replacing K by 0. (4) Deleting duplicate occurrence of 0: (~1 is a set containing n + 1 occurrences of 0 (and no other clauses), and ~72 is a set containing n occurrences of 0 (and no other clauses), for some n > 0. Definition of ~1. Let K be a set of function symbols and predicate-constants. Let L be the schema-definition language over K, and let (~1 and cr2 be schemata in L. Then ~72 E pi ((~1) exactly if one of the following holds: ( 1) Adding negative literal: ~-9 is derived from (+I by adding a most-general negative literal A to some clause K in (+I, where K is not q . (2) Unifying individual-variables: X and Y are distinct individual-variables occurring in q, and (+2 is derived from ~1 by replacing all occurrences of Y by X. (3) Unifying predicate-variables: P and Q are distinct predicate-variables occurring in ut, and c+2 is derived from ut by replacing all occurrences of Q by P. (4) R,eplacing predicate-variable by predicate-constant: P is a predicate-variable occurring in ut , p is a predicate-constant, and u2 is derived from ut by replacing all occurrences of P by p. (5) Replacing individual-variable by most-general term: X is an individual-variable occurring in (~1, t is a most-general term, and (~2 is derived from ut by replacing all occurrences of X by t. Definition of pz. Let K be a set of function symbols and predicate-constants. Let L be a schema-definition language over K, and let ut and (~2 be schemata in L. Then (~2 E p2 (ui ) exactly if one of the following holds: (1) u2 E Pl(Ul). (2) Dleleting a clause: (+I and (~2 do not contain 0, K is a clause in ut, and a:! is derived from ui by deleting K. (3) Replacing 0 by a most-general positive literal: 0 E (~1, and u2 is derived from ~1 by replacing 0 by a most-general positive literal. This rule may only be that is, a set of clauses with only one applied when the result will be a schema- predicate symbol in the positive literals. (4) Duplicating 0: UI is a set containing n occurrences of (cid:144)i (and no other clauses), and u2 is a set containing n + 1 occurrences of 0 (and no other clauses), for some n > 0. We add two definitions for discussing these operators: Definition. Let ui and (~2 be schemata. If ui E y;(q), generalization of 172. We will also say that ui is more general than a~. we will say that ut is a 10 N.L. Tinkham/Artificial Intelligence 98 (1998) 1-47 A schema (T is said to be a generalization of a set of schemata I7 if u is a general- ization of every schema in n. Definition. Let ~1 and ~2 be schemata. specialization of ~2. We will also say that (+I is more specific than ~72. If (TI E pg (q), we will say that (+I is a Example. To illustrate tion of a program mar from the most general schema, indicated the use of the refinement operator ~2, here is an example deriva- at each step are in bold.) First, apply rule 4 of p2 to produce a 2-clause schema: (Changes (0). Then replace each 0 with a most-general literal: --t (P(X1, x2, X3), 0) -+ {P(Xl,X2,X3),P(Yl,Y2,Y3)} Next, add some most-general negative literals to the clauses: --+ {P(Xl,X2,X3),P(Yl,Y2,Y3) :-{R(Y4,YS)}} + {P(Xl,X2,X3) :- {Q(X4,X5)},P(Yl,Y2,Y3) :- {R(Y4,Y5)}} Then replace predicate-variables by predicate-constants: --+ {max(Xl,X2,X3) :- {Q(X4,X5)},mw(Yl,Y2,Y3) :- {R(Y4,Y5)}} 4 {max(Xl,X2,X3) :- {Q(X4,XS)},mau(Yl,Y2,Y3) :- (Y4 > YS}} + {max(Xl, X2, X3) :- {X4 2 X5}, max(Y1, Y2, Y3) :- (Y4 > Y5)) Finally, unify the individual-variables until the goal program is produced: --+ (max(X1, X2, X3) :- {X4 2 X5}, max(Y1, Y2, Y3) :- (Y4 > Yl}} -+ {max(Xl,X2,X3) :- {X4 > X5},max(Yl,Y2,Y3) :- (Y2 > Yl}} -+ {max(Xl,X2,X3) :- {X4 2 X5},max(Yl,Y2,Y2) :- (Y2 > Yl}} + {mau(Xl,X2,X3) :- {X4 > X2},max(Yl,Y2,Y2) :- (Y2 > Yl}} -+ {max(Xl,X2,X3) :- {Xl > X2},max(Yl,Y2,Y2) :- (Y2 > Yl}} -+ {m&X1,X2,X1) :- {Xl > X2},max(Yl,Y2,Y2) :- (Y2 > Yl}} 2.3. Basic properties Tinkham [ 38,391 proves several properties of y2 and ~2, listed here as Properties 1-8. Property 1. Let (+I and ~72 be schemata. UI E yl(u2) and pI are inverse operations.) iff a:! E pl (a~). (That is, y1 N.L. i%kham/Artificial Intelligence 98 (1998) l-47 11 Property 2. Let (~1 and ~2 be schemata. (+I E yz(a2) and p2 are inverse operations.) ifs a2 E pz (01). (That is, y2 Property 3. Let K be a constant set for 79, and let L be the schema-definition language over K. Then yz (8) = L. (That is, yg is suficiently powerful to generate all of a schema-definition language from its minimal element.) Property 4. Let K be a constant set for ~2, and let L be the schema-definition language = L. (That is, & is su.ciently powerful to generate all of the over K. Then p; ((0)) schema-c!e@ition language from its maximal element.) Property 5. Let (~1 and (~2 be schemata. Then (+I ,$ (~2 iff q E pl(az). (That is, the ordering induced by the specialization operator is the same as that of 5 ; hence, generalization and specialization are indeed re$nement operators.) The next two basic properties use a function 6, which maps schemata into integers. (A similar function is used by Reynolds [ 291.) Definition. Define ((a) to be (the number of non-punctuation symbols in o) - (the number of distinct variables occurring in a) + (the number of literals in a). For example, (Punctuation symbols are parentheses, braces, commas, and “:-“. Symbols in ex- pressions containing lists are counted as though the lists were represented as nested binary functions, rather than in abbreviated list notation; e.g., [a, b], is analyzed as . (a, . (b, [ ] ) ) , containing 5 non-punctuation symbols.) Propewy 6. (That is, an application of p1 adds at least 1 to the 5 value of a schema.) if UI and u2 are schemata and u2 E p1( (+I ) , then 5( UI ) + 1 < 5( ~2). PropeWy 7. If UI and u:! are schemata and 172 E p; ((~1) , then (~2 E py ((~1) , where n < [(a-~) - l(q). applicat(ions of PI.) (That is, u2 can be derived from (~1 in (((~2) - [(al) orfewer Property 8 shows that schemata related by 5 are also related by the sets of goals covered: Property 8. Let (+I and u2 be schemata and I be an interpretation. If (+I 5 ~2, then Cl ( UI ) G Cl (~2). (That is, if ul is a specialization of ~2, then u1 covers a subset of the goals covered by ~2.) 12 N.L. Tinkham/Arhjicial Intelligence 98 (1998) 147 3. Finding least generalizations The main intuition being explored in this paper is that it ought to be easier to solve a new problem if one has seen problems with similar solutions before. Capturing the similarity of a collection of programs is the focus of this section. We want to find similarities that are as specific and thus as informative as possible; hence, we ask the question this way: Given a set of programs, how can we $nd a least generalization of that set of programs? A least generalization will be defined as follows: Definition. A schema u is said to be a least generalization of a set of schemata L7 if u is a generalization of II and there is no schema (T’ such that ut is a specialization of u and such that o is a generalization of 17. Observe that a least generalization is not, in general, unique. For example, both {P(X) :- {da, Y>)> and {P(X) :- -Cq(Xd))) are least generalizations of the set ((P(X) :- {da, b), dc, d)}}, {p(X) :- {da, d)))}, but neither can be derived from the other using $. 3.1. A simple algorithm for jinding least generalizations One method for finding a least generalization of a set of programs 17 = {at, . . . , vn} is to perform a breadth-first search on the space defined by yz: Beginning with the n sets {?~i},..., {q,,}, add the schemata in yz(ri) to the ith set, for each i. Next, add the schemata in 3/z ( yz (ri) ) to the ith set, for each i. Continue until the intersection of the generalizations of 7~1, of 7r2, and . . . of 7~~ is nonempty. Return this intersection as output. While this procedure will find a least generalization (possibly several), the search will examine a large number of schemata, since the graph defined by yz has a large branching factor. After examining some properties of our refinement operators, we will be able to describe a better algorithm which performs a much more constrained search. 3.2. Properties of rejkement operators In order to develop a more efficient algorithm for finding least generalizations, we will first explore some of the properties of the generalization and refinement operators y2 and pz, so that these properties can be used to restrict the search space. Two properties of particular interest are degree of branching and degree of recursion. N.L. linkham/Artijkial Intelligence 98 (1998) l-47 13 Definition. The degree of branching of a schema U, bd( CT), is defined to be the number of clauses, in u. Definition. Let u be a schema such that the positive predicate symbol 4. Define or constant) maximum occurrences of {n 1 there of 4). is a clause K in (T whose negative the degree of recursion of u, rd(a), to be the literals contain exactly n literals of g contain the (variable Example.. If CT is {4(4.) 9 400 :- {r(X)), 07 :- {~(I:z,w),q(z),q(w)}}, then bd(cr) = 3 and rd(a) = 2. These are natural measures to consider, since which Prolog programs depart from a straight-line clauses a conditional repetition). (allowing branch) Degree of branching and degree of recursion behaved under application of yi and pi. This feature finding least generalizations. in two of the most obvious ways form are ( 1) by containing multiple recursive calls (creating and (2) by containing are, under certain conditions, well- for leads to an efficient algorithm We begin by noting some results which follow immediately from the definitions of ~2 and P:!. l A single application of y2 will increase or leave unchanged the degree of branching the of a schema, and a single application of p2 will decrease or leave unchanged branching degree. l If y:! or p2 is applied to as to leave the degree of branching unchanged in the case of m), (that is, then the the degree of recursion, and the the degree of recursion. is not added, in the case of 72, or deleted, if a clause application of y2 will decrease or leave unchanged of p2 will increase or leave unchanged application for recursion degree to increase under application of y2 (if clauses are It is possible added) and to decrease under application of p2 (if clauses are removed). However, we can show the existence of a generalization (+ for a set of schemata n with the property that the recursion degree of (+ is the minimum of the recursion degrees of the schemata in the selt, regardless of their degrees of branching. This is the task of the following lemmas a.nd theorem. Lemma 9 gives a generalization of any single-clause schema with degree of recur- sion r. Lemma !a. Let K be a schema, other than {O}, of arity a with bd( K) = 1 and rd(K) = r. Then where P is a predicate-variable and X1.0, . . . , X,,, are distinct individual-variables, is a generar’ization of K. {WXLO....,JL,O) {JYX~J,...,&,I) ,X,,,)}}, ,P(XI,~ ,... ,... :- 14 N.L. Tinkhatn/Art$cial Intelligence 98 (1998) 1-47 in K, replace p by a predicate-variable in K, replace c by an individual-variable of V by a variable W not already occurring of any variable V, replace in K. Repeat the first this step Proof. Apply the following operations to K: p occurring in K. ( 1) For each predicate-constant not already occurring (2) For each individual-constant not already occurring If K contains more occurrence until no variable occurs more than once in K. in K. than one occurrence c occurring (3) (4) K now consists entirely of most-general in the positive occurring a predicate-variable predicate-variable literals. Let P be the predicate-variable literal of K. If there is a literal h in K which contains this step until no than P, delete h from K. Repeat other other than P appears Call the result of this sequence of operations K’. By the derivation, in K. K’ is a general- ization of the original schema K. Since K’ = {~(XI,O,. . . ,xa,o) :- {~(XI,I,. . . ,&,I),. . . ,p(xl,r,. . . ,xa,,)}}, the lemma follows. 0 Lemma 10 gives a generalization of any single-clause schema, independent of its degree of recursion. Lemma 10. Let K be a schema, other than {O}, of arity a with bd( K) = 1. Then where P is a predicate-variable and X1, . . . , X, are distinct indi- {PW1,...JLd), vidual-variables, is a generalization of K. Proof. By Lemma 9, is a generalization negative literals, of K. Since P( x1,0, . . . , Xa,a) can be derived from u by deleting r the result follows. El These described lemmas can be extended in the next definition and theorem. to give a generalization of multi-clause schemata, as Definition. Define G( a, b, r) to be the schema containing the clause P(Xl,l,O,. . . 9 X7,1,0> :- (P(Xl.l.1,~ *. 9 &,I,1 19. . . 9 P(Xl,l,r9.. . 9 Xa.1.r)) and, for 2 < i < b, the clauses P(Xl,;,o, . . . , Xa,i,a). For example, G( 3,4,2) is ~p~xl,l,0~~2,1,0~~3,1,0~ :- {~~~1,1,1~~2,1,1~~3,1,1~~~~~1,1,2,~2,1,2,~3,,,2~}, p(x1,2,0, x2.2.09 x3,2,0) 7 p(x1,3,0, x2,3,0, x3,3,0) 9 p ( xl ,4,0 1 x2,4,0 9 x3,4,0 > } N.L. Iinkham/Amjkial Intelligence 98 (1998) 1-47 15 Observe defines a (variable) predicate of arity a. that G(a, b, r) is a schema of branching-degree b and recursion-degree r and Theorem 11. Let WI,. . . , T,, be schemata not containing 0 which define predicates of is a gener- arity a. Then G(a,max(bd(~l),...,bd(~,)},min{rd(?rl),...,rd(~,)}) alization (of (971, . . . , IT”}. Proof. For each i, 1 < i 6 n, let Ki be a clause Lemma 9, in ri such that rd (Ki) = rd (ri) . By (+I = ~(Xl,l,Ol . . * 7 &,l,O) :- {JYXI,l,I,~ * * 5 x7,1,1 1,. * * , ~(Xl,I,,, . . . , X,,l,,>}, where r == min{rd(rr), let $ be the result of removing By repeated use of Lemma 10, . . . , rd(rr,)}, is a generalization of (~1,. . . , K~}. Ki from ri, and let ci be the number of clauses For each i, in n-f. {~(.~l,l,O>~ . . 9 x&1,0), * * * ~~(Xl,Ci,O. * . * 9 ~w,o)} is a generalization of 7~;; hence (+2 = {~(Xl,l,O9 . . * 3 &l,O)* * * * 3 P(Xl,c,O, * . * 9 &,,,o>}, where c = max{cr , . . . , c,} = max{bd( q ) , . . . , bd( ‘rr,)} - 1, is a generalization IT{,..., 7r;). Thus, of G(a,max{bd(~~),...,bd(~,)},min{rd(~~),...,rd(~,)}) =UI +a:! is a generalization of {n-r, . . . , n-,}. El Example.. A generalization of pre_order: (pre_order( nil, [ ] ) , pre_order( tree( Node, Lefi, Right), [ NodeIT] ) :- (pre-order(L& LL) ,pre_order( Right, RL) , append( LL, RL, T)}} and flatten: Cflatfen([ I, [ I>, $utten([HlITll, [HllT2]) :- {atom(Hl),Jlatten(Tl,T2)}, jZatten( [ [ AIB] IT3], L :- {Jlatten([A~Bl,Ll),~utten(T3,L2),append(Ll, L2, L)}] is G(2,3,2): (PC XI,l,O9 X2.1.0) :- {fYXI,I,I7 X2,I,I)~~(X1,1,2, X2.L,2)}9 P(.X1,2,0* X2.2.0) I p(.x1,3,0~ x2.3.0)) 16 N.L. i’inkham/Artificial Intelligence 98 (1998) 1-47 The importance of Theorem 11 is that a generalization further, since be produced directly, without any search; the maximum generalization which has exactly as many clauses as CT, and this least generalization be derived least generalizations, from u by applying only PI. This gives an efficient algorithm u of a set of schemata can the branching degree of CT is is a least can for finding degrees of the schemata in the next section. in the set, there of the branching as shown 3.3. An improved algorithm for jinding least generalization (PI) refinement for finding is found. Algorithm then apply downward section suggest a procedure to that initial approximation The results of the preceding first find a generalization least generaliza- of appropriate branching degree and recursion degree, until a least of tions: and generalization a set of programs. The computation that this min{rd(rrt), will be a generalization to U, yielding ~9; if CT’ is also a generalization other- wise, we retain (T. This process is repeated until pt can no longer be applied, at which point we will, by definition, have found a least generalization. A set MARKED is used to record past applications begins by taking G(a, max{bd(q), as a first approximation CT, since we have shown of (n-1, . . . , T,}. p1 is then applied to find a least generalization of PI, to prevent needless the new approximation; 1 uses this approach of (9~1, . . . , rn}, then (7’ becomes . . . , bd(rn)}, . . . , rd(a,)}) repetition. Algorithm 1. Derive a schema from a set of programs Input: A set of programs Output: A schema IL7 = {rq , . . . , rn), each of which has arity a. (+ such that fl is a feast generalization of the programs in l7. Data structures: A set MARKED, whose elements are representations of applications of pI. An element of MARKED will have one of the following an application r2 (VI, VZ), where VI and ~2 are individual-variables, of rule i of PI: forms, where ri will record representing the unification of zq and ~2, r3 ( 9, Pz), where PI and PZ are predicate-variables, representing the unification of PI and P2. r4 ( V, f) , where Y is an individual-variable representing the replacement of Y by a most-general and f is a function symbol, term with functor f. r5 (P, p), where P is a predicate-variable, and p is a predicate-constant, representing the replacement of P by p. A set CONST-SET, containing constant and function symbols. Procedure: CONSTSET c the set of all individual-constants, function symbols, and predicate-constants u c G(a,max{bd(q),. MARKED +- 8. repeat occurring in lI7. . . ,bd(m,)}, min{rd(q),.. . ,rd(r,,)}). Select an application CY of PI to u such that N.L. lMham/Art@cial Intelligence 98 (1998) 1-47 17 CY is not a member of MARKED; every constant or function symbol introduced by pl is an element of CONST-SET; and every predicate-variable predicate-constant in CONST_SET with arity b. introduced by p1 has an arity b such that there is some Apply PI to u as determined in the previous step, producing Q’. MARKED +- MARKED U a. If CT’ is a generalization of {al,. . . , IT”}, then u t (T’ (else leave (+ unchanged). until p1 can no longer be applied to U. return ff. In ord,er to discuss the efficiency of Algorithm 1, we introduce the following no- tation: if u is a schema, the length of u, written ((~1, will denote the number of non-punctuation symbols in c~. Similarly, if 17 = (~1, . . . , T,} is a set of schemata, 1n7( will denote 1q I + jr21 +. . . + IT,, I (that is, the total number of non-punctuation symbols in II). The most difficult portion of the computation in Algorithm 1 is the comparison of two schemata ~1 and u2 to determine whether cq 5 ~2. Chandra and Merlin [ 91 show that the graph isomorphism problem is polynomially reducible to the problem of determining whether two sets of first-order atomic formulas are identical to within renaming of variables; the latter problem is trivially reducible to the problem of determining, for two ~2. This problem, in turn, reduces to the problem clauses ICI and ~2, whether ~1 x of comparing two schemata to determine whether one is a generalization of the other, ~2 and ~2 5 KI. Since no since for ~1, polynomial-time algorithm is known for graph isomorphism-Garey and Johnson [ 14, pp. 154-158 and 2851 conjecture that it belongs to a class of problems intermediate in difficulty between P and NP-complete problems-it is unlikely that a polynomial-time algorithm exists to determine in general whether one schema is a generalization of another. ~2 not containing q i, KI iff KI ~2 c 5 If we consider the search space itself, however, we find that Algorithm 1 is efficient in the number of schemata it examines, as the next theorem shows. Theorem 12. Algorithm 1 examines 0( j1713) schemata. Proof. For brevity of notation, let b = max{ bd( q ) , . . . , bd( r,) } and r = min{ru’( ~1) , . . . , r4qTn) I. We begin by examining the number of ways in which p1 can be applied to u. ( 1) A literal can be added to u in bd(u) * A ways, where A is the number of different arities occurring among the predicate-constants in II. (2) There are VI * (VI - 1) ways to unify individual-variables in u, where 6 is the number of individual-variables occurring in u. (3) There are no more than VP * (VP - 1) ways to unify predicate-variables in u, where Vp is the number of predicate-variables occurring in u. (4) There are no more than VP * C ways to replace a predicate-variable in u by a predicate-constant, where C is the number of predicate-constants occurring in 17. 18 N.L. lXham/Artijicial Intelligence 98 (1998) 1-47 (5) There are V, * F ways to replace an individual-variable in (+ by a most-general in 17. term, where F is the number of function curring for each IT which is at most is a generalization of n, Hence, of u examined symbols and individual-constants oc- the number of immediate successors bd(a)*A+b*(&-l)+V,*(Vp-l)+Vp*C+VI*F < joI* A + (v, + VP)* + Iv] * lITI+ ICI* III] 6 IcT[* A + 101~ + 21al* 1171. is derived from G( a, b, r) using pi, bd(cr) = b = bd(ii), By the way in which G(a, b, r) is defined, bd(ii) = b. Since u ii E pi ((T). Applying pt to a schema either leaves the length of the schema unchanged (in the cases of unifying variables and replacing variables by constants) or increases length of the schema of arity greater our previous the literals and replacing variables by terms this into there must be a program +? E n such that and the number of successors of (T is at most that 1~1 < (ii\ 6 Inl. Substituting than 0). Hence, we know the cases of adding formula, (in IL71 * A + lZ7]* + 2(171* 1171= (171* A + 3(L71*. By Property 7, the number of applications from G( a, b, r) is at most of pt needed to derive a least generalization m$C(I4)} - 5(G(a, b,r)) 6 2lnl. Multiplying A 6 (171, we see that the algorithm examines at most this by the bound on the number of applications of pt, and noting that (lZTl* + 3jLTl*) * 21171 = 81U13 successors of G(a, b, r), and thus examines 0( ln13) schemata in all. cl Algorithm 1 runs quickly in actual elapsed in 9 seconds, and the other least generalizations time as well: In the examples discussed of cube-root and reciprocal (the most difficult in Section 5 were in Section 5, the least generalization example) was found all found in less than 2 seconds each. 3.4. Summary: Finding least generalizations In Section 3, we have defined the concepts of degree of branching and degree of recursion and have described how they vary when yz and pz are applied. These observa- tions allowed us to construct an algorithm of a set of schemata then specializing G( a, b, r) until a least generalization the number of nodes examined by this algorithm input. in a two-step process of finding a generalization G(a, b, r) immediately, that in the length of the that is able to find a least generalization is found. We then showed is a polynomial N.L. lTnkham/Arti&ial Intelligence 98 (1998) 147 19 4. Finding programs Now t’hat we can find generalizations of programs, we turn to look at ways to use in program synthesis. The synthesis problem we are considering these generalizations is the following: Given a set of positive examples and a set of negative examples, find a program that covers all of the positive examples and none of the negative examples. Examples will be ground atoms; positive examples are ground atoms that the predicate should succeed on, and negative examples are ground atoms that the being synthesized the desired behavior predicate of a union program should not succeed on. We might, for instance, describe in this way: Pos:itive examples: union( [a, b, cl, [b, c, 4, [a, b, c, 4 > mid [a, bl, [cl, [a, b, cl > Negative examples: union( [a, b, cl, [b, c, 4, [a, b, cl 1 union([a,bl, [aI, [ I) then want to find a program and union( [a, b], [cl, [a, b,c]) that covers union( [a, b, c], [ 6, c, d], [a, b, or but not union( [a, b,c], [a, b,cl) [b,c,dl, We would c,d]) union([tz,bl, We want [al, 1 I). to model a situation be called as subroutines from new programs, a single new program described by some examples. We will assume, synthesi:s algorithms than utility predicates in which some programs are already known and can to synthesize that therefore, of predicate constants other represent are provided with Prolog definitions the predicate being synthesized. such as member and append.) these predicate constants in which we want (Most often, and 4.1. A search algorithm for finding programs Our first synthesis algorithm is Algorithm 2, which takes as input a schema u and a set of positive and negative examples and produces as output a program T that covers if such a n- E p2 * (a) exists. It all of the positive and none of the negative examples, of ~7 until one is found which covers the proper examples. An generates in this search covers oracle COVERS is used to determine whether a program produced a given example, of C0VER.S which is adequate is, in general, undecidable. is discussed (An implementation in Section 5.2.) for practical purposes since that question specializations Algoritlhm 2. Find a program a starting schema Input: to fit a set of positive and negative examples, given A schema ~0. A set of positive examples E+ = {eT, . . . , e$} and a set of negative examples E- ={e,,...,e;}. 20 N. L. Tinkham /ArtiJcial Intelligence 98 (I 998) l-47 Output: A program which covers all examples in E+ and none in E-. Oracle used by algorithm: COVERS(?r, e) returns Y if 7~ is a program which covers e, and returns N otherwise. Data structure: A queue of schemata, Q. Procedure: Q + [vol. while Q is not empty Remove If (T is not a program, from Q its first member CT. then add to the end of Q all members of pz(a) Else if u is a program and COVERS(a, e+) = Y for all e+ E E+ and COWRS(a, e-) = N for all e- E E-, then halt and return u Else if CT is a program and COVERS(a, e+) = Y for all e+ E E+ and COVERS(a, e-) = Y for some e- E E-, then add to the end of Q all members of p2 (a). Theorem 13. Let E+ be a set of positive examples, E- be a set of negative examples, and a0 be a schema. Algorithm 2 with input E+, E-, and a0 will halt and return a program 7~ covering all members of E+ and no members of E- , if such a IT E p2 * (~0) exists. Proof. Assume of Ef and none of E-. Any ancestor CY of v either which covers all members of E+, so if LY is not a goal program and is selected Q, all immediate descendants yet been selected Because of this, if the algorithm halts, it will halt because that is, it will not halt because of exhausting Q without that there exists a program 7~ E p2 * (~0) which covers all members is not a program or is a program from if no goal program has in Q an ancestor of T (possibly T itself). it has found a goal program; of LY will be added to Q. Hence, from Q, then there exists finding a goal. Since the algorithm examines, in order of increasing n, the members of & (a~), and for any particular value of n is finite, there are a finite number of schemata since pz (~0) which can be generated before rr by this systematic application of ~2. Hence, either the algorithm will halt before finding T by selecting a goal program in the computation, halts and a program covering all members of E+ and none of E- or it will halt upon selecting T from Q. In either case, the computation is found. q from Q earlier 4.2. Schema hierarchy In examining Prolog programs, we find that certain structural patterns recur: a program may consist of a recursive clause together with a base case, for instance, or it may contain clauses which select one of several actions based on the a collection of non-recursive N.L. Gkham/Art@cial Intelligence 98 (1998) 1-47 21 (say, deciding that the program the mental actions of selecting the truth of a. condition. A programmer may go through looping and hence basic fonm of a program requires a recursive clause and a non-recursive this form further as details of the required program behavior become clearer (for instance, deciding list). that the recursion will be on the tail of a list and that the base case is the empty Some of these structural patterns are described by schemata, with very general patterns being described by very general schemata and more specific patterns by more specific schemata. and then refine base clause) involves language, in a particular The generalization schemata others. Such a graph can be seen as classifying programs according which they are instances y2 imposes a hierarchical Very general schemata such as operator yz allows us to draw a directed acyclic graph of the than to the schemata of and as grouping programs as being relatively alike or dislike. language. structure on the entirety of a given schema-definition indicating which schemata are more general {P(-,-,-), P(-,-,-), P(-,-,-), P(-,-,-)I appear high up in the hierarchy. More specific schemata such as P( [XlZll, [XIZ21, IXIZ31) :- {P(Zl, Z2,23)}, P(WIZ41, [W(ZSl, tY/Z61) :- {P(Z4,Z5,Z6)}, P( iUIZ71, [V(ZSl, [VIZ91 1 :- {P(Z7,-% Z%}} occur lower in the hierarchy, programs such as I, 1 I, [ I), {awl am~([l~Zl],[l~Z2],[1/Z3]):-{and(Zl,Z2,23)}, amd( [0124], [WIZSI, [OlZ6]) :- {and(Z4,Z5,26)}, und( [ 11271, [OIZS], [OjZS]) :- {und(Z7,ZS,Z9)}} occur lower still, and programs consisting entirely of ground clauses such as appear nfear the bottom. 22 N.L. Tinkham /Artificial Intelligence 98 (1998) 1-47 1 P(X>X) ) I P(a,Y) I ( ww ) lPW.Y)l 0 I P(4Z).Y) 1 I PW.YZ)) 1 ( P(X,Y) :- c!W.Z) I Fig. 1. There are two types of hierarchical the complete graph containing definition language, with each arc representing levels of the complete graph for the language containing and a predicate constant p2, are shown in Fig. 1. as nodes all the schemata expressible graphs we can construct using ~2. The first is in a given schema- a single application of 79. The first three a, f’, a constant a function A second type is obtained by selecting nodes from the complete graph and allowing of yz. These nodes may be selected according groupings of pro- factor), schemata schemata which represent “important” (giving a graph of a desired depth or branching arcs to represent one or more applications to aesthetic criteria grams), computational or for other reasons. This gives us a relatively according to similarity, using a generalization small graph which classifies (selecting criteria relation. patterns labelled bl for binary predicates We see some of the more significant term produces b3, and adding a second in Fig. 2. The is the most general 4-clause schema of arity 2. b2 is more specific a variable by recursive call in the recursive programs and (pre-order, recursion on the tail of a list, and two special cases of this are list as the base. 69 schema than 61 because one of its clauses contains a most-general clause produces b4, a schema which has as specializations such as flatten and some leaf&t). b7, with the empty is a specialization list-traversal list as a base case, and b6, with a singleton two recursive clauses, and b8 describes a recursive call. Replacing in-order, post-order, of b7, containing doubly-recursive b5 represents tree-traversal two-clause programs. programs Fig. 3 shows a graph of schemata schema, ~1, appears at the top. ~2, a schema describing for predicates of arity 3. The most general 4-clause selection of one of two input N.L. lhkham /Artificial Intelligence 98 (1998) l-47 23 7 \ leaf_ El list Fig. 2. Hierarchical graph for selected schemata of arity 2. The full definitions of the programs and schemata in this diagram appear in Appendix B. of max and min. ~3, a schema containing the programs menus, int-div, and mod, and schemata a clause with two recursive calls. ~5, a schema of ~7, in which recursion recursion on the tail of a list, is a generalization of s9, with recursion on three lists. s5 of ~6, in which one of the input parameters appears both on the based on a test, is a generalization parameters a recursive call, has as descendants s4 and $5. ~4 is a schema containing representing occurs o-n two lists; s7 is, in turn, a generalization is also a generalization left-hand1 side and the right-hand and replacement turn, from ~6. The diagram than adjacent or and. of individual-variables illustrates side of the clause. Unification by most-general that intersection is more like deleteall than terms produces s8 and ~10, in in form it does of individual-variables it is like append, and intersection resembles append more closely 4.3. Using a schema hierarchy for program synthesis A schema-hierarchy of schemata a collection schema we retain enough generality from which graph makes synthesizing to begin a new program easier, because it gives a very general for the root of the graph, in the search to ensure finding a solution. But by including the search. By choosing (such as {Cl} or {P(_._,_),P(_,_,_),P(_,_,_)}) 24 N. L. Flinkhatn /Arh’jicial Intelligence 98 (1998) 1-47 Fig. 3. Hierarchical graph for selected schemata of arity 3. The full definitions of the programs and schemata in this diagram appear in Appendix C. to be useful program generalizations. A well-selected in the graph, we concentrate the search around patterns set of schemata will thus other, more specific schemata known greatly narrow the search. Schema-hierarchy graphs are directed acyclic graphs consisting of nodes representing one or more applications of p2. While a schema-hierarchy a tree, we will consider ones that are sufficiently schemata and arcs representing graph is not necessarily as to contain or more nodes of out-degree 0, which we will call leaves. Conceivably, contain all the nodes but more often to be “interesting”. so exactly one node of in-degree 0, which we will call the root, and one a graph could the root and the set of leaves, it will contain a small subset of these nodes which have been deemed in generality between intermediate tree-like Since the full power of p2 can produce lengthy but relatively uninformative (e.g., addition of a number of literals of refinements of K), we will restrict our attention correspond have the same degree of branching. We will further (+I and (+2 in a graph I’, if ~1 is a generalization (+2 in r. to one or more applications of p1 only-hence, in Sections 4.3.1 and 4.3.2 sequences to a clause K, followed by deletion to graphs whose arcs to graphs all of whose nodes that for any to the condition of ~2. then there is a path from ul impose N.L. linkhant/Artijcial Intelligence 98 (1998) 147 25 We will also need a definition for speaking about generalizations in a graph. Definition. Let 2 be a set of schemata and r be a hierarchical graph of schemata. schema or in r such that u’ is a generalization If a 19 of (+ in r of 2, then we say that u is a least generalization of 2 of 2, and if there is no specialization is a generalization relative to r. is the graph Example. union and append, and xor relative If r intersection relative to r. in Fig. 3, then schema ~10 is the least generalization and s5 is the least generalization of of adjacent, to r, 4.3.1. Expanding a schema hierarchy In order to construct and maintain graphs with the properties we have described, we to the graphs. To state this formally: graph r which does not contain a, we want that for any nodes (~1 and (~2 in r, if ut to is a need to be able to add new programs and schemata given a schema u and a hierarchical insert (T into r, preserving generalization the property of (~2 then there is a path from ut below allows a set of schemata to add a group of schemata at once is useful The algorithm graph. The ability several new programs which are known to have grouped under a parent node representing to be closely to (~2 in r. to be added to a schema-hierarchy if, for instance, we have like related and which we would their least generalization. a new root for the graph: the more general of the two begins by finding a least generalization LG of the set of new schemata. to the graph, with the new schemata as LG’s children. We may have to of LG or vice versa, if the root is a generalization a least is the root of the new graph; otherwise, schemata, and in the graph and adds the appropriate calls addschema, which finds all the relative of the new schema of LG and the root is added as the new root. To add individual least generalizations specializations addschemaset LG is added establish then generalization addschemaset most general arcs to the graph. Algorithm 3. Add a set of new schemata Input: to a schema-hierarchy graph A schema-hierarchy A set ‘of schemata SchemaSet = {CT],. . . , u,,}, none of which are in Graph graph Graph, rooted at Root output: A schema-hierarchy nod’es of Graph Procedure: graph New-Graph which contains ut, . . . , u,, and all the % addschemaset( Root, SchemaSet, New-Graph) : % Add the schemata in SchemaSet to the graph rooted at Root, yielding New-Graph is a singleton if SchemaSet add ut return resulting graph as New-Graph set {at}, to Graph, using addschema then else 26 N.L. Tinkhatn/Art~~cial Intelligence 98 (1998) I-47 compute one least generalization LG of SchemaSet if LG is a specialization of Root then add LG to Graph, using addschema add each member of SchemaSet to Graph, using addschema else split SchemaSet into 21 = the set of schemata _I& = the set of schemata in SchemaSet which are specializations in Schema-Set which are not specializations of Root of Root add to Graph all members of 21, using addschema individually compute a least generalization LGz of & compute a least generalization NewRoot of {Root, LG2) add NewRoot if NewRoot to Graph as the parent of Root to within renaming of variables is identical to LGz then add the schemata in & to Graph as the children of NewRoot else add LG;! to the graph as a child of NewRoot add the schemata in 22 to Graph as children of LGz return resulting graph as New-Graph % addschema( Root, Schema, New-Graph) : % Add Schema to the graph rooted at Root, yielding New-Graph add the node Schema to the set of nodes in Graph find-relative_lgs(Root, Schema, ListG) for each o E L&G establish arc from (+ to Schema in Graph jndspecializations(Root, for each u E Lists Schema, Lists) establish arc from Schema to (+ in Graph then if Root and Schema are incomparable, compute a least generalization LG of {Root,Schema} add LG to Graph as the parent of Root and Schema return resulting graph as New-Graph % jind-relative_lgs( Root, Schema, List) : % Find all least generalizations % put these in List. of Schema relative if Root is a generalization of Schema, then to the graph rooted at Root; IL.1 +- r 1 for every child C of Root jnd_relative_lgs(C, Schema, L2) append L2 to Ll if Ll = [ ] then List t else List c Ll [Root] else List + [ ] N.L. Tin!&mn/Artificial Intelligence 98 (1998) 1-47 21 % jindspecializations( Root, Schema, List) : % Find all most general specializations % put these in List. of Schema relative if ROOI’ is a specialization of Schema, then to the graph rooted at Root; List +- [Root] else List + [ ] for (every child C of Root jirzdspecializations( C, Schema, L2) append L2 to List remove from new List all schemata appearing in List 4.3.2. Program synthesis that have an ancestor from the graph Algorithm 2 used breadth-first search from a single starting schema to find a program to fit a set of positive and negative examples. Algorithm 4 uses an alternative approach: it uses a it begins with a schema-hierarchy bounded depth-first to keep the search focused. As in Algorithm 2, we assume the existence of an oracle COVERS( T, e), which, for a given program 7~ and example e, returns Y if T covers e and N otherwise. than a single schema, and search rather graph Algorithm 4. Find a program a hierarchical graph of schemata. Input: to fit a set of positive and negative examples, using graph of schemata, Graph, rooted at Root A hierarchical A set of positive examples, E+, and a set of negative examples E- A nonnegative integer D output: A program within D steps of some node in Graph which covers all examples and none if such a program exists. in E-, in E+ Data structure: A stack of (schema, depth) pairs, S Procedure: for each Node in Graph, S +- [(Node, O)] while S is not empty Pop from S its top member if D, < D then (v, 0,) if (T is not a program, for every schema r_# in p2 (a), push ((+‘, D, + 1) onto S then else if g is a program and COVERS(a, ef ) = Y for all e+ E E+ and COVERS(u, e-) = N for all e- E E-, then 28 N.L. Tinkham/Art$cial Intelligence 98 (1998) 147 halt and return CT else if u is a program and COVERS(a, COVERS((r, e+) = Y for all e+ E Ef and e-) = Y for some e+ E E-, then for every schema U’ in p2 (g), push (u’, D, + 1) onto S 5. A system for learning and using schemata This section describes a system implementing the main ideas of the preceding sections, inference and program synthesis as computed by this illustrated by examples of schema system. 5.1. Description of the system The system writes its output is composed of three modules, each of which to one or more files: the modules communicate takes its input from and by means of these files. (a) Find least generalization Input: A set of programs 17 = (7~1,. . . , rTT,} Output: A least generalization (+ of IZ (b) Add program set Input: A schema-hierarchy graph rt A set of programs 17 = (~1,. . . , VT”}, none of which is in rt Output: A new schema-hierarchy graph r2 which contains ~1, . . . , n-, (c) Find program Input: A set of positive examples E+ and a set of negative examples E- A schema-hierarchy graph r A nonnegative integer D output: A program which covers all of the examples in E+ and none of the examples in E- (if such a program exists) N.L. lb&ham/Artificial Intelligence 98 (1998) I-47 29 structured The system makes the schemata could be grouped long-term use of one or more files of schemata, into a graph. In general, all known schemata defined with a given alphabet could into a number or type of a useful graphs have the of ~2, then there to hierarchical be stored in a single file; more practically, of different files according problem, with a human user selecting schema. As in Section 4.3, we will assume property is a path from ~1 to (+2 in the graph. The algorithm used for creating and adding this property. schema-hierarchy The learning that all schema-hierarchy that for any go and (~2 in the graph, if ~1 is a generalization to such features as arity, degree of branching, cycle consists of repeatedly adding new programs to the files of known the file that is most graphs maintains to contain likely schemata, in one of two ways: ( 1) The system may be told directly about a program or a set of related programs. The new programs are added to the graph with the add program set module. (2) The system may be asked to produce a program for a set of positive and negative is used. Once the new program graph with add In this case, the jind program module found, to the original hierarchical it can be added examples. has been program set. 5.2. Implementation All thr’ee modules have been implemented in SICStus Prolog and run on a Silicon Graphics Indy. The jind least generalization module is an implementation of Algorithm 1. The add program set module uses Algorithm 3 to add a set of new programs to the graph. The jnd program module implements Algorithm 4. It takes as input a set of positive a graph r, and an integer D, and it returns a examples., a set of negative examples, program $7 within D pz-steps of some node in r such that v covers all positive examples and no negative examples. Note that the special case of D = 0 asks us to find a r among the known schemata for a program Since that the system has not seen before. the problem of determining whether a program covers a particular example In general, however, with D > 0, we will be looking in r. for a predetermined in genera& undecidable, runs the program has succe.eded, does not !halt on one of the examples example) will be treated as being candidate programs. (finitely) is, the oracle COVERS( vr, e) is approximated by a predicate which number of steps and then reports that the program that fail on any positive future too general, and will be refined to halt in the allotted it does not finitely time. A program failed, or failed to produce (assuming 5.3. A sample problem As an illustration, we will look at the generalization problems as performed by the system. list-processing We start with the two programs double: {double([ I, [ I>, double( [HITl], [H, HIT2]) :- {double(Tl,T2)}} and synthesis of a sequence of 30 N.L. rinkham/Arti$cial Intelligence 98 (1998) I-47 and double-last: {double_last( [Xl, [X, Xl ) . double-last( [HITl], [HIT2]) :- {\==(Tl, [ ]),doubleJast(Tl,T2)}} and ask for a least generalization. The system will find the schema {P( [WI 7 [q-l > :- {pm -)I* From this we can form the graph I P([H ITl,[HIl) .- I P(T_1 I. Pi-J1 Suppose we then want to synthesize a program for finding prefixes of lists. Taking as input the positive examples yrqW[a,bl, [a,b,cl), pqW[al, [a,hcl), prejW[ I, [a,b,cl), prGW[a9b,cl, [a,b,cl), prefuc([cl, [cl>, prefi([bl, ihal), the negative examples prqW[b,cl, [a,b,cl), pre@([cl, [a,b,cl>, pvW[a,b,cl, [a,bl>, and our first graph, with a search depth of 5, the system finds the program (PreJix([ Iv->, pWW [HIT1 1, [fW21> :- (pr@-Wl, T2))) We can then add pre& to our graph, producing the new graph 1 I P(lH I WH I _1) :- I P(L) la I Supposing that we are now given the program reverse: {reverse( [ I, [ I>, reverse( [HIT], L) :- { reverse( T, TR) , append( TR, [H] , L)}}, N.L. Tinkhatn/Artifcial Intelligence 98 (1998) 1-47 31 we can add reverse to our graph, producing I W-I TL-) :- I P(T.2 I. PL) I 1 I P([H I THH I _I) .- 1 P(L) 1. P(L) t We can use this new graph to synthesize a program for finding suffixes of a list, using positive e:xamples &w[~,hcl9 [b,cl), qYw[4b,cl, [cl>, sufJix([a,b,cl, [ I), wF~([~,~,cl, [a,hcl), ~UfJix([Cl, [cl), Mw[b,al, [al), negative examples &w[Ghcl, [abl), sufJix([a,b,cl, [bl), su.([a,bl, [u,b,cl), and a depth of 3; the system will find the program {SU@..x( X, X) 9 w@-e [-IT1 9 y> :- {&MT, Y>}} When the: suffix program is added to the graph, our final graph becomes 1 ( FTL I Tl3 :- I P(T.J I. I 5.4. Numeric examples This system is not designed with numeric predicates in which in mind, because numeric com- in Prolog in literals appear. For example, the pair of literals X is 2 + 2, X > 0 will succeed, but in the reverse order instantiated putation Prolog, (X > 0, X is 2 + 2) they will cause a run-time error if X is not already to the order is sensitive 32 N.L. TXham/Artificial Intelligence 98 (1998) l-47 to a value from prior context. Since our formalism treats all programs and schemata as order-independent, it cannot guarantee that output programs will contain literals that are ordered properly for a Prolog compiler. However, some numeric programs can be synthesized. 5.4.1. Numeric list examples For a simple example, the predicates % sumsquares(List, Ssq) : Ssq is the sum of the squares of the elements in List {sumsquares( [ I, 0) , sumsquares( [HIT], Ssq) :- { sumsquares(K TS) , Ssq is TS + H * H}) and % prodlist(List, Product): Product is the product of the elements in List (prodlist( [ I, 1), prod_list( [HIT], Product) :- (prod_list( T, TProduct) , Product is TProduct * H}} have the least generalization {P([ I,-), P( [-IT], X) :- {P(T, _), X is _ }} The system constructs the graph From this new schema, predicates sum_list and list-length can be synthesized. Using positive examples sum_list([1,2,3,4],10), sum_list([10,5],15), sum_list( [ 11, 1) , sumlist( [ ] , 0) , and negative examples sum-list( [ 1,2,3,4], ll), sumJist( [ 10,5], 10). sum_list( [ l],O)]), the system synthesizes the program {sum_list( [ ] ,0), sum_list( [HIT], X) :- {sum_list( T, Y), X is H + Y}} N.L. lMham/Art@cial Intelligence 98 (1998) 1-47 33 Starting from the same schema, and using positive examples list_r!ength([l,2,3,41,4), Zist_length([10,5],2), listi!engrh ( [ 1 ] , 1) , list_length( [ ] , 0) , and negative examples listi!engfh([l,2,3,4],3), list_length([lO,5],10), list_i!engrh( [ 10,5], 5), list_length( [ 1 ] , 0) , the system synthesizes the program {Zisl_Length( [ ] , 0) , list_Zength( [-IT], X) :- {Zist_Zength( T, Y) , X is Y + 1)) The resul.ting graph is 5.4.2. Successive approximation example A third example begins with a pair of predicates, cube-root and reciprocal, which are based on a successive approximation algorithm described in [ 121. The cube-root program takes three parameters: l N, tlhe number we are taking the cube root of; l R, Ann uninstantiated parameter which will hold the root at the end of the calculation; l Range, a number indicating the desired precision in the answer. The root found will be such that N is between R3 and (R + Range)3. For example, cube_root(8,0.001, R) will find an R between 2.0 and 2.001. The program proceeds by doubling Range in each recursive call until Range > N; thus, in the recursive calls, we have the sequence Range, 2 * Range, 4 * Range, 8 * Range, . . . . As execution backs out of the recursion, numbers in the sequence are either added to an accumulating sum or not, depending on whether the cube of the sum is less than or equal to !I. For cube_root( 3,O.l) R), for example, the recursive calls are cube_root( 3,0.1, R) , cube_root( 3,0.2, R) , cube_root( 3,0.4, R) , cube-root( 3,3.2, R). The root found in this case will be 1.4 = 0.8 + 0.4 + 0.2. Note that 1.43 < 3 < ( 1.4 + 0.1) 3, so that 1.4 satisfies the precision requirements for the root. 34 N.L. Tinkham/Artij?cial Intelligence 98 (1998) l-47 This algorithm can also be used to find integer cube roots (that is, the largest integer less than or equal to Ne3) by using a range value of 1. cube_root( 9,1, R), for example, finds the integer root 2. % cube._root( N, Range, R) % Find R = cube root of N; more specifically, % R3 < N < (R+Range)3 (thus (cube_root( N) - RI < Range). % % Call with N > 0, Range = desired precision % R uninstantiated, initially. find R such that interval (1 for ints), {cube-root(Nl,Rangel,O) :- (Range1 > Nl}, cube-root( N, Range, R) :- {Range < N, Range2 = Range * 2, cube_root( N, Range2, R2), Square = (R2 + Range) * (R2 + Range) * (R2 + Range), inc-amount( Square 6 N, Range, Inc) , R = R2 + kc)) The reciprocal program uses the same approach as cube-root: % reciprocal( N, Range, R) YO Find R = reciprocal of N; more specifically, find R such that % l/R 6 N < l/(R + Range) (thus Ireciprocal( N) - R( < Range). % % Call with N > 0, Range = desired precision % R uninstantiated, initially. interval ( 1 for ints), {reciprocal( Nl, Rangel, 0) :- (Range1 > Nl}, reciprocal( N, Range, R) :- {Range < N, Range2 = Range * 2, reciprocal( N, Range2, R2), Product = N * (R2 + Range), incamount( Product < 1, Range, Znc) , R = R2 + Znc}} (The utility predicate inc-amount, called by both cube-root and reciprocal, is: % inc-amount( +Condition, +Quantity; --Increment) % Increment is Quantity if Condition is true, 0 if Condition is false. inc_amount( Condition, Quantity, Quantity) :- Condition. incamount( Condition, _, 0) :- \+Condition. ) The system finds the least generalization N.L. linkham/Artijcial Intelligence 98 (1998) l-47 35 {P(Nl,Rangel,O) :- (Range1 > Nl}, P( N, Range, R) :- (Range f N, Range2 = Range * 2, P(N,Range2,R2), X=_* (R2+Range), incamount( X < _, Range, Inc) , R = R2 + kc}} which caiptures the essence of the successive approximation algorithm used by both programsS. A sqaare root program can now be synthesized from this schema. Using positive examples sq_root( 9,l) 3)) sq_root( 25,l) 5), sq_root( 1, 1, l), sq_root(O, 0,O) , sq_root( 100, 1, lo), and negative examples sq_root(9,1,0), sq_root(9,1, l), sq_root(9,1,2), sq_root(9,1,4), the system finds the program {sq_root( Nl, Rangel, 0) :- (Range1 > Nl}, sq_root( N, Range, R) :- {Range < N, Range2 = Range * 2, sq_root( N, Range2, R2), X= (Range+R2)*(R2+Range), in!camount( X < N, Range, Y) , R = R2 + Y}} The resulting schema graph is 6. Conchsions The enumerative search approach to automatic programming carries both benefits and liabilities,. The chief benefit of enumerative search is the thoroughness with which it covers an easily-described space, allowing us to prove theorems about the circumstances under which it is, and is not, guaranteed to locate a target successfully. This thoroughness 36 N.L. Gkham/Art$kial Intelligence 98 (1998) 1-47 is also the main weakness of the approach, an exponentially narrowing avoid having use of schemata examine as a starting point the search, to examine large number of possibilities. We have discussed ( 1) the use of upward and downward the entire space of syntactically the entire space generated by the refinement operators. for the search for a program, refinement operators, correct programs, and (2) to avoid having in that enumerative search generally examines two mechanisms for to the to for finding an algorithm (or schemata) least generalizations It has also described This paper has described of programs a system which stores a collection of these least gen- and uses them as a starting point for program synthesis. By efficiently. eralizations starting with a particular schema, specifications much more quickly provided by the schema. As the system derives more and more programs over its life- the efficient time, synthesis of an increasingly to fit the input/output than it could have if it did not have the information it can add more and more schemata the system can derive a program broad range of programs. to its collection, thus enabling Acknowledgements This paper summarizes my advisor, Alan Biermann, David Plaisted, Bilge Karal Say, Joseph Shoenfield, helpful comments on this research and on drafts of this paper. to and to Pierre Flener, Donald Loveland, Gopalan Nadathur, for their research done at Duke University. and anonymous I am grateful reviewers doctoral Appendix A. Glossary Numbers in parentheses indicate the section in which the term is introduced. ~1: An upward refinement operator. refinement operator. ~2: An upward p1 : A downward ~2: A downward x: For schemata refinement operator. refinement operator. (+I and (~2, (~1 M (+z exactly (2.2) (2.2) (2.2) (2.2) the naming of variables possibly, clause and clauses within a schema. (2.2) (2.2) 5: A partial order on schemata. Ci (a) : The set of goals covered by schema u in interpretation [(a): The number of non-punctuation in a-the symbols and the order of listing negative I. (2.2) number of distinct variables if 01 and az are identical for, except literals within a in c+ the number of literals in u. (2.3) replacing all occurrences of variable V by term t. (2.2) { V\t}: A substitution \==: A built-in Prolog predicate. X \== Y if X and Y are not identical. U: As a multiset operator, {a, b, b} U {a, a, a, c} = {a,~, a, b, b, c}. fl: As a multiset operator, {a, a, b, b} II {a, a, a, c} = {a, a}. 5: As a multiset operator, {a, b, b} c {a, b, b,c}, and {a, b, b} s {a, b, b}, but {a, b, b} $ {a, b, c}. N.L. linkham/Artijcial Intelligence 98 (1998) 1-47 31 C: As a multiset operator, {a,b,b} c {a,b,b,c}, but {a,b,b} @ {a,b,b}, and {a, b, El} $?- {a, b, c}. +: As a multiset operator, {a, b, b} + {a, a, a, c} = {a, a, a, a, b, b, c}. -: As a multiset operator, {a, a, a, a, b, b, c} - (a, b, b} = (a, a, a, c}. bd(a): The degree of branching of schema (+. (3.2) Constant set: A finite set of functions symbols, individual-constants, and predicate con- stants. (2.1) Degree of branching: The number of clauses in a schema. (3.2) Degree @recursion: If (T is a schema such that every positive literal in CT contains the (variable or constant) predicate symbol q5, then the degree of recursion of u is the maximum of {n 1 there is a clause K in g whose negative literals contain exactly n occurrences of ~$1. (3.2) Downward reJinement operator: Given an ordering relation < on expressions, an operator p is a downward refinement operator if p is a recursively enumerable relation on expressions such that p* is <-‘. (2.2) Generalization of a schema: Schema ul is a generalization of schema (+2 if there is a schema (T’ such that (~1 cz 0’ and u’ E 79 * (~2). (2.2) Generalization of a set: A schema CT is a generalization of a set of schemata n if (+ is a gene:ralization of every schema in n. (2.2) Least generalization: A schema CT is a least generalization of a set of schemata 17 if g is a generalization of II and there is no schema (+I such that (+’ is a specialization of (+ and such that CT’ is a generalization of J7. Multiset: A collection of objects in which repetition is significant, but, as in a set, order is not significant. See also the definitions for U, fl, s, C, +, and -, and [22]. (2) Program: A schema containing no predicate-variables and not containing Cl. (2.1) rd( a) : The degree of recursion of schema g. (3.2) Rejinement operator: See upward refinement operator, downward rejinement operator. Schema: A multiset of clauses such that there is only one predicate symbol appearing in the positive literals of the clauses. (2.1) Schema-deJnition language over (a constant set) K: If A is the set of all integers a such that there is a predicate constant in K of arity a, then L is the schema-definition langua,ge over K if L is the set of all schemata c such that every function symbol, individual-constant, or predicate-constant s occurring in fl is an element of K, and every predicate variable P occurring in (T has an arity ap such that ap E A. (2.1) Specialivztion of a schema: Schema 01 is a specialization of schema u2 if there is a schema CT’ such that UI x u’ and u’ E p2 * (~2). (2.2) Upward rejnement operator: Given an ordering relation < on expressions, an operator y is an upward refinement operator if y is a recursively enumerable relation on expressions such that y* is <. (2.2) Appendix B. Schemata of arity 2 Listed below are the schemata and programs mentioned in Fig. 2. To make the programs easier to read, a few lines of comments are included. The symbols “+“, 38 1‘ 9, - N.L. Tinkham/Artifcial Intelligence 98 (1998) l-47 and “?’ preceding parameter names in a comment and a parameter and variable names are used for readability, indicate, that can be either. respectively, an input In many places than adhering rather para’meter, an output parameter, mnemonic constant to the single-letter strictly names used elsewhere. PC-7-l) % b5 {P(-,-), P([-IV,-) PC-,-), PC-9-I) :- {PK.-)}, % 66 {P([-Iv-), PC [-IT1 1-J :- {fYT, -)}} % 61 I,-)* {P([ PC [-ITI, -1 :- {P(T, -I}, PC-,-), PC-v-)) % b8 IT->, {P([ P([.pl,-> :-{P(T,-1)) % bl {P(-,-)v P(-,-)> P(-,-), P(-*-)I % b2 {P(-, P(-, P(-, P(-,-)I -), -) :- {P(-, -), -)}, % b3 {P(-,-), P( I-I-1) P(-,-), P(-3 -)I :- {P(-, -)}, % b4 {P(-, -), P(-,-) P(-,-), :- {P(-, -), P(-, -)}, % 69 {P([ 19-1, P([- Tll,-) P([- T21,-) :- {Pm,-)}} :- {P(TI,J}, -Lisd) the first, third, % alternate( +Listl, % List2 contains % For example, afternate( % Adapted (alternat aZternate( [X, _ITl] , [ XIT2] ) :- {alternate( Tl, T2))) from [33, p.2661. [ I, [ I ), [a, b, c, d, e] , [a, c, e] ). . . ., elements of Listl. % member( ?L, ?X): X is a member of list L. {member( [Xl-l, X) , member( [-IT], X) :- {member( T, X)}} % sufJix( ?S, ?L) : S is a suffix of list L. N.L. Tinkham/Artifcial Intelligence 98 (1998) 1-47 39 {w7w L.. L) , =mx( [-ITI, L) :- {~Qw”? L)}} the last element % double-last( +Listl, -Lisn) % Double % E.g.: a’ouble_last( [a, b,c], [a, b,c, I). {double_iast( [Xl, [X, X] ), double-Iast( [ HIT1 I, [ HIT21 > :- in Listl, producing Lisn. {Tl \=== [ 1, doubleJast( Tl, Z’2))) -Lisa) % intersect_list(+LLl, % List2 is the intersection { interseci!-list( [X] , X) , interseckZist( [HIT], L) :- of all the lists in LLI. {intersect_Zist(T, Ll), intersect(H, Ll, L)}} % telescope( +Listl, -Lisn) % For example, % Adapted {~elescope( [ I, [ 11, from [ 33, pp. 264 and 2661. teZescope( [a, b, c, d], [a, b, c, d, b, c, d, c, d, d]). telescope( [HIT], L) :- {telescope(T, L2), append( [HIT], L2, L)}} % listthm( +Listl, -List2) % Convert a list of elements % For example, % Adapted {listthru([ I, [ I), listth( [HITl], from [[HI to a list of singleton lists. listthru( [a, b, c] , [ [a], [ 33, p. 2661. [b] , [c] ] ). IT2]) :- {listthru(Tl,T2)}} % double( +Listl, -List2) % Double each element % E.g.: Llouble( [a, b,c], [a,a, b, b,c,cl). {double([ 1, [IIt double( [ HIT1 1, [H, HIT21 ) :- {doubZe(TI, T2))) in Listl, producing List2. % reverse( +Listl, -List2) % List2 k List1 reversed. {reveN [ I, 1 I), reverse( [HIT], L) :- { reverse(T,TR),append(TR, [H],L)}} % pre@( -P, +L): P is a prefix of list L. {pre@( [ 1 v -> , prefi([Vll, IHIT2l):-(pre~(Tl,T2)}) % complement(+Vl, -V2). 40 N.L. i’Mham/Artificial Intelligence 98 (1998) 1-47 % V2 is the complement I I, 1 I>, {complement( of Vl (bitwise not). compZement([O]Tl], compZement( [ 1 ]T3], [ O]T4] ) :- {complement( T3, T4))) :- {complement(Tl,T2)}, [l]T2]) % removeduplicates( +Listl % Remove all duplicate elements { removeduplicates( [ 1, [ ] ) , removeduplicates( [HIT], L) :- , List2) in Listl, producing List2. {member( H, T), removeduplicates(T, L)}, remoue_duplicates( [ Hl (Tl] , [ Hl IT21 ) :- {nonmember( Hl, Tl) , removeduplicates( Tl, T2))) % even_elts( +Listl, -EvenList) % EvenList contains the even elements of Listl. % For example, even_eZts([4,5,-3,0,2,-l], {even-cW even_elts( [HlITl], [ I, [ I>, [HllT2]) :- (0 is HI mod2,even_elts(Tl,T2)}, [ H2lT3] L) :- even-&( [4,0,2]). { 1 is H2 mod 2, even_elts(T3, L)}} the positive elements of Listl. , -PosList) % positive_elts( +Listl % PosList contains %Forexample,positive_elts([4,5,-3,0,2,-1],[4,5,2]). (positive_eZts( [ 1, [ ] ) , positive_elts( [ Hl (Tl] , [ Hl IT21 ) :- {Hl > O,positive_elts(Tl,T2)}, positive_eZts( [ H2)T3], L) :- (H2 =< O,positive_eZts(T3, L)}} % fiatten( +LL, -FlatList) % Flatten list LL to produce FlatList. @atten([ I, 1 I>, JIatten( [ Hl IT11 , [ Hl IT21 ) :- {atom(Hl),jlatten(Tl,T2)} Jiatten( [ [ A(B] IT3], L) :- Cflatten([AIBl, Ll)&tten(T3, L2),append(Ll, L2, L)}} : Quicksort. I, [ I), % qsort( +Unsorted, -Sorted) {q-rort([ qsort( [ PivotIT], Sorted) :- {spUt( T, Pivot, Ll, L2), qsort( Ll , Sorted1 ) , N.L. lMham/Artijcial Intelligence 98 (1998) l-47 41 qsort( L2, Sorted2), append( Sorted1 , [ PivotjSorted21, Sorted) }} % pre_order( +Tree, -Nodes) % Nodes is a pre-order list of the nodes in Tree. {pre_order( nil, [ ] ) , pre_order( tree( Node, Left, Right), [ NodeIT] ) :- {prelwder( Left, LL) , pre_order( Right, RL) , uppen LL, RL, 7’))) % in_ordw( +Tree, -Nodes) % Nodes is an in-order list of the nodes in Tree. {inm-der( nil, [ ] ) . in_order( tree( Node, Left, Right), List) :- { in_order( Left, LL) , in_order( Right, RL) , append([Nodel,RL,Ll), append( LL, Ll, List)}} % post_olpder( +Tree, -Nodes) % Nodes is a post-order list of the nodes in Tree. post-order( nil, [ ] ) , post-order( tree(Node, Left, Right), List) :- {poskxder( L.efr, LL) , post_order( Right, RL) , uppen:d( RL, [Node] , Ll ) , appen:d( LL, Ll, List)}} % leaf-l&( +Tree, -List) % List is the list of leaves in Tree, leuf-list( nil, [ ] ) , leafAist( tree( _, Left, Right), List) :- {lecffJist( Lefi, LL) , leafJist(Right, RL) , append( LL, RL, List)}} Appendix C. Schemata of arity 3 Listed below are the schemata and programs mentioned in Fig. 3. As in Appendix B, some comments and mnemonic variable names are used. N.L. lTnkham/Artifcial Intelligence 98 (I 998) l-47 {PC-,-,-), P(-3-9-1 :- {P(->-,-),P(-,-,-)}, P(-,-,-I, PC-,-,-I} % s5 {PC-,-,-1, P( [-IT], -, -1 :- {PC PC-,-,-), P(-, -, 4) -, -I}, % s6 {P([ I,-,-), P([-ITl,X,-1 PC-,-,-), PC-, -I 4) :- {PKX,-)}, 42 % sl {PC-,-,J, PC-,-,-), P(-,-,-I, PC-,-,-I} % s2 {P(W X, W) :- {Q<K XI}, P(PZ, Z) :- {R(KZ)}} % s3 {P(-7-,-), PC-,-,-) :- {PC-,-,-)}, PC-v-,-), PC-v-9 -1) % s4 % s7 {P([l>[l,[l)> P([-(XII, P([-IYll, P([-[Zll, [4X21,-) [-IY21,J r-lz21,-) :- {P(Xl,X2,-)}, :- {P(YLY2,-I}, :- {P(Zl,Z2,-)}} % s8 {P([ I,-,-), P([-ITll,X,-) PC [-IT21, -, -1 :- {PV’L :- {P(Tl,X,-)}, -3 -I}} % s9 {P([l,[l~[l)~ P([-.IXll, P([-lYl1, P([-IZII, % SlO {P([ I*-,->* [-1X21, [4X31) [-IY21, [-IY31) [4Z31) [-(221, :- {P(Xl,X2,X3)}, :- {P(Yl,RY3)}, :- {P(Zl,Z2,Z3)}} P([-ITll,X, PC [-lT21, KZ) [-lT31) :- {P(Tl,X,T3)}, :- (P(T2, XZ)}} % mux( +X, +Y ?Z): Z is the larger of X and Y. {mux(Al,B1,Al) max(A2, B2, B2) :- {>= (Al,Bl)}, :- {> (B2, A2))) % min(+X, +I:?Z): {min(Al, ~~41) :-{=< (AI,BI)), Z is the smaller of X and Y. N.L. l’inkham/ArtQ%zial Intelligence 98 (1998) l-47 43 min( A2, B2, B2) :- { < (B2, A2))) % monusi:+X, +K ?Z) % if X > Y, then Z is X - Y; otherwise Z is 0. in successor notation. % Numbers are represented IV, 0, N) , {monus( monus( 0, _, 0) , monus(s(X),s(Y),Z) :- {monus(X,~Z)}} % intdiv( +x, +x?Z) % Z is X divided by Y, with remainder % Numbers are represented {int_&v( N, N, s( 0) ) , :- int_div(X,I:s(Z)) {greakrJhan( intdiv( A, B, 0) :- {Zess_than( A, B)}} ignored. in successor notation. X, Y) , monus( X, xX2), intdiv( X2, r! Z)}, Z is XmodY. are represented in successor notation. % mod(+-X, +x?Z): % Numbers {mod( N, N, 0) , :- mod(X, KZ) {grea,ter-fhun( X, Y) , monus( X, K X2), mod( X2, Y Z)}, mod(A, B, A) :- {less_thun(A, B))) list X to produce list Z, using Y for work space. % Jlatten2( +X, +I: -Z) % Flatten % Adapted Cflatten2( [ 1, Ws, Ws), &tten2( from [ XIXs] , Zs, Ys) :- [ 36, p. 2861. CfEatte~2(Xs,Zs,Ysl),~7utren2(X,Ysl,Ys)}, JEatten:!( K Vs, [ VIVs] ) :- { constunt( V), \==( y [ ] )}} % udjucent( +X, ?Y?Z) % Y and Z are adjacent elements of list X. {udjucenl-( [A, BI_] , A, B), udjucenl-( [-IT], X, Y) :- {udjucent( T, X, Y) }) % insert( +Listl, +Elt, -List2) % Insert Elt into sorted List1 { inseM [ I, X [Xl ), insert( [ HIT], X, [X, HIT] ) :- {H >= X}, insert( [ HIT11 , X, [ HIT21 ) :- {H < X, insert(T1, X, T2))) to produce sorted List2. % mergei +Listl, +List2, -List3) 44 N.L. lhkham/Artificial Intelligence 98 (1998) 1-47 % Merge sorted List1 and sorted Lisd, producing List3. {merge( [ I, L L), merge(L, 1 I, L) :- {L \== [ I}. merge( [HlITl], [H2]T2], {Hl =< H2, merge(T1, [H2]T2], merge([HlITl], :- [Hl]T3]) [ H2]T2], T3)}, [iY2]T3]) :- {Hl > H2,merge([HljTl],T2,T3)}} % shu.e( +Listl, +List2, -List3) % For example, shufJle( [a, b, cl, Ed, e, f], % Adapted [ 33, p. 2661. from [a, d, b, e, c, f] ). {shwW[ shufJEe([Hl]Tl], I, [ 1, [ I>, [H2lT2], [Hl,H2]T3]) :- (shufJle(T1, T2, T3))) % nd_reverse( +X, +I: -2) % Z is X reversed, with duplicate elements % Y, a scratch list, is initially % From {nd_reuerse( [ 36, p. 1461. [ 1, Es, Es), [ 1. removed. nd_reverse( [A/As], Revs, Bs) :- {member( A, Revs), nd_reverse( As, Revs, Bs)}, nd-reverse( [ CICs] , Revs2, Ds) (nonmember( C, Revs2), nd_reverse( Cs, [C IRevs :- , Ds)}} % uppend( +X, +I: -Z) {uppend( [ I, L L), uppend( [HIT], L2, [ HINewT] ) :- {uppend( T, L2, NewT)}} % urz.d( +X, +Y -Z): I, [ Iv [ I), {ahd([ Z is bitwise “and” of X, Y. {und([l~T1],[1~T2],[1~T3]):-{und(T1,T2,T3)}, {und([OlT4], {und( [ llT7], :- {und(T4,T5,T6)}, [_jT5], [OIT6]) [OJTQ, [OIT9]) :- {und(T7,T8,T9)}} Z is bitwise “or” of X, Y. % or( +X, +x-Z): {or([ I, [ 1, [IIy or([O]Tl], or([lIT4],[$‘5],[1]T6]):-{or(T4,T5,T6)}, or([OlT7l,[llT81, [lJT9l):-{or(n,T8,T9)}} :- {or(Tl,T2,T3)}, [OIT2], [OIT3]) % xor( +X, +y -Z): Z is bitwise “xor” of X, Y. @r(E 1, E 1, [ I>, xor( [H]T4], :- {xor(T4,T5,T6)}, [OIT6]) [H]T5], N.L. linkham/Arti$cial Intelligence 98 (1998) l-47 45 xor([HlITl], [H2)T2], [llT3]) :- {\=(Hl,H2), nor(Tl,T2,T3)}} -L&3) % p&2( +Listl, +List2, % For example, pair% [a, b, cl, [d, e, fl , [ [a, 4, [b, el, [c, fl I). % Adapted {paW 1 1, [ I, [ I) 9 puir2( [Hl(Tl], :- (paiR(Tl,T2,T3)}} [ [Hl,H2]IT3]) [ 33, p. 2661. [H21T2], from % union(+X, +e -2) [ 1, X4, X4), {union( union( [XSlRS], Y5,25) :- {member( X5, Y5), union( R5, Y5,Z5)}} union( [ X61R6], Y6, [ X6(Z61> :- Y6), union(R6, Y6,26)}} (nonmember(X6, % intersection( +X, +I: -Z) { intersection( intersection( [ 1, _, [ ] ) , [ X2jR2], Y2, [ X2lZ2] {member-( X2, Y2), intersection( R2, Y2,22)}} ) :- intersection( [X3lR3], Y3,23) :- {non.member( X3, Y3), intersectiun( R3, Y3,23)}) % delete$rst( +X, +x-Z) % Delete the first occurrence of Y from list X, producing list Z. {delete$rst( delete$rst( deEete$rst( [ I, -, [ I>, [ EIT] , E, T), [ HIT11 , Elt, [ HIT21 ) :- {\===( H, Elt), deleteJirst(T1, Elt, T2))) % deleteall( +X, +x-Z) % Delete all occurrences {delete-u&( [ ] , -, [ ] ), delete_ull( [ EIT] , E, L) :- (deleteall(T, deletexll( [ HIT1 1, Elt, [ H(T2] ) :- E, L)}, of Y from list X, producing list Z. {\===(a Elt) ,deleteall(Tl, Elt,T2)}} % setd@( +X, +I: -Z) {set-difS( [ I,-, 111, setdiJf( [ HITI], Seti, [ HIT31 ) :- {nonmember( H, Set2), set_diff(Tl, Set2, T3)}, set_di#( [HI IT], S2, Difl :- {member( Hl, S2), setd#(T, S2, Difl}} 46 N. L Tinkharn /Artificial Intelligence 98 (1998) I-47 References [ I ] D. Angluin and P.D. Laird, Identifying k-CNF formulas from noisy examples, Technical Report TR-478, Computer Science Department, Yale University, New Haven, CT ( 1986). [ 21 D. Angluin and C.H. Smith, Inductive inference: theory and methods, Computing Surveys 15 (3) ( 1983) 237-269. [ 31 F. Bergadano and D. Gunetti, in: Y. Deville, ed., Logic Program Synthesis and Transformation: Proceedings of LOPSTR 93, International Workshop of Logic Program Synthesis and Transformation Inductive synthesis of logic programs and inductive (Springer, Berlin, 1994) 45-56. logic programming, ]4] W. Bibel and K.M. HGmig, LOPS-a system based on a strategical approach to program synthesis, in: A.W. Biermann, G. Guiho and Y. Kodratoff, (Macmillan, New York, 1984) 69-89. eds., Automatic Program Construction Techniques [ 5 1 A.W. Biermann, The inference of regular LISP programs from examples, IEEE Trans. Systenzs Man Cybernet. 8 (8) (1978) 585-600. [6] A.W. Biermann and J.A. Feldman, A survey of results in: Y.-H. Pao and G.W. Ernst, eds., Tutorial: Context-Directed Pattern Recognition and Machine Intelligence Techniques from: S. Watanabe, (IEEE, Silver Spring, MD, 1982) 113-136; for Information Processing ed., Proceedings of the International Conference of Frontiers of Pattern Recognition (Academic Press, New York, 1972) 36-54. [ 71 M. Broy, Program construction by transformations: in grammatical inference, reprinted a family tree of sorting programs, A.W. Biermann 1983) and G. Guiho, eds., Computer Program Synthesis Methodologies (Reidel, Dordrecht, Netherlands, I-49. ]S] R.M. Burstall and J. Darlington, A transformation system for developing recursive programs, J. ACM 24 (I) (1977) 44-67. [9] A.K. Chandra and P.M. Merlin, Optimal implementation of conjunctive queries in relational data bases, in: Proceedings 9th ACM Symposium on Theory of Computing (ACM, New York, 1977) 77-90. [ lo] W.F. Clocksin and C.S. Mellish, Programming [ I1 ] N. Dershowitz, The Evolution of Programs ] 121 N. Dershowitz, Programming by Analogy, in Prolog (Springer, Berlin, 4th ed., 1994). (Birkhluser, Boston, MA, 1983). in: Machine Learning, Vol. II (Morgan Kaufmann, Los Altos, CA, 1986) 393421. [ 131 P. Flener and Y. Deville, Synthesis of composition for divide-and-conquer (Wiley, New York, 1993) 67-96. 1 141 M.R. Garey and D.S. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness and discrimination in: J.-M. Jacquet, ed., Constructing Logic Programs logic programs, operators (W.H. Freeman, New York, 1979). 1151 D. Gilbert and C. Hogger, Deriving logic programs from observations, in: J.-M. Jacquet, ed., Constructing Lugic Programs (Wiley, New York, 1993) 113-126. [ 161 E.M. Gold, Language identification in the limit, Inform. and Control 10 (5) (1967) 447-474. ] 171 M. Grobelnik, Induction of Prolog programs with Markus, in: Y. Deville, ed., Logic Program Synthesis and Transformation: Proceedings of LOPSTR 93, International Workshop of Logic Program Synthesis and Transformation (Springer, Berlin, 1994) 57-63. Interactive program derivation using program generated in: Y. Deville, ed., Logic Program Synthesis and Transformation: Proceedings of LOPSTR (Springer, Berlin, 1994) [ IS] A.-L. Johansson, strategies, 93, International Workshop of Logic Program Synthesis and Transformation 100-l 12. [ 191 P.D. Laud, inference by refinement, Technical Report TR-376, Computer Science Department, and incrementahy Inductive schemata Yale University, New Haven, CT ( 1986). 120 I I? Laird, Learning CT (1987). from good data and bad, Ph.D. Dissertation, TR-551, Yale University, New Haven, 121 I K.K. Lau and S.D. Prestwich, Top-down logic in: D.H.D. Warren and P Szeredi, eds., Logic Programming: Proceedings of the Seventh of recursive procedures first-order synthesis from specifications, International Conference I22 I C.L. Liu, Elements of Discrete Mathematics (McGraw-Hill, New York, 1977). (MIT Press, Cambridge, MA, 1990) 667-684. N.L. Tinkham/Artificial Intelligence 98 (1998) 1-47 47 [23] Z. Manna and R. Waldinger, A deductive approach to program synthesis, Nilsson, eds., Readings ACM Trans. Programming Languages and Systems 2 ( 1) ( 1980) 120-15 1. in Artificial Intelligence (Tioga, Palo Alto, CA, 1981) 141-172; in: B.L. Webber and N.J. from: reprinted [24] R.S. Michalski, implementation, Techniques Inductive a theory and in: A.W. Biermann, G. Guiho and Y. Kodratoff, eds., Automatic Program Construction learning as rule-guided generalization of symbolic descriptions: (Macmillan, New York, 1984) 5 17-552. [25] T.M. hilitchell, Generalization 1261 G.D. Plotkin, A note on as search, Artificial Intelligence 18 (2) ( 1982) 203-226. in: B. Meltzer and D. Mitchie, inductive Intelligence, Vol. 5 (Halsted Press, New York, 1970) 153-163. generalization, eds., Machine [27] G.D. Plotkin, A further note on inductive generalization, in: B. Meltzer and D. Mitchie, eds., Machine fntelligence, Vol. 6 (Halsted Press, New York, 1971) 101-124. [ 28 I R.J. Popplestone, An experiment Intelligence, Vol. 5 (Ha&d 1291 J.C. Reynolds, Transformational in automatic deduction, Press, New York, 1970) 101-124. systems and the algebraic in: B. Meltzer and D. Mitchie, eds., Machine structute of atomic formulas, in: B. Mehzer and D. Mitchie, eds., Machine Intelligence, Vol. 5 (Halsted Press, New York, 1970) 135-15 I. [30] E.Y. Shapiro, Inductive inference of theories from facts, Research Report 192, Computer Science Department, Yale University, New Haven, CT ( 1981). [31] E.Y. Shapiro, An algorithm that infers theories from facts, in: Proceedings IJCAI-81, Vancouver, BC (1981 I 446451. [32] E.Y. Shapiro, Automatic Program Debugging [33] D. Shaw, W. Swartout and C. Green, Inferring LISP programs (MIT Press, Cambridge, MA, 1982) from examples, in: Proceedings IJCAI-75, Tblisi, Georgia ( 1975) 260-267. [34] D.R. Smith, The Structure Palo Alto, CA (1988). and Design of Global Search Algorithms, KES.U.87.12, Kestrel Institute, 1351 L. Sterling and M. Kirschenbaum, Applying techniques to skeletons, in: J.-M. Jacquet, ed., Constructing Logic Programs (Wiley, New York, 1993) 127-140. [36] L. Sterling and E. Shapiro, The Art of Prolog (MIT Press, Cambridge, MA, 2nd ed., 1994). [ 371 PD. Summers, A methodology for LISP program construction from examples, J. ACM 24 (I ) ( 1977) 161-1’75. [38] N.L. Tinkham, Induction of schemata for program synthesis, Technical Report CS-1990-14, Duke University, Durham, NC ( 1990). 1391 N.L. Tinkham, A theorem on refinement operators Rowan University, Glassboro, NJ (1997). for logic program synthesis, Technical Report TR97-1, [40] L.G. Valiant, A theory of the learnable, Comm. ACM 27 (11) from examples, [41] P.H. Winston, Learning structural descriptions ( 1984) 1134-l 142. in: PH. Winston, ed., The Psychology of Computer Vision (McGraw-Hill, New York, 1975) 157-209. 