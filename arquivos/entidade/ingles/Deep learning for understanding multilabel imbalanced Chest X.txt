Future Generation Computer Systems 144 (2023) 291–306Contents lists available at ScienceDirectFuture Generation Computer Systemsjournal homepage: www.elsevier.com/locate/fgcsDeep learning for understanding multilabel imbalanced Chest X-raydatasetsHelena Liz a,b,∗David Camacho aa Computer Systems Engineering Department, Universidad Politécnica de Madrid, Alan Turing s/n, Madrid, 28031, Spainb Department of Computer Sciences, Universidad Rey Juan Carlos, Tulipán s/n, Móstoles, 28933, Spainc Computer Science Department, Universidad Autónoma de Madrid, Madrid, 28049, Spaind TECNALIA Basque Research & Technology Alliance (BRTA), P. Tecnologico 700, Derio, Bizkaia, 48160, Spaine University of the Basque Country (UPV/EHU), Bilbao, 48013, Spain, Javier Huertas-Tato a, Manuel Sánchez-Montañés c, Javier Del Ser d,e,a r t i c l ei n f oa b s t r a c tArticle history:Received 29 July 2022Received in revised form 28 December 2022Accepted 4 March 2023Available online 6 March 2023Keywords:Convolutional neural networksChest X-raysExplainable AIEnsemble MethodologyOver the last few years, convolutional neural networks (CNNs) have dominated the field of computervision thanks to their ability to extract features and their outstanding performance in classificationproblems, for example in the automatic analysis of X-rays. Unfortunately, these neural networks areconsidered black-box algorithms, i.e. it is impossible to understand how the algorithm has achieved thefinal result. To apply these algorithms in different fields and test how the methodology works, we needto use eXplainable AI techniques. Most of the work in the medical field focuses on binary or multiclassclassification problems. However, in many real-life situations, such as chest X-rays, radiological signsof different diseases can appear at the same time. This gives rise to what is known as "multilabelclassification problems". A disadvantage of these tasks is class imbalance, i.e. different labels do nothave the same number of samples. The main contribution of this paper is a Deep Learning methodologyfor imbalanced, multilabel chest X-ray datasets. It establishes a baseline for the currently underutilisedPadChest dataset and a new eXplainable AI technique based on heatmaps. This technique also includesprobabilities and inter-model matching. The results of our system are promising, especially consideringthe number of labels used. Furthermore, the heatmaps match the expected areas, i.e. they mark theareas that an expert would use to make a decision.© 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-NDlicense (http://creativecommons.org/licenses/by-nc-nd/4.0/).1. IntroductionIn recent years, the field of medicine has faced two relevantproblems that hinder patient care: staff workload and subjectiv-ity in the interpretation of tests [1,2]. These problems have noeasy solution, which is especially dangerous in medicine becauseprocedural errors can lead to serious health complications. Firstly,overwork in medicine, aggravated in recent times by the globalCOVID-19 pandemic, can lead to errors and delays in diagnosisand treatment. As mentioned above, there is also subjectivity inthe interpretation of some medical tests. The expert analysingthese tests, for example X-rays, may arrive at an erroneous di-agnosis due to, for example, the existence of signs of differentdiseases to different degrees [3]. This type of imaging test is oneof the most common in various diagnoses due to its low cost,∗Corresponding author at: Computer Systems Engineering Department,Universidad Politécnica de Madrid, Alan Turing s/n, Madrid, 28031, Spain.E-mail addresses: helena.liz@urjc.es (H. Liz), javier.huertas.tato@upm.es(J. Huertas-Tato), manuel.smontanes@uam.es (M. Sánchez-Montañés),javier.delser@tecnalia.com (J. Del Ser), david.camacho@upm.es (D. Camacho).speed of acquisition and the fact that it does not require muchpreparation [4]. Chest X-rays are useful for detecting a variety ofdiseases of the chest related to different organs such as the heart,lungs or bones. The features of X-rays make them suitable foranalysis with convolutional neural networks (CNN) [5]. The com-bination of AI algorithms and medical knowledge can improve theperformance of medical staff [6] and could also reduce patientwaiting times by speeding up the diagnostic process and reducingthe workload of doctors.CNNs have been a breakthrough in computer vision due totheir ability to extract features from images. These architecturesare composed of different layers. The first has convolutional lay-ers that are inspired by the notion of cells in visual neuroscience.The architectures are based on the visual cortex of animals. Themain reason why these architectures have stood out is their greatcapacity to extract patterns from data, improving the perfor-mance of previous systems based on Machine Learning models.This advantage has made them a benchmark in Deep Learningdue to their high performance in a wide range of tasks, such asspeech recognition, computer vision or text analysis [7].https://doi.org/10.1016/j.future.2023.03.0050167-739X/© 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306The properties of chest X-rays make them susceptible to beanalysed by this type of algorithms. Some of the main advantagesof CNNs over traditional techniques are that it is not necessary tomanually extract image features or perform segmentation, andthat by being able to learn from large volumes of data theycan identify patterns that are difficult for the human eye to de-tect. Although in this article we focus on classification problems,other problems can be solved, such as X-ray segmentation [8],localisation, regression (such as predicting drug dosage), amongothers. CNNs are a potential tool for the analysis of chest ra-diographs. However, most of the work in this field focuses onbinary and multiclass classification problems. Actual problemsare usually more complex than the above; they tend to be mul-tilabel classification problems, i.e. the different labels are notmutually exclusive, whereas in binary and multiclass classifica-tion problems there is only one label per radiograph [9]. To solvemultilabel problems, we need to explore new strategies. Adaptingalgorithms can interpret this kind of problem by transformingthem into simpler problems that can be solved by traditionalalgorithms, i.e., transforming them into binary problems [10]. Inthe field of chest X-rays we can find samples without labels,healthy patients and samples with radiological signs of severaldiseases at the same time. On the other hand, there are a largenumber of different radiological signs in chest X-rays, so if wewant to build and validate a system that approximates realisticconditions, we have to use a dataset with a large number ofmutually non-exclusive labels. This is the case of the PadChestdatabase [11], which has 174 different radiological signs, substan-tially increasing the degree of realism and the complexity of theproblem.Many machine learning algorithms, including CNNs, work bestwhen the classes in the dataset are balanced. However, in reallife it is common to find datasets where this condition is notmet; they are imbalanced datasets, where one or more classeshave substantially more examples than the rest. As a conse-quence, with such datasets, machine learning algorithms learna bias towards the majority class, even though the minorityclass is often more relevant. Therefore, it is necessary to applydifferent methods to improve the recognition rate [12]. Thereare several options to overcome this difficulty: (a) modify thedataset, reducing the samples from the majority class or increas-ing the number of samples from the minority class; (b) modifythe algorithms to alleviate their bias towards the majority class,e.g. weighted learners [13]. The problem of unbalanced databasesis exacerbated in multilabel classification problems, where mul-tiple minority classes may appear, making this challenge moredifficult to solve. In medicine, it is widespread because eachdisease has a different incidence in the population. Heart disor-ders top the list of the deadliest diseases, followed by chronicobstructive pulmonary disease, which causes more than 6 milliondeaths a year. In contrast, other diseases such as lung cancerare the sixth leading cause of death with less than 2 milliondeaths, according to the World Health Organization.1 As a result,most radiographic datasets are imbalanced; a clear example isPadChest, the dataset used in this article, where the number ofsamples in each class approximates the incidence published bythe World Health Organization.These algorithms, like many other Deep Learning and MachineLearning methods, are considered ‘‘black box’’ algorithms becauseend users can only analyse the input and output, but the inferenceprocess is opaque, which reduces confidence in these algorithms.To alleviate this problem, explainable AI techniques have beendeveloped, such as saliency maps, which produce heatmaps thatFig. 1. Visualmethodology.representation ofthe problem and the objective ofthehighlight the pixels with the greatest influence on the final pre-diction [14]. This problem is serious in medicine, where errorscan be dangerous for patients [15]. For this reason, explainableAI techniques are essential, as they allow users to understandhow the system has arrived at the final result and use it to helpdiagnose [16]. However, the combination of medical knowledgeand AI has many advantages, such as helping to reduce medicalerrors and speeding up diagnostic processes, leading to improvedpatient care, as doctors would have more time to attend patients.The contribution of this manuscript is a methodology, seeFig. 1, for classifying imbalanced multilabel datasets with manyclasses The aim of this methodology is to generate robust andquality models; in this case, it has been applied to a highlyimbalanced multilabel chest X-ray dataset with 174 classes. Weselected this dataset for two reasons: (i) the number of classes,which is higher than in other state-of-the-art datasets; and (ii)the high imbalance between these classes. This methodology willallow to establish a suitable benchmark for this dataset againstwhich future works can be compared, as there are currently veryfew published contributions using this dataset and they do notprovide a detailed analysis of the problem.We can summarise the main contributions of this work asfollows:• A methodology for imbalanced multilabel classification pro-blems.• A discussion about the experimental results obtained usinga dataset with a large number of classes (more than 30) anda severe imbalance between them.• An explainability interface using Grad-CAM for multilabeldatasets.• A suitable benchmark for this dataset serving as a refer-ence against which to compare future proposals from thescientific community.Finally, this manuscript is organised as follows. Section 2 sum-marises the most relevant work in the literature, with a specialfocus on chest X-ray classification problems for imbalanced mul-tilabel datasets; Section 3 describes the methodology proposedfor this type of problem, consisting of training a model and gen-erating a visualisation based on heatmaps; Section 4 presents theCNN architectures, the hyperparameters used for training, detailsof the execution environment, and a link to the repository wherethe code used in the experimentation can be found; Section 5presents the experimental results, and Section 6 presents themain conclusions and possible lines of future work.2. Related work1 https://www.who.int/es/news-room/fact-sheets/detail/the-top-10-causes-of-deathSince the first application of AI techniques in medicine inthe 1980s, the use of these algorithms has grown exponentially,292H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306especially in recent years. Deep learning algorithms are appliedto all kinds of clinical data [17]: biosignals, which include elec-trical [18,19], mechanical [20,21] and thermal signals [22,23];biomedicine, which studies molecules of biological processes[24–26]; electronic health records (EHR), focused on optimisingdiagnosis [27–30]; and clinical imaging, widely used in the diag-nosis of many diseases [31–34], as is the case with our problem.The practice of healthcare has evolved from observation-basedmedicine to evidence-based medicine. This makes deep learningand big data algorithms especially useful in this field as theycan identify some radiological signs that medical staff cannotdetect [35]. Although in this manuscript we focus on classifi-cation problems, there are papers where these algorithms areused in regression problems, such as estimating the dose of adrug [36]; generating medical reports from clinical tests [37];support healthcare management [38]; or image processing, suchas image segmentation [39] and image reconstruction [40].The COVID-19 pandemic has had a strong impact on researchinto the application of machine learning and deep learning inmedical image analysis. As expected, many of the classificationsystems investigated have focused on detecting signs of bilateralCOVID-19-associated pneumonia. In Ahmed et al. [4] they usetwo different pre-trained architectures to classify chest X-rays,VGG16 and ResNet, and optimise the hyperparameters. In Pham[41] they train three different pre-trained architectures, AlexNet,GoogleNet and SqueezNet, with six datasets independently, test-ing different percentages of train set samples (50 and 80%),achieving an accuracy of 99.85% with SqueezeNet. However Ah-mad et al. [42] develops an ensemble system based on MobileNetand InceptionV3 that achieves 96.49% accuracy. Soon, binaryclassification was extended to multiclass problems, making itpossible to discern whether pneumonia is caused by COVID-19 oranother virus/bacteria or whether the patient is healthy. As withbinary classification problems, many works, such as Avola et al.[43], use state-of-the-art architectures to find the best performingones, such as AlexNet, GoogleNet, ResNet and ShuffleNet, amongothers. MobilNet_v3 achieves the best result with a precision of84.92% on a dataset composed of 6330 samples. In Zebin andRezvy [44], in addition to training a pre-trained state-of-the-art architecture, a heatmap-based visualisation is generated thatshows two images for each sample. The first is the original X-ray,and the second is the class activation map, i.e. the most importantarea for the CNN. However, the images do not overlap, makinginterpretation difficult. Other works, such as Teixeira et al. [45],apply segmentation techniques to remove all irrelevant areas ofthe system, which should improve performance and visualisation.Their dataset consists of three different classes: COVID-19, normaland lung opacity.As we have discussed in Section 1, the explainability of deeplearning models is a fundamental factor to be taken into ac-count in their application. These models are black-box algorithmsand need explainable AI techniques to make them more trust-worthy [46]. There are two main ways to produce the finalvisualisation, (1) generate a heatmap per label, or (2) generate asingle visualisation for all classes. The first one is more commonlyused, [43,45,47] however, this technique has one main limitation:it is not feasible for a large number of labels, and it makes a globalview difficult. The second one (e.g. Teixeira et al. [48]), shows thedifferent signs as areas with higher colour intensity, but only onecolour scale was used, which makes it difficult to identify whichpathological sign indicates which area of interest. We propose anew technique where each visualisation shows a radiological signincluding the probability and agreement between models.Although most medical datasets have two classes (samples ofa particular pathology and healthy samples), in chest X-rays it iscommon to find signs of more than one pathology. For this reason,Table 1Summary table of multilabel datasets in the field of chest radiography.# samples # patientsLabels ViewsReferenceChestX-ray 14CheXpertPadChest1121202243161600003271765240670001414174frontalfrontal/lateralfrontal/lateral[54][55][11]in the last five years different authors have published multilabelradiological datasets. These datasets are closer to real situationsthan binary ones, with the additional challenge of imbalance ofdifferent classes. The size of each class in a realistic dataset shoulddepend on the incidence of pathology in society, i.e. some classesare more represented than others. These characteristics of thesedatasets are interesting and need to be analysed in detail in orderto address the problem adequately.2.1. Multilabel classification problemsAs we have seen, much of the work generated in recent yearshas focused on binary and multiclass classification problems. Inthese problems, the labels are mutually exclusive, while multi-label classification problems have multiple classes that are notmutually exclusive, which increases the difficulty of the problem.There are two ways to solve these problems: (a) transform themultilabel problem into simple binary problems, or (b) adapt thealgorithms to solve the multilabel problem directly, i.e. attack theproblem globally [49].Binary and multiclass classification systems are very restric-tive, as they only serve to detect one type of radiological finding.However, patients can often present signs of multiple diseasesat the same time. There are very few multilabel datasets thattake into account a large number of signs, as they require alarge number of samples and most of them have only a fewlabels. Three datasets are worth highlighting for their quality andrelevance to the state of the art (see Table 1). The first two havebeen used extensively in image classification problems, but thethird has been used mainly in medical report generation [37,50–53]. However, this third dataset has two advantages that make itvery suitable also for classification problems: (1) the number oflabels is larger; (2) it has the largest number of different patients,which implies a smaller number of similar samples from the samepatient. Given the lack of application of algorithms for this taskto increase the potential and interest of this dataset mentionedabove, it was selected as a case study for this article.ChestX-ray 14 dataset is one of the most widely used datasetsin the field of chest X-ray classification since its publication in2019 [54]. For example, Wang et al. [56] uses DenseNet-121optimising its hyperparameters, obtaining an average AUC of0.82. The AUC achieved for the pneumonia class was 0.662 (thelowest), while for the hernia class it was 0.923 (the highest).Other researchers use different architectures such as Inception-ResNet_v2 and ResNet152_v2 to achieve an AUC for pneumoniaof 0.73 [57]. Much of the work on this dataset retrains state-of-the-art architectures, but there are other strategies for improvingclassification performance; for example, Almezhghwi et al. [58]switches the classifier from AlexNet and VGG16 to SVM withthe intention of improving the results of previous manuscripts,achieving an AUC for pneumonia of 0.98 with both architectures.Having different types of radiographs of the patient can alsoimprove the classification results, for example a frontal and alateral X-ray. Finally, the main disadvantage of ChestX-ray 14dataset is that it only contains radiographs with a frontal view,while CheXpert and PadChest datasets also contain X-rays with alateral view.293H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306The second dataset, CheXpert has 14 different labels andreports on all images. In terms of published classification work,we find a situation similar to ChestX-ray 14, with many worksretraining state-of-the-art architectures, such as Seyyed-Kalantariet al. [59], where they adjust the hyperparameters of DenseNet-121 to optimise its performance. Other authors look for differentstrategies, such as Cohen et al. [60], where they make two mod-ifications to DenseNet to improve its performance. First, theymodify the loss function by assigning weights to the differentlabels, alleviating the imbalance problem. Second, they modifythe threshold for discerning between the presence or not ofeach label, i.e. the probability at which the class is consideredpresent. However, the CheXpert dataset has the same limitationas ChestX-ray 14: they contain only 14 possible diseases, whichrepresents only a small subset of all possible diseases that maybe present in the chest.Finally, PadChest is the most interesting dataset of the three inour opinion because it has many more labels than the others. It isa massive multilabel classification problem, much closer to realitythan the other datasets. The number of patients used is also largerthan in the others, leading to more variability in the dataset, andthe imbalance of the classes is larger too. One of the papers usingthis dataset, [61], combines the PA and lateral views to predictlabels in four different ways: (a) the lateral view is stacked inthe second channel of PA X-ray; (b) both views are processedby two CNNs and the combination of them is processed by afully connected layer; (c) the model input is processed throughtwo separate CNNs, the output is concatenated and passed bytwo dense layers with an average pooling layer between them;(d) a modification of (c) where two dense layers are added. Amajor limitation of that paper is that it shows overall resultswithout performing a detailed analysis per label, which preventscomparison with other works in the area. On the other hand,in Pooch et al. [62] CheXNet is retrained, which is a state-of-the-art architecture previously trained with a multilabel chest X-raydataset. In that paper, different models are trained with fourdatasets, and each model is tested with each dataset separately.The main limitation of that manuscript is the reorganisation ofthe labels of PadChest dataset: the label ‘‘Lesion’’ is generated tounify the samples of the atelectasis classes, using only 8 classesout of the 174 available. Given the limitations we have foundin all classification works using the PadChest dataset and thatsome most of them are not replicable, we propose to create abenchmark that future works can use to compare results, witha methodology adapted for the two main problems: the highnumber of different labels, and the imbalance between them.As we have explained in this section, the PadChest dataset hasseveral advantages over other multilabel datasets: (i) it has themost labels, which makes it closer to real-world scenarios; (ii)the number and diversity of patients is greater; and (iii) it con-tains lateral and frontal radiographs. We propose two ways oforganising the dataset based on the term tree provided by itsauthors, which allows us to group radiological signs into higherclasses. The first one uses the specific labels for a finer-grainedclassification. The second one works with more general labels,which indicate more general radiological signs.2.2. Class imbalance in deep learningAs explained above, most machine learning algorithms workbest when the number of samples for each class is similar. Whenthere is a significant difference between the classes, the systemwill boost the majority class while the minority class(es) will haveless relevance, even though the minority class is often the mostrelevant. There are several classification tasks with this problem,such as Cohen et al. [63], where the majority class is COVID-19 over the rest of the pneumonia classes. As expected, becausethe incidence of COVID-19 has been extremely high, the datasetcontains more than 400 samples of COVID-19 followed by theclass Pneumocystis spp with fewer than 30 samples. This phe-nomenon appears in many classification tasks, especially thosewith more than two classes, both multiclass and multilabel. Forexample, in Wang et al. [54] there are 15 classes and the class‘‘No findings’’/‘‘Normal’’ exceeds 50,000 samples, while the otherlabels have less than 20,000 samples, of which only three exceed10,000 samples.As mentioned in the Introduction section, there are differentstrategies to alleviate the class imbalance problem. Modify thedataset, for example with oversampling techniques, which in-crease the number of samples from minority classes by applyingdata augmentation and histogram equalisation techniques [64].Charte et al. [65] develops a new algorithm, Multilabel SyntheticInstance Generation, for multilabel problems. For each sample,a nearest-neighbour search is performed, the features are ex-trapolated and the label is generated from them. Another optionfor generating synthetic samples is to use generative adversarialnetworks (GANs), i.e. to use deep learning models to producenew samples from the original dataset. Salehinejad et al. [66]uses this method to generate new chest X-rays to balance thedifferent classes. Another strategy for balancing the classes in thedataset is to reduce the samples of the majority of classes. Thistechnique is called undersampling. Typically, random samplesare removed from the majority classes, as in Qu et al. [67],where the maximum number of samples in each class is set tobalance it. Undersampling is not as widespread as oversamplingbecause Deep Learning systems need a large number of samples,so undersampling may not work.Another strategy to alleviate class imbalance is to modify theway the model learns by increasing the weight of minority classesin learning, thus preventing the model from giving more impor-tance to majority classes. One option is to apply class weightsin the loss function that increase the relevance of the minorityclasses. One example is Rajpurkar et al. [68], which uses the chestX-ray14 dataset to classify the presence or absence of pneumonia.Another example is Monowar et al. [69], where the weightedbinary cross-entropy loss function is applied. Ge et al. [70] devel-oped a novel error function, Multilabel Softmax Loss, this methodconsiders the relationship of multiple labels explicitly, the authorcomputes the derivative of the error with respect to each classusing the chain rule. In addition they applied it to a system com-posed of two CNNs combined by a bilinear pooling layer. Teixeiraet al. [48] proposes a dual lesion attention network composed oftwo models, DenseNet-169 and ResNet-152, as feature extractors,after an attention module and average max pooling. The outputsare combined to generate three classifiers. Finally, all classifiersare merged to obtain the final prediction. In addition, they useda variant of the weighted binary cross-entropy loss. To tackle theclass imbalance, we propose using weighted cross-entropy withlogits using class weights.2.3. The challenge of imbalance in multilabel classification problemsAs we have explained, many real classification problems havetwo properties that make them difficult to solve: multilabelingand imbalance. Each of these two properties alone makes clas-sification difficult, so together they can be very challenging. Inmedicine, multilabel and imbalance problems are common be-cause medical staff can find different radiological signs on a chestX-ray, and different diseases do not have the same incidence inthe population. All the datasets mentioned in Section 2.1 haveboth features; however, ChestX-ray 14 and CheXpert have a lownumber of classes, 14 labels, compared to PadChest [11], which iscomposed of 174 different labels with a large imbalance: the label294H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306Fig. 2. Visual representation of the proposed ensemble system. We train eacharchitecture with preprocessed images, and their outputs are combined togenerate the ensemble output. Finally, the system produces the global predictionand heatmap visualisation.‘‘Normal’’ has more than 35000 samples, while other labels, suchas round atelectasis, pleural mass or nephrostomy tube, have lessthan ten samples.Most of the published work using these datasets modifies thearchitecture so that it can directly solve multilabel problems, butdoes not consider or apply any specific technique to solve theimbalance problem. However, other works explore different waysto overcome these difficulties and achieve better results. Suchas Huang and Fu [71], which proposes a multi-attention con-volutional neural network to reduce the performance differencebetween classes and, more interestingly, to extract discriminativefeatures to classify similar classes, which is very common inthis kind of dataset. Wang et al. [72] generates three images:the first one is the original chest X-ray, the second one is asegmentation-based cropping, where areas not interesting forthe model are removed, and the last one is a cropping of thearea where previous models have found pathological signs. Theinformation extracted from the three images is fused and finallyprocessed to obtain the final result. Another interesting strategyis the modification of the loss function to focus on the mostinteresting samples; for example, Qin et al. [73] proposes a lossfunction called ‘‘weight focal loss’’, which forces the model topay more attention to the most difficult samples. This makesthe model pay more attention to minority classes, avoiding falsenegatives.These methods can help in class imbalance problems, but inextreme cases of multilabel and imbalance, such as the PadChestdataset, they may not be sufficient. Most of the published pa-pers attempt to improve the performance of the architecture orsolve these problems using a single strategy, which may not besufficient for datasets such as PadChest.interestingradiologicalsigns, we haveIn contrast to other works in the related literature, we havedecided to address these problems by combining different strate-gies: (1) to avoid confounding the model with areas that do notappliedpresentsegmentation-based cropping; (2) to make the system robustagainst the individual errors of the different architectures, wehave created an ensemble whose hyperparameters have beenadjusted in a validation split to obtain the best possible results;(3) we have applied a specific loss function for imbalanced datathat weights each class by its inverse frequency. The combina-tion of these techniques will allow us to substantially reducethe errors due to imbalance and the high number of labels. Inaddition, we have created a heatmap-based visualisation thathighlights the most important areas for detecting each diseaserepresented in the dataset, the estimated probability of thatpathology, and the agreement between models (how many mod-els have a probability higher than 50% for that disease), whichfacilitates interpretation and shows the degree of confidence inthe result.Fig. 3. A segmentation-based cropped sample. The first image corresponds tothe original X-ray. The second shows the lung segmentation mask. The thirdone show the cropped image with lung mask, and finally the last image showsthe input of our system, the preprocessing result.3. MethodologyWe can summarise the proposed methodology in Fig. 2, whichhas four sections. The first is the data pre-processing step, wherewe prepare the images for the model and apply data augmen-tation to alleviate class imbalance. In the second stage we buildthe model, training different state-of-the-art architectures. Wethen combine the results of each model to obtain the final prob-abilities. Finally, we developed a multilabel heatmap techniqueto areas of the image that are relevant in the classification. Inthis technique, the original X-ray is combined with one or moreregions labelled with different colours to facilitate the applicationof these techniques in health centres or hospitals.3.1. Label selectionAs explained above, multilabel datasets are often imbalancedas they have classes with a low number of samples. For thisreason, we must establish a criterion for choosing the labels toinclude in our classification system, especially in datasets wherethe number of classes is extremely high, as in our case. First, weset the minimum number of samples a label must have to beincluded in the classification problem, and we set the thresholdat 200 X-rays. For a dataset of 90000 samples this is 0.22% ofthe total. The model cannot work correctly for under-representedlabels as it is not a few-shot system. If a sample has only deletedminority labels we will remove it.In this paper we consider two different experiments. First,we use the classes proposed by the authors of the dataset thatcorrespond to the specific labels; this classification system hasa smaller number of samples and labels due to the cleaning ofunder-represented labels explained in the previous paragraph,but is a more fine-grained classification system. In the secondcase, we use more general labels. We create these classes group-ing the specific labels according to their characteristics. The num-ber of samples and classes is larger at the cost of being less precisesystems, but it allows us to cover a larger number of differentclasses.3.2. PreprocessingThe raw images were preprocessed in order to train the modelefficiently. First, we reduced the number of channels to onebecause although the original files are RGB images (three colourchannels), the X-rays are grayscale images, so all three channelscontain the same information. Next, we normalised their size to512 × 512 pixels. The pixel values were then normalised between0 and 1, Fig. 3 (first image).Chest X-rays show an area larger than the area of interest(ROI). Areas such as arms or neck, among others, are irrelevantto the problem we want to solve, so a cropping based on seg-mentation masks was performed, forcing the system to focus onthe relevant areas. This trimming is performed in three differentsteps: first, we generated the lung masks using a segmentationmodel based on the U-Net architecture [74], Fig. 3 (second image).295H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306We also added the area underneath the lungs to the masks asit may contain radiological signs of interest. On many occasions,the segmentation models are not perfect; they generate morethan two masks, leave gaps inside the masks, etc. Therefore,thirdly, we decided to use a mask post-processing system [75].This system fills the possible gaps in the masks by applying theflood fill algorithm, which analyses the pixels neighbouring theone of interest and depending on whether or not they belongto the mask, it will decide to fill the gap or not. Then, if morethan two masks have been generated (one per lung), those whosearea is less than a predetermined value are removed. In addition,in case the lung masks are stuck together, they are separated.Finally, the image is cropped using the mask coordinates and thelower boundary of the sample, Fig. 3 (third image). As the imagescan have different sizes, we normalised their size to 224 × 224pixels, because this is the normalised size of the samples in thestate-of-the-art models, Fig. 3 (last image).3.3. Image classification with CNNsFive state-of-the-art architectures pre-trained with ImageNetwere selected for their relevance:EfficientNetB0. [76]: This architecture uses different scaling coef-ficients to scale width, depth and resolution. In the EfficientNetfamily, this architecture is the smallest. It is based on the ideathat if the images are larger, the network needs more layers toextract the relevant information.DenseNet-201. [77]: Instead of adding more layers to the archi-tecture, the number of connections between units is increasedby connecting each unit to the last, unlike ResNet50, which onlyconnects one unit to the next output. This architecture has severaladvantages: it alleviates the vanishing gradient problem, enforcesfeature propagation and feature reuse, and reduces the number ofparameters.InceptionV3. [78]: This architecture is different from the previousones. It factorises convolutions into smaller convolutions (whichcan be asymmetric) to reduce cost. In addition, this architecturehas an auxiliary classifier between layers that acts as a regulariser.InceptionResNetV2. [79]: This architecture combines ResNet andInceptionV3. It consists of several Inception units with shortcutconnections between them; this enhances the capability of thearchitecture.Xception. [80]: It consists of depth-wise separable convolutionsinvolving two steps: depth-wise convolution, which differs fromthe standard convolution in that it only acts on one channel; andpoint-wise convolution, where a 1 × 1 convolution is applied toall channels. This architecture also includes shortcut connections,such as ResNet50.We applied Transfer Learning on the above five architecturesand retrained them with PadChest dataset, replacing the classifierin all cases with two dense layers. We froze the first 10% ofthe convolutional layers, as they detect basic patterns and donot need to be retrained. The remaining convolutional layers areretrained to learn patterns specific to our problem. The mainrelevant training parameters are summarised in Table 2. In ad-dition, a checkpoint is used to save the best model using thevalidation loss. Finally, an early stopping algorithm was used tofinish training when the validation loss did not improve over thelast 25 epochs by more than a threshold of 0.001.Table 2Summary ofaugmentation and training methodology.the hyperparameters used in training: optimisation, dataOptimisationOptimiserLearning rateLossFeed-forward classifier# NeuronsActivationDropoutData AugmentationShear rangeZoom rangeRotation rangeWidth shift rangeHeight shift rangeHorizontal flipFill modeBrightness rangeChannel shift rangeTraining methodologyMaximum epochsEarly stopping patienceEarly stopping thresholdBatch sizeImage sizeAdam1e−4weighted crossentropy with logits512ReLu0.20.10.1450.10.1Truenearest0.7–1.10.05350250.00132224 × 2243.4. Ensemble techniqueEnsemble learning is an effective way to improve the perfor-mance and robustness of deep learning algorithms. We combinedthe results of all trained models, obtaining a system composed offive different architectures with the same test set. We distinguishtwo approaches [81]: ‘‘Combine then predict’’ (CTP) and ‘‘Predictthen combine’’ (PTC). In the CTP method, the label probabilitiespredicted by the individual models are first calculated, and thenthe average probability at each label is used to obtain the ensem-ble label prediction. The other method, PTC, combines the binarypredictions to obtain the ensemble. We consider two versionsof PTC: label-wise voting (PTC-lw), which calculates the numberof positive and negative individual predictions for each label,adopting the majority. Thus, PTC-lw calculates the prediction ofeach label independently of the others. On the other hand, PTC-mode calculates the set of labels predicted by each individualmodel, and predicts the most frequent set.3.5. Heatmap generationAs explained in Section 2, it is necessary to include XAI tech-niques for the medical staff to understand the output given by oursystem. For this reason, we developed a visualisation techniqueusing heatmaps. A heatmap is a matrix of the same size asthe input image. The value of each pixel is proportional to itsimportance for the classification of the model. A colour scale isused in the heatmap to highlight the most relevant pixels for themodel.The first step in generating the heatmaps is to change theactivation function of the last layer (the classifier layer) fromsoftmax to linear. Then, for each classifier neuron, we computethe weighted average of the last convolutional layer. Each channelis weighted by the gradient of the classifier neuron with respectto that channel. This is the so-called grad-CAM algorithm [82],which allows to compute a heatmap for each class.As explained before, the ensemble consists of five models.We generate ensemble heatmaps by averaging the individualheatmaps generated by those models. Finally, we generate a ofthe average heatmap of each classifier neuron, which is overlaid296H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306Fig. 4. Example of a section of the term tree of the dataset. The general labelis boxed in blue, and the specific labels are marked in black.Table 3Summary table of the two types of experiments performed (general labels, andspecific labels). The total number of labels and the total number of samples ineach of the splits are shown.# classes# samplesTrain sizeVal. sizeTest sizeGeneral labelsSpecific labels543590687853676347559753906985321814317082on the original X-ray using a 10% of transparency to improve theinformation for the medical staff. We include in the title the es-timated probability for this class and the inter-model agreementshowing the confidence of the ensemble in that prediction, whichfacilitates the use of the system by medical staff.4. Experimental setup4.1. DatasetIn this article, we have used the PadChest dataset [11], animbalanced and multilabel dataset. It was published in January2019 by the University of Valencia together with BIMCV. Thesamples were collected at Hospital de San Juan (Spain) between2009 and 2017. This dataset is composed of 160,868 clinicalimages from 67,625 patients, divided into 174 different labels,and corresponds to different signs of thoracic disease. This datasetcontains chest X-rays with different projections: posteroanterior(PA), anteroposterior (AP) and lateral views; however, only PAX-rays were used for experimentation, corresponding to 91,728clinical images from the original dataset. The authors of thedataset provided a term tree2 in which all labels are groupedinto more general labels, as can be seen in Fig. 4. In this exam-ple, the general label is fracture. The specific labels are claviclefracture, humeral fracture, vertebral fracture, and rib and callusrib fractures. Therefore, we designed two experiments, the firstusing specific labels for classification and the second using moregeneral labels, each grouping one or more specific labels. Wethen set the minimum number of samples that each class musthave to be included in the classification system. The more generalclassification system has a larger number of classes that are moreheterogeneous, while the more specific classification system hasa smaller number of classes, but is more precise than the previousone.Table 3 shows the details of the two classification systems,the number of samples, the classes and the size of the training,validation and test sets. In the train/test/validation split we strat-ify the samples according to classes and patient id, which avoidsbiases and problems between subsets. In addition, to facilitatethe replicability and transparency of this article we will make thesplit available on the github in Section 4.2.2 https://github.com/auriml/Rx-thorax-automatic-captioning297Fig. 5. Distribution ofexperiment).the number oflabels per sample (specific labelsFig. 6. Label distribution (specific labels experiment).Label distribution: Specific labels. This experiment, as explained inSection 3.1, label selection, has a smaller number of samples andclasses than the second case, but the radiological signs are moreaccurate. In this experiment we used a total of 85367 samplesand 35 different classes. We can observe in Fig. 5, how morethan half of the samples present a single class; however, we canobserve that there are samples with a high number of classes,four of them presenting 12 different labels at the same time.This distribution of the samples is in line with expectations; thenumber of samples decreases as the number of labels per sampleincreases. In Fig. 6 we can see how the classes in this experimentare extremely imbalanced. Although there are 35 classes, the sixmajority classes account for 82.7% of the dataset. Only the normalclass, which is the majority class, accounts for 47.4% of the totalsamples, while the supra aortic elongation class, which is the leastrepresented class, accounts for only 0.28% of the total.labels.Label distribution: GeneralIn this experiment, differentclasses were unified according to the tree of terms proposedby the authors. Therefore, the number of classes and samplesis higher than in the first experiment. However, the radiologicalsigns used in the classification are less precise, so in the end 54classes and 90,687 samples were used. In Fig. 7 we can see howthe number of classes per sample is distributed in a very similarway to the previous case. However, we can see that there aresamples with 13 different labels, one more than in the previouscase. If we look at Fig. 8, we can see that the six majority classesrepresent 51.3% of the total, while the other 48 classes do notreach 50%. The majority class, as in the previous case, is thenormal class. This class accounts for 22.6% of the total while theminority class, vascular redistribution, accounts for only 0.13% ofH. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306have been able to learn, EfficientNet and DenseNet, while the restof the models were not able to learn and presented a flat trainingcurve with an AUC of 0.5. As expected, the ensemble does notwork correctly, and therefore the preprocessing step is necessary.Tables 6 and 7 show the results training with segmentation-based cropping but without applying data augmentation tech-niques. At first, it is interesting that Inception does not learn,possibly because it is not able to generalise correctly without dataaugmentation techniques. InceptionResNet has the best resultsin most classes, but EfficientNet achieves the best overall result,achieving an AUC of 0.792 while InceptionResNet scores 0.779.Comparing Table 8 with these results shows that the applicationof data augmentation techniques improves the system perfor-mance. If we focus on the results for the different ensembles,we can see that for all labels, the CTP technique performs betterthan the two PTC methods. CTP also performs better than theindividual models except in three cases: in one case it equalsthem, and in two cases it performs worse). We can conclude thatdata augmentation improves the performance of the system.5.2. Performance analysis of CNN modelsThe first step is the comparison of the different architecturesexplained in Section 3.3. They are used as a baseline to comparethe ensemble system. As explained in Section 3, we consider twotypes of classification problems: the first uses the original labelsproposed by the authors of the dataset (‘‘specific labels’’), andthe second uses general labels constructed by grouping specificlabels. In the first problem, a finer-grained classification is per-formed, but it contains a small number of labels, 35, as many ofthe original 144 do not pass the filter of the minimum numberof samples (200). In the second problem, general radiologicalpatterns are classified, but there is a larger number of labels, 54,because when grouping labels there are a larger number of classessatisfying the minimum threshold of 200 samples.Tables 8 and 9 show the results obtained by applying theproposed methodology for the first case study (classification us-ing specific labels). The model with the best global AUC value isDenseNet, followed by EfficientNet, with 0.818 and 0.804 respec-tively. The other models (Inception, InceptionResNet and Xcep-tion) do not achieve an AUC = 0.8. These results are brokendown by class. First of all, we can observe that the labels withfewer samples do not show worse results on average than theclasses with more samples, which means that we have managedto overcome the data imbalance problems of. It can also be seen inthe table that some models perform better with majority classes,such as Inception; others achieve the best results for minorityclasses, such as EfficientNet and Xception. However, DenseNet201 and InceptionResNet perform well in both cases.Secondly, we have analysed the results obtained with theensemble techniques, using the individual models as baselines.Interestingly, only the CTP technique improves the individualmodels, as is also the case in Table 6. If we focus on this ensembletechnique, we can see that there are two classes, Pleural effusionand pacemaker, where the results of the individual models arenot improved. These two classes have 658 and 336 samplesrespectively, i.e. they are not majority classes, so one hypothesiswould be that the ensemble performs worse in minority classes.However, the number of labels for which the ensemble does notoutperform the individual models is very small compared to thetotal. Furthermore, the ensemble achieves an AUC above 0.85 formore than 40% of the labels, which is higher than expected. Sincewe can observe that the ensemble achieves an AUC higher than0.9 for classes such as hemidiaphragm elevation, hiatal hernia, orsternotomy, all of them with less than 300 samples, we concludethat class imbalance does not affect our system significantly. Con-sidering that the model is trained for 35 different classes, reachingFig. 7. Distribution ofexperiment).the number oflabels per sample (generallabelsFig. 8. Label distribution (general labels experiment).the dataset. This shows that even if we group the radiologicalsigns into higher classes, the dataset is very imbalanced.4.2. Execution environment and Github repositoryAll experiments have been run on a 24 GB Nvidia GeForceRTX 3090. The main packages used in these experiments are thefollowing: Tensorflow [83], Scikit-Learn [84] and openCV [85].The code developed in our work is publicly available at GitHub.35. Experimental resultsThis section describes the results obtained with the proposedmethodology and evaluates its performance on a multilabel andimbalanced problem, the PadChest dataset. We considered twostrategies for the classes: directly using the labels proposed by thedataset creators, or grouping them into more generic classes thatencompass similar radiological signs. First, we checked whetherpreprocessing improves the ensemble performance. Next, wechecked the performance of both the individual models and theensemble, and analyse the quality of the visualisations based onexplainable AI techniques. To measure the performance of thedifferent models, we have used three metrics suitable for multi-label problems: Area Under the Curve (AUC), Hamming Loss andF-measure [86].5.1. Impact of preprocessing techniquesFirst, we trained the models with the images withoutsegmentation-based cropping or data augmentation. The resultsobtained, Tables 4 and 5, show that only two individual models3 https://github.com/helenalizlopez/multilabelimbalancedchestxraydataset298H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306Table 4Specific labels experiment: results obtained by training the models without segmentation-based cropping or data augmentation. For each label, the individual modelswith the best performance and the ensembles that outperform all individual models are marked in bold. The best ensemble result is marked in italics unless it tiesthe random classifier.# Samples DenseNetEfficientNetInceptionInceptionResNetXceptionPTC-modePTC-lwCTPAUCF1AUCF1AUCF1AUCF1AUCF1AUCF1AUCF1AUCF134327Normal13419Copd signs8412Cardiomegaly1399Aorticelongation1311Unchanged1073Scoliosis873Chronic changes703Costophrenic angle blunting663Air trapping658Pleural effusion651Pneumonia594Interstitial pattern591Infiltrates578Laminar atelectasis575Vertebral degenerative526Kyphosis469Apical pleural thickening463Vascular hilar enlargement449Fibrotic band449Nodule388Calcified granuloma360Callus rib fracture336Pacemaker318Aortic atheromatosis294Volume loss292Sternotomy290Bronchiectasis287Hiatal hernia275Pseudonodule254Hemidiaphragm elevation248Alveolar pattern239Increased densityVertebral anterior compression 214210Suture material200Supra aortic elongation0.589 0.470 0.500 0.374 0.500 0.374 0.500 0.3740.500 0.457 0.500 0.457 0.500 0.457 0.500 0.4570.620 0.551 0.611 0.563 0.500 0.475 0.500 0.4750.538 0.509 0.553 0.526 0.500 0.479 0.500 0.4790.535 0.483 0.526 0.504 0.500 0.480 0.500 0.4800.500 0.484 0.550 0.522 0.500 0.484 0.500 0.4840.581 0.481 0.578 0.451 0.500 0.487 0.500 0.4870.556 0.525 0.541 0.532 0.500 0.490 0.500 0.4900.500 0.490 0.498 0.510 0.500 0.490 0.500 0.4900.655 0.573 0.656 0.567 0.500 0.490 0.500 0.4900.626 0.556 0.629 0.566 0.500 0.490 0.500 0.4900.597 0.544 0.582 0.547 0.500 0.491 0.500 0.4910.615 0.540 0.594 0.542 0.500 0.491 0.500 0.4910.500 0.491 0.508 0.491 0.500 0.491 0.500 0.4910.500 0.491 0.573 0.485 0.500 0.491 0.500 0.4910.602 0.558 0.538 0.520 0.500 0.492 0.500 0.4920.500 0.493 0.499 0.488 0.500 0.493 0.500 0.4930.584 0.510 0.587 0.475 0.500 0.493 0.500 0.4930.500 0.493 0.489 0.484 0.500 0.493 0.500 0.4930.500 0.493 0.500 0.493 0.500 0.493 0.500 0.4930.500 0.494 0.499 0.494 0.500 0.494 0.500 0.4940.500 0.495 0.500 0.495 0.500 0.495 0.500 0.4950.627 0.543 0.646 0.523 0.500 0.495 0.500 0.4950.500 0.495 0.616 0.457 0.500 0.495 0.500 0.4950.500 0.496 0.512 0.496 0.500 0.496 0.500 0.4960.530 0.517 0.539 0.506 0.500 0.496 0.500 0.4960.500 0.496 0.480 0.496 0.500 0.496 0.500 0.4960.500 0.496 0.533 0.506 0.500 0.496 0.500 0.4960.500 0.496 0.498 0.500 0.500 0.496 0.500 0.4960.515 0.496 0.531 0.496 0.500 0.496 0.500 0.4960.664 0.531 0.663 0.503 0.500 0.496 0.500 0.4960.528 0.513 0.536 0.502 0.500 0.496 0.500 0.4960.546 0.510 0.548 0.487 0.500 0.497 0.500 0.4970.500 0.497 0.542 0.509 0.500 0.497 0.500 0.4970.500 0.497 0.503 0.497 0.500 0.497 0.500 0.4970.500 0.374 0.500 0.374 0.500 0.374 0.589 0.3740.500 0.457 0.500 0.457 0.500 0.457 0.500 0.4570.500 0.475 0.500 0.475 0.500 0.475 0.633 0.4750.500 0.479 0.500 0.479 0.500 0.479 0.558 0.4790.500 0.480 0.500 0.480 0.500 0.480 0.543 0.4800.500 0.484 0.500 0.484 0.500 0.484 0.550 0.4840.500 0.487 0.500 0.487 0.500 0.487 0.585 0.4870.500 0.490 0.500 0.490 0.500 0.490 0.545 0.4900.500 0.490 0.500 0.490 0.500 0.490 0.498 0.4900.500 0.490 0.500 0.490 0.500 0.490 0.676 0.4900.500 0.490 0.500 0.490 0.500 0.490 0.645 0.4900.500 0.491 0.500 0.491 0.500 0.491 0.594 0.4910.500 0.491 0.500 0.491 0.500 0.491 0.612 0.4910.500 0.491 0.500 0.491 0.500 0.491 0.508 0.4910.500 0.491 0.500 0.491 0.500 0.491 0.573 0.4910.500 0.492 0.500 0.492 0.500 0.492 0.606 0.4920.500 0.493 0.500 0.493 0.500 0.493 0.499 0.4930.500 0.493 0.500 0.493 0.500 0.493 0.602 0.4930.500 0.493 0.500 0.493 0.500 0.493 0.489 0.4930.500 0.493 0.500 0.493 0.500 0.493 0.500 0.4930.500 0.494 0.500 0.494 0.500 0.494 0.499 0.4940.500 0.495 0.500 0.495 0.500 0.495 0.500 0.4950.500 0.495 0.500 0.495 0.500 0.495 0.663 0.4950.500 0.495 0.500 0.495 0.500 0.495 0.616 0.4950.500 0.496 0.500 0.496 0.500 0.496 0.512 0.4960.500 0.496 0.500 0.496 0.500 0.496 0.545 0.4960.500 0.496 0.500 0.496 0.500 0.496 0.480 0.4960.500 0.496 0.500 0.496 0.500 0.496 0.533 0.4960.500 0.496 0.500 0.496 0.500 0.496 0.498 0.4960.500 0.496 0.500 0.496 0.500 0.496 0.544 0.4960.500 0.496 0.500 0.496 0.500 0.496 0.695 0.4960.500 0.496 0.500 0.496 0.500 0.496 0.547 0.4960.500 0.497 0.500 0.497 0.500 0.497 0.559 0.4970.500 0.497 0.500 0.497 0.500 0.497 0.542 0.4970.500 0.497 0.500 0.497 0.500 0.497 0.504 0.497Global0.543 0.508 0.547 0.502 0.500 0.488 0.500 0.4880.500 0.488 0.500 0.488 0.500 0.488 0.558 0.488Table 5Specific labels experiment: global results obtained by the individual models and the ensemble without using segmentation-based cropping or data augmentationtechniques.DenseNetEfficientNetInceptionInceptionResNetXceptionPTC-modePTC-lwHamming LossAUCF10.0670.5430.5080.1070.5470.5020.0460.5000.4880.0460.5000.4880.0460.5000.4880.0460.5000.4880.0460.5000.488CTP0.0460.5580.488an imbalance between majority and minority classes of 1:172, wecan say that the performance of the system is sufficiently high,considering its characteristics.In the second case study used to validate the proposed metho-dology, we have grouped the different radiological signs intohigher level classes that are more general, as shown in the ex-ample of fracture types, Fig. 4. After this grouping, the number oflabels passing the minimum 200-sample filter rises to 54 (in thespecific labels experiment only 35 labels passed this threshold).Therefore, we now train the system with a larger number oflabels, which is closer to the reality of health centres. Regardingthe individual models, we can see that the best model is Effi-cientNet B0 followed by DenseNet, with an AUC of 0.767 and0.761, respectively. The rest of the models have a value lowerthan 0.75. Regarding the performance per class of each model, weobserve that Xception, EfficientNet and DenseNet perform betterin majority classes, while Inception and ResNet perform better inminority classes.If we look at the results obtained by the ensemble technique,as in the previous case, CTP is the best performer with an AUCof 0.819, which is an improvement of 0.052 over EfficientNet.There are four classes where the ensemble performs as well as thebest individual model, but there is no class where the individualmodels perform better than the ensemble. The number of labelswhere the ensemble achieves an AUC above 0.85 is slightly lowerthan in the previous case, 37%, but more than 50% of the classeshave an AUC greater than 0.8. This is interesting consideringthe number of classes (54) and their imbalance. Although theensemble performs well, it does not perform well for all classes.For example, with the class ‘‘Sclerotic bone lesion’’ it obtains anAUC close to 0.5.We can observe that in this case the ensemble further im-proves the individual models as the improvement over the bestindividual model is now high. The combination of different ar-chitectures avoids overfitting and improves the generalisationcapacity in a problem where classification is more difficult due tothe specificities of the dataset (high number of classes, multilabel,class imbalance). These results demonstrate that this methodol-ogy works well on highly imbalanced and multilabel datasets (seeTables 10 and 11).299H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306Table 6Specific labels experiment: results obtained by training the models with segmentation-based cropping, but without data augmentation. For each label, the individualmodels with the best performance and the ensembles that outperform all individual models are marked in bold. The best ensemble result is marked in italics.# Samples DenseNetEfficientNetInceptionInceptionResnetXceptionPTC-modePTC-lwCTPAUCF1AUCF1AUCF1AUCF1AUCF1AUCF1AUCF1AUCF134327Normal13419Copd signs8412Cardiomegaly1399Aortic elongation1311Unchanged1073Scoliosis873Chronic changes703Costophrenic angle blunting663Air trapping658Pleural effusion651Pneumonia594Interstitial pattern591Infiltrates578Laminar atelectasis575Vertebral degenerative526Kyphosis469Apical pleural thickening463Vascular hilar enlargement449Fibrotic band449Nodule388Calcified granuloma360Callus rib fracture336Pacemaker318Aortic atheromatosis294Volume loss292Sternotomy290Bronchiectasis287Hiatal hernia275Pseudonodule254Hemidiaphragm elevation248Alveolar patternIncreased density239Vertebral anterior compression 214210Suture material200Supra aortic elongation0.50.374 0.802 0.725 0.453 0.374 0.819 0.7230.777 0.682 0.785 0.672 0.500 0.457 0.799 0.6900.900 0.768 0.898 0.774 0.641 0.474 0.918 0.7670.863 0.700 0.874 0.686 0.500 0.479 0.875 0.7190.612 0.556 0.625 0.549 0.500 0.480 0.597 0.5490.823 0.678 0.808 0.690 0.500 0.484 0.830 0.7020.707 0.537 0.731 0.553 0.500 0.487 0.696 0.5470.810 0.698 0.837 0.691 0.500 0.489 0.842 0.6550.500 0.490 0.671 0.568 0.500 0.490 0.500 0.4900.925 0.839 0.942 0.818 0.479 0.046 0.943 0.7700.759 0.675 0.803 0.671 0.500 0.490 0.808 0.6550.799 0.638 0.795 0.650 0.500 0.491 0.813 0.6370.733 0.620 0.776 0.635 0.500 0.491 0.802 0.5970.500 0.491 0.806 0.639 0.500 0.491 0.754 0.6300.730 0.544 0.721 0.540 0.500 0.491 0.725 0.5640.796 0.611 0.813 0.644 0.500 0.492 0.794 0.6280.798 0.591 0.787 0.573 0.500 0.493 0.775 0.5690.679 0.562 0.741 0.506 0.500 0.493 0.717 0.5590.756 0.568 0.772 0.599 0.500 0.493 0.767 0.6080.616 0.557 0.677 0.567 0.500 0.493 0.688 0.5470.741 0.651 0.752 0.641 0.500 0.494 0.757 0.6220.682 0.600 0.773 0.594 0.500 0.495 0.500 0.4950.996 0.948 0.996 0.945 0.500 0.495 0.996 0.9460.812 0.538 0.810 0.542 0.500 0.495 0.791 0.5670.855 0.687 0.862 0.717 0.500 0.496 0.882 0.6770.991 0.945 0.991 0.939 0.500 0.496 0.993 0.8720.673 0.593 0.726 0.597 0.500 0.496 0.719 0.5870.912 0.852 0.920 0.826 0.500 0.496 0.939 0.8430.632 0.524 0.705 0.547 0.500 0.496 0.536 0.5140.902 0.706 0.879 0.697 0.500 0.496 0.911 0.6960.791 0.626 0.834 0.603 0.500 0.496 0.853 0.5800.580 0.551 0.586 0.521 0.500 0.496 0.619 0.5210.640 0.536 0.645 0.530 0.500 0.497 0.644 0.5370.798 0.663 0.791 0.649 0.500 0.497 0.824 0.6280.697 0.569 0.778 0.564 0.500 0.497 0.832 0.5610.50.374 0.528 0.444 0.500 0.374 0.806 0.3740.777 0.682 0.538 0.534 0.648 0.676 0.825 0.6740.917 0.762 0.596 0.628 0.814 0.792 0.938 0.7950.837 0.705 0.594 0.623 0.767 0.724 0.898 0.7240.602 0.544 0.506 0.495 0.531 0.539 0.642 0.5370.500 0.484 0.591 0.628 0.674 0.711 0.863 0.7080.695 0.538 0.515 0.518 0.625 0.568 0.738 0.5680.810 0.704 0.558 0.587 0.729 0.713 0.884 0.7120.688 0.553 0.508 0.506 0.500 0.490 0.705 0.4900.927 0.838 0.818 0.542 0.901 0.823 0.942 0.8250.806 0.657 0.572 0.603 0.704 0.691 0.851 0.6920.812 0.615 0.562 0.576 0.714 0.678 0.858 0.6800.771 0.627 0.563 0.583 0.668 0.639 0.831 0.6390.745 0.646 0.560 0.587 0.572 0.607 0.837 0.6070.718 0.533 0.571 0.560 0.620 0.568 0.771 0.5680.813 0.615 0.569 0.585 0.683 0.664 0.860 0.6640.758 0.575 0.574 0.567 0.701 0.619 0.838 0.6190.715 0.547 0.531 0.533 0.596 0.578 0.771 0.5820.758 0.593 0.573 0.585 0.688 0.636 0.813 0.6380.626 0.558 0.535 0.545 0.566 0.574 0.719 0.5720.689 0.611 0.578 0.601 0.645 0.654 0.819 0.6560.497 0.495 0.529 0.543 0.500 0.495 0.799 0.4950.996 0.949 0.741 0.799 0.992 0.951 0.996 0.9510.742 0.577 0.544 0.545 0.672 0.605 0.852 0.6070.830 0.691 0.560 0.581 0.762 0.729 0.910 0.7310.993 0.918 0.756 0.814 0.983 0.948 0.996 0.9480.725 0.576 0.541 0.563 0.594 0.613 0.784 0.6140.945 0.726 0.747 0.784 0.877 0.870 0.962 0.8720.639 0.540 0.530 0.536 0.540 0.544 0.718 0.5450.891 0.687 0.749 0.709 0.816 0.751 0.951 0.7510.810 0.604 0.568 0.579 0.715 0.621 0.887 0.6220.569 0.526 0.501 0.500 0.533 0.537 0.634 0.5390.623 0.517 0.516 0.522 0.524 0.524 0.702 0.5250.786 0.665 0.622 0.622 0.742 0.679 0.833 0.6800.738 0.554 0.579 0.574 0.613 0.577 0.861 0.578Global0.751 0.633 0.792 0.648 0.502 0.475 0.779 0.6360.750 0.622 0.584 0.586 0.677 0.650 0.831 0.651Table 7Specific labels experiment: global results obtained by the individual models and the ensemble with preprocessing (segmentation-based cropping) but without dataaugmentation.Densenet201EfficientNetInceptionInceptionResnetXceptionPTC-modePTC-lwHamming LossAUCF1-score0.0770.7510.6330.0790.7920.6480.0720.5020.4750.0700.7790.6360.0770.7500.6220.0560.5840.5860.0570.6770.650CTP0.0570.8310.6515.3. Visual explanation using heatmapsAs explained in Section 2, the visualisation of multilabel prob-lems is an essential element for this methodology, but it is nota simple problem. Most of the work in this field has deficien-cies. Therefore, we have developed a technique that for eachlabel generates a heatmap, an estimated probability, and theensemble agreement. In Fig. 9, we can see the original X-ray andthe heatmaps of the different classes. The areas marked on theradiographs match the radiological signs, and the probabilities arehigh, with three of the four cases showing agreement between allmodels.In the second example, Fig. 10, we can see that the classprobabilities are lower than before. The class Atelectasis has anagreement of three models and a low probability (0.583), whichmeans that the physician should be careful with this label. Thelast example, Fig. 11, belongs to the normal class. In this case,the heat map marks approximately the entire radiograph, as itscans the whole image for radiological signs. The performanceof the visualisations is highly dependent on the performance ofthe model: if the model is better, the visualisations will be moreaccurate, and the probability and agreement between modelswill be higher. An advantage of this technique over the stateof the art is that we generate a grad-CAM map for each signthat includes the probability generated by the system and theagreement between the models of the ensemble.6. DiscussionAs mentioned throughout the article, the PadChest dataset hasa high quality and is really interesting due to the number ofclasses, which is higher than other multilabel datasets, and thechallenge of class imbalance. Although we can find numerouspapers using this dataset for medical report generation, it isunderutilised in chest X-ray classification problems, which makesthe available works for comparison scarce. Moreover, those arti-cles present several problems that complicate an adequate com-parison of our work. Therefore, one of our aims is to generate amethodologically correct baseline that allows comparison for fu-ture work. For this purpose, we have conducted two experiments:in the first one we have used the specific radiological signs, i.e. theoriginal ones from the dataset, while in the second we have usedmore generic radiological signs from a tree of terms provided bythe authors of the dataset. In Table 12 we can find a summary of300H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306Table 8Specific labels experiment: results obtained with by training the models with segmentation-based cropping and data augmentation. For each label, the individualmodels with the best performance and the ensembles that outperform all individual models are marked in bold. The best ensemble result is marked in italics.# Samples Densenet201EfficientNetInceptionInceptionResnetXceptionPTC-modePTC-lwCTPAUCF1AUCF1AUCF1AUCF1AUCF1AUCF1AUCF1AUCF134327Normal13419Copd signs8412Cardiomegaly1399Aortic elongation1311Unchanged1073Scoliosis873Chronic changes703Costophrenic angle blunting663Air trapping658Pleural effusion651Pneumonia594Interstitial pattern591InfiltratesLaminar atelectasis578Vertebral degenerative changes 575526Kyphosis469Apical pleural thickening463Vascular hilar enlargement449Fibrotic band449Nodule388Calcified granuloma360Callus rib fracture336Pacemaker318Aortic atheromatosis294Volume loss292Sternotomy290Bronchiectasis287Hiatal hernia275Pseudonodule254Hemidiaphragm elevation248Alveolar patternIncreased density239Vertebral anterior compression 214210Suture material200Supra aortic elongation0.820 0.7220.823 0.6810.927 0.7730.885 0.6900.636 0.5530.759 0.6360.759 0.5180.862 0.6740.692 0.5570.959 0.8270.815 0.6710.834 0.6250.812 0.6290.843 0.6700.779 0.5450.867 0.6400.808 0.5530.746 0.5490.831 0.5830.706 0.5780.808 0.6530.717 0.6060.993 0.9270.856 0.5210.917 0.6930.992 0.8980.801 0.5490.939 0.8560.612 0.4960.893 0.6670.876 0.6270.643 0.5410.749 0.5320.819 0.6520.865 0.5760.811 0.716 0.832 0.727 0.820 0.7090.785 0.644 0.816 0.678 0.815 0.6750.907 0.749 0.926 0.767 0.927 0.7770.846 0.655 0.882 0.676 0.888 0.6980.614 0.543 0.638 0.549 0.642 0.5440.712 0.598 0.745 0.605 0.732 0.6020.720 0.549 0.768 0.546 0.762 0.5190.845 0.674 0.832 0.662 0.831 0.6650.687 0.560 0.515 0.490 0.469 0.4900.951 0.811 0.956 0.823 0.955 0.8300.821 0.660 0.810 0.663 0.821 0.6720.828 0.636 0.846 0.651 0.843 0.6160.803 0.617 0.803 0.626 0.815 0.6330.812 0.637 0.827 0.643 0.827 0.6540.730 0.544 0.774 0.546 0.785 0.5180.834 0.589 0.845 0.587 0.849 0.6090.801 0.568 0.789 0.573 0.509 0.4930.742 0.568 0.769 0.522 0.755 0.5440.809 0.600 0.813 0.614 0.575 0.4930.675 0.554 0.561 0.493 0.574 0.4930.802 0.649 0.542 0.494 0.554 0.4940.765 0.557 0.614 0.495 0.609 0.4950.997 0.942 0.996 0.919 0.996 0.9310.847 0.516 0.862 0.550 0.852 0.5410.902 0.657 0.904 0.636 0.896 0.6720.987 0.920 0.990 0.926 0.995 0.9360.775 0.561 0.796 0.578 0.805 0.5620.941 0.697 0.947 0.824 0.964 0.8010.670 0.550 0.598 0.496 0.589 0.4960.882 0.679 0.915 0.670 0.894 0.7020.877 0.589 0.885 0.607 0.895 0.6060.641 0.509 0.651 0.534 0.633 0.5260.696 0.524 0.736 0.517 0.743 0.5270.820 0.663 0.818 0.639 0.811 0.6620.821 0.541 0.880 0.546 0.882 0.5630.827 0.732 0.725 0.731 0.725 0.731 0.837 0.7300.800 0.666 0.588 0.610 0.647 0.675 0.833 0.6720.922 0.779 0.746 0.765 0.825 0.789 0.937 0.7910.888 0.702 0.690 0.682 0.777 0.705 0.894 0.7070.636 0.547 0.531 0.537 0.545 0.551 0.641 0.5510.774 0.661 0.630 0.637 0.671 0.659 0.793 0.6640.752 0.533 0.621 0.556 0.684 0.545 0.772 0.5470.855 0.663 0.685 0.676 0.739 0.693 0.877 0.6930.506 0.490 0.525 0.532 0.500 0.490 0.704 0.4900.945 0.816 0.862 0.822 0.886 0.839 0.967 0.8400.821 0.668 0.681 0.668 0.703 0.687 0.850 0.6870.830 0.613 0.727 0.651 0.743 0.650 0.858 0.6510.808 0.639 0.649 0.635 0.662 0.644 0.840 0.6460.833 0.654 0.666 0.658 0.690 0.677 0.858 0.6780.779 0.547 0.627 0.560 0.670 0.557 0.797 0.5560.839 0.625 0.691 0.617 0.736 0.639 0.870 0.6400.500 0.493 0.592 0.574 0.661 0.619 0.830 0.6210.745 0.515 0.618 0.556 0.651 0.562 0.783 0.5630.806 0.611 0.641 0.604 0.716 0.658 0.848 0.6590.551 0.493 0.518 0.526 0.500 0.493 0.704 0.4930.496 0.494 0.572 0.593 0.500 0.494 0.833 0.4940.571 0.495 0.550 0.549 0.500 0.495 0.787 0.4950.984 0.926 0.984 0.930 0.993 0.946 0.997 0.9460.871 0.559 0.739 0.556 0.786 0.558 0.885 0.5570.902 0.640 0.789 0.670 0.809 0.693 0.928 0.6970.992 0.865 0.961 0.912 0.984 0.941 0.997 0.9410.794 0.573 0.682 0.580 0.690 0.587 0.820 0.5880.947 0.851 0.871 0.786 0.876 0.867 0.967 0.8670.545 0.496 0.536 0.543 0.500 0.496 0.672 0.4960.898 0.684 0.759 0.692 0.784 0.725 0.934 0.7240.871 0.594 0.748 0.612 0.774 0.623 0.911 0.6220.668 0.535 0.545 0.531 0.547 0.544 0.673 0.5470.734 0.535 0.573 0.530 0.597 0.540 0.752 0.5390.822 0.612 0.759 0.663 0.768 0.685 0.847 0.6840.857 0.562 0.628 0.558 0.681 0.576 0.894 0.575Global0.818 0.6420.804 0.629 0.797 0.625 0.780 0.6210.782 0.625 0.677 0.637 0.701 0.647 0.840 0.647Table 9Specific labels experiment: global results obtained from the individual models and the ensembles.Densenet201EfficientNetInceptionInceptionResnetXceptionPTC-modePTC-lwHamming LossAUCF10.0820.8180.6420.0780.8040.6290.0810.7970.6250.0770.7800.6210.0740.7820.6250.0630.6770.6370.0650.7010.647CTP0.0650.8400.647Fig. 9. First visualisation example. The heatmaps of four radiological signs detected (cardiomegaly, pacemaker, sternotomy and suture material) are shown. The titleshows the label, the probability estimated by the ensemble, and the agreement between the models of the ensemble. The areas of interest for classification aremarked in blue.the different published systems and their global and class specificperformance. First of all, it is interesting to note how most of thepapers have selected different labels to perform the classification,and all papers, except [61], select a low number of total classescompared to the number of classes available.In the case of Rimeika et al. [87], the publication does not showhow the two models have been built; it does not provide infor-mation on the architecture, the other dataset used, or the criteriafor selecting the classes from PadChest dataset, so there is nopossibility to replicate these models, and therefore we cannot useit for comparison. In Pooch et al. [62], the PadChest classes havebeen adapted to match the classes of other multilabel datasetssuch as ChestX-ray14 and CheXpert. For example, regardless ofthe fact that the class ‘‘Lesion’’ does not exist in two of thedatasets, they generated this class using the ChestX-ray14 labels‘‘Nodules’’ and ‘‘Masses’’. However, PadChest was processed in301H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306Table 10General labels experiment: results obtained by training the models with segmentation-based cropping and data augmentation. For each label, the individual modelswith the best performance and the ensembles that outperform all individual models are marked in bold. The best ensemble result is marked in italics.# Samples Densenet201EfficientNetInceptionInceptionResnetXceptionPTC-modePTC-lwCTPAUCF1AUCF1AUCF1AUCF1AUCF1AUCF1AUCF1AUCF134327Normal13419Copd signs8120Cardiomegaly7778Thoracic cage deformation7436Aortic elongation6706Infiltrates6487Unchanged4312Chronic changes3928Surgery3565Atelectasis3306Costophrenic angle bluntingCalcified densities3253Vertebral degenerative changes 32033162Hilar enlargement3010Pleural thickening2813Mediastinal enlargement2765Air trapping2529Fracture2436Pleural effusion2306Granuloma1936Nodule1781Fibrotic band1772Electrical device1652Pneumonia1581Aortic atheromatosis1451Pseudonodule1430Bronchiectasis1362Hiatal hernia1231Hemidiaphragm elevation1133Increased density757Diaphragmatic eventration684Volume loss659Adenopathy602Bronchovascular markings574Mass562Artificial heart valve545Catheter544Suboptimal study523Pulmonary fibrosis520Heart insufficiency476Hypoexpansion437Gynecomastia410Emphysema352Sclerotic bone lesion336Fissure thickening318Hilar congestion318Osteopenia299Tuberculosis290Bullas272Hyperinflated lung243Cavitation212Mediastinic lipomatosis210Pneumothorax204Vascular redistribution0.735 0.6850.771 0.6290.899 0.7360.706 0.6030.858 0.6910.794 0.6860.630 0.5380.759 0.5480.813 0.7300.798 0.6280.845 0.6550.719 0.6380.744 0.5020.755 0.5490.753 0.5860.795 0.6430.654 0.5280.749 0.6630.942 0.7380.500 0.4930.653 0.5830.738 0.5300.992 0.9590.804 0.5940.834 0.5020.693 0.5610.795 0.5440.916 0.8130.814 0.6510.633 0.4970.500 0.4980.802 0.5420.500 0.4980.712 0.5700.707 0.6210.969 0.6580.871 0.7400.743 0.5240.850 0.5840.875 0.5410.838 0.5410.852 0.5270.780 0.5080.506 0.5110.816 0.5330.785 0.5030.659 0.5080.852 0.5340.746 0.5200.715 0.5060.780 0.5560.648 0.4990.705 0.5720.774 0.4990.707 0.652 0.750 0.691 0.723 0.6580.761 0.649 0.771 0.618 0.793 0.6650.904 0.746 0.890 0.723 0.892 0.7510.728 0.627 0.500 0.478 0.675 0.5950.853 0.690 0.842 0.661 0.866 0.6830.802 0.664 0.791 0.663 0.797 0.6680.636 0.552 0.618 0.543 0.633 0.5450.754 0.542 0.752 0.525 0.734 0.5200.815 0.766 0.750 0.722 0.766 0.7130.756 0.636 0.698 0.570 0.729 0.5960.807 0.638 0.758 0.604 0.784 0.6380.751 0.639 0.500 0.491 0.500 0.4910.726 0.528 0.676 0.497 0.733 0.4870.732 0.544 0.699 0.551 0.738 0.5330.773 0.585 0.737 0.572 0.743 0.5250.798 0.668 0.774 0.688 0.778 0.6750.669 0.536 0.500 0.492 0.665 0.4950.725 0.599 0.50.493 0.640 0.5070.927 0.782 0.930 0.720 0.935 0.7350.777 0.652 0.500 0.493 0.500 0.4930.679 0.571 0.617 0.542 0.643 0.5470.747 0.531 0.712 0.522 0.500 0.4950.992 0.913 0.992 0.889 0.994 0.8710.790 0.567 0.804 0.549 0.813 0.5770.830 0.540 0.813 0.477 0.840 0.5240.727 0.553 0.500 0.496 0.500 0.4960.776 0.571 0.789 0.548 0.814 0.5390.892 0.796 0.906 0.773 0.918 0.8040.841 0.680 0.841 0.596 0.823 0.6490.640 0.524 0.596 0.492 0.609 0.5090.775 0.586 0.500 0.498 0.500 0.4980.776 0.580 0.809 0.531 0.814 0.5130.697 0.538 0.500 0.498 0.548 0.5200.738 0.537 0.777 0.576 0.765 0.5450.715 0.608 0.744 0.570 0.732 0.5740.953 0.730 0.975 0.696 0.977 0.7270.874 0.721 0.866 0.673 0.878 0.6390.693 0.510 0.754 0.522 0.697 0.5310.834 0.587 0.837 0.551 0.864 0.5770.877 0.555 0.896 0.546 0.884 0.5380.745 0.545 0.846 0.534 0.768 0.5710.810 0.552 0.852 0.501 0.858 0.5070.715 0.520 0.801 0.512 0.809 0.5060.500 0.499 0.500 0.499 0.500 0.4990.802 0.573 0.819 0.518 0.842 0.5260.798 0.519 0.790 0.514 0.827 0.5190.688 0.507 0.659 0.483 0.695 0.4660.861 0.567 0.869 0.561 0.824 0.5590.685 0.532 0.739 0.524 0.715 0.5120.630 0.502 0.719 0.504 0.645 0.4850.834 0.575 0.856 0.546 0.789 0.5390.654 0.551 0.50.499 0.500 0.4990.717 0.530 0.717 0.540 0.721 0.5180.752 0.526 0.705 0.508 0.694 0.5160.732 0.674 0.702 0.693 0.690 0.691 0.770 0.6910.779 0.645 0.596 0.621 0.615 0.645 0.816 0.6400.898 0.741 0.766 0.747 0.816 0.760 0.923 0.7620.708 0.609 0.577 0.586 0.601 0.612 0.745 0.6140.866 0.687 0.701 0.690 0.758 0.697 0.886 0.7000.794 0.663 0.703 0.676 0.721 0.690 0.827 0.6920.631 0.552 0.536 0.543 0.536 0.546 0.652 0.5470.740 0.522 0.656 0.567 0.685 0.543 0.768 0.5450.829 0.724 0.726 0.739 0.739 0.762 0.845 0.7650.759 0.628 0.587 0.598 0.647 0.632 0.804 0.6300.828 0.652 0.660 0.634 0.700 0.656 0.864 0.6580.500 0.491 0.520 0.527 0.500 0.491 0.764 0.4910.730 0.512 0.643 0.532 0.664 0.514 0.751 0.5140.731 0.538 0.618 0.562 0.649 0.560 0.765 0.5650.763 0.562 0.651 0.587 0.671 0.586 0.790 0.5870.822 0.657 0.705 0.689 0.710 0.697 0.841 0.7000.672 0.536 0.520 0.523 0.602 0.544 0.692 0.5460.732 0.611 0.574 0.590 0.579 0.615 0.792 0.6170.937 0.771 0.878 0.762 0.900 0.775 0.956 0.7750.500 0.493 0.513 0.519 0.500 0.493 0.777 0.4930.622 0.575 0.560 0.569 0.558 0.575 0.707 0.5730.727 0.519 0.606 0.546 0.654 0.556 0.770 0.5560.992 0.929 0.990 0.935 0.992 0.942 0.997 0.9420.799 0.599 0.712 0.602 0.728 0.606 0.854 0.6070.843 0.519 0.713 0.554 0.769 0.522 0.866 0.5230.708 0.562 0.557 0.555 0.576 0.589 0.759 0.5930.779 0.561 0.639 0.575 0.696 0.573 0.833 0.5740.927 0.788 0.857 0.810 0.889 0.852 0.959 0.8520.811 0.645 0.733 0.670 0.746 0.683 0.890 0.6830.606 0.511 0.539 0.516 0.533 0.514 0.661 0.5150.500 0.498 0.525 0.534 0.500 0.498 0.775 0.4980.776 0.560 0.728 0.561 0.761 0.564 0.865 0.5640.583 0.543 0.500 0.498 0.521 0.528 0.715 0.5280.704 0.585 0.685 0.592 0.703 0.585 0.802 0.5840.746 0.616 0.700 0.615 0.707 0.641 0.806 0.6410.972 0.713 0.941 0.736 0.968 0.730 0.981 0.7310.861 0.717 0.799 0.724 0.848 0.773 0.905 0.7730.727 0.506 0.666 0.526 0.681 0.539 0.784 0.5400.862 0.565 0.795 0.577 0.810 0.591 0.892 0.5910.870 0.547 0.819 0.553 0.856 0.551 0.920 0.5510.500 0.499 0.651 0.556 0.677 0.571 0.900 0.5730.806 0.550 0.772 0.540 0.825 0.554 0.917 0.5550.724 0.521 0.684 0.529 0.732 0.524 0.862 0.5250.500 0.499 0.500 0.499 0.500 0.499 0.506 0.4990.798 0.539 0.746 0.547 0.806 0.557 0.891 0.5580.808 0.520 0.734 0.526 0.756 0.523 0.896 0.5220.701 0.497 0.611 0.500 0.647 0.500 0.752 0.5000.805 0.597 0.760 0.577 0.848 0.592 0.909 0.5920.651 0.549 0.667 0.543 0.714 0.547 0.777 0.5470.659 0.501 0.649 0.513 0.658 0.512 0.728 0.5120.823 0.590 0.679 0.555 0.823 0.585 0.934 0.5850.500 0.499 0.520 0.514 0.500 0.499 0.681 0.4990.620 0.592 0.596 0.546 0.630 0.572 0.847 0.5730.667 0.507 0.635 0.514 0.676 0.519 0.837 0.519Global0.761 0.5890.767 0.600 0.732 0.566 0.739 0.5720.739 0.589 0.669 0.594 0.696 0.601 0.819 0.602Table 11General labels experiment: global results obtained by the individual models and the ensembles.Densenet201EfficientNetInceptionInceptionResnetXceptionPTC-modePTC-lwHamming LossAUCF1-score0.0700.7610.5890.0650.7670.6000.0700.7320.5660.0750.7390.5720.0650.7390.5890.0520.6690.5940.0570.6960.601CTP0.0560.8190.602that paper by unifying all classes related to Atelectasis, withoutproviding any medical explanation for this decision, so the labelsdo not match ours, containing only 8 out of 174 classes available.Therefore, it cannot be compared with our methodology. In thecase of Hashir et al. [61], they first select a single sample fromeach patient and the authors have used 32 different labels, whichis not in line with expectations, since using a lower thresholdthan ours there should have a larger number of labels. In this case,we can compare the overall AUC of the system. These authorsachieve an AUC of 0.800 using 32 labels while we obtain 0.8397using 35 specific labels. Therefore, we have achieved a better AUCthan in Hashir et al. [61]. Because of the above reasons, comparingour methodology with the state of the art is really difficult, andtherefore one of the aims of this paper is to create a baseline302H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306Fig. 10. Second visualisation example. The heatmaps of three radiological signs detected (pleural effusion, laminar atelectasis and suture material) are shown. Thetitle shows the label, the probability estimated by the ensemble, and the agreement between the models of the ensemble. The areas of interest for classification aremarked in blue.classes. Therefore, we can see that our system performs well andthat although it works with a much larger number of labels, itoutperforms some of the published models.7. Conclusions and future workThis paper proposes a Deep Learning methodology for clas-sification tasks with imbalanced multilabel datasets. We havebuilt with this methodology an ensemble of five state-of-the-artarchitectures: DenseNet-201, EfficientNet B0, Inception, Incep-tionResNet and Xception. We have used weighted crossentropywith logit loss to alleviate data imbalance and developed a newtechnique for generating heatmaps in multilabel classificationproblems.The results of our experiments are promising. First, in contrastto state-of-the-art papers, we have established a methodologi-cally sound baseline for future work, regardless of whether spe-cific or general labels are used. It will also allow us to analyse theperformance of these models when the number of labels varies.Our system obtains high AUC values for the number of classesused. In the case of specific labels, high performance is achievedwith an AUC of 0.84. In the case of general labels, we obtain anAUC of 0.819. This value may be due to the fact that the generalclassification has more classes and each of them is composedof different radiological signs. Thus, the variability is high andit is more difficult to classify. The results of the visualisationtechnique show a great potential, as it allows a view of the wholeradiograph that differentiates the different pathological signs.This technique generates a report that includes the visualisationof the heatmap, the probability produced by the system and theagreement between the ensemble models.There are several ways to improve our methodology. First,other strategies can be used to alleviate data imbalance, suchas adding new samples to the dataset. This can be done eitherby obtaining new images from other datasets such as CheXpert,ChestX-ray14, or other single disease datasets, or by creatingthem with generative adversarial networks (GANs). Another wayto improve the performance of the proposed system is to usedifferent X-ray views of each sample.In our proposal, we used segmentation techniques to force themodel to pay more attention to the most relevant areas. However,different techniques have recently been developed for this samepurpose. For example, [88] used soft and hard attention mecha-nisms to prevent the model from focusing on areas that are notrelevant to the problem. Another way to remove non-interestingareas is the application of semi-supervised learning methods tolocate and distinguish different anatomical regions [89]. Based onFig. 11. Third visualisation example. The sample belongs to the normal class.The title shows the label, the probability estimated by the ensemble, and theagreement between the models of the ensemble. The areas of interest forclassification are marked in blue.Table 12Comparative table of the different state-of-the-art models, their global and classperformance.Rimeika G. et al. [87]Pooch, E. H. [62]model1model2cardiomegalynodulenormalpleural effusionpneumonialobar collapseedemasubcutaneous emphysemaconsolidationpneumothoraxtuberculosisLymphadenopathylinear atelectasislymph node calficiationcongestionWidened mediastinummasslesionGlobal90.36%74.97%–95.42%–88.86%95.35%98.52%87.39%89.95%92.62%77.11%84.16%82.64%85.39%75.02%86.90%–86.98%91.94%71.42%–94.93%–86.39%96.05%93.79%85.50%88.19%92.40%75.81%78.26%72.69%87.29%77.50%82.29%–84.97%90.75%–87.10%–79.90%–91.07%–86.07%82.76%––76.41%––––69.75%82.98%to facilitate the comparison of future work with this dataset. Ifwe look at the overall AUC of the published models trained withPadChest and compare them with ours, we see that we onlyoutperform two models, but we use a much higher number of303H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306these recent advances, it would be interesting to study whetherincluding them in our model improves its results.To improve the visualisation technique, we can extend thedisplayed information by including a heatmap that shows thestandard deviation of the visualisation of the ensemble [90]. Thiswould help medical staff to know in which areas of the heatmapthere is more uncertainty. Another line of work we would liketo explore is the generation of a system that returns generaland specific labels. In addition to a combination with reportgeneration techniques, doctors would receive a report explainingthe different radiological signs and a visual interpretation of thesesigns. This could be done using cascade models, which first clas-sify the most general labels and later classify the subcategories.This would allow to include minority classes, or at least part ofthem.Another possible improvement would be to retrain the systemusing feedback from experts in the field on the system’s predic-tions and heatmaps. This poses multiple challenges in practice,mainly due to the need to implement close collaboration betweenspecialised diagnostic models and medical staff, who may lackbackground and expertise to rely on the models’ results. Onthe positive side, semi-supervised Deep Learning techniques areemerging lately and are yielding results that are unprecedentedin the state of the art. For instance, Avilés-Rivero et al. [91]have developed a semi-supervised graph-based framework forclassifying lung diseases (COVID-19, pneumonia and healthy).Such frameworks have been identified as promising for support-ing the construction of human-in-the-loop models in medicalapplications [30], hence future efforts will be devoted in thisdirection.CRediT authorship contribution statementLiz:Conceptualization, Visualization, Writing,HelenaMethodology.Javier Huertas-Tato: Conceptualization, Visu-alization, Validation, Methodology, Writing. Manuel Sánchez-Montañés: Conceptualization, Writing – review & editing.Javier Del Ser: Conceptualization, Writing – review & editing.DavidResources,Funding acquisition, Supervision.Conceptualization, Writing,Camacho:S2018/TCS-4566 (CYNAMON) grant. M. Sánchez-Montañés hasbeen supported by grants PID2021-127946OB-I00 and PID2021-122347NB-I00 (funded by MCIN/AEI/ 10.13039/501100011033and ERDF - ‘‘A way of making Europe’’) and Comunidad Autónomade Madrid, Spain (S2017/BMD-3688 MULTI-TARGET&VIEW-CMgrant). J. Del Ser thanks the financial support of the SpanishCentro para el Desarrollo Tecnológico Industrial (CDTI, Ministryof Science and Innovation) through the ‘‘Red Cervera’’ Programme(AI4ES project), as well as the support of the Basque Government(consolidated research group MATHMODE, ref. IT1456-22)References[1] E. Moustaka, T.C. Constantinidis, Sources and effects of work-related stressin nursing, Health Sci. J. 4 (4) (2010) 210.[2] S. Domínguez-Rodríguez, H. Liz, A. Panizo, Á. Ballesteros, R. Dagan, D.Greenberg, L. Gutiérrez, P. Rojo, E. Otheo, J.C. Galán, et al., Testing theperformance, adequacy, and applicability of an artificial intelligent modelfor pediatric pneumonia diagnosis, 2022.[3] N. Shaw, M. Hendry, O. Eden, Inter-observer variation in interpretation ofchest X-rays, Scott. Med. J. 35 (5) (1990) 140–141.[4] K.B. Ahmed, G.M. Goldgof, R. Paul, D.B. Goldgof, L.O. Hall, Discovery of ageneralization gap of Convolutional Neural Networks on COVID-19 X-raysclassification, Ieee Access 9 (2021) 72970–72979.[5] Y. LeCun, Y. Bengio, et al., Convolutional networks for images, speech, andtime series, in: The Handbook of Brain Theory and Neural Networks, Vol.3361, no. 10, 1995, p. 1995.[6] T. Kontzer, Deep learning drops error rate for breast cancer diagnoses by85%, 2016, URL: https://blogs.nvidia.com/blog/2016/09/19/deep-learning-breast-cancer-diagnosis/.[7] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015)436–444.[9][10][8] T. Agrawal, P. Choudhary, EfficientUNet: Modified encoder-decoder archi-tecture for the lung segmentation in chest X-ray images, Expert Syst.(2022) e13012.I.M. Baltruschat, H. Nickisch, M. Grass, T. Knopp, A. Saalbach, Comparisonof deep learning approaches for multi-label chest X-ray classification, Sci.Rep. 9 (1) (2019) 1–10.J.-Y. Park, Y. Hwang, D. Lee, J.-H. Kim, MarsNet: Multi-label classificationnetwork for images of various sizes, IEEE Access 8 (2020) 21832–21846.[11] A. Bustos, A. Pertusa, J.-M. Salinas, M. de la Iglesia-Vayá, Padchest: A largechest X-ray image dataset with multi-label annotated reports, Med. ImageAnal. 66 (2020) 101797.I. Al-Badarneh, M. Habib, I. Aljarah, H. Faris, Neuro-evolutionary modelsfor imbalanced classification problems, J. King Saud Univ.-Comput. Inf. Sci.(2020).[12]Declaration of competing interestThe authors declare that they have no known competinginterests or personal relationships that could havefinancialappeared to influence the work reported in this paper.Data availabilityThe authors do not have permission to share data.AcknowledgementsThis work has been funded by Grant PLEC2021-007681(XAI-DisInfodemics)and PID2020-117263GB-100 (FightDIS)funded by MCIN/AEI/ 10.13039/501100011033 and, as appro-priate, by ‘‘ERDF A way of making Europe’’, by the ‘‘EuropeanUnion NextGenerationEU/PRTR’’, by the research project CIVIC:Intelligent characterisation of the veracity of the informationrelated to COVID-19, granted by BBVA FOUNDATION GRANTSFOR SCIENTIFIC RESEARCH TEAMS SARS-CoV-2 and COVID-19,by European Comission under IBERIFIER - Iberian Digital MediaResearch and Fact-Checking Hub (2020-EU-IA-0252), by ‘‘Con-venio Plurianual with the Universidad Politécnica de Madrid inthe actuation line of Programa de Excelencia para el ProfesoradoUniversitario’’, and by Comunidad Autónoma de Madrid under304[13] E. Lin, Q. Chen, X. Qi, Deep reinforcementlearning for imbalancedclassification, Appl. Intell. 50 (8) (2020) 2488–2502.[14] T.N. Mundhenk, B.Y. Chen, G. Friedland, Efficient saliency maps forexplainable AI, 2019, arXiv preprint arXiv:1911.11293.[15] A. Holzinger, M. Dehmer, F. Emmert-Streib, R. Cucchiara, I. Augenstein, J.Del Ser, W. Samek, I. Jurisica, N. Díaz-Rodríguez, Information fusion asan integrative cross-cutting enabler to achieve robust, explainable, andtrustworthy medical artificial intelligence, Inf. Fusion 79 (2022) 263–278.[16] R.K. Singh, R. Pandey, R.N. Babu, Covidscreen: explainable deep learningframework for differential diagnosis of COVID-19 using chest X-rays,Neural Comput. Appl. 33 (14) (2021) 8871–8892.[17] F. Piccialli, V. Di Somma, F. Giampaolo, S. Cuomo, G. Fortino, A survey ondeep learning in medicine: Why, how and when? Inf. Fusion 66 (2021)111–137.[18] S.W. Baalman, F.E. Schroevers, A.J. Oakley, T.F. Brouwer, W. van der Stuijt,H. Bleijendaal, L.A. Ramos, R.R. Lopes, H.A. Marquering, R.E. Knops, et al.,A morphology based deep learning model for atrial fibrillation detectionusing single cycle electrocardiographic samples, Int. J. Cardiol. 316 (2020)130–136.[19] E.A. Chung, M.E. Benalcázar, Real-time hand gesture recognition modelusing deep learning techniques and EMG signals, in: 2019 27th EuropeanSignal Processing Conference, EUSIPCO, IEEE, 2019, pp. 1–5.[20] H. Li, X. Wang, C. Liu, Q. Zeng, Y. Zheng, X. Chu, L. Yao, J. Wang, Y.Jiao, C. Karmakar, A fusion framework based on multi-domain featuresand deep learning features of phonocardiogram for coronary artery diseasedetection, Comput. Biol. Med. 120 (2020) 103733.J. Pan, Y. Zi, J. Chen, Z. Zhou, B. Wang, LiftingNet: A novel deep learningnetwork with layerwise feature learning from noisy mechanical data forfault classification, IEEE Trans. Ind. Electron. 65 (6) (2017) 4973–4982.[21][22] Y.-S. Su, T.-J. Ding, M.-Y. Chen, Deep learning methods in internet ofmedical things for valvular heart disease screening system, IEEE InternetThings J. 8 (23) (2021) 16921–16932.H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306[23] F.N. Abdullah, M.N. Fauzan, N. Riza, Multiple linear regression and deeplearning in body temperature detection and mask detection, IT J. Res. Dev.(2022) 109–121.[24] M. Jost, D.A. Santos, R.A. Saunders, M.A. Horlbeck, J.S. Hawkins, S.M. Scaria,T.M. Norman, J.A. Hussmann, C.R. Liem, C.A. Gross, et al., Titrating geneexpression using libraries of systematically attenuated CRISPR guide RNAs,Nature Biotechnol. 38 (3) (2020) 355–364.[25] S. Li, K. Yu, D. Wang, Q. Zhang, Z.-X. Liu, L. Zhao, H. Cheng, Deeplearning based prediction of species-specific protein S-glutathionylationsites, Biochim. Biophys. Acta (BBA)-Proteins Proteomics 1868 (7) (2020)140422.[26] G. Zampieri, S. Vijayakumar, E. Yaneske, C. Angione, Machine and deeplearning meet genome-scale metabolic modeling, PLoS Comput. Biol. 15(7) (2019) e1007084.[27] M.L. Welch, C. McIntosh, A. McNiven, S.H. Huang, B.-B. Zhang, L. Wee,A. Traverso, B. O’Sullivan, F. Hoebers, A. Dekker, et al., User-controlledpipelines for feature integration and head and neck radiation therapyoutcome predictions, Phys. Med. 70 (2020) 145–152.[28] Z. Xu, J. Chou, X.S. Zhang, Y. Luo, T. Isakova, P. Adekkanattu, J.S. Ancker, G.Jiang, R.C. Kiefer, J.A. Pacheco, et al., Identifying sub-phenotypes of acutekidney injury using structured and unstructured electronic health recorddata with memory networks, J. Biomed. Inform. 102 (2020) 103361.[29] K. Rough, A.M. Dai, K. Zhang, Y. Xue, L.M. Vardoulakis, C. Cui, A.J. Butte,M.D. Howell, A. Rajkomar, Predicting inpatient medication orders fromelectronic health record data, Clin. Pharmacol. Therapeutics 108 (1) (2020)145–154.I. Ahmed, D. Camacho, G. Jeon, F. Piccialli, Internet of health things drivendeep learning-based system for non-invasive patient discomfort detectionusing time frame rules and pairwise keypoints distance feature, SustainableCities Soc. 79 (2022) 103672.[30][31] D. Arefan, A.A. Mohamed, W.A. Berg, M.L. Zuley, J.H. Sumkin, S. Wu, Deeplearning modeling using normal mammograms for predicting breast cancerrisk, Med. Phys. 47 (1) (2020) 110–118.[32] M. Byra, M. Wu, X. Zhang, H. Jang, Y.-J. Ma, E.Y. Chang, S. Shah, J. Du, Kneemenisci segmentation and relaxometry of 3D ultrashort echo time conesMR imaging using attention U-Net with transfer learning, Magn. Reson.Med. 83 (3) (2020) 1109–1122.[33] S. Saha, A. Pagnozzi, P. Bourgeat, J.M. George, D. Bradford, P.B. Colditz,R.N. Boyd, S.E. Rose, J. Fripp, K. Pannek, Predicting motor outcome inpreterm infants from very early brain diffusion MRI using a deep learningConvolutional Neural Network (CNN) model, Neuroimage 215 (2020)116807.[34] M. Saminathan, M. Ramachandran, A. Kumar, K. Rajkumar, A. Khanna, P.Singh, A study on specific learning algorithms pertaining to classify lungcancer disease, Expert Syst. 39 (3) (2022) e12797.[35]I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016.[36] S. Kazemifar, A.M. Barragán Montero, K. Souris, S.T. Rivas, R. Timmerman,Y.K. Park, S. Jiang, X. Geets, E. Sterpin, A. Owrangi, Dosimetric evaluation ofsynthetic CT generated with GANs for MRI-only proton therapy treatmentplanning of brain tumors, J. Appl. Clin. Med. Phys. 21 (5) (2020) 76–86.[37] G. Liu, T.-M.H. Hsu, M. McDermott, W. Boag, W.-H. Weng, P. Szolovits, M.Ghassemi, Clinically accurate chest X-ray report generation, in: MachineLearning for Healthcare Conference, PMLR, 2019, pp. 249–269.[38] F. Piccialli, F. Giampaolo, E. Prezioso, D. Camacho, G. Acampora, Artificialintelligence and healthcare: Forecasting of medical bookings throughmulti-source time-series fusion, Inf. Fusion 74 (2021) 1–16.[39] T. Nemoto, N. Futakami, M. Yagi, A. Kumabe, A. Takeda, E. Kunieda, N.Shigematsu, Efficacy evaluation of 2D, 3D U-Net semantic segmentationand atlas-based segmentation of normal lungs excluding the trachea andmain bronchi, J. Radiat. Res. 61 (2) (2020) 257–264.[40] D.C. Benz, G. Benetos, G. Rampidis, E. Von Felten, A. Bakula, A. Sustar,K. Kudura, M. Messerli, T.A. Fuchs, C. Gebhard, et al., Validation ofdeep-learning image reconstruction for coronary computed tomographyangiography: Impact on noise, image quality and diagnostic accuracy, J.Cardiovasc. Comput. Tomography 14 (5) (2020) 444–451.[41] T.D. Pham, Classification of COVID-19 chest X-rays with deep learning:New models or fine tuning? Health Inf. Sci. Syst. 9 (1) (2021) 1–11.[42] F. Ahmad, A. Farooq, M.U. Ghani, Deep ensemble model for classificationof novel coronavirus in chest X-ray images, Comput. Intell. Neurosci. 2021(2021).[43] D. Avola, A. Bacciu, L. Cinque, A. Fagioli, M.R. Marini, R. Taiello, Study ontransfer learning capabilities for pneumonia classification in chest-X-raysimage, 2021, arXiv preprint arXiv:2110.02780.[46] A.B. Arrieta, N. Díaz-Rodríguez, J. Del Ser, A. Bennetot, S. Tabik, A. Barbado,S. García, S. Gil-López, D. Molina, R. Benjamins, et al., Explainable artificialintelligence (XAI): Concepts, taxonomies, opportunities and challengestoward responsible AI, Inf. Fusion 58 (2020) 82–115.[47] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R.M. Summers, ChestX-ray8:Hospital-scale chest X-ray database and benchmarks on weakly-supervisedclassification and localization of common thorax diseases, in: Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition, 2017,pp. 2097–2106.[48] V. Teixeira, L. Braz, H. Pedrini, Z. Dias, Dualanet: Dual lesion attentionnetwork for thoracic disease classification in chest X-rays, in: 2020 In-ternational Conference on Systems, Signals and Image Processing, IWSSIP,IEEE, 2020, pp. 69–74.I. Allaouzi, M.B. Ahmed, A novel approach for multi-label chest X-ray classification of common thorax diseases,IEEE Access 7 (2019)64279–64288.[49][50] M.M.A. Monshi,reports using deep learning,Neural Networks, Springer, 2021, pp. 684–694.J. Poon, V. Chung, F.M. Monshi, Labeling chest X-Rayin: International Conference on Artificial[51] A. Smit, S. Jain, P. Rajpurkar, A. Pareek, A.Y. Ng, M.P. Lungren, CheXbert:combining automatic labelers and expert annotations for accurate ra-diology report labeling using BERT, 2020, arXiv preprint arXiv:2004.09167.[52] W. Boag, T.-M.H. Hsu, M. McDermott, G. Berner, E. Alesentzer, P. Szolovits,in: Machine Learning forBaselines for chest X-ray report generation,Health Workshop, PMLR, 2020, pp. 126–140.[53] S.Jain, A. Smit, S.Q. Truong, C.D. Nguyen, M.-T. Huynh, M.Jain, V.A.Young, A.Y. Ng, M.P. Lungren, P. Rajpurkar, VisualCheXbert: Addressingthe discrepancy between radiology report labels and image labels,in:Proceedings of the Conference on Health, Inference, and Learning, 2021,pp. 105–115.[54] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R.M. Summers, ChestX-ray:Hospital-scale chest X-ray database and benchmarks on weakly supervisedclassification and localization of common thorax diseases, in: Deep Learn-ing and Convolutional Neural Networks for Medical Imaging and ClinicalInformatics, Springer, 2019, p. 369.J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B.Haghgoo, R. Ball, K. Shpanskaya, et al., Chexpert: A large chest radiographdataset with uncertainty labels and expert comparison, in: Proceedings ofthe AAAI Conference on Artificial Intelligence, Vol. 33, no. 01, 2019, pp.590–597.[55][56] H. Wang, Y.-Y. Yang, Y. Pan, P. Han, Z.-X. Li, H.-G. Huang, S.-Z. Zhu,Detecting thoracic diseases via representation learning with adaptivesampling, Neurocomputing 406 (2020) 354–360.[57] S. Albahli, H.T. Rauf, A. Algosaibi, V.E. Balas, AI-driven deep CNN approachfor multi-label pathology classification using chest X-rays, PeerJ Comput.Sci. 7 (2021) e495.[58] K. Almezhghwi, S. Serte, F. Al-Turjman, Convolutional neural networks forthe classification of chest X-rays in the IoT era, Multimedia Tools Appl. 80(19) (2021) 29051–29065.[59] L. Seyyed-Kalantari, G. Liu, M. McDermott, I.Y. Chen, M. Ghassemi, CheX-clusion: Fairness gaps in deep chest X-ray classifiers, in: BIOCOMPUTING2021: Proceedings of the Pacific Symposium, World Scientific, 2020, pp.232–243.J.P. Cohen, M. Hashir, R. Brooks, H. Bertrand, On the limits of cross-domaingeneralization in automated X-ray prediction, in: Medical Imaging withDeep Learning, PMLR, 2020, pp. 136–155.[60][61] M. Hashir, H. Bertrand, J.P. Cohen, Quantifying the value of lateral viewsin deep learning for chest X-rays, in: Medical Imaging with Deep Learning,PMLR, 2020, pp. 288–303.[62] E.H. Pooch, P. Ballester, R.C. Barros, Can we trust deep learning baseddiagnosis? the impact of domain shift in chest radiograph classification,in: International Workshop on Thoracic Image Analysis, Springer, 2020,pp. 74–83.J.P. Cohen, P. Morrison, L. Dao, K. Roth, T.Q. Duong, M. Ghassemi, COVID-19image data collection: Prospective predictions are the future, 2020, arXivpreprint arXiv:2006.11988.[63][64] Y. Wang, L. Sun, Q. Jin, Enhanced diagnosis of pneumothorax with animproved real-time augmentation for imbalanced chest X-rays data basedon DCNN, IEEE/ACM Trans. Comput. Biol. Bioinform. 18 (3) (2019) 951–962.[65] F. Charte, A.J. Rivera, M.J. del Jesus, F. Herrera, MLSMOTE: Approachinglearning through synthetic instance generation,imbalanced multilabelKnowl.-Based Syst. 89 (2015) 385–397.[44] T. Zebin, S. Rezvy, COVID-19 detection and disease progression visu-alization: Deep learning on chest X-rays for classification and coarselocalization, Appl. Intell. 51 (2) (2021) 1010–1021.[66] H. Salehinejad, E. Colak, T. Dowdell, J. Barfett, S. Valaee, Synthesizing chestX-ray pathology for training deep convolutional neural networks, IEEETrans. Med. Imaging 38 (5) (2018) 1197–1206.[45] L.O. Teixeira, R.M. Pereira, D. Bertolini, L.S. Oliveira, L. Nanni, G.D. Cav-alcanti, Y.M. Costa, Impact of lung segmentation on the diagnosis andexplanation of COVID-19 in chest X-ray images, Sensors 21 (21) (2021)7116.[67] W. Qu, I. Balki, M. Mendez, J. Valen, J. Levman, P.N. Tyrrell, Assessingand mitigating the effects of class imbalance in machine learning withapplication to X-ray imaging, Int. J. Comput. Assist. Radiol. Surg. 15 (12)(2020) 2041–2048.305H. Liz, J. Huertas-Tato, M. Sánchez-Montañés et al.Future Generation Computer Systems 144 (2023) 291–306[91] A.I. Aviles-Rivero, P. Sellars, C.-B. Schönlieb, N. Papadakis, GraphXCOVID:Explainable deep graph diffusion pseudo-labelling for identifying COVID-19on chest X-rays, Pattern Recognit. 122 (2022) 108274.Helena Liz is an Associate Researcher at UniversidadRey Juan Carlos (URJC) Computer Science Departmentunder project CYNAMON. She did her undergradu-ate studies in Biology at Universidad Autónoma deMadrid and she has an M.Sc in Bioinformatics andComputational Biology at Universidad Autónoma deMadrid. Currently, she is Ph.D candidate at UniversidadPolitécnica de Madrid. Her research interests includeDeep Learning, AI and Machine Learning applicationsin medicine, among others. Contact her at: helena.liz@urjc.es.Dr. Javier Huertas-Tato obtained his PhD in ComputerScience at Universidad Carlos III de Madrid under a FPIresearch grant. Currently, he is working as a Ph.D. assis-tant lecturer at Universidad Politecnica de Madrid andcollaborating with national and international researchprojects such as CIVIC, FightDIS, and IBERIFIER. Hiscurrent research topics are disinformation detection,tracking, and countering; machine learning applied toissues; and deep learning techniquesenvironmentalsuch as convolutional networks and transformers.Manuel A. Sánchez-Montañés received his B.Sc. de-gree (with honors) in Physics from the UniversidadComplutense de Madrid, Spain, 1997, and Ph.D. degree(cum laude) in Computer Science from the UniversidadAutónoma de Madrid, Spain, 2003. He is currentlyworking with the Computer Science Department, Uni-versidad Autónoma de Madrid. His research activityis focused in Artificial Intelligence and Advanced DataAnalysis, carrying out theoretical developments andapplications.Javier Del Ser received his first PhD in Telecommu-nication Engineering from the University of Navarra(2006), and a second PhD in Computational Intelligencefrom the University of Alcala (2013). Currently he is aResearch Professor in Artificial Intelligence at TECNALIA(Spain) and an adjunct professor at the University ofthe Basque Country (UPV/EHU). His research interestsgravitate on Artificial Intelligence for data modellingand optimisation tasks in a diverse range of applicationfields (Energy, Transport, Telecommunications, Healthand Industry, etc.). He also serves as an associate editorin a number of indexed journals,including Information Fusion, Swarm andEvolutionary Computation and IEEE Transactions on Intelligent TransportationSystems. He is a IEEE Senior Member, and has been included in the list of the2% most influential authors in Artificial Intelligence in 2021 and 2022 elaboratedby Stanford University.David Camacho is currently working as Full Professorwith the Departamento de Sistemas Informáticos atUniversidad Politécnica de Madrid (Spain) and leads theApplied Intelligence and Data Analysis group (AIDA).He received a PhD in Computer Science (2001) fromUniversidad Carlos III de Madrid. His research interestsinclude Data Mining, Evolutionary Computation, So-cial Network Analysis, and Swarm Intelligence, amongothers. Contact him at: david.camacho@upm.es.[68] P. Rajpurkar, J. Irvin, K. Zhu, B. Yang, H. Mehta, T. Duan, D. Ding, A. Bagul,C. Langlotz, K. Shpanskaya, et al., Chexnet: Radiologist-level pneumoniadetection on chest X-rays with deep learning, 2017, arXiv preprint arXiv:1711.05225.[69] K.F. Monowar, M.A.M. Hasan, J. Shin, Lung opacity classification with Con-volutional Neural Networks using chest X-rays, in: 2020 11th InternationalConference on Electrical and Computer Engineering, ICECE, IEEE, 2020, pp.169–172.[70] Z. Ge, D. Mahapatra, S. Sedai, R. Garnavi, R. Chakravorty, Chest X-raysclassification: A multi-label and fine-grained problem, 2018, arXiv preprintarXiv:1807.07247.[71] Z. Huang, D. Fu, Diagnose chest pathology in X-ray images by learningmulti-attention Convolutional Neural Network,in: 2019 IEEE 8th JointInternational Information Technology and Artificial Intelligence Conference,ITAIC, IEEE, 2019, pp. 294–299.[72] K. Wang, X. Zhang, S. Huang, KGZNet: Knowledge-guided deep zoomneural networks for thoracic disease classification, in: 2019 IEEE Inter-national Conference on Bioinformatics and Biomedicine, BIBM, IEEE, 2019,pp. 1396–1401.[73] R. Qin, K. Qiao, L. Wang, L. Zeng, J. Chen, B. Yan, Weighted focal loss: Aneffective loss function to overcome unbalance problem of chest X-ray14,IOP Conf. Ser.: Mater. Sci. Eng. 428 (1) (2018) 012022.J. Islam, Y. Zhang, Towards robust lung segmentation in chest radiographswith deep learning, 2018, arXiv preprint arXiv:1811.12638.[74][75] S. Reza, O.B. Amin, M. Hashem, TransResUNet: Improving U-net architec-ture for robust lungs segmentation in chest X-rays, in: 2020 IEEE Region10 Symposium, TENSYMP, IEEE, 2020, pp. 1592–1595.[76] M. Tan, Q. Le, Efficientnet: Rethinking model scaling for convolutionalneural networks, in: International Conference on Machine Learning, PMLR,2019, pp. 6105–6114.[77] G. Huang, Z. Liu, L. Van Der Maaten, K.Q. Weinberger, Densely con-nected convolutional networks, in: Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, 2017, pp. 4700–4708.[78] C. Szegedy, V. Vanhoucke, S.J. Shlens, Z. Wojna, Rethinkingthe inception architecture for computer vision,in: Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition, 2016, pp.2818–2826.Ioffe,[79] C. Szegedy, S. Ioffe, V. Vanhoucke, A.A. Alemi, Inception-v4, inception-resnet and the impact of residual connections on learning, in: Thirty-FirstAAAI Conference on Artificial Intelligence, 2017.[80] F. Chollet, Xception: Deep learning with depthwise separable convolutions,in: Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, 2017, pp. 1251–1258.[81] V.-L. Nguyen, E. Hüllermeier, M. Rapp, E. Loza Mencía,On aggregation in ensembles of multilabel classifiers,Conference on Discovery Science, Springer, 2020, pp. 533–547.J. Fürnkranz,in: International[82] F. Chollet, et al., Keras, 2015, GitHub, URL: https://github.com/fchollet/keras.[83] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G.S. Corrado,A. Davis, J. Dean, M. Devin, et al., Tensorflow: Large-scale machine learningon heterogeneous distributed systems, 2016, arXiv preprint arXiv:1603.04467.[84] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machinelearning in Python, J. Mach. Learn. Res. 12 (2011) 2825–2830.[85] G. Bradski, The OpenCV Library, Dr. Dobb’s J. Softw. Tools (2000).[86] F. Charte, A.J. Rivera, M.J. del Jesus, F. Herrera, REMEDIAL-HwR: Tack-ling multilabel imbalance through label decoupling and data resamplinghybridization, Neurocomputing 326 (2019) 110–122.[87] G. Rimeika, E. Mockiene, et al., Deep learning model for chest X-raypathology classification performance on an independent spanish dataset,in: European Congress of Radiology-ECR 2020, 2020.[88] T. Zhang, X. Li, Z. Qu, Lesion attentive thoracic disease diagnosis with largedecision margin loss, Biomed. Signal Process. Control 71 (2022) 103202.[89] U. Kamal, M. Zunaed, N.B. Nizam, T. Hasan, Anatomy-xnet: An anatomyaware Convolutional Neural Network for thoracic disease classification inchest X-rays, IEEE J. Biomed. Health Inf. 26 (11) (2022) 5518–5528.[90] H. Liz, M. Sánchez-Montañés, A. Tagarro, S. Domínguez-Rodríguez, R.Dagan, D. Camacho, Ensembles of Convolutional Neural Network modelsfor pediatric pneumonia diagnosis, Future Gener. Comput. Syst. 122 (2021)220–233.306