Artificial Intelligence 175 (2011) 1366–1389Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintComputing rank dependent utility in graphical models for sequentialdecision problems ✩Gildas Jeantet, Olivier Spanjaard∗Laboratoire d’Informatique de Paris 6 (LIP6-CNRS), Université Pierre et Marie Curie (UPMC), 4 Place Jussieu, F-75252 Paris Cedex 05, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 2 March 2009Received in revised form 31 August 2010Accepted 31 August 2010Available online 2 December 2010Keywords:Algorithmic decision theoryRank dependent utilityDecision treesInfluence diagramsPlanning under uncertaintyThis paper is devoted to automated sequential decision in AI. More precisely, we focus hereon the Rank Dependent Utility (RDU) model. This model is able to encompass rationaldecision behaviors that the Expected Utility model cannot accommodate. However, thenon-linearity of RDU makes it difficult to compute an RDU-optimal strategy in sequentialdecision problems. This has considerably slowed the use of RDU in operational contexts.In this paper, we are interested in providing new algorithmic solutions to compute anRDU-optimal strategy in graphical models. Specifically, we present algorithms for solvingdecision tree models and influence diagram models of sequential decision problems. Fordecision tree models, we propose a mixed integer programming formulation that is validfor a subclass of RDU models (corresponding to risk seeking behaviors). This formulationreduces to a linear program when mixed strategies are considered. In the general case (i.e.,when there is no particular assumption on the parameters of RDU), we propose a branchand bound procedure to compute an RDU-optimal strategy among the pure ones. Afterhighlighting the difficulties induced by the use of RDU in influence diagram models, weshow how this latter procedure can be extended to optimize RDU in an influence diagram.Finally, we provide empirical evaluations of all the presented algorithms.© 2010 Elsevier B.V. All rights reserved.1. IntroductionIn many AI problems, agents must act under uncertainty (e.g. in robot control, relief organization, medical diagnosis,games, etc.). When the consequences of an action only depend on events whose probabilities are known, decision theoryunder risk provides useful tools to automate decisions. The purpose of this theory is indeed to design decision criteria toevaluate probability distributions on outcomes (called hereafter lotteries) according to the preferences of a decision maker.A popular criterion is the expected utility (EU) model proposed by von Neumann and Morgenstern [49]. In this model,an agent is endowed with a utility function u that assigns a numerical value to each outcome. The evaluation of a lotteryL = (p1, x1; . . . ; pn, xn) (i.e., the lottery that yields outcome xi with probability pi ) is then performed via the computation of(cid:2)its utility expectation: EU(L) =ni=1 pi u(xi). However, despite its intuitive appeal, the EU model does not make it possibleto account for all rational decision behaviors. An example of such impossibility is the so-called Allais’ paradox [2] (Table 1).We present below a very simple version of this paradox due to Kahneman and Tversky [23] (Table 2).Example 1 (Kahneman and Tverky’s example). Consider a choice situation where two options are presented to a decision(cid:3)maker. He chooses between lottery L1 and lottery L2 in a second(cid:3)1 in a first problem, and between lottery L2 and lottery L✩This paper extends preliminary results of the two authors [20].* Corresponding author.E-mail address: olivier.spanjaard@lip6.fr (O. Spanjaard).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.11.019G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891367Table 1Original Allais’ paradox. Columns represent probabilities,100M stands for 100 millions. Most people prefers si-(cid:3)multaneously L1 to L2 to L2.(cid:3)1 and LLottery0.01L1(cid:3)L1L2(cid:3)L2$100M$0M$100M$0M0.1$100M$500M$100M$500M0.89$100M$100M$0M$0MTable 2Kahneman and Tversky’s version. Columns represent(cid:3)outcomes. Most people prefers simultaneously L1 to L1and L(cid:3)2 to L2.LotteryL1(cid:3)L1L2(cid:3)L2$00.000.100.900.91$3000$40001.000.000.100.000.000.900.000.09(cid:3)1), while in the second problem he prefers L(cid:3)problem (see Table 2). In the first problem he prefers L1 to L1 (he is certain to earn $3000 with L1 while he might earn(cid:3)nothing with L2 is almost thesame as the probability of earning only $3000 with L2). The EU model cannot simultaneously account for both preferences.(cid:3)1 implies u(3000) > 0.1u(0) + 0.9u(4000). This is equivalent to 0.1u(3000) > 0.01u(0) +Indeed, the preference for L1 over L0.09u(4000), and therefore to 0.9u(0) + 0.1u(3000) > 0.91u(0) + 0.09u(4000) (by adding 0.9u(0) on both sides). Hence,(cid:3)1 implies the preference for L2 over Lwhatever utility function is used, the preference for L1 over L(cid:3)2 to L2 (the probability of earning $4000 with L(cid:3)2 in the EU model.Actually, Allais points out that this preference reversal, far from being paradoxical, is the consequence of a reasonablebehavior of preference for security in the neighborhood of certainty [3]. In other words, “a bird in the hand is worth two in the(cid:3)bush” (preference for L1 over L1). It is known as the certainty effect. The preference reversal can be explained as follows:when the probability of winning becomes low, the sensitivity to the value of earnings increases while the sensitivity tothe probabilities decreases. To encompass the certainty effect in a decision criterion, the handling of probabilities shouldtherefore not be linear. Given this situation, new models have been developed: some models are grounded on an alternativerepresentation of uncertainty such as the theory of possibility [12], others try to sophisticate the definition of expectedutility such as prospect theory [23], cumulative prospect theory [48] or the rank dependent utility (RDU) model introducedby Quiggin [40]. This latter model is one of the most popular generalization of EU. In this model, a non-linear probabilityweighting function ϕ is incorporated in the expectation calculus, which gives a greater expressive power. In particular, theRDU model is compatible with both versions of Allais’ paradox. Furthermore, the probability weighting function ϕ is alsouseful to model the attitude of the agent towards the risk. Indeed, unlike the EU model, the RDU model makes it possibleto distinguish between weak risk aversion (i.e., if an option yields a guaranteed utility, it is preferred to any other riskyoption with the same expected utility) and strong risk aversion (i.e., if two lotteries have the same expected utility, thenthe agent prefers the lottery with the minimum spread of possible outcomes). For this reason, the RDU criterion has beenused in search problems under risk in state space graphs, with the aim of finding optimal paths for risk-averse agents [38].Note that, within the AI community, the rank dependent utility function is best known under the name of Weighted OrderedWeighted Averaging operator [45,46]. In particular, the WOWA operator has been studied in several fields of AI where anaggregation function is required: synthesis of information [45], decision making under risk [34,36], metadata aggregationproblems [10], interactive techniques in multicriteria optimization [35].The algorithmic issues related to the use of RDU in sequential decision problems have prevented its adoption in thissetting until today. In a sequential decision problem under risk, one does not make a simple decision but one follows astrategy (i.e. a sequence of decisions conditioned by events) resulting in a non-deterministic outcome. This type of problemis in particular encountered in decision-theoretic planning [6,7]. This term refers to planners involving decision-theoretictools. Formally, the aim of a decision-theoretic planner is to find a plan optimizing a given decision criterion. For thispurpose, the lottery induced by each plan is evaluated according to a decision criterion (usually EU). Several representationformalisms can be used for sequential decision problems, such as decision trees [e.g. 41], influence diagrams [e.g. 44] orMarkov decision processes [e.g. 11,22]. A decision tree is an explicit representation of a sequential decision problem, whileinfluence diagrams or Markov decision processes are compact representations and make it possible to deal with decisionproblems of greater size. It is important to note that, in all these formalisms, the set of potential strategies is combinatorial(i.e., its size increases exponentially with the size of the instance). The computation of an optimal strategy for a givenrepresentation and a given decision criterion is then an algorithmic issue in itself. Contrary to the computation of a strategymaximizing EU, one cannot directly resort to dynamic programming for computing a strategy maximizing RDU (due to itsnon-linearity). Evaluating a decision tree or an influence diagram according to RDU (i.e., computing an optimal strategyaccording to RDU) raises therefore a challenging algorithmic problem. This is precisely the issue we tackle in this paper.The paper is organized as follows. In Section 2, we recall the main features of RDU. In Section 3, we place our work inthe stream of research aiming at incorporating risk-sensitivity in probabilistic planning problems. Then, in Section 4, afterdetailing how the RDU criterion should be used in a sequential decision problem, we propose two approaches for optimizingRDU in a decision tree, and we provide numerical tests for both approaches. In Section 5, we investigate the optimization ofRDU in an influence diagram. After recalling the influence diagram formalism, we highlight a difficulty that was not presentin the decision tree formalism, namely that not all strategies are considered in an influence diagram. We then propose anapproach to overcome this difficulty, and provide numerical tests that show its interest.1368G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13892. Rank dependent utilityGiven a finite set S = {x1, . . . , xn} of outcomes, any strategy in a sequential decision problem can be seen as a lottery,characterized by a probability distribution P over S. In this paper, unless explicitely mentioned, we assume that the out-comes are real numbers ordered as follows: x1 < · · · < xn. We denote by L = (p1, x1; . . . ; pn, xn) the lottery that yieldsoutcome xi with probability pi = P ({xi}). The decumulative function G L is given by G L(α) =i:xi (cid:2)α pi , and is denoted by(G L(x1), x1; . . . ; G L(xn), xn). For the sake of clarity, we will consider a lottery L as a function from S to [0, 1] such thatL(xi) = pi . As indicated above, in decision problems, lotteries are compared according to a decision criterion, as for instanceEU.(cid:2)(cid:3)1(cid:3)= xi0(cid:3)= (p1, x1(cid:3)2 obtained from lotteries L1 and L(where i0 is an arbitrary index in {1, . . . , n}), then L1 preferred to L(cid:3)1, while there is a 89 percent chance to win nothing in L2 and LRank Dependent Utility (RDU), introduced by Quiggin [40], is among the most popular generalizations of EU, and makesit possible to describe sophisticated rational decision behaviors. From the axiomatic viewpoint, the RDU model is groundedon a weakening of the sure thing principle [43] that we now detail. It is indeed well known that the sure thing principleholds when using the EU criterion. In the framework of decision under risk, this principle can be stated as follows: let(cid:3)lotteries L1 = (p1, x1; . . . ; pn, xn) and L; . . . ; pn, xn) be such that the outcomes are not necessarily ranked in(cid:3)increasing order and xi01 implies L2 preferred to(cid:3)(cid:3)L1 by merely replacing the common outcome xi0 by another common2, for lotteries L2, Loutcome yi0 . In Allais’ paradox (Table 1), the sure thing principle is clearly violated. There is indeed a 89 percent chance to(cid:3)win $100 millions in L1 and L2, ceteris paribus. In orderto encompass such examples, the validity of the axiom has to be restricted to cases where the common outcome is ranked(cid:3)similarly in both lotteries, and where its replacement does not affect the ranking in both lotteries: L1 preferred to L1 impliesL2 preferred to L0 common outcome xi0(cid:3)by a common outcome yi0 , again in ith2. This weaker version of the axiom is called comonotonicsure thing principle [9]. It allows preference reversals in cases of extreme change in the level of risk. For instance, in Allais’paradox, there is an extreme change in the level of risk since, in the first comparison, the probability to earn nothing isabout 1 percent, while, in the second comparison, it is about 90 percent. Comonotonic sure thing principle, together witha continuity axiom and an axiom of compatibility with stochastic dominance, characterize the RDU model. A lottery L =(cid:3)(cid:3)k) if for all α ∈ R, G L(α) (cid:2) G L(cid:3) (α). In other(p1, x1; . . . ; pk, xk) is said to stochastically dominate a lottery Lk, xwords, for all α ∈ R, the probability to get an outcome at least α with lottery L is at least as high as the probability withas soon as L stochasticallylottery Ldominates L. Compatibility with stochastic dominance means that lottery L is preferred to lottery L. This property is obviously desirable to guarantee a rational behavior.(cid:3)2 obtained from lotteries L1 and L(cid:3)1 by merely replacing the ith(cid:3)2, for lotteries L2, L0 rank both in L2 and L; . . . ; p(cid:3) = (p(cid:3)(cid:3)1, x1(cid:3)(cid:3)(cid:3)In order to allow preference reversals in cases of extreme change in the level of risk, the handling of probabilities inthe RDU model is non-linear. In this purpose, the first proposal that may come into mind consists in distorting individual(cid:2)nprobabilities by a non-linear function ϕ, which yields a decision criterion of the formi=1 ϕ(pi)u(xi) where u denotesan increasing utility function. Actually, this choice criterion has been proposed by Handa [16], but is not compatible withstochastic dominance. For this reason, the distortion in the RDU model is not performed on the probabilities themselves,but on reverse cumulative probabilities. The formula of rank dependent expected utility can be easily obtained by rewriting[u(xi) −the one of expected utility with respect to reverse cumulative probabilities: EU(L) =u(xi−1)](G L(xi)) (the utility of lottery L is at least u(x1) with probability 1; then the utility might increase from u(x1) tou(x2) with probability G L(x2); the same applies from u(x2) to u(x3) with probability G L(x3), and so on, etc.). The rankdependent utility of a lottery L is then defined as follows:i=1 pi u(xi) = u(x1) +(cid:2)ni=2(cid:2)nRDU(L) = u(x1) +(cid:5)u(xi) − u(xi−1)ϕ(cid:6)(cid:7)G L(xi)n(cid:3)(cid:4)i=2Rank dependent utility thus involves an increasing utility function on consequences u : S → R as in EU, and also a trans-(cid:3)) asformation function on probabilities ϕ : [0, 1] → [0, 1]. It is compatible with stochastic dominance, i.e. RDU(L) (cid:2) RDU(L. The transformation function ϕ is a non-decreasing function, proper to any agent, suchsoon as L stochastically dominates Lthat ϕ(0) = 0 and ϕ(1) = 1. When ϕ(p) = p for all p, RDU obviously reduces to EU.(cid:3)Example 2. Coming back to Example 1, we define the utility function by u(x) = x, and we set ϕ(0.09) = ϕ(0.1) = 0.2,ϕ(0.9) = 0.7. The preferences induced by RDU are then compatible with Kahneman and Tversky’s example. Indeed, wehave:RDU(L1) = u(3000) = 3000(cid:6)= u(0) + ϕ(0.9)RDU(cid:6)(cid:7)L(cid:3)1(cid:7)u(4000) − u(0)= 2800Therefore L1 is preferred to L(cid:3)1. Similarly, we have:(cid:6)(cid:7)= 600u(3000) − u(0)RDU(L2) = u(0) + ϕ(0.1)(cid:7)(cid:6)= u(0) + ϕ(0.09)= 800u(4000) − u(0)RDU(cid:6)(cid:7)L(cid:3)2We conclude that L(cid:3)2 is preferred to L2.G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891369In order to elicit function ϕ, various methods have been used. Several functional forms have been proposed for func-tion ϕ in the economic literature. As indicated by Quiggin [40], the simplest are functions of the form ϕ(p) = pγ withγ ∈ (0, 1). Actually, when trying to exactly reproduce the behavior of human agents, there is empirical evidence that thefunction ϕ is in general inverse S-shaped, i.e., first concave and then convex [48,8]. This means that the probabilities of bestconsequences are overweighted (potential effect) while the probabilities of worst consequences are underweighted (certaintyeffect). The functional form proposed by Karmarkar [24] in this purpose is: pγ /(pγ + (1 − p)γ ) with γ ∈ (0, 1). After settingthe functional form of ϕ, one estimates the right value for parameter γ through standard non-linear regression methods(maximum likehood, least squares). For a more detailed discussion about the different possible functional forms of ϕ, theinterested reader may refer to the book of Quiggin [40]. Note that there also exist parametric-free elicitation methods, i.e.that works without assuming a prespecified shape of function ϕ [1,13,47].3. Position of the paperBy studying the use of RDU from the computational viewpoint, we place our work in the stream of research aimingat incorporating risk sensitivity in probabilistic planning problems. The non-linearity of risk-sensitive criteria raises newalgorithmic difficulties. A pioneering work on this topic has been carried out by Howard and Matheson, in the frameworkof Markov decision processes [17]. They show how risk sensitivity may be treated by evaluating a plan via an expectedutility instead of an expectation. In the case of risk-averse agents, the practicality of the approach relies on the use of anexponential utility function u (to maximize) defined by u(x) = −γ x for an outcome x ∈ R+(γ ∈ (0, 1)). The adoption of sucha utility function involves the agreement of the decision maker with the “(cid:5)-property”, namely that his attitude towardsrisk does not depend on his wealth level. Koenig and Simmons have performed a similar work with a slightly differentrepresentation of probabilistic planning problems, that required the design of new algorithms [25]. The representation theyuse is called a “probabilistic decision graph”, and resembles the decision trees we study here. Then, back to the frameworkof MDPs, Liu and Koenig have proposed to resort to a “one-switch” utility function in order to take into account the wealthlevel in the preferences. For a risk-averse agent that becomes risk-neutral in the limit as its level of wealth increases,the one-switch utility function u is of the form u(x) = x − Dγ x, where D > 0 and γ ∈ (0, 1). After exhibiting conditionsguaranteeing that the optimal expected utilities of the total plan-execution reward exist and are finite for fully observableMDP models with risk-sensitive utility functions [26], the authors have proposed a functional value iteration algorithm toapproximate optimal expected utilities for one-switch utility functions [27]. Finally, they have also proposed a policy iterationalgorithm in a subsequent paper, that enables to return an optimal policy [28].(cid:2)nAll these results are real advances to take into account more accurately risk sensitivity in probabilistic planning problems,especially in high-stakes situations. However, the induced preferences reproduce the biases of EU theory. To explain thisin more details, we need previously to introduce the notions of weak risk-aversion and strong risk-aversion. An agent issaid to be weakly risk-averse if, for any lottery L, he considers that sure lottery (1; E(L)) is as least as good as L, wherei=1 pi xi for L = (p1, x1; . . . ; pn, xn) [4,39]. In EU theory, risk-aversion means that the agent’s utility function u onE(L) =(cid:3)(x) [4]. Strongoutcomes is increasing and concave, the coefficient of risk-aversion of any agent being measured by −urisk-aversion is defined from the notion of mean preserving spread [42]. Basically, an agent is said to be strongly risk-averseif, between two lotteries with the same expectation, he always prefers the less spread one. Interestingly, it has been shown(cid:3)) for all increasing and concave utilitythat a lottery L is a mean preserving spread of a lottery L(cid:2)functions u, where EU(L) =ni=1 pi u(xi) [40]. Consequently, when using EU to compare lotteries, any weakly risk-averseagent is also strongly risk-averse. A nice virtue of the RDU model is precisely that it enables the distinction between weakand strong risk-aversion (contrary to EU). Within this model, for a concave utility function u, the agent is weakly risk-averseiff ϕ(p) (cid:3) p for all p ∈ [0, 1], and strongly risk-averse iff ϕ is convex (this directly follows from a result of Quiggin [40]).As stated above, rank dependent utility is thus a powerful tool for modeling risk-sensitive agents, as illustrated by Allais’paradox. This motivates our study.if and only if EU(L) (cid:3) EU(L(cid:3)(cid:3)(x)/u(cid:3)4. Computing RDU in a decision tree4.1. Decision tree formalismA decision tree is an arborescence with three types of nodes: the decision nodes (represented by squares), the chancenodes (represented by circles), and the terminal nodes (the leaves of the arborescence). The branches starting from a de-cision node correspond to different possible decisions, while the ones starting from a chance node correspond to differentpossible events, the probabilities of which are known. The values indicated at the leaves correspond to the utilities of theconsequences. Note that one omits the orientation of the edges when representing decision trees. For the sake of illustration,a decision tree representation of a sequential decision problem (with three strategies) is given in Fig. 1.More formally, in a decision tree T = (N , E), the set N of nodes is ND ∪ N A ∪ NU , where ND is the set of decisionnodes, N A the set of chance nodes and NU the set of terminal nodes. The root node is denoted by Nr ∈ N \ NU . Thevaluations are defined as follows: every edge E = ( A, N) ∈ E such that A ∈ N A is weighted by probability p(E) of thecorresponding event; every terminal node NU ∈ NU is labeled by its utility u(NU ). Besides, we call past(N) the past of1370G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–1389Fig. 1. A decision tree representation.Fig. 2. A compound lotery.N ∈ N , i.e. the set of edges along the path from Nr to N in T . Finally, we denote by S(N) the set of successors of N in T ,and by T (N) the subtree of T rooted in N.Following Jaffray and Nielsen [19], one defines a strategy as a set of edges (cid:5) = {(N, N(cid:3)): N ∈ N (cid:5)D , N(cid:3) ∈ N (cid:5)} ⊆ E , whereN (cid:5) ⊆ N is a set of nodes including:• the root Nr of T ,• one and only one successor for every decision node N ∈ N (cid:5)D• all successors for every chance node N ∈ N (cid:5)A= N A ∩ N (cid:5).= ND ∩ N (cid:5),Given a decision node N, the restriction of a strategy in T to a subtree T (N), which defines a strategy in T (N), is called asubstrategy.In order to evaluate a strategy, it is important to note that a strategy can be associated to a compound lottery overthe utilities. For instance, in the decision tree of Fig. 1, strategy {(D1, A1), (D2, A3)} corresponds to the compound lotterydepicted in Fig. 2. Evaluating a strategy amounts therefore to evaluating a compound lottery. From now on, for the ease ofpresentation, we will manipulate lotteries directly defined over utilities rather than outcomes. A lottery is then a functionfrom U to [0, 1] such that L(xi) = ui , where U = {u1, . . . , un} with u1 < u2 < · · · < un is the image set of S with respectto u. Coming back to our example, it is natural to assume that the compound lottery of Fig. 2 is equivalent to lotteryL = (0.5, 2; 0.25, 3; 0.25, 10) (actually, this assumption is known as the reduction of compound lotteries axiom [29], which isused in the axiomatizations of EU and RDU). Given a value function V that maps every lottery with a real number (e.g.V ≡ EU or V ≡ R DU ), the evaluation of the strategy is then V (L). For example, if V ≡ EU, then the evaluation of strategy{(D1, A1), (D2, A3)} is 0.5 × 2 + 0.25 × 3 + 0.25 × 10 = 4.25.4.2. Computing RDU in a decision tree: from decision theory to combinatorial optimizationIn a decision tree T , the number of potential strategies may grow exponentially with the size of the decision tree. Forexample, in a binary decision tree with n nodes and a strict alternation of decision/chance nodes, one can easily show thatn ). For this reason, it is necessary to develop an optimization algorithm to determinethe number of strategies is in Θ(2an optimal strategy in a decision tree. It is well known that the rolling back method makes it possible to compute in lineartime an optimal strategy w.r.t. EU. Indeed, such a strategy satisfies the optimality principle: any substrategy of an optimalstrategy is itself optimal. The optimality principle is closely related to a condition of monotonicity [32] on the value function.In our context, given a value function V (e.g. V ≡ EU), this condition can be stated as follows:√(cid:7)(cid:3)(cid:6)L(cid:6)⇒ VαL + (1 − α)L(cid:7)(cid:3)(cid:3)(cid:6)(cid:2) VαL(cid:3) + (1 − α)L(cid:3)(cid:3)(cid:7)V (L) (cid:2) V(cid:3), L(cid:3)(cid:3)(cid:3)(cid:3))(x) =are lotteries, α is a scalar in [0, 1] and αL + (1 − α)Lwhere L, L(cid:3)(cid:3)(x). In the framework of decision theory, this condition can be seen as a weak version of the independenceαL(x) + (1 − α)Laxiom used by von Neumann and Morgenstern [49] to characterize the EU criterion. This axiom states that the mixture oftwo lotteries L and L, thenαL + (1 − α)L. The monotonicity condition holds for V ≡ EU, which justifiesthat the optimality principle holds for EU.with a third one should not reverse preferences (induced by V ): if L is strictly preferred to Lis the lottery defined by (αL + (1 − α)Lshould be strictly preferred to αL(cid:3) + (1 − α)L(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)Hence, starting from the leaves, one can compute recursively for each node the expected utility of an optimal substrategy:the optimal expected utility for a chance node equals the expectation of the optimal utilities of its successors; the optimalexpected utility for a decision node equals the maximum expected utility of its successors.Example 3. In Fig. 1, the optimal expected utility at node D2 is max{6.5, 6} = 6.5. Consequently, the optimal expected utilityat node A1 is 4.25. The expected utility at node A2 is 0.3 × 1 + 0.45 × 2 + 0.25 × 11 = 3.95. The optimal expected utilityG. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891371at the root node D1 is therefore max{4.25, 3.95} = 4.25, and the correspond strategy is {(D1, A1), (D2, A3)}. Note that thisstrategy can be suboptimal when using RDU to evaluate lotteries (see below).In decision theory, the behavior of an agent that adopts such a recursively computed strategy is called consequentialist.More precisely, consequentialism means that the preferences between substrategies in a subtree does not depend on therest of the decision tree. Besides, an agent is said to be dynamically consistent whenever at any decision node he is willingto carry out the plan of action that he determined to be optimal ex-ante. It has been proved that the preferences of anagent that is both consequentialist and dynamically consistent follow the EU model [14,15]. An agent whose preferencesfollow the RDU model (named hereafter RDU-maximizer) should therefore renounce either consequentialism or dynamicconsistency. Assume first that the agent adopts a consequentialist behavior, that is, he computes recursively a strategy fromthe leaves by selecting optimal substrategies for RDU, and then follows this strategy. As shown by Example 4, this strategy,determined ex-ante, could be suboptimal since the monotonicity condition does not hold for V ≡ RDU, as already noticedby Nielsen and Jensen [33]. By committing himself to consequentialism, the agent thus renounces dynamic consistency (hedoes not follow the strategy that is optimal ex-ante).(cid:3) = (0.5, 1; 0.5, 11) (corre-(cid:3)(cid:3) = (1, 2). Assume that the decision maker preferences follow the RDU model0,Example 4. Consider lotteries L = (0.5, 3; 0.5, 10) (corresponding to chance node A3 in Fig. 1), Lsponding to chance node A4 in Fig. 1) and Lwith the following ϕ function:⎧if p = 0if 0 < p (cid:3) 0.25if 0.25 < p (cid:3) 0.5if 0.5 < p (cid:3) 0.7if 0.7 < p (cid:3) 0.75if p > 0.75⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎩ϕ(p) =0.75,0.45,0.6,0.8,1,The RDU values of lotteries L and L(cid:3)are:RDU(L) = 3 + (10 − 3)ϕ(0.5) = 7.2= 1 + (11 − 1)ϕ(0.5) = 7RDU(cid:6)(cid:7)L(cid:3)Thus, we have RDU(L) (cid:2) RDU(Lmonotonicity condition for α = 0.5, one should therefore have RDU(0.5L + 0.5L(cid:3)) (substrategy {(D2, A3)} is preferred to substrategy {(D2, A4)} in D2 in Fig. 1). By the(cid:3)(cid:3)). However, we have:(cid:3)(cid:3)) (cid:2) RDU(0.5L(cid:3) + 0.5L(cid:6)(cid:7)(cid:3)(cid:3)(cid:6)RDURDU0.5L + 0.5L(cid:3) + 0.5L= 2 + (3 − 1)ϕ(0.5) + (10 − 3)ϕ(0.25) = 5.75(cid:7)= 1 + (2 − 1)ϕ(0.75) + (11 − 2)ϕ(0.25) = 6.65(cid:3)(cid:3)) < RDU(0.5LTherefore RDU(0.5L + 0.5Lin Fig. 1). Consequently, the monotonicity property does not hold.(cid:3) + 0.5L0.5L(cid:3)(cid:3)(cid:3)(cid:3)) (strategy {(D1, A1), (D2, A4)} is preferred to strategy {(D1, A1), (D2, A3)}More seriously, a consequentialist RDU-maximizer could even follow a stochastically dominated strategy, as shown bythe following example.Example 5. Consider the decision tree of Fig. 1. In this decision tree, the RDU values of the different strategies are (at theroot):RDURDU(cid:6)(cid:12)(cid:6)(cid:12)(cid:6)(cid:12)(cid:13)(cid:7)(D1, A2)= 5.8(cid:13)(cid:7)(D1, A1), (D2, A3)(cid:13)(cid:7)RDU(D1, A1), (D2, A4)= 5.75= 6.65Thus, the optimal strategy at the root is {(D1, A1), (D2, A4)}. However, by recursion, one gets at node D2: RDU({(D2, A3)}) =7.2 and RDU({(D2, A4)}) = 7. This is therefore substrategy {(D2, A3)} that is obtained at node D2 (see Example 4). Atnode D1, this is thereafter the strategy {(D1, A2)} (5.8 vs. 5.75 for {(D1, A1), (D2, A3)}), stochastically dominated by{(D1, A1), (D2, A4)}, which is finally obtained.At first sight, such an example could be misinterpreted as illustrating a weakness of the RDU model in sequential decisionsituations. Actually, it primarily shows that the RDU model is inappropriate for consequentialist agents. Conversely, theEU model is unable to reproduce non-consequentialist behaviors. This type of behaviors is however routinely displayedby many rational agents. An intuitive example of a non-consequentialist behavior has been proposed by Machina [30].1372G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–1389Assume that Mom has a single treat which she can give to either daughter Abigail or son Benjamin. She is indifferentbetween Abigail getting the treat and Benjamin getting the treat, but she strictly prefers a coin flip over either of the sureoutcomes. Assume that Abigail wins the coin flip. We are in a different state than before the coin flip, and Mom now prefersAbigail getting the treat over a new flip. History, in this case, matters as far as preferences: Benjamin had his chance, andtherefore the fact that Benjamin could have won still matters after the coin is flipped. This is the very justification of non-consequentialism. More generally, a non-consequentialist agent still gives importance to events that could have occurred,contrary to a consequentialist agent that does not take a risk into account once it has been borne.In this paper, we consider therefore a dynamically consistent agent (he sets a plan initially and never deviates fromit later [31]) with a non-consequentialist behavior (the optimal strategy at the root can include substrategies that appearsuboptimal in their subtrees). In other words, we study in the sequel how to compute an RDU-optimal plan viewed fromthe initial situation, and comply to it. By doing so, we are sure to never encounter a stochastically dominated substrategy,contrary to a method that would consist in performing backward induction with RDU. Unfortunately, the determination ofan RDU-optimal strategy in a decision tree is an NP-hard problem (where the size of an instance is the number of involveddecision nodes):Proposition 1 (Jeantet and Spanjaard, 2008 [20]). The determination of an RDU-optimal strategy (problem RDU-OPT) in a decisiontree is an NP-hard problem.Proof. This is proved by polynomial reduction from 3-SAT (see Appendix A). (cid:2)Note that Jaffray and Nielsen also studied the use of RDU in decision trees [19]. Nevertheless, their approach differsfrom ours, since they focus on how RDU should be used by agents close to be consequentialist. Consequently, they do notcompute an optimal strategy viewed from the initial situation.4.3. Two approaches for computing RDUWe propose here two approaches for determining an RDU-optimal strategy in a decision tree. One approach uses amixed integer linear programming formulation, and the other one proceeds by implicit enumeration (neither exhaustiveenumeration nor backward induction are conceivable since RDU-OPT is NP-hard).4.3.1. A Mixed Integer Linear Programming formulationWe now present a Mixed Integer Linear Programming (MIP) formulation of problem RDU-OPT, in the case where functionϕ is concave piecewise linear. Consider a decision tree T . We first detail the set of constraints defining feasible strategies.For this purpose, a boolean variable y(i, j) is created in the MIP formulation for every decision branch (D i, A j). The |ND |constraints defining the set of feasible strategies are then:(cid:3)y(1, j) = 1j(cid:3)jy(i, j) = yprevD (i) ∀i ∈(cid:12)(cid:13)2, . . . , |ND |where y(i, j) = 1 (resp. y(i, j) = 0) if (D i, A j) is selected (resp. not selected), and prevD (i) is the last decision branch precedingD i on the path from the root (in the temporal order).Example 6. For the decision tree represented in Fig. 3, the constraints defining the set of feasible strategies are:y(1,1) + y(1,2) = 1y(2,3) + y(2,4) = y(1,1)y(3,5) + y(3,6) = y(1,1)y(4,7) + y(4,8) = y(1,2)y(5,9) + y(5,10) = y(1,2)We now detail the modeling of the objective function. The set of utilities at the leaves of T is denoted by U ={u1, u2, . . . , un}, with u1 (cid:3) u2 (cid:3) · · · (cid:3) un, and the probability to obtain utility uh is denoted by ph. Probability ph is theproduct of all probabilities on the path from the root to utility uh. The rank dependent utility is then written as follows:n(cid:3)(uh − uh−1)ϕu1 +h=2(cid:15)p j yprevu( j)(cid:14)(cid:3)j(cid:2)hG. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891373Fig. 3. A decision tree and the corresponding variables and parameters in the MIP formulation.where yprevu ( j) is the last decision branch preceding u j on the path from the root. By introducing (n − 1) variables ϕh, itcan be rewritten as follows:n(cid:3)u1 +(uh − uh−1)ϕhh=2where ϕh = ϕ((cid:2)j(cid:2)h p j yprevu ( j)) for h = 2, . . . , n.Example 7. For the decision tree represented in Fig. 3, the objective function is written:1 + (4 − 1)ϕ2 + · · · + (31 − 25)ϕ13 + (31 − 31)ϕ14 + (31 − 31)ϕ15 + (40 − 31)ϕ16= 1 + 3ϕ2 + 2ϕ3 + ϕ4 + 3ϕ5 + 5ϕ6 + 3ϕ7 + 2ϕ8 + ϕ9 + ϕ10 + 2ϕ11 + 2ϕ12 + 6ϕ13 + 9ϕ16Note that variables ϕ14 and ϕ15 do not appear in the final objective function, and can therefore be eliminated from theprogram.The expression defining the value of ϕh is of course non-linear, due to the presence of function ϕ. This difficulty can beovercome in the case where ϕ is concave piecewise linear. Recall that a concave function ϕ reflects a risk-seeking behaviorfor certain forms of utility function (e.g., convex).By using a concave piecewise linear function ϕ, one can approximateany concave regular function. This family of functions is therefore interesting to study. It is well known that any concavepiecewise linear function can be written as the lower envelope of a set of affine functions. Let { f 1, f 2, . . . , fm} denote thisset, where fk(p) = ak p + bk. We have ϕ(p) = min{ f 1(p), f 2(p), . . . , fm(p)}. The value of ϕh can be obtained by optimization:ϕh = maxαα(cid:14)(cid:3)α (cid:3) fkα (cid:2) 0j(cid:2)h(cid:15)p j yprevu( j)∀k ∈ {1, . . . , m}The above problem is a linear program for a given strategy y.Example 8. Assume that ϕ(p) = 1.8p for p (cid:3) 0.5, and ϕ(p) = 0.4p + 0.6 for p > 0.5. We have then ϕ(p) = min{ f 1(p), f 2(p)}with f 1(p) = 1.8p and f 2(p) = 0.4p + 0.6. For a given assignment of boolean values to variables y(i, j) in the decision treerepresented in Fig. 3, the value of ϕ13 can be written as the result of the following linear program:ααϕ13 = maxα (cid:3) 1.8(0.30 y(2,3) + 0.28 y(3,5) + 0.36 y(5,9) + 0.09 y(5,10))α (cid:3) 0.4(0.30 y(2,3) + 0.28 y(3,5) + 0.36 y(5,9) + 0.09 y(5,10)) + 0.6α (cid:2) 01374G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–1389Fig. 4. One-to-one correspondence between assignments of probabilities P ((D i , A j )|D i ) and p(i, j).Since uh −uh−1 (cid:2) 0, the objective function (to maximize) will benefit from maximizing every ϕh, and the MIP formulationis therefore the following in the concave piecewise case:n(cid:3)h=2(uh − uh−1)ϕi(cid:15)u1 +maxϕ(cid:14)(cid:3)ϕh (cid:3) fkp j yprevu ( j)j(cid:2)h(cid:3)j(cid:3)jy(1, j) = 1y(i, j) = yprevD (i) ∀i ∈(cid:12)(cid:13)2, . . . , |ND |y(i, j) ∈ {0, 1} ϕh (cid:2) 0∀h ∈ {2, . . . , n} ∀k ∈ {1, . . . , m}This program includes (n − 1) continuous variables, |ND | binary variables, and (n − 1)m + |ND | constraints (since m con-straints are created for every variable ϕh). Its size is therefore linear in the size of T for a fixed number m of pieces in ϕ.We recall however that the complexity of the solution procedure is of course exponential in the number of binary variablesin the worst case (for a fixed number m of pieces in ϕ).Let us now briefly study a relaxation of the problem, where one considers not only pure strategies but also mixed strate-gies. In a mixed strategy, one chooses randomly (according to a predefined probability distribution) the decision taken ateach decision node. When using expected utility to evaluate a strategy, it is not worth considering mixed strategies sincethere always exists a pure strategy yielding the same expected utility than the best mixed strategy. This is no longer thecase when using rank dependent utility to evaluate a strategy. Consider a decision tree with a single decision node leadingto two different options: a sure outcome lottery (1, 5), and a lottery (0.5, 1; 0.5, 10). Assume that the probability transfor-mation function is defined by ϕ(0) = 0, ϕ(p) = 0.45 if p ∈ (0; 0.7) and ϕ(p) = 1 if p > 0.7. The RDU values of the twopure strategies are respectively 5 and 5.05. Comparatively, the mixed strategy where one chooses the sure outcome lot-tery with probability 0.6, and the other one with probability 0.4, results in a RDU value of 7.25. We now show that theoptimal mixed strategy can be polynomially computed in the two previous cases (concave piecewise linear ϕ, piecewiseconstant ϕ), by slighting modifying the MIP formulations so that no boolean variables appear anymore (and the numberof constraints remains the same). It yields of course a linear programming formulation. In a mixed strategy, the proba-bility of obtaining utility uh equals the product of the probabilities on the chance and decision branches along the pathfrom the root to uh. For this reason, a real variable p(i, j) is created in the MIP formulation for every decision branch(D i, A j), instead of a boolean variable. However, to obtain linear constraints, this variable does not represent probabilityP ((D i, A j)|D i) (probability to make decision (D i, A j) conditionally to reach node D i ) but the product of the probabili-ties of the decision branches from the root to node A j . There is a one-to-one correspondence between assignments ofprobabilities P ((D i, A j)|D i) and p(i, j), since P ((D i, A j)|D i) equals p(i, j)/pprevD (i). This is illustrated in Fig. 4. The proba-bility of obtaining utility uh is then ph pprevu (h) (on the path to uh, ph is the product of the probabilities on the chancebranches, and pprevu (h) is the product of the probabilities on the decision branches). The objective function is thereforewritten:G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891375n(cid:3)(uh − uh−1)ϕu1 +h=2(cid:15)p j pprevu( j)(cid:14)(cid:3)j(cid:2)h(cid:2)Let us now study the constraints that must satisfy variables p(i, j). By definition, we haveA j)|D i). Then,j P ((D i, A j)|D i) = pprevD (i). Consequently,constraints on variables p(i, j) are thus very similar to the previous constraints on variables y(i, j):j pprevD (i) P ((D i, A j)|D i) = pprevD (i)(cid:2)(cid:2)(cid:2)j p(i, j) =(cid:2)j pprevD (i) P ((D i,j p(i, j) = pprevD (i). The(cid:3)p(1, j) = 1j(cid:3)jp(i, j) = pprevD (i) ∀i ∈(cid:12)(cid:13)2, . . . , |ND |where p(i, j) ∈ [0, 1]. All the other constraints are identical to the ones in pure strategies (with y(i, j) replaced byp(i, j)). This proves the polynomial solvability of the determination of an RDU-optimal mixed strategy in the case wherefunction ϕ is concave piecewise linear (since the size of the linear program is linear in the size of the decisiontree).4.3.2. Implicit enumeration algorithmWe now present a branch and bound method for determining an RDU-optimal (pure) strategy. Unlike the previousapproach, this method has the advantage of remaining valid for any probability transformation function ϕ. The branching(cid:3)) at a decisionprinciple is to partition the set of strategies in several subsets according to the choice of a given edge (N, Nnode N. More formally, the nodes of the enumeration tree are characterized by a partial strategy, that defines a subset ofstrategies. Consider a decision tree T and a set of nodes N Γ including:• the root Nr of T ,• one and only one successor for every decision node N ∈ N ΓD= ND ∩ N Γ .(cid:3)): N ∈ N Γ(cid:3) ∈ N Γ } ⊆ E defines a partial strategy of T if the subgraph induced by N Γ is aThe set of edges Γ = {(N, Ntree. A strategy (cid:5) is said compatible with a partial strategy Γ if Γ ⊆ (cid:5). The subset of strategies characterized by a partialstrategy corresponds to the set of compatible strategies. At each iteration of the search, one chooses an edge among the onesstarting from a given decision node. The order in which the decision nodes are considered is given by a priority functionrk : ND → {1, 2, . . . , |ND |}: if several decision nodes are candidates to enter N Γ , the one with the lowest priority rank willbe considered first. The ranking function rk is defined by:D , N⎧⎪⎨⎪⎩rk(Nr) = 1(cid:16)(cid:16)(cid:16)past(N)(cid:16) >(cid:16)(cid:16)(cid:16)past(N)(cid:16) =(cid:6)(cid:6)(cid:16)(cid:16)past(cid:16)(cid:16)pastNN(cid:7)(cid:16)(cid:16) ⇒ rk(N) > rk(cid:7)(cid:16)(cid:7)(cid:16) and EUT (N)(cid:6)(cid:3)(cid:3)(cid:7)(cid:3)(cid:6)N> EU(cid:7)(cid:7)(cid:6)(cid:6)T(cid:3)N⇒ rk(N) < rk(N(cid:3))where EU(T (N)) is the optimal value of EU in T (N) (we recall that T (N) is the subtree rooted in N).Example 9. For the decision tree in Fig. 3, there is a unique ranking function rk defined by: rk(D1) = 1, rk(D2) = 5,rk(D3) = 3, rk(D4) = 4 and rk(D5) = 2 (since EU(T (D5)) < EU(T (D3)) < EU(T (D4)) < EU(T (D2))).Algorithm 1 describes formally the implicit enumeration procedure that we propose:Algorithm 1: BB(Γ, RDUopt)N1 ← {N1 ∈ ND : N1 is candidate};Nmin ← arg minN∈N1 rk(N);Emin ← {(Nmin, A) ∈ E: A ∈ S(Nmin)};for each (N, A) ∈ Emin doif ev(Γ ∪ {(N, A)}) > RDUopt thenRDUtemp ← BB(Γ ∪ {(N, A)}, RDUopt);If RDUtemp > RDUopt thenRDUopt ← RDUtemp;endendendreturn RDUoptIt takes as an argument a partial strategy Γ and the best RDU value found so far, denoted by RDUopt. The search is depth-first. The decision nodes that are candidates to enter N Γ are denoted by N1. Among them, the node with the lowest1376G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–1389priority rank is denoted by Nmin. The set of its incident edges is denoted by Emin. It defines the set of possible extensionsof Γ considered in the search (in other words, the children of the node associated to Γ in the enumeration tree). For everypartial strategy Γ (in other words, at every node of the enumeration tree), one has an evaluation function ev that gives anupper bound of the RDU value of any strategy compatible with Γ . The optimality of the returned value RDUopt is guaranteedsince only suboptimal strategies are pruned during the search as soon as ev is an upper bound.We give below the main features of our algorithm.Initialization. A branch and bound procedure is notoriously more efficient when a good solution is known before startingthe search. In our method, the lower bound (RDUopt) is initially set to the RDU value of the EU-optimal strategy.Computing the lower bound. At each node of the search, one computes the EU-optimal strategy among strategies that arecompatible with Γ . When its RDU value is greater than the best found so far, we update RDUopt. This makes it possible toprune the search more quickly.Computing the upper bound. The evaluation function is denoted by ev. It returns an upper bound on the RDU value ofany strategy compatible with Γ . The principle of this evaluation is to determine a lottery that stochastically dominates anylottery corresponding to a strategy compatible with Γ , and then to evaluate this ideal lottery according to RDU. This yieldsthen RDU(L) (cid:2)an upper bound since RDU is compatible with stochastic dominance, i.e. if L stochastically dominates L(cid:3)). In order to compute such a lottery, one proceeds by dynamic programming in the decision tree. Actually, one canRDU(Lindifferently manipulate decumulative functions or lotteries, since both sets are in bijection (we recall that a lottery onutilities is considered as a function from U to [0, 1] in this paper). For the sake of clarity, we describe the recursion byreferring to decumulative functions. The initialization is performed as follows: at each terminal node T ∈ NU is assigneda decumulative function G L T = (1, u(T )). Next, at each node N ∈ N , one computes the decumulative function of a lotterythat stochastically dominates all the lotteries of subtree T (N). More precisely, at a chance node A, one computes thedecumulative function G L A induced by the decumulative functions of its children as follows:(cid:3)∀u, G L A (u) =(cid:3)(cid:6)(cid:7)( A, N)pG L N (u)N∈S( A)where G L N corresponds to the decumulative function assigned to node N ∈ N . Besides, at each decision node D, we applythe following recurrence relation on the decumulative functions:(cid:17)∀u, G L D (u) = G L N (u)∀u, G L D (u) = maxN∈S(D) G L N (u) otherwiseif ∃N ∈ S(D): (D, N) ∈ ΓFinally, the value returned by ev is RDU(L Nr ) where L Nr corresponds to the lottery of decumulative function G L Nr . Thecomplexity of this recursive procedure for Γ = ∅ is in O (|N |.|U |) (where |U | is the number of distinct utilities at theleaves) since each node in N is examined once, and the support set of a lottery is upper bounded by |U |. However, duringthe branch and bound procedure, when an edge (D i, N j) is inserted into Γ , it is not necessary to recompute G L N in everynode N. One can indeed use functions G L N already computed for evaluating ev(Γ ): it is sufficient to update functions G L Nonly on nodes N that belong to the path from Nr to D i . Since the length of a path in T is upper bounded by height h,the complexity of computing ev(Γ ) for Γ (cid:16)= ∅ is therefore in O (h.|U |). To prove the validity of the recursive procedure, oneproceeds by induction:• At a chance node A: consider a tuple (L N )N∈S( A) of lotteries such that L N stochastically dominates L N for all N ∈N∈S( A) p(( A, N))G L N (u) ∀u whereN∈S( A) p(( A, N))L N denotes a compound lottery. This proves that L A stochastically dominates any lottery correspond-S( A). We have: G L A (u) = G(cid:2)(cid:2)N∈S( A) p(( A, N))G L N (u) (cid:2)N∈S( A) p(( A,N))L N (u) =(cid:2)(cid:2)ing to a strategy in T ( A).• At a decision node D: if there exists N ∈ S(D) with (D, N) ∈ Γ , then the validity is obvious. In the other case, con-sider again a tuple (L N )N∈S(D) of lotteries such that L N stochastically dominates L N for all N ∈ S(D). By definition,G L D (u) = maxN∈S(D) G L N (u) ∀u. Thus G L D (u) (cid:2) G L N (u) (cid:2) G L N (u) ∀u for any lottery L N of the tuple. This proves that L Dstochastically dominates any lottery corresponding to a strategy in T (D).Consequently, lottery L Nr stochastically dominates all lotteries corresponding to a strategy compatible with Γ .Example 10. Let us come back to the decision tree of Fig. 1. Assume that Γ = {(D1, A1)}. The decumulative functions2 , 11). They are represented in the left part of Fig. 5.assigned to nodes A3 and A4 are G L A3The decumulative function G L D2 computed by dynamic programming is then the upper envelope of G L A3 and G L A4 . Moreformally, the decumulative function computed in D2 is defined by: ∀x, G L D2 (x) = max{G L A3 (x), G L A4 (x)}. This decumulativefunction is represented in bold in the right part of Fig. 5. Then we have: G L D22 , 11). Similarly, one computes:2 , 10) and G L A4= (1, 3; 1= (1, 3; 1= (1, 1; 1G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891377Fig. 5. Computation of a stochastically dominating lottery.Table 3Execution times for a concave piecewise linear function ϕ (in seconds).ApproachHeight (nodes)MIPImplicit enumeration4 (11)< 1< 16 (127)8 (511)< 1< 1< 1< 110 (2047)2.1 − 2.3 − 2.8< 0.1 − 2.5 − 5.012 (8191)79.4 − 86.0 − 107.30.3 − 5.6 − 20.2– G L A1u(2),– G L D1= 0.5G L D2+ 0.5G Lu(2) = (1, 2; 12 , 3; 14 , 11) where G Lu(2) is the decumulative function associated to the node of utility= G L A1= (1, 2; 12 , 3; 14 , 11) since Γ = {(D1, A1)}.Decumulative function G L D1 corresponds to lottery ( 1ev(Γ ) = RDU(( 14 , 11)).2 , 2; 14 , 3; 12 , 2; 14 , 3; 14 , 11). The upper bound for Γ = {(D1, A1)} is therefore4.4. Numerical testsAlgorithms were implemented in C++ and the computational experiments were carried out on a PC with a Pentium IVCPU 2.13 GHz processor and 3.5 GB of RAM.Tests on random instances. We have first compared the performances of the MIP approach with the ones of the implicitenumeration approach. Our tests were performed on complete binary decision trees of even height. The height of thesedecision trees varies from 4 to 12, with an alternation of decision nodes and chance nodes. The utilities at the leaves arereal numbers randomly drawn within interval [1, 1000], and the conditional probabilities at the chance nodes are randomlydrawn positive real numbers summing up to 1. Since the MIP approach requires a concave piecewise linear function ϕ,we used function ϕ defined by ϕ(p) = min{ f 1(p), . . . , f 5(p)} with: f 1(p) = 4p,f 4(p) =4 p + 0.85. Table 3 shows the average execution CPU times obtained by both approaches. The mixed2 p + 0.7,1integer linear programs are solved using the ILOG CPLEX v11.1.0 solver. Note that the solution times indicated in Table 3for the MIP approach do not take into account the preprocessing time. When it is informative, the min and max values areindicated under the following format: min – average – max. Both approaches instantly give an optimal strategy for trees ofheight less than 8. However, when height exceeds 12, the implicit enumeration approach is much more efficient than theMIP approach.f 2(p) = 2p + 0.2,f 3(p) = p + 0.5,f 5(p) = 1Next, we have gone further into the study of our implicit enumeration algorithm. For this purpose, we have measured theperformances of the implicit enumeration approach with function ϕ defined by: ϕ(p) = pγ /(pγ + (1 − p)γ ). This functionis not concave nor piecewise linear. This is the one usually proposed to model sophisticated behaviors that the EU modelis unable to describe [24]. Parameter γ takes value in interval [0, 1]. For γ = 1, we have ϕ(p) = p and RDU reduces to EU.We have tested our algorithm for several values of parameter γ , namely γ = 0.2, γ = 0.5 and γ = 0.8. Table 4 presentsthe performances of the algorithm with respect to parameter γ and the height of the decision tree. When it is informative,the min and max values are indicated. For each value of γ and height, we give the average performance computed over 50decision trees. Unsurprisingly, the performances improve when γ is near 1 (i.e. RDU is close to EU). Note that, for biggerinstances (i.e. the height of which is greater than 14), some hard instances begin to appear for which the solution timebecomes high.1378G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–1389Table 4Execution time.γ = 0.2γ = 0.5γ = 0.8Depth (nodes)5 (11)< 0.1< 0.1< 0.17 (127)9 (511)< 0.1< 0.1< 0.1< 0.1< 0.1< 0.111 (2047)< 0.1 − 0.4 − 1.1< 0.1 − 0.1 − 0.5< 0.1 − 0.1 − 0.513 (8191)0.3 − 4.3 − 13.80.1 − 2.7 − 7.40.1 − 1.6 − 8.015 (32767)3.0 − 62.1 − 512.91.2 − 17.3 − 99.40.6 − 10.9 − 101.4Table 5Quality of upper and lower bound.BoundHeight (nodes)γ = 0.2∗ )/RDU∗RDU(LEURDU(LSD)/RDU∗γ = 0.5∗ )/RDU∗RDU(LEURDU(LSD)/RDU∗γ = 0.8∗ )/RDU∗RDU(LEURDU(LSD)/RDU∗4 (11)95.1%104.4%99.6%105.5%99.8%106.4%6 (127)92.46%105.1%99.3%109.6%99.5%106.3%8 (511)90.0%106.9%99.0%109.4%99.4%108.8%10 (2047)12 (8191)91.2%112.4%98.8%109.1%98.9%107.8%90.9%117.5%98.2%110.2%98.6%107.2%∗∗and RDU(LSD)/RDUin order to evaluate the quality of the lower bound and the upper bound, we have investigated ratiosFinally,with respect to parameter γ and the height of the decision tree, where LEU is theRDU(LEU)/RDU∗lottery corresponding to an optimal strategy for EU, LSD is the lottery computed for evaluating the upper bound and RDUis the value of an optimal strategy for RDU. The results are presented in Table 5, where each value is an average over 50instances. One can observes that RDU(LEU) provides a good lower bound that naturally deteriorates when γ becomes closeto 0 (i.e. the probabilities are very distorted). The upper bound appears to be within 10% of the optimal value on mostinstances.However, the complete binary trees considered here are actually the “worst cases” that can be encountered. In fact, inmany applications, the decision trees are much less balanced and therefore, for the same number of decision nodes, anRDU-optimal strategy will be computed faster, as illustrated now on a TV game example.Application to Who wants to be a millionaire? Who wants to be a millionaire? is a popular game show, where a contestantmust answer a sequence of multiple-choice questions (four possible answers) of increasing difficulty, numbered from 1to 15. This is a double or nothing game: if the answer given to question k is wrong, then the contestant quits with nomoney. However, at each question k, the contestant can decide to stop instead of answering: he then quits the game withthe monetary value of question (k − 1). Following Perea and Puerto [37], we study the Spanish version of the game in2003, where the monetary values of the questions were 150, 300, 450, 900, 1800, 2100, 2700, 3600, 4500, 9000, 18 000,36 000, 72 000, 144 000 and 300 000 Euros respectively. Note that, actually, after the 5th and 10th questions, the moneyis banked and cannot be lost even if the contestant gives an incorrect response to a subsequent question: for example, ifthe contestant gives a wrong answer to question 7, he quits the game with 1800 Euros. Finally, the contestant has threelifelines that can be used once during the game: Phone a friend (call a friend to ask the answer), 50:50 (two of the threeincorrect answers are removed), Ask the audience (the audience votes and the percentage of votes each answer has receivedis shown).We applied our algorithm to compute an RDU-optimal strategy for this game. For this purpose, we first used the modelproposed by [37] to build a decision tree representing the game. In this model, a strategy is completely characterized bygiving the question numbers where the different lifelines are used, and the question number where the contestant quits thegame. We have carried out experimentations for various probability transformation functions, modeling different attitudestowards risk. The identity (resp. square, square root) function corresponds to an expected reward maximizer (resp. a riskaverse, risk seeker decision maker). The results are reported in Table 6. For each function ϕ, we give the expected reward(column Exp.) of the optimal strategy, as well as the maximum possible reward (column Max.) and the probability to winat least 2700 Euros (column G L(2.7K )). Note that, in all cases, the response time of our procedure is less than one secondwhile there are 14 400 decision nodes and the height is 30. This good behavior of the algorithm is linked to the shape ofthe decision tree, that strongly impacts on the number of potential strategies.A limitation of the model introduced by Perea and Puerto [37] is that the choice to use a lifeline is not dependent onwhether the contestant knows the answer or not. For this reason, we introduced the following refinement of the model:if the contestant knows the answers to question k, he directly gives the correct answer, else he has to make a decision.A small part of the decision tree for this new modeling is represented in Fig. 6 (the dotted lines represent omitted parts ofG. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891379Table 6Optimal strategies for various ϕ functions.ϕ(p)pp2√p50:50Phone941410515Ask12513Quit138XExp.238715361987Max.36K2.7K300KG L (2.7K )0.100.350.06Fig. 6. Refined decision tree for the TV game.1’s (resp. Q ithe tree). Chance nodes Q i2’s) represent question 1 (resp. 2), with two possible events (know the answer or not).Decision nodes D i1’s represent the decision to use an available lifeline, answer or quit facing question 1 (in fact, this latteropportunity becomes realistic only from question 2). Finally, the answer is represented by a chance node ( Ai1’s) where theprobabilities of the events (correct or wrong answer) depend on the used lifelines. We used the data provided by Perea andPuerto [37] to evaluate the different probabilities at the chance nodes. The whole decision tree has more than 75 millionsof nodes. The problem becomes therefore much harder since the number of potential strategies explodes. Unlike previousnumerical tests, we had to use a computer with 64 GB of RAM so as to be able to store the instance. Despite the high sizeof the instance, the procedure is able to return an optimal strategy in 2992 sec. for ϕ(p) = p2 (risk averse behavior) and4026 sec. for ϕ(p) = p(2/3) (risk seeker behavior). Note that, for risk seeker behaviors, the solution time increases with theconcavity of the probability transformation function.5. Computing RDU in an influence diagramThe previous approaches face the main inconvenience of decision trees: their size grows quickly when the number ofdecision stages increases. For this reason, we study the optimization of RDU in influences diagrams, that provide compactrepresentations of sequential decision problems.5.1. Influence diagram formalismAn influence diagram [18] is a graphical model for a sequential decision problem. Unlike decision trees, the emphasis isput on the decomposability of the underlying probability structure. By taking advantage of independences between involvedrandom variables and utility variables, one gets a much more compact representation than the one obtained by explicitingall possible scenarios, as would be done in a decision tree. An influence diagram including random variables A1, . . . , A p anddecision variables D1, . . . , Dn is an acyclic digraph G = (N , E) such that:1380G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–1389Fig. 7. Influence diagram and corresponding decision tree.• set N is partitioned into three subsets: a set ND = {D1, . . . , Dn} of decision variables (represented by squares), a setN A = { A1, . . . , A p} of random variables (represented by circles), and a set NU = {U 1, . . . , Um} of utility nodes (repre-sented by diamonds);• set E of directed edges is partitioned into three subsets: a set of functional edges from a decision variable or a randomvariable to a random variable or a utility node (edges representing dependences), a set of informational edges from adecision variable or a random variable to a decision variable (edges representing the observed variables before makinga decision);• the nodes representing random variables are endowed with a conditional probability table, indicating the probability ofevery event conditionally to the parent nodes;• the utility nodes are endowed with a table indicating the utility conditionally to the parent nodes.The following structural condition on the graph must also hold: there exists a path (including functional and informationaledges) connecting all the nodes representing decision variables.An influence diagram is represented in the left part of Fig. 7, where the conditional probability tables and the utilitytables have been deliberately omitted for the sake of brevity. The two possible decisions in D1 (resp. D2) are denoted byα and β (resp. γ and δ). The two modalities of random variable A1 (resp. A2) are η1 and η2 (resp. θ1 and θ2). The orderin which the decisions and the observations are made is assumed to be D1 − A1 − A2 − D2 − A3 − U . We adopt here theconvention that the temporal order of the decisions are read from left to right. By “unfolding” the diagram, one obtainsthe decision tree represented in the right part of Fig. 7 (note that the probabilities and the utilities indicated in the treecomply with the conditionings imposed by the diagram). Since both A3 and U are independent from A1 conditionally toD1, several subtrees are identical (see Fig. 7). Note that the presence of identical subtrees leads to repeat several times thesame calculations when determining a strategy maximizing EU in the decision tree. Influence diagrams make it possible toavoid this pitfall, as detailed in the following section.5.2. RDU, influence diagram and consequentialismThe purpose of our work is to “solve” an influence diagram when the preferences of the decision maker do not followthe EU model but the RDU model. The aim of solving the diagram is to determine the best strategy according to a decisioncriterion (EU, RDU or others). In order to define a strategy, it is necessary to know the random variables already observedwhen making each decision, as well as the temporal order in which the decisions are made. A strategy consists then insetting a value to every decision variable conditionally to its past. In a decision tree, the past of a decision variable is simplydefined as the set of random variables and decision variables lying on the path from the root to that variable. The decisionG. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–138913812, D72, D6= δ, D42= δ, D32= γ , D222 and D8= δ}tree in the right part of Fig. 7 includes 32 feasible strategies, among which strategy {D1 = α, D122 cannot be reached when D1 = α). In an influence diagram, the temporal order is less(note that nodes D5apparent. For this reason, set N A is partitioned into disjoint sets I0, I1, . . . , In. Set I0 includes the random variables observedbefore first decision D1 is made (corresponding to the parents of D1), Ik the random variables observed between Dk andDk+1 (corresponding to the parents of Dk+1), and finally In the remaining random variables, i.e. the ones that are neverobserved or are observed after the last decision Dn is made (the variables that are not parent of any decision variable).This induces a partial order ≺ on ND ∪ N A : I0 ≺ D1 ≺ I1 ≺ · · · ≺ Dn ≺ In. For instance, for the diagram in the left part ofFig. 7, the partial order is D1 ≺ { A1, A2} ≺ D2 ≺ { A3}. The past of a decision variable Dk is then the set of variables X suchthat X ≺ Dk. Formally, a strategy in an influence diagram is a set of decision rules for variables Dk, where a decision rulefor Dk maps each instantiation of the variables in the past of Dk with a value in the domain of Dk. However, in practice,only consequentialist strategies (i.e., where every decision made only depends on the variables influencing parameters of thefuture) are considered in influence diagrams. Therefore, it is not necessary to know the values of all variables in the pastto set the value of a decision variable. This property is crucial since the description of a consequentialist strategy remainslinear in the size of the diagram, which is not the case of non-consequentialist strategies. For instance, in the diagram ofFig. 7, only variables D1 and A2 (and not A1) have an influence on the future of D2. Therefore, in a consequentialist strategy,the decision made in D2 only depends on D1 and A2 (and not A1). A decision rule in D2 is for instance {(D2 = γ |D1 = α,A2 = θ1), (D2 = δ|D1 = α, A2 = θ2), (D2 = δ|D1 = β, A2 = θ1), (D2 = γ |D1 = β, A2 = θ2)}. It means: decision γ is made inD2 if decision α was made in D1 and event θ1 occurred in A2, decision δ is made in D2 if decision α was made in D1 andevent θ2 occurred in A2, etc. As a result, the set of strategies in the diagram of Fig. 7 includes 16 strategies. It is importantto note that the set of strategies considered in the influence diagram is only a subset of the strategies considered in thecorresponding decision tree (for instance the above-mentioned strategy for the decision tree is not in this subset). Whenoptimizing EU, this does no harm since it is well known that there always exists an EU-optimal strategy included in thissubset (since there always exists an EU-optimal strategy that is consequentialist). A contrario, a strategy optimizing RDU isnot necessarily included in this subset (since there does not always exist an RDU-optimal strategy that is consequentialist),as illustrated in the following example.2 and D3Example 11. Consider the decision tree in the right part of Fig. 7. A strategy maximizing EU consists in making decisionα in D1, decision γ in D12 and D42. This strategy is consequentialist. Now, assume that oneadopts the following probability transformation function ϕ: ϕ(p) = 0, if 0 (cid:3) p < 0.175, ϕ(p) = 0.09, if 0.175 (cid:3) p < 0.25,ϕ(p) = 0.1 if 0.25 (cid:3) p < 0.35, ϕ(p) = 0.15 if 0.35 (cid:3) p < 0.75, ϕ(p) = 0.5, if 0.75 (cid:3) p < 0.825, ϕ(p) = 0.9 if 0.825 (cid:3) p < 1and ϕ(1) = 1. Then, the unique strategy maximizing RDU consists in making decision α in D1, decision γ in D12 and decision2 and D3δ in D22.2. This strategy is not consequentialist since two distinct decisions are made in D12, and decision δ in D22 and D42, D35.3. AlgorithmsNote that determining an EU-optimal strategy in an influence diagram is proved NP-hard. Determining an RDU-optimalstrategy is even harder since the description of a non-consequentialist strategy may require a memory space whose size isexponential in the size of the diagram.5.3.1. Two-phases methodSince an RDU-optimal strategy is not necessarily consequentialist, a first method that seems natural consists of twophases: (1) “unfolding” the influence diagram in a decision tree, then (2) determining an optimal strategy according toRDU directly in the decision tree. For phase 2, one can use the implicit enumeration approach proposed in Section 4.3.2.However, although it enables the determination of a real RDU-optimal strategy, this method is of course costly in memoryspace, and becomes impracticable when the size of the decision tree is prohibitive.5.3.2. (cid:5)-relaxation methodFor this reason, we propose another method that takes advantage of the compact structure of influence diagrams (with-out unfolding them in decision trees), at the cost of a reduction of the set of considered strategies. For this purpose, a firstidea would be to explore only the space of consequentialist strategies, but one would then lose an important part of thedescriptive power of the RDU model. Consequently, the approach we propose here does not renounce consequentialism: weintroduce a relaxed form of consequentialism in order to realize a compromise between descriptive power and compactnessof representation. By inserting additional functional edges in the diagram, one enlarges the space of considered strategies.Note that creating fictitious dependences between independent variables does not change the problem itself but only itsrepresentation. In Example 12 below, we illustrate the changes induced by the insertion of a new functional edge.Example 12. After inserting edge ( A1, U ) in the influence diagram of Fig. 7, one obtains the diagram represented in Fig. 8.From the point of view of compactness of the representation, it should be noted that, if variable A1 has two modalities, itwill double the number of lines in the table assigned to variable U : the old table (before insertion of the edge) representedindeed utility U (D1, D2, A3), while the new table (after insertion of the edge) represents U (D1, A1, D2, A3). However, the1382G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–1389Fig. 8. Modified influence diagram.benefit is that the space of considered strategies is enlarged: since A1 now influences the parameters in the future of D2,the decision made in D2 is conditioned by the value of A1. Consequently, a decision rule in D2 now takes into accountvariables D1, A1 and A2. In other words, in this case, the insertion of edge ( A1, U ) makes it possible to take into accountall the non-consequentialist strategies.(cid:3)(cid:3)with V ≺ D ≺ VIn the sequel, we name (cid:5)-relaxation of consequentialism the fact of adding (cid:5) dependences on every decision variable.At worst, one adds |ND | × (cid:5) supplementary edges. Adding a dependence on a decision variable D amounts to inserting anedge1 from V to V, provided D does not already depend on V . This latter condition can be graphicallyformalized as follows: (V , W ) /∈ E ∀W /∈ ND ∧ W (cid:19) D. The inserted edges are of course not chosen arbitrarily. We nowdetail the procedure to select them. Our aim is primarily to keep the representation as much compact as possible. In thispurpose, our heuristic to select edges to insert consists in choosing greedily the ones that have the least impact on the sizes(cid:3)) increases the size of theof the tables in the diagram. Note first that, given two variables V and V(cid:3)|.|W |, wheretable in V|Vin the influence diagram.(cid:3))(cid:3)Inserting edge (V , V(cid:3)).(|V | − 1). The aimbefore insertion, the size becomes s(Vof our heuristic in the greedy procedure to insert edges is to minimize this increment at each step. To this end, each time(cid:3)) is minimum.(cid:3)) such that i(V , Van edge has to be selected among a set of candidate edges, we choose an edge (V , V(cid:3)depends on VAlgorithm 2 summarizes the whole procedure, where update table is a primitive that makes the table in V(by duplicating the entries)., inserting edge (V , Vis equal to |V(cid:3)(cid:3)) is the set of predecessors of V(cid:3)).|V | after insertion, thus yielding an increment i(V , V(cid:3)) multiplies therefore the size of the table in V, i.e. the number of entries. More precisely, the size of the table in V(cid:3)| is the number of modalities of variable Vby |V |. In other words, for a table in Vand Pred(Vof size s(V(cid:3)) = s(VW ∈Pred(V (cid:3))(cid:18)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)Algorithm 2: AddEdgesforeach D ∈ ND doi ← 0;while i < (cid:5) and [∃V ≺ D: (V , VV ← arg minW ≺D {|W |: (W , W(cid:3) ← arg minW /∈NDV(cid:3))};E ← E ∪ {(V , V(cid:3);update table in Vi ← i + 1;{|W |.(cid:18)(cid:3)) /∈ E ∀V(cid:3)) /∈ E ∀W|W(cid:3) /∈ ND ∧ V(cid:3) /∈ ND ∧ V(cid:3)|: W (cid:19) D};(cid:3) (cid:19) D] do(cid:3) (cid:19) D};W (cid:3)∈Pred(W )endendWe now detail the procedure for determining a strategy maximizing RDU after a (cid:5)-relaxation of the diagram. As fordecision trees, we propose here a branch and bound procedure to solve the influence diagram. The main differences are inthe way the strategies are enumerated (branching rule) and in the dynamic programming procedure used to compute anupper bound (more precisely, a lottery stochastically dominating all possible lotteries). We give below the main features ofthe implicit enumeration scheme.Initialization. As for decision trees, one determines a strategy optimizing EU. Several solving algorithms have been proposedin the literature to accomplish this task [44,21]. Schachter’s algorithm [44] consists in eliminating incrementally the nodes ofthe diagram while respecting the partial order on them. For this purpose, edge reversals (of edges representing probabilisticdependences) are performed, that are sometimes very costly in computation time. Jensen et al.’s algorithm [21] (inspiredfrom inference algorithms used in Bayesian networks) consists in transforming the influence diagram in a junction tree,and then applying non-serial dynamic programming [5] in the junction tree. We have adopted this latter approach in ourimplementation, since its performances are generally assumed better than the ones of Schachter’s algorithm.1 Note that the insertion of a single edge (V , V(cid:3)(cid:3).such that V ≺ D(cid:3) ≺ VD(cid:3)), with V ∈ N A ∪ ND and V(cid:3) ∈ N A ∪ NU , is likely to create additional dependences on other variablesG. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891383Fig. 9. Enumeration tree.Branching rule. The branching rule we use is to set a value to a decision variable conditionally to the variables that in-fluence it. For instance, for the diagram in Fig. 8, one separates the set of strategies characterized by: (D 1 = α) ∧ (D2 =γ |D1 = α, A1 = η1, A2 = θ1) ∧ (D2 = δ|D1 = α, A1 = η2, A2 = θ1) ∧ (D2 = δ|D1 = α, A1 = η1, A2 = θ2) into two subsetscharacterized respectively by:– (D1 = α) ∧ (D2 = γ |D1 = α, A1 = η1, A2 = θ1) ∧ (D2 = δ|D1 = α, A1 = η2, A2 = θ1) ∧ (D2 = δ|D1 = α, A1 = η1,– (D1 = α) ∧ (D2 = γ |D1 = α, A1 = η1, A2 = θ1) ∧ (D2 = δ|D1 = α, A1 = η2, A2 = θ1) ∧ (D2 = δ|D1 = α, A1 = η1,A2 = θ2) ∧ (D2 = γ |D1 = α, A1 = η2, A2 = θ 2),A2 = θ2) ∧ (D2 = δ|D1 = α, A1 = η2, A2 = θ 2).The enumeration tree is represented in Fig. 9. The instanciated nodes are indicated in bold. The order in which the variablesare considered in the enumeration tree is compatible with the temporal order on the decision nodes (remember that therealways exists a path linking all the decision nodes in the diagram).Computing the lower bound. This is similar to what is done in decision trees. At each node of the enumeration tree, onecomputes a strategy optimizing EU in the subset of considered strategies, and one evaluates it according to RDU.Computing the upper bound. As for decision trees, we need to compute a lottery stochastically dominating any lotterycorresponding to a feasible strategy (so that its RDU value is an upper bound). For this purpose, we use a dynamic pro-gramming procedure. We recursively compute such a lottery for every decision variable and every possible instantiation ofthe variables in its past (compatible with decisions already set). In the following, for simplicity, we assume that there is onlyone single utility variable U , that takes value in {u1, . . . , um} with u1 (cid:3) · · · (cid:3) um. We use a backward induction procedure.The initialization in U is performed by the following formula:1384G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–1389∀i (cid:2) 1,P (U = ui|I0 · ·In, D1 · ·Dn) =(cid:17)1 if U (I0 · ·In, D1 · ·Dn) = ui0 otherwisewhere U (I0 ··In, D1 ··Dn) gives the value taken by variable U according to the values taken by variables I0, . . . , In, D1, . . . , Dn.Consider now a decision variable Dk. For a given realization of random variables I0, . . . , Ik−1 and a given choice of decisionsin D1, . . . , Dk−1, the stochastically dominating lottery in Dk is computed by the following recurrence relation for all i (cid:2) 1:P (U (cid:2) ui|I0 · ·Ik−1, D1 · ·Dk−1) = maxDkm(cid:3)(cid:3)j=iIkP (Ik|I0 · ·Ik−1, D1 · ·Dk) × P (U = u j|I0 · ·Ik−1, Ik, D1 · ·Dk)The stochastically dominating lottery is then easily determined by the following formula:P (U = ui|I0 · ·Ik−1, D1 · ·Dk−1) = P (U (cid:2) ui|I0 · ·Ik−1, D1 · ·Dk−1) − P (U (cid:2) ui+1|I0 · ·Ik−1, D1 · ·Dk−1)The value finally returned is computed by applying RDU to the lottery obtained in D1. Note that the conditionings are infact performed on much smaller sets of variables thanks to the independences displayed in the influence diagram. In orderto optimize the calculations, we used a junction tree, like in Jensen’s algorithm [21].5.4. Numerical testsThe two proposed algorithms (the two-phases method and the one operating directly in the influence diagram endowedwith fictitious dependences) have been implemented in C++ and the computational experiments were carried out on a PCwith a Pentium IV CPU 2.13 GHz processor and 3.5 GB of RAM (as for decision trees). We present below the results obtainedon randomly generated influence diagrams. We first describe the mechanism of the random generator. We then analyze theperformances of the algorithms, both in terms of computation time and solution quality. We used the same function ϕ asfor decision trees: ϕ(p) = pγ /(pγ + (1 − p)γ ), with γ = 0.2 (value of γ for which the probabilities are the most distorted).5.5. Random generatorIn order to control the size of the generated influence diagrams, all nodes of the diagram must be taken into accountwhen computing an optimal strategy. For this purpose, given a fixed number n of decision nodes, one first creates a path oflength 2n + 1 alternating decision variables and random variables, with a utility variable at the leaf node. For instance, fortwo decision variables, it gives:Then, to avoid that some variables in the diagram play no real role (i.e., have no impact on the choice of an optimalstrategy), one imposes that every random variable influences another random variable (as long as there exists at least onerandom variable in its future). Coming back to the previous example, one can obtain:To densify the diagram, edges are randomly inserted between some nodes. Concerning utilities and conditional prob-abilities, they were randomly generated. Finally, we used the greedy algorithm described in Section 5.3 to perform the(cid:5)-relaxation of consequentialism.5.6. Numerical resultsTo evaluate the computational gain when directly working in the influence diagram, we compared the execution timesof the implicit enumeration algorithm in the two-phases method with the execution times of the algorithm performing a(cid:5)-relaxation of consequentialism for various values of (cid:5). Table 7 indicates the execution times obtained (average time inseconds over 40 randomly generated instances for each entry). In each column, we vary the number of decision variables(i.e. we vary the value of n) and in each row is varied the value of (cid:5). The time taken by the two-phases method is indicatedin the last line of the table. Symbol “–” appears when there exist instances for which the execution time is very important(beyond 30 min). Not surprisingly, the more n and (cid:5) increase, the more the execution times increase. The two-phasesmethod does not make it possible to solve instances with more than 7 decision nodes. In comparison, the (cid:5)-relaxationmethod enables to solve instances with up to 10 decision nodes, at the cost of optimality.In order to evaluate the quality of the solutions returned by the (cid:5)-relaxation method, we compared their RDU valueswith the optimal ones in the corresponding decision trees (obtained by applying the two-phases method). Given an influenceG. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891385Table 7Execution time (sec).(cid:5)012345n40000002 ph.0.175000000.080.9360000.100.561.915.02Table 8Influence of parameter (cid:5) on strategy quality.n23456(cid:5) = 097%93%92%93%92%(cid:5) = 2100%96%93%94%93%70.210.230.380.721.073.4317.38(cid:5) = 398%95%95%93%80.850.621.102.334.4619.84–92.783.944.799.2518.6187.40–(cid:5) = 4100%97%95%95%109.1112.1523.3629.9793.52––(cid:5) = 597%97%96%∗(ID) (resp. RDU(cid:5)(ID)) the RDU value of a strategy optimizing RDU in the correspondingdiagram ID, we denote by RDUdecision tree (resp. optimizing RDU in ID after performing a (cid:5)-relaxation of consequentialism). In order to evaluate the∗(ID) for different valuesinfluence of parameter (cid:5) on the strategy quality (w.r.t. RDU), we computed ratio RDU(cid:5)(ID)/RDUof (cid:5) and n. For each value of n, 200 instances were randomly generated. Then, on these instances, we performed a (cid:5)-relaxation of consequentialism for (cid:5) varying from 0 to 5. Table 8 shows the average ratio obtained (in percentage) overthe 200 instances. The empty entries correspond to cases where all dependences are already present, and therefore thevalue is 100%. When (cid:5) = 0, one considers only consequentialist strategies. One can see that, for the instances consideredhere, the values of the returned strategies are significantly closer to the optimum for (cid:5) = 5 than for (cid:5) = 0 (thanks to theconsideration of some non-consequentialist strategies).6. ConclusionIn this article, we have proposed algorithms for optimizing RDU in sequential decision problems represented by decisiontrees or influence diagrams. The problem is NP-hard for both representations. For optimizing RDU in decision trees, we haveprovided a MIP formulation (adapted to risk seeker decision makers) and a branch and bound procedure (adapted to anyattitude toward risk). The upper bound used in the branch and bound procedure is computed by dynamic programming, andis polynomial in the number of decision nodes. Note that this upper bound is actually customizable to any decision criterioncompatible with stochastic dominance. The tests performed show that the dedicated branch and bound algorithm performsbetter than CPLEX applied to the MIP formulation. Our method makes it possible to solve efficiently random instances thenumber of decision nodes of which is near six thousands, and real-world instances the number of decision nodes of whichis 75 millions (thanks to the particular shape of the decision tree).However, when the number of stages grows, the decision tree formalism does not make it possible to compactly storethe problem instance. Influence diagrams, that expresses in a concise way the problem instance by using independencesbetween variables, are then very useful. When optimizing RDU in influence diagrams, we aim therefore at conciliating op-timality and compactness issues (determining a strategy the RDU value of which is close to optimum, and the storage ofwhich does not require too much memory space). We have shown that the space of strategies considered in an influencediagram is smaller than the space of strategies considered in the corresponding decision tree, since it is restricted to con-sequentialist strategies. For this reason, we have proposed an approach where the influence diagram is endowed with newedges representing fictitious dependences ((cid:5)-relaxation of consequentialism). This enables to enrich the space of strategieswith non-consequentialist strategies. Such strategies are indeed appreciated by RDU-maximizers. The numerical experimentscarried out show the interest of (cid:5)-relaxation since the RDU value of the computed strategy significantly improves the valueobtained for a consequentialist strategy.In order to enhance the approach proposed here, the most promising research direction is to identify crucial edges, i.e.edges whose insertion in the influence diagram would significantly increase the RDU value of the returned strategy. It wouldindeed limit the growth of the size of the diagram, and it would improve the quality of the returned strategy. In a differentresearch direction, it is worth investigating a new compact graphical model able to handle non-consequentialist decisioncriteria, since the influence diagram representation seems to be not very suitable for it.1386G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–1389Appendix A. ProofsFig. 10. An example of reduction.Proposition 1. The determination of an RDU-optimal strategy (problem RDU-OPT) in a decision tree is an NP-hard problem.Proof. The proof relies on a polynomial reduction from problem 3-SAT, which can be stated as follows:INSTANCE: a set X of boolean variables, a collection C of clauses on X such that |c| = 3 for every clause c ∈ C .QUESTION: does there exist an assignment of truth values to the boolean variables of X that satisfies simultaneously all theclauses of C ?Let X = {x1, . . . , xn} and C = {c1, . . . , cm}. We now show how to reduce problem 3-SAT to finding a strategy of valuegreater or equal to m in a polynomially generated decision tree. The polynomial generation of a decision tree from aninstance of 3-SAT is performed as follows. One defines a decision node for every variable of X . Given xi a variable in X ,the corresponding decision node in the decision tree, also denoted by xi , has two children: the first one (chance nodedenoted by T i ) corresponds to the statement ’xi has truth value “true”’, and the second one (chance node denoted by F i )corresponds to the statement ’xi has truth value “false”’. The subset of clauses which includes the positive (resp. negative)} ⊆ C ). For every clause cih (1 (cid:3) h (cid:3) j) one generates a childliteral of xi is denoted by {ci1 , . . . , ci j(cid:3)kof T i denoted by cih (terminal node). Besides, one generates an additional child of T i denoted by c0, corresponding to a(1 (cid:3) h (cid:3) k), as well as an additional childfictive consequence. Similarly, one generates a child of F i for every clause cicorresponding to fictive consequence c0. Node T i has therefore j + 1 children, while node F i has k + 1 children. In orderto make a single decision tree, one adds a chance node C predecessor of all decision nodes xi (1 (cid:3) i (cid:3) n). Finally, one addsa decision node as root, with C as unique child. The obtained decision tree includes n + 1 decision nodes, 2n + 1 chancenodes and at most 2n(m + 1) terminal nodes. Its size is therefore in O (nm), which guarantees the polynomiality of thetransformation. For the sake of illustration, in Fig. 10, we represent the decision tree obtained for the following instance of3-SAT: (x1 ∨ x2 ∨ x3) ∧ (x1 ∨ x3 ∨ x4) ∧ ( x2 ∨ x3 ∨ x4 ).} ⊆ C (resp. {ci, . . . , ci(cid:3)1(cid:3)hNote that one can establish a bijection between the set of strategies in the decision tree and the set of assignments inproblem 3-SAT. For that purpose, it is sufficient to set xi = 1 in problem 3-SAT iff edge (xi, T i) is included in the strategy,and xi = 0 iff edge (xi, F i) is included in the strategy. An assignment such that the entire expression is true in 3-SATcorresponds to a strategy such that every clause ci (1 (cid:3) i (cid:3) m) is a possible consequence (each clause appears thereforefrom one to three times). To complete the reduction, we now have to define, on the one hand, the probabilities assigned tothe edges from nodes C , T i and F i , and, on the other hand, the utilities of the consequences and function ϕ. The reductionconsists in defining them so that strategies maximizing RDU correspond to assignments for which the entire expression istrue in 3-SAT. More precisely, we aim at satisfying the following properties;(i) the RDU value of a strategy only depends on the set (and not the multiset) of its possible consequences (in otherwords, the set of clauses that become true with the corresponding assignment),(ii) the RDU value of a strategy corresponding to an assignment that makes satisfiable the 3-SAT expression equals m;G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891387(iii) if a strategy yields a set of possible consequences that is strictly included in the set of possible consequences ofanother strategy, the RDU value of the latter is strictly greater.For that purpose, after assigning probability 1n to edges originating from C , one defines the other probabilities andutilities as follows (i (cid:16)= 0):⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩pi =(cid:14)(cid:15)i110i(cid:3)u(ci) =10 j−1j=1where pi is the probability assigned to all the edges leading to consequence ci . For the edges of type (T j, c0) (or (F j, c0)),one sets u(c0) = 0 and one assigns a probability such that all the probabilities of edges originating from T j (or F j ) sumup to 1. Note that this latter probability is positive since the sum of pi ’s is strictly smaller than 1 by construction. Finally,function ϕ is defined as follows2:⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩0if p ∈piif p ∈1if p ∈(cid:19)(cid:19)(cid:19)ϕ(p) =(cid:15)0; pmnpi+1n; pin(cid:15)p1n; 1(cid:15)for i < mFor the sake of illustration, we now give function ϕ obtained for the instance of 3-SAT indicated above:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ϕ(p) =(cid:19)0if p ∈0;(cid:15)14 × 10001100110if p ∈if p ∈(cid:19)(cid:19)(cid:19)1if p ∈;14 × 100(cid:15)14 × 10(cid:15)14 × 100014 × 10014 × 10; 1;(cid:15)In the following, we consider some strategy (cid:5), inducing a lottery denoted by L, and we denote by I ⊆ {0, . . . , m} theset of indices of the possible consequences of (cid:5). Note that consequence c0 is always present in a strategy (cid:5). We denoteby αi ∈ {1, 2, 3} the number of occurrences of consequence ci in (cid:5). By abuse of notation, we use indifferently ci and u(ci)below.Proof of (i). The RDU value of (cid:5) is RDU(L) = c0 × ϕ(1) +j < i}. We now show that ∀i ∈ I, ϕ((cid:2)(cid:2)p jn ) = ϕ(j∈Ij(cid:2)iα jj∈Ij(cid:2)i(cid:21)p jn(cid:20)(cid:3)ϕj∈Ij(cid:2)i(cid:3) ϕ(cid:20)(cid:3)j∈Ij(cid:2)i(cid:21)α jp jn(cid:3) ϕ(cid:21)3p jn(cid:20)(cid:3)j∈Ij(cid:2)i(cid:2)i∈I (ci − cprevI (i))ϕ((cid:2)j∈Ij(cid:2)ip jn ). By increasingness of ϕ, we haveα jp jn ) where prevI (i) = max{ j ∈ I :Therefore(cid:20)ϕ(cid:21)(cid:15)j(cid:14)1n110(cid:3) ϕ(cid:20)(cid:3)j∈Ij(cid:2)i(cid:21)α jp jn(cid:3) ϕ(cid:20)(cid:3)j∈Ij(cid:2)i(cid:21)(cid:15)j(cid:14)3n110(cid:3)j∈Ij(cid:2)i2 Note that function ϕ is not strictly increasing here, but the reader can easily convince himself that it can be slightly adapted so that it becomes strictlyincreasing.1388SinceG. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–1389(cid:20)(cid:3)ϕj∈Ij(cid:2)i(cid:21)(cid:15)j(cid:14)1n110= ϕ(cid:21)(cid:15)j(cid:14)3n110(cid:20)(cid:3)j∈Ij(cid:2)i= pi−1we have by bounding(cid:21)(cid:20)(cid:3)ϕj∈Ij(cid:2)iHenceα jp jn= ϕ(cid:21)p jn(cid:20)(cid:3)j∈Ij(cid:2)iRDU(L) =(cid:3)(ci − cprevI (i))ϕi∈I(cid:21)p jn(cid:20)(cid:3)j∈Ij(cid:2)isince c0 × ϕ(1) = 0Proof of (ii). Consider a strategy (cid:5)∗∗Lwhere all the consequences ci of C are possible. By (i), we have(cid:20)(cid:21)(cid:6)(cid:7)∗=RDULm(cid:3)(ci − ci−1)ϕm(cid:3)i=1j=ip jn(cid:2)corresponding to an assignment that makes the expression true, and the induced lotteryWe note that for all i (cid:3) m, (ci − ci−1)ϕ(mj=ip jn ) = 10i−1 × pi−1 = 10i−1 × ( 110 )i−1 = 1. Consequently, RDU(L∗) = m.Proof of (iii). Let (cid:5) (resp. (cid:5)(cid:3)) and let I ⊆ {0, . . . , m} (resp.) denote some strategy the induced lottery of which is L (resp. LJ = I ∪ {k}) denote the set of indices of its possible consequences. We assume here that k < max I , the case k = max I beingobvious. By definition, {i ∈ I: i (cid:16)= k} = {i ∈ J : i (cid:16)= k}. We can therefore state the RDU value as a sum of three terms:(cid:21)(cid:20)(cid:21)(cid:20)(cid:20)(cid:21)(cid:3)(cid:3)(cid:3)RDU(L) =(ci − cprev J (i))ϕi∈ Ji(cid:3)k−1j∈Ij(cid:2)ip jn+ (ck − cprev J (k))ϕ(cid:3)p jn+(cid:3)j∈Ij(cid:2)k(ci − cprev J (i))ϕi∈ Ji(cid:2)k+1j∈ Jj(cid:2)i(cid:3)p jncan also be stated as a sum of three terms:(cid:20)(cid:21)(cid:20)(cid:21)p jn+ (ck − cprev J (k))ϕp jn+(cid:3)j∈ Jj(cid:2)k(ci − cprev J (i))ϕ(cid:3)i∈ Ji(cid:2)k+1(cid:21)p jn(cid:20)(cid:3)j∈ Jj(cid:2)iSimilarly, the RDU value of strategy (cid:5)(cid:3)(cid:6)RDUL(cid:7)(cid:3)=(cid:3)(ci − cprev J (i))ϕi∈ Ji(cid:3)k−1By increasingness of ϕ, we have(cid:3)j∈ Jj(cid:2)i(cid:21)I ⊆ J ⇒ ∀i (cid:3) k − 1, ϕ(cid:20)(cid:3)j∈Ij(cid:2)ip jn(cid:3) ϕ(cid:21)p jn(cid:20)(cid:3)j∈ Jj(cid:2)i(cid:2)Thus the first term of RDU(L) is smaller or equal to the first term of RDU(Lp jn ) =p jn ) = pprev J (k) = pk−1, where succI (i) = min{ j ∈ I: j > i}. But psuccI (k)−1 < pk−1 since succI (k) − 1 >(cid:3)). Finally, the third term ofpsuccI (k)−1 and ϕ(k − 1. Therefore the second term of RDU(L) is strictly smaller than the second term of RDU(LRDU(L) is of course equal to the third term of RDU(L(cid:3)). Consequently RDU(L) < RDU(L(cid:3)). One checks easily that ϕ(j∈Ij(cid:2)kj∈ Jj(cid:2)k(cid:3)).(cid:2)From (i), (ii) and (iii) we conclude that any strategy corresponding to an assignment that does not make the expressiontrue has a RDU value strictly smaller than m, and that any strategy corresponding to an assignment that makes the ex-pression true has a RDU value exactly equal to m. Solving 3-SAT reduces therefore to determining a strategy of value m inRDU-OPT. (cid:2)G. Jeantet, O. Spanjaard / Artificial Intelligence 175 (2011) 1366–13891389References[1] M. Abdellaoui, Parameter-free elicitation of utilities and probabilities weighting functions, Management Science 46 (11) (2000) 1497–1512.[2] M. Allais, Le comportement de l’homme rationnel devant le risque: critique des postulats de l’école américaine, Econometrica 21 (1953) 503–546.[3] M. Allais, An outline of my main contributions to economic science, The American Economic Review 87 (6) (1997) 3–12.[4] K.J. Arrow, The Theory of Risk Aversion, Ynjo Jahnsonin Saatio, Helsinki, 1965.[5] U. Bertelé, F. Brioschi, Nonserial Dynamic Programming, Academic Press, 1972.[6] J. Blythe, Decision-theoretic planning, AI Magazine 20 (1999) 1–15.[7] C. Boutilier, T. Dean, S. Hanks, Decision-theoretic planning: Structural assumptions and computational leverage, Journal of AI Research 11 (1999) 1–94.[8] C. Camerer, T. Ho, Non-linear weighting of probabilities and violations of the betweenness axiom, Journal of Risk and Uncertainty 8 (1994) 167–196.[9] A. Chateauneuf, Comonotonicity axioms and rank-dependent expected utility theory for arbitrary consequences, Journal of Mathematical Economics 32(1999) 21–45.[10] E. Damiani, S. De Capitani di Vimercati, P. Samarati, M. Viviani, A WOWA-based aggregation technique on trust values connected to metadata, ElectronicNotes in Theoretical Computer Science 157 (2006) 131–142.[11] T. Dean, L. Kaelbling, J. Kirman, A. Nicholson, Planning with deadlines in stochastic domains, in: Proc. of the 11th AAAI, 1993, pp. 574–579.[12] D. Dubois, H. Prade, R. Sabbadin, Decision-theoretic foundations of qualitative possibility theory, European Journal of Operational Research 128 (3)(2001) 459–478.[13] R. Gonzales, G. Wu, On the shape of the probability weighting function, Cognitive Psychology 38 (1999) 129–166.[14] P. Hammond, Consequentialism and the independence axiom, in: Risk, Decision and Rationality, D. Reidel Publishing Co, Dordrecht, Holland, 1988,pp. 503–515.[15] P. Hammond, Consequentialist foundations for expected utility, Theory and Decision 25 (1) (1988) 25–78.[16] J. Handa, Risk, probabilities and a new theory of cardinal utility, Journal of Political Economics 85 (1977) 97–122.[17] R. Howard, J. Matheson, Risk-sensitive Markov decision processes, Management Science 18 (7) (1972) 356–369.[18] R. Howard, J. Matheson, Influence diagrams, in: Readings on the Principles and Applications of Decision Analysis, Strategic Decisions Group, MenloPark, CA, 1984, pp. 721–762. Reprinted: Decision Analysis 2 (3) (2005) 127–143.[19] J.-Y. Jaffray, T. Nielsen, An operational approach to rational decision making based on rank dependent utility, European Journal of Operational Re-search 169 (1) (2006) 226–246.[20] G. Jeantet, O. Spanjaard, Rank-dependent probability weighting in sequential decision problems under uncertainty, in: Proc. of the International Con-ference on Automated Planning and Scheduling (ICAPS), 2008, pp. 148–155.[21] F. Jensen, F.V. Jensen, S.L. Dittmer, From influence diagrams to junction trees, in: Proc. of the Tenth Annual Conference on Uncertainty in ArtificialIntelligence (UAI), 1994, pp. 367–373.[22] L. Kaebling, M. Littman, A. Cassandra, Planning and acting in partially observable stochastic domains, Artificial Intelligence 101 (1999) 99–134.[23] D. Kahneman, A. Tversky, Prospect theory: An analysis of decision under risk, Econometrica 47 (1979) 263–291.[24] U. Karmarkar, Subjectively weighted utility and the Allais paradox, Organisational Behavior and Human Performance 24 (1) (1979) 67–72.[25] S. Koenig, R.G. Simmons, Risk-sensitive planning with probabilistic decision graphs, in: Proc. of the International Conference on Principles of KnowledgeRepresentation and Reasoning (KR), 1994, PP. 363–373.[26] Y. Liu, S. Koenig, Existence and finiteness conditions for risk-sensitive planning: Results and conjectures, in: Proc. of the Twentieth Annual Conferenceon Uncertainty in Artificial Intelligence (UAI), 2005, pp. 354–363.[27] Y. Liu, S. Koenig, Risk-sensitive planning with one-switch utility functions: Value iteration, in: Proc. of the Twentieth National Conference on ArtificialIntelligence, AAAI, 2005, pp. 993–999.[28] Y. Liu, S. Koenig, An exact algorithm for solving MDPs under risk-sensitive planning objectives with one-switch utility functions, in: Proc. of theInternational Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2008, pp. 453–460.[29] R. Luce, H. Raiffa, Games and Decisions, John Wiley, New York, 1957.[30] M.J. Machina, Dynamic consistency and non-expected utility models of choice under uncertainty, Journal of Economic Literature 27 (4) (1989) 1622–1668.[31] E. McClennen, Rationality and Dynamic Choice: Foundational Explorations, Cambridge University Press, 1990.[32] T. Morin, Monotonicity and the principle of optimality, Journal of Mathematical Analysis and Applications 86 (1982) 665–674.[33] T.D. Nielsen, F.V. Jensen, Learning a decision maker’s utility function from (possibly) inconsistent behavior, Artificial Intelligence Journal 160 (2004)53–78.[34] W. Ogryczak, T. Sliwinski, On decision support under risk by the WOWA optimization, in: ECSQARU 2007, pp. 779–790.[35] W. Ogryczak, WOWA enhancement of the preference modeling in the reference point method, in: Proc. of the Fifth International Conference onModeling Decisions for Artificial Intelligence, in: Lecture Notes in Artificial Intelligence, vol. 5285, Springer, 2008, pp. 38–49.[36] W. Ogryczak, T. Sliwinski, On efficient WOWA optimization for decision support under risk, International Journal of Approximate Reasoning 50 (2009)915–928.[37] F. Perea, J. Puerto, Dynamic programming analysis of the TV game “Who wants to be a millionaire?, European Journal of Operational Research 183(2007) 805–811.[38] P. Perny, O. Spanjaard, L.-X. Storme, State space search for risk-averse agents, in: 20th International Joint Conference on Artificial Intelligence, 2007,pp. 2353–2358.[39] J. Pratt, Risk aversion in the small and in the large, Econometrica 32 (1) (1964) 122–136.[40] J. Quiggin, Generalized Expected Utility Theory: The Rank-Dependent Model, Kluwer, 1993.[41] H. Raiffa, Decision Analysis: Introductory Lectures on Choices under Uncertainty, Addison–Wesley, 1968.[42] M. Rotshild, J. Stiglitz, Increasing risk I: A definition, Journal of Economic Theory 2 (3) (1970) 225–243.[43] L. Savage, The Foundations of Statistics, Wiley, 1954.[44] R. Shachter, Evaluating influence diagrams, Operations Research 34 (1986) 871–882.[45] V. Torra, Weighted OWA operators for synthesis of information in: Proc. of the Fifth IEEE International Conference on Fuzzy Systems (IEEE-FUZZ’96),1996, pp. 966–971.[46] V. Torra, The weighted OWA operator, International Journal of Intelligent Systems 12 (1997) 153–166.[47] V. Torra, The WOWA operator and the interpolation function W∗: Chen and Otto’s interpolation method revisited, Fuzzy Sets and Systems 113 (3)(2000) 389–396.[48] A. Tversky, D. Kahneman, Advances in prospect theory: Cumulative representation of uncertainty, Journal of Risk and Uncertainty 5 (1992) 297–323.[49] J. von Neumann, O. Morgenstern, Theory of Games and Economic Behaviour, Princeton University Press, 1947.