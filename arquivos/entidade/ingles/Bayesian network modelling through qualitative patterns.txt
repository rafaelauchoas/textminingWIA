Artificial Intelligence 163 (2005) 233–263www.elsevier.com/locate/artintBayesian network modelling throughqualitative patternsPeter J.F. LucasInstitute for Computing and Information Sciences, Radboud University Nijmegen, Toernooiveld 1,6525 ED Nijmegen, The NetherlandsReceived 10 July 2004; accepted 31 October 2004Available online 15 December 2004AbstractIn designing a Bayesian network for an actual problem, developers need to bridge the gap betweenthe mathematical abstractions offered by the Bayesian-network formalism and the features of theproblem to be modelled. Qualitative probabilistic networks (QPNs) have been put forward as quali-tative analogues to Bayesian networks, and allow modelling interactions in terms of qualitative signs.They thus have the advantage that developers can abstract from the numerical detail, and thereforethe gap may not be as wide as for their quantitative counterparts. A notion that has been suggested inthe literature to facilitate Bayesian-network development is causal independence. It allows exploitingcompact representations of probabilistic interactions among variables in a network. In the paper, wedeploy both causal independence and QPNs in developing and analysing a collection of qualitative,causal interaction patterns, called QC patterns. These are endowed with a fixed qualitative semantics,and are intended to offer developers a high-level starting point when developing Bayesian networks. 2004 Elsevier B.V. All rights reserved.Keywords: Bayesian networks; Knowledge representation; Qualitative reasoning1. IntroductionReasoning with uncertainty is a significant area of research in Artificial Intelligence atleast since the early 1970s. Many different methods for representing and reasoning withE-mail address: peterl@cs.kun.nl (P.J.F. Lucas).0004-3702/$ – see front matter  2004 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2004.10.011234P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263uncertain knowledge have been developed during the last three decades, including thecertainty-factor calculus [2,22], Dempster-Shafer theory [21], possibilistic logic [8], fuzzylogic [29], and Bayesian networks, also called belief networks and causal probabilistic net-works [3,18,19]. During the last decade a gradual shift towards the use of probability theoryas the foundation of almost all of the work in this area could be observed, mainly due tothe impact, both theoretically and practically, of the introduction of Bayesian networks andrelated graphical probabilistic models into the field.Bayesian networks offer a powerful framework for the modelling of uncertain interac-tions among variables in a given domain. Such interactions are represented in two differentmanners: firstly, in a qualitative manner, by means of a directed acyclic graph, and sec-ondly, in a quantitative manner, by specifying a conditional probability distribution forevery variable represented in the network. These conditional probability distributions allowfor expressing various logical, functional and probabilistic relationships among variables.Much of the appeal of the Bayesian network formalism derives from this feature (cf. [3]for a modern, technical overview).It is well known that ensuring that the graph topology of a Bayesian network is sparseeases the assessment of its underlying joint probability distribution, as the required prob-ability tables will then be relatively small. Unfortunately, designing a network with atopology that is sparse is neither easy nor always possible. Researchers have thereforeidentified special types of independence relationships in order to facilitate the process ofprobability assessment. In particular the theory of causal independence fulfils this pur-pose [15]. The theory allows for the specification of the interactions among variables interms of cause-effect relationships, adopting particular statistical independence assump-tions. Causal independence is frequently used in the construction of practical networks forsituations where the underlying probability distributions are complex. The theory has alsobeen exploited to increase the efficiency of probabilistic inference in Bayesian networks[30,31]. A limitation of the theory of causal independence is that it is usually unclear withwhat sort of qualitative behaviour a network will be endowed when choosing for a particu-lar interaction type. As a consequence, only two types of interaction are in frequent use: thenoisy-OR and the noisy-MAX; in both cases, interactions among variables are modelledas being disjunctive [4,16,19].Qualitative probabilistic networks offer a qualitative analogue to the formalism ofBayesian networks. They allow describing the dynamics of the interaction among variablesin a purely qualitative fashion by means of the specification and propagation of qualitativesigns [6,7,20,28]. Hence, qualitative probabilistic networks abstract from the numericaldetail, yet retain the qualitative semantics underlying Bayesian networks. The theory ofqualitative probabilistic networks, therefore, seems to offer potentially useful tools for thequalitative analysis of Bayesian networks.The aim of the present work was to develop a theory of qualitative, causal interactionpatterns, QC patterns for short, in the context of Bayesian networks. Such a theory couldassist developers of systems based on Bayesian networks in designing such networks, ex-ploiting the qualitative information that is available in the domain concerned as much aspossible. In the paper, various interaction types are defined using Boolean algebra; qual-itative probabilistic networks are then used to provide a qualitative semantic foundationfor these interactions. The Bayesian-network developer is supposed to utilise the theoryP.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263235by selecting appropriate interaction patterns based on domain properties, which thus canguide Bayesian-network development.The remainder of this paper is organised as follows. In the following section, the basicproperties of Bayesian networks are introduced, as are Boolean functions, and the no-tions of causal independence and qualitative probabilistic networks. We start the analysisby considering various causal-independence models, unravelling the qualitative behaviourof these causal models using qualitative probabilistic networks in Section 3. Section 4summarises the various patterns that have been obtained, and discusses these results inthe context of all possible patterns. Finally, in Section 5, it is summarised what has beenachieved by this research.2. PreliminariesTo start, the basic theory of Bayesian networks, causal independence and qualitativeprobabilistic networks are reviewed.2.1. Bayesian networksA Bayesian network is a concise representation of a joint probability distribution on aset of statistical variables [19]. It consists of a qualitative part and an associated quan-titative part. The qualitative part is a graphical representation of the interdependencesbetween the variables in the encoded distribution. It takes the form of an acyclic directedgraph (digraph) G = (V (G), A(G)), where each node V ∈ V (G) corresponds to a sta-tistical variable that takes one of a finite set of values, and A(G) ⊆ V (G) × V (G) is aset of arcs. In this paper, we assume all variables to be binary; for abbreviation, we willoften use v to denote V = (cid:3) (true) and ¯v to denote V = ⊥ (false). Sometimes, we pre-fer to leave the specific value of a variable open (i.e., it is taken as a free variable), andthen we simply state V . In other cases, we use this notation when a variable is actuallybound. The context will make clear which interpretation is intended. Furthermore, for ab-breviation, we use the notation V1, . . . , Vn\Vi, . . . , Vj which stands for the set of variables{V1, V2, . . . , Vi−1, Vi+1, . . . , Vj −1, Vj +1, . . . , Vn}. Furthermore, an expression such as(cid:1)g(I1, . . . , In)ψ(I1,...,In)=estands for summing over g(I1, . . . , In) for all possible values of the variables Ik for whichthe constraint ψ(I1, . . . , In) = e holds. However, if we refer to variables separate fromsuch constraints, such as in(cid:1)g(I1, . . . , In)I1,I2ψ(I1,...,In)=ethen we only sum over the separately mentioned variables, here the variables I1, I2, andthe equality only acts as a constraint.The arcs A(G) in the digraph G model possible dependences between the representedvariables. Informally speaking, we take an arc V → V (cid:6) between the nodes V and V (cid:6) to236P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263represent an influential relationship between the associated variables V and V (cid:6). If this arcis given a causal reading, then the arc’s direction marks V (cid:6) as the effect of the cause V .Absence of an arc between two nodes means that the corresponding variables do not in-fluence each other directly and, hence, are (conditionally) independent. In the following,causes will often be denoted by Ci and their associated effect variable by E.Associated with the qualitative part of a Bayesian network are numerical quantities fromthe encoded probability distribution. With each variable V in the digraph is associated aset of conditional probabilities Pr(V | π(V )), describing the joint influence of values forthe parents π(V ) of V on the probabilities of the variable V ’s values. These sets of prob-abilities constitute the quantitative part of the network. A Bayesian network represents ajoint probability distribution on its variables and thus provides for computing any proba-bility of interest. Various algorithms for probabilistic inference with a Bayesian networkare available [19,23,30].Bayesian networks are successfully applied in a growing number of fields; biomed-ical applications in particular have attracted a great deal of research activity (cf. [1,5,11,12,24,25]). This may be due to the fact that biological mechanisms can often be de-scribed quite naturally in causal terms. Consider, for example, the causal network shownin Fig. 1, which models the causal mechanisms by which patients become colonised bybacteria, for example Pseudomonas aeruginosa, after admission to a hospital. As the actualnames of the bacteria do not matter here, they are simply called A, B and C. After hav-ing been colonised, the patient’s body responds to the bacteria in various ways, dependingon the bacteria concerned; in the end an infection may develop. An infection is clinicallyrecognised by signs and symptoms such as fever, high white blood cell count (WBC),and increased sedimentation rate of the blood (ESR). Clearly, the probability distributionPr(Infection | BRA, BRB , BRC) specified for the network, where BRX stands for ‘Bodyresponse to X’, is of great importance in modelling interactions among the various mech-anisms causing infection; the actual type of interaction depends on the bacteria involved.Fig. 1. Example Bayesian networks, modelling the interaction among bacteria possibly causing an infection inpatients after colonisation.P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263237Fig. 2. Example Bayesian network, modelling the interaction between the antimicrobial agents penicillin andchlortetracyclin on infection.As a second example, consider the interaction between bactericidal antimicrobialagents, i.e., drugs that kill bacteria by interference with their metabolism, and bacteriosta-tic antimicrobial agents, i.e., drugs that inhibit the multiplication of bacteria. Penicillin isan example of a bactericidal drug, whereas chlortetracyclin is an example of a bacteriosta-tic drug. It is well known among medical doctors that the interaction between bactericidaland bacteriostatic drugs can have antagonistic effects; e.g., the drug combination penicillinand chlortetracyclin may be have as little effect against an infection as prescribing no an-timicrobial agent at all, even if the bacteria are susceptible to each of these drugs. Notethat here we interpret drugs as statistical variables, not as decision variables as in clinicaldecision making. The depiction of the causal interaction of the relevant variables is shownin Fig. 2; note the similarity in structure of this network in comparison to Fig. 1.As a last example, this time not concerning infectious disease, consider the interactionbetween natural hormones that have partially related, but possibly opposite, working mech-anisms, such as insulin and glucagon: two hormones that are involved in the regulation ofglucose levels in the blood. Insulin is needed to let glucose cross the membrane of most ofthe body cells (exceptions are the brain cells, where, as a protective mechanisms, glucosetransfer is not insulin dependent) so that it can be utilised as fuel in the cell metabolism.In this way glucose is transferred from blood to cytoplasm. Glucagon, on the other hand,stimulates the release of glucose from the glycogen deposites, such as the liver, into theblood. In order for glucagon to be effective, it is necessary that insulin is present, as oth-erwise there will be little glucose stored in the body cells as glycogen. Too high levels ofinsulin, insulin hypersecretion, as may occur in tumours called insulinomas, may give riseto hypoglycaemia, i.e., abnormally low glucose levels in the blood. In all other cases, levelsof glucose in the blood will not be abnormally low (although the levels may be too high,but this is not considered an acute danger). The causal interaction of the relevant variablesis shown in Fig. 3.238P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263Fig. 3. Example Bayesian network, modelling the interaction between insulin and glucagon secretion from hor-monal gland tissue.Although the Bayesian networks shown in Figs. 1, 2 and 3 have a very similar struc-ture, their underlying interaction semantics is very different as we will see below. Thesenetworks will be used in the following as running examples to illustrate some of the results.2.2. Causal independenceIn this section, we introduce a type of cause-effect interaction, called causal indepen-dence, which essentially is a causal model with rather strong independence assumptions.2.2.1. Probabilistic representationOne popular way to specify interactions among statistical variables in a compact fash-ion is offered by the notion of causal independence [10,13–15]. The global structure of acausal-independence model is shown in Fig. 4; it expresses the idea that causes C1, . . . , Cninfluence a given common effect E through intermediate variables I1, . . . , In and a deter-ministic function f , called the interaction function. The influence of each cause Ck on thecommon effect E is independent of each other cause Cj , j (cid:7)= k. The function f representsin which way the intermediate effects Ik, and indirectly also the causes Ck, interact to yielda final effect E. Hence, this function f is defined in such way that when a relationship, asmodelled by the function f , between Ik, k = 1, . . . , n, and E = (cid:3) is satisfied, then it holdsthat e = f (I1, . . . , In).In terms of probability theory, the notion of causal independence can be formalised forthe occurrence of effect E, i.e., E = (cid:3), as follows:(cid:1)Pr(e | C1, . . . , Cn) =Pr(e | I1, . . . , In) Pr(I1, . . . , In | C1, . . . , Cn)(1)f (I1,...,In)=emeaning that the causes C1, . . . , Cn influence the common effect E through the inter-mediate effects I1, . . . , In only when e = f (I1, . . . , In) for certain values of Ik, k =1, . . . , n. Under this condition, it is assumed that Pr(e | I1, . . . , In) = 1; otherwise, whenP.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263239Fig. 4. Causal independence model.f (I1, . . . , In) = ¯e, it holds that Pr(e | I1, . . . , In) = 0. Note that the effect variable E isconditionally independent of C1, . . . , Cn given the intermediate variables I1, . . . , In, andthat each variable Ik is only dependent on its associated variable Ck; hence, it holds thatPr(e | I1, . . . , In, C1, . . . , Cn) = Pr(e | I1, . . . , In)andPr(I1, . . . , In | C1, . . . , Cn) =n(cid:2)k=1Pr(Ik | Ck).Formula (1) can now be simplified to:Pr(e | C1, . . . , Cn) =(cid:1)n(cid:2)f (I1,...,In)=ek=1Pr(Ik | Ck).Based on the assumptions above, it also holds thatPr(e | C1, . . . , Cn) =(cid:1)I1,...,InPr(e | I1, . . . , In)n(cid:2)k=1Pr(Ik | Ck).(2)(3)Finally, it is assumed that Pr(ik | ¯ck) = 0 (absent causes do not contribute to the effect);otherwise, the probabilities Pr(Ik | Ck) are assumed to be positive.Formula (2) is practically speaking not very useful, because the size of the specificationof the function f is exponential in the number of its arguments. The resulting probabil-ity distribution is therefore in general computationally intractable, both in terms of spaceand time requirements. An important subclass of causal independence models, however, isformed by models in which the deterministic function f can be defined in terms of sepa-rate binary functions gk, also denoted by gk(Ik, Ik+1). Such causal independence modelshave been called decomposable causal independence models [14]; these models are ofsignificant practical importance. Usually, all functions gk(Ik, Ik+1) are identical for eachk; a function gk(Ik, Ik+1) may therefore be simply denoted by g(I, I (cid:6)). Typical examplesof decomposable causal independence models are the noisy-OR [4,10,16,19,26] and noisy-MAX [4,15,26] models, where the function g represents a logical OR and a MAX function,respectively.240P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–2632.2.2. Boolean functionsThe function f in Eq. (2) is actually a Boolean function; recall that there are 22ndif-ferent n-ary Boolean functions [9,27]. Hence, the potential number of causal interactionmodels is huge. The Boolean functions can also be represented by the probabilitiesPr(e | I1, . . . , In)in Eq. (3), with Pr(e | I1, . . . , In) ∈ {0, 1}.As mentioned above, in the case of causal independence it is usually assumed that thefunction f is decomposable, and that all binary functions gk of which f is composed areidentical. As there are 16 different binary Boolean functions, and a causal interaction modelcontains at least two causes, there are at least 16 n-ary Boolean functions, with n (cid:1) 2, inthat case. Some of these Boolean functions can be interpreted as a Boolean expression ofthe formI1 (cid:8) · · · (cid:8) In = Ewhere (cid:8) is a binary, associative Boolean operator. However, not every binary Booleanoperator is associative; Table 1 mentions which operators are associative and which arenot.As a matter of notation, in the following we will frequently make use of the abbrevia-tion:Ij = (· · · (I1 (cid:8) I2) (cid:8) · · ·) (cid:8) Ij −1) (cid:8) Ij )(4)Table 1The binary Boolean operatorsCommutative, associative operators∧∨↔(cid:12)(cid:3)⊥↓|p1p2n1n2→←<>ANDORbi-implicationXOR, exclusive ORalways truealways falseCommutative, non-associative operatorsNORNANDNon-commutative, associative operatorsprojection to the first argumentprojection to the second argumentnegation of first argumentnegation of second argumentNon-commutative, non-associative operatorsimplicationreverse implicationincreasing orderdecreasing orderP.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263241if it is assumed that the Boolean operator (cid:8) is left associative. Similarly, the notationIj = (Ij (cid:8) (Ij +1 (cid:8) (· · · (cid:8) (In−1 (cid:8) In) · · ·)(5)is used if it is assumed that the operator (cid:8) is right associative. Note that In ≡ I1 if (cid:8)is associative. In that case, we will simply use the notation In−1 to denote the Booleanexpression with one variable less than In, as usually will become clear from the context.Finally, note that a Boolean operator (cid:8) need not be commutative, and hence i1 (cid:8) In−1 =In−1 (cid:8) i1, where In−1 = I2 (cid:8) · · · (cid:8) In, need not hold. Table 1 also indicates which of theoperators are commutative and which are not.The commutative and associative binary operators mentioned in Table 1 give rise toBoolean expressions that are special cases of symmetric Boolean functions (the two com-mutative, non-associative operators are only symmetric for two arguments). A Booleanfunction f is symmetric iff (I1, . . . , In) = f (Ij1, . . . , Ijn)for any index function j : {1, . . . , n} → {1, . . . , n} [27]. An example of a symmetricBoolean functions is the exact Boolean function ek, which is defined as:(cid:4)nj =1 ν(Ij ) = k,(cid:3)ek(I1, . . . , In) =(6)(cid:3) if⊥ otherwise,with k ∈ N, and(cid:3)ν(I ) =1 if I = (cid:3),0 otherwise.Hence, this function simply checks whether there are k cases where Ij is true. The follow-ing basic properties of the exact Boolean function are useful in establishing properties ofsymmetric Boolean functions explored below.Lemma 1. Let ek(I1, . . . , In) be the exact Boolean function, then:(1) ∀k ∈ N ∀I1, . . . , In: ek(I1, . . . , ij , . . . , In) ∧ ek(I1, . . . , ¯ıj , . . . , In) ≡ ⊥, and(2) ∀I1, . . . , In ∃k ∈ N: ek(I1, . . . , ij , . . . , In) ≡ ek−1(I1, . . . , ¯ıj , . . . , In), and(3) ∀I1, . . . , In ∃k ∈ N ∀l ∈ N, l (cid:7)= k, l (cid:7)= k − 1: ek(I1, . . . , ij , . . . , In) (cid:2) ¬el(I1, . . . ,Ij , . . . , In).Proof. Straight from the definition. (cid:1)Another useful symmetric Boolean function is the threshold function tk, which simplychecks whether there are at least k trues among the arguments:(cid:4)(cid:3)tk(I1, . . . , In) =nj =1 ν(Ij ) (cid:1) k,(cid:3) if⊥ otherwise.Symmetric Boolean functions can be decomposed in terms of the exact functions ek asfollows [27]:f (I1, . . . , In) =n(cid:5)k=0ek(I1, . . . , In) ∧ ck(7)242P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263where ck are Boolean constants only dependent of the function f . For example, for theBoolean function defined in terms of the AND operator we have: c0 = · · · = cn−1 = ⊥ andcn = (cid:3), for the Boolean function defined in terms of the OR operator we have c0 = ⊥ andc1 = · · · = cn = (cid:3), and for the XOR operator we have that ck = ⊥ if even(k) and ck = (cid:3)if odd(k).Symmetric functions are generally not decomposable in the sense of the previous sec-tion, but as the exact function ek simply checks sums, a symmetric Boolean function cannevertheless be split into parts using equality (7).We return to our example Bayesian-network model shown in Fig. 1. If we assume thatthe bacteria A, B and C are all pathogenic, and thus give rise to an infectious response ifthe patient becomes colonised by them, the interaction among the ‘Body response’ vari-ables can be modelled by a logical OR, ∨. This expresses the idea that an infection must becaused by one or more pathogenic bacteria. The interaction between penicillin and chlorte-tracyclin as depicted in Fig. 2 can be described my means of an exclusive OR, (cid:12), aspresence of either of these in the patient’s body tissues leads to a decrease in bacterialgrowth, whereas if both are present or absent, there will be little or no effect on bacterialgrowth. The interaction between insulin and glucagon secretion as shown in Fig. 3 canbe described my means of the decreasing order operator, >, as insulin hypersecretion is acause of hypoglycaemia, but only if there is no glucagon hypersecretion. If there is onlyglucagon hypersecretion, we will not have hypoglycaemia, whereas if we have neither in-sulin hypersecretion nor glucagon hypersecretion, hypoglycaemia does not occur either.In the following we use the interaction between various types of bacteria as examples toillustrate how Boolean functions can be used to model different interactions with their as-sociated meanings. The way penicillin and chlortetracyclin interact, as well as insulin andglucagon, are, however, kept fixed.2.3. Qualitative probabilistic networksQualitative probabilistic networks, or QPNs for short, are qualitative abstractions ofBayesian networks, bearing a strong resemblance to their quantitative counterparts [28].A qualitative probabilistic network equally comprises a graphical representation of theinterdependences between statistical variables, once again taking the form of an acyclicdigraph. Instead of conditional probabilities, however, a qualitative probabilistic networkassociates signs with its digraph. These signs serve to capture the probabilistic influencesand synergies between variables.A qualitative probabilistic influence between two variables expresses how the values ofone variable influence the probabilities of the values of the other variable. For example, apositive qualitative influence of a variable A on its effect B, denoted S+(A, B), expressesthat observing the value (cid:3) for A makes the value (cid:3) for B more likely, regardless of anyother direct influences on B, that is,Pr(b | a, x) (cid:1) Pr(b | ¯a, x)(8)for any combination of values x for the set π(B) \ {A} of causes of B other than A. A neg-ative qualitative influence, denoted S−(A, B), and a zero qualitative influence, denotedP.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263243Table 2The operators for combining signs⊗ ++−0?+−0?−−+0?00000???0?⊕ ++−0?+?+?−?−−?0+−0??????S0(A, B), are defined analogously, replacing (cid:1) in the above formula by (cid:3) and =, re-spectively. If the influence of A on B is non-monotonic, that is, the sign of the influencedepends upon the values of other causes of B, or unknown, we say that the influence is am-biguous, denoted S?(A, B). With each arc in a qualitative network’s digraph an influenceis associated.The set of influences of a qualitative probabilistic network exhibits various convenientproperties [28]. The property of symmetry guarantees that, if the network includes thequalitative influence S+(A, B), then it also includes S+(B, A). The property of transitiv-ity asserts that the qualitative influences along a path between two variables, specifyingat most one incoming arc for each variable, combine into a single compound influencebetween these variables with the ⊗-operator from Table 2. The property of compositionfurther asserts that multiple qualitative influences between two variables along parallelpaths combine into a compound influence between these variables with the ⊕-operator. Inaddition to influences, a qualitative probabilistic network includes synergies modelling in-teractions between influences. An additive synergy between three variables expresses howthe values of two variables jointly influence the probabilities of the values of the third vari-able. For example, a positive additive synergy of the variables A and B on their commoneffect C, denoted Y +({A, B}, C), expresses that the joint influence of A and B on C isgreater than the sum of their separate influences, regardless of any other influences on C,that is,Pr(c | a, b, x) + Pr(c | ¯a, ¯b, x) (cid:1) Pr(c | a, ¯b, x) + Pr(c | ¯a, b, x)(9)for any combination of values x for the set of causes of C other than A and B. Nega-tive, zero, and ambiguous additive synergy are defined analogously. A qualitative networkspecifies an additive synergy for each pair of causes and their common effect in its digraph.A product synergy between three variables expresses how the value of one variableinfluences the probabilities of the values of another variable in view of an observed valuefor the third variable [17]. For example, a negative product synergy of a variable A on avariable B given the value (cid:3) for their common effect C, denoted X−({A, B}, c), expressesthat, given c, the value (cid:3) for A renders the value (cid:3) for B less likely, that is,Pr(c | a, b, x) · Pr(c | ¯a, ¯b, x) (cid:3) Pr(c | a, ¯b, x) · Pr(c | ¯a, b, x)(10)for any combination of values x for the set of causes of C other than A and B. Positive,zero, and ambiguous product synergy again are defined analogously. For each pair of causesand their common effect, a qualitative probabilistic network specifies two product syner-gies, one for each value of the effect. Upon observation of a specific value for a commoneffect of two causes, the associated product synergy induces an influence between the two244P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263causes; the sign of this influence equals the sign of the synergy. A qualitative influence thatis thus induced by a product synergy is termed an intercausal influence.3. Qualitative analysis of causal independenceEven though the notion of causal independence is described in a qualitative fashion inSection 2.2, the actual interactions obtained are determined by the interaction function fused in defining it. QPNs offer qualitative abstractions of Bayesian networks, and, thus,could serve in principle as tools for describing and analysing qualitative phenomena inBayesian networks. This is exactly what is done in this and subsequent sections. In thissection, we use QPNs to analyse and describe the interactions for various interaction func-tions f . We start by considering qualitative influences among cause and effect variables,which is followed by an analysis of synergies. Throughout the paper it is assumed that thenumber of causes n is greater than or equal to 2.3.1. Qualitative influencesQualitative influences are investigated by considering the sign of the expressionPr(e | C1, . . . , cj , . . . , Cn) − Pr(e | C1, . . . , ¯cj , . . . , Cn)(11)which is denoted by δj (C1, . . . , Cj −1, Cj +1, . . . , Cn). The sign σ of the qualitative influ-ence Sσ (Cj , E) is thus determined by the sign of the latter function.The following result, obtained by using Eq. (3), enables us to investigate qualitativeinfluences in detail:δj (C1, . . . , Cj −1, Cj +1, . . . , Cn)= Pr(e | C1, . . . , cj , . . . , Cn) − Pr(e | C1, . . . , ¯cj , . . . , Cn)= Pr(ij | cj )(cid:1)I1,...,In\IjPr(e | I1, . . . , ij , . . . , In)Pr(Ik | Ck)n(cid:2)k=1k(cid:7)=j+ Pr(¯ıj | cj )(cid:1)I1,...,In\IjPr(e | I1, . . . , ¯ıj , . . . , In)Pr(Ik | Ck)n(cid:2)k=1k(cid:7)=j(cid:1)−I1,...,In\Ij(cid:6)= Pr(ij | cj )Pr(e | I1, . . . , ¯ıj , . . . , In)(cid:1)I1,...,In\Ijde(In\Ij )n(cid:2)k=1k(cid:7)=jn(cid:2)k=1k(cid:7)=jPr(Ik | Ck)(cid:7)Pr(Ik | Ck),where In = I1, . . . , In, andde(In\Ij ) = Pr(e | I1, . . . , ij , . . . , In) − Pr(e | I1, . . . , ¯ıj , . . . , In).(12)P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263245(cid:8)nRecall that it is assumed that Pr(i | ¯c) = 0. The multipliersk=1,k(cid:7)=j Pr(Ik | Ck) are re-sponsible for possible variation among signs of the difference (11) for various values ofC1, . . . , Cj −1, Cj +1, . . . , Cn, as the difference (12) is not influenced by the values of causevariables Ck. As the constituents in the difference (12) represent Boolean functions, thisdifference can be interpreted as a mapping {⊥, (cid:3)} × {⊥, (cid:3)} → {−1, 0, 1}. Hence, fromthe combined effect of the multipliers and the difference it appears that any qualitativeinfluence can be represented using causal independence.Recall that a probability distribution Pr(E | I1, . . . , In) representing a Boolean functioncan be interpreted as a Boolean expression or function. We first consider cases where theBoolean function is symmetric; as the exact and threshold Boolean function are fundamen-tal, we examine these two functions first.Proposition 1. Let B = (G, Pr) be a Bayesian network representing a causal indepen-dence model with interaction function f equal to the exact function ek, then the sign σin Sσ (Cj , E) is equal to ‘?’ for 1 (cid:3) k (cid:3) n − 1, whereas σ = − for k = 0 and σ = + fork = n, n > 0.Proof. Lemma 1 indicates that ek(I1, . . . , ij , . . . , In) ∧ ek(I1, . . . , ¯ıj , . . . , In) is alwaysunsatisfiable. Both expressions are satisfiable for 1 (cid:3) k (cid:3) n − 1, but never both at thesame time according to Lemma (1), and thus it holds that σ = ?. For k = 0, it holdsthat e0(I1, . . . , ij , . . . , In) ≡ ⊥ for any truth value for I1, . . . , Ij −1, Ij +1, In, wherease0(I1, . . . , ¯ıj , . . . , In) is satisfiable. Hence, it holds that σ = −. For k = n, n > 0, it holdsthat en(I1, . . . , ¯ıj , . . . , In) ≡ ⊥, whereas en(I1, . . . , ij , . . . , In) is satisfiable. We concludethat σ = +. (cid:1)For the threshold function, the following result is obtained.Proposition 2. Let B = (G, Pr) be a Bayesian network representing a causal independencemodel with interaction function f equal to the threshold function tk, then the sign σ inSσ (Cj , E) is equal to + for k (cid:1) 1, and σ = 0 for k = 0.Proof. The threshold function can be defined using Eq. (7) by taking c0 = · · · = ck−1 = ⊥and ck = · · · = cn = (cid:3). As a consequence, tk(I1, . . . , ij , . . . , In) and tk(I1, . . . , ¯ıj , . . . , In)can both be satisfied, but it is also possible that tk(I1, . . . , ij , . . . , In) is satisfied becauseek(I1, . . . , ij , . . . , In) is satisfied, which for k (cid:1) 1 implies that tk(I1, . . . , ¯ıj , . . . , In) is notsatisfied. Finally, if tk(I1, . . . , ij , . . . , In) is falsified, so is tk(I1, . . . , ¯ıj , . . . , In). Summaris-ing, for k (cid:1) 1 the qualitative influence σ = +. For k = 0, both t0(I1, . . . , ij , . . . , In) andt0(I1, . . . , ¯ıj , . . . , In) are always true, and hence σ = 0. (cid:1)Next suppose that the interaction function f is decomposable. We start by consider-ing Boolean expressions built up from the commutative, associative Boolean operators, asdiscussed above, which we shall study as special cases of symmetric Boolean functions.Proposition 3. Let B = (G, Pr) be a Bayesian network representing a causal independencemodel with decomposable interaction function f that is defined in terms of the commuta-246P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263Table 3Qualitative influences: commuta-tive, associative operatorsOperator∧∨↔(cid:12)(cid:3)⊥Sign++??00tive, associative binary operators shown in Table 1. Then, the sign σ in Sσ (Cj , E) asindicated in Table 3 holds for any cause variable Cj and given effect variable E.Proof. Let f be the symmetric Boolean function corresponding to Pr(e | I1, . . . , In) indifference equation (12). Then, the following results are obtained, using Eq. (7):• ∧: cn = (cid:3), and ck = ⊥, for each k (cid:7)= n, hence ek(I1, . . . , ij , . . . , In) ∧ ck is only sat-isfiable for k = n, and if the expression is satisfied, it follows from Lemma 1 thaten(I1, . . . , ¯ıj , . . . , In) ≡ ⊥. Thus, it follows that σ = +.• ∨: c0 = ⊥, and ck = (cid:3), for each k > 0, hence, according to Lemma 1 ∃k, k > 0:ek(I1, . . . , ij , . . . , In) ≡ ek−1(I1, . . . , ¯ıj , . . . , In) ≡ (cid:3). However, for k = 1, it holds thate1(I1, . . . , ij , . . . , In) ∧ c1 is satisfiable and e0(I1, . . . , ¬ij , . . . , In) ∧ c0 ≡ ⊥. There-fore, σ = +.• ↔: ck = (cid:3) if n − k is even; otherwise ck = ⊥. We obtain that if for some k, and theappropriate truth values for the variables I1, . . . , In: ek(I1, . . . , ij , . . . , In) ∧ ck ≡ (cid:3)then ek−1(I1, . . . , ij , . . . , In) ∧ ck−1 ≡ ⊥ and vice versa. Hence, σ = ?.• (cid:12): for each k, ck = (cid:3) if odd(k); ck = ⊥ if even(k). For k being odd, ek(I1, . . . , ij , . . . ,In) may be satisfied, but this also holds for ek(I1, . . . , ¯ıj , . . . , In). From Lemma 1 itthen follows that σ = ?.• (cid:3), ⊥: here we have that fn(I1, . . . , In) = (cid:3) or fn(I1, . . . , In) = ⊥ for any Ik, k =1, . . . , n. In both cases: σ = 0. (cid:1)Next, the commutative, non-associative operators are studied. Firstly, consider the NORoperator, and assume it to be right associative. It holds that(I1 ↓ (I2 ↓ (I3 ↓ · · · ↓ (In−1 ↓ In) · · ·)≡ ¬I1 ∧ (I2 ∨ (¬I3 ∧ · · · ∨ (¬In−1 ∧ ¬In) · · ·)if n is even, and(I1 ↓ (I2 ↓ (I3 ↓ · · · ↓ (In−1 ↓ In) · · ·)≡ ¬I1 ∧ (I2 ∨ (¬I3 ∧ · · · ∧ (In−1 ∨ In) · · ·)(13)(14)if n is odd. Clearly, it matters whether a variable Ij takes an odd or even argument position,as this determines whether or not it will be negated. Table 4 gives the signs assuming theoperators to be right associative; one proof is given below.P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263247Table 4Signs of qualitative influences for the commutative, non-associative op-erators; right-associative caseOperatorSign for evenSign for odd↓|Last−−Non-last++Last++Non-last−−Proposition 4. Let B = (G, Pr) be a Bayesian network representing a causal independencemodel with decomposable interaction function f that is equal to the NOR operator ↓. Then,S−(Cj , E) is satisfied for j < n and odd, or j = n and even; S+(Cj , E) holds for j < nand even, or j = n and odd.Proof. Consider the case that the subscript j of Ij is odd, with j < n. Then, Booleanexpressions of the form (¬I1 ∧(I2 ∨(· · ·∨(¬Ij ∧Ij +1) · · ·) have to be considered. Clearly,¬Ij ∧ Ij +1 ≡ ⊥ for any combination of truth values of Ij and Ij +1, with the exception of¯ıj if Ij +1 = (cid:3). Hence, S−(Cj , E) holds.Next, suppose that the subscript j of Ij is even, with j < n. Then, Boolean expressionsof the form (¬I1 ∧ (I2 ∨ (· · · ∧ (Ij ∨ Ij +1) · · ·) need consideration. If Ij +1 = (cid:3), then thevalue of Ij does not matter, and hence de(In\Ij ) = 0. However, if Ij +1 = ⊥, we obtain(Ij ∨ Ij +1) ≡ (cid:3) only for ij . Hence, de(In\Ij ) (cid:1) 0 in that case, i.e., S+(Cj , E) holds.Finally, if j = n then the cases considered above still apply, except that odd and evenneed to be reversed. (cid:1)The left-associative case is simply obtained by determining n − j + 1 for Ij in theleft-associative Boolean expression, and looking up the sign in Table 4, where the resultfor the first argument becomes the result for the last argument. This is a consequence ofcommutativity.For the Boolean operators which are associative but non-commutative, a distinctionmust be made between the situation where the cause variable Cj is in the first or any otherargument position, and in the last or any other argument position. This is illustrated by theproof below for the p1 operator.Proposition 5. Let B = (G, Pr) be a Bayesian network representing a causal independencemodel with decomposable interaction function f that is equal to projection to the firstargument p1. Then, S+(Cj , E) is satisfied for j = 1, otherwise, if j (cid:7)= 1, S0(Cj , E) holds.Proof. Let In and I(cid:6)n be Boolean expressions corresponding to the constituents of dif-ference (12). In general, it holds that I1 p1 In−1 = I1. Now, if j = 1, then In ≡ i1≡ ¯ı1. This implies that S+(Cj , E) holds, as Pr(e | I1, . . . , ij , . . . , In) = 1, andand I(cid:6)nPr(e | I1, . . . , ¯ıj , . . . , In) = 0, which follows from the logical analysis above.= I1, and thus the difference (12) is always equalNext, assume that j (cid:7)= 1, then In = I(cid:6)nto 0, i.e., S0(Cj , E) holds. (cid:1)Finally, the results in Table 6 for the increasing order operator are proven.248P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263Table 5Qualitative influences: non-commutative, associativeoperatorsOperatorp1n1p2n2SignFirst+−Last+−Non-first00Non-last00Proposition 6. Let B = (G, Pr) be a Bayesian network representing a causal independencemodel with decomposable interaction function f that is equal to the logical increasing or-der operator <. Let V (G) contain n (cid:1) 1 interaction variables I1, . . . , In and an effectvariable E. If assuming that the operator < is left associative, it holds that S?(Cj , E) ifj < n, otherwise, if j = n, S+(Cj , E) holds. Assume that the operator < is right associa-tive, then S−(Cj , E) holds for j < n; otherwise, for j = n, S+(Cj , E) is satisfied.Proof. Consider the case where < is assumed to be left associative. Recall the definitionof Ij = (· · · (I1 < I2) < · · ·) < Ij −1) < Ij ). First, we assume j < n. Clearly, if ¯ın holds,then In ≡ ⊥. So, we only consider the case for in. Now, assume that Ij −1 ≡ (cid:3), then forboth ij and ¯ıj it holds that Ij ≡ ⊥. So, there is no difference in the resulting truth valuesfor ij and ¯ıj , and the difference (12) is therefore equal to 0. Next, consider the case thatIj −1 ≡ ⊥. Then, we obtain: (Ij −1 < ij ) ≡ (cid:3) and (Ij −1 < ¯ıj ) ≡ ⊥. For ij +1, this wouldyield Ij +1 ≡ ⊥ and Ij +1 ≡ (cid:3), respectively, inverting the truth values of Ij . The resultingtruth values can also be both ⊥ when taking ¯ij +2. So, this means that the difference canbe 0, −, or +, thus S?(Cj , E) holds for j < n. Now, assume that j = n, then only in cansatisfy the expression. Hence, S+(Cj , E) holds.Next, consider the case that < is right associative. Recall definition (5) of Ij = (Ij <(Ij +1 < (· · · < (In−1 < In) · · ·). Assume that j < n. Now, (ij < Ij +1) ≡ ⊥, whereas (¯ıj <Ij +1) is satisfiable. Hence, S−(Ck, E) holds. Next, consider the case that j = n. Then,only in is able to satisfy I1, i.e., S+(Cj , E) holds. (cid:1)The proofs for the other non-commutative, non-associative operators are similar; theresults are given in Table 6. Tables 3, 5 and 6 clearly indicate that it is possible to modelall possible qualitative influences among causes and effects, even if it is assumed that theinteraction function is decomposable.We return to our example in Fig. 1. It is known that some bacteria may protect a hostagainst infection. Suppose that this holds for bacteria A and B, then each of these wouldmake the development of infection less likely, even though there could be circumstanceswhere these bacteria turn pathogenic. Now, let C be a bacterium with only pathogenicstrains, then the right-associative version of the implication (Table 6) would model thissituation appropriately. For the qualitative influence of penicillin or chlortetracyclin onbacterial growth we obtain an ambiguity due to the exclusive OR. This clearly expressesP.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263249Table 6Qualitative influences: non-commutative, non-associative operators;RA: right associative; LA: left associativeOperator←>→<SignRAFirst++Last++Non-first??Non-last−−LAFirst++Last++Non-first−−Non-last??that the effect of penicillin on bacterial growth as depicted in Fig. 2 is dependent on thepresence or absence of chlortetracyclin and vice versa. For the qualitative influence ofinsulin hypersecretion on hypoglycaemia, as shown in Fig. 3, we obtain a positive signdue to the decreasing order operator, whereas for glucagon hypersecretion we obtain anegative sign. Here we take the left-associative version of the decreasing order operator >,as this is the most specific one, and, therefore, expresses the situation when two variablesare involved. This formal representation is clearly consistent with what has been describedabout the glucose metabolism above.3.2. Analysis of additive synergiesRecall that in the case of causal independence, additive synergies describe how twocauses jointly influence the probability of the effect variable. Using definition (9) of anadditive synergy, and considering interactions between the causes Cj −1 and Cj , we obtain:δj −1,j (C1, . . . , Cj −2, Cj +1, . . . , Cn)= Pr(e | C1, . . . , cj −1, cj , Cj +1, . . . , Cn)+ Pr(e | C1, . . . , ¯cj −1, ¯cj , Cj +1, . . . , Cn)− Pr(e | C1, . . . , cj −1, ¯cj , Cj +1, . . . , Cn)− Pr(e | C1, . . . , ¯cj −1, cj , Cj +1, . . . , Cn)(cid:1)=f (I1,...,In)=ed(Ij −1, Ij )j −2(cid:2)k=1Pr(Ik | Ck)n(cid:2)k=j +1Pr(Ik | Ck),whered(Ij −1, Ij ) = Pr(Ij −1 | cj −1) Pr(Ij | cj ) + Pr(Ij −1 | ¯cj −1) Pr(Ij | ¯cj )− Pr(Ij −1 | cj −1) Pr(Ij | ¯cj ) − Pr(Ij −1 | ¯cj −1) Pr(Ij | cj ).As the function f renders the variables I1, . . . , In\Ij −1, Ij dependent of the variables Ij −1and Ij it is not possible to distribute summation over the expression.Let Pr(ij −1 | cj −1) = p and Pr(ij | cj ) = q, then the difference d(Ij −1, Ij ) correspondsfor different values of Ij −1 and Ij , using the assumptions introduced in Section 2.2, to250P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263Table 7Difference d(Ij −1, Ij ) for various values ofthe variables Ij −1 and IjIj −1ij −1¯ıj −1ij −1¯ıj −1Ijijij¯ıj¯ıjd(Ij −1, Ij )pq−pq−pqpqthe results given in Table 7. As a consequence δj −1,j (C1, . . . , Cj −2, Cj +1, . . . , Cn) can besimplified to obtain the following result:δj −1,j (C1, . . . , Cj −2, Cj +1, . . . , Cn)(cid:1)(cid:1)=σ (Ij −1 (cid:12) Ij )pqI1,...,In\Ij −1,IjIj −1,Ijf (I1,...,In)=en(cid:2)×k=j +1Pr(Ik | Ck),j −2(cid:2)k=1Pr(Ik | Ck)(15)where (cid:12) represents the exclusive or, and(cid:3)−1 if Q ≡ (cid:3),otherwise.1σ (Q) =The multipliersj −2(cid:2)Pr(Ik | Ck)n(cid:2)Pr(Ik | Ck),k=1(cid:8)k=j +1(cid:8)j −2k=1 Pr(Ik | Ck)nk=j +1 Pr(Ik | Ck) (cid:1) 0, will generally differ for various δj −1,j (C1,with. . . , Cj −2, Cj +1, . . . , Cn). The sum of terms σ (Ij −1 (cid:12)Ij )pq will not; which of those termswill actually be included in the final sum is determined by the function f .As before, a distinction has to be made between operators that are associative andcommutative, those that are non-commutative but associative, and those that are neithercommutative nor associative. The results for the associative and commutative operatorsare given in Table 8. In this case, we can simply assume that j = 2 without loss of gener-ality, which simplifies the proofs. Again, the proof for only some of the Boolean operatorsis given here.Proposition 7. Let B = (G, Pr) be a Bayesian network representing a causal independencemodel with decomposable interaction function f that is equal to the logical OR. Then, itholds that Y −({Cj −1, Cj }, E) for any two cause variables Cj −1, Cj and the given effectvariable E.P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263251Table 8Signs of additive synergies for thecommutative, associative operatorsOperator∧∨↔(cid:12)(cid:3)⊥Sign+−??00Proof. If the interaction function is represented by the Boolean expression I1 ∨ I2 ∨ · · · ∨In, then it is easily verified that(cid:1)σ (I1 (cid:12) I2)pqI1,I2I1∨···∨Infor given values of I3, . . . , In is either equal to −pq (= pq − 2pq) or to 0 (= 2pq − 2pq).(cid:8)nk=3 Pr(Ik | Ck) (cid:1) 0, a logical OR interaction function clearly results in a negativeSince,additive synergy. (cid:1)We next present the proof for the case that the Boolean operator is equal to the bi-implication.Proposition 8. Let B = (G, Pr) be a Bayesian network representing a causal independencemodel with decomposable interaction function f that is equal to the logical bi-implication.Then, it holds that Y ?({Cj −1, Cj }, E) for any two cause variables Cj −1, Cj and the giveneffect variable E.Proof. Let the interaction function be represented by the Boolean expression I1 ↔ I2 ↔· · · ↔ In ≡ I1 ↔ I2 ↔ In−2. Two general cases for which the Boolean expression is trueare distinguished. Firstly, assume that (I1 ↔ I2) ≡ (cid:3) and In−2 ≡ (cid:3). It is easily verifiedthat then the inner sum of Eq. (15) is equal to 2pq. Secondly, assume that (I1 ↔ I2) ≡ ⊥,and In−2 ≡ ⊥ as well. Then, the inner sum is equal to −2pq. As the used multipliers willbe different, the result is ambiguous. (cid:1)Next, the two commutative, non-associative operators are considered. Here, we onlysupply a proof for the NAND | operator; the results are summarised in Table 9. Note thatit is now no longer permitted to only look at the variables I1 and I2.Proposition 9. Let B = (G, Pr) be a Bayesian network representing a causal indepen-dence model with decomposable interaction function f that is equal to the logical NAND|.Then, it holds that Y +({Cj −1, Cj }, E) for j < n and even or j = n and odd, andY −({Cj −1, Cj }, E) holds for j < n and odd or j = n and even.252P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263Table 9Signs of additive synergies for the commutative, non-associative opera-tors; right-associative caseOperatorSign for evenSign for odd↓|Last+−Non-last−+Last−+Non-last+−Proof. Note that we have that:(I1 | (I2 | (I3 | · · · | (In−1 | In) · · ·) ≡ ¬I1 ∨ (I2 ∧ (¬I3 ∨ · · · ∧ (¬In−1 ∨ ¬In) · · ·)if n is even, and(I1 | (I2 | (I3 | · · · | (In−1 | In) · · ·) ≡ ¬I1 ∨ (I2 ∧ (¬I3 ∨ · · · ∨ (In−1 ∧ In) · · ·)if n is odd. First, we consider j < n and even. Then, ¬Ij −1 ∨ (Ij ∧ Ij +1) ≡ ⊥ for thecombination ij −1 and ¯ıj ; for the other combinations of truth values this expression is satis-fiable. If j = n and odd, the Boolean expression In−1 ∧ In needs to be considered, and thisis only true for the combination in−1 and in. In both cases, the result is Y +({Cj −1, Cj }, E).Secondly, consider the case that j < n and odd. Then, the Boolean expression Ij −1 ∧(¬Ij ∨ Ij +1) must be considered. This is true for ij −1 and ij , satisfiable for ij −1 and¯ıj , and otherwise false. If j = n and even, we need to consider ¬In−1 ∨ ¬In. It holdsthat ¬in−1 ∨ ¬in ≡ ⊥; otherwise, the Boolean expression is true. It is concluded thatY −({Cj −1, Cj }, E) holds in both cases. (cid:1)We next move on to consider the non-commutative, associative operators.Proposition 10. Let B = (G, Pr) be a Bayesian network representing a causal indepen-dence model with decomposable interaction function f that is equal to projection to thefirst argument. Then, it holds that Y 0({Cj −1, Cj }, E) for any two cause variables Cj −1, Cjand the given effect variable E.Proof. It holds that the Boolean expression representation of the interaction function f isequal to (I1 p1 I2 p1 · · · p1 In) ≡ I1. Now, if j > 2, then d(Ij −1, Ij ) will be computedfor every value of Ij −1 and Ij , and hence, the result of summing these results will be 0.If j = 2, then Ij −1 should always be equal to ij −1, and hence the sum is only taken overd(i1, i2) and d(i1, ¯ı2), which, however, also yields 0. (cid:1)Similar results are obtained for the other non-commutative, associative operators, andtheir proofs are similar. Note that, in contract to the results for the qualitative S relation,there are no differences in results when considering either the first, last of any other pairof causes. The reason for this is that the operators select at most one argument, and hence,either all 4 possible Boolean combinations of Boolean values of the two interaction vari-ables if the selected variable is not among them, or two combinations of Boolean values,with one of them fixed, need to be considered. In both cases, there are an equal number ofP.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263253Table 10Signs of additive synergies for thenon-commutative, associative oper-atorsOperatorp1p2n1n2Sign0000products pq and −pq, which cancel out each other, resulting in a total of 0. The resultsare summarised in Table 10.Finally, the non-commutative and non-associative operators have to be considered. Thisanalysis is more difficult, as a distinction must be made between assuming the operatorsto be right associative or left associative. We present the proof for the increasing orderoperator <. The other proofs are similar.Proposition 11. Let B = (G, Pr) be a Bayesian network representing a causal indepen-dence model with decomposable interaction function f that is equal to the increasing orderoperator <. Let V (G) contain n (cid:1) 2 interaction variables I1, . . . , In and an effect vari-able E. Assume that the operator is left associative, then it holds that Y +({Cj −1, Cj }, E)for j < n; for j = n it holds that Y −({Cj −1, Cj }, E). Next, assume that the operator isright associative, then it holds that Y +({Cj −1, Cj }, E) for j < n; for j = n it holds thatY −({Cj −1, Cj }, E).Proof. Firstly, consider the case that the operator < is assumed to be left associative. Re-call definition (4) of Ij . We take Ij −1 and Ij as the interaction variables, with j < n.Now if ¯ıj holds, then Ij ≡ ⊥, and hence In is satisfiable. We conclude that we needto take into account d(ij −1, ¯ıj ) + d(¯ıj −1, ¯ıj ) = 0. Next, consider the case that ij holds.Then if ¯ıj −1 holds, we have that Ij ≡ (cid:3). As In−1 must be false in order to makeIn ≡ (cid:3), there must be at least one variable Ik, k > j , which falsifies In−1. Finally,for ij −1, the expression Ij is again satisfiable. Now, as d(ij −1, ij ) = pq, we know thatδj −1,j (C1, . . . , Cj −2, Cj +1, . . . , Cn) (cid:1) 0. Next, consider the case that j = n. Then In mustalways be true in order In to be true. Now, if ¯ın−1 holds, we know that In ≡ (cid:3), whereas ifin−1 holds, then In is only satisfiable. Hence, it was shown that δn−1,n(C1, . . . , Cn−2) (cid:3) 0.Secondly, consider the case that the operator < is right associative. Recall defini-tion (5) of Ij . We first consider the case that j < n. Clearly, in order Ij to be satis-fiable, in must holds. However, Ij ≡ ⊥ if it holds that there exists an Ik that is truefor j (cid:3) k < n. Hence, only d(¯ıj −1, ¯ıj ) = pq needs to be taken into account, resultingin δj −1,j (C1, . . . , Cj −2, Cj +1, . . . , Cn) (cid:1) 0. Finally, consider j = n. Here, we simplyhave that only d(¯ın−1, in) = −pq needs to be taken into account, so we conclude thatδn−1,n(C1, . . . , Cn−2) (cid:3) 0. (cid:1)The proofs for the other operators that are non-commutative and non-associative arealong similar lines. The results are summarised in Table 11.254P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263Table 11Signs of additive synergies for the non-commutative, non-associativeoperators; RA: right-associative; LA: left-associativeOperator←>→<SignRAFirst+−Last+−Non-first−+Non-last−+LAFirst+−Last+−Non-first−+Non-last−+We return to our example in Fig. 1. In the previous section, the individual effects, butnot the synergies, of the colonisation by bacteria A, B and C on the patient’s body responsewere modelled. It appears that the right-associative version of implication also rightly ex-presses that colonisation by both bacterium A and B makes development of an infectionless likely, whereas bacterium C is so pathogenic that it overrides the preventive effectsof bacteria A and B. For the interaction between penicillin and chlortetracyclin, modelledas an exclusive OR and shown in Fig. 2, we have an ambiguous additive synergy. Thisis as might be expected, as when these potential causes of decreased bacterial growth areboth present or absent, there will be no antimicrobial effect, in contrast to when only oneof these is present. For the interaction between insulin and glucagon hypersecretion, mod-elled as the decreasing order operator > and shown in Fig. 3, we have a negative additivesynergy, as the two hormones have opposite effects on the glucose level of blood.3.3. Analysis of product synergiesWe basically use the same approach as employed in the previous section on additivesynergies for product synergies in this section. For the analysis of product synergies, theequation of interest is:δEj −1,j (C1, . . . , Cj −2, Cj +1, . . . , Cn)= Pr(E | C1, . . . , cj −1, cj , Cj +1, . . . , Cn)× Pr(E | C1, . . . , ¯cj −1, ¯cj , Cj +1, . . . , Cn)− Pr(E | C1, . . . , cj −1, ¯cj , Cj +1, . . . , Cn)× Pr(E | C1, . . . , ¯cj −1, cj , Cj +1, . . . , Cn)(cid:1)(cid:9)τ (cj −1, cj ; In\Ij −1, Ij ) · τ ( ¯cj −1, ¯cj ; In\Ij −1, Ij )=I1,...,In\Ij −1,Ij(cid:10)− τ (cj −1, ¯cj ; In\Ij −1, Ij ) · τ ( ¯cj −1, cj ; In\Ij −1, Ij )×j −2(cid:2)k=1Pr(Ik | Ck)n(cid:2)k=j +1Pr(Ik | Ck),(16)P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263255Table 12Signs of product synergies for the commutative andassociative operatorsOperator∧∨↔(cid:12)(cid:3)⊥whereτ (Cj −1, Cj ; In\Ij −1, Ij ) =Sign for e0−??00(cid:1)Ij −1,Ijf (I1,...,In)=ESign for ¯e−0??00Pr(Ij −1 | Cj −1) Pr(Ij | Cj ).(17)The arithmetic expression between the braces in (16), consisting of additions and subtrac-tions of products of instances of τ (Cj −1, Cj ; In\Ij −1, Ij ), is the essential element in theanalysis below; it will be denoted by β. Furthermore, we will once more use the abbrevia-tions p = Pr(ij −1 | cj −1) and q = Pr(ij | cj ). In the following, the equation above will bestudied for all possible Boolean operator definitions of the interaction function f , whichis again assumed to be decomposable. As before for the operators which are commutativeand associative, instead of focusing the analysis on two arbitrary cause variables Cj −1 andCj , for simplicity’s sake, the interaction of the two equally arbitrary variables C1 and C2is examined, i.e., we take j = 2. Again, for only some of the Boolean operators a proof isprovided.Proposition 12. Let B = (G, Pr) be a Bayesian network representing a causal indepen-dence model with decomposable interaction function f that is equal to the logical OR.Then, it holds that X−({Cj −1, Cj }, e) for any two cause variables Cj −1, Cj given that theeffect is true; and X0({Cj −1, Cj }, ¯e) when the effect is assumed to be false.Proof. Let the interaction function be represented by the Boolean expression In = I1 ∨I2 ∨ In−2. First, we consider the situation where E is true. There are two cases to consider.Let In−2 ≡ ⊥, then β = 0−pq = −pq. For In−2 ≡ (cid:3), we get β = 1−1 = 0. So, summingk=3 Pr(Ik | Ck) (cid:3) 0. We conclude that X−({C1, C2}, e)over I3, . . . , In yieldsholds.−pq ·(cid:8)n(cid:4)Let us now consider the case that E is false. This implies that both I1 and I2 must be1,2(C3, . . . , Cn) =false. We get β = (1 − p)(1 − q) − (1 − p)(1 − q) = 0; this means that δ ¯e0, and thus X0({C1, C2}, ¯e) holds. (cid:1)For the bi-implication, the following result is obtained.Proposition 13. Let B = (G, Pr) be a Bayesian network representing a causal indepen-dence model with decomposable interaction function f that is equal to the logical bi-256P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263Table 13Signs of product synergies for the commutative, non-associative operators assuming thatE = (cid:3); right-associative caseOperatorSign for e↓|EvenLast0−Non-last0+OddLast−0Non-last+0Table 14Signs of product synergies for the commutative, non-associative operators assuming thatE = ⊥; right-associative caseOperatorSign for e↓|EvenLast−0Non-last+0OddLast0−Non-last0+implication. Then, it holds that X?({Cj −1, Cj }, e) for any two cause variables Cj −1, Cjgiven that the effect is true; and X?({Cj −1, Cj }, ¯e) when the effect is assumed to be false.Proof. Let the interaction function be represented by the Boolean expression In = I1 ↔I2 ↔ In−2. Firstly, take E to be true. Let us assume that In−2 ≡ (cid:3), then I1 and I2 mustbe both true or false. The result is then β = pq. Next, we assume that In−2 ≡ ⊥; then,I1 must be true and I2 must be false, or I1 must be false and I2 must be true. We get:β = [p(1 − q) + (1 − p)q] · 0 − pq = −pq. It is concluded that X?({C1, C2}, e) holds.Assume now that E is false. For In−2 ≡ (cid:3) we get that either I1 or I2 is true, but not both.From this, as above we conclude that β = −pq. Subsequently assuming that In−2 ≡ ⊥yields the same as above for e and In−2 ≡ (cid:3); hence, β = pq. Again, X?({C1, C2}, e) issatisfied. (cid:1)The results for the commutative, non-associative operators ↓ and | are shown in Ta-bles 13 and 14. Again only the right-associative case is covered in the tables, but usingcommutativity, it is easy to obtain the signs for the left-associative case. Below, the prooffor the NOR operator ↓ is given.Proposition 14. Let B = (G, Pr) be a Bayesian network representing a causal indepen-dence model with decomposable interaction function f that is equal to the NOR operator↓. Then, it holds that X0({Cj −1, Cj }, e), resp. X0({Cj −1, Cj }, ¯e), for any two cause vari-ables Cj −1, Cj given that the effect is true, resp. false, with j even, resp. odd. Furthermore,X+({Cj −1, Cj }, e), resp. X+({Cj −1, Cj }, ¯e), holds for j < n and odd, resp. even, whereasX−({Cj −1, Cj }, e), resp. X−({Cj −1, Cj }, ¯e), holds for j = n and odd, resp. even.Proof. Equivalences (13) and (14) are again used as a basis for the proof.P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263257Case I: e (E is true) holds. Let j < n and odd, then Ij −1 ∨ (¬Ij ∧ Ij +1) needs tobe considered. Firstly, suppose that Ij +1 = (cid:3), then, ¯ıj −1, ij are not taken into account,yielding β = [1 − (1 − p)q] · 1 − 1 · [1 − q] = pq (cid:1) 0. Secondly, suppose that Ij +1 = ⊥,then only ij −1, ij and ij −1, ¯ıj must be considered, yielding: β = [pq + p(1 − q)] · 0 − 0 =0. Hence, it can be concluded that X+({Cj −1, Cj }, e) holds.Next, let j = n and odd, then In−1 ∨ In needs to be considered. Here, only ¯ıj −1, ¯ıjis discarded, yielding β = [1 − (1 − p)(1 − q)] · 0 − [p][q] = −pq (cid:3) 0. Hence,X−({Cn−1, Cn}, e) holds.For j < n and j even, the Boolean expression ¬Ij −1 ∧ (Ij ∨ Ij +1) needs to be analysed.Firstly, suppose that Ij +1 = (cid:3), then only the combinations ¯ıj −1, ij and ¯ıj −1, ¯ıj are able tosatisfy this expression. As result, we have that β = [(1 − p)q + (1 − p)(1 − q)] · 1 − [(1 −p)] · 1 = 0. Secondly, suppose that Ij +1 = ⊥, then only ¯ıj −1, i)j needs to be taken intoaccount. This implies that β = 0 − 0 = 0 holds. Summarised, X0({Cj −1, Cj }, e) holds.Next, let j = n and even, then the Boolean expression that needs to be considered is¬In−1 ∧ ¬In. Hence, only the pair ¯ın−1, ¯ın is able to satisfy this expression. It thereforeholds that β = [(1 − p)(1 − q)] · 1 − [1 − p][1 − q] = 0, i.e., X0({Cn−1, Cn}, e) holds.Case II: ¯e (E is false) holds. The proofs are very similar to the ones given above, as thesame cases have to be considered, which we shall not fully repeat. If j < n is odd, then forIj +1 = (cid:3), only ¯ıj −1, ¯ıj needs to be taken into account. This yields β = [(1 − p)(1 − q)] ·1 − [1 − p][1 − q] = 0. Secondly, suppose that Ij +1 = ⊥, then ¯ıj −1, ij and ¯ıj −1, ¯ıj mustbe considered, yielding: β = [(1 − p)q + (1 − p)(1 − q)] · 1 − [1 − p] · 1 = 0. Hence, itcan be concluded that X0({Cj −1, Cj }, e) holds.Next, let j = n and odd, then only ¯ıj −1, ¯ıj are considered, yielding β = [(1 − p)(1 −q)] · 1 − [1 − p][1 − q] = 0. Hence, X0({Cn−1, Cn}, e) holds.For j < n and j even, let Ij +1 = (cid:3), then only the combinations ij −1, ij and ij −1, ¯ıjfalsify the Boolean expression above. As result, we have that β = 0. Secondly, suppose thatIj +1 = ⊥, then all combinations of values for Ij −1 and Ij , with the exception of ¯ıj −1, ij ,need to be taken into account. This implies that β = [1 − (1 − p)q] · 1 − 1 · [1 − q] = pq (cid:1) 0holds. Summarised, X+({Cj −1, Cj }, e) holds.Finally, let j = n and even, then all combinations of values for Ij −1 and Ij with theexception of ¯ın−1, ¯ın need to be taken into account. It therefore holds that β = [1 − (1 −p)(1 − q)] · 0 − [p][q] = −pq (cid:3) 0, i.e., X−({Cn−1, Cn}, e) holds. (cid:1)Next, we consider the operators which are non-commutative, but associative. The resultsare summarised in Table 15, and correspond to those for the additive synergies, discussedTable 15Signs of product synergies for the non-commutative,associative operatorsOperatorSign for both e and ¯ep1p2n1n20000258P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263in the previous section. The explanation why only zero product synergies are obtained isanalogous to that for the additive synergies as well.Finally, the operators which are neither commutative nor associative are considered.The proof for the implication is given below. Again we make a distinction between a right-associative and a left-associative reading of Boolean expressions.Proposition 15. Let B = (G, Pr) be a Bayesian network representing a causal indepen-dence model with decomposable interaction function f that is equal to the logical impli-cation →. Let V (G) contain n (cid:1) 2 interaction variables I1, . . . , In and an effect variableE. For any two cause variables Cj −1, Cj given that the effect is true and assuming →to be left associative, it holds that X+({Cj −1, Cj }, e) for j < 1 (cid:3) n. Assuming that → isright associative, it holds that X−({Cj −1, Cj }, e) for j < n, and X+({Cj −1, Cj }, e) forj = n. If the effect is taken to be false, the following holds. Assuming that → is left asso-ciative, we obtain X+({Cj −1, Cj }, ¯e) for j < n, and X0({Cj −1, Cj }, ¯e) for j = n. Finally,assuming that → is right associative, the product synergy is equal to X0({Cj −1, Cj }, ¯e)for 1 < j (cid:3) n.Proof. Case I: e (E is true) holds. Assume that the operator → is left associative. Recalldefinition (4) of Ij and take j < n. If in is assumed to hold, then we have to take intoaccount all four Boolean combinations of Ij −1 and Ij , resulting in β = 0. Now, assumethat ¯ın holds. Suppose that Ij −2 ≡ (cid:3), then we obtain Ij ≡ ⊥ for ij −1, ¯ıj , and Ij ≡ (cid:3)for the other three combinations. For Ij −2 ≡ ⊥, it holds that Ij ≡ ⊥ for ij −1, ¯ıj and¯ıj −1, ¯ıj . As the truth value of Ik may change from (cid:3) to ⊥ for k > j , we need to takeinto account ij −1, ¯ıj with β = 0, or ij −1, ¯ıj and ¯ıj −1, ¯ıj with β = 0, or ij −1, ij and¯ıj −1, ij with β = 0, or all four combinations with the exception of ij −1, ¯ıj yielding β =[1 − p(1 − q)] · 1 − (1 − p) · 1 = pq (cid:1) 0. Hence, X+({Cj −1, Cj }, e) is satisfied. Finally,assume that j = n. For in it holds that In ≡ (cid:3), whereas for ¯ın, we obtain In ≡ ⊥ forin−1, whereas for ¯ın−1 the truth value depends on the truth value of In−2. The result isequal to β = [pq + (1 − p)q + (1 − p)(1 − q)] · q − (1 − p) · 1 = pq (cid:1) 0; hence againX+({Cj −1, Cj }, e) is shown to hold.Next, the operator → is assumed to be right associative. Recall definition (5) of Ij .Assume that j < n − 1. Again, there are two cases to consider: Ij +1 ≡ (cid:3) and Ij +1 ≡ ⊥.For Ij +1 ≡ (cid:3), we have that β = 0, as we sum over all possible values of both Ij −1 and Ij ,yielding β = 0. For Ij +1 ≡ ⊥, we sum over all values of Ij −1 and Ij , with the exceptionof the combination ij −1 and ij (as ij −1 → (ij → ⊥) ≡ ⊥). We obtain β = (1 − pq) ·1 − 1 · 1 = −pq (cid:3) 0, and thus it can be concluded that X−({Cj −1, Cj }, e) holds. Now,assume that j = n. If in is assumed to hold, then I1 ≡ (cid:3). Similarly, it holds that I1 ≡ (cid:3) ifIn−1 ≡ In ≡ (cid:3). The expression I1 can only be false for the combination in−1 and ¯ın. It isconcluded that β = [1 − p(1 − q)] · 1 − (1 − p) · 1 (cid:1) 0, i.e., X+({Cj −1, Cj }, e) was shownto hold.Case II: ¯e (E is false) holds. The notational conventions as introduced above will againbe adopted. It is clearly possible to reuse most of the results obtained for case I above. First,assume that → is left associative. Take j < n. For in, the summations over Ij −1 and Ij areempty. Now, assume that ¯ın holds. Then we sum over the same values of the interactionvariables as for E = (cid:3). So, the results are exactly the same. Now take j = n; only ¯ın is ofP.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263259Table 16Signs of product synergies for the non-commutative, non-associative operators assumingthat E = (cid:3); RA: right-associative; LA: left-associativeOperatorSign for eRAFirst+0Last+0Non-first++Non-last−0LAFirst+0Last+0Non-first−0Non-last++←>→<Table 17Signs of product synergies for the non-commutative, non-associative operators assumingthat E = ⊥; RA: right-associative; LA: left-associativeSign for ¯eOperatorRAFirst00Last0+Non-first+0Non-last0−LAFirst0+Last00Non-first0−Non-last+0←>→<interest then. For in−1, it holds that In ≡ ⊥, whereas for ¯ın−1 it holds that In is satisfiable.Hence, we obtain β = 0. Next, assume that → is right associative. Take j < n. Then, ifIj +1 ≡ (cid:3), it holds that I1 ≡ (cid:3), and the sums over Ij −1 and Ij are all empty. Assume nowthat Ij +1 ≡ ⊥, then only the combination ¯ıj −1 and ¯ıj yields Ij −1 ≡ ⊥. The correspondingβ is equal to β = 0. Finally, assume that j = n. The only possibility of falsifying I1 is bythe combination ij −1 and ¯ıj . This yields β = 0. (cid:1)The results of the analysis are given in Table 16 assuming that the effect is positive andin Table 17 assuming a negative effect.We return to the example in Fig. 1, where, as in the previous section, we assume thatlogical implication provides a suitable formalisation of the interactions between the bacte-ria involved in infection. Now, assume that there is a patient in hospital having an infectiousdisease. Recall that bacteria A and B are known to be not particularly pathogenic, whereasbacterium C is. Assuming that the patient is colonised with bacterium C makes it morelikely that the patient is colonised with A or B, as if A or B are present, then C is alsopresent. On the other hand, when we assume that the patient is being colonised by bac-terium A (or B), and we use these to explain the infection in the patient, it is less likelythat the patient is colonised by the other bacteria. This is because we are dealing here witha pathogenic strain of bacterium A (or B) causing the infection, as otherwise A or B wouldnot have caused the infection. This probabilistic behaviour is appropriately modelled by the260P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263right-associative version of implication. The causal mechanisms involved in the interactionbetween bacterial growth and antimicrobial agents, as shown in Fig. 2, was modelled bymeans of the exclusive OR. According to Table 12 the intercausal influences modelled bythe exclusive OR with an arbitrary number of causes are ambiguous. However, if we as-sume that there are no other factors involved but these two antimicrobial agents, it can beshown, along the lines of Proposition 13, that assuming that there is decreased bacterialgrowth due to penicillin (chlortetracyclin) it holds that use of chlortetracyclin (penicillin)becomes less likely (negative product synergy); assuming that there is no decrease in bac-terial growth while using penicillin (chlortetracyclin) it holds that use of chlortetracyclin(penicillin) becomes more likely (positive product synergy). This is consistent with ourknowledge of the underlying mechanisms as described in Section 2.1. Recall that the mod-elling of the causal mechanisms involved in hypoglycaemia, as shown in Fig. 3, was doneby means of the decreasing order operator. Assuming that hypoglycaemia is present ina patient forces activation of the insulin hypersecretion mechanism and inactivation ofglucagon hypersecretion mechanism. As a consequence, assumptions about insulin andglucagon hypersecretion can no longer influence each other (zero product synergy). On theother hand, assuming absence of hypoglycaemia in a patient with insulin hypersecretion(glucagon hypersecretion) renders glucagon hypersecretion (insulin hypersecretion) morelikely, as in that case the two complementary mechanisms have to compensate for eachother (positive product synergy). Again we use the most specific result, which in this caseconcerns the first argument for the left-associative version of the interaction as describedin Table 17. This description is again consistent with our knowledge about the underlyingphysiological mechanisms as described in Section 2.1.4. The qualitative patternsFrom the results obtained in the previous section, it follows that it is possible to ex-ploit the semantics of causal independence models using Boolean operators in developinga Bayesian network fulfilling particular qualitative requirements. In this paper, we haveconsidered 26 of the most common ones. Three different qualitative relationships werestudied, with one of them, the product synergy, consisting of two relationships: one for apositive effect e and one for a negative effect ¯e. For each qualitative relationship there are 4different possible signs. As a consequence, the maximum number of different possible in-teraction models, which we have called QC patterns, is 44 = 256. The number of patternsthat can be realised is determined by relationships between the relations S, Y and X.Note that Sσ (Cj , E), with σ ∈ {−, +, ?}, leaves Y σ (cid:6)({Ci, Cj }, E), i (cid:7)= j , undetermined.For example, assume that σ = + it holds thatPr(e | C1, . . . , cj , . . . , Cn) − Pr(e | C1, . . . , ¯cj , . . . , Cn) (cid:1) 0.(18)This implied that σ (cid:6) can still be anything, as (18) simply says that in the expressionPr(e | c1, c2, C3, . . . , Cn) + Pr(e | ¯c1, ¯c2, C3, . . . , Cn)− Pr(e | c1, ¯c2, C3, . . . , Cn) − Pr(e | ¯c1, c2, C3, . . . , Cn)P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263261we have thatPr(e | c1, c2, C3, . . . , Cn) (cid:1) Pr(e | ¯c1, c2, C3, . . . , Cn)andPr(e | ¯c1, ¯c2, C3, . . . , Cn) (cid:3) Pr(e | c1, ¯c2, C3, . . . , Cn)from which it is impossible to determine the sign σ (cid:6). A similar property holds for theproduct synergy. Hence, additive and product synergies do indeed offer something extra.There is, however, one exception to this, as is shown in the following proposition.Proposition 16. Let S0(Cj , E) be the qualitative influence that is satisfied for cause vari-able Cj and effect variable E in the Bayesian network B = (G, Pr), then it holds thatY 0({Ci, Cj }, E), X0({Ci, Cj }, e) and X0({Ci, Cj }, ¯e), with i (cid:7)= j .Proof. From S0(Ci, E) it follows that Pr(e | C1, . . . , cj , . . . , Cn) = Pr(e | C1, . . . , ¯cj , . . . ,Cn). Substituting this in the definitions of the additive and product synergy yields the re-quested result. (cid:1)There are also some other QC patterns which are identical to each other for the 26Boolean functions considered; in summary the Tables 3–6 and 8–17 yield 18 different pat-terns. These are the patterns that can be used in selecting an appropriate Boolean functionin Bayesian-network design.5. Conclusions and further researchThe qualitative characteristics of interactions in Bayesian-network probability tableshave been analysed and described in this paper, taking causal independence and QPNs asa foundation. This paper builds upon results regarding causal independence obtained pre-viously by other researchers. Heckerman et al. [13–15] have previously studied causalindependence assuming that the chosen interaction functions are well understood, andthat their expected probabilistic behaviour matches the intuition underlying this choice.This may no longer be the case if the interactions to be modelled become more complex.Zhang and Poole have previously proposed to use algebraic methods to formalise causalindependence [30]. However, the subject of Zhang and Poole’s work is the algorithmiccomplexity of probabilistic inference—which is why they restrict to commutative and as-sociative operators—not trying to understand the qualitative nature of causal independence.New in the present paper is, therefore, the utilisation of QPNs in a systematic analysis ofprobabilistic interactions in causal independence models, and this is seen as its main sci-entific contribution. By determining the signs of the relations S, Y and X for a specificinteraction function f , we obtain the qualitative, causal pattern or QC pattern for the func-tion. The theory can thus be used in the process of designing a Bayesian network, where,dependent on the problem at hand, a particular QC pattern can be selected, and be used inthe design process and in acquiring the necessary probabilistic information.262P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263Not all QC patterns realisable by Boolean functions may have been identified. As dif-ferent Boolean functions may yield identical QC patterns, it is as yet unknown whetherall possible QC patterns can be realised. This is something that requires further research.Another important topic of future research is to find more examples from reality matchingthe various QC patterns, such that the use of QC patterns can be more easily understoodand used by Bayesian-network researchers interested in developing applications.References[1] S. Andreassen, M. Woldbye, B. Falck, S.K. Andersen, MUNIN—a causal probabilistic network for interpre-tation of electromyographic findings, in: Proceedings of the 10th International Joint Conference on ArtificialIntelligence, Milan, Italy, Morgan Kaufmann, San Mateo, CA, 1987, pp. 366–372.[2] B.G. Buchanan, E.H. Shortliffe (Eds.), Rule-Based Expert Systems: The MYCIN Experiments of the Stan-ford Heuristic Programming Project, Addison-Wesley, Reading, MA, 1984.[3] R.G. Cowell, A.P. Dawid, S.L. Lauritzen, D.J. Spiegelhalter, Probabilistic Networks and Expert Systems,Springer, New York, 1999.[4] F.J. Díez, Parameter adjustement in Bayes networks: the generalized noisy OR-gate, in: Proceedings of the9th Conference on Uncertainty in Artificial Intelligence, 1993, pp. 99–105.[5] F.J. Díez, J. Mira, E. Iturralde, S. Zubillage, DIAVAL: a Bayesian expert system for echocardiography,Artificial Intelligence in Medicine 10 (1997) 59–73.[6] M.J. Druzdzel, L.C. van der Gaag, Elicitation of probabilities for belief networks: combining qualitativeand quantitative information, in: Proceedings of the Eleventh Conference on Uncertainty in Artificial Intel-ligence, 1995, pp. 141–148.[7] M.J. Druzdzel, M. Henrion, Efficient reasoning in qualitative probabilistic networks, in: Proceedings of theEleventh National Conference on Artificial Intelligence, Washington, DC, 1993, pp. 548–553.[8] D. Dubois, J. Lang, H. Prade, Possibilistic logic, in: D. Gabbay, C.J. Hogger, J.A. Robinson (Eds.), Hand-book of Logic in Artificial Intelligence and Logic Programming, vol. 3, Oxford University Press, Oxford,1994, pp. 439–513.[9] H.B. Enderton, A Mathematical Introduction to Logic, Academic Press, San Diego, CA, 1972.[10] I.J. Good, Good Thinking: the Foundations of Probability and its Applications, University of MinnesotaPress, Minneapolis, MN, 1983.[11] D.E. Heckerman, E.J. Horvitz, B.N. Nathwani, Towards normative expert systems: part I—the Pathfinderproject, Methods of Information in Medicine 31 (1992) 90–105.[12] D.E. Heckerman, B.N. Nathwani, Towards normative expert systems: part II—probability-based represen-tations for efficient knowledge acquisition and inference, Methods of Information in Medicine 31 (1992)106–116.[13] D. Heckerman, Causal independence for knowledge acquisition and inference, in: Proceedings of the 9thConference on Uncertainty in Artificial Intelligence, 1993, pp. 122–127.[14] D. Heckerman, J.S. Breese, A new look at causal independence, in: Proceedings of the 10th Conference onUncertainty in Artificial Intelligence, 1994, pp. 286–292.[15] D. Heckerman, J.S. Breese, Causal independence for probabilistic assessment and inference using Bayesiannetworks, IEEE Trans. Systems Man Cybernet. 26 (6) (1996) 826–831.[16] M. Henrion, Some practical issues in constructing belief networks, in: J.F. Lemmer, L.N. Kanal (Eds.),Uncertainty in Artificial Intelligence, vol. 3, Elsevier, Amsterdam, 1989, pp. 161–173.[17] M. Henrion, M.J. Druzdzel, Qualitative propagation and scenario-based approaches to explanation in prob-abilistic reasoning, in: Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, 1991,pp. 17–32.[18] F.V. Jensen, Bayesian Networks and Decision Graphs, Springer, New York, 2001.[19] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kauf-mann, Palo Alto, CA, 1988.P.J.F. Lucas / Artificial Intelligence 163 (2005) 233–263263[20] S. Renooij, Qualitative approaches to quantifying probabilistic networks, PhD Thesis, Utrecht University,2001.[21] G. Shafer, A Mathematical Theory of Evidence, Princeton University Press, Princeton, NJ, 1976.[22] E.H. Shortliffe, B.G. Buchanan, A model of inexact reasoning in medicine, Math. Biosci. 23 (1975) 351–379.[23] S.L. Lauritzen, D.J. Spiegelhalter, Local computations with probabilities on graphical structures and theirapplication to expert systems, J. Roy. Statist. Soc. Ser. B 50 (1987) 157–224.[24] P.J.F. Lucas, H. Boot, B.G. Taal, Computer-based decision-support in the management of primary gastricnon-Hodgkin lymphoma, Methods of Information in Medicine 37 (1998) 206–219.[25] P.J.F. Lucas, N.C. de Bruijn, K. Schurink, I.M. Hoepelman, A probabilistic and decision-theoretic approachto the management of infectious disease at the ICU, Artificial Intelligence in Medicine 19 (3) (2000) 251–279.[26] S. Srinivas, A generalization of the noisy-OR model, in: Proceedings of the 9th Conference on Uncertaintyin Artificial Intelligence, 1993, pp. 208–215.[27] I. Wegener, The Complexity of Boolean Functions, Wiley, New York, 1987.[28] M.P. Wellman, Fundamental concepts of qualitative probabilistic networks, Artificial Intelligence 44 (1990)257–303.[29] L.A. Zadeh, Fuzzy logic and approximate reasoning, Synthese 30 (1975) 407–428.[30] N.L. Zhang, D. Poole, Exploiting causal independence in Bayesian network inference, J. Artificial Intelli-gence Res. 5 (1996) 301–328.[31] N.L. Zhang, Li Yan, Independence of causal influence and clique tree propagation, Internat. J. Approx.Reason. 19 (1997) 335–349.