Artificial Intelligence 239 (2016) 97–142Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintNorm-based mechanism designNils Bulling a,∗a Department of Intelligent Systems, Delft University of Technology, Delft, The Netherlandsb Intelligent Systems Group, Utrecht University, Utrecht, The Netherlands, Mehdi Dastani ba r t i c l e i n f oa b s t r a c tArticle history:Received 21 September 2015Received in revised form 4 July 2016Accepted 7 July 2016Available online 14 July 2016Keywords:NormsMulti-agent systemsMechanism designThe increasing presence of autonomous (software) systems in open environments in general, and the complex interactions taking place among them in particular, require flexible control and coordination mechanisms to guarantee desirable overall system level properties without limiting the autonomy of the involved systems. In artificial intelligence, and in particular in the multi-agent systems research field, social laws, norms, and sanctions have been widely proposed as flexible means for coordinating the behaviour of autonomous agents in multi-agent settings. Recently, many languages have been proposed to specify and implement norm-based environments where the behaviour of autonomous agents is monitored, evaluated based on norms, and possibly sanctioned if norms are violated.In this paper, we first introduce a formal setting of multi-agent environments based on concurrent game structures which abstracts from concrete specification languages. We extend this formal setting with norms and sanctions, and show how concepts from mechanism design can be used to formally analyse and verify whether a specific behaviour can be enforced (or implemented) if agents follow their subjective preferences. We relate concepts from mechanism design to our setting, where agents’ preferences are modelled by linear time temporal logic (LTL) formulae. This proposal bridges the gap between norms and mechanism design allowing us to formally study and analyse the effect of norms and sanctions on the behaviour of rational agents. The proposed machinery can be used to check whether specific norms and sanctions have the designer’s expected effect on the rational agents’ behaviour or if a set of norms and sanctions that realise the effect exists at all. We investigate the computational complexity of our framework, focusing on its implementation in Nash equilibria and we show that it is located at the second and third level of the polynomial hierarchy. Despite this high complexity, on the positive side, these results are in line with existing complexity results of related problems. Finally, we propose a concrete executable specification language that can be used to implement multi-agent environments. We show that the proposed specification language generates specific concurrent game structures and that the abstract multi-agent environment setting can be applied to study and analyse the behaviour of multi-agent programs with and without norms.© 2016 Elsevier B.V. All rights reserved.* Corresponding author.E-mail addresses: n.bulling@tudelft.nl (N. Bulling), M.M.Dastani@uu.nl (M. Dastani).http://dx.doi.org/10.1016/j.artint.2016.07.0010004-3702/© 2016 Elsevier B.V. All rights reserved.98N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–1421. IntroductionThe emergence of autonomous software systems and their increasing number of interactions with open environments such as the Internet, financial markets, large ICT systems, socio-technical systems and industrial platforms, urgently re-quires flexible control and coordination mechanisms in order to guarantee desirable overall system level properties. This urgency became painfully clear in 2010 by the so-called “Flash Crash”, where the uncontrolled and uncoordinated interac-tions between high frequency algorithmic trading systems in financial environments have led to extraordinary upheaval of U.S. equity markets [63]. Similar urgency is experienced in large ICT systems of organisations such as banks or insurance companies, where due to competition and innovation business processes have to be modified in rapid tempo. Such contin-uous changes constitute a potential threat for business processes to become non-compliant with the companies’ business rules and policies. There is, therefore, an increasing need for flexible control and supervision mechanisms that could ensure the compliance of business processes in such dynamic environments without limiting the functionality and performance of the business processes [47,25,49]. In addition to these existing cases, the rapid development of autonomous cars strongly suggests that future traffic will be populated by intelligent autonomous cars that interact in shared (physical) environments called smart roads. In order to guarantee the safety and throughput of such smart road environments, the behaviour of autonomous cars has to be regulated by intelligent monitoring and coordination mechanisms without directly steering their behaviour [40,38]. These and other applications show the urgency of tools and techniques to design, develop and analyse intelligent flexible control and coordination mechanisms.These and many other applications can be best modelled as multi-agent systems. A multi-agent system consists of inter-acting computer systems that are situated in some environment and are capable of autonomous actions in the environment in order to meet their delegated objectives [68]. The individual agents are generally assumed to be heterogeneous in the sense that they may be designed and developed by various parties, using different technologies, and pursuing different ob-jectives. It is exactly the autonomous and heterogeneous character of the computer agents in multi-agent applications that require intelligent flexible control and coordination mechanisms. In general, any control and coordination mechanism used in a multi-agent application should strike a balance between the autonomy of the agents on the one hand and the desirable global properties of the multi-agent system on the other hand, i.e., while the autonomy of agents should be respected, the global properties of multi-agent systems should be ensured.Existing coordination techniques in computer science, such as synchronization techniques or interaction protocols, can be used to ensure the overall desirable properties of the interacting systems. However, these techniques will severely limit the autonomy and intelligence of the involved systems [29]. In artificial intelligence, norms and norm enforcement have been widely proposed as flexible and effective means for coordinating the behaviour of autonomous agents in multi-agent systems [50,45,14]. Singh et al. [62] provide an overview of various uses and applications of norms in multi-agent systems, and Criado et al. [26] discuss some challenges and open issues concerning representation, reasoning, creation and imple-mentation of norms in multi-agent systems. A multi-agent system that uses norms to coordinate the behaviour of agents is often called norm-based multi-agent system or normative multi-agent system.In general, there are two approaches to exploit norms for coordination purposes in multi-agent systems. Norms can be either endogenous to agents in the sense that they form an integral part of the agents’ specifications [59,60], or exogenous to agents in the sense that they are enforced by some external regulatory mechanism [2,27]. Each approach comes with specific assumptions and applications. For example, the endogenous approach assumes that norms are internalised by the agents in the sense that the agents’ decision making mechanisms are designed and developed based on a given set of norms. This assumption implies that norms are available at design time and enforced on agents by the agents’ developers at design time. The endogenous approach can, for example, be used to examine the (emergent) effects of norms in agent-based simulations [59,60,48,6]. In contrast, the exogenous approach is agnostic about norm internalisation, but assumes an authority that monitors agents’ behaviour and enforces norms by means of preventing norm violations or imposing sanctions on violating behaviour [65,2,44,57]. Following the exogenous approach, autonomous systems may respect norms or decide to violate them in which case they may incur sanctions. It is exactly the incursion of sanctions that may incentivize, but not restrict, agents to behave in a particular way. The exogenous approach is also conceived as a way to make the engineering of multi-agent systems easier to manage as it supports the general principle of ‘separation of concerns’, i.e., it supports the encapsulation of coordination and control concerns in a system entity that is separable from the agents’ internals [10,17,70]. In addition, the external authority can be conceived as a mechanism that is designed to implement norms and the norm enforcement process. This perspective on norms and norm enforcement allows us to apply formal tools from game theory and to study and analyse norms and norm enforcement from a mechanism design perspective [18,19,30,69].This paper follows the exogenous approach and provides a mechanism design perspective on norms and norm enforce-ment. A multi-agent environment is modelled as a concurrent game structure where possible paths in the game structure denote possible execution traces of the corresponding multi-agent system. By considering the states of the game structure as the states of the environment in which agents operate, the concurrent game structure becomes the representation of a mechanism (game form) that specifies the effect of the agents’ actions on the multi-agent environment. The enforcement of norms on a multi-agent system may change the effect of the agents’ actions on the environment and thereby its un-derlying concurrent game structure and the set of possible execution traces. We call the modification of concurrent game structure by norm enforcement norm-based update. Clearly, various sets of norms can be used to update a concurrent game structure. The decision regarding which set of norms to use depends on the behaviour of the agents which in turn is a N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–14299result of the agents’ preferences, and, of course, the intended objective of the multi-agent system. We introduce norm-based mechanism design as a formal methodology to analyse norms and norm-based updates, and to find suitable norms taking into consideration agents’ rational behaviour.We aim at bridging the gap between norms and mechanism design by focusing on the relation between multi-agent systems, norm enforcement, and concurrent game structures. This relation sets the stage for studying formal properties of norm enforcement such as whether enforcing a set of norms implements a specific social choice function, which describes the desired system executions in relation to the agents’ preferences, in specific equilibria. This also allows us, for exam-ple, to analyse whether a group of agents is willing to obey some norms. The formal analysis is closely related to work presented in [1] and [65], where the enforcement of norms is modelled by the deactivation of transitions. Adding norms to environments in our model may, however, either deactivate transitions in the case of regimenting norms or change the effect of actions in the case of sanctioning norms. The latter is achieved by (re)labelling states with new propositions that encode, for example, sanctions or incentives which can affect the behaviour of agents. Our work is also motivated by [59]and [60], where social laws were proposed to be used in computer science to control agents’ behaviour.The proposed norm-based mechanism design is an abstract methodology as it assumes concurrent game structures as models of multi-agent environments. In order to ground this methodology, we design an executable specification language to support the development of norm-based multi-agent systems. The aim of the executable specification language is to facilitate the implementation of multi-agent environments that are governed by norms. Multi-agent environments without norms are often implemented using a computational specification language that provides constructs to specify (initial) environment states and actions. The execution of such an environment specification is a process that continuously observes agents’ actions and updates the environment state based on the specification of the observed actions. The implementation of multi-agent environments with norms requires additional constructs to specify norms and sanctions. The execution of an environment specification with norms includes two additional steps through which the observed agents’ actions are further evaluated based on the given set of specified norms after which norm violations are either prevented or sanctioned. Our abstract norm-based mechanism design methodology can be applied to norm-based multi-agent environment programs in order to analyse whether the enforcement of a set of norms by a norm-based environment program can implement a specific social choice function. We also investigate the complexity of verifying whether a given norm-based multi-agent system implements a given system specification. Besides the verification problem we analyse the decision problem whether a norm-based multi-agent system with desirable properties exists at all. In terms of complexity our results are negative, in the sense that the problems are intractable. On the positive side however, the results are in line with existing complexity results of related problems.The novel contribution of this work is a formal methodology for analysing norms and norm enforcement used for coordi-nating the behaviour of rational agents. This is realised by applying game theory/mechanism design techniques to study the effects of logic-based norms on multi-agent systems. We ground the theoretical analysis in an executable multi-agent setting to allow the development of norm-based coordination mechanisms for multi-agent systems. This paper extends and revises the work presented in [19]. The idea of norm-based mechanism design was first proposed in the extended abstract [18]. In comparison with [19] we significantly revise the formal setting, add new complexity results to new decision problems (weak and strong implementability) and give full proofs. In addition, the executable specification language as well as the related work section and a running example are new. The design of the executable specification language is inspired by the programming languages proposed in [28] and [27] for which an interpreter, called 2OPL (Organisation-Oriented Program-ming Language), has been developed.1 One of the main differences between these languages and our proposed executable specification language is the representation of norms. While norms in [28] and [27] are state-based (i.e., norms represent prohibited states), the proposed specification language in this paper considers conditional action-based norms (i.e., norms represent prohibited actions in specific states).The structure of this paper is as follows. First, Section 2 presents concurrent game structures as models for multi-agent environments and connects it to game theory. Section 3 introduces a specification language for norm-based multi-agent en-vironments, extends the formal setting of environments with norms and sanctions, and introduces the concept of norm-based mechanism design. Section 4 investigates the complexity of two implementation problems. Section 5 grounds the proposed approach by linking it to a norm-based multi-agent environment programming language. Finally, related work is discussed and some conclusions are drawn. The formal proofs about the computational complexity can be found in the appendix.2. Multi-agent environment modelIn order to illustrate our proposal, we shall use the following example throughout the paper. The example is closely related to the well-known train-gate controller example presented by [9]. Though, we slightly modify it to highlight the rational, decentralized decision making aspect of the car drivers.Example 1 (Narrow road example). The scenario consists of a narrowed road and two cars at the opposite ends of the road. The cars cannot simultaneously pass through the narrowed road. In this scenario, both cars can wait for each other forever, 1 http :/ /oopluu .sourceforge .net/.100N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142Fig. 1. Road scenario.both can move simultaneously causing a congestion/crash at the middle of the road, or one can move while the other is waiting. There are two alternatives for the latter case: either the first car moves while the second is waiting or vice versa. The system designer prefers one of these two alternatives. In order to realise this behaviour, the system designer aims at synthesizing a set of norms and corresponding sanctions such that when they are enforced the behaviour of the cars satisfies the given (system) preference assuming that the drivers’ behaviours are driven by their own individual preferences. indicates that car i is at position x ∈ {s, m, e} (s: start position, This scenario is illustrated in Fig. 1, where proposition pxim: middle position, e: end position). We assume the following states in this scenario:• q0: the cars are at their starting positions, i.e., ps1• q1: car 1 is at its ending and car 2 is at its starting position, i.e., pe∧ ps1• q2: car 1 is at its starting and car 2 is at its ending position, i.e., ps∧ pe1• q3: the cars are congested in the middle of the road, i.e., pm∧ pm2 holds1• q4: the cars are at their ending positions, i.e., pe2 holds12 holds∧ pe∧ ps2 holds2 holds2.1. Concurrent structures and strategiesIn the following we introduce concurrent game structures (CGSs) from [9] (modulo minor modifications). They serve as models for our formal analysis of the environment in multi-agent systems. An environment in a multi-agent system is assumed to be specified in terms of a set of states, possibly including an initial state, and a set of (synchronized and concurrent) transitions. Informally speaking, a CGS is given by a labelled transition system where transitions are activated by action profiles.Definition 1 (CGS, pointed). A concurrent game structure (CGS) is a tuple M = (Agt, Q, (cid:2), π , Act, d, o), comprising a nonempty finite set of all agents Agt = {1, . . . , k}, a nonempty finite set of states Q , a nonempty finite set of atomic propositions (cid:2) (also called propositional symbols) and their valuation π : Q → P((cid:2)), and a nonempty finite set of (atomic) actions Act. Function d : Agt × Q → P( Act)\{∅} defines nonempty sets of actions available to agents at each state, and o is a (cid:7) = o(q, (α1, . . . , αk)) to state q and a tuple of actions (deterministic) transition function that assigns the outcome state q(α1, . . . , αk) with αi ∈ d(i, q) and 1 ≤ i ≤ k, that can be executed by Agt in q. A pointed CGS is given by (M, q) where M is a CGS and q is a state in it.In the following, we write di(q) instead of d(i, q) and o(q, (cid:9)α) instead of o(q, (α1, . . . , αk)) for (cid:9)α = (α1, . . . , αk). In CGSs it is assumed that all the agents execute their actions synchronously.2 The combination of actions together with the current state determines the next transition of the system.Example 2 (CGS). Our scenario from Example 1 is formally modelled by CGS M1 = (Agt, Q, (cid:2), π , Act, d, o), shown in Fig. 2, where• Agt = {1, 2},• Q = {q0, . . . , q4},• (cid:2) = {pxi• Act = {M, W },• the function d is defined as| i ∈ {1, 2} and x ∈ {s, m, e}},2 We note that the framework allows to model turn-based games as a special case.N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142101Fig. 2. The CGS M1 modelling the scenario from Example 1. Nodes represent system states, bold symbols qi assigned to nodes are state names, and elements pxi within a node are atomic propositions that hold in the state.d1(q0) = {W , M} d2(q0) = {W , M}d1(q1) = {W }d2(q1) = {W , M}d1(q2) = {W , M} d2(q2) = {W }d2(q3) = {W }d1(q3) = {W }d2(q4) = {W }d1(q4) = {W }• π and o are defined as illustrated in Fig. 2, e.g. o(q0, (M, M)) = q3.We note that agent 1 and 2 have no choice other than to wait (W ) in states {q1, q3, q4} and states {q2, q3, q4}, respectively.In the following we assume that a CGS M = (Agt, Q, (cid:2), π , Act, d, o) is given, if not said otherwise. A strategy of agent i is a conditional plan that specifies what i is going to do in each situation. It makes sense, from a conceptual and computational point of view, to distinguish between two types of “situations” (and hence strategies): an agent might base its decision only on the current state or on the whole history of events that have happened. In this paper we only consider the former type of strategies which are often called memoryless or positional. In general, memoryless strategies enjoy better computational properties than the second type of strategies called perfect-recall strategies. Note that ‘memoryless’ sounds more severe than it actually is: an agent is still able to base decisions on the current state; thus, finite histories up to an arbitrary but fixed length could be encoded within states.Definition 2 (Strategy). A (memoryless) strategy for agent i is a function si : Q → Act such that si(q) ∈ di(q). The set of such strategies is denoted by (cid:5)i . A collective strategy for a group of agents A = {i1, . . . , ir} ⊆ Agt is a tuple3 s A = (si1 , . . . , sir )where each si j , 1 ≤ j ≤ r, is a strategy for agent i j . The set of A’s collective strategies is given by (cid:5) A =i∈ A (cid:5)i . The set of all (complete) strategy profiles is defined as (cid:5) = (cid:5)Agt.(cid:2)Given the notation above, (s1, s2) is a collective strategy of agents 1 and 2. A path λ = q0q1 . . . ∈ Qω is an infinite sequence of states such that for each j = 0, 1, . . . there is an action tuple (cid:9)α ∈ dAgt(q j) with o(q j, (cid:9)α) = q j+1. The set of all paths starting in state q is denoted by (cid:8)M(q). We write λ[ j] to refer to the j-th state on path λ, and λ[ j, ∞] to refer to the (sub)path q jq j+1 . . . of λ. Function outM(q, s) returns the unique path that occurs when the agents execute (the complete) strategy profile s from state q onward.Definition 3 (Outcome). The outcome outM(q, s) of a complete strategy profile s = (s1, . . . , sk) from state q in model M is the path λ = q0q1q2 . . . such that q0 = q and for each j = 0, 1, 2, . . . there is an action tuple (cid:9)α j = (α j= si(q j) for every i ∈ Agt, and o(q j, (cid:9)α j) = q j+1. Often, we will omit subscript “M” from outM(q, s) if clear from context.k ) with α j1, . . . , α jiThe following example illustrates how strategies in our example scenario can be represented.3 We assume some implicit ordering among the agents to obtain a unique representative of a strategy profile.102N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142Example 3 (Strategies). In the CGS from Example 2, we encode individual strategies by a 5-tuple prescribing an action for each state. However, as agent 1 (resp. agent 2) has no choice other than to wait (W ) in states q1, q3, and q4 (resp. q2, q3, and q4), we encode strategies by a tuple xy, indicating that agent 1 (resp. agent 2) executes actions x in q0 and y in q2(resp. x in q0 and y in q1). The strategy M W for agent 1 selects M in state q0 and W in state q2 (and also W for states q1, q3, and q4). Each agent has therefore 4 strategies resulting in a total of 16 different strategy profiles. Note that a strategy defines an action for all possible states even those not reachable if the current strategy is implemented. Therefore, some strategy profiles have identical outcome paths. For example, all strategies profiles in the set {(M(cid:9)1, M(cid:9)2) | (cid:9)1, (cid:9)2 ∈ {W , M}}result in the path q0qω3 .2.2. Agents’ preferencesIn the behavioural analysis of multi-agent systems, preferences of agents are often of utmost importance. They are the driving force of agents’ behaviour. In the models we are considering, the agents’ preferences are defined on the executions of the environment and thus paths in the corresponding CGS. Hence, we assume that agents prefer some executions over others. Therefore, we use temporal formulae to describe sets of paths. This idea was already followed in several pieces of work for similar purposes, e.g. [1] used CTL to represent agents’ preferences and [23,21] used ATL for the same purpose, where [65] used ATL for the representation of the objective of the social law. In this paper we use linear temporal logic LTL, first proposed by [56] for the verification of programs, for modelling preferences of agents. The logic extends propositional logic with operators that allow to express temporal patterns over infinite sequences of sets of propositions. It allows to ex-press natural properties related to safety, liveness and fairness properties and combinations thereof. As we aim at evaluating system paths, which are infinite sequences of states, we need a logic which allows to compare paths rather than a logic the formulae of which are evaluated in states. This makes LTL a more natural choice than, e.g., CTL and ATL. The basic temporal operators are U (until), (cid:2) (always), (cid:3) (eventually) and (cid:12) (in the next state). As before, propositions, drawn from a finite and non-empty set (cid:2), are used to describe properties of states.Definition 4 (Language LTL). Let (cid:2) be a finite, non-empty set of propositions. The formulae of LTL(cid:2) are generated by the following grammar, where p ∈ (cid:2) is a proposition: ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | ϕUϕ | (cid:12)ϕ. For convenience, we define the two temporal operators (cid:3)ϕ and (cid:2)ϕ as macros (cid:13)Uϕ and ¬(cid:3)¬ϕ, respectively, where (cid:13) ≡ p ∨ ¬p denotes universal truth. We will omit the set of propositions “(cid:2)” as subscript of LTL(cid:2) if clear from context.LTL-formulae are interpreted over ω-sequences4 (infinite words) w over sets of propositions, i.e. w ∈ P((cid:2))ω. We use the same notation for words w as introduced for paths λ, e.g. w[i] and w[i, ∞]. We note that a model M and a path λ in M—more precisely the valuation function included in the model—induce such an ω-sequence. However, we give the semantics in more general terms because it shall later prove convenient when relating the setting to mechanism design.Definition 5 (Semantics |=LTL). Let (cid:2) be a finite, non-empty set of propositions and w ∈ P((cid:2))ω. The semantics of LTL(cid:2)-formulae is given by the satisfaction relation |=LTL defined by the following cases:w |=LTL p iff p ∈ w[0] and p ∈ (cid:2);w |=LTL ¬ϕ iff not w |=LTL ϕ (we write w (cid:16)|=LTL ϕ);w |=LTL ϕ ∧ ψ iff w |=LTL ϕ and w |=LTL ψ ;w |=LTL (cid:12)ϕ iff w[1, ∞] |=LTL ϕ; andw |=LTL ϕU ψ iff there is an i ∈ N0 such that w[i, ∞] |= ψ and w[ j, ∞] |=LTL ϕ for all 0 ≤ j < i.Given a path λ and a valuation π we also write π (λ) for π (λ[0])π (λ[1]) . . . . Moreover, we just write λ |= ϕ if the model and its valuation function are clear from context. Similarly, we usually omit LTL in |=LTL.Example 4 (Preference). In our scenario, we assume that car 1 has the preference ps1next state to be its ending state (i.e., it prefers to pass through the narrow passage as the first car). The path q0qωthe preference, we have: π (q0qω1 for π being the valuation function of M1 from Example 2.1 modelling that it wants the 2 violates → (cid:12)pe2 ) (cid:16)|= ps∧ ps21→ (cid:12)pe∧ ps2We use preference lists to define preferences of agents [23]. Such a list consists of a sequence of LTL-formulae each coupled with a natural number. The formula is a binary classifier of paths in the model—considering the induced ω-words over propositions. The natural number assigns a utility to the paths which satisfy the respective formula. Thus, a preference list assigns a utility value to all paths in a model.4 We could also give a semantics over paths in a given model. We use ω-sequences over sets of propositions as it makes the semantics independent of the structure of a specific model which shall prove useful when relating the setting to mechanism design in Section 3.N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142103Definition 6 (Preference list, preference profile). A preference list over a set of propositions (cid:2) of an agent i ∈ Agt = {1, . . . , k}is given by a finite sequence γi = ((ϕ1, u1), . . . , (ϕl−1, ul−1), (ϕl, ul)) where each ϕ j ∈ LTL(cid:2), each u j ∈ N for j = 1, . . . , l − 1, ϕl = (cid:13), and ul = 0. A preference profile over (cid:2) and Agt is given by (cid:9)γ = (γ1, . . . , γk) containing a preference list over (cid:2) for each agent in Agt. We typically use Prefs to denote a non-empty set of such preference profiles. We omit mentioning the set of propositions (cid:2) and the set of agents Agt, respectively, if clear from context.A preference list of an agent may be interpreted as the agent’s goals, denoting the behaviours that the agent wants to realise. Given an agent i ∈ Agt with preference list γ = ((ϕ1, u1), . . . , (ϕl, ul)), a word w is assigned utility u j (to denote the utility of outcome path λ with π (λ) = w for agent i) if ϕ j is the formula with the smallest index j in γ that is satisfied on w, i.e., w |= ϕ j and for all l < j we have that w (cid:16)|= ϕl. Note that there is always a formula in γ which is satisfied on wsince the last formula in a preference list is required to be (cid:13).Example 5 (Preference profiles). In our scenario, we consider the following two preference profiles (cid:9)γ1 and (cid:9)γ2 of the cars where proposition finei should be read as agent i has received a fine.(cid:9)γ1 = {(((cid:12)pe∧ (cid:2)¬fine1, 3), ((cid:3)pe∧ (cid:2)¬fine1, 2), ((cid:2)¬fine1, 1), ((cid:13), 0)),11∧ (cid:2)¬fine2, 2), ((cid:2)¬fine2, 1), ((cid:13), 0)) }∧ (cid:2)¬fine2, 3), ((cid:3)pe(((cid:12)pe22(cid:9)γ2 = {(((cid:12)(pe∧ pe1∧ pe(((cid:12)(pe12) ∧ (cid:2)¬fine1, 3), ((cid:12)pe2) ∧ (cid:2)¬fine2, 3), ((cid:12)pe2∧ (cid:2)¬fine1, 2), ((cid:2)¬fine1, 1), ((cid:13), 0)),1∧ (cid:2)¬fine2, 2), ((cid:2)¬fine2, 1), ((cid:13), 0)) }The first preference profile is egoistic in the sense that the cars first prefer to reach their end positions directly without any sanction, then eventually to get to their end positions without any sanction, and finally to get no sanction. The second preference profile is more social because cars now first prefer that they both get to their end positions directly.We note that technically fine1 and fine2 are simply two fresh propositional symbols. As before states in the model can be labelled with them. Thus, it should be intuitive that the preferences can be used to classify paths according to the preferences. Later, in Section 3.1.4 we formally introduce the formal machinery to update a model by such new propositional symbols; in particular, in Example 11 we shall return to a formal treatment in the context of this example.2.3. From CGS to game theoryIn order to analyse the behaviour of agents and the emerging system behaviour, we use the machinery of game theory (cf. [54]). A strategic game form is a tuple ˆG = (Agt, ( Aci)i∈Agt, O , g) where Agt is the set of agents, Aciis the set of actions available to agent i ∈ Agt, O is a set of outcomes, and g : Ac → O with Ac = ×i∈Agt Aci is an outcome function that associates an outcome with every action profile. For the sake of readability, we use the same notation for agents in strategic game forms and CGSs. A strategic game G = ( ˆG, ((cid:17)i)i∈Agt) extends a strategic game form with a preference relation (cid:17)i on O for each agent i ∈ Agt. The preference relation of agent i induces a preference relation on action profiles: a1 (cid:17)i a2 iff g(a1) (cid:17)i g(a2) for any a1, a2 ∈ Ac. We also use payoff functions μi : O → R to represent preference relations where higher payoff values correspond to more preferred outcomes.It is well known how an extensive form game can be transformed to a strategic game without changing specific sets of strategy profiles, e.g. the set of Nash equilibria. The next definition connects CGSs to strategic games following the notation of [23]. The actions of the strategic game correspond to the (memoryless) strategies in the CGS. A payoff function is obtained from preference profiles and the outcome paths resulting from strategy profiles.Definition 7 (CGS (cid:4) strategic game form, strategic game). Let (M, q) be a pointed CGS with Agt = {1, . . . , k} and (cid:9)γ =(γ1, . . . , γk) ∈ Prefs be a preference profile. We define (cid:13)(M, q) as the strategic game form (Agt, ((cid:5)i)i∈Agt, (cid:8)M(q), g) as-sociated with (M, q) where Agt is the set of agents in M, (cid:5)i is the set of strategies of agent i (cf. Definition 2), (cid:8)M(q) the set of all paths in M starting in state q, and g(s) = outM(q, s) for s ∈ (cid:5)Agt. Moreover, we define (cid:13)(M, q, (cid:9)γ ) as the strategic game ((cid:13)(M, q), (μi)i∈Agt) associated with (M, q) and (cid:9)γ , where for all s ∈ (cid:5)Agt the payoff function μi is defined as follows: μi(s) = u j where γi = ((ϕ1, u1), . . . , (ϕl−1, ul−1), (ϕl, ul)) and j is the minimal index such that π (outM(q, s)) |=LTL ϕ j .In general, not all paths from a CGS can be obtained by memoryless strategies, i.e. outM(q, s) (cid:16)= (cid:8)M(q). As mentioned before, we use memoryless strategies because of their better computational properties. Note that the obtained strategic game form is always finite as the set of strategies of each agent is finite. Note also that the just defined strategic game is well-defined, especially because for the last entry (ϕ, u) of each preference list we have that ϕ = (cid:13).s∈(cid:5)Agt(cid:3)Example 6 (Strategic game form). For our car scenario, the strategic game form (cid:13)(M1, q0) associated with (M1, q0) is illus-trated in Fig. 3. Strategies are represented according to the conventions of Example 3.104N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–1421\2M(cid:9)1W MW WM(cid:9)2q0qω3q0q2qω4q0qω2W Mq0q1qω4qω0qω0W Wq0qω1qω0qω0Fig. 3. The strategic game form (cid:13)(M1, q0) associated with the pointed CGS (M1, q0) from Example 2. We have (cid:9)1, (cid:9)2 ∈ {M, W }.(cid:13)(M1, q0, (cid:9)γ1)1\2M(cid:9)1W MW WM(cid:9)2 W M W W3\13\21\11\11\12\31\11\11\3(cid:13)(M1, q0, (cid:9)γ2)1\2M(cid:9)1W MW WM(cid:9)2 W M W W2\12\11\11\11\11\21\11\11\2Fig. 4. The strategic games (cid:13)(M1, q0, (cid:9)γ1) and (cid:13)(M1, q0, (cid:9)γ2). Again, we have (cid:9)1, (cid:9)2 ∈ {M, W }. Payoff profiles given in bold indicate Nash equilibria. We use x\ y to denote a payoff of x and y for agent 1 and 2, respectively.A game theoretic solution concept, e.g. Nash equilibria or dominant strategy equilibrium, can be considered as a function S the domain of which is the set of strategic games and the image of which is a set of strategy profiles [54]. That is, for each strategic game G with strategy profiles (cid:5) we have that S(G) ⊆ (cid:5). We assume that the reader is familiar with the notion of solution concept and refer to [54] for further details. In the present work we are mainly concerned with the concept of Nash equilibrium and also discuss dominant strategy equilibrium. Therefore, we give the basic definitions. An action profile (a1, . . . , ak), containing an action for each agent, is a Nash equilibrium if no agent can unilaterally deviate (cid:7)i of that agent it holds that to get a better payoff; that is, for each agent i it must be the case that for all actions a(cid:7)i, ai+1, . . . , ak). We use N E(G) to refer to the Nash equilibria in a given game G. A dominant (a1, . . . , ak) (cid:17)i (a1, . . . , ai−1, astrategy equilibrium is a stronger notion. Formally, a profile (a1, . . . , ak) is a (weakly) dominant strategy equilibrium if for each agent i, the action aiis called (weakly) dominant strategy.5 We refer to [58] for more details. Furthermore, we use DOE(G) to refer to the set of dominant strategy equilibria in a given game G. We lift the notion to tuples consisting of a CGS M and a preference profile (cid:9)γ by S(M, q, (cid:9)γ ) := S((cid:13)(M, q, (cid:9)γ )).is the best action independent of the choices of the opponents. Such an action aiExample 7 (Example 6 contd.: strategic games). We include the preference profiles (cid:9)γ1 and (cid:9)γ1, respectively, from Exam-ple 5 in the strategic game form of Example 6. We obtain the strategic games (cid:13)(M1, q0, (cid:9)γ1) and (cid:13)(M1, q0, (cid:9)γ2) which are shown in Fig. 4. Bold entries are used to identify Nash equilibria. For example, we have that N E(M1, q0, (cid:9)γ1) ={(W M, M(cid:9)2) | (cid:9)2 ∈ {M, W }} ∪ {(M(cid:9)1, W M) | (cid:9)1 ∈ {M, W }}. The game (cid:13)(M1, q0, (cid:9)γ1) has no dominant strategy equilibria whereas DOE(M1, q0, (cid:9)γ2) = {(M(cid:9)1, M(cid:9)2) | (cid:9)1, (cid:9)2 ∈ {M, W }}.It is worth to note that pure Nash equilibria as well as dominant strategy equilibria may not exist. The matching pennies game6 is a classical, strictly-competitive game without Nash equilibria [54].Concluding remarksIn this section we introduced the formal setting in which we shall study the effects of imposing norms on a multi-agent system. We used LTL to define agent’s preferences. In combination with concurrent game structures they allow us to relate the multi-agent setting to normal form games, which are a well-studied mathematical model to investigate the outcome of interactions among rational agents. We presented the concept of Nash equilibrium and dominant strategy equilibrium as examples to capture agents’ rational behaviour. The general idea of normative mechanism design, introduced in the next section, however, does not depend on a specific solution concept.3. Norm-based mechanism designSo far we have considered a concurrent game structure as a formal tool to model a multi-agent system. We also assumed that agents have preferences over the system behaviour. As the actual agents’ preferences may not be known in advance, we may only assume that a set of possible preference profiles over the system behaviour is given—containing those preferences which seem sensible to the system designer. In the previous section we explained how these ingredients set the stage to analyse the “optimal” behaviours of the multi-agent systems by considering the equilibria of the game constructed from the concurrent game structure and the preference profiles. With “optimal” we refer to the system behaviours that correspond 5 We note that this is a rather weak definition of dominance, other definitions use a strict preference relation a (cid:19)i a6 Each of two agents shows one side of a coin. One agent wins if both coins show the same side. The other wins if this is not the case. The matching pennies game can, e.g., also be found in [54]., defined as a (cid:17)i aand not a(cid:7) (cid:17)i a.(cid:7)(cid:7)N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142105NotationAgtActαiQqM(cid:5)(cid:8)outsγiPrefsμiO(cid:17)iaiG, ˆGGSPf(cid:2)(cid:2)h(cid:2)sANN(cid:14)RNSNINActiDescriptionDefined/UsedPageset of playersset of actionsaction of player i, used in the multi-agent settingset of statesstateconcurrent game structureset of (memoryless) strategiesset of pathsoutcome of strategymemoryless strategypreference list of player iset of preference listsutility function of player ioutcomespreference relation of player iaction of player i, used in the game theoretical settingstrategic game, strategic game form/mechanisma set of strategic game formsa solution conceptplayersnormative behaviour functionpropositions (hard and soft facts)hard factssoft factssubset of action profilesnormative systemempty normative systemset of regimentation normsset of sanctioning normsimplementation settingset of normative systemsset of actions of player iDefinition 1Definition 1Definition 1Definition 1Definition 1Definition 1Definition 2Section 2.1Definition 3Definitions 2Definition 6Definition 6Definition 7Section 2.3Section 2.3Section 2.3Section 2.3Section 3.1.1Section 3.1.1Section 3.1.1Definition 8Section 3.1.3Section 3.1.3Section 3.1.3Definition 9Definition 9Definition 9Definition 9Definition 9Definition 12Definition 12Definition 1510010010010010010099100101101103103103103103103103105105105106106106106107107107107107112112117Fig. 5. Overview of commonly used notation.to the outcome of Nash equilibria. In this section, we further assume that a social choice function is given that models the multi-agent system designer’s preferred system behaviour, also called the social perspective. A social choice function indicates the system behaviour that is socially preferred (or preferred by the system designer) when the agents act according to a specific preference profile.The problem that we consider in this section is the implementation problem that can be formulated as follows. Suppose that for a given set of agents’ preference profiles the socially preferred system behaviour is not aligned with the agents’ preferred system behaviour, i.e. for the given agents’ preference profiles the optimal system behaviour from the agents’ perspective (represented by Nash equilibria) is not aligned/contradicts with the optimal system behaviour from the system designer’s perspective (represented by the social choice function). The question is whether the enforcement of some norms (e.g., by means of regimentation and sanctions) on the agents’ behaviour can align the preferred system behaviour from both perspectives, i.e., whether the enforcement of some norms on the agents’ behaviour can change the agents’ behaviour toward the socially preferred system behaviour. A second more elaborate question is about the existence of a set of norms whose enforcement aligns the preferred system behaviour from the agents’ as well as system designer’s perspectives. We coin the design of a set of norms to change the agents’ behaviour, which is inspired by mechanism design, norm-based mechanism design.In order to ease the reading of the rest of this paper, we list some of the concepts used in our formalisation and their notation in Fig. 5.3.1. PreliminariesWe start with reviewing concepts from classical mechanism design and introducing corresponding concepts for norm-based mechanism design. In particular, we define the concept of normative behaviour function which is the counterpart of the concept of social choice function in classical mechanism design. We then introduce the concept of norms and norm-based multi-agent system, and explain how they can influence the agents’ behaviour. We define the notion of norm enforcement formally as an update function that changes the specification of the multi-agent system based on a given set of norms. These concepts set the stage for norm-based mechanism design.3.1.1. Classical mechanism designIn our exposition of classical mechanism design we mostly follow the presentation of [54], in particular we often use their notation which allows us to clearly relate the classical concepts with the corresponding ones of norm-based mecha-nism design. In social choice theory a social choice rule f : P → P(O ) assigns a subset of outcomes from the set of outcomes 106N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–14211, (cid:17) f ast2O to a preference profile (cid:17) = ((cid:17)i)i∈Agt ∈ P , consisting of a preference relation (cid:17)i over O for each agent from Agt. Here, we consider P to be a set of such preference profiles. In contrast, a social choice function picks exactly one element of the outcome rather than a set of outcomes. For example, a profile ((cid:17) f ast) could express that both agents have a pref-erence for driving fast. In that case a social choice function can be used to map the preference profile to the outcome ohrepresenting “build highway”, i.e. f (((cid:17) f ast, (cid:17) f ast2)) = oh.In the following discussion we focus on social choice rules. The outcome f ((cid:17)) is the social outcome defined by the social choice rule f . Mechanism design is concerned with the problem of constructing a mechanism—a strategic game form—which implements a social choice rule assuming a specific rational behaviour of the agents. The mechanism is constructed over a structure fixing the set of agents Agt, the set of possible outcomes O , the set of possible preference profiles P over outcomes O , and a set of strategic game forms G with outcomes in O . The mechanism is drawn from G. This fixed structure has different names in the literature. Osborne and Rubinstein [54] refer to it as environment while Shoham and Leyton-Brown [58] use a related formalisation in terms of Bayesian game settings. We follow [54] but, in order to avoid confusion with our notation, refer to the structure as implementation setting. Now, a strategic game form G ∈ G together with a preference profile (cid:17)∈ P defines a strategic game (G, (cid:17)). Given a solution concept S (e.g., Nash equilibrium) for strategic games, i.e. a mapping from (G, (cid:17)) to a set of action profiles, we can formulate the S-implementation problem over an implementation setting as follows. A game form G ∈ G S-implements the social choice rule f over P if, and only if, for all preference profiles (cid:17)∈ P and all equilibria (a1, . . . , a|Agt|) ∈ S(G, (cid:17)) we have that the outcome obtained by (a1, . . . , a|Agt|) is contained7 in f ((cid:17)). We emphasize that S(G, (cid:17)) contains the strategy profiles in the game (G, (cid:17)) that satisfy the solution concept S.3.1.2. Normative behaviour functionAs discussed above, in social choice theory a social choice rule assigns outcomes to given profiles of preferences. In our setting, the social choice rule represents the preference of the system designer and is defined as a function8 that assigns an LTL-formula—describing a set of paths—to each preference profile (a sequence of sequences of LTL-formulae) of the agents. From now on, when considering norm-based mechanism design concepts we use our own notation that has been introduced in previous sections.Definition 8 (Normative behaviour function). Let Prefs be a set of preference profiles over (cid:2). A normative behaviour function8fis a mapping f : Prefs → LTL(cid:2).Similar to classical mechanism design, the preference of the system designer describes the designer’s desired outcome if the agents had the given preference profile as their true profile. Thus, representing the preference of the system designer by a (normative behaviour) function allows us to model the system designer’s uncertainty about the true preferences of the agents. The following is a simple example where the preference of the system designer is independent of the agents’ preferences such that it maps possible preference profiles to an LTL-formula. We choose deliberately to keep the example simple, but it should be clear that the preference of the system designer is not always a single formula and often depends on the agents’ preferences.Example 8 (Normative behaviour function). We assume that the preference of the system designer is represented by the following normative behaviour function, which indicates that in all cases the first car should reach its end position directly: f ( (cid:9)γi) = (cid:12)pe1.We refer to the outcome as the normative outcome wrt. a given preference profile. In our view, the aim of norm-based mechanism design is to come up with a norm-based mechanism (i.e., an environment specified in terms of actions, norms and sanctions) such that the agents—again following some rationality criterion according to their preferences—behave in such a way that the possible environment executions stay within the normative outcome. The idea is that norms and sanctions will (de)motivate agents to perform specific actions.Example 9 (Non-aligned preferences). Following Example 7, for (cid:9)γ1, the path q0q2qωwhich is a Nash equilibrium) in M1 does not satisfy the preference f ( (cid:9)γ1) of the system designer.4 (yielded by strategy profile (W M, M W ), 3.1.3. Normative system, hard and soft factsIn the remainder of this section, let (cid:2) again be a finite and non-empty set of propositions. We assume that (cid:2) can be partitioned into two types of propositions: the set (cid:2)h denotes the hard facts and (cid:2)s the soft facts. Hard facts describe the (physical/brute) properties of the multi-agent environment which in turn are used to define the action structure of a CGS. 7 Sometimes, it is assumed that all outcomes specified by f ((cid:17)) can be obtained by some equilibrium. It is also common to consider a social choice function that maps to single outcomes rather than a social choice rule.8 We emphasize its definition as a function. When considering the set of paths satisfying a LTL-formula rather than the formula itself, however, it shows the characteristics of a social choice rule.N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142107For example, typical hard facts describe that a car is at a certain position or that the traffic light is red. Soft facts are used to model properties which do not affect the action structure but can affect the agents’ preferences. For example, they model that a car has violated a traffic norm, a car received a fine, or a car arrived late at its destination (where we use car as a shorthand for “the driver of a car” etc.). In other words, the modification of soft facts does not change the available actions nor their executability but they can affect the evaluation of the agents’ preferences.The classification into hard and soft facts depends on the specific modelling. In the following we assume that we are given a set of hard facts (cid:2)h and a set of soft facts (cid:2)s, where sets are finite, non-empty and disjoint. Therefore, we assume that a CGS M = (Agt, Q, (cid:2), π , Act, d, o) is given with (cid:2) = (cid:2)h ∪ (cid:2)s. In this paper, we also distinguish between two types of norms. Sanctioning norms are enforced through sanctions imposed on the execution of actions from specific states. Regi-menting norms are enforced by modifying the effect of the execution of actions from a specific state; they make the actions effectless [27]. In the following, we use PL X for a set X of propositional atoms to refer to all propositional formulae over Xand denote by |=PL the satisfaction relation of propositional logic.× P( Actk)\{∅} × (P((cid:2)s)\{∅} ∪ {⊥}). Definition 9 (Norms and normative system). A norm (over M) is an element from PL(cid:2)hA norm of type (ϕ, A, ⊥) is a regimenting norm and one of type (ϕ, A, S) with non-empty S ⊆ (cid:2)s a sanctioning norm. A set of such norms N is called a normative system (over M) with a typical norm denoted by η. N(cid:14) = ∅ is the empty normative system, and Nrs = {N1, N2, . . .} is the set of all normative systems. We define Ns and Nr as the set of all normative systems consisting of only sanctioning and regimenting norms, respectively. We often write (ϕ, A, ·) to refer to a sanctioning or regimenting norm.A norm (ϕ, A, ⊥) should be read as: “it is forbidden to perform action profiles from A in ϕ-states (i.e. states in which ϕ holds) and that the enforcement of this norm prevents action profiles in A to be performed”. A norm (ϕ, A, S) should be read as: “it is forbidden to perform action profiles from A in ϕ-states and that the enforcement of this norm imposes sanctions S on the states resulting by performing the very action profile from A in a ϕ-state”. The basic idea is that sanctioning norms enforced on a model change the valuation of states by soft facts only; that is, the underlying physical structure represented by hard facts remains intact and thereby the action structure of the model remains intact. As we will see later in this paper, the agents’ preferences are also defined on soft facts such that any changes in valuations of states by soft facts may affect agents’ rational behaviour. Regimenting norms affect the physical structure as they directly affect the action structure of the model.We note that a norm (ϕ, A, ·) can be seen as a prohibition (“it is prohibited that any action profile in A is executed in states satisfying ϕ”) but just as well as a obligation to perform an action profile not in A in any state satisfying ϕ. This also corresponds to the duality of obligations and prohibitions: a prohibition of doing an action, is the same as being obliged to not doing the action.Example 10 (Norms). In order to avoid cars to congest in the narrow part of the road we introduce two sanctioning norms. The first norm prohibits the first car to wait in the start position ps2 when the second car waits as well. The violation 12, {(W , W )}, {fine1}). The ∧ psof this norm imposes sanction fine1 (i.e., car 1 is sanctioned). This norm is represented as (ps12, {((cid:9), M) | (cid:9) ∈second norm prohibits the second car to move in the start position. This norm is represented as (ps1{M, W }}, {fine2}). Note that these norms implement a priority for passage in the narrowed road by obliging car 1 to move and car 2 to wait.∧ ps∧ psIt is important to note that sanctioning norms can directly influence the (quantitative) utility that an agent obtains as specific elements of a preference list may no longer be satisfiable. To illustrate this, consider the preference profile (cid:9)γ1introduced in Example 5:(cid:9)γ1 = {(((cid:12)pe1(((cid:12)pe2∧ (cid:2)¬fine1, 3), ((cid:3)pe1∧ (cid:2)¬fine2, 3), ((cid:3)pe2∧ (cid:2)¬fine1, 2), ((cid:2)¬fine1, 1), ((cid:13), 0))∧ (cid:2)¬fine2, 2), ((cid:2)¬fine2, 1), ((cid:13), 0)) }In general, there can be an outcome of the system which ensures agent 1 a payoff of 3, i.e. an outcome that satisfies ∧ (cid:2)¬fine1. The enforcement of a norm by sanctioning norms of type (ϕ, A, {fine1}) for an appropriate formula ϕ and (cid:12)pe1set of action profiles A, however, can make it impossible to yield outcomes on which ¬(cid:2)fine1 holds. In such a case, it would be impossible for the agent to obtain a utility of 3. The technical details of this procedure are introduced in the next section.3.1.4. Norm-based updateIn order to examine the impact of the enforcement of a set of norms on multi-agent systems, we need to determine applicable norms and their sanctions. Therefore, we need to decide which norms are applicable in a concurrent game structure and what are the sanctions that should be imposed on the concurrent game structure. A norm η = (ϕ, A, ·) is applicable in a state which satisfies ϕ and in which an action tuple from A is being performed.108N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142Definition 10 (Applicable norms and sanctions). Let N ∈ Nrs be a normative system, X ⊆ (cid:2) be a set of facts, and (cid:9)α be an action profile. The set of norms from N applicable wrt. X and (cid:9)α, denoted by AppN( X, (cid:9)α), is defined as follows:AppN(X, (cid:9)α) = {η ∈ N | η = (ϕ, A, ·) with X |=PL ϕ and (cid:9)α ∈ A}.The set of sanctions from N that should be imposed based on X and (cid:9)α, denoted as SanN( X, (cid:9)α), is computed as follows:SanN(X, (cid:9)α) =(cid:4) (cid:3){⊥}{S | (ϕ, A, S) ∈ AppN(X, (cid:9)α)}if AppN(X, (cid:9)α) contains no regimenting norm,otherwise.We note that the evaluation of X |=PL ϕ can be done in polynomial time as it corresponds to evaluating a propositional formula with respect to a given truth assignment represented by the set X . Note that we use a set of facts X ⊆ (cid:2) (rather than a state q ∈ Q ) that triggers norms and sanctions. We do this in order to reuse this definition later on in this paper. In the following, we use SanN(π (q), (cid:9)α) to determine the sanctions wrt. N, q and (cid:9)α. We emphasize that the computed sanctions in a state q are meant to be imposed on the state which is reached when executing (cid:9)α, i.e., the computed sanctions are imposed on state o(q, (cid:9)α).The enforcement of a set of norms on a multi-agent system can be modelled as updating the concurrent game structure of the multi-agent system by a normative system, i.e. by regimenting or sanctioning behaviour depending on the type of norm.Definition 11 (Update by norms). Let M = (Agt, Q, (cid:2), π , Act, d, o) be a concurrent game structure and N ∈ Nrs be a norma-tive system over M. The update of M with N, denoted by M (cid:2) N, is the CGS (Agt, Q(cid:7), (cid:2), π (cid:7), Act, d(cid:7)) where(cid:7), o(cid:7), (cid:9)α) = q, S = SanN(π (q(cid:7)), (cid:9)α) and {⊥} (cid:16)= S (cid:16)= ∅ }1. Q2. π (cid:7)(x) =(cid:5)(cid:7) := Q ∪ { (q, S) | ∃qπ (x)π (q) ∪ S(cid:7) ∈ Q, (cid:9)α ∈ Actk : o(qif x ∈ Qif x = (q, S)(cid:7)i(x) = di(q) where x = q or x = (q, S), and i ∈ Agt3. d4. o(cid:7)(x, (cid:9)α) =⎧⎪⎨⎪⎩(o(q, (cid:9)α), S)o(q, (cid:9)α)xif S = SanN(π (q), (cid:9)α) and {⊥} (cid:16)= S (cid:16)= ∅if SanN(π (q), (cid:9)α) = ∅otherwise (i.e. {⊥} = SanN(π (q), (cid:9)α))for either x = (q, S(cid:7)) with S(cid:7) ⊆ (cid:2)s, or x = q.The first item shows how to duplicate states in Q that are sanctioned but not regimented. The second item defines the updated evaluation of (sanctioned) states. The third item ensures that the new (duplicated) states have the same options as their original counterparts. This is due to the fact that sanctions are solely defined on soft propositions not affecting the transition structure. Finally, the fourth item ensures that the outcome of actions from either the states in Q or their (new) duplicates are in accordance with the original model whenever the actions in those states are not regimented; otherwise when regimented, the actions have no effect and loop in the same state. It is important to notice that looping of a regimented action in a state models a situation where the action cannot be performed and should therefore be interpreted as non-performance of the action. In this case the system remains in the same state. The following example illustrates how the effect of actions is determined in an updated model.Example 11 (Norm-based update). The norm-based update of the environment model M1, as illustrated in Fig. 2, based on 2, {((cid:9), M) | (cid:9) ∈ {M, W }}, {fine2}) }, denoted by M2 := M1 (cid:2)the normative system N1 = { (ps11N1, results in a new CGS shown in Fig. 6. In particular, the effects of action profiles (M, M), (M, W ), and (W , W ) in the duplicated state (q0, {fine1}) from M1 (cid:2) N1 are defined as follows:2, {(W , W )}, {fine1}), (ps∧ ps∧ ps(cid:7)o(cid:7)((q0, {fine1}), (M, M)) = (o(q0, (M, M)), {fine2})((q0, {fine1}), (M, W )) = o(q0, (M, W ))(cid:7)((q0, {fine1}), (W , W )) = (q0, {fine1})oo∧ ps2, {(M, M)}, ⊥), (psA different norm-based update of the environment model M1 by the normative system that consists of only regimenting norms N2 = { (ps2, {(W , M)}, ⊥) } is illustrated in Fig. 7. It can be observed that the regimen-1tation of these norms does not allow car 2 to move at the starting position. Note that in our system only action profiles can be the subject of norm regimentation. However, this does not mean that individual agents cannot be the subject to norm regimentation. For example, the resulting transition system in Fig. 7 allows car 1 to do a move action in the starting position while it prevents the move action of car 2.∧ ps1N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142109Fig. 6. The CGS M2 models M1 (cid:2) N1, i.e., the norm-based update of M1 based on N1.It is important to notice that sanctions do not accumulate through state transitions. This is reflected by the following proposition which expresses that states carry the sanctions caused by the most recent action only, or the most recent action has been regimented.Proposition 1 (Non-accumulation of sanctions). Let M = (Agt, Q, (cid:2), π , Act, d, o) and M (cid:2) N = (Agt, Qand either x = q or x = (q, S(cid:7) ⊆ (cid:2)s. We have(cid:7)) with S(cid:5)(cid:7), (cid:2), π (cid:7), Act, d(cid:7), o(cid:7)), q ∈ Q(cid:7)π (cid:7)(o(x, (cid:9)α)) ∩ (cid:2)s =SanN(π (q), (cid:9)α)π (cid:7)(x) ∩ (cid:2)sif SanN(π (q), (cid:9)α) (cid:16)= {⊥},otherwise.Proof. Directly follows from Definition 11 because o(cid:7)ensures that S(cid:7)is ignored after (cid:9)α. (cid:2)The non-accumulation of sanctions holds for state transitions that take place based on non-regimented actions. A reg-imented action that loops in a state with sanctions causes state transitions through which sanctions are not removed (consecutive states through regimentation actions are identical). Although this may suggest that sanctions are accumulated through consecutive states, the persistence of the sanctions in state transitions caused by regimented actions should not be interpreted as receiving sanctions repeatedly. Note that this is consistent with our interpretation that regimented actions cannot be performed, i.e., consecutive states through regimented actions should be considered as remaining in one and the same state. However, we also agree that an alternative solution could be the one where the regimented norm causes a state transition to a fresh copy of the current state with all sanctions removed. This is a design choice. We also note that in a previous version of this work [19] we completely removed specific transitions. As a result the selection of individual actions by agents could yield invalid action profiles. The selection of such invalid action profiles was then essentially avoided by assigning to them a negative utility.We also observe that updating a CGS with only regimenting norms does not duplicate any state and may only modify the accessibility relation between the state. However, updating a CGS with sanctioning norms can duplicate and introduce new states. This update is essentially different from norm update as introduced by [1]. In case of regimentation we remove accessibility relation between states (by enforcing a looping transition) and in case of sanctioning we add new copies of states and extend the accessibility relation. In [1] an update results in restricting the accessibility relation by removing transitions from the model. Another characteristic of our model update is that different orders of updates with regimenting norms result in the same outcome, which in turn is the same as the single update with the union of regimenting norms. That is, we have that:(M (cid:2) RN) (cid:2) RN(cid:7) = (M (cid:2) RN(cid:7)) (cid:2) RN = M (cid:2) (RN ∪ RN(cid:7))110N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142Fig. 7. Norm-based update of M1 based on N2.Observe that this is not true for sanctioning norms, i.e., different orders of updates with sanctioning norms do not necessarily result in the same CGS. This means that for some CGS M and non-empty sets SN1 and SN2 we may have(M (cid:2) SN1) (cid:2) SN2 (cid:16)= (M (cid:2) SN2) (cid:2) SN1 (cid:16)= M (cid:2) (SN1 ∪ SN2).This is due to the fact that updates of sanctioning norms may duplicate states such that subsequent updates generate different states. One obvious reason different orders of updates by sanctioning norms result in different models is of a syntactic nature, interpreting = as strict/syntactic equality. We illustrate this with a small example. Suppose that a new state (q, S1) is the result of a norm update. Now, if the model is updated by another normative system, this may yield states of type ((q, S1), S2). Clearly, reversing the order in which the update is performed yields states of type ((q, S 2), S1)which are different from the previous types of states whenever S 1 (cid:16)= S2. Next we consider an example which shows that subsequent updates with sanctioning norms may generate more states than updating with the union of the sanctioning norms at once as illustrated in the example below. Conceptually, all norm violations occurring at the same time should also be sanctioned immediately. This is reflected in our definition of norm update and illustrated in Example 12 and Fig. 8.Example 12 (Update order with sanctioning norms). We consider the CSG M shown in Fig. 8 and update it with SN1 ={(ˆq, {a}, {s1})} and SN2 = {(ˆq, {a}, {s2})} in different orders, and at once (ˆq is a propositional formula that is true only in state q), respectively.The example showed that the models are different. Interestingly, however, in Example 12 if the models (M (cid:2) SN1) (cid:2) SN2, (M (cid:2) SN2) (cid:2) SN1 and M (cid:2) (SN1 ∪ SN2) are restricted to states reachable from q, then these models are identical apart from the names of their states. Essentially, given a pointed CGS, different orders of updates with sanctioning norms result in similar CGSs if we consider the parts of the updated CGSs that are reachable from the initial state only. Indeed, it can be shown that the models (M (cid:2) SN1) (cid:2) SN2 and (M (cid:2) SN2) (cid:2) SN1 are identical apart from the names of states9 if restricted to the part reachable from a given state. One may be tempted to conclude that in the same sense these models are similar to M (cid:2) (SN1 ∪ SN2); however, this is not always the case as the reachable part of the latter model may have strictly less states than the previous models. It is our conjecture, however, that the reachable part of these models are bisimilar to each other. As these results are not directly relevant for the exposition of this paper, we omit a formal treatment of this matter.The order of updates for some non-empty sets of sanctioning and regimenting norms is important as well, i.e., updating first with sanctioning norms and then with regimenting norms may result in different outcomes than updating first with regimenting norms and then with sanctioning norms. Moreover, subsequent updates with regimenting and sanctioning norms do not necessarily result in the same outcome as the single update with the union of regimentation and sanctioning norms. The reason for this is that two consecutive updates by RN and SN result in different outcomes as one single update by RN ∪ SN. The basic intuition is that regimenting norms have priority and if an action has been regimented it does not make sense to sanction the action as it can by the nature of regimentation no longer be executed. This means that for some CGS M and non-empty sets SN and RN, we have9 More formally, they are isomorphic in the sense of Definition 24. Reachability is formally introduced in Definition 23.N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142111Fig. 8. Updating CGS M with sanctioning norms in different orders results in different models.Fig. 9. Updating CGS M with sanctioning and regimenting norms in different orders results in different models.(M (cid:2) RN) (cid:2) SN (cid:16)= (M (cid:2) SN) (cid:2) RN (cid:16)= M (cid:2) (RN ∪ SN)These observations are illustrated in Example 13 and Fig. 9, where (M (cid:2) SN) (cid:2) RN is different from (M (cid:2) RN) (cid:2) SN, which is again different from M (cid:2) RN, which is in this particular case the same as M (cid:2) (RN ∪ SN).Example 13 (Update order with sanctioning and regimenting norms). We consider the CGS M shown in Fig. 9 and update it with SN = {(ˆq, {a}, {s})} and RN = {(ˆq, {a}, ⊥)} in different orders, and at once (ˆq is a propositional formula that is true only in state q), respectively.We summarize the results in the following proposition.Proposition 2 (Properties of norm update). Let M = (Agt, Q, (cid:2), π , Act, d, o) be a CGS, RN, RNand SN ∈ Ns be a set of sanctioning norms over M. Also let N = RN ∪ SN. Then, we have(cid:7) ∈ Nr be sets of regimenting norms, 1. M (cid:2) N(cid:14) = M2. (M (cid:2) RN) (cid:2) RN3. In general, we have: (M (cid:2) SN) (cid:2) RN (cid:16)= M (cid:2) N = M (cid:2) (RN ∪ SN) (cid:16)= (M (cid:2) RN) (cid:2) SN4. If forall q ∈ Q and for all (cid:9)α ∈ dAgt(q) we have that AppRN(π (q), (cid:9)α) = ∅ or AppSN(π (q), (cid:9)α) = ∅ then M (cid:2) N = (M (cid:2) RN) (cid:2) SN.(cid:7)) (cid:2) RN = M (cid:2) (RN ∪ RN(cid:7) = (M (cid:2) RN(cid:7))112N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142classical mechanism design settingagents Agtoutcomes Opreference relation (cid:17)iset of preference profiles Pmechanism G ∈ Gset of mechanisms/strategic game forms Gimplementation setting (Agt, O , P , G)social choice rule f : P → P(O )norm-based mechanism design settingagents Agtω-words P((cid:2)s ∪ (cid:2)h)ωpreference list γi over LTL-formulaeset of preference profile lists Prefsnormative mechanism (M, N)set of norm-based mechanisms N = {N1, N2, . . .} wrt. Mnorm-based implementation setting (M, Prefs, N )normative behaviour function f : Prefs → LTLFig. 10. Correspondence between mechanism design and norm-based mechanism design.Proof. (1) Obvious. (2) Regimenting norms may introduce loops. Applying a regimenting norm on an already regimented action has no effect. (3) Follows from Example 13. (4) If in no state a regimenting norm and a sanctioning norm are applicable, then the updates cannot interfere. The result follows trivially. (cid:2)The properties above concern the iterated applications of the update operation. In the rest of this paper, we focus on single updates.3.2. Implementation settingIn this section we introduce norm-based mechanism design by considering a mechanism using norms to influence the agents’ behaviour. We use a normative behaviour function to assign a set of “desired” environment executions (desired from the system designer’s perspective), represented by an LTL-formula, to each preference profile of the agents. Thus, based on the preferences of the agents the system designer prefers specific outcomes. The aim of norm-based mechanism designis to yield a normative system (i.e., a set of regimenting and sanctioning norms) for a given environment such that the enforcement of this normative system in the environment motivates the agents to behave in such a way that the outcomes preferred by the system designer are realised. The idea is that the enforcement of norms (de)motivates or prevents agents to perform specific actions. In order to predict how agents act we assume that they act rationally. What playing rationally means shall be specified by game theoretic solution concepts.Below we introduce the norm-based implementation setting, where mechanism design is interpreted in terms of norms. The basic structure consists of a CGS, a set of preference profiles, and a set of normative systems which if applied on the CGSchange the underlying CGS. A tuple consisting of the CGS and such a normative system is called norm-based mechanism. In this sense a norm-based mechanism relates to game forms which are mechanisms in classical mechanism design. Outcomes are defined as paths and preferences over those paths are specified by preference profiles based on LTL-formulae. In Fig. 10we summarize the correspondence between (classical) mechanism design and norm-based mechanism design.Definition 12 (Norm-based implementation setting). A (norm-based) implementation setting (NIS) is given by a tuple I =(M, Prefs, N ) where• M = (Agt, Q, (cid:2), π , Act, d, o) is a CGS with (cid:2) = (cid:2)h ∪ (cid:2)s partitioned into hard and soft facts.• Prefs is a set of preference profiles over Agt and (cid:2).• N = {N1, N2, . . . } is a set of normative systems over (cid:2) with Ni = SNi ∪ RNi , where SNi ∈ Ns and RNi ∈ Nr.The tuple (I, q) where q is a state of M is called pointed (norm-based) implementation setting.For the remainder of this section we assume that I = (M, Prefs, N ) is a fixed implementation setting where M =(Agt, Q, (cid:2), π , Act, d, o) and q is a state in M.Definition 13 (Norm-based mechanism). Let N ∈ Nrs be a normative system (over M). The tuple (M, N) is called norm-based mechanism.A norm-based mechanism (M, N) gives rise to a CGS by updating M with N as introduced in Definition 11.Example 14 (Norm-based mechanism). The environment model M1, as illustrated in Fig. 2, together with the normative system N1, as defined in Example 11, constitute a norm-based mechanism (M1, N1), which gives rise to CGS M2 from Example 11 and Fig. 6. The CGS M2 yields in turn the strategic game form (cid:13)(M1 (cid:2) N1, q0) as illustrated in Fig. 11. For simplicity reasons, we encode strategies as triples (x, y, z). For agent 1 (resp. agent 2) the triple encodes that the agent executes action x at q0, y at q(cid:7)2 (resp. x at q0, y at q(cid:7)0, and z at q1).(cid:7)0, and z at qA norm-based mechanism defines regimenting and sanctioning norms that influence agents’ behaviour in certain direc-tions. Agents are assumed to act rationally, in particular in accordance to their preferences. We use the concept of Nash N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–1421131\2M (cid:9)1 (cid:9)(cid:7)1W W WW W MW M WW M MM (cid:9)2 (cid:9)(cid:7)2(cid:7) ωq0q3(cid:7) ωq0q2(cid:7)2qωq0q4(cid:7) ωq0q2(cid:7)2qωq0q4W W Wq0qω1(cid:7) ωq0q0(cid:7) ωq0q0(cid:7)0qωq0q(cid:7)0qωq0q11W W Mq0q1qω4(cid:7) ωq0q0(cid:7) ωq0q0(cid:7)0q1qω(cid:7)0q1qω44q0qq0qW M Wq0qω1(cid:7) ω(cid:7)0qq0q2(cid:7)(cid:7)2qω0qq0q4(cid:7) ω(cid:7)0qq0q3(cid:7) ω(cid:7)0qq0q3W M Mq0q1qω4(cid:7) ω(cid:7)0qq0q2(cid:7)(cid:7)2qω0qq0q4(cid:7) ω(cid:7)0qq0q3(cid:7) ω(cid:7)0qq0q3Fig. 11. The game form associated to M1 (cid:2) N1 where (cid:9)1, (cid:9)(cid:7)1, (cid:9)2, (cid:9)(cid:7)2∈ {M, W }.(cid:13)(M1 (cid:2) N1, q0, (cid:9)γ1)2M (cid:9)2 (cid:9)(cid:7)1\01\02\01\02\02M (cid:9)2 (cid:9)(cid:7)1\01\02\01\02\0W W WW W MW M WW M M3\10\10\10\10\13\20\10\10\20\23\10\10\00\00\03\20\00\00\00\0(cid:13)(M1 (cid:2) N1, q0, (cid:9)γ2)W W WW W MW M WW M M2\10\10\10\10\12\20\10\10\20\22\10\10\00\00\02\20\00\00\00\01\2M (cid:9)1 (cid:9)(cid:7)1W W WW W MW M WW M M1\2M (cid:9)1 (cid:9)(cid:7)1W W WW W MW M WW M MFig. 12. Strategic games (cid:13)(M1 (cid:2) N1, q0, (cid:9)γ1) and (cid:13)(M1 (cid:2) N1, q0, (cid:9)γ2) where bold entries mark the Nash equilibria, where (cid:9)1, (cid:9)(cid:7)1, (cid:9)2, (cid:9)(cid:7)2∈ {M, W }.equilibrium as rationality criterion. As the fulfilment of agents’ preferences depends on the valuation of states on a path, agents’ behaviour may be affected if imposing norms and sanctions would cause a modification of states’ valuations. Thus, norms can provide an incentive (e.g. in the case of sanctioning norms) to agents to change their behaviour. Therefore, a key question is how to engineer good incentives. In [19], we required that for a successful implementation of a normative behaviour function, all resulting Nash equilibria have to agree with the behaviour specified by the normative behaviour function, which is essentially the case in the classical game theoretic implementation setting given by [54]. Following [69]and [30] we also introduce a weaker notion of implementation where the existence of some good behaviour is sufficient. We use the notation of [69] and [30] and introduce weak and strong implementation.Definition 14 (S-implementation). Let I = (M, Prefs, N ) and N ∈ N . We say that a norm-based mechanism (M, N) (strongly) S-implements the normative behaviour function f over (I, q) iff for all (cid:9)γ ∈ Prefs, S((cid:13)(M (cid:2) N, q, (cid:9)γ )) (cid:16)= ∅ and for all s ∈ S((cid:13)(M (cid:2)N, q, (cid:9)γ )) : outM(cid:2)N(q, s) |=LTL f ( (cid:9)γ ). We say that (M, N) weakly S-implements normative behaviour function f over (I, q) iff for all (cid:9)γ ∈ Prefs there is an s ∈ S((cid:13)(M (cid:2) N, q, (cid:9)γ )) such that outM(cid:2)N(q, s) |=LTL f ( (cid:9)γ ). We define the following corresponding sets:SIS (I, q, f ) = {N ∈ N | (M, N) strongly S-implements f over (I, q)}.WIS (I, q, f ) = {N ∈ N | (M, N) weakly S-implements f over (I, q)}.If SIS (I, q) (cid:16)= ∅ (resp. WIS (I, q) (cid:16)= ∅), we say that fis strongly (resp. weakly) S-implementable over (I, q).The next example gives a norm-based mechanism which strongly N E -implements a normative behaviour function.Example 15 (Norm implementation). Adding the preference profiles (cid:9)γ1 and (cid:9)γ2 to the game form (cid:13)(M1 (cid:2) N1, q0) from of Example 14 results in the strategic games shown in Fig. 12. Bold payoffs indicate the Nash equilibria. The equilibria paths do now satisfy the system designer’s preference.Concluding remarksThis concludes our formal model of norm-based mechanism design. The complexity results in the following section are based on Nash equilibrium implementability. In general, other solution concepts, like dominant strategy equilibria, may be considered, depending on the problem at stake. A natural question is why we should be interested in strong or weak implementability. The short answer is: to obtain a stable and desired system behaviour. For illustration let us consider 114N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142smart energy grids involving actors such as producers, consumers and regulation authorities. For the sake of the argument we simply assume producers and consumers to act rationally according to the concept of Nash equilibrium. Clearly, this is an idealised and abstracted view,10 but we believe it delivers the main ideas behind our framework. The regulation authority may have information about the producers’ and consumers’ preferences. The information might be obtained based on common knowledge (power should always be available), previous behaviour of the producers and consumers, or market research techniques. The regulation authority has some objectives such as flattening peak energy demand by consumers or efficient energy production by producers that it wants to have met. As all actors are self interested, the system (1) may not show stable behaviour in the sense that there is no optimal (social) solution, or (2) may not satisfy the objectives of the regulation authority. In order to overcome these problems, the authority can introduce and enforce norms in order to restrict or incentivize consumers and producers to change their behaviour. For example, additional fees11 may be charged on consumers if energy is used at specific hours, or fines are imposed on producers for overproduction. In this context, strong implementability means that all stable behaviours satisfy the authority’s objectives, i.e., if all actors act rationally and play equilibrium strategies the objectives of the regulation authority are guaranteed. On the other hand, weak implementability indicates that there are some stable behaviour that satisfy the objectives of the regulation authority, but additional means are needed to coordinate on such a behaviour. Weak implementability can therefore be seen as a first step. Obviously, there are many applications for which our formal methodology can be used to analyse and improve the overall system behaviour, e.g., transportation systems, traffic, financial markets and business processes. In general, our formal methodology can be applied to applications where the behaviour of involved actors/processes need to be monitored and controlled. In the next paragraph we shall investigate the complexity of weak and strong implementation.4. Verification and complexity issuesIn this section we consider the complexity of the problem whether some normative system implements a normative behaviour function and whether such a normative system exists at all. We focus on implementation in Nash equilibria. These results are important in order to check whether a norm-governed environment ensures that the agents’ rational behaviour satisfies the system specification. We present the proofs of the main results and moved technical details to the appendix.For the complexity results we first need to be clear about how the size of the input is measured. The size of a CGS is defined as the product of the number of states and the number of transitions, which is denoted by |M|. The size of a finite normative system N is given by the number of norms it is comprised of times the maximal length propositional formula contained in a norm of N (i.e. a norm is only measured in terms of the size of its precondition, as it can be assumed that the other elements are bound by the size of the model and the agents’ preferences). The size of a set of normative systems is measured by |N | · |Nmax| where Nmax ∈ N is a normative system of maximal size contained in N . Moreover, we assume that any considered normative behaviour function f is polynomial-time computable. The first result investigates the complexity of performing a norm-based update.Proposition 3. Let I = (M, Prefs, N ) be given. For any N ∈ N , M (cid:2) N can be computed in polynomial time wrt. the size of M and N. The size of M (cid:2) N is bounded by O (|M|4).Proof. Let M(cid:7) := M (cid:2) N, and n and t denote the number of states and transitions in M, respectively. The set AppN(π (q), (cid:9)α)can be computed in polynomial time in the size of N: for each η = (ϕ, A, ·) ∈ N it has to be checked whether π (q) |=PL ϕ. To determine the size of M(cid:7)is bounded by n + n · n · t = O (n2t) as the (cid:7)), (cid:9)α) is bounded by n · t and by Proposition 1 sanctions are not accumulated, i.e. there are at most number of tuples (π (qn2 · t many new states of the form (q, S). In each new state (q, S) the choices are the same as in q, thus also the number of outgoing transitions. The number of transitions in M(cid:7)is bounded by O (n4 · t3) ≤ O (|M|4). (cid:2)is bounded by O (n2t) · t = O (n2t2). Thus, the size of M(cid:7)we inspect Definition 11. The number of states of M(cid:7)The next result is concerned with the special case of the weak implementation problem. We investigate the complexity of verifying whether a given normative system weakly implements a given normative behaviour function. Hardness is shown by a reduction of QSAT2 which refers to the satisfiability problem of Quantified Boolean Formulae (cf. Appendix A). In the appendix we show how to construct a model Mφ and a preference profile from a QSAT2-formula φ such that there is a Nash equilibrium iff φ is satisfiable. The construction of Mφ is based on a construction proposed by [22].10 In particular, we assume that the amount of energy demanded and supplied can be measured qualitatively expressing, e.g., ‘low’, ‘medium’, and ‘high’ demand.11 We note that fines and incentives can be measured quantitatively, assuming that the set of possible numerical values is finite and fixed in advance. We stress that the purpose of this example is to illustrate the conceptual idea of this approach rather than giving a full-fledged real-world modelling of smart grids.N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142115Theorem 4 (Weak implementation: verification). Let (I, q) be a pointed NIS, where I = (M, Prefs, N ) and N ∈ N be a normative system. Let also f be a normative behaviour function. The problem whether N ∈ WIN E (I, q, f ) is (cid:2)P2 -complete in the size of M, Nand Prefs.(cid:7)(cid:7)(cid:7)Proof. Membership: We construct a non-deterministic polynomial time oracle Turing machine M which can make calls to a . It takes as input a strategy profile non-deterministic polynomial time Turing machine Ms and returns true if the profile is not a Nash equilibrium. Therefore, for each agent i, the oracle machine Mguesses an (cid:7)i) yields a better payoff than s. If the profile yields a better payoff individual strategy sthe machine returns true. The machine M works as follows. Firstly, M computes M (cid:2) N. Then, for each profile (cid:9)γ ∈ Prefs the machine guesses a strategy profile s and checks whether π (out(q, s)) |=LTL f ( (cid:9)γ ). The latter check can be done in polynomial deterministic time as s determines a cyclic path in M on which the truth of a linear-time temporal formula can easily be determined. If successful the machine checks whether s is a Nash equilibrium in (cid:13)(M, q, (cid:9)γ ) by calling Mand reverting the answer. This shows that the problem whether N ∈ WIN E (I, q, f ) is in (cid:2)P2(cid:7)i for i and checks whether (s−i, s. We first describe the machine MSketch of hardness: The hardness result is proven in Theorem 18 in the appendix. Here, we only give the high level idea of the reduction of QSAT2 to the weak implementability problem. Given a QSAT2 formula φ we construct a two player CGS Mφ consisting of the refuter player r and the verifier player v. Essentially, the players decide on the value of universally and existentially controlled variables, respectively. Once they decide on a truth assignment, they play a game to evaluate the Boolean formula contained in φ following the game theoretical semantics of propositional logic. That is, the refuter player r and the verifier player v control conjunctions and disjunctions, respectively. Using this result, we construct a preference profile (γv, γr) such that φ is satisfiable iff ∃s ∈ N E(Mφ, q0, (γv, γr)) such that out(q0, s) satisfies a constructed formula f (γv, γr), the state q0 is a distinguished initial state in Mφ . Now, this is the case if the empty normative systems N(cid:14) ∈ WIN E (I, q0, f ). (cid:2)= NPNP.(cid:7)[2]Analogously to the previous result, we next investigate whether a given normative system strongly implements a norma-tive behaviour function. In the following, we show that the verification version of the strong implementation problem is in . This complexity class consists of all problems which can be solved by a polynomial-time deterministic oracle Turing 2 , cf. [67]. Non-adaptive means that queries must be (cid:2)P2P(cid:23)machine which can make two non-adaptive queries to a problem in (cid:2)Pindependent from each other; in other words, it must be possible to perform them in parallel.Theorem 5 (Strong implementation: verification). Let (I, q) be a pointed NIS, where I = (M, Prefs, N ) and N ∈ N be a normative system. Let also f be a normative behaviour function. The problem whether N ∈ SIN E (I, q, f ) is in P2 -hard as well as (cid:3)P2 -hard in the size of M, N and Prefs.. The problem is (cid:2)P(cid:2)P2(cid:23)[2]Proof. Membership: We construct a deterministic polynomial time Turing machine (TM) N with an (cid:2)P2 -oracle, implemented twice: by a non-deterministic TM Nto check whether N E((cid:13)(M (cid:2) N, q, (cid:9)γ )) (cid:16)= ∅ and whether for all s ∈ N E((cid:13)(M (cid:2) N, q, (cid:9)γ )) it holds that π (outM(cid:2)N(q, s)) |=LTLf ( (cid:9)γ ). The first part is done as in the proof of Theorem 4. For the second part, N calls Nwith the additional input ¬ f ( (cid:9)γ )and reverts the answer of Nworks similar to the TM M of Theorem 4. N uses N. This shows that the problem is in Pwith NP-oracle. The TM N[2].(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:2)P2(cid:23)Sketch of hardness. The hardness results are proven in Theorem 21. The intuition is similar to the one given in Theo-rem 4 where the same two-player model Mφ is used, but the preference profiles of both players are changed. This case is technically more difficult as the verification problem consists of two parts: (i) ensuring that the set of Nash equilibria is non-empty; and (ii) ensuring that all Nash equilibria satisfy the outcome of the normative behavior function. For the (cid:3)P2 -hardness we also need to slightly modify the model Mφ by updating the roles of the verifier and the refuter. (cid:2)The next lemma shows that normative systems enjoy a small representation property: any update of a model by a normative systems can be obtained by a “small” normative system which is polynomial in the size of the model.Lemma 6. Let I = (M, Prefs, N ) be given where N ∈ {Nr, Ns, Nrs}. For each N ∈ N there is an N(cid:7)such that M (cid:2) N = M (cid:2) N, where k is the number of agents in M.(cid:7) ∈ N with |N(cid:7)| ≤ 2 · |Q| · | Actk|(cid:10)Proof. Let N be given, comprised of sanctioning norms SN and regimenting norms RN. For each q ∈ Q and (cid:9)α ∈ Actk we de-¬p. We add the sanctioning norm (π ∗(q), { (cid:9)α}, Sq, (cid:9)α)fine Sq, (cid:9)α = SanN(π (q), (cid:9)α) and π ∗(q) as the formula (cid:7)(cid:7)| ≤ |Q| · | Actk|. It remains to show that M (cid:2) N = M (cid:2) N(cid:7). to N(cid:7)By Proposition 2.4 and the properties of N. Thus, we can consider sanctioning and regimenting norms separately.if ⊥ /∈ Sq, (cid:9)α , and (π ∗(q), { (cid:9)α}, ⊥) otherwise. We observe that |Nwe have that M (cid:2) N(cid:7) = (M (cid:2) RN(cid:7)Firstly, we show that M (cid:2) RN = M (cid:2) RNthat AppRN(π (q), (cid:9)α) = ∅ iff AppRN(cid:7)with π (q) |= ϕ and (cid:9)α ∈ A. This implies (π ∗(q), { (cid:9)α}, ⊥) ∈ RN. Let q and (cid:9)α be a state and action profile in M, respectively. It suffices to show (cid:7) (π (q), (cid:9)α) = ∅. “⇐”: AppRN(π (q), (cid:9)α) (cid:16)= ∅ implies that there is an η = (ϕ, A, ⊥) ∈ RN(cid:7) (π (q), (cid:9)α) (cid:16)= ∅(cid:7) (π (q), (cid:9)α) (cid:16)= ∅. “⇒”: AppRNwhich implies AppRNp∈π (q) p ∧(cid:7)(cid:7)) (cid:2) SNp /∈π (q)(cid:10)116N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142implies ∃η = (π ∗(qwith π (q) |= π ∗(q(cid:7)(cid:7)), { (cid:9)α}, ⊥) ∈ RNthen also π (q) |= ϕ which implies AppRN(π (q), (cid:9)α) (cid:16)= ∅.(cid:7)). This implies ∃η(cid:7) = (ϕ, A, ⊥) ∈ RN with (cid:9)a ∈ A and π (q(cid:7)) |= ϕ. But Secondly, we show that SanSN(cid:7) (π (q), (cid:9)α) = SanSN(π (q), (cid:9)α) for all q and (cid:9)α in M whenever AppRN(π (q), (cid:9)α) = ∅:SanSN(cid:7) (π (q), (cid:9)α) ====(cid:11)(cid:11)(cid:11)(cid:11)(cid:7)(cid:7)(q(q), { (cid:9)α}, Sq(cid:7), (cid:9)α) ∈ AppSN), { (cid:9)α}, Sq(cid:7), (cid:9)α) ∈ AppSN{Sq(cid:7), (cid:9)α | (π ∗{Sq(cid:7), (cid:9)α | (π ∗{S | (ϕ, A, S) ∈ AppSN(π (q{S | (ϕ, A, S) ∈ AppSN(π (q), (cid:9)α)}(cid:7)(cid:7) (π (q), (cid:9)α)}(cid:7) (π (q), (cid:9)α), π (q) = π (q(cid:7)(cid:7))}), (cid:9)α), π (q) = π (q)}= SanSN(π (q), (cid:9)α).This implies that the updates yield identical models. (cid:2)Now we consider the problem whether there is a normative systems that weakly implements a normative behaviour function.Theorem 7 (Weak implementation: existence). Let (I, q) be a pointed NIS, N ∈ {Nrs, Nr, Ns}, N ∈ N , and f be a normative behaviour function. The problem whether WIN E (I, q, f ) (cid:16)= ∅ is (cid:2)P2 -complete.(cid:7) ∈ WIN E (I, q, f ). We Proof. Membership. By Lemma 6, if N ∈ WIN E (I, q, f ) then there is also a “small” normative system Nextend the algorithm described in the proof of Theorem 4 in such a way that the TM M guesses, in addition to the strategy profile, a “small” normative system N, and then works as before. Hardness follows from Theorem 4. (cid:2)For strong implementation it is no longer possible to guess a strategy profile but the normative behaviour function must be satisfied on all Nash equilibria. Thus, we can use the result of Theorem 5, but first a small normative system is guessed.Theorem 8 (Strong implementation: existence). Let (I, q) be a pointed NIS and N ∈ {Nrs, Ns, Nr}, N ∈ N , and f be a normative behaviour function. The problem whether SIN E (I, q, f ) (cid:16)= ∅ is (cid:2)P3 -complete.Proof. Membership. By Lemma 6, if N ∈ SIN E (I, q, f ) then there is also a “small” normative system N(cid:7)construct a non-deterministic TM M which uses Nadditionally guesses a small normative system N, and then works as N. This shows that the problem is in NP(cid:4)P(cid:7) ∈ SIN E (I, q, f ). We from Theorem 5 as oracle TM. M works as N from Theorem 5 but Sketch of hardness. The hardness proof for sanctioning norms and for regimentation norms is given in Theorems 25and 29, respectively, where the basic intuition is similar to the one given in Theorems 4 and 5, the construction is slightly more sophisticated. As we now reduce QSAT3 to the strong implementation problem, we need to include the additional existential quantification in QSAT3 in the construction. For this purpose we encode a truth assignment of variables as a normative system. Guessing such a normative system corresponds to guessing a truth assignment of the newly existentially quantified variables. The difficulty in the construction is to ensure that the guessed normative system does not affect “good parts” of the model used in the construction of the two previous hardness results (Theorems 4 and 5) because after a normative system has been guessed the two players should essentially simulate the semantics of φ by guessing truth assignments of the variables under the scope of the other two quantifiers in the given QSAT3-formula, followed by playing the game theoretic game to evaluate the resulting propositional formula. (cid:2)3 = (cid:2)P3 .Concluding remarksIn this section we analysed the complexity of the weak and strong implementation problem. The computational com-plexity of the membership problems, i.e. whether a given normative system weakly or strongly implements a normative behaviour function, respectively, were shown to be essentially located on the second level of the polynomial hierarchy [55]. In [37] it was shown that checking the existence of a pure Nash equilibrium is NP-complete: a strategy profile must be guessed and then verified, in polynomial time, whether it is a Nash equilibrium. The latter check cannot be done in poly-nomial time in the setting considered here, as the number of strategies of each player is exponential in the size of the model; there are | Act||Q|= NPNP seems intuitive for weak implementation (an existential problem: guess a strategy profile with specific properties and verify it against all deviations) = coNPNP for strong implementation (a universal problem). Our results show that the complex-and a lower bound of (cid:3)P2ities of the decision problems considered here are in line with these intuitive bounds: in Theorem 5 we show (cid:2)P2 as well as (cid:3)Pupper bound. Moreover, we show that the weak implementation problem is no more difficult than the verification of checking that a given normative system weakly implements a normative behaviour function. The complexity of the strong implementation problem is located one level up in the polynomial hierarchy. The results do many of these. Given this observation, a lower bound of (cid:2)P22 hardness and prove an P(cid:2)P2(cid:23)[2]N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142117also not appear that bad when compared with the complexity results of [69] in the context of Boolean games. The authors show that the problem whether there is an taxation scheme which ensures the existence of a pure Nash equilibrium with desirable properties is already (cid:2)P2 -complete in the pure setting of Boolean games. Thus, we cannot hope for any better results for our weak implementation problem. That our strong implementation problem is more difficult than its weak ver-sion, (cid:2)P3 -complete, stems from the fact that an additional normative system has to be guessed such that all Nash equilibria satisfy a specific property.5. Multi-agent environment programsThe model of a multi-agent environment, as proposed in Section 2, is abstract in the sense that it assumes that the set of states and the state transitions are without an internal structure. In this section, we propose a specification language to program multi-agent environments. The introduction of this specification language allows us to program mechanisms and apply our formal methodology, as proposed in Section 3, to analyse the behaviour of such programs. In this way, one can check whether an environment program can ensure the system designer’s objectives given that the participating agents behave according to their preferences. The proposed language allows the specification of an initial environment state as well as a set of synchronized actions. In environment programming, we are agnostic about individual agents and how they are programmed. We assume that a set of agents performing synchronized actions in the environment is given. These actions form the input of an environment program. The structural operational semantics of the language specifies the execution of programs. We show that the proposed language can be used to program a broad class of multi-agent environments as defined in Section 2. We extend the specification language with norms, as defined in Section 3, and present its operational semantics. We show that the extended language is expressive enough to program norm-governed multi-agent environments. In particular, we show that adding a set of norms to the program of a given multi-agent environment specifies the multi-agent environment updated with the set of norms.In the rest of this section, we follow our abstract model of norm-based multi-agent systems and distinguish hard and soft facts. We assume that the states of norm-based environment programs is represented by hard and soft facts. We use (cid:2) = (cid:2)h ∪ (cid:2)s to denote the union of disjoint and finite sets of hard and soft facts (i.e. (cid:2)h ∩ (cid:2)s = ∅), (cid:2)l= {p, ¬p | p ∈ (cid:2)h}hbe the set of hard literals, (cid:2)ls= {p, ¬p | p ∈ (cid:2)s} be the set of soft literals, and (cid:2)l = (cid:2)lh∪ (cid:2)ls.5.1. Programming multi-agent environmentsA multi-agent environment can be programmed by specifying its initial state and a set of action profiles. The initial state of an environment program is specified by a set of atomic propositions and the action profiles are specified in terms of pre- and postconditions. The precondition of an action profile is a set of literals that specify the states of the environment programs in which the performance of the action profile results in a state transition. The resulting state is determined by adding the positive atoms of the action’s postcondition to the state in which the action is performed and removing the negative atoms of the postcondition from it. The pre- and postconditions of action profiles are assumed to consist of hard facts such that action profiles are activated by hard facts and change only those facts of the program states. The performance of an action profile by individual agents in a state that does not satisfy its precondition is assumed to have no effect on the state and considered as looping in that state. Please note that in environment programming we are agnostic about individual agents and their internals. We assume that the agents decide and perform synchronized actions, and that the environment program realises the effect of the actions according to their specified pre- and postconditions. So, it is possible that the performance of actions by individual agents do not change the environment state. This is also the case with the performance of any action profile that is not specified by the environment program.Definition 15 (Multi-agent environment program). Let Agt = {1, . . . , k} be a set of agents. A multi-agent environment program (over a finite set of propositional symbols (cid:2) as introduced above) is a tuple (F 0, ( Act1, . . . , Actk), Aspec), where• F 0 ⊆ (cid:2) is the initial state of the environment program,• Acti is the set of actions of agent i ∈ Agt,• Aspec ⊆ { (Pre, (cid:9)α, Post) | (cid:9)α ∈ Act1 × . . . × Actk and Pre, Post ⊆ (cid:2)lhwe assume that each action tuple (cid:9)α can be included in at most one action profile specification.} is a subset of action profile specifications, where We assume that k agents operate in a multi-agent environment such that (cid:9)α is an action profile of the form (α1, . . . , αk), where αi ∈ Acti is the name of the action performed by agent 1 ≤ i ≤ k. It is important to note that pre- and postconditions are not assigned to the actions of individual agents, but to action profiles. This implies that some possible action profiles from Act1 × . . . × Actk may not be specified in Aspec in which case we assume that their executions will not change the state of the multi-agent environment program and loop in those states. Finally, we follow the convention, similar to the programming language Prolog [13], to use “_” as a place-holder for any action in an action profile specification.12 For 12 Thus, _ plays the same role as (cid:9) in the context of CGS. We use Prolog’s notation here to emphasize that we are working in a program setting.118N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142example, we use (Pre, (M, _), Post) to indicate that the performance of action profiles (M, M) and (M, W ) in states that satisfy the precondition Pre results in states that satisfies the postcondition Post.Example 16 (Environment program for the narrowed road). Let Agt = {1, 2}, Acti = {M, W } for i ∈ {1, 2} be the actions that can be performed by the cars in our running example, and propositional symbol p Xi be interpreted as before. The run-}, ( Act1, Act2), {a1, . . . , a5}), where ning example can be implemented by the multi-agent environment program ({ps1, ps2a1 = (a2 = (a3 = (a4 = (a5 = (22{ps{ps{ps{ps{pe1, ps1, ps1, ps1, pe1, ps222},},},},},(W , M),(M, W ),(M, M),(M, _),(_, M),{pm212, ¬ps{pe1, ¬ps{pe2 , ¬ps1 , pm1, ¬ps{pe2, ¬ps{pe}}1, ¬ps}}121})))))Note that aiis used to denote an action profile specification, while (cid:9)αi denotes an action profile. Starting from the initial state of a multi-agent environment program, an execution of the program changes the program state depending on the agents’ actions (the input of the environment program). An arbitrary state of an environment program is specified by a set of atomic propositions F ⊆ (cid:2). The structural operational semantics of the multi-agent environment programs are specified by a set of transition rules, each specifies how the environment program state changes when agents perform actions. Therefore, in the sequel, we use (F , ( Act1, . . . , Actk), Aspec) to denote an environment program in state F . Since the agents’ action repertoire and the action specifications do not change during the execution of the program, we only use the state of the environment program F (instead of (F , ( Act1, . . . , Actk), Aspec)) in the transition rules. We note that not every state F is necessarily reachable from the initial state.Definition 16 (Structural operational semantics). Let (F , ( Act1, . . . , Actk), Aspec) be an environment program in state F ⊆ (cid:2)and (cid:9)α ∈ Act1 × . . . × Actk. For Pre, Post ⊆ (cid:2)lh we write F (cid:27) Pre iff Pre ∩ (cid:2) ⊆ F and (Pre \ (cid:2)) ∩ F = ∅ (the precondition Preis derivable from the facts F iff all positive atoms in Pre are in F and all negative atoms in Pre are not in F ). Further, we define F ⊕ Post = (F \ {p | ¬p ∈ Post}) ∪ {p | p ∈ Post} (updating F with postcondition Post removes negative atoms in Postfrom F and adds positive atoms in Post to F ). The following three transition rules specify possible execution steps of the environment program in state F .(Pre, (cid:9)α, Post) ∈ Aspec and F (cid:27) Pre and F(cid:7) = F ⊕ PostF(cid:9)α−→ F (cid:7)(Pre, (cid:9)α, Post) ∈ Aspec and F (cid:2) Pre(cid:9)α−→ FF: (Pre, (cid:9)α, Post) /∈ Aspec(cid:9)α−→ F∀Pre, Post ⊆ (cid:2)lhFThe first transition rule specifies the execution step of the environment program based on the action profile (cid:9)α when its precondition is satisfied. Such an execution step updates the set of facts F with the postcondition of the action profile (cid:9)α. The second transition rule is the same except that it applies when the precondition of (cid:9)α is not satisfied. Such an execution step does not change the state of the environment program. Finally, the third rule specifies the execution step of the environment program based on an unspecified action profile (cid:9)α. Such an execution step does not change the state of the environment program.In the following, we use Tbasic to denote the set of transition rules from Definition 16, trans(F , Act1, . . . , Actk, Aspec,Tbasic) to denote the set of all transitions that are derivable from transition rules Tbasic based on the environment program (F , ( Act1, . . . , Actk), Aspec), and trans( Act1, . . . , Actk, Aspec, Tbasic) to denote the set of all transitions that are derivable from transition rules Tbasic based on environment programs (F , ( Act1, . . . , Actk), Aspec) for any F ⊆ (cid:2). The latter is the set of all possible transitions based on transition rules Tbasic, actions Act1, . . . , Actk, action specifications Aspec , and all sets of facts F ⊆ (cid:2).An execution of an environment program consists of subsequent execution steps resulting in a sequence of program states. In order to define the set of all possible executions of an environment program, we first define the set of all possible states that can be generated (reached) from an arbitrary state by applying subsequent transitions.Definition 17 (Program states). Let F ⊆ (cid:2) be a set of facts and τ be a set of transitions. The set of states reached from F by subsequent transitions from τ , denoted by gen(τ , F ), is defined as follows:gen(τ , F ) := {F } ∪∞(cid:11)i=1suciτ ({F }) whereN. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142119(X)) . . .) where X ⊆ P((cid:2)) andsuciτ (X) = sucτ (. . . (sucτ(cid:15)(cid:12)(cid:13)(cid:14)i timessucτ (X) = {F 2 | F 1(cid:9)α−→ F 2 ∈ τ and F 1 ∈ X}.Observe that gen(τ , F ) is finite when τ is finite. Given an environment program and the set of transition rules Tbasic, the set of possible executions of the environment program generates a concurrent game structure (as specified in Section 2).Definition 18 (Programs (cid:4) CGS). Let (F 0, ( Act1, . . . , Actk), Aspec) be an environment program, Tbasic be the set of transition rules as defined in Definition 16 and τb = trans( Act1, . . . , Actk, Aspec, Tbasic). The environment program Tbasic-generates a pointed CGS (M, F 0) with M = (Agt, Q, (cid:2), π , Act, d, o) as follows:• Agt = {1, . . . , k}• Q = gen(τb, F 0)• (cid:2) = F 0 ∪ {p | (Pre, (cid:9)α, Post) ∈ Aspec and p ∈ Post}• π (F ) = F• Act = Act1 ∪ . . . ∪ Actk• di(F ) = Acti(cid:7)• o(F , (cid:9)α) = Ffor i ∈ Agt and F ∈ Qfor F , Ffor F ∈ Q(cid:7) ∈ Q , (cid:9)α ∈ d1(F ) × . . . × dk(F ), and F(cid:9)α−→ F(cid:7) ∈ τbThe translation between environment programs and concurrent game structures is restricted to specific classes of con-current game structures as specified in the next definition. In the following, we use also variables q0, q1, . . . to range over the set of states Q .Definition 19 (Finite, distinguished, generated CGS). Let M = (Agt, Q, (cid:2), π , Act, d, o) be a concurrent game structure. We introduce the following notation:• M is called finite if Q is finite,• M is called distinguished if all states differ in their valuations, i.e., for all q, q(cid:7) ∈ Q with q (cid:16)= q(cid:7)we have that π (q) (cid:16)=π (q(cid:7)), and• M is called uniform if for all i ∈ Agt and all q, q(cid:7) ∈ Q we have that d(i, q) = d(i, q(cid:7)), i.e., the agents have the same options in every state.The following proposition formulates the relation between environment programs and their corresponding CGSs.Proposition 9. Let (cid:2) be the set of propositional symbols, (F 0, ( Act1, . . . , Actk), Aspec) be an environment program and (M, F 0) be the pointed CGS that is Tbasic-generated by the environment program. If (cid:2) is finite, then M is finite. Moreover, M is distinguished and uniform.Proof. The sets of atoms in gen(trans( Act1, . . . , Actk, Aspec, Tbasic), F 0), which determine the set of states in Q , are subsets of (cid:2) such there can be only a finite number of them as (cid:2) is finite. Moreover, that M is distinguished follows directly from the fact that all sets in gen(trans( Act1, . . . , Actk, Aspec, Tbasic), F 0) are different. Finally, that M is uniform follows directly from the fact that for each i ∈ Agt it holds: d(i, q) = Acti for all states q ∈ Q , i.e., each agent has one and the same set of options in every state. (cid:2)5.2. Multi-agent environment programs with normsThe environment programs as defined in the previous subsection do not account for norms and norm enforcement. In order to add norms to multi-agent environment programs and to enforce them during program executions, we use (sanctioning and regimenting) norms as introduced in Definition 9. A norm is thus represented as a triple (ϕ, A, S), where ϕ is a propositional formula, A is a set of action profiles, and either S ⊆ (cid:2)s (sanctioning norm) or S = ⊥ (regimenting norm). In the rest of this paper, we just use the term norm when the distinction between sanctioning and regimenting norms is not relevant. Like before, the pre- and postconditions of action profiles are assumed to consist of hard facts only such that adding norms does not change the activation and effect of action profiles. As explained before, throughout this section we assume that (cid:2) = (cid:2)h ∪ (cid:2)s is a finite set of propositional symbols.Definition 20 (Norm-based multi-agent environment program). Let Agt = {1, . . . , k} be a set of agents. A norm-based multi-agent environment program is a tuple (F 0, ( Act1, . . . Actk), Aspec, N), where120N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142• F 0 ⊆ (cid:2),• Acti and Aspec are as introduced in Definition 15, and• N is a set of (sanctioning and regimenting) norms as introduced in Definition 9.Following the narrowed road environment program as specified in Example 16, the behaviour of cars can be regulated by adding norms to the environment program.Example 17 (Norm-based environment program for the narrowed road). Let ({psment program as specified in Example 16, and N = {(ps1norms as explained in Example 10. The norm-based environment program ({psments the narrowed road example where norms N are enforced.2, {(W , W )}, {fine1}), (ps1, ps1, ps∧ ps22}, ( Act1, Act2), {a1, . . . , a5}) be an environ-2, {(_, M)}, {fine2})} be a set of 1}, ( Act1, Act2), {a1, . . . , a5}, N) imple-∧ psThe executions of norm-based multi-agent environment programs are similar to the executions of multi-agent envi-ronment programs (without norms) in the sense that both update the program states with the effects of action profiles. However, the execution steps of a norm-based multi-agent environment program proceed by enforcing norms to the result-ing program states. The enforcement of norms on a program state consists of updating the state with the consequences of applicable norms. So, in order to define the execution steps of norm-based multi-agent environment programs, we need to determine the norms that are applicable in a program state and their consequences. For this purpose, we use the function SanN( X, (cid:9)α), as defined in Definition 10, to determine the sanction set that should be imposed when the agents perform ac-tion profile (cid:9)α in state X . Note that the sanction set may be {⊥}, which means that a regimenting norm should be enforced. In this case, we have to ensure that the performance of (cid:9)α does not have any effect on state X .We distinguish two general cases depending on whether the sanction set is {⊥} or not. When applicable norms are sanctioning norms (i.e., resulting in a set of soft facts), we update the resulting program state with the sanctions. We define the update of a program state with sanctions as being non-cumulative in the sense that previous sanctions are removed before new sanctions are added. We use the update function ⊗ to update a state with sanctions. Note that in Definition 21 ⊗removes all sanctions (soft facts) before adding new ones. This ensures that sanctions become non-cumulative in transitions. This update function should not be confused with ⊕, which is used to update program states with the postcondition of action profiles.The effect of an action profile on an environment program state in the context of some existing norms is specified by distinguishing four cases: 1) the action specification is given and its precondition is satisfied, 2) the action specification is given but its precondition is not satisfied, 3) the action specification is not given, and 4) the action profile is regimented by some norms. In the first case, we update the program state with the postcondition of the action profile and then with possible sanctions, while in the second and third case the program state is updated only with possible sanctions. These two cases indicate that the performance of an action profile in a program state will be sanctioned (if there is a sanctioning norms applicable) even when it has a specification but its precondition is not satisfied or when the action profile has no specification. These two cases capture the intuition that any unsuccessful attempt to violate norms will be sanctioned as well. Note that the sanction set in the first three cases can be an empty set if no norm is applicable. Finally, in the fourth case, when an action is regimented by some norms, the action will have no effect on the environment state.Definition 21 (Structural operational semantics). Let (F , ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program in an arbitrary state F ⊆ (cid:2), (cid:9)α ∈ Act1 × . . . × Actk, and N be a set of norms as defined in Definition 9. Let also ⊕ and (cid:27) be defined as in Definition 16. Finally, let F ⊗ S = (F \ (cid:2)s) ∪ S for S ⊆ (cid:2)s. The following four transition rules specify possible execution steps of the norm-based environment program.(Pre, (cid:9)α, Post) ∈ Aspec and F (cid:27) Pre and F(cid:7) = (F ⊕ Post) ⊗ SanN(F , (cid:9)α) and SanN(F , (cid:9)α) (cid:16)= {⊥}F(Pre, (cid:9)α, Post) ∈ Aspec and F (cid:2) Pre and F(cid:9)α−→ F (cid:7)(cid:7) = F ⊗ SanN(F , (cid:9)α) and SanN(F , (cid:9)α) (cid:16)= {⊥}(cid:9)α−→ F (cid:7)F∀Pre, Post ⊆ (cid:2)lh: (Pre, (cid:9)α, Post) /∈ Aspec and F(cid:7) = F ⊗ SanN(F , (cid:9)α) and SanN(F , (cid:9)α) (cid:16)= {⊥}F(cid:9)α−→ F (cid:7)SanN(F , (cid:9)α) = {⊥}(cid:9)α−→ FFThe first transition rule applies when action profile (cid:9)α is performed, its precondition holds, and no regimenting norms are triggered. The second transition rule is the same except that the precondition of (cid:9)α is not satisfied. In this case applicable sanctioning norms are enforced without realising the effect (i.e., postcondition) of (cid:9)α. The third transition rule applies when an unspecified action profile (cid:9)α is performed and no regimenting norms are triggered. In this case, applicable sanctioning norms are enforced. Note that the environment is assumed to be exogenous to agents in the sense that agents decide on N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142121which actions to perform independently of the environment specification. This allows agents to decide and perform actions for which there is no environment specification. It is also important to note that a norm specifies that the performance of an action profile in a state should be sanctioned, regardless of the specification of the action profile. Thus, agents can perform an unspecified action profile for which a sanction may occur. The first three transition rules ensure that sanctioning norms are enforced regardless of the specification of action profiles. These transition rules capture the idea that any (successful or unsuccessful) attempt to violate norms is sanctioned. Note also that in the first three transition rules no sanctioning norm needs to be applicable in state X , i.e., SanN( X, (cid:9)α) can be an empty set indicating that action profile α does not violate any norm in state X . Finally, the fourth transition rule applies when the performance of an action profile triggers regimenting norms, again regardless of the specification of action profiles. It is important to notice that sanctions do not accumulate through consecutive states. The following proposition shows that the sanctions imposed on a state (propositional symbols from (cid:2)s) are only those caused by the action performed in the previous state.In the following, we use Tnorm for the set of transition rules from Definition 21, trans(F , Act1, . . . , Actk, Aspec, N, Tnorm)to denote the set of all transitions that are derivable from transition rules Tnorm based on the norm-based environment program (F , ( Act1, . . . , Actk), Aspec, N), and trans( Act1, . . . , Actk, Aspec, N, Tnorm) to denote the set of all transitions that are derivable from transition rules Tnorm based on norm-based environment programs (F , ( Act1, . . . , Actk), Aspec, N) for all F ⊆ (cid:2).Proposition 10. Let (F , ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program in an arbitrary program state F , F(cid:7) ∈ trans( Act1, . . . , Actk, Aspec, N, Tnorm) and S = SanN(F , (cid:9)α). Then, we haveF(cid:9)α−→(cid:5)(cid:7) ∩ (cid:2)s =FSF ∩ (cid:2)sif S (cid:16)= {⊥}otherwise.Proof. Directly follows from the definition of ⊗ and the transition rules in Definition 21. (cid:2)For a given norm-based environment program, the set of transition rules Tnorm generates a concurrent game structure. We use Definition 18 to define the concurrent game structure, which is said to be generated by the norm-based environment program. Note that Definition 18 can be applied as norm-based environment programs have the required ingredients such as an initial state F 0, a sets of actions for each agent, and a set of action profile specifications. There is, however, one difference between environment programs and norm-based environment programs which requires a slight modification of Definition 18. The difference is that we now assume the set of transitions is trans( Act1, . . . , Actk, Aspec, N, Tnorm), instead of trans( Act1, . . . , Actk, Aspec, Tbasic). This means that we use gen(trans( Act1, . . . , Actk, Aspec, N, Tnorm), F 0) to generate the set of reachable states, and Fis derivable based on Tnorm. The formal definitions are as before.(cid:7) ∈ trans( Act1, . . . , Actk, Aspec, N, Tnorm) to indicate that the transition F(cid:9)α−→ F(cid:9)α−→ F(cid:7)Definition 22 (Norm-based programs (cid:4) CGS). Let (F 0, ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program and τn = trans( Act1, . . . , Actk, Aspec, N, Tnorm). The program is said to Tnorm-generate a pointed CGS (M, F 0), where M = (Agt, Q, (cid:2), π , Act, d, o) is defined as follows:• Agt, (cid:2), π , Act, d are specified as in Definition 18,• Q = gen(τn, F 0), and• o(F , (cid:9)α) = Ffor F , F(cid:7) ∈ Q, (cid:9)α ∈ d1(F ) × . . . × dk(F ) and F(cid:7)(cid:9)α−→ F(cid:7) ∈ τn.The class of concurrent game structures generated by norm-based environment programs is characterized by the follow-ing proposition.Proposition 11. Let (cid:2) be the set of propositional symbols, (F 0, ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program, and (M, F 0) be the pointed CGS that is Tnorm-generated by it. If (cid:2) is finite, then M is finite. Moreover, M is distinguished and uniform.Proof. Similar to the proof of Proposition 9. Observe that the elements of Q = gen(trans( Act1, . . . , Actk, Aspec, N, Tnorm), F 0)are subsets of (cid:2) and there are only finitely many of them when (cid:2) is finite. Observe also that M is distinguished as the elements of Q are distinct and that M is uniform as di(q) = Acti for all i ∈ Agt and q ∈ Q . (cid:2)The generated CGS does not accumulate sanctions through consecutive states.Proposition 12. Let (F 0, ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program that Tnorm-generates (M, F 0), where M = (Agt, Q, (cid:2), π , Act, d, o). For F ∈ Q , (cid:9)α ∈ Act1 × . . . × Actk and S = SanN(F , (cid:9)α), we have122N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142π (o(F , (cid:9)α)) ∩ (cid:2)s =(cid:5)Sπ (F ) ∩ (cid:2)sif S (cid:16)= {⊥}otherwise.Proof. From Definition 22 we have o(F , (cid:9)α) = Fknow that sanctions are not accumulated through transitions. (cid:2)iff F(cid:7)(cid:9)α−→ F(cid:7) ∈ trans( Act1, . . . , Actk, Aspec, N, Tnorm) and Proposition 10 we Note the correspondence between Proposition 12 and Proposition 1 (on page 109). The execution of a norm-based environment program may generate a different set of states than the execution of the corresponding environment program without norms does. This is due to the fact that the performance of unspecified action profiles or the performance of specified action profiles for which the precondition does not hold can now be sanctioned resulting in new states. Note that the performance of such an action profile in an environment program without norms results in the same state. Also, the application of regimenting norms may prevents reaching new states. These observations are formulated in the following proposition.Proposition 13. Let (F 0, ( Act1, . . . , Actk), Aspec) be an environment program, (F 0, ( Act1, . . . , Actk), Aspec, RN ∪ SN) be the en-vironment program with regimenting norms RN and sanctioning norms SN. Let also τb = trans( Act1, . . . , Actk, Aspec, Tbasic) and τn = trans( Act1, . . . , Actk, Aspec, RN ∪ SN, Tnorm). Then, we have:• When the sets of regimentation and sanctioning norms are empty, i.e., RN ∪ SN = ∅, we have gen(τn, F 0) = gen(τb, F 0).• When there are only regimentation norms and no sanctioning norms, i.e., RN (cid:16)= ∅ and SN = ∅, we have gen(τn, F 0) ⊆ gen(τb, F 0).• When there are only sanctioning norms and no regimentation norms, i.e., RN = ∅ and SN (cid:16)= ∅, we have |gen(τn, F 0)| ≥|gen(τb, F 0))|.Proof.• Since RN ∪ SN = ∅, we have for all F ⊆ (cid:2) and all (cid:9)α ∈ Act1 × . . . × Actk : SanN(F , (cid:9)α) = ∅. This makes the transition rules Tbasic and Tnorm identical.(cid:3)li=1 suciτn(cid:7)• Let RN (cid:16)= ∅ and SN = ∅. Following Definition 17, we have that gen(τ , F ) := {F } ∪(cid:9)α−→ F ∈ τn. If the latter transition is obtained by a regimentation of (cid:9)α in FF ∈ {F 0} ∪({F 0}) then also F ∈ {F 0} ∪(cid:3)Suppose the claim holds for l > 1. Let F ∈ {F 0} ∪with Fhypothesis we obtain that F ∈ {F 0} ∪F ∈ τb and thus F ∈ sucτb ({Fl+1F ∈ {F 0} ∪i=1 suciτb(cid:9)α−→ F ∈ τb for some (cid:9)α ∈ Act1 × . . . × Actk, and SanSN(F , (cid:9)α) (cid:16)= ∅ (note that ⊥ /∈ SanSN(F , (cid:9)α)). Then, we have • Let F ⊆ (cid:2), Fτ ({F }). We show that if i=1 suci(cid:3)({F 0}) by induction on l. The base case, l = 1, is trivial. li=1 suciτbl+1({F 0})({F 0}). Then, there must be an Fi=1 suciτnand by induction (cid:9)α−→({F 0}), we also obtain that ({F 0}). If the action is not regimented, then we also have that F(cid:3)li=1 suciτb(cid:7)}). Then, as by induction hypothesis F({F 0}). The claim follows.(cid:3)li=1 suciτnl+1i=1 suciτbthen F = F(cid:7) ∈ {F 0} ∪(cid:7) ∈ {F 0} ∪(cid:3)(cid:3)(cid:7)(cid:7)(cid:7)(cid:9)α−→ F ⊕ SanSN(F , (cid:9)α) ∈ τn. This implies that F ⊕ SanSN(F , (cid:9)α) ∈ gen(τn, F 0) but F ⊕ SanSN(F , (cid:9)α) /∈ gen(τb, F 0). (cid:2)F(cid:3)∞5.3. Relation between norm-based update and norm-based environmentWe now investigate the relation between the concurrent game structure Mn that is generated by a norm-based environ-ment program, and the concurrent game structure M(cid:7)that is generated by the corresponding environment program without norms (i.e., the same initial state, actions, and action specifications) which is then updated with the same norms. We first show that if states qn and q, respectively, have the same valuation, then the states reached from qn and (cid:7)qby the same action profile have the same valuation as well. This is formulated by the following lemma.from Mn and M(cid:7)(cid:7)Lemma 14. Let (F 0, ( Act1, . . . , Actk), Aspec) be an environment program that Tbasic-generates the concurrent game structure (M, F 0) where M = (Agt, Q, (cid:2), π , Act, d, o). Let (F 0, ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program that Tnorm-generates the concurrent game structure (Mn, F 0) where Mn = (Agt, Qn, (cid:2), π n, Act, dn, on). Let M(cid:7) = M (cid:2) N, where M(cid:7) = (Agt, Q(cid:7)) then π n(on(F n, (cid:9)α)) = π (cid:7)(oand (cid:9)α ∈ Act1 × . . . × Actk, we have that if π n(F n) = π (cid:7)(F(cid:7)). For any F n ∈ Qn, F(cid:7), (cid:2), π (cid:7), Act, d(cid:7), o(cid:7), (cid:9)α)).(cid:7) ∈ Q(cid:7)(F(cid:7)Proof. First, we note that by Proposition 11 we have that M and Mn are uniform. Therefore, by the definition of norm update given in Definition 11 model M(cid:7)is uniform as well. As a consequence, in all states of all three models all action (cid:7), F n ⊆ (cid:2), or F n ⊆ (cid:2)tuples from Act1 × . . . × Actk are available. Let F n ∈ Qn, Fand F(cid:7)) = π (cid:7)(F ) ∪ S = F ∪ S. Therefore, we define:(cid:7) = (F , S) with F ⊆ (cid:2)h, S ⊆ (cid:2)s and π (cid:7)(Fwith π n(F n) = π (cid:7)(F(cid:7)). We note that F(cid:7) ∈ Q(cid:7)N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142123Fig. 13. Diagram showing the relation between a norm update of a generated CGS and a norm generated CGS.(cid:5)G =FF(cid:7)if F(cid:7) = (F , S),otherwise, i.e. F(cid:7) ⊆ (cid:2)hFurthermore, we note that for all (cid:9)α ∈ Act1 × . . . × Actk we have that: S(cid:7)), (cid:9)α) and that the same action profiles are enabled. We show the claim by considering the three cases how a transition can ∗ := SanN(π n(F n), (cid:9)α) = SanN(π (cid:7)(F(cid:7)in F n and Foccur according to Definition 11.Case S∗ = ∅.In that case we have that o(cid:7), (cid:9)α) ∈ τb. As the precondition of (cid:9)α is independent of soft facts and the first three transition rules of Definitions 16 and 21are identical for S(cid:7), (cid:9)α) ⊆ (cid:2)h which is generated by G ∗ = ∅, we obtain that on(F n, (cid:9)α) = o(cid:7), (cid:9)α) = o(G, (cid:9)α), where o(cid:7)(F(cid:7)(F(cid:7)(F(cid:7)(F(cid:9)α−→ oCase S∗ = {⊥}.In this case F n(cid:9)α−→ F n ∈ τn and on(F n, (cid:9)α) = F n as well as o. The claim follows because π (cid:7)(F(cid:7)) =(cid:7), (cid:9)α). The claim follows.(cid:7)(F(cid:7), (cid:9)α) = F(cid:7)π n(F n).Case {⊥} (cid:16)= S∗ (cid:16)= ∅. We further divide this case according to the first three transition rules of Definitions 16 and 21.• First suppose that (Pre, (cid:9)α, Post) ∈ Aspec and F n (cid:27) Pre (and thus also G (cid:27) Pre). We have that on(F n, (cid:9)α) = (F n ⊕∗). We obtain that: π n(on(F n, (cid:9)α)) = π n((F n ⊕ Post) ⊗∗ =∗ = π n((G ⊕ Post))\(cid:2)s) ∪ S(cid:7), (cid:9)α) = (G ⊕ Post, S∗) = π n(((F n ⊕ Post)\(cid:2)s)) ∪ S. Analogously, we have that o∗ = π n(G ⊕ Post) ∪ S(cid:7)(F∗Post) ⊗ S∗) = π n(((F n ⊕ Post)\(cid:2)s) ∪ SSπ (cid:7)((G ⊕ Post, S∗)) = π (cid:7)(o(cid:7)(F(cid:7), (cid:9)α)).• The second case, i.e. F n (cid:2) Pre, follows analogously.• Also the final case, i.e. (Pre, (cid:9)α, Post) /∈ Aspec for any Pre, Post ⊆ (cid:2)h, follows analogously. (cid:2)As the final result of this section, we show that the concurrent game structures generated by an environment program with and without norms are closely related. In particular, we show that the concurrent game structure generated by a norm-based environment program is isomorphic to the reachable part of the concurrent game structure, which is generated by the corresponding environment program without norm, and updated with the same set of norms. We formally introduce the notions of reachable states and isomorphism before showing the correspondence result.Definition 23 (Reachable states, reachable CGS). Let M = (Agt, Q, (cid:2), π , Act, d, o) be a CGS and q0 ∈ Q . A state q ∈ Q is said to be reachable from q0 if there is a path starting in q0 which also contains q. The set of all states reachable from q0 in M(cid:7))(cid:7), ois denoted as Reachable(M, q0). The reachable part of M from q0, denoted as Mq0 , is the CGS (Agt, Qwhere Q(cid:7) = Reachable(M, q0), π (cid:7)(q) = π (q), d(cid:7), (cid:2), π (cid:7), Act, dand i ∈ Agt.(cid:7)(q, (cid:9)α) = o(q, (cid:9)α) for all (cid:9)α ∈ d(cid:7)i(q) = di(q), and o(cid:7)Agt(q), q ∈ Q(cid:7)A model updated by sanctioning norms can yield states of type (q, S). States in norm generated CGSs, on the other hand, have no internal structure; they are plain sets of propositional symbols. We are less interested in such purely syntactic differences and need a way to compare models from a semantic perspective. Therefore, we say that two models M1 and M2 are isomorphic if they are identical besides the names of the states. The next definition captures this formally.Definition 24 (Isomorphic models). Let Mi = (Agti, Qi, (cid:2)i, πi, Acti, di, oi) for i ∈ {1, 2} be two CGSs. M1 and M2 are isomor-= Agt2, (cid:2)1 = (cid:2)2, Act1 = Act2 and there is a bijection phic, written as M1f : Q1 → Q2 such that:∼= M2, if the following conditions hold: Agt11. π1(q) = π2( f (q)) for all q ∈ Q1.2. d1(q) = d2( f (q)) for all q ∈ Q1.3.f (o1(q, (cid:9)α)) = o2( f (q), (cid:9)α) for all q ∈ Q1 and (cid:9)α ∈ d1(q).Note that we focus on the part of the generated CGS that is reachable from the initial state since the application of regimenting norms in the operational semantics of norm-based environment programs blocks transitions and thus causes some states to become unreachable. Thus, having an environment program (F 0, ( Act1, . . . , Actk), Aspec) and a set of norms N, we can now show that the concurrent game structure Mn generated by (F 0, ( Act1, . . . , Actk), Aspec, N) is isomorphic to (M (cid:2) N)F 0 , which is the part of the concurrent game structure generated by (F 0, ( Act1, . . . , Actk), Aspec) (i.e., environment program without norms) and updated with N, and is reachable from F 0. This relation is illustrated in the diagram shown in Fig. 13 and formulated in the following theorem.124N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142Theorem 15. Let (F 0, ( Act1, . . . , Actk), Aspec) be an environment program that Tbasic-generates the pointed concurrent game struc-ture (M, F 0) with M = (Agt, Q, (cid:2), π , Act, d, o), and (F 0, ( Act1, . . . , Actk), Aspec, N) be a norm-based environment program that Tnorm-generates the pointed concurrent game structure (Mn, F 0) with Mn = (Agt, Qn, (cid:2), π n, Act, dn, on). We define M(cid:7) = M (cid:2) Nwith M(cid:7) = (Agt, Q(cid:7)). Then, we have that Mn ∼= (M (cid:2) N)F 0 .(cid:7), (cid:2), π (cid:7), Act, d(cid:7), o(cid:7)(cid:7)(cid:7)by f (F n) = Fif and only if π n(F n) = π (cid:7)(FProof. We define a (partial) function f : Qn → Qis well defined we have that π (cid:7)(F 1) (cid:16)= π (cid:7)(F 2) whenever F 1 (cid:16)= F 2. This is because a norm follows from the fact that for all F 1, F 2 ∈ Qupdate is only performed once, causing states to be of the form F ⊆ (cid:2)h or (F , S) with F ⊆ (cid:2)h and ∅ (cid:16)= S ⊆ (cid:2)s. We show that f constitutes an isomorphism according to Definition 24 between Mn and (M (cid:2) N)F 0 . Therefore, we have to show that is a bijection (and total) and that the three conditions of Definition 24 are satisfied. Condition 1 is true by the definition ff . Condition 2 is true because both models are uniform by Proposition 9 and Definition 11, and by Proposition 11, of(cid:7)( f (q), (cid:9)α) for all q ∈ Qn and (cid:9)α ∈ dn(q), respectively. Thus, only the last of these three conditions, i.e. that f (on(q, (cid:9)α)) = oand Q n consist of all states which are reachable by some remains to be shown. By definition, both sets of states Qsequence of actions from F 0. Therefore, we show that the claim holds by induction on the length of an action sequence.(cid:7)). That f(cid:7)Base case. We have that F 0 ∈ Q n ∩ Q(cid:7)sequence. By Lemma 14 and as f (F 0) = F 0 we obtain that π (cid:7)(oimplies that f (on(F 0, (cid:9)α)) = o(cid:7)(F 0, (cid:9)α) and thus also f (on(F 0, (cid:9)α)) = o, thus, f (F 0) = F 0 is well defined. Moreover, F 0 is reached by the empty action (cid:7)(F 0, (cid:9)α)) = π n(on(F 0, (cid:9)α)) for any (cid:9)α ∈ Act1 × . . . × Actk. This (cid:7)( f (F 0), (cid:9)α) as desired.Induction step. We assume that the claim holds for all action sequences of length i. Thus, let F n ∈ Q n and Fbe two states reached after the same action sequence of length i. By induction, f (F n) = Fand Condition 3 is satisfied for (cid:7), (cid:9)α)) =these states. Let (cid:9)α ∈ Act1 × . . . × Actk be an arbitrary action profile. Again, by Lemma 14 we obtain that π (cid:7)(o(cid:7), (cid:9)α) is well defined. To establish Condition 3 we consider another arbitrary π n(on(F n, (cid:9)α)). Consequently, f (on(F n, (cid:9)α)) = oaction profile (cid:9)β ∈ Act1 × . . . × Actk. Because both states are in relation by f we can once more apply Lemma 14 and obtain (cid:7)( f (on(F n, (cid:9)α)), (cid:9)β). (cid:7), (cid:9)α), (cid:9)β)) which implies that f (on(on(F n, (cid:9)α), (cid:9)β)) = oπ n(on(on(F n, (cid:9)α), (cid:9)β)) = π (cid:7)(oWhich shows that Condition 3 is satisfied.(cid:7), (cid:9)α), (cid:9)β) = o(cid:7)(F(cid:7)(F(cid:7)(F(cid:7)(F(cid:7)(o(cid:7)(o(cid:7)(cid:7) ∈ Q(cid:7)In each inductive step we also showed that fis well defined and total. This implies that fis indeed a bijection as we considered arbitrary action sequences and thus made sure that each state from Q(cid:7)is reached. (cid:2)This theorem shows that the operational semantics of the proposed executable specification languages for environments with and without norms are aligned with the semantics of norms and norm update, as presented in the first part of the paper. This result allows us to apply the proposed abstract mechanism design methodology to analyse the enforcement effect of norms in executable environment programs.Concluding remarksIn this section, an executable specification language for the implementation of multi-agent environments was presented. The language includes constructs to specify the initial state of multi-agent environments as well as the specification of action profiles in terms of pre- and postconditions. The operational semantics of the proposed specification language was presented and its relation with concurrent game structures was established. An execution of an environment specification initializes a multi-agent environment, which is subsequently modified by the performance of the agents’ actions and according to the action specifications. Subsequently, the specification language was extended with norms and sanctions, its operational semantics was presented, and its relation with concurrent game structures that are updated with norms was established. The operational semantics ensures that the norms are enforced by means of regimentation or sanctions. An execution of a norm-based environment specification initializes a multi-agent environment and effectuates agents’ actions in the environment based on the action specifications, the norms, and their corresponding sanctions. The agents are assumed to be aware of the norms and their enforcement. They are also assumed to autonomously decide whether to comply with the norms, or to violate them and accept the consequences. The presented executable specification language with norms can be used in various application domains such as traffic or business process management, where the behaviour of autonomous agents should be externally controlled and coordinated to be aligned with some laws, rules, policies, or requirements. The next section reports on an application of the extended specification language in traffic simulation, where traffic laws (norms) are enforced to reduce traffic congestion in a ramp merging scenario. We argue that although the executable specification language is useful for the implementation of traffic simulations, there are specific issues that should be resolved before we can apply the proposed norm-based mechanism design methodology to such applications.6. Applying norm-based specification language in traffic simulationA characteristic feature of the proposed specification language for norm-based multi-agent environments is the modu-larity of norms in the sense that norms can be programmed as a separate module isolated from the specification of actions and (initial) states of multi-agent environments. This feature allows us to implement different sets of norms and to compare their enforcement effects in one and the same multi-agent environment. The use of the proposed specification language is already illustrated by the running example that is presented in previous sections (see Example 17). In this section we present a more complex and realistic application of the proposed specification language.N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142125Fig. 14. Ramp merging traffic scenario.This application concerns the development of norm-based traffic environments for SUMO (Simulation of Urban MObil-ity) [12]. SUMO [46] is a traffic simulation platform that supports the modelling of traffic, including cars, public transport and pedestrians. In this application, SUMO is extended to simulate traffic scenarios, where norms and traffic laws are ex-plicitly specified as input and enforced during the simulation runs. In particular, SUMO is extended with a norm-based traffic controller module that monitors the simulated traffic by continuously extracting relevant traffic information such as the position and speed of the simulated cars from the SUMO platform, instantiates the given input set of traffic norms to generate traffic directives for the observed cars, communicates traffic directives to the cars, and imposes payment sanctions on the cars that violate their traffic directives. In addition to the traffic controller module, the standard SUMO car model that is responsible for the actual behaviour of individual cars, is extended to allow individual cars to incorporate norms in their driving behaviour.The SUMO extension [12] is used to simulate a ramp merging traffic scenario. A schematic representation of the ramp merging scenario is illustrated in Fig. 14. In this figure, triangles represent cars that drive from left to right and rectangles s1to s5 are sensors that observe the position and speed of cars at various points of the roads. There are two important points on the road: m is the point where the roads merge and e is the ending point of the traffic scenario. In order to manage traffic at the merging point m, observed cars at sensor s1, s2 and s3 receive traffic directives from the traffic controller. The directive for a car is generated based on the given set of traffic norms and consists of a velocity and a fine that will be imposed if the directive is not followed by the car. In this figure, white cars have not received their directives from the traffic controller, while grey cars have received their directives.An example of a traffic norm used in this scenario is (x ∧ y, A(v x,v y ), {finez}) to be read as “cars x and y, observed simultaneously by sensor s2 and s3, are prohibited to have velocities other than respectively v x and v y at the merging point m to avoid a fine of z Euro”. For this traffic norm, the set of prohibited velocities A(v x,v y ) = {(v 1, v 2) | v 1 (cid:16)= v x and v 2 (cid:16)=v y}13 represents the obligation that cars x and y should have velocities v x and v y , respectively, at the merging point m. The traffic controller instantiates the input norm by determining x, y, v x, v y , and finez based on the observed cars and the properties of the current traffic state such as traffic density on the roads. The velocities of the observed cars are determined in such a way that cars arrives at m with a safe distance dsafe from each other given the current traffic density. The fine of z Euro will be imposed on car x (respectively y) if its velocity v x (respectively v y ) at m is not realised. Based on the instantiated norm, the traffic controller sends to each observed cars a corresponding directive. For example, the traffic controller sends to the observed car x the directive (v x, finez), which should be read as “car x should have velocity v x at the merge point m to avoid the fine of z Euro”. For this simulation scenario, the traffic controller generates also directives for the cars that are observed by one of the two sensors s2 or s3, even if there is no car simultaneously observed at the other sensor.In this traffic simulation, a car decides which action (e.g., which velocity on which lane) to perform and whether to follow or ignore the received directives based on its information and preferences. In particular, the car model is designed by means of an action selection function that selects an action that maximizes the car’s utility. The action selection function is defined in terms of the car’s internal state (including its current velocity, position, and the received directive), an expected arrival time function that, given the current traffic situation, determines the impact of a velocity action on the arrival time at the car’s destination (for simplicity it is assumed that all cars have the same destination), an action effect function (that determines the expected consequence of a velocity action on its internal state), a sanction grading function (mapping fines to real values reflecting the utility of a fine), and an arrival grading function (mapping an arrival time to real values reflecting the utility of the given arrival time at the car’s destination). The internal state of the car together with the expected arrival time function and the action effect function constitute an agent’s information about themselves and the traffic situation 13 We assume that v 1 and v 2 are taken from some sensible finite set of velocity values.126N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142including other cars. The sanction grading function and the arrival grading function constitute the preference of the cars. The further details of the car model can be found in [12].The objective of this simulation was to investigate the impact of the enforcement of various norm sets on traffic situation in the ramp merging scenario for a different population of cars. Two types of cars were distinguished: leisure and business cars. These two types were implemented by the sanction and arrival time grading functions. The sanction grading function for a leisure car evaluates a sanction as having a higher impact on the car’s utility compared to the same function for a business car. Conversely, the arrival time grading function for a leisure car evaluates a late arrival time as having less impact on the car’s utility compared to the same function for a business car. The simulation results show that the number of norm violations, and therefore traffic congestion, decreases as the severity of fines increases. In particular, they show that norm violations do not occur when proper fines (i.e., fines that match the cars’ preferences) are imposed.The formal connection between the proposed abstract mechanism design methodology and the specification language, as established in Proposition 14, may suggest that the results of the mechanism design analysis of norms can be related to the results of the implementation of norms in multi-agent simulations. Such a relation can be used to justify the simulation results by means of mechanism design explanations, or vice versa, to verify the results of the mechanism design analysis by means of simulations. For example, if a mechanism design analysis of the ramp merging scenario shows that a set of traffic norms implements the system designer’s objective to avoid traffic congestion (i.e., to avoid simultaneous arrival of cars at the merging point m) assuming that cars follow their Nash equilibrium strategies, then one can use the specification language to simulate the scenario in order to verify whether the enforcement of the traffic norms avoids traffic congestion.However, connecting theoretical results obtained by a mechanism design analyses and by experimental results obtained by running agent-based simulations is not straightforward and requires further considerations. For example, one could argue that the reported experimental results from the traffic simulation in SUMO can be used to claim that an observed reduction of traffic congestion is due to the optimality of norms in the sense that the norms implement the objective of avoiding traffic congestion in Nash equilibrium. However, such claims could only be justified if the simulated cars were capable of strategic reasoning, which is not the case in the reported traffic simulation experiment. The ability of strategic reasoning for cars is not supported by our extension of the SUMO platform and requires further extensions. This is due to the fact that the preferences of the cars are not accessible to each other such that the cars do not share the structure of the game and are thus unable to reason strategically. This implies that the simulations setting in SUMO is not yet rich enough to establish a connection to our mechanism design setting.It should also be noted that our formal mechanism design methodology can now be applied to analyse only one snapshot of the traffic simulation, i.e., to analyse the behaviour of cars that arrive simultaneously at the sensor positions. Such a snapshot of the ramp merging scenario constitutes a game setting that is quite comparable with the setting of our running example (the narrow part of the road in the running example is the merging point of the ramp merging scenario). In order to connect the mechanism design methodology and the experimental results of the simulations, which consists of a continuous stream of cars, one could model the simulation as a sequence of games. Although this may be a reasonable suggestion, one needs to investigate how to model and analyse the change and development of the traffic state in consecutive game settings. For example, a high stream of cars may necessarily cause the creation of traffic congestion at the merging point, which changes the state of the traffic and therefore the structure of the consecutive games. We believe that a profound connection between mechanism design and simulation settings is a challenging future research direction.7. Related workThere have been many proposals on abstract and executable models for norms and norm enforcement in multi-agent systems. However, unlike our work, these proposals focus either on abstract models of norm enforcement or on executable models, ignoring the connection between them. Despite this key difference, our abstract and executable model differ from existing abstract and executable models, respectively. In the following, we first compare our abstract model for norm and norm enforcement to existing abstract models, and then compare our executable specification language with existing exe-cutable models for norms and norm enforcement.Our abstract model of norms and norm enforcement is closely related to [1] and [65], though they consider a norm from a semantic perspective as a set of transitions instead of a syntactic expression as in our case. In particular, they use labelled Kripke structures as multi-agent models, supposing that each agent controls a specific number of transitions. The enforcement of a norm is then considered as the deactivation of specific transitions. In our proposal, we distinguish between regimenting and sanctioning norms, which is also one of the main conceptual differences besides the mechanism design perspective we follow. The enforcement of the regimenting norms is similar to the deactivation of transitions as in [1,65], but the enforcement of sanctioning norms may create new states resulting from a relabelling by soft facts and thereby new transitions. In this sense, our approach is different as the enforcement of norms may change the underlying multi-agent model with new states and transitions. From a conceptual point of view the agent can still perform the action, the physical structure encoded by means of hard facts remains intact, but may have to bear the imposed sanctions depending on the agents’ preferences. Note also, the change in the underlying transition system may only affect some agents, namely those which use relevant soft facts in their preferences. Another difference to our work is that the outcome of norm enforcement is assumed to be independent of the preferences where we consider a more general setting captured in terms of normative behaviour functions. It is also important to recall that our focus is of a more practical nature. We try to implement and to N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142127analyse mechanisms from a practical point of view, i.e., how to implement and verify norm-based multi-agent environment by means of executable specifications. The work presented in [3] also assumes that the designer has multiple objectives the value of which is determined by the (de)activated transitions in the transition system. The authors show how to compactly represent the designer’s objectives by (a set of) CTL formulae each assigned a feature value. Then, the utility of a social law is the sum of all feature values of the satisfied CTL formulae minus the costs to implement the social law. The authors give an algorithm to compute optimal social laws as well as complexity results. The focus of the work is on computing a good social law from the designer’s perspective, not considering agents’ objectives at first place (which is a key concern in our setting).Fitoussi and Tennenholtz [34] put forward a formal framework to engineer and study social laws in multi-agent systems. A social law restricts the set of agents’ actions. The authors investigate two properties of social laws: minimality and simplicity. Minimality refers to the number of restrictions imposed on the system, where simplicity refers to the capabilities of agents. Simpler laws can be followed more easily by simpler agents. The authors consider complexity issues about the existence of appropriate norms and study their properties. The setting is quite abstract, using strategies similar to those known from normal form games, whereas we start from a transition system usually requiring multiple step strategies. Also, the authors consider two specific types of goals, liveness and safety goals, whereas we allow arbitrary LTL-formulae to specify agents’ preferences.Endriss et al. [30] and Wooldridge et al. [69] propose taxation schemes to incentivize agents to change their behavioursuch that the emerging behaviour is stable and shows desirable properties in line with the system specification. In particular, the computational complexity of the weak and strong implementation problems is investigated. We have drawn inspiration from these problems in the present article. A difference with our work is that they study these problems in the context of Boolean games, where we consider a strategic setting in which agents act in a temporal setting. Instead of taxation schemes we follow a norm-based approach, and use techniques of mechanisms design to specify the system designer’s objectives and analyse their implementability. We believe that this approach is quite flexible and allows to model more realistic settings in which the system designer may not know the agents’ preferences and should therefore consider a set of possible preferences.A different avenue of work in this area focuses on norm monitoring in the context of imperfect monitors, e.g., [20,8]. In the present paper, we have assumed that monitors are perfect in the sense that the norm enforcement mechanism can detect and respond to all norm violating behaviours. It should be clear that any work on norm enforcement either implicitly or explicitly assumes that the behaviour of agents is monitored and evaluated with respect to a given set of norms. Our assumption that monitors are perfect is reflected by the fact that updating a multi-agent model (i.e. CGS) with a set of norms covers possible (violating) behaviour. Moreover, although [20] and [8] consider norms syntactically like us, they use LTL-formulae as norm representation to refer to good/bad behaviour. We have considered less expressive norms in order not to complicate the main message of our approach unnecessarily. We believe that our general approach can be instantiated with more expressive norms as well.Another line of related research concerns the issue of norm synthesis. Shoham and Tennenholtz [59,60], have discussed the problem of off-line design of a set of norms in order to constrain the behaviour of agents and to ensure the overall objectives of the multi-agent system. In this work and similar to our approach, the structure of multi-agent systems is required and the norms are generated at design time. In line with this tradition, Morales et al. [53,52] consider the problem of on-line design of a set of norms to constrain and steer the behaviour of agents. In both off-line and on-line approaches, the overall objectives of multi-agent systems are guaranteed by assuming that agents are norm-aware and comply with the generated norms. In this sense norms are considered as being regimented in multi-agent systems. Moreover, in contrast to our work, these approaches neither provide a game theoretic analysis of norms nor consider the generation of norms with sanctions in the context of agents’ preferences. However, we believe that the concepts such as effective, necessary, and concise norms as introduced by [53,52] can also be used in off-line norm synthesis approaches such as ours. Following the tradition of Shoham and Tennenholtz, Boella and van der Torre [15] consider the problem of norm enforcement by distinguishing the choice of off-line designed stable social laws from the choice of control systems. A control system is explained to be responsible for monitoring and sanctioning of norm violations. Although the functionality of their proposed notion of control system is similar to the functionality of our notion of norm-based mechanism, there are some fundamental differences between these approaches. For example, in our approach the behaviour of a multi-agent system is modelled by a concurrent game structure while they consider a multi-agent system in an abstract way as a one-shot game, norms in our approach are explicit and enforced by an update mechanism while they consider norms as integrated in the structure of the games and enforced by a special agent called normative system that selects which game is going to be played, and finally they consider the notion of (quasi-)stable social laws while we consider norms from a mechanism design perspective as implementing a social choice function in Nash equilibria.Our work differs also from verification approaches to norm-based systems or protocols [33,7,27,5]. Of course, one can consider and exploit our work as an approach to verify the impact of norm enforcement on agents’ behaviour in the sense that our approach can be used to check the influence of norm enforcement on agents’ behaviour. However, in contrast to our approach, the mentioned work focuses on different types of norms and norm enforcement mechanisms, and does not provide any game theoretic tool to analyse the impact of norms on the behaviour of rational agents. In particular, in [33]norms are defined in terms of communication actions and enforced by means of regimentation, while [7] and [27] consider norms as state-based obligations/prohibitions enforced by means of both regimentation and sanctions. The approach pre-128N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142sented by [5] focuses mainly on the protocol verification and aims at providing a mechanism that can be used, for example by the agents, to decide whether following a protocol guarantees their objectives without any norm violation.+In the literature of multi-agent systems various proposals focus on the issue of the practical implementation of norms and norm enforcement. They consider norms and norm enforcement mechanisms in broader contexts such as institutions, organisations, or coordination environments. Examples are electronic institutions such as ISLANDER/AMELI [33,32], organ-isational models such as MOISE[42,43] and OperettA [4], coordination models such as ORG4MAS [41], the norm-based framework proposed by [24], the law-governed interaction proposed by [51], and the norm enforcement mech-anism proposed by [35]. The focus on these proposals is primarily on the development of norm-based multi-agent system rather than devising special purpose programming languages to implement norms and norm enforcement. Moreover, the lack of explicit formal syntax and (operational) semantics for norm-related concepts in these approaches makes it difficult, if not impossible, to relate them to the existing abstract models for norms and norm enforcement. These approaches are concerned with agents’ interaction, use different norm types, or focus on norm regimentation only. In the following, we discuss further details of some of these approaches./S–MOISE+Another related approach is MOISEISLANDER [31] is one of the early modelling languages for specifying institutions in terms of institutional rules and norms. In order to interpret institution specifications, an execution platform, called AMELI, has been developed [32]. This platform implements an infrastructure that, on the one hand, facilitates agent participation within the institutional environ-ment supporting their communication and, on the other hand, enforces the institutional rules and norms. The distinguishing feature of ISLANDER/AMELI is that it does not allow any agent to violate norms, i.e., norms are regimenting. Moreover, norms in [32], but also in [36] and [61], are action-based and prescribe actions that should or should not be performed by agents.[43], where a modelling language is proposed to specify multi-agent systems through three organisational dimensions: structural, functional, and deontic. The relevant dimension for our work is the deontic dimension that concerns concepts such as obligations and prohibitions. Different computational frameworks have been proposed to implement and execute MOISE[42] and its artifact-based ver-sion ORG4MAS [41]. These frameworks are concerned with norms prescribing states that should be achieved/avoided. S–MOISEis an organisational middleware that provides agents access to the communication layer and the current state of the organisation. This middleware allows agents to change the organisation and its specification, as long as such changes do not violate organisational constraints. In this sense, norms in S–MOISEcan be considered as regimenting norms. ORG4MAS uses organisational artifacts to implement specific components of an organisation such as group and goal schema. In this framework, a special artifact, called reputation artifact, is introduced to manage the enforcement of norms.specifications, e.g., S–MOISE+++++There are two proposals that specifically aim at implementing norms and norm enforcement. The first proposal, pre-sented in [64] and [27], is a norm-based executable specification language designed to facilitate the implementation of software entities that exogenously control and coordinate the behaviour of agents by means of norms. Similar to our ap-proach, norms in this proposal can be either sanctioning or regimenting norms. Also, similar to our approach, they come with an operational semantics such that executable specifications can be formally analysed by means of verification tech-niques [11]. However, in contrast to the approach presented in the present paper, they consider state-based norms such as obligation or prohibition of states and ignore action-based norms. This proposal comes with an interpreter, called 2OPL, which initiates a process that continuously monitors agents’ actions (i.e., communication and environment actions), deter-mines the effect of the observed actions based on the action specifications, and enforces norms when necessary. We plan to extend 2OPL such that it can interpret and execute action-based norms as presented in the present paper. The second pro-posal, called JaCaMo [16], aims at supporting the implementation of organisational artifacts, which are responsible for the management and enactment of the organisation. An organisational artifact is implemented by a program which implements a MOISEspecifications into norm-based programs is described by [44]. In contrast to our approach, the sanctions in JaCaMo are actions that are delegated to some agents and there is no guarantee that the agents will eventually perform the actions. In particular, the violation of norms is detected by organisational artifacts after which organisational agents have to deal with those violations.specification. A translation of MOISE++We conclude this section by the following observation. We have assumed that agents are norm aware in the sense that agents follow their preferences and choose optimal behaviours in order to maximize their utilities. The norm awareness is reflected in our approach by 1) defining agents’ preferences in terms of specific behaviour and whether the agents incur sanctions, and 2) by applying the equilibrium analysis to characterize the behaviour of rational agents under norm enforcement. However, we did not focus on how individual agents reason to choose their optimal behaviours as studied, for example, by [66] and [6]. In contrast to [66] and [6], we abstract over the specific reasoning schemes of individual agents and assume that whatever reasoning schemes individual agents use, they will always act rationally according to game theoretic concepts, more precisely according to Nash equilibria.8. Conclusions, discussion, and future workOur work focuses on norms and norm enforcement in multi-agent systems. We proposed a formal methodology that allows analysing norms and norm enforcement from a mechanism design perspective, and a programming model for imple-menting them. Using game theoretic tools we showed that the enforcement of norms can change the behaviour of rational agents using regimentation and sanctions. It is also shown that our presented programming model is aligned with the ab-stract model such that our developed game theoretical tools can be applied to analyse norm-based environment programs.N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142129Specifically, we proposed norm-based mechanism design as a formal methodology for analysing norm-based environment programs. We showed how to abstract from a particular environment specification language and how to apply methods from mechanism design to verify whether the enforcement of a set of norms on a multi-agent environment agrees with the behaviour of rational agents that the system designer expects. More precisely, we introduced normative behaviour functions for representing the “ideal” behaviour of multi-agent environments with respect to different sets of agents’ preferences. The latter enabled us to apply concepts from game theory to identify agents’ rational behaviour. These formal ideas can now be used to verify whether the enforcement of a set of norms is sufficient to motivate rational agents to act in such a way that their behaviour become aligned with that described by the normative behaviour function.We defined a normative system in such a way that it can modify (soft) facts of the environment states. As the language used for modelling agents’ preferences and the facts in normative systems are based on the same set of propositional symbols, a norm-based mechanism can steer the behaviour of each agent in flexible ways. This notion of mechanism is powerful. A first refinement could be to identify a subset (cid:2)M ⊆ (cid:2) of soft facts and assume that a normative system can only modify state valuations with respect to this set. Such a mechanism can be much weaker but also more natural.Another direction for future research is to consider robustness against group deviation. Our approach can be extended such that each agent a has its “own” set (cid:2)a of propositional symbols which is used to specify its preference. If we now want that some agents are not sensitive to norms and sanctions we simply define the set (cid:2)N F of facts that are used in a normative system such that (cid:2)N F ∩ (cid:2)a = ∅. Another alternative is to take on a more game theoretic point of view in the line with [1] and [23]. For example, one may consider partial strategies which assume that only subgroups of agents play rationally. Then, the outcome is usually not a single path any more, but rather a set of paths. This gives rise to a notion of (S, A)-implementability.We investigated the problem, given a CGS M and a set of agents’ preferences Prefs, and a normative behaviour function f , whether there is a normative system N which N E -implements f over M, q and Prefs. In future work it would be interesting to identify settings in which such normative systems can be constructed efficiently. We also plan to extend our analysis to other implementability notions apart from Nash equilibria in more detail, e.g. dominant strategy equilibrium implementability.Finally, yet another interesting direction for future research is to investigate core properties of classical mechanism design in our norm-based setting, including budget balanced and individual rational mechanisms. We note that already the interpretation of these properties in our setting is interesting. For example, in the case of individual rationality it is not clear what it means for an agent “not to take part” in the mechanism/in the multi-agent systems. This may require a shift to an open MAS where agents can leave an join the system.AcknowledgementWe thank the anonymous reviewers for their extensive and valuable comments which significantly improved the paper.Appendix A. Quantified Boolean satisfiability problemOur hardness proofs reduce validity of Boolean quantified formulae to the implementation problems. We consider fragments of the Quantified Boolean Satisfiability problem (QSAT), a canonical PSPACE-complete problem. Restricting the number of quantifier alternations of QSAT yields subproblems which are complete for different levels of the polynomial hierarchy. The problem class QSATi starts with an existential quantifier and allows for i − 1 quantifier alternations; sim-ilarly, ∀QSATi-formulae begin with an universal quantifier, i = 1, 2, . . . . The previous problems are (cid:2)Pi -complete, respectively. In the formal definition we write Q X for Q x1 . . . Q xn for a set X = {x1, . . . , xn} of propositional variables and Q ∈ {∀, ∃}.i and (cid:3)PDefinition 25 (QSATi [55]). The QSATi problem is defined as follows.Input: A quantified Boolean formula (QBF) φ = ∃ X1∀ X2 . . . Q i Xi ϕ where ϕ is a Boolean formula in negation normal form (nnf) (i.e., negations occur only at the propositional level) over disjoint sets of propositional variables X1, . . . , Xi and i ≥ 1where Q i = ∀ if i is even, and Q i = ∃ if i is odd. φ does not contain any free variables.Output: True if ∃ X1∀ X2 . . . Q i Xi ϕ is valid, and false otherwise.The problem ∀QSATi is defined analogously but formulae φ start with a universal quantifier and then alternate between quantifier types.By abuse of notation, we refer to a QBF φ that satisfies the structural properties required by the problem class QSATiand ∀QSATi simply as QSATi-formula and ∀QSATi -formula, respectively.A truth assignment or valuation for a set of variables X is a mapping v : X → {t, f }. Given a Boolean formula ϕ over variables X and a truth assignment v over Y ⊆ X we denote by ϕ[v] the formula obtained from ϕ where each y ∈ Y is replaced by ⊥ (falsum) and (cid:13) (verum) if v( y) = f and v( y) = t, respectively. For two truth assignments v 1 and v 2 over X and Y , respectively, with X ∩ Y = ∅ we use the notation v 1 ◦ v 2 to refer to the induced truth assignment over X ∪ Y ; analogously for more than two truth assignments. Using this notation a formula ∃ X1∀ X2 . . . Q i Xi ϕ is true iff there is a 130N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142Fig. B.15. Construction of the concurrent game structure for QSAT: value choice section.truth assignment v 1 over X1 such that for all truth assignments v 2 over X2 etc. the Boolean formula ϕ[v 1 ◦ . . . ◦ v i] is valid. For further details, we refer to [55].Appendix B. Proofs to implementation problems: verificationB.1. Hardness of weak implementation problem: Proposition 4We show that the membership problem “N ∈ WIN E (I, q, f )” is (cid:2)P2 -hard by reducing QSAT2 to it. In the following we consider an instance of QSAT2 of the formφ ≡ ∃{x1, . . . , xm}∀{xm+1, . . . , xn}ϕ(x1, . . . , xn)where X1 = {x1, . . . , xm} and X2 = {xm+1, . . . , xn}. The reduction consists of three main steps:1. We encode φ as a two-player CGS Mφ and show that φ is satisfiable if, and only if, player one has a winning strategy in Mφ to achieve a given property (Lemma 16).We use results from [22] where it was shown that the satisfiability of a QBF φ can be reduced to model checking a two-player CGS such that one of the players, the verifier v, has a winning strategy to enforce a (fixed) formula which is constructed from φ, against all strategies of the other player, the refuter r, if and only if, φ holds.2. We construct a preference profile (cid:9)γ = (γv, γr) such that a winning strategy of the verifier in Mφ is part of a Nash equilibria in (cid:13)(Mφ, q, (γv, γr)) if, and only if, φ is satisfiable.3. We show that the existence of such a specific Nash equilibria can be answered by testing membership in the weak implementation problem.In the following we assume that the QSAT2-formula φ given above is fixed, including the indexes m and n and that it is in negated normal form. Moreover, in the following it is important that X1 and X2 are non-empty. This can be assumed without loss of generality.B.1.1. The model MφWe describe the construction the CGS Mφ from formula φ. The idea of the construction in [22] is that the verifierv (controlling existential variables and disjunctions) and the refuter r (controlling existential variables and conjunctions) firstly choose truth values of the variables they control. This is illustrated in Fig. B.15.For example, if v plays (cid:13) in q2 a state labelled x2 is reached; otherwise, a state labelled notx2. This represents that variable x2 is assigned true or false, respectively. This part of the model is called value choice section and consists of statesQ1 = {qi | i = 1, . . . , n} ∪ {qi v | i = 1, . . . , n and v ∈ {⊥, (cid:13)}}.States qi with 1 ≤ i ≤ m are controlled by v, states with m + 1 ≤ i ≤ n are controlled by r. Afterwards, both agents simulate the game theoretic semantics of propositional logic. Player v tries to make the formula true (thus controls disjunctions) and r tries to falsify the formula (thus controls conjunctions). This part of the model corresponds to the parse tree of a formula, see Fig. B.16. For the formal definition we need additional notation. First, we use sf(φ) to denote the set of subformulae of φ. For every formula ψ = ψ1 ◦ ψ2 with ◦ ∈ {∧, ∨} we use L(ψ) = ψ1 and R(ψ) = ψ2 to refer to the left and right subformula of φ, respectively. If ψ = ¬ψ (cid:7). This allows to use a sequence of L’s and R’s, i.e. an element from {L, R}∗, to uniquely refer to a subformula where (cid:14) refers to φ itself. We refer to such a sequence as index. For a formula φ we use ind(φ) ⊆ {L, R}∗to denote the set of all possible indexes wrt. φ. By slight abuse of notation, we denote the subformula referred to by such an index also by sf(i) where i ∈ {L, R}∗. Note, that different indexes can refer to the same subformula. For example, given the formula (x1 ∧ x2) ∨ (x2 ∧ ¬x1) we have that sf(L R) = sf(R L) = x2. Note, as we stop at the literal level, the indexes R R L and R R R are not contained in ind((x1 ∧ x2) ∨ (x2 ∧ ¬x1)).and ψ is not a literal14 we define L(ψ) = R(ψ) = ψ (cid:7)14 We stop at the literal level as it simplifies our construction.N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142131Fig. B.16. CGS for QSAT: formula structure section where sf(ι1), . . . , sf(ιl) are all literals.Fig. B.17. CGS for QSAT: sections of literals where sf(ι) = xi and sf(ι(cid:7)) = ¬xi .Fig. B.18. Small gadget which can be used by the two players to make their strategy inconsistent.Given this notation, the formula structure section of the model consists of statesQ2 = {qι | ι ∈ ind(φ), sf(ι) is not a literal}.For every index ι where sf(ι) = ψ = ψ1 ◦ ψ2 with ◦ ∈ {∧, ∨} one of the players chooses L(ψ) = ψ1 or R(ψ) = ψ2. If ◦ = ∨the verifier v chooses the subformulae; otherwise the refuter does. The “semantic game” between both players ends up in a literal, modelled by a literal state. The section of literals is built over statesQ3 = {qι | ι ∈ ind(φ), sf(ι) is a literal}for each index ι corresponding to a literal l in (cid:20), we have that the state qι is controlled by the owner of the Boolean variable xi in l (i.e. l = xi or l = ¬xi ). As in the value choice section, the owner of that state chooses a value ((cid:13) or ⊥) for the underlying variable (not for the literal!) which leads to a new state of the evaluation section. These states are denoted byQ 4 = {qιv | ι ∈ ind(φ), sf(ι) is a literal and v ∈ {(cid:13), ⊥}} and Q 5 = {q(cid:13), q⊥}.A state qιv with sf(ι) = xi is labelled with the proposition xi if v = (cid:13) and with notxi if v = ⊥; similarly, qιv with sf(ι) = ¬xiis labelled with the proposition xi if v = ⊥ and with notxi if v = (cid:13). That is, the label v ∈ {(cid:13), ⊥} models the evaluation of the literal and not of the underlying variable. These states shall be used to ensure that a strategy induces a truth assignment, as further explained below. Then, the system proceeds to the winning state q(cid:13) (labelled with the proposition winv) if the valuation of xi makes the literal sf(ι) true, and to the losing state q⊥ (labelled with the proposition winr) otherwise – see Fig. B.17 for details. Finally, we need two special gadgets to ensure that the reduction works. First, we connect a state qdto the initial state q0. This state will be used to ensure the existence of some Nash equilibrium. Secondly, we need to give the players a possibility to mark specific strategies as inconsistent, for reasons which will become clear below. Therefore, we insert a small substructure as shown in Fig. B.18 in-between the start state q0 and q1, the beginning of the value-choice section. The whole construction is illustrated in Fig. B.19. We refer to the CGS just constructed as Mφ . The formal definition is given next.Definition 26 (Model Mφ ). Let QSAT2-formula φ ≡ ∃{x1, . . . , xm}∀{xm+1, . . . , xn}ϕ be given and in negated normal form. We define Xv = {x1, . . . , xm} and Xr = {xm+1, . . . , xn}. The CGS Mφ = (Agt, Q, (cid:2), Act, π , d, o) is defined as follows:132N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142Fig. B.19. Concurrent game structure for the QSAT2 instance (cid:20)1 ≡ ∃x1∀x2(x1 ∧ x2) ∨ (x2 ∧ ¬x1). We have that ind(φ1) = {(cid:14), L, R, LL, L R, R L, R R}. In particular, we have that: sf((cid:14)) = (x1 ∧ x2) ∨ (x2 ∧ ¬x1), sf(L) = x1 ∧ x2, sf(R) = x2 ∧ ¬x1, sf(LL) = x1, sf(R R) = ¬x1 etc. “Light gray” states are owned by the verifier; “dark grey” states are owned by the refuter. “White states” belong to no player. A transition outgoing from a state controlled by v (resp. r) labelled with an action α corresponds to an action profile (α, −) (resp. (−, α)). Transitions without label correspond to an action profile (−, −).• Agt = {v, r},• Q = {q0, q• (cid:2) = {xi | i = 1, . . . , n} ∪ {notxi | i = 1, . . . , n} ∪ {winv, winr, start, d, ir, iv},(cid:7)0) = π (q• π (q0) = {start}, π (q} ∪ Q1 ∪ Q2 ∪ Q3 ∪ Q4 ∪ Q5,(cid:7)0, qd} ∪ {qv, q(cid:7)v, qr, q(cid:7)r(cid:7)r) = {ir}, π (qi(cid:13)) = {xi} for all qi(cid:13) ∈ Q1, π (qi⊥) = {notxi} for all qi(cid:13) ∈ Q1, π (qxi (cid:13)) = {xi} for all qxi (cid:13) ∈ Q4, π (qxi ⊥) = {notxi} for all qxi (cid:13) ∈ Q5, π (q¬xi ⊥) = {xi} for all q¬xi (cid:13) ∈ Q4, π (q¬xi (cid:13)) = {notxi}for all q¬xi (cid:13) ∈ Q5, π (q(cid:13)) = {winv}, π (qd) = {d} and π (q⊥) = {winr}.(cid:7)v) = {iv}, π (q• Act = {L, R, (cid:13), ⊥, −}• the function d is defined as– dv(q0) = dr(q0) = {(cid:13), ⊥}.– dv(qi) = {(cid:13), ⊥} and dr(qi) = {−} for xi ∈ Xv ∪ {qv}.– dr(qi) = {(cid:13), ⊥} and dv(qi) = {−} for xi ∈ Xr ∪ {qr}– dv(qι) = {L, R} and dr(qi) = {−} for all ι ∈ ind(φ) with sf(ι) = ψ1 ∨ ψ2 and sf(ι) not a literal– dr(qι) = {L, R} and dv(qi) = {−} for all ι ∈ ind(φ) with sf(ι) = ψ1 ∧ ψ2 and sf(ι) not a literal– dv(q) = dr(q) = {−} for all other states q.(cid:7)0, o(q0, (α1, α2)) = qv for (α1, α2) ∈ {((cid:13), ⊥), (⊥, ⊥)}, and o(q(cid:7)0, (−, −)) = qv.• and o is defined as:– o(q0, ((cid:13), (cid:13))) = qd, o(q0, (⊥, (cid:13))) = q(cid:7)(cid:7)– o(qv, ((cid:13), −)) = qv, (−, −)) = qrv, o(qv, (⊥, −)) = qr and o(q(cid:7)(cid:7)r, (−, −)) = q1r, o(qr, (−, ⊥)) = q1 and o(q– o(qr, (−, (cid:13))) = q– o(q, (−, −)) = q for q ∈ {qd, q(cid:13), q⊥}.– o(qi, (v, −)) = qi v where v ∈ {(cid:13), ⊥} and xi ∈ Xv– o(qi, (−, v)) = qi v where v ∈ {(cid:13), ⊥} and xi ∈ Xr– o(qι, (x, −)) = qιx where x ∈ {L, R}, ι ∈ ind(φ) and sf(ι) = ψ1 ∨ ψ2.– o(qι, (−, x)) = qιx where x ∈ {L, R}, ι ∈ ind(φ) and sf(ι) = ψ1 ∧ ψ2.N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142133– o(qι, ( y, −)) = qι y where y ∈ {(cid:13), ⊥} and sf(ι) = x ∈ Xv.– o(qι, ( y, −)) = qιz where y, z ∈ {(cid:13), ⊥}, y (cid:16)= z and sf(ι) = ¬x with x ∈ Xv.– o(qι, (−, y)) = qι y where y ∈ {(cid:13), ⊥} and sf(ι) = x ∈ Xr.– o(qι, (−, y)) = qιz where y, z ∈ {(cid:13), ⊥}, y (cid:16)= z and sf(ι) = ¬x with x ∈ Xr.– o(qi v , (−, −)) = qi+1 for i = 1, . . . , n − 1 and v ∈ {(cid:13), ⊥}– o(qnv , (−, −)) = qφ for v ∈ {(cid:13), ⊥}– o(qι(cid:13), (−, −)) = q(cid:13) for sf(ι) a literal.– o(qι⊥, (−, −)) = q⊥ for sf(ι) a literal.Each state of Mφ has at most two outgoing transitions, with the exception of state q0 which has four. Hence, the number of transitions is polynomial in the number of states. In the next section we describe how to ensure that only strategies of the players are taken into consideration which correspond to truth assignments.B.1.2. Strategies and assignmentsIf a path in the model goes through a state labelled xi and notxi this can be interpreted as setting variable xi true and false, respectively. We observe that the states which have as children an xi-state or a notxi-state the transitions to a successor is determined by a single player only. Thus, a strategy of a player completely determines which xi-states and notxi-states, where xi is a variable of the very player, are visited. It may happen that a path contains an xi-state as well as a notxi-state, by performing contrary actions in a state qi of the value-choice section and in qι with sf(ι) = xi in the literal section. If this happens we call the responsible strategy of the player inconsistent as it does not encode a truth assignment of the variables that the player controls. Thus, it has to be ensured that choices are made consistently: the same variable x is always assigned true, or always assigned false. For this purpose the following consistency constraints—temporal formulae—are introduced for each i ∈ {1, . . . , n}:Ti ≡ (cid:2)¬xi ∨ (cid:2)¬notxi.It is easy to see that if Ti is true along a path then this path—or the associated strategy of the player that controls xi —represents a truth assignment of xi . We setm(cid:16)Tv ≡ (cid:2)¬iv ∧Tr ≡ (cid:2)¬ir ∧Tii=1n(cid:16)Tii=m+1where the propositions ir and iv are used to indicate that the current strategy of the refuter and verifier, respectively, are invalid. The latter is needed to prevent specific strategy profiles to constitute a Nash equilibrium. We call a strategy of the verifier v (resp. refuter r) consistent if for all strategies of the refuter (resp. the verifier) the induced outcome path satisfies Tv (resp. Tr). In the case of consistent choices we observe that the formula structure section together with the sections of literals implement the game theoretical semantics of Boolean formula [39].Next we recall from [22] (with small modifications) the following lemma which says that φ is satisfiable iff there is a consistent strategy of player v such that for all consistent strategies of player r—again, consistent means that truth values are assigned consistently to variables and that it is not invalid—such that eventually winv holds. We emphasize that our construction does not assume perfect-recall strategies due to the fact the we are only considering one quantifier alternation. Also, note that if q⊥ or q(cid:13) is reached, each agent can switch from a consistent strategy to an inconsistent one without changing the outcome paths with the possible exception of the transitions between qv and q1.Lemma 16. Let a QSAT2-formula φ ≡ ∃{x1, . . . , xm}∀{xm+1, . . . , xn}ϕ(x1, . . . , xn) in negated normal form be given. The model Mφcan be constructed in polynomial time. We have that φ is satisfiable iff there is a strategy sv of the verifier v with sv(q0) = ⊥ such that for all strategies sr of the refuter r it holds that outMφ (q0, (sv, sr)) |=LTL Tv ∧ (Tr → (cid:3)winv).Proof. “⇒”: Suppose φ is true and let sv be the strategy induced by a truth assignment v of {x1, . . . , xm} witnessing the truth of φ (i.e. ϕ[v] is valid) and by the simulation of the game theoretic semantics which witnesses the truth of ϕ[v]. Moreover, let sv(q0) = ⊥ (this is needed to avoid that the refuter can reach the state qd ) and let sr be any consistent strategy of r. Note that the strategy (sv, sr) induces a truth assignment v 1 ◦ v 2 of Xv ∪ Xr. Then, (cid:3)winv must be true on outMφ (q0, (sv, sr)) otherwise the refuter had a strategy sr which induces a truth assignment v with ϕ[v 1 ◦ v] is false. Which would yield a contradiction.“⇐”: Let sv be a strategy of the verifier v with sv(q0) = ⊥ such that for all strategies sr of the refuter r it holds that outMφ (q0, (sv, sr)) |=LTL Tv ∧ (Tr → (cid:3)winv). sv induces a truth assignment v. For any consistent strategy sr we have that (cid:3)winv is true. It is straight-forward to check that the truth assignment v induced by sv encodes a truth assignment witnessing the truth of φ due to the encoded game theoretic semantics in the model. That is, ϕ[v] is valid and thus φ is true. (cid:2)134N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142B.1.3. Preferences and Nash equilibriaLemma 16 showed that the satisfiability problem of a given QSAT2-formula φ can be reduced to a strategy ex-istence and model checking problem of a rather simple formula in Mφ . What we need for the reduction to the weak implementability problem is that a winning strategy of the verifier is part of a Nash equilibrium iff the for-mula φ is satisfiable. For this purpose, we define the following preference lists for player v and r, respectively: γv = (((cid:12)d, 4),γr = (((cid:12)d, 4),(((cid:3)winv ∧ Tv), 3),((¬(cid:3)winv ∧ ¬Tr ∧ Tv), 2),((¬(cid:3)winv ∧ Tr ∧ ¬Tv), 1),((cid:13), 0)),((¬(cid:3)winv ∧ Tr ∧ Tv), 3),((¬(cid:3)winv ∧ ¬Tr ∧ ¬Tv), 2),(((cid:3)winv ∧ Tr), 1),((cid:13), 0)).Now we can relate the satisfiability of φ to the existence of Nash equilibria which contains a winning strategy of the verifier according to Lemma 16.Proposition 17 (Nash equilibria in Mφ ). Let φ be a QSAT2-formula in nnf and (γv, γr) be the preference profile defined above.(a) We have that (sd, s(b) φ is satisfiable iff there is an s ∈ N E(Mφ, q0, (γv, γr)) with outMφ (q0, s) |=LTL (cid:3)winv.(c) Let γ (cid:7)(cid:7)d) ∈ N E(Mφ, q0, (γv, γr)) for any two strategies sd and s(cid:7)d with sd(q0) = sr equal γv and γr but with the first list entry ((cid:12)d, 4) being removed, respectively. Then, it holds that φ is satisfiable (cid:7)d(q0) = (cid:13).v and γ (cid:7)iff N E(Mφ, q0, (γ (cid:7)v, γ (cid:7)r )) (cid:16)= ∅.Proof.(a) The strategy profile (sd, sd) yields the path q0qωto improve its payoff. It is a Nash equilibrium.(cid:7)d . For that path the payoff for both players is four. No player can deviate (b) “⇒”: If φ holds then, according to Lemma 16, there is a strategy sv such that sv(q0) = ⊥ and for all strategies sr it (cid:7)holds that outMφ (q0, (sv, sr)) |= Tv ∧ (Tr → (cid:3)winv) ((cid:9)). We claim that the profile (sv, srof the refuter is a Nash equilibrium. First, observe that sr(q0) = ⊥, otherwise Tr could not be true on the outcome path. (cid:7)Second, note that such a strategy sr must exist as r always has a consistent strategy. Given these strategies players v and r get a payoff of 3 and 1, respectively. As sr(q0) = sv(q0) = ⊥ no player can deviate to reach state qd. Moreover, player r cannot deviate to a consistent strategy making ¬(cid:3)winv true by Lemma 16. This shows that no player can unilaterally deviate to increase its payoff.“⇐”: Suppose φ is false. That is,(cid:7)r) for any consistent strategy s((cid:9)) for each truth assignment v 1 of {x1, . . . , xm} there is a truth assignment v 2 of {xm+1, . . . , xn} such that ϕ[v 1 ◦ v 2]is false.We consider a strategy profile s = (sv, sr) with outMφ (q1, s) |=LTL (cid:3)winv and show that it cannot be a Nash equilibrium. For the sake of contradiction, suppose s were a Nash equilibrium. Then, first, it must be the case that Tv is true on the path; otherwise, player v can increase its utility by deviating to a consistent strategy that still satisfies (cid:3)winv (note that the strategy of r is fixed and memoryless). Second, we show that player r can always deviate to increase its payoff. If Trholds then r gets a payoff of 1. However, by ((cid:9)) the refuter can deviate to a consistent strategy such that the resulting path satisfies ¬(cid:3)winv. By doing so it can increase its payoff to 3. On the other hand, if Tr does not hold, then the refuter gets a payoff of 0 and can again deviate to increase its payoff by ((cid:9)), i.e. by deviating to a consistent strategy that satisfies ¬(cid:3)winv.(c) We consider the modified preference list. If φ is satisfiable then N E(Mφ, q0, (γ (cid:7)r )) (cid:16)= ∅ by (b). Now suppose φ is false. That is, ((cid:9)) holds. We consider a strategy profile s = (sv, sr) and show that it cannot be a Nash equilibrium. Firstly, suppose outMφ (q0, (sv, sr)) |= (cid:3)winv. Then, it has already been shown in (b) that s cannot be a Nash equilibrium. Thus, we consider the case that λ := outMφ (q0, (sv, sr)) |= ¬(cid:3)winv. We distinguish the four cases which result from players playing consistent or inconsistent strategies. (i) Suppose Tv ∧ Tr holds on path λ. Then, v is better off by playing an inconsistent strategy resulting in a payoff of 1 rather than 0. (ii) If Tv ∧ ¬Tr is true then r is better of playing a consistent strategy which remains to satisfy ¬(cid:3)winv. This is possible by ((cid:9)). (iii) ¬Tv ∧ Tr is true on λ. Then, player rcan deviate to an inconsistent strategy that still makes ¬(cid:3)winv true (this can be achieved by playing (cid:13) in state q0). This increases the player’s payoff from 0 to 2. (iv) Suppose the path satisfies ¬Tv ∧ ¬Tr. Then, player v is better off to deviate to a consistent strategy. This shows that there cannot be any Nash equilibrium. (cid:2)v, γ (cid:7)Theorem 18 (Hardness of weak implementation: verification). Let (I, q) be a pointed NIS and N a normative system included in I. The problem whether N ∈ WIN E (I, q, f ) is (cid:2)P2 -hard in the size of M, N and Prefs.N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–1421352 -hardness is shown by a reduction to the weak implementability problem of f ((γv, γr)) = (cid:3)winv. Given aProof. (cid:2)PQSAT2-formula φ in nnf we construct the model Mφ , which can be done in polynomial time according to Lemma 16, and define I = (Mφ, {(γv, γr)}, {N(cid:14) }). Now, by Proposition 17(b) we have that: φ is satisfiable iff ∃s ∈ N E(Mφ, q0, (γv, γr))such that out(q0, s) |= (cid:3)winv. This equivalence, however, is equivalent to N(cid:14) ∈ WIN E (I, q0, f ). (cid:2)B.2. Hardness of strong implementability: Theorem 5The strong implementation problem requires checking two parts: that the set of Nash equilibria is non-empty and that all Nash equilibria satisfy specific properties. We show that the latter one gives rise to (cid:3)P2 -hardness. In order to be able to use our model Mφ for a reduction of ∀QSAT2, we need to switch the roles of the verifier and refuter. That is, the refuter controls variables in {x1, . . . , xm} and the verifier in {xm+1, . . . , xn}. This revised model denoted by (cid:17)Mφ and will later also be used for the problem of the existence of an appropriate normative system. This reduction requires an additional labelling of some states, modelling the guessing of a normative system.Definition 27 (The model (cid:17)Mφ ). Given a ∀QSAT2-formula φ ≡ ∀{x1, . . . , xm}∃{xm+1, . . . , xn}ϕ(x1, . . . , xn) in nnf we define the CGS (cid:17)Mφ analogously to Mφ but Xr = {x1, . . . , xm} and Xv = {xm+1, . . . , xn}. We further label q0 and each state reachable in an even number of transitions from q0 with a new proposition t.The labelling t will later be used for ensuring that the structure of (cid:17)Mφ is not affected by a norm-based update, more precisely that no transitions are regimented making for instance the winning state of the verifier unreachable. The proof of the following lemma is done analogously to the one of Lemma 16.Lemma 19. Given a ∀QSAT2-formula φ ≡ ∀{x1, . . . , xm}∃{xm+1, . . . , xn}ϕ(x1, . . . , xn) in nnf we have that φ is satisfiable iff for all strategies sr of refuter r with sr(q0) = ⊥ there is a strategy sv of verifier v such that out (cid:18)Mφ (q0, (sv, sr)) |=LTL Tr → (Tv ∧ (cid:3)winv).Proof. [Sketch] “⇒”: Suppose φ is true. For each truth assignment v of Xr let w v denote a truth assignment of Xv such that ϕ[v ◦ w v ] is valid. Let sr denote the strategy induced by v and let ssrv be a consistent strategy induced by w v which witnesses the truth of φ. Thus, (cid:3)winv must be true on out (cid:18)Mφ (q0, (ssrv , sr)) otherwise the refuter had a consistent strategy for which there is no consistent strategy ssrv such that out (cid:18)Mφ (q0, (ssrv , sr)) (cid:16)|=LTL (cid:3)winv. This would imply that there is an vsuch that for all w v , ϕ[v ◦ w v ] is false. Contradiction.“⇐”: Let ssrv be a consistent strategy of the verifier v which witnesses the truth of the formulae when the refuter v , sr)) |=LTL (cid:3)winv. The strategies induce the truth assignments v and w v , plays the consistent strategy sr, i.e. out (cid:18)Mφ (q0, (ssrrespectively (using the same notation as above). It is straight-forward to check that ϕ[v ◦ w v ] is true. (cid:2)The next lemma shows that we can use the previous result to reduce the truth of a ∀QSAT2 instance to a property over all Nash equilibria in (cid:17)Mφ using a slightly modified variant of the preference lists of player v used before. We define(cid:17)γv = (((cid:12)d, 5),(((cid:3)winv ∧ Tv), 4),((¬(cid:3)winv ∧ ¬Tr ∧ Tv), 3),((¬(cid:3)winv ∧ Tr ∧ Tv), 2),((¬(cid:3)winv ∧ Tr ∧ ¬Tv), 1),((cid:13), 0)),and (cid:17)γr = γr.Proposition 20 (Nash equilibria in (cid:17)Mφ ). Let φ be a ∀QSAT2-formula in nnf.d) ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)) for any strategies sd and s(a) We have that (sd, s(b) φ is satisfiable iff for all s ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)) it holds that out(q0, s) |=LTL (cid:3)winv ∨ (cid:12)d.(cid:7)d with sd(q0) = s(cid:7)(cid:7)d(q0) = (cid:13).Proof.(a) Analogously to Proposition 17(a).(b) “⇒”: If φ holds then, according to Lemma 19, for all strategies sr with sr(q0) = ⊥ there is a strategy sv such that out (cid:18)Mφ (q0, (sv, sr))) |= Tr → (Tv ∧ (cid:3)winv) ((cid:9)). Now, suppose there was a Nash equilibrium s = (sv, sr) with out(cid:18)Mφ (q0, (sv, sr)) |= ¬(cid:3)winv ∧ ¬ (cid:12) d. We consider the following cases. First, assume that out (cid:18)Mφ (q0, (sv, sr)) |= Tr. 136N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142Then, by ((cid:9)) the verifier could deviate from sv to obtain a path satisfying Tv ∧ (cid:3)winv. This shows that (sv, sr) is not a Nash equilibrium. Second, assume that out (cid:18)Mφ (q0, (sv, sr)) |= ¬Tr ∧ Tv. In that case, the refuter would be better off playing a consistent strategy. Thirdly, assume that out (cid:18)Mφ (q0, (sv, sr)) |= ¬Tr ∧ ¬Tv. In that case the verifier would be better off switching to a consistent strategy. Also note that not both players can play (cid:13) in q0 as ¬ (cid:12) d holds. This shows that for all s ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)), out (cid:18)Mφ (q0, s) |=LTL (cid:3)winv ∨ (cid:12)d.“⇐”: Suppose φ is false. We need to show that there is a Nash equilibrium s such that out (cid:18)Mφ (q0, s) |=LTL ¬(cid:3)winv ∧¬ (cid:12)d. By Lemma 19 there is a strategy sr with sr(q0) = ⊥ such that for all strategies sv we have out (cid:18)Mφ (q0, (sv, sr)) |=LTL(cid:7)(cid:7)v, sr)) |=LTL Tv and Tr ∧ (Tv → ¬(cid:3)winv) (by contraposition). Now, let sv be any strategy such that out (cid:18)Mφ (q0, (sv) ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)). By Lemma 19, the verifier cannot change its strategy to (cid:7)v(q0) = ⊥. We show that (sr, ss(cid:7)ensure (cid:3)winv. Moreover, as sr(q0) = ⊥ the verifier v cannot deviate to increase its payoff. Analogously, as sv(q0) = ⊥, (cid:7)v) is indeed a Nash equilibrium. Thus we the refuter cannot deviate from sr to increase its payoff. This shows that (sr, shave that there is an s ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)) with out (cid:18)Mφ (q0, s) |=LTL ¬(cid:3)winv ∧ ¬ (cid:12) d. (cid:2)(cid:7)Theorem 21 (Hardness of strong implementation: verification). Let (I, q) be a pointed NIS and N a normative system included in I. The problem whether N ∈ SIN E (I, q, f ) is (cid:2)P2 -hard in the size of M, N and Prefs.2 -hard as well as (cid:3)PProof. (cid:2)Ppolynomial time such that: φ is satisfiable iff N E(Mφ, q0, ((cid:17)γv, (cid:17)γr)) (cid:16)= ∅. This is equivalent to2 -hardness. By Lemma 16 and Proposition 17(c) we can construct a model Mφ from a QSAT2-formula φ in nnf in N E(Mφ, q0, ((cid:17)γv, (cid:17)γr)) (cid:16)= ∅ and ∀s ∈ N E(Mφ, q0, ((cid:17)γv, (cid:17)γr)) : outMφ (q0, s) |=LTL (cid:13) iff N(cid:14) ∈ SIN E (I, q0, f )for f (((cid:17)γv, (cid:17)γr)) = (cid:13).(cid:3)P2 -hardness. We reduce ∀QSAT2 to the strong implementation problem for f (((cid:17)γv, (cid:17)γr)) = (cid:3)winv ∨ (cid:12)d. By Lemma 19and Proposition 20 we can construct a model (cid:17)Mφ from a ∀QSAT2-formula φ in polynomial time such that:φ is satisfiableiff ∀s ∈ N E( (cid:17)Mφ, q0, ( (cid:17)γv, (cid:17)γr)) : out(cid:18)Mφ (q0, s) |=LTL (cid:3)winv ∨ (cid:12)d (Proposition 20(b))iff ∀s ∈ N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)) : out(cid:18)Mφ (q0, s) |=LTL (cid:3)winv ∨ (cid:12)d and N E( (cid:17)Mφ, q0, ((cid:17)γv, (cid:17)γr)) (cid:16)= ∅ (Proposition 20(a))iff N(cid:14) ∈ SIN E (I, q0, f )(cid:2)Appendix C. Proofs of implementation problems: existenceIn the following we consider the hardness proof of the existence problem stated in Proposition 8. The proof that the problem is (cid:2)P3 -hard requires a further technical sophistication. The basic idea is that the normative system is used to sim-ulate the first existential quantification in a QSAT3-formula. Firstly, we show how this can be achieved by using sanctioning norms and afterwards by using regimenting norms.C.1. Sanctioning and regimentation normsFor the hardness part we reduce QSAT3. Therefore, we consider the QSAT3-formulaφ ≡ ∃{x1, . . . , xr}∀{xr+1, . . . , xr+m}∃{xr+m+1, . . . , xn}ϕ(x1, . . . , xn)in nnf and show that φ is true iff SIN E (I, q, f ) = ∅ for appropriate I, q, and f . The idea of the reduction extends the (cid:17)φ from the previous section where (cid:17)φ is a reduction given in the previous section. Essentially, we use the construction (cid:17)Mmodified version of the QSAT3-formula φ. The refuter r additionally controls the literal states referring to the new variables in {x1, . . . , xr}. In order to use our previous construction, we define (cid:17)φ as the formula obtained from φ as follows:(cid:17)φ ≡ ∀{x1, . . . , xr, xr+1, . . . , xr+m}∃{xr+m+1, . . . , xn}ϕ(x1, . . . , xn)That is, the first existentially quantified variable are moved to the universal part controlled by the refuter. A sanctioning (cid:7)norm is used to define a truth assignment of the variables x1, . . . , xr . For this purpose, a sanctioning norm labels states q0and qv with new propositions representing the truth assignment. For illustration assume that the sanctioning norm should encode a truth assignment which assigns true to x1, . . . , xg and false to xg+1, . . . , xr with 1 ≤ g ≤ r. This can be modelled by the normative system:N = {({start}, {((cid:9)1, (cid:9)2) | (cid:9)1, (cid:9)2 ∈ {(cid:13), ⊥}, not (cid:9)1 = (cid:9)2 = (cid:13)}, {x1, . . . , xg, notxg+1, . . . , notxr)}.Then, in the model (cid:17)Mconsider (cid:17)φ the consistency constraint Tr of r in (cid:17)M(cid:17)φ (cid:2) N, states q(cid:17)φ has the form:(cid:7)0 and qv are additionally labelled with {x1, . . . , xg, notxg+1, . . . , notxr}. Given that we N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142137Tr ≡ (cid:2)¬ir ∧r+m(cid:16)Ti ∧r(cid:16)((cid:2)¬xi ∨ (cid:2)¬notxi).i=r+1i=1We also define the formula(cid:19)asgn ≡(cid:12)(cid:21)(xi ∨ notxi) ∧ ¬(xi ∧ notxi)r(cid:16)(cid:20)i=1(cid:22)∨ (cid:12)d(cid:7)expressing that the next state is either qd, or q0 or qv each of the two labelled with a representation of a truth assignment for the variables {x1, . . . , xr}, assuming that the current state is q0. These formulae suffice if we were only given sanc-tioning norms. In the presence of regimenting norms, however, it has also to be ensured that regimenting norms do not regiment transitions invalidating the structure of the model. For example, the transitions leading to state q(cid:13) may simply be regimented which makes it impossible for the verifier to win. The idea is to introduce a formula which “forbids” such undesirable normative systems. Therefore, we define the formula(cid:21)(t → (cid:12)¬t) ∧ (¬t → (cid:12)t) ∨ (cid:12)(d ∨ winv ∨ winr)tick = (cid:2)(cid:20).It describes that a path does not loop in states other than the already looping states qd , q(cid:13), and q⊥. Now, we can prove the following result:Lemma 22. Let N ∈ Nrs be a normative system such that for all paths λ ∈ (cid:8)(cid:18)Mregimentation norm which is applicable in a state Q\{qd, q⊥, q(cid:13)}.(cid:17)φ (cid:2)N(q0) it holds that λ |= tick. Then, N cannot contain a Proof. Suppose that an outgoing transition in a state q ∈ Q\{qd, q⊥, q(cid:13)} is regimented. Then, there is a loop from q to q. Let λ denote the path from q0 to q followed by qω. Clearly, the formula tick is violated on that path which contradicts the assumption. (cid:2)Lemma 23. Given a QSAT3-formula φ ≡ ∃{x1, . . . , xr}∀{xr+1, . . . , xr+m}∃{xr+m+1, . . . , xn}ϕ(x1, . . . , xn) in nnf we have that φis satisfiable iff there is an N ∈ Nrs such that for all strategies sr of r with sr(q0) = ⊥ there is a strategy sv of v such that (cid:17)φ (cid:2)N(q0) we have that λ |= ass ∧ tick. Moreover, for the out(cid:18)Mdirection “⇒” we can always find a normative system in Ns.(cid:17)φ (cid:2)N(q0, (sv, sr)) |=LTL (Tr → (Tv ∧ (cid:3)winv)) and for all paths λ ∈ (cid:8)(cid:18)MProof. “⇒”: Suppose φ holds. Let v be a witnessing truth assignment of the variables {x1, . . . , xr}. Then, φ(cid:7) ≡∀{xr+1, . . . , xr+m}∃{xr+m+1, . . . xn}ϕ(x1, . . . , xn)[v] is satisfiable. By Lemma 19, we have that for all sr with sr(q0) = ⊥ there is an sv such that((cid:9)) out(cid:18)Mφ(cid:7) (q0, (sv, sr)) |=LTL Tr → (Tv ∧ (cid:3)winv).Now letN = {({start}, {((cid:13), ⊥), (⊥, (cid:13)), (⊥, ⊥)}, {xi1 , . . . , xig , notxig+1 , . . . , notxir )}= . . . = v ir= . . . = v i g= t and v i g+1= f where (i1, . . . , ir) is a permutation of (1, . . . , r). Then, we have that for such that v i1all paths λ ∈ (cid:8)(cid:18)M(cid:17)φ (cid:2)N(q0), π (λ) |=LTL ass ∧ tick. The claim follows by ((cid:9)) applied on ϕ[v] as the normative system essentially fixes the choices of the refuter for all variables {x1, . . . , xr}; every deviation of these induced truth values of the refuter would result in an inconsistent strategy.(cid:17)φ (cid:2)N(q0, (sv, sr)) |=LTL (Tr → (Tv ∧ (cid:3)winv)) and for all paths λ ∈ (cid:8)(cid:18)M“⇐”: Let N ∈ Nrs such that for all strategies sr with sr(q0) = ⊥ of r there is a strategy sv of v such that (cid:17)φ (cid:2)N(q0) we have that λ |= ass ∧ tick. By Lemma 22out(cid:18)M(cid:7)no transitions can be regimented apart from those starting in qd , q⊥, and q(cid:13). The valuation of q0 and of qv induce truth assignments v 1 and v 2 of {x1, . . . , xr}, respectively. We can choose N in such a way that v 1 = v 2. This is the case because for all strategies sr of r with sr(q0) = ⊥ there is a strategies sv of v such that out (cid:18)M(cid:17)φ (cid:2)N(q0, (sv, sr)) |=LTL (Tr → (Tv ∧ (cid:3)winv)), we also have that ∀{xr+1, . . . xr+m}∃{xr+m+1, . . . , xn}ϕ[v i] is true for i ∈ {1, 2} (cf. Lemma 19). The claim follows. (cid:2)Now, we can present our reduction to the implementation problem. Again, we need to slightly modify the players’ preference lists:(cid:17)(cid:17)γ v= (((cid:12)d ∨ ¬tick ∨ ¬ass, 5),(((cid:3)winv ∧ Tv), 4),((¬(cid:3)winv ∧ ¬Tr ∧ Tv), 3),((¬(cid:3)winv ∧ Tr ∧ Tv), 2),138N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142(cid:17)(cid:17)γ r((¬(cid:3)winv ∧ Tr ∧ ¬Tv), 1),((cid:13), 0)),= (((cid:12)d ∨ ¬tick ∨ ¬ass, 4),((¬(cid:3)winv ∧ Tr ∧ Tv), 3),((¬(cid:3)winv ∧ ¬Tr ∧ ¬Tv), 2),(((cid:3)winv ∧ Tr), 1),((cid:13), 0)).Proposition 24. Let φ be a QSAT3-formula in nnf.(a) For any N ∈ Nrs, we have that (sd, s(b) φ is satisfiable iff there is an N ∈ Nrs such that for all s ∈ N E( (cid:17)Md) ∈ N E( (cid:17)M(cid:7)(c) φ is satisfiable iff there is an N ∈ Ns such that for all s ∈ N E( (cid:17)M(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) for any strategy sd and sd(cid:7) with sd(q0) = s(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) with out (cid:18)M(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) with out (cid:18)M(cid:17)φ (cid:2)N(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧(cid:17)φ (cid:2)N(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧(cid:7)d(q0) = (cid:13).ass ∧ tick.ass ∧ tick.Proof.(a) The action profile (sd, sd which satisfies (cid:12)d if the transition ((cid:13), (cid:13)) is not regimented, and the 0 which satisfies ¬tick if it is regimented. On both paths the payoff for both players is maximal. No player can d) yields the path q0qω(cid:7)path qωimprove its payoff.(cid:17)φ (cid:2)N(q0, s) |=LTL (ass ∧ tick) → (¬(cid:3)winv ∧ ¬ (cid:12) d).(cid:17)φ (cid:2)N(q0, (sv, sr)) |=LTL Tr → (Tv ∧ (cid:3)winv) and for all paths λ ∈ (cid:8)(cid:18)M(b) “⇒”: Suppose φ holds. By Lemma 23 there is an N ∈ Ns such that for all strategies sr of r with sr(q0) = ⊥ there is (cid:17)φ (cid:2)N(q0) we have a strategy sv of v such that out (cid:18)M(cid:17)φ (cid:2)N(q0, s) |=LTL ass ∧ tick →that λ |= ass ∧ tick. Now suppose that there was a Nash equilibrium (sv, sr) such that out (cid:18)M(¬(cid:3)winv ∧ ¬ (cid:12) d). As ass ∧ tick is true on all paths, this means that out (cid:18)M(cid:17)φ (cid:2)N(q0, s) |=LTL (¬(cid:3)winv ∧ ¬ (cid:12) d) for this Nash equilibrium. We can use the same reasoning as in the proof of Proposition 20(b) to obtain a contradiction. Hence, such a Nash equilibrium cannot exist.“⇐”: Suppose φ does not hold. We have to show that for all N ∈ Nrs there is an s ∈ N E( (cid:17)M((cid:9)) out(cid:18)MFirstly, suppose that (cid:17)M(cid:17)φ (cid:2)N(q0) with λ |= ¬tick. This path can be generated by some strategy profile s which satisfies ((cid:9)), as the antecedent of ((cid:9)) will be false. As this strategy profile gives maximal utility it is a Nash equilibrium. Thus, from now on we can assume that all paths in the norm updated model satisfy tick. Completely analogous, we can also assume that ass is satisfied in the updated models.(cid:17)φ (cid:2) N such that all paths λ ∈ (cid:8)(cid:18)MSecondly, consider (cid:17)M(cid:17)φ (cid:2)N(q0) satisfy tick ∧ ass. As φ does not hold, by Lemma 23 and our assumption about the updated models we have that for all N ∈ Nrs there is a strategy sr of r with sr(q0) = ⊥ such (cid:17)φ (cid:2)N(q0, (sv, sr)) |=LTL Tr ∧ (Tv → ¬(cid:3)winv). The rest of the proof follows analogously that for all strategies sv of v, out (cid:18)M(cid:7)(cid:7)v(q0) = ⊥. By Lemma 23, the v, sr)) |=LTL Tv and sto Proposition 20(b): let sverifier cannot change its strategy to ensure (cid:3)winv. Moreover, as sr(q0) = ⊥ the verifier v cannot deviate to increase (cid:7)its payoff. Analogously, as sv)is indeed a Nash equilibrium. Thus we have that there is an s ∈ N E( (cid:17)M(cid:17)φ (cid:2)N(q0, s) |=LTL¬(cid:3)winv ∧ ¬ (cid:12) d.(cid:7)v(q0) = ⊥, the refuter cannot deviate from sr to increase its payoff. This shows that (sr, s(cid:7)v be any strategy such that out (cid:18)M(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) with out (cid:18)M(cid:17)φ (cid:2) N contains a path λ ∈ (cid:8)(cid:18)M(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) such that (cid:17)φ (cid:2)N(q0, (s(c) Follows immediately from (b) and Lemma 23. (cid:2)Theorem 25 (Hardness strong implementation: existence, sanctioning). Let (I, q) be a pointed NIS and N ∈ {Nrs, Ns}. The problem whether SIN E (I, q, f ) (cid:16)= ∅ is (cid:2)P3 -hard.Proof. First, let N = Nrs and φ be a QSAT3 formula in nnf. We have for f ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r) = ((cid:3)winv ∨ (cid:12)d) ∧ ass ∧ tick:φ satisfiableiff ∃N ∈ N (∀s ∈ N E( (cid:17)Miff ∃N ∈ N (∀s ∈ N E( (cid:17)M(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) : out(cid:18)M(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) : out(cid:18)M(cid:17)φ (cid:2)N(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧ ass ∧ tick(cid:17)φ (cid:2)N(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧ ass ∧ tick(Proposition 24 (b))and N E( (cid:17)M(cid:17)φ (cid:2) N, q0, ((cid:17)(cid:17)γ v, (cid:17)(cid:17)γ r)) (cid:16)= ∅)(Proposition 24(a))N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142139iff SIN E (I, q0, f ) (cid:16)= ∅The case N = Ns follows analogously using Proposition 24(c) in the first step. (cid:2)C.2. Regimentation normsFinally, we consider the case in which N ∈ Nr. We have already seen how to ensure that regimenting norms do not regiment specific transitions, by means of the formula tick. However, we can no longer use the previous construction, based on sanctioning norms, to encode a truth assignment. We have to find a way to achieve this with regimenting norms. The idea of this construction consists of two parts:1. Use regimenting norms to simulate truth assignments of variables x1 to xr by removing some of the outgoing transitions of states in {q1, . . . , qr}.2. Ensure that no other transition is regimented by using a formula which characterizes the structure of (cid:17)M(cid:17)φ .Definition 28 (The model (cid:17)Mfine (cid:17)M(cid:17)φr as (cid:17)M(cid:17)φr ). Let φ ≡ ∃{x1, . . . , xr}∀{xr+1, . . . , xr+m}∃{xr+m+1, . . . , xn}ϕ(x1, . . . , xn) in nnf be given. We de-(cid:17)φ but each state from {q1, q1⊥, q1(cid:13), . . . , qr, qr⊥, qr(cid:13)} is additionally labelled by a fresh proposition rr.For part 1, we observe that the model contains only the three looping states qd , q(cid:13), and q⊥. We introduce the formulaetickr = (cid:2)(((t ∧ ¬rr) → (cid:12)¬t) ∧ ((¬t ∧ ¬rr) → (cid:12)t) ∨ (cid:12)(d ∨ winv ∨ winr))validr = (cid:2)(((t ∧ rr) → (cid:12)¬t) ∧ ((¬t ∧ rr) → (cid:12)t)tick = tickr ∧ validrAs before we use tickr to ensure that no regimentation norm is imposed on any state except from possibly {q1, . . . , qr}. The idea is that some of these transitions in the set may be regimented. A second formula validr is true on a path on which no transition from states in {q1, . . . , qr} which are also contained on the path are regimented. Finally, tick represents that no transition on a path is regimented.Lemma 26. Let N ∈ Nrs be a normative system such that for all paths λ ∈ (cid:8)(cid:18)Ma regimentation norm which is applicable in a state of Q\{qd, q⊥, q(cid:13), q1, q1⊥, q1(cid:13), . . . , qr, qr⊥, qr(cid:13)}.(cid:17)φ (cid:2)N(q0) it holds that λ |= tickr. Then, N cannot contain Lemma 27. Given a QSAT3-formula φ ≡ ∃{x1, . . . , xr}∀{xr+1, . . . , xr+m}∃{xr+m+1, . . . , xn}ϕ(x1, . . . , xn) in nnf we have that φ is satisfiable iff there is an N ∈ Nr such that in (cid:17)M(cid:2) N there is a path from q0 to qr+1 (or to q(cid:14) if r = n), such that for all strategies srof r with sr(q0) = ⊥ there is a strategy sv of v such that out (cid:18)M(q0, (sv, sr)) |=LTL (Tr ∧ validr) → (Tv ∧ (cid:3)winv), and for all paths λ ∈ (cid:8)(cid:18)M(q0) we have that λ |= tickr.(cid:17)φ(cid:2)Nr(cid:17)φr(cid:17)φ(cid:2)NrProof. [Sketch] “⇒”: Suppose φ is satisfiable. Let v be a witnessing valuation of the variables {x1, . . . , xr}. We consider the normative system N which regiments from qi , 1 ≤ i ≤ r, the transition (−, (cid:13)) (resp. (−, ⊥)) if v(xi) = f (resp. v(xi) = t). Now the truth assignment of {xr+1, . . . , xr+m, . . . , xn} induces witnessing strategies following the same reasoning as in Lemma 19. It is also the case that tickr is true on all paths starting in q0.“⇐”: Suppose there is an N ∈ Nr such that for all strategies sr of r with sr(q0) = ⊥ there is a strategy sv of v such that (q0) we have that λ |= tickr and there is (q0, (sv, sr)) |=LTL (Tr ∧ validr) → (Tv ∧ (cid:3)winv), and for all paths λ ∈ (cid:8)(cid:18)M(cid:17)φ(cid:2)Nr(cid:17)φ(cid:2)Nrout(cid:18)Ma path from q0 to qr+1 (or to q(cid:14) if r = n).By Lemma 26 the transitions of all states in which rr does not hold are not affected by the update by the normative system. We can assume wlog that N regiments exactly one outgoing transition for each of the states in {q1, . . . , qr} (it cannot regiment both transitions due to the connectivity property). This is so, because otherwise we move the variable xi , 1 ≤ i ≤ r, of which no transition is regimented to the universally quantified part of φ. Thus, the normative system N induces a truth assignment v of the variables {x1, . . . , xr} as follows: v(xi) = t (resp. v(xi) = f) iff transition (−, ⊥) (resp. (−, (cid:13))) is regimented in qi . Now, we can apply a reasoning similar to that of Lemma 19 wrt. (cid:17)φ[v] and the model (cid:17)M(cid:2) N. We get that (cid:17)φ[v] is satisfiable. The normative systems gives a witnessing truth assignment v of the variables {x1, . . . , xr} showing that φ is satisfiable. (cid:2)(cid:17)φ[v]rIn the previous result it was crucial to assume that the normative system does not regiment both of the transitions outgoing of some state q1, . . . , qr . In the following we have to ensure that normative systems which do not respect this condition, yield some “bad” Nash equilibrium. Therefore, we define the following preference lists:140N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142γ ∗v= (((cid:12)d ∨ ¬tick, 5),(((cid:3)winv ∧ Tv), 4),((¬(cid:3)winv ∧ ¬Tr ∧ Tv), 3),((¬(cid:3)winv ∧ Tr ∧ Tv), 2),((¬(cid:3)winv ∧ Tr ∧ ¬Tv), 1),((cid:13), 0)),γ ∗r= (((cid:12)d ∨ ¬tickr, 4),((validr ∧ ¬(cid:3)winv ∧ Tr ∧ Tv), 3),((validr ∧ ¬(cid:3)winv ∧ ¬Tr ∧ ¬Tv), 2),((validr ∧ (cid:3)winv ∧ Tr), 1),((cid:13), 0)).Now we are able to show the following result:Proposition 28. Let φ be a QSAT3-formula in nnf and N ∈ Nr.(a) We have that (sd, s(b) φ is satisfiable iff there is an N ∈ Nr such that for all s ∈ N E( (cid:17)Mr )) for any strategy sd and s(cid:2) N, q0, (γ ∗(cid:2) N, q0, (γ ∗v , γ ∗d) ∈ N E( (cid:17)M(cid:7)(cid:17)φr(cid:17)φr(cid:7)d with sd(q0) = sv , γ ∗(cid:7)d(q0) = (cid:13).r )) we have that out (cid:18)M(cid:17)φ(cid:2)Nr(cid:12)d) ∧ tick.Proof.(q0, s) |=LTL ((cid:3)winv ∨(a) Either we end up in path q0qω0 . The former satisfies tick ∧ (cid:12)d, the latter satisfying ¬tick.(b) “⇒”: Suppose φ is satisfiable. Then, we can apply Lemma 27. That is, let N ∈ Nr such that in (cid:17)Md or in qω(cid:17)φr(cid:7)(cid:17)φ(cid:2)Nr(cid:17)φ(cid:2)Nr(cid:17)φ(cid:2)Nr(cid:17)φ(cid:2)Nr(cid:17)φ(cid:2)Nr(q0, (sv, s(q0, s) |=LTL tick. Then, also out (cid:18)M(q0, s) (cid:16)|=LTL tick. Then, we also have out (cid:18)Mr)) |= Tr ∧ validr. Second, assume that out (cid:18)M(cid:17)φ(cid:2)Nr(q0, s) |=LTL tick → (¬(cid:3)winv ∧ ¬ (cid:12) d).(q0, (sv, sr) |=LTL (Tr ∧ validr) → (Tv ∧ (cid:3)winv) and for all paths λ ∈ (cid:8)(cid:18)M(cid:2) N there is a path from q0 to qr+1 (or to q(cid:14) if r = n), for all strategies sr of r with sr(q0) = ⊥ there is a strategy sv of v such that (q0) we have that λ |= tickr. Now out(cid:18)Msuppose there were a Nash equilibrium s = (sv, sr) with out (cid:18)M(q0, s) (cid:16)|=LTL tickr by Lemma 27 and the fact First, assume that out (cid:18)M(cid:7)that validr ∧ tickr → tick holds on the path. Thus, the refuter would be better of to deviate to a strategy sr such that (q0, s) |=LTL validr. out(cid:18)MIf sv(q0) = (cid:13) then the refuter can increase its payoff by also deviating to sr(q0) = (cid:13); so, let us suppose that sv(q0) = ⊥. We can also assume that the verifier plays a consistent strategy, otherwise it would deviate to one. In that case we can also assume that sr is consistent; otherwise, the refuter can again deviate to a consistent strategy to increase its payoff. Then, however, by Lemma 27 the verifier can deviate to a better strategy that satisfies (cid:3)winv. If sr(q0) = (cid:13) then we can also assume that sv(q0) = (cid:13); otherwise, the verifier would deviate to such a strategy. This contradicts the existence of a Nash equilibrium with the above stated properties.“⇐”: Suppose φ does not hold. We show that for all N ∈ Nrout(cid:18)Mi.e. that ((cid:9)) for all s ∈ N E( (cid:17)Mv , γ ∗As φ does not hold, we have according to Lemma 27 that for all N ∈ Nr it holds that:(i) there is no path from q0 to qr+1 (or to q(cid:14) if r = n) in (cid:17)M(ii) there is a path λ ∈ (cid:8)(cid:18)M(cid:17)φ(cid:2)Nr(iii) there is a strategy sr of r with sr(q0) = ⊥ such that for all strategies sv of v it holds that ((cid:9)) out (cid:18)Mr )) with (q0, s) |=LTL tick → (¬(cid:3)winv ∧ ¬ (cid:12) d). We suppose, for the sake of contradiction, that this is not the case, (q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧ tick.r )) we have that out (cid:18)Mthere is an s ∈ N E( (cid:17)M(q0) such that λ (cid:16)|= tickr; or(cid:2) N, q0, (γ ∗(cid:2) N, q0, (γ ∗v , γ ∗(cid:2) N;(cid:17)φ(cid:2)Nr(cid:17)φ(cid:2)Nr(cid:17)φ(cid:2)Nr(cid:17)φ(cid:2)Nr(q0,(cid:17)φr(cid:17)φr(cid:17)φr(cid:17)φ(cid:2)Nr(sv, sr)) |=LTL (Tr ∧ validr) ∧ (Tv → ¬(cid:3)winv).First, suppose that (i) holds. Consider the strategies sr and sv with sr(q0) = sv(q0) = ⊥ which end in the looping (q0, s) |=LTLstate in-between q0 and qr+1 (or to q(cid:14) if r = n) which has to exist by assumption. We have that out (cid:18)M¬tick ∧ ¬validr. Thus, (sv, sr) is a Nash equilibrium which contradicts ((cid:9)).Secondly, suppose that (ii). As for (i) suppose both players play sr and sv with sr(q0) = sv(q0) = ⊥ which yield the very path λ with λ (cid:16)|= tickr. On this path it holds that ¬tick and ¬tickr, thus no player has an incentive to deviate from it.Thirdly, suppose that (iii) holds. Let sr be a strategy as defined above, and sv such that it is consistent and sv(q0) = ⊥. By Lemma 27 the verifier cannot change its strategy to a consistent one which ensures (cid:3)winv. The player would also (cid:17)φ(cid:2)NrN. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–142141not deviate to an inconsistent one. Moreover, no player can deviate to ensure a path with (cid:12)d. We can also assume that (i) and (ii) do not hold. Thus, neither the verifier nor the refuter can deviate to a strategy such that the outcome path satisfies ¬tick nor ¬tickr, respectively. Thus, this strategy profile is a Nash equilibrium, contradicting ((cid:9)). (cid:2)Theorem 29 (Hardness of strong implementation: existence, regimentation norms). Let (I, q) be a pointed NIS and N ∈ {Nr}. The problem whether SIN E (I, q, f ) (cid:16)= ∅ is (cid:2)P3 -hard.Proof. Let φ be a QSAT3 formula in nnf. We have for f (γ ∗v , γ ∗r ) = ((cid:3)winv ∨ (cid:12)d) ∧ tick:φ satisfiableiff ∃N ∈ N (∀s ∈ N E( (cid:17)Miff ∃N ∈ N (∀s ∈ N E( (cid:17)M(cid:17)φr(cid:17)φr(cid:2) N, q0, (γ ∗(cid:2) N, q0, (γ ∗v , γ ∗v , γ ∗r )) : out(cid:18)Mr )) : out(cid:18)M(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧ tick(Proposition 28 (b))(q0, s) |=LTL ((cid:3)winv ∨ (cid:12)d) ∧ tick(cid:17)φ(cid:2)Nr(cid:17)φ(cid:2)Nrand N E( (cid:17)Miff SIN E (I, q0, f ) (cid:16)= ∅(cid:2) N, q0, (γ ∗v , γ ∗(cid:2)(cid:17)φrr )) (cid:16)= ∅)(Proposition 28 (a))References(2009) 2629–2652.[1] T. Ågotnes, W. van der Hoek, M. Wooldridge, Normative system games, in: Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS ’07, ACM, New York, NY, USA, 2007, pp. 1–8.[2] T. Ågotnes, W. van der Hoek, M. Woolridge, Robust normative systems and a logic of norm compliance, Log. J. IGPL 18 (2010) 4–30.[3] T. Ågotnes, M. Wooldridge, Optimal social laws, in: Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1, AAMAS ’10, International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 2010, pp. 667–674, http://dl.acm.org/citation.cfm?id=1838206.1838294.[4] H. Aldewereld, V. Dignum, Operetta: organization-oriented development environment, in: Languages, Methodologies, and Development Tools for Multi-Agent Systems – Third International Workshop, LADS 2010, Lyon, France, August 30–September 1, 2010, pp. 1–18, revised selected papers.[5] H. Aldewereld, J. Vázquez-Salceda, F. Dignum, J.C. Meyer, Verifying norm compliancy of protocols, in: O. Boissier, J.A. Padget, V. Dignum, G. Lindemann, E.T. Matson, S. Ossowski, J.S. Sichman, J. Vázquez-Salceda (Eds.), Coordination, Organizations, Institutions, and Norms in Multi-Agent Systems, in: Lecture Notes in Computer Science, vol. 3913, Springer, 2006, pp. 231–245.[6] N. Alechina, M. Dastani, B. Logan, Programming norm-aware agents, in: Proceedings of the 11th International Conference on Autonomous Agents and [7] N. Alechina, M. Dastani, B. Logan, Reasoning about normative update, in: Proceedings of the Twenty Third International Joint Conference on Artificial [8] N. Alechina, M. Dastani, B. Logan, Norm approximation for imperfect monitors, in: Proceedings of the 13th International Conference on Autonomous Multiagent Systems, AAMAS 12, vol. 2, 2012, pp. 1057–1064.Intelligence, IJCAI 2013, AAAI Press, 2013, pp. 20–26.Agents and Multiagent Systems, AAMAS 2014, 2014.[9] R. Alur, T.A. Henzinger, O. Kupferman, Alternating-time temporal logic, J. ACM 49 (2002) 672–713.[10] F. Arbab, Abstract behavior types: a foundation model for components and their composition, in: F. de Boer, M. Bonsangue, S. Graf, W.-P. de Roever (Eds.), Formal Methods for Components and Objects, in: LNCS, vol. 2852, Springer-Verlag, 2003, pp. 33–70.[11] L. Astefanoaei, M. Dastani, J.-J.C. Meyer, F. Boer, On the semantics and verification of normative multi-agent systems, Int. J. Univers. Comput. Sci. 15 (13) [12] J. Baumfalk, B. Poot, B. Testerink, M. Dastani, A sumo extension for norm based traffic control systems, in: Proceedings of the SUMO2015 Intermodal Simulation for Intermodal Transport, DLR, Berlin, Adlershof, 2015, pp. 63–83.[13] P. Blackburn, J. Bos, K. Striegnitz, Learn Prolog Now!, Texts in Computing, vol. 7, College Publications, 2006.[14] G. Boella, L. van der Torre, Regulative and constitutive norms in normative multiagent systems, in: Proceedings of the Ninth International Conference on Principles of Knowledge Representation and Reasoning, KR’04, 2004, pp. 255–266.[15] G. Boella, L.W.N. van der Torre, Enforceable social laws, in: The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems, AMAAS 2005, 2005, pp. 682–689.[16] O. Boissier, R. Bordini, J. Hübner, A. Ricci, A. Santi, Multi-agent oriented programming with jacamo, Sci. Comput. Program 78 (6) (2011) 747–761.[17] F. Brazier, C. Jonker, J. Treur, Compositional design and reuse of a generic agent model, Appl. Artif. Intell. J. 14 (2000) 491–538.[18] N. Bulling, M. Dastani, Normative mechanism design (extended abstract), in: Proceedings of the 10th International Conference on Autonomous Agents and Multi-Agent Systems, AAMAS 2011, ACM Press, Taipei, Taiwan, May 2011, pp. 1187–1188.[19] N. Bulling, M. Dastani, Verification and implementation of normative behaviours in multi-agent systems, in: Proc. of the 22nd Int. Joint Conf. on [20] N. Bulling, M. Dastani, M. Knobbout, Monitoring norm violations in multi-agent systems, in: Twelfth International Conference on Autonomous Agents Artificial Intelligence, IJCAI, Barcelona, Spain, July 2011, pp. 103–108.and Multi-Agent Systems, AAMAS’13, 2013, pp. 491–498.[21] N. Bulling, J. Dix, Modelling and verifying coalitions using argumentation and ATL, Intel. Artif. 14 (46) (March 2010) 45–73.[22] N. Bulling, W. Jamroga, Verifying agents with memory is harder than it seemed, AI Commun. 23 (4) (December 2010) 389–403.[23] N. Bulling, W. Jamroga, J. Dix, Reasoning about temporal properties of rational play, Ann. Math. Artif. Intell. 53 (1–4) (2009) 51–114.[24] H.L. Cardoso, E. Oliveira, Electronic institutions for b2b: dynamic normative environments, Artif. Intell. Law 16 (1) (2008) 107–128.[25] M. Comuzzi, I. Vanderfeesten, T. Wang, Optimized cross-organizational business process monitoring: design and enactment, Inf. Sci. 244 (2013) 107–118.[26] N. Criado, E. Argente, V. Botti, Open issues for normative multi-agent systems, AI Commun. 24 (3) (2011) 233–264.[27] M. Dastani, J.-J.C. Meyer, D. Grossi, A logic for normative multi-agent programs, J. Log. Comput. 23 (2) (2013) 335–354.[28] M. Dastani, N. Tinnemeier, J.-C. Meyer, A programming language for normative multi-agent systems, in: V. Dignum (Ed.), Multi-Agent Systems: Seman-tics and Dynamics of Organizational Models, Information Science Reference, 2009.[29] F. Dignum, Autonomous agents with norms, Artif. Intell. Law 7 (1) (1999) 69–79, http://dx.doi.org/10.1023/A%3A1008315530323.[30] U. Endriss, S. Kraus, J. Lang, M. Wooldridge, Designing incentives for boolean games, in: AAMAS, 2011, pp. 79–86.142N. Bulling, M. Dastani / Artificial Intelligence 239 (2016) 97–1425 (3&4) (2012).[31] M. Esteva, D. de la Cruz, C. Sierra, ISLANDER: an electronic institutions editor, in: Proceedings of the First International Joint Conference on Autonomous Agents and MultiAgent Systems, AAMAS 2002, Bologna, Italy, 2002, pp. 1045–1052.[32] M. Esteva, J. Rodríguez-Aguilar, B. Rosell, J. Arcos, AMELI: an agent-based middleware for electronic institutions, in: Proceedings of AAMAS 2004, New York, US, July 2004, pp. 236–243.[33] M. Esteva, J. Rodriguez-Aguilar, C. Sierra, W. Vasconcelos, Verifying norm consistency in electronic institutions, in: V. Dignum, D. Corkill, C. Jonker, F. Dignum (Eds.), Proceedings of the AAAI-04 Workshop on Agent Organizations: Theory and Practice, AAAI, AAAI Press, San Jose, July 2004, Technical Report WS-04-02.[34] D. Fitoussi, M. Tennenholtz, Choosing social laws for multi-agent systems: minimality and simplicity, Artif. Intell. 119 (1) (2000) 61–101.[35] N. Fornara, M. Colombetti, Specifying and enforcing norms in artificial institutions, in: Proc. of DALT’08, 2009.[36] A. Garcia-Camino, P. Noriega, J.A. Rodriguez-Aguilar, Implementing norms in electronic institutions, in: Proceedings of the Fourth International Joint Conference on Autonomous Agents and MultiAgent Systems, AAMAS’05, New York, NY, USA, 2005, pp. 667–673.[37] G. Gottlob, G. Greco, F. Scarcello, Pure Nash equilibria: hard and easy games, J. Artif. Intell. Res. (2003) 215–230.[38] H. Guo, Automotive Informatics and Communicative Systems: Principles in Vehicular Networks and Data Exchange, Information Science Reference – Imprint of IGI Publishing, Hershey, PA, 2009.[39] J. Hintikka, Logic, Language Games and Information, Clarendon Press, Oxford, 1973.[40] R. Horowitz, P. Varaiya, Control design of an automated highway system, in: Special Issue on Hybrid Systems, Proc. IEEE 88 (7) (2000) 913–925.[41] J. Hübner, O. Boissier, R. Kitio, A. Ricci, Instrumenting multi-agent organisations with organisational artifacts and agents: giving the organisational power back to the agents, Int. J. Auton. Agents Multi-Agent Syst. 20 (2010) 369–400.+[42] J. Hübner, J. Sichman, O. Boissier, S–MOISE: a middleware for developing organised multi-agent systems, in: Proceedings of the International Workshop on Coordination, Organizations, Institutions, and Norms in Multi-Agent Systems, in: LNCS, vol. 3913, Springer, 2006, pp. 64–78.+[43] J. Hübner, J. Sichman, O. Boissier, Developing organised multiagent systems using the MOISEmodel: programming issues at the system and agent levels, Int. J. Agent-Oriented Softw. Eng. 1 (3/4) (2007) 370–395.[44] J.F. Hübner, O. Boissier, R.H. Bordini, From organisation specification to normative programming in multi-agent organisations, in: J. Dix, J. Leite, G. Governatori, W. Jamroga (Eds.), Proceedings of 11th International Workshop on Computational Logic in Multi-Agent Systems, CLIMA XI, Lisbon, Portugal, August 16–17, 2010, in: Lecture Notes in Computer Science, vol. 6245, Springer, 2010, pp. 117–134.[45] A.J.I. Jones, M. Sergot, On the characterization of law and computer systems, in: J.-J.C. Meyer, R. Wieringa (Eds.), Deontic Logic in Computer Science: Normative System Specification, John Wiley & Sons, 1993, pp. 275–307.[46] D. Krajzewicz, J. Erdmann, M. Behrisch, L. Bieker, Recent development and applications of sumo-simulation of urban mobility, Int. J. Adv. Syst. Meas. [47] K. Mahbub, G. Spanoudakis, A framework for requirements monitoring of service based systems, in: Proceedings of the 2nd International Conference on Service Oriented Computing, ICSOC ’04, ACM, New York, NY, USA, 2004, pp. 84–93.[48] F.R. Meneguzzi, M. Luck, Norm-based behaviour modification in BDI agents, in: C. Sierra, C. Castelfranchi, K.S. Decker, J.S. Sichman (Eds.), 8th Interna-tional Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2009, IFAAMAS, 2009, pp. 177–184.[49] A. Metzger, P. Leitner, D. Ivanovic, E. Schmieders, R. Franklin, M. Carro, S. Dustdar, K. Pohl, Comparing and combining predictive business process monitoring techniques, IEEE Trans. Syst. Man Cybern. 45 (2) (2014) 276–290.[50] J. Meyer, R. Wieringa, Deontic Logic in Computer Science: Normative System Specification, Wiley Professional Computing, J. Wiley, 1993.[51] N.H. Minsky, V. Ungureanu, Law-governed interaction: a coordination and control mechanism for heterogeneous distributed systems, ACM Trans. Softw. Eng. Methodol. 9 (3) (2000).[52] J. Morales, M. Lopez-Sanchez, J.A. Rodriguez-Aguilar, M. Wooldridge, W. Vasconcelos, Minimality and simplicity in the on-line automated synthesis of normative systems, in: Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems, AAMAS ’14, International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, 2014, pp. 109–116.[53] J. Morales, M. López-Sánchez, J.A. Rodríguez-Aguilar, M. Wooldridge, W.W. Vasconcelos, Automated synthesis of normative systems, in: International Conference on Autonomous Agents and Multi-Agent Systems, AAMAS ’13, Saint Paul, MN, USA, May 6–10, 2013, pp. 483–490.[54] M. Osborne, A. Rubinstein, A Course in Game Theory, MIT Press, 1994.[55] C. Papadimitriou, Computational Complexity, Addison Wesley, Reading, 1994.[56] A. Pnueli, The temporal logic of programs, in: Proceedings of Foundations of Computer Science, FOCS, 1977, pp. 46–57.[57] A. Ricci, M. Viroli, A. Omicini, Give agents their artifacts: the A&A approach for engineering working environments in MAS, in: E.H. Durfee, M. Yokoo, M.N. Huhns, O. Shehory (Eds.), 6th International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS 2007, IFAAMAS, 2007.[58] Y. Shoham, K. Leyton-Brown, Multiagent Systems – Algorithmic, Game-Theoretic, and Logical Foundations, Cambridge University Press, 2009.[59] Y. Shoham, M. Tennenholtz, On the synthesis of useful social laws for artificial agent societies, in: Proceedings of the Tenth National Conference on Artificial Intelligence, AAAI-92, San Diego, CA, 1992.[60] Y. Shoham, M. Tennenholtz, On social laws for artificial agent societies: off-line design, Artif. Intell. 73 (1–2) (1995) 231–252.[61] V.T. Silva, From the specification to the implementation of norms: an automatic approach to generate rules from norms to govern the behavior of agents, Int. J. Auton. Agents Multiagent Syst. 17 (1) (2008) 113–155.[62] M.P. Singh, M. Arrott, T. Balke, A.K. Chopra, R. Christiaanse, S. Cranefield, F. Dignum, D. Eynard, E. Farcas, N. Fornara, F. Gandon, G. Governatori, H.K. Dam, J. Hulstijn, I. Krueger, H.-P. Lam, M. Meisinger, P. Noriega, B.T.R. Savarimuthu, K. Tadanki, H. Verhagen, S. Villata, The uses of norms, in: G. Andrighetto, G. Governatori, P. Noriega, L.W.N. van der Torre (Eds.), Normative Multi-Agent Systems, in: Dagstuhl Follow-Ups, vol. 4, Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, Dagstuhl, Germany, 2013, pp. 191–229.[63] I. Sommerville, D. Cliff, R. Calinescu, J. Keen, T. Kelly, M. Kwiatkowska, J. Mcdermid, R. Paige, Large-scale complex it systems, Commun. ACM 55 (7) (2012) 71–77, http://doi.acm.org/10.1145/2209249.2209268.[64] N. Tinnemeier, M. Dastani, J.-J.C. Meyer, L. van der Torre, Programming normative artifacts with declarative obligations and prohibitions, in: Proceedings of IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, IEEE Computer Society, 2009, pp. 145–152.[65] W. van der Hoek, M. Roberts, M. Wooldridge, Social laws in alternating time: effectiveness, feasibility, and synthesis, Synthese 156 (1) (2007) 1–19.[66] M.B. van Riemsdijk, K.V. Hindriks, C.M. Jonker, Programming organization-aware agents: a research agenda, in: Proceedings of the Tenth International Workshop on Engineering Societies in the Agents’ World, ESAW’09, in: LNAI, vol. 5881, Springer, 2009, pp. 98–112.[67] K.W. Wagner, Bounded query classes, SIAM J. Comput. 19 (5) (1990) 833–846.[68] M. Wooldridge, An Introduction to Multiagent Systems, 2nd edition, Wiley, Chichester, UK, 2009.[69] M. Wooldridge, U. Endriss, S. Kraus, J. Lang, Incentive engineering for boolean games, Artif. Intell. 195 (2013) 418–439, http://www.sciencedirect.com/[70] F. Zambonelli, N. Jennings, M. Wooldridge, Organizational abstractions in the analysis and design of multi-agent systems, in: First International Work-science/article/pii/S0004370212001518.shop on Agent-Oriented Software Engineering at ICSE, 2000.