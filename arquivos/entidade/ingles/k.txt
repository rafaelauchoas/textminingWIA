ELSEVIER Artificial Intelligence 9 1 ( 1997 ) 15% 17 1 Artificial Intelligence k-Certainty Exploration Method: an act .ion selector to identify the environment in reinforcement learning Kazuteru Miyazaki *, Masayuki Yamamura ‘, Shigenobu Kobayashi * Department of Computational Intelligence and Systems Science, Interdisciplinary Graduate School of Science and Engineering, Tokyo Institute of Technology, 4259, Nagatsuta, Midori-ku, Yokohama, 226 Japan Received April 1996; revised July 1996 Abstract Reinforcement learning aims to adapt an agent to an unknown environment according to rewards. is a representative There are two issues to handle delayed reward and uncertainty. Q-learning reinforcement learning method. It is used in many works since it can learn an optimum policy. However, Q-learning needs numerous trials to converge to an optimum policy. If the target environments can be described in Markov decision processes, we can identify them from statistics of sensor-action pairs. When we build the correct environment model, we can derive an optimum policy with the Policy Iteration Algorithm. Therefore, we can construct an optimum policy through identifying environments efficiently. We separate the learning process into two phases: identifying an environment and determining an optimum policy. We propose the k-certainty Exploration Method for identifying an environment. After that, an optimum policy is determined by the Policy Iteration Algorithm. We call a rule k- certainty if and only if it has been selected k times or more. The k-Certainty Exploration Method excepts any loop of rules that already achieve k-certainty. We show its effectiveness by comparing it with Q-learning the other is an original environment where an optimum policy varies according to a parameter. @ 1997 Elsevier Science B.V. in two experiments. One is Sutton’s maze-like environment, Keywords: Reinforcement k-Certainty Exploration Method learning; Q-learning; Markov decision processes; Policy Iteration Algorithm; author. E-mail: * Corresponding ’ E-mail: my@dis.titech.ac.jp. * E-mail: kobayasi@dis.titech.ac.jp. tem@fe.dis.titech.ac.jp. 0004-3702/97/$17.00 I’IISOOO4-3702(96)00062-S @ 1997 Elsevier Science B.V. All rights reserved. 156 K. Miyazaki et al. /Artijicial Intelligence 91 (1997) 155-171 1. Introduction Reinforcement learning (RL) is a kind of machine learning. It aims to adapt an to to an unknown agent handle: delayed policy efficiently. Yamamura the exploitation exploitation [4,5,8,11,12] rational policy quickly, intensive intensive environment with a clue to rewards. There are two issues reward and uncertainty. The purpose of RL is to acquire an optimum type and [23] divides RL systems the other into is the exploration two approaches. One is type. The an experience of getting a reward. Profit sharing learn a type. Though intensive intensive it can exploitation type emphasizes is a representative The exploration intensive type emphasizes it does not always guarantee the optimality. collection of all experiences [21] as the exploration intensive the optimality. We regard Q-learning learn an optimum policy has attracted many researchers Connell Ballard environment [lo], Takemichi [ 22]), it needs numerous becomes more complex, the environment In MDPs, in Markov decision processes (MDPs). Though Q-learning (for example, Clouse and Utgoff and Kakazu [ 171, Unemi et al. [3], Mahadevan [ 201, and Whitehead and and precisely. As the trials to identify the environment the number of trials increases remarkably. can be statistically has been identified the gathering of identified precisely, a Policy Iteration through to guarantee type since it can If the environment samples of actions. Algorithm (PIA) can be used to find an optimum policy the environment can identify [ 11, that is a computational procedure based on dynamic programing, [ 151. Combining PIA with an algorithm which efficiently, we can achieve the purpose of RL. efficiently, we should give a priority the environment If we want to identify whose number of selection one of such algorithms. environment to getting a reward. To make matters worse, it cannot handle delayed reward. in point of identification for an action which has been contributed times is the fewest. The Interval Estimation Method [6] to an action is of the It may select useless actions it gives the same priority because In this paper, we propose an algorithm which can identify We show its effectiveness by comparing it with Q-learning the environment in two experiments. efficiently. the method, and notations. Section 3 proposes that Section 2 describes the problem, the k-Certainty Exploration Method is an action selector efficiently. Section 4 shows numerical to reveal Certainty Exploration Method. examples to identify the effectiveness the environment of the k- 2. The domain 2.1. The target problem Consider an agent in some unknown environment. At each time step, it gets informa- through its sensors and chooses an action to take. As a result it gets a reward from the environment. A pair of a sensory is called a trial. The function the is called a policy. The policy which maximizes tion about the environment of some sequence of actions, input and an action that maps sensory expected is called an optimum policy. is called a ncle. Executing reward per action to actions a rule inputs K. Miyazaki et al. /Artificial Intelligence 91 (1997) 155-l 71 157 Table 1 Classification of the existing work variables state transitions discrete continuous Markov Class 1 Class 2 non-Markov Class 3 Class 4 In general, it must learn based on imperfect limited because addition to a part of the environment. a correct action does not always to these issues, we can classify There are a lot of works that assume information Furthermore, because it must handle to getting a reward is input reward In the existing work in RL as shown in Table 1. the property of state transitions is treated as stochastic processes, where a sensory its sensory the delayed is Ma&Gun. immediately. lead to some state and an action In this case, the environment corresponds [2] extends RL into the Linear Quadratic Regulation (LQR) which belongs Markovian some dynamical input operator. Bradtke to non- the works of Tan [ 181 and Tenenberg et al. [ 191 treat environments. Though environment, they are numerical to some state examples. transition Many works assume that the range of variables senses a set of discrete attribute-value varieties. Though the work of Lin [9] level. Bradtke al. [7] propose Stochastic Hill Climbing which assures environments. investigates continuous variables [2] pairs and performs an action treats continuous variables, is discrete. In this case, the agent in some discrete it remains prototype in the LQR domain. Kimura et the rationality in continuous Here, we treat Class 1 in Table 1. Class 1 is a good domain in this class. of our method we describe traditional works, we will describe our approach. there are a lot of works since the general framework of the RL system. After showing to show the effectiveness In the next subsection, the position of 2.2. The general framework and our approach into the RL system (Fig. 1). After In this paper, we divide selector and Learner conjicting set of rules is made which match with the current sensory selector decides learner reinforces and acquired targets of RL systems action selector and the learner since the state recognizer three parts, State recognizer, Action a input. The action set. The to get more rewards based on the executed actions to the in Class 1 are restricted is given a priori. some parameters rewards. The design from the conflicting the state recognizer to the environment the environment, to execute the rule senses The purpose of RL is to acquire an optimum policy efficiently. Therefore, we must into two In this paper, we classify RL systems and the efficiency. the optimality pursue approaches. An approach that pursues the optimality is called the exploration it should be emphasized a representative RL system. We classify guarantees the optimality to identify the environment it into in MDPs. We show the framework of Q-learning intensive efficiently. Q-learning intensive the exploration type, where is known as it type since in Fig. 2. 158 K. Miyazaki et al. /Artificial Intelligence 91 (1997) 155-I 71 Fig. 1. Framework of reinforcement learning systems. ~ !action!) 2 -- 4 ,.....,,..I,,..,,.. 1 reward .*.......... i I m Fig. 2. Framework of learning systems baed on Q-learning. it does not directly to every state-action a Q-value is accumulated In Q-learning, with the environment. Q-learning because To make matters worse, we do not know how to decide action selectors the convergence An approach interactions pair through trials to acquire an optimum policy, it through Q-values. but it evaluates to accelerate needs numerous the environment of Q-values. that pursues the exploitation the efficiency is called intensive identify in getting a reward. Profit sharing types. Though learn a rational it can it does not always acquire an optimum type, is a it makes many accounts of experiences where representative method of exploitation policy quickly under some conditions policy. intensive [ 11,121, In Class 1, if the environment find an optimum policy. Therefore, efficiently, environment has been identified precisely, PIA can be used if we can build an algorithm which identifies to the the purpose of RL can be achieved easily. In the next section, we propose the k-Certainty Exploration Method which is an action the k-Certainty Exploration efficiently. Combining the environment to identify selector Method with PIA, it is possible to acquire an optimal policy efficiently. K. Miyazaki et al./Art$cial Intelligence 91 (1997) 155-I 71 159 0 : sensory input 0 : current sensory input O+- : k-Certainty rule 0..+: k-uncertainty rule Fig. 3. An example of a k-certain looped rule. 3. k-Certainty Exploration Method 3.1. The basic idea We propose an algorithm which can identify for efficient as possible. identification of the environment the environment efficiently. to select all rules uniformly It is important and as many We call a rule k-certainty if and only call a rule which is not k-certainty k-uncertainty. A sensory We call a current sensory sensed until that time as a known state. input as a current state and a sensory if it has been selected k times or more. We input is regarded as a state. input which has been Considering a case which returns rule in the current state. We call a rule a k-certain looped rule if and only if the rule leads surely rules. For example, though rule 0 and rule 1 in Fig. 3 can select in the current state, only rule 1 is a k-certain looped rul!e. to a current state after selection of a k-certain only by k-certain to a k-certain is constructed loop which If we select a k-certain looped rule, it is interrupted to uniform looped rules from a candidate selection of rules. It for is the basic concept selection. in this paper to except k-certain In the next subsection, we propose to identify selector rules. Furthermore, we propose action looped Exploration Method the environment to learn an optimum policy. a learning efficiently the k-Certainty Exploration Method which is an through exception of k-certain the k-Certainty system based on 3.2. The proposed method The algorithm of the k-Certainty Exploration Method is shown in Fig. 4. It consists of an action selector based on k-Certainty Exploration which controls level of the environment. the identification and updating a certainty level k k-Certainty Exploration. of the state transition probabilities estimated by k-Certainty Exploration builds a maximum likelihood model -- and the expectation of rewards ry. q$ and T are g 160 K. Miyazaki et al. /Art#cial Intelligence 91 (1997) 155-171 procedure k-certainty exploration method begin if there are any l-uncertainty else if all known rules are k-Certainty then k:=k+l. rules in current state then k:=l. begin if there are any k-uncertainty rules in current state then select the one of these rules at random. else rise all flags in all known states. for all states except for current state do lf there are any k-uncertainty rules or rules that can transfer to the state whose flag is down then be down the flag in the state. while there is a state whose flag is down. select the one of rules that can transfer to the state whose flag is down at random. end. end; Fig. 4. Algorithm of the k-Certainty Exploration Method. + p= I the number of transitions from state i to j by executing rule a the number of executing rule a in state i , the sum of immediate rewards by executing rule a in state i the number of executing rule a in state i k-Certainty Exploration uses flags to except k-certain to each state. The number of flags is unknown looped rules from a candidate in advance is unknown. We show the algorithm of for selection. A flag is assigned because k-Certainty Exploration the number of state in the environment below. If there are k-uncertain rules in a current state, one of these rules it is necessary to avoid repeatedly random. Otherwise, so, all flags of known states are raised. Flags of states that can select k-uncertain are got down. Furthermore, got down. This process transfer selects at random one of the rules which can transfer rules. Therefore, to the flag-down states. to flag-down states can not meet any k-uncertain until no flag can be got down. Rules flags of states that can transfer selecting k-certain to flag-down is continued is selected at rules. To do rules states are also that cannot the algorithm How to update k. The k-Certainty Exploration Method controls of the environment which can select new state is perceived, k is reset to an initial value. Because the new state should be increased level rules, it, become k-certainty, k is set to k + 1. If a the sampling number of the identification level k. Initially, k is set to 1. If all known in states how to transit by a certainty selectively. K. Miyazaki et al. /Artificial Intelligence 91 (I 997) 155-l 71 161 Fig. 5. Framework of learning systems based on k-Certainty Exploration Method. A learning system based on the k-Certainty Exploration Method. We show a leam- ing system based on the k-Certainty Exploration Method in Fig. 5. It is divided into two parts, identifying the environment and deciding a policy. The part of identifying the environment is the k-Certainty Exploration Method. The part of deciding a policy is PIA. When all rules become k-certainty, PIA is applied. This policy is reliable in the sense that all rules select at least k times. 3.3. An example We show a behavioral example of the k-Certainty Exploration Method. Consider the case that the agent can execute two actions and it senses SO for the first time (Fig. 6- 1st). Initially, k is set to 1. Since all rules are l-uncertain, for example rule 0 is selected at random. Then, a new sensory input S3 has been sensed (Fig. 6-2nd). Since all rules are l-uncertain in S3, for example rule 2 is selected at random and S3 has been sensed again (Fig. 6-3rd). Rule 2 is already l-certain in S3. Therefore, rule 3 is sure to be selected. As a result, SO has been sensed and it gets a reward at the same time. Similarly, after rule 1 and rule 5 are selected (Fig. 6-4th and 5th), SO has been sensed again (Fig. 6-6th). Since all rules in SO are l-certain, it cannot decide which rule should be selected in SO. In this case, the k-Certainty Exploration Method works to except l-certain looped rules from a candidate for selection. First, all flags of known states, SO, SI and S3, are raised. The flag of Sl is put down because of the existence of a l-uncertain rule in Sl. Therefore, it selects rule 1 and senses Sl (Fig. 6-7th). Note that there is a case that another sensory input has been sensed because of stochastic state transitions. Since rule 5 is already l-certain in Sl, rule 6 is sure to be selected. As a result, a new 162 K. Miyazaki et al. /Artijicial Intelligence 91 (1997) 155-l 71 8th 10th 4th 5th \.... 11th 6th L.... w ..f ‘thQx&&@.. w 12th SO,Sl, . . . . sensory input w ; rule V ; reward Fig. 6. Execution of the k-Certainty Exploration Method. sensory for example sensed again input S2 has been sensed (Fig. 6-8th). rule 7 is selected at random. (Fig. 6-9th). in S2, Since all rules are l-uncertain It gets a reward at that time and SO has been Since all rules in SO are l-certain, the k-Certainty Exploration Method works to flags in SO, Sl, S2 and S3 are raised. The flag looped l-certain is put down because Sl can rules. First, except of S2 is put down because of the existence of a l-uncertain the flag of Sl flag-down Sl, rule 6 has been selected and S2 has been sensed again already (Fig. 6-12th). in the in looped rules. Then, lth). Since rule 7 is in S2, rule 8 is sure to be selected. Then, SO has been sensed again rule in S2. Furthermore, is already the k-Certainty Exploration Method works to except it selects rule 1 and senses Sl (Fig. 6-10th). l-certain (Fig. 6-l state. Therefore, to S2 which Similarly, l-certain transfer K. Miyazaki et al./Artij?cial Intelligence 91 (1997) 155-l 71 163 All rules become l-certain. If we use PIA, we can find an optimum policy which selects rule 0 in SO and rule 3 in S3. The concrete algorithm of PIA is shown in Appendix A. If we want to update identification levels of the environment, we should repeat the same processes on k = 2. 3.4. Features of the k-Certainty Exploration Method The k-Certainty Exploration Method has the following features: ( 1) Eficient identijication of the environment. We can expect to identify the environ- ment efficiently to except k-certain looped rules from a candidate for selection. (2) Complete identification of the environment in deterministic MDPs. If all rules in deterministic MDPs, the environment has been identified l-certain become completely. (3) Progressive identification of the environment in stochastic MDPs. In stochastic MDPs, we can control identification levels of the environment by updating a certainty level k. (4) Independency on reward. The k-Certainty Exploration Method is not influenced by positions of rewards. It reflects on our belief that using rewards does not always lead to efficient identification of the environment. (5) Palynomial computational costs. The numerical cost of memory is 0(mn2) and the cost of time is 0( mn3) where m is the number of actions and n is the number of sensory inputs. They are polynomial in time. PIA can find an optimum policy in polynomial time too [ 141. The k-Certainty Exploration Method has many features. In the next section, we show the effectiveness of the k-Certainty Exploration Method by comparing it with other methods. 4. Evaluation of k-Certainty Exploration Method 4.1. Estimation in a multi-return environment The k-Certainty Exploration Method identifies the environment efficiently to except k-certain looped rules from a candidate for selection. We confirm the effectiveness of this feature. By the. way, we can consider two criteria for a general learning system. One is to acquire an optimum policy efficiently, the other is to decrease numerical costs for deciding the next action. The former corresponds to decreasing the number of trials, the latter to decreasing the time spent to decide the next action. In general, for RL systems, the former is important. Because the trial in an unknown environment is accompanies by risks which cannot be predicted easily. On the other hand, numerical costs are easy to predict. Therefore, we evaluate IU systems based on the number of trials. We call an action selector which always selects the fewest sampling rule in the current state a Minimal Select Method. Though the k-Certainty Exploration Method is a slightly more complex method, the Minimal Select Method is a very simple method. However, it shows an exponential explosion on the number of trials in some environment. 164 K. Miyazaki et al. /ArhBcial Intelligence 91 (1997) 155-l 71 -... G Sl 52 . . . . . . s* sL > . ..** v SO,Sl. ; sensory input W ; rule V ; reward Fig. 7. A multi-return environment. Considering this environment, multi-return the environment shown it forces a return in Fig. 7, if the agent selects special to SO immediately. We call this environment rules in the environment. If we want to raise a certainty level k by one, then i (n + 1) (n + 2) + n + 1 for the k-Certainty Exploration Method, 3.2”++ 1 for the Minimal Select Method is required as the number of trials. The multi-return environment can be shown very easily. Therefore, we cannot bear the exponential explosion in the Minimal Select Method. 4.2. Comparison with Q-learning in a maze-like environment The k-Certainty Exploration Method can identify terministic MDPs. In this subsection, we confirm comparison with Q-learning in the maze-like environment of [ 161. the environment in de- the effectiveness of this feature through completely 4.2.1. A maze-like environment We show the maze-like environment all states. At each time step, it senses one state and selects an action out of move up, move the down, move right and move goal state G, it can get a reward and returns of [ 161 in Fig. 8. The agent can distinguish to black walls. When left. It cannot move to the start state S. it reaches The shortest path from S to G needs fourteen steps. There are six variations of the The agent must learn an optimum policy for finding to acquire an optimum policy of a the number of trials shortest path in this environment. one of them. We compare learning system based on the k-Certainty Exploration Method and Q-learning. 4.2.2. Performance of the k-Certainty Exploration Method We show an average number of trails and the standard deviation to acquire an optimum policy in Table 2. We have made 100 trials with different random seeds. We can get an optimum policy when all rules have achieved k-certainty. Since there in this environment, Table 2 coincides with the are not any stochastic state transitions K. Miyazaki et al./Art@cial Intelligence 91 (1997) 15%171 165 Fig. 8. A maze-like environment. Table 2 Comparison of the k-certainty Exploration Method aad Q-learning on the environment of Fig. 8 average standard deviation k-Certainty Exploration Q-learning 598.62 4780.42 397.50 2124.46 number of trials 2-certainty is 1503.5 and the standard deviation is 397.7. to make all rules l-certainty. The number of trials to make all rules If we use the k-Certainty Exploration Method, the agent moves to black walls at least once in each state. Because the k-Certainty Exploration Method excepts k-certain looped rules. Therefore, we can suppress the explosion of trials to acquire an optimal policy in large state spaces. 4.2.3. Comparison with Q-learning Q-learning uses the roulette selection in proportion to Q-values for an action selector. Initial Q-values are set to 10.0, the discount factor is 0.9, the reward value is 100.0 and the learning rate of Q-learning is 0.5. These are determined by preliminary experiments. Q-learning needs correct propagation of rewards to converge Q-values. It needs nu- merous trials to propagate a reward from G to S since there are many states in this environment. As a result, Q-learning requires about six times as many trials for learning in comparison with a learning system based on the k-Certainty Exploration Method. Through this numerical example, we can confirm the effectiveness of a learning system based on the k-Certainty Exploration Method in deterministic MDPs. 4.3. Com.parison with Q-learning in a stochastic MDP In stochastic MDPs, the k-Certainty Exploration Method can control the identification level of the environment by updating a certainty level k. Furthermore, it is not influenced by positions of rewards to identify the environment. We confirm these features using an original environment where an optimum policy varies according to a parameter. 166 K. Miyazaki et al. /Artificial Intelligence 91 (1997) 155-l 71 (b) Scl,Sl, . . . ; sensory input W ; rule V ; reward Fig. 9. A stochastic state transition environment. 4.3. I. A stochastic state transition environment au environment We consider shown from SO to S2 by executing probability rule 3 and 200.0 rewards rewards by executing Fig. 9(b), it can get 100.0 rewards by executing rule 7. If p < 0.5, an optimum policy contains if p > 0.5, it contains acquire an optimum policy of a learning Method and Q-learning under p is 0.1, 0.3, 0.7 and 0.9. in Fig. 9. A parameter rule 0. In Fig. 9(a), p is a state transition the agent can get 100.0 to S2. In if it selects rule 0 and moves rule 3 and 200.0 rewards by executing rule 1 and rule 3. On the other hand, to the number of trials system based on the k-Certainty Exploration rule 0, rule 5 and rule 7. We compare 4.3.2. Perjormance of the k-Certainty Exploration Method Fig. 10 shows the acquisition the number of rate of the optimal policy plotted against system based on the k-Certainty Exploration Method. We have made level random seeds. Fig. 11 shows the distribution of a certainty trials in a learning 100 trials with different k until the acquisition rate of the optimal policy achieves 1.0. The acquisition the k-Certainty Exploration Method state transition rates of the optimal policy are the same in Figs. 9(a) and (b) because the rates of it. If p approaches 0.5, the is not influenced by positions of rewards. Only the acquisition probability effects K. Miyazaki et al. /Artificial Intelligence 91 (1997) 155-l 71 167 0 Q) ;;j h g 12 22 g % 3 Y 5.0 4.0 ’ 3 .o 2.0 ! 1.0 0 .o I I I I I I I 0 100 200 300 400 500 600 700 the number of trials Fig. 10. Behavior of the k-Certainty Exploration Method on the environment of Fig. 9. agent tends to make a mistake since 0.5 is a boundary of the optimal policy. In this case, the k-Certainty Exploration Method raises a certainty level k. Therefore, distribution of a certainty level k in p = 0.3 and p = 0.7 spreads out in comparison with p = 0.1 and p = 0.9 as shown in Fig. 11. 4.3.3. Comparison with Q-learning Figs. 12 and 13 show the acquisition rate of the optimal policy plotted against the number of trials in Q-learning under the environment of Figs. 9(a) and (b) , respectively. Q-learning uses the roulette selection in proportion to Q-values for an action selector. Initial Q-values are set to 10.0, the discount factor is 0.9, the reward value is 100.0 and the learning rate of Q-learning is 0.5. They are determined by preliminary experiments. In general, Q-learning acquires an optimum policy through Q-values. Q-values reflect the expected reward per action. Therefore, Q-learning is influenced by positions of the Q-value of rule 0 is more reinforced than that of rule 1 rewards. In Fig. 9(a), the agent can get the because of getting a reward by executing rule 0. Therefore, optimum policy easily within p > 0.5. On the other hand, in Fig. 9(b), it can get the optimum policy easily within p < 0.5 since the Q-value of rule 1 is more reinforced than that of rule 0. 168 K. Miyazaki et al./Arti@ial Intelligence 91 (1997) 155-l 71 A s 100 g fir 50 & 0 10 20 k 30 h 100 s H 5o I 1 0 10 k 20 30 i-;!L Foe7 1 10 20 30 k Fo3 10 20 30 k Fig. 11. Frequency distributions of a certainty level k on the environment of Fig. 9. Q-learning is strictly influenced by positions of rewards. The k-Certainty Exploration Method is not influenced by them. Therefore, it is feasible to some changes of them. 5. Conchsions We have proposed the k-Certainty Exploration Method which is an action selector to identify the environment efficiently. Combining the k-Certainty Exploration Method with PIA, it is possible to acquire an optimal policy efficiently. We show the effectiveness of the k-Certainty Exploration Method by comparing it with other methods. K. Miyazaki et al. /Artijcial Intelligence 91 (I 997) 155-I 71 169 F=OS 0 100 200 300 400 500 600 700 the number of trials Fig. 12. Behavior of Q-learning on the environment of Fig. 9(a). The k-Certainty Exploration Method does not consider any state transition probabil- ity. Therefore, it does not always guarantee the efficiency under stochastic MDPs. We propose the ~-Certainty Exploration Method which is an extension of the k-Certainty Ex- ploration Method to stochastic MDPs [ 131. The fLCertainty Exploration Method realizes efficient identification of the environment by considering state transition probabilities. As future work, we want to extend the k-Certainty Exploration Method to non-MDPs. learning, dynamical After that, we would like to consider multi-agent reinforcement environments, multi-reward environments, and so on. Appendix A In the -following we show the Policy Iteration Algorithm. Step 1.. Select any policy. Step 2. For that policy, resolve the following equations in Wi, i = 1,2,. . . , m: m Wi = r: +yCq$Wj, j=l i=1,2 ,..., 112, 170 K. Miyazaki et al. /Arttjicial Intelligence 91 (1997) 155-171 1 .o 2 a & 9 .o E 8.0 8 ‘_= oa 3 “0 8 E g *- .t: .z 3 8 3 7 .o 6.0 5.0 4.0 3.0 2.0 1 .o 0 .o 0 100 1000 1200 1400 the number of trials fig. 13. Behavior of Q-learning on the environment of Fig. 9(b). where m is the number of states, ki is an action in state i, y is a discount factor. Step 3. Calculate the following equations: wi = lygn{$ + YkCwj}, j=l i= 1,2 ,..., m, where n is the number of actions. Step 4. If Wi = wi for any i, the policy which Wi > Wi, set the k for which Step 3 is maximized Step 2. is an optimum policy. to ki for If there is an i for to any i and return References [ 1] D.P. Bertsekas, Dynamic programming and stochastic control, in: R. Bellman, ed., Marhematics in Science and Engineering 125 (Academic Press, New York, 1976). [ 21 S.J. Bradtke, Reinforcement learning applied to linear quadratic regulation, in: Proceedings NIPS-j, 1993. [3] J.A. Clouse and RE. Utgoff, A teaching method for reinforcement learning, in: Proceedings Ninth International Conference on Machine Learning, Aberdeen, Scotland (1992) 92-101. K. Miyazaki et al. /Arttjicial Intelligence 91 (1997) 155-I 71 171 [4] J.J. Grefenstette, Credit assignment in rule discovery systems based on genetic algorithms, Mach. Learn. 3 (1988) 225-245. [5] J.H. Holland and J.S. Reightman, Cognitive systems based on adaptive algorithms, in: D.A. Waterman and E Hayes-Roth, eds., Pattern-Directed Inference Systems (Academic Press, New York, 1987). [ 61 L.P Kaelbling, An adaptable mobile robot, in: Proceedings 1st European Conference on Artificial Life (1991) 41-47. [7] H. Kimura, M. Yamamura and S. Kobayashi, Reinforcement learning by Stochastic Hill Climbing on discounted reward, in: Proceedings 12th International Conference on Machine Learning, lake Tahoe, CA (1995) 295-303. [8] G.E. Liepins, M.R. Hilliard, M. Palmer and G. Rangarajan, Alternatives for classifier system credit assignment, in: Proceedings IJCAI-89, Detroit, MI ( 1989) 756-761. [9] L. Lin, Scaling up reinforcement learning for robot control, in: Proceedings 10th International Conference on Machine Learning, Amherst, MA (1993) 182-189. [lo] S. Mahadevan and J. Connell, Automatic programming of behavior-based robots using reinforcement learning, in: Proceedings AAAI-91, Anaheim, CA (1991) 774-780. [ 111 K. Miyazaki, M. Yamamura and S. Kobayashi, A theory of profit sharing in reinforcement learning, .I. Japan. Sot. Artificial Intelligence 9 4 (1994) 580-587 (in Japanese). [ 121 K. Mi:yazaki, M. Yamamura and S. Kobayashi, On the rationality of profit sharing in reinforcement learning, in: Proceedings 3rd International Conference on Fuzzy Logic, Neural Nets and Soft Computing (1994) 285-288. [ 131 K. Miyazaki, M. Yamamura and S. Kobayashi, !-Certainty Exploration Method: an action selector to extension of k-Certainty Exploration Method to stochastic identify the environment by an agent-an MDPs, J. Japan. Sot. Artijcial Intelligence, to appear (in Japanese). [ 141 C.H. Papadimitriou and J.N. Tsitsiklis, The complexity of Markov decision processes, Math. Oper. Res. (1987) 441-450. [ 151 S.P. Singh, Transfer of learning by composing solutions of elemental sequential tasks, Mach. Learn. 8 (1992) 323-339. [ 161 R.S. Sutton, Reinforcement learning architectures for animats, in: Proceedings 1st International Conference on Simulation of Adaptive Behavior ( 1990) 337-343. [ 171 Y. Takemichi and Y. Kakazu, An acquisition of action strategy by multi-layered Q-learning, in: Proceedings 7th Annual Conference of JSAI (1993) 93-96. [ 181 M. Tan, Multi-agent reinforcement learning: independent vs. cooperative agents, in: Proceedings 10th International Conference on Machine Learning, Amherst, MA ( 1993) 330-337. [ 191 J. Tenenberg, J. Karlsson and S. Whitehead, Learning via task decomposition, in: Proceedings 2nd International Conference on Simulation of Adaptive Behavior (1993) 337-343. [20] T. Unemi, M. Nagayoshi, N. Hiyayama, T. Nade, K. Yano and Y. Masujima, Evolutionary differentiation case study on optimizing parameter values in Q-learning by a genetic algorithm, of learning abilities-a in: Proceedings 4th International Workshop on Artificial Life (1994) 331-336. [21] C.J.C.H. Watkins and P Dayan, Technical note: Q-learning, Mach. Learn. 8 (1992) 55-68. [22] S.D. Whitehead and D.H. Ballard, Active perception and reinforcement learning, in: Proceedings 7th International Conference on Machine Learning, Austin, TX (1990) 179-188. [23] M. Ya!mamura, Reinforcement learning, J. Japan. Sot. Art@cial Intelligence 8 6 ( 1993) 833-834 (in Japanese). 