Artificial Intelligence 172 (2008) 1470–1494Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintEnhanced qualitative probabilistic networks for resolving trade-offs ✩Silja Renooij∗, Linda C. van der GaagDepartment of Information and Computing Sciences, Utrecht University, P.O. Box 80.089, 3508 TB Utrecht, The Netherlandsa r t i c l ei n f oa b s t r a c tArticle history:Received 18 October 2006Received in revised form 9 April 2008Accepted 14 April 2008Available online 20 April 2008Keywords:Probabilistic reasoningQualitative reasoningTrade-off resolutionQualitative probabilistic networks were designed to overcome, to at least some extent, thequantification problem known to probabilistic networks. Qualitative networks abstract fromthe numerical probabilities of their quantitative counterparts by using signs to summarisethe probabilistic influences between their variables. One of the major drawbacks of thesequalitative abstractions, however, is the coarse level of representation detail that doesnot provide for indicating strengths of influences. As a result, the trade-offs modelledin a network remain unresolved upon inference. We present an enhanced formalism ofqualitative probabilistic networks to provide for a finer level of representation detail.An enhanced qualitative probabilistic network differs from a basic qualitative network inthat it distinguishes between strong and weak influences. Now, if a strong influence iscombined, upon inference, with a conflicting weak influence, the sign of the net influencemay be readily determined. Enhanced qualitative networks are purely qualitative in nature,as basic qualitative networks are, yet allow for resolving some trade-offs upon inference.© 2008 Elsevier B.V. All rights reserved.1. IntroductionThe formalism of probabilistic networks introduced in the 1980s [26], is an intuitively appealing formalism for capturingknowledge of complex problem domains along with the uncertainties involved. Associated with the formalism are powerfulalgorithms for reasoning with uncertainty in a mathematically correct way. These algorithms for probabilistic inference allowfor causal reasoning, diagnostic reasoning as well as case-specific reasoning; probabilistic inference, however, is known tobe NP-hard [7]. Applications of probabilistic networks can be found in areas such as (medical) diagnosis and prognosis,planning, monitoring, vision, and information retrieval (see, for example, [1,2,4,5,20,31]).A probabilistic network basically is a concise representation of a joint probability distribution on a set of statisticalvariables. It consists of an acyclic directed graph encoding the relevant variables from a domain of application along withtheir probabilistic interrelationships. Associated with each variable is a set of conditional probability distributions describingthe relationship of the variable with its predecessors in the graph. The first task in constructing a probabilistic network isto identify the important domain variables, their values, and their interdependencies. This knowledge is then modelled ina directed graph, referred to as the network’s qualitative part. The final task is to obtain the probabilities that constitutethe network’s quantitative part. As (conditional) probability distributions are to be stated for each variable in the graph, thenumber of required probabilities can be quite large, even for small applications. While the construction of the qualitativepart of a probabilistic network is generally considered feasible, its quantification is a far harder task. Probabilistic informationavailable from literature or data is often insufficient or unusable, and domain experts have to be relied upon to assess the✩This research was (partly) supported by the Netherlands Organisation for Scientific Research (NWO). We would very much like to thank Hans Bodlaenderfor his advice relating to the complexity issues addressed in this paper.* Corresponding author.E-mail addresses: silja@cs.uu.nl (S. Renooij), linda@cs.uu.nl (L.C. van der Gaag).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.04.001S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941471required probabilities [14]. Unfortunately, experts are often uncomfortable with having to provide probabilities. Moreover,the problems of bias encountered when directly eliciting probabilities from experts are widely known [19]. The usuallylarge number of probabilities required for a probabilistic network, as a consequence, tends to pose a major obstacle to theirapplication [14,18].To mitigate the quantification bottleneck to at least some extent, qualitative probabilistic networks have been intro-duced [34]. Qualitative networks in essence are qualitative abstractions of probabilistic networks. Like a probabilisticnetwork, a qualitative network encodes variables and the probabilistic relationships between them in a directed graph.However, while the relationships between the represented variables are quantified by conditional probabilities in a prob-abilistic network, these relationships are summarised in its qualitative abstraction by qualitative signs capturing stochasticdominance. The probabilistic information captured by signs is more robust than exact numbers are and is more easilyobtained from domain experts [10]. Elicitation methods to this end are being designed [33].Originally, the benefits of using qualitative probabilistic networks included the complexity of inference: for reasoningwith a qualitative probabilistic network, an efficient algorithm is available, based on the idea of propagating and combiningthese signs [11]. In practice, however, nowadays the complexity of probabilistic inference is less of a problem and interestin qualitative probabilistic networks has shifted more to the construction and validation phase of probabilistic networks forreal-life application domains. As the assessment of the various probabilities required is a hard task, it is performed onlywhen the probabilistic network’s graph is considered robust. Now, by assessing signs for the influences modelled in thegraph, a qualitative network is obtained that can be exploited for studying the projected probabilistic network’s reasoningbehaviour prior to the assessment of probabilities. Patterns of qualitative influences can also be used to recognise differenttypes of causal interaction, such as the noisy-or, which greatly simplify the quantification effort [24]. In addition, qualitativesigns can be used in several ways as constraints on the quantification. For example, by interpreting the signs as continuoussubintervals of the probability interval, the constraints they impose on the conditional probability distributions involved canbe used for stepwise quantification of a probabilistic network: once a conditional probability table for a certain variableis filled, the interval associated with all direct influences upon that variable can be tightened [28]. These semi-qualitativeprobabilistic networks can also include assessments based on probabilistic logic and credal sets [6]. More recently, the signsof qualitative probabilistic networks have been used to constrain the probabilities learned from small data sets [3,15,17]. Ata somewhat higher level, the constraints imposed by qualitative influences can be used to bound the entire space of pos-sible joint probability distributions over the network’s variables [13]. Finally, the qualitative signs can be used for verifyingmonotonicity properties in a probabilistic network [32], and for explanation of the (qualitative) probabilistic network’s rea-soning processes [10]. Given the increasing variety of useful applications of qualitative probabilistic networks, it is importantto derive as much information as possible from such networks.Qualitative probabilistic networks, by their nature, have a coarse level of representation detail. Influential relationshipsbetween variables can be modelled as positive, negative, zero or ambiguous, but no indication of their strengths can beprovided as in a quantified network. One of the major drawbacks of this coarse level of representation detail is the easewith which the ambiguous ‘?’-sign arises upon inference. Ambiguous signs typically arise from trade-offs. A qualitativenetwork models a trade-off if two nodes in the network’s digraph are connected by multiple parallel reasoning chains withconflicting signs. In the absence of a notion of strength of influences, qualitative networks do not provide for resolving suchtrade-offs. Inference with a qualitative network for a real-life domain of application, as a consequence, often introducesambiguous signs. Moreover, once an ambiguous sign has been generated, it will spread throughout major parts of thenetwork. Although not incorrect, ambiguous signs provide no information whatsoever about the influence of one variableon another and are therefore not very useful in practice.Ambiguous results from inference can be averted by enhancing the formalism of qualitative probabilistic networks toprovide for a finer level of representation detail. Roughly speaking, the finer the level of detail, the more trade-offs can beresolved during inference. The finer levels of detail, however, typically come at the price of a higher computational com-plexity of inference. The problem of trade-off resolution for qualitative networks has been addressed by various researchersand we detail the relation between their work and ours in the Related work section of this paper. In short, S. Parsons, for ex-ample, has introduced the concept of categorical influence, which is either an influence that serves to increase a probabilityto 1, or an influence that decreases a probability to 0, and thus serves to resolve any trade-off in which it is involved [25].Parsons has also studied the use of order-of-magnitude reasoning in the context of qualitative probabilistic networks [25].C.-L. Liu and M.P. Wellman have designed two methods for resolving trade-offs based upon the idea of reverting to numer-ical probabilities whenever necessary [23]. While only some trade-offs can be resolved by the use of categorical influences,the methods of Liu and Wellman provide for resolving any trade-off, but require the availability of a fully quantified proba-bilistic network.To provide for qualitative trade-off resolution without resorting to numerical probabilities, we have designed an intu-itively appealing formalism of enhanced qualitative networks. An enhanced qualitative probabilistic network differs from abasic qualitative network in that it introduces a notion of relative strength by distinguishing between strong and weak in-fluences. The distinction between strong and weak influences is very intuitive and domain experts should have no problemsproviding and interpreting the associated signs. Now, if a trade-off is modelled in an enhanced network and the positiveinfluence, for example, is known to be stronger than the conflicting negative one, we may upon inference conclude the netinfluence to be positive. Trade-off resolution during inference thus builds upon the idea that strong influences dominate overconflicting weak influences. To provide for inference with an enhanced network, we have generalised the sign-propagation1472S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494algorithm for basic qualitative networks to deal with strong and weak influences. This generalisation is straightforward oncewe establish that the properties upon which the basic sign-propagation algorithm is based are also provided for in an en-hanced network. The new inference algorithm takes into account that the effect of one variable on another diminishes asvariables are further apart in the network’s graph; it also takes into account that a variable may affect another variablealong multiple pathways with differing strengths. To maintain the correct strengths of indirect influences, the algorithm hasto do some additional bookkeeping, as a result of which it may become less efficient than the inference algorithm for basicqualitative networks.The paper is organised as follows. In Section 2, we provide some preliminaries from the fields of probabilistic networksand qualitative networks to introduce our notational conventions. In Section 3, we present our new formalism of enhancedqualitative probabilistic networks. In Section 4, we detail various properties of enhanced networks, on which we build a newsign-propagation algorithm. Section 5 provides an example of inference with an enhanced qualitative probabilistic networkand discusses some complexity issues concerning sign-propagation. Related work is reviewed in Section 6. The paper isrounded off with our conclusions and directions for future research in Section 7.2. PreliminariesIn this section we briefly review probabilistic networks and their qualitative counterparts.2.1. Probabilistic networksA probabilistic network models a domain of application basically by representing, in a concise way, the joint probabilitydistribution on the set of statistical variables relevant to the application domain [26]. A probabilistic network B = (G, Pr)encodes, in an acyclic directed graph G = (V (G), A(G)), these relevant variables along with their probabilistic interrelation-ships. Each node A ∈ V (G) represents a statistical variable that can take one of a finite set of values. We assume a totalorder ‘>’ on the values of a variable. Variables will be indicated by capital letters. We will restrict ourselves to binary-valued variables, where we write a to denote A = true and ¯a to denote A = false, with a > ¯a. As there is a one-to-onecorrespondence between variables and nodes, we will use the terms ‘node’ and ‘variable’ interchangeably.The probabilistic relationships between the represented variables are captured by the digraph’s set of arcs A(G). In-formally speaking, we take an arc A → B in G to represent an influential relationship between the variables A and B,designating B as the effect of cause A. Given an arc A → B, node A is called a (immediate) predecessor of node B andnode B is called a successor of node A. We write π ( A) to denote the set of all predecessors of node A in G, and π ∗( A) todenote the set of its ancestors; similarly, σ ( A) is used to denote the set of all successors of node A and σ ∗( A) to denote itsdescendants. Two variables A and B are said to be connected by a (simple) trail in G iff they are connected by a (simple)path in the underlying undirected graph of G. Absence of an arc between two variables in the digraph of a probabilisticnetwork means that the variables do not influence each other directly and, hence, are (conditionally) independent. Moreformally, probabilistic independence can be read from the digraph by means of the d-separation criterion, which builds onthe concept of blocking [26]. A trail between two variables is said to be blocked by the available evidence if it includes eitheran observed variable with at least one outgoing arc, or an unobserved variable with two incoming arcs and no observeddescendants. Two variables are now said to be d-separated if all trails between them are blocked, in which case they areconsidered conditionally independent given the available evidence. A trail that is not blocked is called active. If an activetrail connects two Markov-blanket neighbours (i.e. two variables sharing an arc or a common child), then the two variablesare said to be active neighbours.Associated with each variable A ∈ V (G) in the network’s digraph G is a set of conditional probability distributionsPr( A | π ( A)) that describe the strengths of the various dependences between A and its (immediate) predecessors. These(conditional) probabilities with each other provide all information necessary for uniquely defining a joint probability distri-bution on the network’s variables: the probabilistic network B = (G, Pr) defines the distribution Pr on V (G) with(cid:2)Pr(cid:3)V (G)=(cid:4)(cid:2)Pr(cid:3)A | π ( A)A∈V (G)that respects the independences portrayed by the digraph G. Since a probabilistic network thus captures a unique jointprobability distribution, it provides for computing any prior or posterior probability over its variables. To this end, variousalgorithms are available [22,26].We now describe a piece of fictitious medical knowledge that will serve as our example domain of application through-out the paper.Example 2.1. Our example application domain pertains to the effects of administering antibiotics on a patient and involvesfive statistical variables together with their interrelationships. Node A represents whether or not a patient has been takingantibiotics. Node T models whether or not the patient is suffering from typhoid fever, node D represents the presence orabsence of diarrhoea in the patient, and node H represents whether or not the patient is dehydrated. Node F , to conclude,describes whether or not the composition of the bacterial flora in the patient’s intestines has changed. Typhoid fever and achange in bacterial flora are the possible causes of diarrhoea. Diarrhoea, in turn, can cause dehydration. Antibiotics can cureS. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941473Fig. 1. The Antibiotics domain captured by (a) a probabilistic network, and (b) a qualitative probabilistic network.(a)(b)typhoid fever by killing the bacteria that cause the infection. As a result, the probability of a patient contracting diarrhoeadecreases. However, antibiotics can also change the composition of the intestinal bacterial flora, thereby increasing the riskof diarrhoea. Fig. 1(a) depicts the probabilistic network which captures the knowledge from our domain.2.2. Qualitative probabilistic networksQualitative probabilistic networks bear a strong resemblance to their quantitative counterparts [34]. Instead of representingthe joint probability distribution on the set of statistical variables relevant to the application domain, however, it onlyrepresents qualitative constraints on this distribution. A qualitative probabilistic network Q = (G, Δ) also comprises anacyclic digraph G = (V (G), A(G)) modelling variables and the probabilistic relationships between them. Moreover, the set ofarcs A(G) of this digraph again models probabilistic independence. Instead of conditional probability distributions, however,a qualitative probabilistic network associates with its digraph a set Δ of qualitative influences and qualitative synergies.A qualitative influence between two variables expresses how the values of one variable influence the probabilities of thevalues of the other variable; the direction of the shift in distribution (i.e. do higher values become more likely or less likely)is indicated by the sign of the influence. A positive qualitative influence of a variable A on a variable B, for example, expressesthat observing a higher value for A makes a higher value for B more likely, regardless of any other influences on B [34].Definition 2.2. Let G = (V (G), A(G)) be an acyclic digraph and let Pr be a joint probability distribution on V (G) that respectsthe independences in G. Let A, B be variables in G with A → B ∈ A(G). Then, variable A positively influences variable B alongarc A → B, written S+( A, B), iffPr(b | ax) − Pr(b | ¯ax) (cid:2) 0for any combination of values x for the set π (B) \ { A} of predecessors of B other than A.A negative qualitative influence, denoted by S, and a zero qualitative influence, denoted by S 0, are defined analogously,replacing (cid:2) in the above formula by (cid:3) and =, respectively. If the influence of variable A on variable B is not monotonic orif it is unknown, we say that it is ambiguous, denoted S ?( A, B).−With each arc in the digraph of a qualitative probabilistic network, a qualitative influence is associated. Variables, how-ever, not only influence each other directly along arcs, they can also exert indirect influences on one another. The definitionof qualitative influence trivially extends to indirect influences, that is, influences along active trails. We denote an indirectinfluence of sign δ along an active trail t from variable A to variable B by ˆSδ( A, B, t). From here on, the term trail will beused to refer to either a simple trail, basically consisting of a concatenation of arcs, or to a subgraph containing all simpletrails between two variables. The latter type of trail is said to consist of a composition of simple trails. The set of all variableson a trail t will be denoted V (t).The set of influences of a qualitative probabilistic network exhibits various convenient properties that constitute thebasis for an efficient algorithm for qualitative probabilistic inference [34]. The property of symmetry guarantees that, if anetwork includes the influence Sδ( A, B), then it also includes Sδ(B, A) with the same sign δ ∈ {+, −, 0, ?}. The propertyof transitivity asserts that qualitative influences along an active trail without head-to-head nodes, that is, without nodeswith two incoming arcs on the trail, combine into an indirect influence whose sign is determined by the ⊗-operator fromTable 1. The property of composition asserts that multiple qualitative influences between two variables along parallel activetrails combine into a composite influence whose sign is determined by the ⊕-operator. From Table 1, we observe thatcombining non-ambiguous qualitative influences with the ⊕-operator can yield an ambiguous result. Such an ambiguity, infact, results whenever parallel influences with opposite signs are combined. We say that the trade-off that is reflected bythe conflicting influences cannot be resolved. Note that, in contrast with the ⊕-operator, the ⊗-operator cannot introduceambiguities upon combining signs. The operators in Table 1 adhere to the standard algebraic properties of commutativity,associativity, and distributivity of ⊗ over ⊕.In addition to influences, a qualitative probabilistic network includes synergies that model the interactions betweentriples of variables. An additive synergy, for example, captures the joint influence of two variables on a common succes-1474S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494Table 1The ⊗- and ⊕-operators⊗+−0?++−0?−−+0?00000???0?⊕+−0?++?+?−?−−?0+−0??????sor [34]. A positive additive synergy of two variables A and B on a variable C , more specifically, expresses that the jointinfluence of A and B on C is greater than their separate influences.Definition 2.3. Let G = (V (G), A(G)) be an acyclic digraph and let Pr be a joint probability distribution on V (G) thatrespects the independences in G. Let A, B, C be variables in G with A → C, B → C ∈ A(G). Then, variables A and B exhibita positive additive synergy on C iffPr(c | abx) + Pr(c | ¯a¯bx) − Pr(c | a¯bx) − Pr(c | ¯abx) (cid:2) 0for any combination of values x for the set π (C) \ { A, B} of predecessors of C other than A and B.Negative, zero, and ambiguous additive synergies are defined analogously.If two variables A and B have a common successor C , then observation of a value for variable C serves to activate thetrail A → C ← B. The observation thus induces a dependence between A and B. This dependence can be represented by aqualitative influence of A on B, or vice versa. Such an induced influence is commonly known as an intercausal influence. Thesign of the intercausal influence is captured by the sign of the product synergy associated with the variables involved andthe observation. A product synergy thus expresses how the value of one variable influences the probabilities of the valuesof another variable in view of a given value for a third variable [12]. A negative product synergy of A and B on C withvalue c, for example, expresses that, given c, a high value for A renders a high value for B less likely; this reasoning patternis known as explaining away [26].Definition 2.4. Let G = (V (G), A(G)) be an acyclic digraph and let Pr be a joint probability distribution on V (G) thatrespects the independences in G. Let A, B, C be variables in G with A → C, B → C ∈ A(G). Then, variables A and B exhibita negative product synergy on variable C with value c, denoted X−({ A, B}, c), iffPr(c | abx)·Pr(c | ¯a¯bx) − Pr(c | a¯bx)·Pr(c | ¯abx) (cid:3) 0for any combination of values x for the set π (C) \ { A, B} of predecessors of C other than A and B.Positive, zero, and ambiguous product synergies again are defined analogously.With each triple of variables A, B, C in V (G) such that A → C , B → C ∈ A(G), an additive synergy and two productsynergies are associated. Note that a product synergy is defined for every possible value of C . These qualitative synergiesare again trivially extended to trails and also exhibit symmetry, transitivity and composition properties. For details, we referto [27,34].Example 2.5. We consider the qualitative probabilistic network representation of the Antibiotics domain in Fig. 1(b). Thefigure displays the signs of the qualitative influences along the arcs, of the additive synergy over the curve over node D,and of the product synergies over the dotted edge.The qualitative influence S−( A, T ) specifies that the difference in conditional probabilities Pr(t | a) − Pr(t | ¯a) should bezero or less. Indeed, from the conditional probabilities specified for the variable T in the probabilistic network representationof the domain in Fig. 1(a), we have thatPr(t | a) − Pr(t | ¯a) = 0.01 − 0.35 = −0.34 (cid:3) 0Similar observations hold for Shave that+( A, F ) and S+(D, H). From the conditional probabilities specified for the variable D, wePr(d | t f ) − Pr(d | ¯t f ) = 0.95 − 0.15 = 0.80 (cid:2) 0,Pr(d | t ¯f ) − Pr(d | ¯t ¯f ) = 0.80 − 0.01 = 0.79 (cid:2) 0andwhich indeed obey the constraints posed by Sin the quantified network. In both networks, the variables T and F exert a positive additive synergy Yvariable D:+(T , D). We similarly find that the qualitative influence S+(F , D) is preserved+({T , F }, D) onPr(d | t f ) + Pr(d | ¯t ¯f ) − Pr(d | ¯t f ) − Pr(d | t ¯f ) = 0.01 (cid:2) 0S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941475procedure PropagateObservation(Q , O , sign, Observed):for each V i ∈ V (G)do sign[V i ] ← ‘0’;PropagateSign(∅, O , sign).procedure PropagateSign(trail, to, messagesign):sign[to] ← sign[to] ⊕ messagesign;trail ← trail ∪ {to};for each active neighbour V i of to given {O } ∪ Observeddo linksign ← sign of (induced) influence between to and V i ;messagesign ← sign[to] ⊗ linksign;if V i /∈ trail and sign[V i ] (cid:10)= sign[V i ] ⊕ messagesignthen PropagateSign(trail, V i , messagesign).Fig. 2. The sign-propagation algorithm for qualitative probabilistic inference.Either value for D, in addition, indeed induces a negative intercausal influence between the variables T and F in theprobabilistic network representation. For example, we have thatPr(¯d | t f ) · Pr(¯d | ¯t ¯f ) − Pr(¯d | ¯t f ) · Pr(¯d | t ¯f ) = −0.12 (cid:3) 0For reasoning with a qualitative probabilistic network, an efficient algorithm is available from M.J. Druzdzel and M. Hen-rion [11]; this algorithm, termed the sign-propagation algorithm, is summarised in pseudocode in Fig. 2. The basic idea ofthe algorithm is to trace the effect of observing a variable’s value on the probabilities of the values of all other variablesin the network by message-passing between neighbouring nodes. In essence, the algorithm computes the sign of the netinfluence along all active trails between the newly observed variable and the other variables in the network, building uponthe properties of symmetry, transitivity and composition of influences. For each variable, it summarises the net influence ina node-sign that indicates the direction of the shift in the variable’s probability distribution that is occasioned by the newobservation.The sign-propagation algorithm takes for its input a qualitative probabilistic network Q , a set Observed of previouslyobserved variables, the variable O for which an observation has become available, and the sign sign of the new observation,that is, either a ‘+’ for the value true or a ‘−’ for the value false. Prior to the propagation of the new observation, for allvariables V i the node-sign sign[V i ] is set to ‘0’. For the newly observed variable O the appropriate sign is now enteredinto the network. The observed variable updates its node-sign to the sign-sum of its original sign and the entered sign. Itthereupon reports its change of sign to all its active neighbours, that is to all variables in its Markov-blanket that can bereached through a trail not blocked by the set Observed. This notification is done by passing to each of them a messagecontaining an appropriate sign, which is the sign-product of the variable’s current node-sign and the sign linksign of theinfluence associated with the arc or induced intercausal link it traverses. Each message further records its origin in thevariable trail; this information is used to prevent messages being passed on to nodes that have already been visited onthe same trail. Upon receiving a message, a variable to updates its node-sign to the sign-sum of its current node-signsign[to] and the sign messagesign from the message it has just received. The variable then sends a copy of the messageto all its neighbours that need to reconsider their node-signs. In doing so, the variable changes the sign in each copy tothe appropriate sign and adds itself to trail as the origin of the copy. Note that as this process is repeated throughout thenetwork, the trails along which messages have been passed are recorded. Also note that as messages travel simple trailsonly, it is sufficient to just record the nodes on these trails.During sign-propagation, variables are only visited if they need a change of node-sign. A node-sign can change at mosttwice, once from ‘0’ to ‘+’, ‘−’ or ‘?’, and then only from ‘+’ or ‘−’ to ‘?’. From this observation we have that no variable isever visited more than twice upon inference. The algorithm is therefore guaranteed to halt. For a proof of the algorithm’scorrectness we refer the reader to [11].We illustrate the sign-propagation algorithm by means of our running example.Example 2.6. We consider once again the qualitative Antibiotics network from Fig. 1(b). Suppose that a patient is takingantibiotics. This observation is entered into the network by updating the node-sign of the variable A to ‘+’. Variable Athereupon propagates a message, with sign + ⊗ − = −, towards T . Variable T updates its node-sign to ‘−’ and sendsa message with sign − ⊗ + = − to D. Variable D updates its sign to ‘−’ and sends a message with sign − ⊗ + = −to H . Variable H updates its node-sign to ‘−’; it sends no messages as it has no neighbours that need to update their sign.Variable D does not pass on a sign to F , since the trail from T via D to F is not active.Variable A also sends a message, with sign + ⊗ + = +, to F . Variable F updates its node-sign accordingly and passesa message with sign + ⊗ + = + to D. Variable D thus receives the additional sign ‘+’. This sign is combined with thepreviously updated node-sign ‘−’, which results in the ambiguous node-sign − ⊕ + = ? for D. Note that the ambiguoussign arises from the trade-off represented for variable D. D now sends a message with sign ? ⊗ + = ? to H , which updatesits sign to ? ⊕ − = ?. Note that, had the network contained additional variables beyond the variables D and/or H , thenthese variables would have all ended up with the node-sign ‘?’ after inference.1476S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14943. The enhanced formalismQualitative probabilistic networks capture the knowledge from a problem domain at a coarse level of representationdetail. Qualitative influences between variables, for example, are captured by simple signs without any indication of theirstrengths. As a consequence, any trade-off encountered during inference will remain unresolved. In this section, we present anew formalism for qualitative probabilistic networks that allows for a finer level of representation detail, which will enablethe resolving of trade-offs to at least some extent. In this new formalism, we enhance qualitative probabilistic networksby associating an indication of relative strength with their influences. Now, if, for example, upon encountering a trade-offduring inference, the positive influence is known to be stronger than the conflicting negative one, then we may concludethe combined influence to be positive, thereby effectively resolving the trade-off.In an enhanced qualitative probabilistic network, we distinguish between strong and weak influences. Intuitively, a stronginfluence of a variable A on a variable B is an influence that is stronger than any weak influence in the network, that is,the property(cid:5)(cid:5)(cid:5)Pr(b | ax) − Pr(b | ¯ax)(cid:5) (cid:2)(cid:5)(cid:5)(cid:5)Pr(d | c y) − Pr(d | ¯c y)(cid:5)holds for all variables C and D with a weak influence between them, for any combination of values x and y for the sets Xand Y of relevant predecessors. The basic idea now is to partition the set of all direct influences in a network into disjointsets in such a way that any influence from the one subset is stronger than any influence from the other subset. To this end,we introduce a cut-off value α that serves to partition the set of direct qualitative influences into a set of influences thatcapture an absolute difference in probabilities of at least α and a set of influences that model an absolute difference of atmost α. An influence from the former subset will be termed a strong influence; an influence from the latter subset will betermed a weak influence.Definition 3.1. Let G = (V (G), A(G)) be an acyclic digraph and let Pr be a joint probability distribution on V (G) that respectsthe independences in G. Let A, B be variables in G with A → B ∈ A(G). Let α ∈ [0, 1] be a cut-off value. The influence ofvariable A on variable B along arc A → B is strongly positive, denoted S++( A, B), iffPr(b | ax) − Pr(b | ¯ax) (cid:2) αfor any combination of values x for the set π (B) \ { A} of predecessors of B other than A. The influence of variable A onvariable B along the arc is weakly positive, denoted S+( A, B), iff0 (cid:3) Pr(b | ax) − Pr(b | ¯ax) (cid:3) αfor any combination of values x.−−−Strongly negative qualitative influences, denoted S, are definedanalogously; zero qualitative influences and ambiguous qualitative influences are defined as for basic qualitative probabilisticnetworks. In the special case where, for an influence of a variable A on a variable B, the difference between the probabilitiesPr(b | ax) and Pr(b | ¯ax) equals α for all x, we take the influence to be strong., and weakly negative qualitative influences, denoted SA product synergy is defined to be strongly negative if it induces a strongly negative intercausal influence. Weakly negative,strongly positive, and weakly positive product synergies are defined analogously; zero product synergies and ambiguous productsynergies again are defined as for basic qualitative networks. For additive synergies, the distinction between weak and strongis slightly more complicated. Since additive synergies are not used during sign-propagation and therefore do not contributeto the resolution of trade-offs, we will not consider them any further in this paper.Upon abstracting a quantified probabilistic network to an enhanced qualitative probabilistic network, the cut-off value αwould need to be chosen explicitly. This cut-off value will typically vary from application to application, but it is alwayspossible to choose such a cut-off value, since the values α = 0 or α = 1 yield a trivial partitioning of the set of influences.In real-life applications of enhanced qualitative probabilistic networks, however, the cut-off value need not be establishedexplicitly. The partitioning into strong and weak influences is then elicited directly from the domain experts involved in theconstruction of the network.Example 3.2. We consider once again Fig. 1 showing the qualitative and quantitative probabilistic network representationsof the Antibiotics domain from Example 2.1. An enhanced qualitative probabilistic network representation of the domainis given in Fig. 3, showing just the qualitative influences involved. In addition to these qualitative influences, we have astrongly negative product synergy of variables T and F on D = d, and a weakly negative product synergy for D = ¯d.We note that the basic signs of the qualitative influences in the enhanced representation are consistent with the signs ofthe qualitative influences in the basic qualitative network representation in Fig. 1(b). We will now show that the enhancedrepresentation is also consistent with the completely quantified representation of Fig. 1(a), if we choose for our cut-off valueα = 0.30.From S−−( A, T ) we conclude that Pr(t | a) − Pr(t | ¯a) should be negative with an absolute value of at least α = 0.30;indeed, we have thatPr(t | a) − Pr(t | ¯a) (cid:3) 0,and(cid:5)(cid:5)(cid:5) = 0.34 (cid:2) α(cid:5)Pr(t | a) − Pr(t | ¯a)S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941477Fig. 3. The enhanced qualitative Antibiotics network.+( A, F ) andWe similarly find that the conditional probabilities associated with variables F and D are consistent with S++(D, H), respectively. For the influence of variable T on variable D, we observe that Pr(d | t F ) − Pr(d | ¯t F ) (cid:2) 0, regardlessSof the value of F , as well asPr(d | t f ) − Pr(d | ¯t f ) = 0.80,and Pr(d | t ¯f ) − Pr(d | ¯t ¯f ) = 0.79++(T , D); we similarly find thatwhich both exceed the level of the cut-off value α. We therefore indeed have that S+(F , D). The signs of the product synergies exhibited by the variables T and F on variable D, in the presence of aSvalue for D, equal the signs of the corresponding intercausal influences. The intercausal influences are defined in terms ofdifferences between Pr( f | tx) and Pr( f | ¯tx), where x represents different combinations of values for the variables D and A.These probabilities can be found from the network in Example 2.1 by applying Bayes’ rule; we list them here for ease ofreference:Pr( f | tx)Pr( f | ¯tx)x = dax = d¯ax = ¯dax = ¯d¯a0.540.520.200.170.940.920.490.41For the sign of the intercausal influence of variable T on variable F given the value d for D, we now have thatPr( f | tda) − Pr( f | ¯tda) = −0.40 (cid:3) −α,Pr( f | td¯a) − Pr( f | ¯td¯a) = −0.40 (cid:3) −αandWe conclude that the intercausal influence, and therefore its corresponding product synergy, is indeed strongly negative:X−−({T , F }, d); we similarly confirm that X−({T , F }, ¯d).In our enhanced formalism, the semantics of the sign of an influence has slightly changed: while in a basic qualitativeprobabilistic network, the sign of an influence represents the sign of differences in probability only, in an enhanced qual-itative network a sign in addition captures the relative magnitude of the differences. These relative magnitudes should becorrectly preserved upon inference, when the indirect influences between variables are considered. In fact, we will demon-strate in Sections 4.2 and 4.3 that whereas the strength of a direct influence is defined relative only to cut-off value α, thestrengths of indirect influences can be described in terms of a polynomial expression in α. To capture such a polynomial,we introduce a multiplication-index list and augment the signs of indirect influences with such a list. Before defining ourmultiplication-index list and the augmented signs, we observe that in general an n-th order polynomial in α can be writtenasn(cid:6)i=0ci · αi =(cid:6)ci · αi −(cid:6)−ci · αii=0,...,n: ci >0i=0,...,n: ci <0For our purposes, it suffices to consider polynomials in α with coefficients ci ∈ Z and c0 (cid:2) 0. Since all exponents i are non-negative, we can represent any such polynomial by listing each exponent i |ci| times together with an indication of whetherthe associated term should be added or subtracted. This list of, possibly negated, exponents constitutes our multiplication-index list.Definition 3.3. A multiplication-index list I is a multiset {i1, . . . , in}, n (cid:2) 1, where each index i j ∈ I , j = 1, . . . , n, is an integer.The multiplication-index list I captures the polynomial in α(cid:6)(cid:6)αi j −α−i ji j (cid:2)0i j <0A polynomial in α captured by multiplication-index list I will in short be denoted by [α]I .1478S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494As an example, consider the polynomial [α]I = −2 · α3 + 1, which can be written as −α3 − α3 + α0. The multiset{−3, −3, 0} defines a multiplication-index list I that captures this polynomial.A sign augmented with a multiplication-index list is now displayed by attaching the multiplication-index list to it as asuperscript, omitting the curly braces for the sake of readability. For example, a weakly positive sign with multiplication-index list I = {i1, . . . , in} is written as +i1,...,in , or +I for short. The following definition formally describes the meaning ofan indirect influence with such an augmented sign.Definition 3.4. Let G = (V (G), A(G)) be an acyclic digraph in which the variables A and B are connected by an active trail t.Let Pr be a joint probability distribution on V (G) that respects the independences in G. Let α ∈ [0, 1] be a cut-off value. Theinfluence of variable A on variable B along trail t is strongly positive with multiplication-index list I , denoted ˆS( A, B, t), iff++IPr(b | ax) − Pr(b | ¯ax) (cid:2) [α]I (cid:2) 0(cid:7)for every combination of values x for the subset X = (on B along t is weakly positive with multiplication-index list I, denoted ˆS+I( A, B, t), iffC∈V (t)\{ A} π (C) \ V (t)) of relevant ancestors of B. The influence of A0 (cid:3) Pr(b | ax) − Pr(b | ¯ax) (cid:3) [α]Ifor every combination of values x for X .Strongly and weakly negative influences with a multiplication-index list are again defined analogously. Zero and am-biguous influences are once more defined as in basic qualitative probabilistic networks and are not augmented withmultiplication indices.We would like to remark that the multiplication-index list we introduce is used to augment signs in an enhancednetwork during inference only. The list is used solely for the purpose of computation and, although possible, we do notintend to output signs augmented with these indices to the user.4. Enabling inference in an enhanced networkFor inference with a basic qualitative probabilistic network, an efficient algorithm is available. We recall from Section 2that this algorithm builds on the idea of propagating signs throughout a network and combining them with the ⊗- and⊕-operators. We further recall that the algorithm thereby exploits the properties of symmetry, transitivity, and parallelcomposition of influences. In this section we generalise the idea of sign-propagation to inference with an enhanced qual-itative probabilistic network by taking into account the strength of influences. Upon initiating inference, the signs of theinfluences associated with the arcs of the digraph of an enhanced network are now interpreted as having a single multipli-cation index equal to 1. In Section 4.1, we address the property of symmetry, followed by a discussion and enhancement ofthe ⊗- and ⊕-operators to provide for the properties of transitivity and parallel composition of strong and weak influencesin Sections 4.2 and 4.3, respectively.4.1. The property of symmetryIn a basic qualitative probabilistic network, the property of symmetry guarantees that, if a variable A exerts an influenceon a variable B, then variable B exerts an influence of the same sign on variable A. As a result, signs can be propagated duringinference over an arc in both directions. In an enhanced qualitative network, as in a basic qualitative network, an influenceand its reverse are both positive, both negative, both zero, or both ambiguous. The symmetry property, however, does nothold with regard to the strength of an influence: the reverse of a strongly positive qualitative influence, for example, may bea weakly positive influence, and vice versa. There are two ways of ensuring, in an enhanced network, that during inferencesigns can be propagated in both directions of an arc:• elicit the signs of all influences against the direction of an arc explicitly;• alternatively, use positive and negative signs of ambiguous strength, that is, signs whose strength is unknown and maybe anywhere between 0 and 1.Both alternatives have their benefits and drawbacks. The latter option is quite straightforward, since the symmetric coun-terpart of any positive influence, for example, would be an ambiguously positive influence, which we represent by +0.However, upon using such signs of unknown strength, much useful information is lost and we therefore opt for explicitlyspecifying the signs of influences against the arc directions. Upon explicitly specifying these signs, however, care has tobe taken that the two signs specified for an arc are consistent. For example, without going into details, we can derive thefollowing from the definition of qualitative influence and its property of symmetry: for an arc A → B, predecessors X of Aand predecessors Y of B other than A, we have that if Pr(a | xy) lies inbetween Pr(b | xy) and Pr(¯b | xy), then the qualitativeinfluence of B on A is necessarily stronger than the qualitative influence of A on B, otherwise it is weaker. In the former++( A, B).case, S+(B, A), for example, would be inconsistent with SS. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941479Fig. 4. A fragment of a network.With respect to intercausal influences we note that since they can be regarded as a qualitative influence, the aboveobservations also hold with respect to the signs of such influences.4.2. The property of transitivityFor propagating qualitative signs along active trails in an enhanced qualitative probabilistic network, we have to enhancethe ⊗-operator that is defined for this purpose for basic qualitative networks, to apply to strong and weak influences.We recall that the ⊗-operator provides for multiplying signs of influences. In a basic qualitative probabilistic network,an influence in essence captures a difference between two probabilities. Combining two influences with the property oftransitivity then amounts to determining the sign of the product of two such differences. In our formalism of enhancedqualitative probabilistic networks, however, we have associated an explicit notion of strength with influences. It will beevident that these strengths need to be taken into consideration when multiplying signs with the ⊗-operator.To address the sign-product of two signs in an enhanced qualitative probabilistic network, we consider the networkfragment shown in Fig. 4. The fragment includes an (active) trail that is composed of the variables A, B, C , and twoqualitative influences between them. In addition, X denotes the set of all predecessors of B other than A, and Y is the setof all predecessors of C other than B. The following lemma now indicates that the strength of the indirect influence of Aon C along the given trail equals the product of the strengths of the influences of A on B and of B on C .Lemma 4.1. Let G = (V (G), A(G)) be an acyclic digraph where A, B, C ∈ V (G) and A → B, B → C is the only active trail betweenthe variables A and C . Let Pr be a joint probability distribution on V (G) that respects the independences in G. Then,(cid:3)(cid:2)Pr(c | by) − Pr(c | ¯b y)(cid:3)(cid:2)Pr(b | ax) − Pr(b | ¯ax)Pr(c | axy) − Pr(c | ¯axy) =·for any combination of values x for the set of variables X = π (B) \ { A} and any combination of values y for the set Y = π (C) \ {B}.Proof. We observe that, in G, variable C is independent of the variables A and X , given B and Y ; in addition, variable B isindependent of variable Y , given A and X . By conditioning on B we now findPr(c | axy) − Pr(c | ¯axy) = Pr(c | abxy) · Pr(b | axy) + Pr(c | a¯bxy) · Pr(¯b | axy)− Pr(c | ¯abxy) · Pr(b | ¯axy) − Pr(c | ¯a¯bxy) · Pr(¯b | ¯axy)(cid:2)(cid:3)Pr(c | by) − Pr(c | ¯b y)−(cid:2)(cid:3)Pr(c | by) − Pr(c | ¯b y)· Pr(b | ax) + Pr(c | ¯b y)(cid:3)(cid:2)· Pr(b | ¯ax) − Pr(c | ¯b y)Pr(c | by) − Pr(c | ¯b y)(cid:3)(cid:2)Pr(b | ax) − Pr(b | ¯ax)·(cid:2)==Similar lemmas hold for the strengths of the influences along any other possible active trail between the variables Aand C that can be obtained by reversing one or both arcs in Fig. 4 without introducing a head-to-head node on the trail.The lemma can further be easily extended to apply to the situation where A and B, and B and C , respectively, are connectedby indirect active trails rather than direct arcs. We would like to note that the existence of additional (parallel) active trailsbetween the variables A and C is handled by the ⊕-operator, and is therefore disregarded here.The differences Pr(c | axy) − Pr(c | ¯axy) for the various combinations of values xy serve to indicate the strength of theindirect influence of variable A on variable C . We informally investigate these differences using the property stated inLemma 4.1. Suppose that the qualitative influences of A on B and of B on C both are strongly positive, that is, we have++(B, C). Let α be the cut-off value used for distinguishing between strong and weak influences. From the++( A, B) and SSexpression stated in the lemma, we now find thatPr(c | axy) − Pr(c | ¯axy) (cid:2) α · α = α2for any combination of values xy for the set of variables X ∪ Y . Since α (cid:3) 1, we have that α2 (cid:3) α. Upon multiplying thesigns of two strong direct influences, therefore, a sign results that indicates an indirect influence that is not necessarilystronger than a weak direct influence. Similar observations apply to strongly negative influences. Now suppose that both1480S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494qualitative influences in the network fragment from Fig. 4 are weakly positive, that is, we have Sthe indirect influence of variable A on variable C , we then find that+( A, B) and S+(B, C). For0 (cid:3) Pr(c | axy) − Pr(c | ¯axy) (cid:3) α · α = α2for any combination of values xy. Similar observations apply to weakly negative influences. While the indirect influenceresulting from the product of two strong influences cannot be compared to a weak direct influence, we have from the aboveobservation that this indirect influence is always at least as strong as an indirect influence that results from the product oftwo weak influences. Finally, suppose that one qualitative influence in the network fragment from Fig. 4 is weakly positive++(B, C). We then find for the indirect influence of+( A, B) and Sand that the other is strongly positive, for example, Svariable A on variable C that0 = 0 · α (cid:3) Pr(c | axy) − Pr(c | ¯axy) (cid:3) α · 1 = αfor any combination of values xy. Similar observations apply to other combinations of weak and strong influences. We thushave that the strength of an indirect influence resulting from the product of a strong and a weak influence is comparable tothe strength of a weak direct influence.From the previous observations, we conclude that to provide for comparing indirect qualitative influences along differenttrails with respect to their strengths, as required for trade-off resolution, we have to preserve information concerning thenumber of times signs have been multiplied.4.2.1. Enhancing the ⊗-operatorWe employ the multiplication-index list, defined in Section 3, to retain information about the strengths of signs whichhave been multiplied. To be able to combine information from different multiplication-index lists, we now define a sum-operation on these lists.Definition 4.2. Let I and J be two multiplication-index lists. Then the multiplication-index list I + J is the multiset(cid:8)(cid:2)|i| + | j|· sgn(i) · sgn( j) | i ∈ I, j ∈ J(cid:9)where sgn : Z → {−1, 1} is defined assgn(z) =if z (cid:2) 01−1 otherwise(cid:3)(cid:10)As an example, consider the polynomials [α]I with I = {1, 2} and [α] J with J = {1, −3}, then the multiplication-indexlist I + J = {2, 3, −4, −5} actually captures the polynomial [α]I+ J = [α]I · [α] J :(cid:2)= α1+1 + α2+1 − α1+3 − α2+3α1 − α3·[α]I+ J = [α]I · [α] J =(cid:2)α1 + α2(cid:3)(cid:3)Table 2 now defines the enhanced ⊗-operator, which shapes the transitivity property for qualitative influences in anenhanced network. From the table, it is readily seen that the ‘+’, ‘−’, ‘0’, and ‘?’ signs in essence combine just as in a basicqualitative probabilistic network; the only difference is in the handling of the multiplication indices. The following proposi-tion shows that the operator correctly captures the sign of the transitive combination of two weakly positive influences.Proposition 4.3. Let Q = (G, Δ) be an enhanced qualitative probabilistic network. Let A, B, and C be variables in G for which thereexist an active trail t1 from A to B and an active trail t2 from B to C such that their concatenation t1 ◦ t2 is an active trail from A to C .Let I and J be multiplication-index lists. Then,(B, C, t2) (cid:13)⇒ ˆS( A, B, t1) ∧ ˆS( A, C, t1 ◦ t2)+I+ J+ JˆS+IProof. Let Pr be a joint probability distribution on V (G) that respects the independences in G, and let α ∈ [0, 1] be thecut-off value used for distinguishing between strong and weak influences. We will start by assuming that the multiplication-index lists I and J each consist of a single index i and j, respectively. Then, the weakly positive influence ˆS( A, B, t1) ofvariable A on variable B expresses that0 (cid:3) Pr(b | ax) − Pr(b | ¯ax) (cid:3) αi+ITable 2The enhanced ⊗-operator⊗++I+I0−I−−I?++ J++I+ J+I0−I−−I+ J?+ J+ J+I+ J0−I+ J− J?0000000− J− J−I+ J0+I+ J+ J?−− J−−I+ J−I0+I++I+ J????0???S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941481for every combination of values x for the set X =positive qualitative influence ˆS+ J(B, C, t2) of variable B on variable C expresses that(cid:7)D∈V (t1)\{ A} π (D) \ V (t1) of relevant ancestors of B. Similarly, the weakly0 (cid:3) Pr(c | by) − Pr(c | ¯b y) (cid:3) α jfor every combination of values y for the set Y of relevant ancestors of C . For the indirect influence of variable A onvariable C , we thus find from Lemma 4.1 that0 (cid:3) Pr(c | axy) − Pr(c | ¯axy) (cid:3) αi · α j = αi+ jfor every combination of values xy for the set X ∪ Y . More in general, we observe that the strength of the resultinginfluence lies between 0 and the product of the polynomial expressions in α captured by the multiplication-index lists Iand J , respectively. We therefore conclude that ˆS( A, C, t1 ◦ t2). (cid:2)+I+ JFrom the above proposition and the appropriate entry in Table 2, we conclude that for two weakly positive influencesthe enhanced ⊗-operator indeed correctly captures the sign of their transitive combination. Similar observations hold forthe transitive combination of any two weak influences or any two strong influences, be they positive or negative.The following proposition shows that the operator in Table 2 correctly captures the sign of the transitive combination ofa weakly positive and a strongly positive influence.Proposition 4.4. Let Q , A, B, C , t1, t2, t1 ◦ t2, I and J be as in the previous proposition. Then,+IˆS( A, B, t1) ∧ ˆS++ J(B, C, t2) (cid:13)⇒ ˆS+I( A, C, t1 ◦ t2)Proof. Let Pr and α be as in the previous proof. We again start by assuming that the multiplication-index lists I and J eachconsist of a single index i and j, respectively. Then, the weakly positive influence ˆS( A, B, t1) of variable A on variable Bexpresses that+I0 (cid:3) Pr(b | ax) − Pr(b | ¯ax) (cid:3) αifor every combination of values x for the set X =qualitative influence ˆS++ J(B, C, t2) of variable B on variable C further expresses that(cid:7)D∈V (t1)\{ A} π (D) \ V (t1) of relevant ancestors of B. The strongly positiveα j (cid:3) Pr(c | by) − Pr(c | ¯b y) (cid:3) 1for every combination of values y for the set Y of relevant ancestors of C . For the indirect influence of variable A onvariable C , we thus find from Lemma 4.1 that0 (cid:3) Pr(c | axy) − Pr(c | ¯axy) (cid:3) αi · 1for every combination of values xy for the set X ∪ Y . More in general, we observe that the strength of the resulting influencelies between 0 and 1 times the polynomial expression in α captured by multiplication-index list I . We therefore concludethat ˆS( A, C, t1 ◦ t2). (cid:2)+IFrom the above proposition and the appropriate entry in Table 2, we conclude that for a weakly positive and a stronglypositive influence the enhanced ⊗-operator indeed correctly captures the sign of their transitive combination. Similar ob-servations hold for the transitive combination of any weak influence with any strong influence, be they positive or negative.The proofs for the signs of all other transitive combinations of influences stated in Table 2, are analogous to the proofs ofPropositions 4.3 and 4.4.4.3. The property of parallel compositionFor combining multiple qualitative influences between two variables along parallel active trails in an enhanced qualita-tive probabilistic network, we have to enhance the ⊕-operator that is defined for this purpose for basic qualitative networks,to apply to strong and weak influences. We recall that the ⊕-operator provides for summing signs of influences. We fur-ther recall that, upon adding the signs of two conflicting influences during inference with a basic qualitative network, therepresented trade-off cannot be resolved and an ambiguous influence results. In our formalism of enhanced qualitativeprobabilistic networks, we have associated an explicit notion of strength with influences. These strengths can now be takeninto consideration when summing the signs of influences and can be used to resolve trade-offs. For example, if a trade-offis encountered during inference, and the negative influence is known to be stronger than the conflicting positive one, thenwe may conclude that the combined influence is negative, thereby forestalling ambiguous results.Upon addressing the property of transitivity in the previous section, we have argued that the product of two influencesmay yield an indirect influence that is weaker than the influences it is built from. We will now see that the sum of twoinfluences, in contrast, may result in a stronger influence. To address the sign-sum of two signs in an enhanced qualitative1482S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494Fig. 5. Another network fragment.probabilistic network, we consider the network fragment shown in Fig. 5. The fragment includes two active trails betweenthe variables A and C , one of which captures a direct influence of A on C and the other one an indirect influence through B.In addition, the set X denotes the set of all predecessors of B other than A, and Y is the set of predecessors of C other thanA and B. The following lemma now relates the strength of the net influence of variable A on variable C to the strengths ofthe influences it is built from.Lemma 4.5. Let G = (V (G), A(G)) be an acyclic digraph where A, B, C ∈ V (G) and A → B, B → C and A → C are the only activetrails between the variables A and C . Let Pr be a joint probability distribution on V (G) that respects the independences in G. Then,Pr(c | axy) − Pr(c | ¯axy) =(cid:3)(cid:2)Pr(c | aby) − Pr(c | a¯b y)−· Pr(b | ax) + Pr(c | a¯b y)(cid:3)(cid:2)Pr(c | ¯aby) − Pr(c | ¯a¯b y)· Pr(b | ¯ax) − Pr(c | ¯a¯b y)for any combination of values x for the set X of all predecessors of B other than A and any combination of values y for the set Y of allpredecessors of C other than A and B.Proof. The proof of the property stated in the lemma is similar to that of Lemma 4.1. (cid:2)Similar lemmas hold for the strengths of the net influences of A on C along other combinations of multiple paralleltrails that can be obtained by reversing one or more arcs in Fig. 5, as long as both trails remain active. A similar lemmacan also be formulated for situations where one or more of the arcs in Fig. 5 are replaced by active trails. We would like tonote that the existence of additional parallel trails between the variables A and C is handled by repeated application of thecomposition property, and is therefore disregarded here.The differences Pr(c | axy) − Pr(c | ¯axy) for the various combinations of values xy serve to indicate the sum of thestrengths of the direct influence and the indirect influence of the variable A on the variable C . If all the arcs in the networkfragment from Fig. 5 are associated with a weakly positive influence, for example, we find thatPr(c | axy) − Pr(c | ¯axy) (cid:3) α + α2Building upon Lemma 4.5, we will prove this property shortly. From the inequality, we observe that the parallel compositionof two weak influences of the same sign may result in a net influence that is stronger than a weak direct influence.Its relation to a strong influence is unknown, however. So, although the basic sign of the resulting influence is knownunambiguously, its strength is not readily expressible as a simple power of α. Alternatively, if all the arcs in the networkfragment from Fig. 5 are associated with a strongly positive influence, we find thatPr(c | axy) − Pr(c | ¯axy) (cid:2) α + α2and we observe that the parallel composition of two strong influences of the same sign results in a net influence that isslightly stronger than a strong direct influence.From these observations we have that the parallel composition of two or more influences may result in an influencefor which the strength cannot be expressed by a single power of the cut-off value α without losing information; rather,an entire polynomial in α is required. In the remainder of this section, we present two ways of capturing the strengthsof parallel influences, by defining two different ⊕-operators for summing signs. The first enhanced ⊕-operator is discussedin Section 4.3.1 and keeps track of entire polynomials in α by means of the multiplication-index list. The second operatorworks on signs with single multiplication indices only. In the latter case, we minimise the amount of bookkeeping necessaryduring inference by discarding the higher-order terms of the polynomial expression in α, leaving a single α-term whosepower can be taken as a single multiplication index for the resulting sign; if these higher-order terms cannot be discardedwithout introducing a possible error, a sign of unknown strength is yielded. This second operator is called the simpleenhanced operator ⊕s and is briefly described in Section 4.3.2. Obviously, application of the ⊕s-operator can result in loss ofavailable information when adding two signs upon inference.4.3.1. The enhanced operator ⊕We employ the multiplication-index list defined in Section 3 to retain information about the strengths of possiblyconflicting signs which have been summed. To be able to compare the strengths of signs, as captured by their multiplication-index lists, we define three additional list operations.S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941483Table 3The enhanced ⊕-operator for signs with multiplication-index lists⊕++I+I0−I−−I?++ J++I ∪ J++ J++ Jb)??wherea) ++I ∪ − J , if I (cid:3) J ; else ?,b) ++−I ∪ J , if J (cid:3) I ; else ?,c) −−I ∪ − J , if I (cid:3) J ; else ?,d) −−−I ∪ J , if J (cid:3) I ; else ?.+ J++I+I ∪ J+ J?c)?0++I+I0−I−−I?− Ja)?− J−I ∪ J−−I?−− J?d)−− J−− J−−I ∪ J????????Definition 4.6. Let I and J be two multiplication-index lists and let [α]I and [α] J be the polynomials in α captured bylists I and J , respectively. Then,• the multiplication-index list −I is the multiset {−i | i ∈ I};• the multiplication-index list I ∪ J is the multiset {i | i ∈ I or i ∈ J };• I (cid:3) J iff [α]I − [α] J (cid:2) 0.The above definition defines a negation operator ‘−’ which negates each index in the list to which it is applied, and aunion operator ‘ ∪ ’ which combines all elements of two multisets into a single multiset; the comparison operator (cid:3) capturesthe idea that in general lower order polynomials in α ∈ [0, 1] correspond to stronger signs. As an example, consider thepolynomials [α]I with I = {1, −3} and [α] J with J = {2, −3}, then −I = {−1, 3} captures the polynomial [α]−I = −[α]I ,I ∪ J = {1, 2, −3, −3}, and I (cid:3) J , since [α]I − [α] J = α − α2 (cid:2) 0.Table 3 now defines the enhanced ⊕-operator, which shapes the composition property for influences in an enhancedqualitative network. From the table, it is readily seen that the ‘+’, ‘−’, ‘0’, and ‘?’ signs combine as in a basic qualitativeprobabilistic network; the only difference is in the handling of the multiplication indices. The following four propositionsshow, for four different situations, that the operator correctly captures the sign of a combination of two parallel influences;the proofs for the other combinations of influences are quite similar. The first proposition pertains to the situation wheretwo weakly positive influences along parallel trails are combined.Proposition 4.7. Let Q = (G, Δ) be an enhanced qualitative probabilistic network. Let A, C be variables in G and let t1 and t2 beparallel active trails in G from A to C , where t1 (cid:15) t2 is their trail composition. Let I and J be multiplication-index lists. Then,+IˆS( A, C, t1) ∧ ˆS+ J( A, C, t2) (cid:13)⇒ ˆS+I ∪ J( A, C, t1 (cid:15) t2)Proof. Let Pr be a joint probability distribution on V (G) that respects the independences in G. Let α ∈ [0, 1] be the cut-off value used for distinguishing between strong and weak influences. For ease of exposition, we assume that the trail t1consists of a single arc and that the trail t2 consists of the arcs A → B, B → C for some variable B, as in the networkfragment of Fig. 5. Additional trails between A and C can be handled by repeated application of the composition property,and are therefore disregarded here. We recall that with each arc is associated an influence with multiplication index 1,so we have I = {1}. We further recall that Lemma 4.5 gives the net influence of variable A on variable C along the trailcomposition t1 (cid:15) t2. We now write the equation from Lemma 4.5 as the difference between two functions f and h:(cid:3)Pr(c | aby) − Pr(c | a¯b y)(cid:11)(cid:2)−(cid:3)(cid:2)Pr(b | ax)(cid:12)· Pr(b | ax) + Pr(c | a¯b y)(cid:3)Pr(c | ¯aby) − Pr(c | ¯a¯b y)(cid:3)(cid:2)Pr(b | ¯ax)(cid:12)· Pr(b | ¯ax) + Pr(c | ¯a¯b y)Pr(c | axy) − Pr(c | ¯axy) =− h= f(cid:11)(cid:2)for all value combinations x and y for the set X of predecessors of B other than A and the set Y of predecessors of C otherthan A and B, respectively. We note that the functions f and h are both linear in their respective parameter.We now assume that the positive influence along trail t2 is composed of two separate positive influences. From theinfluence of variable B on variable C being positive, we have that the functions f and h are both linearly increasing, asis larger than the gradient of the function hdepicted in Fig. 6; the fact that in the figure the gradient of the function fis an arbitrary choice. From the positive direct influence of variable A on variable C we further have that f (0) (cid:2) h(0) andf (1) (cid:2) h(1). We therefore have that the functions f and h do not intersect. If the two influences along trail t2 are bothnegative, then the functions f and h are decreasing and similar observations apply.To determine the sign of the composite influence of variable A on variable C , we have to consider the sign of thedifference between the functions f and h. We observe that, although the functions f and h are expressed in terms of1484S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494Fig. 6. Possible functions f (Pr(b | ax)) and h(Pr(b | ¯ax)).Fig. 7. The functions f (Pr(b | ax)) and h(Pr(b | ¯ax)) depicted in a single graph, with (a) gradient( f ) > gradient(h), and (b) gradient(h) > gradient( f ).(a)(b)different parameters, these parameters cannot be varied independently as their difference is restricted by the sign of thequalitative influence of variable A on variable B. Under this constraint, we are allowed to compare the function values of fand h for different parameters. For ease of comparison, we have depicted for this purpose the two functions f and h in asingle graph, in Fig. 7.Since the positive indirect influence along trail t2 is composed of two positive influences, we have three possible situa-tions:(1) S(2) S(3) S+( A, B) and S++( A, B) and S+( A, B) and S++(B, C), or+(B, C), or+(B, C).In the first two situations we have ˆS( A, C, t2) with J = {2}. Here, weonly consider the latter situation; the proofs for the other two situations are quite similar. As the direct influence of thevariable A on the variable B is weakly positive, we have that 0 (cid:3) Pr(b | ax) − Pr(b | ¯ax) (cid:3) α. Therefore, when investigatingthe difference between the two functions f and h, we have to satisfy the following constraints:( A, C, t2) with J = {1}; in situation (3) we find ˆS+ J+ J• the parameter Pr(b | ax) for the function f should be greater than or equal to the parameter Pr(b | ¯ax) for the function h;• the difference between the two parameters may not be greater than α.We now show that under these constraints the difference f (Pr(b | ax)) − h(Pr(b | ¯ax)) is greater than or equal to zero.To this end, we consider the graph from Fig. 7(a); similar observations hold for the graph from Fig. 7(b). Under the givenconstraints, we have that the minimal difference between f (Pr(b | ax)) and h(Pr(b | ¯ax)) is attained for f (0) and h(0). Wefind thatPr(c | axy) − Pr(c | ¯axy) (cid:2) f (0) − h(0) = Pr(c | a¯b y) − Pr(c | ¯a¯b y)The minimal difference is positive as a result of the direct influence of A on C being positive. The sign of the compositeinfluence of variable A on variable C is therefore positive. The maximal difference between f (Pr(b | ax)) and h(Pr(b | ¯ax)) isattained for f (1) and h(1 − α). Once again exploiting the information that the signs of the direct influences are all weaklypositive, this difference equals:Pr(c | axy) − Pr(c | ¯axy) (cid:3) f (1) − h(1 − α) = Pr(c | aby) − Pr(c | ¯a¯b y) −(cid:3)(cid:2)Pr(c | ¯aby) − Pr(c | ¯a¯b y)· (1 − α)= Pr(c | aby) − Pr(c | ¯aby) + α ·(cid:3)(cid:2)Pr(c | ¯aby) − Pr(c | ¯a¯b y)(cid:3) α + α · α = α + α2S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941485We conclude that the composite influence of variable A on variable C is weakly positive with multiplication-index list {1, 2}.More in general, we find that the strength of the composite influence lies between zero and the sum of the two polyno-mials in α, captured by the multiplication-index lists I and J , respectively, that is, we conclude that the composite influenceequals ˆS( A, C, t1 (cid:15) t2). (cid:2)+I ∪ JFrom the above proposition and the appropriate entry in Table 3, we conclude that for two weakly positive influencesthe enhanced ⊕-operator correctly captures the sign of their composition. Similar observations hold for the composition oftwo weakly negative signs.The next proposition addresses the situation where two strongly positive influences along parallel trails are combinedinto a composite influence.Proposition 4.8. Let Q , A, C , t1, t2, t1 (cid:15) t2, I and J be as in the previous proposition. Then,++IˆS( A, C, t1) ∧ ˆS++ J( A, C, t2) (cid:13)⇒ ˆS++I ∪ J( A, C, t1 (cid:15) t2)Proof. The proof proceeds in a similar fashion as the proof of Proposition 4.7; more details are provided in Appendix A. (cid:2)From the above proposition and the appropriate entry in Table 3, we conclude that for two strongly positive influencesthe enhanced ⊕-operator correctly captures the sign of their composition. Similar observations hold for the composition oftwo strongly negative signs.The next proposition addresses the combination of a strongly positive and a weakly positive influence; its proof canagain be found in Appendix A.Proposition 4.9. Let Q , A, C , t1, t2, t1 (cid:15) t2, I and J be as in the previous proposition. Then,++IˆS( A, C, t1) ∧ ˆS+ J( A, C, t2) (cid:13)⇒ ˆS++I( A, C, t1 (cid:15) t2)From the above proposition and the appropriate entry in Table 3, we deduce that for a weakly and a strongly posi-tive influence the enhanced ⊕-operator correctly captures the sign of their composition. Similar observations hold for thecomposition of a strongly negative and a weakly negative influence.The main reason for enhancing qualitative probabilistic networks with a notion of strength has been to provide for a finerlevel of representation detail that allows for resolving trade-offs upon inference. Trade-off resolution in essence amounts toassociating an unambiguous basic sign with the composite influence that is built from two or more conflicting influencesalong parallel active trails. The next proposition provides for the combination of conflicting influences and describes thetype of trade-off that can now typically be resolved.Proposition 4.10. Let Q , A, C , t1, t2, t1 (cid:15) t2, I and J be as in the previous proposition. Then, if I (cid:3) J ,++IˆS( A, C, t1) ∧ ˆS− J( A, C, t2) ⇒ ˆS++I ∪ − J( A, C, t1 (cid:15) t2)Proof. Let Pr and α be as before. We again use the functions f and h as defined the proof of Proposition 4.7. Depending onthe sign of the influence of variable B on variable C , we have that the functions f and h are either both linearly increasing,or linearly decreasing functions. We assume that the two functions are increasing, which implies that the influence ofvariable B on variable C is positive. We further assume that the gradient of the function f is larger than the gradient of thefunction h, as depicted in the graph from Fig. 7(a). Similar observations apply to the graph from Fig. 7(b), and to decreasingfunctions.We now distinguish between the two cases (I) and (II) from the proof of Proposition 4.9:(I) the trail t1 consists of a single arc and the trail t2 consists of the arcs A → B, B → C for some variable B;(II) the trail t1 consists of the arcs A → B, B → C and the trail t2 consists of the single arc.First we address case (I), with a strongly positive direct influence of variable A on variable C . From our assumptions we havethat the indirect negative influence along trail t2 is composed of a negative influence of A on B and a positive influenceof B on C . More specifically, we have one of the following three situations:(1) S(2) S(3) S−( A, B) and S−−( A, B) and S−( A, B) and S+(B, C), or+(B, C), or++(B, C).1486S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494The indirect influence of variable A on variable C along trail t2 has associated the sign − J with J = {2} in situation (1) andwith J = {1} in the situations (2) and (3).To establish the sign of the composite influence of A on C , we first establish the minimal difference between thefunctions f and h. We begin by considering the situations (1) and (3) described above. Since the influence of variable A onvariable B is weakly negative, we have that the parameters Pr(b | ax) and Pr(b | ¯ax) for the functions f and h, respectively,have to satisfy the following constraints:• the parameter Pr(b | ax) for function f• the difference between the two parameters is at most α.is smaller than or equal to the parameter Pr(b | ¯ax) for function h;From Fig. 7(a), we observe that under these constraints the minimal difference between f and h is attained for f (0) andh(α). The minimal difference thus isPr(c | axy) − Pr(c | ¯axy) (cid:2) f (0) − h(α) = Pr(c | a¯b y) − Pr(c | ¯a¯b y) −(cid:3)(cid:2)Pr(c | ¯aby) − Pr(c | ¯a¯b y)· αThe difference between the first two terms is α or more, due to the strongly positive direct influence of A on C . Thedifference between the last two terms is captured by the influence of B on C , which is weakly positive in situation (1) andstrongly positive in situation (3). In situation (1) we now have that Pr(c | axy) − Pr(c | ¯axy) (cid:2) α − α · α; for situation (3) wefind that Pr(c | axy) − Pr(c | ¯axy) (cid:2) α − 1 · α = 0.We now consider the situation (2) described above. The strongly negative influence of variable A on variable B imposesthe following constraints on the parameters for f and h:• the parameter Pr(b | ax) for function f• the difference between the two parameters is at least α.is smaller than the parameter Pr(b | ¯ax) for function h;From Fig. 7(a), we observe that under these constraints the minimal difference between f and h is attained for f (0) andh(1):Pr(c | axy) − Pr(c | ¯axy) (cid:2) f (0) − h(1) = Pr(c | a¯b y) − Pr(c | ¯a¯b y) −(cid:3)(cid:2)Pr(c | ¯aby) − Pr(c | ¯a¯b y)We therefore have that Pr(c | axy) − Pr(c | ¯axy) (cid:2) α − α = 0.For all three situations (1), (2), and (3), the maximum difference between the functions f and h is attained for f (1 − α)and h(1). The maximum difference thus isPr(c | axy) − Pr(c | ¯axy) (cid:3) f (1 − α) − h(1) = Pr(c | aby) − Pr(c | ¯aby) −(cid:2)(cid:3)Pr(c | aby) − Pr(c | a¯b y)· αWe find that the maximum difference is at most 1 in situations (1) and (2), and 1 − α in situation (3). We conclude that incase (I), the composite influence of variable A on variable C is positive and at least α I − α J , that is, ˆS( A, C, t1 (cid:15) t2).We now address case (II). Since the indirect influence along trail t1 is strongly positive, it must be composed of twostrong direct influences. Recall that we assume that the influence of variable B on variable C is positive, hence both thestrong influences are positive, that is,++I ∪ − J++S( A, B) and S++(B, C),( A, C). As the proposition addresses only situations where I (cid:3) J , we now assumeresulting in the indirect influence Sthat the weakly negative direct influence of variable A on variable C has a multiplication index of (at least) 2. The aboveobservations result in the following constraints:++2• function f• the parameter Pr(b | ax) for function flies below function h, that is, f (0) (cid:3) h(0) and f (1) (cid:3) h(1);is greater than the parameter Pr(b | ¯ax) for function h, with a difference of atleast α;• the functions f and h are both linearly increasing functions.We again assume the gradient of f to be larger than that of h, with similar observations holding for the opposite case.To establish the sign of the composite influence of variable A on variable B, we once again investigate the minimaland maximal differences between the functions f and h. Under the constraints above, we find that the minimal differencebetween f and h is attained for f (1) and h(0), and thus equalsPr(c | axy) − Pr(c | ¯axy) (cid:2) f (1) − h(0) = Pr(c | aby) − Pr(c | a¯b y) + Pr(c | a¯b y) − Pr(c | ¯a¯b y)From the strongly positive influence of variable B on variable C , we have that Pr(c | aby) − Pr(c | a¯b y) (cid:2) α; from the weaklynegative influence of variable A on variable C we have that 0 (cid:2) Pr(c | a¯b y) − Pr(c | ¯a¯b y) (cid:2) −α2. The minimal differencetherefore equals α − α2. Similarly, the maximal difference between the functions f and h is attained for f (α) and h(0),and equals α2. We conclude that for case (II), the composite influence of variable A on variable C is positive and at leastα I − α J , that is, ˆS( A, C, t1 (cid:15) t2).++I ∪ − JS. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941487To summarise, in all possible situations where the multiplication-index list I of the strong sign is less than or equal tothe multiplication-index list J of the weak sign, the composite influence of variable A on variable C is positive and equalsˆS( A, C, t1 (cid:15) t2). Note that if I > J , then we cannot guarantee that the composite influence is at all positive. (cid:2)++I ∪ − JFrom the above proposition and the appropriate entry in Table 3, we observe that for a strongly positive influencewith multiplication-index list I and a weakly negative influence with multiplication-index list J , I (cid:3) J , the enhanced ⊕-operator correctly captures the sign of their composition. Similar observations apply to other combinations of strong andweak conflicting influences. We conclude that, under certain conditions, the composition of conflicting strong and weakinfluences using the enhanced ⊕-operator leads to an unambiguous result at the level of the basic sign of the compositeinfluence. The enhanced ⊕-operator thus indeed serves to resolve trade-offs upon inference.From Table 3 we observe that multiplication-index lists tend to grow in size upon combining signs. These lists, however,can often be simplified to a large extent. For example, the list I = {1, 2, −1, −3} captures the polynomial α + α2 − α − α3 =α2 − α3, and can be simplified to I = {2, −3}. That is, two complementing indices can be removed as long as this does notresult in an empty multiplication-index list. For example, the list I = {1, −1} represents the constant 0 and not α0 = 1. A listof the form I = {n, −n} should therefore be represented in the given form; the actual value of n, however, is irrelevant andany non-zero integer value could be used without changing the list’s meaning. In the case where a strong sign is augmentedwith a multiplication-index list of the form {n, −n}, an equivalent representation is given by a weak sign with the singlemultiplication index 0, since both represent an influence with a strength anywhere between zero and one. So although wecannot simplify a multiplication-index list of the form {n, −n}, we can, for example, replace the sign ++n,−n by the sign+0 without changing its meaning. We finally note that a multiplication-index list is a true multiset from which duplicatescannot be removed, since for example I = {1, 1} represents the polynomial α + α which equals 2 · α and not simply α.Although simplifying all multiplication-index lists during inference can save a large amount of bookkeeping, we can bespared even more bookkeeping by approximating the polynomial expressions in α by a single term. The next section brieflydescribes how we can safely manage the loss of information incurred by this approximation.4.3.2. The simple enhanced ⊕s-operatorIn this section we introduce a simplified version of the enhanced ⊕-operator, which assumes that all multiplication-index lists consist of a single positive index only. This single index is the result of essentially discarding the higher-orderterms of the polynomial expression in α that captures the strength of a sign. If higher-order terms cannot be discardedwithout introducing a possible error, a sign of unknown strength, denoted +0 or −0, is yielded, which is equivalent to apositive or negative sign, respectively, in a basic qualitative probabilistic network.The simple enhanced ⊕s-operator is defined in Table 4, where the multiplication-index lists of the signs are reduced tosingle indices. When comparing this table to Table 3 for the enhanced ⊕-operator in the previous section, we note thefollowing differences:(1) upon combining two strong signs having the same basic sign, the multiplication index of the resulting sign is theminimum of the multiplication indices of the combined signs, instead of a concatenation;(2) upon combining two weak signs having the same basic sign, we no longer preserve enough information to concludewhether the resulting sign is strong or weak, so a sign of unknown strength results;(3) upon combining a strong and a weak sign with conflicting basic signs, we can unambiguously conclude the basic sign,if the multiplication index of the strong sign is smaller than that of the weak sign, but we do not know its strength.The fact that the simple enhanced ⊕s-operator correctly captures the sign of a combination of two parallel influencesfollows directly from the proofs of the propositions in the previous section. The proof of Proposition 4.8, for example,demonstrates that the strength of the sign which results from combining two non-conflicting strong signs with multiplica-tion indices i and j, respectively, is at least αi + α j (cid:2) αmin(i, j). Similarly, we have from the proof of Proposition 4.7 thatTable 4The simple enhanced ⊕s -operator for signs with single multiplication indices⊕s++i+i++ j+ j0++i+0+ j?c)?++i+i0−i−−i?++m++ j++ jb)??0−i−−i?where m = min(i, j),a) +0, if i (cid:3) j; else ?,b) +0, if j (cid:3) i; else ?,c) −0, if i (cid:3) j; else ?,d) −0, if j (cid:3) i; else ?.− ja)?− j−0−−i?−− j?d)−− j−− j−−m????????1488S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494the strength of the sign which results from combining two non-conflicting weak signs with multiplication indices i and j,respectively, is at most αi + α j and therefore considered unknown. Finally, the proof of Proposition 4.10 shows that thestrength of the sign which results from combining a strong sign with multiplication index i and a conflicting weak signwith multiplication index j is at least αi − α j and therefore considered unknown.We stress that contrary to purely ambiguous signs, signs of unknown strength are valuable since they do not necessarilyspread throughout a network once they occur upon inference. We conclude that application of the simple enhanced ⊕s-operator rather than the enhanced ⊕-operator results in less computational overhead upon qualitative inference. Due toloss of information at the level of the strengths of signs, however, application of the simple ⊕s-operator may result in lesstrade-offs being resolved.5. Probabilistic inference revisitedIn Section 3 we introduced the formalism of enhanced qualitative probabilistic networks. In Section 4, we enhancedthe standard ⊗- and ⊕-operators for combining signs of influences upon inference and have addressed propagation ofsigns against the direction of arcs. Building upon the new, enhanced operators, the basic sign-propagation algorithm forprobabilistic inference with a qualitative network is generalised straightforwardly to apply to enhanced networks: instead ofthe standard ⊗- and ⊕-operators, the enhanced operators are used for propagating and combining signs. In this section weillustrate the application of the resulting algorithm, for both versions of the enhanced ⊕-operator, by means of our runningexample; the qualitative networks associated with the example are reproduced in Fig. 8. In addition, we discuss the algebraicproperties of the enhanced operators which may affect inference results. Finally, we briefly discuss some complexity issuesconcerning the different versions of the sign-propagation algorithm.5.1. Inference using the enhanced operatorsThe idea behind the sign-propagation algorithm is basically to establish the net influence between an observed variableand all other variables in a qualitative probabilistic network, and multiply the sign of this net influence with the sign ofthe observation to return the effect of the observation on all variables. For ease of implementation, the algorithm startsby sending the sign of observation, a ‘+’ or a ‘−’, to the observed variable, thereby already incorporating the effect of theobservation in all messages that are subsequently sent. Due to the algebraic properties of the basic ⊗- and ⊕-operators, theactual implementation does not affect the results.In the next section we will demonstrate that in an enhanced qualitative network, the enhanced operators do not adhereto the algebraic properties that ensure that the order in which signs are combined does not affect the result of theircombination. As a consequence, multiplying the sign of a net influence with the sign of the observation may lead to adifferent result than that obtained by directly incorporating the sign of observation in the messages sent by the observedvariable. To disturb the computation of the signs of net influences as little as possible, we propose entering an observationusing an “identity” sign with respect to strength. More specifically, we require a sign s such that for arbitrary sign t, theresult of s ⊗ t has the strength of t. We note from Table 2 that the signs ++0 and −−0 are suitable for this purpose,since they can be taken to represent the constants 1 and −1, respectively. We now present an example that illustratessign-propagation with the enhanced ⊗- and ⊕-operators.Example 5.1. We consider once again the qualitative Antibiotics network, which is reproduced in Fig. 8(a). Recall that enteringthe sign ‘+’ for variable A results upon inference with the basic sign-propagation algorithm in the ambiguous sign − ⊕ + =‘?’ for variable D, which in turn causes an ambiguous sign for variable H . Now, consider the enhanced Antibiotics networkreproduced in Fig. 8(b); the signs specified are taken to hold in the direction of the corresponding arcs. We recall thatinitially all influences associated with the arcs in the network’s digraph have signs with a multiplication-index of 1. Weonce again apply the sign-propagation algorithm, now using our enhanced operators. We enter the sign ++0 for variable A,reflecting a positive observation for A. Variable A propagates a message with sign ++0 ⊗ −−1 = −−1 towards variable T .Variable T updates its node-sign to −−1 and sends a message with sign −−1 ⊗ ++1 = −−2 to variable D. Variable D(a)(b)Fig. 8. The (a) qualitative Antibiotics network and (b) its enhanced version.S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941489updates its node-sign to −−2 and only sends a message with sign −−2 ⊗ ++1 = −−3 to variable H . Variable H updatesits sign accordingly and sends no messages.Variable A also sends a message, with sign ++0 ⊗ +1 = +1, to variable F . Variable F updates its sign and passes amessage with sign +1 ⊗+1 = +2 to variable D. Variable D receives the additional sign +2. Variable D will now combine thesigns it has received from the two parallel trails originating in A. The result of this combination depends on the enhancedoperator used. More specifically, if the sign-propagation algorithm employs the fully enhanced ⊕-operator then variable Dupdates its sign to −−2 ⊕ +2 = −−2,−2, and then computes for variable H a message with sign −−2,−2 ⊗ ++1 = −−3,−3.On the other hand, if the simple enhanced ⊕s-operator is applied, then variable D updates its sign to −−2 ⊕s +2 = −0,and computes a message with sign −0 ⊗ ++1 = −0 for variable H . Variable H , however, does not need a sign update asits current sign is already correct, regardless of the operator used (−−3 ⊕ −−3,−3 = −−3 and −−3 ⊕s −0 = −−3). Thevariables D and H therefore send no further messages and the algorithm halts.From the above example, it seems at first glance that the results from sign-propagation with the fully enhanced ⊕-operator are similar to the results from using the simple enhanced ⊕s-operator, with only the node-sign for node Ddiffering. This illusion, however, is caused by the specific example network used. With the ⊕-operator, the node-sign ofvariable D is of the form −−i,−i due to the fact that the two trails with conflicting influences have the same length;recall that this node-sign captures the same information as the negative sign −0 of unknown strength returned by the⊕s-operator, and therefore the results of inference in essence do not differ for the two operators. If the conflicting trailshave different lengths, however, then the difference between the two ⊕-operators becomes more important: for those situ-ations in which the algorithm using the simple enhanced ⊕s-operator leads to a node-sign −0, the algorithm using the fullyenhanced ⊕-operator will result in a node-sign −−I ∪ − J , with I (cid:10)= J ; this latter sign captures more information than theambiguous negative sign and may aid in resolving even more trade-offs. We again stress that contrary to purely ambiguoussigns, signs of unknown strength do not necessarily spread throughout a network once they occur. In addition, these signsdo convey useful information about the basic sign of influence between two variables.We conclude that, while in the basic framework of qualitative networks trade-offs cannot be resolved upon inferenceand result in an ambiguous net influence, enhanced qualitative probabilistic networks allow for resolving at least sometrade-offs.5.2. Algebraic propertiesAs a result of the algebraic properties of the basic ⊗- and ⊕-operators used for qualitative inference in a basic qualitativeprobabilistic network, the node-signs computed for the variables in such a network do not depend on the order in whichthe variables receive the messages from their neighbours. Unfortunately, this observation no longer holds in an enhancedqualitative network and the node-signs computed can in fact be different depending on the order in which messagesare received. Given that the enhanced operators correctly capture the transitivity and parallel composition properties ofqualitative influences, the computed node-signs are always correct but may be less informative than they could have been.The following proposition states which algebraic properties the enhanced operators still adhere to; proofs are given inAppendix A.Proposition 5.2. Consider the enhanced operators defined in Tables 2, 3 and 4 for combining signs in an enhanced qualitative network.Then• the enhanced ⊗-operator is commutative;• the enhanced ⊗-operator is associative;• the enhanced ⊕- and ⊕s-operators are commutative.The enhanced ⊕- and ⊕s-operators for combining signs of parallel influences are no longer associative, which can resultin loss of information upon combining several trails having strong and weak conflicting influences. This is illustrated by thefollowing example:(cid:2)++i ⊕ +i(cid:3)⊕ −i = ++i ⊕ −i = ++i,−i++i ⊕(cid:2)+i ⊕ −i(cid:3)= ++i ⊕ ? =?(cid:2)or +0, if ⊕s(cid:3)is usedWe stress that both combinations in this example lead to correct results, regardless of the operator (⊕ or ⊕s) used, the firstis just more informative than the second. Heuristics, such as, for example, separately adding all positive and all negativesigns before combining them, can be designed to prevent unnecessary ambiguous results due to order of combination. Suchheuristics, however, could increase the complexity of inference.Finally, we observe that the enhanced ⊗-operator distributes over neither the ⊕-operator nor the ⊕s-operator. Compare,for example, the following:1490S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494(cid:2)++i ⊕ +i(cid:2)++i ⊗ −i(cid:3)(cid:3)⊗ −i = ++i ⊗ −i = −i(cid:3)(cid:2)+i ⊗ −i⊕= −i ⊕ −2i = −i,2i(cid:2)or−0, if ⊕s(cid:3)is usedAgain, we stress that all these results are correct, but they differ in level of informativeness with respect to the strength ofthe resulting sign.5.3. Complexity of probabilistic inferenceFor quantitative probabilistic networks, in general, exact computation of probabilities is NP-hard [7]. The algorithms forprobabilistic inference in a probabilistic network, however, are known to behave polynomially under certain restrictionsconcerning the topology of the network’s digraph. In general, the sparser the digraph, the better most algorithms perform.The basic sign-propagation algorithm for inference in a basic qualitative network has a worst-case runtime complexitythat is polynomial in the number of nodes of the network’s digraph, regardless of the digraph’s topology. In a singly con-nected digraph, each pair of nodes is connected by a single simple trail. Upon sign-propagation, therefore, each variable Ais visited at most once to receive the single sign which is the sign-product of the sign of observation and the signs asso-ciated with the arcs on the trail between A and the observed variable. In a multiply connected graph, two nodes can beconnected by more than one simple trail. As a result, a variable should be visited as many times as the number of activesimple trails between that variable and the observed variable to receive the sign of influence along each of those trails. Tolimit this possibly exponential number of visits to a variable, the basic propagation algorithm exploits the fact that nodesigns can only change twice: once from ‘0’ to ‘+’, ‘−’ or ‘?’ and then only to ‘?’. As variables therefore need to be visitedat most twice, and each visited variable inspects and constructs a message for at most all other variables, we have that thebasic propagation algorithm halts after a number of operations that is polynomial in the number of nodes in the network’sdigraph.The basic formalism of qualitative probabilistic networks does not allow for resolving trade-offs, as combining two con-flicting influences with the basic ⊕-operator immediately results in an ambiguous node-sign. From the example in theprevious section, we have that the enhanced ⊕- and ⊕s-operators do provide for resolving some trade-offs, using the ad-ditional information carried by the enhanced and augmented signs. The possibility of resolving trade-offs, however, comesat the expense of efficiency of sign-propagation. This is not surprising, since qualitative trade-off resolution is also knownto be NP-hard [23]. The main difference between sign-propagation in a basic qualitative probabilistic network and sign-propagation in an enhanced network is that, in multiply connected digraphs, the limit of two visits to each variable nolonger applies. Although a variable’s basic enhanced node-sign can change at most three times—from zero to strong, toweak and then to ambiguous—the multiplication-index lists associated with the sign may require updating each time thevariable is inspected. The difference between using the fully enhanced ⊕-operator and the simple enhanced ⊕s-operator isthat in the latter case the multiplication-index list of a sign is restricted to a single index, which may require an updateless often. Using the simple enhanced ⊕s-operator may therefore turn out to be more efficient in practice. Further researchis still required, however, to determine the exact complexity class to which inference in an enhanced qualitative networkbelongs and to determine whether the enhanced ⊕- and ⊕s-operators really differ in complexity. Recent results indicatethat for networks in which the enhanced qualitative signs are translated into intervals, interval-propagation is NP-hard [21].We conclude that there exists a trade-off between the amount of information present in inference results after sign-propagation and the complexity of the propagation algorithm. Inference using the basic sign-propagation algorithm hasa runtime complexity that is polynomial in the number of nodes of a qualitative network’s digraph, but always leads toambiguous results when the network models a trade-off. Inference using the enhanced operators may perhaps becomeexponential, but does enable the resolving of trade-offs without resorting to numerical information.6. Related workThe problem of trade-off resolution within the framework of qualitative networks has been addressed before by differentresearchers. In this section we briefly review this related work.S. Parsons introduced the concept of categorical influence [25]. A categorical influence is a qualitative influence thatserves either to increase a probability to 1 or to decrease a probability to 0, disregarding all other influences. For exam-[++]( A, B) of a variable A on a variable B is defined as Pr(b | ax) = 1 for all relevantple, a positive categorical influence Svariables X . A categorical influence thus serves to resolve any trade-off in which it is involved, but can only capture de-terministic relationships between nodes; in real-life applications few to none of such relationships will exist. Parsons alsostudied the use of both relative and absolute order-of-magnitude reasoning in the context of qualitative probabilistic net-works [25]. Using the relative order-of-magnitude system rom[k] [8], Parsons relates different qualitative influences to eachother by specifying one qualitative influence as being, respectively, negligible with respect to, distant from, comparable to, orclose to another influence. The use of relative orders of magnitude thus serves to relate the strengths of different influences,but it requires the specification of a relation between all pairs of influences, instead of a notion of strength per influence.In addition, due to the vague interpretations of the above terms, the relations used seem to be ill-defined, which makesreasoning with them anything but intuitive. For absolute order-of-magnitude reasoning, Parsons proposes a method thatrevolves around the propagation of abstract intervals between −1 and 1, that correspond to labels like ‘Strongly Positive’,S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941491‘Weakly Positive’, etc. Two different sets of labels are required: one for modelling influences that are associated with thearcs in the network’s digraph, and one for modelling changes that occur at the nodes in the graph (comparable to ‘node-signs’). The intervals corresponding to a set of labels do not overlap and together span the interval [−1, 1]. The boundariesof the intervals, however, are not actually quantified, but set to be α, β, etc.; this approach is therefore comparable to ourtreatment of the cut-off value. Probabilistic inference is based on propagating and combining the abstract intervals; theinterval comparisons required to this end are done using (cid:2)int , where [α, β] (cid:2)int [γ , δ] iff α (cid:2) γ and β (cid:2) δ. Note that if oneinterval is considered larger than another with this operator, then they may in fact overlap. To prevent considerable loss ofinformation, assumptions about the actual values of the interval boundaries have to be made.κ -calculus [30] can be considered another absolute order-of-magnitude system and was proposed as a qualitative ver-sion of probabilistic reasoning by M. Goldszmidt and J. Pearl [16]. Using a probabilistic interpretation of the κ -calculus,probabilities can be abstracted to κ -values, where a κ -value of n indicates that the associated probability has the sameorder of magnitude as (cid:10)n for some infinitesimal number (cid:10). This interpretation was subsequently applied in the contextof probabilistic networks, by replacing all (conditional) probabilities with κ -values and computing posterior κ -values usingκ -calculus [9]. More recently, we used the interpretation in another approach to enhance the expressiveness of qualitativeprobabilistic networks [29]. With this approach, an interval of κ -values is associated with the sign of an influence to captureits possible strengths. These κ -intervals are propagated along with the qualitative network’s signs. As a result of the wayκ -values are defined, however, propagation results are only guaranteed to be correct for infinitesimal probabilities. Anotherdrawback of the use of κ -values is that their definition is not very intuitive and such values are therefore hard for domainexperts to specify and interpret.Categorical influences, order-of-magnitude reasoning and κ -calculus are of a purely qualitative nature, yet serve forresolving some trade-offs. C.-L. Liu and M.P. Wellman designed methods for resolving trade-offs based upon the idea ofreverting to numerical probabilities whenever necessary [23]. They propose to reason with a probabilistic network in aqualitative way, thereby exploiting the efficiency of sign-propagation, and only reverting to the full quantification whenevera trade-off leads to an ambiguous result. Two methods are described for resolving the trade-off. The first method providesfor incrementally applying numeric inference to the point where qualitative reasoning can produce a decisive result. That is,a trade-off between two variables is resolved numerically and then abstracted into a net qualitative influence between thetwo variables. The second method amounts to estimating bounds on the net influence along the trails that give rise to atrade-off. These bounds are then again used to compute the qualitative sign of the net influence. The methods presented byLiu and Wellman resolve any trade-off present in the network, but require the availability of an already fully specified, nu-merical probabilistic network. As such, their methods are less interesting for use in the construction phase of a probabilisticnetwork.We would like to mention that several other approaches to dealing with uncertainty in a qualitative way have beenproposed in the literature. As these approaches are not tailored specifically to qualitative probabilistic reasoning or for usewithin the framework of qualitative probabilistic networks, we do not review them here.7. Conclusions and further researchQualitative probabilistic networks can be used to overcome, to at least some extent, the quantification problem known toprobabilistic networks. Qualitative networks in essence are qualitative abstractions of their quantitative counterparts: whilein a probabilistic network relationships between variables are quantified by probabilities, these relationships are expressedby qualitative signs in qualitative probabilistic networks. As a result of their coarse level of representation detail, qualitativenetworks lack the expressive power that allows for resolving trade-offs the way probabilistic networks do.Since qualitative probabilistic networks are more and more recognised as useful tools in different stages of the construc-tion and verification of quantitative probabilistic networks for real-life application domains, we feel that it is important thatthe qualitative formalism is as expressive as possible in order to derive as much information as possible from a qualita-tive network. The formalism of enhanced qualitative networks provides for a step into making qualitative networks moreapplicable, by providing for trade-off resolution in a qualitative way. To this end, we have distinguished between strongand weak influences. We have further enhanced the multiplication and addition operators to guarantee the transitivity andparallel-composition properties of influences. Unfortunately, the additional expressiveness of our enhancement comes atthe expense of the property of symmetry of influences, where the strength of the influence is concerned. To handle theasymmetry of an influence’s strength we have proposed specifying two influences for each arc. With these enhancementswe have generalised the basic sign-propagation algorithm to apply to enhanced qualitative networks. We have shown thatour formalism provides for resolving at least some trade-offs in a qualitative way, that is, without having to fall back onnumerical information.To distinguish between weak and strong influences, we have introduced additional signs and augmented all signs withmultiplication-index lists. Since it may be difficult for domain experts to interpret the meaning of such lists of indices,it is not our intention to output the augmented signs. The multiplication indices are merely used internally for trade-offresolution; the output of inference, as in a basic qualitative network, is a basic sign for each variable that indicates whetherthe net influence of an observation on that variable is positive, negative, zero or ambiguous. If desirable, an additionallevel of strength can be added by introducing, for example, ‘+ + +’ and ‘− − −’ signs using an additional cut-off value andredefining the ⊗- and ⊕-operators. This would, however, render these operators far more complex. Alternatively, signs with1492S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494a multiplication index other than 1 could be allowed on the arcs of the enhanced network’s digraph; this option can beimplemented directly in our current enhanced framework. Both options, however, would require domain experts to be ableto distinguish between more than two levels of strength.When the sign-propagation algorithm is used with the enhanced ⊕-operators, it becomes less efficient than the basicsign propagation algorithm. In fact, inference may then in theory become infeasible. Further research will be necessary todetermine the actual complexity of sign-propagation with the enhanced operators in real-life qualitative networks. Twoapproaches can, however, be used to bound the complexity of inference. The first approach amounts to posing a limiton the number of sign-additions performed for a single variable. If this limit is reached, the node-sign of the variable ischanged into a basic sign (‘+’, ‘−’, ‘0’, or ‘?’) and the basic sign-propagation algorithm is used for further propagation. Notethat this approach may lead to weaker, but correct, results. The other approach is to use enhanced signs only in smallparts of the network, that is, in those parts where trade-offs reside. In constructing the enhanced network, we then focuson the multiply connected parts of the network’s digraph and ask the domain experts whether the possible parallel trailsbetween variables consist of conflicting influences. If so, enhanced signs are elicited for the influences on these trails. Duringinference, the trade-off can be locally resolved using the enhanced sign-propagation algorithm, and the basic sign for thenet influence is then used for further propagation with the basic sign-propagation algorithm. Another advantage of suchlocal computation with enhanced signs is that it requires only local specification of such signs. As a consequence, duringthe elicitation of signs, domain experts then only have to compare differences in strengths for small sets of influences.Since correctly specifying strengths will be harder for experts than correctly specifying the basic sign for an influence, localspecification of enhanced signs will make the resulting signs less prone to error. Local specification also allows for differentinterpretations of strong and weak influences for different parts of the network, that is, it allows for different cut-off valuesto be (implicitly) used in different parts of the network. This may in addition simplify the elicitation of signs from domainexperts.We conclude that including a notion of strength is a logical extension to the original formalism of qualitative probabilis-tic networks. We have formalised such a notion of strength and shown how to cope with it upon qualitative probabilisticinference in a mathematically correct way. As such, we have enhanced the expressiveness of qualitative probabilistic net-works, albeit at the expense of convenient properties such as symmetry of influences, some algebraic properties of theoperators for combining signs, and complexity of inference. Although further research into these latter issues is required,our enhancement has broadened the range of possible applications of qualitative probabilistic networks.Appendix A. Additional proofs of propositionsProof of Proposition 4.8 ( ++I ⊕ ++ J ⇒ ++I ∪ J ). The proof proceeds in a similar fashion as the proof of Proposition 4.7.We again assume that the positive influence along trail t2 is composed of two separate positive influences, with similarobservations applying when both influences are negative. Since the indirect influence of variable A on variable C alongtrail t2 is strongly positive, it must be composed of two strongly positive direct influences. We thus have that++S( A, B) and S++(B, C)++ J( A, C, t2) with J = {2}. We now investigate the difference between the two functions f and hand therefore that Sdefined in the proof of Proposition 4.7. Since the influence of the variable A on the variable B is strongly positive, we havethat the difference between the two parameters for f and h should be at least α. To establish the minimum differencebetween f (Pr(b | ax)) and h(Pr(b | ¯ax)), we once again consider the graph from Fig. 7(a); similar observations again holdfor the graph from Fig. 7(b). Under the constraint mentioned above, it is readily seen that the minimal difference betweenf (Pr(b | ax)) and h(Pr(b | ¯ax)) is attained for f (α) and h(0). We find thatPr(c | axy) − Pr(c | ¯axy) (cid:2) f (α) − h(0)(cid:2)(cid:3)Pr(c | aby) − Pr(c | a¯b y)=· α + Pr(c | a¯b y) − Pr(c | ¯a¯b y)(cid:2) α2 + αWe conclude that the composite influence of variable A on variable C is strongly positive with multiplication-indexlist {1, 2}.More in general, we find that the strength of the composite influence is at least the sum of the two polynomials in α,captured by the multiplication-index lists I and J , respectively, that is, we conclude that the composite influence equalsˆS( A, C, t1 (cid:15) t2). (cid:2)++I ∪ JProof of Proposition 4.9 ( ++I ⊕ + J ⇒ ++I ). We distinguish between two different cases:(I) the trail t1 consists of a single arc and the trail t2 consists of the arcs A → B, B → C for some variable B;(II) the trail t1 consists of the arcs A → B, B → C and the trail t2 consists of the single arc.S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–14941493For each of these cases, the proof proceeds in a similar fashion as the proof of Proposition 4.7. First we address case (I).As before, we assume that the indirect weakly positive influence of variable A on variable C along trail t2 is composed oftwo separate weakly positive influences; the proofs for the other possible situations again are analogous. To establish theminimal difference between the functions f and h defined in the proof of Proposition 4.7, we once again consider the graphfrom Fig. 7(a). Since the influence of variable A on variable B is weakly positive, the difference between the two parametersfor f and h should be at most α. Under this constraint, the minimal difference between f (Pr(b | ax)) and h(Pr(b | ¯ax)) isattained for f (0) and h(0). We thus find thatPr(c | axy) − Pr(c | ¯axy) (cid:2) f (0) − h(0) = Pr(c | a¯b y) − Pr(c | ¯a¯b y)Since the direct influence of variable A on variable C is strongly positive, we have that Pr(c | axy) − Pr(c | ¯axy) (cid:2) α. Weconclude that the composite influence of variable A on variable C is strongly positive with multiplication-index list I = {1},that is, we conclude that the composite influence equals ˆS( A, C, t1 (cid:15) t2) in case (I).++IWe now consider the minimal difference between the two functions f and h in case (II). We again assume that theindirect positive influence of A on C along trail t1 is composed of two separate positive influences, with similar observationsapplying when both influences are negative. Since the indirect influence of A on C now is strongly positive, we have fromTable 2 that the two separate influences from A to B and from B to C must both be strongly positive. We thus have that++S( A, B) and S++(B, C)++Iand, therefore, that ˆS( A, C, t1), with I = {2}. Since the influence of variable B on variable C is positive, we have that thetwo functions f and h are both linearly increasing. Since the influence of A on B is strongly positive, we further have thatparameter Pr(b | ax) for the function f should be greater than the parameter Pr(b | ¯ax) for the function h, with a differenceof at least α. To establish the minimum difference between f (Pr(b | ax)) and h(Pr(b | ¯ax)), we again consider the graphfrom Fig. 7(a), with similar observations applying for the graph from Fig. 7(b). Under the constraints mentioned above, weobserve that the minimal difference between f (Pr(b | ax)) and h(Pr(b | ¯ax)) is attained for f (α) and h(0). We thus find that(cid:3)(cid:2)Pr(c | aby) − Pr(c | a¯b y)Since the direct influence of A on C is weakly positive, we have that 0 (cid:3) Pr(c | a¯b y) − Pr(c | ¯a¯b y) (cid:3) α. We conclude thatthe composite influence of variable A on variable C is strongly positive with multiplication-index list I = {2}, that is, weconclude that the composite influence equals ˆSPr(c | axy) − Pr(c | ¯axy) (cid:2) f (α) − h(0) =· α + Pr(c | a¯b y) − Pr(c | ¯a¯b y)( A, C, t1 (cid:15) t2) in case (II). (cid:2)++IProof of Proposition 5.2 (algebraic properties). The fact that the enhanced ⊗-, ⊕-, and ⊕s-operators are commutativefollows directly from the symmetry in their respective tables. To prove that the enhanced ⊗-operator is associative, wedistinguish between a number of cases.We first observe that the property trivially holds if one of the signs used with the enhanced ⊗-operator is either a ‘0’or a ‘?’. Now consider combining with this operator three signs, be they positive or negative, that are either all weak or allstrong. Since combining two strong signs with their respective multiplication-index lists results in a strong sign augmentedwith the sum of those multiplication-index lists, the order of combination of three such signs will not affect the resultingsign and the property of associativity holds. The same argument applies to combining all weak signs.Now consider the case where the enhanced ⊗-operator is used to combine a single weak sign δ I with two strong signsδδ J and δδ K . If all signs are positive, then(cid:3)(cid:2)δ I ⊗ δδ J(cid:2)δδ J ⊗ δδ Kδ I ⊗⊗ δδ K = δ I ⊗ δδ K = δ Iand(cid:3)= δ I ⊗ δδ J +K = δ ISimilar results hold regardless of whether the signs involved are positive or negative.Finally we consider the case where the enhanced ⊗-operator is used to combine two weak signs δ I and δ J with a singlestrong sign δδ K . If all signs are positive, then(cid:3)(cid:2)δ I ⊗ δ J(cid:2)δ J ⊗ δδ Kδ I ⊗⊗ δδ K = δ I+ J ⊗ δδ K = δ I+ J= δ I ⊗ δ J = δ I+ J(cid:3)andwith similar results holding regardless of whether the signs involved are positive or negative.We conclude based upon the above observations and commutativity of the ⊗-operator that the operator is associa-tive. (cid:2)References[1] B. Abramson, ARCO1: An application of belief networks to the oil market, in: B.D. D’Ambrosio, P. Smets, P.P. Bonissone (Eds.), Proceedings of theSeventh Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Mateo, California, 1991, pp. 1–8.[2] B. Abramson, J. Brown, A. Murphy, R.L. Winkler, Hailfinder: A Bayesian system for forecasting severe weather, International Journal of Forecasting 12(1996) 57–71.1494S. Renooij, L.C. van der Gaag / Artificial Intelligence 172 (2008) 1470–1494[3] E.E. Altendorf, A.C. Restificar, T.G. Dietterich, Learning from sparse data by exploiting monotonicity constraints, in: F. Bacchus, T. Jaakkola (Eds.), Pro-ceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, AUAI Press, Corvallis, Oregon, 2005, pp. 18–25.[4] S. Andreassen, M. Woldbye, B. Falck, S.K. Andersen, MUNIN. A causal probabilistic network for interpretation of electromyographic findings, in: J. Mc-Dermott (Ed.), Proceedings of the Tenth International Conference on Artificial Intelligence, Morgan Kaufmann Publishers, Los Altos, California, 1987,pp. 366–372.[5] I.A. Beinlich, H.J. Suermondt, R.M. Chavez, G.F. Cooper, The alarm monitoring system: A case study with two probabilistic inference techniques forbelief networks, in: J. Hunter, J. Cookson, J. Wyatt (Eds.), Proceedings of the Second Conference on Artificial Intelligence in Medicine, Springer-Verlag,Berlin, 1989, pp. 247–256.[6] C.P. de Campos, F.G. Cozman, Belief updating and learning in semi-qualitative probabilistic networks, in: F. Bacchus, T. Jaakkola (Eds.), Proceedings ofthe Twenty-First Conference on Uncertainty in Artificial Intelligence, AUAI Press, Corvallis, Oregon, 2005, pp. 18–25.[7] G.F. Cooper, The computational complexity of probabilistic inference using Bayesian belief networks, Artificial Intelligence 42 (1990) 393–405.[8] P. Dague, Symbolic reasoning with relative orders of magnitude, in: R. Bajcsy (Ed.), Proceedings of the Thirteenth International Joint Conference onArtificial Intelligence, Morgan Kaufmann Publishers, San Mateo, California, 1993, pp. 1509–1514.[9] A. Darwiche, M. Goldszmidt, On the relation between kappa calculus and probabilistic reasoning, in: Proceedings of the Tenth Conference on Uncer-tainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Francisco, California, 1994, pp. 145–153.[10] M.J. Druzdzel, Probabilistic Reasoning in decision support systems: From computation to common sense, PhD Thesis, Department of Engineering andPublic Policy, Carnegie Mellon University, Pittsburgh, Pennsylvania, 1993.[11] M.J. Druzdzel, M. Henrion, Efficient reasoning in qualitative probabilistic networks, in: R. Fikes, W. Lehnert (Eds.), Proceedings of the Eleventh NationalConference on Artificial Intelligence, AAAI Press, Menlo Park, California, 1993, pp. 548–553.[12] M.J. Druzdzel, M. Henrion, Intercausal reasoning with uninstantiated ancestor nodes, in: D. Heckerman, A. Mamdani (Eds.), Proceedings of the NinthConference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Francisco, California, 1993, pp. 317–325.[13] M.J. Druzdzel, L.C. van der Gaag, Elicitation of probabilities for belief networks: Combining qualitative and quantitative information, in: Ph. Besnard, S.Hanks (Eds.), Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Francisco, California,1995, pp. 141–148.[14] M.J. Druzdzel, L.C. van der Gaag, Building probabilistic networks: Where do the numbers come from?—Guest editors’ introduction, IEEE Transactionson Knowledge and Data Engineering 12 (2000) 481–486.[15] A. Feelders, L.C. van der Gaag, Learning Bayesian network parameters under order constraints, International Journal of Approximate Reasoning 42(2006) 37–53.[16] M. Goldszmidt, J. Pearl, Reasoning with qualitative probabilities can be tractable, in: D. Dubois, M.P. Wellman, B. D’Ambrosio, P. Smets (Eds.), Proceed-ings of the Eighth Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Mateo, California, 1992, pp. 112–120.[17] E.M. Helsper, L.C. van der Gaag, A.J. Feelders, W.L.A. Loeffen, P.L. Geenen, A.R.W. Elbers, Bringing order into Bayesian-network construction, in: Pro-ceedings of the Third International Conference on Knowledge Capture, ACM Press, New York, 2005, pp. 121–128.[18] A.L. Jensen, Quantification experience of a DSS for mildew management in winter wheat, in: M.J. Druzdzel, L.C. van, M. Henrion, F.V. Jensender Gaag(Eds.), Working Notes of the IJCAI Workshop on Building Probabilistic Networks: Where Do the Numbers Come From?, AAAI Press, 1995, pp. 23–31.[19] D. Kahneman, P. Slovic, A. Tversky, Judgment under Uncertainty: Heuristics and Biases, Cambridge University Press, Cambridge, 1982.[20] M. Korver, P.J.F. Lucas, Converting a rule-based expert system into a belief network, Medical Informatics 18 (1993) 219–241.[21] J. Kwisthout, G. Tel, Complexity results for enhanced qualitative probabilistic networks, in: M. Studeny, J. Vomlel (Eds.), Proceedings of the ThirdWorkshop on Probabilistic Graphical Models, Prague, 2006, pp. 171–178.[22] S.L. Lauritzen, D.J. Spiegelhalter, Local computations with probabilities on graphical structures and their application to expert systems, Journal of theRoyal Statistical Society, Series B 50 (1988) 157–224.[23] C.-L. Liu, M.P. Wellman, Incremental tradeoff resolution in qualitative probabilistic networks, in: G.F. Cooper, S. Moral (Eds.), Proceedings of the Four-teenth Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Francisco, California, 1998, pp. 338–345.[24] P.J.F. Lucas, Bayesian network modelling through qualitative patterns, Artificial Intelligence 163 (2005) 233–263.[25] S. Parsons, Refining reasoning in qualitative probabilistic networks, in: Ph. Besnard, S. Hanks (Eds.), Proceedings of the Eleventh Conference on Uncer-tainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Francisco, California, 1995, pp. 427–434.[26] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann Publishers, Palo Alto, California, 1998.[27] S. Renooij, Qualitative approaches to quantifying probabilistic networks, Ph.D. Thesis, Institute for Information and Computing Sciences, Utrecht Uni-versity, The Netherlands, 2001.[28] S. Renooij, L.C. van der Gaag, From qualitative to quantitative probabilistic networks, in: A. Darwiche, N. Friedman (Eds.), Proceedings of the EighteenthConference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Francisco, California, 2002, pp. 422–429.[29] S. Renooij, S. Parsons, P. Pardieck, Using kappas as indicators of strength in qualitative probabilistic networks, in: T.D. Nielsen, N.L. Zhang, (Eds.),Proceedings of the Seventh European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, Lecture Notes in ArtificialIntelligence, 2003, pp. 87–99.[30] W. Spohn, A general non-probabilistic theory of inductive reasoning, in: R.D. Shachter, T.S. Levitt, L.N. Kanal, J.F. Lemmer (Eds.), Uncertainty in ArtificialIntelligence, 4, Elsevier, Amsterdam, 1990, pp. 149–158.[31] L.C. van der Gaag, S. Renooij, C.L.M. Witteman, B.M.P. Aleman, B.G. Taal, Probabilities for a probabilistic network: A case-study in oesophageal carcinoma,Artificial Intelligence in Medicine 25 (2002) 123–148.[32] L.C. van der Gaag, H.L. Bodlaender, A. Feelders, Monotonicity in Bayesian networks, in: M. Chickering, J. Halpern (Eds.), Proceedings of the TwentiethConference on Uncertainty in Artificial Intelligence, AUAI Press, Arlington, Virginia, 2004, pp. 569–576.[33] L.C. van der Gaag, E.M. Helsper, Defining classes of influences for the acquisition of probability constraints for Bayesian networks, in: R. López deMántaras, L. Saitta (Eds.), Proceedings of the Sixteenth European Conference on Artificial Intelligence, IOS Press, Amsterdam, 2004, pp. 1101–1102.[34] M.P. Wellman, Fundamental concepts of qualitative probabilistic networks, Artificial Intelligence 44 (1990) 257–303.