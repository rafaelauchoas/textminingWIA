Artificial Intelligence 197 (2013) 25–38Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintConstraint propagation as information maximization ✩A. Nait Abdallah a,b, M.H. van Emden c,∗a Department of Computer Science, University of Western Ontario, Canadab INRIA Rocquencourt, Francec Department of Computer Science, University of Victoria, Canadaa r t i c l ei n f oa b s t r a c tArticle history:Received 26 January 2012Received in revised form 11 February 2013Accepted 14 February 2013Available online 16 February 2013Keywords:Constraint-satisfaction problemsInformation partial orderIntervalsPropagationThis paper draws on diverse areas of computer science to develop a unified view ofcomputation:• Optimization in operations research, where a numerical objective function is maximizedunder constraints, is generalized from the numerical total order to a non-numericalpartial order that can be interpreted in terms of information.• Relations are generalized so that there are relations of which the constituent tuplesindexes, whereas in other relations these indexes are variables.have numericalThe distinction is essential in our definition of constraint-satisfaction problems.• Constraint-satisfaction problems are formulated in terms of semantics of conjunctions ofatomic formulas of predicate logic.• Approximation structures, which are available for severalapplied to solutions of constraint-satisfaction problems.important domains, areAs application we treat constraint-satisfaction problems over reals. These cover a large partof numerical analysis, most significantly nonlinear equations and inequalities. The chaoticalgorithm analyzed in the paper combines the efficiency of floating-point computation withthe correctness guarantees of arising from our logico-mathematical model of constraint-satisfaction problems.© 2013 Elsevier B.V. All rights reserved.1. Computation as maximization in information spaceThe early history of constraint processing is written in three MIT theses: Sutherland’s, Waltz’s, and Steele’s [16,20,14].Already in this small selection one can discern two radically different approaches. Sutherland and Steele use relaxation:starting form a guessed assignment of values to variables, constraints are successively used to adjust variables in such away as to satisfy better the constraint under consideration. These authors followed an old idea brought into prominenceunder the name of relaxation by Southwell [15].Waltz adopted a radically different approach (and was, to our knowledge, the first to do so). He associated with each ofthe problem’s variables a domain; that is, the set of all values that are not a priori impossible. Each constraint is then usedto eliminate values from the domains of one or more variables affected by the constraint that are incompatible with thatconstraint. In this paper we are concerned with the latter method, which we call the domain reduction method.✩Research Report 746, Dept. of Computer Science, University of Western Ontario, Canada.* Corresponding author.E-mail address: vanemden@cs.uvic.ca (M.H. van Emden).0004-3702/$ – see front matter © 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2013.02.00226A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–38The attraction of domain reduction is its completeness for finite domains: if a solution exists, then it will be found. Thisin contrast with relaxation, which can flounder forever.1In this paper we present domain reduction as an example of the view of computation as monotonic gain of information. Thisview was pioneered by Dana Scott, who was the first to make mathematical sense [12] of a recursively defined function f .f a sequence a of partial functions. If x is such that f (x) requires aHe did this by associating with the definition ofrecursion depth is at most n, then an(x) is defined and equal to f (x); otherwise an(x) is undefined. Thus a is a sequence ofpartial functions in which each function agrees with the previous one, but is “more defined”.In general, if two partial functions g and h of the same type are such that h is defined wherever g is and such thatthey have the same value when both are defined, then Scott proposed to regard g as an approximation to h and noted thatthis notion of approximation is a partial order in the set of partial functions of the same type. Moreover Scott proposedto transfer the information concept from random variables, as it was in Shannon’s information theory, to partial functions,noting that a partial function can be regarded as containing more information than partial functions approximating it.The approach to the semantics of recursive definitions can be summarized by saying that every such definition can beregarded as the limit of a sequence of approximations each containing more information about the limit of the sequencethan the previous one.Scott was aware that it might seem somewhat far-fetched to give such an interpretation to the notion of “information”.As a justification Scott [13] gave another example of a set partially ordered by information: that of numerical intervals.Although this certainly strengthened the case, this suggestion has not, as far as we know, been followed up. In this paperwe do so, motivated by the opportunities for deeper understanding of constraint solving.In numerical applications the view of computation as monotonic gain of information is more than a theoretically in-teresting insight: it is adds an essential capability. Suppose a conventional numerical computation is stopped after 1000iterations and yields 1.912837465 and that it yields 1.912877134 when allowed to run for 10,000 iterations, what do weknow about the improvement obtained, if any? If results, intermediate and final, were expressed as intervals we would, say,have [1.911, 1.938]2 after 1000 iterations and perhaps [1.9126, 1.9283]3 after 10,000 iterations. Here we see that we knowmore about the unknown solution as a result of the additional computational work. Rephrasing “knowing more” as “gain ininformation” suggests that the effect of iteration in interval arithmetic can be described as “monotonic gain of information”.The important qualification “monotonic” is there because in interval arithmetic we never need to settle for less informationas a result of additional computational work, though we may fail to make a gain. Moreover, such a stalling of progress is auseful criterion for halting the iteration.Because of the special importance of solving constraint-satisfaction problems over the reals by means of floating-pointarithmetic, we choose our example problem from this area. Section 3 gives the needed review of interval methods; Section 4describes the example. The new view of domain reduction as monotonic information gain is used in Section 6 to developthe method from first principles. This suggests regarding the set of constraints in a constraint-satisfaction problem as aformula in predicate logic with a fixed interpretation of predicate symbols. The standard semantics only assigns meaningsto closed formulas, whereas here we have a formula with free variables. Accordingly, in Section 5 we develop the requiredextension of the semantics of predicate logic. This needs a novel treatment of relations, also in this section.2. Related workFollowing Mackworth’s AC-3 algorithm [9] there are many other papers concerned with converging fair itera-tions [1,3,17,18,11].For historical references we refer to the textbooks [7,2].We address the connections with the work of Saraswat et al. [11] in Section 7.3. Interval arithmetic and interval constraintsTo facilitate the use of information in computation we do not use interval arithmetic directly, but indirectly via aconstraint-satisfaction problem (CSP). Such problems are solved by associating with each unknown a set of possible val-ues instead of the usual single value. This is especially appropriate for real-valued unknowns. In combination with the useof floating-point arithmetic, the sets of possible values take the form of intervals with floating-point numbers as bounds.This special case of CSP solving is called interval constraints [6,1].We introduce interval constraints by means of an example. In interval arithmetic the rule for adding intervals is[a, b] + [c, d] =(cid:2)(cid:3)x + y: x ∈ [a, b] ∧ y ∈ [c, d]1 But, as one may expect, domain reduction is no cure-all. For some problems, relaxation quickly finds a solution, and domain reduction requires aninfeasible amount of time. The n-queens problem for large n is an example. Van Hentenryck and Michel [19], page 89, mention n = 10,000 as a routineexample for relaxation in combination with their search technique.2 Note the smaller number of decimals: with intervals it becomes clear that additional decimals would be meaningless.3 The smaller interval warrants another decimal.A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–3827so that, e.g., [0, 2] + [0, 2] = [0, 4]. The analogous operation in interval constraints starts by defining the constraintsum(x, y, z) which holds between the reals x, y, and z iff x + y = z. In other words, the formula sum(x, y, z) is true wheneverx + y = z. This leads to the following inferencesum(x, y, z)x ∈ [0, 2] ∧ y ∈ [0, 2] ∧ z ∈ [−∞, +∞]x ∈ [0, 2] ∧ y ∈ [0, 2] ∧ z ∈ [0, 4]We use here the conventional format for inference: the premises above the horizontal line; the conclusion below. The aboveinference coincides, in this special case, with interval arithmetic. Only the interval for z is narrowed.In interval constraints we may have a priori constraints on all variables, as insum(x, y, z)x ∈ [0, 2] ∧ y ∈ [0, 2] ∧ z ∈ [3, 5]x ∈ [1, 2] ∧ y ∈ [1, 2] ∧ z ∈ [3, 4]Here the intervals for all three variables are narrowed. As a result, the effect of the operation can no longer be exclusivelycharacterized as an addition or as its inverse: the effect is a mixture of several operations. We can formulate the effectalgebraically as applying an operator, the contraction operator of the constraint sum, that maps triples of intervals to triplesof intervals, in this case as(cid:5)[0, 2], [0, 2], [3, 5](cid:5)[1, 2], [1, 2], [3, 4](cid:6)→(1)(cid:4)(cid:4).The right-hand side of (1) is the smallest triple (“box”) that can be inferred: any box that is strictly smaller would excludepoints that are possible according the given premises of the inference. Thus this box is the optimal solution to the givenconstraint-satisfaction problem. The optimal solution is obtained by one addition and two subtractions of interval arithmeticplus a few bound comparisons. Similarly efficient algorithms exist for some other constraints, such as product, integer power,trigonometric and logarithmic functions.We may express the contraction operator for the sum constraint as a mapping from a tuple B of intervals to the leastsuch tuple containing the intersection of B and the constraint.In general a CSP is a conjunction of many constraints. After applying the contraction operator for each of these once, itis often the case that another round of applications yields further contractions in the intervals for some of the variables.As the contractions are implemented in floating-point interval arithmetic and are assured valid by outward rounding, thereis a limit and it is reached after a finite number of rounds of contractions.In each of the rounds it may happen that a constraint is found that does not contain variables for which a bound haschanged. In such cases the contraction operator for that constraint has no effect and can be skipped. Algorithms have beendeveloped that perform such optimizations [1].4. An example of solving by interval constraintsLet us consider the problem of determining the intersection points of a parabola and a circle. For example, to solve thesystemy = x2x2 + y2 = 1(2)with x ∈ [0, 1] and y ∈ [0, 1]. One can eliminate y and solve instead x4 + x2 = 1, which has two real roots. However, forthe purpose of illustrating solving by interval constraints, we ignore this opportunity for simplification and we numericallysolve the original system (2).The method of interval constraints applies to a class of constraints in the form of equalities or inequalities betweenreal-valued expressions. The sum constraint in Section 3 is an example: it takes the form of the equation x + y = z. As wementioned in that section, there is an efficient implementation of the optimal contraction operator for it.The second equation in (2) is not primitive; it has to be transformed to an equivalent set of primitive constraints. In thisexample the primitive constraints sq, sum, and one are needed. The constraint sq(u, v) is defined as u2 = v, sum(u, v, w) isdefined as u + v = w, and one(u) is defined as u = 1. In this way (2) becomes the following set of constraints:(cid:2)(cid:3)sq(x, y), sq( y, z), sum( y, z, u), one(u).(3)The unknowns x, y, z, and u are real numbers. The introduction of z and u is the result of reducing the given constraints toprimitive ones. In more typical cases the given constraints are so complex that the introduced variables greatly outnumberthe original ones.121228A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–38In the example it is given that x, y and z satisfy the above constraints. From the original problem statement we havein addition that x ∈ [0, 1] and y ∈ [0, 1]. Of the auxiliary unknown z we initially know nothing: z ∈ [−∞, +∞] and u ∈[−∞, +∞].In effect, we have transformed (2) to the systemy = x2z = y2y + z = uu = 1(4)Instead of solving the original system (2) we solve equivalently the constraints (3). This is done by repeatedly applyingin arbitrary order the contraction operators until there is no change in any of the intervals associated with the unknowns.Applying the contraction operators of sq( y, z) and one(u) results in a drastic narrowing of the intervals for z and u: theychange from [−∞, +∞] to [0, 1] for z and to [1, 1] for u. After this, none of the contraction operators of the four constraintsresults in a change. Therefore this is as far as contraction operator application can take us.To obtain more information about possibly existing solutions, we split the CSP with interval X = [0, 1] for unknown xinto two CSPs that are identical except for the intervals of x. In the first CSP the interval for x is the left half of X ; in thesecond CSP it is the right half. Then we start another round of contraction operator applications starting from one of thehalves as initial box:(cid:6)(cid:7)x ∈0,,y ∈ [0, 1],z ∈ [0, 1],u ∈ [1, 1].(5)Applying the contraction operator for sq(x, y) results in y ∈ [0, 1/4]. Applying the contraction operator for sq( y, z) results inz ∈ [0, 1/16]. Applying the contraction operator for sum( y, z, u) results in u ∈ [0, 5/16]. Applying the contraction operatorfor one(u) causes the interval for u to become empty. This proves that there is no solution in the initial box (5).We now turn to the other half:(cid:6)(cid:7)x ∈, 1,y ∈ [0, 1],z ∈ [0, 1],u ∈ [1, 1].(6)Applying the contraction operator for sq(x, y) results in y ∈ [ 14 , 1]. Continuing in tabular form givesIntervalx[0.5, 1]Applysq(x, y)one(u)sum( y, z, u)sq( y, z)y[0, 1]4 , 1][ 1√3][ 14 , 12z[0, 1]][0, 34[ 116 , 34]u[1, 1][1, 1]Now the intervals for x and y continue getting smaller until the least floating-point box has been reached that contains5 − 1), while the intervals for y converge to a√(cid:8)a solution: the intervals for x converge to a small interval containingsmall interval containing 12 (5 − 1).√12 (5. Notation and terminology for relations and constraintsWe take it that (3) is intuitively clear, but how do we characterize mathematically any solutions that such a CSP mayhave and how do we characterize mathematically an algorithm for obtaining such a solution? Consider for example theconstraints sq(x, y) and sq( y, z). They clearly have something in common: sq, which must be some kind of relation. But theconstraints are different from each other (otherwise their conjunction could be simplified by dropping either of them) andalso different from sq, whatever that may be.In this section we develop a set-theoretic formulation of constraint-satisfaction problems and illustrate it by the examplein Section 4. We find that such a formulation is facilitated by a treatment of relations and operations on them that is in thespirit of the conventional treatment, but differs in details. In particular, we need to clarify the difference between relationsand constraints as well as the connection between these.5.1. FunctionsWe denote by S → T the set of total functions that are defined on S and have values in T . If f ∈ (S → T ) we say that f(cid:9)(cid:9) → T such that for all x ∈ S(cid:9) ⊆ S, then we define f S(cid:9) , the restriction of f to Sas the function in S(cid:9)“has type” S → T . If Swe have f S(cid:9) (x) = f (x).A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–38295.2. TuplesAs is the case conventionally, our relations are sets of tuples of the same arity. However, we need the possibility to indextuples either by variables or by the conventional indexes {0, 1, 2, . . .}. Hence we define a tuple as an element of the functionset I → T , where I is an arbitrary set to serve as index set. I → T is the type of the tuple.Example. If t is a tuple in {x, y} → R, then we may have t(x) = 1.1 and t( y) = 1.21.Example. t ∈ 3 → {a, b, c}, where 3 = {0, 1, 2} and t(0) = b, t(1) = c, and t(2) = c. In cases like this, where the index set isan ordinal, we use the compact notation t = [b, c, c]. In general, we write n for {0, . . . , n − 1}.When a function is regarded as a tuple, then the restriction operation on functions is called projection. E.g. if t = [2, 1, 3]and t(cid:9) = t{0,2}, then t(cid:9)(0) = 2 and t(cid:9)(2) = 3; t(cid:9)(1) is not defined.5.3. Approximation structuresIn [13] Dana Scott proposed that computation steps be viewed as transitions in a partially ordered space of data. In hisview computation consists of generating a time-ordered sequence d0, d1, d2, . . . with the property that the successive data diare each approximated by the previous in the sense of holding information about the limit of the sequence that is compat-ible and is at least as informative. We write d0 (cid:11) d1 (cid:11) d2 (cid:11) · · · where (cid:11) is the partial order.Scott was primarily interested in using his approach to model mathematically the evaluation of recursively defined func-tions. This requires mathematically rather sophisticated constructions. However, the idea also applies to situations coveredby the following definition.Definition 1. An approximation structure for a set D is a set A of subsets of D such that (1) A is closed under finiteintersection, (2) A is closed under intersection of (possibly infinite) ⊆-descending chains of subsets, and (3) A contains Das an element. The information order (cid:11) of A is defined as the inverse of the inclusion ⊆ of subsets.An approximation domain is a pair (cid:12)D, A(cid:13) formed by a set D and an approximation structure A on D. It turns out tobe tiresome to say “an approximation domain (D, A) for some A”, so that we may speak of “an approximation domain D”when no ambiguity arises regarding A.Lemma 1. If D(cid:9) ⊆ D, then there exists in any approximation structure for D a ⊆-least element containing D(cid:9).Definition 2. If A is an approximation structure for D, then for Dcontaining D.(cid:9)(cid:9) ⊆ D we define α A(D(cid:9)) to be the least element of AThe set α A(D(cid:9)) corresponds to the maximum amount of information about D(cid:9)that is expressible within approximationstructure A.Example. The intervals form an approximation structure in the set R of real numbers, where we define an interval as{x ∈ R: a (cid:2) x (cid:2) b}, where a ∈ R ∪ {−∞} and b ∈ R ∪ {+∞}. We write [a, b] for this interval. Note that with this definition,e.g., +∞ /∈ [0, +∞].Example. Let F be a subset of the set R of reals. The F -intervals are an approximation structure in R, where an F -interval is {x ∈ R: a (cid:2) x (cid:2) b} where a ∈ F ∪ {−∞} and b ∈ F ∪ {+∞}. An important case: F is the set of finite double-lengthIEEE-standard floating-point numbers. The latter include −∞ and +∞, so that pairs of these numbers are a convenientrepresentation for the elements of this approximation structure.5.4. RelationsA relation is a set of tuples with the same type. This type is the type of the relation.If r is a relation with type I → T , then the projection of r on I(cid:9) ⊆ I is { f(cid:9) ∈ I(cid:9) → T : ∃ f ∈ r. f I (cid:9) = f(cid:9)} and denoted πI (cid:9) r.Example. sum = {[x, y, z] ∈ (3 → R): x + y = z} is a relation of type 3 → R, where 3 = {0, 1, 2}. Compare this relation tothe relation σ = {s ∈ ({x, y, z} → R): sx + s y = sz}. As their types are different, they are different relations; [2, 2, 4] ∈ sum isnot the same tuple as s ∈ σ where sx = 2, s y = 2, and sz = 4.Example. If S has one element, then a relation of type S → T is a unary relation. Such a relation is often identified witha subset of T . For example, for a in R ∪ {−∞} and b in R ∪ {+∞}, { f ∈ ({x} → R): a (cid:2) f x (cid:2) b} is a unary relation thatis often identified with the interval [a, b]. Maintaining the distinction between the two is important in the current setting(see Section 5.6).30A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–38Definition 3. If r0 and r1 are relations with types I0 → T and I1 → T , respectively, then the join r0 (cid:2) r1 of r0 and r1 is(cid:2)f ∈ (I0 ∪ I1) → T : f I0∈ r0 and f I1∈ r1(cid:3).The join of relations that have disjoint index sets is called the product of these relations.We avoid the term “Cartesian product” because it is usually understood to consist of tuples with index set {0, . . . , n − 1}for some natural number n.Definition 4. Let r be a relation of type I → T and let I ⊆ J . Then we write the cylinder on r with respect to J as π −1define it as the greatest relation g ⊆ ( J → T ) such that πI g = r.Jr andCylindrification is inverse to projection in the sense that πI (π −1Jr) = r.Definition 5. Let I = {i0, . . . , in−1} be an index set. A box is a product of unary relations r0 ⊆ {i0} → D, . . . , rn−1 ⊆{in−1} → D. In case r0, . . . , rn−1 are intervals, then one may refer to the box as an interval box.5.5. Boxes as approximation domainLemma 2. Let I = {i0, . . . , in−1} be a finite index set and let B be the set of boxes of type I → D. Then (cid:12)I → D, B(cid:13) is an approximationdomain.Proof. We need to show the three defining properties (Definition 1). In this case one can show closure under arbitraryfinite or infinite intersection, so that the first two properties can be established simultaneously.Let {r j:Letj ∈ J } be a possibly infinite family of boxes, r j = rj0(cid:2) · · · (cid:2) rjn−1, with rjk⊆ {ik} → D for all k ∈ n.r =(cid:9)j∈ Jr j =(cid:4)rj0(cid:9)j∈ J(cid:2) · · · (cid:2) rjn−1(cid:5).Thenf ∈ r =(cid:9)j∈ Jr j ⇔ ∀ j ∈ J . f ∈ r j ⇔ ∀ j ∈ J .∀k ∈ n. f ik∈ rjk⇔ ∀k ∈ n.∀ j ∈ J . f ik(cid:9)⇔ f ∈(cid:2) · · · (cid:2)j0rj∈ J(cid:9)j∈ Jrjk∈ rjk⇔ ∀k ∈ n. f ik∈(cid:9)j∈ Jrjn−1.Hence(cid:9)j∈ Jr j =(cid:9)j∈ Jrj0(cid:2) · · · (cid:2)(cid:9)j∈ Jrjn−1is also a box, so that the intersection of a possibly infinite family of boxes is a box.We finally need to show that the full relation r = I → D is a box. Letting rk = {ik} → D, we have thatI → D ={i0, . . . , in−1} → D =={in−1} → Dr0 (cid:2) · · · (cid:2) rn−1(cid:5)(cid:4)(cid:2) · · · (cid:2)(cid:4){i0} → D(cid:5)is a box. (cid:3)Therefore, for every relation r of type {i0, . . . , in−1} → D there is a least box containing r, which justifies the followingdefinition.Definition 6. The box operator applied to a relation r with type {i0, . . . , in−1} → D is the least box (cid:3)r that contains r.A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–38315.6. ConstraintsA constraint is a syntactic entity that is used to denote a relation. A constraint has the form of an atomic formula in atheory of predicate logic without function symbols. The semantics of predicate logic assigns a relation r to an atomic formulap(q0, . . . , qn−1) with set V of variables. The relation r depends on the interpretation of p and on the tuple [q0, . . . , qn−1]of arguments. These arguments are variables, not necessarily all different. The first-order predicate logic interpretation ofthe language of atomic formulas, which identifies the argument occurrences by numerical indexes, forces n = {0, . . . , n − 1}to be the index set of the relation M(p), the relation that is the meaning of the predicate symbol p under the giveninterpretation. In our setting, instead, the index set associated with the constraint denoted by p(q0, . . . , qn−1) is the set Vof distinct variables occurring in atomic formula p(q0, . . . , qn−1).The interpretation M that assigns a relation of type {0, . . . , n − 1} to an n-ary predicate symbol p needs to be extendedto an interpretation M that also assigns a relation of type V → D to a constraint.Definition 7. Let c = p(q0, . . . , qn−1) where V is the set of variables in {q0, . . . , qn−1}. We define(cid:10)(cid:11)a(q0), . . . , a(qn−1)(cid:2)a ∈ V → D:(cid:3)∈ M(p)M(c) =.As a result of this definition the meaning of a constraint c with set V of variables is a relation of type V → D. One canview the argument tuple of a constraint as an operator that converts a relation M(p) of type n → D to relation M(c) oftype V → D. This is an extension of the usual semantics of predicate logic.Example. Let sq be the binary relation over the reals where the second argument is the square of the first. That is, M(sq) ={ f ∈ ({0, 1} → R):}. The constraints sq(x, y), sq( y, x), and sq(x, x) denote different relations, as we verify below.(cid:4)(cid:4)(cid:5)(cid:4)M=(cid:5)sq(x, y)f 1 = f 20Given that M(sq) = { f ∈ ({0, 1} → R):(cid:2)a ∈(cid:2)a ∈(cid:2)a ∈(cid:2)a ∈(cid:2)a ∈(cid:2)a ∈{x, y} → R{x, y} → R{x, y} → R{x, y} → R{x} → R{x} → R(cid:5)sq( y, x)(cid:5)sq(x, x)=====MM(cid:5)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:5)(cid:5)(cid:5)(cid:4)f 1 = f 20} we have(cid:10)(cid:11)(cid:3)∈ M(sq)a(x), a( y):(cid:3): a(x)2 = a( y)(cid:10)(cid:11)(cid:3)∈ M(sq)a( y), a(x):(cid:3): a( y)2 = a(x)(cid:10)(cid:11)a(x), a(x)(cid:3)∈ M(sq):(cid:3): a(x) = 0 ∨ a(x) = 1(cid:5)Definition 8. A tuple f ∈ V → D satisfies a constraint c if and only if the restriction of f to the set of variables occurringin c belongs to M(c).5.7. Constraint-satisfaction problemsDefinition 9. A constraint-satisfaction problem (CSP) has the form (cid:12)C, V , D, M(cid:13) and consists of a set C = {s0, . . . , sm−1} ofconstraints, a set V , which is the set of the variables occurring in the constraints, a set D, the domain of the CSP, and aninterpretation M, which maps every n-ary predicate symbol occurring in any of the constraints to a relation of type n → D.A solution to (cid:12)C, V , D, M(cid:13) is a ∈ V → D such that aV i∈ M(si) for all i ∈ m, where V i is the set of variables in si .It follows that the set σ of solutions of the CSP is a relation of type V → D.In (cid:12)C, V , D, M(cid:13),Example.{ f ∈ ({0, 1} → R):f (0) = 1}. The set σ of solutions is a relation σ ⊆ V → R such that π{x, y}σ = {p0, p1} where p0(x) = −p0( y) = 12 (let C = {sq(x, y), sq( y, z), sum( y, z, u), one(u)} (Eq. (3)), V = {x, y, z, u}, D = R, M(sq) =f (2) = f (0) + f (1)}, and M(one) = { f ∈ ({0} → R):5 − 1),f (1) = f (0)2}, M(sum) = { f ∈ ({0, 1, 2} → R):5 + 1), and p1( y) = 12 (5 − 1), p1(x) =5 − 1).12 (12 (√√√√(cid:8)(cid:8)This example shows a CSP with a finite and small solution set. Sudoku puzzles are another such example. It oftenhappens that the solution set has an infinite number of elements, or a finite number that is too large to list or to processon a computer.Theorem 1. Let σ be the solution set of a CSP C = {s0, . . . , sm−1} with M as interpretation for its predicate symbols. Then we haveσ = M(s0) (cid:2) · · · (cid:2) M(sm−1).32A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–38Proof. By induction on the size of set {s0, . . . , sm−1}. The base case C = {s0} is trivial.Assume that the theorem holds for a constraint set Ck = {s0, . . . , sk−1} of size k (cid:3) 1, and let σ (Ck) = M(s0) (cid:2) · · · (cid:2)M(sk−1) denote the solution set of Ck. Consider constraint set Ck+1 = Ck ∪{sk}. Any tuple t which is a solution of Ck+1 = Ck ∪{sk} must be such that the restriction of t to the set of variables occurring in Ck is a solution of Ck, and the restriction of tto the set of variables occurring in sk is a solution of sk. Whence σ (Ck+1) ⊆ σ (Ck) (cid:2) M(sk). Conversely, if t ∈ σ (Ck) (cid:2) M(sk),then by construction t satisfies Ck as well as sk, whence t satisfies Ck+1 = Ck ∪ {sk}. Therefore σ (Ck+1) = σ (Ck) (cid:2) M(sk). (cid:3)6. Solving constraint-satisfaction problemsWhat does it mean to “solve” a CSP? It is rare for the solution set σ to have but few elements, as it does in Sudoku.Though occupying only a small proportion of the type, σ may have a finite and overwhelmingly large number of elements;it may also be an infinite set. Hence we can typically only hope to obtain some information about σ . Useful information cancome in the form of an approximation.If the approximation domain consists of computer-representable sets, as it typically does, then (cid:3)σ is computer-representable, but will usually give too little information about σ . But (cid:3)σ is useful in case one can show that it is empty: inthat case σ is empty; i.e. the CSP has no solutions. This is an advantage of treating numerical problems as CSPs: in conven-tional computation one can only conclude that no solutions were found. By formulating the problem as a CSP with intervalsas approximation structure one may be able to prove that no solutions exist. The possibility of proof of non-existence bymeans of standard floating-point arithmetic (and all its rounding errors) is a valuable complement to conventional numericalanalysis.In case it is not possible to show that (cid:3)σ is empty, one subdivides the box under consideration and one may be ableto show that one of these subdivisions has no solutions. Let box P (“probe”) be such a subdivision. We use it to reduce thepartial solution of the problem of determining σ to that of determining any solutions that might occur in P , or to find, alsousefully, that no solutions occur in P . Thus we proceed to obtain information about σ ∩ P . This intersection is in general nota box, so is not necessarily computer-representable. Hence it is an appropriate task for an algorithm to determine (cid:3)(σ ∩ P )for a given CSP and a suitable P , or an approximation to (cid:3)(σ ∩ P ) (which is itself an approximation).Subdivision of P should result in subsets of P whose union includes P . These subsets are subject to the same consider-ation: if absence of solutions cannot be shown and if amenable to subdivision, the process repeats for such a subset. Anybox P defines a tree of subsets to be processed in this way: solving a CSP requires, in addition to an attempt to showthe absence of solutions in a given box, a search over the tree of subboxes of the initially given box. The “solution” of anumerical CSP is necessarily a list of boxes each of which is too small to subdivide and of which the absence of solutionscannot be shown. Of a solution x ∈ Rn the best one can typically do is to fail to show that (cid:3)({x}) contains no solutions ofthe CSP.6.1. Contraction operatorsA contraction operator transforms a box B into a box B(cid:9) ⊆ B such that there is no solution in B \ B(cid:9). Two kinds ofcontraction operators on boxes are defined here: operators defined by relations, and operators defined by constraints.6.1.1. Contraction operators defined by a relationDefinition 10. Let D be an approximation domain and I an index set. Any relation r of type I → D determines the mappingγr(P ) = (cid:3)(r ∩ P ), the contraction operator of r, that maps boxes with type I → D to boxes with the same type.Benhamou and Older [3] introduced this formula for intervals of reals. Here it is generalized to approximation systemsin general.Lemma 3. The contraction operator γr is idempotent, monotonic, inflationary and correct.Proof. We have that (cid:3)((cid:3)(r ∩ P ) ∩ P ) = (cid:3)(r ∩ P ) ∩ P = (cid:3)(r ∩ P ); hence γr is idempotent.(cid:3) is monotonic and intersection is monotonic in both arguments, so γr is monotonic.γr(P ) = (cid:3)(r ∩ P ) ⊆ (cid:3)P = P , so that P (cid:11) γr(P ). That is, γr moves up in the (information) partial order: γr is inflationary.We have that r ∩ (P \ γr(P )) = ∅ meaning that γr is correct in the sense that it does not remove any part of r from itsargument.An example of a contraction operator. The contraction operator for the sum constraint acting on a box(cid:4)(cid:5){x} → [a, b](cid:4)(cid:5){ y} → [c, d](cid:2)(cid:4)(cid:5){z} → [e, f ](cid:2)where a, b, c, d, e, f are finite IEEE-standard floating-point numbers is given byA. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–3833γM(sum(x, y,z))(cid:10)(cid:4){x} →a=(cid:9)(cid:4)(cid:4)(cid:5){x} → [a, b](cid:2)(cid:11)(cid:5)(cid:4)(cid:9), b{ y} →(cid:2)(cid:4)(cid:5){ y} → [c, d](cid:11)(cid:5)(cid:10)(cid:9)c, d(cid:2)(cid:4)(cid:9){z} →(cid:4)(cid:2)(cid:5)(cid:5){z} → [e, f ](cid:11)(cid:5)(cid:10)(cid:9)e, f.(cid:9)Here(cid:9)(cid:10)a(cid:10)(cid:9)c(cid:10)e(cid:9)(cid:11)(cid:11)(cid:11)(cid:9)(cid:9), b(cid:9), d, f(cid:10)(cid:10)= [a, b] ∩= [c, d] ∩= [e, f ] ∩−−+(e − d), ( f − c)−+(e − b), ( f − a)(cid:10)+−, (b + d)(a + c)(cid:11)(cid:11)(cid:11)+means that the floating-point operation is performed in round-toward-minus-infinity mode and super-means that the floating-point operation is performed in round-toward-plus-infinity mode. In this way correctnesswhere superscriptscriptof γsum is maintained in the presence of rounding errors.In Eq. (1) the contraction operator is applied in the case where a = 0, b = 2, c = 0, d = 2, e = 3, and f = 5. ApplyingγM(sum(x, y,z)) in this special case gives(cid:9)(cid:10)a(cid:10)(cid:9)c(cid:10)e(cid:9)(cid:11)(cid:11)(cid:11)(cid:9)(cid:9), b(cid:9), d, f= [0, 2] ∩ [1, 5] = [1, 2]= [0, 2] ∩ [1, 5] = [1, 2]= [3, 5] ∩ [0, 4] = [3, 4]This only gives the general idea. A practical algorithm has to take care of the possibility of overflow. It also has to allowfor the possibility that a, c or e are −∞ and that b, d or f may be +∞ so that the undefined cases (+∞) − (+∞),(−∞) + (+∞), and (+∞) + (−∞) have to be circumvented. For details about such algorithms see [8].6.1.2. Contraction operators defined by a CSPIn the CSP defined by the constraints {s0, . . . , sm−1}, let us write σi for M(si). Then Theorem 1 says thatσ = σ0 (cid:2) · · · (cid:2) σm−1.The γ operator of Definition 10 is not useful for r = σ , but it can be useful for the r = σi , the solution sets for theconstraints by themselves. In fact, the constraints are chosen to be such that one has an efficient algorithm for each γσi .Definition 11. Let (cid:12){s0, . . . , sm−1}, V , D, M(cid:13) be a CSP. Let σi = M(si) and let V i be the set of variables of si . We defineγi(P ) = π −1V(cid:4)(cid:5)γσi (πV i P ),i = 0, . . . , m − 1,for any box P of type V → D, and call γi the contraction operator of si . We defineΓ (P ) = γ0(P ) ∩ · · · ∩ γm−1(P ),and call Γ the contraction operator of the CSP.Lemma 4. Γ is inflationary, monotonic, and correct.Proof. Since, by Lemma 3, each γσi is inflationary, one hasm−1(cid:9)Γ (P ) =γi(P ) = (cid:2)iγi(P )(cid:4)(cid:5)γσi (πV i P )(cid:4)γσi (πV i P )i=0= (cid:2)iπ −1V(cid:4)= π −1(cid:2)i(cid:21) π −1V ((cid:2)i πV i P )= PV(cid:5)(cid:5)Hence Γ is inflationary.Γ is monotone, as a composition of monotone operators, since both projection πV i and cylindrification π −1are mono-Vtone operators.Finally Γ is correct. Indeed, since by Lemma 3 each σi is correct, i.e. satisfies σi ∩ (πV i P \ γσi (πV i P )) = ∅, one has, forf /∈ γi(P ) i.e., f V i /∈ σi i.e., f /∈ σ . Hence f ∈ (P \ Γ (P )) implies f /∈ σ , thusany tuple f , that f ∈ (P \ Γ (P )) ⇔ f ∈ P and ∃iσ ∩ (P \ Γ (P )) = ∅. Therefore Γ is correct. (cid:3)34A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–38A counterexample to the idempotency of Γ is given by the CSP example discussed earlier, in Section 4:(cid:2)(cid:3)sq(x, y), sq( y, z), sum( y, z, u), one(u).It is enough to take e.g. the approximation domain of (real) boxes included in {x, u, z, u} → R, the corresponding Γ oper-ator operating on that domain, together with the box P informally described in Eq. (6), namely P = { f : {x, y, z, u} → R:2 , 1], f ( y) ∈ [0, 1], f (z) ∈ [0, 1], f (u) ∈ [1, 1].}. The sequence (Γ n(P ))n∈N is strictly decreasing until it stabilizes atf (x) ∈ [ 1the smallest box, in the approximation domain, containing the tuple f : {x, y, z, u} → R, such that f (x) =5 − 1),f ( y) = 12 (5 − 1)2, and f (u) = 1.5 − 1), f (z) = 14 (12 (√√√(cid:8)6.2. AlgorithmsAlgorithms for solving CSPs proceed by applying contraction operators. Hence the algorithms only remove tuples fromconsideration that are not part of the solution. In the course of this process absence of solutions of the CSP may be demon-strated, but solutions are not, in general, constructed.In the case of a discrete D it may happen that applying constraint contractors may result in a box that contains asingle tuple. This tuple will then need to be substituted in the CSP to check whether it is a solution. However, in the typeof CSP we are concerned with here (reals with floating-point intervals as approximation domain), finding a solution thisway is but a remote theoretical possibility (the problem would have to have an exact solution in terms of floating-pointnumbers, which, moreover, upon substitution would miraculously avoid rounding errors). Hence for numerical CSPs the bestwe can expect is an algorithm that results in a small box. This box can be small indeed: in double-length IEEE-standard−17. The result shows that, iffloating-point arithmetic the box can have as projections intervals of relative width around 10a solution exists, it has to be in that box.Among the algorithms that use contraction operators to solve CSPs we distinguish two types of iteration according tothe order in which the operators are applied. We distinguish rigid order from and flexible order. The latter type leaves morechoice in the choice of the next operator to be applied.Consider a CSP (cid:12)C, V , D, M(cid:13) with contraction operators γ0, . . . , γm−1. The rigid-order algorithm applies the m opera-tors in such an order that between two successive applications of any particular operator all other operators are applied.The rigid-order algorithm is susceptible to improvement. In a typical CSP m can be in the order of hundreds or thousands,whereas each of the constraints typically has few arguments. In numerical CSPs, for example, there are three or fewer. Usu-ally each constraint shares an argument with several others. In such a situation most of the contractor applications haveno effect: each application affects only few of many arguments and it may well be that the next operator belongs to aconstraint that does not involve any of these few arguments, so that its application has no effect.This suggests a chaotic algorithm, one that avoids such ineffectual choices of operator applications.4 There is considerablescope for such optimization, as the only constraint on the sequence of operator applications is that this sequence be fair inthe following sense.Definition 12. Let k ∈ (N → A) be an infinite sequence of which the elements are members of a finite set A. k is fair iffeach element of A occurs infinitely many times in k.Thus, in a fair sequence, it is possible, but not necessary, that between two occurrences of the same item all other itemshave occurred.A chaotic algorithm with m operators applies the operators in a fair sequence. Such an algorithm can generate a fairsequence while maintaining a record of the last index in the sequence where a change was effected. As soon as all theoperators have been applied without any resulting change, then, by idempotence, the algorithm can be halted: the rest ofthe infinitely long fair sequence consists of operator applications that have no effect. For details, see [1].6.3. Maximization property of the chaotic algorithmThe chaotic algorithm solves the following problem:(cid:12)maximize Bsubject to B (cid:11) Γ (B)(7)where B ranges over the boxes in the approximation domain, and Γ is the Γ operator associated with the CSP. The prob-lem is stated in a format borrowed from “mathematical programming” in the sense that this includes, for example, linearprogramming. In the above format the total order among real numbers has been replaced by the partial order which is theScott information order described in Section 5. The generalization from the total order of mathematical programming toprogramming with partial orders is due to Parker who captures a wide variety of algorithms in this framework [10].4 The term “chaotic” has been adopted by the constraint processing literature via a detour from a numerical algorithm [5].A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–3835It is easily seen that chaotic iteration solves the maximization problem if the sequence generated by the algorithmconverges to the least fixpoint of Γ . Note that (cid:11) is the information order, where B 0 (cid:11) B1 iff each of the projections of B1is a subset of the corresponding projection of B 0.6.3.1. FixpointsWe review some basic facts about fixpoints. Let (cid:12)D, (cid:11), ⊥(cid:13) be a complete partially ordered set. Completeness means herei=0 ci that is an element of the partially orderedthat every infinite ascending chain c0 (cid:11) c1 (cid:11) · · · has a least upper boundset.(cid:13)∞Let Γ ∈ (D → D) be monotonic and continuous. Continuity of a function f ∈ D → D means that for every infiniteascending chain c0 (cid:11) c1 (cid:11) · · · we have f (i=0 f (ci). In case of a finite D such as the partially ordered set offloating-point intervals, monotonicity implies continuity. By the Knaster–Tarski theorem, Γ has a least fixpoint lfp(Γ ) ∈ D.This may be seen as follows.By monotonicity of Γ ,i=0 ci) =(cid:13)∞(cid:13)∞⊥ (cid:11) Γ (⊥) (cid:11) Γ 2(⊥) (cid:11) · · ·By the completeness of the partially ordered set,(cid:14)∞(cid:15)Γ(cid:16)Γ n(⊥)=∞(cid:15)(cid:4)(cid:5)Γ n(⊥)Γ=(cid:13)∞n=0n=0 Γ n(⊥) is a fixpoint of Γ .n=0Hence∞(cid:15)n=0Γ n(⊥).(cid:13)∞n=0 Γ n(⊥) ∈ D. By the continuity of Γ ,We now turn to the Tarski fixpoint theorem. Let Γ ∈ (D → D) be monotonic, but assume now that partially ordered set(cid:12)D, (cid:11), ⊥(cid:13) is a complete lattice, a richer structure. Completeness means here that any subset of D has a least upper boundand a greatest lower bound. In particular D possesses a largest element (cid:23). Then by the Tarski fixpoint theorem Γ has aleast fixpoint lfp(Γ ) ∈ D. This may be seen as follows.Consider the set S = {a ∈ D: Γ (a) (cid:11) a}. S is non-empty since it contains top element (cid:23) ∈ D. Let l = (cid:24)S be the greatestlower bound of S. Then for any element a ∈ S, one hasa ∈ S ⇒ l (cid:11) a ⇒ Γ (l) (cid:11) Γ (a) (cid:11) aby monotonicity of Γ . Hence Γ (l) is lower bound for S, Γ (l) (cid:11) l = (cid:24)S. Therefore l ∈ S. One then has the chain of implica-tionsΓ (l) (cid:11) l ⇒ Γ(cid:4)(cid:5)Γ (l)(cid:11) Γ (l) ⇒ Γ (l) ∈ S ⇒ l (cid:11) Γ (l) ⇒ l = Γ (l).Hence l is a fixpoint of Γ . It is also the least fixpoint, since S contains every fixpoint, and l = (cid:24)S. Therefore l = (cid:24)S = lfp(Γ )is the least fixpoint of Γ .6.3.2. Application of fixpoint theory to the chaotic algorithmTheorem 2. Given a CSP (cid:12)C, V , D, M(cid:13) with contraction operator Γ and solution set σ . For any box P of type V → D we have(σ ∩ P ) ⊆ (cid:3)(σ ∩ P ) ⊆ Γ n(P )for all n = 0, 1, 2, . . . .Proof. The first inclusion follows from the definition of the (cid:3) operator. We consider the case where there are m = 2constraints, which easily extends to arbitrary greater values of m. We write σi = M(si) and V i for the set of variables in si ,for i = 0, 1. We first consider the case n = 1.(cid:2)(cid:3)a ∈ (V → D): aV 0∈ σ0 ∧ aV 1∈ σ1 ∧ aV 0∈ πV 0 P ∧ aV 1∈ σ0 ∧ aV 1(cid:2)(cid:3)a ∈ (V → D): aV 0(cid:2)(cid:3)a ∈ (V → D): aV 0(cid:4)(cid:3)π −1π −1(cid:4)(cid:3)V∈ (σ0 ∩ πV 0 P ) ∧ aV 1π −1V (σ0 ∩ πV 0 P ) ∩ π −1(cid:3)(σ0 ∩ πV 0 P ) ∩ π −1(cid:3)(σ0 ∩ πV 0 P ) ∩ π −1VVV(cid:3)(σ ∩ P ) ==(cid:3)(cid:3)==∈ σ1 ∧ a ∈ P∈ πV 1 P(cid:3)∈ (σ1 ∩ πV 1 P )(cid:5)V (σ1 ∩ πV 1 P )(cid:5)(cid:3)(σ1 ∩ πV 1 P )(cid:3)(σ1 ∩ πV 1 P ) =γ0(P ) ∩ γ1(P ) =⊆=Γ (P ).36A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–38We have shown that (cid:3)(σ ∩ P ) ⊆ Γ (P ). We also have (cid:3)(σ ∩ P ) ⊆ Γ 2(P ). This is because of the correctness of Γ : it doesnot remove any solution tuples from its argument. Hence we have (cid:3)(σ ∩ P ) ⊆ Γ n(P ) for any n (cid:3) 0. (cid:3)By Definition 11, Γ is the intersection of contraction operators, one for each constraint, each of which can be efficientlycomputed. The results of these operators are exact in the sense that the results are by definition approximations andare therefore exactly representable. Thus Theorem 2 can serve as the basis for an algorithm for approximating the set ofsolutions in P .In terms of the information order (cid:11) Theorem 2 states that Γ n(P ) (cid:11) (cid:3)(σ ∩ P ) (cid:11) (σ ∩ P ).Theorem 3. Γ is monotonic on the partially ordered set of subboxes of P ordered by information order.Proof. Each contraction operator γi : P (cid:6)→ π −1tone. (cid:3)V (γσi (πV i P )) is monotone, and the join of two monotone operators is mono-Observe that the set of boxes contained in P defines an approximation structure for P . Γ is monotonic. The partiallyordered set of subboxes of P is ordered by information order and is a complete lattice with least element P . Accordingly, Γ ,restricted to the approximation structure, has a least fixpoint lfp(Γ ), by the Tarski fixpoint theorem. Summarizing, we haveΓ n(P ) (cid:11) lfp(Γ ) (cid:11) (cid:3)(σ ∩ P ) (cid:11) (σ ∩ P ) for all n.If the box operator (cid:3) is continuous over the approximation domain defined over D, then Γ is also continuous byi=0 Γ i(P ) is the least fixpoint of Γ containedcompositionality of continuous functions, and by the Knaster–Tarski theoremin P .(cid:13)∞In particular, if D is the set F of finite double-length IEEE-standard floating-point numbers, and the approximation do-main is given by the set of F -intervals, then domain D is finite, hence both operators (cid:3) and Γ are continuous. The subboxesof P form a complete partially ordered set trivially because the finiteness of the set of floating-point numbers. Therefore(cid:13)∞(cid:13)ni=0 Γ i(P ), for some finite n, is the least fixpoint of Γ , restricted to P .i=0 Γ i(P ) =Theorem 4. Let a CSP (cid:12){s0, . . . , sm−1}, V , D, M(cid:13), with contraction operator Γ , and contraction operators γi for each individual con-straint si be given. If the approximation structure over D is such that the box operator (cid:3) is continuous, then, for every box P , every fairiteration of continuous operators γi starting with P converges towards the least fixpointj=0 Γ j(P ) of Γ , restricted to P .(cid:13)∞Proof. Let k0, k1, k2, . . . be a fair iteration, where for each n, kn ∈ {0, . . . , m − 1} is the index of the constraint s ∈{s0, . . . , sm−1} selected at the nth iteration step. The corresponding iteration starting from some box P is given by thesequence of boxesP 0 = PP n = γkn (P n−1), n > 0We first show that∀ j ∃q Γ j(P ) (cid:11) P q(8)Indeed, k is a fair sequence, and since all operators γi are inflationary and monotone, for each j, one can choose q suchthat the initial iteration subsequence k0, . . . , kq−1 contains, for each constraint sl in C , at least j occurrences of index l of slin {0, . . . , m − 1}; these occurrences correspond to at least j applications of the contraction operator γl.Next, we observe that∀qP q (cid:11) Γ q(P ),which follows by induction on q.(cid:13)∞(cid:13)∞Whencej=0 Γ j(P ) (cid:11)j=0 P j by (8), and(cid:13)∞j=0 P j (cid:11)(cid:13)∞j=0 Γ j(P ) by (9). The two limits are equal. (cid:3)(9)7. Further workConcurrent constraint programming (CCP) ([11] and further references there) is a model of concurrent programming. Thismodel is based on an abstraction of a computer store that is more abstract than the one used in conventional programminglanguages. Usually the store is modeled as a vector of storable values (numbers, characters) indexed by the variables acces-sible to the program. Thus to every variable there corresponds a single value. The conventional read operation on a variableyields this value. The conventional write operation on a variable changes this value.In CCP it is not assumed that the value of a variable is precisely known: the store is a constraint on the values ofvariables. The conventional read operation is replaced by ask, an operation in the form of a logic formula that succeeds ifand only if it is logically entailed by the store. The conventional write operation is replaced by tell, an operation in theA. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–3837form of a logic formula T that has the effect of replacing the store S by a logical equivalent of S ∧ T , provided that this isconsistent.The generalization of the conventional store to CCP requires that the store becomes a logical theory S that is satisfaction-complete in the sense that for every formula C admissible as ask or tell it is the case that either S |(cid:26) ∃C or S |(cid:26) ¬∃Cwhere ∃ denotes existential closure. See [4] and further references there.CCP seems to have a great deal of unexploited potential. Its motivation and terminology is in the area of concurrent pro-gramming, with the aim of generalizing the many different approaches (Hewitt’s Actors, Hoare’s CSP, Milner’s CCS, variousflavours of concurrent logic programming). CCP is linked to constraint solving by its formulation in terms of predicate logic.Thus CCP promises to be a framework for constraint solving with parallelism built in, a promising feature given the massiveamount of computation that is typical of constraint problems.To realize this promise it is necessary to generalize CCP beyond the restriction of the store as a satisfaction-completetheory. For example, in the case of interval constraints, where the domain is the reals, the theory of the store is notsatisfaction-complete. Consequently, the result of a converging iteration with interval constraints means that if a solutionexists, then it has to be in the remaining intervals. Often one knows from other sources that a solution exists (e.g. that theCSP arises from a polynomial of odd degree being equated to zero) and the remaining intervals are close to the resolutionof the floating-point system. In such a situation the weakness of the conclusion does not stand in the way of it being ofgreat practical value. We have not explored whether the valuable features of CCP can be preserved when the store is not anecessarily a satisfaction-complete theory.8. Concluding remarksWe see the contributions of this paper as the following.Although in the usual definition of CSP the constraints look like atomic formulas of predicate logic, the semantics of aCSP is given independently. We use the standard semantics of first-order predicate logic to define the solution set of a CSPand we define approximation systems as a set-theoretic device to interface our framework for CSPs with the well-knownchaotic iteration algorithm.Parker’s observation [10] was that the operations research paradigm of maximizing a real-valued objective function underconstraints can be generalized to maximization in partially ordered spaces. Scott’s contribution [13] was that computationcan be viewed as information gain. We combine these insights, so that many of Parker’s examples can be seen as iterationsin which information is monotonically gained.Among these examples we concentrate on solving systems where the constraints are nonlinear equations or inequalitiesover the reals. Constraint processing by domain reduction can be viewed as the use of the computer for monotonic gainof information. This is more than a theoretical point of view. What is lacking in the current practice of computing is aquantitative treatment of the work done by the CPU per, say, gigacycle. The domain reduction method can be used tocompare how many gigacycles were required to obtain the most recent domain reduction, expressed, say, as ratio of thecardinalities, or volumes, of the box before and after this reduction. One may conclude that a reduction of x percent isnot worth the y gigacycles it cost, that further diminishing returns for computational effort are to be expected, and thattherefore it is time to terminate the iteration.AcknowledgementsThis research was supported by our universities, by INRIA Rocquencourt, France, and by the Natural Science and Engi-neering Research Council of Canada.References[1] K.R. Apt, The essence of constraint propagation, Theoretical Computer Science 221 (1–2) (1999) 179–210.[2] K.R. Apt, Principles of Constraint Programming, Cambridge University Press, 2003.[3] F. Benhamou, W.J. Older, Applying interval arithmetic to real, integer, and Boolean constraints, Journal of Logic Programming 32 (1997) 1–24.[4] K.L. Clark, Logic-programming schemes and their implementations, in: J.-L. Lassez, G. Plotkin (Eds.), Computational Logic, MIT Press, 1991, pp. 487–541.[5] D. Chazan, W. Miranker, Chaotic relaxation, Linear Algebra and its Applications 2 (1969) 199–222.[6] J.G. Cleary, Logical arithmetic, Future Computing Systems 2 (1987) 125–149.[7] R. Dechter, Constraint Processing, Elsevier Science, 2003.[8] T. Hickey, Q. Ju, M.H. van Emden, Interval arithmetic: from principles to implementation, Journal of the ACM 48 (5) (2001) 1038–1068.[9] A.K. Mackworth, Consistency in networks of relations, Artificial Intelligence 8 (1977) 99–118.[10] D.S. Parker, Partial order programming, Technical Report CSD-870067, Computer Science Department, University of California at Los Angeles, 1987.[11] V.A. Saraswat, M. Rinard, P. Panangaden, Semantic foundations of concurrent constraint programming, in: ACM Conference on Principles of Program-ming Languages (POPL), 1991.[12] D.S. Scott, Outline of a mathematical theory of computation, Technical Report PRG-2, Programming Research Group, Oxford University, 1970.[13] D.S. Scott, Lattice theory, data types and semantics, in: R. Rustin (Ed.), Formal Semantics of Programming Languages, Prentice–Hall, 1972.[14] G. Steele, The definition and implementation of a programming language based on constraints, PhD thesis, MIT, 1980.[15] R.V. Southwell, Relaxation Methods in Engineering, Oxford University Press, 1940.[16] I. Sutherland, Sketchpad: a man-machine graphical communication system, PhD thesis, Dept. of Electrical Engineering, MIT, 1963.38A. Nait Abdallah, M.H. van Emden / Artificial Intelligence 197 (2013) 25–38[17] P. Van Hentenryck, L. Michel, F. Benhamou, Newton: constraint programming over nonlinear constraints, Science of Computer Programming 30 (1998)83–118.[18] P. Van Hentenryck, V. Saraswat, Y. Deville, Design, implementation, and evaluation of the constraint language cc(FD), Journal of Logic Programming 37(1998) 139–164.[19] P. Van Hentenryck, L. Michel, Constraint-Based Local Search, MIT Press, 2005.[20] D.L. Waltz, Generating semantic descriptions from drawings of scenes with shadows, PhD thesis, MIT, 1972.