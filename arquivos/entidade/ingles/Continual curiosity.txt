Artificial Intelligence 247 (2017) 313–335Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintContinual curiosity-driven skill acquisition from high-dimensional video inputs for humanoid robotsVarun Raj Kompella∗, Marijn Stollenga, Matthew Luciw, Juergen SchmidhuberThe Swiss AI Lab IDSIA, USI & SUPSI, Galleria 2, 6928 Manno-Lugano, Switzerlanda r t i c l e i n f oa b s t r a c tArticle history:Received in revised form 12 October 2014Accepted 2 February 2015Available online 12 February 2015Keywords:Reinforcement learningArtificial curiositySkill acquisitionSlow feature analysisContinual learningIncremental learningiCubIn the absence of external guidance, how can a robot learn to map the many raw pixels of high-dimensional visual inputs to useful action sequences? We propose here Continual Curiosity driven Skill Acquisition (CCSA). CCSA makes robots intrinsically motivated to acquire, store and reuse skills. Previous curiosity-based agents acquired skills by associating intrinsic rewards with world model improvements, and used reinforcement learning to learn how to get these intrinsic rewards. CCSA also does this, but unlike previous implementations, the world model is a set of compact low-dimensional representations of the streams of high-dimensional visual information, which are learned through incremental slow feature analysis. These representations augment the robot’s state space with new information about the environment. We show how this information can have a higher-level (compared to pixels) and useful interpretation, for example, if the robot has grasped a cup in its field of view or not. After learning a representation, large intrinsic rewards are given to the robot for performing actions that greatly change the feature output, which has the tendency otherwise to change slowly in time. We show empirically what these actions are (e.g., grasping the cup) and how they can be useful as skills. An acquired skill includes both the learned actions and the learned slow feature representation. Skills are stored and reused to generate new observations, enabling continual acquisition of complex skills. We present results of experiments with an iCub humanoid robot that uses CCSA to incrementally acquire skills to topple, grasp and pick-place a cup, driven by its intrinsic motivation from raw pixel vision.© 2015 Elsevier B.V. All rights reserved.1. IntroductionOver the past decade, there has been a growing trend in humanoid robotics research towards robots with a large number of joints, or degrees of freedom, notably the ASIMO [1], PETMAN [2] and the iCub [3]. These robots demonstrate a high amount of dexterity and are potentially capable of carrying out complex human-like manipulation. When interacting with the real-world, these robots are faced with several challenges, not the least of which is the problem of how to solve tasks upon processing an abundance of high-dimensional sensory data.In the case of well structured environments, these robots can be carefully programmed by experts to solve a particu-lar task. But real-world environments are usually unstructured and dynamic, which makes it a daunting task to program these robots manually. This problem can be substantially alleviated by using reinforcement learning (RL; [4,5]), where a * Corresponding author.E-mail address: varun@idsia.ch (V.R. Kompella).http://dx.doi.org/10.1016/j.artint.2015.02.0010004-3702/© 2015 Elsevier B.V. All rights reserved.314V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335Fig. 1. A playroom scenario for a baby humanoid-robot in a lab environment, where it is placed next to a table with a few moving objects. The robot has a limited field-of-view and encounters continuous streams of images as it holds or shifts its gaze. Figure shows three such perspectives oriented towards the moving objects. How can the robot learn to solve tasks in the absence of an external guidance?robot learns to acquire desired task-specific behaviors, by maximizing the accumulation of task-dependent external rewards through simple trial-and-error interactions with the environment.Unfortunately, for humanoid robots equipped with vision, the sensory and joint state space are so large that it is ex-tremely difficult to procure the rewards (if any exist) by random exploration. For example, if the robot receives a reward for sorting objects, it could take an extremely long time to obtain the reward for the first time. Therefore, it becomes necessary to (a) build lower-dimensional representations of the state-space to make learning tractable and (b) to explore the envi-ronment efficiently. But how can these robots learn to do this in the presence of external rewards that are typically only sparsely available?Much of the human capacity to explore and solve problems is driven by self-supervised learning [6,7], where we seek to acquire behaviors by creating novel situations and learning from them. As an example, consider a simple playroom scenario for a baby humanoid as shown in Fig. 1. Here, the robot is placed next to a table with a few moving objects. The robot has a limited field-of-view and encounters continuous streams of images as it holds or shifts its gaze. If the robot can learn compact representations and predictable behaviors (e.g., to grasp) from its interactions with the cup, then by using these learned behaviors, it can speed up the acquisition of external rewards related to some teacher-defined task, such as placing the cup at a particular location. Continually acquiring and reusing a repertoire of behaviors and representations of the world, learned through self-supervision, can therefore make the robot adept in solving many external tasks.But how can the robot (a) self-supervise its exploration, (b) build representations of the high-dimensional sensory inputs and (c) continually acquire skills that enable it to solve new tasks? These problems have individually been researched in the machine learning and robotics literature [8–29]. However, to develop a single system that addresses all these important is-sues together is a challenging open problem in artificial intelligence (AI) research. We propose an online-learning framework that addresses this open problem.In order to make the robot self-supervised or intrinsically-motivated to explore new environments, we use the theory of Artificial Curiosity (AC; [30,31]). AC mathematically describes curiosity and creativity. AC-driven agents are interested in the learnable but as-yet-unknown aspects of their environment, and are disinterested in the already learned and inherently unlearnable (noisy) aspects. Specifically, the agent receives intrinsic rewards for action sequences, and these rewards are pro-portional to the improvement of the agent’s internal model or predictor of the environment. Using RL and the self-generated intrinsic rewards derived using AC [32–36,25], the agent is motivated to explore the environment where it makes maximum learning progress.Most RL algorithms however, tend to work only if the dimensionality of the state space is small, or its structure is simple. In order to deal with massive high-dimensional streams of raw sensory information obtained, for example through vision, it is essential to reduce the input dimensionality by building low-dimensional but informative abstractions of the environment [37]. An abstraction maps the high-dimensional input to a low-dimensional output. The high-dimensional data sensed by a robot is often temporally correlated and can be greatly compressed if the temporal coherence in the data is exploited. Slow Feature Analysis (SFA; [14,38,39]) is an unsupervised learning algorithm that extracts temporal regularities from rapidly changing raw sensory inputs. SFA is based on the Slowness Principle [40–42], which states that the underlying causes of changing signals vary more slowly than the primary sensory stimulus. For example, individual retinal receptor responses or gray-scale pixel values of video may change quickly compared to latent abstract variables, such as the position of a moving object. SFA has achieved success in many problems and scenarios, e.g., extraction of driving forces of a dynam-ical system [43], nonlinear blind source separation [44], as a preprocessor for reinforcement learning [39], and learning of place-cells, head-direction cells, grid-cells, and spatial view cells from high-dimensional visual input [38].SFA techniques are not readily applicable to open-ended online learning agents, as they estimate covariance matrices from the data via batch processing. We instead use Incremental Slow Feature Analysis (IncSFA; [45,46]), which does not V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335315need to store any input data or computationally expensive covariance matrix estimates. IncSFA makes it feasible to handle high-dimensional image data in an open-ended manner.IncSFA, like most online learning approaches, gradually forgets previously learned representations whenever the statistics of the input change, for example, when the robot shifts its gaze from perspective two to perspective one in Fig. 1. To ad-dress this issue, in our previous work, we proposed an algorithm called Curiosity-Driven Modular Incremental Slow Feature Analysis (Curious Dr. MISFA; [47,48]), which retains what was previously learned in the form of expert modules [29]. From a set of input video streams, Curious Dr. MISFA actively learns multiple expert modules comprising slow feature abstractions, in the order of increasing learning difficulty. The algorithm continually estimates the initially unknown learning difficulty through intrinsic rewards generated by exploring the input streams.Using Curious Dr. MISFA, the robot in Fig. 1 finds its interactions with the plastic cup more interesting (easier to encode) than the complex movements of the other objects. This results in a compact slow feature abstraction that encodes its in-teractions with the cup. Eventually, the robot finds the cup-interaction boring and its interest shifts towards encoding other perspectives while retaining the learned abstraction. Can the robot simultaneously acquire re-usable skills while acquiring abstractions? Each abstraction learned encodes some previously unknown regularity in the input observations, which can therefore be used as a basis for acquiring new skills.Our contribution here is the Continual Curiosity-driven Skill Acquisition (CCSA) framework, for acquiring both abstrac-tions and skills in an online and continual manner. In RL, the options framework [49] formalizes skills as RL policies, active within a subset of the state space, which can terminate at subgoals, after which another option takes over. When the agent has a high-dimensional input, like vision, an option requires a dimensionality reducing abstraction, so that policy learning becomes tractable. CCSA is a task-independent curiosity-driven learning algorithm that combines Curious Dr. MISFA with the options framework. Each slow feature abstraction learned by Curious Dr. MISFA augments the robot’s default state space, which in our case is a set of low-level kinematic joint poses learned using Task Relevant Roadmaps [50]. This augmented state space is then clustered to create new distinct states. A Markovian transition model is learned by exploring the new state space. The reward function is also learned through exploration, with the agent being intrinsically rewarded for making state-transitions that produce a large variation in the slow-feature outputs. This specialized reward function is used to build the option’s policies, to drive the robot to states where such transitions will occur. Such transitions are shown to correspond to bottleneck states, i.e., “doorways”, which are known to be good subgoals in the absence of externally imposed goals [51,52]. Once the transition and reward functions are learned, the option’s policy is learned via Least-Squares Policy Iteration [53]. Skills acquired by the robot in the form of options, are reused to generate new input observations, enabling acquisition of more complex skills in a continual open-ended manner [29,54]. Using CCSA, in our experiments, an iCub humanoid robot addresses the open problems discussed earlier, acquiring a repertoire of skills (topple, grasp) from raw-pixel vision, driven purely by its intrinsic motivation.The rest of this paper is organized as follows. Section 2 discusses related research work carried out prior to this paper. Sections 3 and 4 present an overview and a formulation of the learning problem associated with the CCSA framework. Section 5 discusses details of the internal workings of CCSA. Section 6 contains experiments and results conducted using an iCub humanoid robot; Sections 7–8 present future work and conclusions.2. Related workExisting intrinsically-motivated skill acquisition techniques in RL have been applied to simple domains. For example, Bakker and Schmidhuber [55] proposed a hierarchical RL framework called HASSLE in a grid world environment, where high-level policies discover subgoals from clustering distance-sensor outputs and low-level policies specialize on reaching the subgoals. Stout and Barto [24] explore the use of a competence-based intrinsic motivation as a developmental model for skill acquisition in simple artificial grid-world domains. Pape et al. [25] proposed a method for autonomous acquisition of tactile skills on a biomimetic robot finger, through curiosity-driven reinforcement learning.There have been attempts to find skills using feature-abstractions in domains such as those of humanoid robotics. Hart [56] proposed an intrinsically motivated hierarchical skill acquisition approach for a humanoid robot. The system com-bines discrete event dynamical systems [57] as a control basis and an intrinsic reward function [26] to learn a set of controllers. However, the intrinsic reward function used is task specific, and the system requires a teacher to design a developmental schedule for the robot.Konidaris et al. [58,59] show how each option might be assigned with an abstraction from a library of many senso-rimotor abstractions to acquire skills. The abstractions have typically been hand-designed and learning was assisted by human-demonstration. In their recent work [27], an intrinsic motivation system makes a robot acquire skills from one task to improve the performance on a second task. However, the robot used augmented reality tags to identify target objects and had access to a pre-existing abstraction library. CCSA autonomously learns a library of abstractions and control policies simultaneously from raw-pixel streams generated via exploration, without any prior-knowledge of the environment.Mugan and Kuipers’s [60] Qualitative Learner of Action and Perception system discretizes low-level sensorimotor expe-rience through defining landmarks in the variables and observing contingencies between landmarks. It builds predictive models on this low-level experience, which it later uses to generate plans of actions. It either selects its actions randomly (early) or such that it expects to make fast progress in the performance of the predictive models (artificial curiosity). The sensory channels are preprocessed so that the input variables, for example, track the positions of the objects in the scene. 316V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335A major difference between this system and ours is that we operate upon the raw pixels directly, instead of assuming the existence of a low-level sensory model that can track the positions of the objects in the scene.Baranes and Oudeyer [61] proposed an intrinsic motivation architecture called SAGG-RIAC, for adaptive goal-exploration. The system comprises two learning parts, one for self-generation of subgoals within the task-space and the other for ex-ploration of low-level actions to reach the subgoals selected. The subgoals are generated using heuristic methods based on a local measure of competence progress. The authors show results using a simulated quadruped robot on reaching tasks. The system however, assumes that a low-dimensional task-space is provided. CCSA is a task-independent approach, where subgoals are generated automatically by the slow feature abstractions that encode spatio-temporal regularities in the raw high-dimensional video inputs.Ngo et al. [62,63] investigated an autonomous learning system that utilizes a progress-based curiosity drive to ground a given abstract action, e.g., placing an object. The general framework is formulated as a selective sampling problem in which an agent samples any action in its current situation as soon as it sees that the effects of this action are statisticallyunknown. If no available actions have a statistically unknown outcome, the agent generates a plan of actions to reach a new setting where it expects to find such an action. Experiments were conducted using a Katana robot arm with a fixed over-head camera, on a block-manipulation task. The authors show that the proposed method generates sample-efficient curious exploratory behavior and continual skill acquisition. However, unlike CCSA, the sensorimotor abstractions are hand-designed and not learned by the agent.CCSA uses IncSFA to find low-dimensional manifolds within the raw pixel inputs, providing a basis for coupled per-ceptual and skill learning. We emphasize the special utility of SFA for this task over similar methods such as principal component analysis [64], or predictive-projections [65], which are based on variance or nearest neighbor learning, while Slow features through IncSFA extract temporal invariance from input streams that represent “doorway” or “bottleneck” as-pects (choke-points between two more fully connected subareas), similar to Laplacian-Eigen Maps [66–68]. The hierarchical reinforcement learning literature [69–71,49,51,55,67,52] illustrates that such bottlenecks can be useful subgoals. Finding such bottlenecks in visual input spaces is a relatively new concept, and one we exploit in the iCub experiments. For exam-ple, while it moves its arm around a cup in the scene, the bottleneck state is where it topples the cup over, invariant to the arm position. The two subareas in this case are 1. the cup is upright (stable) while the arm moves around, 2. the cup is on its side (stable) while the arm moves around. More studies on the types of representations learned by the IncSFA algorithm can be found elsewhere [47,46].An initial implementation of Curious Dr. MISFA for learning slow feature abstractions [48], a discussion on its neuro-physiological correlates [47] and a prototypical construction of a skill from a slow feature abstraction [72] can be found in our previous work. The novel contribution of this paper is that we present an online learning algorithm (CCSA) that uses Curious Dr. MISFA for learning slow feature abstractions, such that it enables a robot to acquire, store and reuse skills in an open-ended continual manner. We also formally address the underlying learning problem of task-independent contin-ual curiosity-driven skill acquisition. We demonstrate the working of our algorithm with iCub experiments and show the advantages of intrinsically motivated skill acquisition for solving an external task.3. Overview of the proposed frameworkIn this section, we will briefly summarize the overall framework of the proposed algorithm, which we call Continual Curiosity driven Skill Acquisition (CCSA). Fig. 2 illustrates the overall framework. The learning problem associated with CCSA can be described as follows: From a set of pre-defined or previously acquired input exploratory behaviors, which generate potentially high-dimensional time-varying observation streams, the objective of the agent is to (a) acquire an easily learnable yet unknown target behavior and (b) re-use the target behavior to acquire more complex target behaviors. The target behaviors represent the skills acquired by the agent. A sample run of the CCSA framework to acquire a skill is as follows (see Fig. 2):(a) The agent starts with a set of pre-defined or previously acquired exploratory behaviors. We make use of the optionsframework [49] to formally represent the exploratory behaviors as exploratory options (see Section 4 for a formal definition of the terminology used here).(b) The agent makes high-dimensional observations through a sensor-function, such as a camera, upon actively executing the exploratory options.(c) Using our previously proposed curiosity-driven modular incremental slow feature analysis (Curious Dr. MISFA) algo-rithm, the agent learns a slow feature abstraction that encodes the easiest-to-learn yet unknown regularity in the observation streams (see Section 5.2).(d) The slow feature abstraction outputs are clustered to create feature states that are augmented to the agent’s abstracted-state space, which contains previously encoded feature-states (see Section 5.3).(e) A Markovian transition model is learned by exploring the new abstracted-state space. The reward function is also learned through exploration, with the agent being intrinsically rewarded for making state-transitions that produce a large variation (high statistical variance) in the slow-feature outputs. This specialized reward function is used to learn action-sequences (policy) that drives the agent to states where such transitions will occur (see Section 5.3).V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335317Fig. 2. High-level control flow of the Continual Curiosity-driven Skill Acquisition (CCSA) framework. (a) The agent starts with a set of pre-defined or previously acquired exploratory behaviors (represented as exploratory options). (b) It makes high-dimensional observations upon actively executing the exploratory options. (c) Using the Curious Dr. MISFA algorithm, the agent learns a slow feature abstraction that encodes the easiest-to-learn yet unknown regularity in the observation streams. (d) The slow feature abstraction outputs are clustered to create feature states that are augmented to the agent’s abstracted-state space. (e) A Markovian transition model of the new abstracted-state space and an intrinsic reward function are learned through exploration. (f) A deterministic policy is then learned via model-based Least-Squares Policy Iteration (Model-LSPI) and a target option is constructed. The deterministic target-option’s policy is modified to a stochastic policy in the agent’s new abstracted states and is added to the set of exploratory options.(f) Once the transition and reward functions are learned, a deterministic policy is learned via model-based Least-Squares Policy Iteration (LSPI; [53]). The learned policy and the learned slow feature abstraction together constitute a target option, which represents the acquired skill (see Section 5.3).(f)–(a) The deterministic target-option’s policy is modified to a stochastic policy in the agent’s new abstracted states and is added to the set of exploratory options (see Section 5.4). This enables the agent to reuse the skills to acquire more complex skills in a continual open-ended manner [29,54].CCSA is a task-independent algorithm, i.e., it does not require any design modifications when the environment is changed. However, CCSA makes the following assumptions: (a) The agent’s default abstracted-state space contains low-level kinematic joint poses of the robot learned offline using Task Relevant Roadmaps [50]. This is done to limit the iCub’s exploration of its arm to a plane parallel to the table. This assumption can be relaxed resulting in a larger space of arm-exploration of the iCub, and the skills thus developed may be different. (b) CCSA requires at least one input exploratory option. To minimize human inputs into the system, in our experiments at t = 0, the agent starts with only a single input exploratory option, which is a random-walk in the default abstracted-state space. However, environment or domain specific information can be used to design several input exploratory options in order to shape the resulting skills. For example, random-walk policies mapped to different sub-regions in the robot’s joint space can be used.4. Theoretical formulation of the learning problemIn this section, we present a theoretical formulation of the learning problem associated with our proposed CCSA frame-work. We first formalize the curiosity-driven skill acquisition problem and then later in the section we present a continual extension of it.4.1. Curiosity-driven skill acquisitionGiven a fixed set of input exploratory options, which generate potentially high dimensional observation streams that may or may-not be unique, the objective is to acquire a previously unknown target option corresponding to the easily-encodable observation stream. Fig. 3 illustrates the learning process. The learning process iterates over the following steps:(a) Estimate the easily-encodable yet unknown observation stream, while simultaneously learning a compact encoding (ab-straction) for it.(b) Learn an option that maximizes the statistical variance of the encoded abstraction output. The problem is formalized as follows:4.1.1. NotationEnvironment An agent is in an environment that has a state-space S. It can take an action a ∈ A and transition to a new state according to the transition-model (environment dynamics) P : S × A → S. The agent observes the environment state s as a high-dimensional vector, x ∈ RI , I ∈ N.318V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335Fig. 3. Curiosity-driven Skill Acquisition: Given a fixed set of input exploratory options (represented by red dashed boxes) generating n observation streams, abstractions (represented by circles) and corresponding target options (represented by pink dotted boxes) are learned sequentially in order of increasing learning difficulty. The learning process involves not just acquiring the target options, but also the sequence in which they are acquired. The top figure shows an example of the desired result after the first target option was learned. The bottom figure shows the desired end result after all possible target options have been learned. The curved arrow indicates the temporal evolution of the learning process.Abstraction Let (cid:2) denote some online abstraction-estimator that updates a feature-abstraction φ, where (cid:2)(x, φ) returns an updated abstraction for an input x. The abstraction φ : x (cid:5)→ y maps a high-dimensional input observation stream x(t) ∈ RIto a lower-dimensional output y(t) ∈ R J , J (cid:6) I, J ∈ N, such that y(t) = φ (x(t)).Abstracted-state space The agent’s abstracted-state space S (cid:4) contains the space spanned by the outputs y of all the ab-stractions that were previously learned using (cid:2).Input exploratory options The agent can execute an input set of pre-defined temporally extended action sequences, called the exploratory option set Oe = {O e⊆ S(cid:4)i , π ei: S(cid:4) → [0, 1] is the option termination is the initiation set comprising abstracted states where the option is available, βei× A → [0, 1]condition, which will determine where the option terminates (e.g., some probability in each state), and π eiis a pre-defined stochastic policy, such as a random walk within the applicable state space. Each exploratory-option’s policy generates an observation stream via a sensor-function U , such as an image-sensor like a camera:; n ≥ 1}. Each exploratory option is defined as a tuple (cid:8)I e(cid:9), where I ei1, . . . , O ei , βe: I einxi(t) = U(P(s, π ei (s(cid:4))))where P is the unknown transition model of the environment, s(cid:4) ∈ I eis the agent’s current abstracted state while execut-ii (s(cid:4)) returns an action. Let ing the ith exploratory option O eX = {x1, . . . , xn} denote the set of n I -dimensional observation streams generated by the n exploratory-option’s policies. At each time t however, the learning algorithm’s input sample is from only one of the n observation-streams.i at time t, s ∈ S is the corresponding environment state, and π eCuriosity function Let (cid:7) : X → [0, 1) denote a function indicating the speed of learning an abstraction by the abstraction-estimator (cid:2). (cid:7) induces a total ordering among the observation streams making them comparable in terms of learning difficulty.11 Refer to our previous work [47,73] for a proof on the existence of such a function and an analytical expression of (cid:7) for IncSFA.V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335319Target options Unlike the pre-defined input exploratory-option set, a target-option set OLprocess. A target option Oi , βLi , φi, π Las a tuple (cid:8)IL(S(cid:4) × S(cid:4)), where S(cid:4)φiφitermination condition, and π Lis the outcome of the learning . It is defined ) is the target-option’s initiation set defined over the augmented state-space , xj ∈ X . βi is the option’s idenotes the space spanned by the abstraction φi ’s output y(t) = φcontains a learned abstraction φi and a learned deterministic policy π L) → A is the learned deterministic policy.L ∈ OL(cid:9). IL⊆ (S(cid:4) × S(cid:4)φi(cid:3)xj(t)(cid:2)ii: (S(cid:4) × S(cid:4)φiiEncoded observation streams Let XOL(t) = {φ←outputs, Xi yi, ∀OLiOL(t) denote an ordered set (induced by time t) of pre-images of the learned abstractions ∈ OL(t)}. XOL(t) represents the set of encoded observation streams at time t.Other notation |.| indicates cardinality of a set, (cid:13).(cid:13) indicates Euclidean norm, (cid:8).(cid:9)t indicates averaging over time, (cid:8).(cid:9)τindi-tcates windowed-average with a fixed window size τ over time. δ is a small scalar constant (≈ 0). Var[.] represents statistical variance and ∀ indicates f orall.4.1.2. Problem statementWith the above notation, curiosity-driven skill acquisition problem can be formalized as an optimization problem with , such that the number of the objective that: Given a fixed set of input exploratory options Oe , find a target-option set OLtarget options learned at any time t is maximized:(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)OL(t)(cid:4) , ∀t = 1, 2, . . .maxOLunder the constraints,∀ j ∈ {1, . . . , J }, ∀Oj(cid:6)(cid:8) y(cid:5)j(cid:9)t = 0,iL∀Oi(cid:8)( yi )2(cid:9)t = 1,∈ OL(t), ∃ j ∈ {1, . . . , n},and ∀O∈ OL(t)Lk(cid:16)=i(cid:7)(xi) ≤ (cid:7)(x j), ∀i < j and xi, x j ∈ X(cid:7)π LU(P(s, πi(s(cid:4))))Varφi(cid:2):i= arg supπi∈ OL(t)Li(cid:8)(cid:13)(cid:2)(x j, φi) − φi(cid:13)(cid:9)τ≤ δt(cid:8)(cid:13)(cid:2)(x j, φk) − φk(cid:13)(cid:9)τt > δOL(t)(cid:3)(cid:8), s(cid:4) ∈ IL, ∀OLi∈ OL(t).(1)(2)(3)(4)Constraint (1) requires that the abstraction-output components have zero mean and unit variance. This constraint enables the abstractions to be non-zero and avoids learning features for constant observation streams. Constraint (2) requires a unique abstraction be learned that encodes at least one of the input observation streams, avoiding redundancy. Constraint (3) imposes a total-ordering induced by (cid:7) on the abstractions learned. Easier-to-learn observation streams are encoded first. And finally, Constraint (4) requires that each target-option’s policy maximizes sensitivity, determined by the variance of the observed abstraction outputs [74]. In the rest of the paper, we interchangeably use the word skill to denote a learned target option O.i and a skill-set to denote the target-option set OLLOptimal solution For the objective to be minimized, at any time t, the optimal solution is to learn a target option corre-sponding to the current easiest but not-yet-learned abstraction among the observation streams (to satisfy Constraints (1)–(3)) and a policy that maximizes the variance in the encoded abstraction output (to satisfy Constraint (4)).However, since (cid:7) (see Constraint (3)) is not known a priori, it needs to be estimated online by actively exploring the input exploratory options over time. One possible approach is to find (a) an analytical expression2 of (cid:7) for the particular abstraction-estimator (cid:2) and (b) an observation stream selection technique that can estimate the (cid:7) values for each obser-vation stream. This approach would be dependent on the abstraction-estimator used. However, our proposed framework employs an abstraction-estimator independent approach by making use of reinforcement learning to estimate the (cid:7) values, in the form of curiosity rewards generated through the learning progress made by (cid:2).4.2. Continual curiosity-driven skill acquisitionIn the above formulation, the agent has a fixed set of n(≥ 1) input exploratory options. Therefore, the number of learn-able target options is equal to the total number of learnable abstractions, which is at most equal to the number of input exploratory options:(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)OL(t)(cid:4) ≤ n.(5)limt→∞2 Refer to our previous work [47] for an analytical expression of (cid:7) for IncSFA.320V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335Fig. 4. (a) Exploratory-option policy has two phases: If the estimation error of any already learned abstraction modules for the incoming observations is lower than threshold δ, the exploratory-option’s policy is learned using Least Squares Policy Iteration (LSPI). If the estimation error is higher than the threshold then the policy is a random walk. (b) An example thresholded estimation error and the (c) corresponding exploration policy.To enable continual learning [29], the number of skills acquired by the agent should not necessarily be bounded and the agent needs to reuse the previously acquired skills to learn more complex skills. Therefore, continual curiosity-driven skill acquisition learning problem is a slightly modified version of the above formulation, such that the target options learned form a basis for new input exploratory options:Oe ← Oe ∪ F(OL),(6)where F (.) denotes some functional variation of a deterministic target option to make it stochastic (exploratory). Therefore, the number of input exploratory options (n) increases whenever a new skill is acquired by the agent.Sub-target options Constraint (4) requires that each target-option’s policy maximizes variance of the observed J -dimensional abstraction outputs. However in principle, the constraint can be re-written such that only a subset of J dimensions of the abstraction can be used to learn a policy. This results in a maximum number of 2 J − 1 learnable policies. We denote a set of target options that all share the same abstraction {(cid:8)IL(cid:9); j ≤ (2 J − 1)} as sub-target options. To keep it simple however, in the rest of the paper we use all the J dimensions, as presented in Constraint (4), to learn the target-option’s policy and therefore limiting 1 target option for each learned abstraction.i , φi, π Li , βLi j5. Continual curiosity-driven skill acquisition (CCSA) frameworkSection 3 presented an overview of our proposed framework. Here, we discuss each part of the framework in detail and also show how it addresses the learning problem formalized in Section 4.5.1. Input exploratory optionsAs discussed in Section 4, we defined a set of input exploratory options that the agent can execute to interact with the environment. Here, we present details on how to construct these options.The simplest exploratory-option policy is a random walk. However, we present here a more sophisticated variant that uses a form of initial artificial curiosity, based on error-based rewards [22]. This exploratory-option’s policy π e is determined by the predictability of the observations x(t), but can also switch to a random walk when the environment is too unpre-dictable.This policy π e has two phases. If the estimation error of any already learned abstraction modules for the incoming observations is lower than threshold δ, the exploratory-option’s policy is learned using Least-Squares Policy Iteration Tech-nique (LSPI; [53]), with an estimation of the transition model actively updated over the option’s state-space I e⊆ S(cid:4), and ian estimated reward function that rewards high estimation errors. Such a policy encourages the agent to explore its “un-seen world” (Fig. 4(a)). But if the estimation error of already learned abstraction modules is higher than the threshold δ, then the exploratory-option’s policy is a random-walk over the option’s state-space. Fig. 4 illustrates this error seeking exploratory-option’s policy. We denote this policy as LSPI-Exploration policy. When the agent selects an exploratory option i to execute, it follows the option’s policy, generating an observation stream xi = U (P(s, π ei (s(cid:4)))), until the termination O econdition is met. To keep it general and non-specific to the environment, in all our experiments, each exploratory-option’s termination condition is such that the option terminates after a fixed τ time-steps since its execution.V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335321Fig. 5. Architecture of Curious Dr. MISFA includes (a) a reinforcement learning agent that generates observation-stream selection policy based on intrinsic rewards, (b) an adaptive Incremental SFA coupled with Robust Online Clustering module that updates an abstraction based on the incoming observations, and (c) a gating system that prevents encoding observations that have been previously encoded.Setting a different input exploratory-option set would influence the skills developed by CCSA. In our experiments at t = 0, the agent starts with only a single exploratory option as defined above. The LSPI-Exploration policy only speeds up the agent’s exploration by acting deterministically in the predictable world and randomly in unseen world. Since at t = 0the world is unexplored, LSPI-Exploration policy is just a random walk in the agent’s abstracted states. Environment or domain specific information can be used to design the input exploratory-option set in order to shape the resulting skills. For example, exploratory options with random-walk policies mapped to different sub-regions in the robot’s joint space can be used.5.2. Curiosity-driven abstraction learning: Curious Dr. MISFAAt the core of the CCSA framework is the Curiosity Driven Modular Incremental Slow Feature Analysis Algorithm (Curious Dr. MISFA; [47,48]).3 The order in which skills are acquired in the CCSA framework is a direct consequence of the order in which the abstractions are learned by the Curious Dr. MISFA algorithm. The input to the Curious Dr. MISFA algorithm is a set of high-dimensional observation streams X = {x1, . . . , xn : xi(t) ∈ RI , I ∈ N}, generated by the input exploratory-option’s policies. The result is a slow feature abstraction φi corresponding to the easiest yet unknown observation stream. Apart from learning the abstraction, the learning process also involves selecting the observation stream that is the easiest to encode. To this end, Curious Dr. MISFA uses reinforcement learning to learn an optimal observation-stream selection policy, based on the intrinsic rewards proportional to the progress made while learning the abstraction. In this section, we briefly review the architecture of Curious Dr. MISFA.Fig. 5 illustrates the architecture of Curious Dr. MISFA, which includes (a) a reinforcement learning (RL) agent that gen-erates an observation-stream selection policy based on intrinsic rewards, (b) an adaptive Incremental Slow Feature Analysis coupled with Robust Online Clustering (IncSFA-ROC) module that updates an abstraction based on the incoming observa-tions, and (c) a gating system that prevents encoding observations that have been previously encoded. The RL agent is within an internal environment that has a set of discrete states S int = {sint}, equal to the number of observation streams. 1 , . . . , sint, the agent is allowed to take only one of the two actions (Aint): stay or switch. The action stay makes the In each state sintni3 A Python-based implementation of Curious Dr. MISFA can be found at the URL: www.idsia.ch/~kompella/codes/.322V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335iIt maintains an adaptive abstraction (cid:9)φ ∈ RI× J , (cid:9)φ /∈ (cid:4)tagent’s state to be the same as the previous state, while switch randomly shifts the agent’s state to one of the other internal states. The agent at each state sint, receives a fixed τ time step sequence of observations (x) of the corresponding stream xi .that updates based on the observations x via IncSFA-ROC abstraction-estimator. The agent receives intrinsic rewards proportional to the learning progress made by IncSFA-ROC. The observation stream selection policy π int : S int × Aint → [0, 1] is learned from the intrinsic rewards and then used to select the observation stream for the next iteration, yielding new samples x. These new samples, if not encodable by previously learned abstractions, are used to update the adaptive abstraction. The updated abstraction (cid:9)φ is added to the abstraction set (cid:4)t , when the IncSFA-ROC’s estimation error falls below a low threshold δ. If and when added, a new adaptive abstrac-tion (cid:9)φ is instantiated and the process continues. The rest of this section discusses more details on different parts of the Curious Dr. MISFA algorithm.Abstraction-estimator Curious Dr. MISFA’s abstraction estimator is the Incremental Slow Feature Analysis (IncSFA; [46]) coupled with a Robust Online Clustering (ROC; [75,76]) algorithm. IncSFA is used to learn real-valued abstractions of the observations, while ROC is used to learn a discrete mapping between the abstraction outputs y and the agent’s abstracted-state space S(cid:4). In particular, each abstracted state (s(cid:4) ∈ S(cid:4)) has an associated ROC implementation node that estimates multiple cluster centers within the slow-feature outputs.IncSFA is an incremental version of Slow feature analysis (SFA; [14]), which is an unsupervised learning technique that extracts features from an observation stream with the objective of maintaining an informative but slowly-changing feature response over time. SFA is concerned with the following optimization problem: Given an I -dimensional input signal x(t) =[x1(t), . . . , xI (t)]T , find a set of J instantaneous real-valued functions g(x) = [g1(x), . . . , g J (x)]T , which together generate a J -dimensional output signal y(t) = [ y1(t), . . . , y J (t)]T with y j(t) = g j(x(t)), such that for each j ∈ {1, . . . , J }(cid:10) j = (cid:10)( y j) = (cid:8) ˙y2j(cid:9)is minimalunder the constraints(cid:8) y j(cid:9) = 0 (zero mean),(cid:8) y2j(cid:9) = 1 (unit variance),∀i < j : (cid:8) yi y j(cid:9) = 0 (decorrelation and order),(7)(8)(9)(10)with (cid:8)·(cid:9) and ˙y indicating temporal averaging and the derivative of y, respectively. The goal is to find instantaneous functions g j generating different output signals that are as slowly varying as possible. The decorrelation constraint (10) ensures that different functions g j do not code for the same features. The other constraints (8) and (9) avoid trivial constant output solutions. SFA operates on the covariance of observation derivatives, so it scales with the size of the observation vector instead of the number of states. SFA is originally realized as a batch method, requiring all data to be collected before processing. The algorithmic complexity is cubic in the input dimension I . By contrast, Incremental SFA (IncSFA) has a linear update complexity [46], and can adapt the features to new observations, achieving the slow feature objective robustly in open-ended learning environments.ROC is a clustering algorithm similar to an incremental K-means algorithm [77] — a set of cluster centers is maintained, and with each new input, the most similar cluster center (the winner) is adapted to become more like the input. Unlike K-means, with each input it follows the adaptation step by merging the two most similar cluster centers, and creating a new cluster center at the latest input. In this way, ROC can quickly adjust to non-stationary input distributions by directly adding a new cluster for the newest input sample, which may mark the beginning of a new input process.Estimation error and curiosity reward Each ROC-Estimator node j has an associated error ξ j . These errors are initialized to 0 and then updated whenever the node is activated by: ξ j(t) = min(cid:13)y(t) − vw (cid:13), where y(t) is the slow-feature output wvector, vw is the estimate of the wth cluster of the activated node and (cid:13).(cid:13) represents L2 norm. The total estimation error is calculated as the sum of stored errors of the nodes: ξ(t) =ξ j(t). The agent receives rewards proportional to the derivative of the total estimation error, which motivates it to continue executing an option that is yielding a meaningful learnable abstraction. The agent’s reward function is computed at every iteration from the curiosity rewards ( ˙ξ ) as follows:j=1Rint(sint, sint− , aint) = (1 − η) Rint(sint, sint− , aint) + ηt+τ(cid:10)t− ˙ξ (t),where 0 < η < 1 is a discount factor, τ is the duration of the current option until its termination, (sint, sintaint ∈ {stay, switch}.− ) ∈ S int and p(cid:10)V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335323Observation-stream selection policy The transition-probability model P int of the internal environment is similar to a complete graph and is given by:(cid:11)P inti, j,stay=1,0,if i = jif i (cid:16)= j, P inti, j,switch=0,1N−1 ,if i = jif i (cid:16)= j,(11)(cid:11)∀i, j ∈ [1, . . . , N]. Using the current updated model of the reward function R int and the internal-state transition-probability model P int, we use model-based Least Squares Policy Iteration [53] to generate the agent’s internal-policy π int : S int →{stay, switch} for the next iteration. The agent uses decaying (cid:13)-greedy strategy [5] over the internal policy to carry out an internal-action (stay or switch) for the next iteration.Module freezing and new module creation Once the adaptive (training) module’s (cid:9)φ estimation error gets lower than a thresh-old δ, the agent freezes and saves the IncSFA-ROC module, resets the (cid:13)-greedy value and starts training a new module.Gating system and abstraction assignment The already trained (frozen) modules represent our learned library of abstrac-tions (cid:4)t . If a trained module’s estimation error within an option is below the threshold δ, that option is assigned that module’s abstraction and the adaptive training module (cid:9)φ will be prevented from learning via a “gating signal” (see Fig. 5). There will no intrinsic reward in this case. Hence the training module (cid:9)φ will encode only data from observation streams that were not encoded earlier. Input badly encoded by all other trained modules serve to train the adaptive module.5.3. Learning a target optionFrom the set of observations streams generated by the input exploratory options, Curious Dr. MISFA learns a slow feature abstraction (say φi ) corresponding to the estimated easiest-yet-unlearned exploratory option stream (say xj). The abstrac-tion’s output stream yi = φi(xj) has a zero-mean and unit-variance over time [46], and is a lower-dimensional representation of the input x j (satisfies Constraint (1); see Section 4.1.2). The output values yi(t) are discretized to a set of abstraction states S(cid:4), which represent the newly discovered abstracted states of the agent. A deterministic target option is then con-φistructed as follows:) The initiation set is simply the product state-space: ILInitiation set (ILover a larger abstracted-state space that includes the newly discovered abstraction states.= (I ej× S(cid:4)φii). Therefore, the option is now defined ) The target option policy π LTarget option policy (π L→ A must be done in such a way as to satisfy Constraint (4). To this end, we use Model-based Least-Squares Policy Iteration Technique (LSPI; [53]) over an estimated transition and reward models. The target-option’s transition model P O−) samples generated via the exploratory-option’s policy π ej . As to estimate the reward function, the agent uses rewards proportional to the difference of subsequent abstraction activations:i has been continually estimated from the (s(cid:4), a, s(cid:4): ILiLi(12)(13)Li (t) = (cid:13)yi(t) − yi(t − 1)(cid:13)r OLi (s(cid:4), a) = (1 − α)R OR O(cid:12)Li (s(cid:4), a) + αr O(cid:13)Li (t),(cid:12)(cid:13)j (s(cid:4))))U (P(s−, π eU (P(s, π ewhere yi(t) = φi, s− and s are the corresponding environment states, P is the unknown transition-model of the environment. 0 < α < 1 is a constant smoothing factor. Once the estimated transition and reward models stabilize, LSPI follows the RL objective and learns a policy π Lthat maximizes the expected cumulative reward over time:and yi(t − 1) = φij (s(cid:4)−)))i(cid:14)∞(cid:10)(cid:4)(cid:4)L(cid:4)π , R Oi (t)Liγ tr O(cid:15)π Li= arg supπEt=0where γ is a discount factor close to 1. Therefore, π Lmaximizing variance of the activations [78] (approximately4 satisfying Constraint (4)).i maximizes the average activation differences, which is equivalent to ,(14)Termination condition (βLR Omaximum reward max(s,a).) The option terminates whenever the agent reaches the abstracted-state where it observes the LiEach target option learned is added to the target-option set OLand the learning process iterates until all the learnable exploratory option streams are encoded. Since the expected behavior of Curious Dr. MISFA ensures that the Constraints 4 The error between the true and the estimated target-option policy depends on how well the transition and reward models are estimated based on the samples (s(cid:4), a, s(cid:4)−) generated by the exploratory-option’s policy.324V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335Fig. 6. Reuse of the learned target options. For each target option learned (represented by pink dotted box), two new exploratory options (Biased Initialization and Explore and Policy Chunk and Explore) are added to the input exploratory-option (represented by red dashed boxes) set. Biased Initialization and Explore option biases the agent to explore first the state-action tuples where it had previously received maximum intrinsic rewards, while the Policy Chunk and Explore option executes the deterministic target-option’s policy before exploration.(1)–(3) are satisfied [47] and the learned target-option’s policy satisfies Constraint (4), the target-option set OLtime t, therefore satisfies the required constraints., at any In Section 4, we discussed an alternative to Constraint (4), where different dimensions of the learned abstraction may be used to learn multiple policies, resulting in a set of sub-target options. To keep it simple, we used all dimensions of an abstraction to learn a target-option’s policy. However, a sub-target option set can be constructed by following the approach discussed above. Multiple reward functions can simultaneously be estimated from the (s(cid:4), a, s(cid:4)−) samples generated via exploratory-option’s policy, and the set of sub-target options can be constructed via least-squares policy iteration in parallel.5.4. Reusing target optionsTo make the skill acquisition open-ended and to acquire more complex skills (see Section 4.2), the learned target option Lcan be used to explore the newly discovered abstracted-state space (see Section 5.3). However, a target option may Onot be reused straight-away, since by definition, it differs from an exploratory option, wherein the target-option’s policy is deterministic, while the exploratory-option’s policy is stochastic (see Section 5.1). We construct two new exploratory options instead, which are based on the target option Othat was learned last (see Fig. 6).= ILi. The policy combines the target-option’s policy π LIn the first option, called policy chunk and explore, the initiation-set is the same as that of learned target option I e, which terminates at the state where the variance of subse-n+1iquent encoded observations is highest, with the LSPI-Exploration policy described in Section 5.1. Every time this policy is initiated, the policy-chunk (A policy chunk is a non-adaptive frozen policy) π Lis executed, followed by the LSPI-Exploration policy. This can be beneficial if the target option terminates at a bottleneck state, after which the agent enters a “new world” of experience, within which the LSPI-Exploration policy is useful to explore.iLiIn the second option, called biased initialization and explore, the exploratory-option’s policy uses the normalized value function of the target option as an initial reward function estimate. This initialization biases the agent to explore first the state-action tuples where it had previously received maximum intrinsic rewards. Otherwise it is the same as the standard initial error-seeking LSPI-Exploration policy.For each target option learned, these two exploratory options are added to the input exploratory-option set. In this way, the agent continues the process of curiosity-based skill acquisition by exploring among the new exploratory option set to (cid:9) can be learned as a consequence of chaining multiple discover unknown regularities. A complex skill Oskills that were learned earlier.k , φk, π Lk , βL= (cid:8)ILLkk5.5. PseudocodeThe entire learning process involves determining three policies:1. π e : Exploratory-option’s stochastic policy that is determined (see Section 5.1) to generate high-dimensional observations.2. π int: An internal policy that is learned (see Section 5.2) to determine for which exploratory option O e to encode a slow feature abstraction.V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335325Algorithm 1: Int-Policy-Update (x).//Curious Dr. MISFA Internal Policy Update1 Abstraction-Learned ← False 2 φ ← Gating-System(x) 3 ξt+1 = (cid:13)(cid:2)(x, φ) − φ(cid:13)4 if (cid:8)ξt+1(cid:9)τ > δ then(cid:9)φ ← (cid:2)(x, (cid:9)φ)5if (cid:8)(cid:13)(cid:2)(x, (cid:9)φ) − (cid:9)φ(cid:13)(cid:9)τ < δ then(cid:4)t+1 ← (cid:4)t ∪ (cid:9)φAbstraction-Learned ← True6789end10 end11 Rintt+112 π intt+113 π intt+114 return (π int← UpdateReward ( ˙ξt+1) ← Model-LSPI (P int, Rint← (cid:13)-greedy (π intt+1) t+1, Abstraction-Learned)t+1) Algorithm 2: Continual Curiosity-driven Skill Acquisition (CCSA).1 (cid:4)0 ← {}, π0 ← Random (), (cid:9)φ ← 0, Abstraction-Learned ← False2 for t ← 0 to ∞ do3in state sintsint ← current internal state, aint ← action selected by π intTake action aint, observe next internal state sint//Execute the exploratory option O ewhile not βe− (= i)tii (t) dos(cid:4) ← current abstracted-state, a ← action selected by π eTake action a, observe next abstracted-state s(cid:4)if not Abstraction-Learned then− and the sample xiin state s(cid:4)//Internal Policy Updatet+1, Abstraction-Learned) = Int-Policy-Update (x)(π intelsetL, Rprev ← R O//Learn target optionπ intt+1LR OP Oif ((cid:13)R O← π int(s(cid:4), a) = (1 − α)R O(s(cid:4), a, s(cid:4)−) = (1 − α)P OL − Rprev(cid:13) < δ and (cid:13)P O, R OLLLL, P prev ← P OL(s(cid:4), a) + α((cid:13)yi(t) − yi(t − 1)(cid:13))(s(cid:4), a, s(cid:4)−) + αL − P prev(cid:13) < δ) thenL)Lπ L ← LSPI-Model( P OL = (cid:8)IL, βL, (cid:9)φ, π L(cid:9)OOL ← OL ∪ O//Construct two new exploratory optionsLOe ← Oe ∪ Biased-Init-Explore( O)LOe ← Oe ∪ Policy-Chunk-Explore( O(cid:9)φ ← 0, Abstraction-Learned ← False //Reset)endendend456789101112131415161718192021222324 end//Abstraction learned or not.//Get the assigned abstraction.//Estimation Error//Update the adaptive-abstraction//Update abstraction set//Update the int. reward func.//Update int. policy//Exploration-exploitation tradeoff//Construct target option//Add to target-option set: Target-option’s deterministic policy that is learned (see Section 5.3) to maximize variation in the slow feature 3. π Labstraction output.The resultant target options (skills) are stored and reused as discussed above to facilitate open-ended continual learning. Algorithms 1 and 2 summarize the entire learning process.56. Experimental resultsWe present here experimental results that focus on continual-learning of skills using an iCub humanoid platform. More studies on the types of representations learned by the IncSFA algorithm and curiosity-based abstraction learning with Curi-ous Dr. MISFA can be found elsewhere [47,48,46,68]. The results here are the first in which a humanoid robot such as an 5 Python-based code excerpts can be found at the URL: www.idsia.ch/~kompella/codes/.326V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335Fig. 7. (a) An iCub robot is placed next to a table, with an object (a plastic cup) in reach of its right arm and within its field-of-view. (b) Sample input images captured from both left and right iCub camera-eyes are an input to the algorithm.iCub, learns a repertoire of skills from raw-pixel data in an online manner, driven by its own curiosity, starting with low-level joint kinematic maps.6Learning a skill-set largely depends on the environment that the robot is in. For the sake of developing specific types of skills such as toppling an object, grasping, etc., we pre-selected a safe environment for the iCub to explore, yet the iCub is mostly unaware of the environment properties.Environment Our iCub robot is placed next to a table, with an object (a plastic cup) in reach of its right arm and within its field-of-view (Fig. 7(a)). The cup topples over upon contact, and the resulting images after toppling are predictable. There is a human experimenter present, who monitors the robot’s safety and replaces the cup in its original position after it is toppled. The iCub does not “know” that the plastic-cup and the experimenter exist. It continually observes the gray-scale pixel values from the high-dimensional images (75 × 100) captured by the left and right camera eyes (Fig. 7(b)). In addition to the experimenter and the cup, it also cannot recognize its own moving hand in the incoming image stream, as shown in Fig. 7(b).Task-relevant roadmap We do not induce exploration at the level of joint angles, due to the complexity of the robot’s joint space. Instead we give the robot a map of poses a priori. This compressed actuator joint-space representation is called a Task-Relevant Roadmap (TRM; [50]). This map contains a family of iCub postures that adhere to relevant constraints. The TRM is grown offline by repeatedly optimizing cost-functions that represent the constraints, using a Natural Evolution Strategies (NES; [79]) algorithm, such that the task-space is covered. This allows us to deal with complex cost-functions and the full 41 degrees-of-freedom of the iCub’s upper body. The constraints used: (a) the iCub’s hand is positioned on a 2D plane parallel to the table while keeping its palm oriented horizontally, (b) the left hand is kept within a certain region to keep it out of the way, and (c) the head is pointed towards the table. The task-space of the TRM comprises the x and y position of the hand, which forms the initial discretized 10 × 5 abstracted-state space S (cid:4) = S(cid:4)y . The action space xcontains 6 actions: move North, East, South, West, Hand-close and Hand-open.× S(cid:4)Because the full body is used, the movements look more dynamic, but as a consequence, the head moves around and looks at the table from different directions, making the task a bit more difficult. Even so, IncSFA still finds the resulting regularities in the raw camera observation stream, and the skill learner continues to learn upon these regularities, without any external rewards.Experiment parameters We use a fixed parameter setting for the entire experiment.IncSFA algorithm IncSFA has two learning update rules [46]: Candid-Covariance free Incremental Principal Component Anal-ysis (CCIPCA; [80]) for normalizing the input and Minor Component Analysis (MCA; [81]) for extracting slow features. For CCIPCA, we use learning rates 1/t with amnesic parameter 0.4, while for MCA the learning rate is set to 0.01. CCIPCA does variable size dimension reduction by calculating how many eigenvalues would be needed to keep 99% of the input variance — typically this was between 5–10 — so the 7500 pixels could be effectively reduced to only about 10 dimensions. The output dimension is set to 1, therefore, we use only the first IncSFA feature as an abstraction. However, more number of features can be used if desired.Robust online clustering (ROC) algorithm ROC algorithm maps slow-feature outputs to abstracted states (see Section 5.2). Each clustering implementation has its maximum number of clusters set to N max = 3, such that it can encode multiple slow feature values for each abstracted state. Higher values can be used, however, very high values may lead to spurious clusters. The estimation error threshold, below which the current module is saved and a new module is created, is set to a low value 6 A video for these experiment can be found at URL: http :/ /www.youtube .com /watch ?v =OTqdXbTEZpE.V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335327δ = 0.3. The amnesic parameter is set to β amn = 0.01. Higher values will make ROC adapt faster to the new data, however at the cost of being less stable.Curious Dr. MISFA’s internal reinforcement learner To balance between exploration and exploitation, (cid:13)-greedy strategy is used (see Section 5.2). The initial (cid:13)-greedy value is set to 1.0 (1.0 for pure exploration, 0.0 for pure exploitation), with a 0.995decay multiplier. The window-averaging time constant is set to τ = 20, that is, 20 sample images are used to compute the window-averaged progress error ξ and the corresponding curiosity-reward (see Section 5.2).Target-option’s reinforcement learner Slow features abstractions have unit-variance and are typically in the range of (−1.5, 1.5) [46]. Since in our experiments we are expecting step-like slow features, to keep it simple, each abstraction-output values are discretized to either (−1, 1), therefore into two |S φi | = 2 abstracted states.Experiment initialization The iCub’s abstracted-state space (S (cid:4)) at t = 0 is a 10 × 5 grid found using TRM. To minimize human input into the system, the input exploratory-option set (Oe ) has only one exploratory option to begin with (as de-fined in Section 5.1): Oe = {O e}, which is a random-walk in the iCub’s abstracted-state space. However, one may pre-define 1multiple input exploratory options, which could lead to a different result. The exploratory option terminates after τ = 20time steps since its execution. The internal state-space at t = 0 is S int = {sintcorresponds to the exploratory 1option O e1. The plastic cup is roughly placed around (2, 2) grid-point on the table.}, where sint16.1. iCub learns to topple the cupThe iCub starts the experiment without any learned modules, so the exploratory-option’s policy π e1 is a random-walk over the abstracted state space S (cid:4) (see Section 5.4). It explores by taking one of the six actions: North, East, South, West, Hand-close and Hand-open and grabs high-dimensional images from its camera-eyes. The exploration causes the outstretched hand to eventually displace or topple the plastic-cup placed on the table. It continues to explore and after an arbitrary amount of time-steps the experimenter replaces the cup to its original position. After every τ time-steps, the currently ex-ecuting option terminates. Since there is only one exploratory option, the iCub re-executes the same option. Fig. 8(a) shows a sample input image stream of only the left-camera.7Fig. 8(b) shows the developing IncSFA output over the algorithm execution time, since the IncSFA abstraction was created. The outcome of IncSFA abstraction learning is a step-like function, which when discretized, indicates the pose of the cup (toppled vs non-toppled). Fig. 8(c) shows the ROC estimation error (blue solid line) and an Expected Moving Average (EMA) of the error (green dashed line) over the algorithm execution time. As the process continues, the error eventually drops below the threshold δ = 0.3 and the abstraction module φ1 is saved. Fig. 9(a) shows the ROC cluster centers that map the feature outputs (y) to each of the 10 × 5 abstracted states. There are two well separated clusters each representing the state of the plastic-cup.L1 are learned, followed by a corresponding target-option’s policy π LImmediately after the abstraction is saved, the cluster centers are discretized (red and yellow colors indicate the dis-cretized feature states S (cid:4)in Fig. 9(a)), the transition model (represented by the blue lines in Fig. 9(a)) and reward model φ11 as discussed in Section 5.3. Fig. 9(b) shows a part of Oof the learned policy π L1 before the cup is toppled. The arrows indicate the optimal action to be taken at each grid-location of the iCub’s hand. They direct the iCub’s hand to the grid point (1, 3), which will make the iCub topple the cup placed at (2, 2). Fig. 9(c) shows the part of the policy after the cup has been toppled. The policy directs the iCub’s hand to move towards east. This is because, during the experiment the experimenter happened to replace the cup only when the iCub’s L1 , for the given environment, as a “Topple” skill.hand is around far east. We label the learned target option O6.2. iCub learns to grasp the cupThe iCub continues its learning process by reusing the learned topple skill to construct two additional exploratory options as discussed in Section 5.4. One in which the topple policy (Fig. 9(b)) is executed prior to the LSPI-Exploration policy and the other, where the normalized value function (Fig. 10(b)) is used to initialize the reward-function of the LSPI-Explorer. 3 denote these two exploratory options respectively. Therefore, including the original exploratory option O eLet O e1, a total of 3 exploratory options are an input to CCSA.2 and O e1 or O eInitially, the system explores by executing each of the options until termination, i.e., after τ time steps. When it selects either O e2, the cup gets toppled in the process (Fig. 10(a)-Top) and since there already exists a learned abstraction φ1that encodes the toppling outcome, it receives no internal reward for executing these options because of the gating system (see Section 5.2). This is also the case in the beginning while executing O e2, because the LSPI-Exploration policy initially causes the iCub to topple the cup, yielding no rewards. The initialized values corresponding to the visited state-action tuples soon vanish and the iCub then explores the neighboring state action pairs. Eventually, as a result of the biased exploration, 7 We, however, used both the left and right camera images as an input observation by concatenating them.328V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335Fig. 8. (a) A sample image stream of the iCub’s left-eye camera showing the topple event. (b) Developing IncSFA abstraction output over algorithm execution time, since it was created. The result is a step-like function encoding the topple event. (c) ROC estimation error over algorithm execution time. The estimation error eventually drops below the threshold (δ = 0.3), after which the abstraction is saved.3 is executed.in a few algorithm iterations the iCub ends up grasping the cup (Fig. 10(a)-Bottom). This gives rise to a high estimation error because of the novelty of the event (Fig. 10(c)). Figs. 10(d)–(i) show the state-action LSPI-Exploration reward function after a few time steps. The hand-close action at (2, 2) generates the most novel event. This results in a LSPI-Exploration policy that increases the number of successful grasp trials (77 out of 91 total attempts, with most of the unsuccessful trials in the beginning) when the exploratory option O ecorresponding to the O eNow, upon executing option O e3, the adaptive abstraction ˆφ begins to make progress by encoding samples correspond-ing to the observation stream x3. After a few algorithm iterations, the agent finds that the action stay at the internal state sint3 is rewarding due to the progress made by IncSFA and the ROC estimator (Fig. 11(a)). 3Fig. 11(b) shows the normalized internal reward function of Curious Dr. MISFA over algorithm iterations, since the new adaptive module was created. The internal policy π int quickly converges to select and execute the option O e3 to receive more observations. When the estimation error drops below the threshold (δ = 0.3), it saves the module φ2 = ˆφ. Fig. 11(c) shows the IncSFA output over the time since the new module was created. Fig. 11(d) shows the learned cluster centers mapping the slow-feature output to the abstracted-state space. Note that the abstracted states corresponding to the learned top-ple abstraction S(cid:4)and not are shown in Fig. 11(d), because the grasp abstraction outputs are uncorrelated to the topple φ1abstraction and it is difficult to illustrate a 4-D plot. The iCub then begins to learn the target policy π L2 by learning the target-option’s transition and reward model. Fig. 12(a)–(f) show the target-option’s state-action reward model developed after 8000 observation samples (module time = 8000). And finally, Fig. 12(g) shows the corresponding skill learned, i.e., to perform a Hand-Close at (2, 2) (the anti clockwise circular arrow represents the Hand Close action).V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335329Fig. 9. (a) The resultant ROC cluster centers, which map the abstraction outputs to the abstracted-state space (in this case the X and Y grid locations of . Blue lines connecting the cluster centers illustrate the learned transition the iCub’s hand). Red and yellow colors indicate the discretized feature states S(cid:4)φ1model of the new abstracted-state space. (b) Part of the learned target-option’s policy before the cup is toppled. The arrows indicate the optimal action to be taken at each grid-location (s(cid:4)y ) of the iCub’s hand. They direct the iCub’s hand to the grid point (1, 3), which will make the iCub topple the cup placed at (2, 2). (c) Part of the learned target-option’s policy after the cup is toppled. They direct the iCub’s hand to move to the right. This is a result of the experimenter replacing the cup only when the iCub has moved its hand away from the (2, 2) grid location. (For interpretation of the references to color in this figure, the reader is referred to the web version of this article.)x , s(cid:4)This experiment demonstrated how the iCub reused the knowledge gained by the topple skill to learn a subsequent skill labeled as “Grasp”. The grasp skill includes an abstraction to represent whether the cup has been successfully grasped-or-not and a policy that directs the iCub’s hand to move to (2, 2) and then to close its hand.6.3. iCub learns to pick and place the cup at the desired locationWe present here an experiment to demonstrate the utility of intrinsic motivation in solving a subsequent external ob-jective (see Fig. 13). The iCub is in a similar environment as discussed above. However, it is given an external reward if it picks the plastic cup and places (drops) it at a desired location (at any of the following grid locations (s(cid:4)y ): (6, 2), (6, 3), (6, 1), (5, 2), (7, 2)). The agent with no intrinsic motivation finds the reward almost inaccessible via random −5) However, a cu-exploration over its abstracted-state space S (cid:4), because the probability of a successful trial is low.8 (≈ 10riosity driven iCub greatly improves this by learning to pick/grasp the cup by itself and then reusing the skill to access the reward.x , s(cid:4)Starting from the 10 × 5 abstracted-state space found via TRM, the iCub learns to topple and then grasp as discussed in the previous sections. The process continues and it adds two more exploratory options (O e5) corresponding to the grasp skill as discussed in Section 5.4. The biased initialization and explore option O e4 results in the iCub dropping the cup close to where it has picked it up. Since it doesn’t get any reward in this case, the initialized values to the visited state-actions tuples vanish and it explores the neighboring state-action tuples. This option will take a long time before it can execute the desired state-action tuple to drop the cup. The policy chunk and explore option O e5, however, first executes the grasp policy and then randomly explores until it receives some novelty or curiosity reward. When, it drops the cup in one of the desired states while exploring, it gets an external reward, which results in a LSPI-Exploration policy that executes the rewarding behavior. Curious Dr. MISFA eventually finds the internal action stay at the internal-state sintcorresponding to the option 5O e5 most rewarding. As soon as the experimenter replaces the cup, the iCub repeats the pick and place behavior until the external reward is removed.4, O e8 The probability of a successful pick = 1/300, probability of a drop given a successful pick = 1/300 ∗ 1/60.330V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335Fig. 10. (a) Sample iCub’s left-eye camera images corresponding to the three input exploratory options. x1 and x2 correspond to the original and the policy chunk & explore exploratory option respectively, while x3 corresponds to the biased init. & explore exploratory option. (b) Normalized value function of the previously learned target option (topple). It is used for reward-initialization in the biased init. & explore exploratory option. (c) Estimation error of the learned topple abstraction module (φ1) for each of the three observation-streams. (d)–(i) LSPI-Exploration reward function estimated using the novelty (& curiosity) signal. The Hand-Close action at (2, 2) has the maximum reward value due to the novel grasp event.This experiment demonstrated how CCSA enabled the iCub to reuse the grasp skill, which was previously learned via intrinsic motivation, on learning to pick and place the cup to a desired location. Note that in our experiments, a human experimenter unknown to the robot, acted as a part of the environment to speed up the learning process. Without the experimenter, the robot might not have acquired the same set of skills, instead it might have learned to push the object (refer to our previous work [48] for an experiment where a simulated iCub learns a slow feature abstraction that encodes a push).7. DiscussionWhile, much of the research in humanoid robot learning has been based upon human demonstrations, human-given task-descriptions, or pre-processed inputs, CCSA makes an important step towards combining several aspects needed to develop an online, continual curiosity-driven humanoid robotic agent. In the following we briefly discuss these aspects along with current limitations of our framework and insights for future work:Raw high-dimensional information processing CCSA uses a linear IncSFA algorithm updated online directly from raw-pixels to encode abstractions that lead to acquiring skills. To learn more complex skills however, CCSA might benefit from ex-V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335331Fig. 11. (a) ROC estimation error of the current adaptive-module that is encoding the new regularities. (b) Normalized internal-reward function of Curious 3 -St) is most rewarding due to the learning progress made Dr. MISFA. The action stay in the state corresponding to the exploratory option 3 (shown as sintby the IncSFA-ROC module for the grasp-event. (c) IncSFA output over execution time, since it was created. (d) Resultant ROC cluster centers mapping the IncSFA output w.r.t. the abstracted-state space. Note that the abstracted states corresponding to the learned topple abstraction S(cid:4)are not shown here, φ1since the grasp abstraction outputs are uncorrelated to the topple abstraction and it is difficult to illustrate a 4-D plot. Red and yellow colors indicate the discretized states S(cid:4)and the blue lines illustrate the learned transition model. (For interpretation of the references to color in this figure, the reader is φ2referred to the web version of this article.)Fig. 12. (a)–(f) Estimated reward-function of the new abstracted-state space that is used to learn the target-option’s policy. The hand-close action at (2, 2)receives the maximum reward as it produces a maximum variation in the slow-feature output (from ≈ −1.5 to 1.5). (g) Learned target-option’s policy representing the grasp skill. The arrows indicate the optimal actions to be taken at each grid-location (s(cid:4)y ). The circular arrow represents the hand-close action. The policy directs the iCub’s hand to move to (2, 2) and then to close its hand, which should result in a successful grasp.x , s(cid:4)tracting non-linearities in the video inputs. Hierarchical extensions of IncSFA (H-IncSFA) over an expanded input in quadratic space [82] may remedy this. We plan to combine non-linear hierarchical structures to further improve the quality of the abstractions learned.332V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335Fig. 13. (a) CCSA now has 5 exploratory options as an input. Among the 5 options, only the policy chunk & explore corresponding to the grasp skill makes it easier for the iCub to access the external-reward present for placing the cup at the desired grid locations. This results in a policy — to place the cup in the desired location (the clockwise circular arrow represents the Hand-Open action). (b) Bird’s eye view of the iCub demonstrating the pick & place skill. (b) Figure shows the increasing dimensions in the agent’s abstracted-state space with every new abstraction learned. This experiment demonstrates how CCSA enables the iCub to reuse the grasp skill, which was previously learned via intrinsic motivation, on learning to pick & place the cup to a desired location.Invariant skills The skill labeled “grasp” in our experiments actually represents “grasp the cylindrical cup from the partic-ular location in the given environment, invariant to the experimenter’s actions and the iCub’s head/body movements”. The invariance picked up by the skills acquired in our system largely depend on the invariance learned by IncSFA from the observations sensed by the exploring iCub. Refer to our previous work [47,46] for more details on invariance extracted by IncSFA. In our experiments however, if the human experimenter had replaced the cup at different locations whenever the cup was toppled or dropped, we expect IncSFA to learn an abstraction that encodes whether the cup has been grasped-or-not invariant to the cup’s position (because the events are uncorrelated). This would result in a “grasp skill” that is invariant to the cup’s position.Continual learning CCSA uses previously acquired knowledge in the form of biased explorations or policy-chunks, to learn more complex skills. This facilitates continual learning of skills. A previously acquired skill may be refined or adapted to suit to changing environments. For example, in our experiments, if the cup’s position has changed after acquiring the grasp skill, the biased init. and explore exploratory-option corresponding to the grasp skill can speed up learning a new skill to grasp the cup from the new position. However, the old skills are still retained and reused if the cup’s position is changed back to its original position. The complex skills acquired using CCSA are in the form of a chain-like hierarchy (Fig. 14(a)), i.e., there exists only a single chain-link connecting higher-order to lower-order skills. This is because, a target option in CCSA is learned using observations only from one of the exploratory options (see Section 4). Each node in the chain-like hierarchy has only a single input but can act as an input to many nodes (Fig. 14(b)). Whereas, a node in a general compositional hierarchy (Fig. 14(c)) can have multiple inputs. One way to achieve compositional hierarchy in CCSA is to add the learned target options to the primitive action set A = {North, East, South, West, Hand-close and Hand-open}.V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335333Fig. 14. (a) The higher-order complex skills acquired using CCSA, are in the form of a chain-like hierarchy. There exists only a single chain-link (shown as a unique color) connecting higher-order to lower-order skills. This is because, a target option in CCSA is learned using observations only from one of the exploratory options. (b) An illustration of a node in a chain hierarchy. Each node has only a single input but can act as an input to many nodes. (c) Whereas, a node in a compositional hierarchy can have multiple inputs.Environment openness CCSA could benefit from a larger set of pre-defined input exploratory options. However, to minimize human inputs into the system, in our experiments the iCub starts with only a single exploratory option (random-walk) and autonomously adds more exploratory options derived from the learned target options. Since CCSA acts directly on raw-pixels, no prior calibration of the robot cameras are required. Algorithm parameters are intuitive to tune. Refer to our previous work [47,48,46,68] for a detailed description on tuning IncSFA and Curious Dr. MISFA algorithms. Therefore, CCSA can be used in different environments (and different humanoid robots) without making any design changes to the learning algorithm. On the motor end, we used a kinematic map that transforms the 41 degrees-of-freedom of the iCub joint configurations to 2D positions of its hand parallel to the table. For more complex manipulations, which are required for handling complicated objects, higher dimensional kinematic-maps could be used [50]. As our future work, we plan on using different approaches to tackle easier and safer manipulation with the iCub.Quality of skills acquired We presented formally the underlying learning problem as a constrained optimization problem. The objective function can be used as a metric to tune different parameters of the method. However, the metric does not sufficiently evaluate the quality of skills acquired. One major factor is the type of the abstraction-estimator used. For example, a method that uses a simpler abstraction learning algorithm may acquire a large number of skills, which could be functionally equivalent to acquiring a single skill of a more discriminative abstraction estimator. Therefore, evaluating different task-unrelated intrinsically-motivated (IM) approaches without providing an external goal is an ill-posed problem. As our future work, we plan to build realistic, task-independent, skill-acquisition benchmarks with hidden external tasks to evaluate multiple IM approaches.Scalability For each target option acquired by CCSA, the number of input exploratory options increases by a value of two (See Section 5.4). Observations from previously encoded exploratory options are automatically filtered out due to the gat-ing system of Curious Dr. MISFA. Therefore, for each target option acquired, the number of unknown exploratory options increases by a value of only one. Hence, the space of input exploratory options scales linearly with respect to the number of skills acquired.Sensor fusion And finally, CCSA uses only visual inputs from the onboard cameras and joint angles of the iCub. A humanoid-robot’s actions can be improved however, by using different sensory modalities such as tactile and audio in addition to the visual inputs. This should be straightforward addition to CCSA, since IncSFA is agnostic to the modality of sensory informa-tion. The raw inputs of different modalities can be concatenated as a single input and fed to the IncSFA algorithm, without causing too much computational overhead (since IncSFA has a linear update complexity [46]). Related work on combining sensory modalities using SFA methods have shown to achieve good results [83].8. ConclusionWe proposed an online-learning algorithm that enables a humanoid robotic agent, such as an iCub, to incrementally acquire skills in order of increasing learning difficulty, from its onboard high-dimensional camera inputs and low-level kine-matic joint maps, driven purely by its intrinsic motivation. The method combines our recently introduced active modular Slow Feature learning algorithm, called Curious Dr. MISFA and the options framework. We formally defined the underlying learning problem and provided experimental results conducted using an iCub humanoid robot to topple, grasp and pick-place a cup. To our knowledge, this is the first method that demonstrates continual curiosity-based skill acquisition from high-dimensional video inputs in humanoid robots.334V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335AcknowledgementsWe thank Sohrob Kazerounian and Alan Lockett’s assistance in revising some of this paper. This work was funded through SNF grant #138219 (Theory and Practice of Reinforcement Learning II) and through the 7th framework program of the EU under grant #270247 (NeuralDynamics project).References[1] Honda, Asimo robot, http://world.honda.com/ASIMO.[2] Boston-Dynamics, Petman (protection ensemble test mannequin) humanoid military robot, http://www.bostondynamics.com/robot_petman.html.[3] G. Metta, G. Sandini, D. Vernon, L. Natale, F. Nori, The iCub humanoid robot: an open platform for research in embodied cognition, in: Proceedings of the 8th Workshop on Performance Metrics for Intelligent Systems, PerMIS’08, ACM, New York, NY, USA, 2008, pp. 50–56.[4] L.P. Kaelbling, M.L. Littman, A.W. Moore, Reinforcement learning: a survey, J. Artif. Intell. Res. 4 (1996) 237–285.[5] R.S. Sutton, A.G. Barto, Reinforcement Learning: An Introduction, MIT Press, Cambridge, MA, 1998.[6] R.W. White, Motivation reconsidered: the concept of competence, Psychol. Rev. 66 (5) (1959) 297.[7] G.R. Norman, H.G. Schmidt, The psychological basis of problem-based learning: a review of the evidence, Acad. Med. 67 (9) (1992) 557–565.[8] H. Abut (Ed.), Vector Quantization, IEEE Press, Piscataway, NJ, 1990.[9] I.T. Jolliffe, Principal Component Analysis, Springer-Verlag, New York, 1986.[10] P. Comon, Independent component analysis, a new concept?, Signal Process. 36 (1994) 287–314.[11] D.D. Lee, H.S. Seung, Learning the parts of objects by non-negative matrix factorization, Nature 401 (6755) (1999) 788–791.[12] T. Kohonen, Self-Organizing Maps, 3rd edition, Springer-Verlag, Berlin, 2001.[13] G.E. Hinton, Training products of experts by minimizing contrastive divergence, Neural Comput. 14 (8) (August 2002) 1771–1800.[14] L. Wiskott, T. Sejnowski, Slow feature analysis: unsupervised learning of invariances, Neural Comput. 14 (4) (2002) 715–770.[15] J. Schmidhuber, Learning complex, extended sequences using the principle of history compression, Neural Comput. 4 (2) (1992) 234–242.[16] J. Schmidhuber, Learning unambiguous reduced sequence descriptions, in: J.E. Moody, S.J. Hanson, R.P. Lippman (Eds.), Advances in Neural Information Processing Systems 4, NIPS 4, Morgan Kaufmann, 1992, pp. 291–298.[17] J. Schmidhuber, Learning factorial codes by predictability minimization, Neural Comput. 4 (6) (1992) 863–879.[18] S. Lindstädt, Comparison of two unsupervised neural network models for redundancy reduction, in: M.C. Mozer, P. Smolensky, D.S. Touretzky, J.L. Elman, A.S. Weigend (Eds.), Proc. of the 1993 Connectionist Models Summer School, Erlbaum Associates, Hillsdale, NJ, 1993, pp. 308–315.[19] M. Klapper-Rybicka, N.N. Schraudolph, J. Schmidhuber, Unsupervised learning in LSTM recurrent neural networks, in: Proc. Intl. Conf. on Artificial Neural Networks, ICANN-2001, in: Lecture Notes on Comp. Sci., vol. 2130, Springer, Berlin, Heidelberg, 2001, pp. 684–691.[20] O.C. Jenkins, M.J. Matari ´c, A spatio-temporal extension to Isomap nonlinear dimension reduction, in: Proceedings of the Twenty-First International [21] H. Lee, Y. Largman, P. Pham, A.Y. Ng, Unsupervised feature learning for audio classification using convolutional deep belief networks, in: Advances in Conference on Machine Learning, ACM, 2004, p. 56.Neural Information Processing Systems, 2009, pp. 1096–1104.[22] S. Singh, A.G. Barto, N. Chentanez, Intrinsically motivated reinforcement learning, in: Advances in Neural Information Processing Systems, NIPS, 2004, [23] Y. Girdhar, D. Whitney, G. Dudek, Curiosity based exploration for learning terrain models, in: IEEE International Conference on Robotics and Automation, [24] A. Stout, A.G. Barto, Competence progress intrinsic motivation, in: IEEE 9th International Conference on Development and Learning, ICDL, IEEE, 2010, [25] L. Pape, C.M. Oddo, M. Controzzi, C. Cipriani, A. Förster, M.C. Carrozza, J. Schmidhuber, Learning tactile skills through curious exploration, Front. [26] S. Hart, S. Sen, R.A. Grupen, Intrinsically motivated hierarchical manipulation, in: Proceedings of the 2008 IEEE Conference on Robots and Automation, [27] G. Konidaris, S. Kuindersma, R. Grupen, A.G. Barto, Autonomous skill acquisition on a mobile manipulator, in: Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011, pp. 1468–1473.[28] L. Gisslén, M. Luciw, V. Graziano, J. Schmidhuber, Sequential constant size compressors for reinforcement learning, in: Artificial General Intelligence, pp. 1281–1288.ICRA, 2014.pp. 257–262.Neurorobot. 6 (2012).ICRA, 2008, pp. 3814–3819.Springer, 2011, pp. 31–40.[29] M.B. Ring, Continual learning in reinforcement environments, PhD thesis, University of Texas at Austin, 1994.[30] J. Schmidhuber, Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts, Connect. Sci. 18 (2) (2006) 173–187.[31] J. Schmidhuber, Formal theory of creativity, fun, and intrinsic motivation (1990–2010), IEEE Trans. Auton. Ment. Dev. 2 (3) (2010) 230–247.[32] J. Schmidhuber, Curious model-building control systems, in: Proceedings of the International Joint Conference on Neural Networks, vol. 2, Singapore, IEEE Press, 1991, pp. 1458–1463.[33] J. Storck, S. Hochreiter, J. Schmidhuber, Reinforcement driven information acquisition in non-deterministic environments, in: Proceedings of the Inter-national Conference on Artificial Neural Networks, vol. 2, Paris, EC2 & Cie, 1995, pp. 159–164.[34] J. Schmidhuber, Artificial curiosity based on discovering novel algorithmic predictability through coevolution, in: Congress on Evolutionary Computa-tion, CEC, IEEE Press, 1999, pp. 1612–1618.[35] J. Schmidhuber, Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts, Connect. Sci. 18 (2) (2006) 173–187.[36] J. Schmidhuber, Formal theory of creativity, fun, and intrinsic motivation (1990–2010), IEEE Trans. Auton. Ment. Dev. 2 (3) (2010) 230–247.[37] S. Lange, M. Riedmiller, Deep learning of visual control policies, in: European Symposium on Artificial Neural Networks, Computational Intelligence [38] M. Franzius, H. Sprekeler, L. Wiskott, Slowness and sparseness lead to place, head-direction, and spatial-view cells, PLoS Comput. Biol. 3 (8) (2007) and Machine Learning, ESANN, 2010, pp. 265–270.e166.[39] R. Legenstein, N. Wilbert, L. Wiskott, Reinforcement learning on slow features of high-dimensional input streams, PLoS Comput. Biol. 6 (8) (2010).[40] P. Földiák, M.P. Young, Sparse coding in the primate cortex, in: The Handbook of Brain Theory and Neural Networks, vol. 1, 1995, pp. 895–898.[41] G. Mitchison, Removing time variation with the anti-Hebbian differential synapse, Neural Comput. 3 (3) (1991) 312–320.[42] G. Wallis, E.T. Rolls, Invariant face and object recognition in the visual system, Prog. Neurobiol. 51 (2) (1997) 167–194.[43] L. Wiskott, Estimating driving forces of nonstationary time series with slow feature analysis, arXiv preprint, arXiv:cond-mat/0312317, 2003.[44] H. Sprekeler, T. Zito, L. Wiskott, An extension of slow feature analysis for nonlinear blind source separation, J. Mach. Learn. Res. 15 (2014) 921–947.[45] V.R. Kompella, M. Luciw, J. Schmidhuber, Incremental slow feature analysis, in: Proc. 20th International Joint Conference of Artificial Intelligence, IJCAI, 2011, pp. 1354–1359.V.R. Kompella et al. / Artificial Intelligence 247 (2017) 313–335335[46] V.R. Kompella, M. Luciw, J. Schmidhuber, Incremental slow feature analysis: adaptive low-complexity slow feature updating from high-dimensional [47] M. Luciw, V.R. Kompella, S. Kazerounian, J. Schmidhuber, An intrinsic value system for developing multiple invariant representations with incremental input streams, Neural Comput. 24 (11) (2012) 2994–3024.slowness learning, Front. Neurorobot. 7 (2013).[48] V.R. Kompella, M. Luciw, M. Stollenga, L. Pape, J. Schmidhuber, Autonomous learning of abstractions using curiosity-driven modular incremental slow feature analysis, in: Proc. of the Joint Conference on Development and Learning and Epigenetic Robotics, ICDL-EPIROB, IEEE, San Diego, 2012, pp. 1–8.[49] R.S. Sutton, D. Precup, S. Singh, Between MDPs and semi-MDPs: a framework for temporal abstraction in reinforcement learning, Artif. Intell. 112 (1) (1999) 181–211.2002, pp. 295–306.[50] M. Stollenga, L. Pape, M. Frank, J. Leitner, A. Förster, J. Schmidhuber, Task-relevant roadmaps: a framework for humanoid motion planning, in: IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS, IEEE, 2013, pp. 5772–5778.[51] I. Menache, S. Mannor, N. Shimkin, Q-cut — dynamic discovery of sub-goals in reinforcement learning, in: Machine Learning: ECML 2002, Springer, [52] O. ¸Sim ¸sek, A.G. Barto, Skill characterization based on betweenness, in: NIPS’08, 2008, pp. 1497–1504.[53] M.G. Lagoudakis, R. Parr, Least-squares policy iteration, J. Mach. Learn. Res. 4 (2003) 1107–1149.[54] M.B. Ring Child, A first step towards continual learning, Mach. Learn. 28 (1) (1997) 77–104.[55] B. Bakker, J. Schmidhuber, Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization, in: F. Groen, et al. (Eds.), Proc. 8th Conference on Intelligent Autonomous Systems, IAS-8, IOS Press, Amsterdam, NL, 2004, pp. 438–445.[56] S.W. Hart, The development of hierarchical knowledge in robot systems, PhD thesis, University of Massachusetts Amherst, 2009.[57] M. Huber, R.A. Grupen, A hybrid discrete event dynamic systems approach to robot control, Tech. Rep., Univ. Mass., Dept. Comput. Sci., Amherst, MA, [58] G. Konidaris, A.G. Barto, Skill discovery in continuous reinforcement learning domains using skill chaining, in: Advances in Neural Information Process-1996, pp. 96–143.ing Systems, 2009, pp. 1015–1023.[59] G. Konidaris, S. Kuindersma, A.G. Barto, R. Grupen, Constructing skill trees for reinforcement learning agents from demonstration trajectories, in: Advances in Neural Information Processing Systems, 2010, pp. 1162–1170.[60] J. Mugan, B. Kuipers, Autonomous learning of high-level states and actions in continuous environments, IEEE Trans. Auton. Ment. Dev. 4 (1) (2012) 70–86.[61] A. Baranes, P. Oudeyer, Active learning of inverse models with intrinsically motivated goal exploration in robots, Robot. Auton. Syst. 61 (1) (2013) 49–73.Psychol. 4 (2013).[62] H. Ngo, M. Luciw, A. Förster, J. Schmidhuber, Learning skills from play: artificial curiosity on a Katana robot arm, in: Proceedings of the International Joint Conference of Neural Networks, IJCNN, June 2012, pp. 1–8.[63] H. Ngo, M. Luciw, A. Förster, J. Schmidhuber, Confidence-based progress-driven self-generated goals for skill acquisition in developmental robots, Front. [64] I. Jolliffe, Principal Component Analysis, Wiley Online Library, 2005.[65] N. Sprague, Predictive projections, in: Proceedings of the International Joint Conference of Artificial Intelligence, IJCAI, 2009, pp. 1223–1229.[66] H. Sprekeler, On the relation of slow feature analysis and Laplacian eigenmaps, Neural Comput. 23 (12) (2011) 3287–3302.[67] S. Mahadevan, M. Maggioni, Proto-value functions: a Laplacian framework for learning representation and control in Markov decision processes, J. Mach. Learn. Res. 8 (2169–2231) (2007) 16.[68] M. Luciw, J. Schmidhuber, Low complexity proto-value function learning from sensory observations with incremental slow feature analysis, in: Proc. 22nd International Conference on Artificial Neural Networks, ICANN, Springer, Lausanne, 2012, pp. 279–287.[69] J. Schmidhuber, Learning to generate sub-goals for action sequences, in: T. Kohonen, K. Mäkisara, O. Simula, J. Kangas (Eds.), Artificial Neural Networks, Elsevier Science Publishers B.V., North-Holland, 1991, pp. 967–972.[70] J. Schmidhuber, R. Wahnsiedler, Planning simple trajectories using neural subgoal generators, in: J.A. Meyer, H.L. Roitblat, S.W. Wilson (Eds.), Proc. of the 2nd International Conference on Simulation of Adaptive Behavior, MIT Press, 1992, pp. 196–202.[71] M. Wiering, J. Schmidhuber, HQ-learning, Adapt. Behav. 6 (2) (1998) 219–246.[72] V.R. Kompella, M.F. Stollenga, M. Luciw, J. Schmidhuber, Explore to see, learn to perceive, get the actions for free: skillability, in: International Joint Conference on Neural Networks, IJCNN, IEEE, 2014, pp. 2705–2712.[73] V.R. Kompella, Slowness learning for curiosity-driven agents, PhD thesis, Informatics Department, Università della Svizzera Italiana, 2014.[74] A. Saltelli, K. Chan, E.M. Scott, et al., Sensitivity Analysis, vol. 134, Wiley, New York, 2000.[75] I.D. Guedalia, M. London, M. Werman, An on-line agglomerative clustering method for nonstationary data, Neural Comput. 11 (2) (1999) 521–540.[76] D. Zhang, D. Zhang, S. Chen, K. Tan, K. Tan, Improving the robustness of online agglomerative clustering method based on kernel-induce distance measures, Neural Process. Lett. 21 (1) (2005) 45–51.[77] E.W. Forgy, Cluster analysis of multivariate data: efficiency versus interpretability of classifications, Biometrics 21 (1965) 768–769.[78] Y. Zhang, H. Wu, L. Cheng, Some new deformation formulas about variance and covariance, in: Proceedings of International Conference on Modelling, Identification & Control, ICMIC, IEEE, 2012, pp. 987–992.[79] D. Wierstra, T. Schaul, J. Peters, J. Schmidhuber, Natural evolution strategies, in: IEEE World Congress on Computational Intelligence, IEEE, 2008, pp. 3381–3387.[80] J. Weng, Y. Zhang, W. Hwang, Candid covariance-free incremental principal component analysis, IEEE Trans. Pattern Anal. Mach. Intell. 25 (8) (2003) 1034–1040.[81] D. Peng, Z. Yi, W. Luo, Convergence analysis of a simple minor component analysis algorithm, Neural Netw. 20 (7) (2007) 842–850.[82] M. Luciw, V.R. Kompella, J. Schmidhuber, Hierarchical incremental slow feature analysis, in: Workshop on Deep Hierarchies in Vision, Vienna, 2012.[83] S. Höfer, M. Spranger, M. Hild, Posture recognition based on slow feature analysis, in: Language Grounding in Robots, Springer, 2012, pp. 111–130.