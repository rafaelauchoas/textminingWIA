Artificial Intelligence 172 (2008) 1917–1939Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintMechanisms for information elicitationAviv Zohar∗, Jeffrey S. RosenscheinSchool of Engineering and Computer Science, The Hebrew University of Jerusalem, Jerusalem, Israela r t i c l ei n f oa b s t r a c tArticle history:Received 22 March 2007Received in revised form 11 July 2008Accepted 10 August 2008Available online 23 August 2008Keywords:Information elicitationMechanism designInformation tradeWe study the computational aspects of information elicitation mechanisms in which aprincipal attempts to elicit the private information of other agents using a carefullyselected payment scheme based on proper scoring rules. Scoring rules, like many othermechanisms set in a probabilistic environment, assume that all participating agents sharesome common belief about the underlying probability of events. In real-life situationshowever, the underlying distributions are not known precisely, and small differences inbeliefs of agents about these distributions may alter their behavior under the prescribedmechanism.We examine two related models for the problem. The first model assumes that agentshave a similar notion of the probabilities of events, and we show that this approach leadsto efficient design algorithms that produce mechanisms which are robust to small changesin the beliefs of agents.In the second model we provide the designer with a more precise and discrete set ofalternative beliefs that the seller of information may hold. We show that constructionof an optimal mechanism in that case is a computationally hard problem, which is evenhard to approximate up to any constant. For this model, we provide two very differentexponential-time algorithms for the design problem that have different asymptotic runningtimes. Each algorithm has a different set of cases for which it is most suitable. Finally, weexamine elicitation mechanisms that elicit the confidence rating of the seller regarding itsinformation.© 2008 Elsevier B.V. All rights reserved.1. IntroductionThe old aphorism “Knowledge is power”, stated by Sir Francis Bacon some four centuries ago, is more relevant nowthan ever. The need to make informed choices causes correct and accurate information to be a desired and highly-valuedcommodity. As intelligent automated agents take on more tasks, and need to act independently within large systems, theirneed to buy and sell information increases.Information in stochastic environments is hard to evaluate, and may be easily faked. Any novice can give a predictionregarding the behavior of tomorrow’s stock market; by pure chance, those predictions may outperform those of even themost informed financial wizard.The question that naturally arises is how to pay for information that can only be verified with some probability. Thisis especially important in cases where in order to obtain the information, the seller itself has to invest some effort. Thepayments made by the buyer must be carefully set so as to induce the seller to invest the effort into acquiring the true* Corresponding author.E-mail addresses: avivz@cs.huji.ac.il (A. Zohar), jeff@cs.huji.ac.il (J.S. Rosenschein).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.08.0051918A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–1939information. Otherwise, the seller might be tempted to avoid the cost of obtaining the information, and simply make some-thing up.Most current real-world information trading is done with reliable sources of information over an extended period oftime (for example, buying the same newspaper every day). This repeated form of interaction helps motivate the providerof information to supply accurate and reliable reports (not unlike the “shadow of the future” motivating cooperation in theiterated Prisoner’s Dilemma [1]). The potential for additional interactions in the future makes the information provider’sreputation valuable, and motivates the seller to provide accurate pieces of information.However, advances in technology and infrastructure such as the internet have made a multitude of information sourcesreadily available at a moment’s notice (via web services [2], for example). These tend to be smaller and much more spe-cialized information providers, which can accurately report about a small niche in which they specialize. Interactions withthese sources are often not repeated. Since there is no central authority that governs these sources, and no single authoritycan vouch for the reliability of the information they provide, it is left up to the buyer of information to sift through theinformation that is available and decide what to use.1One approach to the problem of source reliability is the use of reputation systems [3]. These systems are mechanismsthrough which agents provide feedback about the quality of service they received from a specific vendor; this feedbackis later viewed by other potential clients. Unfortunately, solid non-manipulable reputation systems are hard to create, andmost service providers on the internet are not currently rated by any such system.We are therefore interested in other ways of obtaining correct information from a previously unknown informationsource. We will assume that there is no repeated interaction, and the incentive for providing good service must exist withinevery transaction, on its own. The overall approach we take in this work is that of mechanism design. We shall attempt tocreate the incentives for delivering accurate reports by providing payments to the agents in a way that will guarantee thema higher payment when they are behaving well, i.e., when they provide correct information.We shall assume that agents are acting rationally and that they are not intentionally trying to sabotage the buyer—any use the buyer may make of the purchased information does not affect the seller. Instead, we adopt the assumptionthat information providers are only interested in receiving a higher payment and doing the least amount of work. A trulymalicious agent that is trying to intentionally deceive, regardless of monetary loss, will not give good information regardlessof the mechanism applied, and must therefore be dealt with in other ways. Such agents are often handled using securityand encryption tools that we shall not discuss here.1.1. An example scenario for information elicitationThere are many possible scenarios for information exchange, such as reviewing papers, obtaining predictions about thestock market, buying weather information, and so on. We present here one example to which we will refer throughout thepaper.Let us assume that Bob owns a car, and wants to decide if he should upgrade his emergency road service coverage.For this purpose, he wants to evaluate the mechanical condition of the car; this will help him predict the car’s chances ofbreaking down in the near future, and will help him decide whether the extra insurance is worthwhile. Since Bob knowsvery little about cars, he turns to an independent expert, a mechanic named Alice, and asks her to take a look under thehood.Knowing that Bob is not an expert, Alice can decide not to invest any effort in checking the car, and instead make upsome list of malfunctions that threaten to disable the car at any moment, or alternatively she may just say that the car isfine (she has no vested interest in whether Bob upgrades his coverage). How will Bob know that he was told the truth?Even if Alice invests effort in checking the car and says that the car is fine, an accidental malfunction could disable it thenext day (probably making Bob feel cheated).To ensure trust, Alice can make her wages conditioned on the future: if Alice says the car is in poor shape, Bob will geta refund if his car does not break down within the next six months, while if Alice reports that the car is fine, Bob gets arefund if the car does break down within six months. What are the exact payments that will ensure that Alice does herjob? There is naturally some probability that Alice will have to refund some of Bob’s money even if she checked the car andreported the truth to Bob.There might also be a situation in which Alice knowingly lies to Bob. If the chances that a car in good condition willbreak down are too high, Alice could decide to say the car is in bad condition, and thus ensure that she does not refundBob if his car breaks down (even though it was indeed in good condition).1.2. Information elicitation vs. preference elicitationMechanism design [4,5] is the study of how to set the rules and protocols of interaction among agents in a way that willencourage rational agents to behave in a prescribed way that leads to a desired outcome. The mechanism design literature1 As an example, consider querying some foreign weather service before traveling abroad. One will only know if the weather prediction they supplied isgood after arriving at the destination. One may not be likely to require the services of that supplier again.A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–19391919provides many successful examples of mechanisms that “battle” the agent’s self-interest and successfully achieve outcomesthat are more socially oriented, or are beneficial to the designing agent in some way.Many times, in order to decide on an outcome, a mechanism tries to elicit the preferences of participating agents. Infor-mation elicitation scenarios are slightly different from preference elicitation as it is usually understood in the mechanismdesign literature. In preference elicitation scenarios, information revelation is most often used as a means to an end (i.e., toarrive at some desirable outcome). For example, an auctioneer may want to know the valuations potential buyers have foran expensive painting so that he can award this painting to the bidder that values it highest, and in the process make moremoney.In pure information elicitation, the information being revealed is the point of the transaction. The seller is assumed toonly be concerned with its payment, not any other consequence of providing one piece of information or another. In thissense, information elicitation can be seen as a subproblem of mechanism design, where the mechanism has no outcome todetermine.2 This limitation leaves the mechanism with fewer degrees of freedom.Since information elicitation scenarios are all about trading information, it may be important to the participants not togive out any information for free, and to keep all their extra knowledge about the world secret. Later in this article we shallexamine scenarios where the seller and buyer of information possess different beliefs about the world. In classic mechanismdesign, this problem is often addressed by direct revelation mechanisms that require agents to divulge all needed information,including their probability beliefs (i.e., type). The mechanism then takes this information into account and acts optimallyon behalf of the agent, eliminating any need to be untruthful. However, in settings where information is sold, it is unlikelythat the seller would be willing to participate in direct revelation schemes. Since information is the primary commodity,revealing more of it to the mechanism is unwise,3 and the agent’s beliefs about probabilities contain extra information.1.3. ContributionAs computers take on more tasks that require intelligent decisions and reliable information, and as micro-transactionsof information begin to play a larger role in information trade, establishing the proper incentives for truthfulness becomesincreasingly important. We present here a model for one-shot transactions of information that can incorporate these incen-tives, and show how to extend it in four ways:(1) We show that some mechanisms are more robust to varying beliefs of agents than other mechanisms. The notion ofbelief robustness that we define is applicable to many real-world situations where there is no common knowledgebetween the seller and buyer of information, but effective mechanisms can still be constructed. We present efficientalgorithms for finding such mechanisms.(2) We show that if the selling agent has additional knowledge about the state of the world that it is not willing to sell,the design of an optimal mechanism becomes computationally hard. We present two exponential-time algorithms forthe design problem that exploit different aspects in the structure of the problem, and achieve different running timeprofiles.(3) We look at the case where the expert selling information has some uncertainty regarding its quality, and show how theconfidence rating of the seller can sometimes be elicited along with the information itself.1.4. Structure of the paperIn the next section we review related work and give a brief overview of some mathematical and computational back-ground used in the rest of the paper. In Section 3 we define the basic information elicitation model and explore its basicproperties in the case of one seller. Section 4 then explores a model where agents do not hold a common view of the worldand need to design mechanisms that are robust against small differences in beliefs. Next, we turn to a scenario where theseller possesses more knowledge about the probabilities than the buyer does, and show that designing good mechanisms inthis case is often hard. We give two different algorithms to design such mechanisms that have different running times. InSection 6 we discuss elicitation of confidence ratings in scenarios where the seller has some uncertainty about the qualityof its information. We present our conclusions in Section 7.2. Background2.1. Related workThe economic theory of contracts has dealt with principal-agent models in various forms. Some of the canonical familiesof models in this field include those with adverse selection—where an agent is asked to reveal private information about his2 This is similar to the classification of elections as mechanisms where no money changes hands, and only an outcome is selected.3 It remains unwise even if the mechanism is handled by a trusted third party, since revealing extra information would be reflected in payments madeby the buyer.1920A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–1939type but may give false information. The proper incentives for truth-telling are set by a well-designed contract. Anotherwell-known family of models thoroughly explored in economics is that of moral hazard. In these settings, the agents areasked to take a hidden action (one that is costly for them) that is not directly visible to the principal. Instead, the principalobserves some noisy signal that is affected by the action. The aim of the contract is then to make sure that the agents gainmore by performing the action. The reader is urged to refer to [6,7] for a comprehensive introduction to these models andseveral of their variants. Our own work includes elements from both families—hidden action to learn the information thebuyer is interested in acquiring, and truthfulness in revealing it. However, we no longer assume that probabilities of eventsare common knowledge, but instead treat them as beliefs held by the agents. This leads among other things to interestingcomputational questions that are not often explored within the economic context.Research in artificial intelligence and on the foundations of probability theory has considered probabilities as beliefs,4 andseveral models have been suggested—for example, probabilities over probabilities [8]. Cases where agents have uncertaintyabout the utility functions in the world were examined in [9]. There, an agent acts according to the “expected expectedutility” it foresees as it takes into consideration its own uncertainty. The truthful elicitation of such beliefs has also attractedgreat interest [10–12] (see Section 2.2 on scoring rules).The issue of common knowledge and common priors has been studied within the context of probability theory [13,14].Here, the beliefs about beliefs of agents also play a large role.There are many natural uses for information elicitation in computer science. For example, in reputation systems [15,16]information is elicited from agents about their experience with some service provider. This information is important foragents that will interact with that service provider in the future, but the reporting agent that has already completed theinteraction needs to be motivated in some other way to reveal the results of its own interaction.In multi-party computation settings, information is elicited in order to compute some function of the agents’ secrets.Agents are interested in the correct result of the computation but do not wish to reveal their secret [17], and use crypto-graphic tools to conceal it. Multi-party computation scenarios where agents have to invest effort to discover their secretshave also been explored [18,19]. In our work we do not assume that agents have reservations about revealing their se-cret, only that they wish to maximize their gains. We also do not make the assumption that the information providers areinterested in the product that will later be generated using their information.Yet another area in which information elicitation is implemented is polling. The information market [20,21] approachhas been suggested as a way to generate more reliable predictions than can be achieved with regular polls. There, agentsbuy and sell options that will pay them an amount that is dependent on the outcome of some event (like some specificcandidate winning an election).A somewhat different sub-field of information elicitation deals with eliciting information from humans [22,23]. Thechallenges here are to model as accurately as possible the desires of people (as utility functions, for example) and toovercome some of the irrationality that affects human behavior and reporting. The reports that these schemes often relyupon can be noisy and even conflicting.An economic analysis of information as a trade commodity within large markets has also been performed. Broker Agentsthat buy information, filter it, and then sell the results have been examined in [24]. [25] explores the effects of bundlinginformation goods together.Another example of the treatment of information elicitation appears in [26], where a manager in some firm attemptsto obtain information from an employee in a setting where obtaining this information is costly. The manager attempts tocreate the incentive for truthful revelation by comparing it with his own information. The main result in [26] shows that ifthe worker has some signal on the information of the manager, it may become a “yes-man” that attempts to correlate theinformation it reports with the information of the manager, instead of reporting truthfully.The automatic design of general mechanisms has been researched as well. [27,28] proposed applying automated mecha-nism design to specific scenarios as a way of tailoring the mechanism to the exact problem at hand, and thereby developingsuperior mechanisms. Here we propose to do similar things with information elicitation mechanisms.In Section 5 we present mechanisms that use partial revelation of information. This agenda has also been pursuedwithin preference elicitation settings [29,30]. There, mechanisms are designed to approximately implement truth-telling indominant strategies using constrained optimization techniques that are similar to our own.2.2. Strictly proper scoring rulesScoring rules [10] are used in order to assess and reward a prediction given in probabilistic form. A score is given to thepredicting expert that depends on the probability distribution the expert specifies, and on the actual event that is ultimatelyobserved. For a set Ω of possible events and P , a class of probability measures over them, a scoring rule is then defined asa function of the form: S : P × Ω → R.A scoring rule is called strictly proper if the predictor maximizes its expected score by saying the true probability ofthe event, and receives a strictly lower score for any other prediction. That is, when the actual event ω is drawn from the4 This has led to controversy between Bayesians and Frequentists.A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–19391921probability distribution p (which we denote by ω ∼ p) the expected score of the predictor is higher if it reports p ratherthan any other distribution q:(cid:2)(cid:2)(cid:3)S(p, ω)(cid:2) Eω∼p(cid:3)S(q, ω)Eω∼pIn the above, equality is achieved iff p = q. [12] show a necessary and sufficient condition for a scoring rule to be strictlyproper (see a generalized version in [11]), which allows easy generation of various proper scoring rules by selecting abounded convex function over P . Each such function generates a new scoring rule.Several commonly known scoring rules are:• The spherical scoring rule:S(p, ω) =(cid:4)(cid:5)pωω(cid:4)∈Ω pω(cid:4) 2• The logarithmic scoring rule:S(p, ω) = log(pω)• And the quadratic scoring rule:S(p, ω) = 2pω −(cid:6)ω(cid:4)∈Ωp2ω(cid:4)(1)(2)(3)(4)An interesting use of scoring rules within the context of a multi-agent reputation system was suggested by [15], whohave modeled the bad behavior of service providers by a random variable that, with some fixed probability p, determineswhether they will be honest or dishonest in their next transaction. A series of agents interact with this service provider;each is required to give feedback, which is interpreted as giving some refined prediction for the value of p. An agentinvolved in giving feedback is then rewarded with a scoring rule according to how well it predicted the feedback signal ofthe next agent that interacts with the service provider. This mechanism makes true revelation of the experience with theservice provider a Nash equilibrium. Unavoidably, the mechanism also has other Nash equilibria that may attract agents.This may be corrected by relying on some reliable feedback from other sources as well [31].2.3. Stochastic programmingStochastic programming [32] is a branch of mathematical programming where the mathematical program’s constraintsand target function are not precisely known. A typical stochastic program formulation consists of a set of parameterizedconstraints over variables, and a target function to optimize. The program is then considered in two phases. The first phaseinvolves the determination of the program’s variables, and in the second phase, the parameters to the problem are randomlyselected from the allowed set. The variables set in the first stage are then considered within the resulting instantiation ofthe problem. Therefore they must be set in a way that will be good for all (or most) possible problem instances. Thereare naturally several possible ways to define what constitutes a good solution to the problem. In this work, we use theconservative formulation of [33] which requires the assignment of variables to satisfy the constraints of the program forevery possible program instance. For example, if we are given a program of the form:min c · xs.t. Ax (cid:2) bwhere A is considered to be from an allowed set of parameters A, we shall require a solution x to the mathematicalprogram to be feasible for all possible A ∈ A.This type of linear stochastic program has a convex solution space (it is the intersection of a convex space for everypossible A ∈ A). General convex optimization algorithms require a description of the solution space, e.g., via a separationoracle. A separation oracle is simply a program that is able to tell if a certain point x is in the solution space, and if it is not,can provide a linear separator between the set of allowed solutions and x. If an efficient separation oracle exists, the convexoptimization problem can be solved efficiently as well. An efficient oracle can be constructed for the stochastic optimizationproblem above using a linear program solver (see [33] for more details), and thus it is efficiently solvable.We shall make use of this formulation later in Section 4. Each instance will correspond to a different variation in thebeliefs held by the participating agents.3. The information elicitation scenarioThe scoring rule literature usually deals with the case in which the predicting expert is allowed to give a prediction froma continuous range of probabilities. We look at a different problem: we assume each agent (including the principal, i.e., the1922A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–1939Fig. 1. The information transaction.one trying to elicit the information) has access to a privately-owned random variable that takes a finite number of valuesonly. The discrete values allow us to tailor the mechanism to the exact scenario at hand without the need to differentiatebetween infinitesimally differing cases. Bartered knowledge is very often presented in a discrete format.5 Finally, aggregatinginformation from several agents is also much clearer and simpler to do with discrete variables.We assume the buyer wishes to purchase information about the value of a discrete random variable Xi from each seller i,and that the seller can learn the value of that variable at a cost ci . To verify the quality of the information it purchases, thebuyer has access to a random variable Ω . Ω, X1, . . . , Xn are presumably not independent variables, and knowledge aboutthe value of one of them gives some information regarding the value of the others. Using the variable Ω , the buyer can getsome idea if the information sold to him was correct. Without Ω , it would sometimes be impossible to create the necessaryincentives for truthfulness on the part of the sellers. The variable may be redundant in the case of multiple sellers whereinformation from several sources can be compared for validation.We shall denote the probability distribution for Ω, X1, . . . , Xn by pω,x1,...,xn= Pr(Ω = ω, X1 = x1, . . . , Xn = xn). The valuesthe different variables can take, as well as the probability distribution pω,x1,...,xn , and the costs ci are assumed to be commonknowledge. We also assume that agents seek to maximize their expected gains and that they are risk-neutral.In our running example, the variable X that Bob wishes to purchase is the mechanical state of the car, the verificationvariable Ω is the indicator that denotes if Bob’s car breaks down within six months of being evaluated, and the probabilitydistribution P ω,x links these two events. A car in bad condition has a higher chance of breaking down. The cost c representsthe amount of work Alice has to invest to examine Bob’s car.The buyer can now design a payment scheme that will determine the payment it must give to the sellers, based on theinformation the sellers gave and on the value of the verification variable Ω . We shall denote the payment to agent i byui. See Fig. 1 for an illustration of the interaction between the agents.ω,x1,...,xnIn our example, this payment scheme captures the different payments upon which Bob and Alice agree. If Alice says thecar is fine, Bob will pay her more if the car does not break down (and may even claim money if it does), while a differentset of payments will apply if Alice says the car is in poor condition.It is important to stress that the variable Ω must be hidden from the sellers at the time the transaction is carried out.If the sellers possess too much information regarding Ω (in addition to what is implied through the information they sell),they may choose to report a value that best fits the buyer’s signal instead of the real value they learned.In this paper we primarily examine the restricted case of a single seller. The approach we take can be easily extendedto multiple sellers.6 A payment scheme shall be considered proper if it creates the incentive for agents to enter the game,invest the effort into acquiring their variable, and tell the true value that they found. These three requirements are definedmore precisely below.3.1. The requirements from the mechanism in the single agent caseIn the case of one participating agent with a single variable, we need to satisfy three types of constraints in order tohave a proper mechanism. For convenience, we drop the index i of the agent and denote by pω,x the probability Pr(Ω = ω,X = x).5 For example, in the original example from Section 1.1 above, Bob could be interested in knowing the condition of his car but would not really carefor a continuous range of values. The required information in this case might be given just to make a discrete choice of whether or not to purchase moreinsurance. Continuous data can sometimes be made discrete according to the various actions it implies: if the car is in a condition that is worse than somethreshold, Bob would purchase insurance, otherwise he would not. This defines the discrete information in which he is interested.6 With multiple sellers the mechanism designer has to make a decision regarding the exact solution concept the mechanism will use. A wide range isavailable, for example, a dominant strategy implementation, an iterated dominance implementation, or a Nash equilibrium implementation. Each choiceproduces different constraints, but all are similar in spirit to the formulation we present for the single agent. Even more complex mechanisms can bedesigned to resist various forms of collusion among multiple sellers.A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–19391923(1) Truth telling. Once an agent knows its variable is x, it must have an incentive to tell the true value to the principal,(cid:4)rather than any lie x.(cid:4)∀x, xs.t.(cid:4)x (cid:7)= x(cid:6)ωpω,x · (uω,x − uω,x(cid:4) ) > 0(5)Remember that pω,x is the probability of what actually occurs, and that the payment uω,x(cid:4) is based only on what theagent reported.In our example this is the requirement that Alice tells Bob the truth about the state of the car in the event that sheknows it.(2) Individual rationality. An agent must have a positive expected utility from participating in the game:pω,x · uω,x > c(6)(cid:6)ω,xThis assures us that Alice will want to do business with Bob. If she does not stand to gain from the transaction (evenin expectation), she will prefer not to deal with Bob at all.(3) Investment. The value of information for the agent must be greater than the cost of acquiring it. Any guess xthe agentmakes without actually computing its value must be less profitable (in expectation) than paying to discover the truevalue of the variable and revealing it:(cid:4)(cid:6)(cid:4)∀x(cid:6)pω,x · uω,x − c >pω,x · uω,x(cid:4)(7)ω,xω,xThis constraint will ensure that Alice will be better off making an informed judgment regarding Bob’s car rather thanjust guessing its mechanical condition.Note that all of the above constraints are linear, and can thus be applied within a linear program to minimize, forexample, the expected cost of the mechanism to the principal:(cid:5)ω,x pω,x · uω,x.Let us demonstrate a proper mechanism using a numeric example:Example 1. Let us assume that Bob’s car can be in only one of two states: good working condition or poor working condition,X = {good, poor}, and that he wishes to find out which state it is in. He then turns to Alice who can invest an effort thatis equal to $10 to find this out. Bobs wants Alice to tell the truth so he conditions payments on the event in which the carbreaks down (or does not) in the next 6 months: Ω = {break down, ok}.The probability distribution, which is assumed to be common knowledge, is:Car conditionBreak downProbabilitygoodgoodpoorpoornoyesnoyes0.40.10.30.2Note that this distribution implies that the car has an equal probability of being in good condition or of being in poorcondition, and that without knowing the condition of the car, it has a 0.15 probability of breaking down. Now we assumethat Alice and Bob decided on the following payment scheme:Reported conditionBreak downPaymentgoodgoodpoorpoornoyesnoyes$15$0$0$25Now, let us check that each one of the constraints is satisfied:(1) Truth telling. Let us assume that Alice knows the car is in good shape. The car therefore has a 20% chance of breakingdown. If Alice reveals the truth to Bob, she will get paid $15 with probability 0.8 (the case where the car does notbreak down—an expected value of $12). Otherwise, she may lie and report the car is in poor shape, and get paid only ifit breaks down; that means getting $25 with probability 0.2, which is worse ($5 in expectation). So in this case, Alicewould be better off telling the truth.The reader may verify that Alice will also want to tell the truth if she knows the car is in poor condition (a 40% chanceof breaking down). In this case, telling the truth gives her $25 with probability 0.4 (or $10 in expectation) and lyingwill give her $15 with probability 0.6 ($9 in expectation), so she will tell the truth.1924A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–1939(2) Individual rationality. If Alice checks the car and tells the truth, she is expected to get $9 if the car is in poor shapeand $12 if it is in good shape. Since both events have equal probability, she stands to gain $10.5 in expectation, whichis higher than the effort she needs to invest in checking the car.(3) Investment. If Alice decides not to invest effort, then she can estimate the car’s probability of breakdown at 0.15. If shetells Bob the car is in good shape, she stands to make $15 with 85% probability, which is more than her expected valueif she tells the truth (also because she has to invest $10 worth of effort in that case). She may therefore decide not tocheck the condition of the car, and just tell Bob that it is fine.Due to the last constraint being violated, we see that this payment scheme will probably not elicit the truth from Alice.However, as we shall later prove, a proper mechanism does exist.3.2. A geometric interpretation for the truth-telling constraintsIn Section 3.3 we shall see that if the truth-telling constraints are satisfiable, the payments can be adjusted easily tosatisfy the rest of the constraints as well. We are therefore interested in better understanding these constraints.A close look at the truth-telling constraints for some x and x,(cid:4)(cid:6)pω,x · (uω,x − uω,x(cid:4) ) > 0ωreveals that they seem similar to vector multiplication. In fact, if we define vectors(cid:8)px (cid:3) (pω1,x . . . pωk,x)(cid:8)ux (cid:3) (uω1,x . . . uωk,x)we can write the truth-telling constraints in the following form:(cid:4)∀x (cid:7)= x(cid:8)px · ((cid:8)ux − (cid:8)ux(cid:4) ) > 0.Using a slightly different notation we can define:(cid:8)v x,x(cid:4) (cid:3) (cid:8)ux − (cid:8)ux(cid:4) ,(cid:4)∀x (cid:7)= xand write the constraint as:(cid:8)px · (cid:8)v x,x(cid:4) > 0.(cid:4)∀x (cid:7)= x(13)This representation has a geometric interpretation: the vector (cid:8)px is required to be on the positive side of the unbiasedhyperplane perpendicular to the vector (cid:8)v x,x(cid:4) .It is important to notice that the vectors (cid:8)v x,x(cid:4) are not independent of each other, but have the following relationships:(cid:8)v x,x(cid:4) = −(cid:8)v x(cid:4),x(cid:8)v x,x(cid:4)(cid:4) = (cid:8)v x,x(cid:4) + (cid:8)v x(cid:4),x(cid:4)(cid:4)(15)We therefore have a matching requirement to 13 that places the vector (cid:8)px(cid:4) on the negative side of the hyperplane (cid:8)v x,x(cid:4) :(16)A proper assignment of payments is required to give a linear separation between the vectors (cid:8)px and (cid:8)px(cid:4) using the(cid:8)px(cid:4) · (cid:8)v x,x(cid:4) < 0(cid:4)∀x (cid:7)= xhyperplane defined by (cid:8)v x,x(cid:4) (see Fig. 2). This requirement for linear separation is the basis for many of our results.(8)(9)(10)(11)(12)(14)Fig. 2. A linear separation of vectors (cid:8)px and (cid:8)px(cid:4) .A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–193919253.3. Existence and properties of solutions for a single agentThere are naturally cases when it is impossible to satisfy the constraints. For example, if Bob’s car is just as likely tobreak down no matter what condition it is in, we cannot expect that Bob will be able to create the incentive for Alice totell the truth about her examination of the car. Bob will not be able to tell (even probabilistically) if she told the truth ornot, and no mechanism will help. When can we be sure that a mechanism will exist?The following proposition gives a sufficient condition for the existence of a mechanism in the single agent case:Proposition 1. If there exist x, x(cid:4)for x and xat the same time.(cid:4) ∈ X and α (cid:2) 0 s.t. x (cid:7)= x(cid:4) ∀ω pω,x = α · pω,x(cid:4) , then there is no way to satisfy truth-telling constraintsProof. When looking at the two truth-telling constraints for x, x(cid:4)we get (according to Eq. (5) and Eq. (16)):pω,x · (uω,x − uω,x(cid:4) ) < 0(17)(cid:6)0 <ωwhich is a contradiction. (cid:2)We can regard this feasibility condition as a requirement of independence between the vectors (cid:8)px (cid:3) (pω1,x . . . pωk,x) of(cid:4)any two different x, x. If the vectors are dependent, they cannot be linearly separable as required by the constraints. Weshall later see that a high similarity between these vectors which makes them harder to separate, while still allowing for aworking mechanism, actually limits its robustness.Next, we show that if the condition described in Proposition 1 does not hold, we can always construct a proper paymentscheme. Moreover, once we have some working payment scheme, we can easily turn it into an optimal one for the principalwith a cost of c.(cid:4)there is no λ such that (cid:8)px = λ · (cid:8)px(cid:4) , then there is aProposition 2. If the probability vectors (cid:8)px are pairwise independent, i.e., ∀x, xproper payment scheme with a mean cost as close to c as desired. This solution is optimal, due to the individual rationality constraint.Intuitively, this means that if the state of the car influences the chances of it breaking down even to a very small degree,then Bob can find a payment scheme that will properly motivate Alice to tell the truth. The proof idea is that once thetruth-telling constraints are satisfied (using a regular scoring rule), the other constraints can also be satisfied by scaling thepayments, and adding a constant to them.Proof. We can easily build an optimal solution by using a strictly proper scoring rule(cid:7)(cid:8)uω,x = α · SPr(ω | x), ω+ βω(18)for some positive α, and some value βω. Since the independence relation holds for every pair x, xare distinct and the scoring rule assures us (Eq. (1)) of the incentive for truth-telling regardless of the values of α, βω., the probabilities Pr(ω | x)(cid:4)To satisfy the investment constraint, one can scale the payments until the value of information for the agent justifies theinvestment. Setting(cid:9)α > maxx(cid:4)(cid:5)cω,x pω,x(S(Pr(ω | x), ω) − S(Pr(ω | x(cid:4)), ω))(cid:10)(cid:4)satisfies that constraint for every x. This is also shown in [15].(19)Finally, we can use the βω values to satisfy the remaining individual rationality constraint tightly by shifting the pay-ments until their average is just above c:(cid:6)βω = β > c − α(cid:7)pω,x · SPr(ω | x), ω(cid:8)(cid:2)(20)ω,xWe have thus shown a payment scheme with the minimal cost for every elicitation problem where different observationsof X entail different probability distributions of ω. Notice that we are able to achieve the optimal cost of c by allowingnegative payments to the seller as well (penalties). If we allow only positive payments, the cost will be higher.3.3.1. Bad verifiersWe have seen that if the information being sold has no bearing on the distributions of the probabilistic verifier Ω ,no payment scheme can possibly create the incentives we require. But what if Ω provides only a slight indication of thecorrectness of the information?1926A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–1939In our example, this would be a scenario where Bob’s car is only slightly more likely to break down if it is in poorcondition. How does that affect the mechanism that is to be constructed? We will show below that Bob will need a largedifference between payments made to Alice, that will in fact increase the risk involved for both of them.We are given a hint of this by the construction of the mechanism above. In order to satisfy the investment constraints,we needed to scale the payments and thus increase the risk level of the mechanism. This becomes more severe if theverifier variable is poorly correlated with the purchased information. It seems that when Ω is a weak verifier, the differencebetween payments must increase. This increase in the risk of payments causes the value of information for the seller toincrease as well—up to a level in which it is worthwhile to make the effort and obtain the true information. We demonstratethis fact here for the case of | X| = 2.Example 2 (Bad verifiers result in high risk mechanisms). Let us assume that Ω is indeed a poor verifier. The probabilitiesPr(Ω | x1) and Pr(Ω | x2) must be very similar. Let us denote:(cid:8)px1 = (cid:8)q + (cid:8)(cid:7);(cid:8)px2 = (cid:8)q − (cid:8)(cid:7)where (cid:8)(cid:7) is a very small vector. The investment constraints for this case are therefore:((cid:8)q + (cid:8)(cid:7)) · (cid:8)ux1 + ((cid:8)q − (cid:8)(cid:7)) · (cid:8)ux2 > ((cid:8)q + (cid:8)(cid:7)) · (cid:8)ux1 + ((cid:8)q − (cid:8)(cid:7)) · (cid:8)ux1 + c((cid:8)q + (cid:8)(cid:7)) · (cid:8)ux1 + ((cid:8)q − (cid:8)(cid:7)) · (cid:8)ux2 > ((cid:8)q + (cid:8)(cid:7)) · (cid:8)ux2 + ((cid:8)q − (cid:8)(cid:7)) · (cid:8)ux2 + cCombining them gives us:2(cid:8)(cid:7) · (cid:8)ux1 > 2(cid:8)(cid:7) · (cid:8)ux2 + 2cwhich simplifies to:(cid:9)(cid:8)ux1 − (cid:8)ux2(cid:9) · (cid:9)(cid:8)(cid:7)(cid:9) (cid:2) ((cid:8)ux1 − (cid:8)ux2) · (cid:8)(cid:7) > c(21)(22)(23)(24)(25)From this last inequality we see that as the norm of (cid:8)(cid:7) goes to 0, the difference between the payment vectors ((cid:8)ux1 − (cid:8)ux2)goes to infinity—which indicates a high level of variation in payments dictated by the mechanism. If we add the restrictionof paying only positive payments, this implies that the expected cost of the mechanism goes to infinity as well.4. Belief-robust mechanismsIn the previous section, we saw that it is easy to design information elicitation mechanisms in the single agent case.However, we assumed that the mechanism designer has precise knowledge about the probability distribution pω,x, and thatthe seller of information is using the exact same distribution while it is contemplating which action to take. This is generallyan assumption that is unlikely to hold.In our running example, the mechanic Alice may have more expertise and may assign a probability to the event in whichBob’s car breaks down that is different than Bob’s uninformed assessment. Can Bob still assign payments that will properlymotivate Alice? What will be the cost of this missing knowledge?In many real-world scenarios, probabilities are often assessed through modeling or sampling (Alice can know the chanceof a car breaking down more accurately because she has encountered more cars, or has a better understanding of why andwhen they break down), and two agents may have two different notions of the probabilities of certain events. This couldhave serious effects on the reliability of mechanisms designed for real systems.We shall therefore try to relax the assumption of a commonly known probability distribution, which we have used sofar. We will instead assume that agents have “close” notions of the governing probability distributions. This assumptionis reasonable, for example, in cases where distributions are learned by sampling and past experience. If some event hasprobability p of occurring, two agents sampling independently will not disagree greatly about that probability.We denote the beliefs of the mechanism designer by ˆp and the beliefs of a participating agent by p = ˆp + (cid:7), where(cid:7) is small according to some norm. We have opted for the L∞ norm in this work, because it is easily described usinglinear constraints (it simply takes the maximum over all coordinates). Other norms may also be used, and will yield convexoptimization problems that are not linear.Next, we define the notion of belief robustness of the mechanism and through it examine the design of mechanisms thatare still expected to work even if there is some difference between the beliefs of agents. We argue that not all paymentschemes are equal—some may be more robust to changes in beliefs than others and should therefore be the preferred choicefor use in real-world domains.4.1. The robustness level of a payment schemeFig. 3 presents a case in which the probabilities the seller believes in are not exactly known and may be within a certain, are not the same. The schemeregion around what the buyer believes. The two payment schemes portrayed, vand v(cid:4)(cid:4)(cid:4)A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–19391927Fig. 3. An elicitation problem with uncertain probabilities, and two payment schemes with different robustness levels.(cid:4)denoted by vto do so in some cases. We shall therefore want to think of vperturbation is severe and vthat has different beliefs and may conclude that lying is beneficial.ensures that the probability vectors will be linearly separated (as is required by Eq. (13)), while v(cid:4)may fail. Whenever thedoes not linearly separate the vectors, the buyer of information will be lied to by the selleras a more robust payment scheme than v(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)(cid:4)Definition 3. We shall say that a given payment scheme uω,x is (cid:7)-robust for an elicitation problem with distribution ˆpω,x ifit is a proper payment scheme with regard to every elicitation problem with distribution ˆpω,x + (cid:7)ω,x such that (cid:9)(cid:8)(cid:7)(cid:9)∞ < (cid:7),and is not proper for at least one problem instance of any larger norm.The definition above is very conservative, and requires that the mechanism work for every possible difference in be-liefs. Another possible approach is to use an explicit probability distribution over possible continuous beliefs of the agentsinvolved and require that the mechanism work well in a large-enough portion of the cases.7Intuitively, it is appealing to attempt to find a maximal margin separation between the vectors, and use that to constructthe payment schemes in a robust way. We show in Section 4.2.1 that this is indeed related to our definition of robustness,and discuss why such an approach will not work directly.4.1.1. Determining the robustness level of a mechanismGiven an offer for a payment mechanism from Alice, Bob can check to see if it will create the incentives for Alice(according to his own beliefs). What is the level of change in beliefs that will still keep the mechanism proper?We can calculate the robustness level (cid:7) of a given mechanism by solving a linear programming problem for everyconstraint. We do this by looking for the worst-case (cid:7)ω,x, which stands for the worst possible belief that the participatingagent may hold. We are given the values of the payments and use them as parameters in the program to find a minimalperturbation of the probabilities that will violate some constraint. For example, we can write the following program to findthe worst case for one of the truth-telling constraints:min (cid:7) s.t.(cid:6)( ˆpω,x + (cid:7)ω,x)(uω,x − uω,x(cid:4) ) (cid:4) 0ω∀x, ω ˆpω,x + (cid:7)ω,x (cid:2) 0(cid:6)(cid:7)ω,x = 0ω,x∀x, ω − (cid:7) (cid:4) (cid:7)ω,x (cid:4) (cid:7)In the program above, only (cid:7) and (cid:7)ω,x are variables. The linear programs for other constraints are easily built by substi-tuting, for the first constraint above, the negation of one of the constraints in the original design problem:7 This alternative formulation can also be handled using tools similar to those we use here (i.e., stochastic programming). Later, in Section 5, we examinea situation where there is a discrete set of possible beliefs held by the seller.1928A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–1939min (cid:7) s.t.{place the negation of one of the constraints here}∀x, ω ˆpω,x + (cid:7)ω,x (cid:2) 0(cid:6)(cid:7)ω,x = 0ω,x∀x, ω −(cid:7) (cid:4) (cid:7)ω,x (cid:4) (cid:7)Once we have solved similar linear programs for all the constraints in the original design problem (a total of | X|2 + 1linear programs to solve), we take the minimal (cid:7) found for them as the level of robustness for the mechanism. The solutionalso provides us with a problem instance of distance (cid:7) for which the mechanism would fail. We solve several programs hereinstead of just one large program because when formulating the problem this way it can be solved using linear optimizers,which are often simpler than general convex optimizers.4.1.2. Finding a mechanism with a given robustness levelNow that Bob realizes that his original mechanism is not robust enough, he may try to find one that would be (if sucha mechanism exists). It is possible to search for a payment scheme with a given robustness level (cid:7) using the followingstochastic program:(cid:5)ˆpω,x · uω,xminω,x(cid:5)(cid:4)s.t. ∀x (cid:7)= x(cid:5)(cid:5)(cid:4)∀xω pω,x(uω,x − uω,x(cid:4) ) > 0ω,x pω,x · uω,x > cω,x pω,x(uω,x − uω,x(cid:4) ) > cwhere: ∀x, ω pω,x = ˆpω,x + (cid:7)ω,x(cid:5)pω,x (cid:2) 0;−(cid:7) (cid:3) (cid:7)ω,x (cid:3) (cid:7)ω,x pω,x = 1Target functionConstraintsParameter rangeIn this program, the variables are the payments uω,x, while the probabilities pω,x are parameters that are unknown butare within some limited distance from ˆpω,x. The program considers all distributions p that are close to ˆp up to (cid:7), accordingto the L∞ norm. As we have mentioned before (in Section 2.3), this problem is convex.In fact, we have already seen how to build a separation oracle for it—given a payment scheme uω,x we can check itsrobustness as shown in Section 4.1.1. This check will tell us if our payment scheme is within the allowed convex area. If it isnot, it will provide us with a perturbation (cid:7)ω,x for which the solution fails. This gives us a linear condition that all solutionsare required to uphold, but the given scheme does not (and is exactly what a separation oracle is required to provide). Thisprocedure is called constraint generation and is often used in optimization problems. More details can be found in [33].If a norm that is different than the L∞ norm is used, the parameters section in the stochastic program would be different:the perturbation (cid:8)(cid:7) would still be required to reside within a ball of some radius (only it is a ball according to some othernorm) which is also a convex shape. In this case, if we want to check if some perturbation violates any of the constraints,we will have to use a convex program solver to search this ball for such a perturbation. The only difference is that thisconvex program is no longer a linear program as we were assured when we used L∞, but it is still efficiently solvable.4.1.3. The cost of robust mechanismsWe have already seen that for the program instance for which ∀ω, x (cid:7)ω,x = 0 (which corresponds to the original, non-robust design problem), a payment scheme that costs only infinitesimally more than c always exists (if any mechanismexists). A robust payment scheme, however, is required to cope with any possible belief variation, and will cost more toimplement.Consider a mechanism with an expected cost of γ =ˆpω,x · uω,x. Since it is not possible (due to the other constraints)that all uω,x are 0, then there exists a perturbation of beliefs (cid:7)ω,x which is negative for the largest uω,x and is positive for thesmallest one, which then yields a strictly lower payment than γ according to the belief of a participating agent. Therefore,in order to satisfy the individual rationality constraint, γ must be strictly larger than c, and the buyer must pay more inexpectation.(cid:5)ω,x4.2. The robustness level of an elicitation problemBob may want to get the truth from Alice more than he cares about saving money. In cases like this, he may wantthe most robust payment scheme he can find. This in a sense is the best mechanism he can compose with his limitedinformation about the probabilities of events. Is there a limit to the robustness that can be obtained? How can it becomputed?We define the robustness level of the problem in the following manner:A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–19391929of the problem ˆp is the supremum of all robustness levels (cid:7) for which a proper mecha-Definition 1. The robustness level (cid:7)∗nism exists:(cid:11)(cid:7)∗ (cid:3) sup(cid:8)u(cid:7) | (cid:8)u is an (cid:7)-robust payment scheme for ˆp(cid:12)To find the robustness level of a problem, one can perform a binary search; the robustness level is certainly somewherebetween 0 and 1. One may test at every desired level in between to see if there exists a mechanism with some specifiedrobustness by solving the stochastic program above. The space between the upper and lower bounds is then narrowedaccording to the answer that was received.As in the non-robust case, the design of a robust single-agent mechanism relies only on the truth-telling constraints:Proposition 3. If a given solution uω,x is (cid:7)-robust with respect to the truth-telling constraints only, then it can be transformed into an(cid:7)-robust solution to the entire problem.Proof. We achieve this in a manner similar to Eqs. (19) and (20). We simply scale the solution to give robustness for theinvestment constraint, and shift it to add robustness to the incentive compatibility constraint. Since the solution is (cid:7)-robustfor the truth-telling constraints we have:∀(cid:8)(cid:7)(cid:4)s.t. (cid:9)(cid:8)(cid:7)(cid:9) < (cid:7) ∀x (cid:7)= xpω,x(uω,x − uω,x(cid:4) ) > 0(cid:6)ωIf we sum over x we get:pω,x(uω,x − uω,x(cid:4) ) > 0(cid:6)ω,x(cid:6)which implies that there exists a number δx(cid:4),(cid:8)(cid:7) such that:pω,x(uω,x − uω,x(cid:4) ) > δx(cid:4),(cid:8)(cid:7) > 0ω,xNow multiplying every uω,x by a factor α = maxx(cid:4),(cid:8)(cid:7)new payment scheme ˜u for which:(cid:6)(cid:4)∀(cid:8)(cid:7) ∀xpω,x( ˜uω,x − ˜uω,x(cid:4) ) > ccδx(cid:4) (cid:8)(cid:7)will not hurt any of the truth-telling constraints, but will yield a(29)ω,xwhich satisfies all of the investment constraints, for any possible belief change.Next, the solution can be shifted to satisfy the individual rationality constraint, without hurting the robustness withregard to the previous constraints. We can simply add a constant β to every payment:β > c − minω,x[ ˜uω,x]We will thus get a solution u∗ω,x > cand therefore satisfies∀ω, x u(cid:6)(cid:6)pω,x · u∗ω,x >pω,x · c = c∗that satisfiesω,xω,xfor all possible belief changes, meaning that u∗is (cid:7)-robust. (cid:2)4.2.1. A bound for problem-robustnessA simple bound for robustness of the problem can be derived from the requirement of eliciting the truth between justtwo possible statements the information seller may provide. This shows the relation between maximal margin separatorsand our notion of robustness. Proposition 1 for non-robust mechanisms can be viewed as a specific case of the followingproposition (when applied to 0-robust mechanisms):Proposition 4. The robustness level (cid:7)∗plane that separates it from any other vector ˆpx(cid:4) . By selecting the pair of vectors that minimizes this bound we get:of a problem ˆp can be bounded by the distance between any vector ˆpx and the optimal hyper-(cid:7)∗ (cid:4) minx,x(cid:4)(cid:7)(cid:13)(cid:13) ˆpx −(cid:8)(cid:13)(cid:13)ˆptrx· (cid:8)ϕx,x(cid:4)· (cid:8)ϕx,x(cid:4)∞(cid:8)ϕx,x(cid:4) =ˆpx + ˆpx(cid:4)(cid:9) ˆpx + ˆpx(cid:4) (cid:9)2The optimal separating hyperplane is a hyperplane that separates the points and is of maximal (and equal) distance from both of them.(26)(27)(28)(30)(31)(32)(33)(34)1930A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–1939Fig. 4. A bound for the robustness level.Fig. 5. An elicitation scenario with a secret variable S.Here (cid:8)ϕx,x(cid:4) is a normalized vector that passes within equal distance of ˆpx and ˆpx(cid:4) (see Fig. 4). (cid:7)∗equal distance, which we compute by subtracting from ˆpx its projection in the direction (cid:8)ϕx,x(cid:4) .is then limited by that(cid:4)that are within a distance of (cid:7) to the hyperplane, then these two vectors ˆpx, ˆpx(cid:4) can be per-Proof. If there exist x, xturbed towards the hyperplane with a perturbation of norm (cid:7), until they are linearly dependent. For this problem instance,according to Proposition 1, there is no possible mechanism. (cid:2)It appears from the proposition above that the most robust mechanism could be found easily by finding optimal separat-ing hyperplanes between the probability vectors (cid:8)px, but in fact this alone will not do. The payment vectors that define theseparators have additional constraints (see Eqs. (14) and (15)) that relate the various separators to one another. In addition,one must consider robustness with regard to other constraints (not just the truth-telling constraints). With these additionalfactors, the most robust payments may not match the optimal separators at all.However, in the case where |Ω| = 2, the vectors ˆpx are situated in a two-dimensional plane, and it can be shown thatthe bound given above is tight—then the problem robustness is determined exactly by the closest pair of vectors.5. Partial revelation mechanismsIn this section we shall explore, from a different angle, the problem of a common prior between agents. We shall modifyour model of the information transaction and give the seller of information an extra random variable S that it can access.The value of S will not be divulged to the buyer, but may influence the decisions of the seller. We will however assume thatthe buyer is aware of the existence of this extra information and its possible values. As a result of this extra information,the buyer is placed at a disadvantage. It knows even less about the state of the world than the seller. This condition holdseven after the transaction is concluded.For example, in the scenario we presented in Section 1.1, Charlie who is Alice’s boss may design the payment schemeso that Alice tells all of her customers the truth. He knows that Bob’s car may have one of two different motors installedin it. One motor is of higher quality and has less chance of breaking down, and the other is of lower quality. Alice, whois a trained mechanic, can find out which type of motor Bob has (which she can do at a mere glance without any effort),which will influence the probabilities she associates with a malfunction. Bob may not be interested in the exact make of hisengine, only in the likelihood that the car will break down. Can Alice still be financially encouraged to tell the truth?Fig. 5 describes the new elicitation scenario. The seller still needs to pay a cost of c to access the random variableX and report its findings to the buyer, only now it can access (for free) the random variable S as well. The paymentmade by the buyer depends only on the information it has available—not on the value of S. Once again we assume that aprobability distribution pω,x,s governs the three variables, and that it is common knowledge. Note however, that since theseller alone has access to S, it has a clearer and more precise knowledge of the distribution of X and Ω since it knowsPr(Ω = ω, X = x | S = s).A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–19391931Fig. 6. An elicitation scenario with two possible results, and two possible secrets.The three types of requirements from a mechanism are similar to those we have seen before. We shall now say that amechanism is proper for a secret s if the following three conditions hold:(1) Truth telling.(cid:4)∀x, xs.t.(cid:4)x (cid:7)= x,(cid:6)ωpω,x,s · (uω,x − uω,x(cid:4) ) > 0(2) Individual rationality.(cid:6)ω,xpω,x,s · uω,x > c · ps(3) Investment.(cid:4)∀x(cid:6)ω,xpω,x,s · uω,x − c · ps >(cid:6)ω,xpω,x,s · uω,x(cid:4)(35)(36)(37)When designing partial revelation mechanisms, there are often probability distributions that do not allow us to constructan effective mechanism for all possible secrets the seller may hold. The example in Fig. 6 demonstrates such a case.It is impossible to find a separating hyperplane that will separate (cid:8)px1,s1 from (cid:8)px2,s1 and at the same time separate (cid:8)px1,s2work only for a single secret each. Since the buyer is never told about the actualfrom (cid:8)px2,s2. The hyperplanes vsecret s, it has no way of creating the incentives for truthfulness in both cases.and v(cid:4)(cid:4)(cid:4)We must therefore settle on building a mechanism that will work only part of the time. We will naturally aspire tohave a good confidence level in our mechanism—to build a mechanism that will work with high probability. There are twopossible alternatives we examine here:(1) A single-use, disposable mechanism—where we design the mechanism for only a single transaction. We then want thebuyer’s confidence in the received answer to be high:θ1 = Prs,x(cid:7)(cid:8)u is proper for state (s, x)(38)This means that we only require truth-telling in case of secret s and value xIf we refer back to our example, this sort of mechanism will be appealing to Bob if he needs a single evaluation of thecondition of his car. In this case, Alice may report several different findings. Some of them have very low probability(for example, the event of a crack in the motor may be extremely rare) and Bob may not mind if that specific piece ofinformation is not elicited correctly (because it is such a remote occurrence).that occur.(cid:4)(2) A reusable mechanism—where we design the mechanism for multiple transactions. Here, we want the buyer to havehigh confidence that, once the secret s has been set, he will hear the truth for all possible cases of X :θ2 = Prs(u is proper for secret s)(39)Referring again to our example, this mechanism may have appeal if Bob wants several evaluations of his car performed,and wants the truth for as many of them as possible. Since the model of Bob’s engine does not change betweenexaminations of the car, he can take that into account when maximizing the probability of hearing the truth.1932A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–19395.1. Complexity of partial revelation mechanism designProposition 5. Deciding if a reusable revelation mechanism with a confidence level over some threshold θ exists is NP-Complete.Furthermore, the problem of finding the mechanism with the maximal confidence level cannot be approximated within any constant.The design problem is in NP. This is because if we are given access to an oracle that tells us which secrets to try tosatisfy and which to give up on, we can find a payment scheme that satisfies the right constraints in polynomial time. Thisis achieved by solving the linear program that consists of the constraints for all the included secrets.We show that constructing a fully operational mechanism is NP-Complete by presenting a reduction from the Inde-pendent Set problem. The full reduction is presented in the appendix. The Independent Set problem, in addition to beingNP-Complete, is also hard to approximate [34]. The reduction we give is a cost-preserving reduction and therefore demon-strates that our problem is just as hard to approximate as Independent Set.The high complexity of designing proper mechanisms applies in the single-use, disposable case as well.Proposition 6. Deciding if there exists a single-use elicitation mechanism with a confidence level over some threshold θ is also NP-Complete.Proof of this proposition relies on a reduction from the Hyperplane-Consistency problem. The full proof appears in theappendix.5.2. Finding partial revelation mechanismsWe now present two approaches to computing a partial revelation mechanism for a given problem pω,x,s. As we havealready seen, the problem of finding such a mechanism is NP-Complete, and unless P=NP, we cannot hope to locate theoptimal mechanism in polynomial time in all cases. However, in some cases, the problem may be simpler than the worstpossible case. The two approaches we present differ in the complexity of the algorithm. One algorithm will be better incases where |S| is small, while the other will be better in cases where |Ω| · | X| is small.The algorithms we present are for reusable mechanisms. Similar versions can be constructed for the single-use case.5.2.1. Considering all combinations of secretsThe reductions we used in the proofs of Propositions 5 and 6 both relied on the difficulty of selecting the cases in whichwe wish the mechanism to work. This difficulty arises due to the discrete nature of the secrets. If we had an oracle thatshows us which constraints to try to satisfy, we could easily construct a mechanism. Since we do not possess such an oracle,we can try every possible combination by brute force. This method relies on the discrete nature of the problem and thefinite set of possible secrets:Algorithm 1 (Reusable mechanism construction).(1) For all W ∈ 2S :(a) Locate a mechanism that satisfies all constraints for all secrets in W .(b) If such a mechanism exists, compute θW =(2) Return a mechanism for secrets arg maxW (θW ).s∈W ps.(cid:5)In the algorithm above, there are 2|S|to check for feasibility. This therefore gives a running time of O (2number of possible secrets is small.ways to select secrets to satisfy. Each selection then requires poly(|S(cid:9) X(cid:9)Ω|) time|S| · poly(|S(cid:9) X(cid:9)Ω|)) which can still be efficient if the5.2.2. The geometric approach—partitioning into cellsThe second approach we shall examine is based on a geometric interpretation of the problem. The linear constraint forthe mechanism design problem partitions the space of payment vectors into cells. Each cell is a region of the space forwhich some set of constraints holds, while the rest are violated (see Fig. 7 for an illustration).The mechanism design problem is in fact the problem of locating a non-empty cell that satisfies as many constraints aspossible. This naturally leads to an algorithm that builds a list of cells and iterates over them to locate the cell assignmentwith the highest score.Algorithm 2 (Geometric).(1) Construct a list L of cells created by all hyperplanes (cid:8)px,s.(2) Select an assignment σ : X × X → L.A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–19391933Fig. 7. A collection of hyperplanes partitioning the plane into cells.(3) Try to solve the linear problem that consists of constraints placing v x,x(cid:4) in the cell σ (x, x(cid:4)), and satisfying(cid:8)v x,x(cid:4) = −(cid:8)v x(cid:4),x;(cid:8)v x,x(cid:4)(cid:4) = (cid:8)v x,x(cid:4) + (cid:8)v x(cid:4),x(cid:4)(cid:4)(4) If a solution is found, compute:(a) W σ ∈ 2S , the list of secrets that assignment σ of vectors v x,x(cid:4) satisfies.(b) θσ =ps.(cid:5)s∈W σ(5) Return the payment scheme found for arg maxσ (θσ ).In order to generate the list of cells L needed in the algorithm above, one can simply start from a list containing a singlecell that contains the entire vector space and incrementally add hyperplanes. Each hyperplane that is added may partitiona cell in the list into two cells, one on either side of the hyperplane, or may leave the cell intact. At every stage one onlyneeds to iterate over the list of existing cells and check if they are split by the new hyperplane.5.2.3. Complexity of the algorithmIn order to analyze the running time of Algorithm 2, we need to obtain a bound on the number of cells created by thehyperplanes defined by (cid:8)px,s. Such a bound is given in [35]. Given m hyperplanes in d-dimensional space, the number ofcells is bounded by:(cid:14)(cid:15)(40)Φd(m) =d(cid:6)mii=0(cid:7)md(cid:8).= OThe bound is obtained using the VC-Dimension of the concept class implied by cell partitioning and Sauer’s lemma [36].This bound is especially interesting when d is small, since it implies that the number of cells is only polynomial in thenumber of hyperplanes m.In our case, we have | X(cid:9)S| hyperplanes in an |Ω|-dimensional space, which gives a bound of |L| = O (| X||S||Ω|) cells.Generating the list of cells can be done in(cid:7)| X||S|2|Ω| · poly(cid:7)| X||S(cid:9)Ω|(cid:8)(cid:8)Otime steps. The number of possible assignments σ : X × X → L is(cid:8)|L|| X|2 (cid:8)(cid:7)O(cid:7)| X||S|| X|2|Ω|= Oand for each assignment we need to solve a linear program that requires poly(| X||S(cid:9)Ω|)) steps, which gives us a totalrunning time of(cid:7)| X||S|| X|2|Ω| · poly(cid:7)| X||S(cid:9)Ω|(cid:8)(cid:8)Otime steps.This algorithm is therefore better in cases where |S| is large, but |Ω| and | X| are small.6. Elicitation of confidence ratingsIn many cases, the expert that sells the information has some idea regarding the reliability of the information it is selling.For example, a reviewer reading a paper is often asked to rate his or her familiarity with the field, and his or her confidence1934A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–1939Fig. 8. Elicitation of a confidence rating pc .in the submitted review. Confidence level is also quite important when considering what to do with information—if areviewer who is not confident was selected, the paper could be sent for further review to someone else. In other cases, onlya noisy signal can be received—Alice may check Bob’s car and discover that the problem is either in the ignition system, orthe fuel injection system, but be unsure as to the exact origin.It is therefore important to be able to elicit the confidence rating or degree of certainty regarding a piece of information,and not just the information itself. Fortunately, this is often possible with various models for inexact information, as theseoften reduce to regular information elicitation problems. We briefly demonstrate two such models here.6.1. An error modelOne such model for confidence would be to assume that with some probability pc , the information learned by the expertis correct and drawn from the distribution pω,x, while with probability 1 − pc it is erroneous and has a value of X accordingto some other distribution:qω,x = q(Ω = ω, X = x).(41)X and Ω may be independent in the distribution qω,x, but this does not have to be the case.The seller can then be asked to divulge pc , as well as the value of X that it got. Fig. 8 depicts the information transactionin this case. In this model, we allow pc to take continuous values between 0 and 1, and assume that the cost of acquiringthe information is 0, as there is no way to create the incentives to learn the value of X in case the confidence rating ispc = 0.8Proposition 7. The confidence ratings pc and the true value of X can be elicited truthfully if there exists a truthful payment scheme forthe elicitation of X with payments (cid:8)ux such that for any x, x(cid:4) ∈ X ,(cid:8)qx · ((cid:8)ux − (cid:8)ux(cid:4) ) (cid:2) 0.(42)Proof. Let u be a payment scheme that truthfully elicits the value of X , as well as the condition from inequality (42). Now,given two possible values (x, pc), (xwe have:(cid:4), p(cid:4)(cid:4)c), if x (cid:7)= x(43)(44)(45)(cid:8)qx · ((cid:8)ux − (cid:8)ux(cid:4) ) (cid:2) 0;(cid:8)qx(cid:4) · ((cid:8)ux − (cid:8)ux(cid:4) ) (cid:4) 0;(cid:8)px · ((cid:8)ux − (cid:8)ux(cid:4) ) > 0(cid:8)px(cid:4) · ((cid:8)ux − (cid:8)ux(cid:4) ) < 0which implies(cid:7)(cid:8)(cid:7)pc · (cid:8)px + (1 − pc) · (cid:8)qxpc(cid:4) · (cid:8)px(cid:4) + (1 − pc(cid:4) ) · (cid:8)qx(cid:4)· ((cid:8)ux − (cid:8)ux(cid:4) ) (cid:2) 0(cid:8)· ((cid:8)ux − (cid:8)ux(cid:4) ) (cid:4) 0(46)where the inequalities above are strict whenever pc, pc(cid:4) are not 0. Now notice that P (Ω|x, pc) = pc · (cid:8)px + (1 − pc) · (cid:8)qx and(cid:7)= 0. We can then award theEqs. (45), (46) in fact show that the probabilities P (Ω|x, pc) are different as long as pc, pc(cid:4)seller a payment according to some scoring rule:(cid:7)u w,x,pc= SP (Ω = ω | x, pc), ω(cid:8).Since the probabilities P (Ω | x, pc) are different for every report, the scoring rule assures us of the incentive to tell the truevalue. (cid:2)Remark 1. The condition(cid:8)qx · ((cid:8)ux − (cid:8)ux(cid:4) ) (cid:2) 0implies that the payment scheme u also truthfully elicits the value of X under the probability distribution q (as long aswhenever the seller is indifferent between telling the truth and lying it will choose to tell the truth).8 If we are sure the confidence level is always higher than some positive number, then a mechanism can be designed to ensure investment of effort aswell.A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–193919356.2. Inexact knowledgeAnother possible model in which the seller has unreliable information is one where instead of getting a value of x ∈ X itreceives a set T ⊂ X from which the value of X will be chosen (and so it knows something more about the possible valuesof X , but not the exact value).We then assume that learning the subset T implies that X will be chosen only from that subset according to thedistribution pω,x that is reduced to T :Pr( X = x, Ω = ω, x ∈ T ) =(cid:5)(cid:5)pω,xx(cid:4)∈T pω(cid:4),x(cid:4)ω(cid:4)(47)In this case, we can ask the seller to provide the exact subset T that it learned. This problem once again is reduced toa truthful information elicitation problem, this time with T being the elicited variable. Each value of T implies a differentdistribution on the values of ω and can thus be elicited using mechanisms of the form we have shown in Section 3 for thedistribution Pr(Ω|T ).Note, however, that the model we presented did not include information about the probability of a certain T ⊂ X beingselected, and that there is no way to discuss the elicitation of effort without such a model.97. ConclusionsWe have introduced a model for discrete information transactions and have shown simple information elicitation mech-anisms that can provide the sellers with the correct incentives to report honestly and even invest effort into obtaining theinformation they sell. We have shown that in most cases these simple mechanisms exist and can be designed optimallyusing scoring rules. We explored various properties of the solution, such as the cost of the mechanisms and the level of riskthey entail when verification of information is difficult.In order to tackle the problem of belief variations between the sellers and the designer of the mechanism, we introducedthe concept of robust mechanisms. These mechanisms are guaranteed to work if the beliefs of agents are not too far apart.We have shown efficient algorithms for learning the robustness level of a given payment scheme, finding payment schemeswith guaranteed level of robustness, and for finding the robustness level of a problem. The efficiency of their design, as wellas their resilience, makes these mechanisms good candidates for application in real-world scenarios.We have used tools of stochastic programming to solve for robust solutions, but have only scratched the surface ofpotential uses of these tools. Other alternative problem formulations can be explored, especially formulations that includemore detailed information about the possible beliefs of agents. These would fit quite well into mainstream work done instochastic programming.To further explore information transactions, we examined a model in which the seller of information has access to extrainformation that is not sold. We have seen that partial revelation of information can make it impossible to build mechanismsthat work all the time, and that building good mechanisms that work most of the time is a computationally difficult task.Here we have provided proofs of computational difficulty as well as two algorithms with different running times that maybe suitable in different cases.AcknowledgementsThe authors wish to thank the reviewers, whose comments provided guidance for our refining of the paper and itsoverall improvement. Preliminary material from this paper appeared at the Twenty-First National Conference on ArtificialIntelligence (AAAI’06), in the papers “Robust Mechanisms for Information Elicitation” [37] and “Mechanisms for PartialInformation Elicitation: The Truth, But Not the Whole Truth” [38]. This work was partially supported by Israel ScienceFoundation grant #898/05.Appendix A. Computational hardness of reusable mechanism designProposition. Deciding if a reusable revelation mechanism with a confidence level over some threshold θ exists is NP-Complete. Fur-thermore, the problem of finding the mechanism with the maximal confidence level cannot be approximated within any constant.Proof. The proof relies on a reduction from the Independent Set problem. Given an undirected graph G(V , E) and an integerk (cid:4) |V | the Independent Set decision problem is defined as the problem of deciding whether there is a set of vertices W ⊂ Vso that |W | (cid:2) k, and such that for every edge e ∈ E, e does not occur on more than one vertex in V .The intuition behind the reduction is derived from Fig. 6. The selection of vertices in the independent set will be designedto match a selection of secrets to satisfy in the design problem. In order to uphold the restriction that no two vertices9 A possible model is to assume T is selected according to a probability that is proportional toaccording to the original distribution pω,x.(cid:5)x∈T px, which then ensures that x is eventually selected1936A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–1939sharing an edge can be chosen together, a construction similar to Fig. 6 will be created in a dedicated two-dimensionalsubspace to assure that their matching secrets cannot be satisfied at the same time.Let us now proceed with the proof. Given an instance of the Independent Set problem (G(V , E), k) we shall construct amechanism design problem (Ω, X, S, P , θ) in the following manner:10(cid:16)(cid:16)Ω ={ωe1, ωe2};X ={xe1, xe2}e∈ES = V ;θ = k|V |e∈E(A.1)(A.2)We denote by (cid:8)δi the vector that is 0 at all coordinates except for coordinate i, where it takes the value of 1, and by α anormalizing constant that equals α = 1if v /∈ e then:2|E||V | . P is then defined as follows:(cid:8)pxe1,v = α · (cid:8)δωe1;(cid:8)pxe2,v = α · (cid:8)δωe2otherwise e = {v1, v2} and we set:;(cid:8)pxe1,v1 = α · (cid:8)δωe1(cid:8)pxe1,v2 = α· ((cid:8)δωe12(cid:8)pxe2,v1 = α· ((cid:8)δωe1+ (cid:8)δωe2 )2(cid:8)pxe2,v2 = α · (cid:8)δωe2+ (cid:8)δωe2 );With the above construction all secrets have the same probability of occurring:Pr(S = s) =(cid:6)ω,xpω,x,s = 2|E|α = 1|V |Below we show the two steps needed to complete the proof:(A.3)(A.4)(A.5)(A.6)(1) If the graph G has an independent set of size k then there is a mechanism with a confidence level above the threshold θ .Let us assume that G has an independent set W ⊂ V of size k. We shall build a payment scheme that will give a propermechanism for all the secrets matching the vertices in W . For an edge e that has one of its vertices in the independentset,11 we shall define:(cid:8)uxe1=(cid:8)pxe1,v(cid:9)(cid:8)pxe1,v (cid:9);(cid:8)uxe2=(cid:8)pxe2,v(cid:9)(cid:8)pxe2,v (cid:9)(A.7)where v is the vertex (from edge e) that was selected for the independent set. If on the other hand e did not have anyvertex in the independent set, we simply set(cid:8)uxe2= (cid:8)δωe1= (cid:8)δωe2(cid:8)uxe1(A.8);We will next demonstrate that this payment scheme does give a truthful mechanism at least for all the secrets in W .We must therefore show that∀v ∈ W ∀xek (cid:7)= xe(cid:4)l ∈ X(cid:8)pxek,v · ((cid:8)uxek− (cid:8)uxe(cid:4)l) > 0(A.9)Let us examine the following three cases:(a) Edge e does not occur on vertex v, and has no vertex in the independent set. Then the vector (cid:8)pxek,v = α · (cid:8)δωek .= (cid:8)δωek , meaning that it is a unit vector in the direction ofis also a unit vector that is in the other direction, its inner product with (cid:8)pxek,v is smaller and theBecause e has no vertex in the independent set then (cid:8)uxek(cid:8)pxek,v . Since (cid:8)uxe(cid:4)linequality holds.(b) Edge e does not occur on vertex v, but has another vertex in the independent set. In this case we still have(cid:8)pxek,v = α · (cid:8)δωek but now (cid:8)uxek has two possible values, depending on which vertex of e is in the independent set.Either(i) (cid:8)uxek(ii) (cid:8)uxek· ((cid:8)δωek).+ (cid:8)δωek= (cid:8)δωek ,= 1√210 A small comment about notation: e1 and e2 above do not reference the two vertices of edge e, but just serve to denote two different values for thatedge. We shall explicitly make it clear when we refer to vertices of the edge.11 It cannot have both its vertices in the set—only one or none.A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–19391937l(cid:4)− (cid:8)uxekIn both these cases the inner product between (cid:8)pxek,v and (cid:8)uxek is strictly positive. If efrom e then (cid:8)pxek,v · (cid:8)uxe((cid:8)uxek) · (cid:8)δωek > 0 (simply by looking at all the cases)—and this gives us the required inequality exactly.is an edge that is different(cid:4) = e but k (cid:7)= l. In this case we observe thatis zero and the inequality holds. Otherwise, e(c) Edge e occurs on vertex v. Since we are only concerned with v’s that are in the independent set, the vector (cid:8)uxek isis a unit vector in another direction. This meansby our definition a unit vector in the direction of (cid:8)pxek,v , while (cid:8)uxethat the inner product between (cid:8)pxek,v and (cid:8)uxek is greater and the inequality once again holds.We have thus shown that we are able to have a working mechanism for every vertex v ∈ W and thus have a mecha-nism that works well for k secrets. The confidence level of the mechanism designer in its mechanism is then at least(cid:5)(cid:4)l(cid:4)s∈W Pr(S = s) = k|V | = θ .(2) If there is a good mechanism with confidence level above θ then there is an independent set of size k in the graph.k|V | , there must be at least k satisfied secrets in the mechanism. Each such secretSince there is a confidence level ofmatches a vertex in the original problem. It remains to show that the set W of vertices matching satisfied secrets isindependent. Assuming the opposite leads to a contradiction. The secrets matching two vertices that are connected byan edge cannot be satisfied at the same time due to the way the problem was constructed. The probability vectors foreach edge ((cid:8)pxe1,v1, (cid:8)pxe1,v2, (cid:8)pxe2,v1, (cid:8)pxe2,v2) were placed in a separate two-dimensional space, and were set similarly tothe vectors in Fig. 6—in a way that ensures that both pairs cannot be linearly separated at the same time.Let us show that the set W of vertices matching satisfied beliefs is independent. We first assume the opposite: that(cid:4) ∈ W that reside on the same edge in G. By construction we thereforethere are two vertices that we shall denote v, vhave two beliefs for vertices v, vthat were both satisfied. Meaning that(cid:4)∀x (cid:7)= x(cid:4) ∈ X (cid:8)px,v · ((cid:8)ux − (cid:8)ux(cid:4) ) > 0and also that(cid:8)px,v(cid:4) · ((cid:8)ux − (cid:8)ux(cid:4) ) > 0(A.10)(A.11)(cid:4)that we choose, such as for xe1 and xe2, where eMore specifically, the above holds true for any specific values of x, x(cid:4)is the edge that is shared by v, v. Therefore, the following two statements must be true at the same time:(cid:8)pxe2,v · ((cid:8)uxe2(cid:8)pxe1,v(cid:4) · ((cid:8)uxe1− (cid:8)uxe1 ) > 0− (cid:8)uxe2 ) > 0Without loss of generality, we can assume at this point that v is the first vertex in edge e, and vTherefore, by construction we have:(cid:8)pxe2,v = (cid:8)pxe1,v(cid:4) = α2· ((cid:8)δωe1+ (cid:8)δωe2 )(A.12)(A.13)(cid:4)is the second vertex.(A.14)and by substituting this into the above, we reach a contradiction, since it cannot be the case that both of the followingare true at the same time:(cid:8)pxe2,v · ((cid:8)uxe2(cid:8)pxe1,v(cid:4) · ((cid:8)uxe1− (cid:8)uxe1 ) > 0− (cid:8)uxe2 ) = −(cid:8)pxe2,v · ((cid:8)uxe2− (cid:8)uxe1 ) > 0(cid:4)So our assumption that there can be two secrets v, von the same edge is false, and the set W is indeed independent.that are satisfied at the same time but have counterpart vertices(A.15)(A.16)This completes the proof of the reduction. (cid:2)Appendix B. Computational hardness of designing single-use mechanismsProposition. Deciding if there exists a single-use elicitation mechanism with a confidence level over some threshold θ is NP-Complete.Proof. We give a proof of this proposition using a series of reductions from the decision problem associated with Max-Hyperplane-Consistency. This problem is known to be NP-Complete [39].An instance of the Hyperplane-Consistency problem is defined by a tuple (P, N , k) where k is an integer and P, N aresets of vectors in Rn with integer coordinates. The instance should be accepted if a biased linear separator ( (cid:8)w, b) existssuch that:k (cid:4)(B.1)meaning that the separator ( (cid:8)w, b) manages to place more than k points from P on its positive side and from N on itsnegative side.(cid:17)(cid:17){(cid:8)x ∈ N | (cid:8)w · (cid:8)x < b}(cid:17)(cid:17){(cid:8)x ∈ P | (cid:8)w · (cid:8)x (cid:2) b}(cid:17)(cid:17) +(cid:17)(cid:17)Now, given a Hyperplane-Consistency problem we shall reduce it to a single-use mechanism design problem in threesteps:1938A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–1939(1) We will first show that a hyperplane-consistency problem for vectors with only positive coordinates is still NP-complete;(2) We will then show that using an unbiased hyperplane to separate positive vectors is still just as hard;(3) Finally, we shall reduce the last problem to a mechanism design problem, thus showing that it is NP-Complete to designthe mechanism.Let us assume that we are given a Hyperplane-Consistency problem (P, N , k). We now define a new problem Pos-Hyper-plane-Consistency as the tuple (P (cid:4), N (cid:4), k) whereP (cid:4) = {(cid:8)x + (cid:8)δ | x ∈ P}N (cid:4) = {(cid:8)x + (cid:8)δ | x ∈ N }(B.2)(B.3)and where (cid:8)δ has been set large enough in each coordinate to turn all vectors in P (cid:4)and N (cid:4)positive.We will now show that given a hyperplane that separates points in a certain way in the original problem, we can builda hyperplane that will separate the matching points in the new problem in the same manner and vice versa. Given ahyperplane ( (cid:8)w, b), we look at the hyperplane ( (cid:8)w, b + (cid:8)w · (cid:8)δ) in the new problem.(cid:8)w · (cid:8)x (cid:2) b ⇔ (cid:8)w · ((cid:8)x + (cid:8)δ) (cid:2) (cid:8)b + (cid:8)w · (cid:8)δ(B.4)Meaning that a point (cid:8)x and its corresponding point (cid:8)x + (cid:8)δ in the new problem are classified in the same way by thehyperplane. In other words, if some hyperplane exists (in either problem) that manages to correctly classify k points ormore, then a matching classifier exists in the other problem as well. Pos-Hyperplane-Consistency is therefore also NP-Complete.Now, we shall show that Pos-Unbiased-Hyperplane-Consistency (where the hyperplanes are unbiased) is still anNP-Complete problem. Given an instance (P (cid:4), N (cid:4), k) of Pos-Hyperplane-Consistency, we shall reduce it to an instance(P (cid:4)(cid:4), N (cid:4)(cid:4), k) of Pos-Unbiased-Hyperplane-Consistency by adding a coordinate to each vector in P (cid:4)and N (cid:4):(cid:11)P (cid:4)(cid:4) =N (cid:4)(cid:4) =((cid:8)x, 1) | x ∈ P (cid:4)(cid:11)((cid:8)x, 1) | x ∈ N(cid:12)(cid:12)(B.5)(B.6)The last coordinate in the vectors now takes the place of the bias. For every hyperplane defined by (cid:8)w, b, there is amatching unbiased hyperplane with a weight vector ( (cid:8)w, −b) that gives the same classification to the matching point in thenew problem.(cid:8)w · (cid:8)x (cid:2) b ⇔ ( (cid:8)w, −b) · ((cid:8)x, 1) (cid:2) 0(B.7)A correct classification of k or more points exists in the new problem iff it existed in the old problem, and Pos-Unbiased-Hyperplane-Consistency is NP-Complete as well.We shall now see the final step in the proof—a reduction from Pos-Unbiased-Hyperplane-Consistency to the mechanismdesign problem. Given an instance of the former, (P (cid:4)(cid:4), N (cid:4)(cid:4), k) in an n dimensional space, we shall reduce it to the followingdesign problem:Ω = {1, . . . , n + 2};S = P (cid:4)(cid:4) ∪ N (cid:4)(cid:4);(cid:15)X = {0, 1}(cid:14)1 + k|S|θ = 12If (cid:8)s ∈ P (cid:4)(cid:4)we define:= α(cid:9)(cid:8)s(cid:9)1(cid:8)p0,(cid:8)s· ((cid:8)s, 0, 0)= α · ((cid:8)0, 1, 0)(cid:8)p1,(cid:8)sOtherwise (cid:8)s ∈ N (cid:4)(cid:4)and we define:(cid:8)p0,(cid:8)s(cid:8)p1,(cid:8)s= α · ((cid:8)0, 0, 1)= α(cid:9)(cid:8)s(cid:9)1· ((cid:8)s, 0, 0)(B.8)(B.9)(B.10)(B.11)(B.12)(B.13)Where α in the above is a normalizing constant that makes sure the probabilities sum to 1. In the above, we assumewithout loss of generality that no point appears both in P (cid:4)(cid:4), otherwise it can be eliminated from the problemwhile reducing k by 1. Now, with the definitions above, a payment mechanism which is simply a vector (cid:8)v 0,1 works for statex, (cid:8)s if the vector (cid:8)px,(cid:8)s is positioned on the correct side of the hyperplane it represents. The vectors of the form α · ((cid:8)0, 1, 0) andα · ((cid:8)0, 0, 1) can always be placed on the correct side since they have a coordinate dedicated just to them for that purpose.They constitute one half of the probability weight. The other half are actually vectors identical to the vectors in P (cid:4)(cid:4)and inN (cid:4)(cid:4)with zeros in their extra coordinates, and a correct separation of k out of them implies directly the correct separationof vectors in the original problem and vice versa. (cid:2)and in N (cid:4)(cid:4)A. Zohar, J.S. Rosenschein / Artificial Intelligence 172 (2008) 1917–19391939References[1] R. Axelrod, The Evolution of Cooperation, Basic Books, New York, 1984.[2] E.M. Maximilien, M.P. Singh, Reputation and endorsement for web services, SIGecom Exch. 3 (1) (2002) 24–31.[3] P. Resnick, K. Kuwabara, R. Zeckhauser, E. Friedman, Reputation systems, Commun. ACM 43 (12) (2000) 45–48.[4] A. Mas-Colell, M.D. Whinston, J.R. Green, Microeconomic Theory, Oxford University Press, 1995, Chapter 23, pp. 857–926.[5] E. Maskin, T. Sjöström, Implementation theory, Working paper, Harvard University and Penn State, January 2001.[6] J.-J. Laffont, D. Martimort, The Theory of Incentives: The Principal-Agent Model, Princeton University Press, 2002.[7] B. Salanié, The Economics of Contracts: A Primer, MIT Press, 2005.[8] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, second ed., Morgan Kaufmann, San Mateo, CA, 1988.[9] C. Boutilier, On the foundations of expected expected utility, in: The International Joint Conference on Artificial Intelligence (IJCAI’03), Acapulco, 2003,pp. 285–290.[10] L.J. Savage, Elicitation of personal probabilities and expectations, J. Amer. Statist. Assoc. 66 (336) (1971) 783–801.[11] T. Gneiting, A.E. Raftery, Strictly proper scoring rules, prediction, and estimation, Tech. Rep. 463, Department of Statistics, University of Washington,2004.[12] A.D. Hendrickson, R.J. Buehler, Proper scores for probability forecasters, Ann. Math. Statist. 42 (1971) 1916–1921.[13] R.J. Aumann, Agreeing to disagree, Ann. Statist. 4 (6) (1976) 1236–1239.[14] D. Samet, Iterated expectations and common priors, Games Econom. Behav. 24 (1).[15] N. Miller, P. Resnick, R. Zeckhauser, Eliciting honest feedback: The peer prediction method, Management Sci. 51 (9) (2005) 1359–1373.[16] R. Jurca, B. Faltings, An incentive compatible reputation mechanism, in: Proceedings of the IEEE Conference on E-Commerce, Newport Beach, CA, USA,2003, pp. 285–292.[17] S. Goldwasser, Multi party computations: past and present, in: Proceedings of the Sixteenth Annual ACM Symposium on Principles of DistributedComputing (PODC’97), ACM, New York, 1997, pp. 1–6.[18] R. Smorodinsky, M. Tennenholtz, Sequential information elicitation in multi-agent systems, in: Proceedings of the 20th Conference on Uncertainty inArtificial Intelligence (UAI-2004), 2004, pp. 528–535.[19] R. Smorodinsky, M. Tennenholtz, Overcoming free-riding in multi-party computation: The anonymous case, Games Econ. Behav. 55 (2006) 385–406.[20] P. Bohm, J. Sonnegard, Political stock markets and unreliable polls, Scandinavian J. Econ. 101 (2) (1999) 205.[21] J. Wolfers, E. Zitzewitz, Prediction markets, Working Paper 10504, National Bureau of Econ. Research, 2004.[22] K. Gajos, D.S. Weld, Preference elicitation for interface optimization, in: UIST’05: Proceedings of the 18th Annual ACM Symposium on User InterfaceSoftware and Technology, ACM Press, New York, 2005, pp. 173–182.[23] P. Pu, B. Faltings, M. Torrens, User-involved preference elicitation, 2003.[24] J.O. Kephart, J.E. Hanson, J. Sairamesh, Price and niche wars in a free-market economy of software agents, Artificial Life 4 (1) (1998) 1–23.[25] J.O. Kephart, C.H. Brooks, R. Das, Pricing information bundles in a dynamic environment, in: EC’01: Proceedings of the 3rd ACM conference on ElectronicCommerce, ACM Press, New York, 2001, pp. 180–190.[26] C. Prendergast, A theory of “yes” men, Amer. Econ. Rev. 83 (4) (1993) 757–770.[27] N. Nisan, A. Ronen, Algorithmic mechanism design (extended abstract), in: STOC’99: Proceedings of the Thirty-First annual ACM Symposium on Theoryof Computing, ACM Press, New York, 1999, pp. 129–140.[28] V. Conitzer, T. Sandholm, Complexity of mechanism design, in: Proceedings of the Uncertainty in Artificial Intelligence Conference (UAI-2002), Edmon-ton, Canada, 2002, pp. 103–110.[29] N. Hyafil, C. Boutilier, Partial revelation automated mechanism design, in: The Twenty-Second Conference on Artificial Intelligence (AAAI’07), Vancouver,2007, pp. 72–78.[30] N. Hyafil, C. Boutilier, Mechanism design with partial revelation, in: The Twentieth International Joint Conference on Artificial Intelligence (IJCAI’07),Hyderabad, India, 2007, pp. 1333–1340.[31] R. Jurca, B. Faltings, Eliminating undesired equilibrium points from incentive compatible reputation mechanisms, in: Proceedings of the Seventh Inter-national Workshop on Agent Mediated Electronic Commerce (AMEC VII), Utrecht, The Netherlands, 2005.[32] P. Kall, S.W. Wallace, Stochastic Programming, Systems and Optimization, John Wiley, 1995.[33] A. Ben-Tal, A. Nemirovski, Robust solutions of uncertain linear programs, Op. Res. Lett. 25 (1999) 1–13.[34] J. Håstad, Clique is hard to approximate within n1−(cid:7) , Acta Math. 182 (1999) 105–142.[35] H. Edelsbrunner, Algorithms in Combinatorial Geometry, EATCS Monographs on Theoretical Computer Science, vol. 10, Springer-Verlag, 1987.[36] M. Vidyasagar, A Theory of Learning and Generalization, Spinger-Verlag, 1997.[37] A. Zohar, J.S. Rosenschein, Robust mechanisms for information elicitation, in: The Twenty-First National Conference on Artificial Intelligence (AAAI’06),Boston, 2006, pp. 740–745.[38] A. Zohar, J.S. Rosenschein, Mechanisms for partial information elicitation: The truth, but not the whole truth, in: The Twenty-First National Conferenceon Artificial Intelligence (AAAI’06), Boston, 2006, pp. 734–739.[39] E. Amaldi, V. Kann, The complexity and approximability of finding maximum feasible subsystems of linear relations, Theoret. Comput. Sci. 147 (1–2)(1995) 181–210.