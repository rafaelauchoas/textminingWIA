Contents lists available at ScienceDirect Telecommunications Policy journal homepage: www.elsevier.com/locate/telpol AI narratives and unequal conditions. Analyzing the discourse of liminal expert voices in discursive communicative spaces☆ Alexa Robertson a, *, Max Maccarone b a Stockholm University, Sweden b Stockholm University, Sweden  A R T I C L E I N F O  A B S T R A C T  Keywords: AI narratives IG discourse Civil society Unequal conditions Global communicative space Epistemic communities 1. Introduction The stories told by expert activists about the relationship between AI and inequality are the focus of this article. It explores internet governance discourse in two fora - RightsCon and Sweden’s Internet Days - which, it is argued, comprise a communicative space that is both global and liminal. Narrative analysis is used to map how 30 expert activists from around the world, whose engagement is bound neither to state nor corporate interests, talk about how AI can be understood as a boon or a bane to inequality, both social and communicative. While common themes are in evidence (such as the need to safeguard people’s right to own their own data), some noteworthy dissonances are also discernible (such as whether such people should be envisaged as individuals or collectivities). The narratives are critical in that they resist the impetus of rapid, and in some cases unfettered, technological advancement while at the same time pushing back against the apocalyptic AI narratives familiar from popular culture. The study contributes to an under-standing of the socio-technico imaginaries of a category of actors who merit more attention than they have been paid by scholars to date. Their expertise grants them authority, and the stories they tell speak of agency.  Voice and agency have always been central to an understanding of the relationship between communication and inequality, but have taken on new urgency – and complexity – in the age of algorithms and Artificial Intelligence (AI). In their contribution to the International Panel on Social Progress, a team of leading media scholars notes that shifts in the nature and quality of governance have been catalyzed by the emergence of a networked information economy and the globalization of communication flows. They express particular concern over how the deployment of algorithms have ‘ambiguous implications for corporate power and individual rights, for the public sphere and social progress’ (Couldry et al. 2018, pp. 6, 22). Coming from the same field, we share this concern, but are more interested in finding out how tech experts and activists, as opposed to media scholars, understand developments in communication technology and, not least, its deployment. Do they see it as a boon or a bane? Two fora, in which such expert activists from around the world gather to discuss issues related to Internet Governance (IG), provide a point of access to a sort of wired civil society, and a place to look for new answers to old questions about the flow of communicative ☆The research reported in this article was conducted as part of the Just Information project, financed by the Swedish Research Council, grant #2018-02019. * Corresponding author. E-mail address: alexa.robertson@ims.su.se (A. Robertson). https://doi.org/10.1016/j.telpol.2022.102462 Received 27 February 2022; Received in revised form 25 August 2022; Accepted 9 October 2022  TelecommunicationsPolicyxxx(xxxx)xxxPleasecitethisarticleas:AlexaRobertson,MaxMaccarone,TelecommunicationsPolicy,https://doi.org/10.1016/j.telpol.2022.1024620308-5961/©2022TheAuthors.PublishedbyElsevierLtd.ThisisanopenaccessarticleundertheCCBYlicense(http://creativecommons.org/licenses/by/4.0/).A. Robertson and M. Maccarone                                                                                          power between actors on different societal levels: the annual gathering of RightsCon, and Sweden’s Internet Days (Internetdagarna, ID). Understanding IG to be as much about discourse and narrative - the way its challenges are grasped in words - as it is about technology and protocols, we analyze discussions in these fora with the aim of mapping how 30 expert activists from around the world, whose engagement is not uniquely bound to state or corporate interests, talk about the intersection of AI and inequality, both social and communicative. Three research questions operationalize the aim. First, what narratives about AI emerge from the discourse of these fora? Second, is a common narrative discernible, or do voices emanating from different fields - be they professional or geographical - have different stories to tell? And third, what can be learned from the differences that may emerge, when it comes to epistemological and empirical inequalities at the socio-technical interface? The voices in focus in what follows have been selected because they speak from different corners of a communicative space that is, we argue, both global and liminal. It is global because participants come from multiple continents, and some represent global orga-nizations. It is liminal for reasons that will be explained in the next section. That section is followed by ones in which the fora, and our key concept of ‘AI narratives’, are introduced more fully, with an explanation of how such narratives relate to understanding (imaginaries) as well as talking and telling (discourse and storytelling). The relation between the key concept and our analytical approach is set out in the methods section. The presentation of results is organized according to the themes that emerge from the transcripts that serve as our primary source material. In the conclusion we reflect, among other things, on how the experts agree that urgent action is needed to safeguard the communication rights of people everywhere, threatened as they are by the injudicious uses for which AI is sometimes deployed, and to prevent it from further exacerbating social inequalities. More thought-provoking is the lack of agreement as to whether data ownership, called for as a solution to such problems, should have the individual or the community as its common denominator. 2. Liminal communicative spaces: RightsCon and ID Liminality is often treated as synonymous, in social science literature, with being on the margins or in the periphery. The discourse we analyze does contain expressions of this use of the term. In the stories they tell, some of the speakers explain that they deviate from the mainstream (‘I wasn’t really fond of the sort of normal behaviours that were around me’ is how one speaker explains why the internet has always been ‘a really comfortable place for me’). Liminality thus is about being in an ambiguous place; in the centre of an increasingly powerful expert community, but outside the mainstream. We use liminality in this sense of the word, but also as defined by an ordinary dictionary as ‘occupying a position at, or on both sides of, a boundary or threshold’, which is more about being in advance than peripheral. Many of the speakers are difficult to categorize because they occupy several roles and move rapidly from one position or sector to another in their professional lives - from the technical to the entrepreneurial; from policy domains to the NGO sphere. They match the description of liminal actors described by Belair-Gagnon et al. (2019) as ‘often speciality actors who exist in close orbit to larger, often more-established ones, creating a kaleidoscopic structure of work that enables a variety of core and peripheral actors’ (Belair-Gagnon et al., 2019: 1). Kaleidoscopic is an apt descriptor of RightsCon and ID’s specialist multi-stakeholder structure. Liminality can be understood in another sense as well: as something mobile and dynamic. The second dictionary definition of liminal is ‘relating to a transitional or initial stage of a process’. It seems to be commonly understood in these fora (and is often repeated) that societies are at a critical juncture, as AI and machine learning develop more rapidly than human responses. The experts’ accounts are full of histories that compare ancient times (usually the 1960s or ‘70s, but also the olden days of 2012) with the present, to highlight how societies find themselves in a period of transition. As one speaker put it: ‘We face a choice. It’s the most important choice of our time. One option is to be complacent. It can be like, let’s just build machines that can do everything better than us, not worry about the consequences … On the other hand, that would be embarrassingly lame. I think we should be ambitious … and envision a truly inspiring high-tech future and figure out how to steer toward it.’ (S3). The expert activists whose discourse we analyze are liminal actors in this second sense because they are aware of this transition, in contrast to other social actors, who are not. The study concerns a discursive space that both parallels and is intertwined with the acronymic landscape of IG, in which policy discussion and standard-setting fora generally have institutionalized and codified roles, outputs and tasks. The terrain we are inter-ested in is dotted with similar features, but differs in that the people who populate it and speak at these gatherings have a footing in civil society through their participation in fora where there is a more narrow focus on discourse (some of the speakers’ policy-oriented professional roles notwithstanding). But what, more precisely, are these fora, and what are their origins? It was in response to the unrest following the 2009 Iranian election that a movement was founded to rally ‘digital activists and ordinary online citizens around the world, to assist political freedom movements and civil society who are being shut out from their rights to information, political expression and assembly protect digital rights’ (Berkman Center, 2009). It subsequently morphed into the NGO called Access Now, which works for human rights in the digital age by combining technical support with policy engagement to safeguard privacy, freedom of expression, digital security and combat net discrimination (Access, 2021). Part of its work involves convening such gatherings as the annual RightsCon, first held in 2011. The summit (as it is thought of) has enjoyed exponential growth for more than a decade, with meetings in Silicon Valley, Manila, Brussels, Toronto and Tunis. Forced online by the pandemic in 2020, the number of participants grew by 174% as compared with 2019. It is from RightsCon 2020, where 40.8% of its 7828 participants from 158 countries self-identified as members of civil society, that the primary source material for this study has been collected. Internet Days is an annual gathering hosted by The Internet Foundation (Internetstiftelsen), the Swedish organization that ad-ministers the .se and .nu domains. Referred to as ‘a purpose-driven organization’ (S2), the Foundation, like ID, functions indepen-dently, despite being under the umbrella of a state authority (the Swedish Post and Telecom Authority). As with RightsCon, ID developed significantly over the course of the two decades that followed its first iteration in 1999. At the outset, it was a conference for TelecommunicationsPolicyxxx(xxxx)xxx2A. Robertson and M. Maccarone                                                                                          ‘the industry’, with a focus on the technical aspects of the internet in a relatively lawless landscape which, according to the Foundation, was of little relevance to society at large. But in 2013, the ID organizers decided to switch its focus from technology to ways in which the internet is actually used. This put the perspective of external actors and their particular areas of expertise on centre stage. The number of participants leapt from 400 to 1500, and keynote speakers in subsequent years included such celebrities as Edward Snowden and Harper Reed. ID’s civil society focus is tempered with the Swedish tradition of folkbildning or public education (‘everyone should want, dare and be able to use the internet’, prospective attendees are told, without economic constraints interfering with that communicative right: https://internetdagarna.se/om-internetdagarna/). While it must be assumed that the larger communicative space comprised by ID (by which we mean discussions over lunch and coffee that took place during two of the years analyzed here, which were held IRL) is dominated by Sweden-based attendees, the setting was that of a global city and the core of that space, where discourse is generated, is comparable enough to RightsCon to suit the purpose of this study, we maintain. The speakers in both fora, and/or the organizations initiating and hosting their discussions, represent a broad church of politically independent experts, collaborative ventures seeking to bring together academics, governments and companies, entrepeneurs and other professionals such as lawyers, from around the world. 2.1. Myth, narrative, and socio-techno imaginaries Whenever confronted with hype, writes Verdegem in a critical take on AI, ‘it is of utmost importance to untangle what exactly is at stake and who is behind the discourses and myths created’ (Verdegem, 2021: 1). The speakers whose talk we analyze encourage their listeners to do this - to question what could be called myths in the Barthean sense, which are common-sense understandings of the way the world works that tend to circulate unexamined because ‘mythical speech’ is comprised of ‘material which has already been worked on so as to make it suitable for communication’ (Barthes, 2012/1957). A significant amount of scholarship on AI narratives relates to popular culture texts (examples include Devlin & Belton, 2020-02-21; Recchia 2020; Thompson & Graham, 2020 and Yee, 2017). Cave et al. (2020) argue that narratives of intelligent machines are fundamental to the construction of socio-technical imaginaries ‘because they form the backdrop against which AI systems are being developed, and against which these developments are interpreted and assessed’ (Cave et al., 2020: 7). Useful as these are for reifi-cation, it is problematic when fiction provides the ‘go-to’ metaphors for talking about the impact of AI in real-life contexts. In a letter to the editor of Nature, Machine, Intelligence, Isabella Hermann cautions against using science fiction narratives as a reference point for understanding discourse about the ethics, opportunities and risks of AI. Metaphors taken from SF (be it science fiction, science fantasy or speculative fiction) distract from empirically-grounded AI questions which ‘have nothing to do with humanoid robots or conscious machines’. Rather, they have to do with the implementation of ethical values such as fairness, accountability, privacy and trans-parency’ (Hermann, 2020: 654). It is thus important to be clear that the AI narratives in focus in this article originate in the realm of quotidian realities, not popular culture. But as with popular cultural narratives, the work of imagination is involved, although in this case the workers are experts from a diverse array of groups, cultures and geographies. If socio-technical imaginaries were originally conceived as forms of social life and social order reflected in the design of ‘nation- specific scientific and/or technical objects’, they are no longer confined to the state. States continue to generate narratives that contribute to such imaginaries, as will be seen, but those imaginaries also transcend national borders. They are articulated by cor-porations, social movements and professional associations on a global level, where ’technological visions enter into the assemblages of materiality, meaning, and morality that constitute robust forms of social life’ (Jasanoff, 2015:4). Corporations like Google and Microsoft propagate narratives designed to shape public understanding of AI application, benefits and risks, and narratives can in-fluence whether and how regulators decide to legislate around AI, according to Cave et al. (2020:10). Scholars from a wide spectrum of disciplines agree that narrative has the function of providing fundamental interpretive frames, helping us to organize our experiences and make the world comprehensible. Narratives not only provide us with information about the topic in question - the ones studied here are expert testimony - but also provide insights into how individuals imbue those events and actions with meaning. Collectivities and communities tell and share stories too. Cultures work ‘mentally’ in common, through a process of ‘joint narrative accrual’, according to Brunauer. Continuity is provided by a ‘constructed and shared social history in which we locate ourselves and our individual continuities’ (Bruner, 1991: 20). This means that the study of narrative is a way of gaining analytical purchase on the power dynamics that regulate understandings in society (Robertson, 2010, 2017). Narrative analysis, it has been argued, enables scholars to be more aware of and responsive to the voices of the marginalized in society, which otherwise tend to be drowned out by the powerful and the mainstream (Carlisle, 1994). By attending to stories told by individual liminal actors rather than collective actors like corporations or states, and paying attention to how they articulate the potential and the problems that accompany the inroads of AI in a variety of empirical settings worldwide, abstract socio-technical imaginaries are rendered concrete, and resistance to the narratives of the powerful, by those in the know, can be observed. The concept of AI narratives as used in this study signals an interest in gaining purchase on the positionality of the speakers and how they in turn position the ordinary technology user in the stories they tell. Our approach to this task is set out in the next section. 3. Material and method The primary source material analyzed in what follows was collected from the ‘AI and Algorithm’ track of RightsCon 2020, available in digital form, and recordings of discussions pertaining to AI from the Internet Days of 2017–19. One limitation of the research design is that it captures just a few moments in a decade of breathtaking technological development. Another is that the RightsCon and ID TelecommunicationsPolicyxxx(xxxx)xxx3A. Robertson and M. Maccarone                                                                                          samples are taken from different years. We maintain, however, that it is precisely because of the rapid progress in AI development and implementation that it can be instructive to pause and unpack one particular moment in a continually unfolding conversation. Moreover, narrative analysis attends to meaning (-making) rather than specific content. It signals an interest in how speakers un-derstand power relations that are not as fleeting as the technological developments that may cause a buzz in one year rather than another. Our ‘listening in while the experts talk’ approach bears an affinity to the fieldwork done by Graves and Anderson (2020), who studied the global fact-checking movement through observation at annual conferences of fact-checkers from around the world. The speakers comprise a diverse group of 30 expert activists from different parts of the world and fields of expertise. It was not uncommon for an individual to have more than one professional role, as can be seen from Fig. 1 and the Appendix. Audio recordings of talk in these fora were transcribed - perhaps ironically, by algorithmic software - and the transcripts checked by one of the authors, who also listened to all the audio twice. The other author then parsed the transcripts using code questions derived from previous work with narrative analysis, adapted to this particular enquiry. Working together, both authors then aggregated the answers to the code questions in a typology, using a similar approach to that employed by researchers at the Institute for Strategic Dialogue in their categorization of (false) narratives about the covid-19 pandemic (Gallagher et al., 2021:9; McIntyre, 2021). The transcripts, while complete narratives in and of themselves, are also replete with component narratives. The speakers explain and problematize by drawing on anecdotes of their own experiences, or those of colleagues. They proceed by way of exemplification and illustration, unpacking stories to show how AI and autonomous systems work, or fail to do so, and what the reasons and appropriate responses are. In empirical contexts, just as in SF imaginaries, ‘the only way to discuss highly complex computer systems and their implications is by analogies, simplifications and metaphors’, as Rehak (2021: 88) puts it. These come packaged in narrative form. Narrative analysis typically distinguishes between a referential and an evaluative clause. The orientation, comprised of the time, place, situation and participants, is set out in the referential clause, while the meaning of the action or series of events is commented on in the evaluation. The code questions used here probe both the referential and the evaluative. The narratives have also been read across the two intersecting dimensions identified by Lieblich et al. (1998): holistic versus categorical, and content versus form. Reading along the categorical-content dimensions, categories of the broad topic of AI are defined, and separate utterances of the text are extracted, classified and gathered into these categories. This mode of reading focuses on the content of narratives as manifested in separate parts of the story, whatever of the context of the complete story. The categorical-form mode of analysis focuses on discrete stylistic or linguistic characteristics of defined units of the narrative, typically asking what kind of metaphors the narrator uses. Defined instances of this nature are collected from a text or from several texts and counted, as in the categorical-content mode of reading (Lieblich et al. 1998, pp. 16–17). Two aspects of the orientation were coded for: participants and setting. Participants could be the speakers, identifying and posi-tioning themselves, or the figures in their stories. Who did the speakers distinguish as the actors involved in or impacted by AI and its governance or lack thereof, and at what level of society? The setting was operationalized as the perspective from which the story is told. Is the speaker’s position that of insider, outsider, observer or other? And what is their vantage point - that of Silicon Valley/MIT, for example, or of the global south? The next set of questions, still pertaining to the referential clause, was: what is AI, according to the speaker? And/or, what is the technology or technological situation to which the speaker refers? Given our focus on ‘talk’, one question asked whether the speaker has anything to say about the importance of discourse and dialogue (as opposed to engineering and policymaking). Related to this is the question of whether speakers refer to popular discourse in the form of popular-cultural narratives of AI or other imaginaries, and the question of ideology and myth, in the Barthean sense of the word. Are there taken-for-granted assumptions about AI and related technologies that the speaker asks the listener to question? With this, the analysis moves to the evaluative dimension, with narrative themes identified using two open-ended questions. What is the problem identified by the speaker? And what is the solution? Fig. 1. The professional roles of the speakers in the study reported here. As some of the 30 speakers worked in more than one professional category, n = 59 (the number of roles). The letters in the legend (A–G) are used in the appendix, in which the speakers are listed together with the part of the world in which they work. TelecommunicationsPolicyxxx(xxxx)xxx4A. Robertson and M. Maccarone                                                                                          4. Results The code questions structure the presentation of results in this section. It is worth noting again how diverse the voices are, despite the common themes that become discernible when listening to them, particularly when it comes to the intersection of AI and inequality. Machine learning and autonomous systems are seen as complicit in problematic stratification along social, political and economic lines. But AI can be used to solve as well as cause problems, or at least ameliorate some inequalities even if it exacerbates others. To begin understanding these problems and solutions, the orientation of the speech analyzed here is first established. 4.1. Orientation Even if we pull them apart when parsing these stories, the different components of an orientation - time, place, situation and characters - often come in a single sentence, such as this: ‘We’re at this very interesting point where all this new technology that we’re building is only going to make things worse and we’re entering this new realm and I don’t know what’s going to happen, but it’s up to us.’ (S5). Focusing first on the participants, who is the ‘us’ that is referred to in the primary source material? Due to space limitations, the answers to this and the other orientation questions are presented here in compressed form. As speakers often left it to the moderator to introduce them, more complete information is given in the Appendix, where the numbers correspond to speakers identified in this section simply as S5, S15, S25 and so on. As both Fig. 1 and the Appendix show, the speakers include people who work in both public and private sector; in policy, academic, media and activist settings. There are programmers, engineers, people working for NGOs and activists, many of whom work profes-sionally with popular communication as well as in more technical capacities. Some point out that their training makes them sceptical; others that their background makes them enthusiastic. Interestingly, some of the more successful of the self-identified ‘nerds’, who have left their basement rooms to build enterprises and lead campaigns, are keen to emphasize their roots. ‘I’m a hacker, and that really is my core identity’, says one. ‘For the most part, I’m a coder’ (S5). Another says: ‘I love hearing about long histories of geeky culture because I grew up as a geek online in the early days when it was just those of us who were self-identified geeks, freaks and queers who were online.’ (S2). These biographies are important for the ethos of the rhetoric, as it reminds the audience of the speaker’s cultural capital - of why we should listen to their story, be it of how the internet has changed since its early days of democracy, diversity and promise, or why we should be worried about those changes. ‘I came to this discussion through my own experience working at a computer vision company, where it was my job to create big piles of labelled data and feed them to the algorithms, to train them in a way that was responsible. And quite frankly, the experience that I had in watching the ways that these models fail, it guaranteed to me that we should never trust something as unpredictable and unreliable as computer vision to be a sensor that gets to determine whether somebody gets to live or die.’ (S14). While many speakers, despite their origins, speak from the vantage point of the US, Europe or the ‘developed world’ in general (even if not all are ‘a white guy from the US who looks like San Francisco’, S5), other stories are told from the perspective of the Global South. The stories told by the speakers in the sample are populated by others who comprise a rich and varied cast of characters. They are, unsurprisingly, tech workers - engineers, programmers, systems architects and designers - who tend to be portrayed as cogs in the machine (‘I’ve never met an engineer at Facebook Twitter, YouTube, who’s just like, let’s kill democracy’ says S2). Several talk about the state, politicians and lobbyists (S5, S12, S14, S25, S26, S27) and/or the military and defence contractors (S4, S11, S12, S13, S14). Most make reference to economic actors, be they proverbial villains (Big Tech or business executives who pretend to be in favour of creative disruption, but are not really open to change ‘because business as usual pleases the shareholders every three months’ - S1) or role models. The latter include entrepreneurs who are ‘crazy high-risk people’, as S1 affectionately puts it, or a Peruvian who grew up in America, got rich, went back and started funding schools so he could get people who could work in tech, or ‘visionaries’ such as the leader of Google DeepMind, the inventor of Skype, Elon Musk and others ‘who are not just crazy philosophers’ (S3). Several tell stories involving AI researchers and other thinkers (who have tended to be ‘a lot of old white guys’, in the account of S4). The cast includes civil society (S16, S17, S25, S26, S27) or as S3 puts it, people who are here ‘because we’re excited about the future’. Importantly, it includes ordinary people. These who play the role of the extras in stories told by most speakers, but are occasionally cast as the main character in narratives that focus on how human behaviour drives change. 4.2. What is AI? What is the technology that is the lynchpin of these narratives? Perhaps surprisingly, given the widespread expertise in the sample, there is a fuzziness about the key concept (‘Artificial Intelligence - whatever that means’, is how S6 puts it). This lack of clarity is something several speakers remark on and highlight as a problem. ‘AI’s become a little bit of a buzzword. Everybody seems to be working on it, everybody seems to be talking about it’, says S10. Some speakers talk about AI as a good. Endless computing power and sophisticated machine learning is going to solve lots of problems, once we rethink and overcome the biases. ‘And the great news is it doesn’t have to be the people who are dominating today who will lead us to the future.’ (S1) Another says AI accomplishes complex goals and has developed from being an academic exercise into something that can save lives (S3). Several talk about AI in neutral terms, for example as ‘a general purpose tool that we can put into any kind of system to solve a problem’ (S12) and emphasize that it is not as complicated as it is often made out to be. Rather than a TelecommunicationsPolicyxxx(xxxx)xxx5Table 1 Narrative themes: problems distributed according to speaker/concept category.. A.RobertsonandM.MaccaroneTelecommunicationsPolicyxxx(xxxx)xxx6                                                                                            A. Robertson and M. Maccarone                                                                                          mysterious technology, AI is a field, like physics (S7). ‘AI is really simple, right? It’s just machine learning and neural networks, which is just math. And the thing about math...is it’s absolutely not new … [AI is] becoming the standard tool. It’s becoming the tool, the foundation, many of our companies are building, many companies are betting on this’ (S5). But the audience can be left in little doubt that AI - whether those two letters are used to represent a technology, a field, or a societal development - is something of great importance. In some accounts, this is made explicit. AI, says S15, is a transformative force that ‘has resulted in a fundamental shift around how we work, how we interact, how we transact’. For S17 it is ‘the next critical frontier of the modern industrial paradigm as we know it’ and will determine the future course of human development. 4.3. The problem of myth As mentioned above, Verdegem’s insistence on the importance of calling myths about AI for what they are resonates with talk in these fora. One of those myths is that AI is mystical, and concern is expressed that many people seem to think AI ‘means doing magic with data’ (S2). Even where there is not mystification, there is confusion: the general public seems to think AI is a specific technology - whatever ‘happens to be the flavour of the day’ - or confuses specific theories with multiple applications, like Big Data and Deep Learning, thinking mistakenly that they are ‘the be all and end all of AI’ (S7). There are also myths to be debunked about AI and humans. One is that there is nothing humans can do that AI can’t. ‘If you ask Google, AI invents’ (S4). A related myth is that AI is neutral and abstract, and less fallible than humans, who have prejudices. There is a ‘fallacy that sees AI solely as computer science systems, where the whole discipline thinks in terms of abstraction - seeing fairness as a property of the algorithm rather than ‘a property of the entire end-to-end system which is the people, the institutions, the laws, the context, the language’. (S9) Several speakers point out that people seem to think that data can speak for themselves and that tech-nologies can make people smarter and do more ethical decision making than humans; that it is only people ‘who don’t work in AI, who’ve never trained a model, who try to make claims like tech is neutral, or AI can be less biased than a human’, as S14 puts it. Another myth is that tech is automatically beneficial; that ‘AI will solve all our problems … AI has discovered a new drug; AI can tell you what you’re really feeling; AI can write amazing content’ (S6). Related to this is the myth - or what S18 calls hype - that AI has to be used to solve the big problems. We have to ‘escape these universal narratives that try to include us all in a rush into neoliberal development in which data-intensive AI solutions are the only answers to the problems are not sure we have in the end’, warns S16. In the Global South, it is governments rather than corporations that are ‘pushing narratives’ that need to be resisted about the necessity of AI to nation-building. ‘That is a narrative that is being sold to the population. That we need AI to be able to secure our economy to be able to leap-frog kind of development challenges’ (S9). Another myth is that the regulation of AI will ‘kill all the good innovation’ (S6) which works in tandem with the myth that AI and related technology are neutral and not dangerous. There’s this sort of myth of anonymity. “Oh, it’s aggregated collective data.”’ It’s possible to come up with ‘20 examples of how quick and easy it is to actually de-anonymise and actually re-identify people. Even with all the differential privacy and all the tools you throw at it. There are ways that’s it’s pretty possible to actually get very deep sensitive information out of data sets.’ (S8). Most nefarious of all, perhaps, is the myth that the personal data on which AI technologies are built is a commodity to be bought and sold. This is a ‘legal fiction’ that disguises commercial decisions taken by Google and Facebook ‘to commodify something that wasn’t a commodity before.’ (S21). Identifying something as a myth means naming a problem, and some have already been alluded to above. But there are others. 4.4. Narrative themes: problems The AI-related problems that recur in the narratives circulating in these communicative spaces are too numerous to itemize here. They have been grouped in sub-themes in Table 1. Some of these, however, are especially prominent or relevant to the topic of unequal conditions, and the language in which these narrative themes are expressed merits a closer look. 4.4.1. AI is being developed for the wrong purposes (S1, 2, 4, 11, 14) In the early days of the internet, one speaker recalls, folks wanted to build technologies to fix things. ‘The Silicon Valley mantra ‘move fast and break things’ was a proud one for many of them because we wanted to break things that we saw as broken. Never in our wildest dreams did we imagine that we were going to be breaking the social structures that really mattered. We didn’t imagine that we could break democracy. We didn’t imagine that we could break social cohesion. And yet here we are’ (S2). Another speaker describes the problem as being that AI is not being used to build things that are really useful, with a lot of venture capital going into ‘bullshit innovations’. Were it not for these distractions, some big problems could be solved. ‘The start-up culture is mostly slapping each other on the back, encouraging each other’s reality distortion’ (S1). Others call attention to the connection between defence funding and AI development, and express concern about the development whereby autonomous systems ‘delegate life and death decisions to machines, programmes and algorithms’, crossing ethical red lines, contravening international law designed to protect civilians, and potentially destabilizing on global security (S11). In other words, several speakers express concern about what happens when human agency is removed from the equation. The development of killer robots means we will see a lot more accidents and see a lot more incidents of people being wrongly killed … These weapons will be not just used for purposes of war, but will sort of leak into domestic policing all over the world. Here in the US as we see the civil unrest with the Black Lives Matter movement, we saw for the first time ever a predator drone deployed over US soil. (S14). TelecommunicationsPolicyxxx(xxxx)xxx7A. Robertson and M. Maccarone                                                                                          4.4.2. AI was developed for the right purposes but hijacked (S5, 11, 14) The killer robots mentioned above are in some cases developed by programmers who do not know what they are working on until too late. ‘Technology designed for one specific use is used for something else without the knowledge of the programmers or developers’ (S11). Even something that for long have been considered less scary - recommendation engines - were developed to enhance user experience but ‘are leading us normal folks down a path of radicalization’, according to one speaker. Work done in ‘the old days’ of 2012 and 2014 is something tech workers were proud of, but it was weaponized. ‘We did not intend this whatsoever to happen’. The unintended consequences of ‘inventions of hope’ is that they have been turned against ‘us’. (S5). 4.4.3. Tech companies/business interests are the problem (S9, 11, 12, 15, 16, 17, 19) Big Tech is ‘amassing unimaginable amounts of data, establishing control of the critical infrastructures’, with the majority of the world’s distributed data being held by a very few private hands. Amazon, Google and Microsoft ‘will have the unbeatable advantage in finding the next generation of AI solutions’ (S17). The price on ‘my data, my personal data, which has most value to myself’ is set by companies that are commercializing it (S24). We need to stop ‘big tech from writing a new constitution for the global economy through the WTO and other trade agreements that would allow them to rig the rules even further to accumulate even more wealth and power’ (S15). If we allow everything to come into the private sphere, ‘we have a different experience of the world. There’s now a profit motive in our entire lived experience’ (S21). However, this increase in power is not solely in technical arenas. Speakers identify similar problems in the political sphere, albeit with their own unique set of complexities. 4.4.4. Data/AI has become a tool of political power (S2, 5, 7, 8, 9, 11, 12, 13, 14, 17, 18, 21, 25) States, governments, and some political forces are cast as the villains in many of these narratives, alongside those without corporate power. Algorithmic power is seen as tied to other forms of infrastructural and even coercive power (S18). In the US, lawmakers are old and typically white and don’t know about the tech. They’re also smart, so they know they don’t know, which means they are going to ask for help. And I think the first person that’s going to raise their hand are the lobbyists. And so that means that they’re going to outsource their technology to people who have a very strong interest that they are fighting for. (S5). There is ‘a growing appetite amongst governments around the world to collect more data about where individuals are going and how they’re getting there’ (S25). In the Global South, rulers use their authority to propagate nation-building narratives to justify the use of AI for surveillance and control of impoverished populations. They are ‘trying to make the individual completely naked in the eyes of the government’ (S8). In India, the narrative is that the personal data of citizens belongs to the state. In Africa, where there has been a ‘sort of leapfrogging’ in terms of technological development, ‘reality is being platformized’. Governments - both domestic and foreign (in the case of China) - are using facial recognition and other technology in the interest of retaining and expanding their power and controlling populations. These developments are closely connected to the next sub-theme. 4.4.5. Inequality/AI exacerbates inequality (S5, 6, 8, 9, 11, 13, 14, 15, 16, 17, 18, 20, 21, 25, 26, 27) The relationship between AI and inequality is a problem highlighted by speaker after speaker. In the developed world, there is a growing divide between those who can afford the good things offered by AI, and those who can’t. Those with the authority and agency to use AI to their advantage. As S5 puts it, I think that we’re going to have two worlds, people who can afford automation, like driving a Tesla or self-driving car, the fancy Volvo, et cetera, and the people who cannot. And so suddenly safety - not getting killed by the robots - becomes a luxury item. (S5) The world is bifurcated along racial as well as economic lines. Concern is expressed by several speakers about the development of systems which will be used against marginalized communities. ‘Emerging digital technologies often exacerbate and compound existing inequities, many of which exist along racial, ethnic and national origin grounds’ is how one puts it (S11). But there is also the problem of inequality in a global perspective, with technology not only being developed but regulated with the rich North and West as the point of departure, to the detriment of the Global South. There is a bundle of intersecting problems ‘all connected through the pipeline of data, through the funnelling of wealth down from the higher echelons of the software community in Silicon Valley, into the Global South’ (S14). AI technologies act as a key driver of ‘the emergent, platform-based economic order that intensifies an already unequal and really unfair international development context’ (S15). We have a global policy regime which ‘has really impoverished the data capabilities of nations in the developing world through an unquestioned and uncritical push for free data flows … We see gig workers who relentlessly and endlessly serve this algorithm that mines data from them, gains them with rewards and punishments, and atomizes them so thoroughly that they can only see themselves as cogs in this vast machine’ (S17). In addition to inequalities between the rich and poor and between people of different colours in domestic settings, and inequalities between societies (the West and the Rest), for some speakers it comes down to a matter of a divide between global elites, and the victims worldwide - regardless of colour, gender or ethnicity - of the global financial crisis and of a new market regime built on new sources of digital data and artificial intelligence. ‘Data is an intermediary step towards old fashioned wealth accumulation, accu-mulation that is highly unequal and often lines the pockets of foreign investors and corporations’ (S18). This brings us to the important sub-theme of communication and other human rights. 4.4.6. Communication (and other human) rights are threatened; data ownership (S2, 5, 7, 8, 9, 10, 11, 13, 14, 19, 21, 22, 26, 27, 28, 29) The right to be informed, the right to inform, the right to privacy and the right to participate in public communication is enshrined at the global level by UNESCO resolutions and in particular in Article 19 (UNESCO, 1981: 265; Pohle, 2018). ‘The right to commu-nicate is a fundamental human right that underpins the very essence of democracy, and it is a key factor in the fulfillment of other TelecommunicationsPolicyxxx(xxxx)xxx8Table 2 Narrative themes: solutions distributed according to speaker/concept category.. A.RobertsonandM.MaccaroneTelecommunicationsPolicyxxx(xxxx)xxx9                                                                                            A. Robertson and M. Maccarone                                                                                          rights’, as it puts it (https://www.unesco.org/en/communication-information/right-information). AI technologies are depicted in many of these narratives as giving rise to misinformation rather than information. High school students, in one example, might find themselves introduced to a notion like social justice or intersectionality, and use a search engine to figure out what it is. They ‘throw it into YouTube - YouTube is the primary search engine for under-25s. What you get is these videos that are actually designed to push you towards a very specific agenda that suggests that these concepts are not part of a broader social set of issues … they are something that is meant to oppress you in different ways. (S2). People’s data is used without their permission to weaponize social media and feed them misleading information (S5). Journalism and freedom of information are imperilled (S8). AI is implicated in what one speaker refers to as ‘information disorder’ and identified as ‘one of the most serious existential risks that we’re facing in the AI era’. It sets the mindset for entire countries and creates both local and geopolitical polarization. ‘One of the biggest challenges that we’re tackling today is the effect of the amplification of AI. The exponential amplification of information circulation and ranking, prioritisation, and the way that is preying upon humans’ unconscious … Information disorder is preying upon the fact that triggering [the hundreds of biases that are unconscious to us] is extremely profitable for the corporation and also for the politicians’ (S7). When it comes to the right to privacy, the state is using AI tech for surveillance purposes, ‘to make the citizen as visible as possible’ in places like India (S9). Personal data are being used by police and the military with ‘obvious implications on the right to life, on the right to peaceful assembly, the rights to privacy, the rights to non-discrimination’ (S11). AI-related technologies like computer vision computer are about mass surveillance, the consequence of which is a ‘disproportionate violation of rights. Disproportionate surveil-lance of underrepresented marginalized communities - the African-American community, the Muslim community here in the United States as well’ (S14). At the heart of the rights discussion is the question of data ownership. While the consensus in these communicative spaces is that data should not be owned by the state or by corporations, there is less agreement as to whether it should be considered the property of the individual or whether it is a collective good. A fault line is discernible between Western conceptions of property rights and the view from the Global South, but the discussion is too nuanced and too interesting to be compressed into this presentation of results, and must be explored in another paper. 4.5. Narrative themes: solutions The other overarching narrative theme relates to solutions that emerge from these AI narratives - the question of ‘what can be done to preserve that space where people are able to push back against some of the excesses that we’re seeing’ as S10 put it. Here too, a number of sub-themes can be discerned, as can be seen from Table 2. Perhaps unsurprisingly, given the prominence of the sub-themes of inequality and rights under the heading of problems, solutions relating to regulation, data ownership and the retention of human control over autonomous systems account for the lion’s share of solutions in this discourse (S12, S13, S14, S16, S19, S20, S21, S22). There are calls to regulate the entire life-cycle of AI systems in the same way that food and drugs are regulated (S12), to agree on rules and frameworks ‘that firmly put ownership of data into people’s hands’ and create ‘a common space regime where you regulate access to data’ (S18); rules that ensure everyone ‘can enjoy all of the benefits that AI stands to bring to the world’ (S14). What is needed are policies and processes in place that allow civil society to intervene, whether it’s about framing rules about personal protection, whether it’s about getting to the sensitive issues. (S17). But the emphasis is as much - if not more so - on the raising of consciousness and knowledge levels; on the development and maintenance of the sort of socio-techno imaginaries and mindsets that are a precondition for a more just and equitable order, in which communication rights are respected and safeguarded and inequities are dismantled rather than exacerbated. There are calls for ‘fresh’ and ‘different’ thinking, more scientific thinking (S1, S2, S3, S4, S7, S16), and for demystification: ‘busting myths and correcting misconceptions actually gives us more agency in relation to a technology that we find increasingly used in sensitive aspects of our lives’ (S6). People need to become more digitally literate (S22) and to be made aware of the power grasps and political and cultural agendas in play (S2, S12, S13, S22, S24, S29). Despite the insider knowledge of disheartening developments and the problematic uses to which AI is put, there is a noteworthy thread of hope that runs through these stories, not all of which are dystopic. As one speaker put it, if we can think hard about what kind of future really inspires us, and think hard about how we can learn to steer our technology to take us in that direction, then I think we can look forward to an absolutely amazing future where the poor are richer, the rich are richer … everybody is better off both on Earth and maybe one day even elsewhere in the cosmos. (S3). 5. Conclusion We set out to analyze AI narratives circulating in a communicative space of expertise and activism that straddles the boundaries of the spheres of the powerful (states and corporations and others able to influence the regulation of technology) and the private spheres of the citizen (or consumer, or internet-user, or member of society). The extent to which the realm of the private has been colonized by the realm of the powerful is a recurrent theme in the narratives. We undertook this foray because we are interested in gaining a better understanding of the communicative dimensions of inequality, which in the digital age has led us to the discourse on AI - an abbre-viation for technological developments that are understood in different ways by different actors. Such discourse is itself a sort of regulation. ‘Meanings regulate and organize conduct and practices’, writes Thompson (1997:1), so the regulation of information flows is not just a matter of specific policies, but larger ‘struggles over meanings and interpretations’. AI discourse thus refers more generally to how autonomous systems and machine learning have become imbricated in communication between humans, and how that TelecommunicationsPolicyxxx(xxxx)xxx10A. Robertson and M. Maccarone                                                                                          behaviour is being commodified in ways that impact on our communication rights. As discussed in section 3, Cave et al. (2020) and Hermann (2020) have expressed concern that fictional narratives from the realm of SF shape the socio-techno imaginaries that nourish understandings of AI and, in turn, influence attempts by social and political actors to regulate it. Our findings show that expert activists are aware of this problem, rather than susceptible to it. They warn against mystification and myth, and their stories are replete with concrete examples of how AI works in practice, rather than SF allusions and metaphors. Throughout this text, we have written of our interest in a communicative space that is global in nature, in the circulation of narratives, and in socio-techno imaginaries. These are large things. But what we have been able to access is, at best, moments of talk and a tiny population of the many speakers who discuss, and tell stories about AI and unequal conditions. A focus on liminal actors is fruitful, in our view, precisely because they straddle different fields and have the capacity to consider the technoscape from several perspectives. But while influential in their particular realms and in possession of various sorts of capital, liminal actors are by definition in a different position from the political actors who legislate in matters pertaining to AI, or the corporations who regulate its devel-opment and use in other ways. It could be argued that the relationship between AI and inequality could be better studied by analyzing the discourse of policy-makers (were it possible to access their off-stage discussions) or, conversely, listen to the people who suffer unequal conditions throughout the world. This is an important limitation of the research reported here. On a more prosaic level, the critical reader might have well-founded concerns about why we have attended to the voices that we have, and highlighted some passages of their stories rather than others. Narrative analysis is a useful tool for parsing stories, but not necessarily for accessing the realities that underlie them. Without downplaying such limitations, we maintain that liminality is a good place to start (we have no pretensions about having done more than that), and that the expert discourse in these liminal spaces sheds light on bigger issues. And despite its constraints, the study has generated answers to the questions we set out to pursue. The answer to our first research question is that the AI narratives discerned in the talk at these fora are critical in nature, in that they take multiple perspectives and understandings into account. Being critical means resisting the impetus of rapid, and in some cases unfettered, technological advancement, and stopping to take stock of what Oliveira (2017) calls ‘the unfolding of history’. But it also means pushing back against the apocalyptic AI narratives familiar from popular culture, which risk propagating a myth of the inevitable relegation of human agency to decisions made by machines. The speakers are clear not only about who or what is at the root of problems they highlight, but also about who or what could provide solutions. AI has a direct bearing on inequality, both social and communicative, when it breaks things rather than fixes them (the story of killer robots being perhaps the most chilling example) and when it is deployed for unintended purposes (with ‘normal’ tech folks becoming implicated in radicalization when they thought they were supposed to be enhancing user experience). The ways it simplifies peoples’ lives comes at the cost of violations of the right to privacy, be it by political actors intent on ‘making the individual completely naked in the eyes of the government’ in one story, or, in others, corporate actors ‘writing a new constitution for the global economy’ and creating different worlds for those who can afford automation and drive a Tesla, and those who are victims of it, like the poor. There is a pattern in these narratives of AI compounding, rather than ameliorating, racial, ethnic and geographical inequities. In seeking answers to the second and third research questions, however, we find that these expert activists are not all on the same page. While some speakers see individual data ownership as a solution to inequalities, others talk about individualistic understandings of data ownership as being at the very heart of the existence and perpetuation of such inequities. The discrepancy between the view from the global South and the individualistic perspective of the North/Silicon Valley can be assumed to complicate internet governance. Despite the global setting, global frame of reference and global nature of the discourse analyzed here (and, of course, the border- transcending nature of the technology and systems in focus), the empirically-grounded stories of the expert activists highlight how states remain important actors and are, in some cases, using AI to increase their power at the expense of their citizens’ rights to privacy. Other stories emphasize the impact of AI at the local or community level, not least on marginalized groups or minorities. The im-plications for internet governance are that policy-makers must think small, as well as big. Regulators and institutions tend to couch their objectives and strategies in universal terms: UNESCO’s 2022 conference on Artificial Intelligence, e-Governance and Access to Information falls under the heading of ‘Universal Access to Information’, for example (https://www.unesco.org/en/days/universal- access-information-day). The empirically-grounded experiences of the expert activists in this study lead them to caution that we need to ‘escape these universal narratives’, as reported above. This study of the way AI is represented by liminal actors in a communicative space that is, on the one hand, characterized by rapid change and, on the other, delineated by enduring power disparities, is both limited and preliminary. A sample that included more years and different fora would naturally benefit a wider understanding of the socio-technico imaginaries of a category of actors - expert activists - who merit more attention than they have been paid by scholars to date. Their expertise grants them authority, and the stories they tell speak of agency. The voices attended to here tell us that there is much to be done, but that it can and should be done by human regulators and not just machines that learn fast. As one put it, the work to be done involves thinking hard about inspiring futures, and about how to steer our technology towards them. Appendix. Speakers Categories per speaker: (1) Where they work (continent or global). (2) Type of profession [A] Media, [B] Academy, [C] NGO, [D] Government/Policy affiliation [E] Activist [F] Expert (Organization), TelecommunicationsPolicyxxx(xxxx)xxx11A. Robertson and M. Maccarone                                                                                          [G] Private Sector. 1. Speaker 1 - Europe, [A][F][G]. 2. Speaker 2 - North America, [G][F][C][B]. 3. Speaker 3, Europe & North America, [B][F]. 4. Speaker 4, North America [G]. 5. Speaker 5, North America, [F] [G] [D]. 6. Speaker 6, Europe, [C]. 7. Speaker 7, Asia, [B]. 8. Speaker 8, Asia, [F][C]. 9. Speaker 9, Asia, [F][C]. 10. Speaker 10, Europe, [F][C][B]. 11. Speaker 11, Global, [F][C]. 12. Speaker 12, South America [B]. 13. Speaker 13, Global, [B][F]. 14. Speaker 14, North America[C][F][G]. 15. Speaker 15, Global, [C][E]. 16. Speaker 16, South America [C][F]. 17. Speaker 17, Asia, [C][F]. 18. Speaker 18, Europe, [B]. 19. Speaker 19, Europe, [C][F]. 20. Speaker 20, Global, [C][F]. 21. Speaker 21, North America[B][C][F][G]. 22. Speaker 22, North America, [E][F]. 23. Speaker 23, Africa, [D][F]. 24. Speaker 24, Asia, [E][F]. 25. Speaker 25, North America, [G]. 26. Speaker 26, South America, [G]. 27. Speaker 27, North America, [C][E][F]. 28. Speaker 28, North America, [B]. 29. Speaker 29, North America[C][D]. 30. Speaker 30, Europe (failed to connect) [B]. References Access, N. (2021). Access Now defends and extends the digital rights of users at risk around the world. https://www.accessnow.org/about-us/. (Accessed 16 October 2021) accessed. Barthes, R. (2012/1957). Mythologies. New York: Hill and Wang. Belair-Gagnon, V., Holton, A. E., & Westlund, O. (2019). ‘Space for the Liminal’, Media and Communication, 7(4), 1–7. Berkman Center. (2009). #iranelection: The digital media response to the 2009 Iranian election. https://cyber.harvard.edu/events/luncheon/2009/11/iranelection. (Accessed 16 October 2021) accessed. Bruner, J. (1991). The narrative construction of reality. Critical Inquiry, 18, 1–21. Carlisle, J. (1994). Introduction. In R. Schwarz (Ed.), Narrative and cultureJanice Carlisle and Daniel (pp. 1–12). London: University of Georgia Press. Cave, S., Dihal, K., & Dillon, S. (2020). AI narratives. A history of imaginative thinking about intelligent machines. Oxford University Press. Couldry, N., et al. (2018). Inequality and communication struggles in digital times: A global report on communication for social progress. University of Pennsylvania Scholarly Commons: Center for Advanced Research in Global Communication (CARGC) Strategic Documents https://repository.upenn.edu/cargc_strategicdocuments/1/. (Accessed 16 September 2021) accessed. Devlin, K., & Belton, O. (2020-02-21). The measure of a woman: Fembots, fact and fiction. In AI narratives: A history of imaginative thinking about intelligent machines. Oxford University Press. Gallagher, A., Hart, M., & O’Connor, C. (2021). Ill advice: A case study in facebook’s failure to tackle COVID-19 disinformation. Institute for Strategic Dialogue. https:// www.isdglobal.org/wp-content/uploads/2021/10/Ill-Advice_v3.pdf. (Accessed 21 October 2021) accessed. Graves, L., & Anderson, C. W. (2020). Discipline and promote: Building infrastructure and managing algorithms in a “structured journalism” project by professional fact-checking groups. New Media & Society, 22(2), 342–360. Hermann, I. (2020). Beware of fictional AI narratives. Nature Machine Intelligence, 2(11), 654. Internetstiftelsen. (2019). Från doldis till Internetstiftelsen med hela svenska folket. https://internetstiftelsen.se/nyheter/internetstiftelsen-en-doldis-mitt-i- internetsverige/. (Accessed 16 October 2021) accessed. Jasanoff, S. (2015). Future imperfect: Science, technology, and the imagin- ations of modernity. In S. Jasanoff, & S.-H. Kim (Eds.), Dreamscapes of modernity: Sociotechnical imaginaries and the fabrication of power (pp. 1–33). Chicago: University of Chicago Press. Lieblich, A., et al. (1998). Narrative research: Reading, analysis and interpretation. Thousand Oaks, CA, and London: Sage. McIntyre, M. (2021). Group that spread false Covid claims doubled Facebook interactions in six months. The Guardian. https://www.theguardian.com/technology/2021/ oct/21/group-that-spread-false-covid-claims-doubled-facebook-interactions-in-six-months. (Accessed 21 October 2021) accessed. Oliveira, A. (2017). The year in apocalypses | hazlitt. Hazlitt. https://hazlitt.net/feature/year-apocalypses. Pohle, J. (2018). The internet as a global good: UNESCO’s attempt to negotiate an international framework for universal access to cyberspace. International Communication Gazette. https://doi.org/10.1177/1748048518757140 Recchia, G. (2020-02-21). The fall and rise of AI: Investigating AI narratives with computational methods. In AI narratives: A history of imaginative thinking about intelligent machines. Oxford University Press. TelecommunicationsPolicyxxx(xxxx)xxx12A. Robertson and M. Maccarone                                                                                          Rehak, R. (2021). the language labyrinth and constructive critique on the terminology used in AI discourse. In P. Verdegem (Ed.), AI for everyone? Critical perspectives. London: University of Westminster. Robertson, A. (2010). Mediated cosmopolitanism. Cambridge: Polity. Robertson, A. (2017). narrative analysis. In K. Bor´eus, & G. Bergstr¨om (Eds.), Textual meaning and power (pp. 122–145). London: Sage. Thompson, K. (1997). The media and cultural regulation. London: Sage/Open University. Thompson, T., & Graham, B. (2020). More-than-human approach to researching AI at work: Alternative narratives for AI and networked learning. In M. De Laat, T. Ryberg, N. Bonderup Dohn, S. B. Hansen, & J. J. Hansen (Eds.), Networked learning 2020 online. Kolding (pp. 293–300). Denmark: Aalborg University. UNESCO. (1981). Many voices, one world. Towards a new more just and more efficient world information and communication order. London: Kogan Page, Unipub and UNESCO. Verdegem, P. (2021). Introduction: Why we need critical perspectives on AI. In P. Verdegem (Ed.), AI for everyone? Critical perspectives. London: University of Westminster. Yee, S. (2017). You bet she can fuck” – trends in female AI narratives within mainstream cinema: Ex machina and her. Ekphrasis. Images, Cinema, Theory, Media, 17(1), 85–98. TelecommunicationsPolicyxxx(xxxx)xxx13