Artificial Intelligence 172 (2008) 1119–1157www.elsevier.com/locate/artintAlternating-offers bargaining with one-sided uncertain deadlines:an efficient algorithmNicola Gatti ∗, Francesco Di Giunta, Stefano MarinoArtificial Intelligence and Robotics Laboratory, Dipartimento di Elettronica e Informazione, Politecnico di Milano,Piazza Leonardo da Vinci 32, 20133 Milano, ItalyReceived 4 February 2007; received in revised form 9 October 2007; accepted 17 November 2007Available online 4 March 2008AbstractIn the arena of automated negotiations we focus on the principal negotiation protocol in bilateral settings, i.e. the alternating-offers protocol. In the scientific community it is common the idea that bargaining in the alternating-offers protocol will play acrucial role in the automation of electronic transactions. Notwithstanding its prominence, literature does not present a satisfactorysolution to the alternating-offers protocol in real-world settings, e.g. in presence of uncertainty. In this paper we game theoreticallyanalyze this negotiation problem with one-sided uncertain deadlines and we provide an efficient solving algorithm. Specifically,we analyze the situation where the values of the parameters of the buyer are uncertain to the seller, whereas the parameters of theseller are common knowledge (the analysis of the reverse situation is analogous). In this particular situation the results present inliterature are not satisfactory, since they do not assure the existence of an equilibrium for every value of the parameters. From ourgame theoretical analysis we find two choice rules that apply an action and a probability distribution over the actions, respectively,to every time point and we find the conditions on the parameters such that each choice rule can be singularly employed to producean equilibrium. These conditions are mutually exclusive. We show that it is always possible to produce an equilibrium where theactions, at any single time point, are those prescribed either by the first choice rule or by the second one. We exploit this resultfor developing a solving algorithm. The proposed algorithm works backward by computing the equilibrium from the last possibledeadline of the bargaining to the initial time point and by applying at each time point the actions prescribed by the choice rule whoseconditions are satisfied. The computational complexity of the proposed algorithm is asymptotically independent of the number oftypes of the player whose deadline is uncertain. With linear utility functions, it is O(m · T ) where m is the number of the issuesand T is the length of the bargaining.© 2007 Elsevier B.V. All rights reserved.Keywords: Automated negotiations; Game theory; Multiagent systems1. IntroductionAutomated negotiation is a promising scenario of computer science where artificial intelligence can play a crucialrole: it can automate software agents allowing them to negotiate each other on behalf of users for buying and selling* Corresponding author. Tel.: +39 02 2399 3658; fax: +39 02 2399 3411.E-mail address: ngatti@elet.polimi.it (N. Gatti).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.11.0071120N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157items [21]. This automation, as stated in literature, can lead to more effective negotiations since software agents workfaster than humans and are more prone in finding efficient agreements [35].Several negotiation settings can be found in the electronic commerce arena. The most common ones are usuallybilateral: a buyer and a seller negotiate a contract over one or more issues. In this paper we consider the principalsetting in bilateral negotiations: the bargaining [28]. In a bargaining two agents must reach an agreement regardinghow to distribute objects or a monetary amount and each player prefers to reach an agreement, rather than abstainfrom doing so; however, each agent prefers that agreement which most favors her interests. A real-world example thatdepicts this situation is a negotiation between a service provider and a customer over the price and the quality level ofa service.Classically, the study of bargaining is carried out employing game-theoretical tools [28,30] wherein one distin-guishes the negotiation protocol and the negotiation strategies: the protocol sets the negotiation rules, specifyingwhich actions are allowed and when [32]; the strategies define the behavior of an agent in any possible agent’s deci-sion node. Strategies can be pure or mixed.1 For any decision node of the game a pure strategy prescribes one action;a mixed strategy prescribes probability distributions over the actions. Given a protocol, the game-theoretical approachpostulates that rational agents should employ strategies that maximize their payoffs [29]. In this paper we a prioriassume agents to be rational; such an assumption will be supported a posteriori by the results provided in the paper.Indeed, we will show that the problem of computing a solution with rational agents is tractable.The principal protocol for bilateral bargaining, the alternating-offers protocol, pioneered by Ståhl in [37], hasreached an outstanding place in literature thanks to Rubinstein in [33]. It is considered to be the most satisfactorymodel of bargaining present in literature. Basically, an agent starts by offering a value for the issue under dispute(e.g., a price) to her opponent. The opponent can accept the offer or make a counteroffer. If a counteroffer is made,the process is repeated until one of the agents accepts. Rubinstein’s alternating-offers model is not accurate enoughto capture all the aspects involved in the electronic commercial transactions, where, typically, agents have reservationvalues and deadlines, negotiate over multiple issues, and have uncertain information. Therefore, refinements andextensions of [33] are commonly employed in computer science community to provide a more satisfactory model [10].Examples of real-world applications that employ bargaining techniques can be found in [1,2,24,26,31].The solution of the classic Rubinstein’s protocol is well known in the literature [28]. On the contrary, the study ofthe alternating-offers protocol in presence of extensions and refinements is hard and still open. Specifically, the twocrucial problems concern the development of algorithmic techniques to find equilibria in presence of issue multiplic-ity and information incompleteness. The problem of bargaining efficiently over multiple issues when information iscomplete has been recently addressed in [6,7] and refined in [12]. The equilibrium strategies can be easily computedby extending the classic backward induction method [14]. The computational complexity is O(m · T ), where m is thenumber of issues and T is the length of the bargaining. In presence of incomplete information, it is customary in gametheory to introduce probability distributions over the parameters that are not known by the agents. Notwithstanding,the analysis of bargaining with uncertain information is currently more a series of examples than a coherent set ofresults. Game theory provides an appropriate solution concept for extensive-form games with uncertain information,i.e., the sequential equilibrium [22], but no solving technique to find it. We recall that the backward induction methodcan be employed with success exclusively in presence of complete information [14]. Moreover, economic studiesonly provide equilibria in very narrow settings of information uncertainty, focusing mainly on discount factors andreservation values. For instance, in [34] Rubinstein analyzes a scenario with uncertainty over two possible discountfactors of one of the two agents, while in [3] Chatterjee and Samuelson analyze a scenario with uncertainty over thereservation values of both the buyer and the seller, where each player can be of two types. An interested reader canfind an exhaustive survey on bargaining with uncertain information in [4].The employment of the alternating-offers protocol in electronic commerce has put attention on the role of thedeadlines in the negotiation. The infinite horizon assumption, which is usually made in game theory literature, isnot realistic in real-world applications [36]. Furthermore, agents’ deadlines are usually uncertain, not being knowna priori by the agents themselves. Notwithstanding the importance of uncertain deadlines in negotiations, only fewworks have deeply analyzed their effects in the alternating-offers protocol with discount factors, discrete time, andrational agents, and no satisfactory solution is currently known. This prevents the employment of autonomous rational1 For the sake of simplicity, we use in the paper, as Kreps and Wilson in [22], the term “strategies” in the place of appropriate game theoreticalterm “behavioral strategies”.N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571121agents in real-world applications and pushes scientific community to develop solutions for this bargaining problem.Classic results concerning the presence of deadlines in bargaining are the followings. In [25] Ma and Manove considera complete information finite horizon alternating-offers model without temporal discounting with continuous time andplayers’ option of strategic delay. In [13] Fershtman and Seidmann study a complete information bargaining modelwith random proposer and a deadline. In [16] Gneezy et al. study a variation of the ultimatum game. In [36] Sandholmand Vulkan analyze a slight variation of the war-of-attrition game: the surplus can be divided, time is continuous, thedeadlines are uncertain, and there are not discount factors. In [10] Fatima et al. study the alternating-offers protocolwith uncertainty over deadlines and reservation values in presence of bounded rational agents: more precisely, agentsmust employ predefined bidding tactics based on the negotiation decision functions paradigm [8]. Only recently someattempts have been made in computer science community to achieve the solution of the alternating-offers protocolwith uncertain deadlines and rational agents. The first attempt is by Fatima et al. in [11], where they present analgorithm that produces equilibrium strategies in presence of two-sided uncertainty. Their algorithm searches in thespace of the strategies finding equilibria in pure strategies in time linear in the length of the bargaining and polynomialin the number of agents’ types. However, bargaining with only pure strategies in the alternating-offers protocol withuncertain deadlines is not satisfactory, since, as showed in [5], for some values of the parameters there is not anyequilibrium in pure strategies. As it is customary in game theory, such a problem can be overtaken by resorting tomixed strategies; we recall indeed that any game admits at least one sequential equilibrium in mixed strategies byKreps and Wilson’s theorem [22].In this paper we game theoretically study the problem of bargaining one issue in the alternating-offers protocolwith one-sided uncertainty over deadlines and we provide an efficient algorithm to compute it. Exactly, we analyzethe situation where the values of the parameters of the buyer are uncertain to the seller, whereas the parameters of theseller are common knowledge (the analysis of the reverse situation is analogous). We show also how our result canbe easily extended to the multiple issue situation exploiting the result presented in [6]. The extension of our result tothe two-sided situation is not easy instead, and it will be explored in future works. From our game theoretical analysiswe find two choice rules that apply an action and a probability distribution over the actions, respectively, to eachtime point and we find the conditions on the parameters such that each choice rule can be singularly employed toproduce an equilibrium.2 These conditions are mutually exclusive. We show that it is always possible to produce anequilibrium where the actions at any single time point are those prescribed either by the first choice rule or by thesecond one. We exploit this result for developing a solving algorithm. Differently from [11,12], our algorithm doesnot search for the optimal actions of the agents at each time point among all the available ones, but it applies theactions prescribed by the choice rule whose conditions are satisfied. The computational complexity of our algorithmis asymptotically independent of the number of types of the agent whose deadline is uncertain, being O(m · T ) wherem is the number of issues and T is the length of the bargaining. The two choice rules we present are not the only onesthat can be employed to produce an equilibrium. Indeed, since the alternating-offers protocol with uncertain deadlinesadmits more equilibria, other choice rules could be employed. The choice rules we propose have the peculiarity toproduce only one equilibrium for all the values of the parameters and guarantee that there is not any other set of choicerules that allows one for computing faster the solution through dynamic programming techniques.The paper is organized as follows. Section 2 reviews the alternating-offers model and the solution with completeinformation. Section 3 states the problem introducing the appropriate solution concept and the solutions currentlyavailable in the state of the art. Sections 4 and 5 game theoretically analyze the alternating-offers protocol withone-sided uncertainty over deadlines in pure strategies and mixed strategies, respectively. Section 6 presents oursolving algorithm and shows how it can be extended to the multiple issue situation. Section 7 concludes the paper. InAppendix A we report the proofs of the main theoretical results and in Appendix B we report the formulas to computethe equilibrium mixed strategy in presence of more than two types.2. Complete information alternating-offersIn this section we review the basis of the alternating-offers protocol with deadlines, in order to introduce notations,models, and techniques. We present (Section 2.1) the model of the alternating-offers bargaining with deadlines and(Section 2.2) its known solution with complete information.2 The first choice rule has been preliminarily presented in [5].1122N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11572.1. Bargaining modelWe study a discrete time finite horizon alternating-offers bargaining protocol on one continuous issue (e.g., a price).Formally, the buyer agent b and the seller agent s can act at times t ∈ N. The player function ι : N → {b, s} returnsthe agent that acts at time point t and is such that ι(t) (cid:5)= ι(t + 1). Possible actions σ tι(t) of agent ι(t) at any time pointt > 0 are:(i) offer(x), where x ∈ R is the value of the issue to negotiate,(ii) exit,(iii) accept,whereas at time point t = 0 the only allowed actions are (i) and (ii). If σ toutcome is (x, t), where x is the value such that σ t−1ι(t−1)x at time point t. If σ tcontinues to the next time point.= accept the bargaining stops and the= offer(x). This is to say that the agents agree on the value= exit the bargaining stops and the outcome is NoAgreement. Otherwise the bargainingι(t)ι(t)Each agent i has an utility function Ui : (R × N) ∪ {NoAgreement} → R, that represents her gain over the possiblebargaining outcomes. Each utility function Ui depends on three parameters of agent i:• the reservation price RPi ∈ R+,• the temporal discount factor δi ∈ (0, 1],• the deadline Ti ∈ N, Ti > 0.More precisely, if the outcome of the bargaining is an agreement (x, t), then the utility functions Ub and Us arerespectively:(cid:2)Ub(x, t) =(cid:2)Us(x, t) =(RPb − x) · (δb)t−1(x − RPs) · (δs)t−1if t (cid:2) Tbotherwiseif t (cid:2) TsotherwiseIf the outcome is NoAgreement, then Ub(NoAgreement) = Us(NoAgreement) = 0. Notice that the assignment of astrictly negative value (we have chosen by convention the value −1) to Ui after agent i’s deadline allows one tocapture the essence of the deadline concept: an agent, after her deadline, strictly prefers to exit the negotiation ratherthan to reach any agreement.According to classic works in literature, we assume the feasibility of the agreement, i.e., RPb > RPs, and therationality of the agents, i.e., it is common knowledge that each agent will act to maximize her utility.2.2. Complete information solutionWhen the information is complete the appropriate solution concept for a game like the one we are dealing withis the subgame perfect equilibrium [18]. Rigorously speaking, the protocol described above is not a finite horizongame: the deadlines are not in the protocol, but in the agent’s utility functions, and the agents are allowed to offerand counteroffer also after their deadlines have expired. Nevertheless, it is essentially a finite horizon game: a rationalagent will give up bargaining after her deadline. Therefore, subgame perfect equilibrium strategies can be foundemploying the backward induction method [14]. In what follows we informally summarize the backward inductionconstruction (see [28] for more details).The presence of deadlines in the agents’ utility functions induces a time point T where the game, if it is rationallyplayed, stops. This time point is the earliest of the two deadlines, formally, T = min{Tb, Ts}. Indeed, after T no agentcan gain positive utility by bargaining, being NoAgreement the equilibrium outcome of the subgame starting fromt = T . The peculiarity of the time point T with respect to any other time point t < T is that the optimal action agentι(T ) can make, if she does not accept her opponent’s offer, is to make exit. Instead, at any time point t < T the agentsstrictly prefer to make an offer rather than to make exit. From time point t = T − 1 back, the optimal actions of agentN. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571123ι(t) at time point t can be found in two steps. In the first step we find the best offer agent ι(t) can make at t: it is theoffer that gives agent ι(t + 1) the same utility of making at t + 1 her best offer, if t < T − 1, and exit, if t = T − 1.We denote such an offer by x∗(t). In the second step, we find the offers made by agent ι(t − 1) at t − 1 that agent ι(t)would accept at t: they are all the offers that give agent ι(t) an utility equal to or greater than offering x∗(t). The rulewhereby agent ι(t) chooses her optimal action at t is therefore: if t = T , she accepts any offer that gives her an utilityequal to or greater than zero, otherwise she makes exit, and, if t < T , she accepts any offer that gives her an utilityequal to or greater than offering x∗(t), otherwise she offers x∗(t).For the sake of simplicity, let ι(T ) = s; the backward induction construction with ι(T ) = b is analogous. Theunique equilibrium outcome of the subgame starting from time point t = T is NoAgreement, since s makes exit. Beingzero the utility of NoAgreement, s would accept any offer made by b at t = T − 1 that gives her an utility equal to orgreater than zero. Formally, she accepts any offer y such that Us(y, T ) (cid:3) 0, namely, y (cid:3) RPs.Consider the subgame starting from time point t = T − 1. The optimal offer x∗(T − 1) which b can make is RPs.Such an offer leads to the agreement (RPs, T ), which gives b an utility greater than the utility of the outcome shewould reach in the subgame starting from t = T , i.e., NoAgreement, while s is indifferent between NoAgreement and(RPs, T ). It can be easily observed that all the other available actions lead to outcomes that give b an utility strictlylower than offering RPs. More precisely, any other offer y that s would accept gives b a utility lower than offeringRPs, being y > RPs; any offer that s would not accept gives b an utility of zero, since s will exit; and exit givesb zero. Therefore, b would accept at t = T − 1 any offer that gives her an utility equal to or greater than offeringRPs, otherwise she offers RPs. Formally, she accepts any offer y such that Ub(y, T − 1) (cid:3) Ub(RPs, T ), namely,y (cid:2) RPb − (RPb − RPs)δb.Consider the subgame starting from time point t = T − 2. The optimal offer x∗(T − 2) which s can make isRPb − (RPb − RPs)δb. Such an offer leads to the agreement (RPb − (RPb − RPs)δb, T − 1) which gives s an utilitygreater than the utility of the outcome she would reach in the subgame starting from t = T − 1, i.e. (RPs, T ), while bis indifferent between (RPb − (RPb − RPs)δb, T − 1) and (RPs, T ). The utility Us(RPb − (RPb − RPs)δb, T − 1)is positive, since, being δb ∈ (0, 1), it holds RPb − (RPb − RPs)δb > RPs. Also in this case, as it happens in thesubgame starting from t = T − 1, all the other available actions give s an utility strictly lower than offering RPb −(RPb − RPs)δb. Therefore, s would accept at t = T − 2 any offer that gives her an utility equal to or greater thanoffering RPb − (RPb − RPs)δb, otherwise she offers RPb − (RPb − RPs)δb. Formally, she accepts any offer y suchthat Us(y, T − 2) (cid:3) Us(RPb − (RPb − RPs)δb, T − 1), namely, y (cid:3) RPs + (RPb − RPs)(1 − δb)δs.This reasoning can be inductively carried back to the beginning of the game producing a sequence of agreements(x∗(t), t + 1)s where each agreement (x∗(t), t + 1) is the equilibrium outcome of the subgame starting from timepoint t. Due to the presence of the discount factors, this sequence has the property that at each time point t it holdsUι(t)(x∗(t), t + 1) > Uι(t)(x∗(t + 1), t + 2) and Uι(t+1)(x∗(t), t + 1) = Uι(t+1)(x∗(t + 1), t + 2). On the equilibriumpath the agents agree at time point t = 1, since agent ι(0) offers x∗(0) at t = 0 and agent ι(1) accepts it at t = 1.In order to provide a recursive formula for x∗(t), we introduce the notion of backward propagation, whose defini-tion is independent of the protocol we are studying.Definition 1. Given x ∈ R and agent i ∈ {b, s}, we call one-step backward propagation of x along the isoutility curvesof i the value x←i ∈ R such that Ui(x←i, t − 1) = Ui(x, t) for any time t ∈ {1, . . . , Ti}. (The notion is well definedbecause x←i does not depend on the choice of t ∈ {1, . . . , Ti}.) Given x ∈ R and a sequence s = (cid:8)i1, i2, . . . , in(cid:9) ∈{b, s}n, we call backward propagation of x along the curves of s the value x←s ∈ R such that(cid:2)x←s =(x←s(cid:10))←in where s(cid:10) = (cid:8)i1, i2, . . . , in−1(cid:9)x←i1if n > 1if n = 1Two examples of multi-step propagations are x←(cid:8)b,s(cid:9) = (x←b)←s and x←(cid:8)b,s,b(cid:9) = ((x←b)←s)←b. For multi-steppropagations we usually employ a shorter notation for repeated subsequences of agents; for example, x←b3[bs] standsfor x←(cid:8)b,b,s,b,s,b,s(cid:9).On the basis of the notion of backward propagation the values of x∗(t) can be easily defined as follows:(cid:2)∗x(t − 1) =RPι(t)(x∗(t))←ι(t)if t = Tif t < T1124N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157Fig. 1. Backward induction construction with RPb = 1, RPs = 0, δb = 0.7, δs = 0.7, Tb = 9, Ts = 10, ι(0) = b; at each time point t the optimaloffer x∗(t) that ι(t) can make is marked; the dashed lines are isoutility curves.where the formulas to compute (x∗(t))←b and (x∗(t))←s are: (x∗(t))←b = RPb − (RPb − x∗(t))δb and (x∗(t))←s =RPs + (x∗(t) − RPs)δs.Fig. 1 shows an example of backward induction construction with RPb = 1, RPs = 0, δb = 0.7, δs = 0.7, Tb = 9,Ts = 10, ι(0) = b. We report in the figure for any time point t the optimal offer x∗(t) agent ι(t) can make; thedashed lines are agents’ isoutility curves. The time point from which we can apply the backward induction method,T = min{Tb, Ts}, is T = 9. At t = 9 agent ι(9) = s will accept any offer equal to or greater than 0, being RPs = 0. Theoptimal offer x∗(8) of b at t = 8 is thus RPs = 0. The optimal offer x∗(7) of s at t = 7 is (x∗(8))←b = RPb − (RPb −x∗(8))δb = 0.3. Analogously, the optimal offer x∗(6) of b at t = 6 is (x∗(7))←s = RPs + (x∗(7) − RPs)δs = 0.21. Theprocess continues to the initial time point t = 0.The equilibrium strategies can be easily defined by specifying, according to x∗(t), the rules the agents employ tochoose their optimal action at each time point t:⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩t = 00 < t < TT (cid:2) t (cid:2) Tbσ∗b (t) =Tb < tt = 0⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩σ∗s (t) =0 < t < TT (cid:2) t (cid:2) Tsoffer(x∗(0))(cid:2)acceptoffer(x∗(t))acceptexit(cid:7)if σs(t − 1) (cid:2) x∗(t − 1)otherwiseif σs(t − 1) (cid:2) RPbotherwiseexitoffer(x∗(0))(cid:2)acceptoffer(x∗(t))acceptexit(cid:7)if σb(t − 1) (cid:3) x∗(t − 1)otherwiseif σb(t − 1) (cid:3) RPsotherwise(1)(2)Ts < texitThe above strategies constitute the unique subgame perfect equilibrium of bargaining in presence of deadlines withcomplete information (see [28]). The equilibrium outcome is (x∗(0), 1). They can be computed in time linear in thelength of the bargaining.N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–115711253. Problem statement and known solutionsIn this section we state (Section 3.1) the problem of bargaining with the alternating-offers protocol in presenceof one-sided uncertain deadlines, by enriching the complete information bargaining model presented in the previoussection and introducing appropriate solution concepts. We review (Section 3.2) the solutions currently available inliterature and we present (Section 3.3) the solution suggested in this paper.3.1. Model enrichment and appropriate solution conceptWe consider single-issue alternating-offers bargaining when one of the two agents does not exactly know heropponent’s deadline. We will assume that the uncertain deadline is the buyer’s; the case wherein the uncertain deadlineis the seller’s can be treated analogously.As is customary in game theory to avoid situations that cannot be faced, an incomplete information game is castedinto an imperfect information game with the introduction of probability distributions over the unknown parameters. Inour case we assume that b’s possible deadlines are distributed according to a probability distribution on R+ which iscommon knowledge between the agents. We further assume that the support of the probability distribution is bounded;and, since the agents can act only at time points t ∈ N, we can assume, without loss of generality, that the probabilitydistribution is finite on N. We denote by Tb = {Tb1, . . . , Tbn}} the set of possible deadlines of b, by P 0, . . . , ω0bbn(cid:9). Without loss of generality, we assumeb the couple BT 0the pertinent probability distribution, and by BT 0bthat for any i ∈ [1, n − 1] it holds Tbi < Tbi+1 . We denote by bi the type of b whose deadline is t = Tbi . Agent b’s realdeadline is known only to b herself: it is her private information.= (cid:8)Tb, P 0b= {ω0b1The solution concept employed in extensive-form games with complete information, namely, subgame perfectequilibrium, is not satisfactory when information is imperfect. Specifically, it does not have the power to cut the socalled incredible threats [14], i.e., Nash equilibria that are non-reasonable given the sequential structure of the game.The most common refinement of the subgame perfect equilibrium concept in presence of information imperfectnessis the sequential equilibrium of Kreps and Wilson [22]. We now review this concept.Rational agents try to maximize their expected utilities relying on their beliefs about the opponent’s private infor-mation [38] and such beliefs are updated during the game, depending on which actions have been actually made [22].The set of beliefs held by each agent over the other’s private information after every possible sequence of actions inthe game is called a system of beliefs and is usually denoted by μ. These beliefs are probabilistic and their values attime point t = 0 are given data of the problem. How beliefs evolve during the game is instead part of the solutionwhich should be found for the game. A solution of an incomplete information bargaining is therefore a suitable couplea = (cid:8)μ, σ (cid:9) called assessment.An assessment a = (cid:8)μ, σ (cid:9) must be such that the strategies in σ are mutual best responses given the probabilisticbeliefs in μ (rationality); and the beliefs in μ must reasonably depend on the actions prescribed by σ (consistency).Different solution concepts differ on how they specify these two requirements.For a sequential equilibrium a∗ = (cid:8)μ∗, σ ∗(cid:9), with σ ∗ = (cid:8)σ ∗(cid:9), the rationality requirement is specified as sequen-tial rationality. Informally, after every possible sequence of actions S, on or off the equilibrium path, the strategy σ ∗smust maximize s’s expected utility given s’s beliefs prescribed by μ for S, and given that b will act according to σ ∗bfrom there on and vice versa. The notion of consistency is defined as follows: assessment a is consistent in the sense ofKreps and Wilson (or simply consistent) if there exists a sequence an of assessments, each with fully mixed strategiesand such that the beliefs are updated according to Bayes’ rule, that converges to a. By Kreps and Wilson’s theorem anyextensive-form game in incomplete information admits at least one sequential equilibrium in mixed strategies [22].b , σ ∗sMoreover, as is customary in economic studies, e.g. Rubinstein’s [34], we consider only stationary systems ofbeliefs, namely, if s believes a b’s type with zero probability at time point t, then she will continue to believe such atype with zero probability at any time point t > t.3.2. Solutions known in literatureThe computation of a solution of an extensive-form game with imperfect information is known to be a hard task totackle. Contrary to what happens with complete information games, classic game theory does not provide any solvingtechnique to find sequential equilibria. From [19] there is long standing literature in computer science, e.g. [15,20,23],1126N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157that studies algorithms to find Nash equilibria and refinements, e.g. sequential equilibria [27], searching in the spaceof the strategies. These algorithms have two main drawbacks that make them inapplicable in solving bargainingsituations: they work only with games with finite strategies and they produce equilibrium strategies but not systemsof beliefs. The lack of a solving algorithm for games as bargaining has pushed researchers to game theoretically studyeach possible specific setting and develop relative algorithms. Well known examples of alternating-offers settingsstudied in literature are: in [34] Rubinstein analyzes one-sided uncertainty over the discount factors with only twotypes, in [5] Di Giunta and Gatti analyze in pure strategies one-sided uncertainty over deadlines, in [3] Chatterjeeand Samuelson analyze uncertainty over reservation prices where the agents can be of two possible types, and in [12]Fatima et al. consider a slight variation of the alternating-offers protocol where there are multiple issues to negotiateand analyze uncertainty over the weights of the issues.The best known results for bargaining in the alternating-offers protocol with uncertain deadlines are due to Fatimaet al. in [9,10] and in [11]. In [9,10] they study the alternating-offers protocol with two-sided uncertainty over deadlinesand bounds on the agents’ rationality, more precisely, agents are self-constrained to play predefined tactics based on thenegotiation decision functions paradigm [8]. In [11] they consider a slight variation of the alternating-offers protocolwhere there are multiple issues. The bargaining model they consider is exactly the one presented in Section 2.1,except that the agents’ utility functions, offers, and acceptance are defined on a tuple of issues, instead of a singleissue. The solution of this model with complete information is analogous to the solution of the single issue modelpresented in Section 2.2 and can be easily obtained by extending, as showed in [7], the offers’ backward propagationwe defined in Section 2.2. In presence of a single issue the solution of the multiple issue model and the one of thesingle issue model collapse. Fatima et al. analyze the situation where the deadlines are uncertain and propose analgorithm that finds equilibrium assessments in pure strategies for all the values of the parameters. Basically, theiralgorithm searches in the space of the strategies exploiting the backward induction from the last possible deadlineto t = 0 with agents’ initial beliefs, and, once the optimal strategies at time point t = 0 have been found, the systemof beliefs is designed to be consistent with them. The computational complexity of their algorithm is linear in thelength of the bargaining and polynomial in the number of agents’ types. This work fails in finding equilibria for somevalues of parameters. Indeed, as [5] shows, for a non-null measure subset of the space of the parameters there isnot any equilibrium assessment in pure strategies.3 The reason behind the failure of [11] in producing equilibriumstrategies for some settings of parameters is that in each step of backward induction they limit the search to the spaceof the strategies, but they do not verify the existence of a consistent system of beliefs such that the found strategy issequentially rational. As a result, once their algorithm has produced the agents’ strategies at t = 0 and has designedthe system of beliefs consistent with them, the strategies could be not sequentially rational given the designed systemof beliefs. This happens for all the values of the parameters such that there is not any equilibrium in pure strategies [5].We show an example where their algorithm fails in Section 4.1.3.Analogously to the result presented in [5], it can be easily showed that for a non-null measure subset of the spaceof the parameters there is not any equilibrium assessment in fully mixed strategies. Therefore, in the provision ofa satisfactory solution to the bargaining with uncertain deadlines, it is necessary to employ both pure and mixedstrategies.3.3. Overview of our solutionIn this paper we go beyond the state of art along two directions: our algorithm (i) finds a sequential equilibrium forall the values of the parameters and (ii) requires less computational time than the algorithms presented in literature sofar. In particular, the computational time required by the proposed algorithm is asymptotically the same time requiredto compute the solution with complete information, being linear in the length of the bargaining and asymptoticallyindependent of the number of types of the agent whose deadline is uncertain. Furthermore there is not any equilibriumassessment that can be computed backward faster than ours. We present our result by degrees in Sections 4, 5, and 6as follows.We first analyze the alternating-offers protocol with uncertain deadlines in pure strategies (Section 4). We find anassessment in pure strategies ap that is an equilibrium under some conditions on the parameters. We show that, except3 In measure theory, a null set is a set that is negligible for the purposes of the measure in question [17]. As commonly done in literature to studysets in Euclidean n-space Rn, we use Lebesgue measure.N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571127for a null measure subset of the space of the parameters, ap is an equilibrium whenever there is an equilibrium inpure strategies. Moreover, we show that there is not any other equilibrium assessment in pure strategies that can becomputed backward faster than ap. We derive from ap a choice rule that applies to each time point the optimal actionsof the agents and we find the conditions on the parameters such that the choice rule can be employed to produce anequilibrium. Then, we resort to mixed strategies in analyzing the alternating-offers protocol with uncertain deadlines(Section 5). We find an assessment in mixed strategies am that is an equilibrium for all the values of the parameterssuch that ap is not an equilibrium. More precisely, the conditions on the parameters such that am is an equilibriumare complementary to the conditions that make ap an equilibrium. Therefore, there is always exactly one assessment,either ap or am, that is an equilibrium. Moreover, no assessment in mixed strategies can be computed backward fasterthan am. We derive from am a choice rule of the agents and the relative conditions on the parameters. Finally, weprovide (Section 6) our solving algorithm and we show how it can be extended in presence of multiple issues.4. Equilibrium analysis in pure strategiesIn this section we present (Section 4.1) a specific assessment in pure strategies that is an equilibrium for a non-nullmeasure subset of the space of the parameters;4 then we show (Section 4.2) that no equilibrium assessment in purestrategies can be computed backward faster than ours and that, except for a null measure subset of the space of theparameters, there is not any equilibrium in pure strategies when our assessment is not an equilibrium.4.1. A pure strategy equilibrium assessmentDifferently from what happens in presence of complete information, game theory does not provide any solvingtechnique to find equilibrium assessments when information is imperfect. Indeed, the backward induction methodcannot be employed because it does not consider the possible evolution of the beliefs. Here we exploit the idea behindthe backward induction method combining it with an a priori fixed system of beliefs. More precisely, the method wewill employ is the following:(i) a priori fix a reasonable system of beliefs μ,(ii) use backward induction to find the optimal strategies σ ∗ given μ,(iii) identify possible anomalies in the use of backward induction, i.e. situations where the produced strategies σ ∗ arenot sequentially rational,(iv) a posteriori prove the consistency of the assessment.We now describe the application of our method.4.1.1. Fixing the system of beliefsWe base our system of beliefs on the idea that b can signal her type on the equilibrium path only at her real deadline,namely, a buyer bi can signal her type only at t = Tbi . Implicitly, this means that at any time point t all the types bi s= t (if shewhose deadline has not expired (i.e., Tbiexists). Although such an assumption seems very restrictive, we show in Section 4.2 that, if there is an equilibriumassessment in pure strategies, then it respects such an assumption. In other words, bargaining in the alternating-offersprotocol with uncertain deadlines in pure strategies does not admit any equilibrium assessment where b can signal hertype at a time point t different from her real deadline.(cid:3) t) have the same equilibrium strategies but the type bi with TbiWe fix our system of beliefs such that, after any sequence of actions, s just excludes those deadlines Tbi s amongthe initially possible ones that have already expired and normalizes the probabilities of the future ones. Notice thatamong all the possible systems of beliefs that satisfy the property discussed above, ours is the simplest one: it employsthe same upgrading rule both on and off the equilibrium path.We assume, for the sake of generality, that any time point t ∈ [0, Tbnb (t2) the probability, calculated at t = t1 with μ, that b’s deadline is at t = t2, and by (cid:6)t1ωt1] is a possible buyer’s deadline. We denote byb (t2) =b (t) the(cid:8)+∞t=t2ωt14 This result has been preliminarily presented in [5].1128N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157cumulative probability, calculated at t = t1 with μ, that b’s deadline is at t (cid:3) t2, respectively. The value of ω0according to the initial beliefs BT 0b . Our system of beliefs μ can be written as:b(t) is set⎧⎨μ(t) =⎩for any τ such that τ < t ωtfor any τ such that t (cid:2) τ ωtb(τ ) = 0b(τ ) = ω0b(τ )(cid:6)0b(t)4.1.2. Backward induction with the fixed system of beliefsGiven the system of beliefs μ, we can find the optimal strategies using backward induction. However, in thiscontext the use of backward induction is more involved than in the complete information bargaining and requiressome explanations. More precisely, two issues are crucial: the determination of the time point T from which thebackward induction construction starts and the determination of the optimal offers along the construction.We first focus on the determination of T . With complete information the backward induction construction can startat the earliest of the two deadlines, while with our incomplete information framework, the earliest deadline is not a}, Ts}}, because itpriori known. Nevertheless, the backward induction construction can start from T = min{maxi{Tbiis a priori known that after such a time point the agents will exit the negotiation; if the bargaining process reaches thetime point T , then agent ι(T ) would accept any non-negative utility offer; therefore, at time point t = T − 1 agentι(T − 1), if she make an offer, would offer ι(T )’s reservation price.We now focus on the determination of the sequence of the optimal offers x∗(t)s from time point t = T − 1 totime point t = 0. Although b’s types can be many, the sequence of the optimal offers x∗(t)s is the same for all theb’s types. This is because μ prescribes that all b’s types whose deadline is beyond t have the same optimal action att and therefore, if they make an offer, they make the same offer; instead, b’s types whose deadline is at t or beforedo not make any offer, but exit. Like in the complete information setting described in Section 2, it is possible tobackwardly infer the sequence of the offers x∗(t)s that the agents would do if they choose to make an offer. But thereare some complications in the construction of the sequence of offers x∗(t)s. In the complete information case theoffers composing this sequence have two properties:(i) x∗(t) is the value propagated backward from time point t + 1,(ii) x∗(t) is the value to be propagated backward at time point t − 1.Essentially, this makes an offer x∗(t) to be the optimal offer s can make at time point t and, at the same time, theoptimal offer b can accept at time point t + 1. With imperfect information in bargaining, instead, not holding ingeneral properties (i) and (ii), the optimal offer of s at time point t could be different from the optimal offer that bwould accept at time point t + 1. In what follows we preliminarily introduce these topics informally and then wereport the exact formulas.First, when ι(t) = s the optimal offer x∗(t) is not generally the value propagated backward from time point t + 1,being x∗(t) generally different from (x∗(t + 1))←b. This is because different b’s types can have different maximumacceptable offers at time point t + 1 (e.g., b1 accepts any offer (cid:2) x1 and b2 accepts any offer (cid:2) x2 with x1 < x2),and the determination of what offer is to propagate backward depends on the beliefs of s. This happens at any timepoint t where ι(t) = s and the time point t + 1 is a possible deadline of b. In this situation, b’s type whose deadlineis at time point t + 1, obtaining a non-positive utility from the continuation of the game, will accept any offer equalto or lower than RPb, otherwise she will make exit. Instead, since all b’s types whose deadlines are beyond the timepoint t + 1 gain a positive utility from the continuation of the game, they will accept any offer equal to or lower than(x∗(t + 1))←b, otherwise they will offer x∗(t + 1). The value of (x∗(t + 1))←b, determined by backward inductionfrom T to t + 1, is obviously lower than RPb. Let EUs(σ, t) be the expected utility of s in making the strategy σ attime point t. The expected utility EUs(σ, t) when σ = offer(x) is:b(t + 2) · Us(x∗(t + 1), t + 2)(cid:6)tx > RPbRPb (cid:3) x > (x∗(t + 1))←b ωtb(t + 1) · Us(x, t + 1) + (cid:6)t(x∗(t + 1))←b (cid:3) xUs(x, t + 1)b(t + 2) · Us(x∗(t + 1), t + 2)EUs(offer(x), t) =⎧⎨⎩That is, any offer x > RPb will be rejected by all b’s types and the type whose deadline is at t + 1 will make exit,whereas all the other types whose deadline has not expired will make the offer x∗(t + 1) that s will accept at t + 2;N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571129any offer RPb (cid:3) x > (x∗(t + 1))←b will be accepted by the type whose deadline is at t + 1, but it will be rejected byall the other types to make the offer x∗(t + 1) that s will accept at t + 2; any offer (x∗(t + 1))←b (cid:3) x will be acceptedb(t + 2)by all the types. We recall that ωtis the probability calculated at t with μ that b’s deadline is beyond t + 1. In order to maximize EUs(σ, t), s choosesher optimal action between offering (x∗(t + 1))←b and offering RPb. Notice that all the other possible offers that scan make are dominated by making her best offer between (x∗(t + 1))←b and RPb.b(t + 1) is the probability calculated at t with μ that b’s deadline is at t and (cid:6)tSecond, when ι(t) = s the optimal offer x∗(t) is not generally the value to be propagated backward, being x∗(t − 1)generally different from (x∗(t))←s. To determine the value to be propagated backward we need to introduce the notionof equivalent value of an offer x at time point t: given an offer x made by s at time point t, the equivalent value ofx, denoted by e(x, t), is the value such that Us(e(x, t), t) = EUs(offer(x), t). Clearly, the equivalent value of an offerthat will be accepted with a probability equal to 1 is the offer itself. The right value to be propagated backward is theequivalent value of x∗(t); we denote it by e∗(t). Easily, as optimal offers of agent ι(t) = s could be rejected, s couldaccept at time point t an offer x such that EUs(accept, t) = Us(x, t) = EUs(offer(RPb), t). Summarily, if b makesan offer at a time point t greater than or equal to (e∗(t + 1))←s, then s accepts it; if b makes an offer lower than(e∗(t + 1))←s, then s rejects it and counteroffers x∗(t + 1).Formulas to find the equivalent value e(x, t) and the sequence of the optimal offers x∗(t)s can be easily derivedfrom the formula of EUs(offer(x), t):• when ι(t) = b, the optimal offer x∗(t) is x∗(t) = (e∗(t + 1))←s and, being such an offer surely accepted by s att + 1, the relative equivalent is the offer itself, i.e. e∗(t) = x∗(t);• when ι(t) = s, the equivalent value of an offer x depends on s’s beliefs and it is:⎧⎪⎨e(x, t) =⎪⎩b(t + 2) · ((x∗(t + 1))←s − RPs) + RPs(cid:6)tx > RPbRPb (cid:3) x > (x∗(t + 1))←b ωtb(t + 1) · (x − RPs) + (cid:6)t(x∗(t + 1))←b (cid:3) xb(t + 1) · (x − RPs) + RPs(cid:6)tb(t + 2) · ((x∗(t + 1))←s − RPs) + RPsthe optimal offer x∗(t) is the offer between RPb and (e∗(t + 1))←b that maximizes e(x, t), and consequentlye∗(t) = e(x∗(t), t). Notice that for s maximizing e(x, t) corresponds to maximizing EUs(offer(x), t).With the system of beliefs μ the agents’ optimal action is unique at any time point t, except when the beliefs aresuch that both offering RPb and offering (e∗(t + 1))←b maximize EUs(σ, t). In all these situations s can indifferentlyoffer one between RPb and (e∗(t + 1))←b. Being the equivalent value of offering RPb and (e∗(t + 1))←b equal, theoptimal offer of b at t − 1 is independent of what offer s would actually make at t.Given x∗(t), the optimal action σ ∗ι(t)(t) of agent ι(t) at time point t is: exit, if the ι(t)’s deadline is at t and she hasreceived an offer that gives her negative utility; accept, if the ι(t)’s deadline is at t + 1 and she has received an offerthat gives her non-negative utility or if the ι(t)’s deadline is not at t + 1 and she has received an offer not worse forher than (e∗(t))←ι(t); offer(x∗(t)), otherwise. The calculation of x∗(t) can be accomplished recursively on the basisof e(x, t) as follows:⎧⎨if t = TRPι(t)(cid:2)∗x(t − 1) =⎩arg maxx∈{RPb,(e∗(t))←b}{e(x, t − 1)}(e∗(t))←sif ι(t) = bif ι(t) = sif t < TThe time required to compute the sequences of offers x∗(t)s and equivalents e∗(t)s is linear in the length of thebargaining and asymptotically independent of the number of the buyer’s types. With respect to the computation of thesolution with complete information it is needed at most a maximization between two possible values at any time pointwhere ι(t) = s. The equilibrium strategies can be defined specifying the agents’ choice rules on the basis of x∗(t)and e∗(t) as extension of the ones with complete information reported in (1) and (2). More precisely, they are:σ∗bi(t) =⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩t = 00 < t < TT (cid:2) t (cid:2) TbiTbi < toffer(x∗(0))(cid:2)acceptoffer(x∗(t))acceptexit(cid:7)if σs(t − 1) (cid:2) (x∗(t))←botherwiseif σs(t − 1) (cid:2) RPbotherwiseexit(3)1130N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157Fig. 2. Backward induction construction with RPb = 1, RPs = 0, δb = 0.7, δs = 0.7, (cid:8)Tb = {5, 8, 9}, P 0beach time point t the optimal offer x∗(t) that ι(t) can make and the relative e∗(t) are reported.= {0.5, 0.3, 0.2}(cid:9), Ts = 10, ι(0) = b; at⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩t = 00 < t < TT (cid:2) t (cid:2) Tsσ∗s (t) =offer(x∗(0))(cid:2)acceptoffer(x∗(t))acceptexit(cid:7)if σb(t − 1) (cid:3) (e∗(t))←sotherwiseif σb(t − 1) (cid:3) RPsotherwise(4)Ts < texitDifferently from what happens with complete information, the equilibrium agreement can be reached beyond timepoint t = 1. For example, when ι(0) = s, time point t = 1 is a possible b’s deadline but not the real one and s’s beliefsare such that her optimal offer at time point t = 0 is x∗(0) = RPb.Fig. 2 shows an example of backward induction construction with RPb = 1, RPs = 0, δb = 0.7, δs = 0.7, (cid:8)Tb == {0.5, 0.3, 0.2}(cid:9), Ts = 10, ι(0) = b. In the figure we report, at any time point t, the optimal offer x∗(t){5, 8, 9}, P 0bagent ι(t) can make and the relative equivalent value e∗(t); the dashed line from t = 7 to t = 0 is the construction withcomplete information; the dashed line that connects (7, (x∗(8))←b) to (6, (x∗(8))←2[b]) will be taken into account inthe next section. The time point from which we can apply the backward induction method, T = min{max{Tb}, Ts},is T = 9. Since at time points t = 9 and t = 8 s believes b’s deadline to be t = 9, the construction in these timepoints is exactly the one accomplished with complete information. That is, at t = 9 agent ι(9) = s will accept anyoffer equal to or greater than 0, being RPs = 0. The optimal offer x∗(8) of b at t = 8 is thus RPs = 0. At t = 7 theb(9) = 0.4. Being t = 8 a possible b’s deadline, at t = 7 agent ι(7) = s choosesbeliefs of s are ω7her optimal action between two alternatives: to offer (x∗(8))←b = 0.3 that b’s types with deadlines at t = 8 and t = 9will accept or to offer RPb = 1 that will be accepted only by the type with deadline at t = 8 and rejected by the othertype to counteroffer RPs. The first alternative has an equivalent value e((x∗(8))←b, 7) = (x∗(8))←b = 0.3, whereasthe second alternative has an equivalent value e(RPb, 7) = ω7b(9) · (RPs − RPs)δs + RPs = 0.6.The optimal offer x∗(7) of s at t = 7 is therefore RPb with e∗(7) = 0.6. The optimal offer x∗(6) of b at t = 6 is(e∗(7))←s = 0.4. At t = 5 the beliefs of s are ω5b(9) = 0.2. The optimal offer x∗(5)of s at t = 5 is (x∗(6))←b = (e∗(7))←sb = 0.594 and the relative equivalent, being t = 5 a possible deadline of b, ise∗(5) = (cid:6)5b(6) · (x∗(5) − RPs) + RPs = 0.297. The optimal offer x∗(4) of b at t = 4 is (e∗(5))←s = 0.2079. Fromt = 3 to t = 0 the backward induction construction continues as with complete information.b(8) · (RPb − RPs) + ω7b(7) = 0.3, and ω5b(8) = 0.6 and ω7b(5) = 0.5, ω5N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571131Fig. 3. Non-existence of equilibrium with RPb = 1, RPs = 0, δb = 0.7, δs = 0.7, (cid:8)Tb = {5, 8, 9}, P 0bsequential rationality is not respected because (e∗(7))←s > (x∗(8))←2[b]; the dashed lines are isoutility curves.= {0.5, 0.4, 0.1}(cid:9), Ts = 10, ι(0) = b: the4.1.3. Anomalies in backward induction constructionThe above backward induction construction provides the sequentially rational strategies relying on the system ofbeliefs μ we have chosen, when they exist. However, they could not exist for some parameter settings. This canhappen because one agent (precisely, the buyer) might deviate from strategy σ ∗ to offer something unacceptable forthe opponent in order to be refused and to later be in a much stronger position and gain more. An example is depictedin Fig. 3.The bargaining situation considered in Fig. 3 is the same considered in Fig. 2, except for the values of the proba-= {0.5, 0.3, 0.2}. That is, s believes time point t = 8 to be= {0.5, 0.4, 0.1} instead of P 0bilities: in this situation P 0bba possible b’s deadline with higher probability than in the previous situation. The construction from t = 9 to t = 7 isexactly the same of the previous situation, except for the value of e∗(7). In this situation e∗(7) = 0.8. The backwardinduction construction imposes that at t = 6 an offering buyer should offer (e∗(7))←s, but it can be easily seen thatsuch an offer is not the optimal offer of both b’s types with deadline at t = 8 and t = 9. The type with deadline att = 8 can actually maximize her utility offering (e∗(7))←s. The type with deadline at t = 9 can instead maximize herutility offering something unacceptable (e.g., RPs), then receiving counteroffer RPb, and finally re-counterofferingRPs at t = 8 (that s will accepted). Indeed, it can be observed in the figure that the equivalent value (x∗(8))←2[b]of offering something unacceptable and reaching the agreement (RPs, 9) is lower (and then preferable for the buyer)than the equivalent value (e∗(7))←s of offering (e∗(7))←s and reaching the agreement ((e∗(7))←s, 7). Instead, in thebargaining situation depicted in Fig. 2 this anomaly is not present, since (e∗(7))←s < (x∗(8))←2[b].This anomaly arises because different b’s types with Tbi > t might have different optimal offers at time point t,violating what is prescribed by μ. This happens at any time point t where ι(t) = b, the time point t + 2 is a possibledeadline of b, the optimal offer of s at time point t + 1 is RPb, and Ub(x∗(t), t) < Ub(x∗(t + 2), t + 2). In order forthe strategies (3) and (4) to be sequentially rational, no buyer’s type must prefer making at time point t somethingelse x∗(t) to reach the agreement (x∗(t + 2), t + 2). It trivially follows that the condition required for the sequentialrationality of the strategies is that the following inequality holds at any time point t ∈ [0, T − 2] where ι(t) = b:∗(e(t + 1))←s (cid:2) (x∗(t + 2))←2[b].(5)1132N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157The algorithm presented in [11] fails exactly when the inequality (5) does not hold. This algorithm produces bybackward induction the agents’ strategies keeping the initial beliefs and, once the strategies at time point t = 0 havebeen produced, the system of beliefs is designed to be consistent with the produced strategies. We discuss the applica-tion of this algorithm to a subgame of the bargaining situation depicted in Fig. 3, showing that the strategies it producesare not sequentially rational given the system of beliefs. Exactly, we apply the algorithm to the subgame starting fromt = 6. The backward induction construction produced by [11] from t = 9 to t = 7 is the same construction aboveproduced by our method. That is, s makes at t = 7 the offer RPb, otherwise she would accept any offer equal to orgreater than (e∗(7))←s; b’s type with deadline at t = 8 will accept RPb, instead the type with deadline at t = 9 willreject such an offer to make the offer RPs. The b’s optimal actions at t = 6 produced by the algorithm are: b’s typewith deadline at t = 8 makes (e∗(7))←s, whereas b’s type with deadline at t = 9 makes (x∗(8))←2[b]. The system ofbeliefs consistent with them can be easily produced: if s observes the offer (e∗(7))←s, then she believes that b’s dead-line is at t = 8; if s observes the offer (x∗(8))←2[b], then she believes that b’s deadline is at t = 9; we omit the systemof beliefs off the equilibrium path being unnecessary for our discussion. Given this system of beliefs we analyze thesequential rationality of the produced strategies. Consider the optimal action of b’s type with deadline at t = 8: shecan improve her utility making the optimal action of b’s type with deadline at t = 9. If she offers (x∗(8))←2[b], then swill believe that b’s deadline is at t = 9 and will accept such an offer. This is because the optimal offer s can make inthe subgame starting from t = 7 where b’s deadline is t = 9 is (x∗(8))←b and her utility of making this offer is lowerthan her utility of accepting (x∗(8))←2[b]. Therefore, the strategies are not sequentially rational given the system ofbeliefs.4.1.4. Equilibrium assessmentCollecting the system of beliefs μ, the strategies σ ∗, and the sequential rationality condition presented in theprevious sections, we can now state the following theorem whose proof is reported in Appendix A.1.Theorem 2. If for any time point t ∈ [0, T − 2] where ι(t) = b inequality (5) holds, then the assessment a = (cid:8)μ, σ ∗(cid:9)above described is a sequential equilibrium.This assessment can be computed backward from t = T on as prescribed in Section 4.1.2 until either the time pointt = 0 is reached or the condition (5) is not satisfied. The computational time required to compute this assessment islinear in the length of the bargaining and asymptotically independent of the number of b’s types.4.2. Analysis of equilibrium computation and existenceIn this section we show that there is not any equilibrium assessment which can be computed backward faster thanours and that, when our assessment is not an equilibrium, there is not any equilibrium assessment in pure strategies,except for a null measure subset of the space of the parameters.At first we characterize all the possible equilibrium assessments in terms of equilibrium strategies independently ofthe system of beliefs one can adopt. We show that, if a bargaining situation admits at least one equilibrium assessmenta in pure strategies, then (except for a null measure subset of the space of the parameters) the equilibrium strategiesmust be such that at any time point t where ι(t) = b the following condition holds:for all i, j such that Tbi , Tbj > t it must be σ ∗bifor all i, j such that Tbi= t, Tbj > t it must be σ ∗bi(t) = σ ∗(t)bj(t) (cid:5)= σ ∗bj(t)(6)(cid:9)Notice that our μ is a possible, but not the unique, system of beliefs consistent with (6). We base our proof on thespecial case with only two types of buyer, which we call, for the sake of clarity, early buyer be and late buyer bl withTbe < T (cid:2) Tbl . We show subsequently how the proof in the general case can be produced by iteratively applying thespecial case.By considering the agents’ optimal actions at any time point t < Tbe prescribed by a generic equilibrium assess-ment, we state the following lemma whose proof is reported in Appendix A.2.Lemma 3. Given two buyer’s types, at any time point t < Tbe where ι(t) = b it must be that σ ∗bea null measure subset of the space of the parameters.(t) = σ ∗bl(t) except forN. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571133By considering the agents’ optimal actions at time point t = Tbe prescribed by a generic equilibrium assessment,we state the following lemma whose proof is reported in Appendix A.3.Lemma 4. Given two buyer’s types, at time point t = Tbe where ι(t) = b it must be that σ ∗be(Tbe ) (cid:5)= σ ∗bl(Tbe ).By Lemmas 3 and 4 we can now state the following theorem.Theorem 5. Except for a null measure subset of the space of the parameters, if there is an equilibrium assessmenta = (μ∗, σ ∗), then the equilibrium strategies σ ∗ satisfy (6) at any time point t where ι(t) = b.Proof sketch. The proof in the special case with two buyer’s types trivially follows from Lemmas 3 and 4. The proofin the general case with a number n of types can be easily obtained by iteratively applying the special case. Considera set {b1, b2, . . . , bn} of possible b’s types where, without loss of generality, Tbi < Tbi+1 holds for any i. Considerthe subgame starting from Tbn−2 where the possible types are bn−2, bn−1, and bn. Since bn−2 will make exit wecan exclude it from our analysis and we can apply the special case of theorem deriving that bn−1 and bn have thesame strategies at any t ∈ [Tbn−2, Tbn−1). Therefore, we can aggregate bn−1 and bn in a unique ‘fictitious’ type beforet = Tbn−1 and apply the special case of theorem to the subgame starting from Tbn−3 where the possible types are bn−3,bn−2, and the ‘fictitious’ type. The iteratively application of the special case of the theorem continues until the typeb1 has been considered. (cid:2)From Theorem 5 we can state the following corollary.Corollary 6. There is not any equilibrium assessment in pure strategies that can be computed backward faster thanthe assessment provided in Theorem 2.Proof sketch. We firstly notice that no algorithm based on backward induction can have computational complexitystrictly lower than O(C · T ), where C is the computational cost of a single step of backward induction and is, ingeneral, a function of the number of b’s types. This is because the backward induction method requires to analyzeevery possible time point where the agents act. Since the complexity of our algorithm depends linearly on T , we canlimit our analysis to C.By Lemmas 3 and 4, at any time point t where ι(t) = b agent b can have two optimal actions, if time point t is apossible deadline of b, and only one otherwise. Therefore, at any time point t where ι(t) = s and time point t + 1 is apossible deadline of b, s can compute her optimal offer searching for the offer that maximizes her expected utility ina set of at least two possible alternatives. That is, C (cid:3) 2. Our solution, searching in a set of two possible alternatives,requires exactly the minimum computational time needed to compute a step of backward induction. In addition, oursystem of beliefs is the only one, among all the ones consistent with (6), that employs the same upgrading rule bothon and off the equilibrium path. It follows that our assessment is the only one where the optimal strategies of theagents are the same both on and off the equilibrium path. That is, our assessment is the unique that does not requirethe computation of the equilibrium strategies off the equilibrium path, being these equal to the ones on the equilibriumpath. (cid:2)We show now that, if there is a time point t where ι(t) = b and inequality (5) does not hold, then there is notany equilibrium assessment in pure strategies except for a null measure subset of the space of the parameters. InSection 4.1.3 we showed that, if the agents reach a time point t where ι(t) = b and inequality (5) does not hold relyingon the initial beliefs (removed the types whose deadline has expired and normalized the residual probabilities), thenat such time point t our μ is not consistent with the optimal strategies. Here we strengthen such a statement showingthat, given any a = (μ, σ ), where μ is consistent with (6) and σ is sequentially rational given μ, if there is a timepoint t such that ι(t) = b and inequality (5) does not hold, then μ cannot be consistent with σ . Considering the specialcase with two buyer’s types be and bl, we state the following lemma whose proof is reported in Appendix A.4.Lemma 7. With two buyer’s types, if at time point t = Tbeagents can reach such a time relying on the initial beliefs P 0− 2 inequality (5) does not hold and ι(Tbe− 2) (cid:5)= σ ∗b , then it must be that σ ∗blbe(Tbe− 2) = b and the− 2).(Tbe1134N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157Obviously, no system of beliefs consistent with (6) can be consistent also with the strategies prescribed byLemma 7. By considering Lemmas 3 and 7, we can state the following theorem whose proof is reported in Ap-pendix A.5.Theorem 8. Except for a null measure subset of the space of the parameters, if there exists a time point t whereι(t) = b and inequality (5) does not hold, then there is not any equilibrium assessment in pure strategies.Summarily, bargaining with uncertain deadlines can admit more sequential equilibria in pure strategies. Our as-sessment has two properties: there is not any other equilibrium assessment that can be computed backward faster andit is an equilibrium whenever there is an equilibrium assessment in pure strategies.5. Equilibrium analysis in mixed strategiesIn this section we present our assessment in mixed strategies. At first we analyze (Section 5.1) the situation wherethe buyer’s types are two, and then we provide (Section 5.2) the solution in presence of a generic number of buyer’stypes.5.1. Mixed strategy in presence of two typesbbin the place of ωt+1such that, if the agents reach time point t = TbeWe consider a bargaining situation with two buyer’s types, be and bl, and such that condition (5) does not− 2. For the sake of brevity we use ωt+1hold at time point t = Tbe(Tbe ), and analogouslybein the place of ωt+1ωt+1(Tbl ). Condition (5) can be expressed in terms of beliefs as follows: there is a thresholdbl= (x∗(Tbe ))←2[b]−(x∗(Tbe ))←2[s]−2Tbe− 2 with ωω, then(RPb)←s−(x∗(Tbe ))←2[s]bethe equilibrium in the continuation game starting from t = Tbe− 2 can be in pure strategies.5 As we showed in the pre-vious section, since every equilibrium assessment in pure strategies allows the agents to reach time point t = Tbe− 2with initial beliefs on the equilibrium path, then, if ω0> ω, no equilibrium assessment in pure strategies exists.beIn order to overtake such a problem, we introduce an assessment in mixed strategies which does not allow the agentsto reach time point t = Tbeon the equilibrium path. Specifically, in our assessment theTbe− 2.probability ωtbebeThe principle we exploit to make ωbe decreasing on the equilibrium path is: the strategy of bl is pure and prescribes asingle action, whereas the strategy of be is mixed and prescribes the randomization between the optimal action of bland another action. The optimal action of bl is the unique action that allows the game to continue on the equilibriumpath. Therefore, once the optimal action of bl is observed, the probability ωbe is reduced according to the probabilitywhereby be makes the observed action. In order to be an equilibrium it is also necessary that s randomizes.Tbebemonotonically decreases on the equilibrium path and is equal to ωat time point t = Tbe− 2 with ω(cid:2) ωTbebeTbebeTbebeTbebe> ω−2−2−2−2−2−2According to this principle, several equilibrium assessments in mixed strategies can be found when condition (5)does not hold. We provide an assessment with the property that it is an equilibrium if and only if condition (5) doesnot hold. We preliminarily introduce the construction of the assessment and we subsequently discuss the details.We start backward from time point t = T (with two b’s types T = min{Tbl , Ts}). In the continuation game startingfrom t = Tbe the equilibrium strategy must be pure. Easily, be’s dominant action is exit, whereas bl will never makeexit before t = T . Thus be and bl have strictly different optimal actions and then be will never make the optimal actionof bl. In the interval [Tbe , T ] we use the complete information construction presented in Section 2.2, where the uniquepossible b’s type is bl. We denote such a construction by x∗− 2, theblbackward induction construction discussed in Section 4.1.2 is altered introducing a mixed strategy from Tbe on, untilthe mixed strategy leads to an equilibrium. We denote with [<t ] the interval of time wherein the assessment is int ,>t where the assessment cannot be in mixedmixed strategies. It is such that<strategies. The time pointt will be determined during the backward induction construction checking some conditions.(t). When condition (5) does not hold at t = Tbe<t is the first time point beforet = Tbe and>>5 A continuation game is defined as a subgame, but its root node can be a non-singleton information set.N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571135Fig. 4. Backward induction construction with RPb = 1, RPs = 0, δb = 0.85, δs = 0.85, (cid:8)Tb = {7, 10}, P 0b= {0.9, 0.1}(cid:9), Ts = 11, ι(0) = s.<t > 0 and the construction follows from t = <t to t = 0 inIf these conditions are satisfied before reaching t = 0, thent = 0.pure strategies as discussed in Section 2.2, otherwiseThe construction in mixed strategies employed in [<t ,<>t ] is based on two sequences of offers: a “high” sequencedenoted by ˆx(t), and a “low” sequence denoted by ˇx(t) (see Fig. 4 for an example; the details of the construction willbe discussed later). Basically, at each time point t where they act, be and s mix between offering the values given bythe two sequences ˆx(t) and ˇx(t) with probabilities αt and βt , respectively, whereas bl acts a pure strategy offeringˇx(t). The probabilities αt and βt will be computed such that the mixed strategy is sequentially rational. We presentour assessment in mixed strategies as follows:(i) a priori fix two sequences ˆx(t) and ˇx(t) whereby agents’ optimal actions will be based and derive agents’strategies σ ∗,(ii) determine the probabilities αt and βt whereby agents will randomize over their optimal actions,(iii) produce the system of beliefs to be Bayes consistent with the strategies σ ∗ on the equilibrium path,(iv) a posteriori prove the sequential rationality of the strategies and the consistency of the system of beliefs off theequilibrium path.We discuss in what follows the application of our method.5.1.1. Mixed strategiesWe fix the two sequences of offers ˆx(t)s and ˇx(t)s as follows:(t), i.e., the optimal offer at time point t in the complete information game where b is of the type be,• ˆx(t) = x∗be• ˇx(t) = (x∗(Tbe ))←(Tbecalculated as prescribed in Section 2.2,−t)[b], i.e., the backward propagation of x∗(Tbe ) with respect to b’s utility.1136N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157The equilibrium strategies are built backward as follows:• Tbe• <(e(t);(cid:2) t: the agents’ strategies are pure and equal to those provided in Section 4.1, i.e. bl and s base their optimal(cid:5)= 0: the optimal actions at t = Tbeactions on x∗(t) ≡ x∗blt +2 (cid:2) t < Tbe and ωt− 2 cannot be pure since inequality (5) does not hold.blMoreover, by construction, b cannot have pure optimal actions at any time point t ∈ [<t +2, Tbe ). The conditionrequired to have pure optimal actions at time point t when the construction from time point t + 1 on follows inmixed strategies is a generalization of inequality (5). Specifically, such a condition requires that ι(t) = b and thatinequality∗(t + 1)|P 0(7)holds, where e∗(t + 1)|P 0b denotes the optimal equivalent of s at time point t + 1 calculated with initial beliefsP 0b after removing the types whose deadline has expired and normalizing the residual probabilities. (In absenceof mixed strategies at t it holds e∗(t + 1)|P 0b– bl employs a pure strategy based on ˇx(t): if σs(t − 1) (cid:2) ˇx(t − 1), then she accepts, otherwise she offers ˇx(t);– be employs a mixed strategy based on ˇx(t) and ˆx(t): if σs(t − 1) (cid:2) ˇx(t − 1), then she accepts; if ˇx(t − 1) <σs(t − 1) (cid:2) ˆx(t − 1), then she accepts with probability 1 − αt and rejects to offer ˇx(t) with probability αt ;otherwise she offers ˆx(t) with probability 1 − αt and offers ˇx(t) with probability αt ;= e∗(t + 1).) The agents employ mixed strategies as follows:b )←s (cid:2) ( ˇx(t + 1))←b– s employs a mixed strategy based on ˇx(t) and ˆx(t): if σb(t − 1) (cid:3) ˆx(t − 1), then she accepts; if ˆx(t − 1) >σb(t − 1) (cid:3) ˇx(t − 1), then she accepts with probability βt (σs(t − 2)) and rejects to offer ˆx(t) with probability1 − βt (σs(t − 2));6 otherwise she offers ˆx(t);t +2 (cid:2) t < Tbe and ωtbltion game, whereas the optimal action of bl is the one she employs when= 0: the optimal actions of s and of be are those in the corresponding complete informa-<• <• t = <• t (cid:2) <t +1: s employs a pure strategy maximizing her equivalent value between offering ˇx(t : being x∗(t) < ˇx(t), inequality (7) holds and then the optimal actions are pure and are those provided int +1);t +2 (cid:2) t (cid:2) Tbe and ωt(cid:5)= 0;blt +1) and ˆx(<<Section 2.2 where x∗(<t ) = (e∗(<t +1))←s.By construction, time point t = <t is the first time point back from Tbe where inequality (7) holds and the equilibriumcan be in pure strategies. Obviously, when the backward construction reaches the initial time point without holdinginequality (7), part of the above strategy does not appear.The equivalent value e(x, t) of s for any t such that<t < t < Tbe can be calculated as follows (we recall thatEUs(offer(x), t) = Us(e(x, t), t)):⎧⎨e(x, t) =⎩x > ˆx(t)ωtbeˆx(t) (cid:3) x > ˇx(t) ωtbeˇx(t) (cid:3) xx· (1 − αt+1) · ( ˆx(t + 1))←s + (1 − ωtbe· (1 − αt+1) · x + (1 − ωtbe· (1 − αt+1)) · ( ˇx(t + 1))←s· (1 − αt+1)) · ( ˇx(t + 1))←sFormally, the equilibrium strategies can be defined by specifying the agents’ choice rules on the basis of x∗(t),ˆx(t), ˇx(t), αt , and βt as extension of the ones reported in (3) and (4) as follows:σ∗bl(t) =⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩t = 00 < t (cid:2) <t<t < t (cid:2) TbeTbe < t < TT (cid:2) t (cid:2) TblTbl < t(cid:2)offer(x∗(0))(cid:2)acceptoffer(x∗(t))acceptoffer( ˇx(t))acceptoffer(x∗(t))acceptexit(cid:2)(cid:7)if σs(t − 1) (cid:2) x∗(t − 1)otherwiseif σs(t − 1) (cid:2) ˇx(t − 1)otherwiseif σs(t − 1) (cid:2) x∗(t − 1)otherwiseif σs(t − 1) (cid:2) RPbotherwiseexit6 The dependency of βt on σs(t − 2) is needed to grant the existence of the equilibrium also when s acts at time point t − 2 off the equilibrium.σ∗be(t) =⎧⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩t = 00 < t (cid:2) <t<t < t < Tbet = TbeTbe < tt = 00 < t <<tt = <t +1Tbe < t < TT (cid:2) t (cid:2) Tsσ∗s (t) =<t +3 (cid:2) t < TbeN. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571137if σs(t − 1) (cid:2) x∗(t − 1)otherwise⎧offer(x∗(0))(cid:2)acceptoffer(x∗(t))⎪⎪⎪⎪⎨accept(cid:2)⎪⎪⎪⎪⎩acceptoffer( ˇx(t)) αtoffer( ˆx(t))offer( ˇx(t)) αt(cid:2)(cid:7)1 − αt1 − αtacceptexitif σs(t − 1) (cid:2) RPbotherwiseexitif σs(t − 1) (cid:2) ˇx(t − 1)if ˇx(t − 1) < σs(t − 1) (cid:2) ˆx(t − 1)otherwise(cid:9)offer(x∗(0))(cid:2)acceptoffer(x∗(t))acceptoffer(x)x = arg maxaccept(cid:2)⎧⎪⎨if σb(t − 1) (cid:3) x∗(t − 1)otherwisex∈{ ˇx(<t +1), ˆx(<t +1)} e(x,⎪⎩(cid:2)(cid:7)acceptoffer( ˆx(t))βt (σs(t − 2))1 − βt (σs(t − 2))offer( ˆx(t))acceptoffer(x∗(t))acceptexitif σb(t − 1) (cid:3) x∗(t − 1)otherwiseif σb(t − 1) (cid:3) RPsotherwise<t )<t ) (cid:3) x∗(<t +1)if σb(otherwiseif σb(t − 1) (cid:3) ˆx(t − 1)if ˆx(t − 1) > σb(t − 1) (cid:3) ˇx(t − 1)otherwiseTs < texit5.1.2. Mixing probabilitiesThe values of the probabilities αt and βt (σs(t − 2)) must be such that the following conditions hold:• αt is such that s, once she has observed the offer ˇx(t), is indifferent between accepting such an offer and rejectingit to offer ˆx(t + 1) at time point t + 1. Formally, Us( ˇx(t), t + 1) = EUs(offer( ˆx(t + 1)), t + 2) given that σb(t) =offer( ˇx(t));• βt (σs(t − 2)) is such that be, if σs(t − 2) ∈ ( ˇx(t − 2), ˆx(t − 2)], is indifferent between accepting σs(t − 2) andrejecting it to offer ˇx(t − 1) at time point t − 1, and, if σs(t − 2) /∈ ( ˇx(t − 2), ˆx(t − 2)], is indifferent betweenoffering ˆx(t − 1) and offering ˇx(t − 1) at time point t − 1. Formally, Ub(σs(t − 2), t − 1) = EUb(offer( ˇx(t − 1)), t)and Ub( ˆx(t − 1), t) = EUb(offer( ˇx(t − 1)), t), respectively.7The formulas to compute the values of αt and βt (σs(t − 2)) areαt =ωω∗,t+1be∗,tbe· (1 − ω∗,tbe∗,t+1be)),· (1 − ω⎧⎨βt (σs(t − 2)) =⎩σs(t − 2) ∈ ( ˇx(t − 2), ˆx(t − 2)]σs(t − 2) /∈ ( ˇx(t − 2), ˆx(t − 2)]( ˆx(t))←2[b]−σs(t−2)( ˆx(t))←2[b]− ˇx(t−2)( ˆx(t))←2[b]− ˆx(t−2)( ˆx(t))←2[b]− ˇx(t−2)whereω∗,tbe=⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩t > Tbe<t +1 < t (cid:2) Tbe0⎧⎨⎩ι(t) = b ωι(t) = s∗,t+1be(1−ω∗,t+2bet (cid:2) <t +1ω0be7 The expected utility EUb can be defined similarly to EUs.)· ˇx(t−1)+ω· ˆx(t−1)−( ˇx(t+1))←2[s]∗,t+2beˆx(t−1)−( ˇx(t+1))←2[s](8)(9)1138N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157The sequence of ω∗,tbes constitutes the beliefs of s as forced by Bayes’ rule when she observes the optimal offerˇx(t − 1).>t +2,t ]. This ishave well defined∗,tbeIt can be easily showed that the values of βt (σs(t − 2)) are always well defined, i.e. ∈ (0, 1) in [<because at any time point t belonging to this interval ˆx(t) > ˇx(t). We now show that αt and ωvalues of probability, i.e., αt ∈ (0, 1) and ω∈ [ωBy considering time point t = Tbe], only in the interval [<, ω0bet +2,− 2 and formulas (8) and (9), we can state the following lemma whose proof isTbebe∗,tbet ].−2>reported in Appendix A.6.Lemma 9. When at time point t = Tbeprobability values and vice versa.− 2 condition (5) does not hold, then αTbe−2 and ω−1∗,Tbebehave well definedBy considering any time point t such that<t +2 < t < Tbe , we can now state the following lemma whose proof isreported in Appendix A.7.Lemma 10. Consider a time point t such that the construction from time point t + 2 to Tbe is based on the above mixedstrategies: when at such a time point t condition (7) does not hold, then αt and ωhave well defined probabilityvalues and vice versa.∗,t+1beBy the inductively applying Lemma 10 we have the following theorem whose proof is trivial and then omitted.Theorem 11. When condition (5) does not hold at t = Tbeprobabilities αt and ωare well defined and vice versa.∗,t+1be− 2, then exclusively in the interval [<t ,>t ] the values of the⎧⎪⎪⎨⎪⎪⎩This means that the values of the probabilities αt and ω∗,t+1in our mixed strategy are well defined at any time pointbet where agents cannot employ pure strategies. Therefore, the use of our mixed strategy is perfectly complementary tothe use of our pure strategy.5.1.3. System of beliefsNow we specify the system of beliefs μ. For any t such that t (cid:2) <(cid:2) t μ is the system of beliefs employedt +2 (cid:2) t (cid:2) Tbe , on the equilibrium path, μ is the system of beliefs induced by thein Section 4.1.1. For any t such thatBayes’ rule according to the above equilibrium strategies, while off the equilibrium path it is such that s, once she hasobserved σb(t − 1) ∈ [ ˇx(t − 1), ˆx(t − 1)), is indifferent between accepting it or rejecting it to offer ˆx(t). Specifically μis:t +1 or Tbe<= 1σb(t − 1) > ˆx(t − 1)ωtbeˆx(t − 1) (cid:3) σb(t − 1) > ˇx(t − 1) ωtbeˇx(t − 1) (cid:3) σb(t − 1)ωtbe+ (1−ω)·(σb(t−1)− ˇx(t−1))∗,tbe( ˆx(t−1)− ˇx(t−1))μ(t) == ω= ω∗,tbe∗,tbeWe can now illustrate the bargaining situation depicted in Fig. 4, where RPb = 1, RPs = 0, δb = 0.85, δs = 0.85,(cid:8)Tb = {7, 10}, P 0= {0.9, 0.1}(cid:9), Ts = 11, and ι(0) = s. Easily, the backward induction construction starts from timebpoint T = 10 where s acts. Since at any time point from t = 7 to t = 10 s believes b’s deadline to be t = 10, theconstruction in these time points is the one with complete information where b’s deadline is t = 10. On the equilibriumpath be makes exit in this interval of time. The optimal offer x∗(9) of bl at t = 9 is RPs = 0, the optimal offer x∗(8)of s at t = 8 is (x∗(9))←b = 0.15, and the optimal offer x∗(7) of bl at t = 7 is (x∗(8))←s (cid:13) 0.13.Consider time point t = 6, the strategy of s cannot be pure. We build the pure strategy construction as prescribedby Section 4.1 and we show that the condition that must be satisfied to have pure strategies, i.e. condition (5), does nothold. According to the pure strategy construction presented in Section 4.1, since t = 7 is a possible b’s deadline, attime point t = 6 s chooses the offer that maximizes her expected utility between RPb = 1 and (x∗(7))←b (cid:13) 0.26. Sincee(RPb, 6) (cid:13) 0.91 and e((x∗(7))←b, 6) (cid:13) 0.26, the optimal offer x∗(6) of s is RPb and the relative optimal equivalentvalue e∗(6) is 0.91. In the figure, this value of e∗(6) is labeled with e∗(6)|P 0b (this is because in what follows weN. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571139introduce a mixed strategy and then the value of e∗(6) will be modified). At time point t = 5 the condition (5) doesnot hold, being (e∗(6)|P 0b )←s (cid:13) 0.78 > (x∗(7))←2[b] (cid:13) 0.37.Therefore, we introduce a mixed strategy at time points t = 5 and t = 6 as prescribed by Section 5.1.1. The “high”sequence of offers ˆx(t)s is: ˆx(6) = RPb = 1, and ˆx(5) = ( ˆx(6))←s = 0.85. The “low” sequence of offers ˇx(t)s is:ˇx(7) = x∗(7) (cid:13) 0.13, ˇx(6) = ( ˇx(7))←b (cid:13) 0.37, and ˇx(5) = ( ˇx(6))←b (cid:13) 0.46. The optimal actions at time point t = 5are: be randomizes between offering ˆx(5) with probability 1 − α5 and offering ˇx(5) with probability α5, bl offersˇx(5); at time point t = 6 the optimal actions of s are to accept ˆx(5) and to randomize between accepting ˇx(5) withprobability β6 and rejecting it to offer ˆx(6) with probability 1 − β6. The value of α5 is calculated imposing that, whenb offers ˇx(5), the expected utilities of s from accepting it and offering ˆx(6) are the same: EUs(accept, 6) = Us( ˇx(5), 6)and EUs(offer( ˆx(6)), 6) = ωis the probability that b is of thetype be, once she has offered ˇx(5) and s has upgraded her beliefs according to μ. The value of α5 is (cid:13) 0.21. Similarly,the value of β6 is calculated imposing that the expected utilities of be from offering ˆx(5) and ˇx(5) are equal. Thevalue of β6 is (cid:13) 0.24. Due to the presence of the mixed strategy, the optimal equivalent value e∗(6) is lower thane∗(6)|P 0) · Us( ˇx(7), 7) where ω· Us( ˆx(6), 7) + (1 − ωb and it is (e∗(6))←s = ˇx(5).Given the optimal actions of the agents from t = 5 to t = 10, s chooses at time point t = 4 her optimal actionbetween offering ˆx(4) and ˇx(4). In the figure it can be observed that her optimal offer is ˆx(4) and the relative equivalentvalue e∗(4)|P 0s is such that at time point t = 3 condition (7) does not hold. Thus, it is necessary to introduce a mixedstrategy also at time points t = 4 and t = 5. The process continues to the initial time point t = 0. Notice that at timepoint t = 1 condition (7) holds and the construction follows from t = 2 to t = 0 in pure strategies.∗,6be∗,6be∗,6be5.1.4. Equilibrium assessmentCollecting the strategies, the mixing probabilities, and the system of beliefs presented in the previous sections, wecan state the following theorem whose proof is reported in Appendix A.8.Theorem 12. The above assessment is a sequential equilibrium if and only if condition (5) does not hold at time pointt = Tbe− 2.The calculation of the equilibrium can be accomplished step by step from the deadline of the bargaining back to theinitial time, determining, in addition to the sequences of the optimal offers x∗(t)s and of the optimal equivalent valuese∗(t)s computed in the solution with pure strategies, the sequences ˆx(t), ˇx(t), αt , ω, and βt . The computationaltime required to find the equilibrium depends linearly on the length of the bargaining and requires only a maximization− 1 (exactly as the equilibrium in pure strategies). Analogouslybetween two possible offers at the time point t = Tbeto what showed in Section 4.2, it can be showed that there is not any equilibrium assessment in mixed strategies thatcan be computed backward faster than ours.∗,tbe5.2. Mixed strategy in presence of more than two typesWe show here how the mixed strategy presented in the previous section can be generalized to the case where thebuyer’s types are more than two. We omit in this section the formulas to compute the equilibrium, because they are longand unnecessary for the comprehension of the result; we report them in Appendix B. The presence of many types raisesseveral complications in the calculation of the equilibrium strategies. Nevertheless, the principle whereby the mixedstrategy is based still holds: there are two sequences of offers, a “high” sequence ˆx(t) and a “low” sequence ˇx(t),and the agents base their strategies (pure or mixed) on these. The equilibrium strategies can be calculated backwardfrom time point t = T to the initial time point t = 0 employing pure strategies and inserting, when needed, mixedstrategies. Therefore, the equilibrium path will be composed of intervals of time points where the agents have a singleoptimal actions and of intervals of time points where the agents randomize over two optimal actions (see, e.g., Fig. 6).We consider the most general case where all the time points before T are possible deadlines of b and there exists at −2. We need to build the equilibrium in mixedtime pointt ) = b and the equilibrium strategy can be pure. Exactly as with twostrategies fromtypes, the construction can be produced backward checking at each time point t where ι(t) = b whether condition (7)is satisfied or not. In the affirmative case, the construction can be in pure strategies, otherwise, the construction mustbe in mixed strategies.t ) = b and condition (5) does not hold at t = ><t where>t such that ι(>t back to<t is such that ι(<>1140N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157Although b’s types can be many, we have found an equilibrium assessment where different b’s types can have atany time point t at most three different optimal actions. This means that the buyer’s types can be grouped in threedistinct sets and we can limit our analysis to them without searching for the optimal actions of each single buyer’stype. This leads to an efficient computation of the equilibrium: at any time point t no more than three different optimalactions must be computed. Denoting by θb(t) the b’s type whose deadline is at t, at any time point t the buyer’s typescan be grouped as follows:>• ˆ(cid:11)b(t): all the types belonging to ˆ(cid:11)b(t) have pure strategies accepting any offer lower than or equal to ( ˆx(t))←bb (t)t −1) = ∅ the initialization set of ˆ(cid:11)b, and ˆ(cid:6)t1b (t2) =θb(t)∈ ˆ(cid:11)b(t2) ωt1and otherwise offering ˆx(t); let be ˆ(cid:11)b(the cumulative probability of ˆ(cid:11)b(t2) at time point t1;• ˜(cid:11)b(t): all the types belonging to ˜(cid:11)b(t) have mixed strategies accepting any offer lower than or equal to ( ˆx(t))←bt −1) = ∅ the initialization setwith probability 1 − αt and rejecting it to offer ˇx(t) with probability αt ; let be ˜(cid:11)b(of ˜(cid:11)b, and ˜(cid:6)t1b (t) the cumulative probability of ˜(cid:11)b(t2) at time point t1;• ˇ(cid:11)b(t): all the types belonging to ˇ(cid:11)b(t) have pure strategies accepting any offer lower than or equal to ˇx(t −t −1) = {θb(t): t (cid:3) >b (t2) =1) and otherwise offering ˇx(t); let be ˇ(cid:11)b((cid:8)t } the initialization set of ˇ(cid:11)b, and ˇ(cid:6)t1θb(t)∈ ˜(cid:11)b(t2) ωt1b (t2) =(cid:8)(cid:8)>>θb(t)∈ ˇ(cid:11)b(t2) ωt1b (t) the cumulative probability of ˇ(cid:11)b(t2) at time point t1.Analogously to the situation with two types, s has mixed equilibrium strategies based on ˆx(t) and ˇx(t) with aβt (σs(t − 2)).>The construction can be done backward fromt on as follows. The “low” sequence of offers ˇx(t)s is easily de-terminable, being ˇx(t) = (x∗(t −t)[b]. The “high” sequence of offers ˆx(t)s is harder to find and depends on theprobability values of the s’s beliefs. When ι(t) = s the value of ˆx(t) is the optimal offer that s would make at such timepoint relying on the initial beliefs after removing the types whose deadline has expired and normalizing the residualprobabilities. Precisely, at any time point t such that ι(t) = s, s maximizes her expected utility given her beliefs. Ift = >t −1 the equivalent value of s is:t ))←(>>e(x, t) =⎪⎩ˇ(cid:6)tb(t + 1) · (( ˇx(t + 1))←s − RPs) + RPsx > RPbb(t + 1) · (x − RPs) + ˇ(cid:6)tRPb (cid:3) x > ˇx(t) ωtb(t + 1) · (x − RPs) + RPsˇx(t) (cid:3) x(cid:6)tb(t + 1) · (( ˇx(t + 1))←s − RPs) + RPswhereas, if t <e(x, t) =>t −1, the equivalent value of s is:⎧( ˆ(cid:6)tx > RPbRPb (cid:3) x > ( ˆx(t + 1))←b ωt( ˆx(t + 1))←b (cid:3) x > ˇx(t)b(t + 1) + (1 − αt+1) · ˜(cid:6)tb(t + 1) + ˇ(cid:6)t+ (αt+1 · ˜(cid:6)tb(t + 1) · (x − RPs) + ( ˆ(cid:6)t× (( ˆx(t + 1))←s − RPs) + (αt+1 · ˜(cid:6)t× (( ˇx(t + 1))←s − RPs) + RPsb(t + 1) − αt+1 · ˜(cid:6)tb(t + 1)) · (( ˆx(t + 1))←s − RPs)b(t + 1)) · (( ˇx(t + 1))←s − RPs) + RPsb(t + 1) + (1 − αt+1) · ˜(cid:6)tb(t + 1) + ˇ(cid:6)tb(t + 1)) · (x − RPs) + ( ˇ(cid:6)tb(t + 1)) · (( ˇx(t + 1))←s − RPs) + RPsb(t + 1))b(t + 1))b(t + 1)(1 − ˇ(cid:6)tˇx(t) (cid:3) x+ αt+1 · ˜(cid:6)tb(t + 1) · (x − RPs) + RPst −1 the value that maximizes the equivalent value e(x, t) is RPb. At each time point t <At time point t = >t −1 onlythree possible offers can maximize e(x, t): RPb, ( ˆx(t + 1))←b, or ˇx(t). The value x that maximizes e(x, t) is the valueto be assigned to ˆx(t).(cid:6)tThe determination of ˆx(t) when ι(t) = b is more complicated than when ι(t) = s. Indeed, in presence of more thantwo b’s types, the choice rules that each b’s type employs to choose her optimal actions can be of four different forms.Each choice rule has mutually exclusive conditions, i.e., when a specific choice rule can be employed to produce anequilibrium the other three cannot, and vice versa. When the condition of a specific choice rule is satisfied, it holdsthat the strategies it prescribes are sequentially rational given the system of beliefs, the mixing probabilities are welldefined, and the system of beliefs is consistent with the strategies (the proof is omitted, being similar to the proof of>⎧⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571141the case with two types, but long). Furthermore, for each possible setting of the parameters and for each time point t,at least one choice rule can be employed. Each choice rule is characterized by:• a specific composition of the sets ˆ(cid:11)b(t), ˜(cid:11)b(t), and ˇ(cid:11)b(t),• a specific strategy (cid:8)σ ∗(t), σ ∗• a specific system of beliefs μ(t).(t), σ ∗(t), σ ∗˜(cid:11)b(t)ˇ(cid:11)b(t)ˆ(cid:11)b(t)s (t + 1)(cid:9) with specific formulas to compute ˆx(t) and αt ,In what follows we just overview the four choice rules and their construction; we report in Appendix B the formulasto compute them. Given the construction from time point T to time point t + 1, it is possible to find the choice rule thatcan be employed at a time point t when ι(t) = b to produce the equilibrium. The principal condition that characterizesthe four choice rules is the value of ˆx(t + 1) computed in the construction from time point T to time point t + 1.We call choice rule 1 the choice rule that can be employed when ˆx(t + 1) = ( ˆx(t + 2))←b. If ˆx(t + 1) = RPb threepossible choice rules can be employed, we call them choice rules 2.1, 2.2, and 2.3. The conditions that discriminatetheir employment are mathematically complicated, and then omitted here. Once the conditions have been checkedand the choice rule to employ has been found, it is necessary to compose the sets ˆ(cid:11)b(t), ˜(cid:11)b(t), and ˇ(cid:11)b(t), and tocalculate ˆx(t), αt , and βt+1.Summarily, once the optimal action of s has been found at time point t given the construction from time point T tot + 1, the choice rule to employ at time point t − 1 can be found by checking some conditions. The equilibrium canbe found in time linear in the length of the bargaining and asymptotically independent of the number of buyer’s types.6. Solving algorithm and extension to the multiple issue settingIn this section we initially present (Section 6.1) our solving algorithm and then we show (Section 6.2) how ouralgorithm can be extend to address the situation where the issues to negotiate are more than one.6.1. Solving algorithmWe collect the results presented in Sections 4 and 5 and we provide an algorithm to compute the equilibriumassessment. Specifically, in the two previous sections we have provided the equilibrium assessment in function ofsome parameters, here we provide an algorithm to compute such parameters. The inputs of the algorithm are δb, δs,b(t)(cid:9), Ts, RPb, RPs, and ι(t). The output of the algorithm is the set of the parameters needed to specify the(cid:8)Tb, ω0∗,t+1equilibrium assessment: x∗(t), e∗(t), ˆx(t), ˇx(t), αt , βt , ˜(cid:6)bThe algorithm is simple (see Algorithm 1) and works iteratively from time point t = T to time point t = 0. At eachiteration t the algorithm adds a time point backward to the current partial solution according to the agent that acts at(t), ˆ(cid:11)b(t), ˜(cid:11)b(t), and ˇ(cid:11)b(t).}}1: T ← min{Ts, maxi {Tbi2: initialize variables, i.e., x∗, e∗, ˆ(cid:11)b, ˜(cid:11)b, ˇ(cid:11)b, at time point T − 13: for (t = T − 2, t (cid:3) 0, t = t − 1) do4:5:if ι(t) = scompute x∗(t) as the value that maximizes e(x, t) relying on the initial beliefs P 0and normalizing the residual probabilities, as prescribed in Section 5.2b , after removing the types whose deadline has expired6:7:8:9:10:elseif condition (7) holds thencompute x∗(t) as prescribed in Section 4.1elsedetermine the choice rule in mixed strategies whose conditions are satisfied, compose the sets ˆ(cid:11)b, ˜(cid:11)b, ˇ(cid:11)b, and compute ˆx(t), ˇx(t),αt , βt+1 as prescribed in Section 5.2 and Appendix Bend if11:end if12:13: end for∗,t+114: return [x∗(t), e∗(t), ˆx(t), ˇx(t), αt , βt , ˆ(cid:11)b(t), ˜(cid:11)b(t), ˇ(cid:11)b(t), ˜(cid:6)b(t)]Algorithm 1. EQUILIBRIUM_FINDER (δb, δs, (cid:8)Tb, ω0b(t)(cid:9), Ts, RPb, RPs, ι(t)).1142N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157time point t. At each time point t where ι(t) = s, the optimal offer of s is computed as a maximization relying on theinitial beliefs, after removing the types whose deadline has expired and normalizing the residual probabilities, amongRPb, ( ˆx(t + 1))←b, and ( ˇx(t + 1))←b, as prescribed in Section 5.2. At each time point where ι(t) = b, we need tocheck condition (7): if it holds then b’s strategy is pure, otherwise it is mixed. If it is pure, then the optimal offerat time point t can be simply computed in closed form, otherwise a mixed strategy is introduced at time point t andconsequently at time point t + 1. In this latter case it is necessary to determine what choice rule to employ checkingthe conditions reported in Appendix B. Subsequently, the sets ˆ(cid:11)b, ˜(cid:11)b, and ˇ(cid:11)b are composed and the values of ˆx(t),ˇx(t), αt , and βt+1 are computed as prescribed in Appendix B.The algorithm has a computational complexity that is linear in T and asymptotically independent of the numberof buyer’s types. In the worst case it requires to check three conditions at any time point: when ι(t) = s, three valuescan maximize e(x, t), whereas, when ι(t) = b, at most three conditions must be checked to determine the choice tobe employed.We show two examples of backward induction construction produced with our algorithm, reporting also, for anytime point wherein the strategies are mixed, the form of the employed choice rule, the composition of the sets ˆ(cid:11)b,˜(cid:11)b, and ˇ(cid:11)b, and the values of probabilities of αt and βt . In Fig. 5 we report an example of construction wherethe probability distribution is uniform in [1, maxi{Tbi}]. In Fig. 6 we report an example of construction where theprobability distribution is based on two Gaussian-like distributions with means at t = 7 and t = 18.We discuss some details of the bargaining situation depicted in Fig. 5. The time point from which we apply thealgorithm is T = min{max{Tb}, Ts} = 20. At t = 20 s acts and she accepts any offer equal to or greater than RPs = 0,otherwise she makes exit. At t = 19, the optimal offer x∗(19) of θb(20) is RPs = 0, instead θb(19) does not make anyoffer, but makes exit.Consider time point t = 18. The beliefs of s, obtained from P 0b after removing the types whose deadline has expiredb (t) = 0.3 for any t ∈ [18, 20]. Being t = 19 a possible deadline of b,and normalizing the residual probabilities, are ω18at t = 18 s chooses the offer between RPb and (x∗(19))←b that maximizes her expected utility. The equivalent valuee(RPb, 18) is e(RPb, 18) = ω18b (20) · (RPs − RPs) + RPs = 0.5, instead the equivalent valueb (19) · ((x∗(19))←b − RPs) + RPs = 0.016. Therefore, the optimal offere((x∗(19))←b, 18) is e((x∗(19))←b, 18) = (cid:6)18x∗(18) of s is RPb = 1 with e∗(18) = 0.016. The value of e∗(18) is labeled in figure with e∗(18)|P 0b (this is becausein what follows we introduce a mixed strategy and then the value of e∗(18) will be modified).b (19) · (RPb − RPs) + (cid:6)18Consider time point t = 17. Condition (7) does not hold, being (e∗(18))←s > (x∗(19))←2[b]. We need thereforeto introduce a mixed strategy at t = 17 and t = 18. Checking the conditions reported in Appendix B, we see that thechoice rule to be employed is the choice rule 1. It prescribes that at t = 17 the types θb(18) and θb(19) randomizebetween offering ˇx(18) and ˆx(18) with α17 = 0.09 and 1 − α17, respectively, whereas the type θb(20) offers ˇx(17).The value of ˇx(17) can be easily calculated being (x∗(19))←2[b] = 0.049; the value of ˆx(18), calculated as prescribed(RPb − RPs) + RPs)δs = 0.5. The introduction of a mixed strategy at t = 17 inducesby Appendix B, is (a mixed strategy also for s at t = 18. Exactly, at t = 18, if s receives ˆx(17), she accepts it and, if she receives theoffer ˇx(17), she randomizes between accepting it and rejecting it to offer RPs.ω0b(19)b(17)+ω0ω0Consider time point t = 16. The beliefs of s, obtained from P 0b after removing the types whose deadline hasb (t) = 0.2 for any t ∈ [16, 20]. At t = 16 s chooses the offerexpired and normalizing the residual probabilities, are ω16among RPb, ( ˆx(17))←b (cid:13) 0.5, and ( ˇx(17))←b (cid:13) 0.073 that maximizes her expected utility. The equivalent valuescalculated as prescribed in Section 5.2 are: e(RPb, 16) (cid:13) 0.394, e(( ˆx(17))←b, 16) (cid:13) 0.392, and e(( ˇx(17))←b, 16) (cid:13)0.058. Therefore, the optimal action of s is to offer RPb with e∗(16) = 0.394. The value e∗(16) is labeled in figurewith e∗(16)|P 0b (this is because in what follows we introduce a mixed strategy and then the value of e∗(16) will bemodified).b(18)At t = 15, exactly as it happens at time point t = 17, condition (7) does not hold. Therefore a mixed strategy mustbe introduced at time points t = 15 and t = 16. In this case the choice rule to employ is the choice rule 2.3. Theconstruction continues up to the initial time point as illustrated in figure.6.2. Extension to the multiple issue settingMultiple issue bargaining captures situations where agents can find agreements over the values of several parame-ters. A common example is the negotiation between a buyer and a seller over several attributes of a good, such asN.Gattietal./ArtificialIntelligence172(2008)1119–11571143Fig. 5. Backward induction construction with RPb = 1, RPs = 0, δb = 0.975, δs = 0.975, (cid:8)Tb = [0, 20], P 0b0.05, 0.05, 0.05, 0.05, 0.05, 0.05}(cid:9), Ts = 28, ι(0) = s; we report also the sets ˆ(cid:11)b(t), ˜(cid:11)b(t), and ˇ(cid:11)b(t) and the choice rule in mixed strategies whose conditions are satisfied at time point t .= {0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,1144N.Gattietal./ArtificialIntelligence172(2008)1119–1157Fig. 6. Backward induction construction with RPb = 1, RPs = 0, δb = 0.96, δs = 0.96, (cid:8)Tb = [0, 20], P 0bTs = 28, ι(0) = s; we report also the sets ˆ(cid:11)b(t), ˜(cid:11)b(t), and ˇ(cid:11)b(t) and the choice rule in mixed strategies whose conditions are satisfied at time point t .= {0, 0, 0, 0, 0, 0.02, 0.11, 0.24, 0.11, 0.02, 0, 0, 0, 0, 0, 0, 0.02, 0.11, 0.24, 0.11, 0.02}(cid:9),N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571145the price, the level of quality, the guarantee expiration, the delivery time, and so on. The alternating-offers protocolcan be easily extended to capture this situation: the utility functions, the offers, and the acceptance of the agents aredefined on tuples of values, one for each issue (see [7] for more details). For each issue the agents have a specificreservation value and a specific discount factor. However, each agent has usually a single deadline for all the issues,being the deadline relative to the good to negotiate and not to the single issues that characterize the good [6,12] (amodel where agents have different deadlines over different issues is discussed in [7]). Multiple issue bargaining canbe implemented according to different procedures, e.g. the issues can be negotiated sequentially or concurrently. Inwhat follows we consider the in-bundle procedure, where the issues are negotiated concurrently. This is because, asshowed in [11], the in-bundle procedure is the unique procedure that allows one to produce efficient agreements.We denote by m the number of issues to negotiate. In presence of complete information the solution of the multipleissue situation [7] is similar to the solution of the single issue situation described in Section 2.2, but two differencesthere are. First, the optimal offers x∗(t)s are tuples of values that specify a value for each single issue. Second, witha single issue the offers to accept can be easily expressed specifying a threshold on the value of the received offer,e.g. s accepts at t any offer y such that y (cid:3) (x∗(t))←s, with multiple issues instead the threshold is on the utilityof the received offer, e.g. s accepts at t any offer y such that Us(y, t) (cid:3) Us(x∗(t), t). The sequence of the optimaloffers x∗(t)s can be found by backward induction by extending the backward propagation of an offer we defined inDefinition 1. Essentially, the backward induction construction is the same: at each time point t the optimal offer x∗(t)of agent ι(t) is the offer such that agent ι(t + 1) is indifferent at t + 1 between accepting it and making her optimaloffer x∗(t + 1). Formally, Uι(t+1)(x∗(t), t + 1) = Uι(t+1)(x∗(t + 1), t + 2). The difference between the multiple issuesituation and the single issue situation lays on how x∗(t) can be computed given x∗(t + 1). With a single issue, thebackward propagation y = x←i is a function that maps an offer x ∈ R at t to an offer y ∈ R at t − 1 (keeping constantthe utility of agent i) and the calculation of y can be easily accomplished in closed form. With multiple issues, thebackward propagation y = x←i maps an offer x ∈ Rm at t to an offer y ∈ Rm at t − 1 (keeping constant the utilityof agent i) and the calculation of y requires the solution of a convex programming problem [6]. In the most commonsituations where the utility functions are linear, this problem can be solved in time linear in the number of issues, beinga fractional knapsack problem (a proof is provided in [11]). Summarily, the redefinition of the backward propagationof an offer in presence of multiple issues allows one to treat the settings with single issue and with multiple issue inthe same way.The extension of our work in presence of multiple issues can be achieved exactly as in the case with completeinformation. Specifically, for each time point of the bargaining the algorithm produces the sequences x∗(t), e∗(t),ˆx(t), and ˇx(t) solving for each single element of the sequences a fractional knapsack problem. Therefore, the compu-tational complexity of the algorithm is linear in the length of the bargaining, linear in the number of the issues, andasymptotically independent of the number of types of the agent whose deadline is uncertain, i.e. O(m · T ).7. Conclusions and future worksThe game theoretical study of bargaining situations is a prominent issue in computer science, since it allows oneto prescribe the behavior of rational agents. Nevertheless literature lacks of solutions when information is incompleteand the actions available to the agents are infinite. On the one hand, game theory provides an appropriate solutionconcept for extensive-form games with incomplete information, but no solving technique to find it. On the other hand,algorithms available in computer science literature work only with finite games and do not produce systems of beliefs.This pushes researchers to analyze each setting independently and to develop ad hoc specific algorithms.In this paper we have focused on the alternating-offers bargaining with one-sided uncertainty on the deadlines,and we have game theoretically studied it to provide a solution. We have found a couple of agents’ choice rules thatapply an action and a probability distribution over the actions, respectively, and we showed that it is always possibleto produce an equilibrium where the actions at any single time point are those prescribed either by the pure strategychoice rule or by the mixed strategy choice rule. We have showed in addition that our solution is such that there is notany other solution that can be computed backward faster. The solving algorithm we provided is simple, being insteadcomplicated the game theoretical analysis that led to the development of the algorithm. Furthermore, the solvingalgorithm is efficient, being its computational complexity asymptotically independent of the number of types of theplayer whose deadline is uncertain. Exactly, its computational complexity is O(m · T ) where m is the number of issuesand T is the deadline of the bargaining.1146N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157Our work paves the way to a novel approach for employing game theoretical tools in the study of sequential gameswith incomplete information. Schematically, the approach we have followed is organized in the following steps:analysis of the game in the attempt to purge the set of the choice rules to employ to produce an equilibrium, findingthe conditions such that a choice rule can be employed, studying the existence of the equilibrium for all the values ofthe parameters, and, finally, building the solution employing at each time point the choice rule whose conditions aresatisfied.Our intention is to complete our work along two main directions. The first one concerns the extension of ourresults in bargaining situations where other parameters are uncertain (i.e., δi, RPi ) and where the uncertainty overthe deadlines is two-sided. The second one concerns the automatization of the procedure we have followed in ourgame theoretical analysis in order to develop efficient algorithms to solve extensive-form games with incompleteinformation.AcknowledgementsWe are grateful to Francesco Amigoni, Sara Casaletti, Claudio Iuliano, Marusca Morelli, Fioravante Patrone, andManuel Roveri for their advices in the writing of this manuscript and to the three anonymous reviewers for helping inimproving the presentation.Appendix A. Proofs of the main theoretical resultsWe report in this section the proofs of the main results provided in the paper.A.1. Proof of Theorem 2The sequential rationality can be easily proved by mathematical induction. Consistency can be proved by theassessment sequence an = (cid:8)μn, σn(cid:9) where:• σn is the fully mixed strategy profile such that before the real deadline of an agent there is probability 1 − 1performing the action prescribed by σ ∗ and the remaining probability 1allowed actions; while at the deadline or after it there is probability 1 − 1σ ∗ and the remaining probability 1• μn is the system of beliefs obtained applying Bayes’ rule starting from the same a priori probability distributionn2 is uniformly distributed among the other allowed actions.n ofn is uniformly distributed among the othern2 of performing the action prescribed byP 0b as in μ.Each assessment an is “Bayes consistent” by construction. The convergence of σn to σ ∗ is trivial. As to μn, givenany arbitrary legal sequence S of actions (such that the bargaining does not conclude at the end of S and such that s(cid:9) the probabilities that s assigns to b’s deadlines afteris the agent acting after S), call P Snsequence S according to μn. Sequence S might contain actions that could be interpreted as being in accordance tothe strategies σ ∗ (i.e. actions that are the actions prescribed by strategies σ ∗ for some deadline Tbi ); let τ the time ofthe latest of such actions in S (if there are no such actions, set τ = −1 by convention). Let t = |S|. Some calculationshows that, if t (cid:2) T , then, . . . , ωS= (cid:8)ωS, ωSn,bmn,b1n,b2ωSn,bi=(cid:2)t ω0bhTbh⎧⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎩0(cid:8)(cid:8)·ω0bi1t−Tbin(cid:8)+τ <Tbhω0biτ <Tbh(cid:8)+>t ω0bhTbhThereforelimn→+∞ωSn,bi=⎧⎨⎩0ω0bi(cid:8)(cid:2)t ω0Tbhbh(cid:3)t1t−Tbhn·ω0bh<t1t−Tbhn·ω0bhif Tbi(cid:2) tif t < Tbiif Tbi(cid:2) τif τ < Tbi(cid:2) tif t < TbiN. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571147Therefore P Sn converges to the beliefs prescribed by μ in S and, consequently, μn converges to μ. (cid:2)A.2. Proof of Lemma 3Let a∗ = (μ∗, σ ∗) an equilibrium assessment in pure strategies. By contradiction, let σ ∗beby Bayes’ rule the beliefs of s at time point t + 1 are: if action σ ∗bethe sake of brevity we use ωt+1(Tbe ), and analogously ωt+1blbe(cid:8)ωt+1beWe consider at first the case where t (cid:2) Tbein the place of ωt+1= 0, ωt+1bl= 1(cid:9).b(t) is observed, then (cid:8)ωt+1be); if action σ ∗bl(t) (cid:5)= σ ∗(t). Thereforebl= 1, ωt+1= 0(cid:9) (forbl(t) is observed, then− 2. If at time point t b acts on the equilibrium path, then at time pointt + 1 the optimal strategy of s is:• if σ ∗be(t) is observed, then accept any offer greater than or equal to x∗bethe complete information game where be is the type of b) and otherwise offer x∗bepath will be accepted by b at time point t + 2;(t) is observed, then accept any offer greater than or equal to x∗blthe complete information game where bl is the type of b) and otherwise offer x∗blpath will be accepted by b at time point t + 2.• if σ ∗bl(t) (i.e., the optimal offer at time point t in(t + 1) that on the equilibrium(t) (i.e., the optimal offer at time point t in(t + 1) that on the equilibriumWe notice that for a generic time point t where ι(t) = b the two possible equilibrium agreements will be reached no− 2, the agreements are reachedlater than the time point t + 2. Then, since we are considering the case where t (cid:2) Tbeno later than Tbe . We notice also that be and bl have, for any t (cid:2) Tbe , the same utility function. Then, in order to be anequilibrium, σ ∗(t) must be such that the two corresponding equilibrium agreements give the same utilitybeto be and bl, otherwise one type at least deviates. We consider all the possible cases.(t) and σ ∗bl• If σ ∗be(t) and σ ∗bl• If σ ∗be(t) are offers that s will accept at time point t + 1, then, since the outcomes are different byhypothesis, they will give different utilities. As a result, one type at least will deviate from her action to make theaction of the other type.(t) and σ ∗bl(t) are offers that will not be accepted at time point t + 1, then s will make different offersat time point t + 1 according to the observed b’s offer and such offers will be accepted at time point t + 2,(t + 1) > x∗giving thus different utilities. Precisely, being x∗(t + 1), t + 1) <blbeUb(x∗(t + 1), t + 1). As a result, be will deviate from her strategy σ ∗blbe(t) is an offer that will be accepted at time point t + 1 by s, whereas σ ∗• If σ ∗bebl(t + 1), it follows that Ub(x∗be(t) to make σ ∗bl(t) is an offer that will not be(t + 1))←b,(t) given that s will accept it. As a result, bl willaccepted at time point t + 1 by s to offer x∗(t + 1), then, by hypothesis, it should be σ ∗bebl(t + 1))←s = x∗but bl can maximize her utility offering (x∗bebe(t) to make σ ∗deviate from her strategy σ ∗(t).bebl(t) is an offer that will be accepted at time point t + 1 by s, whereas σ ∗bl(t) is an offer that will not be(t) = (x∗accepted at the time point t + 1 by s to offer x∗(t + 1))←b,blbl(t), be can maximize her utility offering x∗(t + 1))←b > x∗(t) or (x∗but if (x∗(t) that willbebeblblbe accepted.8 As a result, be will deviate from her strategy σ ∗(t), except for a null measure subset of the spacebeof the parameters, exactly when x∗(t). (For the sake of clarity, we report inbeFig. A.1 an example of construction wherein be and bl have different equilibrium strategies at time point t = 0.)(t + 1), then, by hypothesis, it should be σ ∗be(t + 1))←b, to make σ ∗bl(t + 1))←b < x∗be(t) = (x∗be(t) ≡ (x∗bl• If σ ∗be(t).Thus, it follows by contradiction that σ ∗beters.(t) = σ ∗bl(t), except for a null measure subset of the space of the parame-We consider now the case where t = Tbet = Tbe the optimal strategy of s is:− 1. If at time point t b acts on the equilibrium path, then at the time point8 Notice that if (x(t + 1).to x∗be∗bl(t + 1))←b < x∗be(t), then (x∗bl(t + 1))←b will be rejected by s to offer x∗be(t + 1). And x∗be(t) is better for be with respect1148N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157Fig. A.1. Equilibrium with RPb = 1, RPs = 0, δb (cid:13) 0.8222, δs = 0.8, (cid:8)Tb = {8, 9}, P 0= {0.8, 0.2}(cid:9), Ts = 10, ι(0) = b where at time point t = 0bthe two types have different pure equilibrium strategies; the equilibrium existence is granted by the singularity (RPs)←3[bs]2[b] ≡ (RPb)←3[sb]s.• if σ ∗be(Tbe− 1) is observed, then accept any offer greater than or equal to RPs and otherwise exit (on the equilib-rium path be will not accept anything beyond Tbe );• if σ ∗bl(t) is observed, then accept any offer greater than or equal to x∗bl(Tbe− 1) and otherwise offer x∗bl(Tbe ) thaton the equilibrium path will be accepted at time point Tbe+ 1.The proof is analogous to the case where t (cid:2) Tbepoint t = Tbe . We consider all the possible cases.− 2. Notice that be and bl have the same utility function at time• If both σ ∗be(Tbe− 1) and σ ∗bl(Tbe− 1) are offers that would be accepted by s at t = Tbe , then one between be andbl would deviate from her strategy to make the strategy of the other type.− 1) are offers that will not be accepted at time point t + 1, then both be and bl deviate• If σ ∗be− 1) and σ ∗blfrom their strategies to offer x∗bl(Tbe(Tbe(Tbe− 1) that will be accepted by s.(Tbe• If σ ∗be− 1) is an offer that will be accepted at time point Tbe by s, whereas σ ∗blbe accepted at time point Tbe by s to offer x∗blbut, independently of the conjectures off the equilibrium path, the offer x∗blboth be and bl would offer x∗− 1).bl(Tbe ), then, by hypothesis, it should be σ ∗bl• If σ ∗(Tbebl(Tbe ), then, by hypothesis, it should be x∗bl(Tbe ))←b. Since type be will gain utility equal to 0 (being σ ∗be− 1) is an offer that will be accepted at time point Tbe by s, whereas σ ∗bebe accepted at time point Tbe by s to offer x∗bl(x∗blgain more offering σ ∗bl− 1).(Tbe(Tbe(Tbe(Tbe(Tbe(Tbe− 1) is an offer that will not− 1) < x∗− 1),bl− 1) will be accepted by s. Thus,(Tbe(Tbe− 1) is an offer that will not− 1) (cid:2)(Tbe− 1) unacceptable, s will exit), type be can− 1) (cid:2) σ ∗bl(TbeThus, it follows by contradiction that σ ∗be(t) = σ ∗bl(t). (cid:2)A.3. Proof of Lemma 4Let a∗ = (μ∗, σ ∗) be an equilibrium assessment in pure strategies. By contradiction, let σ ∗be(Tbe ).(Tbe ) is to accept anything lower than RPb, otherwise exit.(Tbe ) = σ ∗blIndependently of the system of beliefs, the strategy σ ∗beN. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571149However, bl, independently of the system of beliefs, does not accept anything greater than (x∗bllower than RPb. Thus, by contradiction σ ∗be(Tbe ) (cid:5)= σ ∗bl(Tbe ). (cid:2)(Tbe ))←b that is strictlyA.4. Proof of Lemma 7Let a∗ = (μ∗, σ ∗) an equilibrium assessment in pure strategies. We consider at first the case where T = Tbe+ 1.− 2). Assume that the agents have− 2 relying on the initial beliefs. In this situation the equilibrium strategies at time points− 1))←s > (x∗(Tbe ))←2[b] (i.e., inequality (5) is violated at t = TbeLet (e∗(Tbereached the time point Tbet = Tbe and t = Tbl do not depend on the system of beliefs, being:if σs(Tbeotherwise− 1) (cid:2) RPb(Tbe ) =∗be(cid:7)σ(cid:2)acceptexitacceptoffer(RPs)acceptexitif σs(Tbeotherwiseif σb(Tbe ) (cid:3) RPsotherwiseσ∗bl(Tbe ) =(cid:7)σ∗s (T ) =− 1) (cid:2) (RPs)←b(Tbe(Tbe(Tbe(Tbe− 2) = σ ∗bl− 2) = σ ∗bl− 2 and t = Tbe− 2. By contradiction, let σ ∗bewhereas at time points t = Tbeof s. At t = Tbet = Tbeconsistency) the beliefs of s at time point t = Tbeones. Therefore, if at time point t = Tbemal strategy of s is to accept any offer greater than or equal to (e∗(Tbeσ ∗− 2) ∈ [(e∗(Tbebeof μ∗.9 Analogously it must be σ ∗bldently of μ∗.10 Since, by hypothesis, (e∗(Tbe(Tbetype between be and bl deviates. Therefore, it follows by contradiction that σ ∗be− 1 the equilibrium strategies depend on the system of beliefs− 1 the beliefs of s on the equilibrium path depend on the equilibrium actions of b’s types at− 2). By Bayes’ rule (implied by Kreps and Wilson’s− 2), are the initial− 1 the opti-− 1))←s and otherwise offer RPb. It must be− 2) independently(Tbe− 2) (cid:2) (RPs)←2[b], otherwise bl would deviate from σ ∗− 2) indepen-(Tbebl− 2), then at least one(Tbe− 1))←s, (RPb)←s], otherwise be would deviate from offering σ ∗be− 2 b acts on the equilibrium path, then at time point t = Tbe− 1))←s > (RPs)←2[b], if σ ∗be− 2) = σ ∗(Tbebl− 2) (cid:5)= σ ∗(Tbebl− 1, once observed σ ∗beAll the cases where T > Tbe− 2) can be analyzed similarly, because, independently of μ∗, s will accept at time point Tbe− 1))←s > (x∗(Tbe ))←2[b] (i.e., violation of inequality (5) at t =+ 1 any offer x(Tbe ) (i.e., the equilibrium offer at time point Tbe when s believes b to be of type bl). It must be− 2) independently of− 2) indepen-− 1))←s, (RPb)←s], otherwise be would deviate from offering σ ∗beTbesuch that x (cid:3) x∗blσ ∗− 2) ∈ [(e∗(Tbebeμ∗. Analogously it must be σ ∗bldently of μ∗. By construction, x∗(Tbe ) ≡ x∗bl(Tbe ))←2[b], otherwise bl would deviate from σ ∗− 2) (cid:2) (x∗blbl(Tbe ), then it follows that σ ∗be+ 1 and (e∗(Tbe(Tbe− 2). (cid:2)− 2) (cid:5)= σ ∗bl− 2).(Tbe(Tbe(Tbe(Tbe(Tbe(Tbe(TbeA.5. Proof of Theorem 8We start considering the special case with two buyer’s types. By Lemma 3 it is almost always possible to reach− 2 relying on the initial beliefs (if b makes the equilibrium offers, which are the same for bothtime point t = Tbetypes, and s refuses them, it can be seen that Kreps and Wilson’s consistency forces μ∗ to be such that the beliefs− 2, then from Lemma 7 it followsremain the initial ones). In this situation, if inequality (5) does not hold at t = Tbeσ ∗− 2, thenbethe equilibrium assessment a∗ does not exist, except for a null measure subset of the space of the parameters.− 2), that contradicts Lemma 3. Therefore, if inequality (5) does not hold at t = Tbe− 2) (cid:5)= σ ∗be(Tbe(TblThe proof in the general case where the number of types of b is n can be easily obtained by iteratively applyingthe special case where the types are two as suggested in the sketch of the proof of Theorem 5. (cid:2)∗be− 2) < (e∗(Tbe∗be∗be(Tbe(Tbe(Tbe− 2), i.e., σ− 1))←s: if be makes such an offer, s would reject it to offer RPb at t = Tbe− 2) < (e∗(Tbe9 We consider σ− 1 that will be accepted by beat time point t = Tbe ; however, if be makes (RPb)←s, such an offer would be accepted by s independently of her beliefs, giving to be more utility− 2) > (RPb)←s:than offering σif be makes such an offer, s would accept it; however, if be makes (RPb)←s, such an offer would be accepted by s at time point t = Tbe− 1∗− 2) > (RPb)←s cannot be an equilibrium offerindependently of her beliefs, giving to be more utility than offering σbefor be.10 Whatever the system of beliefs of s is, s would accept (RPb)←s at time point t = Tbl , that is equivalent for bl to the offer (RPs)←2[b] acceptedat time point t = Tbe− 1,∗and then, at time point t = Tbe offer RPs that will be accepted, i.e., σbl− 2) > (RPs)←2[b] is dominated for bl by making any offer unacceptable by s at t = Tbe− 1))←s cannot be an equilibrium offer for be. We consider σ− 2) (cid:2) (RPs)←2[b].− 1. Thus offering σ− 2), i.e., σ(Tbe(Tbe(Tbe(Tbe(Tbe∗be∗be∗bl1150N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157A.6. Proof of Lemma 9∗,Tbebe= 0, we have ωAt time point t = Tbe− 1, being ωt ˆx(t) > ˇx(t) and ˇx(t) > ( ˇx(t + 2))←2[s], it follows that 1 > ω∗,Tbeωbethen 1 > αTbe−1> ω−2 > 0 and, vice versa, if 1 > αTbe∗,Tbebe−2 > 0, then ωand, vice versa, if ωTbebeTbebe> ω−1−2−1−2−1∗,Tbebe= ˇx(Tbeˆx(Tbe−1∗,Tbebeand αTbe−2 are well defined if and only if inequality (5) does not hold. (cid:2)> 0. If inequality (5) does not hold, then−1, then inequality (5) does not hold. If ω∗,TbebeTbebe. Thus the probability values of∗,TbebeTbebe> ω> ω−1−2−2,−2)−( ˇx(Tbe ))←2[s]−2)−( ˇx(Tbe ))←2[s]= ωTbebe−2. Being for anyω∗,TbebeA.7. Proof of Lemma 10The proof is by mathematical induction. The base of the induction is given by Lemma 9, exactly it is 1 > ω−1if and only if condition (5) does not hold. The inductive step can be proved as follows. The probability ω−1∗,Tbebe=∗,tbeTbebeωcan be written as:ω∗,tbe= ω∗,t+2be·aa + b+ ba + b>where a = ˆx(t − 1) − ˇx(t − 1) and b = ˇx(t − 1) − ( ˇx(t + 1))←2[s]. The values a and b are such that a, b ∈ (0, 1) for allt ∈ [<t ]. Thus for any time t ∈ [<t ,t ,> 0. If inequality (7)does not hold at time point t, then it is ωtbehold at time point t. If ωtbe∗,t+2probability values of ωbeand, vice versa, if ωtbe, then it is 1 > αt > 0 and, vice versa, if 1 > αt > 0, then ωt> ωbeand αt are well defined if and only if inequality (7) does not hold. (cid:2)t ] where ι(t) = b, if 1 > ω, then inequality (7) does not∗,tbe∗,t+2be> 0, then 1 > ω. Thus the∗,t+2be∗,t+2be∗,t+2be∗,t+2be∗,t+2be> ω> ω> ω> ω>A.8. Proof of Theorem 12We at first provide the proof of sequential rationality of the strategies given the system of beliefs.At any time point t (cid:3) Tbe the proof of sequential rationality of the strategies is trivial, since in these time points thegame is with complete information. At any time point t < Tbe the proof is by mathematical induction, where the baseof the induction is given by the analysis of time points t = TbeConsider time point t = Tbe− 2), she believes her opponent− 1. If s receives any offer equal to or greater than ˆx(Tbeto be of type be and her optimal action is to accept. Therefore she cannot gain more by deviating from σ ∗. If s receives− 2)), the system of beliefs is such that it is indifferent for sany offer belonging to the interval [ ˇx(Tbe− 1) and no action is better than these. Therefore she cannot gainto accept such an offer or reject it to offer ˆx(Tbemore by deviating from σ ∗. Being by construction ˆx(Tbe−1 ∈ (0, 1) and then s can− 1) > ˇx(Tbe− 2), the system of beliefs is such that the expected utilityactually randomize. If s receives any offer lower than ˆx(Tbe− 1) is greater than the utility of accepting the received offer and no action is better than makingof offering ˆx(Tbesuch an offer. Therefore s cannot gain more by deviating from σ ∗.− 1), it holds βTbe− 1 and t = Tbe− 2), ˆx(Tbe− 2.−2) and offering ˇx(Tbe−1 is such that accepting the received offer and rejecting it to offer ˇx(Tbe− 2. Consider be. If be receives any offer greater than ˆx(Tbe−1 is such that offering ˆx(Tbe− 3), her optimal action isConsider time point t = Tbe−2) give the same expected utilityto reject it and the value of βTbeto be. Therefore be does not deviate from σ ∗. If be receives any offer belonging to the interval ( ˇx(Tbe− 3)],− 2) give the same expectedthe value of βTbeutility to be and no action is better than these. Therefore be does not deviate from σ ∗. Not holding by construction−2 ∈ (0, 1) and then be can actually randomize. If be receives any offer equal to or lowercondition (5), it holds αTbe− 3), her optimal action is to accept the received offer. Therefore be cannot gain more by deviating from σ ∗.than ˆx(TbeConsider bl. If bl receives any offer greater than ˇx(Tbe− 2).Therefore bl does not deviate from σ ∗. If bl receives any offer equal to or lower than ˆx(Tbe− 3), her optimal actionis to accept the received offer. Therefore bl cannot gain more by deviating from σ ∗.− 3), her optimal action is to reject it and offer ˇx(Tbe− 3), ˆx(TbeThe two steps above can be inductively applied until αt or βt have not well defined values of probability. Whencondition (7) holds, αt and βt have not well defined values of probabilities and the construction can continue in purestrategies as prescribed in Section 4.1.N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571151The consistency can be proved with the following fully mixed strategy. For any time point t such that t (cid:2) <t (cid:3) Tbe , the fully mixed strategy to employ is the one used in [5], i.e. for any t < Tbi bi puts 1 − 1n distributed over all the other strategies and for t (cid:3) Tbi bi puts 1 − 1strategy and 1and 1(1 − βt (σs − 2)) · (1 − 1strategy of b is more complicated (for the sake of simplicity, we report non-normalized probabilities):t orn on the equilibriumn2 on the equilibrium strategyn ) andn distributed over all the other strategies. The fully mixedn2 distributed over all the other strategies. For all ts such thatn ) on the equilibrium strategies and 1t s puts βt (σs(t − 2)) · (1 − 1<t < t <>• be: she puts αt · (1 − 1n ) and (1 − αt ) · (1 − 1n ) on the equilibrium strategies, 1n distributed over all the offers > ˆx(t),αt · 1n distributed over all the offers < ˇx(t), and Cbe) on the equilibrium strategy,• bl: she puts (1 − 1nover all the offers < ˇx(t), and Cbl− 12Tblnn distributed over all the offers belonging to ( ˇx(t), ˆx(t));· 1distributed over all the offers > ˆx(t), 1n distributed12Tblnn distributed over all the offers belonging to ( ˇx(t), ˆx(t)).· 1The coefficients Cbe and Cbl are computed such that the consistency holds also in the interval ( ˇx(t), ˆx(t)). (cid:2)Appendix B. Formulas to compute mixed strategy with many typesFour choice rules can be employed at time point t when ι(t) = b and the equilibrium assessment cannot be inpure strategies. However, given a setting, only one choice rule can be employed at a single time point t to producean equilibrium, whereas different choice rules can be employed within a construction at different time points. Theprincipal condition that discriminates the use of the choice rules is the value of ˆx(t + 1) computed in the backwardconstruction from time point T to time point t + 1. When ˆx(t + 1) = ( ˆx(t + 2))←b, only one choice rule can beemployed. We call it choice rule 1, and we describe it in Appendix B.1. When ˆx(t + 1) = RPb, it is necessary tocheck other conditions, being three the possible choice rules that can be employed. We call them choice rules 2.1, 2.2,and 2.3, and we discuss them in Appendix B.2.B.1. Choice rule 1It can be employed at time point t when σ ∗s (t + 1) = ( ˆx(t + 2))←b. In this situation it can be easily seen that itis always possible to increase the value of αt to have Us( ˇx(t), t + 1) = EUs(offer( ˆx(t + 1)), t + 1) preserving thats prefers at time point t + 1 offering ( ˆx(t + 2))←b to offering RPb. The rule to use in order to compose the setsˆ(cid:11)b(t), ˜(cid:11)b(t), ˇ(cid:11)b(t) is:⎧⎨⎩ˆ(cid:11)b(t) = {θb(t + 1)}˜(cid:11)b(t) = ˆ(cid:11)b(t + 2) ∪ ˜(cid:11)b(t + 2) ∪ {θb(t + 2)}ˇ(cid:11)b(t) = ˇ(cid:11)b(t + 2)The parameters whereby the equilibrium strategy and the system of beliefs are based can be computed as follows:ˆx(t) =· [( ˆx(t + 1))←s − RPs] + RPs˜(cid:6)0b(t)ˆ(cid:6)0b(t) + ˜(cid:6)0b(t)ˇx(t) · (1 − ˜(cid:6)˜(cid:6)∗,t+1b(t) =∗,t+3b(t + 2)) + ˜(cid:6)∗,t+3b(t + 2) · ( ˆx(t + 1))←s − ( ˇx(t + 2))←2[s]( ˆx(t + 1))←s − ( ˇx(t + 2))←2[s]αt =αt+2 =∗,t+1˜(cid:6)b(1 − ˜(cid:6)˜(cid:6)∗,t+1b∗,t+3b(t) · ˇ(cid:6)0b(t)(t)) · ˜(cid:6)0b(t)(t + 2) · (1 − ˜(cid:6)(1 − ˜(cid:6)∗,t+1b∗,t+3b(t)) · (ω0(t + 2)) · ˜(cid:6)b(t + 2) + ˆ(cid:6)0∗,t+1(t) · ˜(cid:6)0bb(t + 2) + ˜(cid:6)0b(t + 2)b(t + 2))βt+1 = RPb − ˆx(t)RPb − ˇx(t)1152N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157The optimal actions of b at time point t and of s at time point t + 1 can be defined specifying the choice rulesemployed by the agents:(cid:2)if σs(t − 1) (cid:2) ( ˆx(t))←botherwiseσ∗ˆ(cid:11)b(t)(t) =σ∗˜(cid:11)b(t)(t) =⎧acceptoffer( ˆx(t))accept⎪⎪⎪⎪⎨(cid:2)⎪⎪⎪⎪⎩(cid:2)acceptoffer( ˇx(t)) αtoffer( ˆx(t))offer( ˇx(t)) αt(cid:2)1 − αt1 − αtσ∗ˇ(cid:11)b(t)(t) =σ∗s (t + 1) =acceptoffer( ˇx(t))⎧accept⎪⎪⎨(cid:2)⎪⎪⎩acceptoffer( ˆx(t))offer( ˆx(t))The system of beliefs is:if σs(t − 1) (cid:2) ˇx(t − 1)if ˇx(t − 1) < σs(t − 1) (cid:2) ( ˆx(t))←botherwiseif σs(t − 1) (cid:2) ( ˇx(t))←botherwiseif σb(t) (cid:3) ( ˇx(t + 1))←s or σb(t) = ˆx(t)if ( ˆx(t + 1))←s > σb(t) > ˆx(t) or ˆx(t) > σb(t) > ˇx(t)βt1 − βtotherwiseμ(t + 1) =⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩σb(t) (cid:3) ( ˆx(t + 1))←s( ˆx(t + 1))←s > σb(t) > ˆx(t)σb(t) = ˆx(t)ˆx(t) > σb(t) > ˇx(t)ˇx(t) (cid:3) σb(t + 1)⎧⎪⎨⎪⎩⎧⎪⎨⎪⎩⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩⎧⎪⎨⎪⎩⎧⎪⎨⎪⎩ˆ(cid:6)t+1b˜(cid:6)t+1bˇ(cid:6)t+1bˆ(cid:6)t+1b˜(cid:6)t+1bˇ(cid:6)t+1bˆ(cid:6)t+1b˜(cid:6)t+1bˇ(cid:6)t+1bˆ(cid:6)t+1b˜(cid:6)t+1bˇ(cid:6)t+1bˆ(cid:6)t+1b˜(cid:6)t+1bˇ(cid:6)t+1b∗,t+1b(t + 1) = 0(t + 1) = 1(t + 1) = 0(t + 1) = 0(t + 1) = ˜(cid:6)(t + 1) = 1 − ˜(cid:6)t+1b∗,t+1(t + 1) = (1 − ˜(cid:6)b∗,t+1b(t + 1) = 1 − ˆ(cid:6)t+1(t + 1) = 0(t + 1) = 0(t + 1) = ˜(cid:6)(t + 1) = 1 − ˜(cid:6)t+1(t + 1) = 0∗,t+1(t + 1) = ˜(cid:6)b(t + 1) = 1 − ˜(cid:6)∗,t+1b(t)∗,t+1b− ˜(cid:6)bb(t)(t) + (1 − ˜(cid:6)(t + 1)∗,t+1b(t)) · σb(t)− ˇx(t)ˆx(t)− ˇx(t)(t)) · ˜(cid:6)tb(t) · (1 +ˆ(cid:6)0˜(cid:6)0b(t)b(t))(t) · ˇ(cid:6)t(t + 1)b(t)(t) + (1 − ˜(cid:6)(t + 1)∗,t+1b(t)) · σb(t)− ˇx(t)ˆx(t)− ˇx(t)B.2. Choice rule 2It requires that σ ∗s (t + 1) = RPb and there are three possible forms of choice rules. In this situation it is not alwayspossible to increase the value of αt to have Us( ˇx(t), t + 1) = EUs(offer( ˆx(t + 1)), t + 1) preserving that s prefersat time point t + 1 offering RPb to offering ( ˆx(t + 2))←b. For high values of αt s could prefer at time point t + 1offering ( ˆx(t + 2))←b to offering RPb. In order to introduce the conditions of the choice rules, we need to computethe following values:A =(cid:11)· (1 − αt+2)(cid:10) ˆ(cid:6)0b(t + 2)b(t + 3)(cid:6)0(cid:10)˜(cid:6)0(cid:6)0αt+2 ·++˜(cid:6)0(cid:6)0b(t + 2)b(t + 3)b(t + 2)b(t + 3)ˇ(cid:6)0(cid:6)0+b(t + 2)b(t + 3)· (( ˆx(t + 2))←s − RPs)(cid:11)· (( ˇx(t + 2))←s − RPs) + RPsN. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571153+˜(cid:6)0(cid:6)0b(t + 2)b(t + 3)b(t + 2)b(t + 3)ˇ(cid:6)0(cid:6)0+(cid:11)· (1 − αt+2)(cid:11)b(t + 2)b(t + 3)(cid:11)· (RPb − RPs) + RPs· (( ˆx(t + 2))←b − RPs)· (( ˇx(t + 2))←s − RPs) + RPs· δsB =D =C =+(cid:10)αt+2 ·(cid:10) ˆ(cid:6)0b(t + 2)b(t + 3)(cid:6)0(cid:10)˜(cid:6)0(cid:6)0b(t + 2)ω0˜(cid:6)0b(t)ˇx(t)− Aδsb(t+2)ω0˜(cid:6)0b(t)− A∗ =(cid:6)B − A(b(t+2)ω0˜(cid:6)0b(t)− A) − (b(t+2)ω0˜(cid:6)0b(t)· (( ˆx(t + 2))←b − RPs) − B)B.2.1. Choice rule 2.1It can be employed when C (cid:3) (cid:6)∗. In this situation the value of αt is such that s keeps to prefer at time point t + 1offering RPb to offering ( ˆx(t + 2))←b. The rule to use in order to compose the sets ˆ(cid:11)b(t), ˜(cid:11)b(t), ˇ(cid:11)b(t) is:⎧⎨⎩ˆ(cid:11)b(t) = ∅˜(cid:11)b(t) = {θb(t + 1), θb(t + 2)}ˇ(cid:11)b(t) = ˆ(cid:11)b(t + 2) ∪ ˜(cid:11)b(t + 2) ∪ ˇ(cid:11)b(t + 2)(B.1)The parameters whereby the equilibrium strategy and the system of beliefs are based can be computed as follows:ˆx(t) = D∗,t+1˜(cid:6)bαt =(t) = C∗,t+1˜(cid:6)b∗,t+1(1 − ˜(cid:6)b∗,t+3˜(cid:6)b(1 − ˜(cid:6)βt+1 = RPb − ˆx(t)RPb − ˇx(t)αt+2 =∗,t+3b(t) · ˇ(cid:6)0b(t)(t)) · ˜(cid:6)0b(t)(t + 2) · ˇ(cid:6)0b(t + 2)(t + 2)) · ˜(cid:6)0b(t + 2)The optimal actions of b at time point t and of s at time point t + 1 are:σ∗˜(cid:11)b(t)(t) =⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩(cid:2)σ∗ˇ(cid:11)b(t)(t) =σ∗s (t + 1) =accept(cid:2)(cid:2)acceptoffer( ˇx(t)) αtoffer( ˆx(t))offer( ˇx(t)) αt1 − αt1 − αtif σs(t − 1) (cid:2) ˇx(t − 1)if ˇx(t − 1) < σs(t − 1) (cid:2) ( ˆx(t))←botherwiseacceptoffer( ˇx(t))⎧⎪⎨accept(cid:2)⎪⎩acceptoffer( ˆx(t))offer( ˆx(t))if σs(t − 1) (cid:2) ( ˇx(t))←botherwiseβt1 − βtif σb(t) (cid:3) ˆx(t)if ˆx(t) > σb(t) > ˇx(t)otherwise1154N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157The system of beliefs is:μ(t + 1) =⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩σb(t) (cid:3) ˆx(t)ˆx(t) > σb(t) > ˇx(t)ˇx(t) (cid:3) σb(t + 1)(cid:9)(cid:9)(cid:9)˜(cid:6)t+1bˇ(cid:6)t+1b˜(cid:6)t+1bˇ(cid:6)t+1b˜(cid:6)t+1bˇ(cid:6)t+1b∗,t+1b(t + 1) = 1(t + 1) = 0(t + 1) = ˜(cid:6)(t + 1) = 1 − ˜(cid:6)t+1b∗,t+1(t + 1) = ˜(cid:6)(t)b∗,t+1(t + 1) = 1 − ˜(cid:6)b(t)(t) + (1 − ˜(cid:6)(t + 1)∗,t+1b(t)) · σb(t)− ˇx(t)ˆx(t)− ˇx(t)B.2.2. Choice rule 2.2It can be employed when C < (cid:6)∗ and D (cid:3) ( ˆx(t + 2))←2[b]. In this situation the value of αt would be such that sprefers at time point t + 1 offering ( ˆx(t + 2))←b to offering RPb, thus a slight variation of the choice rule 2.1 isneeded. Choice rule 2.2 and choice rule 2.3 are complementary when C < (cid:6)∗. The rule to use in order to composethe sets ˆ(cid:11)b(t), ˜(cid:11)b(t), ˇ(cid:11)b(t) is:⎧⎨⎩ˆ(cid:11)b(t) = {θb(t + 1)}˜(cid:11)b(t) = {θb(t + 2)}ˇ(cid:11)b(t) = ˆ(cid:11)b(t + 2) ∪ ˜(cid:11)b(t + 2) ∪ ˇ(cid:11)b(t + 2)(B.2)The parameters whereby the optimal actions and the system of beliefs are based can be computed as follows:ˆx(t) = D∗,t+1˜(cid:6)bαt =(t) = C∗,t+1˜(cid:6)b∗,t+1(1 − ˜(cid:6)b∗,t+3˜(cid:6)b(1 − ˜(cid:6)αt+2 =βt+1 =(t) · ˇ(cid:6)0b(t)(t)) · ˜(cid:6)0b(t)(t + 2) · ˇ(cid:6)0b(t + 2)b(t + 2)(t + 2)) · ˜(cid:6)0∗,t+3bRPb − ˆx(t)RPb − ( ˆx(t + 2))←2[b]The optimal actions of b at time point t and of s at time point t + 1 are:if σs(t − 1) (cid:2) ˆx(t − 1)otherwiseif σs(t − 1) (cid:2) ˇx(t − 1)if ˇx(t − 1) < σs(t − 1) (cid:2) ( ˆx(t))←botherwiseσ∗ˆ(cid:11)b(t)(t) =σ∗˜(cid:11)b(t)(t) =⎧accept(cid:2)acceptoffer( ˆx(t))⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩(cid:2)acceptoffer( ˇx(t)) αtoffer( ˆx(t))offer( ˇx(t)) αt1 − αt1 − αt(cid:2)(cid:2)acceptoffer( ˇx(t))⎧⎪⎨accept(cid:2)σ∗ˇ(cid:11)b(t)(t) =σ∗s (t + 1) =if σs(t − 1) (cid:2) ( ˇx(t))←botherwiseoffer(( ˆx(t + 2))←b) βtoffer( ˆx(t))1 − βt⎪⎩offer( ˆx(t))if σb(t) (cid:3) ˆx(t)if ˆx(t) > σb(t) > ˇx(t)otherwiseThe system of beliefs is exactly the same of that for choice rule 1.N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571155It can be employed when C < (cid:6)∗ and D < ( ˆx(t + 2))←2[b]. The rule to use in order to compose the setsB.2.3. Choice rule 2.3ˆ(cid:11)b(t), ˜(cid:11)b(t), ˇ(cid:11)b(t) is:⎧⎨⎩ˆ(cid:11)b(t) = {θb(t + 1)}˜(cid:11)b(t) = ˆ(cid:11)b(t + 2) ∪ ˜(cid:11)b(t + 2) ∪ {θb(t + 2)}ˇ(cid:11)b(t) = ˇ(cid:11)b(t + 2)This assessment form presents two anomalies with respect to the others. The first one is that the types that random-ize, i.e, ˜(cid:11)b(t), do it with different probabilities. Specifically, all the types belonging to the set ˆ(cid:11)b(t + 2) ∪ ˜(cid:11)b(t + 2)randomize with probability αt2. The second one is thatthis choice rule modifies the construction from time point T to time point t + 1. Specifically, it modifies the values ofˆx(t + 1). In order to compute the parameters whereby the equilibrium strategies are based, we need to compute thefollowing values:1, whereas the type θb(t + 2) randomizes with probability αtE =(t + 2)∗,t+3˜(cid:6)b1 − ˜(cid:6)∗,t+3b(t + 2)F = RPb − ( ˆx(t + 2))←b − E · (( ˆx(t + 2))←b − ( ˆx(t + 2))←s)G = ( ˆx(t + 2))←b − ( ˆx(t + 2))←sH = (1 + E) · G←sThe parameters whereby the optimal actions and the system of beliefs are based can be computed as follows:ˆx(t) =˜(cid:6)0b(t)ˆ(cid:6)0b(t) + ˜(cid:6)0ˆx(t + 1) = ( ˆx(t + 2))←bb(t)· [( ˆx(t + 2))←bs − RPs] + RPs˜(cid:6)∗,t+1b,1(t) =˜(cid:6)∗,t+1b,2(t) = ˜(cid:6)ˇx(t) + ( ˆx(t))←s · E − (1 + E) · ( ˇx(t + 2))←2[s] + E · GF(1 + (1 + E) · G− E · GFF ) · H∗,t+1b· H(t) · (1 + E) · GF(t) · ˇ(cid:6)0b(t) · (1 + E)∗,t+1b(t + 2) − ˜(cid:6)b,2(t) · ˇ(cid:6)0b(t) · (1 + E)∗,t+1b(t + 2) − ˜(cid:6)b,2· ˜(cid:6)0b(t + 2)∗,t+1˜(cid:6)b,2(t)) · ˜(cid:6)0∗,t+1˜(cid:6)b,2(t)) · ω0(1 − ˜(cid:6)∗,t+1b,2=αt1=αt2(1 − ˜(cid:6)αt2αt =· ω0∗,t+1b,2b(t + 2) + αt1˜(cid:6)0b(t)(t) · ˜(cid:6)0b(t + 2) · E(t) · ω0b(t + 2) · Eαt+2 = E · ( ˆ(cid:6)0b(t + 2) + ˜(cid:6)0b(t + 2)) ·1 − ˜(cid:6)∗,t+1b,2(t)∗,t+1b,1(t) − ˜(cid:6)b(t + 2)˜(cid:6)0βt = RPb − ˆx(t)RPb − ˇx(t)The optimal actions of b at time point t and of s at time point t + 1 are:(cid:2)σ∗ˆ(cid:11)b(t)(t) =acceptoffer( ˆx(t))if σs(t − 1) (cid:2) ˆx(t − 1)otherwise1156N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–1157⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩σ∗˜(cid:11)b,i (t)(t) =(cid:2)σ∗ˇ(cid:11)b(t)(t) =σ∗s (t + 1) =accept(cid:2)(cid:2)1 − αtiacceptoffer( ˇx(t)) αtioffer( ˆx(t))1 − αtioffer( ˇx(t)) αtiif σs(t − 1) (cid:2) ˇx(t − 1)if ˇx(t − 1) < σs(t − 1) (cid:2) ( ˆx(t))←botherwiseif σs(t − 1) (cid:2) ( ˇx(t))←botherwiseacceptoffer( ˇx(t))⎧⎪⎨accept(cid:2)offer(( ˆx(t + 2))←b) βtoffer( ˆx(t))1 − βt⎪⎩offer( ˆx(t))if σb(t) (cid:3) ˆx(t)if ˆx(t) > σb(t) > ˇx(t)otherwiseThe system of beliefs can be trivially obtained extending that of choice rule 1.References[1] F. Amigoni, N. Gatti, A formal framework for connective stability of highly decentralized cooperative negotiations, Autonomous Agents andMulti-Agent Systems 15 (3) (2007) 253–279.[2] C. Bartolini, C. Preist, N.R. Jennings, A Software Framework for Automated Negotiation, Springer, 2005, pp. 213–235.[3] K. Chatterjee, L. Samuelson, Bargaining under two-sided incomplete information: The unrestricted offers case, Operations Research 36 (4)(1988) 605–618.[4] P.C. Cramton, L.M. Ausubel, R.J. Deneckere, Handbook of Game Theory, vol. 3, Elsevier, 2002, pp. 1897–1945.[5] F. Di Giunta, N. Gatti, Alternating-offers bargaining under one-sided uncertainty on deadlines, in: Proceedings of the European Conferenceon Artificial Intelligence (ECAI), Riva del Garda, Italy, August 28–September 1 2006, pp. 225–229.[6] F. Di Giunta, N. Gatti, Bargaining in-bundle over multiple issues in finite-horizon alternating-offers protocol, in: Proceedings of the Sympo-sium on Artificial Intelligence and Mathematics (AIMATH), Fort Lauderdale, USA, January 4–6 2006.[7] F. Di Giunta, N. Gatti, Bargaining over multiple issues in finite horizon alternating-offers protocol, Annals of Mathematics in ArtificialIntelligence 47 (3–4) (2006) 251–271.[8] P. Faratin, C. Sierra, N.R. Jennings, Negotiation decision functions for autonomous agents, Robotic Autonomous Systems 24 (3–4) (1998)159–182.[9] S.S. Fatima, M. Wooldridge, N.R. Jennings, Multi-issue negotiation under time constraints, in: Proceedings of the International Joint Confer-ence on Autonomous Agents and Multi Agent Systems (AAMAS), Bologna, Italy, July 17–19 2002, pp. 143–150.[10] S.S. Fatima, M. Wooldridge, N.R. Jennings, An agenda-based framework for multi-issue negotiation, Artificial Intelligence 152 (1) (2004)1–45.[11] S.S. Fatima, M. Wooldridge, N.R. Jennings, On efficient procedures for multi-issue negotiation, in: Proceedings of Trading Agent Design andAnalysis and Agent Mediated Electronic Commerce (TADA/AMEC) at the International Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS), Hakodate, Japan, May 9 2006, pp. 71–84.[12] S.S. Fatima, M.J. Wooldridge, N.R. Jennings, Multi-issue negotiation with deadlines, Journal of Artificial Intelligence Research 27 (1) (2006)381–417.[13] C. Fershtman, D.J. Seidmann, Deadline effects and inefficient delay in bargaining with endogenous commitment, Journal of Economic The-ory 60 (2) (1993) 306–321.[14] D. Fudenberg, J. Tirole, Game Theory, The MIT Press, Cambridge, MA, USA, 1991.[15] A. Gilpin, T. Sandholm, Finding equilibria in large sequential games of imperfect information, in: Proceedings of the ACM Conference onElectronic Commerce, ACM Press, 2006, pp. 160–169.[16] U. Gneezy, E. Haruvy, A.E. Roth, Bargaining under a deadline: Evidence from the reverse ultimatum game, Games and Economic Behavior 45(2003) 347–368.[17] P.R. Halmos, Measure Theory, Springer, Berlin, Germany, 1974.[18] J.C. Harsanyi, R. Selten, A generalized Nash solution for two-person bargaining games with incomplete information, Management Science 18(1972) 80–106.[19] D. Koller, N. Megiddo, B. von Stengel, Fast algorithms for finding randomized strategies in game trees, in: Proceedings of the Annual ACMSymposium on Theory of Computing (STOC), ACM Press, 1994, pp. 750–759.[20] D. Koller, A. Pfeffer, Representations and solutions for game-theoretic problem, Artificial Intelligence 94 (1) (1997) 167–215.[21] S. Kraus, Strategic Negotiation in Multiagent Environments, The MIT Press, Cambridge, MA, USA, 2001.[22] D.R. Kreps, R. Wilson, Sequential equilibria, Econometrica 50 (4) (1982) 863–894.[23] A. Lazaric, E. Munoz de Cote, N. Gatti, M. Restelli, Reinforcement learning in extensive form games with incomplete information: thebargaining case study, in: Proceedings of the International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS),Honolulu, USA, May 14–18 2007, pp. 216–218.[24] A. Lomuscio, M. Wooldridge, N.R. Jennings, A classification scheme for negotiation in electronic commerce, International Journal of GroupDecision and Negotiation 12 (1) (2003) 31–56.N. Gatti et al. / Artificial Intelligence 172 (2008) 1119–11571157[25] C.A. Ma, M. Manove, Bargaining with deadlines and imperfect player control, Econometrica 61 (6) (1993) 1313–1339.[26] P. Maes, R.H. Guttman, A.G. Moukas, Agents that buy and sell, Communications of the ACM 42 (3) (1999) 81–91.[27] P.B. Miltersen, T.B. Sorensen, Computing sequential equilibria for two-player games, in: Proceedings of the ACM–SIAM Symposium onDiscrete Algorithm (SODA), ACM Press, 2006, pp. 107–116.[28] S. Napel, Bilateral Bargaining: Theory and Applications, Springer, Berlin, Germany, 2002.[29] J.F. Nash, Non-cooperative games, Annals of Mathematics 54 (1951) 286–295.[30] M.J. Osborne, A. Rubinstein, Bargaining and Markets, Academic Press, San Diego, CA, USA, 1990.[31] C.M. Jonker, V. Robu, J. Treur, An agent architecture for multi-attribute negotiation using incomplete preference information, AutonomousAgents and Multi-Agent Systems 15 (2) (2007) 221–252.[32] J.S. Rosenschein, G. Zlotkin, Rules of Encounter. Designing Conventions for Automated Negotiations among Computers, The MIT Press,Cambridge, MA, USA, 1994.[33] A. Rubinstein, Perfect equilibrium in a bargaining model, Econometrica 50 (1) (1982) 97–109.[34] A. Rubinstein, A bargaining model with incomplete information about time preferences, Econometrica 53 (5) (1985) 1151–1172.[35] T. Sandholm, Agents in electronic commerce: Component technologies for automated negotiation and coalition formation, AutonomousAgents and Multi-Agent Systems 3 (1) (2000) 73–96.[36] T. Sandholm, N. Vulkan, Bargaining with deadlines, in: Proceedings of the American Association for Artificial Intelligence Conference(AAAI), Orlando, USA, October 23–26 1999, pp. 44–51.[37] I. Ståhl, Bargaining Theory, Stockholm School of Economics, Stockholm, Sweden, 1972.[38] J. von Neumann, O. Morgenstern, Theory of Games and Economic Behavior, Princeton University Press, Princeton, NJ, USA, 1947.