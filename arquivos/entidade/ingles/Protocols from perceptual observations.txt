Artificial Intelligence 167 (2005) 103–136www.elsevier.com/locate/artintProtocols from perceptual observationsChris J. Needham ∗, Paulo E. Santos 1, Derek R. Magee,Vincent Devin 2, David C. Hogg, Anthony G. CohnSchool of Computing, University of Leeds, Leeds, LS2 9JT, UKReceived 28 July 2004; received in revised form 14 February 2005; accepted 14 April 2005Available online 27 July 2005AbstractThis paper presents a cognitive vision system capable of autonomously learning protocols fromperceptual observations of dynamic scenes. The work is motivated by the aim of creating a syn-thetic agent that can observe a scene containing interactions between unknown objects and agents,and learn models of these sufficient to act in accordance with the implicit protocols present in thescene. Discrete concepts (utterances and object properties), and temporal protocols involving theseconcepts, are learned in an unsupervised manner from continuous sensor input alone. Crucial to thislearning process are methods for spatio-temporal attention applied to the audio and visual sensordata. These identify subsets of the sensor data relating to discrete concepts. Clustering within contin-uous feature spaces is used to learn object property and utterance models from processed sensor data,forming a symbolic description. The PROGOL Inductive Logic Programming system is subsequentlyused to learn symbolic models of the temporal protocols presented in the presence of noise and over-representation in the symbolic data input to it. The models learned are used to drive a synthetic agentthat can interact with the world in a semi-natural way. The system has been evaluated in the domainof table-top game playing and has been shown to be successful at learning protocol behaviours insuch real-world audio-visual environments. 2005 Elsevier B.V. All rights reserved.* Corresponding author.E-mail address: chrisn@comp.leeds.ac.uk (C.J. Needham).1 Paulo Santos is now at Centro Universitario da FEI, Sao Paulo, Brazil.2 Vincent Devin is now at France Telecom R&D, Meylan, France.0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.04.006104C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136Keywords: Cognitive vision; Autonomous learning; Unsupervised clustering; Symbol grounding; Inductive logicprogramming; Spatio-temporal reasoning1. IntroductionThis paper presents a cognitive vision system capable of autonomously learning proto-cols involving rudimentary language and visual objects. In this system, models of visualobjects and utterances are obtained from unsupervised statistical learning algorithms. In or-der to form symbolic data for input into an inductive logic programming (ILP) system, thecontinuous perceptual observations are transformed using the models learned. Perceptualobservations are taken to be any sensory input; here acoustic and visual inputs are used.The concept of qualitative time is introduced, since only key frames are deemed to be ofimportance. The direct link between perception and action is exploited; a change in what isperceived can only be brought about by an action. The ILP system is used to construct setsof definite clauses that express rules of behaviour for the perceived actions from the sym-bolic description of the scenes. In particular, the protocols learned encode the connectionbetween utterances with visual objects that occur in the scene. This is intrinsically a solu-tion to the anchoring problem [8] which is an instance of the symbol grounding problem[15]. The sets of definite clauses obtained are further used by a synthetic agent to performactions in the world. Therefore, in this work we explore closing the loop between learn-ing the connection between perception and action via bridging the gap between computervision, pattern recognition and symbolic knowledge discovery.The framework presented below is evaluated in the domain of simple table-top games.In this domain the system was able to learn complete protocols for the rules of the games,when different verbal utterances are made and also some aspects of the dynamics involvedin playing the game (for instance, when objects should be placed on the table). The en-tire learning process is executed with minimal human intervention and assumes minimaldomain specific knowledge of the scenes or of game concepts. In earlier work [22], wedemonstrated that the same approach is capable of learning simple mathematical conceptssuch as numerical ordering and equality.1.1. The domain of table-top gamesWe have chosen to work in the domain of simple table-top games involving interactionof one or two players with a small number of visual objects and incorporating spokenutterances. The reason for choosing such scenarios is that games contain rich protocolsand their ‘complexity’ can be controlled by adding, excluding or modifying rules, actionsand/or objects in the domain. Moreover, it may be argued that many real-world social-interaction scenarios may be modelled as games [14], which suggests that our frameworkmay be relevant to the development of a fully autonomous system that could learn how tobehave in the real world.Experimental data is collected using two standard PCs, two webcams, and a microphonein the arrangement shown in Fig. 1. The games used in this work are described in greaterdetail in Section 4. Briefly, the following three games are played:C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136105Fig. 1. Example of the data collection phase. A game is played on the table between two players. One camerapoints down at the table, and another captures the face of the participant to be replaced by the synthetic agent.The audio is captured by a microphone worn by this participant.(1) SNAP1. A generalised game of snap where two cards are played simultaneously, fol-lowed by an utterance dependent upon the figures on the cards. The utterances areeither “colour” (when only the colours in the figures are the same), “shape” (whenonly the shapes in the figures are the same), “same” (when shape and colour match)or “nothing” (if no feature in the figures match to each other). The cards are removed,and “play” is then uttered indicating to play the next two cards.(2) SNAP2. A variation of the above game where cards are placed one on top of anotherand the resulting utterance is dependent upon the card which is visible, and the card atthe previous time step.(3) Paper-scissors-stone (PSS). Played with two sets of three cards each depicting one ofpaper, scissors or stone. Two players simultaneously select one of the object cards andplace them on the table. When the two figures in the cards are perceived, utterances“win”, “lose” and “draw” are spoken by one of the players (the one to be simulated bythe synthetic agent). This player says “win” when its card beats the one shown by theother player—paper beats (wraps) stone, scissors beats (cuts) paper, and stone beats(blunts) scissors. A “play” is uttered when there are no cards on the table.1.2. OverviewAn overview of the framework is illustrated in Fig. 2. Firstly, attention mechanisms arenecessary to pick out salient (interesting) sounds, objects and features from the audio andvisual input streams obtained in a setup similar to that shown in Fig. 1.There are two phases of operation of our system: training and execution.• In the training (or learning) phase, the synthetic agent observes the world, withoutparticipating. In this phase, class induction is performed on the blobs and sounds thatare perceived, and class models are formed, which are used for classification of all theperceptual objects that have been seen in training. These are used to form a symbolic106C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136Fig. 2. Overview of the learning framework for the synthetic agent.data description of the input signals. Protocol induction is then performed on thissymbolic data stream, from which a set of protocol models (or rules) is formed.• In the execution (or play) phase, the synthetic agent participates in games using thelearned protocol models. An inference engine is used for protocol instantiation to in-fer the synthetic agent’s (vocal) response to the current symbolic description of theworld.We assume no knowledge about the type of objects or sounds presented to the sys-tem. Therefore, unsupervised clustering of both the audio and visual feature vectors isperformed. Models are learned, based on this clustering, which can classify the perceptualinputs into classes. This provides a symbolic description of the input signals.Protocol models are represented as ordered sets of definite clauses expressed in Prologsyntax. This provides the necessary flexibility to represent the relations between objectsand actions that we require. Others have previously demonstrated the utility of (subsetsof) first-order predicate logic in high-level scene interpretation (e.g., Neumann and Weiss[30]).Inductive Logic Programming (ILP) in the form of PROGOL [28] is used for protocolinduction. The reason for choosing PROGOL resides in its capability to construct theoriesfrom only (possibly noisy) positive examples. This coincides with our aim to learn proto-col behaviour from observation in an unsupervised way. Moreover, ILP enables conceptgeneralisation (e.g., to extend rules learned from observation of certain objects to unseenentities) and for the knowledge learned to be presented in a format that allows us to assessits ‘complexity’ and accuracy, besides serving as tools for further reasoning—the symbolictheories obtained are used in this work to construct equivalence classes between utterancesin order to cope with over-clustering of the utterances in the audio signal.Once learning is complete, the synthetic agent can process the perceptual inputs andinfer the appropriate action response to the state of the world in real-time. The interactiveagent grounds learned perceptual models and learned protocols to descriptions of objectsin the real world, and plays back a suitable utterance.The experiments reported in this paper are evaluated using new definitions of sound-ness and completeness that are inspired by their homonyms used to assess the semantics ofC.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136107inference systems. Soundness accounts for the extent to which the intended protocol (in-tended semantics) is represented by the rules obtained. In contrast, completeness measureshow many of the rules found do not agree with the intended semantics.The remainder of this paper is structured as follows: Section 2 reviews previous workon learning from audio-visual input. Section 3 details the system, discussing the applica-tion of the ideas presented above. Section 4 describes the methods used for evaluation,before presenting the experiments and results in detail. Discussion of our approach is pre-sented in Section 5 alongside future research directions, before conclusions are drawn inSection 6.2. BackgroundMost previous work on modelling the connection of language to the world throughvision has not included the capacity for learning. Typically the aim has been to de-sign a sufficiently rich conceptual framework with which to characterise the activitiesin a target domain [4,20,41]. Nagel [29] for example uses motion verbs as a conceptualframework to mediate between video sequences depicting traffic scenes and natural lan-guage descriptions of those scenes. More recently, Siskind has proposed the use of anevent logic to map from visual input to single motion verbs such as pick up, stack andmove [38].A key issue has been the scalability of conceptual frameworks designed by hand to gobeyond a prototype domain. This has motivated the use of learning procedures to populatea conceptual framework automatically through generalisation from extended observationof a target audio-visual (or language + visual) domain. Cangelosi and Parisi [6] adopt thisapproach within a perception-action setting in which visual input and a language commandtogether determine an action such as the movement of a robotic arm. By representing thismapping as a feed-forward neural network, a standard learning procedure is used to set theparameters of the network from examples of visual and language inputs paired with theresulting actions.For a simpler domain involving only visual and language input, Barnard et al. [5] modelthe relationship between features of image regions and single words naming those regions(e.g., sky, water) as a joint probability distribution. This distribution is learnt using anEM algorithm and can subsequently be used for various purposes through probabilisticinference, for example generating the conditional distribution for the name of a given re-gion.Roy and Pentland [35] demonstrate the use of mutual information between audio andvideo streams to segment words from a corpus containing phoneme sequences associatedwith simultaneous visual situations. The corpus is a video and sound recording of care-givers interacting with infants playing with a variety of objects. The system works fromraw speech utterances and produces phoneme sequences using a trained recurrent neuralnetwork and hidden Markov model.Our work addresses the general problem of unsupervised learning of game protocolsfrom a corpus of raw visual and acoustic data depicting target scenarios. A problem nottackled in earlier work is the likely generation of too many or too few acoustic or visual108C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136object classes during unsupervised learning. With few exceptions (e.g., Roy and Pentland[35]), either an ideal categorisation is imposed by supervised learning or by prior design,or early processing is by-passed altogether and replaced by symbols denoting the acousticor visual objects.If during unsupervised learning too few classes are generated, it will be impossible tolearn structural concepts that depend on a more refined categorisation. If too many classesare generated and this is uncorrected, redundant concepts will be generated which althoughidentical in structure involve distinguished but semantically equivalent objects—this willimpede learning as training examples will be required for each separate instance. A relatedproblem is the misclassification of objects.Within a learning context, earlier work integrating language with vision has addressed anumber of audio-visual tasks involving different levels of language complexity. At its sim-plest, there is the task of naming visual objects, involving single words in isolation [5]. Atthe other end of the spectrum, Roy [34] addresses the task of describing objects using nounphrases which optionally contain a shorter constituent noun phrase (e.g., the pink square;the green rectangle below the peach rectangle). From word sequences segmented from theacoustic signal, the joint probability distribution of consecutive words (bigrams) is esti-mated from relative frequencies and used in turn to generate legal utterances describingnovel visual situations. This bigram model is equivalent to a stochastic regular grammarand could in principle be extended to a higher order language model using instead a sto-chastic context free grammar, although learning the grammar rules would then be morechallenging.Interestingly, stochastic grammars have also been used to model visual activities [2,17,25] when these can be characterised as a temporal sequence of visual primitives. However,they are not ideally suited to many other visual modelling tasks which involve essentiallynon-sequential data, for example the spatial relationships in a single visual image.One way forward for the learning paradigm is to adopt a representational formalism thatis sufficiently powerful to model the concepts of a target domain, such as the event logicused by Fern et al. [11], and then attempt to learn the necessary background knowledgeconnecting language and vision within this formalism. In our work, we have adopted logicprograms expressed in Prolog syntax as our high-level formalism, and inverse entailmentembodied in the PROGOL system as the learning procedure. A limitation of Prolog in thiscontext is the absence of a mechanism for representing uncertainty, for example in thespatial relationship between two objects or the action taken in a given situation. Recentproposals for combining predicate logic with probabilistic inference could overcome thislimitation, assuming equivalent inductive inference procedures can be developed [33]. Forexample, Markov networks have been used successfully for learning and reasoning aboutspatial and temporal uncertainties in physical situations that can be represented by a fixedconjunction of binary predicates [16].PROGOL infers sets of Horn clauses using a statistical optimisation procedure andthereby deals with occasional misclassified objects. Like Roy [34], we work directly fromraw acoustic and visual data, although our language domain contains only single word ut-terances. A key challenge for the future will be to automate learning of richer conceptualdescriptions, such as those that are currently hand-crafted (e.g., Nagel [29]).C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136109Fig. 3. Overview of the learning framework.3. Learning perceptual categories and symbolic protocolsIn this section we present the details of our implemented system, whereby models ofvisual objects and utterances, as well as a symbolic description of the protocol, are learnedin a purely unsupervised manner. Fig. 3 illustrates the overall learning scheme.3.1. From continuous to symbolic dataVideo streams of dynamic scenes contain huge quantities of data, much of which maybe irrelevant to scene learning and interpretation. An attention mechanism is required toidentify ‘interesting’ parts of the stream, in terms of spatial location (‘where’) and temporallocation (‘when’). For autonomous learning, models or heuristics are required to determinewhat is of interest, and what is not. Such models could be based on motion, novelty, high(or low) degree of spatial variation, or a number of other factors. It is highly likely that nosingle factor could provide a generic attention mechanism for learning and interpretationin all scenarios. It is our view that attention from multiple cues is required for fully genericlearning. This section details the attention mechanisms used to segment the continuous au-dio and video streams, and describes the algorithms used to classify the features extractedfrom the input signals in order to create symbolic data for protocol learning. Our aim hasbeen to develop methods that are sufficiently robust to handle environments outside thelaboratory.3.1.1. Attention, learning and classification for visual objectsFor our implementation in the game-playing domain, an attention mechanism whichcan identify salient areas of space and time is necessary. We have chosen motion as itis straight-forward to work with. The spatial aspect of our attention mechanism is basedaround a generic blob tracker [23] that works on the principle of multi-modal (Gaussianmixture) background modelling, and foreground pixel grouping. This identifies the cen-troid location, bounding box and pixel segmentation of any separable moving object in thescene in each frame of the video sequence. The temporal aspect of our attention mecha-nism identifies key-frames where there is qualitatively zero motion for a number of frames(typically three), which are preceded by a number of frames (typically three) containingsignificant motion. Motion is defined as a change in any object’s centroid or bounding boxabove a threshold value (three standard deviations of tracker positional noise, typically fivepixels in our experiments). This method for temporal attention is based on the domain lim-110C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136Fig. 4. Sequence of key-frames identified by the spatio-temporal attention mechanism. Intermediate frames wherethe objects are in motion, or subsequently remain stationary, are filtered out by the attention mechanism.iting assumption that all objects remain motionless following a change in state (and that theprocess of change itself is not important). Fig. 4 shows an example sequence of key-framesidentified by the attention mechanism.Features of the objects identified by the blob tracker (at the key-frames identified bythe attention mechanism) are extracted and clustered. Currently three groups of featuresare extracted, corresponding to the attributes of texture, colour and position. The texturefeature is used to classify global shape (e.g., a star) in addition to conventional textures.The remainder of this section discusses the methods used for unsupervised clustering andformation of classifiers for high dimensional feature vectors, describing colour and texture,which could indeed be applied to many other features that we may wish to use, for exampleglobal shape, local shape, or local textures. For our experiments, position is divided intotwo classes: pos0 for objects on the left of the image, and pos1 for objects on the right ofthe image. This has been done for simplicity (as it suffices for the domains in question anddemonstrates that spatial position may be incorporated in the subsequent symbolic learningprocess). Position could be learned in a similar way to the other groups of features.Rotationally invariant texture featuresThe texture feature set used in this paper was based on the Gabor wavelet [9] and otherconvolution based textural descriptions (Fig. 5). These were applied at the object cen-troid provided by the object tracker, to form a 94-dimensional feature vector. To makethese features rotationally invariant (where they were not already) sets of features at multi-ple orientations were applied to the images and the statistics (mean, minimum, maximumand maximum/mean) of these feature sub-sets used as the feature description, rather thanthe raw values. The features used were: Gabor wavelets (various scales), equal covari-ance Gaussians (various scales), non-equal covariance Gaussians (various scales) and polarwavelet-like features defined by convolution masks of the form:K(r, θ ) = G(r, σ ) sin(K1θ + K2),(1)where r is the distance of a pixel in the mask from the object centroid, θ the angle that a linefrom the object centroid to a pixel in the mask subtends with the horizontal axis, G(r, σ )is a Gaussian distribution with standard deviation σ (centred on the object centroid), K1relates to angular frequency (K1 = 1, 2 or 4 in our experiments) and K2 relates to the filterorientation (typically four values of K2 were chosen in order to form a bank of filters).Convolution kernels of the form in Eq. (1) were applied at various scales (by altering σ ).The scales chosen for all feature descriptor types were selected as (approximately) evenlydistributed between sensible limits, so as to be as general as possible.C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136111Fig. 5. Example wavelet and convolution based textural feature descriptors used.Colour featuresThe colour feature set used in this paper consists of the bins of a 5 × 5 Hue-Saturation(HS) colour histogram. The blob tracker used as part of the attention mechanism provides abinary pixel mask defining the pixels belonging to each object identified. The (RGB) colourvalues of each of these pixel is projected into Hue-Saturation-Intensity (HSI) colour-space[39] and the Hue and Saturation (Intensity independent) dimensions of this space used tobuild a 2D histogram of the object pixel colour distribution. Each dimension is quantisedinto 5 levels (a compromise between the generality and specificity of the representation) togive a histogram with 25 bins (used as a 25-dimensional feature description).Clustering and feature selectionFor each attribute (texture/colour) we wish to form clusters of features vectors whichdefine the different classes denoted by attribute labels. Since our system has no knowledgeof which class each feature description belongs to, an unsupervised clustering approachmust be taken. There are a number of methods in the literature for unsupervised clusteringsuch as K-means [13], agglomerative methods [1,37] and graph partitioning based methods[18]. Such methods generally work by associating similar data items in an n-dimensionalfeature space. All features are used in this clustering process and feature selection is usuallyseen as a supervised pre-processing step (if performed at all). An alternative approach is tocombine the results of multiple clusterings (either multiple methods on a single feature setor a single method on multiple data sets) [40].In the high-dimensional feature vectors (texture responses from a variety of convolutionmasks and colour histogram bins), only a subset of the features will be important in thetask of classifying the visual objects. More importantly, different descriptors will be im-portant in different scenarios, depending upon the visual objects used. It is for this reasonthat a variety of textural descriptors are used to form the 94-dimensional texture featurevector. We use a feature selection method based on agreement between features to selectthe most important. Firstly, clustering is performed independently in each dimension ofthe feature space (1D sub-spaces) in order to induce Nc clusterings of the feature vectors.These clusterings are combined to form weights for edges in a graph with data items asnodes. This graph is then partitioned to form clusters of data items. Finally, supervisedlearning of a classifier is performed, using the cluster membership of data items from thegraph partitioning.We use agglomerative clustering on each sub-space, based loosely on that presentedin [1]. To begin with, the data in each sub-space is normalised so that it has zero meanand unit standard deviation. This is performed so the same stopping criterion (below) forthe sub-space clustering can be applied across all sub-spaces. One cluster per data item is112C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136initialised. For all cluster pairs Ci , Cj (containing Ni and Nj members respectively) wecalculate D(Ci, Cj ): the mean distance between each member of Ci and each member ofCj .D(Ci, Cj ) = 1NiNj(cid:1)(cid:1)s∈Cit∈Cj|s − t|.(2)√Iteratively the closest two clusters Ci and Cj are merged whilst min(D(Ci, Cj )) < T ,where T is a fixed threshold. The value of T = Kd, where d is the dimensionality ofthe sub-space (i.e., 1) and K is a constant (1 in all our experiments). This agglomerativeclustering is performed on all 1D sub-spaces and the results of these clusterings are com-bined using a novel weighted version of the cluster-based similarity partitioning algorithmof Strehl and Ghosh [40]. In the original algorithm, a (non-directional, fully connected)weighted graph is formed where the vertices (unweighted) represent data items and the(weighted) edges represent the similarity between pairs of data items. Similarity is mea-sured as the number of times the pair of data items occur in the same cluster over allsub-space clusterings. This graph is then partitioned using a graph partitioning algorithmsuch as [18].3 Such algorithms attempt to form a set of sub-graphs, such that the edgecut required is minimised. This is an NP complete problem; however there are a numberof suitable approximate methods at our disposal. We extend the method in [40] to forma graph in which the similarity is re-defined as the sum of the weights relating to eachsub-space in which data items occur in the same cluster (Eq. (3)). In this way the relativeimportance of the individual features in our feature description can be weighted:EdgeWeight(Va, Vb) =(cid:2)Ncc=1 WcSc(Va, Vb)Ncc=1 Wc(cid:2),(3)where Wc is the weight associated with clustering c and Sc(Va, Vb) is 1 if the data itemsrelating to vertices Va and Vb are contained within the same cluster for clustering c, and 0otherwise. Nc is the number of clusterings used. It should be noted that the denominatoris simply a constant and, as such, has no effect on the clustering produced. However, theinclusion of this normalisation enables edge weights (and thus edge cuts) to be comparedacross different feature sets and weightings.Once the initial clustering has been performed (with equal weights) the discrimina-tive power of the individual sub-spaces used to build the clustering can be evaluated withrespect to this clustering (i.e., assuming this clustering is “correct”). If a sub-space candiscriminate well between any two classes then its discrimination power is good, as it isunusual for a low-dimensional sub-space (1D in our experiments) to be able to discrimi-nate between all classes. We re-weight the edges of the graph used to partition the data intoclusters, to reflect how “correct” the current (in this case initial) clustering is. This involvesevaluating how well each pair of clusters is discriminated between in each sub-space.3 Implementations of several graph partitioning algorithms are freely available to download from http://www-users.cs.umn.edu/~karypis, as part of the METIS and CLUTO packages.C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136113Fig. 6. Accuracy of a stochastic classifier for two class discrimination.To do this, we consider the accuracy of a two class classifier based on a particular sub-space clustering (C1, C2, etc.). If a class labelled dataset is available, such a classifier maybe formed from the statistics of the associations of classes to clusters, as in Eq. (4)P (class | Ci) =|{X : X ∈ class ∧ X ∈ Ci}||{Y : Y ∈ Ci}|.(4)However, class membership is not known in our case. If the clustering obtained from thegraph partitioning method is assumed to be the “correct” class labelling, such a classifiermay be formed. Fig. 6 illustrates the theoretical accuracy of such a classifier as a Bayesiannetwork.In Fig. 6, the diagram on the left illustrates the probability distribution of the data itemsfor a two-class discrimination problem. From this diagram it is easy to see the joint proba-bility P (L, Ci), where L is a label for the true class (A or B) and Ci is one of the sub-spaceclusters, is given by:(cid:3)P (L, Ci) = P (Ci | L)P (L)/(cid:4)P (A) + P (B).(5)On the right under the heading P (Correct), alongside each node, is the probabilityP (L | Ci) that classification is correct for a stochastic classifier (i.e., P (Correct | L, Ci)).The sum of the products of P (Correct | L, Ci) and P (L, Ci) over all pairs of clustersand labels gives the overall probability of a correct classification for this classifier basedon a particular low-dimensional cluster set C = {C1, C2, . . . , Cn}, and a particular pair oflabellings A, B:P (Correct | A, B, C) =(cid:1)L=A,BP (L)P (A) + P (B)(cid:1)Ci ∈C(cid:5)(cid:6)P (Ci | L)P (L | Ci),(6)where P (A), P (B), P (Ci | L) and P (L | Ci) can be calculated from a co-occurrencefrequency matrix of clusters formed from the graph partitioning vs. sub-space clusters forthe training data. Thus we define discriminative power, D(C), as the maximum value ofP (Correct | A, B, C) over all pairs of classes, as in Eq. (7).D(C) =maxa,b=A,B,C,...,a!=bP (Correct | a, b, C).(7)D(C) lies between 0.5 (no discriminative power) and 1.0 (perfect discrimination for atleast one pair of classes). This value can be used to weight the graph partitioning based114C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136clustering described in Eq. (3); however a better approach is to threshold this value and usebinary weights (Eq. (8)). We use a fairly low threshold (Tw) of 0.6 to exclude only thosesub-spaces that are very poorly discriminative. Essentially this is an unsupervised featureselection approach.(cid:7)Wc =0 if D(C) < Tw,1 if D(C) (cid:1) Tw.(8)In principle, our method could be applied iteratively by repeatedly using the clusteringoutput to calculate new weights and re-clustering. Our experiments suggest that improvedperformance and convergence occurs in some circumstances; however in other circum-stances over-fitting and other stability problems have been observed. We use only a singleapplication of the re-weighting for this reason. It should be noted that our approach relieson the initial clustering having a (more or less) one-to-one mapping with the true classlabelling (i.e., at least 50% of items in a cluster belong to the same true class). If this is notthe case, it is unlikely an improvement will be obtained.Once a set of examples is partitioned, the partitions may be used as supervision for aconventional supervised statistical learning algorithm such as a Multi-Layer Perceptron,Radial Basis Function or Vector Quantisation based nearest neighbour classifier (the latteris used in our implementation). This allows for the construction of models that encapsulatethe information from the clustering in such a way that they can be easily and efficientlyapplied to novel data. These models are used to generate training data suitable for symboliclearning. For each object identified by the attention mechanism, a (symbolic) property isassociated with it for each semantic group using these learned classifiers.3.1.2. Attention, learning and classification for spoken utterancesA symbolic description of audio events must be obtained from the input audio signal.This task is relatively straight-forward since we are dealing with a small number of isolatedsounds (generally single words). Speech recognition and production software could be usedto perform this task (e.g., HTK [42]), normally requiring supervised learning [7,12,32];however an unsupervised approach to this task is favoured to fit in with the philosophy oflearning models and rules for an autonomous synthetic agent. Such an approach also hasthe advantage that participants can make non-word utterances (e.g., animal noises) or makesounds using objects or instruments. The methods presented in this section are crude, yetadequate for our purposes. For more complex applications, more sophisticated techniquesare likely to be required.The participant of the game who we wish to replace with a synthetic agent speaks intoa microphone. The audio is sampled at 8172 Hz and an attention mechanism based on theenergy of the signal is used to segment sounds. Non-overlapping windows are formed eachcontaining 512 samples, which is the power of 2 (needed for the Fourier transform) givingwindows with the most similar duration to a frame of video. The energy for each window iscalculated as the sum of absolute values of the samples. Utterances are contiguous windowswith energy above the threshold and are of maximal duration (Fig. 7).C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136115Fig. 7. The attention mechanism segments the audio signal.Fig. 8. Spectral analysis.Spectral analysis is performed on each detected window. The dimensionality of eachspectrum is reduced from 512 to 174 by histogramming the absolute values. Reducing thespectrum to this dimensionality makes the clustering more robust to small variations in thepitch of the voice [19]. Each utterance detected is then represented by a temporal sequenceof L reduced-dimensionality windows. L is chosen such that it is equal to the length of theshortest utterance (in windows) in the training set. This is achieved by linearly resamplingthe temporal sequence of spectral histograms for the windows that span an utterance. Theutterances are now of identical length, and K-means clustering is performed on the set ofutterances several times with different numbers of clusters.Fig. 8 shows a set of nine clusters formed during one run of the process, visualised asspectrograms (plots of frequency vs normalised time) relating to the cluster centres. Thenumber of clusters is automatically chosen such that the ratio between the mean distanceof each utterance to the centre of the closest cluster and the mean distance between allthe cluster centres is minimised. Each utterance of the training set is classified (nearestcluster centre) to create a symbolic data stream as shown in Fig. 9; thus an utterance may4 This depends upon the rate that the audio is sampled (8172 Hz), the fundamental pitch frequency of the humanvoice (average around 275 Hz) and the size of the window used (512 samples equivalent to windows at 16 Hz).275/16 is approximately 17.116C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136be represented symbolically as one of a set of labels utt1,utt2,.... This method forautomatically choosing the number of clusters tends to over-cluster the data (too manyclusters are created); our approach to this problem is to build equivalence classes betweenthe utterances, as discussed in Section 3.3.2.3.2. Symbolic data representationTo drive a synthetic agent capable of interacting with its environment we wish to learnthe protocols for the agent’s actions. In this work, actions are the vocal utterances; how-ever, the principle should be applicable to other possible action types, such as movementof a robotic arm. Our aim is to encapsulate the protocol of the activity in a way in whichactions can be fired following observation of perceptual inputs. Thus utterances of a par-ticipant are described in the form action(utterance, time). The playing area is describedby a list of objects, each described by their attributes of texture, colour and position in theform: state([[texture1, colour1, position1],[texture2, colour2, position2]],time),which represents a state containing two objects. In addition, each time (time(time).)and temporal successor (successor(previous time, current time).) is denoted. Exam-ple audio-visual input for SNAP1 and the corresponding symbolic data stream is shownin Fig. 9. When the table is empty, the description of perceptual inputs is an empty list,as no objects have been tracked, e.g., state([],t518). Further details are discussed inSection 3.3.1.Our synthetic agent interacts with its environment through actions (utterances) and per-ceives the environment through visual input of objects in the playing area. Thus an actionby the agent will be fired immediately after the interpretation of perceptual input states.Therefore in the symbolic data presented for protocol learning, we wish for the actions tooccur at the same qualitative time as the states (although quantitatively they occur succes-sively). This creates a direct link between perception and action.Our prototype system works in real time on live input data. As such, the frame rate ofthe tracker is variable, depending upon the number of objects it is tracking, the amount ofmotion in the scene, and what features it is extracting from the images for clustering. Thus,the use of frame numbers is not sufficient to link times in the input video stream with theutterances obtained through processing the separate audio stream.To form a direct link between states and actions, at the beginning of a training phase,a clock is started, and the number of seconds since the start is recorded for each (state,time, successor) triple from the object tracker. The number of seconds since the startis also recorded for each action utterance from the audio stream. The timings for this sym-bolic data stream from the audio is then synchronised with the timings from the video datastream. Each utterance is back-dated to the timing of the previous state description if thetiming is not coincident already. This simple method works for our application. A morecomplex approach could be undertaken to provide a richer description where not only tem-poral successor relationships between events are used, but descriptions such as ‘before’,or ‘during’ may be useful for extended temporal events (in the future, the tracking of themovement of objects may be of interest, particularly when applied to the learning of ro-botic actions) for which a temporal representation such as Allen’s interval calculus [3,31]may be useful.C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136117Fig. 9. Example audio-visual inputs and corresponding symbolic data representation for the game SNAP1. Thesymbols correspond to the perceptual categories learned by unsupervised clustering. In this case, the symbolsutt10, utt9, utt1 and utt6 are symbols for utterances representing, respectively, the words “colour”,“play”, “shape” and “same”, and the texX relate to the texture/shape of objects, colX relate to the colour ofthe objects, and posX to the object’s position. The textual description in italics is provided for readers viewing agreyscale copy. (For colours see the web version of this article.)3.3. Learning symbolic protocols from perceptual data3.3.1. Inductive logic programmingInductive logic programming (ILP) [21,26] is the name given to the field of AI thatstudies inference methods by which given background knowledge B and examples E, the118C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136:- modeh(1,action(#utterance,+time))?:- modeb(100,state([[-shape,-colour,-position],[-shape,-colour,-position]],+time))?:- modeb(100,state([],+time))?:- modeb(100,successor(+time,-time))?Fig. 10. Mode declarations for the game SNAP1.simplest consistent hypothesis H is found such that: B ∧ H |= E where B, H, E are logicprograms (informally: when B and H are true, E is also true). In our terms, we seek toinduce a simple logic program H which when combined with the ‘state’ and ‘successor’atoms (from the training data) B, entails the ‘action’ atoms E. No generic domain knowl-edge is included in the background B.5Our work uses CProgol4.4 [28] which is an implementation of the definite modes lan-guage [27]. PROGOL searches for the most statistically plausible hypothesis H usinginverse entailment—a depth-first search through constructed hypotheses. PROGOL worksby generalising a set of examples, given a list of types, accounting for the categories ofobjects in the domain under consideration, and a set of syntactic biases. These syntacticbiases are user defined mode declarations that restrict the possible form of the proposedgeneralisations, restricting the possible search space. Mode declarations describe the pred-icates (with their argument types) that can be used either in the head or in the body ofhypothesised generalisations. These declarations also state how variables in the argumentof predicates in the head and body of formulae should be used in the formulae sought. Inother words, variables can be declared in the modes as either input, output or grounded(constant). An example of mode declarations for the input data described in Section 3.2 isshown in Fig. 10, where modeh and modeb represent, respectively, predicates that shouldbe in the head and in the body of the generalising formulae. The symbols +, - and #, rep-resent respectively input, output and grounded variables and the terms shape, colour,position and time are term types. The number in the first argument of modeh andmodeb bounds the number of alternative solutions for instantiating the predicate stated inthe second argument.The mode declarations in Fig. 10 state that the generalising formula should have thepredicate action/2 in the head (with a grounded utterance and an input variable fortime as arguments) and predicates state/2 and successor/2 in the body. In principle,there is no limit on the number of occurrences of each predicate in the mode declarationsused with different combinations of input, output and grounded variables. Examples ofPROGOL type declarations for the game SNAP1 are shown in Fig. 11. The constant symbolnames are chosen to make the example clear to read; in practice the semantic meaning ofthe object property or utterance labels is unknown, as the labelling come from the lowerlevel systems and is devoid of semantics (except as provided by the lower level grounding).Although PROGOL is capable of generalising from positive and negative examples, we onlyhave positive examples available.5 Implicit domain knowledge is provided by the mode declarations.C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136119utterance(play).shape(ring).utterance(colour). shape(star).utterance(shape).utterance(both).utterance(nothing).colour(red).position(pos0).colour(green). position(pos1).shape(flash). colour(blue).Fig. 11. Type declarations for the game SNAP1.:- state([],A).(1) action(play,A):- state([[B,C,D],[E,C,F]],A).(2) action(colour,A)(3) action(shape,A):- state([[B,C,D],[B,E,F]],A).(4) action(nothing,A) :- state([[B,C,D],[E,F,G]],A).:- state([[B,C,D],[B,C,E]],A).(5) action(both,A)Fig. 12. Unordered PROGOL output for the game SNAP1.Briefly, PROGOL’s search can be described as follows. For each positive example, PRO-GOL initially generates a most specific Horn clause, according to the mode declarations.This clause is further contrasted with the remaining examples in the search for a moregeneral formula capable of subsuming the examples in the data set. An example of PRO-GOL’s output for the SNAP1 game (given mode and type declarations as described above)is presented in Fig. 12.Formula (1) in Fig. 12 shows that PROGOL built the connection between the utteranceplay and the empty state. The variable A in both the head (e.g., action(play,A))and the body (e.g., state([],A)) indicates that a generalisation over time has beenmade; A represents an ungrounded time state, i.e., any time. Thus formula (1) is interpretedas “if the table is empty at time A, then respond at time A with the action of utteringplay”. Formulae (2)–(5) show the rules learned for non-empty states, expressed using co-occurring variables as arguments to state/2. E.g., in formula (2), the repeated variableC in the second position in each object’s feature list encodes the condition that two objectshave the same colour and that the appropriate action is to utter “colour”. Note that the rulesare unordered and do not constitute a logic program that correctly specifies the intendedprotocol. In Section 3.4, we discuss how an inference engine orders these rules.The key motivation for our use of inductive logic programming is that the generatedrules can be applied to situations never seen before. For example, the complete rules forSNAP1 may be learned without an example in the training data of the utterance “colour”after two red objects are in the scene; it can generalise from the other examples in thetraining data where “colour” is uttered after two green or two blue objects are in the scene.Moreover, if the learning of object classifiers (for texture and colour) was allowed at run-time (i.e., during ‘play mode’), then the rules learned could also be applicable to objectsand features never having been seen at all during training.3.3.2. Building equivalence classes of utterancesAs mentioned in Section 3.1.2, the method for utterance clustering is prone to clusterinto more than the true number of clusters. However, we are able to use the generalisationrules found by PROGOL to construct equivalence classes among utterances. The procedure120C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136for generating equivalence classes is based on the hypothesis that rules with similar bodiesare related to equivalent utterances in the rule heads. There are many possible ways ofdefining similarity in logic programs [10], and an investigation of those is outside the scopeof this paper. As we shall see in Section 4.2, in this work we use unification and identity ofbodies as our similarity measures between clause bodies.To construct equivalence classes, firstly, every pair of input rules whose heads are ofthe form action(utterance,time) are checked for whether their bodies are sim-ilar. Clauses with empty bodies are discarded as they do not provide any evidence forequivalence. Assume a transitive, reflexive and symmetric binary relation equiv/2 de-fined over utterances. Thus, for every pair of clause heads whose bodies are similar, astatement equiv/2 is created (and asserted), stating the hypothesis of equivalence be-tween the utterances in their arguments. For instance, let action(utt_i,t_x) andaction(utt_j,t_y) be two clause heads with similar bodies, then the hypothesisof equivalence between the utterances utt_i and utt_j is created and asserted as thestatement equiv(utt_i,utt_j). For notational convenience, we call atomic formula,such as equiv(utt_i,utt_j), equivalence pairs. Two utterances utt_i and utt_jare hypothesised as being equivalent if the equivalence pair equiv(utt_i,utt_j) ap-pears once (or more) in the training examples.6 Finally, equivalence classes are created bytaking the transitive closure of the relation equiv/2.The process of building equivalence classes can be exemplified in the game SNAP1if we assume that the audio clustering algorithm found many distinct representations forthe same utterances. For instance, suppose this algorithm clustered the word “play” into 4distinct classes, namely p_1, p_2, p_3 and p_4 and that PROGOL found the followingfour rules involving these symbols:action(p_1,A) :- state([],A).action(p_2,A) :- state([],A).action(p_3,A) :- state([],A).action(p_4,A) :- state([],A).Therefore, the method for constructing equivalence classes suggests that [p_1, p_2,p_3, p_4] forms one class that is pairwise disjoint with respect to the classes combiningthe remainder of the utterances. In this case the bodies are all trivially similar, as they areidentical, but this need not always be the case.In this paper we are only obtaining equivalence classes of utterances, leaving aside theproblem of over clustering on learning the models of visual objects. In fact, the problem ofover clustering models of perceptual objects, in our setting, is more evident on the audiothan on the visual domains. This is because the visual representation is invariant to variationin orientation and lighting, whereas there is little invariance in the audio representation tovariation in pitch and dynamics (e.g., the participant may get excited when winning or ifsomething unexpected happens).6 The number of times that an equivalence pair is hypothesised as equivalent in the training examples couldbe used as a measure of confidence on the equivalence hypotheses, thus providing a thresholding value for theinclusion of objects in equivalence classes. This issue, however, is a matter for future research.C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136121action(both,A):- state([[B,C,D],[B,C,E]],A).action(shape,A):- state([[B,C,D],[B,E,F]],A).:- state([[B,C,D],[E,C,F]],A).action(colour,A)action(nothing,A) :- state([[B,C,D],[E,F,G]],A).action(play,A):- state([],A).Fig. 13. Sorted PROGOL output for the game SNAP1 corresponding to the unordered output in Fig. 12.3.4. Inference engine for agent behaviour generationThe symbolic protocols learned by PROGOL are used by a Prolog program as a set ofrules used to drive an interactive synthetic agent that can participate in its environment.This set of rules is, prior to input to the agent, automatically filtered and ordered accordingto the following criteria. Ground atomic formulae are kept at the beginning of the rule set(as initially given by PROGOL). These formulae will never be selected by the agent as theyhave their temporal variable instantiated to a very specific time point. Non-ground atomicformulae are moved to the end of the data set, since these are the most general rules (i.e.,the action expressed in their heads is not constrained to any particular perceptual input).Non-atomic rules whose bodies unify are ordered from the one with the most specific tothe one with most general body.This ensures that more specific rules are fired first (otherwise they would be subsumedby more general rules). This ensures that rule 4 in Fig. 12 (which is satisfied for everyperceptual input of two objects) is ordered after rules 2 and 3. It also ensures that rule 5(the most specific) is the first rule. Fig. 13 shows the result of ordering the ruleset of Fig. 12.Further examples are given in Section 4. The remaining rules are kept in the order outputfrom PROGOL. It is worth pointing out that PROGOL ranks its output formulae accordingto their predictive power and compression with respect to the data set; therefore, rules atthe bottom of the rule set are those that predict fewer examples than those handled by therules at the top. The former usually represent idiosyncrasies of the data and, due to theirpositions in the rule set, may never be selected by Prolog. In addition to this, a cut is addedas the last conjunct of each non-ground formula’s body. This guarantees that only a singleaction will fire at any time step.The synthetic agent, loaded with the learned rules (ordered as above), receives inputfrom the perceptual system, and outputs the inferred utterance through a sound synthesismodule. This module replays an audio clip of the appropriate response (the one closest tothe cluster centre), automatically extracted from the training session.For greater effect, a visual waiting head (Fig. 14) is displayed on a screen, and in timewith the replay of the audio clip, a corresponding facial movement is made by the talkinghead. The video for this is captured with the audio of the participant to be replaced by asynthetic agent. The details of the video-realistic waiting head is not the focus of our work,but it adds realism to the demonstration system.77 Movies demonstrating the work of this paper, both the learning and execution phases are online athttp://www.comp.leeds.ac.uk/vision/cogvis/.122C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136Fig. 14. Synthetic agent. (a) During training, the audio and video stream of a participant are captured frommicrophone and webcam, objects are tracked by a separate webcam pointing at the table. (b) In the executionphase, the participant no longer says anything, he follows the instructions of the interactive talking head displayedon the screen and broadcast from the speakers. (c) Closeup of the waiting head displayed on the screen in (b).(d) The head talks by replaying the audio-video stream associated with the inferred action.As there is no robotic arm in the system, a human participant is required to follow theinstructions uttered by the synthetic agent. Currently the agent executes a single actiongeneration per time step. This restriction, however, might be relaxed without the need tochange the central ideas of the system presented above.4. Experiments and resultsIn this section we present some experimental results of applying the framework forlearning protocol behaviour on three game playing scenarios (Section 4.2). The resultsare analysed using an evaluation method (Section 4.1) based on verifying soundness andcompleteness of the learned rule sets of Section 3.3. Note that the ordering constraints ofSection 3.4 are not taken account of in the learned rule sets—they are just taken to be sets ofdefinite clauses. As noted below, the experimental results for completeness would in somecases be better if the ordering constraints were to be taken into account. Informally, themethod provides the extent to which the given model is represented in the learned rule set—soundness—and the extent to which the predictions of the learned rule set agree with theC.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136123predictions of the model (for the same perceptual inputs)—completeness. Therefore, thismethod utilises logic-based concepts to provide a principled way of evaluating our system,which is one of the reasons for choosing a symbolic learning tool within this framework.4.1. Evaluation methodIn order to evaluate how well the protocol rules were learned from noisy vision data, wecontrast the rules learned from each dataset with a hand-coded set of formulae; these handcoded rules can be understood as the underlying semantics of the game, i.e., they perfectlycapture the protocol rules to be learned. If perfect learning takes place, then a logicallyequivalent set of rules should be learned from vision data. In practice, we are not worriedabout exact logical equivalence, but rather whether the same actions would be performedin identical circumstances (the learned formulae might possibly entail statements involv-ing the performance of actions, and thus might not necessarily be in the hand coded set).Therefore we want to evaluate both:(1) on how many actions the learned set agrees with hand coded set; and(2) on how many actions the hand coded set agrees with learned set.Making an analogy with model theory for inference systems, we call the first conditionsoundness, as it checks whether learned actions agree with the semantics, i.e., whetherthe learned rule set correctly predicts actions (given the gold standard of the hand codedset). The second condition might be called completeness, as it checks whether all the ac-tions predicted by the semantics (under particular conditions) are similarly indicated in thelearned rule set. Formally, we can characterise these ideas as follows.Definition (Agreement between rule sets). If X , Y1 |= Z implies X , Y2 |= Z, then Y1agrees with Y2 on Z in situation X .Definition (Total soundness and completeness of a learned rule set with respect to a handcrafted rule set). Let A be a set of formulae representing possible actions (typically char-acterised syntactically as atomic formulae formed by a distinguished predicate). Let H andL be two sets of formulae representing the handcrafted and learned rule sets respectively.Let X represent a symbolic description of the world at a given time (i.e., a set of groundatomic formulae—state, time, successor, action—as illustrated in Figs. 9,16 and 18).If L agrees with H on every action θ ∈ A for all X , then L is sound with respect to H.If H agrees with L on every action θ ∈ A for all X , then L is complete with respectto H.However, in practice, the data may be noisy and incomplete causing rules to be missed,over-generalised or mis-represented. Therefore, we define partial soundness and complete-ness as follows.124C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136Definition (Partial soundness and completeness of a learned rule set with respect to ahand crafted rule set). Let A, H and L and X be as above.If L agrees with H on a maximal subset of actions Θ ⊂ A for some X , then L issound with respect to H.complete with respect to H.If H agrees with L on a maximal subset of actions Θ ⊂ A for some X , then L is|Θ||A||Θ||A|A set of three games are used to show the performance of our cognitive vision system.The next section describes each game in detail, presenting typical outputs of the systemand discussing experimental results based on the evaluation method above.4.2. Experimental evaluation4.2.1. Experiment 1: SNAP1This game demonstrates the system’s ability to generalise rules across perceptual cate-gories, rather than grounding each instance. SNAP1 is a generalised snap game where twocards are played, followed by an utterance dependent upon the state of the two cards whichare either the “same”, the same “colour”, the same “shape” or “nothing”. The cards arethen removed, and “play” is uttered indicating to play the next two cards. Fig. 9 illustratesan example sequence of the game. For SNAP1, the learning process has been completedthree times, each time with a dataset containing 100 objects and 100 utterances, equivalentto 50 rounds of the game.Fig. 15 presents an example of a typical rule set output by the system. This set shows thatfour distinct versions of the utterance “play” (represented by the symbolic labels utt2,utt3, utt8 and utt9) were correctly learned; rules for “same”, “colour”, “shape” and“nothing” (utt6, utt10, utt1 and utt4 respectively) were properly built, along withone sub-optimal rule (in terms of compactness) required to explain utt7 which was placedas the last rule in the data set, according to the criteria discussed in Section 3.4.Table 1 presents the evaluation of three runs of SNAP1 according to the evaluationmethod introduced in Section 4.1. Runs 1 and 2 are totally sound (scored 100% for sound-ness) meaning that every prediction made by the intended semantic was also obtained bythe learned set for the same perceptual input. Both runs scored less than 100% for com-pleteness (i.e., 71.4% for run 1 and 88.8% for run 2) as idiosyncratic rules for “play” werefound. These rules represented that an action “play” should be valid at a time point t if, inthe previous time point, there were any two objects on the table. Although this is a true factin the domain in question, this rule does not represent the correct protocol of producing theutterance “play” as it allows it to be pronounced when the table is not empty.Run 1 could not find rules for “colour” or “same”, thus scoring 60% with respect tosoundness. However, 80% of the rules found could be mapped into the intended semantics.The misbehaviour of this experiment with respect to soundness may be explained by erro-neous clustering of the colour feature, which in our current implementation is unreliable.Analysis of run 3 reveals the sensitivity of PROGOL to misclassified data. We are learn-ing from small amounts of data which is locally sparse, thus a classification error may makeup a significant amount of the data used to form a generalisation. This may seem very frag-ile; however it is the behaviour that we should expect. SNAP1 run 3 contains six examplesC.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136125action(utt4,t65).action(utt5,t198).action(utt1,t237).action(utt6,A) :- state([[B,C,D],[B,C,E]],A).action(utt10,A) :- state([[B,C,D],[E,C,F]],A).action(utt1,A) :- state([[B,C,D],[B,E,F]],A).action(utt4,A) :- state([[B,C,D],[E,F,G]],A).action(utt9,A) :- state([],A).action(utt8,A) :- state([],A).action(utt2,A) :- state([],A).action(utt3,A) :- state([],A).action(utt7,A) :- successor(B,A), state([[C,D,E],[F,G,H]],B).Fig. 15. Sorted learned rule set for SNAP1 run 1. Key to symbolic utterance labels: “same” = utt6, “colour” =utt10, “shape” = utt1, “nothing” = utt4, “play” = utt2, utt3, utt5, utt7, utt8, utt9.Table 1Results of the evaluation of the protocol rules learned from per-ceptual observation of three runs of the game SNAP1GameSNAP1 run 1SNAP1 run 2SNAP1 run 3SoundnessCompleteness100.0%100.0%60.0%71.4%88.8%80.0%of two objects having the same colour and same texture, and one of the same shape butdifferent colours, followed each time by the utterance “same”. PROGOL generalises therule action(same,A) :- state([[B,C,D],[B,E,F]],A). If the single ac-tion (following a state in which colour is misclassified) is removed from the trainingset, then SNAP1 run 3 would contain six examples of two objects having the same colourand same texture, followed each time by the utterance “same”, and PROGOL generalisesthe correct rule action(same,A) :- state([[B,C,D],[B,C,E]],A).The sound clustering of run 1 produced three classes for “play”, and one class for each ofthe other utterances. The method for constructing equivalence classes (proposed in Section3.3.2) used identity of bodies as similarity measure in this experiment. With this it was ableto cluster two of the “play” utterances into one single class leaving the remainder as classescontaining one symbol (unary classes). In run 2 the system found the correct number ofequivalence classes, namely, one class for “play” containing five different instances ofthis utterance, and unary classes for the other words. In run 3 only unary equivalenceclasses were obtained, while the expected result would be one class for the two utterances“nothing” and unary ones for “shape”, “same”, “colour” and “play”.4.2.2. Experiment 2: SNAP2This game demonstrates the system’s ability to generalise rules across perceptual cate-gories, and introduces a temporal dependency on the previous state. SNAP2 is a variationof SNAP1 where cards are placed one on top of another and the resulting utterance isdependent upon the card which is visible, and the card at the previous time step. An exam-126C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136Fig. 16. Example audio-visual inputs and corresponding symbolic data representation for the game SNAP2.Table 2Results of the evaluation of the protocol rules learned from per-ceptual observation of three runs of the game SNAP2GameSNAP2 run 1SNAP2 run 2SNAP2 run 3SoundnessCompleteness100.0%100.0%75.0%100.0%71.4%100.0%ple sequence of the game is shown in Fig. 16. For SNAP2, the learning process has beencompleted three times, each time with a dataset containing 50 objects and 50 utterances,equivalent to 50 rounds of the game.Table 2 shows the evaluation of the three runs of this game according to the methoddescribed in Section 4.1. The results in run 1 in SNAP2 show that our system could learna totally sound and complete data set for this game (see Fig. 17 for the rule set). Run 2(not shown) was 100% sound but only 71.4% complete, because one rule for “nothing”C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136127action(utt11,t48).action(utt12,t101).action(utt8,t131).action(utt8,t160).action(utt8,t184).action(utt11,t218).action(utt6,t256).action(utt3,A) :- state([[B,C,D]],A), successor(E,A),state([[B,C,D]],E).action(utt1,A) :- state([[B,C,D]],A), successor(E,A),state([[F,C,D]],E).action(utt5,A) :- state([[B,C,D]],A), successor(E,A),state([[F,C,G]],E).action(utt7,A) :- state([[B,C,D]],A), successor(E,A),state([[B,F,D]],E).action(utt9,A) :- state([[B,C,D]],A), successor(E,A),state([[B,F,D]],E).action(utt10,A):- successor(B,A), state([[C,D,E]],B),successor(F,B), state([[G,D,E]],F).action(utt4,A) :- successor(B,A), state([[C,D,E]],B),successor(F,B), state([[G,H,E]],F).action(utt2,A).Fig. 17. Sorted learned rule set for SNAP2 run 1. Key to symbolic utterance labels: “same” = utt3, utt6,utt9, “colour” = utt1, utt5, “shape” = utt2, utt7, utt11, “nothing” = utt4, utt8, utt10, utt12.and one for “same” were misleading. These rules were probably the result of systematicmisclassification of the colour feature during the training period. However, the sortingmethod (described in Section 3.4) placed both of these misleading rules at the end of therule set. Authentic rules for these words were thus higher ranked. Therefore, the erroneousrules, in this case, do not jeopardise the behaviour of the agent. Run 3 scored 75% forsoundness since its learned set did not find any rule representing the word “nothing”. Thiswas due to a lack of examples about this utterance in the data set considered.The same equivalence criteria used in SNAP1 (identity of bodies) was assumed inSNAP2. In run 1 of SNAP2, twelve utterances were clustered by the audio clusteringmethod (four representing “nothing”, three “same”, two “shape” and two for “colour”);the equivalence method constructed the correct equivalence class for “colour”, and found aclass with two (out of three) symbols for same, the remainder were not found. The reasonfor missing elements in equivalence classes, even though the result was sound and com-plete, was due to the fact that there were no useful rules in the answer set for some ofthe symbols representing the utterances. For example, action(utt6,t256) in Fig. 17is the only rule containing utterance utt6 and it has no body, so will never fire. Thisusually occurs when the data set contains a restricted number of examples about theseutterances. Therefore, the equivalence method did not have enough information to buildthe appropriate classes. This method suffered from the same problem, in runs 2 and 3 ofSNAP2.128C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–1364.2.3. Experiment 3: PSS—paper, scissors, stoneThis game demonstrates the system’s ability to ground utterance actions with sets ofspecific perceptual inputs. PSS is played by two players, each simultaneously selectingone of the object cards that are placed on the table. The game protocol is as follows. Ut-terances “play”, “win”, “lose” and “draw” are spoken by the player to be replaced by asynthetic agent. The action “win” occurs when the object paper beats (wraps) the objectstone, scissors beats (cuts) paper, and stone beats (blunts) scissors. In this game, the posi-tion of object cards as well as their texture is crucially important, since if the position ofthe cards was swapped then the resulting action would be altered from “win” to “lose” orvice-versa. Fig. 18 illustrates an example sequence of the game, where the utterance is saidby the player who places his card on the right of the playing area; a typical output of thelearning process is shown in Fig. 19.For PSS, the learning process has been completed three times, each time with a datasetcontaining 200 objects and 200 utterances, equivalent to 100 rounds of the game. This islarger than the other games, since a greater number of rules are being learned in this case.It is worth noting that if fewer examples are available, the whole process does not failcompletely, but a partial description of the protocol is learned. Table 3 presents the resultsof three runs of PSS.Runs 1 and 2 were both totally sound and (approximately) 88% complete. The reasonwhy they are not totally complete is due to occasional misclassification’s of visual objects,causing misleading formulae to be generalised from the data sets. Poor results were ob-tained for soundness in run 3 since none of the rules representing “lose” were constructed.This was due to the fact that only one class for the utterance “lose” was obtained by theaudio clustering algorithm and this happened to occur in the head of a idiosyncratic rule inthe learned set.In PSS we use term unification as our equivalence criteria. In run 1, from the 14 clustersrepresenting the four utterances in this game, our method for hypothesising equivalencesfound five distinct classes. The three different representations of the utterance “win” andthe two for “lose” were accurately combined into two distinct classes. Six out of the eightsymbols representing the word “play” where clustered into one class. The single symbolrepresenting “draw” occupied an unary class. Two symbols for “play” are missing fromtheir relative equivalence class as they occurred in the head of idiosyncratic rules. Theexact number of equivalence classes were built for run 2, the four distinct versions of“play” and the two for “win” were joined into two distinct classes. The single “lose” and“draw” were assigned two unary classes. This method presented the same accuracy whenapplied to run 3.In summary, from the experiments reported in this section we can conclude that oursystem could learn correct protocols from noisy visual data, as expressed by the many100% soundness results shown in Tables 1, 2 and 3. Even in situations where rules for someitems in the protocol were not found, our system was able to construct a partial descriptionof behaviour that allows the agent to behave suitably given appropriate perceptual input.Most of the partial results for completeness do not jeopardise the actuation of the syntheticagent as the idiosyncratic rules obtained were, in general, occupying the end of the rule setas dictated by our sorting procedure (described in Section 3.4). The ordering procedure,however, has a palliative effect, as the cause of idiosyncratic rules resides in the occasionalC.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136129Fig. 18. Example audio-visual inputs and corresponding symbolic data representation for the game PSS.erroneous clustering of the colour feature and the generalisation of irrelevant facts from thedata sets. A de facto solution for these issues is related to the integration of a more robustcamera apparatus in our setting and the development of methods, within ILP, for choosingthe most relevant formulae from a set of generalising rules.The proposed method for building equivalence classes produced the exact number ofclasses for the game PSS, however it did not behave with the same accuracy in the SNAP1and SNAP2 experiments. The reason for this difference in accuracy of the results for PSS130C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136action(utt13,t294).action(utt13,t458).action(utt11,t462).action(utt9,A) :- state([[tex2,B,pos0],[tex0,B,pos1]],A).action(utt13,A) :- state([[tex1,B,pos0],[tex0,B,pos1]],A).action(utt9,A) :- state([[tex1,B,pos0],[tex2,C,pos1]],A).action(utt1,A) :- state([[tex1,B,pos0],[tex1,C,pos1]],A).action(utt3,A) :- state([[tex0,B,pos0],[tex1,C,pos1]],A).action(utt11,A) :- state([[tex2,B,pos0],[tex1,C,pos1]],A).action(utt1,A) :- state([[tex2,B,pos0],[tex2,C,pos1]],A).action(utt1,A) :- state([[tex0,B,pos0],[tex0,C,pos1]],A).action(utt11,A) :- state([[tex0,B,pos0],[tex2,C,pos1]],A).action(utt3,A) :- state([[tex2,B,pos0],[tex0,C,pos1]],A).action(utt7,A) :- state([[tex2,B,pos0],[tex0,C,pos1]],A).action(utt7,A) :- state([[tex1,B,pos0],[tex2,C,pos1]],A).action(utt11,A) :- state([[tex1,B,pos0],[tex0,C,pos1]],A).action(utt9,A) :- state([[tex0,B,pos0],[tex1,C,pos1]],A).action(utt7,A) :- state([[tex0,B,pos0],[tex1,C,pos1]],A).action(utt13,A) :- state([[tex0,B,pos0],[tex2,C,pos1]],A).action(utt13,A) :- state([[tex2,B,pos0],[tex1,C,pos1]],A).action(utt8,A) :- state([],A).action(utt2,A) :- state([],A).action(utt14,A) :- state([],A).action(utt5,A) :- state([],A).action(utt6,A) :- state([],A).action(utt12,A) :- state([],A).action(utt10,A) :- successor(B,A), state([[tex2,C,pos0],[tex0,C,pos1]],B).action(utt5,A) :- successor(B,A), state([[tex0,C,pos0],[tex2,D,pos1]],B).action(utt12,A) :- successor(B,A), state([[tex1,C,pos0],[tex2,D,pos1]],B).action(utt4,A).Fig. 19. Sorted learned rule set for PSS run 1. Key to symbolic labels: “win” = utt3, utt7, utt9, “lose” =utt11, utt13, “draw” = utt1, “play” = utt2, utt4, utt5, utt6, utt8, utt10, utt12, utt14, paper= tex0, scissors = tex1, stone = tex2.Table 3Results of the evaluation of the protocol rules learned from per-ceptual observation of three runs of the game PSSGamePSS run 1PSS run 2PSS run 3SoundnessCompleteness100.0%100.0%75.0%88.5%88.8%76.9%C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136131and the SNAP games resides in the fact that the protocols obtained for the SNAP gamesinvolve only free variables in the body of the rules representing them, whereas rule bodiesfor PSS are composed of ground atoms. Therefore, similarity between bodies in the latterwas trivially defined as equality. However, we had to use unification as similarity measurefor the SNAP games, which resulted in a poorer set of equivalence classes than thoseobtained for PSS. Further research should concentrate on the development of appropriatesimilarity measures in order to overcome this problem.5. DiscussionIn this section we discuss the main pitfalls faced during the development of the researchreported in the present paper. This discussion follows the order in which information isprocessed through the system, i.e., from attention mechanisms to symbolic learning ofprotocol behaviours.The attention mechanisms used have been robust for our scenario. Very occasionally anutterance may be missed if spoken very softly, and it works in environments in which thereis noise in the background. On the visual side, the attention mechanism based on motionfollowed by a number of static frames works 100% of the time unless objects are placed inthe scene very quickly (if motion lasts for less than 3 frames (< 0.5 seconds)).Once features have been extracted from interesting objects and sounds, continuousunsupervised methods are used to cluster the data items. It may appear that we are us-ing simple objects and few words; however, the main purpose of this work is to createa synthetic agent which integrates learning at both perceptual and task levels, with asfew (general) assumptions as possible. It would be trivial to classify more complex ob-jects than those used in this paper in a perfect way, using a supervised classifier andspecific feature descriptors or model based approach. However, classifying a number ofexamples into an unknown number of classes in an unsupervised manner is an open prob-lem.The ideas presented treat video and audio in largely the same way; i.e., as an input signalfrom which interesting perceptual objects are extracted and clustered. Presently, only oneclustering is used. We could form several clusterings and choose the one that works bestin the context, i.e., use symbolic reasoning to decide which clustering is best, based on theprotocols it can learn. We would also like to explore feeding back information from thesymbolic learning phase, in order to re-label the classes to which each perceptual objectbelongs based on the context of the learned protocol. This would allow classifiers to bere-learned using the new labels as supervision.The main limitation of the vision system presented is reliable colour classification. Weare using inexpensive U.S.B. webcams, which struggle to capture colour well enough toeasily distinguish different coloured objects. This has led to a large number of misclassifi-cations of the colours of objects. However, it has allowed us to investigate how the systembehaves in the presence of noise. In situations when examples are locally sparse (e.g., whenthere are few examples of a particular utterance), then PROGOL tends to over-generalise thedata, particularly if noisy examples are present. This is understandable, and generally PRO-GOL does learn rules of the right specificity from very few examples (in SNAP2 run 1 sound132C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136and complete rules are learned for the game where, on average, each utterance class hasonly four examples!).The use of PROGOL for inductive logic programming has allowed the formulationof protocol models from positive only examples. In summary, it has learned protocolsthat:• generalise temporal sequences of state observations;• generalise equality of perceptual classes of objects;• ground symbols to perceptual objects autonomously;• can be used by a synthetic agent to interact with its environment.Learning complex temporal dependency in the protocols is, perhaps, the next step ofdevelopment of this research. The present work assumes the successor8 predicate asthe only temporal relation of interest. This should be extended to include a range of tempo-ral relations, which would allow a greater range of scenarios to be captured. McCallum’swork on Nearest Sequence Memory [24] in instance-based state identification may be auseful way to proceed in order to capture variable length temporal dependencies betweenstates.The complexity of the symbolic learning task would be reduced if negative exampleswere supplied, constraining the search space. We have not wanted to incorporate supervi-sion into our framework, which would be necessary if we chose to learn from negative aswell as positive examples.The symbolic protocols learned were used to construct accurate sets of equivalenceclasses among over-clustered sets of utterance symbols. The criteria for deciding whichelements should be included in each class was based on user-defined similarity criteriabetween formulae bodies. The development of automatic procedures for choosing suchcriteria is subject of current research.5.1. Future workThere are many ways that this work may be extended. Outside of the game playingscenarios presented, it would be of more interest to generalise the approach to human be-haviour in traffic scenes and sporting activities, where the learning task is likely to be morechallenging. It may also be possible to learn from observation in industrial inspection tasksto accept or reject industrial parts based on unsupervised learning of perceptual categoriesand symbolic models learned using ILP. In all of these examples, particularly interestingpoints include dealing with:• higher (and variable) order temporal models;• when the actions that we wish to learn are not only grounded to both the present andprevious states, but also to the action at the previous time-step;8 This does not specifically preclude higher 1st order temporal models, but it makes their formulation unlikely.C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136133• scenarios involving a large number of rules;• stochastic actions, when a non-deterministic choice can be made;• incremental learning:◦ learning from all the information seen so far, and adapting the models as more ex-amples are seen;◦ multi-stage learning, where concepts such as orderings are incorporated in the formof background knowledge gained in a previous learning phase from perceptual ob-servation;• a richer language structure, within a similar framework.We would like to propose a game of greater ‘complexity’ than those presented in thispaper (but feasible to be solved by our approach), and discuss the adaptations that ourframework should incur in order to learn the protocols of this and other scenarios wheresimilar behaviours are present.The game of ‘play your cards right’, played on some TV shows, involves a pack ofplaying cards, a sequence of which are laid in a row face down. The first card is turnedover, and based on the value of this card the contestant has to say “higher” or “lower”.Then the next card is turned over, the contestant “loses” if they were wrong, or continuesin the game saying “higher” or “lower” until the final card, when if they correctly guess“higher” or “lower”, then they “win”.With the aim of learning the protocol of the game for an autonomous agent, this gameencompasses a number of interesting challenges:• the actions (utterances) that we wish to learn are not only grounded to both the presentand previous state, but also to the action at the previous time-step;• the number of rules is large; even if only five values of cards are used then many rulesare necessary;• stochastic actions exist; if the middle value card is turned over, then there is no singlecorrect protocol to follow—a stochastic choice of either “higher” or “lower” must bemade (in fact the action is strictly non-deterministic always).• incorporating concepts such as the ordering of the cards, in the form of backgroundknowledge from a previous learning phase from perceptual observation.This game may also seem to be simple, yet a number of background concepts areneeded to tackle it. Sensible approaches that we should consider, include the develop-ment of an incremental learning system allowing the agent to learn the protocol whileexperimenting with it. This learning system should also search for abstract formulaethat could encompass basic mathematical concepts about the domain to be fed backinto the learning process as background knowledge. Preliminary results in this direc-tion were reported in [36]. An engine for interpreting stochastic actions should alsobe developed to handle cases were multiple, distinct, actions are possible from thesame perceptual input. Research into the development of such an engine is well underway.134C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–1366. ConclusionCompletely unsupervised learning of rules of behaviour from observation is a crucialissue in the development of fully autonomous agents that are able to act appropriately inthe real world, including interacting with other individuals.We have presented a novel approach for learning protocol behaviours from perceptualobservation that incorporates computer vision, audio processing and symbolic learningmethods. The results obtained on learning the protocols of simple table-top games showthat this system is able to successfully learn perceptual classes and rules of behavioursfrom real world audio-visual data in an autonomous manner. Through selecting simplecomponents (e.g., for acoustic processing), we have been able to focus on the potentialfor constructive interaction between these components. For example, the ability to resolvefailings in sound categorisation through examining induced rule-sets was an unexpectedfinding that may not have surfaced had we used a more sophisticated acoustic processorfrom the start. Progressively scaling this approach to more complex games in order to reachthe level of interactions between any number of agents in any environment is a challengefor future investigations.AcknowledgementsThis work was funded by the European Union, as part of the CogVis project (ContractIST-2000-29375). We would like to thank Brandon Bennett and Aphrodite Galata for manyuseful discussions throughout the course of this research and also the anonymous reviewersfor the comments that helped improve this paper.References[1] S. Agarwal, D. Roth, Learning a sparse representation for object detection, in: Proc. European Conferenceon Computer Vision, vol. 4, 2002, pp. 113–127.[2] S. Aksoy, C. Tusk, K. Koperski, G. Marchisio, Scene modeling and image mining with a visual grammar,in: Frontiers of Remote Sensing Information Processing, World Scientific, Singapore, 2003, pp. 35–62.[3] J.F. Allen, Maintaining knowledge about temporal intervals, Comm. ACM 26 (11) (1983) 832–843.[4] N. Badler, Temporal scene analysis: Conceptual descriptions of object movements, PhD thesis, ComputerScience, University of Toronto, 1975.[5] K. Barnard, P. Duygulu, N. de Freitas, D.M. Blei, M.I. Jordan, Matching words and pictures, J. MachineLearning Res. 3 (2003) 1107–1135.[6] A. Cangelosi, D. Parisi, The processing of verbs and nouns in neural networks: Insights from synthetic brainimaging, Brain and Language 89 (2) (2004) 401–408.[7] R.A. Cole, J. Mariani, H. Uszkoreit, A. Zaenen, V. Zue, Survey of the State of the Art in Human LanguageTechnology, Cambridge University Press, Cambridge, 1996.[8] S. Coradeschi, A. Saffiotti, An introduction to the anchoring problem, Robotics and Autonomous Sys-tems 43 (2–3) (2003) 85–96.[9] J. Daugman, Uncertainty relation for resolution in space, spatiol frequency, and orientation optimised bytwo-dimensional visual cortical filters, J. Optical Soc. Amer. 2 (7) (1985) 1160–1169.[10] M.I. Sessa, F. Formato, G. Gerla, Similarity-based unification, Fund. Informaticae 40 (2000) 393–414.[11] A. Fern, R. Givan, J. Siskind, Specific-to-general learning for temporal events with application to learningevent definitions from video, J. Artificial Intelligence Res. 17 (2002) 379–449.C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136135[12] J.L. Flanagan, L.R. Rabiner (Eds.), Speech Synthesis, Dowden Hutchinson and Ross Inc., Stroudburg, 1973.[13] E. Forgy, Cluster analysis of multivariate data: Efficiency vs. interpretability of classifications, Biometrics 21(1965) 768.[14] S. Hargreaves-Heap, Y. Varoufakis, Game Theory, A Critical Introduction, Routledge, London, 1995.[15] S. Harnard, The symbol grounding problem, Phys. D 42 (1990) 335–346.[16] S. Hongeng, Unsupervised learning of multi-object event classes, in: Proc. 15th British Machine VisionConference (BMVC’04), London, UK, 2004, pp. 487–496.[17] Y. Ivanov, A. Bobick, Recognition of visual activities and interactions by stochastic parsing, IEEE Trans.Pattern Anal. Machine Intell. 22 (8) (2000) 852–872.[18] G. Karypis, V. Kumar, Multilevel graph partitioning schemes, in: Proc. International Conference on ParallelProcessing, vol. 3, 1995, pp. 113–122.[19] E. Keller, Fundamentals of Speech Synthesis and Speech Recognition: Basic Concepts, State-of-the-Art andFuture Challenges, Wiley, New York, 1994.[20] H. Kollnig, H.-H. Nagel, M. Otte, Association of motion verbs with vehicle movements extracted from denseoptical flow fields, in: Proc. of European Conference on Computer Vision, vol. 2, 1994, pp. 338–347.[21] N. Lavrac, S. Dzeroski, Inductive Logic Programming: Techniques and Applications, Ellis Horwood, NewYork, 1994.[22] D. Magee, C.J. Needham, P. Santos, A.G. Cohn, D.C. Hogg, Autonomous learning for a cognitive agentusing continuous models and inductive logic programming from audio-visual input, in: Proceedings of theAAAI Workshop on Anchoring Symbols to Sensor Data, 2004, in press.[23] D.R. Magee, Tracking multiple vehicles using foreground, background and motion models, Image VisionComput. 20 (8) (2004) 581–594.[24] A.R. McCallum, Hidden state and reinforcement learning with instance-based state identification, IEEETrans. Systems Man Cybernet. (Special issue on Robot Learning) 26 (3) (1996) 464–473.[25] D. Moore, I. Essa, Recognizing multitasked activities from video using stochastic context-free grammar, in:Proc. AAAI National Conf. on Artificial Intelligence, 2002.[26] S. Muggleton (Ed.), Inductive Logic Programming, Academic Press, New York, 1992.[27] S. Muggleton, Learning from positive data, in: S. Muggleton (Ed.), Proc. ILP96, in: Lecture Notes of Arti-ficial Intelligence, vol. 1314, Springer, Berlin, 1996, pp. 358–376.[28] S. Muggleton, J. Firth, CProgol4.4: A tutorial introduction, in: S. Dzeroski, N. Lavrac (Eds.), RelationalData Mining, Springer, Berlin, 2001, pp. 160–188.[29] H.-H. Nagel, From image sequences towards conceptual descriptions, Image Vision Comput. 6 (2) (1988)59–74.[30] B. Neumann, T. Weiss, Navigating through logic-based scene models for high-level scene interpretations,in: Proc. 3rd International Conference on Computer Vision Systems—ICVS 2003, Springer, Berlin, 2003,pp. 212–222.[31] C. Pinhanez, A. Bobick, Interval scripts: a programming paradigm for interactive environments and agents,Pervasive and Ubiquitous Computing 7 (1) (2003) 1–21.[32] L.R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proc.IEEE 77 (2) (1989) 257–286.[33] L. De Raedt, K. Kersting, Probabilistic inductive logic programming, in: Algorithmic Learning Theory, 15thInternational Conference, ALT 2004, Padova, Italy, 2004, pp 19–36.[34] D. Roy, Learning words and syntax for a visual description task, Computer Speech and Language 16 (3)(2002) 353–385.[35] D. Roy, A. Pentland, Learning words from sights and sounds: A computational model, Cognitive Sci. 26 (1)(2002) 113–146.[36] P. Santos, D. Magee, A. Cohn, D. Hogg, Combining multiple answers for learning mathematical structuresfrom visual observation, in: Proc. of the 16th European Conference on Artificial Intelligence (ECAI-04),Valencia, Spain, 2004, pp. 17–24.[37] H. Shin, C. Kim, A simple yet effective technique for partitioning, IEEE Trans. VLSI Syst. 1 (3) (1993)380–386.[38] J.M. Siskind, Grounding the lexical semantics of verbs in visual perception using force dynamics and eventlogic, J. Artificial Intelligence Res. 15 (2001) 31–90.[39] A.R. Smith, Color gamut transform pairs, Computer Graph. 12 (3) (1978) 12–19.136C.J. Needham et al. / Artificial Intelligence 167 (2005) 103–136[40] A. Strehl, J. Ghosh, Cluster ensembles—a knowledge reuse framework for combining multiple partitions,J. Machine Learning Res. 3 (2002) 583–617.[41] J. Tsotsos, J. Mylopoulos, H.D. Covvey, S.W. Zucker, A framework for visual motion understanding, IEEEPattern Anal. Machine Intell. (Special Issue on Computer Analysis of Time-Varying Imagery) (1980) 563–573.[42] S. Young, D. Kershaw, J. Odell, D. Ollason, V. Valtchev, P. Woodland, The HTK Book, Microsoft Corpora-tion, 2000.Further reading[43] T. Mitchell, Machine Learning, McGraw-Hill, London, 1997.