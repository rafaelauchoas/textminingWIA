Artificial Intelligence 174 (2010) 1254–1276Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintPractical performance models of algorithms in evolutionary programinduction and other domainsMario Graff∗, Riccardo PoliSchool of Computer Science and Electronic Engineering, University of Essex, Colchester, CO4 3SQ, UKa r t i c l ei n f oa b s t r a c tArticle history:Received 22 October 2008Received in revised form 18 July 2010Accepted 19 July 2010Available online 23 July 2010Keywords:Evolution algorithmsProgram inductionPerformance predictionAlgorithm taxonomiesAlgorithm selection problemEvolutionary computation techniques have seen a considerable popularity as problemsolving and optimisation tools in recent years. Theoreticians have developed a varietyof both exact and approximate models for evolutionary program induction algorithms.However, these models are often criticised for being only applicable to simplistic problemsor algorithms with unrealistic parameters. In this paper, we start rectifying this situation inrelation to what matters the most to practitioners and users of program induction systems:performance. That is, we introduce a simple and practical model for the performance ofprogram-induction algorithms. To test our approach, we consider two important classes ofproblems — symbolic regression and Boolean function induction — and we model differentversions of genetic programming, gene expression programming and stochastic iterated hillclimbing in program space. We illustrate the generality of our technique by also accuratelymodelling the performance of a training algorithm for artificial neural networks and twoheuristics for the off-line bin packing problem.We show that our models, besides performing accurate predictions, can help in the analysisand comparison of different algorithms and/or algorithms with different parameterssetting. We illustrate this via the automatic construction of a taxonomy for the stochasticprogram-induction algorithms considered in this study. The taxonomy reveals importantfeatures of these algorithms from the performance point of view, which are not detectedby ordinary experimentation.© 2010 Elsevier B.V. All rights reserved.1. IntroductionEvolutionary Algorithms (EAs) are popular forms of search and optimisation [1–6]. Their invention dates back manydecades (e.g., see [7]). So, one might imagine that, by now, we should have a full theoretical understanding of their opera-tions and a rich set of theoretically-sound guidelines for their parametrisation and customisation. However, this is not thecase.Despite the simplicity of EAs, sound theoretical models of EAs and precise mathematical results have been scarce andhard to obtain, often emerging many years after the proposal of the original algorithm (e.g., see [8–18]). A key reason for thisis that each algorithm, representation, set of genetic operators and, often, fitness function requires a different theoreticalmodel. In addition, the randomness, non-linearities and immense number of degrees of freedom present in a typical EAmake life very hard for theoreticians.This applies also to techniques for the automatic evolution of computer programs, or Evolutionary Program-inductionAlgorithms (EPAs), including Genetic Programming (GP) [6,15,19], Cartesian GP (CGP) [20], Grammatical Evolution (GE)* Corresponding author.E-mail addresses: mgraff@essex.ac.uk (M. Graff), rpoli@essex.ac.uk (R. Poli).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.07.005M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–12761255[21] and Gene Expression Programming (GEP) [22] among others. Our theoretical understanding of EPAs has been evenslower to develop than for other EAs chiefly because of the objective difficulty of modelling stochastic searchers in infinitespaces (programs have unbounded size) where search operators can dynamically change the dimension and structure of thesolutions being explored, as it is the case for most EPAs. So, despite recent successes in developing solid theory for GP andrelated EPAs (e.g., see [15,17,23] and the review in [19]), there is a growing gap between EPA theory and practice.Often theoretical studies and models of EAs are criticised for not being easily applicable to realistic situations (e.g., see[24]). One reason for this is that producing a comprehensive theory for complex adaptive systems such as EAs is objectivelyhard and slow, as we mentioned earlier. Another reason is that, sometimes, theoreticians focus on approaches and problemsthat are too distant from practice. So, despite the proven effectiveness of EAs and EPAs (see for example [19]), there is anurgent need for a theory that can clarify the applicability of different types of algorithms to particular problems, providedesign guidelines and, thereby, avoid the current, very time-consuming, practice of hand-tuning algorithms, parameters andoperators.This paper attempts to rectify this situation by proposing a practical model of EPAs. The model, by design, does notcapture all the characteristics of an algorithm nor models it exactly (which is extremely difficult). Instead, it focuses onwhat matters the most to practitioners, the performance of EAs in realistic problems, accepting the fact that, in practice,modelling performance cannot be done exactly. This model will allow us to give answers to questions such as: How likely isit that a particular algorithm will solve a particular problem of interest? What fitness should we expect to find at the end ofa run? What’s the best algorithm to solve a problem or a class of problems? Since no alternative model of EPA performanceis available at present, the only alternative is to seek answers to these questions by direct empirical experimentation!Although our approach was initially aimed at modelling EPAs, it can easily be extended beyond program induction bystochastic search to capture the characteristics of other forms of search and problem solving. To illustrate this we will alsomodel the performance of two heuristics for the off-line bin packing problem and one learning algorithm for feed-forwardneural networks.Our models are related to techniques used to solve the algorithm selection problem [25] (i.e., the problem of decidingwhich tool to choose to solve a problem out of a set of available tools) and, in particular, to the modelling techniques usedin algorithm portfolios [26–35] (i.e., collections of algorithms that are run in parallel or in sequence to solve a problem).The methodology presented here is complementary to (but competitive with) such approaches, as we will illustrate throughin the creation of effective portfolios of program induction algorithms: an area where no algorithm selection technique hadbeen tested before.Our models can also be used beyond the pure prediction of performance. For example, they enable the analysis of thesimilarities and differences between algorithms in relation to performance. To illustrate this, from a collection of models ofdifferent algorithms (or the same algorithms but with different parameters) we will obtain a meaningful and informativetaxonomy of evolutionary and stochastic program-induction algorithms, with a completely automatic process.The paper is organised as follows. In Section 2 we review related theoretical work in the field of EAs. In Section 3 wedescribe our performance model, how we arrived at it and the methodology used to instantiate it. Section 4 presents theproblems used to test the approach, while Section 5 describes the systems and parameter settings used in the experimen-tation. Experimental results that corroborate the validity of the models’ predictions are presented in Section 6. Section 7looks at the algorithm selection problem surveying relevant literature and applying our models to the creation of two algo-rithm portfolios for program-induction problems. The similarities and differences with other approaches are also discussed.Applications of our models in the comparison and categorisation of algorithms are discussed in Section 8. Some conclusionsand possible directions for future work are given in Section 9.2. Related workOur work is related to the problem of understanding what makes a problem easy or hard for EAs. Problem-difficultystudies in EAs focused initially on the building-block hypothesis for Genetic Algorithms (GAs) [2] and the related notion ofdeception [36]. The approach consisted in constructing artificial fitness functions that, based on certain a priori assumptions,would be easy or hard for GAs. This produced useful results but also some puzzling counter examples [37].The notion of fitness landscape, originally proposed in [38], underlies many recent approaches to problem difficulty. It isclear, for example, that a smooth landscape with a single optimum will be relatively easy to search for many algorithms,while a very rugged landscape, with many local optima, may be more problematic [39,40]. However, the graphical visuali-sation of fitness landscapes is rarely possible given the size of typical search spaces. So, one really needs to condense usefulinformation on fitness landscapes into one or a few numeric descriptors.In [41], Jones introduced one such descriptor of problem difficulty for GAs: the fitness distance correlation (fdc). The studyof fdc has been extended to GP [42–45]. These studies show that fdc is often a reliable indicator of problem hardness.However, it has one big flaw: it requires the optimal solution(s) to be known beforehand. This prevents the use of fdcto estimate problem difficulty in practical applications. A measure that does not suffer from this problem, the negativeslope coefficient (nsc), has recently been proposed [46]. This is based in the concept of fitness cloud (a scatter plot ofparent/offspring fitness pairs). The nsc uses the idea of first dividing the cloud into a certain number of bins along theparent-fitness axis, then computing the mean offspring fitness for each bin, and finally analysing the changes in slopebetween adjacent bins in the resulting histogram. The nsc has been shown to be a reliable measure in a number of different1256M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276benchmark problems in GP [46–48]. A slightly modified version of nsc called fitness proportional nsc has also given goodresults in GAs [49].While fdc and nsc (and other measures of problem difficulty) provide reasonable indications of whether a problem ishard or easy, they do not really give a direct estimation of how hard or easy a problem is in relation to any particularperformance measure, such as the success rate of an EA or the expected end-of-run fitness. To the best of our knowledge,no approach has ever been proposed that achieves this for EPAs, and this paper attempts to fill precisely this important gapin our knowledge. However, there are a small number of approaches (including one that has been a source of inspirationfor the work presented here) that have achieved a good degree of success at predicting actual performance in other areasof EAs. So, we briefly review them below.Precise bounds on the expected run time for a variety of EAs can be obtained by using computational complexity tech-niques [50–56]. Typically, this has been done only for specific classes of functions although there are some exceptions whererun-time bounds have been derived for more general combinatorial optimisation problems [57–62].At the opposite end of the generality spectrum is the No Free Lunch (NFL) theorem [63], which shows that averagedover all possible algorithms, all problems have the same difficulty, and, averaged over all possible problems, all algorithmshave the same efficiency. The implications and applicability of NFL have been clarified (e.g., see [64–67]). While NLF-typeresults are very important as they limit what can or cannot be achieved by a search algorithm performance-wise, theycan only provide indications of the performance of an algorithm only if the class of problems is closed under permutation[66]. However, we know that for function and program induction only very artificial problem classes are closed underpermutation [68–70].The situation is not much better for continuous optimisation, where we have a reasonably clear understanding of be-haviour and performance essentially only for Evolutionary Strategies [71] applied to particularly simple functions (e.g.,spheres and ridges). Markov chain models with continuous state spaces can be defined and general results have been ob-tained using them [72]. However, the complexity of the calculations involved makes them impractical for the analysis of theperformance of continuous optimisers. These can also be studied using discrete Markov chain models that can approximatethem on arbitrary continuous problems to any precision [73]. While this is promising, it is not clear how to extend thework in [72,73] to EPAs, given that they explore either a space of discrete structures or a hybrid discrete space with contin-uous subspaces (corresponding to real-valued numerical constants) embedded within it. In principle, it would be possibleto apply Markov chains to predict the performance of EPAs (without real-valued constants) using some recently developedmodels [18], but these models are immense, making their application to practical problems effectively impossible.The simple approach in [74], where the performance of a GA was modelled with surprisingly reliable results, has partic-ularly inspired the work presented in this paper. The idea there was that, when selection is based on comparing the fitnessof different solutions (as is often the case), the performance of an EA really only depends on relative fitness values. Thefitness function can, therefore, be re-represented using a comparison matrix. The matrix represents the outcomes of all thepossible comparisons between pairs of solutions that the selection mechanism might need to perform. So, the value of theelement (i, j) in the matrix is the sign of f (i) − f ( j), f being the fitness function. Because of the matrix is skew-symmetric,all the relevant information is stored in its upper triangle. By reading the elements of the upper triangle one by one, weobtain a vector v = (v 1, v 2, . . .), called an information landscape, that represents all the information the EA can ever needabout f to perform its search. Thus, the performance of an EA on a particular problem can only be a function of v. Clearly,this function is expected to be non-linear, quite complex, and practically impossible to derive from first principles for anyalgorithm of any complexity. However, [74] obtained good results by modelling the performance of a GA using the simplefunctionP (v) ≈ a0 +(cid:2)ai v i,(1)where ai are coefficients. These were found by applying the least squares method to a training set containing a sufficientlylarge set of (v, P (v)) tuples obtained by running the GA on a variety of problems and recording the associated performance.In principle, any comparison-based algorithm can be modelled in this way. Also, potentially users are free to chooseany performance measure. However, the technique does not scale well with the size, n, of the search space, the numberof elements in v being n(n−1). Thus, to uniquely identify the coefficients ai , one needs a training set containing at leastn(n−1)+ 1 problem/performance pairs. Also, because of the stochasticity of GAs, performance evaluation on a problem2typically requires gathering statistics over multiple runs. Thus, the construction of a training set suitable for the applicationof the method is practically impossible, except for very small search spaces.121 As one of the reviewers observed, if n is so small that enumerating n objects is feasible, the optimisation problem is not interesting since it can besolved in a trivial way. This implies that for very small problems, modelling the performance of algorithms other than the simplest, say random search orenumeration, is essentially an academic exercise, since complex algorithms are unlikely to be used in such trivial cases. However, when the search space islarge, as the search spaces we consider in this paper, performance models may become very important.M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–127612573. Modelling EPA performanceIn this section we describe our performance model and how we arrived at it. We start by considering the applicabilityof information landscapes to program spaces.3.1. Information landscapes for search in program spaces?Inspired by the information-landscape idea of [74] summarised in Section 2, we tried to model the performance of aGP system using the v vector re-representation of the fitness function and Eq. (1). Unfortunately, upon a detailed study, wefound that there are problems that limit the applicability of the original information landscape modelling technique to EPAs.The first issue is the scalability of the information-landscape approach, which in program-induction is particularly prob-lematic because of the relatively slow fitness evaluations associated with typical induction and regression problems. In fact,the information landscape technique could not even be applied to program induction because, at least in principle, the sizeof program search spaces is infinite. So, the required v vector would be infinitely dimensional, and so, an infinitely largetraining set would be needed to determine its associated coefficients ai . Of course, in practice there is always some upperbound on the size of the programs one is interested in exploring. Nonetheless, the problem remains serious because fortypical primitive sets, the number of distinct programs up to a certain size grows exponentially with the size and the num-ber of coefficients that need identifying grows like the square of that number. This makes it difficult to use the informationlandscape approach even for the smallest program spaces.√, +, ×}, the expressions p1 =The size of the training set is not the only problem. Another problem is that, in most primitive sets, there are symmetrieswhich imply that two syntactically different programs may, in fact, present the same functionality. For example, within thesearch space generated by {x, y,y × x are functionally indistinguish-able. If fitness is computed (as usual) based on the behaviour of programs, syntactically distinct programs with identicalbehaviours will always have identical fitness irrespective of the problem. This translates into constraints between elementsof the v vector re-representation of fitness. For example, if p3 = x and v j represents the comparison between the fitnessof programs p1 and p3 while vk represents the comparison between programs p2 and p3, it will always be the case thatv j ≡ vk. As a result any choice of the coefficients a j and ak such that a j + ak = constant produces a model of identicalquality. This makes the problem of identifying such coefficients via ordinary linear regression generally ill-posed [75] in thesense that it does not have a unique solution. More generally, the coefficients in Eq. (1) associated with elements in the vvector that remain identical across problems cannot univocally be determined.2x × y and p2 =√√All this suggests that the information landscape approach, as originally formulated, is unsuitable to model EPAs. Somemodification of the approach are thus necessary to overcome its limitations. Multiple alternatives are possible. In the nextsections we will focus on the direction we took in this work, which is based on a different re-representation of the fitnessfunction. We will also briefly discuss another possibility to make information landscapes viable for EPAs.3.2. A sparse representation of fitnessLet Ω be a program search space and f a fitness function over Ω . We assume that Ω is ordered. One can then representf using a corresponding ordered set F (Ω) = { f (p) | p ∈ Ω}. Clearly, F (Ω) is an exact representation of the fitness function.So, borrowing ideas from information landscapes, one could imagine estimating the performance of an EPA using the linearmodelP ( f ) ≈ a0 +(cid:2)p∈Ωap f (p).(2)Note that this model has far fewer parameters than Eq. (1). Nonetheless, their identification via ordinary linear regressionis problematic due to the size of typical program spaces and the indeterminacy resulting from the semantic equivalence ofdistinct programs. However, Eq. (2) can be transformed into a workable model if we accept to only partially represent thefitness function. That is, instead of using F (Ω) as a representation for f , we use F (S) = { f (p): p ∈ S}, where S ⊆ Ω . Thisleads to in the following modelP ( f ) ≈ a0 +(cid:2)p∈Sap f (p).(3)The advantage of Eq. (3) is that the cardinality of S is under our control. Therefore, we can easily ensure that there arenot too many coefficients to identify via regression, making the problem well-posed and keeping the size of the training setunder control. For these reasons we adopted Eq. (3) as a stepping stone towards our performance model.2 The problem is not simply caused by Eq. (1) being linear in the d(.,.) terms. For example, if one used the highly non-linear model P (v) ≈and≡ (a jak)v j which implies that any model where a j × ak = constant would have the same quality, making the choice of a jav iiwe had that v j ≡ vk , then aand ak non-unique.v jvkj ak(cid:3)1258M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276While, in principle, this approach could be applied to any domain, when the objective is to model EPAs, Eq. (3) can befurther specialised, thereby revealing a deeper structure that we will exploit for modelling purposes.In particular, it is common practice in EPAs to use a fitness function, f (p), which evaluates how similar the functionalityof a program p ∈ Ω is to a target functionality, t. Typically, t is represented via a finite set of training input–output pairs(cid:4)(cid:3)(called fitness cases in GP). So, fitness is generally computed as f (p) =i=1 g(p(xi), t(xi)) where {(xi, t(xi))} is a set offitness cases of cardinality (cid:3) (each xi being a set of inputs and t(xi) the corresponding desired output) and g is a functionwhich evaluates the degree to which the behaviour of p (i.e., its outputs) matches t in each fitness case.3 Typically, the setof fitness-case inputs, {xi}, is fixed, the corresponding outputs, i.e., t(xi) and p(xi), are numbers and g(a, b) = |a − b|k withk = 1 or k = 2.Under these conditions (see [68]) we can think of both t and p as being (cid:3)-dimensional vectors, t = (t1, . . . , t(cid:3)) andp = (p1, . . . , p(cid:3)), respectively, where ti = t(xi) and pi = p(xi). We can then represent f (p) = d(p, t) where d is a similaritymeasure between vectors. So, different t vectors induce different fitness measures on the program search space. Also, Eq. (3)transforms intoP (t) ≈ a0 +(cid:2)(cid:5)p(p), t(cid:6)ap · d(4)p∈Swhere we used P (t) instead of P ( f ) since fitness is really determined by t, and p(p) stands for the output produced byprogram p when tested on the fitness cases associated to a problem.Eq. (4) is specialised to model performance of program induction algorithms. In the following section, we will generaliseit to make it possible to model performance of both EPAs and searchers in other domains. Before we do this, however, weshould note that F (S) is only a sparse representation of f and, so, Eqs. (3) and (4) will necessarily be less accurate thanEq. (2). Of course, since the latter is itself an estimate, it makes sense to ask whether a careful choice of S might stillproduce reasonable results.4 We should also note that, as suggested by one of the reviewers, an alternative approach tomaking information landscapes viable for EPAs would be to sample them, i.e., to use only a subset of the components of vin Eq. (1).53.3. Performance models and problem landscapes(cid:2)In program induction, in principle, the search space may not contain a program with exactly the target functionality t.This acts as the second term of comparison in the factors d(p(p), t) in Eq. (4). It stands to reason that by also allowing thefirst term of comparison, p(p), to not be the functionality of a program actually in Ω we can generalise Eq. (4), obtainingP (t) ≈ a0 +ap · d(p, t)(5)p∈Swhere S is a subset of the set of all possible program behaviours (independently on whether or not there is a programin Ω implementing them). In particular, S ⊂ R(cid:3) for continuous regression problems and S ⊂ {0, 1}(cid:3) for Boolean inductionproblems.Note that, while we derived Eq. (5) for the case where p and t are (cid:3)-dimensional vectors, there is no reason why wecannot generalise it to objects of any type and structure as long as one can define a similarity measure d(p, t) for them.With this further generalisation, effectively Eq. (5) can also be applied to problems other than program induction. In particular, wecan now interpret S as a subset of the set of all possible problems, rather than a subset of the set of all possible programs(or, more precisely, program behaviours). For example, S could be a subset of all possible Boolean functions of 4 inputs aseasily as it could be a subset of bin packing problems. In this case the terms d(p, t) in Eq. (5) effectively define a problemlandscape, i.e., they represent the degree of similarity between two problems.6This interpretation of Eq. (5) is the model we will explore in the rest of the paper. The idea is general and, as wewill show, can successfully be applied to a variety of program-induction algorithms as well as other machine learners andproblem solvers. However, there is an important question that needs answering before we can proceed with using Eq. (5).Clearly, we should expect different choices of S to produce models of different accuracy. So, how should we choose theelements of S? The process we adopted in this work is described in the next section.3 This means that one is not free to associate any fitness to a program: fitness is always computed indirectly via some form of similarity measure. Thisconstrains the number and type of fitness functions one can create for EPAs.4 Naturally, selecting S is a delicate task that, if done inappropriately, could introduce undesired biases. As discussed in Section 3.4, here we havesuccessfully used a simple procedure that involves the sampling of Ω followed by a further selection step. Such procedures can be generalised to otherdomains. However, it is conceivable that in some cases the identification of a suitable S could be more difficult.5 We tested this idea in experiments (not reported for space limitations) with the class of 3-input Boolean-induction problems and all the different EPAsconsidered in this paper. The models produced were in all cases inferior to those obtained with sparse fitness landscapes. Nonetheless, results indicatedthat the approach is viable and worthy of further exploration and development in future research.6 In the case of program induction algorithms, problems and programs can be represented as structures (e.g., the vectors t and p, respectively) within thesame space whenever problems are described in terms of desired outputs to specific inputs. So, there is not a great deal of difference between Eqs. (4) and(5) nor between interpreting S as a set of program behaviours or as a set of problems. However, if our performance model is applied to general problemsolving situations, then it is important to remember the distinction between the two.M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–127612593.4. Model identificationTo instantiate Eq. (5) for EPAs and other algorithms we need a training set of problems T , a validation set V , a closenessmeasure d and a set of problems Σ from which to draw the elements of S. The sets T and S are used to identify the coeffi-cients ap so as to obtain a good fit. The validation set V is used to test the generality of the model. T and V are composedby pairs (t, P (t)) where t is a problem (e.g., for EPAs, a target vector) and P (t) is the corresponding performance of thealgorithm under study, which, for stochastic algorithms, is estimated by averaging performance over multiple independentruns.An important issue is the choice of Σ . For discrete domains of reasonably small size, one option is to use all the possibleelements in a class of problems. Alternatively, if the cardinality of a class of problems is too big or infinite, one couldconstruct Σ by drawing a representative set of samples from the class. In this work we decided to use the latter approach.Since typically also the training set T needs to be produced in via sampling, we decided to take Σ ≡ T . Section 4 describesthe simple procedures used to obtain T for different benchmark problems.We used the Least Angle Regression (LARS) algorithm (see [76]) as a method to decide which problems from T to includein S. LARS works as follows. It starts by setting all the coefficients ap to zero and finds the predictor most correlated with theresponse (i.e., the performance measure). Then it takes the largest possible step in the direction of this predictor until someother predictor has as much correlation with the residual. At this point, LARS proceeds in the direction of the equiangularbetween the two predictors until a third variable has as much correlation with the residual. The process continues until thelast predictor is incorporated into the model.7In our version of LARS we stop the algorithm after m steps, where m is the desired size for the set S, and we pick them problems from T chosen by the algorithm so far as the elements of S. In this way we are certain to retain in S elementsof T having a high correlation with the performance, thereby increasing the accuracy of the model over alternative ways ofchoosing S.To automate the choice of the parameter m, we wrapped the procedure just described within a 5-fold cross-validationloop. That is, T was split into five sets of equal size: four sets were used to produce a model while the remaining set wasused to assess its generalisation. The process was repeated 5 times, each time leaving out a different fifth of T , resulting in aprediction of performance for all problems in T . This procedure was iterated for m = 1, 2 . . . , |T | with the aim of identifyingthe value of m which provided the best generalisation. The overall generalisation error was measured via the Relativei(P i − ¯P )2, where i ranges over T , P i is the averagei(P i − ˜P i)2/Squared Error (RSE) [77] which is defined as RSE =performance recorded for problem i, ˜P i is the performance predicted by the model, and ¯P is the average performance overall problems.(cid:4)(cid:4)Having identified the optimal set S through the procedure described above, we finally used ordinary least squares toobtain the model’s coefficients ap.4. Test problems and performance measuresTo illustrate the scope and effectiveness of the approach, we considered different classes of problems as well as differentalgorithms and performance measures. Firstly, we tested our approach in problems related to EPAs, namely continuous sym-bolic regression of rational functions and Boolean inductions problems using different versions of GP, GEP, and a StochasticIterated Hill Climber (SIHC). Secondly, we modelled an artificial neural network (ANN) training algorithm applied to Booleaninduction problems. Finally, we also tested the approach on the one-dimensional off-line bin-packing problem modellingthe well-known First Fit Decreasing (FFD) and Best Fit Decreasing (BFD) heuristics. We present the problems in more detailin the rest of this section, while we describe the algorithms used to solve them in Section 5.4.1. EPA’s problemsFor program induction, we considered two radically different classes of problems, namely continuous symbolic regressionand Boolean function induction problems, and three typical performance measures: Best of Run Fitness (BRF), a normalisedversion of BRF (see below), and success rate. Continuous symbolic regression requires finding a program (seen as a functionthat transforms some numerical inputs into one output) which fits a set of data points. In Boolean function inductionproblems an algorithm is asked to find a program that implements a given truth table. In both cases the sum of absoluteerrors was used as a fitness measure.A benchmark set was created for continuous symbolic regression by generating 1100 different rational functions ofthe form t(x) = W (x)/Q (x), where W (x) and Q (x) are polynomials, using the following procedure: W (x) and Q (x) werebuilt by randomly choosing their degrees in the range 2 to 8, and then choosing random real coefficients in the interval[−10, 10] for the powers of x up to the chosen degrees. Each of the rational functions in the set was then sampled at 217 We decided to use LARS because it is less greedy than other model selection algorithms like forward selection or all-possible-subsets regression.Moreover, simple modifications of it implement Lasso or the forward stepwise linear regression algorithm, which we would like to test in future refinementsof this research.1260M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276points uniformly distributed in the interval [−1, 1], resulting in a target vector t ∈ R21 (the number of fitness cases andtheir range were chosen based on typical values used in the GP literature).For symbolic regression problems, for each t vector, we performed 100 independent runs recording BRFs. The corre-sponding P (t) value was obtained by averaging such BRFs. We also computed a normalised version of BRF (or NBRF forshort), which involved first normalising the target vector and the behaviour of the best individual found in each run, andthen summing the absolute differences between the components of these normalised vectors.8We also created a benchmark set of 1100 random problems from the class of 4-input Boolean induction problems. Eachproblem was represented by a vector of length 16 (a truth table). So, in this case t ∈ {0, 1}16. As a performance measure foran algorithm we took the success rate, i.e., the fraction of successful runs, in 100 independent runs.Each benchmark set of 1100 random problems was divided up into two sets: a training set T composed of 500 problems,and a validation set V comprising the remaining 600 problems.4.2. Artificial neural network problemsWe used Boolean induction problems also to test our approach when modelling a learning algorithm for ANNs. Thejob of the ANN was to learn functions with 3 inputs and 2 outputs. These were represented with 8 × 2 truth tables. Weselected 1100 such tables randomly without replacement: 500 went in the training set T , and the remaining 600 formedthe validation set V .For each problem in the benchmark set, an ANN was iteratively trained using the 8 possible different inputs patterns.Training continued until the mean square error on the outputs was less than 0.05. The training process was repeated 500times (starting with random sets of weights and biases). The average number of epochs required to train the network wasthen taken as the performance of the learning algorithm on the problem.The ANN domain differs from program induction in two important ways: (a) typically, ANNs have multiple outputs whileabove, for simplicity, we only considered the case of programs with one output; (b) the functionality of ANNs is determinedby their topology, weights, biases and activation functions, rather than a sequence of instructions. However, from the pointof view of modelling performance the two domains are very similar. Like for programs, also the behaviour of ANNs can berepresented via the collection of the outputs they produce in response to a set of input patters. Thus, we can linearise the8 × 2 table representing an ANN-training problem (by reading its elements column by column and row by row) therebyobtaining a vector t ∈ {0, 1}16 which we can directly use in Eq. (5) to model of the performance of ANN training algorithms.4.3. One-dimension off-line bin packingThe objective in bin packing is to pack a certain number of items using the smallest possible number of bins, such thatno bin’s content overflows. The main difference between this and the other domains described above is that, in bin packing,problems and algorithm outputs/behaviours do not live in the same space. In bin packing a solution to a problem is anassignment of items to bins, while a problem itself is a list detailing the size of each item to be packed. Also, typically thereis no predefined target behaviour, e.g., in the form of an assignment, which could then be used to measure similarity withthe output produced by an algorithm. This suggests that when modelling bin packers with Eq. (5) it is best to interpret theterms p and t as problems.The natural representation for bin-packing problems is a list of item sizes. However, in the off-line version of the problemconsidered here, the order of the items in the list is unimportant because the solver can freely choose the order in whichto bin them. For this reason, we represented problems using histograms indicating how many items of each size neededpacking. The number of slots in a histogram of item sizes is upper-bounded by the size of the largest possible item to bepacked. Since in our experiments item sizes were integers between 1 and 100, problems were represented with histogramsincluding 100 slots. Such histograms were then represented as 100-dimensional vectors.We created random bin-packing problems using the procedure presented in [78]. All problems required packing 1000items. We considered five problem classes with the following ranges of item sizes: 1–100, 10–90, 20–80, 30–70, and 40–60.For each range, 800 different histograms were created, from which 400 were used in the training set, and the remaining400 were included in the validation set. If B min and Bmax are the minimum and maximum size of the items in a problemclass, respectively, the histograms followed a multinomial distribution with success probabilities given by: pi =1Bmax−Bmin+1for i = Bmin, . . . , Bmax and pi = 0 otherwise. In total the training set consisted of 2000 different problems and the validationset included a further 2000 problems.The performance of a bin-packing algorithm was the number of bins used to pack all the items. Since we consideredonly deterministic algorithms, we performed one run with each problem.8 If μ and σ are the mean and standard deviation of the elements ti of t, respectively, the elements of the normalised target vector were set to ti −μσ .The normalised program-behaviour vectors were similarly obtained by shifting and scaling the components of p.M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–12761261Table 1Parameters and primitives used in the GP experiments. Only combinations of pxo and pm such thatpxo + pm (cid:2) 100% were used since crossover and mutation are mutually exclusive operators in GP.{+, −, ∗, / (protected to avoid divisions by 0)}{AND, OR, NAND, NOR}{x, R}{x1, x2, x3, x4}100%, 90%, 50%, and 0%100%, 50%, and 0%4Tournament (size 2) and roulette-wheel100050100Function set (rational problems)Function set (Boolean problems)Terminal set (rational problems)Terminal set (Boolean problems)Crossover rate pxoMutation rate pmMaximum tree depth used in mutationSelection mechanismPopulation sizeNumber of generationsNumber of independent runsTable 2Parameters used in the GEP experiments.Function set (rational problems)Function set (Boolean problems)Terminal set (rational problems)Terminal set (Boolean problems)Head lengthNumber of genesMutation rate pm1 point recombination rate2 point recombination rateGene recombination rateIS-transposition rateIS elements lengthRIS-transposition rateRIS elements lengthGene transposition rateSelection mechanismPopulation sizeNumber of generationsNumber of independent runs{+, −, ∗, / protected}{AND, OR, NAND, NOR}{x, R}{x1, x2, x3, x4}6335%20%50%10%10%1, 2, 310%1, 2, 310%Tournament (size 2) and roulette-wheel1000501005. Systems and parameter settings5.1. GP systemsWe used two different implementations of GP, both tree-based and both using subtree crossover and subtree mutation.One system was essentially identical to the one used by Koza [6], the only significant difference being that we selectedcrossover and mutation points uniformly at random, while [6] used a non-uniform distribution. The other system wasTinyGP [79] with the modifications presented in [19] to allow the evolution of constants. The main difference between thetwo is that Koza’s system is generational (all selected individuals in the population reproduce in parallel to create the nextgeneration) while TinyGP uses a steady-state strategy (offspring are immediately inserted in the population without waitingfor a full generation to be completed).Table 1 shows the parameters and primitives used. With the generational GP system, besides the traditional roulette-wheel selection (which gives parents a probability of being chosen proportional to their fitness), we also used tournamentselection (which sorts a random set of individuals based on fitness and picks the best). So, in total we tested three variantsof GP: generational with tournament selection, generational with roulette-wheel selection, and steady-state with tournamentselection.5.2. GEP systemsThe performance of GEP in preliminary runs was much worse than that of GP. We found that this was mainly dueto the standard GEP initialisation method. Thus, we replaced it with a technique equivalent to the ramped-half-and-halfmethod [6] obtaining a considerable performance improvement. We used three different versions of GEP: generationalwith tournament selection, generational with roulette-wheel selection, and steady-state with tournament selection. Theparameters and primitives used in our GEP runs are shown in Table 2.There are important differences between GP and GEP. Firstly, in GP, programs are represented using trees which can varyin size and shape, while GEP uses a fixed-size linear representation with two components: a head that can contain functionsand terminals, and a tail that can only contain terminals. Secondly, in tree-based GP, the operators act on trees, modifyingnodes and exchanging subtrees, while in GEP, the genetic operators are more similar to those used in GAs having only to1262M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276Table 3Parameters used in the SIHC experiments.MutationSub-treeUniformMaximum number of mutationsMaximum number of fitness evaluations50, 500, 1000, and 25,00025,00050,00050,000comply with the constraints on the primitives allowed in the head and the tail. Finally, in GP, no transformation is requiredbefore the fitness of a program tree can be computed, while in the GEP the fixed-length representation is transformed intoa tree structure first. This is done by picking primitives from the fixed-length string and inserting them into a tree followinga breadth-first order. These differences between GP and GEP translate into radically different behaviours and performanceof these two classes of EPAs.5.3. SIHC systemsWe used a Stochastic Iterated Hill Climber similar to the one presented in [80] but with different mutation operators:sub-tree mutation, which is the same type of mutation used in the GP runs, and a mutation operator, which we will calluniform mutation, where each node in the tree is mutated with a certain probability. A node to be mutated is replaced with:(a) one of its children, (b) a random node of the same arity, or (c) a randomly generated tree which includes also the treeoriginally rooted at the mutated node as a subtree.SIHC starts by creating a random program tree using the same procedure and primitives as for the GP and GEP ex-periments. SIHC then mutates this program until a fitter one is found. This replaces the initial program and the mutationprocess is resumed. When a maximum number of allowed mutations is reached the individual is set aside, a new randomindividual is created and the process is repeated. SIHC terminates when a solution has been found or when a maximumnumber of fitness evaluations is reached. Table 3 shows the parameters used for the SIHC experiments.5.4. ANN and bin packing heuristicsThe ANN we used to exercise our training algorithm was a fully connected feed-forward network with 3 layers and 7hidden neurons. The activation function was a symmetric sigmoid. The algorithm used to train it was iRPROP [81]. Theinitial weights and biases for the network were randomly and uniformly chosen in the range [−0.1, +0.1].For the case of bin packing, we used two well-known human-designed heuristics: First Fit Decreasing (FFD) and Best FitDecreasing (BFD). Both work by first sorting the items by non-increasing size. Then, FFD places each item in the first binwhere it can fit, while BFD places items where they fit most tightly. We used two different bin sizes: 100 and 150.96. Results6.1. Not all similarity measures are equal for EPA performance modellingEq. (5) was derived considering that in many GP problems the fitness function,f , is effectively a distance. However,having extended the interpretation of the model in Section 3.3, it is clear that we are free to choose a similarity measure,d, which does not coincide with f . It, therefore, makes sense to compare the quality of the models obtained with differentd measures.(cid:4)Table 4 shows how the 5-fold cross-validation RSE of the models varies across three GP systems for different d functions.For symbolic regression, RSEs were computed for the BRF and NBRF performance measures, while we used the successrate in Boolean induction. The quality of the models depends on d. For the case of rational problems, when using BRF thei ln(1 + |ti − pi|) was|ti − pi| was best overall, while when performance was evaluated using NBRF,closeness measure|ti − pi| a close second. For the case of Boolean induction problems, (p · t)2 provided the lowest RSE values.best withSimilar results were obtained with all program induction systems described in Section 5. So, in the rest of the paper, wewill use the optimal similarity measure identified above for each class of problems and performance measure, across allalgorithms.(cid:4)(cid:4)ii6.2. Performance models of EPAsTable 5 presents an accuracy comparison (in terms of RSE) for the performance models of the GP, GEP and SIHC systemspresented in Section 5. The lowest RSE values are associated with the generational systems with roulette-wheel selectionand the BRF measure. However, in virtually all cases RSEs are small for both training and validation sets and always wellbelow 1 (i.e., even the worst models are able to predict performance much better than the mean).9 When the size of the bins is 150 both heuristics gave the same performance so in the experiments we only present the results for FFD.M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–12761263Table 4Quality of the models of GP (with crossover rate of 90% and no mutation) for different closeness measures (BRF = best of run fitness, NBRF = normalisedBRF).ConfigurationTypeSelectionGenerationalRouletted(cid:7)1(cid:4)nGenerationalTournamentSteady stateTournamentRational functionsBoolean functionsBRF|S|2655511399123406736725110251100RSE0.02920.02150.11971.08621.00120.56210.32900.15770.52690.89311.00200.35700.51680.31480.90761.24061.00290.2949Normalised BRF|S|RSESuccess rate|S|25113148139012622715345138012531011352223841330.52610.49800.62511.00100.57800.48950.46790.36580.60781.00270.54080.35930.45670.45540.56400.99880.59540.4524399112616839913941118150399139911041283991RSE0.46291.01020.30070.37360.80451.01020.57881.00870.41580.48760.89931.00870.73031.01000.61170.68220.92451.0100(cid:4)i (ti − pi )2|ti − pi |i(t · p)2(t · p)3exp (−(cid:9)t − p(cid:9)2)(cid:4)i ln(1 + |ti − pi |)(cid:4)i (ti − pi )2(cid:7)1(cid:4)n|ti − pi |i(t · p)2(t · p)3exp (−(cid:9)t − p(cid:9)2)(cid:4)i ln(1 + |ti − pi |)(cid:4)i (ti − pi )2(cid:7)1(cid:4)n|ti − pi |i(t · p)2(t · p)3exp (−(cid:9)t − p(cid:9)2)(cid:4)i ln(1 + |ti − pi |)Table 5Quality of the model (RSE) for different GP, GEP and SIHC systems and parameter settings. The models used the optimal closeness measures identified inSection 6.1.ConfigurationTypeSelectionpxopmRational functionsBest of run fitness|S|T setGenerationalRouletteGenerationalTournamentSteady stateTournamentSys.SIHCMut.Sub-treeUnif.1.000.900.500.001.000.900.500.001.000.900.500.00GEPGEP0.000.000.501.000.000.000.501.000.000.000.501.00GEPMax Mut.50500100025,00025,00047554358180122340181129910611010011214|S|170150220193930.01230.00620.01930.01790.00050.00650.00030.15030.01070.00480.01930.01820.01810.01380.1787T set0.00540.00460.00260.00410.0192V set0.02090.02670.02430.03590.01010.26830.58570.82910.40090.12700.62240.43000.41680.35490.5608V set0.16740.40790.89740.25790.3437Normalised BRF|S|T set127126143128118137125160167129131133130132126|S|11411393841300.23040.22510.18320.19860.23750.16120.17940.13820.13520.18770.20670.20920.22860.23310.1845T set0.20910.19340.23610.25100.2044V set0.52460.53750.49990.49070.52120.40820.42570.41300.42910.44770.57780.56340.59670.63670.4243V set0.43490.45400.43780.45870.4890Boolean functionsSuccess rate|S|T set12812612712512911811711612112411610411410988|S|1181211221251180.16400.15100.15050.17120.19690.22160.25300.24130.27600.24240.35010.35310.37350.41590.3966T set0.22940.21090.20660.17860.2174V set0.28770.29620.28330.30580.37450.40650.39410.40100.46860.45010.54010.58200.63790.63360.5512V set0.40450.39890.37870.31200.3641RSE figures provide an objective indication of model quality. However, it may be difficult to appreciate the accuracy ofour models from just such figures. To give a clearer illustration of this, Fig. 1 shows scatter plots of the actual performance ofa GP system vs. the performance estimates obtained by the model in the training and validation sets for symbolic regressionand Boolean induction. The solid diagonal line in each plot represents the behaviour of a perfect model. The fact thatdata form tight clouds around that line is a clear qualitative indication of the accuracy of the models. Other systems andparameter settings provided similar results.1264M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276Fig. 1. Scatter plots of the performance measured vs. the performance obtained from the model for continuous regression problems using BRF (a & b) andnormalised BRF (c & d) as performance measures, and Boolean induction problems (e & f), for both the training set and the validation set. The data referto a GP system with 90% crossover rate, no mutation, and roulette-wheel selection.6.3. Performance models of ANN trainingTable 6 shows the quality of the models of ANN training resulting from the use of different similarities measures. Thetable reports the RSE obtained using cross validation (column 2) as well as the RSE obtained on the training and validationM. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–12761265Table 6Quality of the models of ANN training for different closeness measures.Closeness measure(cid:7)(cid:4)i (ti − pi )21(cid:4)n|ti − pi |i(t · p)2(t · p)3exp (−(cid:9)t − p(cid:9)2)(cid:4)i ln(1 + |ti − pi |)|S|39971341893997RSE0.51660.99540.37110.47980.82120.9954T RSE0.00110.94110.18140.12200.00160.9411Table 7Quality of the model for the different heuristics in the off-line bin packing problem.NameFFDFFDBFDSize of bins150100100|S|1315828855Cross-val. RSE0.00310.14050.1342T RSE0.00020.04140.0377V RSE0.48461.00040.35670.45020.79531.0004V RSE0.00280.14130.1361Fig. 2. Scatter plots of the actual performance (epochs) vs. the performance estimated by the model for ANN training problems.sets (columns 3 and 4, respectively). The d function with the best cross-validation RSE was (t · p)2 which also correspondsto the lowest RSE on the validation set. RSE values suggest that our model produced good predictions.Fig. 2 reports a scatter plot of the actual performance vs. the performance estimated by the model (when d = (t · p)2).This shows that the model was able to accurately predict actual performance for most problems. Only for problems wheretraining took longer than about 15 epochs, the model significantly underestimated performance. The reason of this is thatthere are only very few problems requiring a high number of learning epochs in our benchmark set.106.4. Performance models of bin packing algorithmsWe tested how the quality of models obtained with different closeness measures varied also for bin packing. We foundthat the sum of absolute differences produced models with lowest RSE value. Thus, we adopted this for the experimentsreported below.Table 7 presents the RSE values for the FFD and BFD heuristics for two bin sizes. In all cases our models predicted veryaccurately the performance of the two bin-packers and generalised very well. The size of S was considerably bigger than formodels of other systems, most likely because of the much larger number of degrees of freedom (100) associated with ourbin packing problems. We show scatter plots of the actual performance vs. the estimated performance in the validation setfor FFD in Fig. 3. Again, the data closely follow the perfect-model line.10 If accurate prediction of performance of rare cases (such as long runs) is important for a user, one can correspondingly bias the training set so as toensure that these are over-represented and, thus, more accurately modelled.1266M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276Fig. 3. Scatter plots of the actual performance vs. the estimated performance on the validation set for the FFD off-line bin packing algorithm with two binsizes.7. Performance models and the algorithm selection problemThe process of solving a problem often starts with looking at the different procedures available for the job and decidingwhich one to apply. In automated problem solving this is known as the algorithm selection problem [25]. Here we consider theapplicability of our models in the context of the algorithm selection problem. We are interested in this because techniquesto solve the problem rely on some form of prediction of algorithm performance, and, thus, they bear some similarities withour approach. However, as we will highlight below, there are also important differences.7.1. Algorithm selection problemMost approaches for solving the algorithm selection problem require the same set of ingredients [82]: (1) a large col-lection of problem instances of variable complexity; (2) a diverse set of algorithms, each having the best performance onsome such instances; (3) a measure to evaluate the performance of algorithms; and (4) a set of features that describe theproperties of the problems in the collection. These elements are necessary because most approaches use machine learningtechniques to predict which algorithm from the collection will have the best performance starting from a description of theproblem to be solved. Of course, for this to work, the features in (4) must be such that algorithms have similar performanceon problems with similar features.The methods used to solve the algorithm selection problem can be divided into two groups, depending on when thedecision on the strategy to use to solve a problem is made. In dynamic selection the decision is made during the run of analgorithm, while in static selection the decision is taken before the search starts.One algorithm that uses a prediction model to guide the search process dynamically is STAGE [83]. STAGE is effectivelya hill climber with intelligent restarts. After each hill climber search, STAGE uses the points sampled to produce or updatethe prediction model (linear regression) of the heuristic. The heuristic then suggests a promising point where to restart thehill climber. STAGE was successfully tested on SAT problems. Note that the models constructed during a run of STAGE canonly predict performance on the problem instance it is trying to solve. Following a similar idea, in [84] a prediction model(linear regression) was used to decide which path to follow in a search tree. The approach was tested on Knapsack problemsand set partitioning problems.In [85] the algorithm selection problem for SAT was modelled as a Markov decision process (MDP). The Davis–Putnam–Logemann–Loveland algorithm was enhanced with this model to decide, for each node of the search tree, which branchingrule to use next. Similarly, in [86] a Bayesian model was created to predict the run time of an algorithm and dynamicallyuse this prediction to decide when the search should be restarted. Also, as suggested in [87], when different recursivealgorithms are available to solve a problem, one can modify them in such a way that they can dynamically call each otherat each recursive call. Using a MDP to decide which function to call gave good results with sorting algorithms and with theorder statistic selection problem.Static-selection methodologies, instead, do not alter the course of an algorithm: they first decide which procedure to useand then wait until the chosen algorithm terminates. A substantial body of work falls in this category which attempts topredict the run-time of algorithms for matrix multiplication [88,89], sorting [90,88,89], solving partial differential equations[90] and signal processing [91]. The objective is choosing the algorithm’s most efficient implementation based on factorssuch as the computer’s architecture, the workload, and the instance size. Methodologies for predicting the run-time ofSAT solvers based on linear regression [29] and a combination of linear regression and a mixture-of-experts [33–35] haveM. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–12761267also been proposed. Techniques based on linear regression were also used in [26,27,32,31,30] to solve the auction winnerdetermination problem while case-based reasoning has been used in constraint programming [92–94].Algorithm portfolios [26–35] are a particularly important class of static algorithm selection techniques. An algorithmportfolio is a collection of algorithms that are run in parallel or in sequence to solve a particular problem. Given a newproblem instance, performance models are used to rank algorithms. The top ranking algorithms are then executed, stoppingas soon as a solution is found. Because in this way the algorithms used are good matches for the problem instance, theaverage performance of the portfolio over a set of instances is often better than the average performance of any algorithmin the portfolio. Performance models can also be used to identify sets of particularly hard problems for an algorithm [26,27,32].When attacking the algorithm selection problem, normally the features used to describe a problem within a performancemodel are obtained by an expert (perhaps via a careful analysis of the literature on a particular class of problems) and areproblem-specific. In [95], however, such features were obtained automatically by doing preliminary runs of the algorithmsin a collection on a problem instance. Solutions found at different times within runs of the algorithms were collected andthen used to form a feature set. Following a similar approach, in [96] preliminary runs were used to measure the expectedtime that the algorithm would spend in processing a node in a search tree. Using Knuth’s method for estimating the size ofa search tree, this made it possible to predict the run-time of different algorithms.7.2. Similarities and differences with our performance modelThe algorithm selection problem is approached using some form of machine learning technique to predict the perfor-mance of a collection of algorithms (and then match problems with algorithms), which is very similar with what we dowith our performance models. However, our methodology presents significant differences with respect to prior work onalgorithm selection.Firstly, we focus primarily on program induction and more specifically EPAs (although as we have seen our methodextends naturally to other domains). This is an area neglected by prior work on algorithm selection. Secondly, we charac-terise problems using features that are radically different from those used in prior work. This has mainly focused on theuse of problem-specific features that experts in the problem domain consider to be useful to measure the hardness of aproblem. These features work well, but their selection require considerable domain knowledge (and effectively relies onprevious attempts to characterise hardness). Also, the idea of computing features based on preliminary runs of the algo-rithms being modelled works well. However, this method produces models that lack generality having been derived for onespecific problem instance. Instead, in Eq. (5) our features simply measure how similar a problem is to a set of reference problems,automatically identified via LARS and cross-validation. Therefore, the features are generic rather than problem-specific andthe models of algorithms we obtain are applicable to whole classes of problems, not single instances. Thirdly, we do notjust use performance models to predict performance: we also elicit important knowledge on the similarities and differencesbetween algorithms from such models (as we will show in Section 8).7.3. Program-induction portfoliosDespite some good attempts (see Section 2), so far researchers have had relatively little success in the practical charac-terisation of the difficulty of program induction. This has effectively prevented the extension of the work on portfolios tosuch a domain. The good results obtained with our models, however, suggest that they might allow such an extension. We,thus, decided to attempt to develop algorithm portfolios for symbolic regression and Boolean induction.As suggested in [26], the algorithms forming a portfolio should behave differently on different problems. Also, eachalgorithm should beat all other algorithms in the portfolio on some problems. So, we decided to form portfolios using asubset of the 20 program induction algorithms considered in Section 6. To determine which algorithms to consider forinsertion in the portfolio we looked at performance on the problems in the training set and considered only the algorithmsthat had best performance in at least one problem. This resulted in the exclusion of 13 algorithms for symbolic regressionand 2 algorithms for Boolean induction.To decide which of the remaining algorithms to include in the portfolio, we used a cross-validation technique equivalentto the one used in Section 3.4. We started by creating a portfolio having only the algorithm that resulted best in the biggestnumber of problems in the training set. Then we added the algorithm that was overall second on the training set. Weused the predictions made in the cross-validation to decide which of the two algorithms should be used for each of theproblems in the training set, we simulated running the predicted best algorithm on each problem, and averaged the resultingperformance values to estimate the performance of the portfolio on the training set.11 We then added the third, fourth, etc.best algorithm to the portfolio, repeating the phases above until all the algorithms under consideration were included. Thiswhole procedure was repeated for each of the closeness measures, d, described in Table 4 plus the Sokal–Sneath similaritymeasure (which for steady-state GEP gave considerably better results than any other function). This allowed us to selectboth the portfolios and the d functions with the lowest RSEs.11 Since we had already run all algorithms on all problems to create our training set, this phase amounted to a simple look up operation.1268M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276Fig. 4. Performance of the algorithm portfolio (on the training set), when different criteria are used to select algorithms. The “performance model” usescross-validation on the training set to choose algorithms. The “perfect model” decides based on the performance measured in actual runs, and the “bestmodel” is the algorithm having the best measured average performance across all problems in the training set.Fig. 4 illustrates the performance of the algorithm portfolios constructed using this methodology. The “performancemodel” curves show the portfolio’s performance when cross-validation is used in conjunction with our model to decidewhich algorithm from a portfolio to use for each problem in the training set. The “perfect model” curves show the portfolio’sperformance that would be obtained if a perfect model of performance was available. The “best algorithm” curves presentthe performance of a portfolio when we always select the algorithm with the best average performance across the trainingset. For reference we also provide “mean of the portfolio” curves obtained by averaging the performance of the algorithmscomposing a portfolio, which indicate the performance we should expect if the choice of algorithm within a portfolio wasrandom.Since we have ordered algorithms based on their performance in the training set, it is not surprising to see that as weadd more and more algorithms to portfolios the average portfolio’s performance represented by the “mean of the portfolio”curves decreases. Looking at the “performance model” curves, in all cases the portfolios based on our model have betterperformance than the best algorithm. As we increase the number of algorithms, the portfolio’s performance rapidly peaksand then either remains stable (as for Boolean induction) or slightly decreases (as for symbolic regression).12 This may beslightly surprising: one might have expected that the more algorithms are available in a portfolio the higher the chancesof finding a good match for a problem and, thus, the better the performance. The reason why the “performance model”curves peak at some portfolio size is simply that the gains provided by a larger portfolio in terms of an increased ability tomatch algorithms to problem are offset by an increasing risk of making incorrect decisions due to there being more chancesof picking a sub-optimal algorithm. So, one really needs to compare the “performance model” and “mean of the portfolio”curves. Such a comparison reveals that the improvement in selection ability provided by our models is very significantirrespective of the size of the portfolio and, in fact, increases as the selection problem becomes harder.We tested the portfolios identified via cross-validation on the validation set to see if they generalised well. As shownin Fig. 5, the portfolio performance obtained by choosing algorithms using our models is only second to that achievableby a perfect model, which, of course, we cannot expect to ever match. However, for both symbolic regression and Booleaninduction problems, the pairwise differences in performance between selecting algorithms using our performance modelsand using the best algorithm in the portfolio are statistically significant, the one-sided, two-sample Kolmogorov–Smirnovtest reporting p values < 0.01.8. Eliciting knowledge from performance models8.1. Comparing algorithms and parameter settingsWhen considering different algorithms to solve a problem it is important to understand the similarities and differences inthe behaviour of such algorithms. It is reasonable to attempt to infer some such similarities and differences via a comparisonbetween performance models.12 The best performance for symbolic regression of rational functions was obtained when the portfolio is composed by just two algorithms (the TinyGPsystem with 100% mutation and the SIHC with 25,000 maximum mutations and sub-tree mutation) while, for Boolean induction, the best portfolio includedfour algorithms (the three TinyGP systems for which pxo + pm = 100% and the steady state GEP system).M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–12761269Fig. 5. Mean performance of our algorithm portfolios on the validation set. Error bars represent the standard error of the mean.(cid:10)(cid:10)p1(cid:10)p1, . . . , a, . . . , aWe start by noting that Eq. (5) represents a hyperplane as clarified by rewriting it in the normal form (−1, ap1 , . . . , ap|S| ) ·((P (t), d(p1, t), . . . , d(p|S|, t)) − x0) = 0, where ·is the scalar product, p1, . . . , p|S| are the elements of S and x0 =(a0, 0, . . . , 0) is a point on the hyperplane which depends only on the term a0. Then, one could measure the sim-(cid:10)p|S| ) and one represented by the vectorilarity between an algorithm characterised by the vector n(cid:10)(cid:10) = (−1, an(cid:10)(cid:10)p|S| ) by simply computing the angle between them, α = arccos( n(cid:10) = (−1, aUnfortunately, since the sets S are independently chosen in different models, in principle the i-th coefficient of onemodel’s hyperplane might be associated to a problem, while the i-th coefficient in another model’s hyperplane might beassociated to a different problem. To circumvent this problem we used the following slightly more sophisticated procedure.Let S(cid:10)be the sets of reference vectors associated with two performance models we want to compare. The models also(cid:10)(cid:10)include two corresponding sets of coefficients ap|S(cid:10)(cid:10)| , respectively. We construct two new sets of(cid:10)p|S(cid:10)(cid:10)| , is obtained by re-running linear regression on the training set associated(this is why the subscripts range from 1 to |S(cid:10)(cid:10)|); the second,(cid:10) =(cid:10)p|S(cid:10)| ), acoefficients: one, which we will call bwith the first model but using the reference vectors S(cid:10)(cid:10)(cid:10)(cid:10)bp1(−1, b(cid:10)(cid:10)(cid:10)p|S(cid:10)| , is obtained symmetrically. We then define the vectors a, . . . , b, . . . , b(cid:10)(cid:10) = (−1, aand b. Thus, we define the angle (or dissimilarity) between two algorithms as α = 12(cid:10)p1are not comparable, and so are b, we can compare(cid:9)a(cid:10)(cid:9)(cid:9)b(cid:10)(cid:10)(cid:9) ) +[arccos((cid:10)p|S(cid:10)(cid:10)| ) and bwith b(cid:10)(cid:10) = (−1, b(cid:10)(cid:10)(cid:10)p|S(cid:10)| ). While a, . . . , b(cid:10)p1(cid:10)p|S(cid:10)| and a(cid:10) = (−1, a(cid:10)(cid:10)p|S(cid:10)(cid:10)| ), b(cid:10)·n(cid:9)n(cid:10)(cid:9)(cid:9)n(cid:10)(cid:10)(cid:9) ).and S(cid:10)(cid:10), . . . , b, . . . , a, . . . , a, . . . , a, . . . , awith band a(cid:10)(cid:10)p1(cid:10)(cid:10)(cid:10)·ba(cid:10)p1(cid:10)(cid:10)p1(cid:10)p1(cid:10)(cid:10)p1a(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)(cid:10)and a(cid:10)(cid:10)(cid:10)·b(cid:9)a(cid:10)(cid:10)(cid:9)(cid:9)b(cid:10)(cid:9) )].aarccos(If the angle between two algorithms is small, the algorithms can reasonably be expected to produce similar performanceacross all problems. If one algorithm succeeds on a problem, the other will likely succeed and vice versa. So, one mightdecide to favour the faster algorithm. If the angle between two algorithms is big, then we can expect that at least on someproblems the performance of the two algorithms differs. Upon failure to solve a problem with one algorithm, one couldthen have some hope to solve it with the other.8.2. Toward automated taxonomiesIn the presence of more than two algorithms, we can build a matrix collecting the angles between all pairs of algo-rithms under consideration and infer useful information on their mutual relationships. However, when considering manyalgorithms, this comparison matrix is very large and manually finding interesting patterns in it may be difficult. Here wepropose a simple automated procedure which can aid and complement such a manual analysis. To exemplify the approach,we will focus on the 20 GP, GEP and SIHC systems presented in Section 5.We start by feeding the pair-wise comparison matrix into a clustering algorithm to group systems based on the similarityof their performance. More specifically, we adapted the hierarchical clustering algorithm of [97] to create the clusters. The al-j∈Y M(i, j)gorithm works as follows. Firstly, we chose the following similarity measure for pairs of clusters: s(X , Y) =,where M is the matrix containing the average similarity between all pairs of algorithms under study, | · | denotes the num-ber of elements in a cluster and X and Y are clusters. More specifically, M = 13 (MBRF + MNBRF + MB ) where the matricesMBRF , MNBRF and MB were obtained by performing pair-wise comparisons between our 20 program induction systemsfor each of our three different performance measures (using the best d functions associated to each). Then, we performed|X ||Y|i∈X(cid:4)(cid:4)1270M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276the following three steps: (1) each system was assigned to a separate cluster; (2) a new cluster was created by merging thetwo closest clusters based on s(X , Y), thereby reducing the number of clusters by one; (3) we repeated step (2) until therewas only one cluster left.This resulted in a cluster hierarchy with 20 levels. To simplify interpretation, we decided to focus only its 8 topmostclusters. To visualise such clusters we treated them as nodes in a fully-connected graph and we used the graph-drawingpackage neato, which is part of the GraphViz library, to obtain and draw a graph layout where pairs of nodes corre-sponding to clusters with high similarity were placed closer to each other than clusters corresponding to systems that weredissimilar performance-wise.The strategy used by neato to position the nodes of a graph is to interpret them as physical bodies connected bysprings (the edges in the graph). The user can set both the springs’ stiffness and their rest length. To produce a layout, the(virtual) physical system is initialised in some suboptimal configuration, which is then iteratively modified until the systemrelaxes into a state of minimal energy. In this work, we associated to each edge a length proportional to s(X , Y), so thatthe nodes would be pushed apart when producing the graph’s layout proportionally to their dissimilarity. We also set thestiffness of springs using the formula 1/(0.01 + s(X , Y)).Fig. 6 shows the output produced by neato. The edges between clusters in this figure are represented using dashedlines, while edges connecting systems to their mother cluster are drawn with solid lines. We can think of this diagramas a taxonomy of the systems under study. To distinguish between different forms of selection, reproduction and type ofmutation we used different symbols, as indicated at the bottom left of the figure.From the figure we can see how the steady state GP systems are grouped in a cluster (left of the figure). The generationalGP systems with tournament selection are arranged in another cluster (bottom left). At the top, we can find the clustercontaining all the SIHCs with sub-tree mutation. The generational GP systems with roulette-wheel selection are groupedaccording to whether crossover or mutation is the dominant operator. More specifically, at the bottom of the figure wefind a cluster containing the generational GP systems with no mutation. Just above it is a cluster with the generationalGP systems with 50% and 100% mutation. Finally, somehow surprisingly, each of the GEP systems was placed in a separatecluster (middle and right of the figure) indicating that these systems are very different performance-wise.Overall, our taxonomy suggests that the type of selection and reproduction used have a bigger impact on performancethan crossover and mutation rates. This is evident from the clusters formed by the steady state GP systems, the generationalGP systems with tournament selection, and the generational GP systems with roulette-wheel selection. The taxonomy alsosuggests that the reproduction strategy is very important in determining the behaviour of GEP systems. For SIHCs, thetaxonomy indicates that the type of mutation used is more important than the maximum number of mutations. Surprisingly,the taxonomy also suggests that the SIHC with uniform mutation is very similar to generational GP systems with tournamentselection, which is something one could hardly guess by looking at structural similarities between these algorithms.13Given that systems were grouped based on performance similarity, it is reasonable to expect that if a particular systemconsistently fails to solve a problem, it will be more efficient to try one or more alternative systems from a different cluster,rather than finely optimise the parameters of the first system. This should be done to further improve performance oncea satisfactory system is found. For the same reasons, perhaps in algorithm portfolios one should not just pick the best nalgorithms, but also look at how independent the performance of such algorithms is as portfolios with a good coverage ofthe performance space might be expected to generalise better.8.3. What knowledge can we extract from measuring performance empirically?In this section, we compare what we have learnt from analysing our performance models with what users of programinduction systems might be able to learn by using traditional approaches. These typically consist of computing some per-formance statistics on sets of test problems with the systems and parameter settings under comparison over a number ofindependent runs.We followed this approach to construct in Table 8, which reports the performance of the GP, GEP and SIHC systemsand parameter settings considered in this paper on the rational-function and the Boolean-function testbeds. Statistics werecollected by running each system on the 1100 different problems in the training and validation sets for each problem class.Performance was estimated by averaging results of 100 independent runs. This required a total of 4,400,000 runs.As can be seen from the table, SIHC with sub-tree mutation and a maximum of 25,000 mutations between restarts hasthe best performance on the rational functions problems irrespective of whether we used the BRF or the NBRF measures.TinyGP with 100% crossover has the best performance on Boolean induction problems. Also, there are large performancedifferences between roulette-wheel selection and tournament selection and between generational and steady state systems.These are statistically significant. For the generational GP system in the rational problems the mean and standard devia-tion of the performance measure decrease as the mutation rate increases, suggesting that there might be differences inbehaviour between the high-mutation and high-crossover search modes. However, all differences in performance observedwhen crossover and mutation rates are varied are not statistically significant.13 Whether these similarities and differences are present only in the classes of problems considered here and their origin is something that needs to beexplored in future research, possibly using alternative means.M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–12761271Fig. 6. Taxonomy of GP, GEP and SIHC systems with different parameter settings based on performance models.In other words, like the analysis based on our performance models, the data suggest that for the systems and problemsstudied, changing the selection mechanism has a bigger effect than varying the crossover and mutation rates. However,there are a variety of other phenomena that we were able to capture (see Section 8.2) that simple performance statisticscannot reveal.This does not mean, of course, that the information provided by our models cannot be corroborated empirically. It can,but this may require targeted empirical analyses. To illustrate this, in Table 9 we report the average Pearson’s correlationcoefficients obtained by comparing the performance results of pairs of program-induction algorithms on the same problemsand with the same performance measures used in Table 8. Careful inspection of the table confirms all the relationshipshighlighted by our taxonomy. This includes the unexpected finding that generational systems with tournament selection aremore similar to steady state systems with tournament selection than to generational systems with roulette-wheel selectionand that SIHC with uniform mutation is similar to generational GP systems with tournament selection and quite differentfrom the SIHC with sub-tree mutation. Why cannot this be inferred from Table 8? Simple: if one system does well on asubset of problems and not so well on another subset while another does the opposite, means and standard deviations ofperformance may not be able tell such systems apart.1272M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276Table 8Standard experimental results with the GP, GEP, and SIHC systems under study.ConfigurationRational functionsBoolean functionsTypeSelectionpxopmBest of run fitnessNormalised BRFSuccess rateGenerationalRouletteGenerationalTournamentSteady stateTournamentSys.SIHCMut.Sub-treeUnif.9. Conclusions1.000.900.500.001.000.900.500.001.000.900.500.00GEPGEP0.000.000.501.000.000.000.501.000.000.000.501.00GEPMax Mut.50500100025,00025,000Mean6.56356.61534.75024.27186.49792.58282.53412.35522.28644.16440.85760.87200.86820.88562.1178Mean1.24401.19431.07730.68161.4295Std. Dev.27.491228.017717.658015.327620.58525.84015.73555.48065.326710.77531.79701.78861.86661.93484.5666Std. Dev.2.57802.49022.43261.21053.2838Mean0.46580.46520.45160.44620.48160.40030.39770.39160.38780.45150.25350.25350.25110.24940.3580Mean0.24300.24190.23280.20210.2978Std. Dev.0.11460.11500.11760.11890.11090.11390.11410.11400.11260.11280.08600.08570.08170.08000.1101Std. Dev.0.07890.07850.07480.06860.0970Mean0.65540.65750.66930.68910.48690.81360.80940.81920.83270.49830.85180.84160.84550.84370.7894Mean0.40000.41430.42980.53260.7370Std. Dev.0.23740.23860.23260.22370.35730.16710.17200.16180.15040.37350.13330.13750.13290.13200.2445Std. Dev.0.21850.22650.23050.24600.2317We presented a set of techniques to build efficient and accurate models of the performance of problem solvers. Wemodelled three versions of GP with multiple parameter settings, three versions of GEP, two versions of SIHC (one withmultiple parameters settings), one ANN learning algorithm, and two bin-packing heuristics. These algorithms were appliedto the following problems: symbolic regression of rational functions, Boolean induction, and off-line bin packing.Many applications are possible for our models. They can be used to determine what is the best algorithm for a problem,as shown in Section 7 where we obtained algorithm portfolios for EPAs which had significantly better average performancethan the best overall algorithm in the each portfolio. As we showed in Section 8, they can also be used to reveal similaritiesand differences between algorithms across whole problem classes and to build highly-informative algorithm taxonomies.Our taxonomy of EPAs, for example, provided numerous new findings, including that the EPAs studied are little sensitive tothe choice of genetic operator rates, while reproduction and selection strategies influence performance significantly. Onlysome of this information can readily be provided by standard empirical analyses, although once interesting relationshipshave been identified through the use of our models and taxonomies, it is important to mine empirical results (or performad-hoc runs) to corroborate such relationships, as we did with Table 9.A difference between our models and other approaches to modelling EAs is that our models can simply be used toaccurately predict the performance of algorithms on unseen problems from the class from which the training set was drawn.Other techniques, such as the fitness distance correlation, fdc, and the negative slope coefficient, nsc, can only predict if aparticular problem is hard or easy, but not precisely how hard or easy (see Section 2). Also, our approach allows the userto choose the performance measure they want to model, while fdc and nsc don’t.The execution of our models involves an extremely low computational load. Of course, their instantiation requires run-ning a system, possibly multiple times, on a suitably large training set of problems. However, the cost of this is similarto that required to compare the performance of different algorithms empirically. The difference here is that once reliablemodels are constructed, they can be used over and over again to test performance on new problems, while empirical testingrequires effectively re-running all systems on each new problem to see how they behave.The main difference between our models and approaches used to solve in the algorithm selection problem is that ourmodels do not require the manual selection of sets of features to describe problems, while in other approaches these featuresare typically defined by an expert and are problem-specific. Instead, our models use the concept of closeness between theproblem for which performance is being estimated and some reference problems previously automatically selected (i.e., theset S). This makes it easy to apply our approach to different classes of problems.An important similarity between our models and those used in algorithm selection techniques is that they are linearfunctions of their features. One might wonder why such simple models work so well. Firstly, in both types of models thefeatures used are related to the hardness of the problem. In the case of our models, the d functions measure the similarity(e.g., the square of the dot product) or dissimilarity (e.g., the sum of absolute differences) between the input problem and aTable 9Pearson’s correlation coefficient between the performance results obtained in 1100 symbolic regression problems and 1100 Boolean induction problems averaged across three performance measures (BRF, NBRFand success rate).ConfigurationGenerationalRouletteGenerationalTournamentSteady stateTournamentSIHCTypeSelectionGenerationalRouletteTypeSelectionGenerationalTournamentTypeSelectionSteady stateTournamentpxo1.000.900.500.00GEPpxo1.000.900.500.00GEPpxo1.000.900.500.00GEPSys.SIHCMut.Sub-treeUnif.Max Mut.50500100025,00025,000pxo1.001.000.990.980.980.860.860.870.860.860.830.760.770.770.780.740.770.780.810.700.830.900.500.000.991.000.980.980.860.860.870.860.860.830.760.770.770.780.740.770.780.810.700.830.980.981.000.990.870.890.890.890.890.850.800.800.800.810.770.800.810.840.730.870.980.980.991.000.860.890.890.890.890.840.800.810.810.820.770.800.810.840.730.87GEP0.860.860.870.861.000.780.780.770.770.970.690.690.690.700.860.710.720.740.640.75pxo1.000.860.860.890.890.781.000.980.970.970.820.920.920.910.910.830.810.810.800.800.940.900.500.000.870.870.890.890.780.981.000.970.970.820.920.920.920.910.840.810.820.800.800.940.860.860.890.890.770.970.971.000.980.820.930.930.930.930.840.810.820.800.800.950.860.860.890.890.770.970.970.981.000.820.930.930.940.930.850.810.810.800.790.95GEP0.830.830.850.840.970.820.820.820.821.000.740.740.740.740.900.730.750.750.680.79pxo1.000.760.760.800.800.690.920.920.930.930.741.000.970.970.960.800.800.800.780.810.940.900.500.0050500100025,00025,000GEPSub-tree Mut.Unif.0.770.770.800.810.690.920.920.930.930.740.971.000.970.960.790.800.800.790.810.940.770.770.800.810.690.910.920.930.940.740.970.971.000.970.800.780.780.770.780.930.780.780.810.820.700.910.910.930.930.740.960.960.971.000.800.770.770.760.760.920.740.740.770.770.860.830.840.840.850.900.800.790.800.801.000.700.710.700.680.820.770.770.800.800.710.810.810.810.810.730.800.800.780.770.701.000.960.960.950.840.780.780.810.810.720.810.820.820.810.750.800.800.780.770.710.961.000.960.930.840.810.810.840.840.740.800.800.800.800.750.780.790.770.760.700.960.961.000.920.840.700.700.730.730.640.800.800.800.790.680.810.810.780.760.680.950.930.921.000.830.830.830.870.870.750.940.940.950.950.790.940.940.930.920.820.840.840.840.831.00M.Graff,R.Poli/ArtificialIntelligence174(2010)1254–127612731274M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276set of reference problems. So, by properly setting the magnitude and sign of the coefficients in Eq. (5), linear regression cancreate a scheme by which similarity with a difficult reference problem leads to reducing the performance estimate and viceversa. Secondly, while our (and other) models are linear in their features, such features are typically non-linear functionsof the degrees of freedom in the problem’s representation. The same architecture is used in some types of multi-layerperceptrons and radial-basis neural networks, which are powerful function approximators. So, it is not entirely surprisingthat our models can fit performance functions well.Finally, we would like to briefly discuss possible future research avenues. Our approach is generally able to make accuratepredictions on unseen problems from the class from which the training set was drawn. However, as we have seen in thecase of ANN learning in the presence of rare problems requiring long training times, if new problems are not sufficientlysimilar to any of the problems in the training set, model predictions can significantly deviate from actual performance. Inthis paper we have not studied the problem of outliers in depth. By their own nature, outliers are rare and, thus, obtainingstatistically meaningful results on model outliers would require an enormous computational effort. We expect outliers to begenerated via two mechanisms: either a problem is very different (as assessed by the similarity measure d used to build amodel) from all the problems in the reference set S and/or a problem falls in an region of the performance function whichpresents rapid changes (or discontinuities as in a phase transition) which cannot be well modelled with the simple kernelprovided by the d function. In future work we will investigate the possibility of detecting such cases to inform the user thatthe model might make incorrect predictions and/or to take counter measures.Sizing the population is a major step in all population-based algorithms. So, in future research we also intend to applyour models to look at how population sizes influence performance. Also, as shown in Section 6.1, some closeness measuresproduce better models than others. It is possible that there exist even better measures than the ones we settled for inthis paper. In future research we want to use GP itself to obtain even more predictive closeness measures. Furthermore, inthis paper we used angles to measure the similarity between the models of different algorithms. In the future, we wantto explore different ways of measuring such a similarity (e.g., via distances) since there is hope these may reveal evenfiner details. Also, introducing regularisation terms is a standard technique to transform ill-posed problems into well-posedp be minimised) to leastones. In future research we will explore the benefits of adding such terms (e.g., that the sumsquares. Finally, we want to explore whether there are some performance measures and problem classes for which theapproach is particularly suitable or unsuitable and why.p a2(cid:4)AcknowledgementsWe would like to thank the editor, associate editor and reviewers for their fair and useful comments and ideas. Thepaper has been considerably strengthened thanks to their feedback.The first author acknowledges support from the National Council of Science and Technology (CONACyT) of Mexico topursue graduate studies at the University of Essex.References[1] J.H. Holland, Adaptation in Natural and Artificial Systems, University of Michigan Press, Ann Arbor, USA, 1975.[2] D.E. Goldberg, Genetic Algorithms in Search, Optimization and Machine Learning, Addison–Wesley, 1989.[3] Z. Michalewicz, Genetic Algorithms + Data Structures = Evolution Programs, 2nd edition, Springer, Berlin, 1994.[4] M. Mitchell, An Introduction to Genetic Algorithms, MIT Press, Cambridge, MA, 1996.[5] T. Bäck, D.B. Fogel, Z. Michalewicz (Eds.), Evolutionary Computation 1: Basic Algorithms and Operators, Institute of Physics Publishing, 2000.[6] J.R. Koza, Genetic Programming: On the Programming of Computers by Natural Selection, MIT Press, Cambridge, MA, USA, 1992.[7] D.B. Fogel (Ed.), Evolutionary Computation. The Fossil Record. Selected Readings on the History of Evolutionary Computation, IEEE Press, 1998.[8] A.E. Nix, M.D. Vose, Modeling genetic algorithms with Markov chains, Annals of Mathematics and Artificial Intelligence 5 (1992) 79–88.[9] M.D. Vose, The Simple Genetic Algorithm: Foundations and Theory, MIT Press, Cambridge, MA, 1999.[10] T.E. Davis, J.C. Principe, A Markov chain framework for the simple genetic algorithm, Evolutionary Computation 1 (3) (1993) 269–288.[11] G. Rudolph, Convergence analysis of canonical genetic algorithm, IEEE Transactions on Neural Networks 5 (1) (1994) 96–101.[12] C.R. Stephens, H. Waelbroeck, Schemata evolution and building blocks, Evolutionary Computation 7 (2) (1999) 109–124.[13] J. He, X. Yao, Drift analysis and average time complexity of evolutionary algorithms, Artificial Intelligence 127 (1) (2001) 57–85.[14] R. Poli, Exact schema theory for genetic programming and variable-length genetic algorithms with one-point crossover, Genetic Programming andEvolvable Machines 2 (2) (2001) 123–163.[15] W.B. Langdon, R. Poli, Foundations of Genetic Programming, Springer, 2002.[16] J. He, X. Yao, Towards an analytic framework for analysing the computation time of evolutionary algorithms, Artificial Intelligence 145 (1–2) (2003)59–97.[17] R. Poli, N.F. McPhee, General schema theory for genetic programming with subtree-swapping crossover: Part II, Evolutionary Computation 11 (2)(2003) 169–206.[18] R. Poli, N.F. McPhee, J.E. Rowe, Exact schema theory and Markov chain models for genetic programming and variable-length genetic algorithms withhomologous crossover, Genetic Programming and Evolvable Machines 5 (1) (2004) 31–70.[19] R. Poli, W.B. Langdon, N.F. McPhee, A field guide to genetic programming, published via http://lulu.com and freely available at http://www.gp-field-guide.org.uk, 2008 (with contributions by J.R. Koza).[20] J.F. Miller, P. Thomson, Cartesian genetic programming, in: R. Poli, W. Banzhaf, W.B. Langdon, J.F. Miller, P. Nordin, T.C. Fogarty (Eds.), Proceedings ofthe Third European Conference on Genetic Programming (EuroGP-2000), in: LNCS, vol. 1802, Springer-Verlag, Edinburgh, 2000, pp. 121–132.[21] M. O’Neill, C. Ryan, Grammatical evolution, IEEE Transactions on Evolutionary Computation 5 (4) (2001) 349–358.[22] C. Ferreira, Gene expression programming: A new adaptive algorithm for solving problems, Complex Systems 13 (2) (2001) 87–129.[23] R. Poli, C.R. Stephens, The building block basis for genetic programming and variable-length genetic algorithms, International Journal of ComputationalIntelligence Research 1 (2) (2005) 183–197, invited paper.M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–12761275[24] R. Poli, Y. Borenstein, T. Jansen, Editorial for the special issue on: Evolutionary algorithms – bridging theory and practice, Evolutionary Computa-tion 15 (4) (2007) iii–v.[25] J.R. Rice, The algorithm selection problem, Advances in Computers 15 (1976) 65–118.[26] K. Leyton-Brown, E. Nudelman, Y. Shoham, Empirical hardness models: Methodology and a case study on combinatorial auctions, J. ACM 56 (4) (2009)1–52.[27] K. Leyton-Brown, E. Nudelman, Y. Shoham, Empirical hardness models for combinatorial auctions, in: P. Cramton, Y. Shoham, R. Steinberg (Eds.),Combinatorial Auctions, MIT Press, 2006, pp. 479–504 (Ch. 19).[28] F. Hutter, Y. Hamadi, H.H. Hoos, K. Leyton-Brown, Performance prediction and automated tuning of randomized and parametric algorithms, in: F. Ben-hamou (Ed.), CP, in: Lecture Notes in Computer Science, vol. 4204, Springer, 2006, pp. 213–228.[29] E. Nudelman, K. Leyton-Brown, H.H. Hoos, A. Devkar, Y. Shoham, Understanding random SAT: Beyond the clauses-to-variables ratio, in: Wallace [100],pp. 438–452.[30] K. Leyton-Brown, E. Nudelman, G. Andrew, J. McFadden, Y. Shoham, Boosting as a metaphor for algorithm design, in: F. Rossi (Ed.), CP, in: LectureNotes in Computer Science, vol. 2833, Springer, 2003, pp. 899–903.[31] K. Leyton-Brown, E. Nudelman, G. Andrew, J. McFadden, Y. Shoham, A portfolio approach to algorithm selection, in: G. Gottlob, T. Walsh (Eds.), IJCAI,Morgan Kaufmann, 2003, pp. 1542–1543.[32] K. Leyton-Brown, E. Nudelman, Y. Shoham, Learning the empirical hardness of optimization problems: The case of combinatorial auctions, in: P.V.Hentenryck (Ed.), CP, in: Lecture Notes in Computer Science, vol. 2470, Springer, 2002, pp. 556–572.[33] L. Xu, F. Hutter, H.H. Hoos, K. Leyton-Brown, SATzilla: portfolio-based algorithm selection for SAT, Journal of Artificial Intelligence Research 32 (2008)565–606.[34] L. Xu, F. Hutter, H.H. Hoos, K. Leyton-Brown, SATzilla-07: The design and analysis of an algorithm portfolio for SAT, in: Bessiere [101], pp. 712–727.[35] L. Xu, H.H. Hoos, K. Leyton-Brown, Hierarchical hardness models for SAT, in: Bessiere [101], pp. 696–711.[36] K. Deb, D.E. Goldberg, Analyzing deception in trap functions, in: L.D. Whitley (Ed.), Foundations of Genetic Algorithms Workshop, vol. 2, MorganKaufmann, 1993, pp. 93–108.[37] M. Mitchell, S. Forrest, J.H. Holland, The royal road for genetic algorithms: fitness landscapes and ga performance, in: F.J. Varela, P. Bourgine (Eds.),Toward a Practice of Autonomous Systems: Proceedings of the First European Conference on Artificial Life, The MIT Press, 1992, pp. 245–254.[38] S.J. Wright, The roles of mutation, inbreeding, crossbreeding and selection in evolution, in: D.F. Jones (Ed.), Proceedings of the Sixth InternationalCongress on Genetics, vol. 1, 1932, pp. 356–366.[39] J. Horn, D.E. Goldberg, Genetic algorithm difficulty and the modality of the fitness landscapes, in: L.D. Whitley, M.D. Vose (Eds.), Foundations ofGenetic Algorithms Workshop, vol. 3, Morgan Kaufmann, 1995, pp. 243–269.[40] S.A. Kauffman, S. Johnsen, Coevolution to the edge of chaos: Coupled fitness landscapes, poised states, and coevolutionary avalanches, Journal ofTheoretical Biology 149 (4) (1991) 467–505.[41] T. Jones, S. Forrest, Fitness distance correlation as a measure of problem difficulty for genetic algorithms, in: L.J. Eshelman (Ed.), ICGA, MorganKaufmann, 1995, pp. 184–192.[42] M. Clergue, P. Collard, M. Tomassini, L. Vanneschi, Fitness distance correlation and problem difficulty for genetic programming, in: W.B. Langdon,E. Cantú-Paz, K.E. Mathias, R. Roy, D. Davis, R. Poli, K. Balakrishnan, V. Honavar, G. Rudolph, J. Wegener, L. Bull, M.A. Potter, A.C. Schultz, J.F. Miller,E.K. Burke, N. Jonoska (Eds.), Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2002), Morgan Kaufmann, New York, USA,July 2002, pp. 724–732.[43] L. Vanneschi, M. Tomassini, P. Collard, M. Clergue, Fitness distance correlation in structural mutation genetic programming, in: C. Ryan, T. Soule,M. Keijzer, E.P.K. Tsang, R. Poli, E. Costa (Eds.), EuroGP, in: Lecture Notes in Computer Science, vol. 2610, Springer, 2003, pp. 455–464.[44] L. Vanneschi, M. Tomassini, M. Clergue, P. Collard, Difficulty of unimodal and multimodal landscapes in genetic programming, in: E. Cantú-Paz, J.A.Foster, K. Deb, L. Davis, R. Roy, U.-M. O’Reilly, H.-G. Beyer, R.K. Standish, G. Kendall, S.W. Wilson, M. Harman, J. Wegener, D. Dasgupta, M.A. Potter,A.C. Schultz, K.A. Dowsland, N. Jonoska, J.F. Miller (Eds.), Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2003), in:Lecture Notes in Computer Science, vol. 2723, Springer, Chicago, IL, USA, July 2003, pp. 1788–1799.[45] M. Tomassini, L. Vanneschi, P. Collard, M. Clergue, A study of fitness distance correlation as a difficulty measure in genetic programming, EvolutionaryComputation 13 (2) (2005) 213–239.[46] L. Vanneschi, M. Clergue, P. Collard, M. Tomassini, S. Vérel, Fitness clouds and problem hardness in genetic programming, in: K. Deb, R. Poli,W. Banzhaf, H.-G. Beyer, E.K. Burke, P.J. Darwen, D. Dasgupta, D. Floreano, J.A. Foster, M. Harman, O. Holland, P.L. Lanzi, L. Spector, A. Tettamanzi,D. Thierens, A.M. Tyrrell (Eds.), Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2004), in: Lecture Notes in ComputerScience, vol. 3103, Springer, Seattle, WA, USA, June 2004, pp. 690–701.[47] L. Vanneschi, M. Tomassini, P. Collard, M. Clergue, A survey of problem difficulty in genetic programming, in: S. Bandini, S. Manzoni (Eds.), Advancesin Artificial Intelligence: Proceedings of the Ninth Congress of the Italian Association for Artificial Intelligence (AI*IA-2005), in: Lecture Notes inComputer Science, vol. 3673, Springer, Milan, Italy, September 2005, pp. 66–77.[48] L. Vanneschi, M. Tomassini, P. Collard, S. Vérel, Negative slope coefficient: A measure to characterize genetic programming fitness landscapes, in:P. Collet, M. Tomassini, M. Ebner, S. Gustafson, A. Ekárt (Eds.), EuroGP, in: Lecture Notes in Computer Science, vol. 3905, Springer, 2006, pp. 178–189.[49] R. Poli, L. Vanneschi, Fitness-proportional negative slope coefficient as a hardness measure for genetic algorithms, in: Lipson [98], pp. 1335–1342.[50] S. Droste, T. Jansen, I. Wegener, A rigorous complexity analysis of the (1 + 1) evolutionary algorithm for separable functions with Boolean inputs,Evolutionary Computation 6 (2) (1998) 185–196.[51] S. Droste, T. Jansen, I. Wegener, On the analysis of the (1 + 1) evolutionary algorithm, Theoretical Computer Science 276 (1–2) (2002) 51–81.[52] I. Wegener, On the expected runtime and the success probability of evolutionary algorithms, in: U. Brandes, D. Wagner (Eds.), WG, in: Lecture Notesin Computer Science, vol. 1928, Springer, 2000, pp. 1–10.[53] T. Jansen, K.A.D. Jong, I. Wegener, On the choice of the offspring population size in evolutionary algorithms, Evolutionary Computation 13 (4) (2005)413–440.[54] C. Witt, Runtime analysis of the (mu + 1) ea on simple pseudo-boolean functions, Evolutionary Computation 14 (1) (2006) 65–86.[55] T. Jansen, I. Wegener, The analysis of evolutionary algorithms – a proof that crossover really can help, Algorithmica 34 (1) (2002) 47–66.[56] T. Jansen, I. Wegener, Real royal road functions – where crossover provably is essential, Discrete Applied Mathematics 149 (1–3) (2005) 111–125.[57] O. Giel, I. Wegener, Evolutionary algorithms and the maximum matching problem, in: H. Alt, M. Habib (Eds.), STACS, in: Lecture Notes in ComputerScience, vol. 2607, Springer, 2003, pp. 415–426.[58] F. Neumann, I. Wegener, Randomized local search, evolutionary algorithms, and the minimum spanning tree problem, Theoretical Computer Sci-ence 378 (1) (2007) 32–40.[59] J. Scharnow, K. Tinnefeld, I. Wegener, The analysis of evolutionary algorithms on sorting and shortest paths problems, Journal of MathematicalModelling and Algorithms 3 (4) (2004) 349–366.[60] S. Baswana, S. Biswas, B. Doerr, T. Friedrich, P.P. Kurur, F. Neumann, Computing single source shortest paths using single-objective fitness, in: Founda-tions of Genetic Algorithms Workshop, vol. 10, ACM, New York, NY, USA, 2009, pp. 59–66.[61] T. Storch, How randomized search heuristics find maximum cliques in planar graphs, in: M. Cattolico (Ed.), Proceedings of the Genetic and Evolution-ary Computation Conference (GECCO-2006), ACM, New York, NY, USA, 2006, pp. 567–574.1276M. Graff, R. Poli / Artificial Intelligence 174 (2010) 1254–1276[62] J. Reichel, M. Skutella, Evolutionary algorithms and matroid optimization problems, in: Lipson [98], pp. 947–954.[63] D.H. Wolpert, W.G. Macready, No free lunch theorems for optimization, IEEE Transactions on Evolutionary Computation 1 (1) (1997) 67–82.[64] T.M. English, Optimization is easy and learning is hard in the typical function, in: A. Zalzala, C. Fonseca, J.-H. Kim, A. Smith (Eds.), Proceedings of theCongress on Evolutionary Computation (CEC-2000), IEEE Press, July 2000, pp. 924–931.[65] T.M. English, Practical implications of new results in conservation of optimizer performance, in: M. Schoenauer, K. Deb, G. Rudolph, X. Yao, E. Lutton,J.J.M. Guervós, H.-P. Schwefel (Eds.), Proceedings of the Sixth International Conference on Parallel Problem Solving from Nature (PPSN VI), in: LectureNotes in Computer Science, vol. 1917, Springer, Paris, France, September 2000, pp. 69–78.[66] C. Schumacher, M.D. Vose, L.D. Whitley, The no free lunch and problem description length, in: L. Spector, E.D. Goodman, A. Wu, W.B. Langdon, H.-M.Voigt, M. Gen, S. Sen, M. Dorigo, S. Pezeshk, M.H. Garzon, E. Burke (Eds.), Proceedings of the Genetic and Evolutionary Computation Conference(GECCO-2001), Morgan Kaufmann, San Francisco, CA, USA, 2001, pp. 565–570.[67] C. Igel, M. Toussaint, On classes of functions for which no free lunch results hold, Information Processing Letters 86 (6) (2003) 317–321.[68] R. Poli, M. Graff, N.F. McPhee, Free lunches for function and program induction, in: Foundations of Genetic Algorithms Workshop, vol. 10, ACM, NewYork, NY, USA, 2009, pp. 183–194.[69] R. Poli, M. Graff, There is a free lunch for hyper-heuristics, genetic programming and computer scientists, in: L. Vanneschi, S. Gustafson, A. Moraglio,I.D. Falco, M. Ebner (Eds.), EUROGP-2009, in: Lecture Notes in Computer Science, vol. 5481, Springer, 2009, pp. 195–207.[70] R. Poli, M. Graff, Free lunches for neural network search, in: F. Rothlauf (Ed.), GECCO, ACM, 2009, pp. 1291–1298.[71] T. Bäck, Evolutionary Algorithms in Theory and Practice: Evolution Strategies, Evolutionary Programming, Genetic Algorithms, Oxford University Press,New York, 1996.[72] G. Rudolph, Convergence of evolutionary algorithms in general search spaces, in: Proceedings of IEEE Conference on International Conference onEvolutionary Computation (ICEC-1996), IEEE Press, Nagoya, Japan, May 1996, pp. 50–54.[73] R. Poli, W.B. Langdon, M. Clerc, C.R. Stephens, Continuous optimisation theory made easy? Finite-element models of evolutionary strategies, ge-netic algorithms and particle swarm optimizers, in: C.R. Stephens, M. Toussaint, L.D. Whitley, P.F. Stadler (Eds.), Foundations of Genetic AlgorithmsWorkshop, vol. 9, in: Lecture Notes in Computer Science, vol. 4436, Springer, Mexico City, Mexico, 2007, pp. 165–193.[74] Y. Borenstein, R. Poli, Information landscapes, in: H.-G. Beyer, U.-M. O’Reilly (Eds.), Proceedings of the Genetic and Evolutionary Computation Confer-ence (GECCO-2005), ACM, Washington DC, USA, June 2005, pp. 1515–1522.[75] J. Hadamard, Sur les problèmes aux dérivées partielles et leur signification physique, Princeton University Bulletin (1902) 49–52.[76] B. Efron, T. Hastie, I. Johnstone, R. Tibshirani, Least angle regression, Annals of Statistics 32 (2) (2004) 407–499.[77] E. Alpaydin, Introduction to Machine Learning, MIT Press, Cambridge, MA, USA, 2004.[78] R. Poli, J. Woodward, E.K. Burke, A histogram-matching approach to the evolution of bin-packing strategies, in: IEEE Congress on Evolutionary Com-putation, IEEE, 2007, pp. 3500–3507.[79] R. Poli, TinyGP, see Genetic and Evolutionary Computation Conference (GECCO-2004) competition at http://cswww.essex.ac.uk/staff/sml/gecco/TinyGP.html, June 2004.[80] U.-M. O’Reilly, F. Oppacher, Program search with a hierarchical variable length representation: Genetic programming, simulated annealing and hillclimbing, in: Y. Davidor, H.-P. Schwefel, R. Manner (Eds.), Proceedings of the Third International Conference on Parallel Problem Solving from Nature(PPSN VI), in: Lecture Notes in Computer Science, vol. 866, Springer-Verlag, Jerusalem, 1994, pp. 397–406.[81] C. Igel, M. Hüsken, Empirical evaluation of the improved Rprop learning algorithms, Neurocomputing 50 (2003) 105–123.[82] K.A. Smith-Miles, Cross-disciplinary perspectives on meta-learning for algorithm selection, ACM Comput. Surv. 41 (1) (2008) 1–25.[83] J.A. Boyan, A.W. Moore, Learning evaluation functions for global optimization and boolean satisfiability, in: Proceeding of the Fifteenth National Con-ference on Artificial Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference (AAAI-1998, IAAI-1998), AAAI Press, Madison,WI, USA, 1998, pp. 3–10.[84] O. Telelis, P. Stamatopoulos, Combinatorial optimization through statistical instance-based learning, in: Proceedings of the 13th IEEE InternationalConference on Tools with Artificial Intelligence (ICTAI 2001), IEEE Computer Society, Dallas, TX, USA, 2001, p. 203.[85] M.G. Lagoudakis, M.L. Littman, Learning to select branching rules in the dpll procedure for satisfiability, Electronic Notes in Discrete Mathematics 9(2001) 344–359.[86] E. Horvitz, Y. Ruan, C.P. Gomes, H.A. Kautz, B. Selman, D.M. Chickering, A Bayesian approach to tackling hard computational problems, in: J.S. Breese,D. Koller (Eds.), UAI, Morgan Kaufmann, 2001, pp. 235–244.[87] M.G. Lagoudakis, M.L. Littman, Algorithm selection using reinforcement learning, in: Langley [99], pp. 511–518.[88] R. Vuduc, J. Demmel, J. Bilmes, Statistical models for automatic performance tuning, in: V.N. Alexandrov, J. Dongarra, B.A. Juliano, R.S. Renner, C.J.K.Tan (Eds.), International Conference on Computational Science, vol. 1, in: Lecture Notes in Computer Science, vol. 2073, Springer, 2001, pp. 117–126.[89] N. Thomas, G. Tanase, O. Tkachyshyn, J. Perdue, N.M. Amato, L. Rauchwerger, A framework for adaptive algorithm selection in STAPL, in: K. Pingali,K.A. Yelick, A.S. Grimshaw (Eds.), PPOPP, ACM, 2005, pp. 277–288.[90] E.A. Brewer, High-level optimization via automated statistical modeling, in: Proceedings of the Fifth ACM SIGPLAN Symposium on Principles andPractice of Parallel Programming (PPOPP-1995), ACM Press, Santa Barbara, CA, USA, 1995, pp. 80–91.[91] B. Singer, M.M. Veloso, Learning to predict performance from formula modeling and training data, in: Langley [99], pp. 887–894.[92] C. Gebruers, B. Hnich, D.G. Bridge, E.C. Freuder, Using CBR to select solution strategies in constraint programming, in: H. Muñoz-Avila, F. Ricci (Eds.),ICCBR, in: Lecture Notes in Computer Science, vol. 3620, Springer, 2005, pp. 222–236.[93] C. Gebruers, A. Guerri, B. Hnich, M. Milano, Making choices using structure at the instance level within a case based reasoning framework, in: J.-C.Régin, M. Rueher (Eds.), CPAIOR, in: Lecture Notes in Computer Science, vol. 3011, Springer, 2004, pp. 380–386.[94] C. Gebruers, A. Guerri, Machine learning for portfolio selection using structure at the instance level, in: Wallace [100], p. 794.[95] T. Carchrae, J.C. Beck, Applying machine learning to low-knowledge control of optimization algorithms, Computational Intelligence 21 (4) (2005)372–387.[96] L. Lobjois, M. Lemaître, Branch and bound algorithm selection by performance prediction, in: Proceeding of the Fifteenth National Conference onArtificial Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference (AAAI-1998, IAAI-1998), AAAI Press, Madison, WI, USA,1998, pp. 353–358.[97] S. Johnson, Hierarchical clustering schemes, Psychometrika 32 (3) (1967) 241–254.[98] H. Lipson (Ed.), Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-2007), ACM, London, England, UK, July 2007.[99] P. Langley (Ed.), Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000), Stanford University, Stanford, CA, USA,June 29–July 2, 2000, Morgan Kaufmann, 2000.[100] M. Wallace (Ed.), Principles and Practice of Constraint Programming – CP 2004, 10th International Conference, CP 2004, Toronto, Canada, September27–October 1, 2004, Proceedings, Lecture Notes in Computer Science, vol. 3258, Springer, 2004.[101] C. Bessiere (Ed.), Principles and Practice of Constraint Programming – CP 2007, 13th International Conference, CP 2007, Providence, RI, USA, September23–27, 2007, Proceedings, Lecture Notes in Computer Science, vol. 4741, Springer, 2007.