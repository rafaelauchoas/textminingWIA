Artificial Intelligence 164 (2005) 1–22www.elsevier.com/locate/artintA formal analysis of why heuristic functions workB. John Oommen a,∗,1, Luis G. Rueda b,2a Senior Member, IEEE. School of Computer Science, Carleton University, 1125 Colonel By Dr.,Ottawa, ON, K1S 5B6, Canadab School of Computer Science, University of Windsor, 401 Sunset Ave., Windsor, ON, N9B 3P4, CanadaReceived 10 May 2001AbstractMany optimization problems in computer science have been proven to be NP-hard, and it is un-likely that polynomial-time algorithms that solve these problems exist unless P = NP. Alternatively,they are solved using heuristics algorithms, which provide a sub-optimal solution that, hopefully,is arbitrarily close to the optimal. Such problems are found in a wide range of applications, in-cluding artificial intelligence, game theory, graph partitioning, database query optimization, etc.Consider a heuristic algorithm, A. Suppose that A could invoke one of two possible heuristic func-tions. The question of determining which heuristic function is superior, has typically demanded ayes/no answer—one which is often substantiated by empirical evidence. In this paper, by using Pat-tern Classification Techniques (PCT), we propose a formal, rigorous theoretical model that providesa stochastic answer to this problem. We prove that given a heuristic algorithm, A, that could utilizeeither of two heuristic functions H1 or H2 used to find the solution to a particular problem, if theaccuracy of evaluating the cost of the optimal solution by using H1 is greater than the accuracy ofevaluating the cost using H2, then H1 has a higher probability than H2 of leading to the optimal solu-tion. This unproven conjecture has been the basis for designing numerous algorithms such as the A*algorithm, and its variants. Apart from formally proving the result, we also address the correspond-ing database query optimization problem that has been open for at least two decades. To validateour proofs, we report empirical results on database query optimization techniques involving a fewwell-known histogram estimation methods. 2005 Elsevier B.V. All rights reserved.* Corresponding author.E-mail addresses: oommen@scs.carleton.ca (B.J. Oommen), lrueda@scs.carleton.ca (L.G. Rueda).1 Partially supported by NSERC, the Natural Science and Engineering Research Council of Canada. Fellow ofthe IEEE.2 This work was partially supported by Departamento de Informática, Universidad Nacional de San Juan,Argentina, and by NSERC. Member of the IEEE.0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2002.02.0012B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–22Keywords: A* algorithms; Heuristic algorithms; Pattern recognition; Optimization1. Introduction1.1. OverviewThe area of computer science has still quite a few open, unsolved problems. In thispaper, we are concerned with one such problems, namely that of using heuristics to solveoptimization problems.Any arbitrary optimization problem3 is typically defined in terms of instances which aredrawn from a (finite) set, X , an objective function, and some feasibility functions. The aimis to find an (and hopefully, the unique) instance of X , which leads to the maximum (or theminimum) value of the objective function subject to the feasibility constraints. A formaldefinition of an optimization problem can be found in [10]. But to be more specific, con-sider the well-known Traveling Salesman Problem (TSP), in which the cities are numberedfrom 1 to n, and the salesman starts from city 1, visits every other city once, and returnsto city 1. An instance of X is a permutation of the cities, for example, 1 4 3 2 5, if weare considering a world consisting of five cities. The objective function for that instance,f (1 4 3 2 5) is obtained by performing the summation of the inter-city distances: 1 → 4,4 → 3, 3 → 2, 2 → 5, and 5 → 1. The optimal solution is the instance that minimizes thevalue of f .A heuristic algorithm is an algorithm that attempts to find a certain instance of X thatmaximizes f (or the profit) by iteratively invoking a heuristic function. The instance thatmaximizes f will be the optimal solution4 to the optimization problem. A heuristic is amethod that performs one or more modifications to a given solution or instance, in order toobtain a different solution which is either superior, or which leads to a superior solution.The heuristic, in turn, invokes a heuristic function, which estimates (or measures) the costof the solution at the particular state in the search process. This is the context in which weuse these terms.Many heuristic algorithms and heuristic functions have been reported in the literature,where the former include the alpha-beta search [11], backtracking, hill-climbing [10], sim-ulated annealing [1], genetic algorithms [13], tabu search [7], learning automata [15], etc.The issue of how heuristic functions are used in such heuristic algorithms in searching,game playing, etc., can be found in [16,24] and is, indeed, an enormous field of study initself. This question is not addressed here.To clarify issues, let us consider the classical n-puzzle problem [16]. This problem con-sists of a square board containing n square tiles and an empty position called the “blank”.The aim is to rearrange the tiles from some pre-defined (usually random) initial configura-tion into a pre-determined goal configuration, by sliding any tile adjacent to the blank into3 Every optimization problem can also be formulated as a decision problem [6].4 We use the term “solution” to refer to an element x ∈ X , and the term “profit” to refer to the value of f (x).In minimization problems, f (.) will be a cost function.B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–223the blank position. A heuristic algorithm solves this problem by examining, using a heuris-tic function, some of the possible valid movements. Viewed from the perspective of theunderlying state graph, the possible states encountered at the next level form the childrennodes of the current node in the search structure. Other variants of heuristic algorithmsinvolve the examination of lower levels as well. The breadth-first search and depth-firstsearch schemes are examples of heuristic algorithms, useful in any such problem solvingstrategy. An example of a heuristic function, however, is the measurement (or estimate) ofthe number of tiles that are out of place. Another measure is the sum of the depth of thenode and the number of tiles that are out of place.One of the better-known solutions to the n-puzzle problem is the A* algorithm. Thisalgorithm is a graph search algorithm that is used to find the path of minimum cost betweentwo nodes, the start node and the goal node. The A* maintains a tree which stores the pathsthat are already explored. Using these paths, a measure, f , of the potential advantage ofchoosing each path is calculated. The value of f , which is the cost of traversing the graphbetween two nodes, can be calculated by using different heuristic functions. A heuristic issaid to be admissible, and the A* converges to the correct result, if the heuristic function isan upper bound of the true cost from all nodes to the goal node.In general, for any arbitrary problem, the question of how useful a heuristic functionis, in determining the cost of traversing from one node to another, has no known analyticsolution—it has traditionally been empirically analyzed. In this paper, we present a formalanalysis that provides a stochastically positive answer to the question of comparing therelative advantages of potential heuristic functions.The A* algorithm and its variants (like the A+ algorithm) have also been success-fully applied to other problems, such as object recognition using deformable templates[16,26,28]. Various solutions to optimization problems using different heuristic functionsare found in [28]; we shall use this paper, [28], to highlight the difference between theheuristic algorithms, and the effect of the same algorithm using various potential heuristicfunctions. The authors of [28] address the problem of tracking roads in satellite imagesusing the twenty-question search paradigm, and the A+ algorithm, a “cousin” of the A*algorithm. Using these algorithms the roads can be represented in terms of straight-linesegments. The various paths are expanded by the application of an ensemble of heuristicfunctions. One such heuristic function is the one based on the conditional entropy mea-surements of the branches, which are used to choose the most “promising” path. Whilethe paper discusses other heuristic functions, the question of how one can compare thesolutions obtained using the various heuristic functions is achieved by comparing the em-pirical simulation results. We hope that our formal analysis can be a tool to achieve a morerigorous comparison of these heuristic functions in [28], and other similar scenarios.5The tools we propose to use are drawn from the well-established theory of PatternRecognition (PR) [5,27]—a prominent field of machine intelligence. Broadly speaking, PRinvolves decision-making, based on a priori and learned knowledge of the classes and ob-jects being recognized. More specifically, the system learns information about the features5 The model presented here has some limitations when investigating the quality of solutions yielded by anA*-like algorithm. These limitations will be discussed in a later sub-section.4B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–22of a set of classes. Subsequently, given an object of unknown identity, and this informa-tion, the system attempts to recognize the unknown object as belonging to one of the knownclasses with some arbitrary accuracy. Necessarily, our overview of PR is brief!There are many applications of PR, including face and speech recognition, fingerprintidentification, character recognition, medical diagnosis, etc. In each of these applications,the information about the classes can be structural or statistical. In the former, we dealwith the field of structural and syntactic pattern recognition, and in the latter, with the fieldof statistical pattern recognition. Furthermore, in the latter, the statistical information, orfeatures, about the classes is represented by random vectors. The procedure of obtainingthe features consists of mapping the feature values of each sample to a vector. Featurevalues, for example, can be the width or the height of a figure, the value of a pixel of animage, etc. Statistical pattern recognition can also be subdivided into two well-defined ap-proaches, parametric and non-parametric. In the former, the random vectors have a knownprobability distribution, e.g., normal (or Gaussian), exponential, multinomial, etc. No suchmodel is assumed in a non-parametric case.Although we are aware of the use of PR principles in real life scenarios, we are notaware of any previous results in which PR principles have been used to solve a theoreticalunsolved problem in a completely different field.Our result can be crystallized as follows: Given two heuristic functions, the questionof determining which is superior, has typically demanded a yes/no answer which is of-ten substantiated based on empirical evidence. We have solved the problem of decidingon the superior heuristic function by using PR techniques. It should be mentioned thatthere are numerous well-known techniques that have been utilized in the context of patternclassification, such as hypothesis testing, bootstrap methods, Neyman–Pearson methods,etc. A good reference for such methods can be found in [21]. However, the results derivedin this paper essentially use the methods that have been traditionally applied to optimalBayesian Classification, as described in the statistical pattern recognition literature [4].Using these principles, we prove the following assertion: Given two heuristic functions,H1 and H2, used by a heuristic algorithm in finding a solution to a particular problem, ifthe accuracy in obtaining the optimal solution by using H1 is greater than that of using H2,then H1 has a higher probability of leading to the optimal solution than H2. To the bestof our knowledge, this is an open problem. However, this unproven conjecture has beenthe basis for designing numerous algorithms such as the A* algorithm, and its variants, insearching, game playing, and numerous other applications [16,24,25,28].Our strategy for achieving this analysis is as follows. The first task is to model thecost of the solution. Since the optimal “true” cost is unknown, we represent it in termsof its estimate, as estimated using the heuristic function. Observe that since the latter isinaccurate, this “cost” is represented in terms of a random variable. Note that by “cost”, wedo not mean the cost of the search process involved in determining the optimal solution,but rather the cost of the optimal solution, as estimated by the heuristic function. Thisdifference is crucial.Now that the modelling of the heuristic function is in place, the question of quantifyingthe quality of any heuristic function has to be considered. Informally speaking, we cansay that this paper concerns this “heuristic-function quality assessment” problem, whichis addressed, in turn, by viewing it as a pattern recognition problem. We solve this pat-B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–225tern recognition problem by considering two independent random variables, the first forthe optimal solution and the second for the sub-optimal, both of them being pursued by aheuristic function, H1. We use a reasonable model for the accuracy of the heuristic func-tion, in which the error of H1 is a doubly-exponential random variable.6 This distribution,which as we shall presently see, is used to approximate the Gaussian distribution, is typ-ically used in reliability and failure models, and hence is reasonable in this scenario. Inour model, the accuracy of the heuristic function is related to the variance of the randomvariable used to represent it. The analysis for the Gaussian distribution follows thereafter.If we now consider another heuristic function, H2, whose variance is greater than thatof H1, and whose mean is the same as that of H1, we have a model by which the efficiencyof heuristic functions can be compared. Indeed, using this model, we have theoreticallyproven that H1 is more likely to succeed in obtaining the optimal solution than H2. Forthis model, we have also proved the uniqueness of the result, and the conditions for whichboth heuristic functions lead to coincident probabilities of success.The doubly exponential distribution is actually meant to be an approximation of theGaussian distribution, typically used to model errors. However, the algebraic analysis forGaussian distributions is impossible as there is no closed-form expression for integrating itsprobability density function. Consequently, we have extended the analysis for the doublyexponential distribution to formulate a reasonable analysis for the Gaussian distributionusing numerical integration. By means of this analysis, we have corroborated the validityof our hypothesis for Gaussian distributions also.We also provide empirical results on using a few histogram-like estimation methods indatabase query optimization, which demonstrate the validity of our theoretical analysis.1.2. ApplicationsThere are many heuristic algorithms that can be used to solve a wide variety of NP-hardproblems. Such problems can be found in a wide range of applications spanning the wholespectrum of artificial intelligence, and include game playing and game theory, graph the-ory, database query optimization, networking, computational geometry, number theoreticproblems, parallel processing, etc. The results presented in this paper are applicable to anyheuristic algorithm that uses different heuristic functions to solve a particular problem. Inthis introductory section, we just describe a few of them.In the area of database query optimization, when more than two tables have to be joined,intermediate join operations are performed to ultimately obtain the final relation. As a re-sult, the same query can be performed by means of different intermediate (join) operations.A simple sequence of join operations that leads to the same final result is called a queryevaluation plan (QEP). Each QEP has associated an internal cost, which depends on thenumber of operations performed in the intermediate joins. The problem of choosing thebest QEP is a combinatorially explosive optimization problem. This problem is currently6 The reasoning used in this paper assumes that the errors are on either side of the true value. However, webelieve that if the distribution is one-sided, similar arguments will be true as long as the distribution is not“heavily-tailed”. We are grateful to the anonymous referee who brought this to our attention.6B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–22solved by estimating the query result sizes of the intermediate relations and selecting themost efficient QEP.Since the analysis of selecting the best QEP must be done in “real” time, it is not pos-sible to inspect the real data in this phase. Consequently, query result sizes are usuallyestimated using statistical information about the structures and the data maintained in thedatabase catalogue. This information is used to approximate the distribution of the attributevalues in a particular relation. Hence the problem of selecting the best QEP depends on howwell that distribution is approximated.In [8], it has been shown that errors in query result size estimates may increase exponen-tially with the number of joins. Since current databases and the associated queries increasein complexity, numerous efforts have being made to devise more efficient techniques thatsolve the query optimization problem.Many techniques have been proposed to estimate query result sizes, including his-tograms, sampling, and parametric techniques [9,12,14,22]. Histograms are the most com-monly used form of statistical information. They are incorporated in most of the commer-cial database systems such as Oracle, Microsoft SQL Server, Teradata, and DB2, whichmainly use the Equi-depth histogram. The prominent models of histograms known in theliterature are: Equi-width [2,9], Equi-depth [14,22], the Rectangular Attribute Cardinal-ity Map (R-ACM) [18], the Trapezoidal Attribute Cardinality Map (T-ACM) [19], and theV-Optimal Histograms [8,23].In this scenario, the heuristic algorithm is the actual algorithm that uses a histogramas the heuristic function, and obtains an optimal (or a sub-optimal) QEP. The heuristicfunction used by this algorithm is the actual histogram that approximates the distribution ofthe attribute values of the relevant tables. Thus, in our model (and using our terminology),Equi-width, Equi-depth, the R-ACM and the T-ACM are the heuristic functions.Other areas in which our model can be used to answer open questions are in the fields ofgame theory and game playing [25]. In game playing, the most widely used structure usedto analyze the best possible move and strategy is a game tree, whose root node representsthe initial status of the board. All possible moves of the first player are the edges from theroot to the first level, the edges of each child represent all possible moves of the secondplayer, the opponent. Continuing in the same fashion, the game is played (or rather plansexecuted) until one of the players wins. The aim is to optimize the moves of the first playerbased on searching all the branches of the tree until the leaves, and perform the best movebased on maximizing the reward of the first player and minimizing that of the second one.There are many techniques used to optimize the moves of the first the player. One ofthem is the minimax search algorithm, which searches over a fixed number of levels of theentire tree, and finds the best moves at each node. This exhaustive search procedure has acomplexity that grows exponentially with the number of nodes of the tree. A more efficientmechanism is the alpha-beta search algorithm [11], a heuristic that significantly reducesthe number of nodes explored. Both of these assume that the heuristic function that theyuse, which typically evaluates the position of the board viewed from the perspective of thefirst player, is advantageous in determining a superior strategy. This is the question thatwe address in this paper. The model presented in this paper has important consequences inchoosing such a heuristic function. Such a heuristic function could be, for example, the costof a path from the current state to a goal state, which unfortunately is not exactly known,B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–227but is estimated. The search scheme, such as the alpha-beta search and the minimax searchalgorithm, uses this heuristic function to search for a, hopefully, optimal path in the gametree.Another application of our result is in graph theory, for example, in solving the uniformgraph partitioning problem. Given a complete graph on 2n vertices, G = (V, E), alongwith a cost function f : E → Z+ ∪ {0}, the aim is to find a partition whose sum of costs ofthe individual subsets is minimized. This problem is also known to be NP-hard, and hasseveral applications especially in VLSI design, hydrology, networks, etc. Many heuristicalgorithms have been proposed to solve this problem, including simulated annealing, ge-netic algorithms, learning automata, etc. [10,20]. When considering a particular heuristicalgorithm, we can incorporate different heuristic functions to approximate the sum of costsof the individual subsets of a particular partitioning. It is intuitive that a more accurateheuristic function is more likely to succeed in finding the optimal solution. However, thisis not what happens in all cases. We rather provide a stochastic answer to this question. Bymeans of a rigorous theoretical analysis, we prove that a particular heuristic function, whichprovides more accurate approximations for the sum of costs of the individual subsets, ismore likely to obtain the minimal cost for a partitioning, than a less accurate heuristicfunction.1.3. Problem statementIn this paper, we propose a theoretical model that solves this fundamental open problemin computer science, namely that of relating heuristic functions with solution optimality,using the principles of the theory of pattern classification. This problem has been (to ourknowledge) open. In particular, the corresponding database query optimization problemhas been unsolved for more than two decades.More specifically, we prove the following: Given a heuristic algorithm, A, that invokestwo heuristic functions, H1 and H2, used in a decision problem, if the accuracy in approx-imating the optimal solution by using H1 is greater than that of using H2, then H1 has ahigher probability of leading to the optimal solution than H2.The importance of the results of this paper is that we show that the answer to the accu-racy/optimality question is “stochastically positive”. In other words, we prove that althougha superior heuristic function may not always yield a better solution, the probability that thesuperior heuristic function yields an optimal solution exceeds the probability that an in-ferior heuristic function yields an optimal solution. This paper thus justifies and gives aformal rigorous basis for why heuristic functions work.We analytically prove that under the well-acclaimed models of inaccuracy, the betterthe accuracy of a heuristic function, the greater the probability of it choosing the optimalsolution. We have also provided some empirical results related to the field of databasequery optimization. These results show the superiority of the R-ACM over the traditionalhistogram estimation methods, the Equi-width and the Equi-depth. The empirical resultsobtained by testing these properties for many of the above histogram methods in randomdatabases show that the R-ACM is significantly superior to both the Equi-width and theEqui-depth schemes.8B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–221.4. Restrictions of our modelAs mentioned above, this paper addresses the problem of quantifying the quality of aheuristic function, and it achieves this by posing the problem in a fairly general framework.However, for the results to be applicable for a particular application domain7 which uses aspecific search strategy such as the A∗ algorithm, the logistics of the search process itselfhave to be considered.Informally speaking, the main result of our paper proves the following: Given twoheuristic functions evaluating the same “cost”, a search mechanism utilizing these func-tions will converge (with a higher probability) to a superior solution, when it utilizes afunction with a lesser variance. However, comparing the performance of heuristic func-tions in the search process initiated by A∗ is a more complicated issue. The reason for thiscan be argued as follows. In each iteration, A∗ computes the values of the heuristic function(say, “f (.)”) for all candidate nodes (the OPEN list), which represent how promising theyare. A∗ then selects the one with the highest value of f (.), generates its children, computestheir values of f (.), and inserts them into the OPEN list. For an algorithm like A∗, the mostwe can claim is that it is more expedient to use a heuristic function which better estimatesthe “cost” than one which estimates it poorly. The question of how the nodes in the OPENlist lead to solutions, is really a problem-dependent question which we cannot answer here.We intend to study this problem in the database query optimization domain mentioned later,by incorporating a search strategy to search the set of QEPs whose costs are estimated bythe various histogram methods. Note that this does not invalidate the query-optimizationresults presented in this paper, because, in our simulations, we exhaustively search the QEPspace without using any intelligent search strategy like A∗.2. Heuristic function accuracy vs. optimalityConsider a heuristic algorithm, A, that invokes either of two heuristic functions, H1and H2. The probability of correctly estimating a cost value of a particular solution by H1and that of estimating a cost value by H2 are represented by two independent randomvariables. In our model, we assume that these two heuristic functions are independent, andthus, the value obtained by one heuristic function should not affect the value obtained bythe second.For the analysis done below, we work with two models for the error function: the doublyexponential distribution and the normal distribution. In the former, the probability of ob-taining a value that deviates from the mean (or true value) falls exponentially as a functionof the deviation. The exponential distribution is more typical in reliability analysis and infailure models, and in this particular domain, the question is one of evaluating how reli-able the quality of a solution is, if only an estimate of its performance is available. Moreimportantly, it is used as an approximation to the Gaussian distribution for reasons whichwill be clarified momentarily. The Gaussian model is much more difficult to analyze, since7 We are grateful to the anonymous referee who brought this limitation to our attention.B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–229there is no closed-form algebraic expression for integrating the probability density func-tion. However, a formal computational proof is included, which confirms our hypothesis.2.1. Analysis using exponential distributionsA random variable, X, is said to be doubly exponentially distributed with parameter λif the density function is given by:fX(x) = 12λe−λ|x−c|, −∞ < x < ∞.(1)If X is a doubly exponential random variable, by elementary integration and straight-forward algebraic steps, it can be shown that:E[X] = c,Var[X] = 2λ2and.(2)(3)Without loss of generality, if the mean of the cost of the optimal solution is c1, byshifting the origin by c1, we can work with the assumption that the cost of the best solutionis 0, which is the mean of these two random variables. The cost of the second best solutionis given by another two random variables (one for H1 and the other one for H2) whosemean, c2 > 0, is the same for both variables. An example will help to clarify this.Example 1. Suppose that using H1 leads to the optimal cost with a probability representedby a doubly exponential random variable, X(opt), whose mean is 0 and λ1 = 0.4. Thisheuristic function also leads to another sub-optimal cost according to X(subopt)whose meanis 8 and λ1 = 0.4.11H2 is another heuristic function using which the optimal cost is chosen with a prob-whose parameters are c1 = 0 and λ2 = 0.2. It leadswhoseability distribution given by X(opt)to the second sub-optimal cost value with a probability density given by X(subopt)parameters are c2 = 8 and λ2 = 0.2.The fact that 2/λ22 signifies that the probability of using H1 could lead to a sub-optimal cost is smaller than the probability of using H2 leading to a sub-optimal cost. Thisscenario is depicted in Fig. 1, and is formalized presently.1 < 2/λ222The result depicted above is formalized in the following theorem, which is the first pri-mary result of this paper, and answers the open question referred to above. The theorem isformulated in terms of the probabilities that the two heuristic functions lead to the wrongdecision, which we show is inherently related to the probability that these heuristic func-tions lead to the convergence to the sub-optimal solutions. The formulation of the resultand the proof utilize techniques typically foreign to database theory, game theory, artificialintelligence, or for that matter any computer science area in which this approach can beapplied. They belong to the theory of PR.The second theorem, extends the results of the first, and shows how the results can alsobe geometrically interpreted.10B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–22Fig. 1. An example of doubly exponential distributions for the random variables X(opt)X(subopt), whose parameters are λ1 = 0.4 and λ2 = 0.2.12, X(opt)2, X(subopt)1andTheorem 1. Suppose that A is a heuristic algorithm that can potentially utilize either oftwo heuristic functions, H1 and H2. Let:• X1 and X2 be two doubly exponential random variables that represent the estimated• X(cid:6)2 be two other doubly exponential random variables representing the esti-costs of the optimal solutions obtained by using H1 and H2 respectively.1 and X(cid:6)mated costs of non-optimal solutions obtained by using H1 and H2 respectively.] = c.• 0 = E[X1] = E[X2] (cid:1) E[X(cid:6)1• p1 and p2 be the probabilities that H1 and H2 respectively lead to the wrong decision.] = E[X(cid:6)2Then,if Var[X1] = Var[X(cid:6)1] = 2λ21(cid:1) 2λ22= Var[X2] = Var[X(cid:6)2], p1 (cid:1) p2.Proof. Consider a particular cost, x. The probability that x leads to a wrong decisionwhen A uses H1 is that of incorrectly classifying x as being obtained from the non-optimalsolution. This is, indeed, the error in classification, and is the area under the curve of thepdf function of X(cid:6)1 or the cumulative probability of x under the pdf of H1 when it refers tothe sub-optimal solution. Because of the discontinuity of the doubly exponential functionat c, this area is decomposed into the following two integrals:x(cid:1)I11 =−∞c(cid:1)I12 =−∞1212λ1eλ1(u−c) du,if x (cid:1) c,andλ1eλ1(u−c) du +x(cid:1)c12−λ1(u−c) du if x > c.λ1e(4)B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–2211Solving the integrals, (4) results in:I11 = 12I12 = limu→−∞eλ1(x−c) − limu→−∞−λ1(−u+c) + 121212ee−λ1(u−c) = 12−λ1(x−c),e− 12e−λ1(x−c) + 12and= 1 − 12−λ1(x−c).(5)eThe probability that using H1 leads to the wrong decision for all the values of x is thefollowing function of λ1 and c:0(cid:1)p1 = I (λ1, c) =−∞I1112λ1eλ1x dx +c(cid:1)0I1112−λ1x dx +λ1e∞(cid:1)cI1212−λ1x dx,λ1e(6)which, after applying the distributive law and substituting the values of I11 and I12, can bewritten as:0(cid:1)−∞λ14e2λ1x−λ1c dx −c(cid:1)0λ14−λ1c dx +e(cid:2)∞(cid:1)cλ12e−λ1x − λ14e(cid:3)−2λ1x+λ1cdx.(7)After solving the integrals, (7) is transformed into:−λ1c + 1−λ1c + 348−λ1c = 12Similarly, we do the same analysis for p2, which is a function of λ2 and c:−λ1c + 14−λ1c.λ1ceλ1ce18eeep2 = I (λ2, c) = 12e−λ2c + 14λ2ce−λ2c.We have to prove that:(8)(9)ep1 = 12−λ1c + 14−λ2c + 14Multiplying both sides by 2, and substituting λ1c for α1 and λ2c for α2, (10) can be−λ1c (cid:1) 12−λ2c = p2.λ2ceλ1ce(10)ewritten as follows:−α1 + 12e−α2 + 12Substituting α2 for kα1, α1 (cid:2) 0 and 0 < k (cid:1) 1, (11) results in:−α1 (cid:1) e−α2.α2eα1eq1 = e−α1 + 12α1e−α1 (cid:1) e−kα1 + 12kα1e−kα1 = q2.(11)(12)We now prove that q1 − q2 (cid:1) 0. After applying natural logarithm to both sides of (12)and some algebraic manipulations, q1 − q2 (cid:1) 0 implies:1 + 12F (α1, k) = kα1 − α1 + ln1 + 12− lnα1(cid:5)(cid:4)(cid:4)(cid:5)kα1(cid:1) 0.(13)To prove that F (α1, k) (cid:1) 0, we use the fact that ln x (cid:1) x − 1. Hence, we have:12B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–22F (α1, k) = α1(k − 1) + ln(cid:5)(cid:4)1 + 11 + 12 α12 kα1− 11 + 11 + 12 α1(cid:1) α1(k − 1) +2 kα1= α1(k − 1) + α1 − kα12 + kα1− α1 − kα21kα1 + k2α212 + kα1== α1(k − 1)(kα1 + 1)2 + kα1(cid:1) 0,(14)(15)(16)(17)(18)because:(i) 0 < k (cid:1) 1 and α1 (cid:2) 0 ⇒ α1(k − 1) (cid:1) 0 and kα1 + 1 > 0. Hence α1(k − 1)(kα1 + 1) (cid:1)0, and(ii) 0 < k (cid:1) 1 and α1 (cid:2) 0 ⇒ 0 < kα1 (cid:1) α1 ⇒ kα1 + 2 > 2 > 0.Hence the theorem. (cid:1)The above theorem can be viewed as a “sufficiency result”. In other words, we haveshown that q1 − q2 (cid:1) 0 or that p1 (cid:1) p2. We now show a “necessity result” stated as auniqueness result. This result states that the function p1 (cid:1) p2 has its equality ONLY at theboundary condition where the two distributions are exactly identical.To prove the necessity result, we consider q2 − q1 which, derived from (12), can bewritten, as a function of α1 and k, as:G(α1, k) = e−kα1 + 12kα1e−kα1 − e−α1 − 12−α1.α1e(19)By examining its partial derivatives, we shall show that there are two solutions for equal-ity. Furthermore, when α1 (cid:2) 0 and 0 < k (cid:1) 1, we shall see that for a given k, there is onlyone solution, namely α1 = 0 and k, 0 < k (cid:1) 1, proving the uniqueness.Theorem 2. Suppose that α1 (cid:2) 0, 0 < k (cid:1) 1. Let G(α1, k) be:G(α1, k) = e−kα1 + 12kα1e−kα1 − e−α1 − 12−α1.α1e(20)Then G(α1, k) (cid:2) 0, and there are exactly two solutions for G(α1, k) = 0, being: {α1 =−1, k = 1} and {α1 = 0, k}.Proof. We must prove that, as defined in the theorem statement, G(α1, k) (cid:2) 0.We shall prove that this is satisfied by determining the local minima for G(. , .), whereα1 (cid:2) 0 and 0 < k (cid:1) 1. We first find the partial derivatives of (19) with respect to α1 and k:∂G∂α1= − 12ke−kα1 − 12k2α1e−kα1 + 12e−α1 + 12α1e−α1 = 0,and(21)B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–22∂G∂k= − 12α1e−kα1 − 12kα21e−kα1 = 0.We now solve (21) and (22) for α1 and k. Eq. (22) can be written as follows:− 12α1e−kα1 = 12kα21e−kα1,13(22)(23)which, after canceling some terms results in kα21we have: α1 = − 1we obtain:k and α1 = 0. Substituting α1 = − 1+ α1 = 0. Solving this equation for α1,k in (21), and canceling some terms,12e−α1 + 12α1e−α1 = 0,(24)which results in the solution to be α1 = −1, and consequently, k = 1.The second root, α1 = 0, indicates that the minimum is achieved for any value of k.We have thus found two solutions for (21) and (22), {α1 = 0, k} and {α1 = −1, k = 1}.Since α1 (cid:2) 0, it means that α1 can have at least a value of 0, and hence the local minima isin {α1 = 0, k}. Substituting these two values in G, we see that G(α1, k) = 0, which is theminimum. Therefore, G(α1, k) (cid:2) 0 for α1 (cid:2) 0 and 0 < k (cid:1) 1.Hence the theorem. (cid:1)To get a physical perspective of these results, let us analyze the geometric relation ofthe function G and the heuristic functions. G is a positive function in the region α1 (cid:2) 0,0 < k (cid:1) 1. When α1 → 0, G → 0. This means that for small values of α1, G is also small.Since α1 = λ1c, the value of α1 depends on λ1 and c. When c is small, G is very close toits minimum, 0, and hence both probabilities, p1 and p2, are very close. This behavior canbe noticed in Fig. 2, and the phenomenon is observed if the heuristic functions are bothcomparable and almost equally efficient.In terms of histogram methods and in database query optimization, when c is small, theoptimal and the sub-optimal QEP are very close. Since histogram methods such as Equi-Fig. 2. Function G(α1, k) plotted in the ranges 0 (cid:1) α1 (cid:1) 1 and 0 (cid:1) k (cid:1) 1.14B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–22width and Equi-depth produce a larger error than the R-ACM and the T-ACM, the formerare less likely to find the optimal QEP than the latter.Interpreted alternatively, G is very small when λ1 is close to 0. This means that Var[X1]is very large. Since Var[X1] (cid:1) Var[X2], Var[X2] is also very large, and both are closeeach other (in Fig. 1, we would observe almost flat curves for both distributions). Randomvariables for histogram methods such as Equi-width and Equi-depth yield similar error es-timation distributions with large and similar variances. Hence, the probabilities p1 and p2are quite close, and consequently, similar results are expected for these estimation meth-ods. However, when the heuristic functions yield widely different estimated costs (as inthe case when the new histogram methods, the R-ACM and the T-ACM, are compared tothe traditional methods), these effectively imply random variables with smaller variancesbeing compared to random variables with larger variances. In such a case, the value of Gis very high—implying that the former would yield superior solutions.2.2. Analysis considering normal distributionsFor the analysis done in this section, we consider that we are given two heuristic func-tions, H1 and H2, for which the probabilities of choosing optimal or suboptimal solutionsare represented by two normally distributed random variables, X1 and X2, whose means1 and σ 2are µ1 and µ2, and whose variances are σ 2Although the model using normal distributions is more realistic in real life problems,the analysis becomes impossible because there is no closed-form algebraic expression forintegrating the normal probability density function. Alternatively, we have used numericalintegration and we have obtained rather representative values for which the implicationbetween efficiency and optimality is again corroborated.2 respectively.Without loss of generality, if the mean cost of the optimal solution is µ1, by shifting theorigin by µ1, we again assume that the cost of the best solution is 0, which is the meanof these two random variables. The cost of the second best solution is given by anothertwo random variables (one for using the heuristic function H1, and the other one for usingthe heuristic function H2) whose mean, µ2 > 0, is the same for both variables. We alsoassume that, by scaling both distributions,8 the variance of using H1 and leading to theoptimal solution is unity. An example will help to clarify this.Example 2. Suppose that using H1 leads to the optimal cost with probability represented bythe normal random variable X(opt)whose mean is 0 and standard deviation is σ1 = 1. Thisheuristic function also estimates another sub-optimal cost according to X(subopt)whosemean is 4 and σ1 = 1.11H2 is another heuristic function that is used to estimate the optimal cost with probabil-ity given by X(opt)whose parameters are µ1 = 0 and σ2 = 1.4. The other correspondingsub-optimal cost given by the heuristic function H2 is obtained with probability given byX(subopt)whose parameters are µ2 = 4 and σ2 = 1.4.22−28 This can be done by multiplying σ 22 by σ1simultaneous diagonalization between d-dimensional normal random vectors for which d = 1 [5].−1, and µ1 and µ2 by σ11 and σ 2. This is a particular case of theB.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–2215Fig. 3. An example showing the probability density function of four normal random variables whose parametersare σ1 = 1, σ2 = 1.4, µ1 = 0, and µ2 = 4.Observe that σ1 < σ2, and hence we are expecting that the probability of using H1and leading to a wrong decision is smaller than that of using H2. The probability densityfunctions for these four random variables are depicted in Fig. 3. Note that, as in the doublyexponential distribution, given a particular value of x, if its probability under X(opt)is high,then the area for which using H1 leads to the wrong decision (i.e., its cumulative probabilityunder X(subopt)) is small. Since these two quantities are multiplied and integrated, the final1value is smaller than that of using H2, since σ2 is greater than σ1 = 1. This is what weformally show below.1Result 1. 9 Suppose that A is a heuristic algorithm that can potentially utilize either of twoheuristic functions, H1 and H2. Let:• X1 and X2 be two normally distributed random variables that represent the costs of• X(cid:6)2 be two other normally distributed random variables that represent the coststhe optimal solutions obtained by H1 and H2 respectively.1 and X(cid:6)of non-optimal solutions obtained by using H1 and H2 respectively.] = E[X(cid:6)2• 0 = E[X1] = E[X2] (cid:1) E[X(cid:6)] = µ.1• p1 and p2 be the probabilities that using H1 and H2 respectively lead to the wrongdecision.9 We cannot claim this result as a theorem, since the formal analytic proof is impossible. This is because there isno closed-form expression for integrating the Gaussian probability density function. However, the computationalproof that we present renders this to be more than a conjecture.16B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–22Then,if Var[X1] = Var[X(cid:6)1] = σ 21(cid:1) σ 22= Var[X2] = Var[X(cid:6)2], p1 (cid:1) p2.Computational Proof. To achieve this proof, we proceed by doing the same analysis thatwe did for the doubly exponential distributions (Theorem 1). If we consider a particularcost x, the probability that x leads to a wrong decision made by using H1, is given by:x(cid:1)I1 =−∞1√2πσ1− (u−µ)22σ 21 du.e(25)The probability that using H1 leads to the wrong decision for all values of x is obtainedby integrating the function resulting from multiplying every value of I1 for each x with therespective probability density function of X(opt), which results in:1∞(cid:1)p1 =I1−∞1√2πσ1− x22σ 21 dx.eSimilarly, p2 can also be expressed as follows:∞(cid:1)p2 =I2−∞1√2πσ2− x22σ 22 dx,e(26)(27)where I2 is obtained in the same way as in (25) for the distribution with variance σ 22 .Since there is no closed-form algebraic expression for integrating the normal probabilitydensity function, no analytical solution for proving that p1 (cid:1) p2 can be formalized.Alternatively, we have invoked a computational analysis by calculating these integralfor various representative values of σ1 and σ2 by using the trapezoidal rule. The valuesof G = p2/p1 (cid:2) 1 (i.e., for 1 (cid:1) σ1 (cid:1) 10 and 1 (cid:1) σ2 (cid:1) 10, where σ1 (cid:1) σ2) are depicted inTable 1 in the form of a lower-diagonal matrix. All the values of the upper-diagonal matrix(not shown here) are less than unity. Note that by making the value of σ1 = 1, the analysisreduces to the first and second columns of this table. For example, if σ1 = 1 and σ2 = 2,p2/p1 ≈ 33.6276. For more neighboring values of σ1 and σ2, e.g., σ1 = 9 and σ2 = 10(σ1 = 1 and σ2 ≈ 1.2345 after scaling), p2/p1 ≈ 1.0318, which is very close to unity. Theratio for σ1 = 1 and σ2 = 10 is much bigger, i.e., more than one hundred times. (cid:1)In order to get a better perspective of the computational analysis, we study the behaviorof the function G = p2/p1. Using the values of G given in Table 1, we have plotted thisfunction in the three-dimensional space as G(σ1, α1), where α1 = kσ1, 1 (cid:1) k (cid:1) 10. Theplot is depicted in Fig. 4.In order to enhance the visualization of G, we have approximated it by using the regres-sion utilities of the symbolic mathematical software package Maple V [3]. When k = 1,the surface lies on the z = 0 plane, in the form of a straight line x = y (labeled “k = 1or σ1 = σ2” in the figure). This is the place in which G reaches its minimum, when bothheuristic functions have identical variances. When k is larger (i.e., k = 10), the function GB.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–2217Table 1Ratio between the probability of making the wrong decision for two normally distributed random variables whosestandard deviations are σ1 and σ2σ21.002.003.004.005.006.007.008.009.0010.001.002.003.004.005.006.007.008.009.0010.00σ11.000033.627673.9210102.5081122.1988136.2472146.6138154.7078161.0448166.17161.00002.19823.04833.63394.05164.35994.60064.78914.94151.00001.38671.65311.84311.98342.09292.17862.24801.00001.19211.32911.43031.50921.57101.62111.00001.11501.19981.26601.31791.35981.00001.07611.13551.18201.21961.00001.05521.09841.13341.00001.04101.07411.00001.03181.0000Fig. 4. Function G(σ1, kσ1) plotted in the ranges 1 (cid:1) σ1 (cid:1) 10 and 1 (cid:1) kσ1 (cid:1) 10, where σ2 = kσ1.becomes much larger (up to 166.1716 in Table 1). This clearly shows the importance ofminimizing the variance in deciding on a heuristic function.When it concerns histograms in database query optimization, when k is small, it impliesthat the optimal and sub-optimal QEP are very close. Therefore, histogram methods like theEqui-width and the Equi-depth are less likely to find the optimal QEP, since they producelarger errors than histogram approximation methods such as the R-ACM and the T-ACM.The latter produce very small errors, and hence, when comparing any of them with theEqui-width or the Equi-depth, we will have a much larger value of k. This will be reflectedin our empirical results presented in the next section.18B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–223. Simulation results for database heuristic functions3.1. Empirical resultsIn order to provide practical evidence of the theoretical results presented above,10 wehave performed some simulations in database query optimization. In the experiments, wehave conducted four independent runs. In each run, 100 random databases were generated.Each database was composed of six relations, each of them having six attributes. Eachrelation was populated with 100 tuples.For each database, a random query including the six relations and arbitrary attributeswas performed. The cost of executing the query using the estimates of the histogramsobtained from the Equi-width, the Equi-depth, and the R-ACM was evaluated. This cost iscalculated by counting the number of tuples of the intermediate relations involved in thequery processing tree. More details of the simulations can be found in [17].The efficiency of the R-ACM was compared with that of the Equi-width and the Equi-depth after performing these simulations using 50 values per attribute. We set the numberof bins for the Equi-width and the Equi-depth to be 22. In order to be impartial with theevaluation, we set the number of bins for the R-ACM to be approximately half of that ofthe Equi-width and the Equi-depth, because the former needs twice as much storage as thatof the latter.The simulation results obtained from 400 independent runs, used to compare the effi-ciency of the R-ACM with that of the Equi-width and that of the Equi-depth, are givenin Table 2. The column labeled “R > W” is the number of times that the R-ACM obtainsa better solution than that of the Equi-width. The column labeled “W > R” indicates thenumber of times in which the Equi-width leads to a better QEP than the one determined bythe R-ACM. Similarly, the column labeled “R > D” represents the number of times that theR-ACM yields a better solution than the Equi-depth, and the column labeled “D > R” is theTable 2Simulation results for the R-ACM, the Equi-width, and the Equi-depth, after optimizingqueries on 400 randomly generated databases. The column labeled “R > W” containsthe number of times in which R-ACM obtained a better solution than the Equi-width on100 randomly generated databases. The information contained in the other columns has asimilar interpretation, where “R”, “W” and “D” stand for the R-ACM, the Equi-width andthe Equi-depth respectively. The last row contains the sum of the values in each columnSimulationR > WW > RR > DD > R1234Total262435291141215111553354246461691213884110 The empirical results presented in this paper are not intended to compare the various histogram methods: Equi-width, Equi-depth, R-ACM, T-ACM, V-optimal, etc. The experimental results submitted are merely included todemonstrate that the theoretically proven results can be experimentally justified.B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–2219number of times in which the Equi-depth is superior to the R-ACM. The last row, the totalof each column, gives us the evidence that the superiority of the R-ACM over the Equi-width is demonstrated more than twice as often. The same factor relating the superiority ofthe R-ACM over the Equi-depth is about four.3.2. Geometric justification of the rationaleWe now present a different perspective for the formulation of the QEP model that hasbeen used earlier. Indeed, we shall analyze the suitability of using the doubly exponentialdistribution for the query optimization problem. To demonstrate this suitability, we ex-amined 200 randomly selected queries. Since the cost of each query is different for eachdatabase, we computed the difference between the actual cost of executing the query andthe estimated cost. For each of the histogram methods, namely the Equi-width, the Equi-depth and the R-ACM, we obtained two hundred points.11 Using these points (or samples)we estimated the parameters of the doubly exponential distribution, λ, for each histogrammethod, using a Maximum Likelihood Estimate (MLE) method [4].Given N samples, {x1, . . . , xN }, obeying a doubly exponential distribution, it is easy(almost purely algebraic) to see that the maximum likelihood parameter, ˆλ, satisfying thedistribution obeys:ˆλ =(cid:6)NNi=1.|xi|(28)Using the estimate of (28), we computed the parameters for the doubly exponential dis-tribution for the Equi-width, the Equi-depth, and the R-ACM, which resulted in 0.6399,0.6120, and 0.7089 respectively. We have also calculated their variances as in (3) – theyare 4.8834, 5.3401, and 3.9791 for the Equi-width, the Equi-depth and the R-ACM respec-tively. As expected, the variance for the R-ACM is smaller than that of the Equi-width andthe Equi-depth. This can also be observed in Fig. 5, in which the corresponding doubly ex-ponential probability distribution functions are plotted for the three histograms. This slightdifference between the R-ACM, the Equi-width and the Equi-depth schemes reflects in thecorresponding results leading to superior QEPs as shown in Table 2. Clearly, the R-ACM,whose variance is smaller than that of the Equi-width and the Equi-depth, is a superiorheuristic function.In order to observe the similarities between the doubly exponential distribution and thedistribution of the actual cost of executing a query, we have plotted the expected values ofthe doubly exponential distribution and the actual costs obtained when optimizing queriesusing the R-ACM histogram. The plot depicted in Fig. 6 was obtained by grouping the datain bins of width two, for the values in the ranges [x1, x2), where x2 = x1 + 2, and x2 = 2ifor i = −4, . . . , 5. In the figure, “R-ACM” (in light gray) represents the actual cost valuesof the queries, and “d-exp” (in dark gray) represents the expected population in each binwhen the random variable is doubly exponential with a value of λ being determined by11 Since these histograms always tend to under-estimate the costs of the queries, we have shifted all the pointsso that the estimated mean of these samples is zero. In this way, we could work with zero-mean random variables.20B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–22Fig. 5. Estimated probability density function for three doubly exponential random variables that represent theerror in estimation for the Equi-width, the Equi-depth and the R-ACM.Fig. 6. Expected values for a doubly exponential random variable, and the actual costs obtained after optimizingqueries on 400 random databases using the R-ACM histogram.using (28). Observe the similarity between both histograms. We further corroborate thevalidity of our model for the database query optimization problem.4. ConclusionsThe theory of PR is quite developed, and has many applications. In this paper, we haveapplied pattern classification techniques to solve a fundamental open problem in computerscience that relates heuristic function accuracy and solution optimality. More specifically,in this paper, we have discussed the efficiency of using heuristic functions for optimizationproblems and resolved an open problem, which has been (to our knowledge) open for atleast twenty years. The problem involves how the accuracy of a heuristic function relatesto the quality of the corresponding solution obtained. The efficiency has been quantified bymeans of the probability of the heuristic function leading to the optimal solution. We haveB.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–2221shown analytically (using a reasonable model of accuracy, namely the doubly exponentialdistribution for errors) that as the accuracy of a heuristic function increases, the probabilityof it leading to a superior solution also increases.Due to the constraints involved in deriving a closed-form expression for integrating thenormal probability density function, we have presented a computational analysis of theaccuracy/optimality result for the Gaussian distribution. Again, our analysis corroboratesthe result that heuristic functions producing smaller errors lead more often to optimal so-lutions.For the field of database query optimization, we have highlighted that for histogrammethods that produce errors with similar variances (the Equi-width and the Equi-depth),the query processing results are also quite similar. However, we have also shown that theR-ACM and the T-ACM, which produce errors with smaller variances than the traditionalmethods, yield better query optimization plans more often. This result, earlier shown the-oretically, has been experimentally verified. Thus, our empirical results on database queryoptimization show that the R-ACM provides superior solutions more than twice as manytimes as the Equi-width, and more than four times as often as the Equi-depth. More de-tailed empirical results including the design of random databases and random queries inthese random databases can be found in [17].We have also estimated the parameters of the doubly exponential distributions repre-senting the Equi-width, the Equi-depth and the R-ACM, and shown graphically how ourexperiments relate to the theoretical model presented in this paper.AcknowledgmentsThe authors are very grateful to the anonymous referee for his/her suggestions. Thesesuggestions have allowed us to significantly enhance the quality of the paper. In particular,we would like to thank him/her for his/her critical remarks which enabled us to clearlyelucidate the motivation for the paper in terms of examples, and an explanation of thedifference between “heuristic algorithms” and the “heuristic functions” that they utilize.References[1] E. Aarts, J. Korst, Simulated Annealing and Boltzmann Machines: A Stochastic Approach to CombinatorialOptimization and Neural Computing, Wiley, New York, 1989.[2] S. Christodoulakis, Estimating selectivities in data bases, Technical Report CSRG-136, Computer ScienceDepartment, University of Toronto, 1981.[3] E. Deeba, A. Gunawardena, Interactive Linear Algebra with MAPLE V, Springer, Berlin, 1997.[4] R. Duda, P. Hart, D. Stork, Pattern Classification, second ed., Wiley, New York, 2000.[5] K. Fukunaga, Introduction to Statistical Pattern Recognition, Academic Press, New York, 1990.[6] M. Garey, D. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness, Freeman,New York, 1979.[7] F. Glover, M. Laguna, Tabu Search, Kluwer Academic, Dordrecht, 1997.[8] Y. Ioannidis, S. Christodoulakis, On the propagation of errors in the size of join results, in: Proceedings ofthe ACM-SIGMOD Conference, 1991, pp. 268–277.[9] R.P. Kooi, The optimization of queries in relational databases, PhD thesis, Case Western Reserve University,1980.22B.J. Oommen, L.G. Rueda / Artificial Intelligence 164 (2005) 1–22[10] D. Kreher, D. Stinson, Combinatorial Algorithms: Generation, Enumeration, and Search, CRC Press, BocaRaton, FL, 1998.[11] D. Levy, How Computers Play Chess, Computer Science Press, New York, 1991.[12] M.V. Mannino, P. Chu, T. Sager, Statistical profile estimation in database systems, in: ACM ComputingSurveys, vol. 20, 1988, pp. 192–221.[13] M. Michell, An Introduction to Genetic Algorithms, MIT Press, Cambridge, MA, 1998.[14] M. Muralikrishna, D. Dewitt, Equi-depth histograms for estimating selectivity factors for multi-dimensionalqueries, in: Proceedings of ACM-SIGMOD Conference, 1988, pp. 28–36.[15] Kumpati Narendra, M.A.L. Thathachar, Learning Automata: An Introduction, Prentice Hall, EnglewoodCliffs, NJ, 1989.[16] N. Nilsson, Artificial Intelligence: A New Synthesis, Morgan Kaufmann, San Mateo, CA, 1998.[17] B.J. Oommen, L. Rueda, The efficiency of modern-day histogram-like techniques for query optimization,Comput. J. 45 (5) (2002) 494–510.[18] B.J. Oommen, M. Thiyagarajah, The Rectangular Attribute Cardinality Map: A New Histogram-like Tech-nique for Query Optimization, in: Proceedings of the International Database Engineering and ApplicationsSymposium, IDEAS’99, Montreal, Canada, 1999, pp. 3–15.[19] B.J. Oommen, M. Thiyagarajah, On the use of the trapezoidal attribute cardinality map for query result sizeestimation, in: Proceedings of the 2000 International Database Engineering and Applications Symposium,Yokohama, Japan, 2000, pp. 236–242.[20] B.J. Oommen, T. De St. Croix, Graph partitioning using learning automata, IEEE Trans. Comput. 45 (2)(1995) 195–208.[21] F. Peracchi, Econometrics, Wiley, New York, 2001.[22] G. Piatetsky-Shapiro, C. Connell, Accurate estimation of the number of tuples satisfying a condition, in:Proceedings of ACM-SIGMOD Conference, 1984, pp. 256–276.[23] W. Poosala, Histogram based estimation techniques in databases, PhD thesis, University of Wisconsin-Madison, 1997.[24] E. Rich, K. Knight, Artificial Intelligence, second ed., McGraw Hill, New York, 1991.[25] G. Romp, Game Theory: Introduction & Applications, Oxford University Press, Oxford, 1997.[26] S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, second edition, Prentice-Hall, New York,2002.[27] A. Webb, Statistical Pattern Recognition, Oxford University Press, New York, 1999.[28] A. Yuille, M. Coughlan, An A* perspective on deterministic optimization for deformable templates, PatternRecognition 33 (2000) 603–616.