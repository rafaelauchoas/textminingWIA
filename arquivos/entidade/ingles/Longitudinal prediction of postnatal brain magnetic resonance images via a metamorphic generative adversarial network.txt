2202guA9]VI.ssee[1v52840.8022:viXraLongitudinal Prediction of Postnatal Brain MagneticResonance Images via a Metamorphic GenerativeAdversarial NetworkYunzhi Huanga,b, Sahar Ahmadb, Luyi Hanc, Shuai Wangd, Zhengwang Wub,Weili Linb, Gang Lib, Li Wangb, Pew-Thian Yapb,∗aSchool of Automation, Nanjing University of Information Science and Technology,Nanjing 210044, ChinabDepartment of Radiology and Biomedical Research Imaging Center (BRIC), University ofNorth Carolina, Chapel Hill, USAcDepartment of Radiology and Nuclear Medicine, Radboud University Medical Center,Geert Grooteplein 10, 6525 GA, Nijmegen, The NetherlandsdDepartment of Computer Science, Shandong University (Weihai), ChinaAbstractMissing scans are inevitable in longitudinal studies due to either subject dropoutsor failed scans. In this paper, we propose a deep learning framework to pre-dict missing scans from acquired scans, catering to longitudinal infant studies.Prediction of infant brain MRI is challenging owing to the rapid contrast andstructural changes particularly during the first year of life. We introduce atrustworthy metamorphic generative adversarial network (MGAN) for translat-ing infant brain MRI from one time-point to another. MGAN has three keyfeatures: (i) Image translation leveraging spatial and frequency information fordetail-preserving mapping; (ii) Quality-guided learning strategy that focusesattention on challenging regions. (iii) Multi-scale hybrid loss function that im-proves translation of tissue contrast and structural details. Experimental resultsindicate that MGAN outperforms existing GANs by accurately predicting bothcontrast and anatomical details.Keywords:Infant brain MRI, Longitudinal prediction, Metamorphic GAN∗Corresponding authorEmail address: ptyap@med.unc.edu (Pew-Thian Yap)Preprint submitted to Journal of Pattern RecognitionAugust 10, 2022   1. IntroductionBrain MRI is commonly used to investigate normative and aberrant brainevolution through infancy [1]. To precisely chart brain growth trajectories,temporally dense longitudinal datasets are often required but are difficult toacquire. Moreover, infant studies often involve incomplete longitudinal datasets,given the unique challenges associated with infant MRI acquisition. The missingdata at different time points can be due to subject dropouts or failed scans owingto excessive motion, insufficient coverage, or imaging artifacts [2].Longitudinal prediction of infant brain scans is challenging as brain MRIcontrasts change rapidly through the first year of life. The brain volume doublesto about 65% of the adult brain by the end of the first year [3]. The gray matter(GM) follows a faster growth trajectory (108% − 149% increase) compared towhite matter (WM; 11% increase) [4]. The rapid brain evolution is characterizedby both structural and contrast variations [5, 6]. As shown in Figure 1, theWM appears to be darker than the GM during the neonatal phase as the brainis going through myelination, and by sixth month, WM and GM are almostindistinguishable due to the poor tissue contrast.1.1. Related WorkThe longitudinal prediction of infant brain MRI can be formulated as animage-to-image translation task — mapping images from a source time point toa target time point [7]. Several studies in the field of computer vision [7, 8, 9, 10]have shown that generative adversarial networks (GANs) [11] yield superiorperformance in translating images from a domain to another.In the fieldof medical image analysis, [12] introduced an auto-context GAN to progres-sively refine MRI-to-CT synthesis. In their follow-up study, [13] incorporateddifficulty-aware attention mechanism to improve predictions in challenging re-gions. Similarly, [14] introduced self-attention to encourage the transformationof a foreground object while retaining the background. Medical image-to-imagetranslation network (MedGAN) [15] uses a pre-trained classification network as2Figure 1: Appearance and structural changes at two time points during the first year of life.Wavelet decomposition for capturing structural details.feature extractor to match textures and structural details of synthetic and targetCT images. All the aforementioned methods for cross-modality synthesis focuson appearance changes and neglect morphological changes. The longitudinalprediction of infant MR brain images, however, requires dealing with fast-pacedstructural and appearance changes.To promote structural consistency in cross-modality synthesis, several recentapproaches incorporate segmentation similarity as a learning constraint [16, 17].However, tissue segmentation of infant brain MRI is challenging due to theoverlap of GM and WM intensity distributions (Fig. 1). Several approachesattempted to ensure structural consistency without relying on tissue maps. [18]employed gradient differences in a loss function to improve the prediction ofboundaries.[19] incorporated gradient correlation differences in a structure-consistency loss to improve edge alignment in MRI-to-CT synthesis. Althoughsuccessful, the gradient-based constraint introduces noise and fail to capturesufficient boundary information in images with low contrast. [20] incorporateda patch-based self-similarity loss by comparing each patch with all its neighbors3in a pre-defined non-local region to ensure structural consistency. However, thesearch for corresponding non-local regions is computationally expensive.1.2. ContributionsIn this paper, we employ CycleGAN [8], a cycle consistent generation frame-work, to simultaneously learn structural and appearance changes between twotime points. Major contributions of our work are summarized below:(i) We propose a trustworthy adversarial learning metamorphosis frameworkthat accounts for both the appearance and structural changes in infantbrain MRI.(ii) We use a spatial-frequency transfer block equipped with wavelet decom-position to transform features from multiple frequency bands to learn thestructural changes.(iii) We employ a quality guidance strategy to incorporate a quality-driven lossfunction to improve predictions in challenging regions.(iv) We devise a multi-scale hybrid loss function to improve the matching ofboth the textural details and the anatomical edges between the predictedimage and the desired target image. The discriminator network is evokedat multiple resolutions via deep-supervision, thus allowing accurate pre-diction of anatomical structures through adversarial learning.The rest of the paper is organized as follows: Section 2 details the proposedmethod. Section 3.2 describes the dataset used for evaluation and presents theexperimental results. Section 4 provides additional discussion and concludes thepaper.2. MethodsIn this work, we implement a framework for prediction of metamorphicchanges using a GAN. Details of our method are described next.42.1. Network ArchitectureWe propose a metamorphic GAN (MGAN) to predict the infant brain MRimage scanned at time point tb from a time point ta. Without loss of gen-erality, we assume that tb > ta. Our network architecture, shown in Fig. 2,is cycle-consistent and learns a reversible translation between the two time-points.It consists of (i) a forward path for earlier-to-later time-point imageprediction and (ii) a backward path for later-to-earlier time-point image predic-tion. The two generators Ga and Gb and their corresponding discriminators Daand Db follow an encoder-decoder architecture. Both the generators incorporatea spatial-frequency transfer (SFT) block to transform the appearance and struc-tural features via multiple branches detailed in Fig. 3. The two discriminatorsestimate voxel-level uncertainty maps, enabling the corresponding generators tofocus on challenging regions. We will describe the components of our networkin the subsequent sections.2.1.1. Metamorphic GeneratorThe metamorphic generator (Fig. 3) takes a 3D patch of size 64 × 64 × 64as input and predicts a 3D patch. The generator consists of an encoder, SFTblock, and a decoder.Encoder. The encoding path consists of two convolution blocks, each with a3 × 3 × 3 convolution layer, followed by 3D instance normalization (IN) [21] anda rectified linear unit (ReLU) [22]. For downsampling, we use convolution witha stride of 2 instead of pooling to avoid potential information loss. We keep a1-stride convolution in the first stage of the encoder to retain details, and use a2-stride convolution in the second stage. The resulting numbers of feature mapsin the two-stage encoder are 64 and 32.Spatial-frequency transfer block. Longitudinal prediction requires translatingboth contrast and structure between two time points. We propose to embeda spatial-frequency transfer block in between enocder-decoder to extract thespatial and frequency domain information of feature maps. The SFT block is5Figure 2: Overview of the metamorphic GAN.divided into two branches: (i) frequency transform branch, and (ii) spatial trans-form branch. The frequency transform branch is equipped with discrete wavelettransform (DWT) that takes into account the low frequency tissue contrast andhigh frequency structural details. The DWT layer decomposes the feature mapinto low frequency approximation and high frequency details along three dimen-sions, resulting in eight subvolumes: LLL, LLH, LHL, LHH, HLL, HLH, HHLand HHH. This decomposition allows more effective transfer of spatial-frequencydetails.6Figure 3: Network architecture of the metamorphic generator.Given the i-th channel feature map f i of size (sx × sy × sz), the decomposedj at frequency band j is obtained by convolving f i with waveletfeature map f ifilter wj:j = f i (cid:126) wj.f i(1)The wavelet filters for each frequency band are calculated by DWT decompo-sition and are preset in the convolution layer. Correspondingly, the featuremaps are reconstructed in the decode path via inverse discrete wavelet trans-form (IDWT) layer. We show the representative feature maps from the DWTand IDWT layers in Fig. 4. The DWT layer is akin to pooling layer as the DWTdecomposition halves the size of the input feature maps. The IDWT layer cor-responds to the deconvolution operation with the fixed weights obtained viawavelet filters. There is also an intermediate transfer operation between theDWT and IDWT layer. This transfer operation is realized through 9 residualblocks [23]; the input of each block is processed by two 3 × 3 × 3 convolutionlayers with 64 channels followed by IN and ReLU for activation. A shortcut con-nection is added between the input and the output of every residual convolutionblock. Residual transfer learning simplifies feature generation and transfer froma source domain to a target domain.The second branch in the SFT block — spatial transform branch — is in-tegrated to compensate for the information truncated by the wavelets.It is7Figure 4: Feature maps obtained from the DWT and IDWT layers.implemented using a convolution layer with kernel size 3 × 3 × 3 and strideof 2 to downsample the feature maps in spatial domain. These downsampledfeature maps undergo transfer operation and are later upsampled by strideddeconvolution layer with kernel size 3 × 3 × 3.Each branch in the SFT block is trained independently without weight shar-ing. The feature maps from both the frequency and spatial transform branchesare concatenated using a 3 × 3 × 3 convolution layer and stride of 1, followed byIN and ReLU operation; capturing both the contrast and structural informationfor translating from the source domain to the target domain.Decoder. Deep supervision [24] is leveraged in the decoding path to strengthenthe gradient flow and encourage learning useful representations at multiplescales. The feature maps are upsampled by a 2-stride deconvolution layer andare then convolved with a 3 × 3 × 3 kernel to get the predicted output.2.1.2. Uncertainty QuantizationThe uncertainty associated with the prediction stems from two aspects: epis-temic uncertainty (model uncertainty) and aleatoric uncertainty (data uncer-tainty) [25, 26]. As shown in Fig. 3, two Monte-Carlo (MC) dropout layersare incorporated in our generator to estimate the epistemic uncertainty. MCdropout regularizes the network weights as Bernoulli distributions for varia-tional Bayesian inference [27, 28]. Note, MC dropout is only enabled duringinference. A set of predictions {ˆy1, ˆy2, . . . , ˆyN } are sampled from the distribu-tion p(ˆy|I, wn) via N stochastic inferences using the metamorphic generator.8The epistemic uncertainty is estimated as the variance over the predictions:(cid:115)Ue =(cid:80)Nn=1(ˆyn − y)2N,(2)where ˆy denotes to the prediction by feeding the generator G an input imageI, wn represents the generator weights after the n-th dropout, N refers to thenumber of prediction instances, and y is the mean of the predictions.The aleatoric uncertainty is typically measured with the test-time augmenta-tion technique [29, 30]. During inference, we perturb the input data with spatialtransformations (flip and rotation) and random noise. Similar to the estimationof the epistemic uncertainty, we sample a set of predictions {ˆy1, ˆy2, . . . , ˆyN } fromthe distribution p(ˆy|I, S)) and estimate the aleatoric uncertainty as the varianceover the predictions:(cid:115)Ua =(cid:80)Nn=1(S−1(ˆy(S(x + rn)) − y)2N,(3)where S represents the spatial transformation, S−1 corresponds to the inversetransformation, and rn corresponds to random noise.2.1.3. Multi-scale discriminatorThe discriminator in MGAN has a U-shaped architecture, as shown in Fig. 5,to locally distinguish the predicted images from real images. It takes as inputa 64 × 64 × 64 image patch and outputs the quality probability map for thegiven 3D patch. The continuous probability map quantifies the quality of thepredicted image patch. Inferior quality, associated with lower probability val-ues, is commonly associated with complex structures, e.g., the cortical ribbon.Superior quality, associated with higher probability values, corresponds to flatregions with simple structures. In the encoding path of the discriminator, theinput is downsampled three times; in the decoding path, the feature maps areupsampled three times. For downsampling/upsampling, we use a 4×4×4 convo-lution/deconvolution layer, followed by IN and ReLU activation. The numbersof feature channels are 64, 128, and 256 in the three stages of the discrimi-9nator. Deep supervision strategy [24] is incorporated in the decoding path tostrengthen gradient back propagation.Figure 5: Network architecture of the multi-scale discriminator.2.2. Loss FunctionsWe incorporated supervised learning with multi-scale information via deepsupervision strategy [24]. The loss function LMGAN is defined as:LMGAN = Ls1 + Ls2 + Ls3,(4)where s1, s2, and s3 refer to the three scales employed [31, 32]. For each scale,the objective function is composed of three loss functions to effectively learn theprediction task. The loss functions are described next.2.2.1. Adversarial LossWe propose to use the standard adversarial loss function, which aims tomatch the distribution of the predicted images with that of the real images. Itis given byL(Ga, Gb, Da, Db) = EIta+ EItb+ EItb+ EIta[log(Da(Ita )][log(1 − Da(Gb(Itb )))][log(Db(Itb )][log(1 − Db(Ga(Ita )))],(5)10where Ita and Itb refer to the images at time-point ta and tb, respectively, Gaand Gb are the mapping functions, and Da and Db are the discriminators.2.2.2. Paired LossThe generators Ga and Gb seek to minimize the difference between real andpredicted images. We propose to enhance the performance of the generators bydefining a paired loss function that constraints the difference at voxel-, feature-,and frequency-level. Our paired loss function L(·)gen consists of three loss terms:(i) quality-driven loss, (ii) texture loss, and (iii) frequency loss.gen = LGaLGagen = LGbLGbQ + LGaQ + LGbT + LGaF ,T + LGbF .(6)Quality-driven loss. The low tissue contrast and the dramatic brain growthhinder translation of regions such as the convoluted cerebral cortex. Here, wepresent a quality-guided learning strategy to strengthen the transformation ofthe unfathomable regions. The discriminator outputs a quality map that de-fines the voxel-wise probabilities for each predicted image. The heterogeneousdistribution of the probabilities in the quality map motivates us to treat voxelsdifferently. Voxels with lower probabilities correspond to poor prediction andrequire more attention compared to those with higher probabilities. This en-hances the image translation power of the generator at complex regions in theinfant brain MRI. The quality-driven loss L(·)Q is defined as:LGaQ (Ga; θGa ) = EIta ,Itb ,QDb [(cid:107)Itb − Ga(Ita )(cid:107)1 (cid:12) (1 − QDb )β],LGbQ (Gb; θGb ) = EIta ,Itb ,QDa [(cid:107)Ita − Gb(Itb )(cid:107)1 (cid:12) (1 − QDa )β],(7)where Q(·) is the quality map, θ(·) denotes the parameters of the network, (cid:12)defines the element-wise multiplication and β represents the parameter thatenables to focus on difficult-to-predict regions. If β is set to zero, then L(·)Q willbe equivalent to L1 norm; losing the ability to define adaptive weights based onquality map. In this study, we empirically set its value to 1.5.11Texture loss. This loss ensures that the predicted image has a texture similarto the target image, and it is defined as the mean square error (MSE) betweenthe Gram matrix of the target and the predicted image [33, 34]:LGaT = (cid:107)M (Itb ) − M (Ga(Ita ))(cid:107)2,LGbT = (cid:107)M (Ita ) − M (Gb(Itb ))(cid:107)2.(8)The gram matrix M (·) is the inner product of the generated images.Frequency loss. The frequency loss L(·)F is incorporated via wavelet decompo-sition of the generators’ outputs, which steers the effective prediction of thestructural details. L(·)F is defined as:LGaF =LGbF =(cid:88)k∈K(cid:88)k∈K(cid:107)DWT(Itb )k − DWT(Ga(Ita ))k(cid:107)1,(cid:107)DWT(Ita )k − DWT(Gb(Itb ))k(cid:107)1,(9)where K = {LLL, LLH, LHL, HLL, LHH, HLH, HHL, HHH}; LLL corre-sponds to the approximation coefficients which encode the image contrast, andthe remaining terms correspond to the detail coefficients, encoding the highfrequency structural details. The wavelet coefficients are decomposed usingbior1.3 [35], which is compactly supported by a biorthogonal spline wavelet [36].2.2.3. Cycle Consistency LossThe cycle consistency loss function Lcyc ensures that the image predictioncycle brings the predicted image back to the original image, i.e., Gb(Ga(Ita )) ≈Ita and it is given by:Lcyc(Ga, Gb) = EIta+ EItb[(cid:107)Ita − Gb(Ga(Ita ))(cid:107)1],(10)[(cid:107)Itb − Ga(Gb(Itb ))(cid:107)1].This loss function constraints both the forward and backward image predictioncycles, causing Ga and Gb to be consistent with each other.123. Experimental Results3.1. Data Acquisition and PreprocessingThe dataset consists of longitudinal T1-weighted (T1w) and T2-weighted(T2w) MR images of healthy infant subjects enrolled in the Multi-visit AdvancedPediatric Brain Imaging (MAP) study. Informed written consent was obtainedfrom the parents of all the participants and all study protocols were approvedby the University of North Carolina at Chapel Hill Institutional Review Board.Each subject was scanned every three months in the first postnatal year. Theimaging parameters for T1w MRI data were: TR = 1900 ms, TE = 4.38 ms,flip angle = 7◦. All the images had 144 sagittal slices and 1 mm isotropic voxelresolution. The imaging parameters for T2w MR images were TR = 7380 ms,TE = 119 ms, flip angle = 150◦, 64 sagittal slices, and 1.25 × 1.25 × 1.95 mm3voxel size.The dataset was preprocessed using our infant-dedicated preprocessing pipeline [37,38]. Then, all the postnatal images of each subject were linearly aligned to theircorresponding 12-months-old images and resampled to the size of 256×256×256with 1 × 1 × 1 mm3 voxel resolution. We randomly split the MRI data from 30healthy infants into 20 and 10 for training and testing, respectively. Five-foldcross-validation was performed to tune the hyper-parameters.3.2. Implementation DetailsThe proposed metamorphic GAN was implemented using TensorFlow li-brary [39] on a single Nvidia TitanX (Pascal) GPU. Adam optimizer [40] wasadopted with an initial learning rate of 1 × 10−4 and batch size of 1. Training,validation, and testing were performed separately for T1w and T2w images.During training, we uniformly sampled 3D patches from each image encom-passing the brain region with a dense stride of 10, providing sufficient samplesfor training. The generator was first trained with 5 epochs before the adversarialtraining. The adversarial training was stopped at 50 epochs.During inference, the N = 20 inferences were performed for the estimationof epistemic and aleatoric uncertainty. The keep rate of the dropout layers was13set to 0.8. Test-time data augmentation was carried out using a combinationof random flip, rotation along each of the three axes, and random noise, whichwere modeled respectively with discrete Bernoulli distribution B(0.5), uniformdistribution U(0, 2π), and normal distribution N (0, 0.05).3.3. Evaluation CriteriaWe employed two commonly used metrics to evaluate the quality of thepredicted images: (i) peak signal-to-noise ratio (PSNR), and (ii) structural sim-ilarity (SSIM) [41]. Higher PSNR and SSIM correspond to accurate imageprediction.3.4. Comparison with Existing TechniquesWe compared MGAN with three widely used GANs: CycleGAN [16], Pix2Pix [7],and WGAN [42]. All the compared models were used to predict the 12-month-old brain MRI from the 2-week-old brain MRI. The prediction task is challeng-ing due to the extent of changes between the two time points (Fig. 1). For faircomparison, we re-trained the GANs for optimal parameters.The image prediction results shown for the compared models in Fig. 6 indi-cate that MGAN yields T1w and T2w image predictions that are closer to theground truth with richer details than the other models. The error maps indicatethat MGAN achieves the lowest error among all methods, especially around theventricles and cerebral cortex. Summary statistics for PSNR and SSIM are re-ported in Table 1. MGAN achieves significant improvement (p < 0.05, pairedt-test) for PSNR and SSIM over other methods.3.5. Ablation StudyHere, we investigate the effectiveness of three components of MGAN — theSFT block, quality-guided learning, and the hybrid loss function. The influenceof frequency transform on longitudinal prediction was verified using two vari-ants of the metamorphic generator: (i) incorporating the SFT block equippedwith both the frequency and spatial transform branches, and (ii) replacing the14(a) T1w image predictions15(b) T2w image predictionsFigure 6: Longitudinal image prediction with various GANs.Table 1: Summary statistics of PSNR and SSIM for different GANs.T1wT2wMethodPSNRSSIM (%)PSNRSSIM (%)CycleGAN 22.6±1.174.2±2.821.8±1.075.3±2.2Pix2Pix23.0±1.376.2±3.422.9±0.977.2±2.8WGAN24.1±1.279.4±2.423.4±0.979.5±2.0MGAN 26.4±0.984.0±2.225.5±0.784.8±1.8SFT block with a conventional spatial transform branch. We also investigatedthe efficacy of quality-guided learning by conducting experiments with/withoutquality maps generated by the discriminators. The configurations are summa-rized as follows:• Backbone: SFT with only spatial transform branch and without qualityguidance.• SFT-NCG: SFT without quality guidance.• ST-CG: Conventional spatial transform and quality guidance.• MGAN: SFT and quality guidance.Table 2 indicates that MGAN achieves the highest PSNR and SSIM witha significant improvement (p < 0.05, paired t-test). SFT-NCG and ST-CGperform better than Backbone, validating that wavelet-based feature mappingand quality-guidance improve prediction accuracy.Fig. 7 shows that Backbone and ST-CG predict the 12-month scan poorlydue to the spatial complexity of the cortical ribbon. SFT-NCG generated un-satisfactory results at difficult-to-predict regions as indicated by the high valuesin the error map. MGAN yields the most accurate prediction, which matchesthe ground truth both in terms of tissue contrast and anatomical structure.We investigated the contribution of the uncertainty-aware loss LQ, tex-ture loss LT, and frequency loss LF. Table 3 indicates that including all loss16(a) T1w image predictions17(b) T2w image predictionsFigure 7: Longitudinal image prediction results obtained with different MGAN configurations.Table 2: Ablation study with different MGAN configurations.T1wT2wModelPSNRSSIM (%)PSNRSSIM (%)Backbone24.9±0.580.3±1.324.2±0.481.1±1.2SFT-NCG 25.2±1.083.7±2.025.1±0.983.0±1.6ST-CG25.9±1.282.5±2.024.8±1.082.2±1.5MGAN 26.4±0.984.0±2.225.5±0.784.8±1.8terms (Eq. 6) yields the highest PSNR and SSIM. In contrast, using only theuncertainty-aware loss yields the lowest PSNR and SSIM. This implies that boththe texture and frequency losses improve the predictive power of the generator.Table 3: Ablation study with different combinations of losses.T1wT2wLQ LT LFPSNRSSIM (%)PSNRSSIM (%)(cid:88)(cid:88)(cid:88)(cid:88)26.0±1.383.3±1.425.1±0.883.1±1.4(cid:88)26.2±1.083.5±2.025.3±0.983.7±1.9(cid:88)26.1±1.083.7±1.825.2±0.884.3±1.7(cid:88) (cid:88) 26.4±0.984.0±2.225.5±0.784.8±1.83.6. Longitudinal PredictionWe demonstrate the effectiveness MGAN in predicting a 12-month-old imagefrom any earlier time-point, i.e., 2 weeks, 3 months, 6 months, and 9 months.Predictions from the forward and backward prediction paths are evaluated. Thepredicted images along with the error maps and uncertainty maps are shown inFig. 8. The quantitative results are presented in Table 4. Despite the significant18(a) T1w image predictions(b) T2w image predictionsFigure 8: Longitudinal prediction results for different time points. (Left) The forward pathpredicts a 12-month-old image from images at earlier time points. (Right) The backward pathpredicts images of earlier time points from a 12-month-old image.differences in appearance and structure, MGAN is able to predict the imageswith great resemblance to the ground-truth images in both tissue contrast andanatomical structure. This is validated by the high PSNR and SSIM values.The corresponding epistemic and aleatoric uncertainty maps of the predictionsare also shown in Fig. 8. The epistemic and aleatoric uncertainty is positively19Figure 9: Quality visualization for the 0-to-12-month-old prediction.correlated with prediction errors.Table 4: Statistical summary of evaluation metrics for longitudinal prediction.T1wT2wPSNRSSIM(%)PSNRSSIM(%)n 0m→12m 26.4±0.984.0±2.225.5±0.784.8±1.8oitciderpdrawrofnoitciderpdrawkcab3m→12m 26.1±1.384.7±4.025.7±0.983.8±2.26m→12m 27.7±2.289.1±3.626.8±1.887.5±2.89m→12m 29.0±2.989.9±4.628.5±1.988.3±2.812m→0m 27.1±0.986.7±0.226.5±1.186.7±1.212m→3m 26.9±1.786.9±3.126.4±1.286.4±2.212m→6m 27.8±1.789.7±2.727.1±1.889.4±3.212m→9m 28.4±2.590.5±3.127.6±2.290.1±3.4204. DiscussionIn this paper, we presented a metamorphic GAN that can be trained topredict infant brain MRI from one time point to another. Longitudinal predic-tion of infant brain MRI is challenging owing to rapid contrast and structuralchanges in the first year of life. To capture these changes, our MGAN incor-porates a SFT block and integrates quality-guided learning via a hybrid lossfunction.We compared our method with existing generative adversarial networks, suchas CycleGAN, Pix2Pix, and WGAN. We found that these networks are effectivein prediction structures at a global scale but are less effective in predictingfine-scale structural details, especially in the cortex (Fig. 6). In contrast, ourprediction network capture spatially heterogeneous changes by employing bothspatial and frequency transforms to generate feature maps. Particularly, DWT-based frequency transform decomposes the image into low and high frequencycomponents to help the translation of image contrast and subtle details (Fig. 7).The quality-guided learning strategy involves using an estimation map forcharacterizing voxel-wise prediction quality. Fig. 9 shows that regions withcomplex structures, e.g., the cerebral cortex, are associated with higher biasvalues. In contrast, regions with simple structure, e.g., lateral ventricles, areassociated with lower bias values. As shown in Fig. 7, employing the quality-driven LQ loss results in more accurate predictions at challenging regions withcomplex patterns. Additionally, the wavelet decomposition and gram matrixenhance the similarity between predictions and ground truths both in terms ofcontent and style (Table 3).5. ConclusionWe have proposed a trustworthy learning-based framework for longitudinalpostnatal brain MRI prediction. The key feature our method is the utilizationof wavelet transform to enable image prediction at multiple frequencies. Weutilize quality guidance to strengthen the learning of prediction of challenging21regions. We employ a hybrid loss function and a multi-scale discriminator tocapture differences in global intensity, style, and structure. Experimental resultsdemonstrate that our method achieves superior performance over several state-of-the-art image-to-image translation networks. Despite the effectiveness of ourmethod, it is currently trained with paired data. In future, it can be extendedto be trainable with unpaired data.AcknowledgmentsThis work was supported in part by United States National Institutes ofHealth (NIH) grants EB008374, EB006733, and AG053867. Y. Huang was sup-ported by the China Scholarship Council and the National Natural ScienceFoundation of China under Grant 6210011424.References[1] J. Dubois, M. Benders, A. Cachia, F. Lazeyras, R. Ha-Vinh Leuchter, S. V.Sizonenko, C. Borradori-Tolsa, J.-F. Mangin, P. S. H¨uppi, Mapping theearly cortical folding process in the preterm newborn brain, Cerebral Cortex18 (6) (2008) 1444 – 1454.[2] B. R. Howell, M. A. Styner, W. Gao, P.-T. Yap, L. Wang, K. Baluyot,E. Yacoub, G. Chen, T. Potts, A. Salzwedel, G. Li, J. H. Gilmore, J. Piven,J. K. Smith, D. Shen, K. Ugurbil, H. Zhu, W. Lin, J. T. Elison, TheUNC/UMN baby connectome project (BCP): An overview of the studydesign and protocol development, NeuroImage 185 (2019) 891 – 905.doi:https://doi.org/10.1016/j.neuroimage.2018.03.049.URLhttp://www.sciencedirect.com/science/article/pii/S1053811918302593[3] J. H. Gilmore, R. C. Knickmeyer, W. Gao, Imaging structural and func-tional brain development in early childhood, Nature Reviews Neuroscience19 (3) (2018) 123.22[4] J. Matsuzawa, M. Matsui, T. Konishi, K. Noguchi, R. C. Gur, W. Bilker,T. Miyawaki, Age-related volumetric changes of brain gray and white mat-ter in healthy infants and children, Cerebral Cortex 11 (4) (2001) 335 –342. doi:https://doi.org/10.1093/cercor/11.4.335.[5] R. C. Knickmeyer, S. Gouttard, C. Kang, D. Evans, K. Wilber, J. K. Smith,R. M. Hamer, W. Lin, G. Gerig, J. H. Gilmore, A structural MRI study ofhuman brain development from birth to 2 years, Journal of Neuroscience28 (47) (2008) 12176 – 12182.[6] T. Paus, D. Collins, A. Evans, G. Leonard, B. Pike, A. Zijdenbos, Matura-tion of white matter in the human brain: a review of magnetic resonancestudies, Brain research bulletin 54 (3) (2001) 255 – 266.[7] P. Isola, J.-Y. Zhu, T. Zhou, A. A. Efros, Image-to-image translation withconditional adversarial networks, 2017 IEEE Conference on Computer Vi-sion and Pattern Recognition (CVPR) (2016) 5967 – 5976.[8] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image transla-tion using cycle-consistent adversarial networks, 2017 IEEE InternationalConference on Computer Vision (ICCV) (2017) 2242 – 2251.[9] X. Huang, M.-Y. Liu, S. J. Belongie, J. Kautz, Multimodal unsupervisedimage-to-image translation, in: ECCV, 2018.[10] M.-Y. Liu, T. Breuel, J. Kautz, Unsupervised image-to-image translationnetworks, in: NIPS, 2017.[11] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,S. Ozair, A. C. Courville, Y. Bengio, Generative adversarial networks,ArXiv abs/1406.2661.[12] D. Nie, R. Trullo, J. Lian, C. Petitjean, S. Ruan, Q. Wang, D. Shen, Med-ical image synthesis with context-aware generative adversarial networks,in: International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, 2017, pp. 417 – 425.23[13] D. Nie, D. Shen, Adversarial confidence learning for medical image segmen-tation and synthesis, International Journal of Computer Vision (2020) 1 –20.[14] K. Lee, M.-K. Choi, H. Jung, DavinciGAN: Unpaired surgical instrumenttranslation for data augmentation, in: MIDL, 2018.[15] K. Armanious, C. Yang, M. Fischer, T. K¨ustner, K. Nikolaou, S. Gatidis,B. Yang, MedGAN: Medical image translation using GANs, Computerizedmedical imaging and graphics : the official journal of the ComputerizedMedical Imaging Society 79 (2019) 101684.[16] Z. Zhang, L. Yang, Y. Zheng, Translating and segmenting multimodalmedical volumes with cycle- and shape-consistency generative adversar-ial network, 2018 IEEE/CVF Conference on Computer Vision and PatternRecognition (2018) 9242 – 9251.[17] A. Chartsias, T. Joyce, R. Dharmakumar, S. A. Tsaftaris, Ad-versarialimage synthesis for unpaired multi-modal cardiac data,in:SASHIMI@MICCAI, 2017.[18] D. Nie, R. Trullo, J. Lian, L. Wang, C. Petitjean, S. Ruan, Q. Wang,D. Shen, Medical image synthesis with deep convolutional adversarial net-works, IEEE Transactions on Biomedical Engineering 65 (12) (2018) 2720– 2730.[19] Y. Hiasa, Y. Otake, M. Takao, T. Matsuoka, K. Takashima, A. Carass,J. L. Prince, N. Sugano, Y. Sato, Cross-modality image synthesis fromunpaired data using CycleGAN, in: International workshop on simulationand synthesis in medical imaging, Springer, 2018, pp. 31 – 41.[20] H. Yang, J. Sun, A. Carass, C. Zhao, J. Lee, Z. Xu, J. L. Prince, Unpairedbrain MR-to-CT synthesis using a structure-constrained CycleGAN, in:DLMIA/ML-CDS@MICCAI, 2018.24[21] D. Ulyanov, A. Vedaldi, V. S. Lempitsky, Instance normalization: Themissing ingredient for fast stylization, ArXiv abs/1607.08022.[22] X. Glorot, A. Bordes, Y. Bengio, Deep sparse rectifier neural networks, in:AISTATS, 2011.[23] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recogni-tion, 2016 IEEE Conference on Computer Vision and Pattern Recognition(CVPR) (2016) 770 – 778.[24] S. Xie, Z. Tu, Holistically-nested edge detection, International Journal ofComputer Vision 125 (2015) 3–18.[25] A. Der Kiureghian, O. Ditlevsen, Aleatory or epistemic? does it matter?,Structural safety 31 (2) (2009) 105–112.[26] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu,M. Ghavamzadeh, P. Fieguth, A. Khosravi, U. R. Acharya, V. Makarenkov,et al., A review of uncertainty quantification in deep learning: Techniques,applications and challenges, arXiv preprint arXiv:2011.06225.[27] A. Kendall, V. Badrinarayanan, R. Cipolla, Bayesian segnet: Model uncer-tainty in deep convolutional encoder-decoder architectures for scene under-standing, arXiv preprint arXiv:1511.02680.[28] Y. Gal, Z. Ghahramani, Dropout as a bayesian approximation: Represent-ing model uncertainty in deep learning, in:international conference onmachine learning, PMLR, 2016, pp. 1050–1059.[29] M. S. Ayhan, P. Berens, Test-time data augmentation for estimation ofheteroscedastic aleatoric uncertainty in deep neural networks.[30] G. Wang, W. Li, M. Aertsen, J. Deprest, S. Ourselin, T. Vercauteren,Aleatoric uncertainty estimation with test-time augmentation for medicalimage segmentation with convolutional neural networks, Neurocomputing338 (2019) 34–45.25[31] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, B. Catanzaro, High-resolution image synthesis and semantic manipulation with conditionalGANs, in: Proceedings of the IEEE conference on computer vision andpattern recognition, 2018, pp. 8798 – 8807.[32] A. Karnewar, O. Wang, MSG-GAN: Multi-scale gradients for generativeadversarial networks, in: Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2020, pp. 7799 – 7808.[33] L. Gatys, A. S. Ecker, M. Bethge, Texture synthesis using convolutionalneural networks, in: Advances in neural information processing systems,2015, pp. 262 – 270.[34] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, M.-H. Yang, Diversified texturesynthesis with feed-forward networks, in: Proceedings of the IEEE Con-ference on Computer Vision and Pattern Recognition, 2017, pp. 3920 –3928.[35] A. Cohen, Biorthogonal bases of compactly supported wavelets, 2006.[36] R. Szewczyk, K. Grabowski, M. Napieralska, W. Sankowski, M. Zubert,A. Napieralski, A reliable iris recognition algorithm based on reversebiorthogonal wavelet transform, Pattern Recognition Letters 33 (8) (2012)1019 – 1026.[37] Y. Dai, F. Shi, L. Wang, G. Wu, D. Shen, iBEAT: a toolbox for infant brainmagnetic resonance image processing, Neuroinformatics 11 (2) (2013) 211– 225.[38] G. Li, J. Nie, L. Wang, F. Shi, W. Lin, J. H. Gilmore, D. Shen, Mappingregion-specific longitudinal cortical surface expansion from birth to 2 yearsof age, Cerebral cortex 23 (11) (2013) 2724 – 2733.[39] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,26S. Moore, D. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,X. Zhang, TensorFlow: A system for large-scale machine learning.[40] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, CoRRabs/1412.6980.[41] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli, Image quality as-sessment: from error visibility to structural similarity, IEEE Transactionson Image Processing 13 (2004) 600 – 612.[42] M. Arjovsky, S. Chintala, L. Bottou, Wasserstein generative adversarialnetworks, in: ICML, 2017.27