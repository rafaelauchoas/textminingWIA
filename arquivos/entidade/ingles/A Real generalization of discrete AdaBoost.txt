Artificial Intelligence 171 (2007) 25–41www.elsevier.com/locate/artintA Real generalization of discrete AdaBoost ✩Richard Nock a,∗, Frank Nielsen ba Université des Antilles-Guyane, UFR DSE—Ceregmia, Campus de Schoelcher, BP 7209, 97275 Schoelcher, Martinique, Franceb SONY CS Labs (FRL), 3-14-13 Higashi Gotanda, Shinagawa-Ku, Tokyo 141-0022, JapanReceived 1 June 2006; received in revised form 16 October 2006; accepted 16 October 2006Available online 21 November 2006AbstractScaling discrete AdaBoost to handle real-valued weak hypotheses has often been done under the auspices of convex optimization,but little is generally known from the original boosting model standpoint. We introduce a novel generalization of discrete AdaBoostwhich departs from this mainstream of algorithms. From the theoretical standpoint, it formally displays the original boostingproperty, as it brings fast improvements of the accuracy of a weak learner up to arbitrary high levels; furthermore, it bringsinteresting computational and numerical improvements that make it significantly easier to handle “as is”. Conceptually speaking,it provides a new and appealing scaling to R of some well known facts about discrete (ada)boosting. Perhaps the most popularis an iterative weight modification mechanism, according to which examples have their weights decreased iff they receive theright class by the current discrete weak hypothesis. In our generalization, this property does not hold anymore, as examples thatreceive the right class can still be reweighted higher with real-valued weak hypotheses. From the experimental standpoint, ourgeneralization displays the ability to produce low error formulas with particular cumulative margin distribution graphs, and itprovides a nice handling of those noisy domains that represent Achilles’ heel for common Adaptive Boosting algorithms.© 2006 Elsevier B.V. All rights reserved.Keywords: AdaBoost; Boosting; Ensemble learning1. IntroductionIn supervised learning, it is hard to exaggerate the importance of boosting algorithms. Loosely speaking, a boostingalgorithm repeatedly trains a moderately accurate learner, gets its weak hypotheses, combines them, to finally outputa strong classifier which boosts the accuracy up to arbitrary high levels [14,15]. (Discrete) Adaboost, undoubtfullythe most popular provable boosting algorithm [7], uses weak hypotheses with outputs restricted to the discrete setof classes that it combines via leveraging coefficients in a linear vote. Strong theoretical issues have motivated theextension of this discrete AdaBoost [8] to handle real-valued weak hypotheses as well [8,17,26,29]. Even when onlyfew of them are true generalizations of discrete AdaBoost [17,29], virtually all share a strong background in convex✩ Extends the paper from the same name that was awarded the Best Paper Award at the 17th European Conference on Artificial Intelligence(2006).* Corresponding author. Fax: (+596) 596 72 74 03.E-mail addresses: Richard.Nock@martinique.univ-ag.fr (R. Nock), Nielsen@csl.sony.co.jp (F. Nielsen).URLs: http://www.univ-ag.fr/~rnock (R. Nock), http://www.csl.sony.co.jp/person/nielsen/ (F. Nielsen).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.10.01426R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–41optimization originally rooted in a “key” to boosting in AdaBoost: a strictly convex exponential loss integrated intoa weight update rule for the examples, loss which upperbounds the error and approximates the expected binomiallog-likelihood. However, very little is often known for these algorithms from the seminal boosting model standpoint[14,15,27], a model which roughly requires convergence to reduced true risk under very weak assumptions (with highprobability).In this paper, we propose a new real AdaBoost, a generalization of discrete AdaBoost that handles arbitrary real-valued weak hypotheses. With respect to former real AdaBoosts, the weight update is fundamentally different as itdoes not integrate anymore the convex exponential loss; also, the leveraging coefficients for the weak hypothesesdiffer in the output; finally, these leveraging coefficients are given in closed form and their computation can noweasily be delayed until the end of boosting, which is not the case for conventional real AdaBoosts [8,17,29]. The majortheoretical key feature of this algorithm is that it is a provable boosting algorithm in the original sense. Another point isthat it saves computation time with respect to previous generalizations of discrete AdaBoost, that need to approximatethe solution of a convex minimization problem at each boosting iteration [17,29]. From the experimental standpoint,the weight update rule, which does not require anymore the approximation of logarithms or exponentials, is less proneto numerical errors. Finally, it prevents or reduces some numerical instabilities that previous generalizations [17,29]face when the weak hypotheses reach perfect, or perfectly wrong, classification. This might explain why experimentsclearly display that our algorithm handles noise more efficiently than discrete or real AdaBoosts. Noise handling hassoon be described as AdaBoost’s potential main problem, see [2].As a matter of fact, it is quite interesting that our algorithm is indeed a generalization of discrete AdaBoost, as whenthe weak hypotheses have outputs constrained to the set of classes, both algorithms coincide. From this standpoint,our paper also brings a relevant conceptual contribution to boosting. Indeed, we give a complete generalization to R ofpopular (discrete) boosting properties, and this is sometimes clearly not trivial. For example, discrete AdaBoost is veryoften presented as an algorithm that reweights lower the examples that have received the right class. Scaled to R, thisis not true anymore. Roughly speaking, provided a so-called Weak Learning Assumption holds (which states that theclassifier is slightly different from random), lower reweighting occurs only for examples that receive the right class,and on which a measure of the classifier’s confidence exceeds a measure of its average confidence (over all examples,known as a margin). Only on the discrete prediction framework do these two properties coincide. Furthermore, thisscaling property does not hold for previous real AdaBoosts [8,17,26,29].Section 2 presents some definitions, followed by a section on our generalization of discrete AdaBoost. Section 4presents and discusses experimental results, and a last section concludes the paper.2. Definitions and related workOur framework is rooted into the original weak/strong learning and boosting frameworks, and Valiant’s PAC (Prob-ably Approximately Correct) model of learnability [7,15,30]. We have access to a domain X of observations, whichcould be {0, 1}n, Rn, etc. Here, n is the number of description variables. More precisely, we collect examples, that is,couples (observation, class) written (x, y) ∈ X × {−1, +1}. “+1” is called the positive class (or label), and “−1” thenegative class. In this paper, we deal only with the two-classes case. Well known transformations exist that allow itsextension to multiclass, multilabel frameworks [29]. In this paper, boldfaces such as x denote n-dimensional vectors,calligraphic faces such as X denote sets and blackboard faces such as S denote subsets of R, the set of real numbers.Unless explicitely stated, sets are enumerated following their lower-case, such as {xi: i = 1, 2, . . .} for vector sets,and {xi: i = 1, 2, . . .} for other sets (and for vector entries). We make the assumption that examples are sampled inde-pendently, following an unknown but fixed distribution D over X × {−1, +1}. Our objective is to induce a classifieror hypothesis H : X → R, that matches the best possible the examples drawn according to D.For this objective, we define a strong learner as an algorithm which is given two parameters 0 < ε, δ < 1, samplesaccording to D a set S of m examples, and returns a classifier or hypothesis H : X → R such that with probability(cid:2) 1 − δ, its true risk (cid:4)D,H is bounded as follows:(cid:4)(cid:3)H (x)(1)Here, sign(a) is +1 iff a (cid:2) 0, and −1 otherwise. The time complexity of the algorithm is required to be polynomial inrelevant parameters, among which 1/ε, 1/δ, n. To be rigorous, the original models [15,30] also mention dependenceson concepts that label the examples. Examples are indeed supposed to be labeled by a so-called target concept,= (cid:4)D,H (cid:3) ε.Pr(x,y)∼D(cid:2)sign(cid:6)= y(cid:5)R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–4127Input: sample S = {(xi , yi ), xi ∈ X , yi ∈ {−1, +1}}w1 ← u;for t = 1, 2, . . . , T doGet (ht : X → S) ← WL(S, wt );Find αt ∈ R;Update: ∀1 (cid:3) i (cid:3) m,wt+1,i ← wt,i × exp(−αt yi ht (xi ))/Zt ; (2)endOutput: HT (x) =(cid:6)Tt=1 αt ht (x)Fig. 1. An abstraction of AdaBoost.which is unknown but fixed. Distribution D is in fact used to retrieve the examples from this target concept, andthe time complexity of the algorithm is also required to be polynomial in its size. Hereafter, we shall omit for thesake of clarity this notion of target concept, which is not important for our purpose, since our analysis may also befit to handle it as well. A weak learner (WL) has basically the same constraints, with two notable exceptions: (i) theweak hypotheses it delivers have outputs that can be restricted to a subset S ⊆ R, and (ii) (1) is only required tohold with ε = 1/2 − γ for some γ > 0 a constant or inverse polynomial in relevant parameters (this still has tobe verified regardless of D). Since predicting the classes at random, such as with an unbiased coin, would yieldPr(x,y)∼D[sign(random(x)) (cid:6)= y] = 1/2, ∀D, it comes that a weak learner is only required to perform slightly betterthan random prediction. In the original models, it is even assumed that δ is also an inverse polynomial in relevantparameters, which makes that the constraints on WL are somehow the lightest possible from both the statistical and thecomputational standpoints. The (discrete) Weak Learning Assumption (WLA) assumes the existence of WL [14,27].Simple simulation arguments of WL [16] allow to show that the weakening on δ is superficial, as we can in fact weaklearn with the same arbitrary δ as for strong learning. However, the conditions on ε are dramatically different, andthe question of whether weak and strong learning are equivalent models has been a tantalizing problem until the firstproof that there exists boosting algorithms that strong learn under the sole access to WL [27], thus proving that thesemodels are indeed equivalent. This first boosting algorithm outputs a large tree-shaped classifier with majority votesat the nodes, each node being built with the help of WL. The point is that it is not easy to implement, and it does notyield classifiers that correspond to familiar concept representations.AdaBoost [7] has pioneered the field of easily implementable boosting algorithms, for which S = {−1, +1}. After[7], we refer to it as discrete AdaBoost (see Fig. 1 for an abstraction of the algorithm). Basically, AdaBoost usesa weak learner as a subprocedure and an initial distribution u over S, generally uniform, which is repeatedly skewedtowards the hardest to classify examples. After T rounds of boosting, its output, HT , is a linear combination of theweak hypotheses. Below, we give a useful abstraction of AdaBoost, in which Zt is the normalization coefficient,the elements of S are enumerated si = (xi, yi) and their successive weight vector is noted wt , for t (cid:2) 1. In discreteAdaboost, we would have:1 − (cid:4)wt ,ht(cid:4)wt ,htαt = 12(3)ln,(cid:4)expw1,HT= E(x,y)∼w1where “ln” denotes the natural logarithm. The two key steps in AdaBoost are the choice of αt and the weight up-date rule (see Fig. 1). They are strongly related and follow naturally if we seek to minimize the following observedexponential loss [8,29] through the induction of HT :(cid:3)(cid:3)−yHT (x)exp(4)with E. the mathematical expectation. Since I [sign(HT (x)) (cid:6)= y] (cid:3) exp(−yHT (x)) (with I the indicator function),(cid:4)w1,HT, and so minimizing the exponential loss amounts to minimizing the empirical risk as well [8,17,26,28], and it turns out that it brings a boosting algorithm as well [7,28]. There are other excellent reasons to focuson the exponential loss instead of the empirical risk: it is smooth differentiable and it approximates the binomiallog-likelihood [8]. Its stagewise minimization brings both the weight update in (2), and the following choice for αt :(cid:3) (cid:4)exp(cid:4)(cid:4),w1,HTαt = arg minα∈RE(x,y)∼wt(cid:3)(cid:3)−αyht (x)exp(cid:4)(cid:4)= arg minα∈RZt .(5)28R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–41One more reason, and not the least, to focus on the exponential loss, is that it brings a stagewise maximization ofmargins. While the empirical risk focuses only on a binary classification task (the class assigned is either good orbad), margins scale it to real classification, as they integrate both the binary classification task (sign) and a realmagnitude which quantifies a “confidence” in the label given. Large margin classification can bring very fast truerisk minimization [28]. Margins justify to scale S = {−1, +1} to an interval such as [−1, +1] [8,29]; in this case, thesign of the output gives the class predicted. Whenever we still enforce ht : X → {−1, +1}, (5) admits a closed-formsolution, which is naturally (3), and the boosting algorithm is discrete AdaBoost [7,8].In many domains, real-valued classification with AdaBoost encompasses by far the concept itself. One of the mostimportant and challenging application field for ensemble classifiers is vision [32,33], a domain in which it is easyto obtain weak classifiers via simple features. The task requires however both fast and accurate combinations, whichis everything but simple. In these pioneering papers, the authors have chosen to use AdaBoost, which brings theaccuracy of the combination. To satisfy the condition of fast processing, the authors consider simple features for weakclassifiers, and they choose the discrete version of AdaBoost [7], which makes it necessary to discretize the real valuesof features by thresholding, and thus eventually loses some useful information (it costs also a little time to computethe thresholds).Relaxing S to [−1, +1] can still be handled by algorithm AdaBoost (Fig. 1) and (5), but (5) does not have a closed-form solution in this case [29]. The algorithm obtained is called real AdaBoost, and can be found in [8,17,29]; itis popular, as demonstrated by many applications in language or image processing [11,21,35]. Iterative techniquesexist for its fast approximation [24], but they have to be performed at each boosting iteration (which buys overalla significant load increase), and it may be the case that the solution found lies outside the boosting regime if thenumber of approximation steps is too small. Approximations exist to (5), but they do not necessarily yield a validgeneralization of discrete AdaBoost [12,24]. For the purpose of fast processing, some authors have devised variousad-hoc approximations of real AdaBoost, but almost all of them are not known to be formal boosting algorithms: forexample, Huang et al. [9] use a version of real AdaBoost in which αt = 1, and the update rule is adapted from discreteAdaBoost (they roughly do the same in [10]). Friedman et al. [8], Ridgeway [26] also pick αt = 1. Ridgeway [26]further proposes to leverage the vote by arbitrary values λ ∈ (0, 1] to dampen the variation of the leveraging coeffi-cients. Other authors have devised modifications of discrete AdaBoost to integrate class-dependent misclassificationcosts, that can be viewed as lifting discrete AdaBoost to handle (few) real values [6,19]. Finally, many papers havemodified AdaBoost, to optimize other losses; many of them are rooted into the maximization of the expected binomiallog-likelihood [8,18] instead of just the exponential loss (4).3. Our Real generalization of AdaBoostWe now give up with the direct minimization of (4), and scale S = {−1, +1} up to any subset of R itself. Thismeans that the weak classifiers can even be authorized to output values outside interval [−1, +1]. Suppose we replacethe weight update (2) and Eq. (5) by what follows:(cid:7)(cid:8),1 − (μt yiht (xi)/ h(cid:8)t )1 − μ2t.wt+1,i ← wt,i ×αt = 12h(cid:8)tln1 + μt1 − μtHere, we have fixed h(cid:8)t= max1(cid:2)i(cid:2)m |ht (xi)| ∈ R, the maximal value of ht over S, and:μt = 1h(cid:8)tm(cid:9)i=1wt,iyiht (xi) ∈ [−1, +1]t < ∞. Infinite values for h(cid:8)the normalized margin of ht over S. For the sake of clarity, we suppose in the following subsection that ∀1 (cid:3) t (cid:3)T , h(cid:8)t can be handled in two ways: either we bias/threshold the output of ht to make itfinite [29], or we code it as ∞, which yields αt = 0 and μt =i:|ht (xi )|=∞ wt,isign(yiht (xi)). In both cases, wt+1is a valid distribution and the properties below are kept. Let us call AdaBoostR our real generalization of discreteAdaBoost. Notice that (6) and (7) can be retrieved from AdaBoost via the following gentle approximations: (i) approx-imate the leveraging coefficient as αt ≈ μt (first order approximation to the logarithm in (7)), and (ii) approximate the(cid:6)(6)(7)(8)R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–4129exponential update rule via a first-order approximation of the exponential, exp(z) ≈ 1 + z. These two approximationsare typically not the ones carried out for the implementations of real AdaBoost, as they usually prefer to keep the ex-ponential update rule (AdaBoost’s main “trademark”), and the eventual approximations of the leveraging coefficientsapproximate the solution of (5) via logarithmic quantities [24] (they do not carry out “one more approximation”).3.1. Basic propertiesWe first show that AdaBoostR is indeed a generalization of discrete AdaBoost.Lemma 1. When S = {−1, +1}, AdaBoostR = discrete Adaboost.Proof. In this case, we have h(cid:8)t(cid:4)wt ,ht )/(cid:4)wt ,ht ), i.e. like in discrete AdaBoost. Our update rule simplifies to:= 1 and μt = 1 − 2(cid:4)wt ,ht , which brings that Eq. (7) is also αt = (1/2) ln((1 −wt+1,i ← wt,i(1 − yiht (xi) + 2yiht (xi)(cid:4)wt ,ht )2(cid:4)wt ,ht (1 − (cid:4)wt ,ht ),i.e.:(cid:10)wt+1,i ←wt,i/(2(1 − (cid:4)wt ,ht ))wt,i/(2(cid:4)wt ,ht )iffiffyiht (xi) = +1,yiht (xi) = −1.This is the same expression for the weight update of discrete AdaBoost. (cid:2)Now, we show that AdaBoostR is a boosting algorithm for arbitrary real-valued weak hypotheses. In fact, we showa little bit more, and for this objective, we define the margin of HT on example (x, y) as:(cid:4)(cid:3)(x, y)νT= exp(yHT (x)) − 1exp(yHT (x)) + 1∈ [−1, +1].(9)This definition of margin extends a previous one for discrete AdaBoost [34], and its choice is discussed in Section 3.3.∀θ ∈ [−1, +1] we also define the classifier’s “margin error” as the proportion of examples whose margin does notexceed θ (see also [28]):νu,HT ,θ =m(cid:9)i=1(cid:2)νT(cid:3)(cid:4)(xi, yi)(cid:5).(cid:3) θuiI(10)= νu,HT ,0, and νu,HT ,θWhenever no example has zero margin (i.e. HT predicts a label for all examples), (cid:4)u,HTgeneralizes (cid:4)u,HT . Ties are extremely seldom, but even when they occur, νu,HT ,0 is still an upperbound for (cid:4)u,HT .We let H ∗ denote some real-valued prediction that matches the empirical Bayes rule, computed over S: for anyobservation x ∈ S, the sign of H ∗(x) would be the majority class over all examples of S whose observation matches x.We now prove a first theorem on AdaBoostR.Theorem 1. ∀S ⊆ R, ∀θ ∈ [−1, +1], after T (cid:2) 1 iterations, we have:(cid:7)(cid:8)(cid:11)(cid:13)(cid:12)(cid:10)νu,HT ,θ (cid:3) (cid:4)u,H ∗ + max1,1 + θ1 − θ− 12T(cid:9)t=1× expμ2t.(11)Proof. We need the following simple lemma.Lemma 2. ∀a ∈ [−1, 1], ∀b ∈ [−1, 1], 1 − ab (cid:2)√1 − a2 exp(− b2 ln 1+a1−a ).Proof. The function in the right-hand side is strictly convex in b for a (cid:6)= 0, and both functions match for b = ±1 anda = 0. Writing the right-hand side (1 + a)(1−b)/2(1 − a)(1+b)/2 implies that its limits when a → ±1 are zero. (cid:2)30R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–41Consider some example (xi, yi) ∈ S and some 1 (cid:3) t (cid:3) T . The way we use Lemma 2 is simple: fix a = μt andb = yiht (xi)/ h(cid:8)t . They satisfy the assumptions of the lemma, and we obtain:(cid:3)μt yiht (xi)/ h(cid:8)t(cid:4)(cid:2)1 −(cid:14)(cid:7)1 − μ2t exp− yiht (xi)2h(cid:8)tln1 + μt1 − μt(cid:8).Unraveling the weight update rule, we obtain:wT +1,i ×T(cid:15)(1 − μ2t ) = ui ×t=1(cid:3)1 −(cid:3)μt yiht (xi)/ h(cid:8)t(cid:4)(cid:4).T(cid:15)t=1Using T times (12) on the right-hand side of (13) and simplifying yields:(wT +1,i/ui) ×(cid:14)T(cid:15)t=11 − μ2t(cid:4)(cid:3)−yiHT (xi)(cid:2) exp.(12)(13)(14)We let u∗x (resp. u◦For any (xi, yi) ∈ S, we have 1 = I [νT ((xi, yi)) (cid:3) θ ] + I [νT ((xi, yi)) (cid:2) −θ ] − I [−θ (cid:3) νT ((xi, yi)) (cid:3) θ ], ∀θ (cid:2) 0,and 1 = I [νT ((xi, yi)) (cid:3) θ ] + I [νT ((xi, yi)) (cid:2) −θ ] + I [θ < νT ((xi, yi)) < −θ ], ∀θ < 0.class is (resp. is not) the one chosen by the empirical Bayes rule. We also let y∗S∗ ⊆ S to be the set that contains, for each observation x present in S, exactly one example (x, y∗∀(x, y∗x ) denote the total weight in u for the examples of S whose observation matches x, and whosex denote this Bayes class. Finally, we letx). Suppose θ (cid:2) 0.x) ∈ S∗, since νT ((xi, yi)) = −νT ((xi, −yi)), ∀1 (cid:3) i (cid:3) m, we obtain:m(cid:9)(cid:5)(cid:4)(cid:3)(xi, yi)(cid:2)uiI [xi = x]IνT(cid:4)(cid:3)∗(x, yx)(cid:3)(cid:4)∗(x, yx)(cid:3)(cid:2)(x, yνT(15)x)) (cid:3) θ ] (cid:3) I [νT ((xi, yi)) (cid:3) θ ]. It is easy to show that inequality (15) also holds when θ < 0,(cid:2)= IνT(cid:2)= IνT◦(cid:3) u+ Ixsince I [−θ (cid:3) νT ((x, y∗even when the inequality becomes much looser in this case. We obtain with (15):(cid:3) θ(cid:2)∗+ I× uνTx◦∗− u× (ux) +x(cid:5)∗× u(cid:3) θx,(cid:4)(cid:3)∗(x, yx)(cid:3)1 + I(cid:2) −θ(cid:2)−θ (cid:3) νT◦× ux(cid:3)(x, y(cid:3) θ(cid:3) θ(cid:4)∗x)◦× ux(cid:4)∗x)(cid:3) θi=1(cid:5)(cid:4)(cid:5)(cid:5)(cid:5)(cid:2)νT(cid:4)(cid:3)(xi, yi)uiI(cid:3) θ(cid:5)m(cid:9)i=1m(cid:9)(cid:9)uiI [xi = x]I(cid:2)νT(cid:3)(cid:4)(xi, yi)(cid:3) θi=1(x,y∗(cid:9)x )∈S∗m(cid:9)uiI [xi = x]I(cid:2)νT(cid:3)(cid:4)(xi, yi)(cid:3) θ(cid:5)(cid:5)==x )∈S∗(x,y∗(cid:3) (cid:4)u,H ∗ +i=1(cid:9)∗(ux◦+ ux)ρxI(cid:2)νT(cid:3)(x, y(cid:4)∗x)(cid:5),(cid:3) θx/(u∗with ρx = u∗(cid:2)(cid:3)(x, yIνTx(x,y∗+ u◦(cid:4)∗x)x )∈S∗x). Consider some example (x, y∗1 + θ1 − θ(cid:8)(cid:11) m(cid:9)(cid:3) max(cid:3) θ1,(cid:7)(cid:10)(cid:5)i=1x) ∈ S∗. Then, we show:uiI [xi = x] exp(−yiHT (xi))u∗x+ u◦x(16)(17).If I [νT ((x, y∗1, i.e. exp(−y∗m(cid:9)x)) (cid:3) θ ] = 0, (17) is true since the right-hand side cannot be negative. So, suppose I [νT ((x, y∗xHT (x)) (cid:2) (1 − θ )/(1 + θ ). Fix for short z = −y∗(cid:4)(cid:3)−yiHT (xi)uiI [xi = x] expxHT (x). We have:x)) (cid:3) θ ] =i=1◦∗x exp(−z)x exp(z) + u= u(cid:4)(cid:3)exp(z) + exp(−z)◦◦x) exp(z) + u− ux∗= (ux.R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–4131Fig. 2. Graphical representation of Theorem 1 (with (cid:4)u,H ∗ = 0): the bold curve upperbounds the empirical margin distribution graph (located insidethe dotted area). As t increases, provided each |μt | is not too small, this bold curve converges (bold arrows) towards the step function I [θ = 1](bold dashed curve), which means that all examples receive the right class with infinite confidence (see text for details).If θ (cid:2) 0, exp(z) + exp(−z) (cid:2) 2 (cid:2) 2(1 − θ )/(1 + θ ), and we immediately obtain (u∗xexp(−z)) (cid:2) (u∗+ u◦xand we obtain (u∗x(exp(z) + exp(−z)) (cid:2) u∗x) exp(z) + u◦xxThere remains to plug (17) in (16), and then use (14), to obtain:x(exp(z) +x) × (1 − θ )/(1 + θ ), from which the right-hand side of (17) is (cid:2) 1. Now, if θ < 0, exp(z) > 1,+ u◦− u◦x , and again the right-hand side of (17) is (cid:2) 1.x) exp(z) + u◦− u◦m(cid:9)(cid:2)νT(cid:3)(cid:4)(xi, yi)uiI(cid:3) θ(cid:5)i=1(cid:10)(cid:3) (cid:4)u,H ∗ + max1,(cid:7)1 + θ1 − θ(cid:10)(cid:3) (cid:4)u,H ∗ + max1,(cid:7)1 + θ1 − θ(cid:8)(cid:11) (cid:9)(x,y∗(cid:8)(cid:11) T(cid:15)x )∈S∗(cid:14)uiI [xi = x] exp(cid:4)(cid:3)−yiHT (xi)ρxm(cid:9)i=11 − μ2t(cid:9)m(cid:9)ρxwT +1,iI [xi = x].x )∈S∗The two last sums are an expectation of ρ(.) computed using distribution wT +1; since ρ(.) (cid:3) 1, this expectation is (cid:3) 1.√The statement of the theorem follows after remarking that1 − a2 (cid:3) exp(−a2/2), ∀a ∈ [−1, 1]. (cid:2)(x,y∗t=1i=1Fig. 2 gives a visual interpretation of Theorem 1, when (cid:4)u,H ∗ = 0: provided each normalized margin is not toosmall in absolute value, there is a fast convergence of the empirical margin distribution graph towards right classi-fication with infinite confidence for all examples. When the empirical Bayes rule has (cid:4)u,H ∗ > 0 this convergence isestablished towards the step function (cid:4)u,H ∗ + I [θ = 1](1 − (cid:4)u,H ∗): all examples still receive infinite confidence intheir classification, even those receiving the wrong label.3.2. AdaBoostR boosts labels and confidencesTheorem 1 generalizes a well-known convergence theorem for AdaBoost’s empirical risk [29] (θ = 0). This gener-alization is important, as it says that virtually any margin error is subject to the same convergence rate towards zero,and not simply the empirical risk. Thus, more than a single point, it gives also a complete curve f (θ ) upperboundingthe (empirical) margin distribution graph, which plots the margin error (10) as a function of θ ∈ [−1, 1] [28]. To provethat AdaBoostR is a boosting algorithm, we need a WLA that a real-valued WL should satisfy. Its formulation forreal-valued hypotheses follows that for the discrete case [14,15,29]: basically, it amounts to say that we want ht toperform significantly different from random, a case which can be represented by μt = 0. A natural choice is thusfixing the WLA to be (∀t (cid:2) 1):(real)WLA|μt | (cid:2) γ , for the same γ > 0 as in the discrete WLA.32R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–41This, in addition, provides us with a generalization of the discrete WLA [14,27], since we have in this case μt =(cid:2) (1/2) + γ /2. It has been previously remarked1 − 2(cid:4)wt ,ht . This brings that either (cid:4)wt ,htthat this second condition, although surprising at first glance since the empirical risk is worse than random, is in factequivalent to the first from the boosting standpoint, as it “reverses” the polarity of learning: when ht satisfies thesecond constraint, −ht satisfies the first [8].(cid:3) (1/2) − γ /2, or (cid:4)wt ,htNow proving that AdaBoostR is a boosting algorithm amounts first to using Theorem 1 in the conventionalweak/strong learning frameworks, i.e. fix θ = 0 and (cid:4)u,H ∗ = 0, to obtain under the WLA that after T iterations of(cid:3) exp(−T γ 2/2). Thus, if we run AdaBoostR for T = (cid:12)((1/γ 2) ln m) iterations, we getAdaBoostR, we have (cid:4)u,HTan HT consistent with S. Since T is polynomial in all relevant parameters, classical VC-type bounds on the deviationof the true risk for linear separators [16,31] immediately bring the following theorem.Theorem 2. ∀S ⊆ R, provided WLA holds, AdaBoostR is a boosting algorithm.This theorem relies on the use of Theorem 1 with θ = 0, that is, it does not take into account the influence of themargins’ magnitude. We can integrate it in a somewhat stronger boosting-type result, that says that AdaBoostR doesmore than fitting well the classes under the WLA: it also brings large confidences into right classification. Thisamounts to integrate one more parameter (θ ) in the strong learning model as described in (1). Suppose that some−1 < θ (cid:14) < 1 is also given by the user along with ε, δ, and replace the strong learning condition in (1) by this one:Pr[νD,H,θ (cid:14) (cid:3) ε] (cid:2) 1 − δ,(18)and the time complexity of the algorithm has to be polynomial also in 1/(1 − θ (cid:14)). This means that, with high proba-bility, we want to limit the probability that some example drawn according to D has a “small” local margin. From thestandpoint of the true margin distribution graph, when the event νD,H,θ (cid:14) (cid:3) ε is satisfied, the curve is located below thestep function ε + I [θ (cid:2) θ (cid:14)](1 − ε). From Theorem 1, we just have to make T = (cid:12)((1/γ 2) ln(m/(1 − θ (cid:14)))) steps tohave νu,HT ,θ (cid:14) = 0. We also remark that the condition νT ((xi, yi)) > θ (cid:14) is equivalent to stating yi(HT (xi) − yi ln((1 +θ (cid:14))/(1 − θ (cid:14)))) > 0, i.e. the (T + 1)-dimensional linear separator HT (xi) − yi ln((1 + θ (cid:14))/(1 − θ (cid:14))) is consistent with S.There finally remains to use the same arguments as for Theorem 1 to prove that the WLA is also enough to stronglearn in the model described by (18), thus proving a boosting-type result integrating both labels and confidences.3.3. DiscussionPerhaps one of the most important difference with discrete AdaBoost and offsprings [5,7,8,29] lies in the fact thatthey have been early motivated or built around the appealing intuition that reweighting favors the hard examples ondiscrete AdaBoost, and more precisely that examples receiving the right class are reweighted lower. This propertyis appealing, and has certainly participated to their spread and use. However, when scaling the binary classificationproblem (S = {−1, +1}) to R, for this property to fully integrate the extended framework, it should rely entirely onmargins (classes + confidences) instead of just classes. This becomes true with AdaBoostR: lower reweighting occursonly for examples on which the current weak classifier’s “performance” exceeds its average margin (when μt > 0):yiht (xi)/ h(cid:8)t(cid:2) μt .(19)Thus, there can be examples that receive the right class by ht , and yet that have their weights increased. When μt < 0,the polarity of boosting (and reweighting) is reversed in the same way as when (cid:4)wt ,ht > 1/2 for discrete AdaBoost.Finally, these properties are true generalization of discrete AdaBoost’s, as all coincide again on the discrete case.3.3.1. MarginsThe normalized margin of weak hypothesis ht in (8) can also be written as μt = E(x,y)∼wt (νt ((x, y))), with(cid:4)(cid:3)(x, y)νt= y × ht (x)h(cid:8)t∈ [−1, 1](20)R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–4133being the margin of ht on example (x, y). Except from its output domain, this “local” margin might not seem to bearany other similarity with our corresponding definition for the “strong” hypothesis HT , (9). Moreover, (20) is evidentlymuch closer to a previous definition coined for real AdaBoosts in [8,28,29]: here, (9) is replaced by:(cid:4)(cid:3)(x, y)νT= y × HT (x)(cid:6)Tt=1 αt∈ [−1, +1],(21)with the assumption that αt (cid:2) 0, ∀1 (cid:3) t (cid:3) T . In fact, (20) and (21) exactly match when (i) all weak hypotheses haveoutput in {−1, 1}, (ii) all weak hypotheses have empirical risk (cid:3) 1/2 on wt , and (iii) there exists (xi, yi) ∈ S with(cid:6)THT (x) =t=1 αt , i.e. the maximal possible value of HT is realized over S. Thus, the best lifting of (20) to HT mightseem (21) at first glance.However, the outside appearances are misleading. The reason why (9) turns out to be more convenient comes fromthe models approximated by the AdaBoost family. It is indeed known since [8] that various offsprings of AdaBoost(including discrete and real versions) can be viewed as carrying out a direct or approximate additive fitting of thesymmetric logistic transform:HT (x) = lnPr(x,y)∼D[y = +1|x]Pr(x,y)∼D[y = −1|x],(22)with the probabilities Pr(x,y)∼D[.|x] to be estimated while learning (this connection is made crisp in [18]).AdaBoostR can also be viewed as an approximation algorithm of this family. Plugging (22) in (9) yields:(cid:4)(cid:3)∗= y × δ∈ [−1, +1],(x, y)x= 2Pr(x,y)∼D[y = +1|x] − 1.νT∗δx(23)This time, we obtain a local margin similar to (20). This margin has a fundamental property that (21) approximatespoorly with (22): it turns out to be the theoretical local margin of the true Bayes rule (as opposed to the empirical Bayesrule, see Section 3), with real prediction (∈ [−1, 1]) computed as δ∗= Pr(x,y)∼D[y = +1|x] − Pr(x,y)∼D[y = −1|x].xThis real prediction, which leverages Bayes rule to real values, has the remarkable property to be the best possiblein the sense of Bregman divergences. More precisely, Theorem 1 in [1] yields that regardless of the (properly defined)Bregman divergence B(. (cid:15) .) measuring the proximity between a true label y and a real prediction p, we shall alwayshave:∀x ∈ X ,∗δx= arg minp∈RE(x,y)∼ ˜Dx(cid:4)(cid:3)B(y (cid:15) p).Here, ˜Dx is distribution D with support restricted to examples (x, .), normalized with the total weight of these ex-amples. By means of words, δ∗= Pr(x,y)∼D[y = +1|x] − Pr(x,y)∼D[y = −1|x] is the value that best summarizes onxaverage the different classes of observation x [1]. This quantity is also called gentle logistic approximation in [8], andit is reported to be more stable than the full logistic model itself.There are more reasons to prefer (9) over (21). The first reason is more technical. Theorem 1 shows that νu,HT ,θvanishes under the WLA regardless of the value of θ ∈ (−1, 1) with margin definition (9). Upperbounds for νu,HT ,θwith Eq. (21) are not as easy to read, as all would require to vanish that θ be smaller than fluctuating upperboundsthat can be (cid:16) 1 [28,29]. It does not seem that it is the boosting algorithm which is responsible, as in our case, using(21) would not yield a vanishing νu,HT ,θ when θ (cid:2) maxt |μt |/2, a situation identical to previous analyses [7,28,29].The second reason is an experimental consequence of the first. Definition (9) makes cumulative margin distributionseasier to read, since there is no fluctuating theoretical upperbound <1 for “boostable” margins.Following (9), we can fully characterize the margin distribution graph of the true Bayes rule. Indeed, this is just thesum of “local” margin distribution graphs, built for each possible observation x ∈ X . For some observation x, denoteits true Bayes class:y∗x= arg maxb∈{−1,+1}Pr(x,y)∼D[y = b|x].Fig. 3 presents an example of a local margin distribution graph. To better catch the picture, consider a domain with zeroBayes error, which is noisified with η ∈ [0, 1/2] class noise rate: each example gets its class flipped with probability η.In this case, we would have |δ∗x)] in Fig. 3),xwould be located at z = η (see the experiments for examples).| = 1 − 2η, and the intermediate stair (for which z = Pr(x,y)∼D[(x, −y∗34R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–41Fig. 3. Local margin distribution graph for the true Bayes rule and for a single observation x ∈ X (bold curve). The dotted line with equationz = Pr(x,y)∼D[x](θ + 1)/2, depicts the location inside which the negative stair of Bayes rule’s prediction is located as a function of δ∗x (the circle).The dotted line with equation z = Pr(x,y)∼D[x] does the same for the positive stair (the square; see text for details).The reasons why margin distribution graphs, better than just errors, are good tools to evaluate the goodness-of-fitof HT , come again from the models built. In Valiant’s seminal PAC model [30], we only have to use the sign of HT ,eventually leaving its absolute value, if needed, to represent some sort of confidence in the prediction [29]. Thus, usinglabels and counting errors is enough. The logistic framework [8] makes it possible to do more, by integrating sign andconfidence into an estimator of the class conditional probabilities:ˆPr[y = +1|x] = exp(HT (x))1 + exp(HT (x))11 + exp(HT (x))ˆPr[y = −1|x] =,.(24)(25)Thus, when plotting both the margin distribution graph of HT on testing, and that of the true Bayes rule, the goodness-of-fit should be as better as both curves come closer to each other, meaning not only that the labels tend to be thesame, but also that the local class conditional probabilities tend to match. Lifting the usual properties of classifiers inthe classical labels/errors framework, to the logistic framework, is not immediate. Consider for example overfitting.Such a situation typically occurs when a classifier becomes “so” complicated that it starts to model better S at theexpense of the whole domain itself, meaning that its true risk increases while its empirical risk may still decrease. Thissituation has an impact on class conditional probabilities as well, but it remains “punctual”, as for example it onlymeans a greater intersection between the (test) margin distribution graph and axis θ = 0. In fact, overfitting as definedhere only means differences with respect to the threshold 1/2 for the class conditional probabilities, but it scarcelymeans anything for their estimation. For example, a variation of 0.01 of ˆPr[y = +1|x] may be enough to flip the labelpredicted for observation x (e.g. passing from 0.495 to 0.505), while a variation of 0.49 of ˆPr[y = +1|x] may not beenough to flip it (e.g. passing from 0.005 to 0.495). In the logistic framework, overfitting rather comes from differencesbetween the estimations and the true values of class conditional probabilities, or similarly from the differences betweenthe corresponding margin distribution graphs. Consider for example a situation in which, for many x ∈ X , we haveˆPrH [y = +1|x] = 0 for a first classifier H (in the PAC framework vocabulary, we predict class −1 with infiniteconfidence), ˆPrH (cid:14)[y = +1|x] ∈ [0.40, 0.45] for another classifier H (cid:14), while Pr(x,y)∼D[y = +1|x] ∈ [0.49, 0.50) forthe true Bayes rule. Clearly, H and H (cid:14) achieve the same true risk over all corresponding examples. However, Hpredicts the wrong label for nearly half of the examples with observation x, still with infinite confidence, while H (cid:14)is more cautious as its wrong predictions have confidence approaching that of the true Bayes rule (approximatelyzero). When it comes that such a situation (of H ) occurs and/or as it becomes more visible, we can say that therestarts to be overfitting. Visually, the corresponding margin distribution graph becomes “sticked” to axis θ = −1. Itis worthwhile remarking that the margin distribution graph of the true Bayes rule can never be sticked to this axis,regardless of the domain, see Fig. 3. Thus, overfitting also means margin distribution graphs that cannot match Bayesrule. The experimental section presents examples of such curves. More generally, when comparing two algorithms(different from Bayes rule), the “best” of both is the one which misclassifies the examples with the smallest possibleR. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–4135confidences, i.e. the one with the margin distribution graph below the other for values of θ (cid:3) 0. From this standpoint,the classical true risk comparison in Valiant’s PAC framework is thus a particular case of the logistic framework.To finish up with margins, let us comment on (8). When HT is a linear separator, it is particularly relevant toperform boosting with domain-partitioning weak hypotheses [8,23], i.e. classifiers that fit to a local, bucket-wisecomputation of the conditional probabilities in (22). The most popular examples are decision trees, but other possibleexamples include decision lists and symmetric functions [23]. In such cases, the margin of a classifier as depicted in(8) admits nice expressions related to Bregman divergences [17]. Suppose for short that domain X is partitioned by htinto a finite number of subsets, whose general term is X(cid:14). Let ˜wt,(cid:14) be the two-dimensional distribution vector whose+entries, written ˜wt,(cid:14), respectively denote the normalized proportion of examples (with respect to wt ) from the= 1. Let ˜wt denote the distribution induced by thenegative and positive classes in S ∩ X(cid:14), thus satisfying ˜w(cid:6)whole partition, with ˜wt,(cid:14) =wt,i . There are essentially two ways to define real values for ht,(cid:14), the output of−ht on some x ∈ X(cid:14). The first is to use the logistic prediction, and define ht,(cid:14) = (1/2) ln( ˜wt,(cid:14)) [8,29]. The second−is to use its gentler alternative, and define ht,(cid:14) = ˜wt,(cid:14) [8]. In both cases, it is straightforward to show that theclassifier’s margin simplifies to:−t,(cid:14) and ˜w+t,(cid:14)/ ˜wi:si ∈X(cid:14)− ˜w+ ˜w+t,(cid:14)−t,(cid:14)+t,(cid:14)μt = 12h(cid:8)tE(cid:14)∼ ˜wt(cid:3)(cid:4)B( ˜wt,(cid:14) (cid:15) 1 − ˜wt,(cid:14)),where B(. (cid:15) .) is a Bregman divergence [17]: the Kullbach–Leibler divergence for the logistic prediction, and theL22 divergence for its gentler alternative. Since a Bregman divergence quantifies a distortion that is non negative,and zero iff its two arguments are equal, this helps to see the classifier’s margin, and the WLA, as the expectationof local discrepancies between the positive and the negative class: the larger they are, the better is the classifier.Obviously, if we had picked discrete values ht,(cid:14) ∈ {−1, +1}, such as the local majority class, we would have obtainedμt = 1 − 2(cid:4)wt ,ht .3.3.2. Computations and numerical stabilityA first difference with previous generalizations of discrete AdaBoosts that do not fix ad hoc values for αt [8,17,29]is computational. Eq. (5) has no closed form solution in the general case, so they all need to approximate αt . Theproblem is convex and single variable, so its approximation is simple, but it needs to be performed at each iteration,which buys a significant additional computation time with respect to AdaBoostR, for which αt is exact. Approximatinghas another drawback: if not good enough, the current iteration may lie outside the boosting regime [8,17,29].The extensions of discrete AdaBoost [8,17,29] face technical and numerical difficulties to compute αt when ht or−ht reaches consistency, that is, when (cid:4)wt ,ht approaches its extremal values, 0 or 1. On the extreme values, there isno finite solution to Eq. (5), and thus theoretically no weight update. In our case, the problem does not hold anymore,as the multiplicative update of Eq. (6) is never zero nor infinite if we adopt the convention that 0/0 = 1. Indeed,a numerator equals zero iff all numerators equal zero iff all denominators equal zero. Thus, zeroing any numeratoror denominator, which amounts to making either perfect or completely wrong classification for ht on S, brings noweight change in wt+1.A second, well known technical difficulty for some extensions of discrete AdaBoost [8,17,29], occurs when theempirical risk approaches 0 or 1, regions where |αt | has extremely large regimes. In this case, the numerical approxi-mations to exponentials in Eq. (5), with the approximations of αt , make the computation of the weights very instable.Clearly, large multiplicative coefficients for the weight update are possible for AdaBoostR. However, instability is lesspronounced, since we have also split the computation of the leveraging coefficients and that of the weight update,allowing the computation of all αt to be delayed till the end of boosting.4. ExperimentsExperiments were carried out on 25 domains, most of which come from the UCI repository [3]. Domains withmore than two classes (indicated 2C) were transformed in a two class problem by grouping all classes but the first intoone: sometimes, this brought domains with highly unbalanced classes, thereby complicating even further the learningtask. On each domain, we have run discrete AdaBoost [7], AdaBoostR and the real AdaBoost of [17,29]. WL is setto a rule (monomial) learner, with fixed maximal rule size r (attribute number) [22,25]. When the output is restrictedto {−1, +1}, we pick for the output of ht (when triggered) the majority class according to wt . When the output is R36R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–41(unrestricted), we use the same method as for decision trees with real values at their leaves [29]. Let ˜wbt be the totalweight in wt of the examples that fire rule ht , and that belong to class b1, with b ∈ {+, −}. Then, for all the examplesthat trigger the rule, the output of ht is a local approximation of the logistic transform (22), [29]:ht = 12log˜w˜w+t−t.The same computations are done for the set of examples that do not trigger the rule (majority class or logistic approx-imation). ht is grown following a procedure similar to decision trees, the repetitive minimization of an index function,which is Matsushita’s error in our case [4,20], see [22,29] for details. True risks are estimated using a 10-fold strati-fied cross validation procedure on the whole data. Since each rule ht has two possible outputs, yht has four possiblevalues, and so the analytic solution for αt of discrete AdaBoost does not apply for real AdaBoost [17,29]. The true αtis approximated from (5) using a simple dichotomous search until the relative error does exceed 10−6, using resultsof [24] to make it faster. With this, we have empirically found that the execution time for real AdaBoost [8,17,29] isstill on average more than 100 times that of discrete AdaBoost and AdaBoostR.4.1. General resultsWe first give some general comments on results that were obtained at early and reasonable stages of boosting,namely after T = 10 and T = 50 steps of boosting, for a rule learner configured with r = 2. Fig. 4 summarizes theresults obtained. While AdaBoostR tends to perform the best, interesting patterns emerge from the simulated domainsfrom which we know everything about the concept to approximate. Easy domains such as Monks(1+2) [3] are thoseDomainT = 10T = 50Balance (2C)Breast-WiscBupaEchocardioGlass2Hayes Roth (2C)HeartHeart-CleveHeart-HungaryHepatitisHorseLabor (2C)Lung cancer (2C)LEDevenLEDeven+17Monks1Monks2Monks3ParityPima (2C)Vehicle (2C)VotesVotes w/oXD6Yeast (2C)#best#second#worstD8.734.5134.5731.4322.9416.4718.5123.8719.3316.4717.1018.3327.509.7622.6825.1834.752.8545.9324.4226.004.789.7820.3228.93799U8.734.6534.5726.4318.2424.1116.6721.2919.3315.2916.316.6727.5017.0721.4625.1833.933.5745.1924.5527.175.008.8621.6428.731168T9.054.7932.8530.7119.4114.7118.8919.6816.3317.0518.1611.6730.0011.2225.6016.0032.282.1447.7825.3226.355.459.7819.5126.809511D4.444.2230.8130.0017.6519.4119.6317.4221.3314.7120.008.3325.0010.2426.3413.3934.261.4346.3024.9425.413.8610.2315.7426.93799U3.813.3827.7125.7117.6514.7116.6719.3516.3315.2916.315.0025.009.5125.3617.5037.701.7947.7824.0325.295.0010.2314.9227.331546T4.924.5128.5727.8615.8815.8819.6320.9718.3318.2333.688.3330.0010.9826.101.5010.171.7946.3025.7125.885.6810.4513.7726.676514Fig. 4. Estimated true risks on 25 domains, comparing discrete AdaBoost [7] (D), AdaBoostR (U) and the real AdaBoost of [8,17,29] (T). Foreach domain, we put in emphasis the best algorithm(s) and the worst algorithm(s) out of the three. The last 3 rows count the number of times eachalgorithm counts respectively among the best, second, and worst.R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–4137on which real AdaBoost performs the best and converges the fastest. Increasing further T makes that real AdaBoostoutstrips even more the two other algorithms. However, as the domain gets complicated, AdaBoostRbecomes thealgorithm that beats the other two when T increases. Consider the following domain ordering, from the easiest to thehardest: Monks(1+2) (no noise, no irrelevant attributes), XD6 (10% class noise, one irrelevant attribute), LEDeven(10% attribute noise), LEDeven+17 (LEDeven+17 irrelevant attributes) [3,22]. Real AdaBoost beats the other twoalgorithms on XD6, but another experiment on larger classifiers (r = 3; T = 100) reveals that AdaBoostRbecomesthe winner and approaches Bayes risk with 11.15% error, while discrete and real AdaBoost respectively achieve11.47% and 12.46% error (statistically worse in that latter case). Winning occurs even sooner on LEDeven (T = 50)and LEDeven+17 (T = 10). One reason for this phenomenon might be the fact that the reweighting scheme ofAdaBoostRis actually gentler than the others, especially on noisy examples: discrete and real AdaBoost are subjectto very large weight update, due to the exponential update rule and the fact that higher reweighting can occur on thesole basis of the binary classification result (good/bad class), even when the classifier has minute confidence on thelabel it predicts. This cannot happen in our case if the classifier’s margin is negative; whenever it is positive, examplesthat receive the right class can still be reweighted higher, thus counterbalancing higher reweighting for eventual noisyexamples. Gentler updating, such as by thresholding, has soon been proposed as a line of research to improve noisehandling [5]. In fact, it may well be also useful to tackle overfitting.4.2. Noise handling and overfittingIn order to shed some more light on noise handling, we have drilled down into the results of domains LEDevenand XD6 [3], by plugging in variable noise rates to see the way the margin errors degrade when the problems getnoisier, and harder. LEDeven is a seven bits problem that describe the ten digits of old pocket calculators. Examplesare picked uniformly at random and the ten possible classes and grouped in two: even/odd. Each description variablegets flipped with η% chances (in the original domain, η = 10%). XD6 is an n = 10 problem that describes a noisydisjunctive normal form formula. Name v1, v2, . . . , v10 ten Boolean variables. Observations are picked at random, andthen labeled positive iff (v1 ∧ v2 ∧ v3) ∨ (v4 ∧ v5 ∧ v6) ∨ (v7 ∧ v8 ∧ v9) is true. Thus, v10 is irrelevant in the strongestsense [13]. Afterwards, with η% chances, the class gets flipped (in the original domain, η = 10%). LEDeven and XD6cover a broad range of difficulties to study noise handling and overfitting: while LEDeven has variable attribute noise,XD6 has variable class noise, an irrelevant attribute and more unbalanced classes.We have computed margin distribution graphs on training and testing for both domains, and for small and largevalues of parameters r and T (respectively 2..6 for r, and 20..1000 for T ). Each time, S is simulated with m = 300examples. On test margin distribution graphs, we have computed the exact curve for Bayes rule in order to makecomparisons following Section 3.3.1. Among the numerous curves obtained, we have chosen to summarize the wholeresults obtained, and report here only the most important curves, obtained for the largest rules (r = 6) and variousnoise rates η ∈ {10%, 20%, 30%, 40%}: see Figs. 5 and 6. Remark that the shape of Bayes curves are more complexfor LEDeven, as noise affects attributes instead of classes. In both domains, the training margins clearly display thatthe real AdaBoost of [8,17,29] is the fastest to converge to perfect classification, followed by discrete AdaBoost [7],and then by AdaBoostR(“us”). The phenomenon is naturally more prominent for XD6, as the domain is easier tohandle than LEDeven, and more prominent as r increases, as it allows a faster fitting of data. The empirical margindistribution graphs for real AdaBoost exhibit two-steps plateaus when the rules become complex (r = 6, domainXD6), one plateau near 0% for θ smaller than 0.1 (approximately), and one plateau which increases with η for largervalues of θ . This plateau shape is in good accordance with Theorem 1, which proves that after a sufficiently largenumber of boosting rounds under the WLA, the empirical margin distribution graph is bounded above by a plateaulocated at the empirical Bayes risk.The test margin distribution graphs display a completely different pattern. On both domains and a majority ofcurves (r, η, T , θ (cid:3) 0), AdaBoostR beats both other algorithms. The phenomenon becomes even more visible as r,η or T increase. A glimpse at the curves for r = 6, η = 40% for both domains (Figs. 5 and 6) is enough to see thisphenomenon, as well as the fact that the test curves of real AdaBoost also tend to exhibit a multi plateaus shape thatbecomes more visible as η increases. This shape observed on testing for real AdaBoost on these cases (which is alsoobservable—though less visible—for discrete AdaBoost, and almost not observed for AdaBoostR) indicates that theconfidence in classification becomes virtually “infinite” for almost all examples, i.e. for those that receive the rightclass, and also for many receiving the wrong class. As discussed in Section 3.3.1, this, we think, indicates a tendency38R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–41Fig. 5. Test margin distribution graphs on domain LEDeven, with different attribute noise rates (r = 6, see text for details).to overfit while trying to model these noisy data. This tendency is clearly less pronounced for AdaBoostR, and if welook at the margin distribution graphs for θ (cid:3) 0, the fact that AdaBoostR’s curve are almost systematically below bothothers tends to indicate that AdaBoostR performs indeed sensibly better than both discrete and real AdaBoosts (seeSection 3.3.1). It is worthwhile remarking that Theorem 1 provides a rough primer for the appearance of these plateauR. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–4139Fig. 6. Test margin distribution graphs on domain XD6, with different class noise rates (r = 6, see text for details).shapes even on testing, as when m increases, (cid:4)u,H ∗ converges towards (cid:4)D,H ∗ , so that Theorem 1, combined with theresults of [28], brings that up to statistical penalties, there should still be a Bayes plateau that upperbounds the testmargin errors. However, the results of Theorem 1 apply to all algorithms: discrete, real AdaBoosts, and AdaBoostR.The fact that the exponential rates of convergence known for the empirical risk are the same for all algorithms indicates40R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–41that real AdaBoost might converge much faster in practice. However, this might not be systematic: worst-case resultsfor the top-down induction of decision trees [23] indicate that this exponential rate of convergence might well be thebest possible. Tests with AdaBoostR for very large values of T (typically, tens or hundreds of thousands) tend to makeappear the plateau shapes, but this is obtained for an iteration regime outside most that would be used on commondatasets, even noisy or hard. It seems thus reasonable to think that the gentler updates of AdaBoostR are indeed thekeys to beating the other AdaBoosts on such domains.5. ConclusionIn this paper, we have proposed a new generalization of discrete AdaBoost to handle weak hypotheses with realvalues. Our algorithm, AdaBoostR, departs from usual generalizations as it does not rely explicitly on the exactminimization of the exponential loss, a loss that upperbounds the empirical risk. While we formally prove that ourgeneralization is a boosting algorithm in the original sense, it provides interesting computational and numerical fea-tures with respect to former real extensions of discrete AdaBoost, as well as a generalization of well-known factsabout discrete boosting. Theoretical and experimental results give insights into the way all algorithms compare withrespect to each others, and give some clues that might be helpful to obtain boosting algorithms with a better handlingof hard or noisy domains.AcknowledgementsWe would like to thank the reviewers for insightful comments that helped to significantly improve the quality of thepaper. R. Nock gratefully thanks Sony Computer Science Laboratories Inc., Tokyo, for a visiting grant during whichpart of this work was done.References[1] A. Banerjee, S. Merugu, I. Dhillon, J. Ghosh, Clustering with Bregman divergences, in: Proc. of the SIAM International Conference on DataMining, 2004.[2] E. Bauer, R. Kohavi, An empirical comparison of voting classification algorithms: Bagging, boosting, and variants, Machine Learning 36(1999) 105–139.[3] C.L. Blake, E. Keogh, C. Merz, UCI repository of machine learning databases, http://www.ics.uci.edu/~mlearn/MLRepository.html, 1998.[4] L. Devroye, L. Györfi, G. Lugosi, A Probabilistic Theory of Pattern Recognition, Springer, 1996.[5] C. Domingo, O. Watanabe, MadaBoost: A modification of AdaBoost, in: Proc. of the 13th International Conference on Computational Learn-ing Theory, 2000.[6] W. Fan, S.-J. Stolfo, J. Zhang, P.-K. Chan, AdaCost: Misclassification cost-sensitive boosting, in: Proc. of the 16th International Conferenceon Machine Learning, 1999.[7] Y. Freund, R.E. Schapire, A decision-theoretic generalization of on-line learning and an application to boosting, Journal of Computer andSystem Sciences 55 (1997) 119–139.[8] J. Friedman, T. Hastie, R. Tibshirani, Additive logistic regression: A statistical view of boosting, Annals of Statistics 28 (2000) 337–374.[9] C. Huang, H. Ai, Y. Li, S. Lao, Vector boosting for rotation invariant multi-view face detection, in: Proc. of the 11th IEEE InternationalConference on Computer Vision, 2005.[10] C. Huang, B. Wu, H. Ai, S. Lao, Omni-directional face detection based on real AdaBoost, in: Proc. of the 10th IEEE International Conferenceon Image Processing, 2004.[11] R. Huang, J.-H.-L. Hansen, Dialect/accent classification via boosted word modeling, in: Proc. of the 30th IEEE International Conference onAcoustics, Speech and Signal Processing, 2005.[12] J.-C. Janodet, R. Nock, M. Sebban, H.-M. Suchier, Boosting grammatical inference with confidence oracles, in: Proc. of the 21st InternationalConference on Machine Learning, 2004.[13] G.H. John, R. Kohavi, K. Pfleger, Irrelevant features and the subset selection problem, in: Proc. of the 11th International Conference onMachine Learning, 1994.[14] M. Kearns, Thoughts on hypothesis boosting, ML class project, 1988.[15] M. Kearns, L. Valiant, Cryptographic limitations on learning boolean formulae and finite automata, in: Proc. of the 21st ACM Symposium onthe Theory of Computing, 1989.[16] M.J. Kearns, U.V. Vazirani, An Introduction to Computational Learning Theory, MIT Press, 1994.[17] J. Kivinen, M. Warmuth, Boosting as entropy projection, in: Proc. of the 12th Int. Conf. on Comp. Learning Theory, 1999.[18] G. Lebanon, J. Lafferty, Boosting and maximum likelihood for exponential models, in: Advances in Neural Information Processing Systems14, 2001.[19] J. Leskovec, J. Shawe-Taylor, Linear programming boosting for uneven datasets, in: Proc. of the 20th International Conference on MachineLearning, 2003.R. Nock, F. Nielsen / Artificial Intelligence 171 (2007) 25–4141[20] K. Matsushita, Decision rule, based on distance, for the classification problem, Annals of the Institute of Statistical Mathematics 8 (1956)67–77.[21] R. Nishii, S. Eguchi, Supervised image classification by contextual AdaBoost based on posteriors in neighborhoods, IEEE Transactions onGeoscience and Remote Sensing 43 (2005) 2547–2554.[22] R. Nock, Inducing interpretable Voting classifiers without trading accuracy for simplicity: Theoretical results, approximation algorithms, andexperiments, Journal of Artificial Intelligence Research 17 (2002) 137–170.[23] R. Nock, F. Nielsen, On domain-partitioning induction criteria: Worst-case bounds for the worst-case based, Theoretical Computer Science 321(2004) 371–382.[24] R. Nock, F. Nielsen, On weighting clustering, IEEE Transactions on Pattern Analysis and Machine Intelligence 28 (2006) 1223–1235.[25] B. Popescu, J.H. Friedman, Predictive learning via rule ensembles, Tech. Report, Stanford University, 2005.[26] G. Ridgeway, The state of boosting, Computing Science and Statistics 31 (1999) 172–181.[27] R.E. Schapire, The strength of weak learnability, Machine Learning (1990) 197–227.[28] R.E. Schapire, Y. Freund, P. Bartlett, W.S. Lee, Boosting the margin: A new explanation for the effectiveness of voting methods, Annals ofStatistics 26 (1998) 1651–1686.[29] R.E. Schapire, Y. Singer, Improved boosting algorithms using confidence-rated predictions, Machine Learning 37 (1999) 297–336.[30] L.G. Valiant, A theory of the learnable, Communications of the ACM 27 (1984) 1134–1142.[31] V. Vapnik, Statistical Learning Theory, John Wiley, 1998.[32] P. Viola, M.-J. Jones, Robust real-time face detection, International Journal of Computer Vision 57 (2004) 137–154.[33] P. Viola, M.-J. Jones, D. Snow, Detecting pedestrians using patterns of motion appearance, International Journal of Computer Vision 63 (2005)153–161.[34] I. Witten, E. Frank, Data Mining: Practical Machine Learning Tools and Techniques with Java Implementation, Morgan Kaufmann, 1999.[35] Z. Yang, M. Li, H. Ai, An experimental study on automatic face gender classification, in: Proc. of the 18th International Conference on PatternRecognition, 2006.