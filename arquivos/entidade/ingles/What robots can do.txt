Artificial Intelligence 101 (1998) 201-226 Artificial Intelligence What robots can do: robot programs and effective achievability Fangzhen Lin a,*, Hector J. Levesque b, ’ ’ Department of Computer Science, The Hong Kong University of Science and Technology, Hong Kong b Department of Computer Science, University of Toronto, Toronto, Canada M5S 3H5 Received 14 May 1997; received in revised form 19 March 1998 Abstract result of the paper is a proof that a simple robot programming In this paper, we propose a definition of goal achievability: given a basic action theory describing an initial state of the world and some primitive actions available to a robot, including some actions information, what goals can be achieved by the robot? The main which return binary sensing technical in that any effectively achievable goal can be achieved by getting the robot to execute one of the robot programs. The significance of this result is at least twofold. First, it is in many ways similar to to robots the equivalence whose actions are specified by an action theory. Secondly, for using for our work on robotics. 0 1998 Elsevier the simple robot programming Science B.V. All rights reserved. functions, but applied it provides formal justifications theorem between Turing machines and recursive language as a foundation is universal, language Kqword.sc Robotics; Cognitive Theories of actions; Situation calculus robotics; Robot programs; Achievability; Effective achievability; Abilities; 1. Introduction Imagine that in the not too distant future, you are given a robot of some sort, and that you want to figure out what it can do. Browsing through the manual that came with it, you discover primitive that actions the robot al, is capable . . , a,. According of performing any of a set of to the manual, what each action ai actually does under computer control depends on the state of the environment. First, to complete successfully, a precondition * Corresponding ’ Email: hector@ai.toronto.edu. author. Email: flin@cs.ust.hk. 0004.3702/98/$19.00 PII: SOOO4-3702(98)0004 I - I 0 1998 Elsevier Science B.V. All rights reserved. 202 E: Lin, H.J. Levesque /ArtiJicial Intelligence I01 (1998) 201-226 of the action must hold in the environment. Next, assuming its effect on the environment may also depend on certain other conditions. Finally, some of to sensors and can return a binary value indicating when a certain the actions are connected to do some programming, what condition holds. The question do we expect to be able to achieve with the robot? is: assuming we are willing is successful, the action In this paper, we propose an answer to this question. Specifically, we propose an abstract of what goals are effectively achievable as a function of a given logical to the framework where questions the initial state of the world and the primitive actions available characterization theory describing robot. The main contribution of the paper is a precise technical of goal achievability the universality of the simple robot programming out that a goal is effectively achievable according program result is a proof of can be posed and answered. The main technical introduced in [7]: it will turn theory T iff there is a robot that achieves it according language to logical to T. 1.1. A motivating example To make the problem more concrete, that you are also given a solid steel box imagine that contains a treasure. There is a small robot-sized door on the box, which is currently closed, and there are two buttons beside it, a green one and a red one. The primitive actions to the robot are pressGreen, pressRed, and fetch. The manual says that if the robot available to be pressed, to be beside happens andpressRed similarly. The manual also says that the robot has a heat sensor so that a press action returns 1 when the button pressed was hot, and 0 otherwise. Thefetch action causes that the robot is the robot to trundle is beside getting the treasure, under assumptions the door and the door is open. 2 The goal we are interested the closed door, pressGreen causes the green button the box and retrieve what’s inside, provided in here, obviously, inside (1) (2) (3) the treasure by forcing the robot to press the buttons like the following: If we know nothing else about the environment, we want our account of achievability that we cannot achieve the goal. Of course, we might end up eventually to predict the door open with a crowbar, or by saying some getting magic words, or even by getting in some order. But there is no reason to believe a priori that any of these methods will work. If we know that the red button opens the door of the box, we want our account of the robot: we get it to do achievability the sequence pressRed, the door might jam, lightning might strike the robot, a comet might hit the earth. But there is no reason to believe If we know that one of the buttons opens the door of the box, and the other button locks the door permanently, but we don’t know which is which, our account should predict the robot. As in (2), we know that there is a sequence of actions but here we do not know what that sequence the goal using then fetch. Of course, something might go wrong: that the sequence will fail given what we have been told. the goal using that will work-press to say that we can achieve that we cannot achieve one of the buttons thenfetch- is. ’ In a more realistic setting, of course, there would be a large number of other preconditions for actions like these. E Lin, H.J. Levesque /Art@ial Intelligence 101 (1998) 201-226 203 the following situation: we know that the door can be opened by first (4) But consider the green button, and then pressing one more button, but we are not told pressing it wrong locks the door permanently. However, suppose which, and again, getting that felt that we know that the safe will lock forever iff the robot pushes a button hot on the previous press. As in (3), we know that there is a sequence of actions that will work, and again we cannot say what that sequence is. This time, however, our account should predict that we can achieve the goal: we get the robot to pressGreen, and then pressGreen once more if the button was cold, but pressRed if it was hot. (5) Finally, that after pressing suppose we know the green button some unspecified the red button will open the door and number of times and at least once, pressing the green one will lock it forever. With no other information, we clearly pressing cannot obtain if we also know as in (4) that the door will lock forever iff the robot presses a button that was just hot, then we can once again achieve the goal: we get the robot to repeatedly press the green button until it feels hot, then press the red one to open the door, and then fetch the treasure. the treasure. However, To the best of our knowledge, intuitively correct answers for examples like these. there is as yet no formal framework that would give the 1.2. Relation to other work There are, however, three areas of research that come close to providing these answers. related as in [2,13,19,23]. to the concept of planning As the five examples above illustrate, Planning. a robot is clearly conditional planning, that the treasure was obtainable precisely when we could formulate obtain it. Why then not simply define goal achievability the idea of a goal being achievable by the sensing, and especially, given In all of the variants above, we ended up saying some sort of plan to in terms of the existence of a plan? exactly what we mean by a plan. An The problem with this involves characterizing is sufficient. But in some of the variants obvious case is when a fixed sequence of actions and iterative plans, which suggests a structure above, we needed in a traditional these would not be programs more like that of a program language like C or LISP. For one thing, the primitive statements of the program would have to involve the actions ai, rather than the usual variable assignment or read/write statements. statement? How What would we use as the conditions should the execution of programs containing in an if-then-else or a while-loop to consider conditional the ai be defined? [14]. Clearly We believe achievability design decisions first define achievability language programming in terms of such programs that these questions can be resolved and that it is possible to characterize (see Section 4 below) . However, to avoid making to that a to be arbitrary or restrictive, we prefer way, and then prove initially appear in a general program-independent that might is adequate according to this definition. A second concept related to achievability Computability. ity [ 161. As will become clear, we will end up defining achievable goals as those where what to do next to achieve them, given what is known about the actions and the initial state of the world, can be “computed” as a function of what the sensors tell the robot. is that of effective computabil- 204 E LitI, H.J. Levesque /Arti$cial Intelligence 101 (1998) 201-226 information to typical accounts of computability, for two reasons. the surrounding However, we cannot simply use an existing account of computability about the environment First, we want to allow for incomplete robot. In contrast the need only be partially specified by a collection of axioms. The second reason concerns primitive actions. In typical computability models, the available actions are predefined and internal to write and to the machine registers, or to assign values read a Turing machine to variables, and so on. In our case, by contrast, the primitive actions for a robot are not outside of the predefined robot. These actions are also described by a collection of axioms, which specify the action preconditions (or formalism). For instance, we might have actions tape, or to increment and are e_xternal, in that they have effects in the environment and effects, and deal with the frame problem. the initial state of the environment and decrement Thus our account of goal achievability depends crucially on what the given axioms say about the initial state and the available actions. In some of the examples above, we had two theories Tt G T2 describing unachievable additional as a rdation between a formal existing account of computability the same initial state and set of actions. A goal was considered relative to T2 where information was available. We would like to define a notion of goal achievability theory T and the goals we would like to achieve, and no relative to the information provided by Tl , but achievable does this. Finally, the two concepts concerns the concept of achievability in [ 1,24,25]. One difference between Knowing how. is very closely related to the concept of an agent knowing how (or being able to) achieve a goal or execute a plan, as discussed for example, the issue of effectiveness. As far as we know, no existing account of knowing how or ability considers whether or not the know how of an agent would be effective, the in the sense of allowing aside, there is also a difference agent to “compute” what to do. But putting effectiveness that we in point of view: who has to know what and when. There may be conditions to be achievable, but that the agent does not know how to bring about. would consider is For example, is the correct achievable by the agent/robot; but if the agent does not know which button one, we would not say that it knew how to get the treasure. Conversely, we can imagine a situation where we do not consider (in the sense of being able to produce a plan) because we do not know which buttons to use, but where we know that the agent does. We can also imagine situations where the agent initially knows less than we do, but after obtaining When reasoning from its sensors, knows as much or more than we do. the concept of knowing- if the red button opens the door, we know that goal of getting the treasure about what one agent knows about another, the goal to be achievable information how or ability may be the more useful one; when attempting agent or robot to do for us, our notion of goal achievability may be the more appropriate. to be the case that the two notions coincide when the agent knows Moreover, exactly what we do about the environment and the actions. The precise relation between the two concepts is subtle, however, and we will not explore it further here (see [5]). to analyze what we can get an it ought In sum, while the concept of goal achievability is clearly related to the areas of planning, computability, example, in the five situations above. and agent ability, none of these can give us the answers we want, for The rest of the paper is organized as follows. In the next section, we review the situation and the in terms of which the state of the environment calculus, a formal logical language F: Lin, H.J. Levesque /Artijcial Intelligence 101 (1998) 201-226 205 primitive actions can be described by a collection of axioms we call a basic action theory. In Section 3, we define precisely what we mean by effective achievability (and related theory. In Section 4, we review the syntax notions) as a function of a given basic action in [7] as a language language and semantics of a simple robot programming for plans. In Section 5, we present some results, including result of the language. This is a robot analogue of paper: the universality iff there the classic universality is a program/machine the paper and suggest topics for further research. theory: a function in Section 6, we summarize of the robot programming result in computability the main technical that computes first proposed is computable it. Finally, 2. The situation calculus and basic action theories Since the goal of this research on a given action theory T describing we need to describe the representation a dialect of the situation calculus The language of the situation [ 151. calculus is to make the specification of goal achievability depend the initial state of the world and the available actions, the theories, which is language we use to formulate that situation is many-sorted. Normally, in which no actions have yet occurred; the action a; relations whose truth values vary from situation binary function symbol do where do(a, s) denotes the successor situation there is a sort for situations, a sort for actions, and a sort for objects like blocks and people that are elements in the domain of interest. We assume that there is a special constant SO used to denote the there is a initial situation, namely distinguished to s resulting from performing taking a to situation, are called (relational)$uents, there is a special predicate Poss(u, s) used to state that situation action a is executable in situation s; and finally, there is a special predicate SF(u, s) used to state that the sensor associated with action a (if any) returns the value 1 in situation s. 3 the initial state of actions, that define POD; we specify the condition measured by a of the world and the actions available for example, by writing axioms sensor by writing axioms that define SF, and so on. Here, we use a theory which contains only the following axioms: to the robot. We specify the preconditions this language, we can formulate domain and are denoted by predicate symbols term as their last argument; theories which describe Within l Axioms describing mention any other situation the initial situation, terms except So. So. Syntactically, these axioms cannot l Action precondition Syntactically, axioms, one for each primitive action A, characterizing Poss(A , s ) form: these axioms all have the following Poss(A, S) = PA(S), (1) where PA ((s) is a formula does not quantify over situation variables, and does not mention like Pass, SF, or < (introduced below). that does not mention any other situation terms except s, the special predicates 3 In [7], the predicate SF was used to characterize what an agent knew in a situation in terms of a fluent K. In this paper, we will not be concerned with the knowledge of agents. 206 E Lin, H.J. Levesque /Artificial Intelligence 101 (1998) 201-226 l Sensed fluent axioms, one for each primitive these axioms have the form: Syntactically, action A, characterizing SF(A, s). SF(A, S) = nA(S), where flA (s) satisfies the same conditions have nothing to do with sensors, this should be [SF(A, s) = True]. as those for PA(s) above. For actions that l Successor state axioms, one for each fluent F, characterizing under what conditions F(2, do(a, s)) holds as function of what holds in situation s. These take the place of the so-called effect axioms, but also provide a solution [20]. Syntactically, successor state axioms have the form: to the frame problem Poss(a, s) > [F (2, do(a) s)) = QF (2, a, s)], (2) where 0,~ satisfies the same conditions as those for $?A (s) above. l Unique names axioms for the primitive actions: For any two different actions A(,?) and A’(y), we have A(;) #A’($ and for any action A(xl , . . . , x,), we have AW,..., ~,)=A(yl,...,y,,)>xl=y~~...~/\n,=y,. l Foundational, domain-independent the structure of the space of situations, and define a predicate < so that st < s2 holds iff s2 can be reached from st by a sequence of executable actions, i.e., there are actions at, . . . , a,, 0 < IZ, such that axioms that characterize s2 = do(h 3.. . , anI, a) A Pcm(Ul, Sl) A.. . A Puss(u,,do([u~, . . .) a,-,], s,)) holds, where for any situation s, &([I, s) = s, and inductively, do([u]L], s) = do(L, &(a, s)). These axioms are: dO(Ul( Sl) = d&Q, s2) 3 (al = a2 A Sl = 4, (VP).P(So) A (Vu, s)[P(s> 3 P(do(u, s))] 3 (Vs)P(s), 1s < so, s < do(u, s’) = (Poss(u, s’) AS < s’). these axioms and Peano Arithmetic. The first two Notice the similarity between finite cycles, and merging. The axioms are unique names assumptions; third axiom is second-order to a domain closure axiom which induction; says that every situation must be obtained by repeatedly applying do to Se. 4 The last two axioms define < inductively. they eliminate it amounts 4 For a discussion of the use of induction in the situation calculus, see (Reiter [21]). E Lin, H.J. Levesque /Art@ial Intelligence 101 (1998) 201-226 207 [ 111, we call a theory of this form a basic action theory. In this paper we shall the question of effective achievability with respect to a basic action theory. Before to this task, we first make some remarks about the generality of our basic action [3] Following consider turning theories and their relationships with more popularly used formalisms and ADL [ 171 used in AI planning. such as STRIPS language, which is more expressive If we do not consider sensing actions, then as far as the action effects are concerned, theories have more or less the same expressive power as Pednault’s ADL basic action action description than STRIPS and used by UCPOP [ 181. The main reason is that if there are no sensing actions, as far as the effects of actions and successor are concerned, a basic action theory consists of a set of action precondition state axioms of the forms (1) and (2), respectively. Under some reasonable conditions, and vise versa following these axioms can be reformulated [17]. In any event, for our a procedure discussed purposes here, an action theory that does not have any sensing actions is not that interesting: if (Va).SF(a, executable theory. In this case, induction provides a powerful cannot be achieved, as shown by Reiter [21]. To the best of our knowledge, UWL in a situation S iff there is an r of ground actions such that G(do(r, S)) is entailed by the action for proving what can and in detail by Reiter [20] and Pednault as Pednault’s ADL descriptions s) = True, then a goal G(s) [2] is the only STRIPS-like action description is achievable technique sequence formula that axiomatizes than UWL domain descriptions to be sensed while UWL only allows a conjunction language the one hand, our action theories are more general we allow arbitrary On the other hand, UWL is more general be a knowledge goal such as ( find-out ( P maintenance goals, although can be handled paper, and are interesting of effective achievability algorithm the effects of a sensing action on the agent’s knowledge state. On in that of literals. of an operator to ) (find out the truth value of P) or a . v) goal such as (hands-of f P ) (do not change the truth value of P). These the scope of this in the situation calculus, 5 are beyond these differences, our account the planning to check whether future research topics. Ignoring can then be used, for example, for UWL given in [2] is sound and/or complete. For further details on the generality of our approach, in that it allows a precondition are given of goals of examples presence/absence achieved by plans containing account that has this generality. goal achievability of sensing. This paper contains that are intuitively see also [7], where a number the that can only be loops as well as sensing, and as far as we know, it is the only It was this paper that inspired us to look for a definition of achievable/unachievable examples of goals in that did not appeal to a predefined programming language of plans. 3. Effective achievability To define in its most general form what a robot armed with primitive actions al, . . a,. from the point of view of a to begin by looking at the problem it is useful can achieve, robot controller, for instance, an onboard computer. 5 See [5,22] for a treatment of knowledge goals, and [S-lo] for some example formalization of temporal constraints in the situation calculus. 208 E Lin, H.J. Zhesque /Artificial Zntelligence IO1 (1998) 201-226 What a robot controller needs to do at any given point in time is to select the primitive to perform next (or to stop). We are willing and intelligence in making theory. We do not want to assume, however, amounts of to assume arbitrary action this decision, as well as full access to the given computation that the controller necessarily basic action there is to know about the current state of the environment. For example, knows everything if it is part of the basic action theory that a door is open initially, the controller can use this fact; but if the action theory does not specify the state of the door, the robot may need to such an action is perform some (sensing) action to find out whether available. it is open, assuming So what does a robot controller have access to beyond theory? all of it has selected until now, as well as the sensing results of all these actions. In these sensing results must be compatible with the given action theory, but will not information In its most general the actions general, be entailed by it, and so provide additional that the robot controller the given basic action form, we might to the controller. remembers imagine Once we have specified what a robot controller goals. Roughly, a goal will be considered such that if we were to repeatedly do the primitive action it prescribes, the sensing the action relevant notions. theory, the goal condition would hold. We now proceed turns out, we would eventually to be achievable terminate is, we can then define the achievable if there exists a robot controller then no matter how to to formally define the in a situation where, according 3.1. Robot controllers and environments We assume a finite set A of actions that are parameterless, the robot will be in some state determined by the actions and represented by constant it has the readings of its sensors. More precisely, so far and, in the event of sensing, symbols. At any point, performed we define: Definition 1 (History). A history CT is an element of the set R = (A x (0, l})*. Intuitively, i, if the sensing in the situation where so far, and B1, . . , /In are the respective sensing the history (~1, B1) o . . . o (an, ,&) means that ~1, . . . , a, is the sequence of results of the actions: is then pi = 1, else pi = 0. Notice that by the form of basic action theories (cf. then j?i = 1. Notice also fluent SF holds for oi actions performed For any performed, Section 2) if czi is an action that has nothing that the empty sequence E is a history. is then a mapping In addition following, not in A. Intuitively, end of the computation, from such a history to the next action to perform. In the to the given primitive actions let ,4+ = A U {stop, abort, I], where stop, abort, and I are special symbols stop will be used to denote termination, abort to signal exit before the to denote an undefined computation. 6 Formally, we define: in A, we assume some special symbols. to do with sensors, A robot controller the action and I 6 The reason we need abort and I will be made clear later in the context of robot programs. F: Lin, H.J. Levesque /Artijicial Intelligence I01 (1998) 201-226 209 Definition 2 (Robot controller). A (robot) controller C is any function actions or special symbols, C : R =+ A+. from histories to Definition 3 (Effective controller). A controller is effective if the function C is recursive. It should be clear that the only feedback the robot gets from the environment is through specifies the next action to perform, an environment its sensors. Just as a robot controller specifies the sensing result of that action. More precisely, we define: Definition 4 (Environment). An environment E is any function totheset(O,l], E:Rxd*{O,l}. from histories and actions In other words, &(a, cz) tells us what the sensor associated with action (Y will report given the history D. Intuitively, the picture is this. We start with the empty history E; the robot controller C chooses an action the value returned by the ~1 sensor: fit = &(F, at); given this result, the robot then chooses another action to perform a2 = C((at the ~2 sensor value: 82 = E((crt , Bl), CQ,); then ~3 = C((crt , /3t) o ((2’2,82)) and so on, until C says stop. , PI)), and the environment the environment I determines (111 = C(E); to perform determines Definition 5 (System). A system environment. is a pair (C, E), where C is a controller, and I an Frequently, we shall refer to the system (C, E) as the controller C under the environ- ment E. run). A history 0 is a run of a system (C, E) if, inductively, Definition 6 ((Terminating) either 0 is the empty sequence E or cr = cr’ o (a, /l) such that ~9 is a run of the system, C(a’) = a! E A, and E(a’, a) = ,I!?. A history 0 is a terminating run of (C, E) if it is a run of (C, I) and C(a) = stop. Clearly, a system can have at most one terminating run. 3.2. Achievability and effective achievability Note that neither controllers nor environments they are simply abstract functions over the domain of histories. To make a connection with the situation calculus, we first relate histories are part of the situation calculus; to situations: Definition 7 (Run and situation). Given any history 0, and any situation another situation inductively, term s, we define term, the end situation of 0 on s, written end(a, s), as: end(s, s) = s; and if (T = o’ o (a, ,!?), then end(a, s) = do(a, end(cr’, s)). Next, we relate environments to logical interpretations of a basic action theory. 210 E Lin, H.J. Levesque /Artijicial Intelligence 101 (1998) 201-226 Definition 8 (Environment situation and any action cz, E(cr, a) = 1 iff I b SF(a, end(a, S)). I and a ground and interpretation). Given an interpretation term S, an environment & is said to be determined by I at S iff for any history c, It is clear that there is exactly one such environment for any given Z and S. In other of a basic action theory (and a starting situation), of the SF predicate completely determines how the sensing will turn out, words, once we specify an interpretation the interpretation and hence the environment. In general, we expect a basic action theory to be satisfied by many interpretations, turn out. Goal achievability corresponding requires a controller to the various ways the environment could to work in all such interpretations: Definition 9 ((Effective) achievability). Given an action a formula with a single free situation variable s, and a ground situation that G is (effectively) achievable such that for any model I of T, there is a terminating determined by I at S such that I b S < end(a, S) A G(end(a, S)). theory T, a goal G(s) which is term S, we say to T iff there is an (effective) controller C run o of C under the environment in S according Notice that the condition S < end(o, S) means from S by a sequence of executable actions. This means that the actions prescribed by the run ~7 must be executable that for a goal to be achievable, it must be at least executable. in their respective situations. This condition that end(a, S) is reachable the sequence of actions that is used to achieve reflects our intuition there will be goals that are achievable However, as we are going achievable, then it is also effectively achievable. to show below, for context-free action but not effectively theories, achievable. if a goal is ’ In general, 3.3. Achievability in context-free action theories By a context-free action theory we mean a theory in which all actions are context-free of the state in which they are executed. For in the sense that their effects are independent in the blocks world, the action stack(x, y), that picks up block x on the table and example, puts in on top of block y, is context-free-as it will always cause x to be on y. On the other hand, in the extended blocks world in which there may be more than one block on top of another block, the action unstuck(x, y), that removes x from y, for example, depends on is not context-free-whether whether x was the only block on top of y . block y will be clear afterwards, long as it is executable, ’ An interesting question motivated by a comment redundant referee is whether basic action theories for any are strong enough goal G, whenever that that T t= G(S), T + So < 5” A G(S’). These are presently open questions. However, except possibly making our definition of achievability from an anonymous in Definition 9. A simpler question asks whether to them have otherwise no effect on the results of this paper. to make this condition there a little simpler, the answers is a situation S such situation S’ such then there is another E Lin, H.J. Levesque /Artificial Intelligence I01 (1998) 201-226 211 Now an action theory is context-free following free. Formally, state axiom in it has the following form: [ 121, we call an action theory T context-free if, according to the theory, all actions are context- if every successor Poss(a, s) > [F@, &(a, X,) = yF+.($ u) v (F($ s) A -y&u))], (3) where vg (2, a) and y; (2, a) are situation-independent among those in x’, a. Under the following consistency condition [20]: formulas whose free variables are c I== (k+(&m w&s)), (4) (3) implies that for any action a, after the action the axiom true (added) for tuples in {T / y,‘(,?, a)}, false (deleted) persist for tuples the conditions YF and v,$ are situation-independent. STRIPS, with add and delete lists, every action is considered in (2 ( -y,i$(;, a) A -yF(;, is performed, F will be for tuples in (x’ / y;(x’, a)}, and a)}. The action a is context-free because Note that in the usual formulation of to be context-free. Theorem 1. Let T be a context-free action theory, and the consistency condition (4) holds for everyjuent in S, then it is also effectively achievable. F. ’ If a goal G is achievable Proof. See Appendix A. q Informally, the theorem holds because a context-free legal states. One can read the theorem number of possible it points to some potential computational theories. On the other hand, it also points out their expressive theorem ajkite set of context-free actions. that it is impossible implies advantages of working with context-free action theory can only have finite in two ways. On the one hand, action this action with limitations. For example, to simulate an arbitrary context-sensitive 4. A robot program language [4]: a plan In [7], the following question was considered: what should the output of a planning clear and dates back procedure be? In the absence of sensing, to Green in a final that results situation where the goal condition holds, In the presence of sensing, however, a planner to execute to satisfy the goal cannot simply return a sequence of actions since the actions could depend on the runtime in the introduction). is reasonably sequence of actions result of earlier sensing operations is a legally executable (as in the examples the answer Clearly, what is needed is something more like a program, with branches and loops. On (in the of the primitive actions at each step are satisfied), and leads to terminates and the goal condition holds in the to execute the other hand, it would need to be a program sense that the preconditions a goal state (in the sense that the program terminating situation), but also a program that does not require more information that is not only legally executable ’ Technically, this condition is not necessary. However, it simplifies our proof, and is a reasonable condition to impose on action theories. 212 E Lin, H.J. Levesque /Artijicial Intelligence 101 (1998) 201-226 than what we expect the robot to have. For example, steel box opens by pushing either the red or the green button, something other one” might satisfy the first two conditions, but not the last one. if all we know is that the door to the then the program which says is the one that opens the door then push it, else push the like “if the red button There are various ways to ensure this last requirement. The approach taken in [7] is to invent a simple any conditions involving without such conditions, that contains branches and loops, but that does not mention fluents. The resulting programs are then trivial to execute since there is nothing for the robot executing the programs to know. language Consider the following simple programming language, defined as the least set of terms satisfying the following: (1) nil and exit are programs. (2) Ifa (3) If ~1 and r2 are programs, is a&action and rl and r2 are programs, then brunch(a, ~1, r-2) is a program. then loop(rl , ~2) is a program. set of terms R, and the resulting language. these programs are executed by an agent as follows: to execute exit it must be executing a loop, in which case see below; We will call such terms robot programs programming to execute nil the agent Informally, to does nothing; , ~2) it first executes primitive action a, and then it executes rl if the execute brunch(a, to execute loop(q) Q), it executes the sensor associated with a returns 1, and r2 otherwise; body r] , and if it ends with r&l, it repeats PI again, and continues doing so until it ends with e&, in which case it finishes by executing r2. the robot r1 Note that many actions will not have an associated sensor and will always return 1. We r) for brunch(u, r, r) for those cases where the returned thus use the abbreviation ~(a, value is ignored. Here are some robot programs for the examples in the introduction. (Recall that the pressGreen action returns 1 if the button is hot.) (1) Sequence of actions: x@ressRed, -(fetch, nil)). - (2) Conditional plan: brunch(pressGreen, w@ressRed, &$etch, nil)), - x@ressGreen, x(fetch, nil))). - (3) Iterative plan: loop(brunchCpressGreen, exit, nil), s(pressRed, =(fetch, r&Z))). Intuitively at least, the following should be clear: l An agent can always be assumed programs are completely deterministic, binary sensing actions return a single bit of information else it should need to know. to know how to execute a robot program. These the and do not mention any fluents. Assuming to the agent, there is nothing E Lin, H.J. Levesque /Artificial Intelligence 101 (1998) 201-226 213 . The example the goal conditions robot programs above, when executed, are satisfied. from the introduction result in final situations where To be precise about this, we need to first define what situation from executing a robot program conceivably that Y terminates abbreviation is the final one resulting r in an initial situation s. Because a robot program could loop forever (e.g., loop(nil, nil)), we will use a formula Rdo(r, s, s’) to mean legally when started in s, and s’ is the final situation. Formally, Rdo is an for the following second-order formula: Rdo(r,sl,sz) EfVP[ ... > P(r,sr,sz, l)] (5) where the ellipsis is (the conjunction of the universal closure of) the following: normal case: P(nJ, s, s, 1). (1) Termination, loop body: P(&, (2) Termination, (3) Primitive actions returning 1: s, s, 0). Poss(a, s) A SF(a, s) A P(r’, do(a, s), s’, x) > P(brunch(u, r’, r”), s, s’, x). (4) Primitive actions returning 0: Poss(u, s) A -SF(u, s) A P(r”, do@, s), s’, x) > P(brunch(u, r’, r”), s, s’, x). (5) Loops, exit case: P(r’, s, s”, 0) A P(r”, s”, s’, x) > P(loop(r’, r”), s, s’, x). (6) Loops, repeat case: P(r’, s, s”, 1) A P(loop(r’, r”), s”, s’, x) > P(loop(r’, r”), s, s’, x). By using second-order quantification least predicate P satisfying here since iteration the constraints there is no way to characterize in first-order terms. in this way, we are defining Rdo recursively in the ellipsis. Second-order the transitive closure implicit as the logic is necessary in unbounded The relation P(r, s, s’, 0) in this definition in s terminates difference continue with r’; in the latter, we continue to hold when executing r starting legally at s’ with e&; P(r, s, s’, 1) is the same but terminating with nil. The loop(r, r’): in the former case, we exit the loop and r’) once more. that are in place, we can now characterize the iteration by repeating &jr, shows up when executing is intended the goals precisely With this definition achievable using robot programs: Given an action situation theory T, a robot program r, a goal condition G(s) and a ground term S, we say that r achieves G in S according to T iff T /= 3s’.Rdo(r, S, s’) A G(s’). We now relate this definition to effective achievability. 214 E Lin, H.J. Levesque /Artijicial Intelligence 101 (1998) 201-226 5. Robot programs are universal Our main technical result in this paper is that a goal is achievable by an (augmented) iff it is achievable by an effective controller. We shall prove this in two parts. for then any effective robot program First, we show that for any robot program, it. We then show that if 5 special “Turing machine actions” are included, controller can be simulated by a robot program. there is a corresponding effective controller 5. I. From robot programs to effective controllers Theorem 2. For any robotprogram controller C such thatfor any interpretation r and any ground situation term S, there is an effective I and any ground situation term S’: (1) If I /= Rdo(r, S, S’), then there is a terminating run o of the system (C, E) such that S’ = endfo, S), where E is the environment determined by I at S. (2) Ifthere is a terminating run o of the system (C, E) such that end(a, S) = S’, then I + S < S’ > Rdo(r, S, S’). Here & is the environment determined by I at S. Proof. See Appendix B. 0 5.2. From effective controllers to robot programs Given an effective controller, it. 9 The easiest way to remedy in [7]. there may not always be a robot program this is to add some special Turing machine actions as that simulates Formally, we assume that in addition to the actions left, right, mark, erase, read-mark, and two special fluents Marked, the following axioms: (1) Precondition: the five actions are always possible in A, we have five special actions, lot, characterized by Poss(left, s) A Poss(right, s) A Poss(mark, s) A Poss(erase, s) A Poss(read_mark, s). (2) Successor state: only erase and mark change the Marked fluent Poss(a, s) 1 {Marked(n, do(a, s)) = a = mark A lot(s) = n v Marked(n, s) A -[a = erase A lot(s) = n]). (3) Successor state: only left and right change the lot fluent laborious. Observe 9 The proof of this is somewhat that despite the presence of loops, a robot program has no internal memory and so no way of counting. So consider an action theory that encodes a string problem: decide if a string of OS and 1s has more OS than Is. A Turing machine can do this, but a finite-state automaton cannot. Similarly, an effective controller can achieve the corresponding in general cannot. We omit the details. goal, but a robot program E Lin, H.J. Levesque /Artificial Intelligence 101 (1998) 201-226 215 Poss(a, s) > (loc(do(a, s)) = n = a = lefr A lot(s) = n + 1 v a = righr A lot(s) = n - 1 v lot(s) = n A a # left A a # right}. (4) Sensed fluent: read_mark tells the agent whether the current location is marked SF(lej?. s) A SF(right, s) A SF(erase, s) A SF(mark, s) A [SF(read_mark, s) E Murked(loc(s), s)]. These axioms ensure that the five special actions provide the robot with what amounts Turing machine tape. to a In the following, we will be using what we will call a TM basic action theory. This is a basic action theory as before, but where A includes the five special actions, T contains the above axioms, and where the successor state axioms in T for fluents other than lot and for any fluent F Marked are such they are unaffected by the five actions. More precisely, the that is different theory T entails: from Marked and lot, and when A is any of the five special actions, Poss(A,s) > [F(do(A,s)) = F(s)]. In the following, for any ground situation term S, we define clean(S) term obtained from S by deleting all the special five actions: clean($) to be the situation = SO, and clean(do(cr, S)) = do(a, clean(S)) if Q! E A is not a Turing action, and clean(do(a, S)) = clean(S) otherwise. Theorem 3. For any TM basic action theory T, any effective controller C, and any ground situation term S, there is a robot program r such that for any model I of T (as above) and any ground situation S’, we have: (1) Ij’I + Rdo(r, S, S’), then there is a terminating run D of the system (C, E) such that (2) If there is a terminating clean(S’) = end(a, S), where & is the environment determined by I at S. run o of the system (C, E) such that S’ = end(a, S), then there is a situation Srf such that S’ = clean(S”), and 1 b S < end(o, S) > Rdo(r. S, S”), where & is the environment determined by I at S. Proof. See Appendix C. III 5.3. The main theorem By Theorems 2 and 3, we have the following result: Theorem 4. Let T be any TM basic action theory and G be any goal that does not mention the specialfluents to T tjfthere is a robot program r such that r achieves G in S according lot and Marked. Then G is effectively achievable in S according to T. 216 E Lin, H.J. Levesque /Art$cial Intelligence 101 (1998) 201-226 Proof. Suppose G is achieved by the effective controller C. We show that the robot program r for C as in Theorem 3 achieves G. Suppose Z is any model of T. Then, by the definition of achievability, run o of (C, E) such that there is a terminating I + S 6 end(o, S) A G(end(a, S)), where & is the environment situation S” such that cZean(S”) = end(a, S) and determined by I and S. According to Theorem 3, there is a I k S 6 end(cr, S) > Rdo(r, S, 9’). I b Z?do(r, S, S”). By Z b G(end(a, S)) and cZean(S”) = end(a, S), we have (by a property about clean). So Z + (L’)Rdo(r, S, s’) A G(s’). Therefore Thus Z b G(S”) T + @s’)Rdo(r, S, s’) A G(d), and so r achieves G. Conversely, suppose r achieves G in S. We show that the effective controller C for (C, E) such that Z /= S < end(a, S) A G(end(a, S)), where & is the r as in Theorem 2 achieves G in S, i.e., for any model Z of T, there is a terminating run 0 of the system environment that r achieves G, i.e., T + (3s’)Rdo(r, S, s’) A G(d), such that Z + Rdo(r, S, S’) A G(S’). By Theorem 2, there is a terminating such that S’ = end(o, S). So determined by Z and S. Now suppose Z is a model of T, by the assumption term S’ run o of (C, E) there is a ground situation Z + Rdo(r, S, end(a, S)) A G(end(a, S)). But by the definition of Rdo, we have + (b’s], sz)Rdo(r, ~1,s~) 3 sl < ~2. Thus we have Z + 5’ < end(a, S). This shows that G is achieved by the controller C. q 6. Conclusion to the robot. Our main technical contribution We have provided a definition of what it might mean for a condition to be achievable by the initial state of the world and the a robot relative to a given action theory which describes that primitive actions available by a simple this notion of effective achievability in [7]. The significance of this result class of robot programs is at least twofold. First, it is in many ways similar theorem between Turing machines and recursive functions, but applied to robots whose actions are specified for using the simple class of by an action theory. Secondly, it provides formal justifications robot programs as a foundation [7] uses this class of robot programs as a basis for robot planning. We are also beginning work on compiling high-level GOLOG programs coincides with a notion of achievability for our work on robotics. For instance, [6] into this class of robot programs. to the equivalence is in showing independently introduced that there are only a finite number of parameterless There are some limitations with our current model that are worth mentioning here. First actions. We of all, we have assumed by the special SF have also assumed is predicate. Furthermore, we have assumed the result of these sensing actions. In particular, we have not concerned ourselves here with possible action failure or exogenous actions. Some of these assumptions, such as the binary nature of sensing, are easy to relax; others will require more effort. that the sensing actions are binary, characterized that the only feedback from the environment F: Lin, H. J. Levesque /Artijicial Intelligence 101 (1998) 201-226 217 In concluding, we want to mention that we are working on relating this work to our [5]. Another direction worth pursuing i.e., the power of robot version of achievability, other work on agent ability and knowing-how is investigating the “finite automaton” programs without the special Turing machine actions. Acknowledgements support for the second author was gratefully This work was done as part of the Cognitive Robotics project at the University Toronto, and we greatly benefited from discussions with Yves Lesptrance, Financial and Engineering Research Council of Canada, Centre of Ontario, and the Institute first author was also supported Competitive Earmarked Research Grant HKUST609 1/97E. of and Ray Reiter. received from the Natural Sciences the Information Technology Research for Robotics and Intelligent Systems of Canada. The in part by the Research Grants Council of Hong Kong under Appendix A. Proof of Theorem 1 Theorem 1. Let T be a context-free action theory, and the consistency condition (4) holds for every fluent F. If a goal G is achievable in S, then it is also effectively achievable. We first prove a lemma which says that the set of legal states theory is finite. To formulate action FI (X; , s), . . . , F, (X;, , s) be all the fluents meaning that s and s’ yield the same state, as follows: the lemma, we first introduce in the language. We define SameStute(s, in a context-free a few notations. Let s’), SameStute(s, s’) dzf (V;,)(F,(&,s) 3 Fl(;,,s’)) A...A (V&)(F,(&,s) = F,(&s’)). The following are some simple properties about SameState: Lemma A.l. For any basic action theory T, we have: T + (Va, s, s’).SumeState(s, s’) 3 [Poss(a, s) = Poss(a, s’)], T t= (Vu, s, s’).SameStare(s, s’) II [SF@, s) = SF(u, s’)], T /= (Vu, s, s’).Poss(a, s) A SameStute(s, s’) > SameStute(do(a, .s), do(a, s’)). Proof. Trivially from the definition of basic action theories. [? Lemma A.2. Let T be a context-free action theory with finite number of parameterless actions. Under the consistency condition following (4), there is a natural number N such that the set ( ]]do(<, SO) I] 1 6 is a list of actions} contains less than N elements, where for any list of actions 6, 218 E Lin, H.J. Levesque /Art$cial Intelligence 101 (19981 201-226 Ild4C, So)lI = {do&‘, so) I T k so 6 do(C, So) A So < do@‘, So) 3 SumeStute(do(~, So), do(c’, SO))}. In other words, the number of possibly diflerent legal states is bounded by N. Proof. To simplify our proof, without has only two actions A and B. Consider an arbitrary state axiom is as (3). By the consistency condition loss of generality, we assume that the action theory fluent F(x’, s). Suppose its successor (4), we have T I= F(?, do([l, So)) = F(k So), T + SO ,< do([Al, SO) > F(;, do([Al, SO)) = False if YF (2, A), True ifv$(g, A), F(k So) I otherwise, True if yF+(i, B), T I= So < doUB1, So) 3 F(2, do([Bl, So)) = False if v,(;, B), [ F(;, So) otherwise, T I= So 6 d&A, 4, So) 3 True True if yc(?, A) A yyF(;, B), if yFf(2, B), F(?, do([A, Bl, So)) = False if yF(.?, A) ~-y$(.?, B), False if v,(;, B), F(k So) otherwise, T i= So < do([B, Al, So) 3 True True if yT(2, B) A-)/;(;, A), if y:(;, A), F(% do(EB, Al, So)) = False if yF(2, B) ~-yg(?, A), False if y,(i, A), F(% So) otherwise. Furthermore T I= So 6 doHA, Al, So) 3 F(k doGA, Al, So)) = F(% doCAl, So)), T t= So 6 do([B, Bl, So) 3 F(% do([B, Bl, So)) = F(t do([Bl, So)), T t= So < do([A, B, Al, So) A So 6 do([B, Al, So) 3 E Lin, H.J. Levesque /Artijcial Intelligence 101 (1998) 201-226 219 F(i, do@, B, Al, So)) = F(% do@% Al, So)), 7’ k So < doGA, B, Bl, So) 3 F(k do([A, B, Bl, So)) = F(k do(M 4, So)), T I= So < do([B, A, Al, So) 3 F(%o([B, A, Al, So)) = F(% dofIB, Al, So)), T k So 6 do([B, A, 4, So) A So < do([A, 4, So) 3 F(-t do([B, A, Bl, So)) = F(t doGA, 4, So)). Since the fluent F is arbitrary, the finiteness of the set in question follows. 0 Proof of Theorem 1. Suppose G is achievable there is a recursive controller We’ll show that there is ajnite achieves G according to T, and let controller that achieves G in S according in S according to T. We need to show that to T. In fact, we can do better. that that achieves G. Let C be a robot controller S = (a 1 C-J is a terminating run of C under the environment determined by a model of T at S}. Then the following controller I if 0 is not a prefix of any history in S, C’(Q) = stop if c E S, (A.1) a I if for some B, CJ o (a, j?) is a prefix of a history in S, is clearly well (uniquely) defined, and achieves G in S as well. Furthermore, then C’ is finite, thus recursive. We now show that S is indeed finite. if S is finite, Observe, however, that even though there are only finitely many states, we cannot bound the length of a run by removing “loops” starting and ending in the same state (and guarantee the finiteness of S this way). This is because a controller may be using pure sensing actions which do not change the state to obtain information. So we need a slightly more complex approach. Given a set of histories 3-1, and a history CJ E E, a segment some ~1 and 02, is said to be determinate of D’, then DI o r is a prefix of 0’. In other words, the underlying 7-1 according outcome of the actions that the empty sequence in r. Notice segment of any history 0 with respect to any ~1 and 02 above. t in c: 0 = 01 o t o 02 for if for any 0’ in ‘H, whenever (~1 is a proper prefix controller determined by the alternative is trivially a determinate to (A. 1) with S replaced by ‘Ft, if any, does not need to consider Given any run 0 E S, we can decompose it into 0 =ol O(~l~~l>~‘~‘~~k~(~k,~k)~~k+l (A.21 such that (1) OI>..., ~+t (2) 01 o(o1,/4)3..., are determinate segments of 0 in S. crk 0 (CQ, j3k) are not determinate segments of c in S. Clearly, this decomposition (I) For any cr’ E S, if is unique. Furthermore, it has the following properties: cr’ = a; 0 ((2; ) p;> 0 . . .o a; 0 (c&) &J 0 o,;+, 220 E Lin, H.J. Levesque /Arti&ial Intelligence 101 (1998) 201-226 is a similar decomposition of r~‘, and (al,Bl)0...O(ak,Bk)=(a;,B;)O..~O(~:,,BI)~ c = 0’. Suppose then I < i 6 k. We show by induction on 0 < i < k that the above equation holds, then m = k, ai = ai, and ,8i = ,6/, 01 O(~l~Bl>O~ ~~O~~O(~~,~~)~~~O(~/l~~~)O~~~O~~O(~j~~~)~ The base case is trivial because both of them are empty sequence. Suppose ~]O(~],~])O~~~O~~O((;Y~,~~)~~~~(~~~~~)O~~~o~~o(~~~~~)~ we show that cl 0 (al 3 Bl I 0 . O ai+l O C&i+1 2 Bi+l I =a; 0 (a;>/$) o-~;+, 0 (olj+],B;+])> O 4+, that is, ai+] = pi’,, . Because aj+] is a determinate segment, by definition, we have that CJ] o (o], B]) o ... o ai+] must be a prefix of (5’. Similarly, 0; o (a;, /3;) o . is a proper is a prefix of c. So either ai+] = a;+] or one of them prefix of the other. The latter is impossible because otherwise, say ci+] is a proper prefix of cri’,] , then o:+, cannot be a determinate segment of o, which violates our assumptions. This proves that 01 0 (aI 3 PI) 0 . . . o,o~,,,~~)=a;o(a;,~;)O...Oa~o(a;,~~). Finally, o = cr’. For otherwise, one of them would be a proper prefix of the other, which is impossible because (II) For any 1 < i < j < k, if OQ = oj, they are both runs of C under some environment. then TpS<end(t,j-] Ooj,S)> SU??Zf?StUte(e?Zd(~i_] OCJi,S),etd(~j-1 OCTj,S)), where for any 0 6 m < k, cfn is the history: For otherwise, suppose SameStute(end(&~ 0 Di, S),UUi(~j-1 0 C7j, S)). (A.3) that aj o (oj, pj) would have to be a determinate segment of c, a We claim that we made about Eq. (A.2). To show that contradiction with the assumptions segment of o , suppose cr’ E S, and cj - 1 is a proper aj o (oj , /3j) is a determinate segment prefix of cr’. Then <j_] o aj must be a prefix of a’ for oj is a determinate of o. Because both c# and o are terminating runs of the controller C, for some /?, cj_] o oj o (oj, B) must be a prefix of &. NOW let I be the model of T under which (T’ is the terminating run of the controller C. Since c’ E S, by Definition 9, 1 + S < end(a’, S). Thus by (A.3), I + SameStUte(end(&] 0 Oi, S). ULd(<j-1 0 Gj, S)) E Lin, H.J. Levesque /Artijicial intelligence 101 (1998) 201-226 221 Since oi = aj, by Lemma A. 1 B = pi. Similarly, prefix of (T’. This shows that Oj o (aj, @j) is a determinate /lj = pi. SO /3 = pj. Thus <j is a segment of 0. Thus by Lemma A.2 and the above property (II), for any run o in S, if it is decomposed as (A.2), then k < M x N, where M is the number of actions, and N is the upper bound (I), this of the number of possible equivalent that S must be finite. implies classes given in Lemma A.2. By property Appendix B. Proof of Theorem 2 We shall prove a more general result than Theorem 2. To that end, we introduce a relation Rexit(r, s, s’) meaning in s’. It is defined in the way similar to Rdo in Section 4, as an abbreviation of the following second- order formula: that when started in s, the program r will exit (abort) Rexit(r, s, s’) %f (VP)[ . > P(s, s’, O)]. where the ellipsis in Section 4. From this definition, of any basic action theory: is exactly the same as in (5), i.e., the conjunction of six conditions given it is easy to verify that the following are consequences -Rexit@, s, s’), Rexit(&, s, s), Rexit(brunch(a, rl, Q), s, s’) 3 Poss(a, s) A [SF(a, s) > Rexit(q, do(u, s), s’)] A [-SF(u, s) II Rexit(r2, do(u, s), s’)]. The purpose of introducing Rexit is for the following lemma: Lemma B.l. Let T be an action theory, and I a model of T. For any ground situation terms terms S and S’, I f== Rdo(&(rl, q), S, S’) iff there are some ground situation Sly..., S,, II 3 3, such that undS’=S,. (1) S=St (2) For any 1 < i < n - 3, Z + Rdo(rl, Sit &‘+I). (3) I k Rexit(rl, Sn-2, Sn-l), and I t= Rdo(r2, Sn-l, S,). Similarly, I b Rexit(loop(q, undS’=S,. (1) S=St (2) For any 1 < i < n - 3, I /== Rdo(q, Si, $+I). (3) I t= Rexit(rl, Sn-2, Sn_l), and I + Rexit(r2, $_I, S,). rz), S, S’) ifs there are some Sl, . . , S,, n 3 3, such that Proof. By induction on the structure of rl . q In the following, a history o is called an exiting run of a system of the system, and C(D) = abort. The following case. theorem (C, E) iff it is a run includes Theorem 2 as a special 222 F: Lin, H.J. Levesque /Art$cial Intelligence 101 (1998) 201-226 Theorem B.2. For any robot program r and any ground situation effective robot controller C such that for any interpretation term S’: term S, there is an I and any ground situation (1) (a) (b) (2) (a) (b) is a terminating run o of (C, &) such If Z + Rdo(r, S, S’), then there S’ = end(a, S); if I + Rexit(r, S, S’), then there is an exiting run o of (C, E) such that S’ = end(o, S), where E is the environment determined by I at S. If there is a terminating S’ > Rdo(r, S, S’); if there is an exiting run o of (C, E) such that end(a. S) = S’, then I + S 6 S’ > Rexit(r, S, S’), where & is the environment determined by I at S. run o of (C, &) such that end(a, S) = S’, then I + S < that Proof. We shall construct a recursive robot programs, satisfies the two conditions the structure of robot programs: such that for any robot program r, har(r, function r : P x R -+ A+, where P is the set of that inductively on o) is a robot controller in the theorem with respect to r. We define r f (nJ, 0) = y l f-;e;;se if c = E otherwise, r(branch(a, t-1, rz), a) = a r(q,a’) r(rz,a’) l_ ifa=& ifo=(a,l)oa’ ifa=(a,O)oa’ otherwise, r(looph, r2), a) , f(r2, a”) ifforsomen>O,o=ato...oa,oa’oa”suchthat r(rt.ai)=stopforl <i<n, andr(q,a’)=abort. = . f(rl, 0’) is a history otherwise, where 0 suchthatforsomen>O,a=cqo~~~oo;,oa’, f(rl,q)=stopforl<i<n, proper prefix a” of 0’ such that f (rl , CT”) = stop. andthereisno Clearly, for any program r, ha r (r, a) is a recursive function. We now show that ha r (r, a) satisfies the two conditions in Theorem B.2. We do so by induction over the structure of programs. l r is nil. For any ground situation - term S, and any interpretation I: I b (Vs)(Rdo@, S, s) = s = S) A -(Zls)Rexit(niJ, S, s). this, and the definition of f (9, a), From trivially satisfied. the two conditions in the theorem are l r is exit. This case is analogous to the case of nil: For any S, and any I, - I + (Vs)(Rexit(&t, S, s) = s = S) A -@s)Rdo(&, S, s). F: Lin, H.J. Levesque /Artijcial Intelligence 101 (1998) 201-226 223 l r is branch(a, r-1, r2). Inductively, we assume that the two conditions o) and haZ’(r2, a). Let S be an arbitrary ground situation interpretation. I b Rdo(branch(a, Suppose are satisfied for term, and I an rl, Q), S, S’). By the definition krT(rl, arbitrary of Rdo, there are two cases: (1) I + SF(a, S) A Rdo(r1, do(u, S), S’). By inductive assumption, nating run t of har(r1, such that S’ = end(t, do(a, S)). By our construction (u, 1) o t is a terminating determined by Z at S. r” Furthermore, a) under the environment run of Aar(brunch(u, there is a termi- determined by I at do(a, S) rl, r-2), a), of r(branch(a, rl, r2), a) under the environment end((a, 1) o r, S) = end(r, do(a, S)) = S’. S) A Rdo(r2, do(a, S), S’). Analogous to the previous case. (l)(a) of Theorem B.2. The proof of (l)(b) run of har(brunch(u, t is a terminating is analogous. ~1, Q), a) under I determined by Z at S. Again, there are two cases: (2) Z b -SF(a, This proves condition To prove (2)(a), suppose the environment (1) Z + SF(a, S). By our construction, of ha r(rl assumption, , a) under the environment t = (a, 1) o t’, and r’ is a terminating run determined by I at do(a, S). By inductive Z I= do(u, S) < S’ > Rdo(q, do(a, S), S’), where S = end(t’, do(u, S)) = end(t, S). By I + Poss(u, S) A SF(u, S) A Rdo(q, do(u, S), S’) > Rdo(branch(u, rl, rz), S, S’), we have I + S < S’ > Rdo(brunch(u, rl, rz), S, S’). (2) I b -SF(u) S). Analogous This proves (2)(a). The proof of (2)(b) is again analogous. to the previous case. l r is loop(r1, t-2). The proof for this case is exactly like that for the branch case, but using Lemma B. 1. q Appendix C. Proof of Theorem 3 We restate Theorem 3: Theorem 3. For any TM basic action theory T, any effective controller C, and any ground term S, there is a robot program r such thatfor any model Z oj‘T and any ground situation situation S’, we have: (1) Zf Z b Rdo(r, S, S’), then there is a terminating run o of the system (C, I) such that clean(s) = end(a, S), where & is the environment determined by Z at S. lo This makes use of easy lemma that if r is a terminating &(a, S), and I + SF(a. S), then (a, 1) o r is a terminating S, where C’ is a controller such that C’(E) = a and C’((a, I) o o) = C(o). run of C under the environment determined by I at run of C’ under the environment determined by I at 224 (2) F: Lin, H.J. Levesque /Artificial Intelligence 101 (1998) 201-226 If there is a terminating that S’ = end(a, S), then there is a situation S” such that S’ = clean(S”), and I + S < end(a, S) > Rdo(r, S, S”), where & is the environment determined by I at S. run o of the system (C, E) such Proof. The proof involves using the fluents Marked and lot to emulate a Turing machine tape. If Murked(n, s) holds, then the nth position of the tape will be 1, and otherwise, 0. To indicate the state of some segment of the tape, we will show a sequence of OS and Is, with the current pair of OS and 1s. Thus if the initial segment of the tape in SO is @loll, +Iurked(l, taken to be the position of the first digit of the underlined So), Murked(3, So), etc. then Zoc(Su) = 1, So), lMurked(2, (i.e., lot(s)) location We assume without loss of generality that there are only two action A and B. We can encode a history as follows: OOQl/$ . . .a,&ll, - where Ui is 01 or 10 for action A or action B respectively, and where /?i is 01 or 10 for outcome 0 or 1 respectively. Since C is a recursive there is a Turing machine M it. Given a history encoded as above, it will terminate with the tape looking that computes like function, where a! is either 0 0 (for abort or I), 11 (for stop) or 0 1 or 10 as above for A or B. Since there is a robot program the five special actions have the same power as a Turing machine, rM that consists of only Turing actions that senses and marks the tape in exactly this way. that achieves the same goal as the controller C. that writes z on We can now describe l For any fixed string z of OS and Is, let write(z) be the robot program the robot program the tape and sets lot to the position just right of z. It is defined inductively by: write(e) = &l; write(Oz) = x(eruse, write(lz) =&mark, g(right, write(z))); @(right, write(z))). l The robot program home is such that whenever the tape encodes some history like: OO@lBl.. .%B& home will reset Zoc to 1: loop(seq(left, branch(read_mark, &left, @, %(lefi, branch(read_mark, nil exit)))), _1_ nil). l The robotprogram ini is [write( 0 0 11) ; home]. ’ ’ 1 1 For any robot programs r and I’, r ; r’ means executing r followed by Y'. Formally, the “;" notation is defined inductively: [branch(a, ‘1,~); [loopPI. r2); rl =loop(ri, lr2: rl). r] = branch(u, [i-j; r], [r2; r]); F Lin, H.J. Levesque /Artijiciul Intelligence 101 (1998) 201-226 225 . The robot program rA performs the action A, and writes either 0 111 or 10 11 on the tape depending on the sensing result returned: brunch(A, [w&(1011); home], [write(Olll); home]) The program rg is analogous. l The robot program interpret is branch(read_mark, x(right, branch(read_mark, g(right, c&), %(right, rB ))h g( right, brunch( read-mark, g(right, s&right, rA 1, loop(nil,nil))))). l Finally, the desired robot program r is [ini; loop([rM ; interpret], nil)] where rM is the robot program associated with a single step of the effective controller C. the empty history, and then returns the robot program r, starting at some home position on the tape, first writes to the home position. Next, within a To paraphrase, 0011 encoding loop, it repeatedly uses rM to place a suitable u at the end of the history, and then interprets this (II: if it is 0 0 it goes into an infinite loop; if it is 11 it exits the loop (and so terminates); if it is 10, it performs action A, writes either 0 111 or 10 11 on the tape depending on the if it is 0 1, it does the same as sensing result returned, and then returns to the home position; B.Note thatwhenczis OlorlO,theeffect of writing 0111or1011 abovebutforaction on the tape depending on the result of action A or B ensures that the tape now encodes an extended history which is then ready for the next iteration. It can be verified that from this construction the conditions in the theorem follow. q References [ I] E. Davis, Knowledge preconditions 121 0. Etzioni, S. Hanks, D. Weld, D. Draper, N. Lesh and M. Williamson, An approach for plans, J. Logic Comput. 4 (5) (1994) 721-766. to planning with of the incomplete Third International Conference, Cambridge; MA, Morgan Kaufmann, San Mateo, CA, 1992, pp. 115-125. in: Principles of Knowledge Representation and Reasoning: Proceedings information, 131 R.E. Fikes and NJ. Nilsson, STRIPS: a new approach to theorem proving in problem solving, Artificial Intelligence 2 (197 1) 189-208. [4] C. Green, Theorem proving by resolution as a basis for question-answering in: B. Meltzer and D. Michie (Eds.), Machine Intelligence, Vol. 4, Edinburgh University Press, Edinburgh, 1969, pp. 183-205. systems, [5] Y. Lesperance, H. Levesque and F. Lin. Ability and knowing how in the situation calculus, 161 H. Levesque, R. Reiter, Y. Lesptrance, F. Lin and R. Scherl, GOLOG: a logic programming in preparation. language for dynamic domains, J. Logic Programming 3 1 (l-3) in the presence of sensing?, (1997) 59-83. 171 H. Levesque, What is planning in: Proceedings AAAI-56, Portland, OR, 1996, pp. 1139-I 146. 1 S] F. Lin. Applications of the situation calculus cut operator, Nagoya, Japan, Morgan Kaufmann, San Mateo, CA, 1997, pp. 1412-1418. in: Proceedings Fifteen International to formalizing control and strategic Joint Conference on Artificial information: Intelligence the prolog (IJCAI-97). 226 F: Lin, H.J. Levesque /Artificial Intelligence IO1 (1998) 201-226 [9] F. Lin, An ordering on subgoals for planning, Ann. Math. Artificial Intelligence 21 (1997) 321-342. [lo] F. Lin, On measuring plan quality, in: Proceedings Sixth International Conference on Principles of Knowledge Representation and Reasoning (KR’98), 1998. [ 111 F. Lin and R. Reiter, State constraints [ 121 F. Lin and R. Reiter, How to progress a database, Artificial Intelligence 92 (l-2) [ 131 K. Krebsbach, D. Olawsky and M. Gini, An empirical revisited, study of sensing and defaulting 1st Conference on AI Planning Systems, San Mateo, CA, 1992, pp. 136144. [14] Z. Manna and R. Waldinger, How to clear a block: a theory of plans, J. Automated Reasoning 3 (1987) in planning, Proceedings in: J. Logic Comput. 4 (5) (1994) 655-678. ( 1997) 13 1-167. in: 343-317. [IS] J. McCarthy and P.J. Hayes, Some philosophical in: B. Meltzer and D. Michie (Eds.), Machine 1969, pp. 463-502. problems intelligence, from the standpoint of artificial Intelligence, Vol. 4, Edinburgh University Press, Edinburgh, [ 161 E. Mendelson, An Introduction [17] E.P. Pednault, ADL: exploring to Mathematical Logic, Van Rostrand Reinhold Company, New York, 1964. in: the middle ground between STRIPS and the situation Proceedings (KR’89), Toronto, Ont., Morgan Kaufmann, San Mateo, CA, 1989, pp. 324-332. First International Conference on Principles of Knowledge Representation calculus, and Reasoning [18] J.S. Penberthy and D.S. Weld, UCPOP: a sound, complete, partial order planner for ADL, in: Proceedings (KR’92), of Knowledge Representation International Conference Third Cambridge, MA, Morgan Kaufmann, San Mateo, CA, 1992, pp. 103-l 14. and Reasoning on Principles [19] M. Peot and D. Smith, Conditional nonlinear planning, in: Proceedings 1st Conference on AI Planning Systems, San Mateo, CA, 1992, pp. 189-197. [20] R. Reiter, The frame problem result Computation: Papers in Honor of John McCarthy, Academic Press, San Diego, CA, 1991, pp. 359-380. in the situation calculus: a simple solution (sometimes) and a completeness Intelligence and Mathematical in: V. Lifschitz (Ed.), Artificial regression, for goal Theory of [21] R. Reiter, Proving properties of states in the situation calculus, Artificial [22] R. Scherl and H. Levesque, The frame problem and knowledge-producing Intelligence 64 (1993) 337-35 1. actions, in: Proceedings AAAI- 93, Washington, DC, AAAI Press/The MIT Press, CA, 1993, pp. 689-695. [23] M. Schoppers, Building plans to monitor and exploit open-loop and closed-loop dynamics, in: Proceedings 1st Conference on AI Planning Systems, San Mateo, CA, 1992, pp. 204-213. [24] S. Thomas, PLACA, an agent oriented programming language, Ph.D. Thesis, Department of Computer Science, Stanford University, 1993. [25] W. van der Hoek, B. van Linder and J.-J.Ch. Meyer, A logic of capabilities, in: A. Nerode and Y. Ma- the 3rd International Symposium on the Logical Foundations of tiyasevitch (Eds.), Proceedings LFCS-94, Computer Science, Springer, Berlin, 1994, pp. 366378. 