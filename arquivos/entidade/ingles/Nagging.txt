Artificial Intelligence 140 (2002) 71–106www.elsevier.com/locate/artintNagging: A scalable fault-tolerant paradigm fordistributed searchAlberto Maria Segre a,∗, Sean Forman b, Giovanni Resta c,Andrew Wildenberg da Department of Management Sciences, The University of Iowa, Iowa City, IA 52242, USAb Mathematics and Computer Science Department, Saint Joseph’s University, Philadelphia, PA 19131, USAc Istituto di Informatica e Telematica, Consiglio Nazionale delle Ricerche, I-56124 Pisa, Italyd Computer Science Department, SUNY Stony Brook, Stony Brook, NY 11794, USAReceived 5 April 2001; received in revised form 23 January 2002AbstractThis paper describes nagging, a technique for parallelizing search in a heterogeneous distributedcomputing environment. Nagging exploits the speedup anomaly often observed when parallelizingproblems by playing multiple reformulations of the problem or portions of the problem against eachother. Nagging is both fault tolerant and robust to long message latencies. In this paper, we show hownagging can be used to parallelize several different algorithms drawn from the artificial intelligenceliterature, and describe how nagging can be combined with partitioning, the more traditional searchparallelization strategy. We present a theoretical analysis of the advantage of nagging with respectto partitioning, and give empirical results obtained on a cluster of 64 processors that demonstratenagging’s effectiveness and scalability as applied to A∗search, αβ minimax game tree search, andthe Davis–Putnam algorithm. 2002 Published by Elsevier Science B.V.Keywords: Parallel/distributed search algorithms; Search pruning; Game tree search; Branch and bound;Boolean satisfiability1. IntroductionMany artificial intelligence problems of practical interest can be posed in terms ofsearch. Not surprisingly, the development of a robust network infrastructure coupled* Corresponding author.E-mail addresses: segre@cs.uiowa.edu (A.M. Segre), sforman@sju.edu (S. Forman), resta@iit.cnr.it(G. Resta), awilden@cs.sunysb.edu (A. Wildenberg).0004-3702/02/$ – see front matter  2002 Published by Elsevier Science B.V.PII: S 0 0 0 4 - 3 7 0 2 ( 0 2 ) 0 0 2 2 8 - X72A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106with the advent of reasonably-priced computing equipment has helped focus attentionon techniques that use multiple processors operating in parallel to improve searchperformance. Some of this work has centered on developing application-level toolkits toaccess a distributed computing environment as well as resource-management tools thatenable an application to exploit computing resources that span organizational boundaries(e.g., “grid computing”) [2,11,15,22].Aside from differences in enabling technology, most attempts to parallelize search arequite similar, involving some sort of partitioning [13,14]. The general idea is that eachprocessing element takes responsibility for a portion of the search space, and that theindividual solutions to these subproblems can then be compared or composed to obtaina solution to the original problem. Different partitioning schemes usually differ in theirtarget computer architecture (e.g., SIMD vs. MIMD, shared memory multiprocessors vs.networks of workstations, etc.) and how they address a number of technical problems, suchas load balancing (how best to assign work in order to exploit all available processors all ofthe time) and fault tolerance (how to notice and recover from the momentary inaccessibilityor outright loss of one or more processing elements). Nevertheless, partitioning is the basisof a diverse set of projects, including SETI@Home, the search for radio signal evidenceof extraterrestrial life, GIMPS, the search for Mersenne prime numbers, and various codebreaking efforts from distributed.net.Unfortunately, many problems do not partition well. One reason is that informationacquired early in a serial search process can often be used to reduce the amount of searchperformed overall: for example, consider how the α and β values are used to prune thegame tree in αβ minimax search. If the search is partitioned, the information acquiredwhile searching one subspace may come too late to help reduce the search in anothersubspace explored simultaneously, resulting in a search that may actually be slower withmultiple processors than it is with a single processor.1 This problem is an instance ofthe more general speedup anomaly problem, first studied for branch-and-bound stylealgorithms on two well-known NP-hard problems in [19]. A speedup anomaly occurs whena solution is obtained more slowly with more processors than it is with fewer processors,or, alternatively, when a solution is obtained superlinearly faster with multiple processors.Later, such superlinear speedups were routinely studied, particularly within the parallellogic programming and theorem proving communities (see, e.g., [7]).This paper describes a distributed search paradigm called nagging that exploitsthe speedup anomaly often observed when parallelizing problems by playing multiplereformulations of the problem or portions of the problem against each other. Nagging hasseveral advantages over partitioning techniques; it is intrinsically fault tolerant, naturally1 Even if the information were made available in a timely fashion, sharing the information among multipleprocessors would entail some amount of communication overhead. In general, the cost of communication tendsto increase as the number of processors increases. Depending on the underlying architecture, sharing informationmay involve interprocessor communication or the use of shared memory. For shared-memory multiprocessors,there are practical design limits on the number of processors one can incorporate in a single machine. Formore loosely-coupled processors, interprocessor communication (either in directed or broadcast form) requiresoverhead for generating and servicing messages; furthermore, as the number of processors increases, highermessage latencies associated with larger networks generally entail larger communication overheads.A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10673load-balancing, requires relatively brief and infrequent interprocessor communication, andis robust in the presence of reasonably large message latencies. These properties help makenagging suitable for use on geographically-distributed networks of processing elements.Originally developed in the course of our work on distributed automated deduction [35,39,40], we show here how nagging can be generalized and applied to a broad range of searchalgorithms from the artificial intelligence literature. We develop an analytical performancemodel comparing nagging and partitioning, and use this model to make some predictionsabout their respective performance. Finally, our performance claims are justified via anempirical evaluation of nagging and partitioning on three well-known yet significantlydifferent search algorithms; A∗ search [20], αβ minimax game tree search [27], and theDavis–Putnam search algorithm [6].2. Nagging and searchNagging is an asynchronous parallel search pruning technique where a single masterprocessor (or master), performing some standard search procedure, is advised by one ormore nagging processors (or naggers), each performing identical search procedures, aboutportions of its master’s search space that need not be explored.Let us consider a search procedure that is designed to find a globally-optimal solutionin a finite, implicitly-defined, search space; this is the case, for example, when trying todetermine the best next move to make in game tree search with fixed horizon (similararguments hold for situations where any legal solution suffices, such as for theoremproving or satisfiability problems). At initialization, each nagger obtains the problemspecification from its master processor. It then engages in a series of nagging episodes,each initiated by the nagger when it becomes idle, until the master has completed itssearch.A nagging episode begins when the nagger requests a snapshot of its master’s currentstate, which is concisely described by communicating the sequence of choices madethrough the predefined search space (see Fig. 1). The master selects a nagpoint (a nodealong its own current path from which the nagger will begin its own exploration of thespace) according to some predefined nagpoint selection criteria; it then communicates thisnagpoint and the value describing its own current best solution to the nagger.The master and the nagger now race to exhaust their respective search spaces; whilethe two search spaces are semantically equivalent, they may well be searched differently,leading to different expected solution times. If we are searching for an optimal solution,there are only three possible outcomes to consider:(1) Abort: If the master backtracks over the nagpoint before the nagger completes its ownsearch, the master signals the nagger to abort the nagging episode, which causes thenagger to once again become idle and initiate a new nagging episode.(2) Prune: If the nagger completes its search and finds there are no better solutions thanthat already known by the master, then the nagger interrupts the master and forces it tobacktrack to the nagpoint, resulting in a reduction in the master’s search space.74A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106Fig. 1. Nagging episode. The square node indicates the current position of the master process, which is executinga depth-first search. A nagpoint is selected along the master’s search path, and is described to the nagging processby communicating the series of choices from the root of the search tree to the nagpoint. The nagger reconstructsthe master’s search space up to the nagpoint and then commences exploring its own, transformed, version of thespace rooted at the nagpoint.(3) Solve: Finally, should the nagger find a better solution than that communicated by themaster, it can abort its own current search and report the new value to the master sothat the master might use this new value to reduce its own search space.Of course, for nagging to have the greatest possible positive effect on the master’s searchefficiency, we would prefer the second and third cases occur with high probability, and wehope the first case occurs only rarely. Should the second and third cases never occur, wecan expect no improvement in the master’s expected time to solution: indeed, the master’ssearch will surely be less efficient due to the small—but measurable—additional overheadof servicing its naggers.Two techniques are applied to improve the odds. First, a nagger can be naggedrecursively by yet another processor in order to help it exhaust its own search space morequickly. Second, and more importantly, each nagger may apply a problem transformationfunction to reduce, in practice, the size of its search space while retaining at least someof the information content implicit therein (note that effective problem transformationfunctions are typically dependent on both the search algorithm in use and the problemdomain itself). We will discuss both of these techniques later in this paper.The main characteristics of nagging are clear even within this simplified context.First, nagging does not require explicit load balancing, since idle machines alwaysinitiate nagging episodes to keep themselves busy. As long as the master processor isA.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10675still searching, new nagpoints can be provided at will. Second, nagging is intrinsicallyfault tolerant: since the master’s search is unaffected should a nagger fail or becomeinaccessible, losing a nagger will never compromise the correctness or quality of thesolution produced by the master, nor will the master ever stop and wait for the missingnagger. Finally, communication is brief, since even deep search states can be describedconcisely as a sequence of choices, while the results reported by a nagger often reduce to asingle bit (e.g., prune/don’t prune) or just a few bits (e.g., a new value for the best solutionto date).3. NICE: A Network Infrastructure for Combinatorial ExplorationTo support our work on nagging, we have developed NICE, the Network Infrastructurefor Combinatorial Exploration. NICE is specifically designed to support both naggingand partitioning for search algorithms; it allows application programmers to parallelizetheir search procedures by making appropriate function calls within their code. The NICEdistribution includes niced, a Unix resource management daemon, niceq, a daemon statusquery program, and niceapi, the applications programmer’s interface library. The code iswritten in ANSI C with BSD sockets over TCP/IP, and is known to run on multiple variantsof the Unix operating system, including Linux, Sun OS, and HPUX.The NICE daemon, or niced, must be running on every participating machine, whethermaster or nagger. Typically, it is started automatically as part of the system boot process,and runs as long as its host CPU is running (the daemon itself is single threadedand extremely lightweight, having no noticeable impact on system performance). NICEdaemons are arranged hierarchically, with each daemon reporting to a single parent daemonwhile answering to zero or more child daemons. The NICE daemon fulfills three primaryfunctions:(1) The daemon maintains contact with the NICE hierarchy, occasionally exchanging hostload and availability information with its parent and child daemons, and managingfailure recovery should its parent and/or child daemons become unreachable orunresponsive. Each daemon may also initiate local reorganization of the NICEhierarchy in a greedy attempt to enhance overall search performance according to eachhost’s current actual load.(2) The daemon provides the interface through which a qualifying application mayrequest additional processors. It is also responsible for security, certifying whichapplications on which hosts are allowed to request support from other processors,which executables can be run on the local host, as well as which files, if any, canbe accessed locally.(3) The daemon manages the local host’s resources according to prespecified host-specificconstraints: for example, some hosts may be available only during specified times,such as during nighttime hours. Thus the daemon must decide when a processor canrespond to requests for new processes, and must also manage previously spawned butstill running processes, putting them to sleep and later waking them when the hostbecomes available once more.76A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106A NICE-enabled application communicates with the NICE infrastructure via a set ofcallable functions contained in the NICE applications programmer’s interface. This link-able library contains functions that, when invoked, request new copies of the application bespawned on other machines. It also contains functions to support communication betweenapplications and between the application and the NICE daemon. Note that the library doesnot actually contain code for the search algorithms, but rather only the handful of functionsthat are needed to parallelize—via either nagging or partitioning—appropriately designedserial search algorithms.Both the NICE daemon and NICE API represent fairly mature software efforts: versionsof the NICE daemon have been running continuously on our systems for several years, withno noticeable impact on performance. The code is very robust, with NICE daemons runningreliably and unobtrusively for periods of many months between system restarts. But moreto the point, while the NICE infrastructure represents a necessary enabling technology thatdirectly supports research in distributed search algorithms without the additional featuresprovided in more general toolkits such as PVM [2] or MPI [15], the more interestingparallelization issues are algorithmic ones. What is perhaps most remarkable about NICEis that such a simple and lightweight infrastructure naturally supports parallelization of abroad array of search algorithms.4. Applications of naggingWhile, as we have seen in Section 2, the main idea that underlies nagging is quitesimple, there are a number of important details (e.g., the design of appropriate problemtransformation functions) that have a direct effect on how well nagging works. These issuesare best discussed in the context of specific search algorithms: here, we show how threewell-known search algorithms drawn from the artificial intelligence literature—the Davis–Putnam algorithm, the A∗ search algorithm, and the αβ minimax search algorithm—canbe parallelized using nagging.4.1. The Davis–Putnam algorithmFirst proposed by Davis and Putnam, and later refined by Loveland, the Davis–Putnamalgorithm is the fastest known solution technique for Boolean satisfiability problems thatis both sound and complete.2 We say it is complete because if there is a solution, theDavis–Putnam algorithm guarantees that solution will eventually be found. Other fastsolution methods for satisfiability problems such as GSAT, WSAT or simulated annealingare local search procedures that are sound, but not complete [36]. Note that completeness2 Recall a Boolean variable is a variable that can only be true or false. A Boolean formula consists of variablesrelated via the usual logical connectors ¬, ∧, ∨, → and ↔; a formula is in conjunctive normal form (CNF) if itis a conjunction of clauses, where each clause is a disjunction of literals, and each literal is a Boolean variableor its negation. By definition, the SAT-CNF problem (determining whether or not a given CNF Boolean formulais satisfiable) is NP-complete; that is, it is possible to certify a solution is correct in polynomial time, but it iscommonly believed that actually finding a solution requires exponential time [5].A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10677does not necessarily imply an exhaustive search; rather, if the given problem is satisfiable,any solution is equally correct, so we can terminate the search as soon as any solution isfound. The search space must only be searched exhaustively when we need to guaranteean unsatisfiable problem has no solution. This kind of search-until-first-solution behaviorarises most often in automated deduction or theorem proving environments; precisely thecontexts where nagging was first proposed.The Davis–Putnam algorithm solves a particular type of Boolean satisfiability problem,usually called SAT-3-CNF or simply 3SAT, that deals only with Boolean formulas havingat most three literals per clause (note that any Boolean formula that can be expressed inCNF can also be expressed in 3CNF by direct manipulation along with the addition ofsome number of new Boolean variables) [16]. The general idea is simple; if there are atotal of N variables, then one can systematically examine all 2N possible combinations oftruth assignments in order to determine if at least one of these truth assignments satisfiesthe formula.Two observations serve to reduce the number of variable combinations the Davis–Putnam algorithm need look at in practice. First, if a partial solution having m variablebindings is inconsistent (that is, fails to satisfy one of the clauses), then all 2(N−m)completions of this partial solution must also be inconsistent and can be safely pruned:there can exist no solution in this portion of the search space. Second, if a partial solutioncontains the negations of any k − 1 literals from a given k-literal clause, then, if there isto be any hope of finding a satisficing solution, the lone remaining literal in the clausemust be satisfied by binding its variable appropriately. Of course, once bound, the newlybound variable may force other variable bindings as well, thus effectively reducing thesearch space: this process is called unit propagation. Thus the Davis–Putnam algorithmoperates by systematically examining combinations of truth assignments, with periods ofunit propagation occurring whenever possible (see Fig. 2).What is the expected solution time for this algorithm? In principal, larger formulaedefine a larger search space and therefore entail longer solution times. In practice, however,exactly how much time is required depends on more subtle characteristics of the specificproblem instance (e.g., ratio of number of variables to number of clauses as well as thedistribution of variables within the clauses themselves). Put simply, not all like-sized 3SATproblems are equally hard [26,43]. Some problem instances can be easy (imagine, forexample, a Boolean formula in CNF where every clause shares a single literal), while othersof exactly the same size may result in exponential-time performance. In practice, the orderwith which variable settings are tried has a critical effect on the time to solution. Numeroussplitting rules, or heuristics for “good” orderings, exist, but no universal rule will work forall problem instances: existence of a universal splitting rule providing polynomial timeperformance on all 3SAT problem instances would imply P = NP.Parallelizing the algorithm of Fig. 2 using partitioning is relatively straightforward: weexplore recursive calls to search() on separate processors as long as additional processorsare available. Of course, we can’t really know a priori how large each individual subspaceis, as some branches may quickly lead to an inconsistent partial solution. This means that itis hard to ensure that the work is fairly distributed over the available processing elements.Furthermore, without some notion of subspace size, it is difficult to determine whether aprocessor assigned to a subspace has actually failed, gone offline, or is simply taking a78A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–1063sat(F : formula) : boolean;return(search(F , extractVars(F ));search(F : formula, S: variables) : boolean;V : variable ← head(S);C: clause;if (F = ∅) then return(true);elseif (∃C ∈ F | size(C) = 0) then return(false);elseif (∀C ∈ F | V /∈ C ∧ ¬V /∈ C) then return(search(F, S\{V }));elseif (search(propagate(substitute(F , V )), S\{V })) then return(true);elseif (search(propagate(substitute(F , ¬V )), S\{V })) then return(true);else return(false);substitute(F : formula, V : variable) : formulaG : formula ← ∅;C : clause;for C in Fif(V /∈ C)G ← G ∪ (C\{¬V });return(G);propagate (F : formula) : formulaC : clause;while(∃C ∈ F | size(C) = 1)F ← substitute(F, head(C));return(F );Fig. 2. Davis–Putnam algorithm for 3SAT. The problem instance is a Boolean formula F in 3CNF representedas a list of clauses, where each clause is a list of at most three variables, and each variable is either a literal ora negated literal. The search() function operates recursively, removing satisfied clauses from F until either thereare no more clauses left in F or one of the clauses in F is shown to be unsatisfiable. The splitting rule is encodedexplicitly in the ordering and pattern of negations in the list S, which initially contains all of the literals in F ,some of which may be negated. Variables whose values are set “early” by unit propagation are “skipped” in thethird clause of the large conditional statement. The substitute() function constructs and returns G, a new copy ofF with satisfied clauses filtered out and references to the negated sense of variable V removed from the remainingclauses. The functions head() and size() functions return the first element and the cardinality of their argument,respectively, and the function extractVars() returns a list of the variables contained in the given formula.long time to search what turned out to be an unexpectedly large space. Addressing loadbalancing and fault tolerance issues can only add to the overhead costs associated withpartitioning [32].Compare this partitioning strategy with nagging (see Fig. 3). At initialization, eachnagger is provided with the original Boolean formula that specifies the 3SAT problemwe wish to solve. A nagging episode begins when the nagger requests a nagpoint from itsmaster, who briefly interrupts its own search to select a random nagpoint. Each nagpointcorresponds to one of the partially-instantiated Boolean formulae considered by the masterin the course of its recursive calls to search(). Upon receiving the nagpoint, the masterresumes its own search, while the nagger first applies a problem transformation functionto the nagpoint (e.g., by reordering the list of as-yet-unbound variables, or by randomlyA.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10679status ≡ true | false | abortnag3sat(F : formula) : boolean;N : formula;result : status;niceInit();if(niceRoot()) then return(searchExplicit(F , extractVars(F )));elsewhile(true)N ← niceIdle();result ← searchExplicit(N, transform(extractVars(N)));if (result = true) then niceSolve(N, result);elseif (result = false) then nicePrune(N);Fig. 3. Sketch of nagging implementation for the algorithm of Fig. 2. The call to niceInit() connects to theNICE infrastructure and ensures copies of this process are spawned on all participating processors; it alsoensures spawned processes are provided with copies of the original problem. The root process (the functionniceRoot() returns true only on the root processor) performs the normal Davis–Putnam search, while non-rootprocesses engage in a series of nagging episodes, each initiated when niceIdle() requests a new nagpoint, denotedN . The function searchExplicit() is logically equivalent to the search() function of Fig. 2, but with explicitstack manipulation and interrupt handling capabilities for processing messages to/from parent/child processes(convenient primitives to support this functionality are provided in the NICE API). For the nagging processors,the transform() function randomly reconfigures the splitting rule as described in the text. Depending on theoutcome of the nagger’s search, the nagger may pass a solution to its parent (function niceSolve()) or force itsparent to backtrack (function nicePrune()). Note that a nagger’s search may also be interrupted by its parent ifthe parent exhausts the space rooted at the nagger’s nagpoint before the nagger does; in this case, searchExplicit()would immediately return abort, and the nagger would simply request a new nagpoint.inverting their logical sense, thereby switching the order in which V and ¬V are explored),and then begins its own search. Should the nagger find an assignment that makes theformula true, it interrupts the master and provides the solution, which the master can thenin turn provide as the solution to the problem. Should the nagger instead exhaust its spacewithout finding a solution, it can interrupt the master and force the master to backtrack pastthe nagpoint. Should the master backtrack over the assigned nagpoint before the naggercompletes its own search, the master should abort the nagger, who is then free to seek anew nagpoint from the master.Of course, simply racing the nagger against the master may not produce usefulspeedups; what is really needed is a good problem transformation function that willincrease the nagger’s chances of beating the master within the subspace defined by thenagpoint, hence reducing the master’s own search space. The insight is that a serialsearch procedure must necessarily commit to searching a single incarnation of the currentproblem’s search space, while alternate versions of the same search space may entaildiffering effort to search. Since expected time to solution for a given problem instanceis critically dependent on the splitting rule used, changing the splitting rule may lead thenagger to complete its search more quickly than the master.To gauge if such a strategy might succeed, we can, as a first approximation, empiricallyexamine how a random permutation applied to both variable selection and descendentordering (i.e., a random splitting rule) affects the overall solution time. We randomly80A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106Fig. 4. Davis–Putnam algorithm applied to 100 randomly-generated “difficult” 3SAT problems of various sizes.Each problem is solved twice, once using the default search ordering (tstd) and the second time with thepermutation transformation applied before solving (trnd ). Results are shown on a log–log scale for clarity. Hadall the datapoints been tightly clustered along the diagonal trnd = tstd line, the probability of a nagging processorbeating the master would be low; since this is not the case here, permutation appears to be a good candidateproblem transformation function for this particular domain.generate 100 “difficult” 3SAT problems of varying sizes [37]. For this demonstration,time to solution is measured and recorded twice for each 3SAT: once using the defaultsearch ordering (tstd) and the second time with the random permutation transformationapplied before solving (trnd). The results are shown in Fig. 4; each datapoint in the graphrepresents the CPU time required to solve the permuted problem (ordinate) plotted againstthe CPU time for default problem (abscissa). Datapoints appearing below the diagonaltrnd = tstd line represent problems that were solved more quickly with the permutationfunction applied, while those falling above the diagonal were solved more quickly usingthe default ordering.We observe that the random splitting rule beats the default splitting rule about half thetime, sometimes by a significant margin (note the plot uses a log–log scale for clarity,although unfortunately this somewhat obscures the magnitudes of the differences). Thedatapoints do not lie tightly clustered about the diagonal line: the farther off the line theyare, the greater advantage one might expect to see in a nagging system using this problemtransformation function.3 This demonstration illustrates how speedups are possible even ifonly a single nagging episode is allowed per problem and even if the selected nagpoint isalways the root node of the search. Indeed, since each nagging episode gives you another3 Of course, this is just an example; more effective problem transformation functions might make use ofalternative splitting heuristics, or might even elect to throw away a subset of clauses in order to decrease thesize of the search space. In the latter case, solutions in the reduced space no longer correspond to solutions ofthe original Boolean formula; however, failure to find a solution for this smaller space still implies no possiblesolution exists for the original space, so the master can still be forced to backtrack.A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10681chance of beating the default splitting rule, multiple nagging episodes over a broad arrayof properly selected nagpoints should have a better chance of improving a system’s overallsearch performance, an effect we’ll observe in the experiments of Section 6.4.2. A∗ searchConsider the well-known traveling salesperson problem, or TSP. The problem is assimple to state as it is hard to solve:Given a collection of N points, find the shortest tour that visits each point and returnsto the original starting point.TSP problems lie hidden in a surprisingly large number problems from operations researchand engineering; since TSP is known to be NP-hard, much research has focused onappropriate algorithms and the use of heuristics to find good yet less-than-optimal solutionsquickly [33]. Yet devising an algorithm that is guaranteed to provide the (globally) optimalsolution is simple, if one is willing to accept poor worst-case performance. Here, weexamine one such optimal algorithm for Euclidean TSP (i.e., where the points lie inEuclidean space) based on A∗ search, a heuristically-guided branch-and-bound searchstrategy.4The obvious solution technique is to enumerate all possible tours and then return theshortest one. Starting with an arbitrary 3-point tour and using symmetry to reduce thespace, it is easy to see that there are (N − 1)!/2 different possible tours. We can do a littlebetter by showing that the H points defining the convex hull of the point set must appearin fixed order in any optimal tour; by starting with the convex hull (as opposed to a random3-point tour), we can reduce the size of the search space, slightly, to (N − 1)!/(H − 1)!. Ineither case, an exhaustive search algorithm operating on this search space will require, bySterling’s formula, O(N N ) time.The basic insight required to turn this simple enumeration algorithm into a branch-and-bound algorithm is that information garnered during the search can be used to reduce thecombinations that must be examined. If the cost of the current partial solution is greaterthan that of the shortest solution found so far, we can exclude all completions of thepartial solution from the search—since, in Euclidean space, adding more points to thetour can only make it longer—without sacrificing optimality. We can do even better byincorporating a heuristic estimate of the cost to complete a partial tour, pruning subtreesrooted at partial tours where the partial tour cost plus an estimate of the additional costrequired to complete the tour exceeds the cost of the current best solution. If the heuristicestimate used always underestimates the true additional cost of the partial solution, we saythe heuristic is admissible, and it can be shown that the resulting A∗ search algorithm will4 Note that we are not recommending this as a solution technique for TSP problems encountered in practice;rather, we are using TSP as an intuitively accessible example with which to describe the parallelization of A∗search. Most real applications would be better served by using one of the many efficient heuristic algorithmsfor TSP that yield good, but not optimal solutions, although advanced cut techniques have been combined withpartitioning strategies to find optimal solutions to problems as large as 15,000 points [1].82A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106tsp(S : points) : tourH : tour ← convexHull(S);return(search(H , (S\points(H )), ∅));search(T : tour, S : points, B : tour) : tourP : point;if(S = ∅ ∧ (B = ∅ ∨ cost(T ) < cost(B))) then return(T );elseif(S = ∅) then return(B);elseif(cost(T ) + estimate(S) (cid:1) cost(B)) then return(B);elseP ← head(S);for i from 0 to |T | by 1B ← search(insert(P , i, T ), (S\{P }), B);return(B)∗algorithm for Euclidean TSP. The tsp() function takes a set of points S as input and returns theFig. 5. Alowest-cost tour. The heart of the code is the recursive function search(), which takes a partial tour T , a set Sof points yet to be visited, and B, the lowest-cost tour found so far, and recursively explores the space rooted atthat partial tour. Note that if no solution better than B can be found below T , then there is no need to search anyfurther. The cost() function returns the cost of its tour argument, or 0 if the argument is the null tour ∅, whileestimate() returns a lower bound on the additional cost of adding the points in its argument to any existing partialtour. As before, the function head() returns the first element of its argument, while points() returns the point setof its tour argument and insert() inserts a new point P into the ith position of partial tour T . Parallelization ofthis algorithm with nagging follows in a manner similar to the sketch given in Fig. 3.still return the optimal solution while exploring no more nodes (and generally far fewernodes) than branch-and-bound search (see Fig. 5) [28].As for the Davis–Putnam algorithm, applying partitioning to the serial algorithm ofFig. 5 is relatively simple: the idea is to explore different recursive calls to search()on separate processors [4,23,31]. Of course, load balancing and fault tolerance issuesare present here, just as with the Davis–Putnam algorithm. But there are also additionalcomplications due to fact that A∗ search is not just a satisficing search (i.e., search tofirst solution) but rather an optimizing search (i.e., search to best solution). The differenceis that every node in the search space must be either searched or safely pruned so thatthe search procedure can certify that no better solution exists (compare with the Davis–Putnam algorithm, where only unsatisfiable formulae entail an exhaustive search, whilefinding any solution to a satisfiable formula terminates the search immediately). The neteffect is to make the subspaces more interdependent: to see why this is so, consider whathappens when a new (and presumably favorable) best solution is found in the first subspaceexplored by a serial search process. Clearly, the new solution’s lower cost may lead tosignificantly less search in subsequent subspaces. When these subspaces are searched inparallel, however, this new best solution may not be discovered until most of the workin the other subspaces has already been performed. And even if the new best solution isfound soon enough to have an impact on the concurrent subspace searches, its cost mustbe communicated to all the other processors, entailing some additional communicationoverhead.A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10683We now consider parallelizing the serial algorithm of Fig. 5 using nagging; the generalidea is identical to that of Fig. 3 for the Davis–Putnam algorithm. Idle naggers request anagpoint (corresponding to a partial tour passed in one of the recursive calls of the search()function still on the master processor’s calling stack) and search this space in parallel.Unlike the partitioning case, there are no load balancing or fault tolerance problems here:nagpoints are generated whenever they are solicited, and lost or unresponsive naggers can’taffect the master’s own search. Of course, as with the Davis–Putnam algorithm, the key toeffective nagging—that is, nagging that actually provides some speedup—is an effectiveproblem transformation function. One reasonable approach might be to randomly perturbthe order in which points are inserted into the growing partial tour (e.g., by reorderingthe list of as-yet-unvisited points), or perturb the order in which descendent nodes aregenerated (e.g., by changing the insertion order within the loop of the searchExplicit()function). Alternatively, we might rely on problem-specific knowledge and choose amixture of the many TSP-specific heuristic ordering strategies from the operations researchliterature. Or one might even adopt an abstraction transformation, where a nagger simplythrows away a certain number of points in the point set. This transformation can stillprovide speedup if the optimal tour in the abstracted search space is longer than the master’scurrent best solution (in Euclidean space, the cost of the optimal tour for the reducedproblem is a lower bound to the cost of the optimal tour on the original points). We’ll lookat how nagging compares with partitioning for the Euclidean TSP algorithm just describedin Section 6.4.3. SPAM: αβ minimax searchHistorically, much research within the artificial intelligence community has focusedon the problem of playing zero-sum two-player games, such as chess or checkers. Mostof this research involves derivatives or variants of αβ minimax search, which is itselfa straightforward refinement of the original notion of minimax search. Within this field,various forms of parallelism have also received a lot of attention [3,8,9,17]. Here, we showhow the αβ minimax search algorithm can be parallelized using nagging, producing analgorithm we call SPAM, for Scalable Parallel Alpha-Beta Minimax.The idea underlying any minimax procedure is to generate the tree of legal moves toa fixed depth (given by the ply argument) and evaluate the quality of the resulting boardpositions using a static board evaluation function, or SBE.5 The SBE looks at a board andreturns a value on [−∞, ∞], with −∞ representing a loss and ∞ representing a win forthe specified player. By selecting the maximum/minimum values at alternating levels, theseleaf SBE values can be propagated up to the root of the tree, by definition a maximizinglevel, where they can be used to select the branch leading to the best attainable outcome forthe current player. Deeper searches generally lead to more informed choices; unfortunately,5 There exist numerous alternative formulations of the minimax search procedure. This particular formulationwas selected for its simplicity. Of course, any formulation of the minimax algorithm assumes that what is good forone’s opponent is symmetrically bad for the player him or herself, and that the opponent is a rational one, makingdecisions based on an identical SBE. Also, note that our formulation only returns the best SBE value found in thetree of given ply rooted at the given board. In practice, the procedure should also return the corresponding move.84A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106alphabeta(B : board, P : player, D : integer) : scorereturn(search(B, P , D, −∞, +∞)search(B : board, P : player, D : integer, α : score, β : score) : scoreM : move;if(won(B, opponent(P ))) then return(α);elseif(D = 0) then return(SBE(B, P ));elsefor M in legalMoves(B, P )α = max(α, −search(applyMove(B, M), opponent(P ), D − 1, −β, −α);if(α > β) return(α);return(α);Fig. 6. “Negamax” formulation of αβ minimax search algorithm. Return the best possible score player P can hopeto attain in the search space of D depth rooted at board B. Pruning occurs when α exceeds β within the loop overthe possible legal moves at this board position, causing the function to return immediately. We assume the won(),legalMoves(), SBE(), and applyMove() functions have the appropriate game-specific definitions; opponent()returns the other player, and max() has the usual semantics.the size of the game tree that must be examined increases exponentially with the depth ofexploration. Thus for meaningfully large games (e.g., chess) this exponential growth makesall but the most shallow searches impractical.The intuition underlying αβ pruning is to exploit information generated during theexploration of one portion of the search space to justify skipping or pruning other parts ofthe space. Judicious pruning can extend the computational horizon under which the searchoperates, allowing deeper searches in the same amount of computation time. Of course, onecan’t really beat the exponential nature of the search: there will always be some practicalmaximum on the depth of search. The idea is to extend that maximum as much as possibleby reducing the number of branches that must be examined.Extending the minimax algorithm to perform αβ pruning is relatively straightforward;the basic idea is to pass two additional parameters called α and β, which serve as boundson the interesting values at any given node (see Fig. 6). Paths whose leaf values areguaranteed not to fall in the interval [α, β] (initially [−∞, +∞]) would never be chosen bythe minimax procedure and therefore can be safely ignored. As the game tree is searched, αvalues will only increase and β values will only decrease, further constraining the search. Itshould also be clear that, by construction, αβ minimax search produces identical choices tominimax search in all cases. Furthermore, as with A∗ search, the search reduction producedby αβ pruning depends on the order in which paths are searched; for some pathologicalgame trees, αβ minimax and standard minimax will search identical game trees.6 Froman analytic perspective, the number of calls to the SBE will vary between roughly 2bd/2(the best case) and bd (the worst case) for a game tree of depth d with uniform branching6 Indeed, many real-world implementations of αβ minimax search try to improve performance by generatinginternal choice points in an ordered fashion, usually guided by a cheap, fast, secondary SBE function appliedto the board positions represented by these internal nodes. Of course, board positions which look “bad” locallymay actually turn out to be “good” several ply deeper, so internal choice point reordering is nothing more than agreedy optimization technique that may lead to—but does not guarantee—more efficient search.A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10685factor b [18]. Thus while the exponential factor is still present, it may be significantlyreduced, allowing deeper, more informed, searches in the same amount of time.While implementing αβ minimax search requires only minor extensions to thebasic minimax procedure, these modifications make attempts to parallelize the searchsignificantly more complicated. This is because minimax search is easily decomposedat any node, and game trees rooted at each child node can be considered independentlyby separate processing elements. Once child SBE values are computed in parallel, it is asimple matter to compare these estimates and select the best alternative. In contrast, for αβminimax search, partitioning and parallel exploration of the game tree may well requiremore time than the corresponding serial search. This is because the savings realized byαβ pruning over standard minimax search result from exploiting the information obtainedsearching one part of the game tree while exploring another (like for A∗ search, but unlikefor the Davis–Putnam algorithm). Even if one could instantly share new α and β valueswith all of the processing elements at zero communication cost, the new values may cometoo late to matter. In short, the parallel algorithm might well search a greater number ofnodes than would the serial algorithm.The SPAM algorithm exploits all of the search constraints embodied by the α and βvalues while keeping communication between processing elements infrequent and brief.The nagging algorithm (see Fig. 7) is essentially identical to that described for A∗ searchnagpoint = {B : board, P : player, D : integer, α : score, β : score}nagalphabeta(B : board, P : player, D : integer) : score, β(cid:26) : score;N : nagpoint;result, α(cid:26)niceInit();if (niceRoot()) then return (searchExplicit(B, P , D, −∞, +∞)elsewhile(true)(cid:26), β(cid:26)} ← narrow(N.α, N.β)N ← niceIdle();{αresult ← searchExplicit(N.B, N.P , N.D, α(cid:26), β(cid:26)if (result > α(cid:26) ∧ result < β(cid:26)elseif (result = α(cid:26)elseif (result = β(cid:26)) then niceSolve(N, result);) then niceRestrict(N, N.α, result);) then niceRestrict(N, result, N.β);)Fig. 7. Sketch of nagging implementation for the algorithm of Fig. 6. As for Fig. 3, the function searchExplicit() isidentical to the search() function of Fig. 6, but with explicit stack manipulation and interrupt handling capabilities.The root process performs the normal αβ minimax search, while non-root processes engage in a series of naggingepisodes, each initiated when niceIdle() requests a new nagpoint. Nagpoints are again denoted N , but here consistof a board position, the next player to move, a search depth limit, and α and β values. For the nagging processors,the narrow() function randomly reduces the search range as described in the text, and serves as a part of theproblem transformation function in combination with permutations, provided by modifying the nagger’s copy ofthe legalMoves() function (Fig. 6) to randomly perturb the sequence of legal moves generated. Depending onthe outcome of the nagger’s search, the nagger may pass a solution to its parent (function niceSolve()) or forceits parent to refine its own α or β parameters (function niceRestrict()). Note that a nagger’s search may also beinterrupted by its parent if the parent exhausts the space rooted at the nagger’s nagpoint before the nagger does;in this case, searchExplicit() would immediately return abort, and the nagger would request a new nagpoint.86A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106and the Davis–Putnam algorithm, except that here we will exploit a problem transformationfunction based on both permutation and window narrowing. The idea is that a nagger canartificially restrict its master’s αβ window; using a narrow interval [α(cid:26), β(cid:26)] (where α < α(cid:26)and β(cid:26) < β) ensures that the nagger’s search procedure will prune aggressively, exploringonly a relatively small number of nodes in the search space, thereby increasing the oddsthat it will beat the master (of course, we can also permute the node exploration orderas well).7 In exchange for the reduction in search, the value computed by the nagger’stransformed search may not always be directly substituted for the true value that wouldbe computed by the master for the same subspace. More precisely, if the nagger returns avalue that is greater than α(cid:26) and less than β(cid:26), then this corresponds to the true value thatwould be computed by the master, and we can force the master to backtrack and continueits search from that point on. But if the nagger returns a value less than α(cid:26) (alternatively:greater than β(cid:26)), then it means that the true value lies between α and α(cid:26) (alternatively:β(cid:26) and β). This information can be used to reduce the master’s search space by setting themaster’s β to α(cid:26) (alternatively: α to β(cid:26)), which may well lead to immediate improvement ofthe search efficiency for the master’s current search. In Section 6 we’ll empirically examinethe behavior of SPAM with this problem transformation function.5. AnalysisWe now introduce simple analytical models of both nagging and partitioning that helpto explain how and when nagging can be expected to provide a performance advantageover more traditional partitioning methods. Our analysis relies on techniques developed forreliability data analysis; specifically, understanding how a problem transformation functionused defines a probability distribution of solution times for a specific problem [21,24,25].Let us assume that a random variable x drawn from a distribution X represents thesolution time of a specified problem under a given problem transformation function. Ofcourse, the exact nature of X depends on the search algorithm and problem transformationfunction applied; we’ll examine different choices for X later. The behavior of x can bedescribed by its density function f (x) and its corresponding cumulative density functionF (t), which measures the probability that the solution time x is less than some specifiedtime value t:t(cid:1)F (t) = Pr(x (cid:2) t) =f (x) dx.x=0(5.1)Note that the physical interpretation of x imposes certain constraints on allowable valuesfor f (x) and F (t): more precisely, f (x) (cid:1) 0, f (x) = 0 for x (cid:2) 0, F (0) = 0, andF (∞) = 1 follow from the fact that real problems are never solved in less than zero timeand the probabilistic semantics of F (t).We can use this statistical model to study the behavior of the coarsest possible form ofnagging, where each of the n nagging processors operates on an identical copy of the entire7 In the limit, where α(cid:26) + ε = β(cid:26)this reduces to zero-window search [29].A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10687problem instance. In essence, the n naggers are racing to find a solution, each operating ona different, solution-equivalent, transformation of the original space.8 Once any processorcompletes its search, the solution it finds (or fails to find) applies without modificationto the original problem instance. In practice, multiple naggers usually search different,randomly selected, nodes along the master’s current search path in service of a mastersearch process; furthermore, some of the more effective transformation functions in usemay not be solution equivalent (see, e.g., the abstraction transformation of Sections 4.1and 4.2, and the window narrowing transformation of Section 4.3).Let the random variable vn represent the time elapsed before one of the n independentprocessors finds the problem solution. Clearly, for the coarse nagging model, vn =min(x1, x2, . . . , xn) where each xi is an independent random variable drawn from theoriginal distribution X. Let Gn(t) be the corresponding cumulative density function ofvn. Since vn = min(x1, x2, . . . , xn), and each xiis independent, Gn(t) represents theprobability that at least one of the xi values is less than t, which is 1 minus the probabilitythat all the xi values exceed t. Thus using Eq. (5.1):Gn(t) = Pr(vn (cid:2) t) = 1 −1 − F (t)(cid:4)(cid:3)= 1 −1 − F (t)(cid:4)n.n(cid:2)(cid:3)i=1(5.2)Taking the derivative of Gn(t) with respect to t evaluated at vn yields the density functiongn(vn):gn(vn) = nf (vn)(cid:3)1 − F (vn)(cid:4)n−1.(5.3)A similar argument can be made to construct a coarse model of partitioning. Technically,the argument is somewhat more problematic, since once a problem is partitioned anddistributed to different processors, each processor is indeed solving a different problem,whose solution time distributions might vary significantly from the original one. However,as is the case with nagging, we can make reasonable assumptions to support some crude—but still informative—comparisons between the two models.First, we assume that the original problem, whose solution time is still describedby a random variable x drawn from a distribution X, is partitioned in n subproblemseach having identically-sized search spaces; essentially, we’re claiming a perfect a priorisolution to the load balancing problem. Second, we assume that fault tolerance is not anissue, and that all processors actually will terminate their search and return their partialsolutions. Third, we assume that run times scale linearly, that is, that the run time of asubproblem of size 1/n is governed by x/n, where the random variable x refers to thesolution time of the original problem. Finally, we assume that the cost of merging thesubproblem solutions together to form a solution to the original problem is negligible. Thelast assumption is the most problematic, since for NP-hard problems the cost is likely tobe high if the merger is even feasible. However, since it is our intent to compare this model8 A solution-equivalent transformation is one that transforms the original space without losing any existingsolutions or adding spurious solutions; the permutation transformation is a good example of a solution-equivalenttransformation, while the abstraction transformation of Section 4.2 and the window narrowing transformation ofSection 4.3 are not.(5.4)(5.5)88A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106of partitioning with nagging, we can afford to make generous assumptions on behalf ofpartitioning without compromising the essence of our comparisons.With these assumptions in place, the time required to solve the partitioned problemunder this coarse partitioning model is described by a random variable wn defined aswn = max(x1/n, x2/n, . . . , xn/n), since it is necessary to solve each of the subproblemsbefore merging their solutions together.9 Proceeding in the same fashion as for nagging, weobtain the density function hn(wn) from the cumulative density function Hn(t) as follows.Since Hn(t) represents the probability that wn < t, i.e., all the x/n are less than t, we have(cid:6)(cid:5)Hn(t) = Pr(wn (cid:2) t) =< t= F (nt)nn(cid:2)Pri=1xinfrom which we obtainhn(wn) = n2f (nwn)(cid:3)(cid:4)n−1.F (nwn)Note that g1(v1) = h1(w1) = f (x) and thus G1(t) = H1(t) = F (t), just as one shouldexpect given that the single processor system is the trivial case of both the coarse naggingand coarse partitioning models.Once the appropriate distribution X has been fixed, it is relatively easy to makeperformance comparisons between serial execution, coarse nagging and coarse partitioningby comparing their expected solution times.5.1. The uniform distributionThe first sample distribution we look at is the simplistic case where X is the uniformdistribution with values x ranging between two constants tlo and thi.10 For this uniformdistribution, the density function isf (x) =1(thi − tlo)for tlo (cid:2) x (cid:2) thi and f (x) = 0 elsewhere. It easy to see thatt(cid:1)F (t) =x=tlo1(thi − tlo)dx = t − tlothi − tlo(5.6)(5.7)9 Unfortunately, the model becomes more complicated for satisficing—as opposed to optimizing—search.Consider the Davis–Putnam algorithm: if the Boolean formula is true, the algorithm will terminate when thefirst subproblem that finds a solution terminates, or wn = min(x1/n, x2/n, . . . , xn/n). On the other hand, if theBoolean formula is false, the search will have to exhaust the entire search space to guarantee no solution isoverlooked, and thus the required time should be wn = max(x1/n, x2/n, . . . , xn/n) as given in the text. Thus amixture model that blends these two cases together might provide a more appropriate model of satisficing search.10 From a practical perspective, the uniform distribution is not of great interest, since there is relatively littlereason to believe solutions times of real problems would fit. Nonetheless, it does serve as useful point ofcomparison for the other distributions considered later in this paper.A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10689for tlo < t < thi, F (t) = 0 for t (cid:2) tlo and F (t) = 1 for t (cid:1) thi. Applying Eq. (5.3), we obtainthe density function for an n-processor coarse nagging system:gn(vn) = n(thi − vn)n−1(thi − tlo)n.(5.8)In a similar fashion, but using Eq. (5.5) for the n-processor coarse partitioning model, weobtainhn(wn) = n2 (nwn − tlo)n−1(thi − tlo)n.5.2. The exponential distribution(5.9)Using the same approach we can consider other, more realistic, probability distributions.Here we look at an exponential distribution with a fixed minimum time tlo and decayparameter λ. The exponential distribution has long been used to model equipment failure inreliability studies; it follows from a uniform random failure pattern, modeled as a Poissonprocess [24]. This distribution’s density function is given byf (x) = λe−λxe−λtlo= λeλ(tlo−x)with cumulative density functionF (t) = −eλ(tlo−t ).Appropriate substitution in Eq. (5.3) yieldsgn(vn) = nλeλn(tlo−vn)for the coarse nagging case, while Eq. (5.5) produceshn(wn) = n2λeλ(tlo−nwn)for the coarse partitioning case.(cid:3)1 − eλ(tlo−nwn)(cid:4)n−1(5.10)(5.11)(5.12)(5.13)5.3. The lognormal distributionRecently, some have characterized the observed behavior of backtracking search onsatisfiability problems using distributions of the Pareto–Lévy form. Such distributionsdiffer from the exponential distribution used in Section 5.3 because they are heavy tailed,that is, their complementary cumulative density function 1 − F (t) decays slower thanexponentially. Heavy tailed distributions have been used to justify a random restart strategy(a sort of single processor version of coarse nagging) for satisfiability problems [12].Many different heavy-tailed distributions are used in reliability analysis, although themost commonly used are the Weibull and the lognormal distributions (others includethe Gumbel or extreme value distribution, the Birnbaum–Saunders distribution, etc.). Thekey question remains how to choose which distribution best models the observed searchbehavior—not only for satisfiability problems, but for all the search problems studiedhere. Fortunately, exploratory data analysis techniques for testing distributional adequacy90A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106such as the Kolmogorov–Smirnov or the (somewhat more sensitive) Anderson–Darlinggoodness-of-fit tests are well known [38], and support our use of the lognormal distributionfor this analysis.11Consider a lognormal distribution with fixed minimum time tlo, scale parameter µ, andshape parameter σ . The distribution’s density function isf (x) =1σ (x − tlo)√2π−(log(x−tlo)−µ)22σ 2.e(5.14)The cumulative density function can be expressed in terms of Φ, the cumulative densityfunction of the standard normal distribution, or erf, the error function(cid:6)(cid:6)(cid:5)(cid:5)F (t) = Φlog(t − tlo) − µσ= 12+ 12erflog(t − tlo) − µ√2σ.(5.15)Appropriate substitutions can be made into Eqs. (5.3) and (5.4) to obtain gn(vn) andhn(wn), although the resulting expressions are not terribly informative since expressionscontaining erf are notoriously difficult to simplify.5.4. Comparing nagging and partitioningWe are now ready to make direct comparisons between performance estimates forcoarse nagging and for coarse partitioning for some range 1 . . . n of processing elements.One simple comparison is to look at the performance ratios, defined as the ratio of the serialexpected solution time E(x) to the parallel expected solution time E(vn) (for nagging) orE(wn) (for partitioning), where the expected solution times E(x), E(vn) and E(wn) aresimply the average elapsed times.A more meaningful statistic is the expected speedup, defined as the expected valueof x/vn for nagging (alternatively: x/wn for partitioning). We say this metric is moremeaningful because it represents the expected speedup observed in an experiment whereserial performance is compared directly with parallel performance on each individualproblem. We define a new random variable φn = x/vn (alternatively: ψn = x/wn) andcompute its expected value E(φn) (alternatively: E(ψn)). Since both φn and ψn are ratios,we consider their geometric means, that is∗E(φn) = eE(log(φn))(5.16)11 We generated four sets of 100 datapoints each by solving a single problem for each of A∗/TSP, Davis–Putnam/3SAT/unsatisfiable, Davis–Putnam/3SAT/satisfiable, and SPAM/Othello 100 times using a strictlysolution-equivalent problem-transformation function (i.e., permutation in this case). We then applied theAnderson–Darling test to see which of the set of tested distributions (normal, lognormal, exponential, Weibull,Gumbel, and logistic) were consistent with the observed data. In all but two cases, the Anderson–Darling testrejected (p < 0.05) all of the tested distributions except for the lognormal distribution (the exceptions are theDavis–Putnam/3SAT/satisfiable data, where the Anderson–Darling test rejected all but the lognormal and Weibulldistributions, and the SPAM/Othello data, where the Anderson–Darling test rejected all but the lognormal andGumbel distributions). While these tests are not entirely conclusive (they are, after all, based on just a fewrandomly-generated problems), they do seem to suggest that the lognormal is well suited to modeling the rangeof search behaviors studied in this analysis.A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10691andE∗(ψn) = eE(log(ψn))(5.17)rather than E(φn) or E(ψn) directly, since the geometric mean is more representative ofthe expected speedup over many such trials. Exploiting the additive properties of expectedvalues and the fact that x and vn (alternatively: x and wn) are independent, we obtain:(cid:3)Elog(φn)(cid:4)(cid:4)(cid:3)log(x)(cid:3)log(vn)(cid:4)− E= Ethi(cid:1)thi(cid:1)=log(x)f (x) dx −log(vn)gn(vn) dvn(5.18)x=tloand, similarly,(cid:3)Elog(ψn)thi(cid:1)(cid:4)=vn=tlothi(cid:1)log(x)f (x) dx −log(wn)hn(wn) dwn.(5.19)x=tlown=tlo/nUnfortunately, the formulae for E∗(φn) and E∗(ψn) are often quite complex in the gen-eral case. However, values for both E∗(φn) and E∗(ψn) are easily tabulated for spe-cific values of n, which lend themselves to graphical comparison (see Fig. 8). Qual-itatively speaking, Fig. 8 makes clear the noticeable scaling advantage of naggingover partitioning within this analytical model, especially for the exponential and log-normal distributions, which correspond more closely to distributions observed in prac-tice.Another interesting metric is suggested by closer examination of the performance ratiofor coarse nagging in the exponential distribution case:E(x)E(vn)=tlo + 1λtlo + 1nλ.(5.20)We note that, when both λ and tlo are small, the performance advantage obtained by coarsenagging can—on average—approach n, or linear speedup. This implies that coarse naggingcan be expected to provide superlinear speedups with respect to a serial search about halfthe time in the exponential distribution case. For heavy-tailed distributions, the advantageof coarse nagging is even more decisive, providing some theoretical justification for theobserved effectiveness of random restart strategies on serial processors.More formally, it is interesting to compute and compare the probability that a coarsenagging or partitioning system will exhibit superlinear speedup with respect to the averagesequential case, which is easily expressed as:(cid:6)(cid:6)(cid:5)(cid:5)vn (cid:2) E(x)nPr= GnE(x)nfor nagging, and(cid:5)wn (cid:2) E(x)nPr(cid:6)(cid:5)= Hn(cid:6)E(x)n(5.21)(5.22)92A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106Fig. 8. Expected speedups E∗(φn) and E∗(ψn) vs. number of processing elements for uniform (tlo = 0.01,thi = 1.0), exponential (tlo = 0.01, λ = 2), and lognormal distributions (tlo = 0.01, µ = 0.5, σ = 1.5). Thisstatistic illustrates the scaling advantage of nagging over partitioning within this simple analytic model for allthree distributions studied, and how the advantage of nagging grows as the distribution becomes more heavytailed.for partitioning. As before, while the resulting expressions may be difficult to simplify, itis easy to tabulate values for n = 2, . . . , 10 (see Table 1).It is clear that, for all of the distribution models studied here, the coarse model ofnagging retains its potential for producing superlinear speedup as the number of processorsA.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10693Table 1TechniqueDistributionn = 2n = 3n = 4n = 5n = 6n = 7n = 8n = 9n = 10NaggingUniformPartitioning UniformNaggingPartitioningNaggingPartitioningExponentialExponentialLognormalLognormal0.430.250.620.400.850.600.410.130.620.250.880.460.390.060.610.160.890.360.380.030.600.100.900.280.370.020.590.060.910.210.360.010.590.040.910.170.360.000.580.030.910.130.350.000.570.020.910.100.340.000.560.010.910.08is increased, while the already limited potential of partitioning to do so rapidly vanishes asmore processors are added.Of course, the analytic models presented here are relatively simple, and do notcorrespond exactly with how nagging and partitioning are actually applied in practice.We’re mostly interested in how well the observations made using the coarse models hold upin more realistic situations: for example, we might nag or partition recursively, use problemdistribution functions that are not strictly solution equivalent (e.g., window narrowing orabstraction), or elect to nag or partition multiple times per problem at internal nodes of thesearch tree rather than just once at the root node. We turn to these rather more practicalquestions in the next section, using experimentally-obtained quantitative data to supportour claims about nagging’s performance in a principled manner.6. Empirical evaluationEmpirical studies, if carefully done, can give a realistic picture of a system’s behavior.Here, we focus on performance issues, using experimental data to contrast the relativeperformance of nagging and partitioning as well as to support our claims regarding thescalability of nagging.6.1. Experiment 1The first experiment compares implementations of nagging and partitioning that arepurposefully designed to evoke the coarse analytic models of the previous section. Theexperimental procedure is straightforward. First, for each of the three tested algorithms(Davis–Putnam/3SAT, A∗/TSP, and SPAM/Othello), 100 randomly-generated problemsare solved serially using a fixed search order on a 450 MHz Celeron machine runningthe Linux operating system.12 Next, a second 450 MHz Celeron machine is added to theNICE hierarchy, and each problem is solved twice more, once using the second machine12 The random problem sets were generated to provide a good cross-section of solution times ranging from0.01 seconds (the resolution of the Linux system clock) to roughly 20 minutes on a single processor system.Davis–Putnam/3SAT problems ranged from 120 to 140 variables (with between 514 to 604 clauses), and were, asmentioned in Section 4.1, intended to be “difficult” problems, while the randomly-generated A∗/TSP problemsranged from 29 to 33 cities. The SPAM/Othello problems consisted of random, legal, mid-game Othello boards(having 18 to 22 pieces placed) searched to an 8 or 9 ply horizon.94A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106as a nagging processor and once using is as a partitioning processor instead. For eachproblem, the solution found and elapsed processor time used by the master processor (asreturned by the ANSI C clock() function) are recorded. Any search that is not completedby a prespecified resource limit is marked as censored, and the best solution found so faris returned for comparison.Like the coarse analytic models of the previous section, the only problem transformationfunction used here is the permutation transformation, which is used for all three algorithms(we’ll examine other transformation functions in Section 6.2). Unlike the coarse analyticmodels, however, for optimizing search algorithms (SPAM/Othello and A∗/TSP), morethan one nagging event may occur in the course of the experiment. While naggingalways takes place at the root node, a nagger that finds a better solution is allowed toimmediately report its new bound to the master and begin a new nagging event, applyingthe newly obtained bound and a new permutation transformation to the root node oncemore.A similar change is made for the partitioning case: to address the load-balancing issuesnormally associated with partitioning, we use an asynchronous partitioning protocol thatis similar to that used for nagging. As with nagging, an idle slave processor initiates theprocess by requesting additional work from its master. Instead of a nagpoint, however, themaster provides its slave the last available sibling node of the “highest” available ancestornode along its own search path. The slave then searches this partition using the identicalsearch ordering as the master. When the slave completes its search, it reports the resultto the master who then marks the assigned sibling node as solved. The master is thenfree to assign another partition to the slave. Note that if the master’s search enters thesubspace assigned to a slave, the slave in effect becomes a nagger, albeit one withoutthe benefit of a problem transformation function, but with a head start on its searchspace.All of the 3SAT problems were solved by every system configuration within theprespecified resource limit. For the Othello problems, one of the 100 random problems wascensored (not solved within the resource limit) by the serial search system, yet was easilysolved by both the nagging and partitioning systems. The situation is more complicated forthe TSP problems, as a total of 14 problems were censored by the serial system. Of these14 problems, five were solved to optimality by both the nagging and partitioning system,and one additional problem was solved to optimality by the nagging system alone. It isimportant to note that where censoring occurred in both serial and parallel configurations,qualitatively better solutions were produced by the parallel systems (for six of eight suchproblems, nagging found the shortest tour, while partitioning found the shortest tour in theremaining two cases). So, at least in terms of number of problems solved to optimality aswell as quality of solution when problems were not solved optimally, the performance edgeappears to belong to nagging.Differences in solution quality aside, we are mostly interested in quantifying changesin system performance. Here, we compare computed speedup values (recall speedup isdefined as the ratio of serial solution time to parallel solution time), where a speedupvalue of 1.0 implies no difference between the serial and parallel systems, while largerA.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10695Table 2/TSPA∗SPAM/OthelloDavis–Putnam/3SATsatisfiableunsatisfiableN921001004951minµNagging speedupµ∗1.511.552.002.501.622.433.287.5412.932.360.860.980.450.450.98max69.73148.32221.00221.0014.19N911001004951µminPartitioning speedupµ∗1.681.751.872.251.571.802.147.3913.431.590.551.010.550.551.07max5.2322.55481.17481.171.99speedups imply the parallel system is faster.13 Table 2 presents minimum, arithmeticmean (µ), geometric mean (µ∗), and maximum speedups computed excluding doubly-censored datapoints for all tested systems. Given the methodological difficulties just noted,some interpretation is in order. Consider, for example, the A∗/TSP and SPAM/Othellovalues shown in the table. For both of these algorithms, the mean speedup µ reported bynagging is larger than that for partitioning, but the geometric mean µ∗ is less than that forpartitioning. This is a consequence of the nondeterministic nature of nagging; the amountof speedup you get can vary dramatically from one trial to the next, even when solving thesame problem. In contrast, partitioning is by nature more conservative and likely to providemore uniform amounts of speedup on subsequent trials. This behavior is also at leastpartially evident in the maximum speedup values shown, as nagging’s best performancein the test suite represents an order of magnitude improvement over partitioning. Notealso that, as one would expect, the minimum values hover by and large at or just below1.0. Values below 1.0 represent a performance penalty incurred by the parallel systemswith regard to the serial system. This is partly due to initial setup costs (e.g., connectingwith the NICE infrastructure) and partly due to communication overhead (as we shall soonsee, the smallest values observed are usually associated with problems that can be solvedquickly with a single processor, hence precluding the amortization of startup costs overlonger solution times).The results for the Davis–Putnam/3SAT problems are notably different from those forthe other tested systems: over the entire set of problems, nagging’s measured speedupsexceed those of partitioning (as measured by both µ and µ∗). If restricted to satisfiableformulae only, partitioning’s measured performance is similar to that of nagging. Recall13 Methodologically, direct comparison of sets of speedup values is somewhat difficult for a number of reasons.First, as noted earlier, reporting arithmetic means for ratios like speedup is problematic; reporting geometricmeans would perhaps be a better choice, but this is not consistent with the general practice of the parallelprocessing community, where arithmetic means are the norm. Furthermore, it is important to keep in mind thatthe distribution of observed speedup values are quite skewed (not surprisingly, given that, by definition, they arebounded below at 0): simply reporting summary statistics that evoke normal distributions in the minds of somereaders is misrepresentative. Finally, some caution must be exercised when comparing censored datapoints [34].Since identical resource limits are imposed on both parallel and serial trials, doubly-censored datapoints will haveunit speedup values. Singly-censored datapoints are harder; fortunately, in our experiments, all singly-censoreddatapoints are censored by the serial system (never by the parallel system), so their computed speedup valuesrepresent underestimates of true speedup. Note, however, that direct comparisons of computed speedup valuesbetween uncensored and singly-censored datapoints or singly-censored and doubly-censored datapoints shouldbe made only with care.96A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106that the Davis–Putnam/3SAT algorithm searches until it encounters a satisficing solution,exhausting the search space only if the given formula is not satisfiable. This early-termination behavior implies that either nagging or partitioning could just get lucky andquickly encounter a solution, yielding large observed speedup values (this is consistentwith the maximum observed speedup values reported in the table). In contrast, the searchon unsatisfiable formulae unfolds in a manner more consistent with that of the other searchalgorithms, since one must exhaust the entire search space before labeling a formulaas unsatisfiable. On these problems, nagging’s performance clearly dominates that ofpartitioning by all reported measures.Of course, summary statistics such as the speedup values just shown obscure therelation between speedup values and problem difficulty: observing a 200-fold performanceimprovement on a problem that takes several hours to solve on one processor should bemore meaningful than observing a similar speedup on a problem that might be solved injust a few milliseconds. To provide a gestalt view of speedup with respect to problemdifficulty, we turn to a graphical representation of the data (see Figs. 9–11): these plotsshow parallel solution time against serial solution time.14 Interpretation of this kind of plotis relatively straightforward, as datapoints falling below the upper diagonal line are faster inparallel (i.e., have speedup values larger than 1.0), while datapoints falling above the upperdiagonal line are faster on one processor. The lower line represents a speedup value equalto the number of processors in use; hence a datapoint falling below this line correspond tosuperlinear speedups.Fig. 9 shows results for the A∗/TSP system. Since identical resource limits areimposed on all serial and parallel trials, doubly-censored datapoints should fall on thediagonal line. Singly-censored datapoints are best understood as datapoints that havebeen artificially shifted to the left of their true position, because their plotted serialsolution times (ordinate) represent lower bounds on their true serial solution times. Asis clear from the plot, nagging generally provides some speedup and occasionally providesexceptional speedups, while partitioning is mostly constrained to the region between thetwo diagonal lines. As expected, the few datapoints that are slower for either naggingor partitioning (i.e., those datapoints corresponding to those speedup values less than 1.0reported earlier) are relegated to the left hand side of the plot, and represent small problemswhere the startup costs of parallel execution are not effectively amortized over longersolution times. A similar trend is observed on more difficult (i.e., larger serial solutiontimes) problems, where superlinear speedups are more likely to occur. Aside from theamortization argument, a second factor may also be at work here: one would naturallyexpect a concomitantly greater payoff by finding a good solution early within a largersearch space.While the mechanism by which a nagging system attains superlinear speedup is clear, itis somewhat less clear how a partitioning system can achieve this kind of performance. To14 Note that, for clarity, the data are plotted in log–log space, even if this transformation does tend to obscurethe relative performance differences on large and small problems: i.e., a 1 unit vertical (alternatively: horizontal)difference on the top half (alternatively: right side) of the plot represents a much larger time interval than anidentical 1 unit vertical (alternatively: horizontal) difference on the bottom half (alternatively: left side) of theplot.A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10697Fig. 9. Two-processor A∗/TSP performance plot in log–log space. Datapoints falling below the upper diagonalline are faster in parallel, while datapoints falling below the lower diagonal line are superlinearly faster in parallel.Doubly-censored datapoints should fall on the diagonal line, while singly-censored datapoints appear artificiallydisplaced to the left of their true positions.understand how this can happen, recall that the partitioning system implementation testedhere differs from the coarse model of the previous section with respect to the asynchronousload balancing policy adopted; as a consequence, multiple partitioning events may occurin the course of solving a single problem. We might therefore attribute those few occasionswhere partitioning attains superlinear speedups to situations where exceptionally goodbounds are found in early partitions, so that subsequent partitioning events can enjoy thebenefits in terms of additional pruning. A similar explanation accounts for the occasionalsuperlinear speedups reported by the SPAM/Othello partitioning system (see Fig. 10). Notethat, as for A∗/TSP, the SPAM/Othello partitioning system still seems less likely to attainsuperlinear speedups, especially on larger problems, than does the nagging system, while98A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106Fig. 10. Two-processor SPAM/Othello performance plot in log–log space. Datapoints falling below the upperdiagonal line are faster in parallel, while datapoints falling below the lower diagonal line are superlinearly fasterin parallel.the nagging system on occasion delivers large speedups. Particularly noticeable is thelone censored problem, where because the serial solution time plotted is a lower boundto the true serial solution time, the speedup attained by nagging is at least 148.32, ascompared with a lower bound speedup of only 22.54 for partitioning. The results shown inFig. 11 for the Davis–Putnam/3SAT solver show a similar pattern. Recall that both naggingand partitioning can be expected to result in large speedups on satisfiable formulae, dueto the algorithm’s early-termination behavior. Thus many of the unsatisfiable problems’datapoints lie well in the superlinear speedup zone of the plot. Yet, once again, onlynagging seems likely to result in superlinear speedups for (unsatisfiable) problems of anymeaningful size.6.2. Experiment 2Our analysis of the previous section relied on using strictly solution-equivalenttransformation functions. In this experiment, we explore the performance of windownarrowing, a non solution-equivalent problem transformation function, in SPAM. Recallthat the main idea is that a nagger can artificially restrict its αβ window in order togain execution speed at the expense of information: indeed, window narrowing will prunemore aggressively, but may not be as informed as solution-equivalent transformations. Ourprotocol (somewhat arbitrarily) randomly narrows each nagpoint window, while forcingprocessors corresponding to leaf nodes in the nagging hierarchy to use a unit window, thusessentially performing zero-window search at leaf processors.The experimental procedure is identical to that of Experiment 1, and the same 100random Othello problems are used. Note that since we are only using two processors, thenagging processor is always a leaf processor, and is therefore always operating with unitA.M. Segre et al. / Artificial Intelligence 140 (2002) 71–10699Fig. 11. Two-processor Davis–Putnam/3SAT performance plot in log–log space. Datapoints falling below theupper diagonal line are faster in parallel, while datapoints falling below the lower diagonal line are superlinearlyfaster in parallel.Fig. 12. Two-processor SPAM/Othello performance plot in log–log space, both with and without the use of thewindow narrowing transformation. Datapoints falling below the upper diagonal line are faster in parallel, whiledatapoints falling below the lower diagonal line are superlinearly faster in parallel.window size. The results, plotted against serial solution times, are shown in Fig. 12 (notethat the non-narrowing system data is identical to that shown in Fig. 10).In addition to graphical comparisons, we can also test, statistically, the null hypothesis“permutation with window narrowing is no faster than permutation alone”. If we can rejectthis null hypothesis, then we can conclude that window narrowing is beneficial. To test100A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106the hypothesis without making any distributional assumptions, we’ll use a nonparametricstatistic, the paired Wilcoxon signed-ranks test [42]; what such nonparametric statisticsmay sacrifice in terms of power is more than counterbalanced by their broad applicability.Using 100 paired samples as the input, the paired sign test easily rejects the null hypothesisusing the traditional critical value for statistical significance (p < 0.05). Of course,statistics notwithstanding, Fig. 12 clearly shows that both transformation functions aredoing the job; yet the results shown here do underline the critical importance of the natureof the transformation function used.6.3. Experiment 3While both nagging and partitioning are often capable of delivering effective—and,in the case of nagging, often superlinear—speedups, exceptional speedups will notnecessarily be the rule for every problem. In this section, we shall examine a problemdomain where both nagging and partitioning deliver some speedup, but neither approachmanages to produce exceptional results.Consider the following team assignment problem, or TAP, drawn from the sportseconomics literature:Given a collection of N players and T teams, where the ith player has an associatedquality value Qi , find an assignment of players to teams so that there are an equalnumber of players on each team and the differences between relative team qualities,computed as a function of their constituent player qualities, are minimized.There are many variants of this problem, depending on the team quality metric used; somemore complex variants may involve higher order effects (e.g., individual player qualitymay be a function of teammate qualities) [41]. For this experiment, however, we’ll choosea simple linear metric, so that we are in effect minimizing the sum, over the set of allteams, of the absolute value of the difference between team quality (the sum of playerqualities) and the hypothetical average team quality (computed as the product of teamsize and average player quality). Our goal is to find the best-matched team assignmentsin terms of team quality; for this particular metric, a perfect solution produces T teamsof exactly average quality if such a solution exists. Our solution applies the same A∗search algorithm described in Section 4.2 to the team assignment problem. Formulatingan admissible heuristic function that properly bounds the solution value for any partialassignment is not overly difficult; the function used here estimates the potential deviationfrom the target team quality (i.e., the average player quality times the team size) [30].The experiment follows the same protocol as Experiment 1 using 100 randomlygenerated matching problems; the results are shown in Fig. 13.15 The most striking featureof the plot in Fig. 13 is the extent to which the partitioning system tracks the linear speedupline: only on smaller problems, where solution times are of the same order as the system15 Random problems were once again generated so that serial solution times ranged between 0.01 seconds, theresolution of the Linux system clock, up to about 5 minutes. The resulting problem set assigned between 15 and32 players to 2, 3, or 4 teams.A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106101Fig. 13. Two-processor A∗parallel, while datapoints falling below the lower diagonal line are superlinearly faster in parallel./TAP performance plot. Datapoints falling below the upper diagonal line are faster inclock resolution, is there any significant deviation from this line. On the other hand, theperformance of nagging is decidedly worse than that of partitioning, rarely even attaininglinear speedup.It is tempting to attribute the poor performance of nagging to the use of an inadequateproblem transformation function, as permutation of the search space here clearly does notprovide additional pruning as for, say, TSP. Yet it is much more likely that the problemlies with the heuristic estimate: if it is not informative (i.e., close to the true additionalcost of the partial solution), then A∗ search cannot be expected to explore fewer nodesthan simple branch-and-bound. One cannot compensate for a poor heuristic estimate witha better problem transformation function. Alternatively, the difficulty may lie with theproblem itself. If the cost landscape tends to be populated with many local minima whosevalues are near that of the global minima, even a highly informative heuristic will not leadto much pruning. As mentioned in Section 4.1, we must acknowledge that some NP-hardproblems are harder than others. Unlike 3SAT problems, however, it is possible that TAPproblems are simply all uniformly difficult.6.4. Experiment 4In this experiment, we provide empirical support for the scalability of nagging, showinghow additional processors have a beneficial effect on the performance of the system, andprovide direct comparison with the behavior of partitioning. The experimental procedureis like that of Experiment 1, except that we now use 8, 16, 32, or 64 essentially identical102A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106Table 3CPUs264N92100µNagging speedupµ∗1.514.952.4312.29min0.860.96max69.73256.22Partitioning speedupN9195min0.550.38µ1.804.49µ∗1.683.90max5.2310.21processors arranged in a hierarchy with branching factor less than or equal to three.16 Inthis section, we will focus on results obtained with 64 processors.17We turn first to some simple descriptive statistics. Recall that in Experiment 1 usingonly two processors, there were 8 problems left unsolved by nagging, and 9 problems leftunsolved by partitioning (these correspond to the doubly-censored datapoints of Fig. 9).When 64 processors are applied, the nagging system solves all 100 problem optimally,while the partitioning system still leaves five unsolved problems. So, at least qualitatively,the performance of nagging exceeds that of partitioning. We can also use observed speedupvalues to help quantify this trend (see Table 3; as before, doubly-censored datapoints areexcluded). Direct comparison of both µ and µ∗ confirms the advantage of nagging in thisexperiment; indeed, the maximum observed speedup for nagging exceeds the maximumobserved speedup for partitioning by more than a factor of twenty. Moreover, the valuesgiven in the table are fully consistent with the argument first advanced in Section 5.3,that is, that the probability of obtaining a superlinear speedup is higher for nagging thanfor partitioning (here, the nagging system produces superlinear speedups on at least threeof 100 problems—and possibly more, given the amount of censoring observed—whilepartitioning fails to produce any superlinear speedup at all).One troubling fact is that the observed mean speedups µ and µ∗ are significantly lessthan N , the number of processors employed. In our analytical model, the predicted µ∗values—while still less than N —were significantly more in line with N . We can attributethis discrepancy to two differences. First, the model of Section 5 is a coarse model, whereall processors engage in a single nagging episode on the root node of the search, whilethe experimental model allows repeated nagging episodes applied at internal nodes of thesearch process. Second, and more to the point, the analytical model had all N − 1 naggersreporting directly to the single master search process, while the experiment allowed nomore than three processors to nag the master directly (the remaining N − 4 processorswere used to recursively nag the naggers). That the NICE hierarchy limits each daemonto only three descendents is quite arbitrary; while increasing the branching factor of thehierarchy raises the communication overhead incurred by the master, one must balancethe increased overhead on the performance benefits obtained. Note that the tradeoff iscomplicated, since the optimal configuration may differ depending on the search algorithmor even the problem instance. In any case, adaptive configuration of the NICE hierarchy isstill an area we are actively exploring.16 The actual hierarchy depends on the NICE resource management daemon, and is constantly changing inresponse to local system load and availability.17 For the record, in our tests, both nagging and partitioning scale smoothly from 2 to 64 processors.A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106103Fig. 14. Sixty-four processor A∗with two processor A∗are faster in parallel, while datapoints falling below the lower diagonal line are superlinearly faster in parallel./TSP performance plots for nagging (top) and partitioning (bottom); compare/TSP performance plots given in Fig. 9. Datapoints falling below the upper diagonal lineFig. 14 presents a graphical view of the same data, and can be compared directly with theplots of Fig. 9 to confirm the descriptive statistics outlined above: nagging indeed appearsmore effective than partitioning in applying additional processors to reduce solution time.Not surprisingly, this observation appears more striking on larger problems, where the costof initializing additional processors is readily amortized over longer solution times. Weconjecture that this trend extends to still larger problems; indeed, the fact that nagging104A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106solves all problems optimally while partitioning still leaves five unsolved problems withinthe prespecified resource bound is consistent with this conjecture.7. ConclusionNagging is a paradigm for parallel search in a heterogeneous distributed computingenvironment. It is applicable to a broad range of different artificial intelligence searchalgorithms, and scales easily to a large number of processors. We have shown, usingan analytical performance model, how nagging is often a superior approach to the moretraditional partitioning strategy commonly employed to parallelize search. We have alsopresented empirical results that confirm the predictions of our analytical model and supportour claims regarding both the performance and scalability of nagging across severaldifferent domains and search algorithms.We are currently working on a number of refinements to nagging. First, inspired bythe empirical and analytical results reported here, we have already experimented withrandomly mixing nagging and partitioning within the same search. The idea is thatsince nagging and partitioning appear to be complementary, one should actively managea mixture of approaches in order to more effectively guide the use of computationalresources. We are currently focusing on how to decide whether a new event should bea nagging event or a partitioning event. Based on the analytic model of Section 5, it shouldbe possible to use statistical evidence obtained in the course of a problem solving episodeto decide how best to use an idle processor on a particular problem. This decision mightwell change over the course of the search: for example, early events might be primarilynagging events and later events might be primarily partitioning events. The basic idea is tolet information about this particular problem solving process guide the best application ofprocessor power over the course of the search process.We are also working on extensions to the NICE infrastructure itself. We are experi-menting with better hierarchy-formation and restructuring algorithms in order to betterapply the available computational resources to a given problem. We are also looking atcryptographic certification techniques for distributing new NICE-enabled applications toprocessors within the NICE hierarchy.Much of this work is being performed in the context of an extraordinarily challengingcomputational biology application. Over the last two years, we have been working onHOPS, an ab initio distributed hybrid optimization protein structure prediction engine(see, e.g., [10]). HOPS is large, interdisciplinary, project involving faculty and studentsfrom biochemistry, computer science, operations research and applied mathematics. Itcombines a distributed search (using both nagging and partitioning) over a discretespace of protein conformations with traditional continuous optimization techniques (e.g.,nonlinearly constraint nonlinear programming, interior point methods, etc.) to find theenergetically most favorable conformation of a specified protein according to an energymodel of our own design. Given the sheer size of these search problems, HOPS is a perfectexample of the kind of application where nagging’s distinguishing features—effectiveness,scalability, and fault tolerance—should shine.A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106105AcknowledgementsThe authors would like to thank Karl Arndt, Bruno Codenotti, Mauro Leoncini,Harry Paarsch, Marco Pellegrini, Tianbing Qian and David Sturgill for their advice,comments and suggestions. Ted Herman and Hantao Zhang graciously provided accessto the University of Iowa Computer Science Department Linux cluster used to collect theexperimental results reported here. We would like to acknowledge the contributions ofRagothaman Balakumar, Satyanarayanan Jayar, Shantan Kethireddy, Sumantra Kundu, thelate Jinghou Li, Vivek Narayanamurthy, Neela Patel, Zhaxian Que, Sunita Raju, KevinSagon and Sumana Vijayagopal to portions of the NICE distributed search infrastructure.The authors also wish to thank two anonymous reviewers for their helpful comments whichresulted in a more precise and better written paper. Support for this research was providedby the Office of Naval Research under grant N0014-94-1-1178, the National ScienceFoundation through grant NSF-BIO-9730053, the Italian National Research Councilthrough a visiting professorship, the University of Iowa through a Faculty Scholar award,and with equipment support provided by NSF-CDA-9529518, NSF-IRIS-9729807, and theUniversity of Iowa Office of Sponsored Programs.References[1] D. Applegate, R. Bixby, W. Cook, and V. Chvátal, On the solution of traveling salesman problems, TechnicalReport 98744, Center for Research on Parallel Computation, Rice University, Houston, TX, July 1998.[2] A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, V. Sunderam, PVM Users Guide and Reference Manual,Oak Ridge National Laboratory, Oak Ridge, TN, 1994.[3] M. Brockington, J. Schaeffer, APHID: Asynchronous parallel game-tree search, J. Parallel DistributedComput. 60 (2) (2000) 247–273.[4] D. Cook, R.C. Varnell, Adaptive parallel iterative deepening search, J. Artificial Intelligence Res. 9 (1998)139–166.[5] S. Cook, The complexity of theorem-proving procedures, in: Proc. 3rd Annual ACM Symposium on theTheory of Computing, 1971, pp. 151–158.[6] M. Davis, G. Logemann, D. Loveland, A machine program for theorem-proving, Comm. ACM 5 (7) (1962)394–397.[7] W. Ertel, Performance analysis of competitive or-parallel theorem proving, Technische Universität München,FKI-162-91, 1992.[8] R. Feldmann, P. Mysliwietz, B. Monien, Game tree search on a massively parallel system, in: H. vanden Herik, I. Herschberg, J. Uiterwijk (Eds.), Advances in Computer Chess VII, University of Limburg,Maastricht, the Netherlands, 1994, pp. 203–218.[9] C. Ferguson, R.E. Korf, Distributed tree search and its application to alpha-beta pruning, in: Proc. AAAI-88,St. Paul, MN, 1988, pp. 128–132.[10] S.L. Forman, Torsion angle selection and emergent non-local secondary structure in protein structureprediction, Ph.D. Thesis, Department of Mathematics, The University of Iowa, Iowa City, IA, August 2001.[11] I. Foster, C. Kesselman, Globus: A metacomputing infrastructure toolkit, Internat. J. Supercomput. Appl.High Performance Comput. 11 (2) (1997) 115–128.[12] C. Gomes, B. Selman, N. Crato, H. Kautz, Heavy-tailed phenomena in satisfiability and constraintsatisfaction problems, J. Automat. Reasoning 24 (1–2) (2000) 67–100.[13] A. Grama, V. Kumar, Parallel search algorithms for discrete optimization problems, ORSA J. Comput. 7 (4)(1995) 365–385.[14] A. Grama, V. Kumar, State of the art in parallel search techniques for discrete optimization problems, IEEETrans. Knowledge Data Engrg. 11 (1) (1999) 28–35.106A.M. Segre et al. / Artificial Intelligence 140 (2002) 71–106[15] W. Gropp, E. Lusk, N. Doss, A. Skjellum, A high-performance, portable implementation of the MPImessage-passing interface standard, Parallel Comput. 22 (6) (1996) 789–828.[16] J. Gu, P.W. Purdom, J. Franco, B.W. Wah, Algorithms for the satisfiability (SAT) problem: A survey,in: Satisfiability Problem: Theory and Applications, in: DIMACS Series in Discrete Mathematics andTheoretical Computer Science, American Mathematical Society, Providence, RI, 1997, pp. 19–151.[17] C. Joerg, B. Kuszmaul, Massively parallel chess, in: Proc. Third DIMACS Parallel ImplementationChallenge, Rutgers University, Rutgers, NJ, 1994.[18] D.E. Knuth, R.W. Moore, An analysis of alpha-beta pruning, Artificial Intelligence 6 (4) (1975) 293–326.[19] T.H. Lai, S. Sahni, Anomalies in parallel branch-and-bound algorithms, Comm. ACM 27 (4) (1984) 594–602.[20] E. Lawler, D. Wood, Branch and bound methods: A survey, Oper. Res. 14 (4) (1966) 699–719.[21] E. Lee, Statistical Methods for Survival Data Analysis, Second Edition, Wiley, New York, 1992.[22] M. Litzkow, M. Livny, M. Mutka, Condor: A hunter of idle workstations, in: Proc. Eighth Conference onDistributed Computing Systems, San Jose, CA, 1988.[23] A. Mahanti, C.J. Daniels, A SIMD approach to parallel heuristic search, Artificial Intelligence 60 (2) (1993)243–282.[24] N. Mann, R. Schafer, N. Singpurwalla, Methods for Statistical Analysis of Reliability and Life Data, Wiley,New York, 1974.[25] W. Meeker, L. Escobar, Statistical Methods for Reliability Data, Wiley, New York, 1998.[26] D. Mitchell, B. Selman, H. Levesque, Hard and easy distributions of SAT problems, in: Proc. AAAI-92, SanJose, CA, 1992, pp. 459–465.[27] A. Newell, J. Shaw, H. Simon, Chess playing programs and the problem of complexity, IBM J. Res.Development 2 (1958) 320–335.[28] N. Nilsson, Problem-Solving Methods in Artificial Intelligence, McGraw Hill, New York, 1971.[29] P. Norvig, Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp, MorganKaufmann, San Mateo, CA, 1992.[30] H.J. Paarsch, A.M. Segre, Extending the computational horizon: Effective distributed resource-boundedcomputation for intractable problems, in: Proc. Fifth International Conference of the Society for Computa-tional Economics, 1999.[31] C. Powley, C. Ferguson, R.E. Korf, Depth-first heuristic search on a SIMD machine, Artificial Intelli-gence 60 (2) (1993) 199–242.[32] A. Reinefeld, V. Schnecke, Work-load balancing in highly parallel depth-first search, in: Proc. 1994 ScalableHigh-Performance Computing Conference, 1994, pp. 773–780.[33] G. Reinelt, The Traveling Salesman: Computational Solutions for TSP Applications, Springer, Berlin, 1994.look at experimental evaluations of EBL, Machine[34] A.M. Segre, C.P. Elkan, A. Russell, A criticalLearning 6 (2) (1991) 183–196.[35] A.M. Segre, D.B. Sturgill, Using hundreds of workstations to solve first-order logic problems, in: Proc.AAAI-94, Seattle, WA, 1994, pp. 187–192.[36] B. Selman, H. Levesque, D. Mitchell, A new method for solving hard satisfiability problems, in: Proc.AAAI-92, San Jose, CA, 1992, pp. 440–446.[37] B. Selman, D. Mitchell, H. Levesque, Generating hard satisfiability problems, Artificial Intelligence 81(1996) 17–29.[38] M.A. Stephens, EDF statistics for goodness of fit and some comparisons, J. Amer. Statist. Soc. 69 (1974)720–727.[39] D.B. Sturgill, A.M. Segre, Nagging: A distributed adversarial search-pruning technique applied to first-orderlogic, J. Automat. Reasoning 19 (3) (1997) 347–376.[40] D.B. Sturgill, A.M. Segre, A novel asynchronous parallelization scheme for first-order logic, in: Proc.Twelfth Conference on Automated Deduction, 1994, pp. 484–498.[41] C. Whittinghill, Social choice and resource allocation in a professional sports league, Ph.D. Thesis,Department of Economics, The University of Iowa, Iowa City, IA, 1999.[42] F. Wilcoxon, Individual comparisons by ranking methods, Biometrics 1 (1945) 80–83.[43] W. Zhang, State-Space Search: Algorithms, Complexity, Extensions, and Applications, Springer, Berlin,1999.