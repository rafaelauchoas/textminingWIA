Artificial Intelligence 142 (2002) 179–203www.elsevier.com/locate/artintBelieving others: Pros and consSandip SenDepartment of Mathematical and Computer Sciences, University of Tulsa, 600 South College Avenue,Tulsa, OK 74104-3189, USAReceived 13 May 2001; received in revised form 28 April 2002AbstractIn open environments there is no central control over agent behaviors. On the contrary, agents insuch systems can be assumed to be primarily driven by self interests. Under the assumption thatagents remain in the system for significant time periods, or that the agent composition changesonly slowly, we have previously presented a prescriptive strategy for promoting and sustainingcooperation among self-interested agents. The adaptive, probabilistic policy we have prescribedpromotes reciprocative cooperation that improves both individual and group performance in the longrun. In the short run, however, selfish agents could still exploit reciprocative agents. In this paper,we evaluate the hypothesis that the exploitative tendencies of selfish agents can be effectively curbedif reciprocative agents share their “opinions” of other agents. Since the true nature of agents is notknown a priori and is learned from experience, believing others can also pose its own hazards. Weprovide a learned trust-based evaluation function that is shown to resist both individual and concerteddeception on the part of selfish agents in a package delivery domain. 2002 Elsevier Science B.V. All rights reserved.Keywords: Reciprocity; Agents; Cooperation; Adaptation; Trust; Relationships1. IntroductionWith the burgeoning of agent based electronic commerce, recommender systems,personal assistant agents, etc., it is becoming increasingly clear that agent systems mustinteract with a variety of information sources in an open, heterogeneous environment [7,9,10,19,20,34]. One of the key factors for successful agent based systems (ABSs) of thefuture would be the capability to interact with other ABSs and humans in different rolecontexts and over extended periods of time. The ABSs of the future will be situated inE-mail address: sandip@ens.utulsa.edu (S. Sen).0004-3702/02/$ – see front matter  2002 Elsevier Science B.V. All rights reserved.PII: S 0 0 0 4 - 3 7 0 2 ( 0 2 ) 0 0 2 8 9 - 8180S. Sen / Artificial Intelligence 142 (2002) 179–203a social context, playing a variety of roles in different relationships and problem solvingsituations. Borrowing on the social cliche leveled at humans, we would like to conjecturethe following about the agents of the future: Agents must be social entities. In particular, webelieve that typical real-world environments abound in cooperation possibilities: situationswhere one agent can help another agent by sharing work such that the helping cost ofthe helper is less than the cost saving of the helped agent. Social agents can benefitfrom such cooperation possibilities by identifying and sustaining mutually beneficialrelationships.Whereas economic models can provide a basis for structuring agent interactions [24,35], research in multiagent systems involving non-monetary, social reasoning proceduresas behavioral strategies for self-interested agents has been relatively scarce. We believethat such societal approaches inspired by non-monetary mechanisms [1–3,15,27] mayprovide more effective social relationships in certain situations.1 For example, agents cantake advantage of cooperation possibilities by trading helps, where the cost incurred forhelping is the time spent in helping the other agent. One strong argument for incurring“time costs” for help rather than “monetary costs” is that time cannot be stored like money.Given a choice, it is preferable to trade non-storable resources, than storable resources andindividual agents can benefit in the long run by using their unoccupied time to developmutually beneficial relationships.Cooperative relationships not only benefit individual agents, but can also enhance thecondition of the entire society or the environment. Whereas as individual agent designers,we want to develop strategies which makes our agents profitable, as designers of entireagent systems or infrastructures, we want to maximize the performance of the entiresystem. For example, as designers of agent systems or infrastructures, we want the entiresystem to operate smoothly with little or no congestion, high throughput, balanced loadson resources, etc. Can these different viewpoints be reconciled? Put another way, arethere possible worlds where individually rational action leads both to maximizing localutility and improving system-level performance? These two goals cannot be reconciledunder all situations. Under a reasonably practical set of assumptions, which apply to asignificant set of realistic domains, however, we believe that the desired synthesis can beachieved. For example, as agent designers, we can design interaction protocols, feedbackmechanisms, etc., to create abundant cooperation possibilities, which can then be utilizedby well-designed agents. Such a scenario can produce high individual as well as systemperformance. But even though system designers provide cooperation possibilities, agent1 It is often argued that all interactions can be assigned to economic agents. If, in the future, all interactionsbetween any two computational entities on the Internet involved monetary exchanges, then either these agents ortheir owners have to decide on whether to interact or conserve its monetary allocation for some more important orurgent task that may arrive later. For example, my information gathering agent has to decide between whether toproactively search for information on the net (for which it has to pay) or reactively respond to my search requestsonce it has been allocated $X for the day. This decision making may be difficult to optimize as my requests mayvary widely over different days, and I will not take kindly to my agent who cannot process my explicit requestbecause it has already spent its allocation on proactive searches which may have generated useful information butis of less importance to me right now. Neither do I want to micro-manage this monetary allocation to my agent asthen the purpose of having an automated assistant is defeated.S. Sen / Artificial Intelligence 142 (2002) 179–203181designers must endow their agents with the capability of identifying and benefiting fromsuch opportunities in the environment.In this paper, we assume that real-life environments do provide sufficient cooperationpossibilities, and focus on the need to design social agents that can form cooperativerelationships with other agents to benefit from such cooperation possibilities. Butunsuspecting, naive agents, who cooperate with any agent in the environment, can beexploited by malevolent agents who receive but do not return help. We have been interestedin agent strategies for interactions with other agents that can promote cooperation in groupswhile resisting exploitation [5,27–29]. Our approach is different from other researcherswho have designed effective social laws that can be imposed on agents [8,30]. Inparticular, we have studied environments where agents can mutually benefit from sustainedinteractions. In such environments, appropriately designed agent strategies can lead toboth improved local performance for individual agents and effective global behavior forthe entire system. These are the desirable features for open systems where self-interestedagents are required to share resources.More specifically, we have developed and analyzed probabilistic reciprocity schemes asstrategies to be used by self-interested agents to decide on whether or not to help otheragents [27]. The goal of this work has been to identify procedures and environmentsunder which self-interested agents may find it beneficial to help others. By helpingwe imply incurring some local cost to benefit another agent. We claim that if thegroup composition changes only slowly, and there is sustained interaction between theagents, probabilistic reciprocity based strategies can be rational, i.e., maximize individualutilities. Probabilistic reciprocity strategies can be considerably more effective thansimple deterministic reciprocity schemes like tit-for-tat [2,13] and avoid major problemsassociated with the latter [27].In our experimental package delivery domain, an agent helps another agent by carryingout a task, i.e., by delivering a package, on behalf of the helped agent. But noneof the mechanisms presented here are limited to that particular kind of cooperation.For example, the reciprocity approach presented can also be used in domains wherecooperation implies the helping agent working together with the helped agent to reducethe latter’s workload [26]. Our experiments under a variety of environmental conditions,group composition, work estimate difference, etc. have shown that under prolongedinteraction, the probabilistic reciprocity strategy produces close to optimal individual andgroup performance [5,27–29]. Additionally, this strategy is stable against selfish intruders,i.e., in the long run, selfish agents perform worse than reciprocative agents in a mixedgroup.We now turn to the focus of the current paper. Even though probabilistic reciprocativeagents outperform selfish agents in mixed groups, they still waste some effort in helpingout selfish agents. This is because the reciprocative agents have a bias to initiate help topromote cooperative relationships in the future. A selfish agent can then benefit from thisinitial cooperative advances from each of the reciprocative agents in a mixed group. This isaided by the fact that reciprocative agents do not share their experiences or impressionsof the other agents. In other words, there is no “words of mouth” transmission of thereputation or reliability of the agents in the agent group. As a result, the reciprocativestrategy was dominant over exploitative strategies only if the agent group was stable, i.e.,182S. Sen / Artificial Intelligence 142 (2002) 179–203the agents interacted over a relatively long period of time. Our current work is driven bythe need to augment the reciprocative strategy so that it becomes dominant even if theagents interacted with each other for only a limited period of time. We realize that in theextreme cases, where an agent typically interacts with others in the environment for oneor a very few number times, the selfish strategy will be the logical choice, as there isnot enough time to identify and develop mutually beneficial relationships. In the currentwork, however, we show that we can significantly reduce the number of interactions anagent should have with others before the reciprocative strategy dominates over exploitativeones.A hypothesis that follows from the above considerations is the following: Sharing ofexperiences about other agents among reciprocative agents will limit the exploitative gainsof selfish agents. Operationalizing this hypothesis, however, requires a closer inspectionof the issues at hand. Since it is not clear a priori who is a selfish agent and who is areciprocative agent (otherwise this whole exercise is moot because accurate identificationimmediately gives the right strategy to adopt while interacting with others), at the outsetit is not possible to limit sharing of experiences only between reciprocative individuals.When an agent Z decides to use information supplied by an agent X to decide whether ornot to help agent Y, then believing X can be advantageous or disadvantageous to Z basedon the true nature of X. If X is selfish, it might find it useful to taint Y’s reputation, andthat of other agents, so that Z will consider X to be a relatively trustworthy agent. Assuch, we need to augment the reciprocative agents’ strategy to believe only the agents whoare trustworthy. In this paper, we evaluate the effectiveness of these strategies in mixedgroups.The principal contribution of this paper, therefore, is the development of robustreciprocative strategies, using which self-interested agents can form mutually beneficialrelationships with other agents in the environment. These agents are also resistant toexploitation by malevolent agents. Given an environment with sufficient cooperationpossibilities, such reciprocative agents can produce desirable individual as well as systemlevel performances.2. Related workThe evolution of cooperative behavior among a group of self-interested agents hasreceived considerable attention among researchers in the social sciences and economicscommunity. Researchers in the social sciences have focused on the nature of altruism andthe cause for its evolution and sustenance in groups of animals [21,25,33]. Our goal in thispaper is not to model altruistic behavior in animals; so we do not address the issues raisedin the social science literature on this topic [18].Most of the work by mathematical biologists or economists on the evolution ofaltruistic behavior deals with the idealized problem called Prisoner’s dilemma [23] orsome other repetitive, symmetrical, and identical “games”. Some objections have alreadybeen raised to using such sanitized, abstract games for understanding the evolution ofcomplex phenomena like reciprocal altruism [6]. In the following we analyze in somedetail one of the often-cited work that share the typical assumptions made by economistsS. Sen / Artificial Intelligence 142 (2002) 179–203183and mathematical biologists, and then present our own set of suggestions for relaxing therestrictive assumptions made in that work.In a seminal piece of work Robert Axelrod has shown how stable cooperative behaviorcan arise in self-interested agents when they adopt a reciprocative attitude towards eachother [2]. The basic assumptions in this work include the following: agents are interestedin maximizing individual utilities and are not predisposed to help each other; agents ina group repeatedly interact over an extended period of time; all interactions are identical(they are playing the same “game” again and again, where each game can be representedby payoff matrices which specify the payoff to be received by each agent given theirsimultaneous choice of actions); agents can individually identify other agents and maintaina history of interactions with other agents; individual agents do not change their behavioralstrategy over time; composition of agent groups change infrequently and the changes areminimal (only a few agent leaves or joins a group at a time). Axelrod shows that a simple,deterministic reciprocal scheme of cooperating with another agent who has cooperated inthe previous interaction (this strategy, for obvious reasons, is referred to as the tit-for-tatstrategy), is quite robust and efficient in maximizing local utility.Though Axelrod’s work is interesting and convincing, we believe that the assumptionsused in his work make the results inapplicable in a number of domains of practical interest.In real-life situations, a particular help-giving interaction between two agents often meansone agent helps and incurs a cost while the other receives help and obtains a savings incost or effort. As only one agent decides or acts in each interaction, such interactions arenecessarily asymmetrical in nature in contrast to the symmetrical formulation of gameslike the prisoner’s dilemma. Another key restrictive feature of Axelrod’s experiment withthe iterated prisoner’s dilemma game is that identical scenarios are repeated. This is notlikely in real life as every interaction is different from others. The assumption of repetitionof identical scenarios enable Axelrod to work with strategies that do not compare differentinteractions. In real life, history of interaction will have to capture not only the outcomes,but also the context in which a certain outcome was produced. Also, there has to be a meansto compare two different scenarios or two help-giving actions of different magnitude.Comparison of two such different scenarios requires the use of some measure of workor cost involved in help-giving. Such a metric will allow systematic evaluation of differentscenarios under different interaction histories.Based on these observations, we believe that a simple tit-for-tat like deterministicstrategy is not adequate for more realistic agent domains.2 We now identify the desirablefeatures of a behavioral strategy that will be suitable for open environments: a risk attitudethat allows the agent to initiate help-giving to a new agent but quickly shun it if requestsfor help are rejected repeatedly; ability to compare cooperation costs across differentscenarios; ability to adjust help-giving behavior based on local work-load.Over the last few years, multiagent systems researchers have started evaluatingnon-monetary mechanisms for supporting agent interactions and developing fruitfulrelationships in a societal context. Our own work [5,27–29] have emphasized the use ofa probabilistic reciprocity scheme for exchanging help, using which self-interested agents2 There are other, orthogonal criticisms to the generality of the conclusions drawn in Axelrod’s work [4,22].184S. Sen / Artificial Intelligence 142 (2002) 179–203can develop mutually beneficial relationships with other agents while avoiding exploitativeagents. Castelfranchi and Falcone have argued the necessity of trust in social interactionsbetween agents with complex mental attitudes [12]. They argue that trust can be based onmental background, and though it necessarily entails risks for delegation and collaboration,considerations of morality and use of reputation can be used to mitigate that risk. Weconcur with the observation that trust can play a critical role in initiating, nurturing,and supporting collaborative relationships between self-interested agents. Goldman andRosenschein [16] have suggested an intrinsic cooperation level parameter for agents,whereby agents have an innate desire to work to benefit everyone in the environment. Thiscan be viewed as an agent’s trust in or attitude towards other agents in the environment.Our reciprocity scheme is a more targeted, one-to-one modeling mechanism, which can beinterpreted as the cooperation level of an agent towards another agent depending on theirhistory of interaction.An example of using morality to promote social relationships, as suggested byCastelfranchi and Falcone, can be found in the SPIRE framework developed by Grosz andcollaborators [15,31]. They examine a framework where agents in a group have to decidewhether or not to reneg on commitments to other agents in the group to accept lucrativeoutside offers. In their model, socially conscious agents combine utility considerations witha brownie points model, whereby agents analyze whether their actions will make them“feel good” or be viewed as being a “good guy” by others. Based on their presentation,their agents cannot be termed as self-interested, as these agents evaluate their actionsbased on not only how that will affect individual payoff but also how the group utilitywill be affected. In our model, the agents are completely self-interested, and their help-giving behavior is predicated on receiving future help from the other agent to more thancompensate for the current help-giving cost.Cesta, Micelli, and Rizzo evaluate simplistic non-adaptive help-giving, parasitic, andselfish agents in a food-gathering domain [13]. Their results suggest that for verysupportive environmental conditions and for a particular choice of evaluation criteria,that suppresses extremes of performance, non-adaptive helping strategies may resistexploitation. But they also report that for a number of environmental settings, selfish andparasitic agent can severely affect the viability of helping agents. A thorough analysisof their work reveals several fundamental weaknesses of agents that do not adapt theirbehaviors and shun exploiters. In our previous work [27], we have observed similarproblems with naive, philanthropic agents, i.e., agents who always help when asked. Asthese agents get easily exploited by selfish agents, they perform poorly in mixed groups.Hence, we have not included these agents in the current study.Castelfranchi, Conte, and Paolucci use normative reputation [11] to enhance theperformance of agents that comply with social norms. They also experiment with afood-gathering domain, where agents prolong life by finding and consuming food in theenvironment. Norm-following, respectful agents do not attack agents who are consumingfood, but aggressors or cheaters can snatch food away from those who found it first.Without the use of reputation, cheaters outperform respectful agents. When respectfulagents are modified to share their opinion of other agents and to respect the norms onlyfor other agents believed to be respectful, the performance of respectful agent improvesto being close to that of cheater agents. There are two basic shortcomings of theirS. Sen / Artificial Intelligence 142 (2002) 179–203185approach. The first problem is that a respectful agent believes the opinions providedby another agent, a strategy that is shown to be easily undermined by lying agents inour current work. The second problem is that their deterministic decisions about notfollowing norms for a norm violator is not robust enough to be applied to domainswhere agents can erroneously or inadvertently violate norms. In domains where an agentcannot help because they are currently occupied or if they fail to complete help-givingbehavior because of environmental factors, a decision to shun that agent in the futurewill be counter-productive and will disrupt the growth and sustenance of collaborativerelationships. In a sense, this deterministic reputation mechanism implicitly assumesrepetition of identical scenarios and does not compare the costs and benefits of onesituation with another. Such simplifications limit the applicability of these strategies indomains where different help-giving situations may result in largely varying costs andsavings for the helper and the helped agents. The variants of probabilistic reciprocitystrategy, that we evaluate in the current paper, are not susceptible to the above-mentionedproblems.3. Probabilistic reciprocityNow, we present our probabilistic reciprocity mechanism for deciding whether or not tohelp an agent who has requested for help. We assume a multiagent system with N agents.Each agent is assigned to carry out T tasks. The j th task assigned to the ith agent is tij andcosts it Cij . If agent k carried out this task together with its own task tkl , the cost incurredfor task tij is Cklij .If an agent, k, can carry out the task of another agent, i, with a lower cost than thecost incurred by the agent who has been assigned that task (Cij > Cklij ), the first agent cancooperate with the second agent by carrying out this task. If agent k decides to help agenti, then it incurs an extra cost of Cklij but agent i saves a cost of Cij . Since the cost of helpingto the helper agent is less than the saving of the helped agent, there exists a cooperationpossibility.We now propose a probabilistic decision mechanism that satisfies the set of criteriafor choosing when to honor a request for help that we described at the end of theprevious section. We will define Sik and Wik as respectively the savings obtained fromand extra cost incurred by agent i from agent k over all of their previous exchanges.Also, let Bik = Sik − Wik be the balance of these exchanges (note that, in general,Bik (cid:3)= −Bki ). We will see later in Section 5 that in place of the individual balance,an agent can use a combination of balances reported by other agent, i.e., use a socialreputation mechanism, to determine if agent i should be helped. The probability thatagent k will carry out task tij for agent i while it is carrying out its task tkl is givenby:Pr(i, k, j, l) =1Ckl−β∗Ckijavg,−Bkiτ1 + exp(1)where Ckavg is the average cost of tasks performed by agent k, and β and τ are constants.This is a sigmoidal probability function where the probability of helping increases as the186S. Sen / Artificial Intelligence 142 (2002) 179–203Fig. 1. Probability function for accepting request for cooperation.balance increases and is more for less costly tasks.3 We include the Cavg term becausewhile calculating the probability of helping, relative cost should be more important thanabsolute cost.We present a sample probability function in Fig. 1. The constant β can be used to movethe probability curve right (more inclined to cooperate) or left (less inclined to cooperate).At the onset of the experiments Bki is 0 for all i and k. At this point there is a 0.5probability that an agent will help another agent by incurring an extra cost of β ∗ Ckavg.The constant τ can be used to control the steepness of the curve. For a very steep curveapproximating a step function, an agent will almost always accept cooperation requestswith extra cost less than β ∗ Ckavg, but will rarely accept cooperation requests with anextra cost greater than that value. Similar analyses of the effects of β and τ can be madefor any cooperation decision after agents have experienced a number of exchanges. Inessence, β and τ can be used to choose a cooperation level [17] for the agents. The level ofcooperation or the inclination to help another agent is dynamically adapted with problemsolving experience. Over time, an agent will adapt to have different cooperation levels fordifferent agents.We emphasize that we chose the probability function in Eq. (1) as it satisfies thedesirable features of a behavioral strategy for agent interaction we presented at the endof the last section. In addition, this function provides two well-understood parameters,β and τ , with which agent designers can easily control the degree and nature of cooperationof their agents. Also, a large class of other functions, including step functions and linearfunctions, can be approximated by this sigmoidal function. We believe that other functional3 Note that this function does not represent a probability distribution. In particular f (x) gives the probabilitythat the agent will agree to help when the cost of helping is x. f (x) and 1 − f (x) together determine theprobability distribution for helping cost x, where the only two options for the agent is to accept or deny therequest for help. Also, there does not need to be any correlation between f (x) and f (y) values, where x (cid:3)= y.S. Sen / Artificial Intelligence 142 (2002) 179–203187forms can also satisfy a number of our requirements. The choice of this particular functionwas dictated primarily by the fact that it is well known, well understood, and easy touse.4. AssumptionsIn this section we present several assumptions about the agents and environmentalconditions that have motivated the design of the agent strategies and experimentalframework that we describe in the following sections:– We assume that an agent does not change its strategy in the course of an experiment.For example, a selfish agent does not adopt a reciprocative strategy because agentsof the latter type are performing well in the environment. A rational agent will beexpected to change its strategy based on its observations of the relative performances ofagents of different strategies. In the current paper, however, we are primarily interestedin evaluating the effectiveness of different strategies and do not address the issue ofagents adopting strategies that are perceived to produce higher performance.– The motivation for the reciprocity work comes from self-interested agents interactingin open environments that abound in cooperation possibilities. In a typical openenvironment, however, the agents may be using a large number of different strategies.Also, the agents may enter and leave the population at any time resulting in a veryvolatile agent group. In this paper, we evaluate a restricted class of exploitativeand reciprocative strategies in a stable agent population. The particular strategieschosen are representative of their respective classes and are meant to illustrate generalproperties of such strategies. The variants of reciprocative strategies that we study aredesigned to reduce the number of interactions necessary between agents to developmutually beneficial relationships without succumbing to exploitation by malevolentagents. The success of these strategies will allow us to use it in less stable (morevolatile) groups. Our goal is to reduce the time period for which a typical agent willhave to be a part of the group before reciprocative strategies prove to be more usefulthan exploitative strategies. While we have made notable progress, it is unlikely thatany non-monetary approach will work for a completely open, volatile system whereany agent can enter and leave the system at any time, and in particular in thosesituations where a typical agent interacts with other agents in an environment onlyonce or for very few interactions.– We assume that helpful agents are also honest, while exploitative agents can bedeceitful and can lie to gain undue advantage or hurt other agents. In practical settings,even helpful agents can lie. In those situation, an agent must not only estimate thehelp-giving nature of another agent, but also the reliability of its opinion about otheragents. Techniques similar to action estimation and reinforcement learning [32] can beused to learn such estimates. Our goal in this paper was to focus on the developmentof trust-based reciprocal relationships, and hence we assumed that agents who aretrustworthy help-givers are also trustworthy for their opinions. In domains where such188S. Sen / Artificial Intelligence 142 (2002) 179–203assumptions are expected to be violated, one has to augment the reciprocity mechanismwith learning strategies.– In this paper, we assume that tasks do not require specialization, i.e., any agent canperform a given task. In domains where agents have differing expertise and tasksrequire specific expertise, to form fruitful collaborative relationships, agents must learnabout the competence level of other agents for different task types [14].– The selfish agents in this paper are not rational utility maximizers. We have used themto study the effects of disruptive elements in the population. For example, they maynot gain significantly by lying, but that behavior can disrupt the formation of beneficialrelationships between reciprocative agents.The communication structure used in this work is simple and is not the focus of ourresearch. When approaching another agent for help, the requesting agent simply stateswhat the task is. The helping agent then decides whether to help the requesting agent bytaking over this task. In the process of deciding whether to help or not, the requested agentcan ask other agents about their opinion of the help-giving nature of the requesting agent.An agent can be truthful or lying about its opinion of another agent.We believe that the conclusions drawn in this paper will hold for domains andenvironments where the following conditions are met:– The composition of agent group is stable for some amount of time as measured bynumber of tasks executed or the number of interactions between agents.– There exists sufficient number of cooperation possibilities (the cost of helping to thehelper is less than the saving obtained by the helped agent) with roughly symmetricalpossibilities, i.e., sometime one agent can help another and at other times the roles arereversed.– Agent strategies are fixed for the period under consideration.In the real-world, we observe reciprocal, mutually beneficial, non-monetary relationsdevelop universally among neighbors, friends, colleagues, etc. who help each other bytaking on chores, extra work, etc., without any monetary compensation. Even nationaland international agencies share information routinely with counterparts as such reciprocalsharing is recognized to be an effective and cost-saving approach to gathering high-quality information which may be difficult or costly to obtain otherwise. We believethat development of reciprocative relationships among diverse groups in widely varyingdomains cannot be explained by simply attributing this phenomenon to lack of rationalityof the participating agencies. One possible benefit of such non-monetary relationships isto eliminate the need for calculating a utility value for each help-giving and help-seekingbehavior, thereby reducing the computational load on the agent. In our formulation in thispaper, we use time costs for determining help-giving decisions. Time estimates are mucheasier to form and use and does not require a considerably more complex and uncertainutility calculation.As a practical example of a an application domain, software agents performinginformation tasks on behalf of associated users can help each other by sharing information,processing capability, etc. For example personal assistant agents of two users chargedS. Sen / Artificial Intelligence 142 (2002) 179–203189with fetching general news and financial news may decide to split up the work where onegathers general news and the other gathers financial news and then share the retrieved andprocessed information. This way each agent spends considerably less system resources toprovide the required service. We have used a simulation of such a domain to evaluate thebasic reciprocity strategy augmented with learning capabilities [14].5. Agent strategiesThere are two types agents that we have used in our previous work on which we willexpand in this paper:– Selfish agents: Agents who will request for cooperation but never accept a cooperationrequest. Selfish agents can benefit in the presence of philanthropic agents (agents whoalways help when asked) by exploiting their benevolence.– Reciprocative agents: Agents that uses the balance of cost and savings to stochasticallydecide whether to accept a given request for cooperation.The augmentations on these strategies are as follows:– Believing reciprocative agents: These are agents who use not only their own balancewith another agent, but also the balances as reported by all other agents when decidingwhether or not to provide help. More precisely, in place of using Bki in Eq. (1), abelieving reciprocative agent k usesj (cid:3)=i Bj i while calculating the probability ofhelping agent i.4(cid:1)(cid:1)– Learned-Trust based reciprocative agents: These agents also use combined balances,but includes balances of only those agents with whom it has a favorable balance. Moreprecisely, in place of using Bki in Eq. (1), a learned-trust based reciprocative agent kj (cid:3)=i∧Bkj >0 Bj i while calculating the probability of helping agent i.5uses– Individual lying selfish agents: These agents are designed to exploit the fact thatbelieving or trusting reciprocative agents use balances provided by other agents. Theseagents reveal false impressions about other helpful agents to ruin their reputation.64 We assume that while k is deciding to help i, it finds out the balances that everyone else has with i, butdoes not ask i itself about it. If k were to ask i about its balance with others, lying agents would be able to easilyexploit k.5 A key assumption of this strategy is that helpful agents are also truthful, i.e., agents who have reciprocatedor provided help in the past are also likely to provide honest estimates of the helping nature of other agents.This assumption, of course, can be violated in practice where helpful agents can lie to prevent its partner fromengaging with other agents and thereby being less available for gainful interaction. A solution out of this dilemmais to separately learn the truthfulness and helpful nature of an agent. That extension is beyond the scope of thecurrent paper, where we want to primarily evaluate the feasibility of identifying helpful partners in the presenceof deceitful selfish agents.6 Another motivation for an agent A to give bad rating for another helpful agent B is to deter other agents fromseeking B’s help, thereby leaving B available to help A more often.190S. Sen / Artificial Intelligence 142 (2002) 179–203More precisely, when such an agent, j , is asked for its balance with another agent i, itreveals B(cid:7)j i given by:(cid:2)B(cid:7)j i=C ∗ (−Bj i ), when Bj i > 0,Bj i ,otherwise,where C is a positive constant. This means that the more an agent i helps it, thelarger the negative balance an individual selfish agent will report about agent i to otheragents.– Collaborative lying selfish agents: These agents not only try to spoil the reputation ofhelping agents, but also collaboratively bolsters the reputation of other selfish agentsor agents with whom it has zero balance. More precisely, when such an agent, j isasked for its balance with another agent i, it reveals B(cid:7)j i given by:(cid:2)B(cid:7)j i=C ∗ (−Bj i ), when Bj i > 0,P,otherwise,where C is a positive constant as above and P is a large positive constant. Note thatwe assume that since the selfish agent never helps anyone, other agents with whom ithas 0 balance is to be treated as selfish agents. This means, initially it treats all agentsequivalently. Only when the reciprocative agents start helping it does a collaborativelying selfish agent turn against them!6. Package delivery domainIn the simple package delivery problem that we have used for experimentally evaluatingstrategies, we assume there are N agents, each of which is assigned to deliver T packets.All the packets are located in a centralized depot. The packet destinations are located onone of R different radial fins, and at a distance between 1 and D from the depot. Agentscan only move towards or away from the depot following one of the fins; they cannot movedirectly between fins (see Fig. 2). On arriving at the depot, an agent is assigned the nextpacket it is to deliver. At this point, it checks if any other agents are currently located in thedepot. If so, it can ask those agents to deliver this packet.The cost of an agent to deliver one of its packets individually is double the distanceof the delivery point from the depot. If it carries another package to help another agent,it incurs one unit of extra cost per unit distance traveled when it is carrying this extrapacket. In addition, if the helping agent is going beyond its destination without this packet,then unit extra cost per unit distance is incurred to account for the return journey. If d1 isthe maximum destination distance of all the packets the helping agent is now planning todeliver, and d2 is the destination distance of the packet of the agent requesting help, thenthe extra cost is given by:(cid:2)e(d1, d2) =d2d2 + (d2 − d1)if d2 (cid:1) d1,otherwise.S. Sen / Artificial Intelligence 142 (2002) 179–203191Fig. 2. An agent picks up one package from the central depot and delivers it to one of the marked locations onone of the radial fins before going back to the depot to retrieve its next package.7. Experimental resultsIn this section, we present experimental results on the package delivery problem withagents using the reciprocity mechanism described in Section 3 to decide whether or notto honor a request for cooperation from another agent (see Figs. 3–8). Unless otherwisenoted, the parameters for the experiments are as follows: N = 100, T = 500, R = 4, D = 3,τ = 0.75, β = 0.5, C = 1, and P = 10. Each of our experiments are run on 10 differentrandomly generated data sets, where a data set consist of an ordered assignment of packagedeliveries to agents. All the agents are assigned the same number of deliveries. We alsoensure that all agents are assigned packages such that sum of the destination distances ofpackages are the same. The evaluation metric is the average time taken by the agents tocomplete all the deliveries and hence a lower value of this metric is preferred over a largervalue. Experiments were run with mixed group of reciprocative and selfish agents. In eachfigure we present the average performance of the reciprocative agents (Reci), the selfishagents (Self) and the entire group of agents (All).7.1. Performance in different mixed populationsThe first set of experiments we report is from our previous work where reciprocative andselfish agents are evaluated in mixed groups while varying the percentage of selfish agents.In this set of experiments, β = 0.5, which means the reciprocative agents are quite cautiousabout giving help. From the results presented in Fig. 3 we see that though the selfish agentsare able to exploit the reciprocative agents to reduce their delivery cost (if they had todeliver all of their packets by themselves, their average time taken would be approximately192S. Sen / Artificial Intelligence 142 (2002) 179–203Fig. 3. Performance of cautions Reciprocative and Selfish agents in mixed groups (100 agents, 500 tasks/agent,β = 0.5, τ = 0.75).2000), the reciprocative agents outperform the selfish agents for a wide range of groupcomposition. Only when the percentage of selfish become a large majority ((cid:2) 80%) dothe selfish performance dominate the performance of the reciprocatives. The performanceof both group of agents, and correspondingly the average for all agents, deteriorate withincreasing selfish agent percentage in the population. The performance of reciprocativeagents decrease as there are fewer reciprocatives to form mutually beneficial partnerships,and also because there are more selfish agents who do not reciprocate help-giving behavior.At about 90% selfish population percentage, the reciprocatives end up doing more workthan they would have done if each agent had ignored all other agents and just completeddelivery of the packages assigned to it. The performance of selfish agents deteriorate asthere are fewer reciprocative agents from whom they can extract help.Next, we ran a set of experiments where we decreased the cautiousness of thereciprocative agents by increasing β to 2. This meant that the reciprocative agents werewilling to incur a larger up-front helping cost to jump-start cooperative relationships. Thisdecrease of cautiousness, however, also meant that selfish agents can exploit reciprocativeagents more often and for larger gains. This intuition was verified from the experimentalresults presented in Fig. 4. We observe that the performance of the selfish dominate thatof the reciprocatives at all population mixes. From Figs. 3 and 4 we see that for selfishpercentage of 10%, both the reciprocatives and the selfish perform better with β = 2than with β = 0.5. As the cooperation level, i.e., the willingness of agents to help others,increase, reciprocatives can form more beneficial partnerships with other reciprocatives.The same attitude can also be exploited by the selfish agents for more gains. When thepercentage of selfish agents in the population is small, the increased loss of reciprocativeagents to selfish agents is compensated by increased gains from other reciprocative agents.That is why they perform better with β = 2 than with β = 0.5. The corresponding increasedS. Sen / Artificial Intelligence 142 (2002) 179–203193Fig. 4. Performance of less cautious Reciprocative and Selfish agents in mixed groups (100 agents, 500tasks/agent, β = 2, τ = 0.75).gains of the selfish is sufficient to outperform the reciprocatives. As the selfish percentageincreases, the performance of both groups deteriorate for reasons mentioned above. Whenselfish percentage increases beyond 50% the performance gulf between the selfish and thereciprocative rapidly widens.The above two sets of experiments underlines the need for agent designers to set theβ values or cooperation levels appropriately such that mutually beneficial relationshipswith other reciprocative agents can be nurtured without exposing the reciprocative agentsto overt exploitation by selfish agents. The tradeoff can be summarized as follows: withincreasing β values, or cooperation level, reciprocatives can more consistently identifyother reciprocatives as beneficial partners, but reciprocatives become more vulnerable toincreased exploitation by selfish agents.We hypothesized that a way out of this dilemma would be for reciprocatives to usethe opinions of other agents about an agent who has sought help before making thehelp-giving decision. The underlying motivation is that if the reciprocative agents couldshare their balances, an agent that refuses to reciprocate help will be identified earlyby all reciprocative agents. Such early identification will severely limit the exploitativepotential of these selfish agents and also enable the reciprocative agents to perform betterby eliminating cost incurred in helping these selfish agents. This line of reasoning led usto designing the believing and learned-trust based reciprocative agents.In the next set of experiments we evaluated mixed groups of believing reciprocativeagents and selfish agents. For this and the following set of experiments, unless otherwisenoted, we used β = 2. We wanted to evaluate the effectiveness of augmented reciprocitymechanisms for producing good performance for a reasonable range of values of β and τ .The goal was to develop mechanisms such that the agent designers do not have to spendsignificant amount of time on fine tuning these parameter values.194S. Sen / Artificial Intelligence 142 (2002) 179–203Fig. 5. Performance of believing reciprocative and selfish agents in mixed groups (100 agents, 500 tasks/agent,β = 2, τ = 0.75).As we see from the results presented in Fig. 5,the sharing of balances doesindeed severely restrict the exploitative edge of the selfish agents. More importantly,the increase in the proportion of selfish agents in the population does not noticeablyincrease their capability of exploiting the reciprocative agents. As expected, the earlyidentification of selfish agents also enable the reciprocative agents to improve theirperformance significantly. At selfish percentage of 10%, the believing reciprocative agentsperform much better than the corresponding group of reciprocative agents who do notincorporate opinions of other agents (see Fig. 4). The average performance of reciprocativeagents suffer with increasing percentage of selfish agents as there are fewer and fewerreciprocative agents with whom mutually beneficial, i.e., cost saving, relationships can beformed.Even though the believing reciprocity approach appear to be effective from theseexperiments, it has a serious shortcoming. As it does not know, a priori, which of the otheragents are selfish or cooperative, a believing reciprocative agent includes balances fromall other agents in its calculations. The selfish agents in the population, therefore, havethe opportunity and the incentive to undermine and disrupt this word-of-mouth reputationmechanism by giving false balances about other agents.In the next set of experiments, we form mixed groups of believing reciprocativeagents and individual lying selfish agents. From Fig. 6 we observe that when there arefew selfish agents, their lying behavior does not noticeably affect the performance ofbelieving reciprocative agents. But as the the percentage of such lying agents increasesabove a threshold of about 50%, critical mass of negative information surmounts thepositive impression created by mutual help between reciprocative agents. At this point thereciprocative agents stop helping each other, and since they do not receive any help fromselfish agents, they end up doing all of their work by themselves. With further increase inS. Sen / Artificial Intelligence 142 (2002) 179–203195Fig. 6. Performance of believing reciprocative and individual lying selfish agents in mixed groups (100 agents,500 tasks/agent, β = 2, τ = 0.75).the percentage of selfish agents, the reciprocatives under-perform the selfish agents as thelying agents are able to extract some help from the reciprocative agents.A more sinister form of lying occurs when selfish agents collude not only to vilify thereputation of reciprocative agents, but falsely tout themselves to be helpful. The believingreciprocative agent are gullible enough to be swayed by this false group impression whichwill even override any negative balance it might have individually with those agents. Thisis actually the other extreme of the effect of word-of-mouth reputation schemes or groupbalances: instead of correctly identifying “bad guys”, now one will incorrectly identify thebad guys as “good guys”.In this set of experiments, we experimented with mixed groups of believing reciproca-tive agents and collaborative lying selfish agents. From Fig. 7 we observe that the col-laborative lying agents are able to exploit the reciprocative agents quite effectively andoverwhelms them when their percentage in the group is more than 20%. In contrast to theindividually lying agents, the collaborative lying agents not only cause poor performanceof reciprocative agents, but saves themselves significant delivery costs by receiving helpfrom the reciprocative agents. The reciprocative agents end up doing about 75% more workthan if they had just delivered all of their assigned packets on their own, i.e., if they hadnever explored collaboration possibilities by helping other agents. In contrast, the lyingselfish agents can cut their workload by about half when their percentage in the popula-tion is about 60%. It is interesting to note that the performance of the selfish agents startdeteriorating when selfish percentages increase above 70%. This happens because thereare fewer reciprocative agents to receive benefit from. It also means that at less than 50%selfish percentage, there are not enough selfish agents to exploit the reciprocative agents tothe extent they are susceptible to exploitation.196S. Sen / Artificial Intelligence 142 (2002) 179–203Fig. 7. Performance of believing Reciprocative and Collaborative lying Selfish in its calculations agents in mixedgroups (100 agents, 500 tasks/agent, β = 2, τ = 0.75).It is clear that collaborative lying is a threat which, if not countered, will make the be-lieving reciprocative strategy unusable. One can always revert to using the base reciproca-tive agent, which does not believe others, and hence is not susceptible to either individualor group lying. But then we have to be happy to concede non-trivial exploitation by evennon-lying selfish agents. Our conjecture for a fix to this problem was to alter the believingreciprocative agent strategy to believe only those agents who have proven to be helpfulin the past. That is, if someone has consistently been of help, it is reasonable to believeits opinion. On the other hand it is unwise to believe someone who has not reciprocatedprior help-giving behaviors. We believed that such a learned-trust based reciprocative agentstrategy may withstand both individual and collaborative lying by selfish agents.In this set of experiments, we evaluated mixed groups of learned-trust based reciproca-tive and collaborative lying selfish agents. Results presented in Fig. 8 show a significantperformance improvement for reciprocative agents. The amount of help received by thelying selfish agents is much less than what non-lying selfish agents received from basicreciprocative agents (see Fig. 4). An interesting observation is the level of exploitation andhence the performance of selfish agents vary only by a small amount over different groupmixes. This set of experiments clearly demonstrated that learned-trust based reciprocativeagents can effectively handle lying selfish agents. This variant of reciprocative agents arealso able to effectively deal with selfish agents who do not lie. The performance curves ofmixed group of learned-trust based reciprocative and individual selfish agents were identi-cal to those for learned-trust based reciprocative and collaborative lying selfish agents. Thisis because, the learned-trust based reciprocative strategy does not consider the opinion ofagents that are not trusted. So, it does not matter if that agent is saying bad things aboutother reciprocative agents or praising other selfish agents as well.S. Sen / Artificial Intelligence 142 (2002) 179–203197Fig. 8. Performance of Learned-trust based Reciprocative and Collaborative lying Selfish agents in mixed groups(100 agents, 500 tasks/agent, β = 2, τ = 0.75).7.2. Performance sensitivity to parameter valuesTo further evaluate the robustness of our proposed strategies we run experimentsby varying system and agent parameters with mixed groups of learned-trust basedreciprocative agents and collaborative lying agents. In all of the following set ofexperiments, the population was split equally between selfish and reciprocative agents.In the first set of experiments in this group, we varied the number of tasks assigned,i.e., packages to be delivered, to each agent from 10 to 500. The number of agents usedwas 100. From the results plotted in Fig. 9 we see that when agents have to deliveronly a few tasks, the selfish agents can perform marginally better, but as the numberof tasks increase beyond 100, the reciprocatives dominate. We have observed that thedominance of reciprocatives happen with even fewer tasks if the selfish agents are not lying.These two results combine to show that learned-trust based reciprocative strategies aredominant when agents interact for much shorter periods compared with basic reciprocitystrategy (which takes about 500 tasks per agent to dominate selfish strategies for similarenvironmental parameters). The performance difference between reciprocative and selfishagents increases with increasing number of tasks. When agents have to deliver only a fewtasks, selfish agents can exploit different reciprocative agents before their bad reputationcatches on. For the believing reciprocative strategy to be effective, reciprocative agentsmust first identify helpful partners and then use their opinion to shun selfish agents. In thisinitial period of trust development, reciprocative agents are vulnerable to exploitation byselfish agents. When agents have to deliver only a small number of tasks, this initial periodof vulnerability is sufficient for selfish agents to extract enough benefits to outperformreciprocative agents.198S. Sen / Artificial Intelligence 142 (2002) 179–203Fig. 9. Performance of Learned-trust based Reciprocative and Collaborative lying Selfish agents in mixed groupswhen varying number of tasks (100 agents, 50% selfish agents, β = 2, τ = 0.75).Fig. 10. Performance of Learned-trust based Reciprocative and Collaborative lying Selfish agents in mixed groupswhen varying number of agents (500 tasks per agent, 50% selfish agents, β = 2, τ = 0.75).In the next set of experiments, we varied the number of agents from 50 to 500, whereeach agent was assigned the delivery of 500 packages. From Fig. 10 we see that theperformance of both selfish and reciprocative agents improve as the number of agentsincrease. This is because as the number of agents increase there are also more agents whoare willing to provide help. Initially, the performance of the reciprocative agents improveS. Sen / Artificial Intelligence 142 (2002) 179–203199Fig. 11. Performance of Learned-trust based Reciprocative and Collaborative lying Selfish agents in mixed groupswhen varying β (500 tasks per agent, 50% selfish agents, N = 100, τ = 0.75).dramatically, but when the number of agents increases beyond 200, their performance startsto degrade gradually. At this stage the performance of the selfish agents steadily improveand they outperform the reciprocative agents at population size of 500. This happensbecause with a large population, each reciprocative agent gets exploited a little by a largenumber of selfish agents. This loss cannot be compensated by gains from the increasednumber of reciprocative agents in the population unless there are sufficient number oftasks to be delivered. We conjecture that if the number of tasks are increased from 500, thepopulation size at which selfish performance overtakes reciprocative performance will alsoincrease. Moreover, the reciprocative will continue to dominate the selfish if the number oftasks per agent is proportional to the number of agents. The latter situation is not realisticin practice. Hence, in large populations, it is perhaps more effective to be cautious or use adecreased cooperation level, i.e., use a smaller value of β.In the next set of experiments, we varied the cooperation level or the inclination ofreciprocative agents to incur initial cooperation costs to identify mutually helpful partners.We varied β from 0.5 to 30 keeping population size at 100 and number of deliveries at500 tasks per agent. Results from Fig. 11 show that with increasing β value reciprocativeperformance degrade and selfish performance improve until selfish starts dominating atβ > 15. This is expected because if each reciprocative agent is willing to incur significanthelping costs before turning its back on exploiters, the selfish agents can extract enoughhelp to reduce its own workload. The exact β value for performance crossover of selfishand reciprocative agents will depend on the number of agents in the population, thepercentage of selfish agents, the number of tasks per agent, etc. But the general natureof the plots presented in Fig. 11 will hold for other values of strategy and systemparameters.200S. Sen / Artificial Intelligence 142 (2002) 179–203Fig. 12. Probability functions for τ = 0.04, 0.75, and 4.Table 1Performance with varying τ valueτ0.040.754.0Reci-avg-timeStd-devSelf-avg-timeStd-dev1616.451630.091677.375.325096.062217.32551968.821944.331858.23.715495.660312.1221In the final set of experiments, we used τ values of 0.04, 0.75, and 4 to denote threedifferent shapes of the probability function used to make help giving decisions (seeEq. (1)). The three probability functions are depicted in Fig. 12. Strategy and systemparameters used in this set of experiments were as follows: N = 100, T = 500, β = 2,selfish percentage = 50%. From Table 1, we see that reciprocative performance dominateselfish performance for all three τ values used. With increasing τ , however, there is asmall decrease in performance of reciprocative agents and a corresponding increase inperformance of selfish agents.The above set of experiments demonstrate that the believing reciprocative agentsare successful in identifying and benefiting from mutually cooperative relationships andresisting exploitation by lying selfish agents for a wide range of system and strategyparameters. Though we present experimental results under certain values of parameterslike R, D, etc., we have observed that the qualitative nature of the plots are consistentacross a much larger set of parameter values. We also believe our results are more robustthan those derived by Axelrod, and most of the criticisms levied against that body ofwork [4] does not apply to our case. For example, learned trust based reciprocativeagents will be able to collaboratively identify and then shun exploiters if the agentsinteract for any extended period of time. The deterministic tit-for-tat strategy cannot beused effectively in the package delivery and other real-life task-based situations wherethe costs for performing different tasks can be significantly different. For example, anexploitative agent can help a tit-for-tat agent on a task with small cost and then extracta large cost from the latter by assigning it a much more complex, time-intensive, orcostly task. There are other, related problems with the tit-for-tat strategy. In our previousexperiments comparing tit-for-tat strategies, we have found that the performance variationsof these agents are significant [27]. This means that one tit-for-tat agent can becomeS. Sen / Artificial Intelligence 142 (2002) 179–203201envious of other tit-for-tat agents and change strategy. The standard deviation of ourprobabilistic reciprocity agents are quite small, and hence these strategies are much morestable.8. Conclusions and future workIn this paper, we consider the effects of believing other agents’ opinions when decidingto help an agent. Such pooling of opinions is found to effectively restrict the exploitativegains of selfish agents. We then investigate the performance of lying selfish agents,where both individual and group level exploitative schemes may be used. We study theweaknesses of the probabilistic reciprocity based help-giving strategy when using opinionsof individual and group based exploitative strategies. These schemes are shown to beable to “invade” a homogeneous group of believing reciprocative agents, the latter beingparticularly susceptible to group exploitation by lying selfish agents. We introduce anexperience based trust mechanism for reciprocative agents that is able to successfullywithstand invasion by both individual and group level exploitative schemes. The addition ofthe trust mechanism restores the stability of the probabilistic reciprocity based strategy. Thelearned trust based strategy will enable self-interested agents take advantage of cooperationpossibilities in the environment by developing stable, mutually beneficial relationshipswith similar agents without being exposed to exploitation by malevolent agents. As a result,both individual and system level performance can be significantly improved.One of our future goals is to analytically capture the dynamics of the evolutionof balance of helps in homogeneous and heterogeneous groups. For example, given aparticular group composition and random interactions between members, how do thebalances of selfish and reciprocative agents change as a function of time? Differenceor differential equation models can be constructed to represent the dynamics of thesesocieties. In addition to identifying the ascendancy of exploitative or cooperativerelationships, such models can also allow us to identify the formation of demes or workingcoalitions based on interaction histories.We are currently studying the viability of these strategies in an evolutionary setting.In such a scenario, the population would start with some random proportion of differentagent types. Based on the relative performances of the different agent types, the populationwould be modified either continually or periodically with “clones” of better performingindividuals replacing individuals performing poorly. Such a scenario corresponds to thereal-world scenario where self-interested agents are not limited to a fixed strategy but canadopt behaviors that is found to be more effective in practice. It would be instructive tostudy both the final converged populations given different initial distributions and also toanalyze the dynamics of the population as it evolves.AcknowledgementsThis work has been supported in part by an NSF CAREER award IIS-9702672. Wewould like to acknowledge the programming efforts of Anish Biswas and Sandip Debnath.202S. Sen / Artificial Intelligence 142 (2002) 179–203Special thanks go to Partha Sarathi Dutta for redesigning the code and re-running theexperiments as well as generating the plots. The comments of anonymous reviewers wereof invaluable help in revising and improving both the content and the presentation of thispaper.References[1] A.A. Armstrong, E.H. Durfee, Mixing and memory: Emergent cooperation in an information marketplace,in: Proc. Third International Conference on Multiagent Systems, Los Alamitos, CA, 1998, pp. 34–41.[2] R. Axelrod, The Evolution of Cooperation, Basic Books, New York, 1984.[3] R. Azoulay-Schwartz, S. Kraus, Stable strategies for sharing information among agents, in: Proc. IJCAI-01,Seattle, WA, 2001, pp. 1128–1134.[4] K. Binmore, Playing Fair: Game Theory and the Social Contract, MIT Press, Cambridge, MA, 1994.[5] A. Biswas, S. Sen, S. Debnath, Limiting deception in groups of social agents, Applied Artificial Intelligence(Special Issue on Deception, Fraud, and Trust in Agent Societies) 14 (8) (2000) 785–797.[6] R. Boyd, Is the repeated Prisoner’s Dilemma a good model of reciprocal altruism?, Ethol. Sociobiology 9(1988) 211–222.[7] J.M. Bradshaw, Software Agents, AAAI Press/The MIT Press, Menlo Park, CA, 1997.[8] W. Briggs, D. Cook, Flexible social laws, in: Proc. IJCAI-95, Montreal, Quebec, 1995, pp. 688–693.[9] CACM July 1994 issue: Communications of the ACM, Special Issue on Intelligent Agents 37 (7) (July1994).[10] CACM March 1999 issue: Communications of the ACM, Special Issue on Multiagent Systems on the Netand Agents in E-commerce 42 (3) (March 1999).[11] C. Castelfranchi, R. Conte, M. Paolucci, Normative reputation and the costs of compliance, J. ArtificialSocieties and Social Simulation 1 (3) (1998).[12] C. Castelfranchi, R. Falcone, Principles of trust for MAS: Cognitive autonomy, social importance, andquantification, in: Proc. Third International Conference on Multiagent Systems, Los Alamitos, CA, 1998,pp. 72–79.[13] A. Cesta, M. Miceli, Help under risky conditions: Robustness of the social attitude and system performance,in: Proc. Second International Conference on Multiagent Systems, Menlo Park, CA, 1996, pp. 18–25.[14] P.S. Dutta, S. Sen, Identifying partners and sustenance of stable, effective coalitions,in: Proc. 5thInternational Conference on Autonomous Agents, New York, 2001, pp. 23–24.[15] A. Glass, B. Grosz, Socially conscious decision-making, in: Proc. Fourth International Conference onAutonomous Agents, New York, 2000, pp. 217–224.[16] C. Goldman, J.S. Rosenschein, Emergent coordination through the use of cooperative state-changing rules,in: Working Papers of the 12th International Workshop on Distributed Artificial Intelligence, 1993, pp. 171–185.[17] C. Goldman, J.S. Rosenschein, Emergent coordination through the use of cooperative state-changing rules,in: Proc. AAAI-94, Seattle, WA, 1994, pp. 408–413.[18] E. Hoffman, K.A. McCabe, V.L. Smith, Behavioral foundations of reciprocity: Experimental economics andevolutionary psychology, Economic Inquiry 36 (3) (1998) 335–352.[19] M.N. Huhns, M.P. Singh, Readings in Agents, Morgan Kaufmann, San Francisco, CA, 1997.[20] N. Jennings, K. Sycara, M. Wooldridge, A roadmap of agent research and development, Internat.J. Autonomous Agents and Multi-Agent Systems 1 (1) (1998) 7–38.[21] D. Krebs, Altruism—An examination of the concept and a review of the literature, Psychological Bull. 73 (4)(1970) 258–302.[22] J. Martinez-Coll, J. Hirshleifer, The limits of reciprocity, Rationality and Society 3 (1991) 35–64.[23] A. Rapoport, Prisoner’s dilemma, in: J. Eatwell, M. Milgate, P. Newman (Eds.), The New Palgrave: GameTheory, Macmillan, London, 1989, pp. 199–204.[24] T. Sandholm, V. Lesser, Coalitions among Computationally bounded agents, Artificial Intelligence 94 (1)(1997) 99–137.S. Sen / Artificial Intelligence 142 (2002) 179–203203[25] D. Schmitz, Reasons for Altruism, Social Philosophy and Policy 10 (1) (1993) 52–68.[26] M. Sekaran, S. Sen, To help or not to help, in: Proc. Seventeenth Annual Conference of the Cognitive ScienceSociety, Hillsdale, NJ, 1995, pp. 736–741.[27] S. Sen, Reciprocity: A foundational principle for promoting cooperative behavior among self-interestedagents, in: Proc. Second International Conference on Multiagent Systems, Menlo Park, CA, 1996, pp. 315–321.[28] S. Sen, A. Biswas, Effects of misconception on reciprocative agents, in: Proc. Second InternationalConference on Autonomous Agents, New York, 1998, pp. 430–435.[29] S. Sen, A. Biswas, S. Debnath, Believing others: Pros and cons, in: Proc. Fourth International Conferenceon Multiagent Systems, Los Alamitos, CA, 2000, pp. 279–285.[30] Y. Shoham, M. Tennenholtz, On the synthesis of useful social laws for artificial agent societies (PreliminaryReport), in: Proc. AAAI-92, San Jose, CA, 1992, pp. 276–281.[31] D.G. Sullivan, B. Grosz, S. Kraus, Intention reconciliation by collaborative agents, in: Proc. FourthInternational Conference on Multiagent Systems, Los Alamitos, CA, 2000, pp. 293–300.[32] R.S. Sutton, A.G. Barto, Reinforcement Learning: An Introduction, MIT Press, Cambridge, MA, 1998.[33] R. Trivers, The evolution of reciprocal altruism, Quarterly Review of Biology 46 (1972) 35–57.[34] G. Weiß, Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence, MIT Press,Cambridge, MA, 1999.[35] M.P. Wellman, A market-oriented programming environment and its application to distributed multicom-modity flow problems, J. Artificial Intelligence Res. 1 (1) (1993) 1–22.