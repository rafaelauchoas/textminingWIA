Artificial Intelligence 173 (2009) 413–436Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintUsing arguments for making and explaining decisions ✩Leila Amgoud∗, Henri PradeInstitut de Recherche en Informatique de Toulouse, CNRS – University of Toulouse III, 118 route de Narbonne, 31062 Toulouse, Cedex 09, Francea r t i c l ei n f oa b s t r a c tArticle history:Received 8 November 2006Received in revised form 13 November 2008Accepted 17 November 2008Available online 24 November 2008Keywords:Decision makingArgumentationArguments play two different roles in day life decisions, as well as in the discussion ofmore crucial issues. Namely, they help to select one or several alternatives, or to explainand justify an already adopted choice.This paper proposes the first general and abstract argument-based framework for decisionmaking. This framework follows two main steps. At the first step, arguments for beliefs andarguments for options are built and evaluated using classical acceptability semantics. At thesecond step, pairs of options are compared using decision principles. Decision principlesare based on the accepted arguments supporting the options. Three classes of decisionprinciples are distinguished: unipolar, bipolar or non-polar principles depending on whetheri) only arguments pros or only arguments cons, or ii) both types, or iii) an aggregationof them into a meta-argument are used. The abstract modelis then instantiated byexpressing formally the mental states (beliefs and preferences) of a decision maker. Inthe proposed framework, information is given in the form of a stratified set of beliefs.The bipolar nature of preferences is emphasized by making an explicit distinction betweenprioritized goals to be pursued, and prioritized rejections that are stumbling blocks tobe avoided. A typology that identifies four types of argument is proposed. Indeed, eachdecision is supported by arguments emphasizing its positive consequences in terms ofgoals certainly satisfied and rejections certainly avoided. A decision can also be attackedby arguments emphasizing its negative consequences in terms of certainly missed goals,or rejections certainly led to by that decision. Finally, this paper articulates the optimisticand pessimistic decision criteria defined in qualitative decision making under uncertainty,in terms of an argumentation process. Similarly, different decision principles identified inmultiple criteria decision making are restated in our argumentation-based framework.© 2008 Elsevier B.V. All rights reserved.1. IntroductionDecision making, often viewed as a form of reasoning toward action, has raised the interest of many scholars includingphilosophers, economists, psychologists, and computer scientists for a long time. Any decision problem amounts to selecting✩The present paper unifies and develops the content of several conference papers [L. Amgoud, J.-F. Bonnefon, H. Prade, An argumentation-based approachto multiple criteria decision, in: Proceedings of the 8th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty(ECSQARU’05), 2005, pp. 269–280; L. Amgoud, H. Prade, A bipolar argumentation-based decision framework, in: Proceedings of the 11th InternationalConference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU’06), 2006, pp. 323–330; L. Amgoud, H. Prade,Comparing decisions in an argumentation-based setting, in: Proceedings of the 11th International Workshop on Non-Monotonic Reasoning (NMR’06), 2006;L. Amgoud, H. Prade, Explaining qualitative decision under uncertainty by argumentation, in: Proceedings of the 21st National Conference on ArtificialIntelligence (AAAI’06), 2006, pp. 219–224 [2,7–9]].* Corresponding author.E-mail address: amgoud@irit.fr (L. Amgoud).URL: http://www.irit.fr/~Leila.Amgoud/ (L. Amgoud).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.11.006414L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436the “best” or sufficiently “good” action(s) that are feasible among different alternatives, given some available informationabout the current state of the world and the consequences of potential actions. Note that available information may beincomplete or pervaded with uncertainty. Besides, the goodness of an action is judged by estimating, maybe by means ofseveral criteria, how much its possible consequences fit the preferences or the intentions of the decision maker. This agentis assumed to behave in a rational way [41,42,49], at least in the sense that his decisions should be as much as possibleconsistent with his preferences. However, we may have a more requiring view of rationality, such as demanding for theconformity of decision maker’s behavior with postulates describing how a rational agent should behave [45].Decision problems have been considered from different points of view. We may distinguish two main trends, which arecurrently influencing research in artificial intelligence (AI): classical decision theory on the one hand, and cognitively-orientedapproaches such as practical reasoning or beliefs-desires-intentions (BDI) settings on the other hand.1.1. Classical decision making vs. practical reasoningClassical decision theory, as developed mainly by economists, has focused on making clear what is a rational decisionmaker. Thus, they have looked for principles for comparing different alternatives. A particular decision principle, such asthe classical expected utility [45], has been justified on the basis of a set of rationality postulates to which the preferencerelation between actions should obey. This means that in this approach, rationality is captured through a set of postulatesthat describe what is a rational decision behavior. Moreover, a minimal set of postulates is identified in such a way that itcorresponds to a unique decision principle. The inputs of this approach are a set of candidate actions, and a function thatassesses the value of their consequences when the actions are performed in a given state, together with complete or partialinformation about the current state of the world. In other words, such an approach distinguishes between knowledge andpreferences, which are respectively encoded in practice by a distribution function assessing the plausibility of the differentstates of the world, and by a utility function encoding preferences by estimating how good a consequence is. The output is apreference relation between actions encoded by the associated principle. Note that such an approach aims at rank-orderinga group of candidate actions rather than focusing on a candidate action individually. Moreover, the candidate actions aresupposed to be feasible. Roughly speaking, we may distinguish two groups of works in AI dealing with decision that followthe above type of approach. The first group is represented by researches using Bayesian networks [40], on planning underuncertainty (e.g. [21]). Besides, some AI works have aimed at developing more qualitative frameworks for decision, but stillalong the same line of thoughts (e.g. [22,27,47]).Other researchers working on practical reasoning, starting with the generic question “what is the right thing to do foran agent in a given situation” [41,43], have proposed a two steps process to answer this question. The first step, oftencalled deliberation [49], consists of identifying the goals of the agent. In the second step, they look for ways of achievingthose goals, i.e. for plans, and thus for intermediary goals and sub-plans. Such an approach raises issues such as: howare goals generated ? are actions feasible ? do actions have undesirable consequences ? are sub-plans compatible ? arethere alternative plans for achieving a given goal, . . . . In [16], it has been argued that this can be done by representingthe cognitive states, namely agent’s beliefs, desires and intentions (thus the so-called BDI architecture). This requires arich knowledge/preference representation setting, which contrasts with the classical decision setting that directly uses anuncertainty distribution (a probability distribution in the case of expected utility), and a utility (value) function. Besides,the deliberation step is merely an inference problem since it amounts to finding a set of desires that are justified on thebasis of the current state of the world and of conditional desires. Checking if a plan is feasible and does not lead to badconsequences is still a matter of inference. A decision problem only occurs when several plans or sub-plans are possible,and one of them has to be chosen. This latter issue may be viewed as a classical decision problem. What is worth noticingin most works on practical reasoning is the use of argument schemes for providing reasons for choosing or discarding anaction (e.g. [30,35]). For instance, an action may be considered as potentially useful on the basis of the so-called practicalsyllogism [48]:• G is a goal for agent X• Doing action A is sufficient for agent X to carry out goal G• Then, agent X ought to do action AThe above syllogism is in essence already an argument in favor of doing action A. However, this does not mean that theaction is warranted, since other arguments (called counter-arguments) may be built or provided against the action. Thosecounter-arguments refer to critical questions identified in [48] for the above syllogism. In particular, relevant questionsare “Are there alternative ways of realizing G?”, “Is doing A feasible?”, “Has agent X other goals than G?”, “Are thereother consequences of doing A which should be taken into account?”. Recently in [10,11], the above syllogism has beenextended to explicitly take into account the reference to ethical values in arguments. Anyway, the idea of using argumentsfor justifying or discarding candidate decisions is certainty very old, and its account in the literature at least dates back toAristotle. See also Benjamin Franklin [33] for an early precise account on the way of balancing arguments pros and cons achoice, more than two hundred years ago.L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–4364151.2. Argumentation and decision makingGenerally speaking, argumentation is a reasoning model based on the construction and the evaluation of interact-ing arguments. Those arguments are intended to support/explain/attack statements that can be decisions, opinions, . . . .Argumentation has been used for different purposes [44], such as non-monotonic reasoning (e.g. [28]). Indeed, severalframeworks have been developed for handling inconsistency in knowledge bases (e.g. [1,3,13]). Moreover, it has been shownthat such an approach is general enough to capture different existing approaches for non-monotonic reasoning [28]. Argu-mentation has also been extensively used for modeling different kinds of dialogues, in particular persuasion (e.g. [5]), andnegotiation (e.g. [37]). Indeed, an argumentation-based approach for negotiation has the advantage of exchanging in additionto offers, reasons that support these offers. These reasons may lead their receivers to change their preferences. Consequently,an agreement may be more easily reached with such approaches, when in other approaches (where agent’s preferences arefixed) negotiation may fail. Adopting such an approach in a decision problem would have some obvious benefits. Indeed,not only would the user be provided with a “good” choice, but also with the reasons underlying this recommendation, in aformat that is easy to grasp. Note that each potential choice usually has pros and cons of various strengths. Argumentation-based decision making is expected to be more akin with the way humans deliberate and finally make or also understand achoice. This has been pointed out for a long time (see e.g. [33]).1.3. Contribution of the paperIn this paper we deal with an argumentative view of decision making, thus focusing on the issue of justifying thebest decision to make in a given situation, and leaving aside the other related aspects of practical reasoning such as goalgeneration, feasibility, and planning. It is why we remain close to the classical view of decision, but now discussed in termsof arguments. The idea of articulating decisions on the basis of arguments is relevant for different decision problems orapproaches such as decision making under uncertainty, multiple criteria decisions, or rule-based decisions. These problemsare usually handled separately, and until recently without a close reference to argumentation. In practical applications, forinstance in medical domain, the decision to be made has to be chosen under incomplete or uncertain information, thepotential results of candidate decisions are evaluated from different criteria. Moreover, there may exist some expertise inthe form of decision rules that associate possible decisions to given contexts. This makes the different decision problemssomewhat related, and consequently a unified argumentation-based model is needed. This paper proposes such a model.This paper proposes the first general, and abstract argument-based framework for decision making. This framework fol-lows two main steps. At the first step, arguments for beliefs and arguments for options are built and evaluated using classicalacceptability semantics. At the second step, pairs of options are compared using decision principles. Decision principles arebased on the accepted arguments supporting the options. Three classes of decision principles are distinguished: unipolar,bipolar or non-polar principles depending on whether i) only arguments pros or only arguments cons, or ii) both types, oriii) an aggregation of them into a meta-argument are used. The abstract model is then instantiated by expressing formallythe mental states (beliefs and preferences) of a decision maker. In the proposed framework, information is given in the formof a stratified set of beliefs. The bipolar nature of preferences is emphasized by making an explicit distinction between prior-itized goals to be pursued, and prioritized rejections that are stumbling blocks to be avoided. A typology that identifies fourtypes of argument is also proposed. Indeed, each decision is supported by arguments emphasizing its positive consequencesin terms of goals certainly satisfied and rejections certainly avoided. A decision can also be attacked by arguments empha-sizing its negative consequences in terms of certainly missed goals, or rejections certainly led to by that decision. Anothercontribution of the paper consists of applying the general framework to decision making under uncertainty and to multiplecriteria decision. Proper choices of decision principles are shown to be equivalent to known qualitative decision approaches.The paper is organized as follows: Section 2 presents an abstract framework for decision making. Section 3 discussesa typology of arguments supporting or attacking candidate decisions. Section 4 applies the abstract framework to multiplecriteria decision making, and Section 5 applies the framework to decision making under uncertainty. Section 6 compares ourapproach to existing works on argumentation-based decision making, and Section 7 is devoted to some concluding remarksand perspectives.2. A general framework for argumentative decision makingSolving a decision problem amounts to defining a pre-ordering, usually a complete one, on a set D of possible options(or candidate decisions), on the basis of the different consequences of each decision. Let us illustrate this problem througha simple example borrowed from [31].Example 1 (Having or not a surgery). The example is about having a surgery (sg) or not (¬sg), knowing that the patient hascolonic polyps. The knowledge base contains the following information:• having a surgery has side-effects,• not having surgery avoids having side-effects,• when having a cancer, having a surgery avoids loss of life,416L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436• if a patient has cancer and has no surgery, the patient would lose his life,• the patient has colonic polyps,• having colonic polyps may lead to cancer.In addition to the above knowledge, the patient has also some goals like: “no side effects” and “to not lose his life”.Obviously it is more important for him to not lose his life than to not have side effects.In what follows, L will denote a logical language. From L, a finite set D = {d1, . . . , dn} of n options is identified. Notethat an option di may be a conjunction of other options in D. Let us, for instance, assume that an agent wants a drinkand has to choose between tea, milk or both. Thus, there are three options: d1 : tea, d2 : milk and d3 : tea and milk. InExample 1, the set D contains only two options: d1 : sg and d2 : ¬sg.Argumentation is used in this paper for ordering the set D. An argumentation-based decision process can be decomposedinto the following steps:(1) Constructing arguments in favor/against statements (pertaining to beliefs or decisions)(2) Evaluating the strength of each argument(3) Determining the different conflicts among arguments(4) Evaluating the acceptability of arguments(5) Comparing decisions on the basis of relevant “accepted” argumentsNote that the first four steps globally correspond to an “inference problem” in which one looks for accepted arguments, andconsequently warranted beliefs. At this step, one only knows what is the quality of arguments in favor/against candidatedecisions, but the “best” candidate decision is not determined yet. The last step answers this question once a decisionprinciple is chosen.2.1. Types of argumentsAs shown in Example 1, decisions are made on the basis of available knowledge and the preferences of the decisionmaker. Thus, two categories of arguments are distinguished: i) epistemic arguments justifying beliefs and are themselvesbased only on beliefs, and ii) practical arguments justifying options and are built from both beliefs and preferences/goals.Note that a practical argument may highlight either a positive feature of a candidate decision, supporting thus that decision,or a negative one, attacking thus the decision.Example 2 (Example 1 cont.). In this example, α = [“the patient has colonic polyps”, and “having colonic polyps may leadto cancer”] is considered as an argument for believing that the patient may have cancer. This epistemic argument involvesonly beliefs. While δ1 = [“the patient may have a cancer”, “when having a cancer, having a surgery avoids loss of life”] isan argument for having a surgery. This is a practical argument since it supports the option “having a surgery”. Note thatsuch argument involves both beliefs and preferences. Similarly, δ2 = [“not having surgery avoids having side-effects”] is apractical argument in favor of “not having a surgery”. However, the two practical arguments δ3 = [“having a surgery hasside-effects”] and δ4 = [“the patient has colonic polyps”, and “having colonic polyps may lead to cancer”, “if a patient hascancer and has no surgery, the patient would lose his life”] are respectively against surgery and no surgery since they pointout negative consequences of the two options.In what follows, Ae denotes a set of epistemic arguments, and Ap denotes a set of practical arguments such thatAe ∩ Ap = ∅. Let A = Ae ∪ Ap (i.e. A will contain all those arguments). The structure and origin of the arguments areassumed to be unknown. Epistemic arguments will be denoted by variables α1, α2, . . . , while practical arguments will bereferred to by variables δ1, δ2, . . . . When no distinction is necessary between arguments, we will use the variables a, b, c, . . . .Example 3 (Example 1 cont.). Ae = {α} while Ap = {δ1, δ2, δ3, δ4}.Let us now define two functions that relate each option to the arguments supporting it and to the arguments against it.• Fp : D → 2the option.• Fc : D → 2option.Ap is a function that returns the arguments in favor of a candidate decision. Such arguments are said proAp is a function that returns the arguments against a candidate decision. Such arguments are said con theThe two functions satisfy the following requirements:• ∀d ∈ D, (cid:2)δ ∈ Ap s.t. δ ∈ Fp(d) and δ ∈ Fc(d). This means that an argument is either in favor of an option or againstthat option. It cannot be both.L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436417• If δ ∈ Fp(d) and δ ∈ Fp(done option.• Let D = {d1, . . . , dn}. Ap = (concern options of the set D.(cid:9)) (resp. if δ ∈ Fc(d) and δ ∈ Fc(d(cid:2)(cid:2)(cid:9))), then d = d(cid:9). This means that an argument refers only toFp(di)) ∪ (Fc(di)), with i = 1, . . . , n. This means that the available practical argumentsWhen δ ∈ Fx(d) with x ∈ {p, c}, we say that d is the conclusion of δ, and we write Conc(δ) = d.Example 4 (Example 1 cont.). The two options of the set D = {sg, ¬sg} are supported/attacked by the following arguments:Fp(sg) = {δ1}, Fc(sg) = {δ3}, Fp(¬sg) = {δ2}, and Fc(¬sg) = {δ4}.2.2. Comparing argumentsAs pointed out by several researchers (e.g. [19,29]), arguments may have forces of various strengths. These forces playtwo key roles: i) they may be used in order to refine the notion of acceptability of epistemic or practical arguments, ii)they allow the comparison of practical arguments in order to rank-order candidate decisions. Generally, the strength of anepistemic argument reflects the quality, such as the certainty level, of the pieces of information involved in it. Whereas thestrength of a practical argument reflects both the quality of knowledge used in the argument, as well as how important itis to fulfill the preferences to which the argument refers.In our particular application, three preference relations between arguments are defined. The first one, denoted by (cid:2)e , is a(partial or total) preorder1 on the set Ae . The second relation, denoted by (cid:2)p , is a (partial or total) preorder on the set Ap .Finally, a third relation, denoted by (cid:2)m (m stands for mixed relation), captures the idea that any epistemic argument isstronger than any practical argument. The role of epistemic arguments in a decision problem is to validate or to underminethe beliefs on which practical arguments are built. Indeed, decisions should be made under “certain” information. Thus,∀α ∈ Ae , ∀δ ∈ Ap , (α, δ) ∈ (cid:2)m and (δ, α) /∈ (cid:2)m.Note that (a, b) ∈ (cid:2)x, with x ∈ {e, p, m}, means that a is at least as good as b. At some places, we will also write a (cid:2)x b.In what follows, >x denotes the strict relation associated with (cid:2)x. It is defined as follows: (a, b) ∈ >x iff (a, b) ∈ (cid:2)x and(b, a) /∈ (cid:2)x. When (a, b) ∈ (cid:2)x and (b, a) ∈ (cid:2)x, we say that a and b are indifferent, and we write a ≈x b. When (a, b) /∈ (cid:2)x and(b, a) /∈ (cid:2)x, the two arguments are said incomparable.Example 5 (Example 1 cont.). (cid:2)e = {(α, α)} and (cid:2)m = {(α, δ1), (α, δ2)}. Now, regarding (cid:2)p , one may, for instance, assumethat δ1 is stronger than δ2 since the goal satisfied by δ1 (namely, not loss of life) is more important than the one satisfiedby δ2 (not having side effects). Thus, (cid:2)p= {(δ1, δ1), (δ2, δ2), (δ1, δ2)}. This example will be detailed in a next section.2.3. Attacks among argumentsSince knowledge may be inconsistent, the arguments may be conflicting too. Indeed, epistemic arguments may attackeach others. Such conflicts are captured by the binary relation Re ⊆ Ae × Ae . This relation is assumed abstract and itsorigin is not specified.Epistemic arguments may also attack practical arguments when they challenge their knowledge part. The idea is that anepistemic argument may undermine the beliefs part of a practical argument. However, practical arguments are not allowedto attack epistemic ones. This avoids wishful thinking. This relation, denoted by Rm, contains pairs (α, δ) where α ∈ Aeand δ ∈ Ap .We assume that practical arguments do not conflict. The idea is that each practical argument points out some advantageor some weakness of a candidate decision, and it is crucial in a decision problem to list all those arguments for eachcandidate decision, provided that they are accepted w.r.t. the current epistemic state, i.e. built from warranted beliefs.According to the attitude of the decision maker in face of uncertain or inconsistent knowledge, these lists associated withthe candidate decisions may be taken into account in different manners, thus leading to different orderings of the decisions.This is why all accepted arguments should be kept, whatever their strengths, for preserving all relevant information in thedecision process. Otherwise, getting rid of some of those accepted arguments (w.r.t. knowledge), for instance because theywould be weaker than others, may prevent us to have a complete view of the decision problem and then may even lead usto recommend decisions that would be wrong w.r.t. some decision principles (agreeing with the presumed decision maker’sattitude). This point will be made more concrete in a next section. Thus, the relation Rp ⊆ Ap × Ap is equal to the emptyset (Rp = ∅).Each preference relation (cid:2)x (with x ∈ {e, p, m}) is combined with the conflict relation Rx into a unique relation betweenarguments, denoted by Defx and called defeat relation, in the same way as in ([4], Definition 3.3, page 204).1 A preorder is a binary relation that is reflexive and transitive.418L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436Definition 1 (Defeat relation). Let A be a set of arguments, and a, b ∈ A. (a, b) ∈ Defx iff:• (a, b) ∈ Rx, and• (b, a) /∈ >xLet Defe , Defp and Defm denote the three defeat relations corresponding to the three attack relations. In case of Defm,the second bullet of Definition 1 is always true since epistemic arguments are strictly preferred (in the sense of (cid:2)m) to anypractical arguments. Thus, Defm = Rm (i.e. the defeat relation is exactly the attack relation Rm). The relation Defp is thesame as Rp , thus it is empty. However, the relation Defe coincides with its corresponding attack relation Re in case allthe arguments of the set Ae are incomparable.2.4. Extensions of argumentsNow that the sets of arguments and the defeat relations are identified, we can define the decision system.Definition 2 (Decision system). Let D be a set of options. A decision system for ordering D is a triple AF = (D, A, Def) whereA = Ae ∪ Ap2 and Def = Defe ∪ Defp ∪ Defm.3Note that a Dung style argumentation system is associated to a decision system AF = (D, A, Def), namely the system(A, Def). This latter can be seen as the union of two distinct argumentation systems: AFe = (Ae, Defe), called epistemicsystem, and AFp = (Ap, Defp), called practical system. The two systems are related to each other by the defeat relationDefm.Due to Dung’s acceptability semantics defined in [28], it is possible to identify among all the conflicting arguments,which ones will be kept for ordering the options. An acceptability semantics amounts to define sets of arguments thatsatisfy a consistency requirement and must defend all their elements.Definition 3 (Conflict-free, Defence). Let AF = (D, A, Def) be a decision system, B ⊆ A, and a ∈ A.• B is conflict-free iff (cid:2)a, b ∈ B s.t. (a, b) ∈ Def.• B defends a iff ∀b ∈ A, if (b, a) ∈ Def, then ∃c ∈ B s.t. (c, b) ∈ Def.The main semantics introduced by Dung are recalled in the following definition. Note that other semantics have beendefined in the literature (e.g. [12]). However, these will not be discussed in this paper.Definition 4 (Acceptability semantics). Let AF = (D, A, Def) be a decision system, and B be a conflict-free set of arguments.• B is admissible extension iff it defends any element in B.• B is a preferred extension iff B is a maximal (w.r.t. set ⊆) admissible set.• B is a stable extension iff it is a preferred extension that defeats any argument in A\B.Using these acceptability semantics, a status is assigned to each argument of AF as follows.Definition 5 (Argument status). Let AF = (D, A, Def) be a decision system, and E1, . . . , Ex its extensions under a givensemantics. Let a ∈ A.• a is skeptically accepted iff a ∈ Ei , ∀Ei with i = 1, . . . , x.• a is credulously accepted iff ∃Ei such that a ∈ Ei .• a is rejected iff (cid:2)Ei such that a ∈ Ei .A direct consequence of the above definition is that an argument is skeptically accepted iff it belongs to the intersectionof all extensions, and that it is rejected iff it does not belong to the union of all extensions. Formally:Property 1. Let AF = (D, A, Def) be a decision system, and E1, . . . , Ex its extensions under a given semantics. Let a ∈ A.• a is skeptically accepted iff a ∈• a is rejected iff a /∈(cid:2)Ei .xi=1(cid:3)xi=1Ei ;2 Recall that options are related to their supporting and attacking arguments by the functions Fp and Fc respectively.3 Since the relation Defp is empty, then Def = Defe ∪ Defm .L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436419Let Acc(x, y) be a function that returns the skeptically accepted arguments of decision system x under semantics y( y ∈ {ad, st, pr} with ad (resp. st and pr) stands for admissible (resp. stable and preferred) semantics). This set may containboth epistemic and practical arguments. Such arguments are very important in argumentation process since they supportthe conclusions to be inferred from a knowledge base or the options that will be chosen. Indeed, for ordering the differentcandidate decisions, only skeptically accepted practical arguments are used. The following property shows the links betweenthe sets of accepted arguments under different semantics.Property 2. Let AF = (D, A, Def) be a decision system.• Acc(AF, ad) = ∅.• If AF has no stable extensions, then Acc(AF, st) = ∅ and Acc(AF, st) ⊆ Acc(AF, pr).• If AF has stable extensions, then Acc(AF, pr) ⊆ Acc(AF, st).Proof. Let AF = (D, A, Def) be a decision system.• In [28],(cid:3)it has been shown that the empty set is an admissible extension of any argumentation system. Thus,Ei=1,...,x = ∅ where E1, . . . , Ex are the admissible extensions of AF. Consequently, Acc(AF, ad) = ∅.• Let us assume that the system AF has no stable extensions. Thus, according to Definition 5, all arguments of A arerejected. Thus, Acc(AF, st) = ∅.• Let us now assume that the system AF has stable extensions, say E1, . . . , En. Dung has shown in [28] that any stableextension is a preferred one, but the converse is not true. Thus, E1, . . . , En are also preferred extensions. Let us nowassume that the system has other extensions that are preferred but not stable, say En+1, . . . , Ex with x (cid:2) n + 1. Fromset theory, it is clear thatEi . According to Property 1, it follows that Acc(AF, pr) ⊆ Acc(AF, st). (cid:2)Ei ⊆(cid:3)(cid:3)ni=1xi=1From the above property, one concludes that in a decision problem, it is not interesting to use admissible semantics. Thereason is that no argument is accepted. Consequently, argumentation will not help at all for ordering the different candidatedecisions. Let us illustrate this issue through the following simple example.Example 6. Let us consider the decision system AF = (D, Ae ∪ Ap, Def) where D = {d1, d2}, Ae = {α1, α2, α3}, Ap = {δ}and Def is depicted in figure below. We assume that Fp(d1) = δ whereas Fp(d2) = Fc(d2) = ∅.The admissible extensions of this system are: E1 = {}, E2 = {α1}, E3 = {α2}, E4 = {α1, δ} and E5 = {α2, δ}. Under admissiblesemantics, the practical argument δ is not skeptically accepted. Thus, the two options d1 and d2 may be equally preferredsince the first one has an argument but not an accepted one, and the second has no argument at all. However, the samedecision system has two preferred extensions: E4 and E5. Under preferred semantics, the set Acc(AF, pr) contains theargument δ (i.e. Acc(AF, pr) = {δ}). Thus, it is natural to prefer the option d1 to d2.Consequently, in the following, we will use stable semantics if the system has stable extensions, otherwise preferredsemantics will be considered for computing the set Acc(AF, y).Since the defeat relation Defp is empty, it is trivial that the practical system AFp has exactly one preferred/stableextension which is the set Ap itself.Property 3. The practical system AFp = (Ap, Defp) has a unique preferred/stable extension, which is the set Ap .Proof. This follows directly from the fact that the set Ap is conflict-free since Defp = ∅. (cid:2)It is important to notice that the epistemic system AFe in its side is very general and does not necessarily presentparticular properties like for instance the existence of stable/preferred extensions.In what follows, we will show that the result of the decision system depends broadly on the outcome of its epistemicsystem. The first result states that the epistemic arguments of each admissible extension of AF constitute an admissibleextension of the epistemic system AFe .Theorem 1. Let AF = (D, Ae ∪ Ap, Defe ∪ Defp ∪ Defm) be a decision system, E1, . . . , En its admissible extensions, and AFe =(Ae, Defe) its associated epistemic system.420L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436• ∀Ei , the set Ei ∩ Ae is an admissible extension of AFe .• ∀E (cid:9)is an admissible extension of AFe , ∃Ei such that E (cid:9) ⊆ Ei ∩ Ae .such that E (cid:9)Proof.• Let Ei be an admissible extension of AF. Let E = Ei ∩ Ae . Let us assume that E is not an admissible extension of AFe .There are two cases:Case 1:Case 2:E is not conflict-free. This means that ∃α1, α2 ∈ E such that (α1, α2) ∈ Defe . Thus, ∃α1, α2 ∈ Ei such that(α1, α2) ∈ Def. This is impossible since Ei is an admissible extension, thus conflict-free.E does not defend its elements. This means that ∃α ∈ E , such that ∃α(cid:9) ∈ Ae , (α(cid:9), α) ∈ Defe and (cid:2)α(cid:9)(cid:9) ∈ E suchthat (α(cid:9)(cid:9), α(cid:9)) ∈ Defe . Since (α(cid:9), α) ∈ Defe , this means that (α(cid:9), α) ∈ Def with α ∈ Ei . However, Ei is admissi-ble, then ∃a ∈ Ei such that (a, α(cid:9)) ∈ Def. Assume that a ∈ Ap . This is impossible since practical arguments arenot allowed to defeat epistemic ones. Thus, a ∈ Ae . Hence, a ∈ E . Contradiction.• Let E (cid:9)does not defend all its elements in the system AF. This means that ∃a ∈ E (cid:9)be an admissible extension of AFe . Let us prove that E (cid:9)an admissible extension of AF. There are two possibilities: i) E (cid:9)is an admissible extension of AFe , thus conflict-free.such that E (cid:9)ii) E (cid:9)does not defend a.This means also that ∃b /∈ E (cid:9)such that (c, b) ∈ Def. There are two cases: eitherb ∈ Ae or b ∈ Ap . b cannot be in Ae since E (cid:9)is an admissible extension thus defends its arguments against any attack,consequently it defends also a against b. Assume now that b ∈ Ap , this is also impossible since practical arguments arenot allowed to attack epistemic ones. Thus, E (cid:9)is notis not conflict-free in AF. This is not possible since E (cid:9)is an admissible extension of AF. Assume that E (cid:9)is an admissible extension of the system AF. (cid:2)such that (b, a) ∈ Def and (cid:2)c ∈ E (cid:9)Note that the above theorem holds as well for stable and preferred extensions since each stable (resp. preferred) exten-sion is an admissible one.It is easy to show that when Defm is empty, i.e. no epistemic argument defeats a practical one, then the extensions ofAF (under a given semantics) are exactly the different extensions of AFe (under the same semantics) augmented by theset AFp .Theorem 2. Let AF = (D, Ae ∪ Ap, Defe ∪ Defp ∪ Defm) be a decision system. Let E1, . . . , En be the extensions of AFe under agiven semantics. If Defm = ∅ then ∀Ei with i = 1, . . . , n, then the set Ei ∪ Ap is an extension of AF.Proof. Let E be an admissible extension of AFe . Let us assume that E ∪ Ap is not an admissible extension of AF. There aretwo cases:Case 1:Case 2:E ∪ Ap is not conflict-free. Since E and Ap are conflict-free, then ∃α ∈ E and ∃δ ∈ Ap such that (α, δ) ∈ Def.Contradiction with the fact that Defm = ∅.E ∪ Ap does not defend its elements. This means that: i) ∃α ∈ E such that ∃α(cid:9) ∈ Ae , (α(cid:9), α) ∈ Defe and E ∪ Apdoes not defend it. Impossible since E is an admissible extension then it defends its arguments. ii) ∃δ ∈ Ap suchthat ∃a ∈ A, and (a, δ) ∈ Def and δ is not defended by E ∪ Ap . Since Defm = ∅ then a ∈ Ap . This is impossiblesince Rp = ∅. Contradiction. (cid:2)Finally, it can be shown that if the empty set is the only admissible extension of the decision system AF, then the emptyset is also the only admissible extension of the corresponding epistemic system AFe . Moreover, each practical argument isattacked by at least one epistemic argument.Theorem 3. Let AF = (D, Ae ∪ Ap, Defe ∪ Defp ∪ Defm) be a decision system. The only admissible extension of AF is the emptyset iff :(1) The only admissible extension of AFe is the empty set, and(2) ∀δ ∈ Ap , ∃α ∈ Ae such that (α, δ) ∈ Defm.Proof. Let AF = (D, Ae ∪ Ap, Defe ∪ Defp ∪ Defm) be a decision system.Case 1: Assume that the empty set is the only admissible extension of AF. Assume also that the epistemic system AFe hasa non-empty admissible extension, say E. This means that E is not an admissible extension of AF. There are twocases:a) E is not conflict-free. This is impossible since E is an admissible extension of AFe .b) E does not defend its elements. This means that ∃a ∈ Ae ∪ Ap such that ∃b ∈ E and (a, b) ∈ Def and (cid:2)c ∈ EL. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436421such that (c, a) ∈ Def. There are two possibilities: i) a ∈ Ap . This is impossible since practical arguments are notallowed to attack epistemic arguments. ii) a ∈ Ae . Since E is an admissible extension of AFe , then ∃c ∈ E such that(c, a) ∈ Defe . Thus, (c, a) ∈ Def. Contradiction.Let us now assume that the empty set is the only admissible extension of AFe and that ∀δ ∈ Ap , ∃α ∈ Ae suchthat (α, δ) ∈ Defm. Assume also that ∃E (cid:13)= ∅ such that E is an admissible extension of the decision system AF.From Theorem 1, E ∩ Ae is an admissible extension of AFe . Since the only admissible extension of AFe is theCase 2:empty set, then E ∩ Ae = ∅. Thus, E ⊆ Ap .Let δ ∈ E. By assumption, ∃α ∈ Ae such that (α, δ) ∈ Defm. Since E is an admissible extension, thus it defendsall its elements. Consequently, ∃δ(cid:9) ∈ E such that (δ(cid:9), α) ∈ Def. Since E ⊆ Ap , then δ(cid:9) ∈ Ap . It is impossible to have(δ(cid:9), α) ∈ Def since practical arguments are not allowed to attack epistemic ones. (cid:2)At this step, we have only defined the accepted arguments among all the existing ones. However, nothing is yet saidabout which option to prefer. In the next section, we will study different ways of comparing pairs of options on the basisof skeptically accepted practical arguments.2.5. Ordering optionsComparing candidate decisions, i.e. defining a preference relation (cid:14) on the set D of options, is a key step in a decisionprocess. In an argumentation-based approach, the definition of this relation is based on the sets of “accepted” argumentspros or cons associated with candidate decisions. Thus, the input of this relation is no longer Ap , but the set Acc(AF, y) ∩Ap , where Acc(AF, y) is the set of skeptically accepted arguments of the decision system (D, A, Def) under stable orpreferred semantics. In what follows, we will use the notation Acc(AF) for short.Note that in a decision system, when the defeat relation Defm is empty, the epistemic arguments become useless forthe decision problem, i.e. for ordering options. Thus, only the practical system AFp is needed.Depending on what sets are considered and how they are handled, one can roughly distinguish between three categoriesof principles:Unipolar principles: are those that only refer to either the arguments pros or the arguments cons.Bipolar principles: are those that take into account both types of arguments at the same time.Non-polar principles: are those where arguments pro and arguments con a given choice are aggregated into a uniquemeta-argument. It results that the negative and positive polarities disappear in the aggregation.Whatever the category is, a relation (cid:14) should suitably satisfy the following minimal requirements:(1) Transitivity: The relation should be transitive (as usually required in decision theory).(2) Completeness: Since one looks for the “best” candidate decision, it should then be possible to compare any pair ofchoices. Thus, the relation should be complete.2.5.1. Unipolar principlesIn this section we present basic principles for comparing decisions on the basis of only arguments pro. Similar ideasapply to arguments cons. We start by presenting those principles that do not involve the strength of arguments, then theirrespective refinements when strength is taken into account.A first natural criterion consists of preferring the decision that has more arguments pros.Definition 6 (Counting arguments pros). Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Letd1, d2 ∈ D.d1 (cid:14) d2iff(cid:4)(cid:4)(cid:4) (cid:2)(cid:4)Fp(d1) ∩ Acc(AF)(cid:4)(cid:4)(cid:4).(cid:4)Fp(d2) ∩ Acc(AF)Property 4. This relation is a complete preorder.Note that when the decision system has no accepted arguments (i.e. Acc(AF) = ∅), all the options in D are equallypreferred w.r.t. the relation (cid:14). It can be checked that if a practical argument is defined as done later in Definition 18, then(cid:9)with such a principle, one may prefer a decision d, which has three arguments pointing all to the same goal, to decision d,which is supported by two arguments pointing to different goals.When the strength of arguments is taken into account in the decision process, one may think of preferring a choice thathas a dominant argument, i.e. an argument pros that is preferred w.r.t. the relation (cid:2)p⊆ Ap × Ap to any argument pro theother choices. This principle is called promotion focus principle in [2].Definition 7. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D.d1 (cid:14) d2iff ∃δ ∈ Fp(d1) ∩ Acc(AF) such that ∀δ(cid:9) ∈ Fp(d2) ∩ Acc(AF), δ (cid:2)p δ(cid:9).422L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436With this criterion, if the decision system has no accepted arguments, then all the options in D are equally preferred. Theabove definition relies heavily on the relation (cid:2)p that compares practical arguments. Thus, the properties of this criteriondepend on those of (cid:2)p . Namely, it can be checked that the above criterion works properly if (cid:2)p is a complete preorder.Property 5. If the relation (cid:2)p is a complete preorder, then (cid:14) is also a complete preorder.Note that the above relation may be found to be too restrictive, since when the strongest arguments in favor of d1 andd2 have equivalent strengths (i.e. are indifferent), d1 and d2 are also seen as equivalent. However, we can refine the abovedefinition by ignoring the strongest arguments with equal strengths, by means of the following strict preorder.Definition 8. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D, and (cid:2)p bea complete preorder. Let (δ1, . . . , δr ), (δ(cid:9)∈ Fp(d2) ∩Acc(AF).s) such that ∀δi=1,...,r , δi ∈ Fp(d1) ∩ Acc(AF), and ∀δ(cid:9)j=1,...,s, δ(cid:9)1, . . . , δ(cid:9)jEach of these vectors is assumed to be decreasingly ordered w.r.t. (cid:2)p (e.g. δ1 (cid:2)p · · · (cid:2)p δr ). Let v = min(r, s).d1 (cid:14) d2 iff:1, or• δ1 >p δ(cid:9)k and ∀ j < k, δ j ≈p δ(cid:9)• ∃k (cid:3) v such that δk >p δ(cid:9)• r > v and ∀ j (cid:3) v, δ j ≈p δ(cid:9)j .j , orTill now, we have only discussed decision principles based on arguments pros. However, the counterpart principles whenarguments cons are considered can also be defined. Thus, the counterpart principle of the one defined in Definition 6 is thefollowing complete preorder:Definition 9 (Counting arguments cons). Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Letd1, d2 ∈ D.d1 (cid:14) d2iff(cid:4)(cid:4)(cid:4) (cid:3)(cid:4)Fc(d1) ∩ Acc(AF)(cid:4)(cid:4)(cid:4).(cid:4)Fc(d2) ∩ Acc(AF)The principles that take into account the strengths of arguments have also their counterparts when handling argumentscons. The prevention focus principle prefers a decision when all its cons are weaker than at least one argument against theother decision. Formally:Definition 10. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D.d1 (cid:14) d2iff ∃δ ∈ Fc(d2) ∩ Acc(AF) such that ∀δ(cid:9) ∈ Fc(d1) ∩ Acc(AF), δ (cid:2)p δ(cid:9).As in the case of arguments pros, when the relation (cid:2)p is a complete preorder, the above relation is also a completepreorder, and can be refined into the following strict one.Definition 11. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D.Let (δ1, . . . , δr ), (δ(cid:9)s) such that ∀δi=1,...,r , δi ∈ Fc(d1) ∩ Acc(AF), and ∀δ(cid:9)∈ Fc(d2) ∩ Acc(AF).Each of these vectors is assumed to be decreasingly ordered w.r.t. (cid:2)p (e.g. δ1 (cid:2)p · · · (cid:2)p δr ). Let v = min(r, s).d1 (cid:15) d2 iff:j=1,...,s, δ(cid:9)1, . . . , δ(cid:9)j1 >p δ1, or• δ(cid:9)• ∃k (cid:3) v such that δ(cid:9)• v < s and ∀ j (cid:3) v, δ j ≈p δ(cid:9)j .k >p δk and ∀ j < k, δ j ≈p δ(cid:9)j , or2.5.2. Bipolar principlesLet’s now define some principles where both types of arguments (pros and cons) are taken into account when com-paring decisions. Generally speaking, we can conjunctively combine the principles dealing with arguments pros with theircounterpart handling arguments cons. For instance, the principles given in Definition 6 and Definition 9 can be combinedas follows:Definition 12. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D. d1 (cid:14) d2 iff• |Fp(d1) ∩ Acc(AF)| (cid:2) |Fp(d2) ∩ Acc(AF)|, and• |Fc(d1) ∩ Acc(AF)| (cid:3) |Fc(d2) ∩ Acc(AF)|.L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436423However, note that unfortunately this is no longer a complete preorder. Similarly, the principles given respectively inDefinition 7 and Definition 10 can be combined into the following one:Definition 13. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D. d1 (cid:14) d2 iff:• ∃δ ∈ Fp(d1) ∩ Acc(AF) such that ∀δ(cid:9) ∈ Fp(d2) ∩ Acc(AF), δ (cid:2)p δ(cid:9)• (cid:2)δ ∈ Fc(d1) ∩ Acc(AF) such that ∀δ(cid:9) ∈ Fc(d2) ∩ Acc(AF), δ (cid:2)p δ(cid:9)., andThis means that one prefers a decision that has at least one supporting argument which is better than any supportingargument of the other decision, and also has not a very strong argument against it. Note that the above definition can bealso refined in the same spirit as Definitions 8 and 11.Another family of bipolar decision principles applies the Franklin principle which is a natural extension to the bipolar caseof the idea underlying Definition 8. This principle consists, when comparing pros and cons a decision, of ignoring pairs ofarguments pros and cons which have the same strength. After such a simplification, one can apply any of the above bipolarprinciples. In what follows, we will define formally the Franklin simplification.Definition 14 (Franklin simplification). Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Letd ∈ D.Let P = (δ1, . . . , δr), C = (δ(cid:9)Each of these vectors is assumed to be decreasingly ordered w.r.t. (cid:2)p (e.g. δ1 (cid:2)p · · · (cid:2)p δr ). The result of the simplification(cid:9) = (δ j+1, . . . , δr), Cm) such that ∀δi, δi ∈ Fp(d) ∩ Acc(AF) and ∀δ(cid:9)∈ Fc(d) ∩ Acc(AF).1, . . . , δ(cid:9)(cid:9) = (δ(cid:9)j, δ(cid:9)is Pjj+1, . . . , δ(cid:9)m) s.t.• ∀1 (cid:3) i (cid:3) j, δi ≈p δ(cid:9)• If j = r (resp. j = m), then Pi and (δ j+1 >p δ(cid:9)j+1 or δ(cid:9)(cid:9) = ∅ (resp. Cj+1 >p δ j+1).(cid:9) = ∅).2.5.3. Non-polar principlesIn some applications, the arguments in favor of and against a decision are aggregated into a unique meta-argumenthaving a unique strength. Thus, comparing two decisions amounts to compare the resulting meta-arguments. Such a view iswell in agreement with current practice in multiple criteria decision making, where each decision is evaluated according todifferent criteria using the same scale (with a positive and a negative part), and an aggregation function is used to obtain aglobal evaluation of each decision.Definition 15 (Aggregation criterion). Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Letd1, d2 ∈ D. Let (δ1, . . . , δn)4 and (δ(cid:9)k )7) the vectors of the arguments pro andcon the decision d1 (resp. d2).d1 (cid:14) d2 iff h(δ1, . . . , δn, δ(cid:9)1, . . . , δ(cid:9)m) (cid:2)p h(γ1, . . . , γl, γ (cid:9)m)5 (resp. (γ1, . . . , γl)6 and (γ (cid:9)k), where h is an aggregation function.1, . . . , γ (cid:9)1, . . . , γ (cid:9)1, . . . , δ(cid:9)A simple example of this aggregation attitude is computing the difference of the number of arguments pros and cons.Definition 16. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D.d1 (cid:14) d2 iff |Fp(d1) ∩ Acc(AF)| − |Fc(d1) ∩ Acc(AF)| (cid:2) |Fp(d2) ∩ Acc(AF)| − |Fc(d2) ∩ Acc(AF)|.This has the advantage to be again a complete preorder, while taking into account both pros and cons arguments.3. A typology of formal practical argumentsThis section aims at presenting a systematic study of practical arguments. Epistemic arguments will not be discussed herebecause they have been much studied in the literature (e.g. [3,13,46]), and their handling does not make new problems inthe general setting of Section 2, even in the decision process perspective of this paper. Moreover, they only play a role whenthe knowledge base is inconsistent. Before presenting the different types of practical arguments, we start first by introducingthe logical language as well as the different bases needed in a decision making problem.4 Each δi ∈ Fp(d1) ∩ Acc(AF).5 Each δ(cid:9)∈ Fc(d1) ∩ Acc(AF).i6 Each γi ∈ Fp(d2) ∩ Acc(AF).7 Each γ (cid:9)∈ Fc(d2) ∩ Acc(AF).i424L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–4363.1. Logical representation of knowledge and preferenceThis section introduces the representation setting of knowledge and preference which are here distinct, as it is in classicaldecision theory. Moreover, preferences are supposed to be handled in a bipolar way, which means that what the decisionmaker is really looking for may be more restrictive than what it is just willing to avoid.In what follows, a vocabulary P of propositional variables contains two kinds of variables: decision variables, denotedby v 1, . . . , vn, and state variables. Decision variable are controllable, that is their value can be fixed by the decision maker.Making a decision then amounts to fixing the truth value of every decision variable. On the contrary, state variables arefixed by nature, and their value is a matter of knowledge by the decision maker. He has no control on them (although hemay express preferences about their values).(1) D is a set of formulas built from the decision variables. Elements of D represent the different alternatives, or candidatedecisions. Let us consider the following example of an agent who wants to know whether she should take her umbrella,her raincoat or both. In this case, there are two decision variables: umb (for umbrella) and rac (for raincoat). Assumethat this agent hesitates between the three following options: i) d1 : umb (i.e. to take her umbrella), ii) d2 : rac (i.e.to take her raincoat), or iii) d3 : umb ∧ rac (i.e. to take both). Thus, D = {d1, d2, d3}. Note that elements of D are notnecessarily mutually exclusive. In the example, if the agent chooses the option d3 then the two other options aresatisfied.(2) G is a set of propositional formulas built from state variables. It gathers the goals of an agent (the decision maker).A goal represents what the agent wants to achieve, and has thus a positive flavor. This means that if g ∈ G, the decisionmaker wants that the chosen decision leads to a state of affairs where g is true. This base may be inconsistent. In thiscase it would be for sure impossible to satisfy all the goals, which would induce the simultaneous existence of practicalarguments pros and cons. In general G contains several goals. Clearly, an agent should try to satisfy all goals in itsgoal base G if possible. This means that G may be thought as a conjunction. However, the two goal bases G = {g1, g2}and G(cid:9) = {g1 ∧ g2} although they are logically equivalent, will not be handled in the same way in an argumentativeperspective, since in the second case there is no way to consider intermediary objectives such as here satisfying g1,or satisfying g2 only, in case it turns out that it is impossible to satisfy g1 ∧ g2. This means that our approach issyntax-dependent.(3) The set R is a set of propositional formulas built from state variables. It gathers the rejections of an agent. A rejectionrepresents what the agent wants to avoid. Clearly rejections express negative preferences. The set {¬r | r ∈ R} describingwhat is acceptable for the agent is assumed to be consistent, since acceptable alternatives should satisfy ¬r due to therejection of r, and at least there should remain some possible worlds that are not rejected. There are at least tworeasons for separately considering a set of goals and a set of rejections. First, since agents naturally express themselvesin terms of what they are looking for (i.e. their goals), and in terms of what they want to avoid (i.e. their rejections), itis better to consider goals and rejections separately in order to articulate arguments referring to them in a way easilyunderstandable for the agents. Moreover, recent cognitive psychology studies [17] have confirmed the cognitive validityof this distinction between goals and rejections. Second, if r is a rejection, this does not necessarily mean that ¬r is agoal, and thus rejections cannot be equivalently restated as goals. For instance, in case of choosing a medical drug, onemay have as a goal the immediate availability of the drug, and as a rejection its availability only after at least two days.In such a case, if the candidate decision guarantees the availability only after one day, this decision will for sure avoidthe rejection without satisfying the goal. Another simple example is the case of an agent who wants to get a cup ofeither coffee or tea, and wants to avoid getting no drink. If the agent obtains a glass of water, again he would avoid itsrejection, without being completely satisfied.We can imagine different forms of consistency between the goals and the rejections. A minimal requirement is to haveG ∩ R = ∅, otherwise it will mean that an agent both wants to have p true and to avoid it.(4) The set K represents the background knowledge that is not necessarily assumed to be consistent. The argumentationframework for inference presented in Section 2 will handle such inconsistency, namely with the epistemic system.Elements of K are propositional formulas built from the alphabet P , and assumed to be put in a clausal form. Thebase K contains basically two kinds of clauses: i) those not involving any element from D, which encode pieces ofknowledge or factual information (possibly involving goals) about how the world is; ii) those involving one negation ofa formula d of the set D, and which states what follows when decision d is applied.Thus, the decision problem we consider will always be encoded with the four above sets of formulas (with the restrictionsstated above). Moreover, we suppose that each of the three bases K, G, and R are stratified. Having K stratified wouldmean that we consider that some pieces of knowledge are fully certain, while others are less certain (maybe distinguishingbetween several levels of partial certainty such as “almost certain”, “rather certain”, . . . ). Clearly, formulas that are notcertain at all cannot be in K. Similarly, having G (resp. R) stratified means that some goals (resp. rejections) are imperative,while some others are less important (one may have more than two levels of importance). Completely unimportant goals(resp. rejections) do not appear in any stratum of G (resp. R).It is worth pointing out that we assume that candidate decisions are all considered as a priori equally potentially suitable,and thus there is no need to have D stratified.L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436425For encoding the stratifications, we use the set {0, 1, . . . , n} of integers as a linearly ordered scale, where n stands forthe highest level of certainty if dealing with K (resp. level of importance if dealing with G or R) and ‘0’ corresponds to thecomplete lack of certainty (resp. importance). Other encodings (e.g. using levels inside the unit interval or using the integerscale in a reversed way) would be equivalent.Definition 17 (Decision theory). A decision theory (or a theory for short) is a tuple T = (cid:17)D, K, G, R(cid:18).• The base K is partitioned and stratified into K1, . . . , Kn (K = K1 ∪ · · · ∪ Kn) such that formulas in Ki have the samecertainty level and are more certain than formulas in K j where j < i. Moreover, K0 is not considered since it gathersformulas that are completely uncertain.• The base G is partitioned and stratified into G1, . . . , Gn (G = G1 ∪ · · · ∪ Gn) such that goals in Gi have the same impor-tance and are more important than goals in G j where j < i. Moreover, G0 is not considered since it gathers goals thatare completely unimportant.• The base R is partitioned and stratified into R1, . . . , Rn (R = R1 ∪ · · · ∪ Rn) such that rejections in Ri have the sameimportance and are more important than rejections in R j where j < i. Moreover, R0 is not considered since it gathersrejections that are completely unimportant.3.2. A typology of formal practical argumentsEach candidate decision may have arguments in its favor (called pros), and arguments against it (called cons). In thefollowing, an argument is associated with an alternative, and always either refers to a goal or to a rejection.Arguments pros point out the “existence of good consequences” or the “absence of bad consequences” for a candidatedecision. A good consequence means that applying decision d will lead to the satisfaction of a goal, or to the avoidance of arejection. Similarly, a bad consequence means that the application of d leads for sure to miss a goal, or to reach a rejectedsituation.We can distinguish between practical arguments referring to a goal, and those arguments referring to rejections. Whenfocusing on the base G, an argument pros corresponds to the guaranteed satisfaction of a goal when there exists a consistentsubset S of K such that S ∪ {d} (cid:19) g.Definition 18 (Positive arguments pros). Let T be a theory. A positively expressed argument in favor of an option d is a tupleδ = (cid:17)S, d, g(cid:18) s.t.:(1) S ⊆ K, d ∈ D, g ∈ G, S ∪ {d} is consistent(2) S ∪ {d} (cid:19) g, and S is minimal for set inclusion among subsets of K satisfying the above criteria (arguments of Type PP).S is called the support of the argument, and d is its conclusion. Let APP be the set of all arguments of type PP that can bebuilt from a decision theory T .In what follows, Supp denotes a function that returns the support S of an argument, Conc denotes a function thatreturns the conclusion d of the argument, and Result denotes a function that returns the consequence of the decision.The consequence may be either a goal as in the previous definition, or a rejection as we can see in the next definitions ofargument types.The above definition deserves several comments.• The consistency of S ∪ {d} means that d is applicable in the context S, in other words that we cannot prove from Sthat d is impossible. This means that impossible alternatives w.r.t. K have been already taken out when defining theset D. In the particular case where the base K would be consistent, then condition 1, namely S ∪ {d} is consistent, isequivalent to K ∪ {d} is consistent. But, in the case where K is inconsistent, independently from the existence of a PP(cid:9) (cid:19) ¬d. This would mean that there is some doubtargument, it may happen that for another consistent subset Sabout the feasibility of d, and then constitute an epistemic argument against d. In the general framework proposed inSection 2, such an argument will overrule decision d since epistemic arguments take precedence over any practicalargument (provided that this epistemic argument is not itself killed by another epistemic argument).of K, S• Note that argument of type PP are reminiscent of the practical syllogism recalled in the introduction. Indeed, it em-phasizes that a candidate decision might be chosen if it leads to the satisfaction of a goal. However, this is only a cluefor choosing the decision since this last may have arguments against, which would weaken it, or there may exist othercandidate decisions with stronger arguments. Moreover, due to the nature of the practical syllogism, it is worth noticingthat practical arguments have an abductive form, contrarily to epistemic arguments that are defined in a deductive way,as revealed by their formal respective definitions.(cid:9)Another type of arguments pros refers to rejections. It amounts to avoid a rejection for sure, i.e. S ∪ {d} (cid:19) ¬r (where S isa consistent subset of K).426L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436Definition 19 (Negative arguments pros). Let T be a theory. A negatively expressed argument in favor of an option is a tupleδ = (cid:17)S, d, r(cid:18) s.t.:(1) S ⊆ K, d ∈ D, r ∈ R, S ∪ {d} is consistent(2) S ∪ {d} (cid:19) ¬r and S is minimal for set inclusion among subsets of K satisfying the above criteria (arguments of Type NP).Let ANP be the set of all arguments of type NP that can be built from a decision theory T .Arguments cons highlight the existence of bad consequences for a given candidate decision. Negatively expressed argu-ments cons are defined by exhibiting a rejection that is necessarily satisfied. Formally:Definition 20 (Negative arguments cons). Let T be a theory. A negatively expressed argument against an option d is a tupleδ = (cid:17)S, d, r(cid:18) s.t.:(1) S ⊆ K, d ∈ D, r ∈ R, S ∪ {d} is consistent,(2) S ∪ {d} (cid:19) r and S is minimal for set inclusion among subsets of K satisfying the above criteria (arguments of Type NC).Let ANC be the set of all arguments of type NC that can be built from a decision theory T .Lastly, the absence of positive consequences can also be seen as an argument against (cons) an alternative.Definition 21 (Positive arguments cons). Let T be a theory. A positively expressed argument against an option d is a tupleδ = (cid:17)S, d, g(cid:18) s.t.:(1) S ⊆ K, d ∈ D, g ∈ G, S ∪ {d} is consistent,(2) S ∪ {d} (cid:19) ¬g and S is minimal for set inclusion among subsets of K satisfying the above criteria (arguments of Type PC).Let APC be the set of all arguments of type PC that can be built from a decision theory T .Let us illustrate the previous definitions on an example.Example 7. Two decisions are possible, organizing a show (d), or not (¬d). Thus D = {d, ¬d}. The knowledge base K containsthe following pieces of knowledge: if a show is organized and it rains then small money loss (¬d ∨ ¬r ∨ sml); if a showis organized and it does not rain then benefit (¬d ∨ r ∨ b); small money loss entails money loss (¬sml ∨ ml); if benefitthere is no money loss (¬b ∨ ¬ml); small money loss is not large money loss (¬sml ∨ ¬lml); large money loss is moneyloss (¬lml ∨ ml); there are clouds (c); if there are clouds then it may rain (¬c ∨ r). All these pieces of knowledge are in thestratum of level n, except the last one which is in a stratum with a lower level due to uncertainty. Consider now the casesof two organizers (O 1 and O 2) having different preferences. O 1 does not want any loss R = {ml}, and would like benefitG = {b}. O 2 does not want large money loss R = {lml}, and would like benefit G = {b}. In such case, it is expected thatO 1 prefers ¬d to d, since there is a NC argument against d and no argument for ¬d. For O 2, there is no longer any NCargument against d. He might even prefer d to ¬d, if he is optimistic and he considers that there is a possibility that it doesnot rain (leading to a potential PP argument under the hypothesis to have ¬r in K).Due to the asymmetry in human mind between what is rejected and what is desired, the former being usually consid-ered as stronger than the latter, one may assume that NC arguments are stronger than PC arguments, and conversely PParguments are stronger than NP arguments.In classical decision frameworks, bipolarity is not considered. Indeed, we are in the particular case where rejectionsmirror goals in the sense that g is a goal iff ¬g is a rejection. Consequently, in our argumentation setting the two types NCand PC coincide. Similarly, the two types PP and NP are the same.4. Application to multiple criteria decision making4.1. Introduction to multiple criteria decision makingIn multiple criteria decision making, each candidate decision d in D is evaluated from a set C of m different points ofview (i = 1, m), called criteria. The evaluation can be done in an absolute manner or in a relative way. This means thatfor each i, d can be either evaluated by an absolute estimate ci(d) belonging to the evaluation scale used for i, or there(cid:9)) of elementsexists a possibly valued preference relation R i(d, dof D. Then one can distinguish between two families of approaches: i) the ones based on a global aggregation of valuecriteria-based functions where the obtained global absolute evaluations are of the form g( f 1(C1(d), . . . , fm(Cm(d)))) where(cid:9)) associated with each i that is applicable to any pair (d, dL. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436427the mappings f i map the original evaluations on a unique scale, which assumes commensurability, and ii) the ones that(cid:9)) from which a ranking of the elements in D canaggregate the preference indices R i(d, dbe obtained. In the following, only the first type of approach is considered.(cid:9)) into a global preference R(d, d4.2. Arguments in multiple criteria decision makingThe decision maker uses a set C of different criteria. For each criterion ci , one assumes that we have a bipolar univariateordered scale T i which enables us to distinguish between positive and negative values. Such a scale has a neutral point,or more generally a neutral area that separates positive and negative values. The lower bound of the scale stands for totaldissatisfaction and the upper bound for total satisfaction, while neutral value(s) stand for indifference. The closer to the upperbound the value of criterion ci for choice d, denoted ci(d) is, the more satisfactory choice d is w.r.t. ci ; the closer to thelower bound the value of criterion ci for choice d is, the more dis-satisfactory choice d is w.r.t. ci . As in multiple criteriaaggregation, we assume that the different scales T i can be mapped on a unique bipolar scale T , i.e. for any i, f i(ci(d)) ∈ T .Moreover, we assume here that T is discrete and will be denoted T = {−k, . . . , −1, 0, +1, . . . , +k} with the classical orderingconvention of relative integers.Example 8 (Choosing an apartment). Imagine we have a set C of three criteria for choosing an apartment: Price (c1),Size (c2), and Location w.r.t. downtown (c3). The criteria are valued on the same bipolar univariate scale {−2, −1, 0, +1, +2}(this means that all the f i mappings are the identity). Prices of apartments may be judged ‘very expensive’,‘expensive’,‘reasonably priced’,‘very large’. Distancemay be ‘very far’,‘very close’. In each case, the five linguistic expressions would be valued by−2, −1, 0, +1, +2 respectively. Thus an apartment d that is expensive, medium-sized, and very close to downtown willbe evaluated as c1(d) = −1, c2(d) = 0, and c3(d) = +2. It is clear that this scale implicitly encodes that the best apartmentsare those that are very cheap, very large, and very close to downtown.‘very cheap’. Size may be ‘very small’,‘cheap’,‘far’,‘normal sized’,‘medium’,‘small’,‘close’,‘large’,From this setting, it is possible to express goals and rejections in terms of criteria values. A bipolar-valued criterion canbe straightforwardly translated into a set of stratified goals, and a stratified set of rejections. The idea is the following. Thecriteria may be satisfied either in a positive way (if the satisfaction degree is higher than the neutral point 0 of T ) or in anegative way (if the satisfaction degree is lower than the neutral point of T ). Formally speaking, the two bases G and R aredefined as follows: having the condition f i(ci(d)) (cid:2) + j satisfied, where + j belongs to the positive part of T , is a goal g jfor the agent that uses ci as a criterion. This goal is all the more important as j is small (but positive), since as suggestedby the above example, the less restrictive conditions are the most imperative ones. The importance of g j can be taken asequal to k − j + 1 for j (cid:2) 1 (using the standard order-reversing map on {1, . . . , k}). Indeed the most important conditionf i(ci(d)) (cid:2) +1 will have the maximal value in T , while the condition f i(ci(d)) (cid:2) +k will have the minimal positive levelin T , i.e. +1. We can proceed similarly with rejections. The rejection r j corresponding to the condition f i(ci(d)) (cid:3) − j willhave importance j (importance uses only the positive part of the scale). This corresponds to the view of a fuzzy set as anested family of level cuts, which translates in possibilistic logic [25] into a collection of propositions whose extensions areall the larger as the proposition is more imperative. In the above example, consider for instance the price criterion. We willhave two goals: g1 = very cheap and g2 = cheap with respective weights 1 and 2. Thus, being cheap is more imperativethan being very cheap as expected. Similarly, r1 = very expensive and r2 = expensive are rejections with respective weights2 and 1. Note that if an apartment is normally sized, then there will be no argument in favor or against it w.r.t. its size.In multiple criteria aggregation, criteria may have different levels of importance. Let w i ∈ {0, +1, . . . , +k} be the impor-tance of criterion ci . Then, we can apply the above translation procedure where, now the importance k − j + 1 of conditionf i(ci(d)) (cid:2) + j is changed into min(w i, k − j + 1). Indeed, if w i is maximal, i.e. w i = +k, the importance is unchanged; incase the importance w i of criterion ci would be minimal, i.e. w i = 0, then the resulting importance of the associated goal(the condition f i(ci(d)) (cid:2) + j) is indeed also 0 expressing its complete lack of importance.In addition to the bases D, C, G and R, the decision maker is also equipped with a stratified knowledge base K encodingwhat he knows. In particular, K contains factual information about the values of the f i(ci(x))’s for the different criteria andthe different candidate decisions. K also contains rules expressing that values in T are linearly ordered, i.e. rules of the form(cid:9) ∈ T . More generally, K can also contain pieces of knowledge that enable the decisionif ci(x) (cid:2) j, then ci(x) (cid:2) jmaker to evaluate criteria from more elementary evaluation of facts. This may be useful in practice for describing complexnotions, e.g. comfort of a house in our example, which indeed may depend on many parameters. A goal is assumed to beis associated to a criterion ci by a propositional formula of the formassociated with a unique criterion. Then, a goal gi refers to the evaluation of criterion ci . Such formulas will be added to Kn. Notegthat in classical multiple criteria problems, complete information is usually assumed w.r.t. the precise evaluation of criteria.Clearly, our setting is more general since it leaves room to incomplete information, and facilitates the expression of goalsand rejections.→ ci meaning just that the goal gif j (cid:2) jjiji(cid:9)jNow that the different bases are introduced, we can apply our general decision system, and build the arguments prosand cons for any candidate decision. In addition to the completeness of information, it is usually assumed in classicalapproaches to multiple criteria decision making that knowledge is consistent. In such a case, it is not possible to have428L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436conflicting evaluations of a criterion for a choice. Consequently, the whole power of our argumentation setting will not beused, in particular all arguments will be accepted.4.3. Retrieving some classical multiple criteria aggregationsThe aim of this subsection is to show the agreement of the argumentation-based approach with some classical (ele-mentary) approaches to multiple criteria decision making. It is worth mentioning that until recently, most multiple criteriaapproaches use only positive evaluations, i.e. unipolar scales ranging from “bad” to “good” values, rather than distinguishinggenuinely good values from really bad ones that have to be rejected. The argumentation-based approach makes natural thereference to the distinction between what is favored and what is disfavored by the decision maker for giving birth to ar-guments for and arguments against candidate decisions. Here, only two types of arguments are needed: positive argumentspros of type PP, and negative arguments cons of type NC since rejections in this case are just the complement of goals.Indeed, the negative values of a criterion reflect the fact that we are below some threshold while the positive values expressto what extent we are above. Thus, Ap = APP ∪ ANC.In what follows, the base K is supposed to be consistent, fully certain (i.e. K = Kn), and to contain complete informationw.r.t. the evaluation of criteria. Thus, the set A of arguments is exactly Ae ∪ Ap . Since K is consistent, then the two attackrelations Re and Rm are empty (i.e. Re = Rm = ∅). Consequently, Defe = Defm = ∅, and the set of skeptically acceptedarguments of the decision system AF = (D, A, Def = ∅) is exactly Acc(AF) = A.The first category of classical approaches to multiple criteria decision making that we will study is the one that givesthe same importance to the different criteria of the set C. The idea is to prefer the alternative that satisfies positively more(cid:9)i(d) = 0 if ci(d) < 0, where ci(d) is the evaluation of choice d by the ith criterion.criteria. Let cIn order to capture this idea, a particular unipolar principle is used. Before introducing this principle, let us first define afunction Results that returns for a given set B of practical arguments, all the consequences of those arguments, i.e. allthe goals and rejections to which arguments of B refer to.(cid:9)i(d) = 1 if ci(d) > 0 and cDefinition 22. Let d1, d2 ∈ D. d1 (cid:14) d2 iff Results(Fp(d2)) ⊆ Results(Fp(d1)).Note that in our case, Fp(d) ⊆ APP and Fc(d) ⊆ ANC for a given d ∈ D.Property 6. Let AF = (D, A, Def) be a decision system. Let d1, d2 ∈ D. When C = Cn, d1 (cid:14) d2 (according to Definition 22) iff(cid:5)(cid:5)(cid:9)i(d1) (cid:2)i c(cid:9)i(d2).i cIf we focus on arguments cons, the idea is to prefer the option that violates less criteria. Let c(cid:9)(cid:9)i (d) = 1 if ci(d) < 0. This idea is captured by the following unipolar principle.c(cid:9)(cid:9)i (d) = 0 if ci(d) > 0 andDefinition 23. Let d1, d2 ∈ D. d1 (cid:14) d2 iff Results(Fc(d1)) ⊆ Results(Fc(d2)).Property 7. Let (D, A, Def) be a decision system. Let d1, d2 ∈ D. When C = Cn, d1 (cid:14) d2 (according to Definition 23) iff(cid:5)(cid:9)(cid:9)i (d2).i c(cid:5)(cid:9)(cid:9)i (d1) (cid:3)i c(cid:9)i(d) with cWhen the criteria do not have the same level of importance, the promotion focus principle given in Definition 7 amounts(cid:9)i(d) = 0 if ci(d) < 0 as an evaluation function for comparing decisions.to use maxicRecall that the promotion focus principle is based on a preference relation (cid:2)p between arguments. In what follows, we willpropose a definition of the force of an argument, as well as a definition of (cid:2)p . The two definitions are chosen in such away that they will allow us to retrieve the above idea on the promotion focus principle.(cid:9)i(d) = ci(d) if ci(d) > 0 and cIn our application, the force of an argument depends on two components: the certainty level of the knowledge involvedin the argument, and the importance degree of the goal (or rejection). Formally:Definition 24 (Force of an argument). Let δ = (cid:17)S, d, g(cid:18) ∈ APP (resp. δ = (cid:17)S, d, r(cid:18) ∈ ANC). The strength of δ is a pair (Lev(δ),Wei(δ)) s.t.• The certainty level of the argument is Lev(δ) = min{i | 1 (cid:3) i (cid:3) n such that S i (cid:13)= ∅}, where S i denotes S ∩ Ki . If S = ∅then Lev(δ) = n.• The weight of the argument is Wei(δ) = j if g ∈ G j (resp. r ∈ R j ).Note that the above definition is general (and will be re-used as such in the next section). However, here, all argumentshave the same certainty level which is equal to n. The levels of satisfaction of the criteria should be balanced with theirrelative importance. Indeed, for instance, a criterion ci highly satisfied by d is not a strong argument in favor of d if cihas little importance. Conversely, a poorly satisfied criterion for d is a strong argument against d only if the criterion isreally important. Moreover, in case of uncertain criteria evaluation, one may have to discount arguments based on suchL. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436429evaluation. In other terms, the force of an argument represents to what extent the decision will satisfy the most importantcriteria. This suggests the use of a conjunctive combination of the certainty level, the satisfaction/dissatisfaction degree andthe importance of the criterion. This requires the commensurateness of the scales.Definition 25 (Conjunctive strength). Let δ, δ(cid:9) ∈ APP. δ >p δ(cid:9)iff min(Lev(δ), Wei(δ)) > min(Lev(δ(cid:9)), Wei(δ(cid:9))).Property 8. Let AF = (D, A, Def) be a decision system. Let d1, d2 ∈ D. When C = Cn, d1 (cid:14) d2 (according to Definition 7 and usingDefinition 25 for the relation (cid:2)p ) iff maxic(cid:9)i(d1) (cid:2) maxic(cid:9)i(d2).The prevention focus principle (see Definition 10) amounts to use minicif ci(d) < 0.(cid:9)(cid:9)i (d) with c(cid:9)(cid:9)i (d) = 0 if ci(d) > 0 and c(cid:9)(cid:9)i (d) = −ci(d)Property 9. Let AF = (D, A, Def) be a decision system. Let d1, d2 ∈ D. When C = Cn, d1 (cid:14) d2 (according to Definition 10 and usingDefinition 25 for the relation (cid:2)p ) iff minic(cid:9)(cid:9)i (d1) (cid:3) minic(cid:9)(cid:9)i (d2).When each criterion ci is associated with a level of importance w i ranging on the positive part of the criteria scale, theabove c(cid:9)i(d) is changed into min(c(cid:9)i(d), w i) in the promotion case.Property 10. Let AF = (D, A, Def) be a decision system. Let d1, d2 ∈ D. d1 (cid:14) d2 (according to Definition 7 and using Definition 25for the relation (cid:2)p ) iff maximin(c(cid:9)i(d1), w i) (cid:2) maximin(c(cid:9)i(d2), w i).This expresses that d is all the more preferred as there is an important criterion that is positively evaluated. A similarproposition holds for the prevention focus principle. Thus, weighted disjunctions and conjunctions defined in [26] are re-trieved. It would even be possible to provide the argumentative counter-part of a general qualitative weighted conjunctionof the form minimax(ci(d), neg(w i)), where neg is the reversing map of the discrete scale where w i takes its value. However,this would be quite similar to the qualitative decision making under uncertainty problem which is now discussed in greatdetail, and where aggregations having the same structure are encountered.5. Application to decision making under uncertaintyDecision making under uncertainty relies on the comparative evaluation of different alternatives on the basis of a de-cision principle, which can be usually justified by means of a set of rationality postulates. This is, for example, the Savageview of decision making under uncertainty based on expected utility [45]. Thus, standard approaches for making decisionsunder uncertainty consist in defining decision principles in terms of analytical expressions that summarize the whole de-cision process, and for which it is shown that they encode a preference relation obeying postulates that are supposedlymeaningful. Apart from quantitative principles such as expected utility, another example of such an approach is provided bythe qualitative pessimistic or optimistic decision principles, which have been more recently proposed and also axiomaticallyjustified [27,34]. The qualitative nature of these decision evaluations make them more liable to be unpacked in terms ofarguments in favor/against each choice, in order to better understand the underpinnings of the evaluation. We successivelystudy the pessimistic and optimistic decision principles. Note, however, that these qualitative decision criteria do not makeuse of a bipolar univariate scale, so in the following we apply our general decision system with an empty set of rejections.Thus, we will consider a decision theory T = (cid:17)D, K, G(cid:18). Consequently, the set of practical arguments built from such atheory is Ap = APP ∪ APC. Recall that arguments of type PP are pro their conclusions whereas arguments of type PC arecons their conclusions. In classical decision systems, the knowledge base and the goals base are assumed to be consistent.Thus, in what follows, we will assume that they are consistent as well. Thus, the three defeat relations Defe , Defp andDefm are empty. Consequently, the decision system that will be used is (D, Ae ∪ Ap, Def = ∅). In such a system, the wholedecision process is reduced to the last step, which consists of ordering pairs of options. This means that in order to showhow pessimistic and optimistic principles are captured, it is sufficient to choose the most suitable decision principle amongthe ones proposed in Section 2.5.5.1. Pessimistic criterionThe pessimistic decision criterion is defined as follows: given a possibility distribution πd restricting the plausible statesthat can be reached when a decision d takes place, and a qualitative utility function μ, the so-called pessimistic qualitativedecision principle, which estimates a kind of qualitative expectation, is defined as [27]:E∗(d) = minωmax(cid:6)μ(ω), neg(cid:6)πd(ω)(cid:7)(cid:7)(1)where πd is a mapping from a set of interpretations Ω to a linearly ordered scale U = {0, 1, . . . , n}, and μ is a mappingfrom Ω to the same scale U , and neg is the involutive order-reversing map on U = {0, 1, . . . , n} such that neg(0) = n andneg(n) = 0, where 0 and n are the bottom and the top elements of U . Namely, neg(n − k) = k. Thus, πd(ω) (resp. μ(ω))430L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436is all the greater as ω is more plausible (resp. satisfactory), 0 standing for the minimal level, and n for the maximal level.Moreover, πd and μ are assumed to be normalized, i.e. ∃ω ∈ Ω such that πd(ω) = 1, and similarly ∃ω(cid:9) ∈ Ω , μ(ω(cid:9)) = 1.E∗(d) is all the greater as all the states ω that have some plausibility according to πd are among the most preferredstates according to μ. E∗(d) is in fact a degree of inclusion of the fuzzy set of plausible states (when d is applied) into thefuzzy set of preferred states. The pessimistic utility E∗(d) is small as soon as there exists a possible consequence of d thatis both highly plausible and has a low satisfaction level with respect to preferences. This is clearly a risk-averse and thus apessimistic attitude.In [25], it has been shown that a stratified knowledge base has a possibility distribution as semantic counterpart. SeeAppendix for a refresher on possibilistic logic. Let Kd be the knowledge base built from the base K to which the decision dis added to the stratum Kn. Let πd be the possibility distribution associated with Kd, and μ be the possibility distributionassociated with the base G of goals. The normalization of πd and μ is equivalent to the non-emptiness of the highest strataKn and Gn. It has been shown in [23] that it is possible to compute E∗(d), as expressed by formula (1), by only using aclassical logic machinery on x-level cuts of the two bases Kd and G.Proposition 1. (See [23].) E∗(d) is the maximal value of x ∈ U s.t.(Kd)x (cid:19) (G)neg(x)(2)where (B)x (resp. (B)x) is the set of formulas of a base B that appear in the strata x, . . . , n (resp. in the strata x + 1, . . . , n). Mind that(B)x is a set of strata, while Bx is a stratum. By convention, E∗(d) = 0 if there is no such x.E∗(d) is equal to n (x = n) if the completely certain part of Kd entails the satisfaction of all the goals, even the oneswith low priorities.In the pessimistic view, as pointed out by Proposition 1, we are interested in finding a decision d (if it exists) such thatKx ∪ {d} (cid:19) G y with x high and y low, i.e. such that the decision d together with the most certain part of K entails thesatisfaction of the goals, even those with low priority (provided that those with higher priority are also satisfied).Example 9 (Surgery example cont.). The example is about having or not a surgery, knowing that the patient has colonicpolyps. The knowledge base is K = Kn ∪Kx, with Kn = {cp, sg → se, ¬sg → ¬se, sg → ¬ll, ca∧¬sg → ll}, and Kx = {cp → ca},(0 < x < n) where se: having side-effect, ca: cancer, ll: loss of life, sg: having a surgery, cp: having colonic polyps. The integerx < n refers to a lack of complete certainty.The goals base is G = Gn ∪ G y with Gn = {¬ll}, and G y = {¬se} (where 0 < y < n). We do not like to have side effectsafter a surgery, but it is more important to not lose life.The set of decisions is D = {sg, ¬sg}.Note that (Ksg)n (cid:19) (G)n. However, (Ksg)n (cid:3) (G) y . Note also that (Ksg)1 (cid:19) (G)n while (Ksg)1 (cid:3) (G) y . It is clear that (Ksg)n (cid:3)(G)neg(n) = (G)1. Thus, the only value that satisfies Proposition 1 is neg( y). Indeed, (Ksg)neg( y) (cid:19) (G) y . Consequently, E∗(sg) =neg( y).In its side, the option ¬sg violates the most important goal (¬ll). Thus, there is no value that satisfies Proposition 1.Consequently, E∗(¬sg) = 0.We are going to show that the result of Proposition 1 can be captured in terms of arguments pros, i.e., arguments oftype PP are underlying the pessimistic criterion. We first relate the absence of PP arguments to some consequences on thevalue of E∗. We then conversely relate the value of E∗ to the existence of some PP arguments. Lastly, we show that thecomparison of decisions in terms of criterion E∗ can also be handled directly in terms of PP arguments.Let us first recall the definition of the strength of an argument already used in the previous section (Definition 24).Definition 26 (Strength of an argument). Let δ = (cid:17)S, d, g(cid:18) ∈ Ap . The strength of δ is a pair (Lev(δ), Wei(δ)) s.t.• The certainty level of the argument is Lev(δ) = min{i | 1 (cid:3) i (cid:3) n such that S i (cid:13)= ∅}, where S i denotes S ∩ Ki . If S = ∅then Lev(δ) = n.• The weight of the argument is Wei(δ) = y s.t. g ∈ G y .The two following theorems state that the absence of strong PP argument can only weaken E∗(d).Theorem 4. Let d ∈ D. If ∃g ∈ Gk s.t. (cid:2)δ = (cid:17)S, d, g(cid:18) ∈ APP then E∗(d) (cid:3) neg(k).Proof. By reduction ab absurbo. Assume E∗(d) (cid:13)(cid:3) neg(k). Then E∗(d) (cid:2) neg(k) + 1. By Proposition 1, (Kd)neg(k)+1 (cid:19)(G)neg(neg(k)+1). But neg(neg(k) + 1) = k − 1. Thus, (Kd)neg(k)+1 (cid:19) (G)k. This clearly contradicts the hypothesis that ∃g ∈ Gks.t. (cid:2)δ = (cid:17)S, d, g(cid:18) ∈ APP. (cid:2)Theorem 5. Let d ∈ D. If ∀g ∈ G, (cid:2)δ = (cid:17)S, d, g(cid:18) ∈ APP with Lev(δ) > l and l (cid:2) 1, then E∗(d) (cid:3) l.L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436431Proof. By reduction ab absurbo also. Assume that E∗(d) (cid:13)(cid:3) l. Then E∗(d) (cid:2) l + 1. By Proposition 1, (Kd)(l+1) (cid:19) (G)neg(l+1).Since l (cid:2) 1, neg(l + 1) (cid:3) n − 2. Thus, (Kd)(l+1) (cid:19) (G)(n−1). This contradicts the hypothesis that ∀g, (cid:2)δ = (cid:17)S, d, g(cid:18) ∈ APP withLev(δ) > l, which means that ∀(cid:17)S, d, g(cid:18) ∈ APP, Lev(δ) (cid:3) l, since (G)(n−1) cannot be empty. (cid:2)The third theorem states that the value of E∗(d) is determined by the existence of sufficiently certain PP arguments infavor of decision d with respect to all important goals whose priority is above some value, and the absence of any morecertain PP argument in favor of decision d with respect to some goal whose priority may be smaller.Theorem 6. Let d ∈ D. If E∗(d) = x, then ∀g ∈ (G)neg(x)+1, ∃δ = (cid:17)S, d, g(cid:18) ∈ APP, and Lev(δ) (cid:2) x. Moreover, ∃g ∈ G s.t. (cid:2)δ =(cid:17)S, d, g(cid:18) ∈ APP with Lev(δ) (cid:2) x + 1 and Wei(δ) (cid:2) neg(x).Proof. Assume that E∗(d) = x. By Proposition 1, (Kd)x (cid:19) (G)neg(x). Thus, ∀g ∈ (G)neg(x)+1, ∃δ = (cid:17)S, d, g(cid:18) ∈ APP andLev(δ) (cid:2) x. Besides, (Kd)x+1 (cid:3) (G)neg(x+1), and then ∃g ∈ G s.t. (cid:2)δ = (cid:17)S, d, g(cid:18) ∈ APP with Lev(δ) > x and Wei(δ) (cid:2)neg(x + 1) + 1 = neg(x). (cid:2)However, as illustrated by the following example, E∗(d) = x does not necessarily mean that there does not exist asufficiently good PP argument for some goal in Gneg(x).Example 10. Let n = 4. Let G4 = {g1}, G3 = {g2}, G2 = {g3}. Assume (K)4 (cid:19) {g2, g3}, and (K)2 (cid:19) {g1}, but (K)3 (cid:13)(cid:19) {g1}. Then(K)2 (cid:19) (G)3, i.e. (K)2 (cid:19) (G)2, and then E∗(d) = 2. Note that here (K)2 (cid:19) (G)1, but (K)3 (cid:13)(cid:19) (G)1 (here neg(3) = 1), since themost important goal has only a rather weak proof when d takes place, namely (K)3 (cid:13)(cid:19) {g1}, although stronger proofs existfor less important goals: (K)2 (cid:19) (G)1, and thus E∗(d) (cid:13)= 3.The above results show the links between PP arguments supporting candidate decisions and the pessimistic values E∗assigned to those decisions. In what follows, we will show that an instantiation of our decision system returns the sameordering on the set D as the one obtained by comparing the pessimistic values of elements of D. As already said, thisamounts to choose the most appropriate decision principle. In the case of pessimistic decision making, the most suitableprinciple is the one proposed in Definition 7. Let us recall that principle:Definition 27. Let AF = (D, A, Def) be a decision system and Acc(AF) its accepted arguments. Let d1, d2 ∈ D.d1 (cid:14) d2iff ∃δ ∈ Fp(d1) ∩ Acc(AF) such that ∀δ(cid:9) ∈ Fp(d2) ∩ Acc(AF), δ (cid:2)p δ(cid:9).Note that this principle is based on a preference relation (cid:2)p between practical arguments. For our purpose, this relationprefers the argument that is based on a subset of K made of beliefs that are more certain and that together entail a goalhaving a higher priority. Formally, using the usual Pareto strict partial order between vectors:Definition 28 (Comparing arguments of type PP). Let δ, δ(cid:9) ∈ PP. δ is stronger than δ(cid:9)(Lev(δ), neg(Wei(δ))) >Pareto (Lev(δ(cid:9)), neg(Wei(δ(cid:9))))., denoted δ >p δ(cid:9),if and only ifLet us now relate the result of our decision system to that of pessimistic decision making.Theorem 7. Let T = (cid:17)D, K, G(cid:18) be a decision theory, and AF = (D, Ae ∪ Ap, Def = ∅) be the decision system. If ∀g ∈ G s.t. ∀δ(cid:9) =(cid:17)S, d2, g(cid:18) ∈ APP, ∃δ = (cid:17)S, then E∗(d1) (cid:2) E∗(d2).(cid:9), d1, g(cid:18) ∈ APP and δ >p8δ(cid:9)Proof. Assume 0 < E∗(d1) < E∗(d2). Then ∃x s.t. E∗(d1) = x. Then (Kd1 )x (cid:19) (G)neg(x) and (Kd1 )x+1 (cid:13)(cid:19) (G)neg(x+1), while(Kd2 )x+1 (cid:19) (G)neg(x+1). This contradicts the hypothesis. Indeed ∀δ ∈ APP s.t. Conc(δ) = d1, Lev(δ) < x + 1, but ∃δ(cid:9) ∈ APPs.t. Conc(δ) = d2 and Lev(δ) = x + 1. Assume 0 = E∗(d1) < E∗(d2). Then (Kd1 )0 (cid:13)(cid:19) (G)n, and (cid:2)δ ∈ APP s.t. Conc(δ) = d1. (cid:2)The converse of the above theorem is false as shown by the example below, where E∗(d1) > E∗(d2).Example 11. Let G = {g1, g2}, Gn = {g1}, Gn−1 = {g2}. Assume (Kd1 )n (cid:19) {g1}, (Kd1 )n−2 (cid:19) {g2} (but (Kd1 )n (cid:13)(cid:19) {g2} and(Kd1 )n−1 (cid:13)(cid:19) {g2}). Similarly, (Kd2 )n (cid:19) {g2}, (Kd2 )n−1 (cid:19) {g1}, and (Kd2 )n (cid:13)(cid:19) {g1}. We can take n = 3. Thus, E∗(d1) = 2 since(Kd1 )2 (cid:19) (G)2, while E∗(d2) = 1 since (Kd1 )1 (cid:19) (G)1. So, E∗(d1) > E∗(d2). Let δ2 = (cid:17)(K)3, d2, g2(cid:18) ∈ APP, so Lev(δ2) = 3 andneg(Wei(δ2)) = neg(2) = 1. But, regarding d1, (cid:2)δ1 ∈ APP s.t. Conc(δ1) = d1 and Lev(δ1) = 3 and neg(Wei(δ1)) = 2 = neg(1),i.e. s.t. δ1 >p δ2 (according to Definition 28). Indeed, the best arguments for d1 are δ1 = (cid:17)(K)3, d1, g1(cid:18) with Lev(δ1) = 3 andneg(Wei(δ1)) = 0, and δ(cid:9)= (cid:17)(K)1, d1, g2(cid:18) with Lev(δ(cid:9)1) = 2 and neg(Wei(δ(cid:9)1)) = 1.18 According to Definition 28.432L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436Going back to our running example, we have,Example 12 (Surgery example cont.). In the above example, there is an argument of type PP in favor of sg: δ = (cid:17){sg →¬ll}, sg, ¬ll(cid:18), and there is an argument of type PP in favor of ¬sg: δ(cid:9) = (cid:17){¬sg → ¬se}, ¬sg, ¬se(cid:18).is (cid:17)n, σ (cid:18). Thus, δ is preferred to δ(cid:9)(according to Definition 28).The strength of δ is (cid:17)n, n(cid:18), whereas the strength of δ(cid:9)Consequently, the decision sg is preferred to the decision ¬sg.(cid:9)The agreement between the pessimistic qualitative decision criterion and the argument-based view is due to a decom-posability property of arguments of type PP w.r.t. the conjunction of goals. Namely, Kx ∪ {d} (cid:19) g and Kx ∪ {d} (cid:19) gisequivalent to Kx ∪ {d} (cid:19) g ∧ g. Indeed, the pessimistic evaluation sanctions the fact that all the most important goals aresatisfied for sure up to a level where this is no longer true. However, things are not as simple with consistency since onemay have Kx ∪ {d} consistent with both g and g. This means thatthe absence of arguments of type PC is only a necessary condition for consistency w.r.t. the whole set of goals. Thus, theoptimistic criterion can only be approximated in terms of the evaluation of elementary practical arguments. Indeed, as itwill be recalled in the next section the optimistic evaluation refers to the fact that the most certain part of K, and the mostimportant goals are consistent together in presence of a candidate decision.separately without having it consistent with g ∧ g(cid:9)(cid:9)(cid:9)5.2. Optimistic criterionThe optimistic qualitative criterion [27] is given by∗E(d) = maxωmin(cid:7)(cid:6)μ(ω), πd(ω).(3)It is a consistency evaluation since it amounts to estimate to what extent the intersection of the fuzzy set of good states∗(d) corresponds(in the sense of μ) with the fuzzy set of plausible states (when d is applied) is not empty. The criterion Eto an optimistic attitude since it is high as soon as there exists a possible consequence of d that is both highly plausible∗(d) is equal to n (is maximal) as soon as one fully acceptable choice ω (i.e., such that μ(ω) = n) isand highly prized. Ealso completely plausible. As for the pessimistic case, the optimistic utility can be expressed in logical terms.Proposition 2. (See [23].) E∗(d) is equal to the greatest x ∈ U such that (Kd)neg(x) and (G)neg(x) are logically consistent together.The above proposition means that in the optimistic point of view, we are interested in finding a decision d (if it exists)which is consistent with the knowledge base and the goals (i.e. K ∧ {d} ∧ G (cid:13)= ⊥). This is optimistic in the sense that itassumes that goals may be attained as soon as their negation cannot be proved.Example 13 (Surgery example cont.). In this example, Eoptimistic case depends on the values x and y.∗(sg) = neg( y) and E∗(¬sg) = neg(x). Thus the best decision in theIn order to capture the result of Proposition 2, arguments of type PC are needed. The strength of such arguments isgiven using Definition 24.Theorem 8. Let d ∈ D. If ∃δ ∈ APC s.t. Conc(δ) = d, then E∗(d) (cid:3) max(neg(Lev(δ)), neg(Wei(δ))).∗(d) = maxωmin(πKd (ω), μG (ω)) = max[maxω:ω|(cid:22)(Kd)β min(πKd (ω), μG (ω)), maxω:ω(cid:13)|(cid:22)(Kd)β min(πKd (ω), μG (ω))].Proof. ELet δ = (cid:17)S, d, g(cid:18) be a PC argument with Lev(δ) = x and Wei(δ) = y. Then, Kx ∪ {d} (cid:19) ¬g. Thus, ∀ω, ω |(cid:22) (Kd)x, thenπKd (ω) (cid:3) neg(x). Thus, E∗(d) (cid:3) max[min(n, neg( y)), min(neg(x), n)] = max(neg( y), neg(x)). (cid:2)Conversely, we have the following theoremTheorem 9. Let d ∈ D. If E∗(d) = x, then there is a PCargument δ = (cid:17)S, d, g(cid:18) for d such that Wei(δ) (cid:3) neg(x).∗(d) = x, then x is the maximal value such as (Kd)neg(x) ∪ (G)neg(x) is consistent, from Proposition 2. This entailsProof. If Ethat (Kd)neg(x(cid:9)) (cid:19) ¬Gneg(x(cid:9)) for neg(x) (cid:2) neg(x(cid:9)).Example 14 (Surgery example cont.). In the above example, there is one strong argument against the decision ‘sg’: (cid:17){sg →se}, sg, ¬se(cid:18). There is also a unique strong argument against the decision ¬sg: (cid:17){cp, cp → ca, ca ∧ ¬sg → ll}, ¬sg, ¬ll(cid:18).The level of the argument (cid:17){sg → se}, sg, ¬se(cid:18) is n whereas its weight is y. Concerning the argument (cid:17){cp, cp → ca, ca ∧¬sg → ll}, ¬sg, ¬ll(cid:18), its level is x, and its weight is n.In this example, the comparison of the two arguments amounts to compare x with y. Namely, if y (the priority of thegoal “no side effect”) is small then the best decision will be to have a surgery. If the certainty degree x of having cancer inL. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436433presence of colonic polyps for the particular patient is small enough then the best optimistic decision will not be to have asurgery.In order to retrieve the exact value of E∗(d) as weight of an argument, we would have to use a non-elementary notionof arguments, described in [6], that considers as a whole the goals base G.6. Related worksDifferent works have combined the ideas of argumentation and decision in artificial intelligence systems. In particular,Fox and Parsons [31] have developed an inference-based decision support machinery, which have been implemented inmedical applications (see e.g. Fox and Das in [30]). In this approach the knowledge base is made of recommendation rulesthat conclude on candidate decisions. However, in [31,32], no explicit distinction is made between knowledge and goals.However, in their examples, values (belonging to a linearly ordered scale) are assigned to formulas which represent goals.These values provide an empirical basis for comparing arguments using a symbolic combination of strengths of beliefs andgoals values. This symbolic combination is performed through dictionaries corresponding to different kinds of scales thatmay be used. Only one type of arguments in favor of or against is used. Another recent example of argument-based decisionsystem that is purely based on an inference system is proposed by Chesnevar et al. in [20] for advising about language usageassessment on the basis of corpus available on the web.We now survey works that handle classical multiple decision or decision making under uncertainty problems in anargumentative manner. This means that recommended decisions have to be found or explained from user’s preferences andinformation about the current state of the world. Moreover, no “pre-compiled” rules that explicitly recommend decisions ina given situation are supposed to be available in these works.In [14], Bonet and Geffner have also proposed an original approach to qualitative decision, inspired from Tan and Pearl[47], based on “action rules” that link a situation and an action with the satisfaction of a positive or a negative goal. Howeverin contrast with the previous work and the work presented in this paper, this approach does not refer to any model inargumentative inference. In their framework, there are four parts:(1) a set D of actions or decisions.(2) a set I of input propositions defining the possible input situation. A degree of plausibility is associated with each input.Thus, I = {(ki, αi)} with αi ∈ {likely, plausible, unlikely}.. G+(3) a set G of prioritized goals such that G = G+ ∪ G−gathers the positive goals that one wants to achieve and G−gathers the negative goals that one wants to avoid. Thus, G = {(gi, βi)} with βi ∈ [0, 1, . . . , N].Note that in our framework what they call here negative goals are considered in our goal base as negative literals.(4) a set of action rules AR = {( Ai ∧ Ci ⇒ xi, λi), λi (cid:2) 0}, where Ai is an action, Ci is a conjunction of input literals, and xiis a goal. Each action rule has two measures: a priority degree which is exactly the priority degree of the goal xi , and aplausibility degree. This plausibility is defined as follows: A rule A ∧ C ⇒ x is likely if any conjunct of C is likely. A ruleA ∧ C ⇒ x is unlikely if some conjunct of C is unlikely. A rule A ∧ C ⇒ x is plausible if it is neither likely nor unlikely.In this approach only input propositions are weighted in terms of plausibility. Action rules inherit these weights through thethree above rules in a rather empirical manner which depends on the chosen plausibility scale. The action rules themselvesare not weighted since they are potentially understood as defeasible rules, although no non-monotonic reasoning system isassociated with them.In contrast, our approach makes use of an abstract scale. Moreover, weighted possibilistic clauses have been shown tobe able to properly handle non-monotonic inference in the sense of Kraus, Lehmann and Magidor [36]’ preferential systemaugmented with rational monotony. So a part of our weighted knowledge may be viewed as the encoding of a set of defaultrules. From the above four bases, reasons are constructed for (against) actions in [14]. Indeed, goals provide reasons for(or against) actions. Positive goals provide reasons for actions, whereas negative goals provide reasons against actions. Thebasic idea behind this distinction is that negative goals should be discarded, and consequently any action which may leadto the satisfaction of such goals should be avoided. However, the approach makes no distinction between what we callpessimism and optimism. The definition of a ‘reason’ in [14] only involves facts. Finally, in Bonet and Geffner’s framework,decisions which satisfy the most important goals are privileged. This is also true in our approach, but the comparisonbetween decisions can be further refined, in case of several decisions yielding to the satisfaction of the most importantgoals, by taking into account the other goals which are not violated by these decisions.Amgoud and Prade in [6] have already proposed an argumentation-based reading of possibilistic decision criteria. How-ever, their approach has some drawbacks from a pure argumentation point of view. In their approach, there was only onetype of arguments pros and one type of arguments cons. Moreover, these arguments were taking into account the goal baseas a whole, and a consequence for a given decision there was at most a unique argument pros and a unique argumentcons. This does not really fit with the way human are discussing decisions, for which there are usually several argumentspros and cons, rather than a summarized one. On the contrary in this paper, we have discussed all the possible types ofarguments pros and cons in a systematic way, and each argument pertains to only one goal.434L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436Dubois and Fargier [24] have studied a framework where a candidate decision d is associated with two distinct setsof positive arguments (pros) and negative arguments (cons). It is assumed that positiveness and negativeness are not amatter of degree. If one considers that the arguments refers to criteria, this means that an implicit scale {−, 0, +} wouldbe used for evaluating a candidate decision according to each criterion. Moreover, there is no uncertainty. However, afunction π assesses the level of importance of each argument for the decision maker. Importance ranges on a totallyordered scale from “no important at all” to a maximal level of importance. If π (x) > π ( y), “the strength of x is consideredat least one order of magnitude higher than the one of y, so that y is negligible in front of x”. The authors provide anaxiomatic characterization of different natural rules in this setting, with a possibility theory interpretation of their meaning.In particular, a “bipolar lexicographic” preference relation (which is among the decision principles discussed in Section 6) ischaracterized. It amounts to compare two candidate decisions d and dby comparing the difference of the cardinalities of thesets of positive and negative arguments they have (thus allowing for cancellation between positive and negative arguments),starting with the highest level of importance; in case of equality at a given level, the level immediately below is consideredto solve the ties and so on. In direct relation to this work, an empirical study of the different decision rules considered(Bonnefon and Fargier [15]) has shown that the bipolar lexicographic rule is largely favored by humans in practice.(cid:9)Another trend of works relating argumentation and decision is mainly interested in the use of arguments for explainingand justifying multiple criteria decisions once they have been made using some definite aggregation function. A systematicstudy for different aggregation functions can be found in [38,39]. The implemented system developed by Carenini and Moore[18] is an example of such a use for an aggregation process based on weighted sums associated to value trees.7. ConclusionThe paper has proposed an abstract argumentation-based framework for decision making. The main idea behind thiswork is how to define a complete preorder on a set of candidate decisions on the basis of arguments. The frameworkdistinguishes between two types of arguments: epistemic arguments that support beliefs and practical arguments thatjustify candidate decisions. Each practical argument concerns only one candidate decision, and may be either in favor ofthat decision or against it. The framework follows two main steps:(1) An inference step in which arguments are evaluated using acceptability semantics. This step amounts to return amongthe practical arguments, those which are warranted in the current state of information, i.e. the “accepted” arguments.(2) A pure decision step in which candidate decisions are compared on the basis of accepted practical arguments.For the second step of the process, we have proposed three families of principles for comparing pairs of choices. Anaxiomatic study and a cognitive validation of these principles are worth developing, in particular in connection with [15,24].The abstract framework is then instantiated in order to handle decision under uncertainty and multiple criteria decisionmaking. For that purposes, the framework emphasizes clearly the bipolar nature of the consequences of choices by distin-guishing goals to be pursued from rejections to be avoided. These bipolar preferences are encoded by two sets of stratifiedformulas stating goals and rejections with their level of importance. In addition, the knowledge about the current state ofthe world in encoded in another stratified base which may be inconsistent. The bipolar nature of the setting has led us toidentify two types of arguments pro a choice (resp. against a choice).The proposed approach is very general and includes as particular cases already studied argumentation-based decisionsystems. Moreover it is suitable for multiple criteria decision making as well as decision making under uncertainty. Inparticular, the approach has been shown to agree with qualitative decision making under uncertainty, and to distinguishbetween a pessimistic and an optimistic attitude of the decision maker.Although our model is quite general, it may be still worth extending along different lines. First, the use of default knowl-edge could be developed. Second, our approach does not take into account rules that recommend or disqualify decisionsin given contexts. Such rules should incorporate modalities for distinguishing between strong and weak recommendations.Moreover, they are fired by classical argumentative inference. This contrasts with our approach where the only argumentspertaining to decisions have an abductive structure. Recommendation rules may also turn to be inconsistent with otherpieces of knowledge in practical arguments pros or cons w.r.t. a decision. Lastly, agents may base their decision on two typesof information, namely generic knowledge and a repertory of concrete reported cases. Then, past observations recorded inthe repertory may be the basis of a new form of arguments by exemplification of cases where a decision has succeeded orfailed. This would amount to relate argumentation and case-based decision.AcknowledgementThe authors would like to thank the two anonymous referees for their very constructive reviews that helped us toimprove this work.Appendix. Brief refresher on possibility logicA possibilistic logic base K can be viewed as a stratified set of classical logical formulas, such that K = K1 ∪ · · · ∪ Ki ∪· · · ∪ Kn, with ∀i, j, Ki ∩ K j = ∅. It is assumed that formulas in Ki are associated with a higher level of certainty or priorityL. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436435than formulas in Ki−1. Thus, Kn contains the formulas with the highest level, and K1 the ones with the smallest non-zerolevel.Let ρ be a function that returns the rank of stratum to which a formula belongs, i.e. ρ(k j) = i such that k j ∈ Ki .In the following, ρ(k j) will be denoted for short ρ j . Thus, K can be rewritten as K = {(k j, ρ j); j = 1, l}, as often done inpossibilistic logic [25]. K may represent the available knowledge about the world, or goals having different levels of priority.The pair (k j, ρ j) is understood as N(k j) (cid:2) ρ j , where N is a necessity measure obeying the characteristic decomposabilityaxiom N(p ∧ q) = min(N(p), N(q)). Namely (k j, ρ j) encodes that the piece of knowledge “k j is true” is certain or prioritizedat least at level ρ j , where ρ j belongs to a linearly ordered valuation scale whose top and bottom elements are resp. nand 1. At the semantic level, a possibilistic base K is associated with a possibility distribution defined by(cid:6)πK(ω) = min j=1,lmax(cid:7)vω(k j), neg(ρ j),where neg is the order reversing map of the scale (0, 1, . . . , n), and where vω(k j) = n if ω is a model of k j and vω(k j) = 0if ω falsifies k j . An interpretation ω is thus all the less plausible or satisfactory, as it falsified a proposition k j associatedwith a high level ρ j . It rank-orders the more or less plausible states of the world.The equivalence between a possibilistic logic base and its possibility distribution-based semantic counter-part has beenestablished in terms of correction and completeness of the inference mechanism that is associated with these represen-tations [25]. This inference mechanism is governed at the syntactic level by the resolution rule (¬p ∨ q, ρ), (p ∨ r, λ) (cid:19)(q ∨ r, min(ρ, λ)).References[1] T. Alsinet, C. Chesnevar, L. Godo, S. Sandri, Modeling defeasible argumentation within a possibilistic logic framework with fuzzy unification, in: Pro-ceedings of 11th International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU’06), 2006,pp. 1228–1235.[2] L. Amgoud, J.-F. Bonnefon, H. Prade, An argumentation-based approach to multiple criteria decision, in: Proceedings of the 8th European Conferenceon Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU’05), 2005, pp. 269–280.[3] L. Amgoud, C. Cayrol, Inferring from inconsistency in preference-based argumentation frameworks, International Journal of Automated Reasoning 29 (2)(2002) 125–169.[4] L. Amgoud, C. Cayrol, A reasoning model based on the production of acceptable arguments, Annals of Mathematics and Artificial Intelligence 34 (2002)197–216.[5] L. Amgoud, N. Maudet, S. Parsons, Modelling dialogues using argumentation, in: Proceedings of the International Conference on Multi-Agent Systems(ICMAS’00), 2000, pp. 31–38.[6] L. Amgoud, H. Prade, Using arguments for making decisions: A possibilistic logic approach, in: Proceedings of the 20th Conference on Uncertainty inArtificial Intelligence (UAI’04), 2004, pp. 10–17.[7] L. Amgoud, H. Prade, A bipolar argumentation-based decision framework, in: Proceedings of the 11th International Conference on Information Process-ing and Management of Uncertainty in Knowledge-Based Systems (IPMU’06), 2006, pp. 323–330.[8] L. Amgoud, H. Prade, Comparing decisions in an argumentation-based setting, in: Proceedings of the 11th International Workshop on Non-MonotonicReasoning (NMR’06), 2006.[9] L. Amgoud, H. Prade, Explaining qualitative decision under uncertainty by argumentation, in: Proceedings of the 21st National Conference on ArtificialIntelligence (AAAI’06), 2006, pp. 219–224.[10] K. Atkinson, Value-based argumentation for democratic decision support, in: Proceedings of the First International Conference on Computational Mod-els of Natural Argument (COMMA’06), 2006, pp. 47–58.[11] K. Atkinson, T. Bench-Capon, P. McBurney, Justifying practical reasoning, in: Proceedings of the Fourth Workshop on Computational Models of NaturalArgument (CMNA’04), 2004, pp. 87–90.[12] P. Baroni, M. Giacomin, G. Guida, Scc-recursiveness: a general schema for argumentation semantics, Artificial Intelligence 168 (1–2) (2005) 162–210.[13] Ph. Besnard, A. Hunter, A logic-based theory of deductive arguments, Artificial Intelligence 128 (2001) 203–235.[14] B. Bonet, H. Geffner, Arguing for decisions: A qualitative model of decision making, in: Proceedings of the 12th Conference on Uncertainty in ArtificialIntelligence (UAI’96), 1996, pp. 98–105.[15] J.-F. Bonnefon, H. Fargier, Comparing sets of positive and negative arguments: Empirical assessment of seven qualitative rules, in: Proceedings of the17th European Conference on Artificial Intelligence (ECAI’06), 2006, pp. 16–20.[16] M. Bratman, Intentions, Plans, and Practical Reason, Harvard University Press, MA, 1987.[17] J.T. Cacioppo, W.L. Gardner, G.G. Bernston, Beyond bipolar conceptualizations and measures: The case of attitudes and evaluative space, Personality andSocial Psychology Review 1 (1997) 3–25.[18] G. Carenini, J.D. Moore, Generating and evaluating evaluative arguments, Artificial Intelligence 170 (2006) 925–952.[19] C. Cayrol, V. Royer, C. Saurel, Management of preferences in assumption-based reasoning, in: Lecture Notes in Computer Science, vol. 682, 1993, pp.13–22.[20] C. Chesnevar, A. Maguitman, M. Sabaté, An argument-based decision support system for assessing natural language usage on the basis of the webcorpus, International Journal of Intelligent Systems (2006), ISSN 0884-8173.[21] T. Dean, L. Kaelbling, J. Kirman, A. Nicholson, Planning under time constraints in stochastic domains, Artificial Intelligence 76 (1–2) (1995) 35–74.[22] J. Doyle, R. Thomason, Background to qualitative decision theory, Artificial Intelligence Magazine 20 (2) (1999) 55–68.[23] D. Dubois, D. Le Berre, H. Prade, R. Sabbadin, Using possibilistic logic for modeling qualitative decision: ATMS-based algorithms, Fundamenta Informat-icae 37 (1999) 1–30.[24] D. Dubois, H. Fargier, Qualitative decision making with bipolar information, in: Proceedings of the 10th International Conference on Principles ofKnowledge Representation and Reasoning (KR’06), 2006, pp. 175–186.[25] D. Dubois, J. Lang, H. Prade, Possibilistic logic, in: Handbook of Logic in Artificial Intelligence and Logic Programming, vol. 3, 1994, pp. 439–513.[26] D. Dubois, H. Prade, Weighted minimum and maximum operations, an addendum to a review of fuzzy set aggregation connectives, InformationSciences 39 (1986) 205–210.[27] D. Dubois, H. Prade, Possibility theory as a basis for qualitative decision theory, in: Proceedings of 14th International Joint Conference on ArtificialIntelligence (IJCAI’95), 1995, pp. 1924–1930.436L. Amgoud, H. Prade / Artificial Intelligence 173 (2009) 413–436[28] P.M. Dung, On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games, ArtificialIntelligence 77 (1995) 321–357.[29] M. Elvang-Goransson, J. Fox, P. Krause, Dialectic reasoning with inconsistent information, in: Proceedings of 9th Conference on Uncertainty in ArtificialIntelligence (UAI’93), 1993, pp. 114–121.[30] J. Fox, S. Das, Safe and Sound. Artificial Intelligence in Hazardous Applications, AAAI Press, The MIT Press, 2000.[31] J. Fox, S. Parsons, On using arguments for reasoning about actions and values, in: Proceedings of the AAAI Spring Symposium on Qualitative Preferencesin Deliberation and Practical Reasoning, Stanford, 1997.[32] J. Fox, S. Parsons, Arguing about beliefs and actions, in: Applications of Uncertainty Formalisms, 1998, pp. 266–302.[33] B. Franklin, Letter to J.B. Priestley, 1772, in: J. Bigelow (Ed.), The Complete Works, Putnam, New York, 1887, p. 522.[34] P.H. Giang, P. Shenoy, Two axiomatic approaches to decision making using possibility theory, European Journal of Operational Research 162 (2) (2004)450–467.[35] R. Girle, D. Hitchcock, P. McBurney, B. Verheij, Decision support for practical reasoning, in: C. Reed, T. Norman (Eds.), Argumentation Machines: NewFrontiers in Argument and Computation, in: Argumentation Library, Kluwer Academic, Dordrecht, The Netherlands, 2003.[36] S. Kraus, D. Lehmann, M. Magidor, Nonmonotonic reasoning, preferential models and cumulative logics, Artificial Intelligence 44 (1990) 167–207.[37] S. Kraus, K. Sycara, A. Evenchik, Reaching agreements through argumentation: a logical model and implementation, Artificial Intelligence 104 (1998)1–69.[38] Ch. Labreuche, Argumentation of the results of a multi-criteria evaluation model in individual and group decision aiding, in: Proceedings of the 4thConference of the European Society for Fuzzy Logic and Technology (Eusflat’05), 2005, pp. 482–487.[39] Ch. Labreuche, Argumentation of the decision made by several aggregation operators based on weights, in: Proceedings of the 11th InternationalConference on Information Processing and Management of Uncertainty in Knowledge-Based Systems (IPMU’06), 2006, pp. 683–690.[40] J. Pearl, Probabilistic Reasoning in Intelligent Systems, Morgan-Kaufmann, 1988.[41] J. Pollock, The logical foundations of goal-regression planning in autonomous agents, Artificial Intelligence 106 (2) (1998) 267–334.[42] J. Pollock, Rational cognition in OSCAR, in: Proceedings of the International Workshop on Agent Theories and Languages (ATAL’99), 1999.[43] J. Raz, Practical Reasoning, Oxford University Press, Oxford, 1978.[44] C. Reed, T. Norman (Eds.), Argumentation Machines: New Frontiers in Argument and Computation, Argumentation Library, Kluwer Academic, Dordrecht,The Netherlands, 2003.[45] L.J. Savage, The Foundations of Statistics, Dover, New York, 1954. Reprinted by Dover, 1972.[46] G.R. Simari, R.P. Loui, A mathematical treatment of defeasible reasoning and its implementation, Artificial Intelligence and Law 53 (1992) 125–157.[47] S.W. Tan, J. Pearl, Qualitative decision theory, in: Proceedings of the 11th National Conference on Artificial Intelligence (AAAI’94), 1994, pp. 928–933.[48] D. Walton, Argument Schemes for Presumptive Reasoning, Lawrence Erlbaum Associates, Mahwah, NJ, 1996.[49] M.J. Wooldridge, Reasoning about Rational Agents, MIT Press, Cambridge, MA, 2000.