9102luJ71]VC.sc[2v45280.4091:viXraUSE-Net: incorporating Squeeze-and-Excitation blocksinto U-Net for prostate zonal segmentationof multi-institutional MRI datasetsLeonardo Rundo†,a,b,c,d,∗, Changhee Han†,e, Yudai Naganoe, Jin Zhange,Ryuichiro Hatayae, Carmelo Militellob, Andrea Tangherlonia,f,g,Marco S. Nobilea,h, Claudio Ferrettia, Daniela Besozzia,Maria Carla Gilardib, Salvatore Vitabilei, Giancarlo Mauria,h,Hideki Nakayamae, Paolo Cazzanigaj,haDepartment of Informatics, Systems and Communication, University of Milano-Bicocca,20126 Milan, ItalybInstitute of Molecular Bioimaging and Physiology, Italian National Research Council,90015 Cefal´u (PA), ItalycDepartment of Radiology, University of Cambridge, CB2 0QQ Cambridge, UKdCancer Research UK Cambridge Centre, CB2 0RE Cambridge, UKeGraduate School of Information Science and Technology, The University of Tokyo,113-8656 Tokyo, JapanfDepartment of Haematology, University of Cambridge, CB2 0XY Cambridge, UKgWellcome Trust Sanger Institute, Wellcome Trust Genome Campus,CB10 1SA Hinxton, UKhSYSBIO.IT Centre of Systems Biology, 20126 Milan, ItalyiDepartment of Biomedicine, Neuroscience and Advanced Diagnostics,University of Palermo, 90127 Palermo, ItalyjDepartment of Human and Social Sciences, University of Bergamo,24129 Bergamo, ItalyAbstractProstate cancer is the most common malignant tumors in men but prostateMagnetic Resonance Imaging (MRI) analysis remains challenging. Besides∗Corresponding author.E-mail address: leonardo.rundo@disco.unimib.it; lr495@cam.ac.uk (L. Rundo)†These authors contributed equally.Preprint submitted to NeurocomputingJuly 18, 2019   whole prostate gland segmentation, the capability to differentiate betweenthe blurry boundary of the Central Gland (CG) and Peripheral Zone (PZ)can lead to differential diagnosis, since the frequency and severity of tu-mors differ in these regions. To tackle the prostate zonal segmentation task,we propose a novel Convolutional Neural Network (CNN), called USE-Net,which incorporates Squeeze-and-Excitation (SE) blocks into U-Net, i.e., oneof the most effective CNNs in biomedical image segmentation. Especially, theSE blocks are added after every Encoder (Enc USE-Net) or Encoder-Decoderblock (Enc-Dec USE-Net). This study evaluates the generalization ability ofCNN-based architectures on three T2-weighted MRI datasets, each one con-sisting of a different number of patients and heterogeneous image character-istics, collected by different institutions. The following mixed scheme is usedfor training/testing: (i ) training on either each individual dataset or multipleprostate MRI datasets and (ii ) testing on all three datasets with all possibletraining/testing combinations. USE-Net is compared against three state-of-the-art CNN-based architectures (i.e., U-Net, pix2pix, and Mixed-ScaleDense Network), along with a semi-automatic continuous max-flow model.The results show that training on the union of the datasets generally out-performs training on each dataset separately, allowing for both intra-/cross-dataset generalization. Enc USE-Net shows good overall generalization un-der any training condition, while Enc-Dec USE-Net remarkably outperformsthe other methods when trained on all datasets. These findings reveal thatthe SE blocks’ adaptive feature recalibration provides excellent cross-datasetgeneralization when testing is performed on samples of the datasets used dur-ing training. Therefore, we should consider multi-dataset training and SE2blocks together as mutually indispensable methods to draw out each other’sfull potential. In conclusion, adaptive mechanisms (e.g., feature recalibra-tion) may be a valuable solution in medical imaging applications involvingmulti-institutional settings.Keywords: Prostate zonal segmentation, Prostate cancer, AnatomicalMRI, Convolutional neural networks, USE-Net, Cross-dataset generalization1. IntroductionAccording to the American Cancer Society, in 2019 the Prostate Cancer(PCa) is expected to be the most common malignant tumor with the secondhighest mortality for American males [1]. Given a clinical context, severalimaging modalities can be used for PCa diagnosis, such as Transrectal Ul-trasound (TRUS), Computed Tomography (CT), and Magnetic ResonanceImaging (MRI). For an in-depth investigation, structural T1-weighted (T1w)and T2-weighted (T2w) MRI sequences can be combined with the functionalinformation from Dynamic Contrast Enhanced MRI (DCE-MRI), DiffusionWeighted Imaging (DWI), and Magnetic Resonance Spectroscopic Imaging(MRSI) [2]. Recent advancements in MRI scanners, especially those relatedto magnetic field strengths higher than 1.5T, did not decrease the effect ofmagnetic susceptibility artifacts on prostate MR images, even though theshift from 1.5T to 3T theoretically leads to a doubled Signal-to-Noise Ratio(SNR) [3]. However, 3T MRI scanners permitted to obtain high-quality im-ages with less invasive procedures compared with 1.5T, thanks to a pelviccoil that reduces prostate gland compression/deformation [4, 5].Therefore, MRI plays a decisive role in PCa diagnosis and disease mon-3itoring (even in an advanced status [6]), revealing the internal prostaticanatomy, prostatic margins, and PCa extent [7]. According to the zonalcompartment system proposed by McNeal, the prostate Whole Gland (WG)can be partitioned into the Central Gland (CG) and Peripheral Zone (PZ) [8].In prostate imaging, T2w MRI serves as the principal sequence [9], thanksto its high resolution that enables to differentiate the hyper-intense PZ andhypo-intense CG in young male subjects [10].Besides manual detection/delineation of the WG and PCa on MR im-ages, distinguishing between the CG and PZ is clinically essential, sincethe frequency and severity of tumors differ in these regions [11, 12]. Asa matter of fact, the PZ harbors 70-80% of PCa and represents a targetfor prostate biopsy [13]. Furthermore, the PZ volume ratio (i.e., the PZvolume divided by the WG volume) can be considered for PCa diagnosticrefinement [14], while the CG volume ratio can help monitoring prostate hy-perplasia [15]. Therefore, according to the Prostate Imaging-Reporting andData System version 2 (PI-RADSTM v2) [16], radiologists must perform azonal partitioning before assessing the suspicion of PCa on multi-parametricMRI. However, an improved PCa diagnosis requires a reliable and automaticzonal segmentation method, since manual delineation is time-consuming andoperator-dependent [17, 18]. Moreover, in clinical practice, the generaliza-tion ability among multi-institutional prostate MRI datasets is essential dueto large anatomical inter-subject variability and the lack of a standardizedpixel intensity representation for MRI (such as for CT-based radiodensitymeasurements expressed in Hounsfield units) [19]. Hence, we aim at au-tomatically segmenting the prostate zones on three multi-institutional T2w4MRI datasets to evaluate the generalization ability of Convolutional NeuralNetwork (CNN)-based architectures. This task is challenging because imagesfrom multi-institutional datasets are characterized by different contrasts, vi-sual consistencies, and heterogeneous characteristics [20].In this work, we propose a novel CNN, called USE-Net, which incorpo-rates Squeeze-and-Excitation (SE) blocks [21] into U-Net after every Encoder(Enc USE-Net) or Encoder-Decoder block (Enc-Dec USE-Net). The ratio-nale behind the design of USE-Net is to exploit adaptive channel-wise featurerecalibration to boost the generalization performance. The proposed USE-Net is conceived to outperform the state-of-the-art CNN-based architecturesfor segmentation in multi-institutional studies, whilst the SE blocks (ini-tially proposed in [21]) were originally designed to boost the performanceonly for classification and object detection via feature recalibration, by cap-turing single dataset characteristics. Unlike the original SE blocks placedin InceptionNet [22] and ResNet [23] architectures, we introduced them intoU-Net after the encoders and decoders to boost the segmentation perfor-mance with increased generalization ability, thanks to the representation ofchannel-wise relationships in multi-institutional clinical scenarios, analyzingmultiple heterogeneous MRI datasets. This study adopted a mixed schemefor cross- and intra-dataset generalization: (i) training on either each in-dividual dataset or multiple datasets, and (ii ) testing on all three datasetswith all possible training/testing combinations. To the best of our knowl-edge, this is the first CNN-based prostate zonal segmentation on T2w MRIalone. By relying on both spatial overlap-/distance-based metrics, we com-pared USE-Net against three CNN-based architectures: U-Net, pix2pix, and5Mixed-Scale Dense Network (MS-D Net) [24], along with a semi-automaticcontinuous max-flow model [25].Contributions. Our main contributions are:• Prostate zonal segmentation: our novel Enc-Dec USE-Net achievesaccurate CG and PZ segmentation results on T2w MR images, remark-ably outperforming the other competitor methods when trained on alldatasets used for testing in multi-institutional scenarios.• Cross-dataset generalization: this first cross-dataset study, inves-tigating all possible training/testing conditions among three differentmedical imaging datasets, shows that training on the union of multipledatasets generally outperforms training on each dataset during testing,realizing both intra-/cross-dataset generalization—thus, we may trainCNNs by feeding samples from multiple different datasets for improvingthe performance.• Deep Learning for medical imaging: this research reveals thatSE blocks provide excellent intra-dataset generalization in multi-insti-tutional scenarios, when testing is performed on samples from thedatasets used during training. Therefore, adaptive mechanisms (e.g.,feature recalibration in CNNs) may be a valuable solution in medicalimaging applications involving multi-institutional settings.The manuscript is structured as follows. Section 2 outlines the back-ground of prostate MRI zonal segmentation, especially related work on CNNs.6Section 3 describes the analyzed multi-institutional MRI datasets, the pro-posed USE-Net architectures, the investigated state-of-the-art CNN- andmax-flow-based segmentation approaches, as well as the employed evaluationmetrics; the experimental results are presented and discussed in Section 4.Finally, conclusive remarks and future directions of this work are given inSection 5.2. Related WorkDue to the crucial role of MR image analysis in PCa diagnosis andstaging [2], researchers have paid specific attention to automatic WG de-tection/segmentation. Classic methods mainly leveraged atlases [19, 26] orstatistical shape priors [27]: atlas-based approaches realized accurate segmen-tation when new prostate instances resemble the atlas, relying on a non-rigidregistration algorithm [27, 28]. Unsupervised clustering techniques allowedfor segmentation without manual labeling of large-scale MRI datasets [17, 29].In the latest years, Deep Learning techniques [30] have achieved accurateprostate segmentation results by using deep feature learning combined withshape models [31] or location-prior maps [32]. Moreover, CNNs were usedwith patch-based ensemble learning [33] or dense prediction schemes [34]. Inaddition, end-to-end deep neural networks achieved outstanding results inautomated PCa detection in multi-parametric MRI [35, 36].Differently from WG segmentation and PCa detection, less attention hasbeen paid to CG and PZ segmentation despite its clinical importance in PCadiagnosis [12]. In this context, classic Computer Vision techniques have beenmainly exploited on T2w MRI. For instance, early studies combined classi-7fiers with statistical shape models [37] or deformable models [38]; Toth etal. [28] employed active appearance models with multiple level sets for si-multaneous zonal segmentation; Qiu et al. [25] used a continuous max-flowmodel—the dual formulation of convex relaxed optimization with region con-sistency constraints [39]; in contrast, Makni et al. [40] fused and processed3D T2w, DWI, and contrast-enhanced T1w MR images by means of an ev-idential C-means algorithm [41]. As the first CNN-based method, Clark etal. [42] detected DWI MR images with prostate relying on Visual GeometryGroup (VGG) net [43], and then sequentially segmented WG and CG usingU-Net [44].Regarding the most recent computational methods in medical image seg-mentation, along with traditional Pattern Recognition techniques [45], sig-nificant advances have been proposed in CNN-based architectures. For in-stance, to overcome the limitations related to accurate image annotations,DeepCut [46] relies on weak bounding box labeling [47]. This method aims atlearning features for a CNN-based classifier from bounding box annotations.Among the architectures devised for biomedical image segmentation [48, 49],U-Net [44] showed to be a noticeably successful solution, thanks to the com-bination of a contracting (i.e., encoding) path, for coarse-grained contextdetection, and a symmetric expanding (i.e., decoding) path, for fine-grainedlocalization. This fully CNN is capable of stable training with reduced sam-ples. The authors of V-Net [34] extended U-Net for volumetric medical im-age segmentation, by introducing also a different loss function based on theDice Similarity Coefficient (DSC ). Schlemper et al. [50] presented an At-tention Gate (AG) model for medical imaging, which aims at focusing on8target structures or organs. AGs were introduced into the standard U-Net,so defining Attention U-Net, which achieved high performance in multi-classimage segmentation without relying on multi-stage cascaded CNNs. RecentlyMS-D Net [24] was shown to yield better segmentation results in biomedi-cal images than U-Net [44] and SegNet [51], by creating dense connectionsamong features at different scales obtained by means of dilated convolutions.By so doing, features at different scales can be contextually extracted usingfewer parameters than full CNNs. Finally, also image-to-image translationapproaches—e.g., pix2pix [52] that leverages conditional adversarial neuralnetworks—were exploited for image segmentation.However, no literature method so far coped with the generalization abilityamong multi-institutional MRI datasets, making their clinical applicabilitydifficult [53].In a previous work [54], we compared existing CNN-basedarchitectures—namely, SegNet [51], U-Net [44], and pix2pix [52]—on twomulti-institutional MRI datasets. According to our results, U-Net gener-ally achieves the most accurate performance. Here, we thoroughly verifythe intra-/cross-dataset generalization on three datasets from three differentinstitutions, also proposing a novel architecture based on U-Net [44] incor-porating SE blocks [21]. To the best of our knowledge, this is the first studyon CNN-based prostate zonal segmentation on T2w MRI alone.3. Materials and MethodsThis section first describes the analyzed multi-institutional MRI datasetscollected by different institutions. Afterwards, we explain the proposed USE-Net, the other investigated CNN-based architectures, as well as a state-of-9the-art prostate zonal segmentation method based on a continuous max-flowmodel [25]. Finally, the used spatial overlap- and distance-based evaluationmetrics are reported.3.1. Multi-institutional MRI DatasetsWe segment the CG and PZ from the WG on three completely differentmulti-parametric prostate MRI datasets, namely:#1 dataset (21 patients/193 MR slices with prostate), acquired with awhole body Philips Achieva 3T MRI scanner at the Cannizzaro Hospital(Catania, Italy) [17]. MRI parameters: matrix size = 288 × 288 pixels;slice thickness = 3.0 mm; inter-slice spacing = 4 mm; pixel spacing= 0.625 mm; number of slices per image series (including slices withoutprostate) = 18. Average patient age: 65.57 ± 6.42 years;#2 Initiative for Collaborative Computer Vision Benchmarking (I2CVB)dataset (19 patients/503 MR slices with prostate), acquired with awhole body Siemens TIM 3T MRI scanner at the Hospital Center Re-gional University of Dijon-Bourgogne (Dijon, France) [2]. MRI param-eters: matrix size ∈ {308 × 384, 336 × 448, 360 × 448, 368 × 448} pixels;slice thickness = 1.25 mm; inter-slice spacing = 1.0 mm; pixel spac-ing ∈ {0.676, 0.721, 0.881, 0.789} mm; number of slices per image series= 64. Average patient age: 64.36 ± 9.69 years;#3 National Cancer Institute – International Symposium on BiomedicalImaging (NCI-ISBI) 2013 Automated Segmentation of Prostate Struc-tures Challenge dataset (40 patients/555 MR slices with prostate) via10The Cancer Imaging Archive (TCIA) [55], acquired with a whole bodySiemens TIM 3T MRI scanner at Radboud University Medical Center(Nijmegen, The Netherlands) [56]. The prostate structures were manu-ally delineated by five experts. MRI parameters: matrix size ∈ {256 ×256, 320 × 320, 384 × 384} pixels; slice thickness ∈ {3.0, 4.0} mm; inter-slice spacing ∈ {3.6, 4.0} mm; pixel spacing ∈ {0.500, 0.600, 0.625} mm;number of slices per image series ranging from 15 to 24. Average pa-tient age: 63.90 ± 7.17 years.All the analyzed MR images are encoded in the 16-bit Digital Imagingand Communications in Medicine (DICOM) format. It is worth noting thateven MR images from the same dataset have intra-dataset variations (suchas the matrix size, slice thickness, and number of slices). Furthermore, inter-rater variability for the CG and PZ annotations exists, as different physiciansdelineated them. For clinical feasibility [10], we analyzed only axial T2w MRslices—the most commonly used sequence for prostate zonal segmentation—among the available sequences.In our multi-centric study, we conductedthe following seven experiments resulting from all possible training/testingconditions:• Individual dataset #1, #2, #3: training and testing on dataset #1(#2, #3, respectively) alone in 4-fold cross-validation, and testing alsoon whole datasets #2 and #3 (#1 and #3, #1 and #2, respectively)separately for each round;• Mixed dataset #1/#2, #2/#3, #1/#3: training and testing on bothdatasets #1 and #2 (#2 and #3, #1 and #3, respectively) in 4-fold11(a)(b)(c)Figure 1: Examples of input prostate T2w MR axial slices in their original image ratio:(a) dataset #1; (b) dataset #2; (c) dataset #3. The CG and PZ are highlighted with redand blue transparent regions, respectively. Alpha blending with α = 0.2.cross-validation, and testing also on whole dataset #3 (#1, #2, respec-tively) separately for each round;• Mixed dataset #1/#2/#3: training and testing on whole datasets #1,#2, and #3 in 4-fold cross-validation.For clinical applications, such a multi-centric research is valuable for ana-lyzing CNNs’ generalization ability among different MRI acquisition options,e.g., different devices and functioning parameters. In our study, for instance,both intra-/cross-scanner evaluations can be carried out, because dataset#1’s scanner is different from those of datasets #2 and #3. Fig. 1 showsan example image for each analyzed dataset; in the context of generaliza-tion among different datasets, Yan et al. [57] evaluated the average vesselsegmentation performance on three retinal fundus image datasets under thethree-dataset training condition, while pair-wisely assessing the cross-datasetperformance on two datasets under the other one-dataset training condition.12Yang et al. [58] proposed an alternative approach using adversarial appear-ance rendering to relieve the burden of re-training for Ultrasound imagingdatasets. Differently, we thoroughly evaluate all possible training/testingconditions (for a total of 21 configurations) on each dataset to confirm theintra- and cross-dataset generalization ability by incrementally injecting sam-ples from the other datasets at hand.With regard to the 4-fold cross-validation, we partitioned the datasets #1,#2, and #3 into 4 folds by using the following patient indices: {[1, . . . , 5],[6, . . . , 10],[11, . . . , 15],[16, . . . , 21]}, {[1, . . . , 5],[6, . . . , 10],[11, . . . , 15],[16, . . . , 19]}, and {[1, . . . , 10], [11, . . . , 20], [21, . . . , 30], [31, . . . , 40]}, respec-tively. Finally, the results from the different cross-validation rounds wereaveraged to obtain a final descriptive value. These patient indices representa permutation of the randomly arranged original patient ordering to por-tray a randomized partition scheme. This allowed us to guarantee a fixedpartitioning among the different training/testing conditions with a generalnotation valid for all datasets, regardless of the number of patients in eachdataset.Cross-validation strategies aim at estimating the generalization ability ofa given model; the hold-out method fixedly partition the dataset into thetraining/test sets to train the model on the first partition alone and test itonly on the unseen test set data. Unlike the leave-one-out cross-validationwith high variance and low bias, the k-fold cross-validation is a natural wayto improve the hold-out method: the dataset is divided into k mutually exclu-sive folds of approximately equal size [59]. The statistical validity increaseswith less variance and less dependency on the initial dataset partition, av-13eraging the results for all the k cross-validation rounds. Consequently, thek-fold cross-validation is the most common choice for reliable generalizationresults, minimizing the bias associated with the random sampling of thetraining/test sets [59]. However, this statistical practice is computationallyexpensive due to the k times-repeated training from scratch [60]. Moreover,the results could underestimate the actual performance allowing for conser-vative analyses [61]; thus, we chose 4-fold cross-validation for reliable and fairtraining/testing phases, according to the number of patients in each dataset,calculating the evaluation metrics on a statistically significant test set (i.e.,25% of each prostate MRI dataset).3.2. Prostate Zonal Segmentation on Multi-institutional MRI DatasetsThis work adopts a selective delineation approach to focus on internalprostatic anatomy: the CG and PZ, denoted by RCG and RP Z, respectively.Let the entire image and the WG region be IΩ and RW G, respectively, thefollowing relationships can be defined:IΩ = RW G ∪ Rbg and RW G ∩ Rbg = ∅,(1)where Rbg represents background pixels. Relying on [7, 25], RP Z was ob-tained by subtracting RCG from RW G meeting the constraints:RW G = RCG ∪ RP Z and RCG ∩ RP Z = ∅.(2)3.2.1. USE-Net: Incorporating SE Blocks into U-NetWe propose to introduce SE blocks [21] following every Encoder (EncUSE-Net) or Encoder-Decoder (Enc-Dec USE-Net) of U-Net [44], as shown14Figure 2: Scheme of the proposed USE-Net architecture: Enc USE-Net has only 4 (red-contoured) SE blocks after every encoder, whilst Enc-Dec USE-Net has 9 SE blocks inte-grated after every encoder/decoder (represented with red/blue contours, respectively).in Fig. 2. As pointed out before, U-Net allows for a multi-resolution decom-position/composition technique [62], by combining encoders/decoders withskip connections between them [63]; in our implementation, encoders anddecoders consist of four pooling operators that capture the context and up-sampling operators that conduct precise localization, respectively.We introduce SE blocks to enhance image segmentation, expecting anincreased representational power from modeling the channel-wise dependen-cies of convolutional features [21]. These blocks were originally envisioned forimage classification using adaptive feature recalibration to boost informative15SEInput Image Prediction 1×H₁×W₁F₁×H₁×W₁F₁×H₂×W₂F₂×H₂×W₂F₁×H₁×W₁Nc×H₁×W₁SESESESEF₁×H₂×W₂F₂×H₂×W₂F₂×H₃×W₃F₃×H₄×W₄F₄×H₄×W₄F₄×H₄×W₄F₃×H₄×W₄F₃×H₃×W₃F₂×H₃×W₃F₃×H₃×W₃F₄×H₅×W₅F₅×H₅×W₅F×H×WF×1×1F⁄r×1×1F⁄r×1×1F×1×1F×1×1F×H×WXX(cid:2)ResidualScaleSigmoidFCFCReLUGlobal-poolingSE BlockLayers(Conv 3×3 + ReLU) (×2)Max-pooling 2×2Deconvolution 2×2Skip ConnectionSESE ModuleSESESESEfeatures and suppress the weak ones at minimal computational burden.Enc USE-Net and Enc-Dec USE-Net are investigated to evaluate the effectof strengthened feature recalibration. Since the template of the SE blocks isgeneric, they can be exploited at any depth of any architecture. Consideringthat SE blocks should be placed after output feature maps for feature recal-ibration, we have three possible places to integrate them for U-Net, namely:(i) after encoders; (ii ) after decoders; (iii ) after a classifier. SE blocks aremore powerful in the encoding path than in the decoding path and more pow-erful in the decoding path than after a classifier, as they affect lower-levelfeatures in the U-Net architecture and thus increase the overall performancesignificantly; consequently, instead of placing only a single SE block afterthe first encoder/decoder, we place SE blocks after each encoder/decoder forboth coarse-grained context detection in the earlier layers and fine-grainedlocalization in the deeper layers for the best segmentation performance.The SE blocks can be formally described as follows:Squeeze. Let U = [u1, u2, . . . , uF ] be an input feature map, where uf ∈RH×W is a single channel with size H × W . Through spatial dimensionsH × W , a global average pooling layer generates channel-wise statistics z ∈RF , whose f -th element is given by:zf =1H × WH(cid:88)W(cid:88)h=1w=1[uf ]i,j.(3)Excitation. To limit the model complexity and boost generalization, twofully-connected layers and the Rectified Linear Unit (ReLU) [64] functionδ transform z with a sigmoid activation function σ(·):s = σ(g(z, W)) = σ(W2δ(W1z)),(4)16where W1 ∈ R Fr ×F , W2 ∈ RF × Fr , and r is the reduction ratio controling thecapacity and computational cost of the SE blocks. Hu et al. [21] showed thatthe SE blocks can overfit to the channel inter-dependencies of the trainingset despite a lower number of weights with respect to the original archi-tecture; they found the best compromise of r = 8, which guarantees thelowest overall error (in terms of top-1 and top-5 errors) with ResNet-50 [23]for the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2017classification competition [65]. Therefore, we also selected r = 8 for theUSE-Net. In order to obtain an adaptive recalibration that ignores less im-portant channels and emphasizes important ones (allowing for non-mutualexclusivity among multiple channels, differently from one-hot encoding), Uis rescaled into (cid:101)X = [(cid:101)x1, (cid:101)x2, . . . , (cid:101)xF ] by applying Eq. (5):(cid:101)xf = Fscale(uf , sf ) = sf · uf , for f = 1, 2, . . . , F,(5)where Fscale(uf , sf ) represents the channel-wise multiplication between thefeature map uf ∈ RH×W and the scalar sf ∈ [0, 1].3.2.2. Pre-processingTo fit the image resolution of dataset #1, we either center-cropped orzero-padded the images of datasets #2 and #3 to resize them to 288 × 288pixels. Afterwards, all images in the three datasets were masked using thecorresponding prostate binary masks to omit the background and only focuson extracting the CG and PZ from the WG. This operation can be performedeither by an automated method [17] or previously provided manual WGsegmentation [2]. As a simple form of data augmentation, we randomlycropped the input images from 288 × 288 to 256 × 256 pixels and horizontally17flipped them.3.2.3. Post-processingTwo efficient morphological operations were applied on the obtained RCGbinary masks to smooth boundaries and deal with disconnected regions:• a hole filling algorithm on the segmented RCG to remove possible holesin a predicted map;• a small area removal operation dealing with connected componentssmaller than (cid:98)|RW G|/8(cid:99) pixels, where |RW G| denotes the number ofpixels contained in WG segmentation. This adaptive criterion takesinto account the different sizes of RW G (ranging from the apical to thebasal prostate slices).3.2.4. Comparison against the State-of-the-Art MethodsWe compare USE-Net against three supervised CNN-based architectures(i.e., U-Net, pix2pix, and Mixed-Scale Dense Network) and the unsupervisedcontinuous max-flow model [25]. All the investigated CNN-based architec-tures were trained using the LDSC loss function (i.e., a continuous version ofthe DSC ) [34] through the N pixels to classify:LDSC = −2 (cid:80)Ni=1 si · rii=1 si + (cid:80)Ni=1 ri(cid:80)N,(6)where si and ri refer to the continuous values in [0, 1] of the prediction mapand the Boolean ground truth annotated by experienced radiologists at thei-th pixel, respectively. The LDSC loss function was designed by Milletari etal. [34] to deal with the imbalance of the foreground labels in medical imagesegmentation tasks.18USE-Net and U-Net. Using four scaling operations, U-Net and USE-Net wereimplemented on Keras with TensorFlow backend. We used the StochasticGradient Descent (SGD) method [66] with a learning rate of 0.01, momentumof 0.9, weight decay of 5 × 10−4, and batch size of 4. Training was executedfor 50 epochs, multiplying the learning rate by 0.2 at the 20-th and 40-thepochs.pix2pix. This image-to-image translation method with conditional adversar-ial networks was used to translate the original image into the segmentedone [52]. The generator and discriminator (both U-Nets in our implemen-tation) include eight and five scaling operations, respectively. We developedpix2pix on PyTorch. Adam [67] was used as an optimizer with a learning rateof 0.01 for the generator—which was multiplied by 0.1 every 20 epochs—and2 × 10−4 for the discriminator. Training was executed for 50 epochs with abatch size of 12.MS-D Net. This dilated convolution-based method, characterized by denselyconnected feature maps, is designed to capture features at various imagescales [24]. It was implemented on PyTorch with a depth of 100 and widthof 1. We used Adam [67] with a learning rate of 1 × 10−3 and trained it for100 epochs with a batch size of 12.Continuous Max-flow Model. This model [25] exploits duality-based convexrelaxed optimization [39] to achieve better numerical stability (i.e., conver-gence) than classic graph cut-based methods [68]. This semi-automatic ap-proach simultaneously segments both RW G and RCG under the constraintsgiven in Eq. (2), relying on user intervention. The initialization procedure19consists in two closed surfaces defined by a thin-plate spline interpolating10-12 control points interactively selected by the user (considering both theaxial and sagittal views). These 3D partitions estimate the intensity proba-bility density functions associated with three sub-regions of background, CG,and PZ. This allows for defining the region appearance models for globaloptimization-based multi-region segmentation [39].Since the supervised CNN-based architectures rely on the gold standardRW G for zonal segmentation, we apply the continuous max-flow method onCG for single-region segmentation for a fair comparison. Moreover, in ourtests, a very accurate slice-by-slice RCG initialization is provided by erodingthe gold standard CG with a circular structuring element (radius = 6 pixels).The continuous max-flow model[25] was implemented in MatLab R(cid:13)R2017a 64-bit (The Mathworks, Natick, MA, USA).3.3. Evaluation MetricsWe evaluate the segmentation methods by comparing the segmented MRimages (S) to the corresponding gold standard manual segmentation (G)using spatial overlap- and distance-based metrics [69, 70, 71]. Those metricsare calculated using a slice-wise comparison and then averaged per patient;thus, each single result regarding a patient represents an aggregate value.Overlap-based metrics. These metrics quantify the spatially-overlapping seg-mented Region of Interest (ROI). Let true positives be T P = S ∩ G, falsenegatives be F N = G − S, false positives be F P = S − G, and true negativesbe T N = IΩ − G − S. In what follows, we denote the cardinality of the pixelsbelonging to a region A as |A|.20• Dice similarity coefficient [72] is the most used measure in medicalimage segmentation to compare the overlap of two regions:DSC =2 · |T P ||S| + |G|· 100.• Sensitivity measures the correct detection ratio of true positives:SEN =|T P ||T P | + |F N |· 100.• Specificity measures the correct detection ratio of true negatives:T N R =|T N ||T N | + |F P |· 100.(7)(8)(9)However, this formulation is ineffective when data are unbalanced (i.e.,the ROI is much smaller than the whole image). Consequently, we usethe following definition:(cid:18)SP C =1 −(cid:19)|F P ||S|· 100.(10)Distance-based metrics. As precise boundary tracing plays an important rolein clinical practice, overlap-based metrics have limitations in evaluating seg-mented images.In order to measure the distance between the two ROIboundaries, distance-based metrics can be considered. Let the manual con-tour G consist in a set of vertices {ga : a = 1, 2, . . . , A} and the automatically-generated contour S consist in a set of vertices {sb : b = 1, 2, . . . , B}. Wecalculate the absolute distance between an arbitrary element sb ∈ S and allthe vertices in G as follows:d(sb, G) = mina∈{1,2,...,A}(cid:107)sb − ga(cid:107).(11)21• Average absolute distance measures the average difference between theROI boundaries of S and G:AvgD =1BB(cid:88)b=1d(sb, G).(12)• Maximum absolute distance represents the maximum difference be-tween the ROI boundaries of S and G:M axD = maxb∈{1,2,...,B}d(sb, G).(13)4. Experimental ResultsThis section shows how the CNN-based architectures and the continu-ous max-flow model segmented the prostate zones, through the evaluationof their cross-dataset generalization ability. Aiming at showing the perfor-mance boost achieved by integrating the SE blocks into U-Net, we performeda fair comparison against the state-of-the-art architectures under the sametraining/testing conditions. In particular, due to the lack of annotated MRimages for prostate zonal segmentation, we used three different datasets bycomposing a multi-institutional dataset. This allowed us to show the SEblocks’ cross-dataset adaptive feature recalibration effect, better capturingeach dataset’s peculiar characteristics. Therefore, we exploited all possibletraining/testing conditions involving the three analyzed datasets (for a to-tal of 21 configurations) on each dataset to overcome the limitation fromthe small sample size, confirming the intra- and cross-dataset generalizationability of the CNN-based architectures.Table 1 shows the 4-fold cross-validation results, as assessed by the DSCmetrics, obtained under different training/testing conditions (the values of22the other metrics are given in Supplementary Material, Tables S1-S4). Forvisual and comprehensive comparison, the Kiviat diagrams (also known asradar or cobweb charts) [73, 59] for each CNN-based architecture are alsodisplayed in Fig. 3. Here, we can observe the impact of leaving dataset #3out of the training set and, at the same time, using it as test set: the corre-sponding spokes III, VI, and XII generally show lower performance, proba-bly due to the peculiar image characteristics of dataset #3 (comprising thehighest number of patients) that are not learned during the training phaseon datasets #1/#2. In general, Enc USE-Net performs similarly to U-Net,which stably yields satisfactory results. More interestingly, Enc USE-Net ob-tains considerably better results when trained/tested on multiple datasets.Enc-Dec USE-Net (characterized by a higher number of SE blocks with re-spect to Enc USE-Net) consistently and remarkably outperforms the othermethods on both CG and PZ segmentation when trained on all the inves-tigated datasets, also performing well when trained and tested on the samedatasets.We executed the Friedman’s test to quantitatively investigate any statis-tical performance differences among the tested approaches. Regarding thethree-dataset condition: p = 0.0009 and p = 1.3 · 10−6 for the CG andPZ, respectively. Considering all training/testing combinations: p = 0.01and p = 1.7 · 10−10 for the CG and PZ, respectively. Since the p-values al-lowed us to reject the null hypothesis, we performed the Bonferroni-Dunn’spost hoc test for both the three-dataset condition and all training/testingcombinations [74].In order to visualize the achieved results, example im-ages segmented by each method are compared in Fig. 4 under the three-23Table 1: Prostate zonal segmentation results of the CNN-based architectures and the unsu-pervised continuous max-flow model (proposed by Qiu et al. [25]) in 4-fold cross-validationassessed by DSC (presented as the mean value ± standard deviation). The supervisedexperimental results are calculated under the different seven conditions described in Sec-tion 3.1. Numbers in bold indicate the best DSC values (the higher the better) for eachprostate region (i.e., RCG and RP Z) among all architectures.MethodTesting on dataset #1Testing on dataset #2Testing on dataset #3CGPZCGPZCGPZMS-D Net84.3 ± 1.686.7 ± 1.677.3 ± 4.165.6 ± 11.266.2 ± 3.750.9 ± 1.2pix2pixU-Net81.9 ± 2.285.9 ± 5.077.2 ± 2.973.7 ± 4.352.5 ± 3.247.1 ± 1.378.6 ± 4.178.3 ± 7.677.4 ± 5.475.3 ± 1.473.6 ± 6.250.9 ± 1.5Enc USE-Net79.3 ± 3.577.7 ± 2.781.3 ± 1.574.7 ± 1.875.0 ± 4.250.3 ± 1.2Enc-Dec USE-Net78.8 ± 2.979.4 ± 7.976.9 ± 5.572.7 ± 1.763.7 ± 14.646.3 ± 1.8MS-D Net78.7 ± 1.170.0 ± 4.486.8 ± 3.781.1 ± 0.583.2 ± 1.054.6 ± 0.8pix2pixU-Net78.3 ± 0.967.3 ± 3.287.1 ± 2.981.8 ± 1.080.0 ± 2.551.1 ± 1.578.6 ± 1.070.9 ± 3.287.7 ± 2.082.4 ± 2.483.8 ± 1.854.9 ± 1.8Enc USE-Net78.8 ± 1.472.3 ± 5.687.4 ± 2.582.6 ± 2.182.9 ± 2.554.5 ± 2.0Enc-Dec USE-Net77.5 ± 2.170.6 ± 5.587.8 ± 2.782.8 ± 1.982.7 ± 1.553.6 ± 1.0MS-D Net81.2 ± 1.373.3 ± 3.782.5 ± 1.974.7 ± 2.091.6 ± 1.171.4 ± 5.6pix2pixU-Net79.1 ± 5.664.6 ± 22.181.2 ± 4.266.6 ± 19.189.4 ± 4.862.8 ± 10.075.9 ± 3.463.3 ± 5.082.1 ± 2.966.6 ± 8.491.7 ± 2.476.1 ± 4.1Enc USE-Net77.3 ± 3.664.7 ± 6.482.7 ± 4.366.7 ± 15.991.5 ± 3.274.0 ± 7.8Enc-Dec USE-Net76.1 ± 4.258.9 ± 13.781.8 ± 4.867.6 ± 13.290.7 ± 3.176.6 ± 7.8MS-D Net84.4 ± 3.186.5 ± 2.786.4 ± 2.881.2 ± 1.381.7 ± 2.354.9 ± 2.5pix2pixU-Net83.8 ± 2.684.8 ± 3.187.1 ± 2.781.0 ± 0.482.1 ± 2.554.0 ± 1.882.6 ± 3.390.0 ± 2.786.4 ± 2.082.2 ± 2.781.8 ± 2.155.3 ± 2.5Enc USE-Net81.7 ± 5.490.0 ± 2.187.0 ± 2.182.2 ± 1.880.8 ± 2.755.8 ± 1.7Enc-Dec USE-Net82.9 ± 3.490.6 ± 1.885.9 ± 2.182.9 ± 1.481.1 ± 2.755.1 ± 2.1MS-D Net85.4 ± 1.887.7 ± 2.580.9 ± 2.772.6 ± 3.791.0 ± 2.972.2 ± 1.9pix2pixU-Net85.2 ± 1.686.8 ± 2.482.7 ± 1.975.7 ± 3.691.5 ± 1.971.0 ± 3.684.8 ± 0.490.4 ± 2.882.1 ± 2.972.5 ± 4.592.6 ± 1.578.9 ± 4.0Enc USE-Net83.8 ± 1.491.1 ± 1.481.6 ± 3.771.9 ± 8.192.5 ± 1.979.6 ± 2.1Enc-Dec USE-Net83.3 ± 3.290.4 ± 2.181.5 ± 4.671.8 ± 6.792.2 ± 2.480.8 ± 1.8MS-D Net81.0 ± 1.372.5 ± 5.686.2 ± 2.577.4 ± 4.791.7 ± 0.969.8 ± 3.6pix2pixU-Net81.1 ± 1.173.4 ± 3.387.4 ± 2.279.6 ± 5.792.0 ± 1.371.3 ± 3.479.2 ± 2.065.7 ± 6.388.1 ± 2.981.4 ± 2.692.9 ± 1.177.6 ± 3.0Enc USE-Net79.8 ± 1.870.3 ± 7.688.5 ± 2.482.0 ± 3.292.8 ± 1.076.3 ± 2.7Enc-Dec USE-Net79.4 ± 2.567.4 ± 8.988.2 ± 2.982.0 ± 4.193.7 ± 0.676.1 ± 3.4MS-D Net84.8 ± 4.583.6 ± 6.986.8 ± 2.678.6 ± 5.191.1 ± 1.069.4 ± 4.5pix2pixU-Net85.5 ± 2.687.6 ± 3.587.5 ± 2.080.9 ± 5.391.8 ± 1.369.7 ± 4.884.6 ± 1.990.5 ± 3.086.6 ± 2.080.9 ± 3.392.9 ± 1.177.2 ± 2.0Enc USE-Net84.8 ± 2.391.1 ± 2.587.4 ± 1.881.4 ± 4.493.2 ± 0.779.1 ± 3.5Enc-Dec USE-Net87.1 ± 3.691.9 ± 2.188.6 ± 1.583.1 ± 2.993.7 ± 1.080.1 ± 5.5#1dataset#2dataset#3dataset#1/#2datasets#1/#3datasets#2/#3datasets#1/#2/#3datasetsTrainingonTrainingonTrainingonTrainingonTrainingonTrainingonTrainingonNoneQiu et al. [25]78.0 ± 4.975.3 ± 6.471.0 ± 7.077.3 ± 2.682.1 ± 1.561.9 ± 4.624(a)(b)(c)Figure 3: Kiviat diagrams showing the DSC values achieved by each method under differ-ent conditions. RCG and RP Z results are denoted by blue and cyan colors, respectively.Each variable represents a “training-set → test-set” condition as follows:(a) one-dataset training: I) #1 → #1; II) #1 → #2; III) #1 → #3; IV) #2 → #1; V)#2 → #2; VI) #2 → #3; VII) #3 → #1; VIII) #3 → #2; IX) #3 → #3.(b) two-dataset training: X) #1/#2 → #1; XI) #1/#2 → #2; XII) #1/#2 → #3; XIII)#1/#3 → #1; XIV) #1/#3 → #2; XV) #1/#3 → #3; XVI) #2/#3 → #1; XVII)#2/#3 → #2; XVIII) #2/#3 → #3.(c) three-dataset training: XIX) #1/#2/#3 → #1; XX) #1/#2/#3 → #2; XXI)#1/#2/#3 → #3.25IIIIIIIVVVIVIIVIIIIXMS-DNetIIIIIIIVVVIVIIVIIIIXpix2pixIIIIIIIVVVIVIIVIIIIXU-NetIIIIIIIVVVIVIIVIIIIXEncUSE-NetIIIIIIIVVVIVIIVIIIIXEnc-DecUSE-NetXXIXIIXIIIXIVXVXVIXVIIXVIIIMS-DNetXXIXIIXIIIXIVXVXVIXVIIXVIIIpix2pixXXIXIIXIIIXIVXVXVIXVIIXVIIIU-NetXXIXIIXIIIXIVXVXVIXVIIXVIIIEncUSE-NetXXIXIIXIIIXIVXVXVIXVIIXVIIIEnc-DecUSE-NetXIXXXXXIMS-DNetXIXXXXXIpix2pixXIXXXXXIU-NetXIXXXXXIEncUSE-NetXIXXXXXIEnc-DecUSE-Net(a)(b)(c)Figure 4: Segmentation results obtained by the six investigated methods (under the three-dataset training condition) on two different images for each dataset: (a) #1; (b) #2; (c)#3. Automatic RCG segmentations (solid lines) are compared against the correspondinggold standards (dashed red line). RP Z segmentations can be obtained from RCG andRW G (dashed green line) according to the constraints in Eq. (2).26Cont. max-flowMS-D Netpix2pixU-NetEnc USE-NetEnc-Dec USE-NetCont. max-flowMS-D Netpix2pixU-NetEnc USE-NetEnc-Dec USE-NetCont. max-flowMS-D Netpix2pixU-NetEnc USE-NetEnc-Dec USE-Netdataset training condition. The critical difference diagram (Fig. 5) using theBonferroni-Dunn’s post hoc test also confirms this trend, considering DSCvalues for every round of the 4-fold cross-validation.However, as shown in Fig. 6, Enc-Dec USE-Net shows less powerfulcross-dataset generalization when trained and tested on different datasets,achieving slightly lower average performance than Enc USE-Net (consid-ering all training/testing combinations). This implies that the SE blocks’adaptive feature recalibration—boosting informative features and suppress-ing weak ones—provides excellent intra-dataset generalization in the caseof testing performed on multiple datasets used during training (i.e., whentraining samples from every testing dataset are fed to the model).On the contrary, pix2pix achieves good generalization when trained andtested on different datasets, especially under mixed-dataset training condi-tions, thanks to its internal generative model. MS-D Net generally works bet-ter in single dataset scenarios, using a limited amount of training samples,according to [24]. The unsupervised continuous max-flow model achievescomparable results to the supervised ones only when trained and tested ondifferent datasets. However, this semi-automatic approach is outperformedby the supervised methods when trained and tested on the same datasets, asit underestimates RCG.The results also reveal that training on multi-institutional datasets gener-ally outperforms training on each dataset during testing on any dataset/zone,realizing both intra-/cross-dataset generalization. For instance, training ondatasets #1 and #2 generally outperforms training on dataset #1 duringtesting on all datasets #1, #2, and #3, without losing accuracy.27(a) Central Gland(b) Peripheral ZoneFigure 5: Critical Difference (CD) diagram comparing the DSC values achieved by all theinvestigated CNN-based architectures using the Bonferroni-Dunn’s post hoc test [74] with95% confidence level for the three-dataset training conditions. Bold lines indicate groupsof methods whose performance difference was not statistically significant.Therefore, training schemes with mixed MRI datasets can achieve reliableand excellent performance, potentially useful for other clinical applications.Comparing the CG and PZ segmentation, the results on the CG are generallymore accurate, except when trained and tested on dataset #1; this could bedue to intra- and cross-scanner generalization, since dataset #1’s scanner isdifferent from those of datasets #2 and #3.The trend characterizing the best DSC accuracy performance, especiallyin the case of three-dataset training/testing conditions, is reflected by boththe SEN and SPC values (Tables S1 and S2). As shown in Tables S3 and S4,the achieved spatial distance-based indices are consistent with overlap-basedmetrics. Hence, Enc-Dec USE-Net obtained high performance also in terms2812345Enc-Dec USE-NetEnc USE-Netpix2pixU-NetMS-D NetCD12345Enc-Dec USE-NetEnc USE-NetU-Netpix2pixMS-D NetCD(a) Central Gland(b) Peripheral ZoneFigure 6: Critical Difference (CD) diagram comparing the DSC values achieved by all theinvestigated CNN-based architectures using the Bonferroni-Dunn’s post hoc test [74] with95% confidence level considering all training/testing combinations. Bold lines indicategroups of methods whose performance difference was not statistically significant.of difference between the automated and the manual boundaries.Considering more permutations in the random partitioning and runningmultiple 4-fold cross-validation instances may increase the robustness of theresults, by evaluating the combination of the multiple executions. How-ever, with particular reference to the three-dataset training/testing condi-tion, where the feature recalibration can effectively capture the dataset char-acteristics with the most available samples, the Bonferroni-Dunns post hoctest showed significant differences in the multiple comparisons among thecompeting architectures (Fig. 5). On the contrary, no significant statisti-cal difference was detected when considering all training/testing conditions(Fig. 6). The achieved results suggest that cross-validation with a singlerandom permutation is methodologically sound. In addition, we can state2912345Enc USE-NetEnc-Dec USE-NetU-NetMS-D Netpix2pixCD12345Enc USE-NetEnc-Dec USE-NetU-NetMS-D Netpix2pixCDthat the patterns arising from the 4-fold cross-validation experiments arenot just by chance or biased by the increased training samples, so USE-Netsignificantly outperforms the other techniques.To conclude, the comparison of U-Net and USE-Nets shows the individ-ual contribution of SE blocks under each of the 21 dataset combinations.Interestingly, USE-Net is not always superior on one- or two-dataset cases,but consistently outperforms U-Net on three-dataset training/testing. Thisarises from USE-Net’s higher number of parameters than U-Net, generallyrequiring more samples for proper tuning.5. Discussion and ConclusionsThe novel CNN architecture introduced in this work, Enc-Dec USE-Net,achieved accurate prostate zonal segmentation results when trained on theunion of the available datasets in the case of multi-institutional studies—significantly outperforming the competitor CNN-based architectures, thanksto the integration of SE blocks [21] into U-Net [44]. This also derives fromthe presented cross-dataset generalization approach among three prostateMRI datasets, collected by three different institutions, aiming at segmentingRCG and RP Z; Enc-Dec USE-Net’s segmentation performance considerablyimproved when trained on multiple datasets with respect to individual train-ing conditions. Since the training on multi-institutional datasets analyzed inthis work achieved good intra-/cross-dataset generalization, CNNs could betrained on multiple datasets with different devices/protocols to obtain bet-ter outcomes in clinically feasible applications. Moreover, our research alsoimplies that state-of-the-art CNN architectures properly combined with inno-30vative concepts, such as feature recalibration provided by the SE blocks [21],allow for excellent intra-dataset generalization when tested on samples com-ing from the datasets used for the training phase. Therefore, we may arguethat multi-dataset training and SE blocks represent not just individual op-tions but mutually indispensable strategies to draw out each other’s full po-tential. In conclusion, such adaptive mechanisms may be a valuable solutionin medical imaging applications involving multi-institutional settings.As future developments, we will refine the output images considering the3D spatial information among the prostate MR slices. Finally, for bettercross-dataset generalization, we plan to use domain adaptation via transferlearning by maximizing the distribution similarity [20]. In this context, Gen-erative Adversarial Networks (GANs) [75, 76] and Variational Auto-Encoders(VAEs) [77] represent useful solutions.AcknowledgmentThis work was partially supported by the Graduate Program for SocialICT Global Creative Leaders of The University of Tokyo by JSPS.We thank the Cannizzaro Hospital, Catania, Italy, for providing one ofthe imaging datasets analyzed in this study.ReferencesReferences[1] R. L. Siegel, K. D. Miller, A. Jemal, Cancer statistics, 2019, CA CancerJ. Clin. 69 (1) (2019) 7–34. doi:10.3322/caac.21551.31[2] G. Lemaˆıtre, R. Mart´ı, J. Freixenet, J. C. Vilanova, P. M. Walker,F. Meriaudeau, Computer-aided detection and diagnosis for prostatecancer based on mono and multi-parametric MRI: A review, Comput.Biol. Med. 60 (2015) 8–31. doi:10.1016/j.compbiomed.2015.02.009.[3] O. Rouvi`ere, R. P. Hartman, D. Lyonnet, Prostate MR imaging at high-field strength: evolution or revolution?, Eur. Radiol. 16 (2) (2006) 276–284. doi:10.1007/s00330-005-2893-8.[4] C. K. Kim, B. K. Park, Update of prostate magnetic resonance imagingat 3 T, J. Comput. Assist. Tomogr. 32 (2) (2008) 163–172. doi:10.1097/RCT.0b013e3180683b99.[5] S. W. Heijmink, J. J. Futterer, T. Hambrock, S. Takahashi, T. W.Scheenen, H. J. Huisman, et al., Prostate cancer: body-array versusendorectal coil MR imaging at 3 T—Comparison of image quality, lo-calization, and staging performance, Radiology 244 (1) (2007) 184–195.doi:10.1148/radiol.2441060425.[6] A. R. Padhani, F. E. Lecouvet, N. Tunariu, D.-M. Koh, F. De Keyzer,D. J. Collins, et al., Rationale for modernising imaging in advancedprostate cancer, Eur. Urol. Focus 3 (2-3) (2017) 223–239. doi:10.1016/j.euf.2016.06.018.[7] G. M. Villeirs, G. O. De Meerleer, Magnetic resonance imaging (MRI)anatomy of the prostate and application of MRI in radiotherapy plan-ning, Eur. J. Radiol. 63 (3) (2007) 361–368. doi:10.1016/j.ejrad.2007.06.030.32[8] S. H. Selman, The McNeal prostate: A review, Urology 78 (6) (2011)1224–1228. doi:10.1016/j.urology.2011.07.1395.[9] T. W. Scheenen, A. B. Rosenkrantz, M. A. Haider, J. J. F¨utterer, Multi-parametric magnetic resonance imaging in prostate cancer management:current status and future perspectives, Invest. Radiol. 50 (9) (2015) 594–600. doi:10.1097/RLI.0000000000000163.[10] C. M. Hoeks, J. O. Barentsz, T. Hambrock, D. Yakar, D. M. Somford,S. W. Heijmink, et al., Prostate cancer: multiparametric MR imagingfor detection, localization, and staging, Radiology 261 (1) (2011) 46–66.doi:10.1148/radiol.11091822.[11] Y. J. Choi, J. K. Kim, N. Kim, K. W. Kim, E. K. Choi, K.-S. Cho,Functional MR imaging of prostate cancer, Radiographics 27 (1) (2007)63–75. doi:10.1148/rg.271065078.[12] E. Niaf, O. Rouvi`ere, F. M`ege-Lechevallier, F. Bratan, C. Lartizien,Computer-aided diagnosis of prostate cancer in the peripheral zone usingmultiparametric MRI, Phys. Med. Biol. 57 (12) (2012) 3833. doi:10.1088/0031-9155/57/12/3833.[13] J. Haffner, E. Potiron, S. Bouy´e, P. Puech, X. Leroy, L. Lemaitre,A. Villers, Peripheral zone prostate cancers: location and intraprostaticpatterns of spread at histopathology, Prostate 69 (3) (2009) 276–282.doi:10.1002/pros.20881.[14] Y. Chang, R. Chen, Q. Yang, X. Gao, C. Xu, J. Lu, Y. Sun, Peripheralzone volume ratio (PZ-ratio) is relevant with biopsy results and can33increase the accuracy of current diagnostic modality, Oncotarget 8 (21)(2017) 34836. doi:10.18632/oncotarget.16753.[15] R. Kirby, R. Gilling, Fast Facts: Benign Prostatic Hyperplasia, 7thEdition, Health Press Limited, Abingdon, UK, 2011.[16] J. C. Weinreb, J. O. Barentsz, P. L. Choyke, F. Cornud, M. A. Haider,K. J. Macura, et al., PI-RADS prostate imaging–reporting and datasystem: 2015, version 2, Eur. Urol. 69 (1) (2016) 16–40. doi:10.1016/j.eururo.2015.08.052.[17] L. Rundo, C. Militello, G. Russo, A. Garufi, S. Vitabile, M. C. Gi-lardi, G. Mauri, Automated prostate gland segmentation based onan unsupervised fuzzy c-means clustering technique using multispec-tral T1w and T2w MR imaging, Information 8 (2) (2017) 49. doi:10.3390/info8020049.[18] B. G. Muller, J. H. Shih, S. Sankineni, J. Marko, S. Rais-Bahrami,A. K. George, et al., Prostate cancer: interobserver agreement and ac-curacy with the revised prostate imaging reporting and data systemat multiparametric MR imaging, Radiology 277 (3) (2015) 741–750.doi:10.1148/radiol.2015142818.[19] S. Klein, U. A. Van Der Heide, I. M. Lips, M. Van Vulpen, M. Staring,J. P. Pluim, Automatic segmentation of the prostate in 3D MR imagesby atlas matching using localized mutual information, Med. Phys. 35 (4)(2008) 1407–1417. doi:10.1118/1.2842076.34[20] A. van Opbroek, M. W. Vernooij, M. A. Ikram, M. de Bruijne, Weight-ing training images by maximizing distribution similarity for supervisedsegmentation across scanners, Med. Image Anal. 24 (1) (2015) 245–254.doi:10.1016/j.media.2015.06.010.[21] J. Hu, L. Shen, S. Albanie, G. Sun, E. Wu, Squeeze-and-excitationnetworks, IEEE Trans. Pattern Anal. Mach. Intell. arXiv preprintarXiv:1709.01507 (Submitted manuscript).[22] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinkingthe Inception architecture for computer vision, in: Proc. Conference onComputer Vision and Pattern Recognition (CVPR), 2016, pp. 2818–2826. doi:10.1109/CVPR.2016.308.[23] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for imagerecognition, in: Proc. IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2016, pp. 770–778. doi:10.1109/CVPR.2016.90.[24] D. M. Pelt, J. A. Sethian, A mixed-scale dense convolutional neuralnetwork for image analysis, Proc. Natl. Acad. Sci. 115 (2) (2018) 254–259. doi:10.1073/pnas.1715832114.[25] W. Qiu, J. Yuan, E. Ukwatta, Y. Sun, M. Rajchl, A. Fenster, Dualoptimization based prostate zonal segmentation in 3D MR images, Med.Image Anal. 18 (4) (2014) 660–673. doi:10.1016/j.media.2014.02.009.[26] S. Martin, V. Daanen, J. Troccaz, Atlas-based prostate segmentation35using an hybrid registration, Int. J. Comput. Assist. Radiol. Surg. 3 (6)(2008) 485–492. doi:10.1007/s11548-008-0247-0.[27] S. Martin, J. Troccaz, V. Daanen, Automated segmentation of theprostate in 3D MR images using a probabilistic atlas and a spatiallyconstrained deformable model, Med. Phys. 37 (4) (2010) 1579–1590.doi:10.1118/1.3315367.[28] R. Toth, J. Ribault, J. Gentile, D. Sperling, A. Madabhushi, Simulta-neous segmentation of prostatic zones using active appearance modelswith multiple coupled levelsets, Comput. Vis. Image Underst. 117 (9)(2013) 1051–1060. doi:10.1016/j.cviu.2012.11.013.[29] L. Rundo, C. Militello, G. Russo, D. D’Urso, L. M. Valastro, A. Garufi,et al., Fully automatic multispectral MR image segmentation of prostategland based on the fuzzy c-means clustering algorithm, in: Multidis-ciplinary Approaches to Neural Computing, Vol. 69 of Smart Inno-vation, Systems and Technologies, Springer, 2018, pp. 23–37. doi:10.1007/978-3-319-56904-8_3.[30] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi,M. Ghafoorian, et al., A survey on deep learning in medical image anal-ysis, Med. Image Anal. 42 (2017) 60–88. doi:10.1016/j.media.2017.07.005.[31] Y. Guo, Y. Gao, D. Shen, Deformable MR prostate segmentation viadeep feature learning and sparse patch matching, IEEE Trans. Med.Imaging 35 (4) (2016) 1077–1089. doi:10.1109/TMI.2015.2508280.36[32] J. Sun, Y. Shi, Y. Gao, D. Shen, A point says a lot: An interac-tive segmentation method for MR prostate via one-point labeling, in:Proc. International Workshop on Machine Learning in Medical Imaging,Springer, 2017, pp. 220–228. doi:10.1007/978-3-319-67389-9_26.[33] H. Jia, Y. Xia, Y. Song, W. Cai, M. Fulham, D. D. Feng, Atlas regis-tration and ensemble deep convolutional neural network-based prostatesegmentation using magnetic resonance imaging, Neurocomputing 275(2018) 1358–1369. doi:10.1016/j.neucom.2017.09.084.[34] F. Milletari, N. Navab, S.-A. Ahmadi, V-Net: fully convolutional neuralnetworks for volumetric medical image segmentation, in: Proc. Inter-national Conference on 3D Vision (3DV), IEEE, 2016, pp. 565–571.doi:10.1109/3DV.2016.79.[35] X. Yang, C. Liu, Z. Wang, J. Yang, H. Le Min, L. Wang, K.-T. T. Cheng,Co-trained convolutional neural networks for automated detection ofprostate cancer in multi-parametric MRI, Med. Image Anal. 42 (2017)212–227. doi:10.1016/j.media.2017.08.006.[36] Z. Wang, C. Liu, D. Cheng, L. Wang, X. Yang, K.-T. Cheng, Auto-mated detection of clinically significant prostate cancer in mp-MRI im-ages based on an end-to-end deep neural network, IEEE Trans. Med.Imaging 37 (5) (2018) 1127–1139. doi:10.1109/TMI.2017.2789181.[37] P. D. Allen, J. Graham, D. C. Williamson, C. E. Hutchinson, Differentialsegmentation of the prostate in MR images using combined 3D shapemodelling and voxel classification, in: Proc. International Symposium on37Biomedical Imaging (ISBI): Nano to Macro, IEEE, 2006, pp. 410–413.doi:10.1109/ISBI.2006.1624940.[38] Y. Yin, S. V. Fotin, S. Periaswamy, J. Kunz, H. Haldankar, N. Mu-radyan, et al., Fully automated 3D prostate central gland segmentationin MR images: a LOGISMOS based approach, in: Medical Imaging:Image Processing, Vol. 8314 of Proc. SPIE, International Society forOptics and Photonics, 2012, p. 83143B. doi:10.1117/12.911778.[39] J. Yuan, E. Bae, X. C. Tai, A study on continuous max-flow and min-cutapproaches, in: Proc. IEEE Conference on Computer Vision and PatternRecognition (CVPR), IEEE, 2010, pp. 2217–2224. doi:10.1109/CVPR.2010.5539903.[40] N. Makni, A. Iancu, O. Colot, P. Puech, S. Mordon, N. Betrouni, Zonalsegmentation of prostate using multispectral magnetic resonance images,Med. Phys. 38 (11) (2011) 6093–6105. doi:10.1118/1.3651610.[41] M. H. Masson, T. Denoeux, ECM: An evidential version of the fuzzyc-means algorithm, Pattern Recognit. 41 (4) (2008) 1384–1397. doi:10.1016/j.patcog.2007.08.014.[42] T. Clark, J. Zhang, S. Baig, A. Wong, M. A. Haider, F. Khalvati, Fullyautomated segmentation of prostate whole gland and transition zonein diffusion-weighted MRI using convolutional neural networks, J. Med.Imaging 4 (4) (2017) 041307. doi:10.1117/1.JMI.4.4.041307.[43] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-38scale image recognition, in: International Conference on Learning Rep-resentations (ICLR), 2015, arXiv preprint arXiv:1409.1556.[44] O. Ronneberger, P. Fischer, T. Brox, U-Net: Convolutional networksfor biomedicalimage segmentation,in: Proc. Conference on Medi-cal Image Computing and Computer-Assisted Intervention (MICCAI),Vol. 9351 of LNCS, Springer, 2015, pp. 234–241.doi:10.1007/978-3-319-24574-4_28.[45] L. Rundo, C. Militello, A. Tangherloni, G. Russo, S. Vitabile, M. C.Gilardi, G. Mauri, NeXt for neuro-radiosurgery: A fully automatic ap-proach for necrosis extraction in brain tumor mri using an unsupervisedmachine learning technique, Int. J. Imaging Syst. Technol. 28 (1) (2018)21–37. doi:10.1002/ima.22253.[46] M. Rajchl, M. C. Lee, O. Oktay, K. Kamnitsas, J. Passerat-Palmbach,W. Bai, et al., DeepCut: Object segmentation from bounding box anno-tations using convolutional neural networks, IEEE Trans. Med. Imaging36 (2) (2017) 674–683. doi:10.1109/TMI.2016.2621185.[47] L. Rundo, C. Militello, G. Russo, S. Vitabile, M. C. Gilardi, G. Mauri,GTVcut for neuro-radiosurgery treatment planning: an MRI braincancer seeded image segmentation method based on a cellular au-tomata model, Nat. Comput. (2017) 1–16 (In press). doi:10.1007/s11047-017-9636-z.[48] M. Havaei, A. Davy, D. Warde-Farley, A. Biard, A. Courville, Y. Bengio,39et al., Brain tumor segmentation with deep neural networks, Med. ImageAnal. 35 (2017) 18–31. doi:10.1016/j.media.2016.05.004.[49] K. Kamnitsas, C. Ledig, V. F. J. Newcombe, J. P. Simpson, A. D. Kane,D. K. Menon, et al., Efficient multi-scale 3D CNN with fully connectedCRF for accurate brain lesion segmentation, Med. Image Anal. 36 (2017)61–78. doi:10.1016/j.media.2016.10.004.[50] J. Schlemper, O. Oktay, M. Schaap, M. Heinrich, B. Kainz, B. Glocker,D. Rueckert, Attention gated networks: learning to leverage salient re-gions in medical images, Med. Image Anal. 53 (2019) 197–207. doi:10.1016/j.media.2019.01.012.[51] V. Badrinarayanan, A. Kendall, R. Cipolla, SegNet: A deep convo-lutional encoder-decoder architecture for image segmentation, IEEETrans. Pattern Anal. Mach. Intell. 39 (12) (2017) 2481–2495. doi:10.1109/TPAMI.2016.2644615.[52] P.Isola,J.-Y. Zhu, T. Zhou, A. A. Efros,Image-to-imagetranslation with conditional adversarial networks, arXiv preprintarXiv:1611.07004.[53] E. A. AlBadawy, A. Saha, M. A. Mazurowski, Deep learning for seg-mentation of brain tumors: Impact of cross-institutional training andtesting, Med. Phys. 45 (3) (2018) 1150–1158. doi:10.1002/mp.12752.[54] L. Rundo, C. Han, J. Zhang, R. Hataya, Y. Nagano, C. Militello, et al.,CNN-based prostate zonal segmentation on T2-weighted MR images: a40cross-dataset study, in: Neural Approaches to Dynamics of Signal Ex-changes, Smart Innovation, Systems and Technologies, Springer, 2018,(In press) arXiv preprint arXiv:1903.12571.[55] F. Prior, K. Smith, A. Sharma, J. Kirby, L. Tarbox, K. Clark, et al.,The public cancer radiology imaging collections of The Cancer ImagingArchive, Sci. Data 4 (2017) 170124. doi:10.1038/sdata.2017.124.[56] N. Bloch, A. Madabhushi, H. Huisman, J. Freymann, J. Kirby,M. Grauer, et al., NCI-ISBI 2013 Challenge: Automated Segmentationof Prostate Structures. The Cancer Imaging Archive, online; Accessedon July 30, 2018 (2015). doi:10.7937/K9/TCIA.2015.zF0vlOPv.[57] Z. Yan, X. Yang, K.-T. Cheng, Joint segment-level and pixel-wiselosses for deep learning based retinal vessel segmentation, IEEE Trans.Biomed. Eng. 65 (9) (2018) 1912–1923.doi:10.1109/TBME.2018.2828137.[58] X. Yang, H. Dou, R. Li, X. Wang, C. Bian, S. Li, D. Ni, P.-A. Heng,Generalizing deep models for ultrasound image segmentation, in: Proc.International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), Springer, 2018, pp. 497–505. doi:10.1007/978-3-030-00937-3_57.[59] B. Diri, S. Albayrak, Visualization and analysis of classifiers performancein multi-class medical data, Expert Syst. Appl. 34 (1) (2008) 628–634.doi:10.1016/j.eswa.2006.10.016.41[60] T. Gandhi, B. K. Panigrahi, M. Bhatia, S. Anand, Expert model fordetection of epileptic activity in EEG signature, Expert Syst. Appl.37 (4) (2010) 3513–3520. doi:10.1016/j.eswa.2009.10.036.[61] R. Kohavi, A study of cross-validation and bootstrap for accuracy esti-mation and model selection, in: Proc. 14th International Joint Confer-ence on Artificial Intelligence (IJCAI), Vol. 2, Morgan Kaufmann Pub-lishers Inc., San Francisco, CA, USA, 1995, pp. 1137–1143.[62] K. Suzuki, H. Abe, H. MacMahon, K. Doi, Image-processing techniquefor suppressing ribs in chest radiographs by means of massive trainingartificial neural network (MTANN), IEEE Trans. Med. Imaging 25 (4)(2006) 406–416. doi:10.1109/TMI.2006.871549.[63] W. Yao, Z. Zeng, C. Lian, H. Tang, Pixel-wise regression using U-Netand its application on pansharpening, Neurocomputing 312 (2018) 364–371. doi:10.1016/j.neucom.2018.05.103.[64] V. Nair, G. E. Hinton, Rectified linear units improve restricted Boltz-mann machines, in: Proc. 27th International Conference on MachineLearning (ICML), 2010, pp. 807–814.[65] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, L. Fei-Fei,ImageNet Large Scale Visual Recognition Challenge, Int. J. Comput.Vis. 115 (3) (2015) 211–252. doi:10.1007/s11263-015-0816-y.[66] L. Bottou, Large-scale machine learning with stochastic gradient de-42scent, in: Proc. COMPSTAT’2010, Springer, 2010, pp. 177–186. doi:10.1007/978-3-7908-2604-3_16.[67] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization,arXiv preprint arXiv:1412.6980.[68] D. Freedman, T. Zhang, Interactive graph cut based segmentationwith shape priors,in: Proc. IEEE Conference on Computer Visionand Pattern Recognition (CVPR), Vol. 1, IEEE, 2005, pp. 755–762.doi:10.1109/CVPR.2005.191.[69] A. Taha, A. Hanbury, Metrics for evaluating 3D medical image segmen-tation: analysis, selection, and tool, BMC Med. Imaging 15 (1) (2015)29. doi:10.1186/s12880-015-0068-x.[70] A. Fenster, B. Chiu, Evaluation of segmentation algorithms for medicalimaging, in: Proc. Annual International Conference of the Engineeringin Medicine and Biology Society, IEEE, 2005, pp. 7186–7189. doi:10.1109/IEMBS.2005.1616166.[71] Y. Zhang, A review of recent evaluation methods for image segmen-tation, in: Proc. IEEE International Symposium on Signal Process-ing and its Applications (ISSPA), Vol. 1, IEEE, 2001, pp. 148–151.doi:10.1109/ISSPA.2001.949797.[72] K. Zou, S. Warfield, A. Bharatha, C. Tempany, M. Kaus, S. Haker,et al., Statistical validation of image segmentation quality based on aspatial overlap index, Acad. Radiol. 11 (2) (2004) 178–189. doi:10.1016/S1076-6332(03)00671-8.43[73] K. W. Kolence, P. J. Kiviat, Software unit profiles & Kiviat figures,ACM SIGMETRICS Perform. Eval. Rev. 2 (3) (1973) 2–12. doi:10.1145/1041613.1041614.[74] J. Demˇsar, Statistical comparisons of classifiers over multiple data sets,J. Mach. Learn. Res. 7 (Jan) (2006) 1–30.[75] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, et al., Generative adversarial nets, in: Proc. Advancesin Neural Information Processing Systems (NIPS), 2014, pp. 2672–2680,http://papers.nips.cc/paper/5423-generative-adversarial-nets.[76] C. Han, H. Hayashi, L. Rundo, R. Araki, W. Shimoda, S. Muramatsu,et al., GAN-based synthetic brain MR image generation, in: Proc. IEEEInternational Symposium on Biomedical Imaging (ISBI), 2018, pp. 734–738. doi:10.1109/ISBI.2018.8363678.[77] D. Kingma, M. Welling, Auto-encoding variational Bayes, in: Proc.International Conference on Learning Representations (ICLR), 2014,arXiv preprint arXiv:1312.6114.44