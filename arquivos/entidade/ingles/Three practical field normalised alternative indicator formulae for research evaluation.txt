1 Three practical field normalised alternative indicator formulae for research evaluation1 Mike Thelwall, Statistical Cybermetrics Research Group, University of Wolverhampton, UK. Although altmetrics and other web-based alternative indicators are now commonplace in publishers’ websites, they can be difficult for research evaluators to use because of the time or expense of the data, the need to benchmark in order to assess their values, the high proportion of zeros in some alternative indicators, and the time taken to calculate multiple complex indicators. These problems are addressed here by (a) a field normalisation formula, the Mean Normalised Log-transformed Citation Score (MNLCS) that allows simple confidence limits to be calculated and is similar to a proposal of Lundberg, (b) field normalisation formulae for the proportion of cited articles in a set, the Equalised Mean-based Normalised Proportion Cited (EMNPC) and the Mean-based Normalised Proportion Cited (MNPC), to deal with mostly uncited data sets, (c) a sampling strategy to minimise data collection costs, and (d) free unified software to gather the raw data, implement the sampling strategy, and calculate the indicator formulae and confidence limits. The approach is demonstrated (but not fully tested) by comparing the Scopus citations, Mendeley readers and Wikipedia mentions of research funded by Wellcome, NIH, and MRC in three large fields for 2013-2016. Within the results, statistically significant differences in both citation counts and Mendeley reader counts were found even for sets of articles that were less than six months old. Mendeley reader counts were more precise than Scopus citations for the most recent articles and all three funders could be demonstrated to have an impact in Wikipedia that was significantly above the world average. 1 Introduction Citation analysis is now a standard part of the research evaluation toolkit. Citation-based indicators are relatively straightforward to calculate and are inexpensive compared to peer review. Cost is a key issue for evaluations designed to inform policy decisions because these tend to cover large numbers of publications but may have a restricted budget. For example, reports on government research policy or national research performance can include citation indicators (e.g., Elsevier, 2013; Science-Metrix, 2015), as can programme evaluations by research funders (Dinsmore, Allen, & Dolby, 2014). Although funding programme evaluations can be conducted by aggregating end-of-project reviewer scores (Hamilton, 2011), this does not allow benchmarking against research funded by other sources in the way that citation counts do. The increasing need for such evaluations is driven by a recognition that public research funding must be accountable (Jaffe, 2002) and for charitable organisations to monitor their effectiveness (Hwang & Powell, 2009). The use of citation-based indicators has many limitations. Some well discussed issues, such as the existence of negative citations, systematic failures to cite important influences and field differences (MacRoberts & MacRoberts, 1996; Seglen, 1998; MacRoberts & MacRoberts, 2010), can be expected to average out when using appropriate indicators and comparing large enough collections of articles (van Raan, 1998). Other 1 Thelwall, M. (2017). Three practical field normalised alternative indicator formulae for research evaluation. Journal of Informetrics, 11(1), 128–151. doi: 10.1016/j.joi.2016.12.002 ©2016 This manuscript version is made available under the CC-BY-NCND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/                             2 problems are more difficult to deal with, such as language biases within the citation databases used for the raw data (Archambault, Vignola-Gagne, Côté, Larivière, & Gingras, 2006; Li, Qiao, Li, & Jin, 2014). More fundamentally, the ultimate purpose of research, at least from the perspective of many funders, is not to understand the world but to help shape it (Gibbons, Limoges, Nowotny, Schwartzman, Scott, & Trow, 1994). An important limitation of citations is therfore that they do not directly measure the commercial, cultural, social or health impacts of research. This has led to the creation and testing of many alternative types of indicators, such as patent citation counts (Jaffe, Trajtenberg, & Henderson, 1993; Narin, 1994), webometrics/web metrics (Thelwall, & Kousha, 2015a) and altmetrics/social media metrics (Priem, Taraborelli, Groth, & Neylon, 2010; Thelwall, & Kousha, 2015b). These indicators can exploit information created by non-scholars, such as industrial inventors’ patents, and may therefore reflect non-academic types of impacts, such as commercial value. A practical problem with many alternative indicators (i.e., those not based on citation counts) is that there is no simple cheap source for them. It can therefore be time-consuming or expensive for organisations to obtain, say, a complete list of the patent citation counts for all of their articles. This problem is exacerbated if an organisation needs to collect the same indicators for other articles so that they can benchmark their performance against the world average or against other similar organisations. Even if the cost is the same as for citation counts, alternative indicators need to be calculated in addition to, rather than instead of, citation counts (e.g., Dinsmore, Allen, & Dolby, 2014; Thelwall, Kousha, Dinsmore, & Dolby, 2016) and so their costs can outweigh their value. This can make it impractical to calculate a range of alternative indicators to reflect different types of impacts, despite this seeming to be a theoretically desirable strategy. The problem is also exacerbated by alternative indicator data usually being much sparser than citation counts (Kousha & Thelwall, 2008; Thelwall, Haustein, Larivière, & Sugimoto, 2013; Thelwall & Kousha, 2008). For example, in almost all Scopus categories, over 90% of articles have no patent citations (Kousha & Thelwall, in press-b). These low values involved make it more important to use statistical methods to detect whether differences between groups of articles are significant. Finally, the highly skewed nature of citation counts and most alternative indicator data causes problems with simple methods of averaging to create indicators, such as the arithmetic mean, and complicate the task of identifying the statistical significance of differences between groups of articles. This article addresses the above problems and introduces a relatively simple and practical strategy to calculate a set of alternative indicators for a collection of articles in an informative way. The first component of the strategy is the introduction of a new field normalisation formula, the Mean Normalised Log-transformed Citation Score (MNLCS) for benchmarking against the world average. As argued below, this is simpler and more coherent than a previous similar field normalisation approach to deal with skewed indicator data. The second component is the introduction of a second new field normalisation formula, the Equalised Mean-based Normalised Proportion Cited (EMNPC), that targets sparse data, and an alternative, the Mean-based Normalised Proportion Cited (MNPC). The third component is a simple sampling strategy to reduce the amount of data needed for effective field normalisation. The final component integrated software environment for collecting and analysing the data so that evaluators can create their own alternative indicator reports for a range of indicators with relative ease. The methods are illustrated with a comparative evaluation of the impact of the research of three large is a single, 3 medical funders using three types of data: Scopus citation counts; Mendeley reader counts; and Wikipedia citations. 2 Mean Normalised Log-transformed Citation Score The citation count of an article must be compared to the citation counts of other articles in order to be assessed. The same is true for collections of articles and a simple solution would be to calculate the average number of citations per article for two or more collections so that the values can be compared. This is a flawed approach for the following reasons that have led to the creation of improved methods. Older articles tend to be more cited than younger articles (Wallace, Larivière, & Gingras, 2009) and so it is not fair to compare averages between sets of articles of different ages. Similarly, different fields attract citations at different rates and so comparing averages between sets of articles from different mixes of fields would also be unfair (Schubert & Braun, 1986). One solution would be to segment each collection of articles into separate sets, one for each field and year, and only compare corresponding sets between collections. Although this may give useful fine grained information, it is often impractical because each set may contain too few articles to reveal informative or statistically significant differences. The standard solution to field differences in citation counts is to use a field (and year) normalised indicator. The Mean Normalised Citation Score (MNCS), for example, adjusts each citation count by dividing it by the average for the world in its field and year. After this, the arithmetic mean of the normalised citation counts is the MNCS value (Waltman, van Eck, van Leeuwen, Visser, & van Raan, 2011ab). This can reasonably be compared between different collections of articles or against the world average, which is always exactly 1, as long as all articles are classified in a single field. If some articles are in multiple fields then weighting articles and citations with the reciprocal of the number of fields containing the article ensures that the world average is 1 (Waltman et al., 2011a). A limitation of the MNCS is that the arithmetic mean is inappropriate for citation counts and most alternative indicators because they are highly skewed (de Solla Price, 1976; Thelwall & Wilson, 2016). In practice, this means that confidence limits must be calculated with bootstrapping and large sample sizes are needed for accurate results. An alternative approach that solves both of these problems is to switch from the arithmetic mean to the geometric mean because this is suitable for skewed data (Eysenbach, 2011; Fairclough & Thelwall, 2015b; Zitt, 2012). When the geometric mean replaces the arithmetic mean in the MNCS then the resulting geometric MNCS (gMNCS) does not have problems with skewed indicator data and confidence limits can be calculated with a mathematical formula (Thelwall & Sud, 2016). Alternative solutions for confidence intervals include stability intervals (Waltman, Calero-Medina, Kosten, Noyons, Tijssen, et al., 2012), although these use a type of bootstrapping. An alternative solution for the field normalisation issue is to calculate the percentage of articles in the top X% (e.g., 1%, 10% or 50%) of the world separately for each field and year, then appropriately averaging the percentages across fields to give a single value (Schubert & Braun, 1996; Waltman & Schreiber, 2013). This is less precise overall than the geometric mean because it discards some of the citation information for any given percentile (Thelwall, 2016d). Choosing a set of percentiles to report (i.e., percentile rank classes: Bornmann, Leydesdorff, & Mutz, 2013) reduces this problem but generates multiple indicators.  4 A problem with the gMNCS is that the gMNCS world average is not guaranteed to be exactly 1 but can be slightly different. This is an unwanted complicating characteristic when combining the results of multiple fields. A simpler solution, proposed here (a variant of: Lundberg, 2007), is to log normalise citation or indicator data first, and then apply the standard MNCS formula. This allows the world average to be 1 without further calculations and introduces a single additional calculation step (see Appendix A for a worked example). Since citation counts and Mendeley readers for a single field and year typically conform reasonably closely to a discretised lognormal distribution (Fairclough & Thelwall, 2015b; Thelwall, 2016ab), a log transformation should result in data that approximately follow the normal distribution for these two. Although the discrete distribution can differ broadly in shape, it can be expected to be close enough to allow inferences based on the normal distribution assumption (e.g., Thelwall, 2016e). Because of this, it is reasonable to use the arithmetic mean on the log-transformed data and also to use standard formulae from the normal distribution to calculate confidence limits for both citations and Mendeley readers. For other alternative indicator sources if they can be proved to approximately follow the lognormal distribution then it would also be reasonable to use the normal distribution formula. In the case of data with high numbers of zeros, including most web indicators, the normal distribution formulae do not work well (see Appendix B for the Wikipedia citation data used in this paper) and so the assumptions below are not valid for them. Since the logarithm is undefined at 0, the standard solution of adding 1 before applying the function is necessary. The formula for the resulting Mean Normalised Log-transformed Citation Score (MNLCS) for 𝑛 articles with citation counts or alternative indicator values 𝑐1, 𝑐2, … 𝑐𝑛 is: MNLCS = (ln(1+𝑐1)𝑙1+ln(1+𝑐2)𝑙2+ ⋯ln(1+𝑐𝑛)𝑙𝑛)/𝑛 (1a) In the above formula, 𝑙𝑖 is the arithmetic mean of the 𝑙𝑛(1 + 𝑐) log-transformed world set ′ = ln(1 + 𝑐𝑖)/𝑙𝑖 so that a simpler of papers from the same field and year as 𝑐𝑖. Let 𝑐𝑖formulation is the following. ′ )/𝑛 ′ + 𝑐2′ + ⋯ 𝑐𝑛MNLCS = (𝑐1(1b) This is similar to Lundberg’s (2007) item oriented field normalized logarithm-based citation z-score average. Both formulae use the same 𝑙𝑛(1 + 𝑐) transformation and normalise the scores for individual articles with reference to the world average for a field and year. The difference is that Lundberg’s formula normalises with (ln(1 + 𝑐𝑖) − 𝑙𝑖)/𝑠𝑖  instead of ln(1 + 𝑐𝑖)/𝑙𝑖 where 𝑠𝑖 is the standard deviation of the world log-transformed set for the field and year. Lundberg’s formula therefore takes into account the variability in citation rates for individual fields and normalises for this to produce a difference indicator rather than a ratio indicator. 2.1 MNLCS confidence intervals, assuming accurate population data Confidence limits can be calculated with the standard formula, where tn−1,α is the two tailed critical value of Student’s t distribution with 𝑛 − 1 degrees of freedom and confidence ′ , … 𝑐𝑛′ . level 1 − α and 𝑠 is the sample deviation of the log-transformed, normalised set 𝑐1(2a) (2b) For the large sample sizes typical of citation analysis, tn−1,α is approximately 1.96 for a 95% confidence interval. This calculation assumes that the world averages used for each field/year are exact in the sense of being derived from the entire population, rather than MNLCS𝐿 = MNLCS − tn−1,α𝑠/√𝑛 MNLCS𝑈 = MNLCS + tn−1,α𝑠/√𝑛 ′ , 𝑐2        5 from a sample of the population of interest, as discussed further in the paragraph below. If this is believed not to be true, then the methods of the next section can be used to calculate revised confidence limits. In the social sciences it is standard practice to treat populations as being samples for statistical purposes, even though there is not universal agreement on this (Berk, Western, & Weiss, 1995; Bollen, 1995). The logic here is that social processes can never be precisely replicated and can be thought of instead as a sample of all possible outcomes from similar situations (for other justifications, see: Williams & Bornmann, 2016). The sample-based formulae are therefore recommended even when the entire population of articles is available. 2.2 Sampling methods When calculating field normalised indicators, it seems to be standard practice to use the entire collection of articles from a given field and year, as recorded in the Web of Science (WoS) or Scopus, as the reference set for each article. This may be simple in practice for people with access to a complete set of citation counts from one of these databases but can be problematic for others that only have, or that can only afford, online access. Obtaining large sets of alternative indicator data can also be impractical when the values need to be calculated separately for each article rather than purchased in bulk from a data provider. For example, few webometric indicators (Thelwall & Kousha, 2015a) are currently available from data providers. The numbers involved can also be prohibitively large. If a medical funder wishes to evaluate a collection of its articles from the five previous years to identify trends, then these articles could span many different medical and health-related subject categories. The Scopus Medicine broad category includes 916,428 articles from 2015 alone [using the query SUBJAREA (medi) AND PUBYEAR = 2015]. For webometric indicators this number may be too large to be practical because of the time taken to gather the data and the need to pay for automatic Bing queries (see below). The logical solution to this problem is to use sampling. For field normalisation purposes, this means calculating indicators for a random sample of the publications rather than for all of them. As long as the sample is large enough and random, or a close approximation to random, then replacing the full set with a random sample should not affect the results much, otherwise it will increase the size of the confidence intervals. Sampling would reduce precision more if the data were highly skewed, such as for raw citation counts, but this is not a problem here due to the log transformation. If the random sample for the world set has a standard deviation that is not much smaller than its mean, however, then the indicator confidence intervals can be affected more substantially. This is because the world set is used for the denominator of the indicator formula and if numbers close to zero are plausible for the denominator, then very high values are also plausible for the fraction as a whole. This can occur when the majority of the data values are zero, as for most webometric indicators and for citation counts from the current year, and so an alternative formula is needed for this case. This is dealt with in the section below. 2.3 MNLCS confidence intervals, not assuming accurate population data Assuming that the world and group data are approximately normally distributed (see ′ = ln(1 + 𝑐𝑖)/𝑙𝑖 in the MNLCS formula follow the above), the normalised group values 𝑐𝑖ratio of two normal distributions, which is the Cauchy (or Lorenz) distribution. A confidence  6 interval can be calculated for the Cauchy distribution using Fieller’s theorem (Fieller, 1954; Motulsky, 1995, p. 285), although it is likely to be wide or undefined if the standard deviation of the world set is not small compared to the mean (see the discussion in the paragraph above). The formula for the lower and upper confidence limits MLNCS𝐹𝐿 and MLNCS𝐹𝑈 for the ratio of the group and world mean in the numerator and denominator of ′ values is as follows for a single field and year. Here, the confidence interval is the 𝑐𝑖undefined if ℎ > 1. Also 𝑆𝐸𝑔 and 𝑆𝐸𝑤 are the standard errors and 𝑐𝑔̅ and 𝑐𝑤̅̅̅ are the arithmetic means of the normalised citations ln(1 + 𝑐𝑖) for the group and world sets, respectively. 2𝑆𝐸𝑤𝑐𝑤̅̅̅̅)MNLCS(3a) 2𝑆𝐸𝑤𝑐𝑤̅̅̅̅2 √(1 − ℎ)MNLCS𝐹𝐿 =𝑆𝐸MNLCS =MNLCSℎ = (𝑡𝑛1+𝑛2−2,∝2𝑆𝐸𝑔𝑐𝑔̅̅̅2 +1−ℎ− 𝑡𝑛1+𝑛2−2,∝𝑆𝐸MNLCS + 𝑡𝑛1+𝑛2−2,∝𝑆𝐸MNLCS (3d) This method of calculating a confidence interval cannot be used if the data is taken from at least two different field/year sets because then the final ratio is a combination of at least two ratios, whereas the Cauchy distribution is for a single normal distribution in the ratio numerator and a single normal distribution in the denominator. In this case, the following heuristic approach is recommended. 1. Calculate the standard (2) and Fieller (3) versions of the confidence limits for each MNLCS𝐹𝑈 =1−ℎMNLCS(3b) (3c) 1−ℎindividual field/year set MNLCS. 2. Calculate the average width increase between the normal distribution (2) and Cauchy (3) formulae for each individual field/year. A weighted average should be used, with weights proportional to the number of articles in each field/year collection. Separate expansions should be calculated for the left hand side and right hand side of the confidence intervals since these are asymmetric. 3. Apply the standard formula (2) to the combined data and widen it by the average amount calculated as above.  In mathematical notation, the procedure is as follows. Suppose that the set of articles to be analysed 𝐹 = {𝑐′1, 𝑐′2, … 𝑐′𝑛} is partitioned into 𝑘 different field/year subsets, 𝐹1, 𝐹2, … 𝐹𝑘. ′̅̅̅̅ be the arithmetic mean of the normalised values in the For each field/year subset 𝐹𝑗, let 𝑐𝐹𝑗jth subset 𝐹𝑗, and let MLNCS𝐿(𝐹𝑗) and MLNCS𝑈(𝐹𝑗) be the lower and upper 95% confidence let limits calculated using the standard normal distribution formula (2). Similarly, 𝑀𝐿𝑁𝐶𝑆𝐹𝐿(𝐹𝑗) and 𝑀𝐿𝑁𝐶𝑆𝐹𝑈(𝐹𝑗) be the lower and upper 95% confidence limits calculated using Fieller’s method (3). Then the weighted average expansion rate from (2) to (3) can be calculated as follows. 𝐴𝑣𝑒𝐸𝑥𝑝𝐿(𝐹1, … 𝐹𝑘) =1𝑛∑ |𝐹𝑗|𝑗=1,..𝑘(MNLCS𝐿(𝐹𝑗) − MNLCS𝐹𝐿(𝐹𝑗)) /(𝑐𝐹𝑗′̅̅̅̅ − MNLCS𝐿(𝐹𝑗)) 𝐴𝑣𝑒𝐸𝑥𝑝𝑈(𝐹1, … 𝐹𝑘) =(4a) ′̅̅̅̅)∑ |𝐹𝑗| (𝑀𝑁𝐿𝐶𝑆𝐹𝑈(𝐹𝑗) − MNLCS𝑈(𝐹𝑗)) /(MNLCS𝑈(𝐹𝑗) − 𝑐𝐹𝑗𝑗=1,..𝑘1𝑛Suppose now that the standard formula (2) is applied to the combined set 𝑐1arithmetic mean  𝑐′̅ = MNLCS giving a single set of 95% (4b) ′ , 𝑐2′ , with limits MNLCS𝐿(𝐹) and ′ , … 𝑐𝑛        7 MLNCS𝑈(𝐹). Then the estimated MNLCS confidence limits can be calculated by widening these limits by the above weighted average. MNLCS𝐸𝐿(𝐹) = MNLC𝑆𝐿(𝐹) − (𝐴𝑣𝑒𝐸𝑥𝑝𝐿(𝐹1, … 𝐹𝑘) + 1)( 𝑐′̅ − MNLCS𝐿(𝐹)) 𝑀𝑁𝐿𝐶𝑆𝐸𝑈(𝐹) = 𝑀𝑁𝐿𝐶𝑆𝑈(𝐹) + (𝐴𝑣𝑒𝐸𝑥𝑝𝑈(𝐹1, … 𝐹𝑘) + 1)(𝑀𝑁𝐿𝐶𝑆𝑈(𝐹) − 𝑐′̅ )  (5b) In summary, Fieller’s theorem can be used to generate confidence intervals for MNLCS values for individual field/year sets when samples are taken, and when multiple fields/years are involved then a heuristic formula (5) can be used to estimate the confidence limits. An alternative solution is to use a statistical bootstrapping approach to generate confidence intervals. When samples are taken, the world set should be bootstrapped as well as the group sets. (5a) 3 Equalised Mean-based Normalised Proportion Cited If a data set for an indicator includes many zeros (i.e., uncited articles or articles with an alternative indicator score of 0) then, as argued above, the MNLCS confidence limits may be wide or undefined. A possible solution to this would be to remove all journals with too many zeros (Bornmann & Haunschild, 2016a) but this produces a biased subset of journals and may reduce the sample sizes substantially if there is a high overall proportion of zeros. A different approach is to calculate the proportion of articles that are cited or the proportion that have a non-zero indicator score. Focusing on the former case for terminological convenience, it could also be helpful to calculate a single combined proportion for each group. Changing mathematical notation from the previous section, suppose that a group 𝑔 publishes 𝑛𝑔𝑓 articles in field/year 𝑓 and 𝑠𝑔𝑓 of them are cited. Suppose also that 𝐹 is the set of field/year combinations in which the group publishes. Similarly, let 𝑛𝑤𝑓 be the number of articles that the world publishes in field/year 𝑓, with 𝑠𝑤𝑓 of them being cited. The overall proportion of 𝑔’s articles that are cited is then the number of cited articles divided by the total number of articles. 𝑝𝑔 = ∑⁄∑(6) If there are field/year differences in the proportion of cited articles and 𝑔 publishes different numbers of articles in each field/year then this proportion could be misleading. This is because if 𝑔 publishes more articles than average in fields with high proportions of cited articles then 𝑝𝑔 will tend to be larger. 𝑛𝑔𝑓𝑠𝑔𝑓𝑓∈𝐹𝑓∈𝐹For example, suppose that there are two medical humanities research groups, A and B, that produce research that is exactly average for the fields in which they publish, Medicine and Humanities. Within Medicine, 80% of the world’s articles are cited and within Humanities, 20% of the world’s articles are cited. Since Group A and Group B both produce sets of publications that are exactly average, 80% of both groups’ Medicine articles are cited and 20% of both groups’ Humanities articles are cited. The only difference between the two groups is that Group A publishes 200 articles in Medicine and 100 in Humanities, whereas Group B publishes 100 articles in Medicine and 200 articles in Humanities. Now the is (0.8 × 200 × 0.2 × 100)/(100 + 200) =proportion of articles cited for group A 180/300 or 60%. In contrast, the proportion of articles for group B is 0.8 × 100 × 0.2 ×200)/(200 + 100) = 120/300 or 40%. Thus, Group A appears to be substantially better than Group B even though they both publish average research for their fields. The reason for this anomaly is that Group A publishes more research in the high citation field.      8 This problem can be avoided by artificially treating 𝑔 as having the same number of articles in each field/year combination, fixing this to be the arithmetic mean of numbers in each field/year. For this calculation, fields in which a group has published only a few articles should be excluded because these articles will have their importance inflated, although not in a biasing way if the group has the same citation differential in each field. As an initial heuristic, each field/year combination used should have at least 100 articles and should not be less than 25% of the mean. A more systematic and evidence-based approach is needed for this decision, however, based upon estimating the extent of additional variation introduced into the system for different group sizes and data distribution parameters. Thus, for group 𝑔, the proportion of cited articles in field 𝑓 is still 𝑠𝑔𝑓 𝑛𝑔𝑓⁄ but when /|𝐹|. Thus, the the different proportions are combined, 𝑛𝑔𝑓 is replaced by 𝑛̂𝑔 = ∑equalised group sample proportion 𝑝̂𝑔 is the simple average of the proportions in each set, as follows: 𝑛𝑔𝑓𝑓∈𝐹∑𝑓∈𝐹𝑛̂𝑔𝑠𝑔𝑓𝑛𝑔𝑓𝑛̂𝑔𝑛̂𝑔 ∑𝑓∈𝐹=∑𝑓∈𝐹𝑠𝑔𝑓𝑛𝑔𝑓𝑛̂𝑔∑𝑓∈𝐹𝑠𝑔𝑓𝑛𝑔𝑓⌈𝐹⌉=(7a) ∑𝑓∈𝐹𝑝̂𝑔 =The equalised world sample proportion has a similar formula. ∑𝑓∈𝐹𝑠𝑤𝑓𝑛𝑤𝑓⌈𝐹⌉𝑝̂𝑤 =The equalised group sample proportion has the disadvantage that it treats 𝑔 as if the average impact of its articles did not vary between fields. If 𝑔 had published more articles in fields/years in which it created higher impact articles (or at least more likely to be cited relative to the world average), then the equalised sample proportion 𝑝̂𝑔 would be unfairly low. If, however, the assumption is made that 𝑔’s articles had the same level of impact relative to the world average in every field/year then the calculation would be an unbiased estimator. An alternative to this simplifying assumption is discussed in the section below. (7b) Continuing the above example, Group A and Group B will both be treated as having 150 articles in each of Medicine and Humanities. In both cases the equalised group sample proportion is the same: (0.8 × 150 × 0.2 × 150)/(150 + 150) = 150/300 or 50%. Thus Group A no longer gets an advantage for publishing more in a high citation specialism. Because of the way in which the equalised group proportions are calculated, under the above assumption it is fair to compare 𝑝̂𝑔 between groups and also against the world average 𝑝̂𝑤. Confidence limits for these proportions can be calculated using Wilson’s score interval (Wilson, 1927). These intervals would be optimistic if any of the group sample sizes 𝑛𝑔𝑓 are much smaller than the others because their sampling deviations from the population proportion can be relatively large and exaggerated by the equalisation formula. For this reason, it is recommended to remove groups from the calculation when their sample sizes are small relative to the others. The world Equalised Mean-based Normalised Proportion Cited (EMNPC) for each group 𝑔 is the ratio of the equalised sample proportions. EMNPC = 𝑝̂𝑔/𝑝̂𝑤 (8) Now  𝑝̂𝑔/𝑝̂𝑤 > 1 implies that 𝑔 has a greater proportion of cited articles than the world average. This mirrors the situation for existing field normalised citation indicators, such as MNLCS, gMNCS and MNCS.        9 3.1 EMNPC confidence intervals The ratio of two proportions is known as a risk ratio, and there are standard techniques for calculating lower EMNPC𝐿 and upper EMNPC𝑈 confidence limits in this case (Bailey, 1987). These assume that both the group and world sets are samples rather than populations. A continuity correction of 0.5 may be added to all 𝑛𝑝 terms. (𝑛𝑔−𝑝̂𝑔𝑛𝑔)/(𝑝̂𝑔𝑛𝑔) 𝑝̂𝑔𝑝̂𝑤𝑛𝑔(𝑛𝑤−𝑝̂𝑤𝑛𝑤)/(𝑝̂𝑤𝑛𝑤)𝑛𝑤EMNPC𝐿 = exp (ln () − 1.96√(9a) +) EMNPC𝑈 = exp (ln (𝑝̂𝑔𝑝̂𝑤) + 1.96√(𝑛𝑔−𝑝̂𝑔𝑛𝑔)/(𝑝̂𝑔𝑛𝑔)𝑛𝑔+(𝑛𝑤−𝑝̂𝑤𝑛𝑤)/(𝑝̂𝑤𝑛𝑤)𝑛𝑤) (9b) Here, 𝑛𝑔 and 𝑛𝑤 are the combined group and world sample sizes, respectively, so that 𝑝̂𝑔𝑛𝑔 and 𝑝̂𝑤𝑛𝑤 in the formula are the number of group and world articles cited. A continuity correction (adding 0.5 to the number of cited articles for both the group and the world classes for the confidence interval width calculations) should be included in case the number of uncited articles is very small. These confidence intervals seem to be only approximations, however, as they can differ from bootstrapping estimates (Appendix C). 4 Mean-based Normalised Proportion Cited As suggested by an anonymous referee, an alternative approach for calculating the proportion of articles cited is to echo the MNCS and, for each article, replace its citation count by the reciprocal of the world proportion cited for the field and year, if the citation count is positive, otherwise 0. Let 𝑝𝑔𝑓 = 𝑠𝑔𝑓 𝑛𝑔𝑓⁄ be the proportion of articles cited for group 𝑔 in field/year 𝑓 and let  𝑝𝑤𝑓 = 𝑠𝑤𝑓 𝑛𝑤𝑓 be the proportion of the world’s articles cited in field/year 𝑓. Then ⁄𝑟𝑖 = {01/𝑝𝑤𝑓𝑖𝑓 𝑐𝑖 = 0𝑖𝑓 𝑐𝑖 > 0 𝑤ℎ𝑒𝑟𝑒 𝑎𝑟𝑡𝑖𝑐𝑙𝑒 𝑖 𝑖𝑠 𝑓𝑟𝑜𝑚 𝑓𝑖𝑒𝑙𝑑 𝑓 (10) Following the MNCS approach, these zeros and reciprocals can now be averaged. This gives the same result as setting all citation counts that are greater than 1 to 1 and then applying the MNCS formula to the transformed binary data. This binary MNCS formula, or Mean-based Normalised Proportion Cited (MNPC), for the 𝑛𝑔 articles from group 𝑔 is therefore the arithmetic mean of the individual article values: (11) This can be simplified to a weighted sum of the group ratio cited 𝑝𝑔𝑓 to the world ratio cited 𝑝𝑤𝑓 for all field/year combinations. MNPC = (𝑟1 + 𝑟2 + ⋯ 𝑟𝑛)/𝑛𝑔 MNPC = ∑𝑓∈𝐹𝑛𝑔𝑓𝑛𝑔×𝑝𝑔𝑓𝑝𝑤𝑓(12) There is a simple way to contrast MNPC and EMNPC, since (12) expresses MNPC as a sum of ratios, whereas from (8) EMNPC can be expressed as a ratio of sums: 𝑓∈𝐹)  𝑝𝑔𝑓𝑝𝑤𝑓)/(∑ EMNPC = (∑(13) 𝑓∈𝐹This mirrors the difference between the new and old crown indicators (Waltman et al., 2011a) because the old crown indicator and EMNPC are ratios of sums whereas the new crown indicator and MNPC are sums of ratios. The old crown indicator was replaced by the new crown indicator partly because the old crown indicator effectively gave more influence to fields with higher average citation rates (Lundberg, 2007; Opthof & Leydesdorff, 2010; van Raan, van Leeuwen, Visser, van Eck, & Waltman, 2010). This occurred because high citation fields could numerically dominate both the (single) numerator and (single) denominator of the old crown indicator. In the ratio of sums format above (13), it is clear        10 that EMNPC inherits this disadvantage of the old crown indicator. For example, it gives an advantage to research groups that are particularly successful in fields with a high proportion of cited articles, because the other fields have less numerical influence in the EMNPC calculation. Suppose that groups A and B publish equal numbers of articles in fields X and Y with world average proportions cited 0.8 and 0.2, respectively. Suppose that A has 10% more articles cited than the world average in field X (i.e., 0.8x1.1=0.88) and 10% less in field Y (i.e., 0.18), whereas B has the opposite (0.72 cited in X but 0.22 in Y). Then the denominator for both is the same (1) and EMNPC for A is 0.88+0.18=1.06 whereas EMNPC for B is 0.94). In contrast MNPC for both is 1, which is fairer. Thus MNPC is intrinsically fairer than EMNPC, at least from a theoretical perspective. The ratio of sums comparison (12, 13) also highlights the stability advantage of EMNPC for which it was created because a zero EMNPC denominator (13) is only possible if the denominators are zero for all fields. In contrast, the MNPC has a zero denominator (12) if there is a zero in any of the fields. This could be solved with the same approach as the new crown indicator by replacing the ratio 0/0 by 1 (Waltman et al., 2011a). No reasonable confidence interval could be calculated for this, however. When the denominator for a field includes the entire population then it is impossible to get a positive numerator and a 0 denominator since the numerator is based on a subset of the articles used for the denominator. When the MNPC denominator for a field is based on a sample rather than the whole set of articles published in the year and field then it is possible to get a positive numerator and 0 denominator (e.g., 1/0 and this occurs for the case study below) because the group set is not a subset of the world sample. It would not be reasonable to replace this infinite quantity with 1 and so MNPC could not be calculated without removing the field or replacing it with 1, both of which would be unfair because the group has a higher proportion cited than the world average. This makes MNPC intrinsically more fragile than EMNPC in the sense of either sometimes being undefined or unfair (when the sampling approach is used, which is likely to be standard practice in practical webometric applications) or confidence intervals to estimate its value being likely to be wider or undefined (when the sampling approach is not used, or when the sampling approach is used but no infinite ratio occurs). This advantage is more important for the EMNPC/MNPC context than for the new/old crown indicator context because MNPC and EMNPC were introduced to deal with the situation in which very low proportions of articles are cited and sampling is needed. Thus the fragility of MNPC can make it impractical for webometric indicators with a low proportion of cited articles. 4.1 MNPC confidence intervals There is not a simple formula for the MNPC confidence interval, but an approximate confidence interval can be constructed by taking the weighted sum of the confidence intervals for the individual risk ratios 𝑝𝑔𝑓/𝑝𝑤𝑓 using the standard formula (Bailey, 1987). More specifically, if MNP𝐶𝑓𝐿 is the lower limit for group 𝑔 and if MNPC𝑓𝑈 is the upper limit for group 𝑔 using the standard formula (Bailey, 1987) for field 𝑓 (see Appendix D), then the overall limits are: MNPC𝐿 = MNPC − ∑𝑓∈𝐹MNPC𝑈 = MNPC + ∑𝑓∈𝐹𝑛𝑔𝑓𝑛𝑔𝑛𝑔𝑓𝑛𝑔(𝑝𝑔𝑓𝑝𝑤𝑓− MNPC𝑓𝐿)(MNPC𝑓𝑈 −𝑝𝑔𝑓𝑝𝑤𝑓)(14a) (14b)    11 A disadvantage of this formula is that if any of the constituent confidence intervals are large, then the overall confidence interval is also likely to be large, whereas this is not true for EMNPC since it comprises a single overall ratio. Moreover, if any of the world proportions are zero then the formula cannot be calculated unless the corresponding field is removed from all data, whereas EMNPC can always be calculated unless all world proportions are zero. In contrast to the EMNPC case, removing problematic sets is likely to bias the results since they would be removed for the potential to have very high or infinite values. These confidence intervals are only approximations, however, and can differ substantially from bootstrapping estimates (Appendix C). 5 Research Questions This article primarily introduces a strategy for calculating field normalisation formulae and associated confidence intervals. The following research questions are designed to demonstrate the indicators rather than to give conclusive evidence of their value.  RQ1: Are MNLCS, MNPC and EMNPC practical in sense of being able to distinguish between different groups?  RQ2: Are EMNPC and MNPC preferable to MNLCS when the proportion of cited articles is low? 6 Data and Methods Large medical research funders were selected to test the new formulae because these are important users of citation analysis and they fund research within a relatively narrow area. The National Institutes of Health (NIH) conducts and funds biomedical research in the U.S.A. The U.K. equivalent is the Medical Research Council (MRC). The U.K.-based Wellcome Trust biomedical research charity is the largest similar non-government research funder. All are based in advanced English-speaking nations and have similar remits and so are broadly comparable. The three Scopus broad categories with the most articles overall for the three medical funders were chosen for the analysis: Medicine (MEDI); Biochemistry, Genetics and Molecular Biology (BIOC); and Immunology and Microbiology (IMMU). These three categories seem core to the work of the funders whereas other categories in which they have articles suggest a more peripheral contribution overall (e.g., Agricultural and Biological Sciences; Neuroscience). Any subject category is necessarily an oversimplification in the context of overlapping and evolving fields, as well as multidisciplinary articles. Alternative approaches have been proposed for categorising individual articles, including with the use of Mendeley data (Bornmann & Haunschild, 2016a; Haunschild & Bornmann, 2016). Articles were analysed from 2013 to 2016 (the year of data collection) to allow an analysis of recent data and to assess the influence of time. The inclusion of recently published articles is important for many evaluations because recent research is likely to be the most relevant for practical applications and is important for web indicators (Priem, Taraborelli, Groth, & Neylon, 2010). Nevertheless, articles that were published early in a year have had relatively long to attract attention compared to articles published later in the year and this difference is most substantial for recent years. Hence, this can introduce a biasing factor if one of the groups analysed has published disproportionately few or many articles early or late in the year. Note also that for many scientometric evaluations, it is important to insist on only analysing articles that are old enough to have attracted sufficient 12 citations to estimate their likely long term impact. Many bibliometricians recommend using citation windows of at least 3 years for useful results. Thus, the Scopus results for 2015 and 2016 are not relevant for many scientometric purposes. For each year and category, all articles funded by each organisation were extracted from Scopus by searching for the funder name in the Scopus funder record. For each year and category, approximately 10,000 Scopus articles were also selected (the first and last 5000 published in each year) to form the world reference set (Tables 1-3). This is the maximum number of records that can be downloaded by querying Scopus for a complete list from a category. It is not a random sample but is balanced in terms of time and should not result in any systematic bias towards groups unless they tend to publish with a different temporal pattern than average for academia. Funding information is incomplete and sometimes incorrect in Scopus (Sirtes, 2013). The figures below may therefore cover only about a third of the journal articles funded by each source but since the information was collected in the same way for each organisation, this seems adequate for the purposes of comparing the methods. The three funders presumably have more complete lists of publications produced by their own databases and additional ad-hoc methods but, for testing the methods, gathering information in an identical way from Scopus seems to be reasonable. Assuming that the sample covers a third of the Scopus articles from each organisation then the confidence intervals would probably be √3 times narrower. For the calculations, papers that occurred in multiple subject categories were counted as whole articles for all purposes. Thus, an article from any of the funders or the world set could have been included up to three times, once for each category. Although it might have given better results to weight each article by the reciprocal of the number of categories containing it (e.g., Waltman et al., 2012), this was not done here in order to keep the technique as simple as possible. Nevertheless, the fractional counting method should be used if it is clear that one group publishes its best (or worst) work in an unusually high proportion of multiply-classified (or single classified) fields. Table 1. Descriptive statistics for the Scopus citation data. Each set contains three fields: Medicine; Biochemistry, Genetics and Molecular Biology; and Immunology and Microbiology. Sample Mean Nonzero 2013 29928 6.0 71% 1695 14.2 94% 22571 13.1 95% 1950 14.4 96% 2014 29952 3.3 72% 3410 8.9 90% 24447 7.7 91% 2208 11.1 91% 2015 29875 1.1 42% 2231 3.4 75% 23489 2.7 67% 1363 3.7 72% 2016 29950 0.20 10% 783 0.36 21% 14212 0.30 19% 535 0.30 19% World MRC NIH Wellcome   13 Table 2. Descriptive statistics for the Mendeley readership data. Each set contains three fields: Medicine; Biochemistry, Genetics and Molecular Biology; and Immunology and Microbiology. Sample Mean Nonzero 2013 29928 12.4 69% 1695 30.8 96% 22571 26.0 95% 1950 33.8 97% 2014 29952 10.5 77% 3410 27.3 95% 24447 22.0 96% 2208 34.9 96% 2015 29875 7.0 70% 2231 19.4 91% 23489 14.6 92% 1363 21.7 95% 2016 29950 2.8 55% 783 8.7 81% 14212 7.1 79% 535 9.3 80% World MRC NIH Wellcome Table 3. Descriptive statistics for the Wikipedia URL count data. Each set contains three fields: Medicine; Biochemistry, Genetics and Molecular Biology; and Immunology and Microbiology. When the MEDI set is excluded from 2016, as needed for some formulae, the sample sizes are: World 1000; MRC 690; NIH 1000; Wellcome 470. Sample Mean Nonzero 2013 1500 0.025 2.0% 1150 0.036 2.4% 1500 0.039 2.6% 1211 0.057 3.5% 2014 1500 0.242 2.1% 1253 0.033 2.4% 1500 0.033 2.1% 1186 0.066 5.0% 2015 1500 0.009 0.6% 1190 0.016 1.3% 1500 0.028 2.0% 1084 0.030 2.0% 2016 1500 0.003 0.4% 768 0.025 0.7% 1500 0.011 0.8% 532 0.009 0.6% World MRC NIH Wellcome MNLCS, MNPC and EMNPC values and confidence intervals were calculated for each field and year using the above formulae (1,5,8,9,11,14). The same calculations were repeated for the Mendeley reader counts, as extracted by Webometric Analyst from the free Mendeley API, and for the Wikipedia citation counts, as extracted by Webometric Analyst via Bing API searches. Here, a Mendeley reader is a user of the social reference sharing site Mendeley (Gunn, 2013) that has registered the article within their library. Such users typically have already read, or intend to read, these articles (Mohammadi, Thelwall, & Kousha, 2016). Mendeley reader counts tend to reflect academic impact (Li, Thelwall, & Giustini, 2012;  14 Mohammadi, Thelwall, Haustein, & Larivière, 2015) and appear earlier than citation counts (Thelwall & Sud, in press). The Wikipedia citation count is the number of pages in Wikipedia that cite a given paper and reflects to some degree that the paper is transmitting knowledge to a wider public via the encyclopaedia (Kousha & Thelwall, in press-a; Rainie & Tancer, 2007). It is included as an example of a relevant webometric indicator. The Wikipedia citation count for each article was obtained by submitting a standardised Bing Wikipedia site-specific query for the article by name, journal name, publication year and author name, as in the following example. Mendoza Villanueva Vargas "Vitamin D deficiency among medical residents" "Endocrine Practice" 2013 site:wikipedia.org/wiki/ Since large scale automated Bing API queries are not free, a random sample of size 500 was taken for each group and world set in order to limit the data cost. This number was selected heuristically to be large enough to give a reasonable chance of detecting differences between groups. To assess the impact of this restricted sample size, a second data set for Wikipedia was constructed by expanding the world reference sets to 5000 each. The world reference sets have the largest influence on the MNLCS confidence limits and are therefore the logical first choice for expansion. The data for this article was extracted approximately half way through 2016. Scopus citation counts were extracted on 29 June 2016, Mendeley reader counts were downloaded between 29 June and 1 July 2016, and the Wikipedia citation count searches were conducted on 30 June 2016 for the main set and additional data for the expanded world set was collected on 16-17 July 2016. 7 Results This section describes the results from the perspective of evaluating the funders and the research questions are returned to in the discussion. Here, graphs with confidence intervals are reported rather than hypothesis tests because these reveal effect sizes (Cumming, 2012) as well as being suitable for situations were multiple comparisons are possible. For the MNLCS, the effect size is in terms of the ratio of the average logged citations per article for a group to the world average. For EMNPC and MNPC, the effect size is in terms of the ratio of the average proportion of cited articles for a group to the world average. In both cases there is not a natural choice about how large the differences should be in order to count as substantial enough to be of interest and therefore the focus here is simply on whether the differences are large enough to be unlikely to be a result of chance factors. 7.1 MNLCS: World field/year normalised average Each organisation tends to fund research that is more highly cited than the world average for the field and year (Figure 1). Both Wellcome and MRC also tend to fund research that is more highly cited than that of NIH, although the difference is not statistically significant for the most recent year, 2016.  15 Figure 1. MNLCS for Scopus citation counts for articles funded by MRC, NIH and Wellcome in the Scopus categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap. The data is from 29 June 2016. Each organisation tends to fund research that is more read by Mendeley users than the world average for the field or year (Figure 2). Both Wellcome and MRC also tend to fund research that is more read than that of NIH. Wellcome-funded research has significantly more readers than that of MRC in both 2014 and 2015, but they have similar numbers of readers in 2016 and the difference is at the margins of statistical significance in 2013.    16 Figure 2. MNLCS for Mendeley reader counts for articles funded by MRC, NIH and Wellcome in the categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap. The data is from 29 June- 1 July 2016. Each organisation tends to fund research that is more cited in Wikipedia than the world average in 2013 and Wellcome also in 2014 (Figure 3), suggesting that they all fund research that helps transfer knowledge to a wider public via Wikipedia. The same may be true for articles published in 2016 but the evidence is not statistically significant. Figure 3. MNLCS for Wikipedia citation counts for articles funded by MRC, NIH and Wellcome in the categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap.  A maximum of 500 articles were randomly sampled from   17 each field/year/group combination. The 2016 MEDI category was excluded because the world set contained no matches and so field normalisation was not possible for it. Confidence limits are infinite for 2015 and 2016. The data is from 29 July 2016. The MEDI set from 2016 is excluded from the data due to a world average of 0. Confidence intervals are only approximate and are likely to be optimistic overall (see Appendix B). The influence of the relatively small world sample can be seen from the much wider approximate confidence intervals in Figure 3 compared to Figure 4, which uses the same Wellcome, MRC and NIH data, but uses an expanded random sample of 5000 world articles for each set. The rank order between the three funders is different in 2016 because the 2016 MEDI category was excluded from the 500 article set due to the world set in this category for Figure 3 containing no matches. Figure 4. MNLCS for Wikipedia citation counts for articles funded by MRC, NIH and Wellcome in the categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap. A maximum of 500 articles were randomly sampled from each field/year/group combination, except for the world sets, for which a maximum of 5000 articles were randomly sampled. The group sets use the same data as for the previous figure. The group data is from 29 July 2016 and the world data is from 16-17 July 2016. Confidence intervals are only approximate and are likely to be optimistic overall (see Appendix B). The confidence intervals for MNLCS values for each funder, using (1) and (5) on the combination of all three fields and four years, do not overlap when the data is Scopus citations or Mendeley readers (Table 4). When all the group/field combinations are restricted to 500 articles then the Wikipedia confidence intervals cannot be calculated because the variability of the world average is too large for some article sets (in 2015 and 2016). For the larger world sample size of 5000 articles per set (but a maximum of 500   18 articles for the group), the approximate confidence intervals can be calculated but are relatively wide (and optimistic: see Appendix B). Nevertheless, the intervals are narrow enough to exclude 1 for NIH and Wellcome, suggesting that both attract more Wikipedia citations overall than the world average. This cannot be confirmed because of the problems discussed in Appendix B. The MNLCS confidence intervals for the world set are provided to show the stability of this figure (Table 4), even though it is, by definition always exactly 1. A wide confidence interval for a world set is likely to lead to wide confidence intervals for all group MNLCS, because world set instability translates to denominator instability in the MNLCS calculations for all groups. Table 4. MNLCS for 2013-2016 and BIOC, IMMU and MEDI combined, together with sample 95% confidence intervals. The MEDI set from 2016 is excluded from the data due to a world average of 0. The MEDI set from 2016 is excluded from the Wiki 500 data due to a world average of 0. Wikipedia confidence intervals are only approximate and are likely to be optimistic overall (see Appendix B). Group N World 119693 MRC NIH 8107 84707 Wellcome 6044 Scopus MNLCS Mendeley MNLCS 1.000 (0.985, 1.016) 2.004 (1.954, 2.054) 1.840 (1.813, 1.867) 1.968 (1.916, 2.021) 1.000 (0.993, 1.007) 1.811 (1.790, 1.833) 1.631 (1.620, 1.643) 1.900 (1.876, 1.924) Wiki 500 MNLCS 1.000 (-,-) 1.898 (-,-) 2.521 (-,-) 3.290 (-,-) Wiki 500/5k MNLCS 1.000 (0.871, 1.230) 1.606 (0.954, 2.489) 1.627 (1.190, 2.249) 2.272 (1.817, 2.907) 7.2 EMNPC: World field/year normalised proportion of cited articles For Scopus citations, the three funders all have proportions of cited articles that are substantially and statistically significantly above the world average for all four years (Figure 5). Although there are differences between them in EMNPC values, these are not large except for 2015 and 2016. This is because, for all three funders, a similarly high proportion of articles had attracted citations after two years. Hence EMNPC is less discriminatory than MNLCS for older articles.   19 Figure 5. EMNPC for Scopus citation for articles funded by MRC, NIH and Wellcome in the Scopus categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap. The data is from 29 July 2016. Confidence intervals are approximate and may be slightly too wide (see Appendix C). For Mendeley readers (Figure 6), the situation is very similar to that for Scopus citations. The main difference is that the groups are virtually indistinguishable in 2015 and there are only small differences between them in 2016. This is consistent with Mendeley readers appearing earlier than Scopus citations and so a shorter time is needed for a high proportion of each group’s papers to have attracted at least one reader.   20 Figure 6. EMNPC for Mendeley readers for articles funded by MRC, NIH and Wellcome in the Scopus categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap. The data is from 29 June- 1 July 2016. Confidence intervals are approximate and may be slightly too wide (see Appendix C). EMNPC values are unable to distinguish between the groups and the world average for a small majority of individual years on the Wikipedia citations 500/500 data set (Figure 7), although there are five exceptions out of 12 (Wellcome 2013-2015; NIH 2015; MRC 2016). The confidence limits for Wellcome in 2014 are also narrow enough to distinguish it from both NIH and MRC.   21 Figure 7. EMNPC for Wikipedia citations for articles funded by MRC, NIH and Wellcome in the Scopus categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap. A maximum of 500 articles were randomly sampled from each field/year/group combination. The data is from 30 June 2016. The MEDI set from 2016 is excluded due to a world average of 0. Confidence intervals are approximate and may tend to be too wide (see Appendix C). EMNPC values are mostly able to distinguish between the groups and the world average for individual years on the Wikipedia citations 500/5000 data set, although there are two exceptions out of 12 (MRC 2013; Wellcome 2016). The confidence limits for Wellcome are also narrow enough to distinguish it from MRC in 2013, and from both NIH and MRC in 2014. Unsurprisingly, therefore, the expansion of the world data set from 500 to 5000 increased the EMNPC stability.  22 Figure 8. EMNPC for Wikipedia citations for articles funded by MRC, NIH and Wellcome in the Scopus categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap. A maximum of 500 articles were randomly sampled from each field/year/group combination, except for the world sets, for which a maximum of 5000 articles were randomly sampled. The group data is from 30 June 2016 and the world data is from 16-17 July 2016. Confidence intervals are approximate and may tend to be too wide (see Appendix C). Combining all fields and years into a single EMNPC calculation (8 and 9), the ratio confidence intervals for each group (Table 5) do not overlap with those of the world for Scopus citations and for Mendeley readers. The confidence intervals do not contain 1 for two of the three groups for Wikipedia citations, even when there is a maximum of 500 articles per set, and the confidence intervals do not overlap when the world set is expanded to 5000 articles. Thus, overall, EMNPC seems to be more powerful than MNLCS for smaller samples. For reference, Table 6 shows the original proportions before world normalisation. Statistically significant differences are also evident here for Wikipedia citations even though only 1% of the world’s articles had received any.    23 Table 5. EMNPC (sample equalised world normalised proportion of cited articles) for 2013-2016 and BIOC, IMMU and MEDI combined, together with 95% confidence intervals. Wiki 500/5k EMNPC Mendeley EMNPC Wiki 500 EMNPC All N World 119693 MRC NIH 8107 84707 Wellcome 6044 Scopus EMNPC 1.000 (0.992, 1.008) 1.463 (1.441,1.485) 1.419 (1.408, 1.429) 1.443 (1.418,1.468) 1.000 (0.995, 1.006) 1.329 (1.318, 1.340) 1.348 (1.343, 1.354) 1.347 (1.336, 1.359) 1.000 (0.728, 1.373) 1.350 (0.982, 1.856) 1.373 (1.023, 1.844) 2.043 (1.522, 2.741) 1.000 (0.895, 1.118) 1.649 (1.299, 2.095) 1.678 (1.364, 2.062) 2.495 (2.030, 3.066) Table 6. Sample equalised proportion of cited articles for 2013-2016 and BIOC, IMMU and MEDI combined, together with Wilson’s score interval 95% confidence intervals. All Mendeley > 0 Wiki 500 > 0 Scopus > 0 N World 119693 MRC NIH 8107 84707 Wellcome 6044 0.487 (0.484, 0.490) 0.713 (0.703, 0.722) 0.691 (0.688, 0.694) 0.703 (0.691,0.714) 0.679 (0.677, 0.682) 0.903 (0.896,0.909) 0.916 (0.914, 0.918) 0.915 (0.908, 0.922) 0.013 (0.010, 0.016) 0.017 (0.013, 0.021) 0.017 (0.014, 0.021) 0.026 (0.021, 0.031) Wiki 500/5k > 0 0.010 (0.009, 0.011) 0.017 (0.013, 0.021) 0.017 (0.014, 0.021) 0.026 (0.021, 0.031) 7.3 MNPC For Scopus, the MNPC graph (Figure 9) has a slightly different shape to the EMNPC graph (Figure 5) for the Wellcome line due to the different averaging mechanisms, but otherwise the graphs are similar. The confidence intervals tend to be wider for MNPC graphs, although the difference is not large. For example, in 2015, Wellcome is indistinguishable from the other two in Figure 9 but not in Figure 5.   24 Figure 9. MNPC for Scopus citation for articles funded by MRC, NIH and Wellcome in the Scopus categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap. The data is from 29 July 2016. Confidence intervals are approximate and may tend to be too wide (see Appendix C). For Mendeley (Figure 10), the overall shape is similar but not identical to that of the corresponding EMNPC graph (Figure 6) and the MNPC confidence intervals are again wider. Figure 10. MNPC for Mendeley readers for articles funded by MRC, NIH and Wellcome in the Scopus categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error   25 bars do not overlap. The data is from 29 June- 1 July 2016. Confidence intervals are approximate and may tend to be too narrow (see Appendix C). For the Wikipedia citations 500/500 data set (Figure 11), the overall shape diverges from that of the corresponding EMNPC graph (Figure 7) in 2016 partly due to omitted MEDI data set for 2016 because of the divide by zero in the world set. The MNPC confidence intervals are again wider and as a result some of the differences between groups are not statistically significant in Figure 11, despite being statistically significant in Figure 7. Figure 11. MNPC for Wikipedia citations for articles funded by MRC, NIH and Wellcome in the Scopus categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap. A maximum of 500 articles were randomly sampled from each field/year/group combination. The data is from 30 June 2016. The MEDI set from 2016 is excluded due to a world average of 0. Confidence intervals are approximate and may tend to be too narrow (see Appendix C). For the Wikipedia citations 500/5000 data set (Figure 12), the Wellcome line diverges from that of the corresponding EMNPC graph (Figure 8) in 2016 due to the wider data variability. The MNPC confidence intervals are also mostly wider.    26 Figure 12. MNPC for Wikipedia citations for articles funded by MRC, NIH and Wellcome in the Scopus categories BIOC, IMMU and MEDI. Funder lines are horizontally offset so that error bars do not overlap. A maximum of 500 articles were randomly sampled from each field/year/group combination, except for the world sets, for which a maximum of 5000 articles were randomly sampled. The group data is from 30 June 2016 and the world data is from 16-17 July 2016. Confidence intervals are approximate and may tend to be too narrow (see Appendix C). Combining all fields and years the confidence intervals tend to be wider for MNPC (Table 7) than those for EMNPC (Table 5). Whilst the differences are moderate for Scopus and Mendeley, they are large for the two Wikipedia data sets and result in three additional cases of group confidence intervals containing the world average (1). These three cases are NIH (both Wikipedia data sets) and MRC for the Wikipedia citations 500/5000 data set. Thus, overall, MNPC is inferior to EMNPC in terms of stability. The difference is particularly substantial for the Wikipedia data sets, presumably due to the high proportion of zeros in them. This conclusion is specific to the data sets analysed and subject to the hypothesis that the underlying advantage for each group is the same for all fields.    27 Table 7. MNPC for 2013-2016 and BIOC, IMMU and MEDI combined, together with 95% confidence intervals. A dash indicates that the calculation includes a divide by zero because the world proportion cited is zero for at least one set. The MEDI set from 2016 is excluded from the Wiki 500 data due to a world average of 0. All N Scopus MNPC Mendeley MNPC World 119693 MRC NIH 8107 84707 Wellcome 6044 1.000 (0.963, 1.039) 1.539 (1.454, 1.639) 1.558 (1.506, 1.613) 1.502 (1.415, 1.609) 1.000 (0.981, 1.019) 1.335 (1.304, 1.368) 1.350 (1.330, 1.371) 1.375 (1.342, 1.410) Wiki 500 MNPC 1.000 (0.320, 4.146) 2.279 (0.616, 10.076) 2.456 (0.657, 10.669) 3.600 (1.087, 14.634) Wiki 500/5k MNPC 1.000 (0.651, 1.573) 1.789 (0.819, 4.360) 1.754 (0.819, 3.972) 2.661 (1.481, 5.176) 7.4 Comparison with MNCS bootstrapped confidence intervals For comparison with the above results, MNCS values were calculated for the same data sets (Table 8) using bootstrapping to estimate confidence intervals. Following standard MNCS practice (although not a necessary assumption) the bootstrapping assumed that the world figure of 1 is exact (i.e., bootstrapping the world normalised data rather than also bootstrapping for the world normalisation calculation). Although this extra assumption would tend to narrow the confidence intervals, they are still wider for Scopus and Mendeley than for the MNLCS values in Table 4. For example, the Wellcome Scopus MNLCS confidence interval width is 2.021-1.916=0.105 whereas the Wellcome Scopus MNCS confidence interval width is nearly five times wider 3.396-2.899=0.497. Even after compensating for the larger MNCS values, the confidence intervals are still substantially wider than for MNLCS (e.g., 0.497/3.132=1.16 rather than 0.105/1.968=0.05 for Wellcome). This confirms that the MNLCS formula can give narrower confidence intervals than MNCS bootstrapping, despite relaxing the assumption that the world average is exact. Thus, the confidence intervals in this case are both more theoretically robust and narrower. The theoretical robustness would be irrelevant for very large sample sizes, but the confidence intervals should still be narrower for the MLNCS. For the Wikipedia 500 data sets, MNCS confidence intervals can be calculated, in contrast to the situation for MNLCS, due to the assumption that the world mean is exact for standard MNCS calculations. For the Wikipedia 500/5k data sets, the confidence intervals have similar relative widths between the MNCS and MNLCS. In both of these cases, however, the high variability of the Wikipedia citation data makes the world averages unreliable and so the MNCS confidence intervals are also unreliable. This is under the theoretical assumption that the world set is a sample of the possible articles that could have been produced under similar situations, thereby treating the population as a sample, as discussed at the end of Section 2.    28 Table 8. MNCS for 2013-2016 and BIOC, IMMU and MEDI combined, together with bootstrapped 95% confidence intervals. All Mendeley > 0 Wiki 500 > 0 Scopus > 0 World MRC NIH Wellcome 1.000 (0.969, 1.036) 2.867 (2.760, 2.979) 2.313 (2.278, 2.352) 3.132 (2.899, 3.396) 1.000 (0.988, 1.011) 2.856 (2.764, 2.955) 2.255 (2.226, 2.286) 3.308 (3.131, 3.518) 1.000  (0.659, 1.407) 1.703 (1.176, 2.314) 2.836 (1.779, 4.109) 3.231 (2.270, 4.416) Wiki 500/5k > 0 1.000 (0.823, 1.198) 1.506 (0.624, 3.086) 1.311 (0.890, 1.823) 1.421 (1.115, 1.748) 8 Discussion The main limitation of this study is that only three groups were investigated (MRC, NIH, Wellcome) and that the empirical results are therefore not conclusive. They primarily serve to illustrate the new methods introduced, show that the claims are broadly credible, and demonstrate that the formulae are capable of generating useful results. A practical limitation of the indicators is that some subject categories in Scopus and WoS contain periodicals that are rarely cited (e.g., trade publications and arts magazines) and these categories therefore have unusually high numbers of uncited articles (Thelwall, 2016c). This can inflate EMNPC, MNPC and MNLCS values for sets of articles that do not include any rarely cited periodical articles by reducing the world category average. It is therefore important for world normalisation purposes to ensure that such periodicals are identified and excluded from the data. Most of the graphs show a dip in the lines in 2014 for all three funders. Since it seems unlikely that all three funded less impactful research in the same year, the most likely cause is that the world average increased relative to them in 2014. This could be through the publication of a batch of highly cited articles from another funder but it may also be due to a technical issue, such as a change in the journal composition of one of the three Scopus categories in 2014 by removing low-cited journals that do not publish much research funded by Wellcome, NIH and MRC. For example, these might be non-English journals since international coverage of citation databases is uneven (de Moya-Anegón, Chinchilla-Rodríguez, Vargas-Quesada, et al., 2007). As mentioned above, a limitation for the specific example here is that Scopus acknowledgements seem to index only about a third of Wellcome-funded papers and if this is due to primarily indexing acknowledgements in high impact journals then the world normalised comparisons would be unfair. An important limitation from the perspective of the validity of the results is that there are many different reasons why each organisation apparently funds articles with relatively high indicator values. These results could be due to the organisations being successful in selecting excellent research to fund, to researchers receiving funding being able to conduct better research in consequence of the funding, to the organisations funding high citation sub-specialisms, or to the funded research having a higher profile due to funder publicity. Thus, practical interpretations of the graphs and tables require detailed knowledge about the types of research that they fund and their publicity policies (e.g., open access mandates, press release strategies).  29 In answer to the first research question, the MNLCS was able to distinguish statistically between the different groups assessed and the world average for most individual years and overall for Scopus citations and Mendeley reader counts. It was much less discriminatory for Wikipedia citations, being unable to distinguish between groups and the world average on the 500/500 data set (using the approximate confidence intervals), except for Wellcome in 2013 and 2014. It was more discriminatory on the 500/5000 data set, but still with relatively wide confidence intervals and not for the MRC. Nevertheless, since the MNLCS confidence intervals are unreliable for the Wikipedia citations (Appendix B), these conclusions are not statistically robust. For the normalised proportion cited, the EMNPC values for the Mendeley and Scopus citations were discriminatory overall and for most individual years for Scopus citations, Mendeley readers and Wikipedia citations from the 500/5000 data set (although the results are not robust for Wikipedia). EMNPC and MNPC do not seem to be very useful to distinguish one group from another for Scopus citations or Mendeley readers, except perhaps for the most recent year, because of the high proportion of cited or read articles. This may be different for non-medical subjects with lower citation rates. For the Wikipedia citations from the 500/500 data set, EMNPC values are sufficient to distinguish all groups from the world average overall and for some individual years, showing that EMNPC is partially effective in this low data case. MNPC is not quite as stable but still represents an advantage over MNLCS. As a reminder, this conclusion depends upon the data sets analysed and the hypothesis that the underlying advantage for each group is the same for all fields. In answer to the second research question, the data sets for which the proportion of cited articles was low were the two Wikipedia citation data sets. As the above paragraph argues, EMNPC and MNPC are preferable to MNLCS in these contexts for more discriminatory power as well as more robust confidence intervals. The comparison between EMNPC and MNPC suggests that EMNPC is superior for stability and also has the advantage of being able to use all data, whereas the MNPC calculation (and MNCS, MNLCS) had to exclude the MEDI 2016 Wikipedia 500/500 set because none of the world’s articles were cited. Hence the data for this paper suggests that EMNPC is preferable to MNPC in all respects. The cause of the reduced stability of MNPC is that, as a weighted sum of ratios, if any of the ratios in the weighted sum has a wide confidence interval then this has a substantial effect on the overall confidence interval. Nevertheless, MNPC has two advantages that the current data sets have not revealed: it is not restricted to sets of fields of approximately equal sizes and therefore for some data sets can have more complete coverage than EMNPC, and it is fairer because each article is weighted exactly equally, whereas the EMNPC calculation gives higher weightings to articles in smaller fields. Thus, MNPC is recommended for cases where field sizes are unequal and it is important not to exclude any, especially when none of the world groups has few uncited articles so that the confidence intervals are not large. A limitation of the method is that the heuristic confidence intervals introduced for both MNPC and EMNPC are estimates and the MNPC formula estimates differ substantially from MNPC estimates from bootstrapping (Appendix C). A simulation approach is needed to assess which of the two is most accurate (simulating datasets with a range of different parameters and comparing bootstrapping confidence intervals, confidence intervals from runs of the model, and confidence intervals from the formula). An important omission in the current work is for guidelines to select when field/year combinations contain too few articles relative to other field/year combinations, so that they 30 should be excluded from EMNPC calculations. Future work is needed to investigate this issue. 9 Conclusions This article introduced new field (and year) normalised formulae for indicators, MNLCS, MNPC and EMNPC, all of which can be used to generate estimates of how far above or below the world average a set of articles is. It is possible to calculate approximate confidence intervals for them without bootstrapping as long as the raw data approximately follows the discretised lognormal distribution (MNLCS) and does not have too many zeros (MNPC), and so they seem to be superior to many previous indicators for citations in this regard (although not percentile indicators). Nevertheless, for the MNLCS and MNPC the confidence intervals are estimates and for EMNPC the calculation relies upon the assumption that the group advantage or disadvantage of the set of articles assessed is the same in all fields and years. The MNLCS indicator is the preferred option, when it is applicable, because it exploits more information – the exact citation count for each article – and therefore has the potential to be more stable. The EMNPC is recommended for cases where few articles are cited, as occurs for many alternative indicators. The EMNPC is preferable in these cases because it impossible to construct narrow or robust confidence intervals for the MNLCS when most of the data consists of zeros. The MNPC formula is more suitable than EMNPC when there are small field/year groups and there are no wide confidence intervals in the MNPC calculation. Note that these conclusions are consistent with the analytical arguments presented in this paper and the medical funders example provided but still need to be evaluated with a range of different examples to verify them. Based on the arguments above and consistent with the example assessed in this article, a practical overall strategy for calculating a set of field normalised alternative indicators for a group of publications is now possible for the first time and is supported by functions added to the free software Webometric Analyst (http://lexiurl.wlv.ac.uk/) for this article (see also the guidelines here: Thelwall, 2017). 1. Identify the group of publications to be assessed and categorise them by field (e.g., using Scopus or WoS subject categories). 2. Save the article information (authors, title, journal, publication year) in a standard tab-delimited format in a separate file for each subject category/year combination. Discard publications that are in small subject/year combinations (e.g., <100 publications). 3. For each retained subject/year combination, download all articles from Scopus/WoS (if possible) or a large balanced sample (e.g., the first and last 5000 articles published in the category) for the world reference set. Filter out any large trade or art journals with a high proportion of uncited articles. Name the files using the standard Webometric Analyst naming convention (see http://lexiurl.wlv.ac.uk/). 4. Decide which alternative indicators are to be used for the data. 5. For free alternative indicators (e.g., Mendeley readers). Use Webometric Analyst to download all indicator values, or use another altmetric data source if available. 6. For paid alternative indicators, use Webometric Analyst to generate a random sample of articles from the world and group sets (e.g., 500 per set) and use these samples instead of the full set. 7. Use Webometric Analyst to calculate MNLCS, MNPC and EMNPC values and confidence limits for both. These values can be calculated separately for each year, 31 combined across all years, or both. MNLCS is recommended as the main indictor unless the proportions cited are low, in which case EMNPC or MNPC are preferred. MNPC is preferable to EMNPC if it is stable enough. This is likely to occur for very large sample sizes or proportions cited that are not too low. If MNPC is used, then the its confidence intervals should be treated with caution. Finally, the results in this article also give new evidence that Mendeley reader counts give earlier evidence of impact than do Scopus citations (e.g., see: Fairclough & Thelwall, 2015ab), and also show, for the first time, that web-based indicators can be used to assess whether a group of articles has had a type of impact that is significantly different from the world average. 10 References Archambault, É., Vignola-Gagne, É., Côté, G., Larivière, V., & Gingras, Y. (2006). Benchmarking scientific output in the social sciences and humanities: The limits of existing databases. Scientometrics, 68(3), 329-342. Bailey, B. J. R. (1987). Confidence limits to the risk ratio. Biometrics, 201-205. Berk, R. A., Western, B., & Weiss, R. E. (1995). Statistical inference for apparent populations. Sociological methodology, 25, 421-458. Bollen, K. A. (1995). Apparent and nonapparent significance tests. Sociological Methodology, 25, 459-468. Bornmann, L., & Haunschild, R. (2016a). How to normalize Twitter counts? A first attempt based on journals in the Twitter Index. Scientometrics, 107(3), 1405-1422. Bornmann, L., & Haunschild, R. (2016b). Normalization of Mendeley reader impact on the reader- and paper-side: A comparison of the mean discipline normalized reader score (MDNRS) with the mean normalized reader score (MNRS) and bare reader counts. Journal of Informetrics, 10(3), 776-788. Bornmann, L., Leydesdorff, L., & Mutz, R. (2013). The use of percentiles and percentile rank classes in the analysis of bibliometric data: opportunities and limits. Journal of Informetrics, 7(1), 158-165. Cumming, G. (2012). Understanding the new statistics: effect sizes, confidence intervals, and meta-analysis. London, UK: Routledge. de Moya-Anegón, F., Chinchilla-Rodríguez, Z., Vargas-Quesada, B., Corera-Álvarez, E., Muñoz-Fernández, F., González-Molina, A., & Herrero-Solana, V. (2007). Coverage analysis of Scopus: A journal metric approach. Scientometrics, 73(1), 53-78. de Solla Price, D. (1976). A general theory of bibliometric and other cumulative advantage processes. Journal of the American society for Information science, 27(5), 292-306. Dinsmore, A., Allen, L., & Dolby, K. (2014). Alternative perspectives on impact: The potential of ALMs and altmetrics to inform funders about research impact. PLoS Biology, 12(11), e1002003. Elsevier (2013). International Comparative Performance of the UK Research Base - 2013. https://www.gov.uk/government/publications/performance-of-the-uk-research-baseinternational-comparison-2013 Eysenbach, G. (2011). Can tweets predict citations? Metrics of social impact based on Twitter and correlation with traditional metrics of scientific impact. Journal of medical Internet research, 13(4), e123. Fairclough, R., & Thelwall, M. (2015a). National research impact indicators from Mendeley readers. Journal of Informetrics, 9(4), 845-859. 32 Fairclough, R., & Thelwall, M. (2015b). More precise methods for national research citation doi: of Informetrics, 895-906. Journal 9(4), impact 10.1016/j.joi.2015.09.005 comparisons. Fieller, E.C. (1954). Some problems in interval estimation. Journal of the Royal Statistical Society Series B, 16(2), 175-185. Gibbons, M., Limoges, C., Nowotny, H., Schwartzman, S., Scott, P., & Trow, M. (1994). The new production of knowledge: The dynamics of science and research in contemporary societies. Sage. Gunn, W. (2013). Social signals reflect academic impact: What it means when a scholar adds a paper to Mendeley. Information standards quarterly, 25(2), 33-39. Hamilton, S. (2011). Evaluation of the ESRC’s participation in European collaborative research projects (ECRPs). http://www.esrc.ac.uk/_images/ECRP_full_report_tcm8-22049.pdf Haunschild, R., & Bornmann, L. (2016). Normalization of Mendeley reader counts for impact assessment. Journal of Informetrics, 10(1), 62-73. Hwang, H., & Powell, W. W. (2009). The rationalization of charity: The influences of professionalism in the nonprofit sector. Administrative Science Quarterly, 54(2), 268-298. Jaffe, A. B., Trajtenberg, M., & Henderson, R. (1993). Geographic localization of knowledge spillovers as evidenced by patent citations. the Quarterly journal of Economics, 577-598. Jaffe, A. B. (2002). Building programme evaluation into the design of public research‐support programmes. Oxford Review of Economic Policy, 18(1), 22-34. Kousha, K. & Thelwall, M. (2008). Assessing the impact of disciplinary research on teaching: An automatic analysis of online syllabuses, Journal of the American Society for Information Science and Technology, 59(13), 2060-2069. Kousha, K. & Thelwall, M. (in press-a). Are Wikipedia citations important evidence of the impact of scholarly articles and books? Journal of the Association for Information Science and Technology. doi:10.1002/asi.23694 Kousha, K. & Thelwall, M. (in press-b). Patent citation analysis with Google. Journal of the Association for Information Science and Technology. Li, J., Qiao, L., Li, W., & Jin, Y. (2014). Chinese-language articles are not biased in citations: Evidences from Chinese-English bilingual journals in Scopus and Web of Science. Journal of Informetrics, 8(4), 912-916. Li, X., Thelwall, M., & Giustini, D. (2012). Validating online reference managers for scholarly impact measurement. Scientometrics, 91(2), 461–471. Lundberg, J. (2007). Lifting the crown—citation z-score. Journal of Informetrics, 1(2), 145-154. MacRoberts, M., & MacRoberts, B. (1996). Problems of citation analysis. Scientometrics, 36(3), 435-444. MacRoberts, M. H., & MacRoberts, B. R. (2010). Problems of citation analysis: A study of uncited and seldom‐cited influences. Journal of the American Society for Information Science and Technology, 61(1), 1-12. Mohammadi, E., Thelwall, M., Haustein, S., & Larivière, V. (2015). Who reads research articles? An altmetrics analysis of Mendeley user categories.  Journal of the Association for Information Science and Technology, 66(9), 1832-1846. doi: 10.1002/asi.23286 33 Mohammadi, E., Thelwall, M. & Kousha, K. (2016). Can Mendeley bookmarks reflect readership? A survey of user motivations. Journal of the Association for Information Science and Technology, 67(5), 1198-1209. doi:10.1002/asi.23477 Motulsky, H. (1995). Intuitive biostatistics: a nonmathematical guide to statistical thinking. Oxford, UK: Oxford University Press. Narin, F. (1994). Patent bibliometrics. Scientometrics, 30(1), 147-155. Opthof, T., & Leydesdorff, L. (2010). Caveats for the journal and field normalizations in the CWTS (“Leiden”) evaluations of research performance. Journal of Informetrics, 4(3), 423-430. Priem, J., Taraborelli, D., Groth, P., & Neylon, C. (2010). Altmetrics: A manifesto. http://altmetrics.org & L., Rainie, memo. B. http://www.pewinternet.org/files/oldmedia//Files/Reports/2007/PIP_Wikipedia07.pdf.pdf Tancer, (2007). Data Schubert, A., & Braun, T. (1986). Relative indicators and relational charts for comparative assessment of publication output and citation impact. Scientometrics, 9(5-6), 281-291. Schubert, A., & Braun, T. (1996). Cross-field normalization of scientometric indicators. Scientometrics, 36(3), 311-324. Science-Metrix (2015). Analysis of Bibliometric indicators for European policies 2000–2013. http://ec.europa.eu/research/innovationunion/pdf/bibliometric_indicators_for_european_policies.pdf Seglen, P. O. (1998). Citation rates and journal impact factors are not suitable for evaluation of research. Acta Orthopaedica Scandinavica, 69(3), 224-229. Sirtes, D. (2013). Funding acknowledgements for the German Research Foundation (DFG). The dirty data of the web of science database and how to clean it up. In Proceedings of the 14th International Society of Scientometrics and Informetrics Conference. Vienna, Austria: University of Vienna (pp. 784-795). Thelwall, M., Haustein, S., Larivière, V. & Sugimoto, C. (2013). Do altmetrics work? Twitter and ten other candidates. PLOS ONE, 8(5), e64841. doi:10.1371/journal.pone.0064841 Thelwall, M., Kousha, K., Dinsmore, A. & Dolby, K. (2016). Alternative metric indicators for funding scheme evaluations. Aslib Journal of Information Management, 68(1), 2-18. doi:10.1108/AJIM-09-2015-0146 Thelwall, M. & Kousha, K. (2008). Online presentations as a source of scientific impact? An analysis of PowerPoint files citing academic journals, Journal of the American Society for Information Science and Technology, 59(5), 805-815. Thelwall, M., & Kousha, K. (2015a). Web indicators for research evaluation, part 1: Citations and links to academic articles from the web. El Profesional de la Información, 24(5), 587-606. doi:10.3145/epi.2015.sep.08 Thelwall, M., & Kousha, K. (2015b). Web indicators for research evaluation, part 2: Social 607-620. Información, Profesional 24(5), de la media metrics. doi:10.3145/epi.2015.sep.09 El Thelwall, M., & Sud, P. (2016). National, disciplinary and temporal variations in the extent to which articles with more authors have more impact: Evidence from a geometric field normalised 48-61. Journal doi:10.1016/j.joi.2015.11.007 Informetrics, indicator. citation 10(1), of 34 Thelwall, M. & Sud, P. (in press). Mendeley readership counts: An investigation of temporal and disciplinary differences. Journal of the Association for Information Science and Technology. doi:10.1002/asi.23559 Thelwall, M. & Wilson, P. (2016). Mendeley readership altmetrics for medical articles: An analysis of 45 fields, Journal of the Association for Information Science and Technology, 67(8), 1962-1972. doi:10.1002/asi.23501 Thelwall, M. (2016a). Are the discretised lognormal and hooked power law distributions 454-470. citation data? Informetrics, Journal of 10(2), for plausible doi:10.1016/j.joi.2016.03.001 Thelwall, M. (2016b). Citation count distributions for large monodisciplinary journals. Journal of Informetrics. 10 (7), 863-874. Thelwall, M. (2016c). Are there too many uncited articles? Zero inflated variants of the discretised lognormal and hooked power law distributions. Journal of Informetrics, 10(2), 622-633. doi:10.1016/j.joi.2016.04.014 Thelwall, M. (2016d). The precision of the arithmetic mean, geometric mean and percentiles for citation data: An experimental simulation modelling approach. Journal of Informetrics, 10(1), 110-123. doi:10.1016/j.joi.2015.12.001 Thelwall, M. (2016e). The discretised lognormal and hooked power law distributions for complete citation data: Best options for modelling and regression. Journal of Informetrics, 10(2), 336-346. doi:10.1016/j.joi.2015.12.007. Thelwall, M. (2017). Web indicators for research evaluation: A practical guide. San Rafael, CA: Morgan & Claypool. van Raan, A. F., van Leeuwen, T. N., Visser, M. S., van Eck, N. J., & Waltman, L. (2010). Rivals for the crown: Reply to Opthof and Leydesdorff. Journal of Informetrics, 4(3), 431-435. van Raan, A. F. (1998). In matters of quantitative studies of science the fault of theorists is offering too little and asking too much. Scientometrics, 43(1), 129-139. Wallace, M. L., Larivière, V., & Gingras, Y. (2009). Modeling a century of citation distributions. Journal of Informetrics, 3(4), 296-303. Waltman, L., & Schreiber, M. (2013). On the calculation of percentile‐based bibliometric indicators. Journal of the American Society for Information Science and Technology, 64(2), 372-379. Waltman, L., van Eck, N. J., van Leeuwen, T. N., Visser, M. S., & van Raan, A. F. (2011a). indicator: Some theoretical considerations. Journal of Towards a new crown Informetrics, 5(1), 37-47. Waltman, L., van Eck, N. J., van Leeuwen, T. N., Visser, M. S., & van Raan, A. F. (2011b). Towards a new crown indicator: An empirical analysis. Scientometrics, 87(3), 467-481. Waltman, L., Calero-Medina, C., Kosten, J., Noyons, E. C. M., Tijssen, R. J. W., van Eck, N. J.,… Wouters, P. (2012). The Leiden Ranking 2011/2012: data collection, indicators, and interpretation. Journal of the American Society for Information Science and Technology, 63(12), 2419-2432. Williams, R., & Bornmann, L. (2016). Sampling issues in bibliometric analysis. Journal of Informetrics. Wilson, E. B. (1927). Probable inference, the law of succession, and statistical inference. 209-212. Association Statistical 22(158), Journal doi:10.1080/01621459.1927.10502953. American the of Zitt, M. (2012). The journal impact factor: angel, devil, or scapegoat? A comment on JK Vanclay's article 2011. Scientometrics, 92(2), 485-503. 35 Appendix A: Indicator calculation examples This section gives a tiny example of the MNLCS calculations. Here a group publishes 5 articles in field A and 5 in field B (Table A1) and the rest of the world publishes an additional 5 articles in field A and 5 in field B (Table A2). All calculations are performed to full calculator accuracy but reported to two decimal places. The first and fourth columns in each table give the raw citation counts and the second and fifth columns give the log transformed citation counts. The following paragraph explains the third and sixth columns. For normalisation, world average log citation counts are needed. The world average log citation count for field A is the average number of log citations to all articles in field A, which are the 5 articles from the group (Table A1 left hand side) and the 5 from the rest of the world (Table A2 left hand side). The world average log citation count for field A is therefore (4.19+2.20)/10=0.64 (2 DP). The normalised citation count for all articles in field A is therefore Ln(1+ citations)/0.64. Similarly, the world average log citation count for field B is (3.58+4.68)/10=0.83. The normalised citation count for all articles in field B is therefore Ln(1+ citations)/0.83. Ignoring Field B, the MNLCS value for the group within field A alone is the arithmetic mean of the normalised log citations (Ln(1+ citations)/0.64), which is 6.56/5=1.31. Similarly, the MNLCS value for the group within field B alone is 4.34/5=0.87. For field A, the world MNLCS is the average of the normalised log citations, or (6.56+3.44)/10=1, as expected. Similarly, for field B world MNLCS is (4.34+5.66)/10=1, as expected. For the complete set of publications, the group MNLCS is the average of all is (6.56+4.34)/10=1.09. world MNLCS The log normalised (6.56+3.44+4.34+5.66)/20=1, as again expected. citations, Table A1. Artificial sample of 5 articles in field A and 5 articles in field B published by a research group. Field A Citations 0 0 1 2 10 Sum Ln(1+ citations) 0 0 0.69 1.10 2.40 4.19 Ln(1+ citations)/ 0.64 0 0 1.09 1.72 3.75 6.56  Field B Citations 0 1 1 2 2  Sum Ln(1+ citations) Ln(1+ citations)/ 0.83 0 0.69 0.69 1.10 1.10 3.58 0 0.84 0.84 1.33 1.33 4.34        36 Table A2. Artificial sample of 5 articles in field A and 5 articles in field B published by the rest of the world. Field A Citations 0 0 0 2 2 Sum Ln(1+ citations) 0 0 0 1.10 1.10 2.20 Ln(1+ citations)/ 0.64  Field B Citations 0 0 0 1.72 1.72 3.44  Sum 0 1 2 2 5 Ln(1+ citations) 0 0.69 1.10 1.10 1.79 4.68 Ln(1+ citations)/ 0.83 0 0.84 1.33 1.33 2.17 5.66 The EMNPC calculations for the same example are given in Table A3. Here the sample sizes are already equal so no calculations are needed for the equalisation of sample sizes. Table A3. EMNPC calculations for field A, field B and overall (combining fields A and B) for the citation counts in Table A1 and A2. Number cited Field A 3 Field B All 7 4 5 8 13  Proportion cited  Field A Field B 0.80 0.60 =4/5 =3/5 0.80 0.50 =8/10 =5/10 All 0.70 =7/10 0.65 =13/20 Group World  EMNPC  Field A Field B All 1.20 =0.60/0.50 1.00 =0.50/0.50 1.00 =0.80/0.80 1.00 =0.80/0.80 1.08 =0.70/0.65 1.00 =0.65/0.65 Using the figures in Table A3 and the formula above (12), the MNPC calculations can be expressed as a weighted sum of the ratios of the group proportion cited to the world proportion cited for each field. 𝑛𝑔𝑓𝑛𝑔MNPC = ∑3/55/104/58/10𝑝𝑔𝑓𝑝𝑤𝑓1110510510= 1.1 =+=×××𝑓∈{𝐴,𝐵}As an additional example, suppose that another group publishes 100 articles in Field C, with 8% of them cited compared to a world average of 4% and the group also publishes 200 articles in field D with 5% of them cited compared to a world average of 20%. Using simplified formulae (12) and (13) EMNPC and MNPC can be calculated as follows. EMNPC =MNPC = ∑𝑓∈{𝐶,𝐷}𝑓∈{𝐶,𝐷}∑∑𝑓∈{𝐶,𝐷}𝑛𝑔𝑓𝑛𝑔𝑝𝑔𝑓𝑝𝑤𝑓𝑝𝑔𝑓𝑝𝑤𝑓×=0.08 + 0.050.04 + 0.2=0.130.24= 0.542 =100300×0.080.04+200300×0.050.2= 0.833 Appendix B: Normal distribution formula bootstrapping tests Table B1 reports comparisons of confidence intervals calculated with the normal distribution formula with confidence intervals calculated with 1000 bootstrapping iterations. For each iteration, a new sample of the same size as the original data set was created by randomly selecting data points from the original data set with replacement. The mean was then calculated of this artificial sample. After 1000 repetitions, a 95% bootstrap         37 confidence interval was created by arranging the 1000 means in ascending order and selecting the means at the 2.5 and 97.5 percentiles. The lower limit percentage difference was calculated by subtracting the width of the lower half of the 95% confidence interval (i.e., subtracting the lower limit from the mean) from bootstrapping from the width of the lower half of the 95% confidence interval, as calculated by the formula. The difference in widths was then divided by the width of the bootstrapping confidence interval to give a percentage difference. The purpose of this was to test whether it was reasonable to use the normal distribution confidence interval formula. The test is for the log-transformed data (type Ln(1+x) in Table B1), with the other values included in the table for comparison purposes. The tests were conducted 48 times for each data source, once for each year (a total of 4) as well as once for the world set and each group (4) and once for each field (3). From Table B1, the assumption that the data sets for each field, group and year are approximately normal after the log transformation is supported for Scopus and Mendeley by the average absolute difference in confidence interval widths between the bootstrap methods and formula being 3%-4%, and the maximum difference for any of the 48 data sets being 14%. In conjunction with evidence that the discretised lognormal distribution fits both Scopus citation counts and Mendeley reader counts well (Fairclough & Thelwall, 2015b; Thelwall, 2016ab), this supports the use of the normal distribution formula. Data of these types cannot exactly fit the normal distribution because they are discrete, but can be thought of as following the normal distribution for the purpose of using the confidence interval formula. Without the log transformation, the confidence interval formula is only half as accurate and has a small systematic bias (type x in Table B1). The confidence interval widths from the formula agree considerably less with the bootstrap confidence interval widths for the two Wikipedia data sets, with average absolute differences of up to 23%, a systematic bias (the formula interval tends to be too narrow on the lower half and too wide on the upper half), and differences in widths of up to 100%. Thus, the formulae are unreliable for the Wikipedia data and their results should therefore be interpreted with great caution. Possible reasons for the discrepancy are: (a) Wikipedia citations counts do not follow a discretised lognormal distribution; (b) Wikipedia citations follow a discretised lognormal distribution but (b1) the high number of zeros in the data means that the discretisation process breaks the connection with the continuous normal distribution to the extent that the formula does not work, or (b2) larger sample sizes are needed for the formula to be effective due to the high number of zeros.    38 Table B1. A comparison of confidence interval left and right hand side widths, as calculated with the normal distribution formula and bootstrapping with 1000 iterations. Positive numbers in the average % difference columns indicate that the formula confidence interval is narrower than the bootstrap confidence interval. All rows are calculated from 48 datasets. Data Wiki 500 500 Wiki 500 5k Scopus Mendeley Wiki 500 500 Wiki 500 5k Scopus Mendeley Type Ln(1+x) Ln(1+x) Ln(1+x) Ln(1+x) x x x x Lower limit average % difference 23% 19% 1% 0% 28% 24% 7% 7% Upper limit average % difference Lower limit average % absolute difference Upper limit average % absolute difference -6% -7% 0% 1% -6% -7% -4% -4% 23% 19% 4% 4% 28% 24% 9% 8% 7% 8% 3% 3% 7% 7% 5% 5% Lower limit max. % difference 100% 100% 12% 10% 100% 100% 39% 35% Upper limit max. % difference 15% 34% 9% 14% 22% 22% 20% 17% Appendix C: EMNPC and MNPC confidence interval formula bootstrapping tests The confidence interval widths predicted by the EMNPC formula agree very approximately with confidence intervals calculated by bootstrapping (Table C1). For the complete set, the confidence interval for the formula is relatively optimistic in the sense of being narrower than the bootstrap confidence intervals. It is not clear which confidence interval is the most accurate. Table C1. A comparison of confidence interval left and right hand side widths, as calculated with the EMNPC formula and bootstrapping with 10000 iterations. Positive numbers indicate that the formula confidence interval is narrower than the bootstrap confidence interval. Av. % difference Year(s) Low. 95 Group Scopus Mendeley Mendeley Wiki Wiki 500 500 500 500 Low. 95 Upp. 95 -7% -5% -8% -4% -6% -9% -28% -32% -30% -24% -52% -52% 23% 3% -4% -10% 1% -4% -5% -3% -2% -4% 3% 0% -23% -12% -22% -17% -3% 1% Wiki Wiki 500 5k 500 5k Low. 95 Upp. 95 -4% 10% 5% 12% 8% 1% 8% 11% 11% -13% 17% 69% -1% 6% 7% -19% -8% -10% -11% -9% -12% -17% -11% -13% -34% -18% -30% -16% -7% -4% Scopus Low. 95 Upp. 95 -7% -2% 5% -13% -2% -7% -17% -2% -16% -17% -18% -6% -12% 14% -3% -17% 2% 0% -18% -2% -10% -20% -4% -19% -24% -15% -17% -11% 12% -3% 2013 MRC 2013 NIH 2013 Wellcome 2014 MRC 2014 NIH 2014 Wellcome 2015 MRC 2015 NIH 2015 Wellcome 2016 MRC 2016 NIH 2016 Wellcome MRC NIH Wellcome All All All Upp. 95 -4% -1% -10% -12% -1% -10% -11% -2% -11% -22% -3% -15% -35% -1% -37% -12% 0% -12% -20% 2% -15% -16% 0% -20% -26% -4% -23% -37% -6% -38%  39 The confidence interval widths predicted by the MNPC formula are substantially optimistic compared to the formulae calculated by bootstrapping (Table C2). The MNPC calculation is most relevant for the Wiki data sets because of their low proportions cited (for which the MNPC/EMNPC was designed). For one of these, Wiki 500 500, bootstrapping confidence intervals could not be calculated for 2015 onwards for at least 2.5% of the bootstrapping samples due to a divide by zero (no uncited articles in the bootstrapped world set). Scopus Mendeley Mendeley Low. 95 Wiki Wiki 500 500 500 500 Low. 95 Upp. 95 -28% -36% -41% 29% 37% 15% Table C2. A comparison of confidence interval left and right hand side widths, as calculated with the MNPC formula and bootstrapping with 10000 iterations. Positive numbers indicate that the formula confidence interval is narrower than the bootstrap confidence interval. Av. % diff. Year(s) Group 2013 MRC 2013 NIH 2013 Wellcome 2014 MRC 2014 NIH 2014 Wellcome 2015 MRC 2015 NIH 2015 Wellcome 2016 MRC 2016 NIH 2016 Wellcome MRC NIH Wellcome Wiki Wiki 500 5k 500 5k Low. 95 Upp. 95 92% 105% 90% 88% 105% 79% 114% 103% 100% 134% 103% 62% 321% 273% 246% Scopus Low. 95 Upp. 95 42% 41% 36% 56% 63% 51% 64% 64% 69% 69% 53% 76% 158% 143% 169% 39% 43% 37% 53% 64% 52% 58% 64% 59% 44% 52% 37% 139% 152% 131% 33% 46% 43% 38% 49% 51% 27% 32% 27% 11% 14% -14% 106% 127% 30% 24% 42% 39% 28% 38% 39% 21% 33% 21% 0% 10% -25% 126% 146% 153% 34% 42% 48% 43% 63% 46% 56% 61% 58% 59% 62% 56% 195% 199% 204% - - - - - - - - - All All All Upp. 95 43% 46% 47% 50% 60% 50% 58% 57% 66% 66% 54% 75% 205% 211% 206% 11 Appendix D: MNPC individual field formulae These are the formulae for lower and upper limits for MNPC for the single field 𝑓 (Bailey, 1987). A continuity correction of 0.5 may be added to all 𝑝𝑛 terms. MNPC𝑓𝐿 = exp (ln (𝑝𝑔𝑓𝑝𝑤𝑓)− 1.96√(𝑛𝑔𝑓 − 𝑝𝑔𝑓𝑛𝑔𝑓)/(𝑝𝑔𝑓𝑛𝑔𝑓) 𝑛𝑔𝑓+(𝑛𝑤𝑓 − 𝑝𝑤𝑓𝑛𝑤𝑓)/(𝑝𝑤𝑓𝑛𝑤𝑓)𝑛𝑤𝑓) MNPC𝑓𝑈 = exp (ln (𝑝𝑔𝑓𝑝𝑤𝑓)+ 1.96√(𝑛𝑔𝑓 − 𝑝𝑔𝑓𝑛𝑔𝑓)/(𝑝𝑔𝑓𝑛𝑔𝑓) 𝑛𝑔𝑓+(𝑛𝑤𝑓 − 𝑝𝑤𝑓𝑛𝑤𝑓)/(𝑝𝑤𝑓𝑛𝑤𝑓)𝑛𝑤𝑓)   