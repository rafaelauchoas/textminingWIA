Artificial Intelligence 194 (2013) 62–85Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintTransforming Wikipedia into a large scale multilingual concept networkVivi Nastase∗, Michael StrubeHITS gGmbH, Heidelberg, Germanya r t i c l ei n f oa b s t r a c tArticle history:Available online 26 June 2012Keywords:Knowledge baseMultilingualityKnowledge acquisition1. IntroductionA knowledge base for real-world language processing applications should consist of a largebase of facts and reasoning mechanisms that combine them to induce novel and morecomplex information. This paper describes an approach to deriving such a large scale andmultilingual resource by exploiting several facets of the on-line encyclopedia Wikipedia.We show how we can build upon Wikipedia’s existing network of categories and articles toautomatically discover new relations and their instances. Working on top of this networkallows for added information to influence the network and be propagated throughout itusing inference mechanisms that connect different pieces of existing knowledge. We thenexploit this gained information to discover new relations that refine some of those foundin the previous step. The result is a network containing approximately 3.7 million conceptswith lexicalizations in numerous languages and 49+ million relation instances. Intrinsic andextrinsic evaluations show that this is a high quality resource and beneficial to various NLPtasks.© 2012 Elsevier B.V. All rights reserved.While the availability of large amounts of data has encouraged the development of successful statistical techniques fornumerous natural language processing tasks, there is a concurrent quest for computer accessible knowledge. Knowledgeallows a system to counter data sparsity (e.g. lexical semantic knowledge), as well as make connections between entities(e.g. BARACK OBAMA president_of UNITED STATES OF AMERICA).Shortly after its launch in January 2001, the potential of Wikipedia as a large scale source of knowledge for Artificial Intel-ligence and Natural Language Processing in particular became apparent to researchers in the field. The appeal of Wikipediais that it strikes a middle ground between accurate, manually created, limited-coverage resources such as WordNet [9], Cyc[18], general purpose (SUMO, [33]) or domain-specific ontologies (Gene Ontology,1 UMLS2), dictionaries and thesauri, andautomatic, wide-coverage, but still noisy knowledge mined from the web [38].Unlike resources prepared by trained linguists, Wikipedia’s structures have arisen through the collaboration of contribu-tors and, with the exception of the category structure which was encouraged by the contribution guidelines, without priorplanning. This may bring the quality of a resource based on such underspecified criteria into question, but its usefulness in avariety of Natural Language Processing (NLP) tasks has already been shown [22]. The category structure was not intended tobe an ontology-like structure, but what has emerged is a folksonomy, mirroring the shared categorization preferences of thecontributors. The collaborative aspect has also led to the implicit encoding of much information that when made explicit,reveals millions of new bite-sized pieces of knowledge.* Corresponding author.E-mail addresses: vivi.nastase@h-its.org (V. Nastase), michael.strube@h-its.org (M. Strube).1 http://www.geneontology.org.2 http://www.nlm.nih.gov/research/umls/.0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.06.008V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8563Wikipedia contains a wealth of multi-faceted information: articles, links between articles, categories which group articles,infoboxes, a hierarchy that organizes the categories and articles into a large directed network, cross-language links, andmore. These various types of information have been usually exploited independently from each other.This paper presents WikiNet3 – the result of jointly bootstrapping several information sources in Wikipedia to produce alarge scale, multilingual and self-contained resource. The starting point is the category and article network. The most inter-esting feature of our approach is that it works completely automatically, in that it itself discovers relations in Wikipedia’scategory names for which it then finds numerous instances based on the category structure.Building WikiNet involves three main steps. First, category names are deconstructed to retrieve the categorization crite-rion, which leads to the discovery of numerous binary relation instances. In the second step the relation instances discoveredin the first step are refined based on information in the articles’ infoboxes. In the last step the network obtained up to thispoint is formalized by merging nodes that refer to the same concept, and by adding lexicalizations for these concepts fromredirect, disambiguation and cross-language links from Wikipedia versions in different languages. The resulting resource is anetwork consisting of 3 707 718 concepts and 49 931 266 relation instances (for 454 relations),4 and covers multiple dimen-sions: multilinguality, world knowledge, lexical semantics, collocations, paraphrases, named entities. Because the processingdoes not rely on manual feedback, and both the relations and their instances in the network are automatically discoveredin Wikipedia’s categories and infoboxes, the algorithm can easily be applied to the latest Wikipedia versions to generate anupdated resource.Intrinsic evaluation of the knowledge extracted shows that combining different types of information leads to the deriva-tion of accurate facts, not overtly expressed within articles or infoboxes, and as such not to be found by processing singleaspects of Wikipedia. We contrast this approach with DBpedia [1] and YAGO [45] – the largest repositories of facts extractedfrom Wikipedia to date. We perform extrinsic evaluation through two tasks – semantic relatedness computation betweenpairs of terms, and metonymy resolution, i.e. finding the correct interpretation of terms which are not used in any of theirliteral senses (e.g. White House is often used to refer to the President of the United States). The extrinsic evaluation resultsshow that the resource is of good quality – evidenced by high correlation results with manually assigned relatedness scoreson disambiguated data – but it also has high ambiguity which cannot be solved for pairs of terms out of context. ApplyingWikiNet to the task of metonymy resolution shows consistent increase in precision and recall when using world knowledgeto find the correct interpretation of potentially metonymic words, but due to the small size of the available data theseincreases are not statistically significant.2. Building WikiNetThe starting point for building WikiNet is the category and article network from one language version of Wikipedia.This network is modified step by step as more types of information from Wikipedia are taken into account. In the finalstep the nodes in the network are considered to represent concepts. Concepts and their lexicalizations are separated, andeach concept – now represented through a language independent ID – has associated numerous lexicalizations in a varietyof languages. An overview of the processing is shown in Algorithm 1, and each step is presented in more detail in theremainder of the section.Algorithm 1 Algorithm for building a large scale multilingual knowledge network.Input:W – the English Wikipedia dumpR – a set of relational nouns{W X } – a set of additional Wikipedia dumps in different languagesOutput:WikiNet – a graph with nodes as concepts, and edges as relations between them1: R 1 = DeconstructWikipediaCategories(W , R)2: R 2 = PropagateInfoboxRelations(W , R1)3: return WikiNet = BuildConceptNetwork(R2, W , {W X })The Wikipedia dump W is the file containing all English Wikipedia articles in XML format,5 and R is a set of relationalnouns extracted from an existing resource (NOMLEX,6 Meyers et al. [23]) used for detecting one of four classes of rela-tions in Wikipedia category names. The result of the category deconstruction process – R 1 – is a set of relation instances,represented as tuples (x, r, y), where x, y are strings, some of which are Wikipedia article or category names, others arefragments of category names, and r is a relation, also derived from the category names. R2, the result of infobox relation3 This article builds upon and expands [27,29]. It expands on this previous work by using a list of relational nouns extracted from NOMLEX. Itpresents a new method for computing semantic relatedness between a pair of terms, and its evaluation with standard data sets.It presents ex-periments on embedding the resource into an NLP task, in particular metonymy resolution. WikiNet can be downloaded from http://www.h-its.org/english/research/nlp/download/wikinet.php.4 The statistics reported in this paper refer to the WikiNet built starting from the English Wikipedia dump of 2011/01/15, and adding several otherlanguage versions. Details are in Section 3.1.5 Wikipedia dumps are available from http://download.wikimedia.org/. We use the pages-article.xml.bz2 file.6 http://nlp.cs.nyu.edu/meyers/nombank/nombank.1.0/NOMLEX-plus.1.0.64V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–85propagation, has the same structure as R1, with the difference that some of the previously extracted relation instances areassigned new relations. WikiNet, derived from R 2 and additional information from W and {W X }, is a graph. The nodesare concepts, which are identified through a unique numeric ID and have associated multiple lexicalizations in various lan-guages. The edges are relation instances between concepts corresponding to the tuples in R 2, after mapping the argumentsonto concepts and filtering out the tuples for which at least one argument could not be mapped onto a concept.Each processing stage transforms the structure produced by the previous stage. The starting point is a Wikipedia dump,based on which we build a network whose nodes are the pages and categories and whose edges are the category–categoryand category–page links. The category deconstruction step (DeconstructWikipediaCategories) adds new nodes – substringsof category names – and (named) edges to the network, and it renames some of the existing edges. The infobox relationpropagation step (PropagateInfoboxRelations) renames existing edges. After these two steps we build the actual network(BuildConceptNetwork), by collapsing together nodes (page/category nodes/category name substrings) that refer to the sameentity, and by associating with each node numerous lexicalizations in multiple languages.In the following discussion we will make use of the terminology included in Fig. 1.part of speech (POS): The part of speech is the word class of a word – e.g. noun, verb, adjective, adverb, determiner,phrase:pronoun.Phrases are (grammatical) elements of clauses. There are five types of phrases: verb phrases, noun phrases,adjective phrases, adverbial phrases and prepositional phrases.head word: The head word defines the syntactic (and frequently also the semantic) properties of the phrase.noun phrase (NP): A noun phrase is a syntactic phrase whose head word is a noun.constituent: A constituent is a fragment of a larger grammatical construction that is itself a proper grammatical con-struction (word/phrase/clause). In this paper we refer to constituents of a noun phrase, and we only considerconstituents which are noun phrases themselves.dominant/head constituent: A dominant constituent is the constituent of a phrase that has the same head word as theparent phrase.relational noun: Relational nouns are nouns that imply a relationship, e.g. member, president.relation:A relation – such as is_a, president, caused_by – describes a type of connection. In our case, we assume binaryrelations, that require two arguments.relation instance: A relation instance is a triple (x, r, y), whererconcept:In this paper we use the term concept to refer to what are called concepts (e.g. MATHEMATICS, SPACE SHUTTLE,etc.) as well as named entities (e.g. SPACE SHUTTLE ATLANTIS), because from an algorithmic point of view theyare all treated the same. The resource however includes named entity information, thus allowing the two tobe distinguished.category:By category we denote the Wikipedia category with all its implied structure (subsumed categories and pages).category name: When we need to differentiate between the category as a structure and its name, we refer to the nameis a relation, and x and y are specified concepts.explicitly.Fig. 1. Glossary of relevant terminology.2.1. Deconstructing Wikipedia categoriesTo organize Wikipedia for easy access to pages, contributors are given guidelines for categorizing articles and namingnew categories. A quick inspection reveals that categories7 are noun phrases, many of which – e.g. Albums by artist, Peoplefrom Heidelberg, Members of the European Parliament – do not correspond to the type of lexical concepts we would ex-pect to encounter in texts and are not needed for processing a text automatically. Instead, they capture examples of humanclassification and relations that can be used as a source of information [27]. Complex categories combine multiple classi-fication criteria – People from Heidelberg contains pages about people who are from Heidelberg. From this perspective,deconstruction of categories can be interpreted as separating each classification criterion captured in the category.Analysis of category names reveals several types, based on the type of information they encode. We present themsuccinctly in Table 1, and then in more detail in the subsections to follow.2.1.1. Explicit relation categoriesThese categories overtly express a relation that is common to all articles in a category. The relation can be expressedthrough a relational noun – e.g. member, president – or through a verb–preposition combination – e.g. caused by, founded in– corresponding to two types of explicit relation categories:Relational nouns Relational nouns are nouns that indicate a relation. Their meaning is complete when their arguments arealso present: e.g. prime-minister. Such nouns are present in category names and give clues about specific properties of thesubsumed articles. Let us take for example the category Members of the European Parliament: All articles in this categorydescribe some Xi , such that Xi member_of EUROPEAN PARLIAMENT.7 Sans Serif is used for patterns and words, italics for relations, Small Caps for Wikipedia categories and pages, BOLD SMALL CAPS for concepts. Part ofspeech tags and grammatical categories are capitalized.V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8565Table 1Examples of information encoded in category names and the knowledge we extract.Category name typePatternRelation instancesexplicit relation (Section 2.1.1)Queen (band)membersMoviesdirected byWoody AllenX membersmembers of XX [VBN IN] Ypartly explicit relation (Section 2.1.2)Villages inBrandenburgimplicit relation (Section 2.1.3)Mixedmartial artstelevisionclass attribute (Section 2.1.4)Albumsby artistX [IN] YX YX by YFREDDY MERCURY member_of QUEEN (BAND)BRIAN MAY member_of QUEEN (BAND). . .ANNIE HALL directed_by WOODY ALLENANNIE HALL is_a MOVIEDECONSTRUCTING HARRY directed_by WOODY ALLENDECONSTRUCTING HARRY is_a MOVIESIETHEN located_in BRANDENBURGSIETHEN is_a VILLAGEMIXED MARTIAL ARTS R TELEVISION PROGRAMSTAPOUT (T V SERIES) R MIXED MARTIAL ARTSTAPOUT (T V SERIES) is_a TELEVISION PROGRAMARTIST attribute_ of ALBUMMILES DAVIS is_a ARTISTBIG FUN is_a ALBUMCategories of this type can be identified if their name matches a pattern NP1 NP2 or NP2 (of|of the) NP1, where the headof the noun phrase NP2 is a relational noun, such as member, president, prime-minister. To recognize explicit relations thatinvolve a relational noun, we use a set R of singular and plural forms of relational nouns from NOMLEX [23] (699 wordforms). member is an extreme example of a relational noun, in that it does not have meaning in the absence of both itsarguments. president, on the other hand, is informative even when only one of its arguments is present – e.g. Barack Obamais_a rn are also added, where rn ∈ R is the relational noun thatis a president. For this reason, the relations instances Pimatches (part of) the category name.Verb–preposition combinationsIn Wikipedia categories, verb past-participle and preposition combinations – such as directedby, built in – indicate a relation. The category Airplane crashes caused by pilot error provides an example. All articles inthis category describe airplane crashing events, caused_by pilot error. These categories match the part of speech patternNP1 VBN IN NP2.8 The relation consists of the verb–preposition combination in the category name. Recognizing this type ofexplicit relations relies on part of speech tags; to obtain them we use Stanford’s POS tagger.92.1.2. Partly explicit relation categoriesPrepositions, although sometimes ambiguous, are strong indicators of semantic relations [17]. The preposition of, forexample, may indicate a spatial (Treasure troves of Europe) or a temporal (Treasure troves of the Iron Age) relation, andthe same is the case for the preposition in: Villages in Brandenburg encodes a spatial relation, Conflicts in 2000 capturesa temporal one.Categories of this kind have the pattern NP1 IN NP2 (IN is the part of speech tag for (all) prepositions). Note that thereis no overlap with explicit relation categories whose patterns are more restrictive (and will be attempted first). For partlyexplicit relation categories, we do not have the constraint that the noun phrase is headed by a relational noun, nor do wehave a verb appearing in the category name.To determine the relation R expressed in a category with the pattern NP1 IN NP2, we use the supercategories of NP1 andNP2 – SNP1 and SNP2 :• if SNP1 matches person or people, and SNP2 is organization or group, the relation assigned is member_of ;• if SNP2 matches location or geography, the relation assigned is spatial. Once a spatial relation is detected, specificationscan be made based on the connecting preposition (e.g. located_in for the preposition in, etc.). To facilitate the evaluationprocess, all spatial relations detected are named spatial, and their instances are labeled accordingly.• if SNP2 matches time, the relation assigned is temporal.8 VBN is the part of speech tag for participles, and IN is the part of speech tag for prepositions in the Penn Treebank POS tag set [43].9 http://nlp.stanford.edu/software/tagger.shtml.66V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–852.1.3. Implicit relation categoriesSome category names are complex noun compounds. These do capture relations, but do not give any (overt) indicationof what the relation is: Mixed martial arts television programs has two noun phrase components – MIXED MARTIAL ARTSand TELEVISION PROGRAMS – and the relation between them, encoded in the category name, is topic. These categories havethe pattern NP1 NP2.For category names that are complex noun compounds, we use the parse tree to extract all embedded phrases (NP, PP,VP, ADJP, ADVP). An example is presented in Fig. 2.Each embedded phrase is considered to be a constituent C j of the category name (C1 = mixed martial arts, C2 = televisionprograms). Each C j is dominated by another constituent C hj according to the syntactic structure of the category name (inour example, C2 = C h1 , i.e. C2 dominates C1). The constituent which corresponds to the phrase head is the dominant/headconstituent of the category name and is denoted by C h (C2 is also C h in the above example). We use only the noun phraseconstituents of a category name, and denote the constituents accordingly as NP j , NPh.Fig. 3 shows examples of relations and some of their instances induced for this type of category. The process is shownin detail below.1. add relation instances Pi is_a NPh;2. form pairs (NP j , NPh) for all NP j for which NPhj= NPh – form constituent pairs in which the first constituent isdominated by the main dominant constituent. Determine the relation r that holds between (NP j , NPh) (detailed below);3. add relation instances Pi r NP j .Propagating the relation rif P j is_a NPh and NPh r NPx (cid:4)⇒ P j r NPx.from the category constituents to the pages follows the rule captured in Fig. 4:Fig. 2. Example of parse tree for a category name.Fig. 3. Example of relations and some of their instances induced after extracting components of a category name.Fig. 4. Propagating the relation between category constituents to the subsumed pages.V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8567Finding the relation between one pair (NPx, NPh) means automatically finding the relation between numerous (P j , NPx)pairs.2.1.4. Class attribute categoriesFor categories with names that match the pattern [NP1 by NP2], we identify NP1 as a class and NP2 as an attribute.Categories with this pattern usually have subcategories that further group the pages, according to values of the classattribute. For example, albums by artist has subcategories Miles Davis albums, The Beatles albums, . . . . We then identify thevalue of the attribute in the subcategory names. In many cases, like the example presented in Fig. 5, NP1 appears in thesubcategory name – albums by artist → Miles Davis albums. It is then easy to identify the attribute value (Miles Davis forartist), and we add the relation instance MILES DAVIS is_a ARTIST, as shown in Fig. 5.Fig. 5. Example of relations and some of their instances inferred from “by” categories.Not all situations follow the patterns described above: The category Heads of government by country is an example.Subcategories of this category include Prime ministers of Canada, Chancellors of Germany. In this case we start processingthe attribute first (NP2):1. if the attribute is a category in Wikipedia, collect the pages it subsumes ( P ai ) as possible attribute values;2. if a P ai appears in the subcategory name, it serves as confirmation that this is a possible attribute value and we addthe links Pai is_a NP2 and Pi is_a NP1 for each page P i subsumed by the “by” category;3. extract the remainder of the subcategory name as an instance of NP1.In the example above, NP1 = heads of government, NP2 = country. We expand country10 to all its pages and test whetherany of them appear in the name of the subcategory Prime ministers of Canada. We thus identify P ai = Canada, and add thelinks CANADA is_a COUNTRY (step 2) and PRIME MINISTER is_a HEADS OF GOVERNMENT (step 3). For all pages P i under Headsof government by country, the relation instances P i is_a HEADS OF GOVERNMENT are added (step 2).2.1.5. The algorithmUsing the information obtained from processing the category names as detailed above, we induce knowledge which isadded to the original category and article network. This process is sketched out in Algorithm 2. In the preceding discussionwe have presented category types based on the type of relation they encode. In the algorithm the ordering is in reverseorder of the specificity of the matching pattern, meaning that more constraining patterns are applied first.To apply the aforementioned rules, the category names are processed with the POS tagger and parser developed by theStanford NLP group.11The result of this first stage of the process is a network with a heterogeneous mixture of nodes: Some represent cate-gories, some pages, some strings obtained after splitting category names. To obtain the WikiNet version whose statistics weincluded in this paper, at this point of processing we had a network with 70 540 640 edges and 3 885 940 nodes.2.2. Propagating infobox relationsIn the previous step the information encoded in Wikipedia’s category names was used to induce relations and theircorresponding instances. Some category names provide only very general clues and general relations, such as temporal orspatial. For practical reasons it may be useful to have more specific relations. To obtain them we look at links betweencategory names and information in the infoboxes: The category name encodes the categorization/grouping criterion that isrespected by all of the subsumed pages, while the infoboxes contain a summary of the most important information in thecorresponding pages and the categorization criterion may be part of that.10 Wikipedia categories are usually in plural. Before extracting the pages we transform Y to its plural form.11 http://www-nlp.stanford.edu/software/.68V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–85Algorithm 2 Deconstructing Wikipedia categories.Input:W – a Wikipedia dumpR – a set of relational nounsOutput:R1 – a set of binary relation instances// Class attribute categories, Section 2.1.4R1 = R1 ∪ ProcessByCategory(w)R1 = R1 ∪ ExtractClassAttributes(w, W)elseelseP = set of pages under wif w matches “NP1 by NP2” then1: W C = the set of categories from W2: R 1 = {}3: for w ∈ W C do4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:28: return R 1elseelseif w matches “r (of|of the) [NP]” or “[NP] r”, where r ∈ R then// Explicit relation category, relational nouns, Section 2.1.1R 1 = R1 ∪ {P i r [NP] | ∀P i ∈ P }if w matches “NP1 VBN IN NP2” then// Explicit relation category, verb–prep combination, Section 2.1.1R1 = R1 ∪ {(P i [VBN IN] NP2), (P i is_a NP1) | ∀P i ∈ P }if w matches “NP1 IN NP2” then// Partly explicit relation category, Section 2.1.2r = DetermineRelation(IN, NP1, NP2)R1 = R1 ∪ {(P i r NP2), (P i is_a NP1) | ∀P i ∈ P }if w is a complex noun compound then// Implicit relation category, Section 2.1.3C = ExtractConstituents(w)C = {NPx, NPh}, NPh is the dominant (head) constituent of wR1 = R1 ∪ {(P i related_to NPx), (P i is_a NPh) | ∀P i ∈ P }We hypothesize a connection between the information encoded in category names and the information summarizedin infoboxes, and use this to propagate relations from infoboxes through the category network, as shown in Fig. 6. Let ushave a closer look at this example. First, the category name deconstruction step splits the string Military equipment of theSoviet Union into two parts: Military equipment and Soviet Union. It then proposes a rather generic spatial relation based onthe preposition of and the fact that Soviet Union has a corresponding article whose ancestor is the Geography category. Inthe final step of the category deconstruction process, we have added the relation instances ( P i spatial Soviet Union) for allpages P i subsumed by the category Military equipment of the Soviet Union. We would now like to find a more informativerelation to replace the generic spatial in the relation instances mentioned before. Several of the articles under the categoryMilitary equipment of the Soviet Union contain infoboxes,12 in which we find Soviet Union as the value of the attributeplace-of-origin. This attribute becomes the relation, and the previously extracted relation instances are replaced by ( P i place-of-origin Soviet Union) for all pages P i subsumed by the category Military equipment of the Soviet Union. This can be seenas propagating a relation first from the pages that contain infoboxes to the category which subsumes them, and then fromthis parent category to all the other page siblings that did not have an infobox.Working “on top” of an existing network introduces an important difference and advantage relative to work that extractsknowledge from open texts: A piece of extracted knowledge – a predicate–arguments tuple – is immediately connected toother such facts, and it has a context. Because of this, every piece of information we add to this network has an impacton its neighbours, and, depending on its type, its influence can reach far. This happens when establishing the connectionbetween the information in infoboxes for some pages and their parent categories – it impacts their siblings.In other words, r is a candidate relation for the constituents pair (NPh, NPi) corresponding to category C , if it is associatedwith the same value V in all infoboxes in which it appears under C , and if V is compatible with NPi . Two values arecompatible if they are identical, paraphrases of each other, or are connected in a systematic way – V is an instance or aconcept more specific than the one corresponding to NPi , or for locations for example V part-of NPi (a specific location inEurope is compatible with EUROPE). This predicate can become more specialized as more relation instances are added tothe fact base.Algorithm 3 shows the processing steps for linking a category name with information in the infoboxes of its subsumedpages and for propagating the induced relations to the page–category links.The result of this processing step is a network consisting of the same nodes and edges as in the previous processingstep, with the difference that the names of certain edges are now changed, according to the information in the infoboxes.12 When processing a category, we consider all subsumed articles, including those subsumed by subcategories.V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8569Fig. 6. Some articles under a Wikipedia category contain infoboxes with hand-picked relations, from which we can determine the relation that holdsbetween the corresponding concept and the concepts derived from its parent category, and then propagate these relations in the network.Algorithm 3 Linking to infoboxes and propagating relations.Input:W – a Wikipedia dumpR 1 – the set of relations extracted in the category name processing stepOutput:R2 – a set of binary relationsP is the set of pages subsumed by wbuild set of candidates R = {ri | (ri , NPx) ∈ P j } – the set of attributes which are associated with one of the category’s constituents NPx extracted from1: W C = the set of categories from W2: R 2 = R13: for w ∈ W C , w (cid:9)→ {NP1, . . . , NPi } according to the previous deconstruction step do4:5:infoboxes from pages P j ∈ Pif |R| = 1 and matching constituent is NPx thenr ∈ Rfor P j ∈ P doelse6:7:8:9:10:11:12:13:14:15: return R 2replace relations (P j rx NPx) in R2 with (P j r NPx)if |R| > 1 and matching constituent is NPx thenif all relations in R are compatible thenfor P j ∈ P doreplace relations (P j rx NPx) in R2 with (P j r NPx)2.3. Building a concept network2.3.1. Mapping relation arguments to conceptsUp until this point we have built a heterogeneous network based on various types of information in Wikipedia. Nowthis network will be transformed into a concept network. In works using Wikipedia as a source of knowledge, categoriesand articles are considered to represent concepts [36,25]. We start from this premise as well, and constrain it to eliminate70V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–85redundancy – there should only be one node representing a concept. Mapping onto one node articles and categories thatrefer to the same concept (e.g. Cities and City) or are homonyms (there is a Rome article and a category) is trivial. A morecomplex issue arises when mapping the fragments resulting from deconstructing the category names onto the originalWikipedia nodes – a prerequisite for the final node mapping step.The category Chemistry albums, for example, is split into two parts: Chemistry and albums, both of which are ambiguouswith respect to the nodes corresponding to articles or categories in the network. Chemistry, for example, can refer to thescience, the band, various albums and songs. To determine the correct corresponding article or category we compute aconnectivity score that counts the number of times each of these potential referents is linked to from pages subsumed bythe category Chemistry albums. Formally, if the fragment NP X of a category C is ambiguous, we collect the possible nodes(articles/categories) Ni that NP X can refer to, and all pages P j subsumed by C . We choose as an interpretation for NP X thenode Ni towards which there are the most hyperlinks (outlinks) in the pages P j :NP X (cid:9)→ N = argmaxNi(cid:2)outlink(P j, Ni)(cid:3)outlink(P j, Ni) =P j subsumed_by C1 ∃ a link from page P i to Ni;0 otherwiseNote that only the strings obtained by splitting the category names are ambiguous, whereas the article and categorynodes are not ambiguous in the network (i.e. each node has a unique title and ID extracted from the Wikipedia dump).This disambiguation step impacts the number of relation instances extracted in the previous step, because some of thearguments of the relation instances could not be linked to a node in the network. This step and duplicate filtering reducesthe set to 49 931 266 unique instances.2.3.2. Extracting alternative lexicalizationsFollowing WordNet’s example, which separates synsets and their lexicalizations, in the newly created network nodes arereplaced with numeric IDs, and then each ID is associated with the corresponding node’s name. More lexicalizations areadded to each node through the redirect, the disambiguation and the cross-language links.Redirect links pair a redirect page with its target. Redirect pages can be viewed as containing name variations forWikipedia articles. They may contain morphological variants (actor, actors and actress redirect to the article Actor), propersynonyms (adrenaline redirects to Epinephrine), paraphrases (Seinfeld, The show about nothing for the article Seinfeld (TVseries)) or even misspellings (Sienfeld for Seinfeld). The names of redirects are added to the ID corresponding to the articlethey point to.Disambiguation links map a disambiguation page onto its possible targets. Disambiguation pages encode polysemy – thename of the source of this link applies to all possible targets. King points to the pages corresponding to the monarch, thechess piece, and many others. The name of the source of the disambiguation link is added to the IDs of all of the possibletargets.Cross-language links link an article to its variants in other languages. The corresponding names can be used as trans-lations. For the concept ACTOR we find the following language variations through the cross-language links Schauspieler(German), Attore (Italian), etc.The network presented here is built from the information extracted from the English version of Wikipedia (W en). Therewere several reasons for choosing the English Wikipedia: It is the most comprehensive version, there are numerous languageprocessing tools that allow for deeper processing of the article texts if necessary (at this point only POS tagging and headextraction were used for processing the category names), capitalization rules allow for easy and accurate extraction ofnamed entity information. More information can be added from Wikipedia versions for other languages. We use page,redirect, disambiguation and cross-language information from other language versions (W l) to add lexicalizations of conceptsin different languages, following Algorithm 4. Essentially we map together pages in different languages either throughexisting cross-language links, or by checking for overlap in their respective cross-language links, following the “triangulation”algorithm from Wentland et al. [48].Note that we do not aim to build a lexicon of synonyms for each concept based on cross-language links, as was done byde Melo and Weikum [7], who filter out inaccurate cross-language links. As shown before with redirect and disambiguationlinks, the index contains more than synonyms, reflecting the various ways a concept is lexicalized in different languages.Table 2 shows how the coverage of concepts and number of included lexicalizations changes when adding informationfrom the German Wikipedia. Because the supplemental information comes from the German Wikipedia, it is not surpris-ing that the highest increase is for German, but we see substantial differences for other languages as well, indicating thatmerging lexicalizations from different Wikipedia language versions is useful. The number of covered concepts has increased,meaning that we have established a mapping between concepts that were not linked through cross-language links; further-more the number of lexicalizations has increased, meaning that for the existing entries we now have more lexicalizationvariations. The increase is much higher with respect to lexicalizations (5.13 times) than concept coverage (1.17 times),which is to be expected as there are several sources for lexicalizations (redirect, disambiguation, cross-language links),while concept coverage relies on mapping entries to each other. Other language versions are added in a similar fashion.V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8571Algorithm 4 Adding lexicalizations from a Wikipedia version.Input:WikiNet = {ID, Lex, R} – the current version of WikiNet consisting of:ID a set of concept IDs| ∀IDx ∈ ID} a set of concept lexicalizationsLex = {LIDxR the set of relation instances between (concept) IDsW Lg – the Wikipedia dump in a specific language Lgt – a threshold for overlap in concept lexicalizationsOutput:WikiNet(cid:11)– WikiNet enriched with additional lexicalizations(cid:11) = Lex1: Lex2: extract IDLg – the set of page IDs from W Lg3: extract LexLg – the lexicalizations of pages from W Lg through mapping page titles, redirect and disambiguation lexicalizations onto the correspondingpage IDs4: map pages from different language versions:5: for IDLg,i ∈ IDLg dofor ID j ∈ ID do6:7:compute overlap oi j = |LIDLg,i∩ LID j| – the size of the intersection of the lexicalizations of concepts corresponding to concepts IDLg,i in languageLg and ID j in the current version of WikiNet8:9:10:11:12:if maxID j ,ID j ∈ID oi j > t thenn = argmax j,ID j ∈ID oi jIDLg,i (cid:9)→ IDn(cid:11)= LIDLg,iLIDnreplace LIDn in Lex’ with L∪ LIDn(cid:11)IDn13: return WikiNet(cid:11) = {ID, Lex(cid:11), R}Table 2Partial language statistics: Concept coverage and number of lexicalizations in WikiNet before and after addinginformation from the German Wikipedia.LanguageEnglishFrenchGermanItalian. . .HungarianRomanianTurkish. . .RussianJapaneseChineseNo. of lexicalizations (no. of entries)Before7 239 290 (2 996 357)523 835 (509 051)484 688 (474 455)384 192 (378 896)89 413 (87 526)89 030 (84 694)86 543 (80 429)265 062 (254 740)264 547 (263 186)154 404 (151 957)After7 515 310 (2 996 357)728 349 (589 448)2 488 983 (556 977)555 452 (457 158)127 487 (115 558)137 582 (114 284)117 662 (103 733)415 638 (334 106)383 387 (323 239)221 398 (186 938)The current version of WikiNet merges lexicalizations from English, Chinese, Dutch, French, German, Italian, Japanese andKorean Wikipedia dumps.Anchor texts – the text fragments associated with hyperlinks in a page – are an additional lexicalization source. Whilethey were shown to be useful for detecting concepts in texts [26], they are too noisy to be added to the resource.2.3.3. Named entity informationNamed entity (NE) information is an important feature of a concept. To retrieve it we use an approach similar to Bunescuand Pa ¸sca [5] for single words. For multi-word terms we resort to syntactic analysis, as detailed below. This processing isapplied to texts from the English Wikipedia, and it relies on capitalization conventions for this language. Because the namedentity information once extracted is attached to concepts, it becomes available in all languages that have a lexicalization forthat concept.• For single word entries w, we compute ccs(w) as the number of occurrences of the word (case-sensitive) in the text ofits corresponding article, and c(w) is its total number of occurrences (case-insensitive) in the text:⎧⎨⎩TRUE:FALSE:ccs(w)c(w) < τccs(w)(cid:2) τc(w)NE(w) =τ = 0.272V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–85• For multi-word entries (w), we match the NE information with the NE information of its syntactic head: NE(w) =NE(w), where w is the syntactic head of w. This replaces[5] heuristic of assigning NE information based on capitaliza-tion only, which fails for categories like History of Europe, or Economy of the United States of America.Of the 3 707 718 concepts in the network, 2 507 052 are tagged as named entities according to this method, leaving1 200 666 common nouns/phrases.3. EvaluationTo verify the quality of the extracted network we carry out intrinsic and extrinsic evaluations of the resource. Theintrinsic evaluation only looks at the resource itself; to do this we evaluate the result of each step of processing. Theextrinsic evaluation looks at the resource in context, and its usefulness for specific tasks. We perform this through twotasks – semantic relatedness computation between pairs of terms and metonymy resolution.3.1. WikiNet overviewBefore proceeding with the evaluation, we provide a summary of information regarding the latest version of WikiNet,built on the following Wikipedia dumps: 2011/01/15 English, 2011/01/11 German, 2011/02/01 French, 2011/01/30 Italian,2011/06/28 Japanese, 2011/06/21 Korean, 2011/01/26 Dutch and 2011/06/28 Chinese.conceptsnamed entities(general) conceptsnumber of languages13lexicalizationsaverage lexicalization per conceptunique stringsunique strings when taking language into accountrelationsrelation instancesbetween two named entitiesbetween a named entity and (general) conceptsbetween two (general) concepts3 707 7182 507 0521 200 666196128 505 70434.6614 376 80619 582 97249449 931 26613 529 38231 423 0784 978 80667.62%32.38%27.09%62.93%9.97%Detailed information about instances per relation and concepts covered for each language can be found here:http://www.h-its.org/downloads/nlp/stats.rels, http://www.h-its.org/downloads/nlp/stats.lang.3.2. Intrinsic evaluation3.2.1. Deconstructing Wikipedia categoriesProcessing the Wikipedia categories starts with loading the category network – in the form of nodes and category linksextracted from Wikipedia dumps – and filtering out administrative categories (identified using keywords, e.g. stubs, articles,wikipedia). After this preprocessing, there are 277 918 categories in the network. We obtain 70 540 640 relation instances,66 184 610 of which are distributed among the category processing steps as follows:explicit relations categories:(Section 2.1.1)relational noun pattern: 42 823 category names had a relational noun head and express 185 relations (e.g. alum-nus_of, president_ of, member_of, player_of, collections_of ).VBN IN pattern: 14 238 category names match this pattern, and express 192 relations (e.g. caused_by, written_by,based_in).partly explicit categories: 172 196 category names encode partly explicit relations, most of which are assigned temporal orspatial relations in this step, to be refined in the infobox relation propagation step (Section 2.1.2).implicit relations categories: 39 049 category names give no overt information about the relation encoded (Section 2.1.3).The relations in this case are generically named related_to, and part of these will be named in the infobox relationpropagation step.class attribute categories: 9612 categories. Processing the category names reveals 840 classes with an average of 2.27attributes (Section 2.1.4). A sample is presented in Table 3.13 We include in the table only statistics for the 196 languages that have at least 1000 terms represented in the resource.V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8573Table 3Classes and attributes extracted from Wikipedia’s “by” categories.ClassARTBOOKBUILDINGMUSICIANWORKWRITERAttributescountry, media, nationality, origin, period, region, typeauthor, award, country, head of state or government, ideology, nationality, publisher, series, subject, university, writer, yeararchitect, area, city, community, county, country, function, grade, locality, province, region, shape, state, territory, townband, community, ethnicity, genre, instrument, language, nationality, region, religion, state, territoryartist, author, genre, head of state or government, nationality, writer, yeararea, award, ethnicity, format, genre, language, movement, nationality, period, religion, state, territoryTable 4Extracted relations and instances for each category type and manual evaluation results of some of the most frequent relations.Category typeRelation# categories# inst. extracted57 06129 1896 268 0364 355 964Precision∩∪95.56%97.17%explicitpartly explicitimplicitclass attributealumnus_of,member_of,president_of,player_of, . . .caused_by,written_by,based_in, . . .is_aspatial17 2971 912 07294.37%96.38%172 19639 049961224 058 1178 144 59727 713 86017 714 64813 087 05276.40%87.09%84.00%97.98%The difference of 4 356 030 instances from the total extracted is made up of category–category and category–article links. Some of these instances will be updated with more informative relations after mapping relationarguments to concepts (Section 2.3.1).Table 4 shows the number of (unique) extracted relation instances and evaluation results in terms of precision. For each ofthe two types of explicit relations – based on the VBN IN pattern, and the relational noun pattern – we extracted a randomsample of 250 instances (covering different relations), which was manually annotated by two human judges. We evaluatedseparately is_a and spatial, which were two of the most frequent relations, by comparing against two other random samplesof 250 instances annotated by two judges. For each of these four evaluation sets, the table includes two scores – one thatcorresponds to evaluation against the intersection ∩ (instances that the annotators agree are correct) and against the union∪ (instances that at least one annotator marks as correctly assigned).14In addition to the fact that it is easier to analyze a short phrase to extract a relation rather than a sentence or even adocument, analyzing category names and the category and page network for knowledge acquisition has other advantagesas well. The category names express very concisely a relation which may also appear in the article, but is expressed therein a more complex manner. We took the 42 711 member_of relation instances discovered through category name analysis,and extracted from the Wikipedia article corpus the sentences in which the two elements of the pair appear together:131 691 sentences. Of these, only 1985 sentences contained the word member, indicating that further processing wouldhave been necessary to derive this particular type of information, while by analyzing category names this information isreadily available.3.2.2. Propagating infobox relationsAt this point, the data consists of 70 540 640 relation instances obtained by deconstructing categories and keeping thecategory–category and category–article links. Some of the relations’ arguments correspond to Wikipedia pages, 1 049 724of which contain instances of one of 4459 infobox types. Of the 277 918 categories deconstructed, for 42 060 a link wasestablished between the category name (specifically, a constituent of the category name) and a value in the infobox. The linkwas established through 130 123 pages that contained infoboxes and an entry in the infobox corresponding to a category14 Cohen’s κ on the four manually annotated data sets are 0.82, 0.73, 0.76 and 0.25 respectively (following the order in Table 4). However, the datasetsare generated based on the system’s output (the instances are randomly selected from the system’s output), and as such are biased, which may lead tohigher random agreement between the judges than would otherwise be expected in a purely random collection of relation instances, that both the systemand the judges would then annotate.74V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–85name constituent, for a total of 175 350 (P j, NPi) page-constituent links. The information was propagated to a further544 702 pages and their 698 929 relation instances to the corresponding category name constituent, as shown graphicallyin Fig. 6 (on page 69). This set of 698 929 relation instances is the result to be evaluated. Table 5 shows some of the mostfrequent relations, and their number of occurrences in this set.Table 5The most frequent propagated relations.Relationcountrysubdivision_namelocationbattlesbirth placecontinentregiongenreindustryCount67 72458 46340 52539 42524 92524 22818 59010 5282056ExampleDuchy of Parma–ItalyAylmer, Quebec–GatineauValvelspitze–South TyrolJohn Paton–WWIFranklin D. Roosevelt–New York1928 British Home Championship–Europe1927 Crimean earthquakes–CrimeaCinepaint–graphicsGoogle–InternetThe propagation step is evaluated through two methods: (i) manual evaluation of two sets of relations carried out bytwo judges, (ii) manual and automatic evaluation of the overlap with YAGO’s fact base15 [45].Manual evaluation Due to the large number of relations extracted and due to variation in their frequency, we split theset of relation instances obtained through propagation into two roughly equally large subsets – one corresponding to highfrequency relations (they have more than 5000 instances), and one to low frequency ones. We extracted two samples of250 relation instances – one from each of these two subsets – which contain the same distribution of relations as thesubset they represent. This allows us to analyze a wider spectrum of relations, as low frequency relations would not appearin a small random sample that maintains the distribution of relations. The high frequency sample contains the followingrelations16:battles (8), birth_place (8), country (22), date (7), founded (6), genre (26), group (5), headquarters (6), industry (10), language(6), location (34), nationality (14), occupation (5), place (5), pushpin_map (6), region (5), ship_ country (6), sport (6), subdivi-sion_name (24), subdivision_type (4), type (18), work (6), year (6), years_active (7)and the low frequency sample (we give a partial list):address (2), airdate (2), alma_mater (3), area_served (2), artist (2), associated_acts (2), author (4), awards (4), basin_countries(3), birthdate (2), birthplace (5), body (2), born (2), branch (4), bundesland (3), etc.These two samples were manually annotated by two human judges. The guidelines instructed the annotators to assign atrue/false/not_relation tag to each instance. The instances in the annotation set were grouped by relation, andbefore each batch corresponding to one relation was included a positive example from an infobox to help the annotators.During annotation a few issues became apparent: There are “attributes” in infoboxes which are not really attributes (e.g.pushpin map, caption) which link to the included map or image (we call these “false” relations). There were 15 instancesof such relations in each of the two sample files, 14 on which the judges had agreed, and 1 on which they did not. Thepushpin map replaced the rather general spatial relation assigned in the category deconstruction step. Another issue arosefrom wrongly categorized articles (at least in the opinion of the annotators). While the propagation process may have beencorrect, the relation instance was tagged “false”. For example TRICIA LEIGH FISHER was assigned the occupation relation toCHILD ACTORS, because it was categorized under American child actors. She started her acting career when she was 16 or17, and one of the annotators considered the assigned category American child actors – and consequently the inducedTRICIA LEIGH FISHER occupation CHILD ACTORS relation instance – “false”.The results in terms of precision are presented in Table 6, relative to true tags assigned by both judges (∩) or by atleast one judge (∪). The agreement between judges in terms of Cohen’s Kappa is 0.62 for the high frequency sample, and0.81 for the low frequency one.17Comparison with YAGO The versions of WikiNet and YAGO that we compare were not generated from the same Wikipediadownload. To maximize the matching between the extracted relation instances and YAGO, we processed the arguments ofour relations and those in YAGO by removing information in parentheses (e.g. Time (Unix) → Time), by lowercasing and byremoving all blanks and all non-letter/digit characters. Additionally, we considered that a WikiNet relation instance matchesa YAGO one if their arguments match, as the resources do not share the same relations. The overlap of the set of 698 92915 http://www.mpi-inf.mpg.de/yago-naga/yago/downloads.html.16 The list formatting is relation (frequency).17 Again, these numbers should be taken with the proverbial grain of salt. The samples are biased because they are extracted after the infobox propagationstep, and as such the agreement between judges may be higher than random. See also footnote 14.V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8575Table 6Manual annotation results and evaluation, on the sample without “false” relations (filtered)/on the full sample.Samplehigh freqlow freqInstancesfiltered/all235/250235/250Evaluation∩78.3%/73.6%75.7%/71.2%∪86.8%/81.6%77.9%/73.2%relation instances renamed through relation propagation with YAGO’s fact base is 7143 concept pairs. This small overlapshows that categories, the category structure and infoboxes are the combined source of novel information, not easily ordirectly accessible through the article texts or categories alone. We look in a bit more detail at the relations connectingthese pairs in our approach and in YAGO. 306 YAGO relations are represented within the 7143 pairs. We consider the top 5,which cover 5854 of the pairs: locatedIn (3163), wrote (972), directed (757), politicianOf (572) and created (390). To the pairsassigned created in YAGO correspond the following relations assigned through the method presented in this paper:artist (126), writer (89), developer (58), director (37), manufacturer (14), producer (12), composer (8).18The relations assigned by propagating relations from the infoboxes are more specific than the relations in YAGO for theoverlapping pairs. In the manually annotated sample we have 14 instances annotated with relations from this set. Theirprecision is 87.5% (both ∩ and ∪). The same phenomenon occurs for locatedIn – it is a rather general relation, and it corre-sponds to a variety of more specific spatial relations in our assignment: subdivision_name (1288), prefecture (660), location(257), district (142), country (46), basin_countries (37), bundesland (30), county (18). Of these, the relations subdivision_name,location, country also appear in the manually annotated data (80 instances), and (together) achieve a precision of 86.25%(∪)/83.75% (∩) (Table 7).Table 7Evaluation relative to the overlap with YAGO.YAGO relationfull evaluationwrotedirectedOverlapPrecision97275799.07%98.94%estimation based on manually annotated samplelocated increated316339086.25% (∪)/83.75% (∩)87.5%The wrote and directed YAGO relations are easily mapped onto the propagated relations: For 963 of the instances withrelation wrote in YAGO, the inference process assigned the relation author (99.07%), and 749 instances of the relation directedhave the relation director after propagation (98.94%).The relation politicianOf is harder to evaluate. None of the relations assigned through relation propagation expresses thesame relation, however they are not erroneous: birth_place (350), death_place (93), residence (16), nationality (15). Theserelations were represented in the manually annotated data (40 instances), and their precision was 72.5% (∪)/70% (∩).3.2.3. Full comparison with YAGOIn the previous subsection we evaluated the results of the infobox propagation method through a comparison withYAGO. Here we perform a full evaluation of the network at this stage as compared with YAGO’s core set of facts extractedfrom Wikipedia. Matching our relation instances and YAGO’s is done as described in the previous section. The discussionon the mapping between the two lists of relations (types) was partly done in the previous section. After this processingwe compute three measures of the overlap between our relation instances (R W ) and YAGO’s (R Y ), following the method ofPonzetto and Strube [37] (itself derived from Navigli and Ponzetto [30]):Coverage is the ratio between the number of relation instances shared by the two resources, and the relation instances inthe reference resource (here, YAGO)19:|R W ∩ R Y ||R Y |= 598 78210 329 767Coverage(R W , R Y ) == 5.8%It is clear from the low overlap between the resources that each contains information that the other one does not.Novelty and ExtraCoverage quantify this:Novelty quantifies the novelty rate of our relation instances, as the ratio between relation instances that appear only in ourset, and the full size of the extracted set:Novelty(R W , R Y ) =|R W \ (R W ∩ R Y )||R W |= 49 332 48449 931 266= 98.8%18 We show only the most frequent relations, which cover the majority of the pairs.19 For our version of YAGO (downloaded 11/12/2009), we have counted 10 329 767 relation instances between Wikipedia concepts.76V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–85The Novelty of YAGO’s core relations relative to our relations is:|R Y \ (R Y ∩ R W )||R Y |= 9 730 98510 329 767Novelty(R Y , R W ) == 94.2%ExtraCoverage shows the ‘gain’ in knowledge provided by our set of relation instances with respect to YAGO’s set of facts,as the ratio between the number of relation instances found only in our set and the number of relations in YAGO:ExtraCoverage(R W , R Y ) =|R W \ (R W ∩ R Y )||R Y |= 49 332 48410 329 767= 477.5%The ExtraCoverage of YAGO’s core with respect to our resource is:ExtraCoverage(R Y , R W ) =|R Y \ (R Y ∩ R W )||R W |= 9 730 98549 931 266= 19.5%These measures show that the resources are not redundant, they introduce additional knowledge relative to each other.A particular surprise in the comparison with YAGO was the low overlap in relation instances (598 782). Our analysisshows that this is due to several factors: (i) the use of a different Wikipedia version, leading to missed matches due tochanges in the article names; (ii) the instanceOf relation in YAGO link a Wikipedia entity to a WordNet synset, which willnot match WikiNet relations; (iii) the overlap between WikiNet and YAGO consists mostly of category–category links, andthere is a small number of relations that link two named entities common to the two resources.3.2.4. Partial comparison with DBpediaDBpedia [3] is a very large repository that transforms all the structured information that Wikipedia provides into a largedatabase. It also includes links to other resources and a classification of entities into a manually defined ontology based oninfobox types. While DBpedia is based on the automatic reformatting of already existing (and structured) information inWikipedia, in building our resource the purpose was to uncover information that is not explicitly given. Because DBpediacontains the explicit information in Wikipedia – including category–category, category–article links and relations extractedfrom infoboxes – we can perform a partial comparison between WikiNet and DBpedia by evaluating the differences in struc-ture between WikiNet and Wikipedia. This way we can also avoid an imperfect mapping between the two resources. Becausewe started with Wikipedia’s category–category and category–article links, we first evaluate how many relation instances weadd to this initial structure by deconstructing categories. 69.81% of the relation instances obtained after processing categorynames are novel. We further test how many of these novel relation instances appear in an infobox – 2.91% (2.03% of thetotal number of relation instances) appear in an infobox (917 999 instances). This means that 66.65% of the relations inWikiNet are novel both with respect to the category structure and with respect to the information in the infoboxes. On theother hand, we do not explicitly add relations from the infoboxes, as these are available directly and can, as such, be added,as long as they connect two Wikipedia pages, and do not express a value (e.g. Germany area 357 021 km2).3.2.5. RelationsThe version of WikiNet described here has 454 relations. A partial histogram is presented in Fig. 7.Compared to other resources built from Wikipedia, WikiNet has more relations. DBpedia and YAGO are based on theexplicit information in Wikipedia, and as such their relation sets consist of the attributes in infoboxes. YAGO also identifiesa small number of specific categories that provide relational information, such as 1975 births, categories starting withFig. 7. Partial relations histogram on a log10 scale.V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8577Countries in ..., Rivers of ..., Attractions in ..., and exploit them as a source of the following relations: bornInYear, diedInYear,establishedIn, locatedIn, writtenInYear, politicianOf, hasWonPrize. In the first stage of processing, the relation instances extractedby WikiNet are not explicit in Wikipedia, but rather encoded in the category names. They cover relational nouns, verb–preposition phrases, plus a small number of general and structural relations (is_a, temporal, spatial, subcat_is_a, related_to,category). Some of the relations extracted through this process are unique to WikiNet – e.g. caused_by, . . . . The comparisonwith YAGO shows that for the same pairs, the relations in WikiNet are more fine-grained.3.2.6. Named entity informationWe evaluate the results of the named entity (NE) identification process against the manually annotated data used toevaluate HeiNER,20 which reimplements the Bunescu and Pa ¸sca [5] heuristics. The data and manual annotation statistics arepresented in detail in Wentland et al. [48]. Two judges annotated a dataset containing 2000 NE candidates from Wikipedia,and three judges annotated a second set of 2000 candidates. In both cases a candidate is considered a true NE if at leasttwo judges tag it as such. Comparison with our NE tags results in the following precision scores: 94.42% against set 1 and96.16% against set 2, for an average of 95.29%.21 Wentland et al. [48] report an average precision of 95% for HeiNER.3.3. Extrinsic evaluationThe evaluation of the individual processing steps was shown as we progressed through the network building process. Inthis section we show the results of an extrinsic evaluation of the complete network, through semantic relatedness compu-tation and metonymy resolution. These two tasks assess different aspects of the network. In relatedness computations theinformation provided by the structure is “compacted” in a numeric score. In metonymy resolution we use the structure ofthe network explicitly, by mining for specific relations.3.3.1. Semantic relatednessOne of the basic tasks for which a lexical/knowledge resource is used in NLP is establishing the similarity or relatednessbetween terms. The suitability of a resource is assessed based on word pairs previously annotated by human judges, such asRubenstein and Goodenough’s 65 noun pairs [42] (R&G), and Miller and Charles (M&C ) 30 pairs subset of R&G [24]. Highcorrelation between similarity and relatedness scores automatically computed on a resource and manually assigned scoresis usually considered evidence of the high quality of a resource.Previously used resources include WordNet and Roget’s Thesaurus, and most methods for computing similarity and re-latedness have been designed by taking these resources’ specific attributes (structure, size) into account. Budanitsky andHirst [4] present a comprehensive comparison of existing methods with their own for such computations. Recently Tsatsa-ronis et al. [46] proposed a new method for similarity computations in WordNet. Ponzetto and Strube [35,37], Milne andWitten [25], Gabrilovich and Markovitch [10,11] propose methods for semantic similarity/relatedness computation based onWikipedia.In comparison with WordNet and Roget’s Thesaurus and even with WikiTaxonomy [36,37], WikiNet is larger by an orderof magnitude, and has many more relation instances (two orders of magnitude) and more varied relations. To deal withthis structure and size, we implemented a novel metric. The structure is still a network, so the new methods still rely onconnecting nodes in this network. From the methods implemented for WordNet we have learned that not all concepts aretreated the same: For establishing similarity, nodes in the WordNet is_a hierarchy are considered more “important” the morespecific they are, or the higher their information content relative to a corpus [41]. This observation is particularly relevantto our network, where nodes representing concepts such as PEOPLE or CITIES have hundreds of thousand of instances. Whencomputing paths between concepts, expanding such a node causes a massive growth in the number of partial paths, whichmakes the computation very expensive both in terms of time and of CPU. On the other hand, such nodes do serve to connectmany different concepts, and should be taken into account, as should the concepts they relate to through relations otherthan is_a.To connect two nodes in the network while keeping the computation within a reasonable amount of time, the numberof partial paths must be kept small. There are two ways to control this: have an upper bound on the number of steps takenfrom the starting node, and control the paths taken from each node reached. The upper bound for the path length betweenany two nodes is 2 ∗ maxDepth, where maxDepth = 16 is the maximum depth of the network22 computed with respect tothe original category hierarchy. This translates to an upper bound of 32 in the number of expansion steps.To control the multiplication of partial paths, we also impose a bound on which nodes to expand, and how. For example,in searching for paths we would not want to expand a node with all of its instances, especially for nodes that have thou-sands or even hundreds of thousands of children. But we would want to follow links that lead to their supercategories, orsuperconcepts, for instance. Because of this, the bound imposed on expanding concepts is as follows: For each node Nx with20 http://heiner.cl.uni-heidelberg.de.21 For set 1 there were 8 instances for which we could not identify a corresponding entry in WikiNet’s index of concepts, and for set 2 there were 16 suchcases. This situations arises from the fact that a non-ambiguous name in the version used by Wentland et al. [48] has become ambiguous in our version.22 For the 2009/07/13 English Wikipedia version.78V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–85relations rxi , and relation counts crxi , relations of the type rxi are expanded if and only if crxidetermined empirically. In our experiments we set τr = 10.(cid:3) τr where τr is a threshold3.3.2. Computing relatednessTo compute the similarity between two terms we proceed as usually done, by determining all possible concepts theterms may refer to, and then choosing the pair with the highest similarity score. The similarity score is computed by takingall paths between the corresponding starting nodes into account. To develop the similarity measure we used the 30 pairsin the Miller and Charles (M&C ) set. We experimented with various formulas, starting from those developed for WordNet.The formula that gave best results is presented, justified and evaluated in the remainder of the section. We make no claimthat this is the best measure; it should rather be considered a lower bound. To compute the similarity, nodes closest to thelower level of the hierarchy (and thus more specific) and those closest to the edges of the path have a higher value. Thevalue decreases as we go higher up in the hierarchy, and further from the starting nodes. A path’s value is lower the longerit is and the higher up in the hierarchy its nodes are.(cid:7)sim(nx, n y) =path(nx,n y )∈Paths(nx,n y) value(path(nx,n y))|Paths(nx, n y)|Paths(nx, n y) = {path(nx, n y)} is the set of paths from nx to n y , and value(path(nx,n y )) is the value of the path, computed asfollows:value(path(nx,n y)) =1 +(cid:7)ni ∈path(nx,n y )1value(ni, path(nx,n y))where value(ni, path(nx,n y )) is the value of node ni on path path(nx,n y ):value(ni, path(nx,n y)) =ic(ni)D − depth(ni) + level(ni, path(nx,n y)) + 1D is the maximum depth of the category hierarchy, depth(ni) is the depth of the node ni in this hierarchy, level(ni, path(nx,n y ))is the distance from ni to the closest of nx or n y , and ic(ni) is an approximation of the information content for node ni ,computed in the same manner as done by Ponzetto and Strube [36]:ic(ni) = 1 − log(hypo(ni) + 1)log(|Wkn|)|Wkn| is the number of nodes in WikiNet, and hypo(ni) is the number of hyponyms of node ni , computed as the number ofpages subsumed by ni in Wikipedia’s category network.In using similarity/relatedness measures between word pairs it is assumed that choosing the highest score leads to thecorrect disambiguation of the words in the pair (e.g. [4]). In previous work on evaluating WordNet, Wikipedia or otherresources relative to similarity or relatedness computations, the issue of disambiguation has been disregarded, and thefocus has been on obtaining good correlation scores with the human judges. In our case the assumption that words in apair disambiguate each other to the correct senses does not hold. WikiNet is highly interconnected and contains numerouslexicalization variations for the included concepts. Because of this, all measures overestimate similarity/relatedness, becausefor most term pairs there are senses that are close or closely connected in the network. To evaluate the properties of thenetwork built we separate the two tasks – disambiguating the concepts, and evaluating the similarity/relatedness measureagainst the manually assigned scores. Two judges have manually assigned concept IDs to the words in the Miller andCharles (M&C ) and the Rubenstein and Goodenough (R&G) lists. More than one ID was allowed, and only IDs that hadassociated the particular term as a lexicalization were allowed as options. This caused some problems for the word journey,for example, whose best sense (as trip or voyage) cannot be retrieved from the information extracted from the Wikipediadump: Journey is a disambiguation page, and while the first sentence is very helpful for the human reader (A journey is atrip or voyage), it does not contain a hyperlink to either of these concepts.The disambiguated versions produced by the two human judges for each of M&C and R&G were intersected. The factthat for both data sets each word had at least one sense (ID) upon which the annotators agreed allows us to directly usethe intersective annotations, without further need for adjudication. The results on the gold standards thus obtained and onthe non-annotated data are presented in Table 8.One of the strengths of the resource is its multilinguality. To test this feature we used the German version of the R&Glists – G [12]. We had fewer German lexicalizations of concepts than English ones; this reduced the number of possibleambiguities, which is reflected in the relatively high scores for this data set even without disambiguating annotations. Eightwords from the G data did not appear in the multilingual index: Grinsen and Mittagsstunde (do not appear in the GermanWikipedia), Irrenhaus had no overlap through cross-language links, and Schnur, Bursche, Fahrt do not have an appropriatesense. This leaves 45 pairs (out of the original 65) for which both terms had concept correspondents in WikiNet, and 51pairs when manually disambiguated and the judges assigned a sense to Irrenhaus.V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8579Table 8Evaluation of similarity on the Miller and Charles, Rubenstein and Goodenough and Gurevych sets.DatasetM&CResourceWikiNetWikiTaxonomyWordNetWikiNetR&GWikiTaxonomyWordNetWikiNetGWikiTaxonomyGermaNetdisambigrawrawrawdisambigrawrawrawdisambigrawrawrawPearsonSpearman0.860.590.870.860.700.660.780.880.720.620.690.760.830.580.790.860.670.640.750.860.720.63––The best previous results on semantic similarity/relatedness based on Wikipedia were reported on WikiTaxonomy byPonzetto and Strube [37]. Using a taxonomy built based on the category network, they report highest results for Resnik’sinformation content-based measure – 0.87 (Pearson) and 0.79 (Spearman) for M&C and 0.78 (Pearson) and 0.75 (Spearman)for R&G, computed without manual disambiguation on a 2008 version of Wikipedia with 337 741 categories and 2 276 274articles. Since then the resource has grown, and it is now bigger, more connected and has more ambiguities. A directcomparison based on the 2009/07/13 English Wikipedia dump was not possible. The best results based on WordNet forR&G are 0.8614 (Spearman) and 0.876 (Pearson), and for M&C 0.856 (Spearman) and 0.864 (Pearson) [46].We tested WikiNet on the German version of the Rubenstein and Goodenough word pairs. On this data set Ponzetto andStrube [35] report 0.69 Pearson correlation on WikiTaxonomy, while the best score on GermaNet – the German version ofWordNet – was 0.76 [12]. The reported scores were computed on the pairs for which both terms had an entry in Wikipedia.WikiNet’s high scores on the manually disambiguated data is additional (to the intrinsic evaluation) proof of the qualityof its underlying structure. On the other hand, the relatively low scores on non-disambiguated data – the more realisticsetting in which WordNet is the clear winner – shows that more information is needed to disambiguate terms. The semanticsimilarity task between two words out of context is artificial, as it assumes that the two words disambiguate each other,and the correct senses are the ones closest according to the similarity/relatedness metric. This, however, does not hold ina resource like WikiNet, with highly interconnected concepts. To better assess WikiNet we use it in an NLP task, which wedescribe in the following section.3.4. Metonymy resolutionMetonymies are figures of speech whereby the speaker is “using one entity to refer to another that is related to it.” [15]– e.g. in the news article sentence ‘Buckingham Palace announced at 8am on Friday that the Queen had bestowed the title of Dukeof Cambridge on her grandson.’, Buckingham Palace stands for the representative of the Queen whose official residence is thepalace.The task of metonymy resolution implies identifying the correct interpretation of a term in context. For example, theinterpretation of the term New Zealand in the text fragment shown in Fig. 8 is not literal – as the country New Zealand –rather it stands in for a sports team representing the country in a sporting event.The most common view of metonymies is that they violate semantic constraints in their immediate context. To resolvemetonymies one must detect violated constraints, usually from those imposed by the verbs on their arguments [8,13,40].Most of the recent work on metonymy resolution (for an overview see [21]) relies on syntactic clues from the local sententialcontext of a potentially metonymic word (PMW) to determine which interpretation (reading) is most appropriate. In theexample from Fig. 8, the noun kicker suggests a place-for-people reading, because sports teams have kickers, ratherthan countries. Previously, Markert and Hahn [19] have shown that global context is useful in detecting metonymies thatdo not violate selectional restrictions. In this case one can use referential cohesion relations. At the time comprehensiveknowledge bases were not available to help establish such relations. We test the usefulness of WikiNet for this task.We use the data for the metonymy task at SemEval 2007 [21], in which the PMW are names of countries and companies.The data comes partitioned into training and testing (for country PMWs 925 training and 908 testing instances, for companyPMWs 1090/842). As shown in Fig. 8, there is a larger context surrounding the PMW than just the corresponding sentence.We explore this for global constraints on the interpretation of the target word. The starting point is the system described in[28], which uses, as other work does, selectional preferences imposed by the grammatical context to decide on the correctinterpretation.80V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–85<sample id="samp369"><bnc:title> [Title unknown/unassigned] </bnc:title><par>If the Lions trip here is to be worthwhile in terms of Test results and tour moraleit is essential that this one is a victory. New Zealand have not played together as aunit since last August in South Africa, where they narrowly beat the Springboks. “Tenmonths is a long time to be apart and then to be brought together to compete againstsuch a good, experienced team as the Lions,” said Grant Fox, <annot><location read-ing="metonymic" metotype="place-for-people" notes="ORG, sports, team"> New Zealand</location></annot>’s masterly goal kicker. Fox’s kicking is crucial to the AllBlack’s game but there are inevitably, other great and experienced winners around him.</par></sample>Fig. 8. Sample data from the metonymy resolution task at SemEval 2007.To incorporate encyclopedic knowledge, we first identify concepts in the given paragraph surrounding a PMW usingthe existing lexicalizations in WikiNet. Solving ambiguities is done implicitly (albeit not necessarily very accurately) in theprocessing steps presented in Algorithm 5.Algorithm 5 Extracting concepts for metonymy resolution.Input:T = {ti } – the set of texts given for each instance in the data WikiNetν – a threshold for related concepts frequency within a paragraphτ – a threshold for concept (feature) frequency in the data setOutput:C – a list of concepts1: C = {}2: for ti ∈ T doCi = {}3:for ck ∈ WikiNet do4:5:6:7:8:9:10: return CCik = {}if ck appears in ti thenCik = Cik ∪ {ck} ∪ {c j | (ck, R, c j ) ∈ WikiNet}Ci = Ci ∪ {c j | c j appears in at least νCik}C = C ∪ {c j | c j appears in at least τ Ci }The purpose of the ν threshold is to compensate for the rather simple disambiguation algorithm: If a concept is relatedto at least ν other (potential) concepts in the given context, it is more probable that it is an appropriate concept for thiscontext. For the experiments described later in the paper ν = 3 – it is the smallest value that counters possible noise inconnectivity in WikiNet and cohesiveness of the context (in terms of concepts). The τ threshold is used to filter features forthe machine learning stage. Features infrequent across instances would cause overfitting. We arbitrarily choose τ = 5.We consider the identified concepts as global context features, and added them to the features described in [28]: thesyntactic features proposed in [20] and selectional preference features.The features described above are used to represent each instance in the data. We used the Weka implementation ofSupport Vector Machine (SMO) [49] (default settings) to build a model for each metonymy type. The task had three set-tings: coarse – distinguish between literal and non-literal interpretations; medium – distinguish between literal, metonymicor mixed interpretations; fine – distinguish between all possible interpretations (a small number or prespecified possibleinterpretations) for the potentially metonymic word.Table 9 shows the (non-zero) results obtained, in all three task settings, for the two types of PMWs – locations andorganizations. Base shows the class distribution, the Selectional preferences columns show the results obtained using selec-tional preferences (described in detail in [28]), and the + WikiNet information shows the results obtained by adding theconcepts identified in the paragraph surrounding the PMW to the selectional preference features. Including the encyclope-dic knowledge from WikiNet shows consistent improvement in terms of precision and recall, especially for the non-literalinterpretations. Because of the relatively small number of instances with a non-literal interpretation, the improvement doesnot appear to be statistically significant.The evaluation shows that global context is useful in interpreting PMWs despite the simple concept identification ap-proach used.4. WikiNet in contextThe work described in this paper fits within the rather vast and intensively explored areas of knowledge acquisition andontology induction. Unlike the majority of other such work which involves Wikipedia, our approach does not tie WikipediaV. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8581Table 9Accuracy (acc), precision (f), recall (r) and F-score (f) for detecting metonymic interpretations.TaskMethodBaseacc79.479.479.461.861.861.8location – coarseliteralnon-literallocation – mediumliteralmetonymicmixedlocation – fineliteralplace-for-peoplemixedorganization – coarseliteralnon-literalorganization – mediumliteralmetonymicmixedorganization – fineliteralorg-for-membersorg-for-productorg-for-facilityorg-for-namemixedSelectional preferences+ WikiNet informationfaccprfaccprf85.685.284.474.271.770.379.420.679.418.42.279.415.52.261.838.261.831.07.261.819.18.02.00.77.287.971.587.970.133.387.964.333.374.573.574.565.850.074.559.368.860.045.550.094.949.794.953.35.094.957.45.088.750.988.750.219.788.755.332.818.883.320.091.358.791.360.58.791.360.78.780.960.280.956.928.280.957.244.428.658.828.686.285.985.175.473.371.788.174.688.173.640.088.167.540.075.475.375.468.457.175.461.866.783.350.057.195.650.395.653.310.095.657.410.089.253.189.254.019.789.258.435.831.383.320.091.760.191.761.816.091.762.116.081.862.381.860.429.381.860.146.645.562.529.6to additional resources, but is instead focused on generating a self-contained, easily renewable resource. We review themost closely related work in decreasing order of the similarity of either the approach or resource generated.Ponzetto and Strube [35,37] build on the category network from Wikipedia and induce is_a links based on severalcriteria: head matching, modifier matching, structural analysis of shared categories and pages between two linked categories,and patterns indicative of is_a relations and not_is_a relations. The result is WikiTaxonomy, with 208 208 is_a relations,evaluated at 84.0% F-score.DBpedia23 [1,3] converts Wikipedia’s content into structured knowledge using information from Wikipedia’s relationaldatabase tables and the structured information in infoboxes, and connects it to a variety of other resources, such as Freebase,WordNet, OpenCyc and more. The DBpedia dataset covers approximately 3.64 million entities, 1.83 million of which areclassified into a shallow ontology consisting of 170 classes.24 This ontology was built by manually organizing the mostfrequently used infobox templates from Wikipedia into a hierarchy. A distinguishing feature of DBpedia is live extraction:Its database is continuously updated whenever a Wikipedia article is changed. DBpedia offers a web-based interface to itsdatabase.YAGO [47] also extracts information from Wikipedia, and links it to GeoNames and WordNet’s hierarchies to takeadvantage of WordNet’s manually produced taxonomy. In addition to structured information in Wikipedia, facts represent-ing relations of 100 types are extracted from specific categories that provide relational information, such as 1975 births,categories starting with Countries in ..., Rivers of ..., Attractions in ...,. These are used as a source of the following rela-tions: bornInYear, diedInYear, establishedIn, locatedIn, writtenInYear, politicianOf, hasWonPrize. Accuracy is estimated based ona small sample of manually annotated relation instances out of the approximately 10 million extracted, and lies between90.84 ± 4.28% and 98.72 ± 1.30%. YAGO’s core knowledge base (derived from Wikipedia) contains approximately 11 mil-lion relations, while the full system covers 460 million. Distinguishing characteristics of YAGO are its accompanying searchengine NAGA, and SOFIE, a logical reasoning mechanism for expanding YAGO with information from open texts and forchecking internal consistency of the resource when adding external facts.23 http://dbpedia.org.24 December 2011.82V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–85MENTA [6] is a multilingual extension of the core YAGO knowledge base. It extracts entities from Wikipedia versions inall available languages, linking them through cross-language links, category–category and category–article links, and linksthem to WordNet. Evaluated on random samples between 104 and 322 instances, the accuracy of subclass and instance-oflinks to WordNet ranges between 83.38% and 92.30%. The resource covers 5.4 million entities.BabelNet [30] is an automatically built resource that integrates WordNet and Wikipedia. It is obtained by disambiguatingWikipedia articles relative to WordNet senses, by adding different language information using the corresponding cross-language links, and by adding synonyms using redirect links and selected output from a machine translation system.The current version includes lexicalizations of existing WordNet synsets in 6 languages. In version 1.0.1, BabelNet con-tains 83 156 merged Wikipedia/WordNet entities, and provides 2 955 552 additional multilingual “babel-synsets”. Because ofthe Wikipedia–WordNet mapping, BabelNet can take advantage of WordNet’s clean hierarchy.The largest ongoing project for acquiring knowledge from general texts is the Machine Reading project at the Universityof Washington [39]. One of the goals of the project is to induce a large scale ontology, which is now pursued throughmultiple interacting threads: rely on Wikipedia for annotated data from which to learn models for acquiring relationsfrom open text [2,51,52], jointly perform knowledge extraction, ontology induction and population via recursive relationalclustering [38]. Kylin [50] uses the existing infoboxes and their corresponding articles as sources of training data, to learnhow to fill in infobox templates for articles which do not have such structured information. For four concepts, Wu and Weld[50] obtain precision between 73.9% and 97.3%, and recall between 60.5% and 95.9%. Read the Web25 is a Never-EndingLanguage Learning (NELL) project at Carnegie Mellon University [16]. It has an iterative approach of adding more and morefacts extracted from open text. As of yet, its results are not at the level of the Machine Reading project, with 978 383 factsextracted.Nguyen et al. [32] filter article sentences, parse and analyze them for entity detection and keyword extraction. Theseelements are used to learn how to detect instances of previously seen relations, with 37.76% F-score.Snow et al. [44] present an approach for automatically inducing semantic taxonomies, which relies on combining evi-dence from heterogeneous relations to derive the structure with the highest probability. This approach can be used to builda taxonomy from scratch, or to expand upon an existing one, such as WordNet. This approach is based on representing wordpairs through lexico-syntactic patterns that capture how the pair is connected in sentences in the corpus. Because it dealswith open texts, the method must address the sense disambiguation task as well. For 10 000 added hyponyms, precisionwas 84% (evaluated on a random sample of 100 pairs), for 20 000 precision was 68% (also on a sample of 100 pairs).Navigli et al. [31] also investigate the automatic induction of semantic taxonomies from a given corpus. The processinvolves first obtaining a terminology from the corpus, which is then used iteratively to obtain taxonomic relations and newpotential terms from the texts. In the final step, the graph is trimmed based on connectivity information and the restrictionsimposed by taxonomic relations.Freebase26 is a large online collaborative knowledge base, originally populated with information from Wikipedia, Mu-sicBrainz, and other online resources, open for editing by human contributors. Compared to DBpedia and YAGO, Freebase isstructured as a graph, with entities as nodes and edges representing the links between them. It consists of approximately22 million entities (called topics in Freebase),27 which are grouped into types (e.g. people, places), and which can haveassociated attributes (e.g. birth date). While it does have multilingual content, there are no explicit links between the sameconcept in different languages.ProBase28 [53] is a probabilistic taxonomy automatically built from a corpus of 1.68 billion web pages and uses two years’worth of search log data for filtering concepts. Taxonomy induction starts with Hearst patterns for detecting is_a relations inthe corpus, which are used to detect the concepts, distinguish their senses, and establish is_a links between them. ProBasecontains 2 653 872 concepts, 16 218 369 distinct concept–instance pairs, and 4 539 176 distinct concept–subconcept pairs(20 757 545 is_a pairs in total).Related to our work on deriving class attributes, Pa ¸sca [34] processes search engine queries to obtain similar information.The idea is that when writing a query, users have some elements of a relation on which they require further information– such as side effects for the class drugs, or wing span for the class aircraft model. From extensive logs of even noisy queries,a weakly supervised system can acquire large sets of relevant class attributes. Similarity between automatically ranked classattributes and manually assigned correctness labels on a sample of extracted attributes for the 40 classes considered rangefrom 90% precision for 10 attributes to 76% for 50.Compared to WikiTaxonomy, DBpedia, MENTA, and BabelNet, in building WikiNet we have revealed implicit knowledgein Wikipedia, and used it to derive novel connections between entities in Wikipedia. YAGO processes a limited number ofspecific name patterns to derive novel information, although not on the same scale as WikiNet. We also worked under theassumption that each manually generated resource (such as Wikipedia, WordNet, OpenCyc) was built according to its ownprinciples and captures different aspects of linguistic or encyclopedic knowledge. Because of this, we aimed for WikiNet tobe self-contained, and endowed it with the type of knowledge that research in the natural language processing field hasshown to be useful and desirable (such as a taxonomy backbone).25 http://rtw.ml.cmu.edu/rtw/.26 http://www.freebase.com.27 December 2011.28 http://research.microsoft.com/en-us/projects/probase.V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8583Deriving a resource like WikiNet has both advantages and disadvantages. The disadvantage is that, while it can serve asa starting point for further development, WikiNet is limited to the information included in Wikipedia. There are, however,multiple advantages: (i) we do not need to rely on external resources for the set of entities to be extracted or to validate thetype of relations that are extracted; (ii) the structure arises through the building process, and combines the “folksonomy”that is the backbone of Wikipedia with a structure that arises from the knowledge decoded from the category names;(iii) it is clear how the extracted relations fit in the network and we do not have the problem of disambiguating termsextracted from text to their corresponding concepts; (iv) multilinguality. Thanks to the connection between languages inWikiNet, information easily obtained through one language can be ported to others. An example of this is the named entityinformation, which based on the article texts and the conventions of capitalization in English can be obtained with highaccuracy, whereas for German this would not be as easy, because nouns in German are capitalized.The work presented here was developed based on observations about the types of information that appear in Wikipediaspecifically, and the way those types of information interact. However, some of the lessons learned are more general. Wehave learned how knowledge can be propagated throughout a network by connecting different pieces of individual informa-tion. This provides interesting avenues for further knowledge acquisition: A novel entity discovered in open text can first beclassified under existing categories in Wikipedia, which then triggers the addition of links to the rest of the network throughthe relations implicit in the newly assigned parent categories. We have also learned that knowledge may come from uncon-ventional, and probably “unintentional”, sources. In Wikipedia’s case, these sources are the category names (there may beothers as well). Such situations may arise in processing other repositories of (manually built) information where knowledgeis implicitly coded, for example, in directory names in a file hierarchy or in class names in manual annotations.A question that we cannot yet answer is what is the impact of the structure – which is derived exclusively from theEnglish Wikipedia – when applying the resource to NLP tasks in different languages, as we do not know to what extentontologies are language specific. We can only speculate that encyclopedic knowledge, the kind reorganized into WikiNet, isless prone to cultural biases.5. ConclusionsThis paper described the construction of a multilingual, large scale resource, based on exploiting several facets – someobvious, some less so – of Wikipedia. Compared to related work in the domain of knowledge acquisition/ontology induction,the approach presented and the resource built has both advantages and disadvantages. Its main advantages are the rapidderivation of a large multilingual resource, easy to regenerate on new versions of Wikipedia. It can serve both Englishand languages poorer in resources, either as a stable resource or as a starting point for ontology population. Compared toWordNet, WikiNet has high coverage of named entities, and named entity lexicalizations in various languages are a usefulresource for machine translation. Another advantage is the ability to port information – such as named entity – from onelanguage to another. The main disadvantage is that WikiNet is not complete, and in the long run it is only a seed – althougha large one – for further knowledge acquisition.We plan to explore methods for enriching WikiNet with information extracted from open text, following an approachsimilar to the one for propagating information from infoboxes, by first linking (classifying) a novel entity to Wikipedia’s cat-egories, and then linking it to the rest of the network through the relations implied in its newly assigned parent categories.Future work plans also include further exploring WikiNet’s use for different tasks, including coreference resolution and textalignment. Identifying common concepts in different language texts should arise naturally from WikiNet’s structure. Theresource is freely available for download,29 together with a tool kit – WikiNetTK30 – for visualization and various methodsto facilitate embedding the world knowledge encoded in WikiNet into applications [14]. The scripts used to build WikiNetare also available at the WikiNet download site.AcknowledgementsWe thank the reviewers for their detailed and constructive comments, and the Klaus Tschira Foundation, Heidelberg forfinancial support. This work has also been partially supported by the EC-funded project CoSyne (FP7-ICT-4-24853).References[1] S. Auer, C. Bizer, J. Lehmann, G. Kobilarov, R. Cyganiak, Z. Ives, DBpedia: A nucleus for a Web of open data, in: Proceedings of the 6th InternationalSemantic Web Conference and 2nd Asian Semantic Web Conference, Busan, Korea, November 11–15, 2007, pp. 722–735.[2] M. Banko, M.J. Cafarella, S. Soderland, M. Broadhead, O. Etzioni, Open information extraction from the Web, in: Proceedings of the 20th InternationalJoint Conference on Artificial Intelligence, Hyderabad, India, 6–12 January 2007, pp. 2670–2676.[3] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, S. Hellmann, DBpedia – A crystallization point for the Web of data, Journal of WebSemantics 7 (2009) 154–165.[4] A. Budanitsky, G. Hirst, Evaluating WordNet-based measures of semantic distance, Computational Linguistics 32 (1) (2006) 13–47.29 http://www.h-its.org/english/research/nlp/download/wikinet.php.30 http://sourceforge.net/projects/wikinettk/.84V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–85[5] R. Bunescu, M. Pa ¸sca, Using encyclopedic knowledge for named entity disambiguation, in: Proceedings of the 11th Conference of the European Chapterof the Association for Computational Linguistics, Trento, Italy, 3–7 April 2006, pp. 9–16.[6] G. de Melo, G. Weikum, MENTA: Inducing multilingual taxonomies from Wikipedia, in: Proceedings of the ACM 19th Conference on Information andKnowledge Management (CIKM 2010), Toronto, Ont., Canada, 26–30 October 2010, pp. 1099–1108.[7] G. de Melo, G. Weikum, Untangling the cross-lingual link structure of Wikipedia, in: Proceedings of the 48th Annual Meeting of the Association forComputational Linguistics, Uppsala, Sweden, 11–16 July 2010, pp. 844–853.∗[8] D.C. Fass, met[9] C. Fellbaum (Ed.), WordNet: An Electronic Lexical Database, MIT Press, Cambridge, MA, 1998.: A method for discriminating metonomy and metaphor by computer, Computational Linguistics 17 (1) (1991) 49–90.[10] E. Gabrilovich, S. Markovitch, Computing semantic relatedness using Wikipedia-based explicit semantic analysis, in: Proceedings of the 20th Interna-tional Joint Conference on Artificial Intelligence, Hyderabad, India, 6–12 January 2007, pp. 1606–1611.[11] E. Gabrilovich, S. Markovitch, Wikipedia-based semantic interpretation for natural language processing, Journal of Artificial Intelligence Research 34(2009) 443–498.[12] I. Gurevych, H. Niederlich, Accessing GermaNet data and computing semantic relatedness, in: Companion Volume to the Proceedings of the 43rdAnnual Meeting of the Association for Computational Linguistics, Ann Arbor, Mich., 25–30 June 2005, pp. 5–8.[13] J.R. Hobbs, M.E. Stickel, D.E. Appelt, P. Martin, Interpretation as abduction, Artificial Intelligence 63 (1993) 69–142.[14] A. Judea, V. Nastase, M. Strube, WikiNetTk – A tool kit for embedding world knowledge in NLP applications, in: Proceedings of the IJCNLP 2011 SystemDemonstrations, Chiang Mai, Thailand, 9 November 2011, pp. 1–4.[15] G. Lakoff, M. Johnson, Metaphors We Live By, University of Chicago Press, Chicago, IL, 1980.[16] N. Lao, T.M. Mitchell, W.W. Cohen, Random walk inference and learning in a large scale knowledge base, in: Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing, Edinburgh, UK, 27–29 July 2011, pp. 529–539.[17] M. Lauer, Designing statistical language learners: Experiments on noun compounds, Ph.D. thesis, Macquarie University, Dept. of Computing, Sydney,Australia, 1995.[18] D.B. Lenat, R. Guha, K. Pittman, D. Pratt, M. Shepherd, Cyc: Towards programs with common sense, Communications of the ACM 33 (8) (1990) 30–49.[19] K. Markert, U. Hahn, Metonymies in discourse, Artificial Intelligence 135 (1/2) (2002) 145–198.[20] K. Markert, M. Nissim, Comparing knowledge sources for nominal anaphora resolution, Computational Linguistics 31 (3) (2005) 367–401.[21] K. Markert, M. Nissim, SemEval-2007 Task 08: Metonymy resolution at SemEval-2007, in: Proceedings of the 4th International Workshop on SemanticEvaluations (SemEval-1), Prague, Czech Republic, 23–24 June 2007, pp. 36–41.[22] O. Medelyan, D. Milne, C. Legg, I.H. Witten, Mining meaning from Wikipedia, International Journal of Human–Computer Interaction 67 (9) (2009)716–754.[23] A. Meyers, C. Macleod, R. Yangarber, R. Grishman, L. Barrett, R. Reeves, Using NOMLEX to produce nominalization patterns for information extraction,in: Proceedings of the COLING-ACL ’98 Workshop on The Computational Treatment of Nominals, Montréal, Québec, Canada, 16 August 1998, pp. 25–32.[24] G.A. Miller, W.G. Charles, Contextual correlates of semantic similarity, Language and Cognitive Processes 6 (1) (1991) 1–28.[25] D. Milne, I.H. Witten, An effective, low-cost measure of semantic relatedness obtained from Wikipedia links, in: Proceedings of the Workshop onWikipedia and Artificial Intelligence: An Evolving Synergy at AAAI-08, Chicago, Ill., 13 July 2008, pp. 25–30.[26] D. Milne, I.H. Witten, Learning to link with Wikipedia, in: Proceedings of the ACM 17th Conference on Information and Knowledge Management (CIKM2008), Napa Valley, Cal., USA, 26–30 October 2008, pp. 1046–1055.[27] V. Nastase, M. Strube, Decoding Wikipedia category names for knowledge acquisition, in: Proceedings of the 23rd Conference on the Advancement ofArtificial Intelligence, Chicago, IL, 13–17 July 2008, pp. 219–1224.[28] V. Nastase, M. Strube, Combining collocations, lexical and encyclopedic knowledge for metonymy resolution, in: Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Processing, Singapore, 6–7 August 2009, pp. 910–918.[29] V. Nastase, M. Strube, B. Börschinger, C. Zirn, A. Elghafari, WikiNet: A very large scale multi-lingual concept network, in: Proceedings of the 7thInternational Conference on Language Resources and Evaluation, La Valetta, Malta, 17–23 May 2010.[30] R. Navigli, S.P. Ponzetto, BabelNet: Building a very large multilingual semantic network, in: Proceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, Uppsala, Sweden, 11–16 July 2010, pp. 216–225.[31] R. Navigli, P. Velardi, S. Faralli, A graph-based algorithm for inducing lexical taxonomies from scratch, in: Proceedings of the 22nd International JointConference on Artificial Intelligence, Barcelona, Spain, 19–22 July 2011, pp. 1872–1877.[32] D.P. Nguyen, Y. Matsuo, M. Ishizuka, Relation extraction from Wikipedia using subtree mining, in: Proceedings of the 22nd Conference on the Advance-ment of Artificial Intelligence, Vancouver, B.C., Canada, 22–26 July 2007, pp. 1414–1420.[33] I. Niles, A. Pease, Towards a standard upper ontology, in: Proceedings of the International Conference on Formal Ontology in Information Systems,Ogunquit, Maine, 17–19 October 2001, pp. 2–9.[34] M. Pa ¸sca, Organizing and searching the World Wide Web of facts – Step two: Harnessing the wisdom of the crowds, in: Proceedings of the 16th WorldWide Web Conference, Banff, Canada, 8–12 May 2007, pp. 101–110.[35] S.P. Ponzetto, M. Strube, Deriving a large scale taxonomy from Wikipedia, in: Proceedings of the 22nd Conference on the Advancement of ArtificialIntelligence, Vancouver, B.C., Canada, 22–26 July 2007, pp. 1440–1445.[36] S.P. Ponzetto, M. Strube, Knowledge derived from Wikipedia for computing semantic relatedness, Journal of Artificial Intelligence Research 30 (2007)181–212.[37] S.P. Ponzetto, M. Strube, Taxonomy induction based on a collaboratively built knowledge repository, Artificial Intelligence 175 (9/10) (2011) 1737–1756.[38] H. Poon, J. Christensen, P. Domingos, O. Etzioni, R. Hoffmann, C. Kiddon, T. Lin, X. Ling, Mausam, A. Ritter, S. Schoenmackers, S. Soderland, D. Weld, F.Wu, C. Zhang, Machine reading at the University of Washington, in: Proceedings of the NAACL HLT 2010 First International Workshop on Formalismsand Methodology for Learning by Reading, Los Angeles, CA, 6 June 2010, pp. 87–95.[39] H. Poon, P. Domingos, Unsupervised ontology induction from text, in: Proceedings of the 48th Annual Meeting of the Association for ComputationalLinguistics, Uppsala, Sweden, 11–16 July 2010, pp. 296–305.[40] J. Pustejovsky, The generative lexicon, Computational Linguistics 17 (4) (1991) 209–241.[41] P. Resnik, Using information content to evaluate semantic similarity in a taxonomy, in: Proceedings of the 14th International Joint Conference onArtificial Intelligence, Montréal, Canada, 20–25 August 1995, vol. 1, 1995, pp. 448–453.[42] H. Rubenstein, J.B. Goodenough, Contextual correlates of synonymy, Communications of the ACM 8 (10) (1965) 627–633.[43] B. Santorini, Part of speech tagging guidelines for the Penn Treebank Project, http://www.cis.upenn.edu/~treebank/home.html, 1990.[44] R. Snow, D. Jurafsky, A.Y. Ng, Semantic taxonomy induction from heterogeneous evidence, in: Proceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Sydney, Australia, 17–21 July 2006, pp. 801–808.[45] F. Suchanek, G. Kasneci, G. Weikum, YAGO: A large ontology from Wikipedia and WordNet, Elsevier Journal of Web Semantics 6 (3) (2008) 203–217.[46] G. Tsatsaronis, I. Varlamis, M. Vazirgiannis, Text relatedness based on a word thesaurus, Journal of Artificial Intelligence Research 37 (2010) 1–39.[47] G. Weikum, G. Kasneci, M. Ramanath, F. Suchanek, Database and information-retrieval methods for knowledge discovery, Communications of theACM 52 (4) (2009) 56–64.[48] W. Wentland, J. Knopp, C. Silberer, M. Hartung, Building a multilingual lexical resource for named entity disambiguation, translation and transliteration,in: Proceedings of the 6th International Conference on Language Resources and Evaluation, Marrakech, Morocco, 26 May–1 June 2008.V. Nastase, M. Strube / Artificial Intelligence 194 (2013) 62–8585[49] I.H. Witten, E. Frank, Data Mining: Practical Machine Learning Tools and Techniques, 2nd edition, Morgan Kaufmann, San Francisco, CA, 2005.[50] F. Wu, D. Weld, Automatically semantifying Wikipedia, in: Proceedings of the ACM 16th Conference on Information and Knowledge Management (CIKM2007), Lisbon, Portugal, 6–9 November 2007, pp. 41–50.[51] F. Wu, D. Weld, Automatically refining the Wikipedia infobox ontology, in: Proceedings of the 17th World Wide Web Conference, Beijing, China, 21–25April 2008.[52] F. Wu, D. Weld, Open information extraction using Wikipedia, in: Proceedings of the 48th Annual Meeting of the Association for ComputationalLinguistics, Uppsala, Sweden, 11–16 July 2010, pp. 118–127.[53] W. Wu, H. Li, H. Wang, K.Q. Zhu, Towards a probabilistic taxonomy of many concepts, Tech. Rep. MSR-TR-2011-25, Microsoft Research Asia, 2011.