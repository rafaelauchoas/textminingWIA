Artificial Intelligence 137 (2002) 217–238www.elsevier.com/locate/artintA hybrid graphical model for rhythmic parsing ✩Christopher RaphaelDepartment of Mathematics and Statistics, University of Massachusetts, Amherst, MA, USAReceived 4 October 2001AbstractA method is presented for the rhythmic parsing problem: Given a sequence of observed musicalnote onset times, we simultaneously estimate the corresponding notated rhythm and tempo process.A graphical model is developed that represents the evolution of tempo and rhythm and relates thesehidden quantities to an observable performance. The rhythm variables are discrete and the tempo andobservation variables are continuous. We show how to compute the globally most likely configurationof the tempo and rhythm variables given an observation of note onset times. Experiments arepresented on both MIDI data and a data set derived from an audio signal. A generalization tocomputing MAP estimates for arbitrary conditional Gaussian distributions is outlined.  2002Elsevier Science B.V. All rights reserved.Keywords: Conditional Gaussian distribution; Rhythmic parsing; MAP estimate; Dynamic programming; Musicrecognition; Hybrid graphical model1. IntroductionRhythm is the aspect of music that deals with when events occur. Typically, rhythm inWestern music is notated in a way that expresses the position of each note as a rationalnumber, usually in terms of some relatively small common denominator. For instance, ifwe use the measure as our unit of notated position, then the sequence of measure positionsm0 = 0, m1 = 1/4, m2 = 1, . . . expresses the notion that the first note occurs at thebeginning of the 1st measure, the second note occurs 1/4 the way through the 1st measure,the third note occurs at the beginning of the 2nd measure, etc. If the music is performedwith mechanical precision then a single number, the tempo, will map the measure positions✩ This is an extended version of the paper presented at the 17th Conference on Uncertainty in ArtificialIntelligence (UAI-2001), Seattle, WA, USA. This work is supported by NSF grants IIS-0113496 and IIS-9987898.E-mail address: raphael@math.umass.edu (C. Raphael).0004-3702/02/$ – see front matter  2002 Elsevier Science B.V. All rights reserved.PII: S 0 0 0 4 - 3 7 0 2 ( 0 2 ) 0 0 1 9 2 - 3218C. Raphael / Artificial Intelligence 137 (2002) 217–238to actual times. For instance, if the tempo is 3 seconds per measure, then the notes wouldoccur at 0 secs, 3/4 secs, 3 secs, etc. However, such a performance would be nearlyimpossible for the human performer to create, and, moreover, would be undesirable. Muchof the expressive quality of a musical performance comes from the way in which the actualnote times deviate from what is prescribed by a literal interpretation of the printed music.In particular, there are two primary components to this expressive timing [8]. Firstly, theactual tempo is often not constant, but rather continually varied throughout the evolutionof the performance. Secondly, there are more local (note by note) distortions which can beaccidental, or can result from interpretive considerations.We focus here on a problem encountered in music information retrieval (MIR): Given asequence of measured note onset times, we wish to identify the corresponding sequence ofmeasure positions. We call this process rhythmic parsing. The time sequences formingthe input to our procedure could be estimated from an audio signal or could comedirectly from a MIDI (musical instrument digital interface) file—a sequence of time-taggedmusical events such as note beginnings and endings. For example, consider the data inthe left panel of Fig. 1 containing estimated note times from an excerpt of Schumann’s2nd Romance for Oboe and Piano (oboe part only). The actual audio file can be heardat http://fafner.math.umass.edu/rhythmic_parsing. Our goal is to assign the proper scoreposition, in measures, to each the observed times. When this is done correctly, as in Fig. 1,the observed times, in seconds, plotted against the score positions, in measures, trace out acurve whose local slope gives the player’s local tempo.Applications of rhythmic parsing are numerous. Virtually every commercial score-writing program now offers the option of creating scores by directly entering MIDIdata from a keyboard. Such programs must infer the rhythmic content from the actualtimes at which musical events occur and, hence, must address the rhythmic parsingproblem. When the input data is played with anything less than mechanical precision,the transcription degrades rapidly, due to the difficulty in computing the correct rhythmicparse. Rhythmic parsing also has applications in musicology where it could be usedto separate the inherently intertwined quantities of notated rhythm and expressivetiming. Either the rhythmic data or the timing information could be the focal pointof further study. Additionally, several applications of rhythmic parsing are related toefforts in music information retrieval, as follows. The musical world eagerly awaits thecompilation of music databases containing virtually every kind of (public domain) music,thereby facilitating the searching, studying, comparing, and understanding of music. Theconstruction of such data bases will likely involve several transcription efforts includingoptical music recognition, musical audio signal recognition, and MIDI transcription.Rhythmic parsing is an essential ingredient to the latter two efforts. Finally, the last decadehas seen a virtual explosion in music data available on the World Wide Web. Unfortunately,content-based searches analogous to those performed on text are not possible at the presenttime. However, if automated music transcription were to progress to a sufficient level,searchable descriptions of musical content could be constructed automatically. Such adevelopment would dramatically increase access to music on the web. Rhythmic parsingwill play a significant role in this endeavor too.As already mentioned, mostly commercial score-writing address the rhythmic parsingproblem. Usually these efforts attempt to quantize the observed note lengths, or moreC. Raphael / Artificial Intelligence 137 (2002) 217–238219Fig. 1. Left: Real time (seconds) vs. Musical time (measures) for the Schumann data. Right: The actual durations(seconds) of notes grouped by the musical duration (measures).precisely inter-onset intervals (IOIs), to their closest note values (eighth note, quarternote, etc.), given a known tempo, or to quantize the observed note onset times to theclosest points in a rigid grid [24]. While such quantization schemes can work reasonablywell when the music is played with robotic precision (often a metronome is used), theyperform poorly when faced with the more expressive and less accurate playing typicallyencountered. Consider the right panel of Fig. 1 in which we have plotted the written notelengths in measures versus the actual note lengths (IOIs) in seconds from our musicalexcerpt. The large degree of overlap between the empirical distributions of each note lengthclass demonstrates the futility of assigning note lengths through note-by-note quantizationin this example. In this particular example, the overlap in empirical distributions is mostlyattributable to tempo fluctuations in the performance.In addition to the commercial systems, we are also aware of several research effortsrelated to rhythm transcription. Some of this research addresses the problem of beatinduction, or tempo tracking in which one tries to accomplish the equivalent of “foot-tapping”—estimating a sequence of times corresponding to evenly spaced rhythmicintervals (e.g., beats) for a given sequence of observed note onset times [1,3,6,9,11–14].A complementary research effort addresses the problem of assigning rhythmic values assimple integer ratios to observed note lengths without any corresponding estimation oftempo [4,8,10]. The latter two assume that beat induction has already been performed,where as the former assumes that tempo variations are not significant enough to obscurethe ratios of neighboring note lengths.In many kinds of music we believe it will be exceedingly difficult to independentlyestimate tempo and rhythm, as in the previously cited research, since the observed datais formed from a complex interplay between the two. That is, independent estimationof tempo or rhythm leads to a “chicken and egg” problem: One cannot easily estimaterhythm without knowing tempo and vice-versa. In this work we address the problem ofsimultaneous estimation of tempo and rhythm. From a problem domain point of view, thisis the most significant contrast between our work and other efforts cited.The research effort closest to ours in spirit is the recent work of Cemgil [2] whichprobabilistically models tempo and rhythm jointly and seeks globally optimal datainterpretations by computing the posterior distribution through particle filtering techniques.220C. Raphael / Artificial Intelligence 137 (2002) 217–238There are two significant distinctions between this work and ours. Cemgil deals withthe “chicken and egg” problem by approximating the marginal distribution on rhythm byintegrating out the tempo variables; we instead estimate tempo and rhythm jointly. Perhapsa more important distinction is that we provide a dynamic programming technique thatidentifies the globally optimal data interpretation; Cemgil’s method is approximate.The paper is organized as follows. Section 2 develops a generative graphical modelfor the simultaneous evolution of tempo and rhythm processes that incorporates both priorknowledge concerning the nature of the rhythm process and a simple and reasonable modelfor tempo evolution. This section then describes a computational scheme for identifyingthe most likely configuration, a MAP estimate, of the unobserved processes given observedmusical data. Section 3 demonstrates the application of our scheme to a several musicalexamples. Section 4 then briefly summarizes and discusses some aspects of our approachto rhythmic parsing. Our method of identifying the MAP estimate for the specific modeltreated here generalizes well beyond this particular model, however. Section 5 sketches thegeneralization of our methodology to the generic MAP estimation of unobserved variablesfor conditional Gaussian (CG) distributions. To our knowledge, MAP estimation in CGdistributions has not be studied previously, however is potentially quite useful. Finally, theappendix lists some easily-derived results about Gaussian kernels that are used in preced-ing sections.2. Rhythmic parsing2.1. The modelWhile musical rhythm is not usually composed of rhythmic fragments that repeatverbatim, rhythm typically has a cyclic component in which certain tendencies repeat in aperiodic fashion. This periodic nature of music is so basic that it figures prominently in theway most Western music is notated: As a sequence of measures—units of musical time—that obey similar subdivision rules. The probabilistic modeling of this periodic behavioris central to the approach taken here, and we will present evidence in Section 3 of itsadvantage. In what follows we use the term measure to denote the most obvious period ofthe rhythmic structure. Usually this will be the same as the notated measure.Suppose a musical instrument generates a sequence of times o0, o1, . . . , oN , in seconds,at which note onsets occur. Suppose we also have a finite set S composed of the possiblemeasure positions a note can occupy. For instance, if the music is in 4/4 time and webelieve that no subdivision occurs beyond the eighth note, then S = {i/8: i = 0, . . . , 7}.More complicated subdivision rules could lead to sets, S, which are not evenly spacedmultiples of some common denominator. We assume only that the possible onset positionsof S are rational numbers in [0, 1), decided upon in advance. Our goal is to associate eachnote onset on with a score position—a measure number and an element of S.We model this situation as follows. Let S0, S1, . . . , SN be the discrete measure positionprocess, Sn ∈ S, n = 0, . . . , N . In interpreting these positions we assume that eachconsecutive pair of positions differs by less than one measure. For instance, in the 4/4example given above Sn = 0/8, Sn+1 = 1/8 would mean the nth note begins at the startC. Raphael / Artificial Intelligence 137 (2002) 217–238221of the measure and the (n + 1)th note begins one eighth note later, while Sn = 0/8,Sn+1 = 0/8 would mean that the two notes both begin at the start of the same measure.We can then use(cid:1)l(sn, sn+1) =sn+1 − sn1 + sn+1 − snif sn+1 (cid:1) sn,otherwise,(cid:2)to unambiguously represent the gap, in measures, associated with the transition from sn tosn+1. Thus, if s0, s1, . . . , sN is known, we assign a score position, mn, to every observationn−1on by mn = s0 +ν=0 l(sν, sν+1). We believe the assumption that inter-onset intervalsare less than one measure is appropriate for many examples—especially those in whichthe composite rhythm generated by superposing the musical parts is studied, as in theChopin example of Section 3. However, more complicated models can allow for longerIOIs without greatly increasing the number of parameters to be learned. We model the Sprocess as a time-homogeneous Markov chain with initial distributionI (s0) = P (S0 = s0)and transition probability matrixR(sn, sn+1) = P (Sn+1 = sn+1 | Sn = sn).The tempo is the most important link between what is prescribed by the score and whatis observed. Let T1, T2, . . . , TN be the continuously-valued tempo process, measured inseconds per measure, which we model by(cid:4)(cid:3)T1 ∼ Nν, φ2andTn = Tn−1 + δnfor n = 2, 3, . . . , N where δn ∼ N(0, τ 2(Sn−1, Sn)). This model captures the property thatthe tempo tends to vary smoothly and allows the variance in the tempo increment to dependon the transition from Sn−1 to Sn. For instance, we would expect greater variability to beassociated with longer transitions.Finally we assume that the observed inter-onset intervals (IOI) yn = on − on−1 forn = 1, 2, . . . , N are approximated by the product of l(Sn−1, Sn) (measures) and Tn(secs. per measure). SpecificallyYn = l(Sn−1, Sn)Tn + εnwhere εn ∼ N(0, ρ2(Sn−1, Sn)). Note that the observation variance is also allowed todepend on the transition which can capture the notion that long transitions will beassociated with greater variability of the IOIs. The variables {T1, δ2, . . . , δN , ε1, . . . , εN }are assumed to be mutually independent with T1 independent of S0, and δn and εnindependent of S0, . . . , Sn−1.These modeling assumptions lead to a graphical model whose directed acyclic graphis given in Fig. 2. The model is composed of both discrete and Gaussian variables withthe property that, for every configuration of discrete variables, the continuous variableshave a multivariate Gaussian distribution. Thus, the S0, . . . , SN , T1, . . . , TN , Y1, . . . , YNcollectively have a conditional Gaussian (CG) distribution.222C. Raphael / Artificial Intelligence 137 (2002) 217–238Fig. 2. The DAG describing the dependency structure of the variables of our model. Circles represent discretevariables while squares represent continuous variables.Such distributions were introduced by Lauritzen and Wermuth [18,19], and havebeen developed by Lauritzen [16], and Lauritzen and Jensen in [17], in which evidencepropagation methodology is described, enabling the computation of local marginaldistributions. Using these ideas, we could, in principle, fix Y1 = y1, . . . , YN = yN andproceed to compute marginal distributions on the {Sn} and choose as our estimate of Sn¯sn = arg maxs∈SP (Sn = s | Y1 = y1, . . . , YN = yN ).However, these computations rely on construction of a triangulated graph with a strongroot. The additional edges involved in the construction of such a strong root leads toa graph in which a single clique contains the entire collection of {Sn} variables [20].The following computations are intractable. Furthermore, there is no guarantee that thesequence ¯s0, . . . , ¯sN is reasonable, or even thatP (S0 = ¯s0, . . . , SN = ¯sN | Y1 = y1, . . . , YN = yN ) > 0calling this estimate into question.Rather, we desire the configuration of unobserved variables which has greatestprobability given the observation. Thus, regarding y1, . . . , yN as fixed, we seek the estimate(ˆs, ˆt ) = arg maxs,tL(s, t, y)(1)where L(s, t, y) is the joint likelihood of s = (s0, . . . , sN ), t = (t1, . . . , tN ), y = (y1,. . . , yN ).The computation of such MAP estimators for networks composed entirely of discretevariables is well known [5,7]. In what follows we demonstrate new methodology for theexact computation of the global maximizer, (ˆs, ˆt ) in our mixed discrete and continuouscase.2.2. Computing the rhythmic parseDefine the n-dimensional Gaussian kernelK(x; θ ) = K(x; h, m, Q) = he(2)where x is an n-vector, h is a nonnegative constant, m is an n-vector, and Q is an n × nnonnegative definite matrix. Note that we do not require Q to be invertible, hence the2 (x−m)tQ(x−m)− 1C. Raphael / Artificial Intelligence 137 (2002) 217–238223function K(x; θ ) does not necessarily correspond to a scaled Gaussian density function.We write θ = (h(θ ), m(θ ), Q(θ )) to represent the components of θ . It is possible toperform a number of operations on Gaussian kernels such as multiplication of two kernels,maximizing over a subset of variables, and representing conditional Gaussian distributionsby performing transformations of the parameters involved. The appendix gives an accountof some easily derived results involving Gaussian kernels.The joint likelihood function L(s, t, y) with y held fixed can be represented as follows.We defineL1(s0, s1, t1) = I (s0)R(s0, s1)N(cid:4)(cid:3)t1; θ (cid:14)(s0, s1)= K(cid:3),t1; ν, φ2(cid:3)(cid:4)Ny1; l(s0, s1)t1, ρ2(s0, s1)(cid:4)(3)where N(· ; µ, σ 2) = K(· ; (2πσ 2)−1/2, µ, 1/σ 2) is the univariate normal density function.In Eq. (3), θ (cid:14)(s0, s1) is computed for each configuration of s0, s1 by representing theconditional density for y1 as a Gaussian kernel in y1 and t1 using Eq. (A.7), eliminating y1from the same kernel by holding it fixed using Eq. (A.5), multiplying the two kernelstogether using Eq. (A.1), and absorbing the two constants I (s0) and R(s0, s1) intoh(θ (cid:14)(s0, s1)). Using the notation aji(cid:4), t n−11= (ai, ai+1, . . . , aj ), we then define(cid:4)Cn(sn−1, sn, tn−1, tn)(cid:3)sn−10Ln(cid:3)0 , t nsn= Ln−11for n = 2, . . . , N whereCn(sn−1, sn, tn−1, tn)(cid:3)= R(sn−1, sn)N(cid:3)tn−1, tn; θ c= Ktn; tn−1, τ 2(sn−1, sn)(cid:4),n(sn−1, sn)(cid:3)(cid:4)Nyn; l(sn−1, sn)tn, ρ2(sn−1, sn)(cid:4)(4)where Eq. (4) is computed by representing the two conditional normal densities asGaussian kernels using Eq. (A.7), eliminating yn from the second density using Eq. (A.5),extending the second density to be a function of tn and tn−1 using Eq. (A.6), andmultiplying the two factors together using Eq. (A.1), and absorbing the constantR(sn−1, sn).Note that LN (sN1 ) is the joint likelihood L(s, t, y) with y held fixed to the vectorof observations. We will compute our MAP estimate by maximizing LN using dynamicprogramming as follows.0 , t NDefineH1(s1, t1) = maxs0Hn(sn, tn) = maxL1(s0, s1, t1),(cid:4)(cid:3)0 , t nsn1Lnsn−10,t n−11= maxsn−10,t n−11(cid:3)sn−10Ln, t n−11(cid:4), sn, tnfor n = 2, . . . , N . The fundamental observation of dynamic programming is that we cancompute Hn recursively by224C. Raphael / Artificial Intelligence 137 (2002) 217–238Hn+1(sn+1, tn+1) = max0 ,t nsn1= maxsn0 ,t n1= maxsn,tnfor n = 1, . . . , N − 1.(cid:3)(cid:4)Ln+1(cid:3)Ln, t n+11sn+10(cid:4)Cn(sn, sn+1, tn, tn+1)0 , t nsn1Hn(sn, tn)Cn(sn, sn+1, tn, tn+1)(5)Consider first the computation of H1(s1, t1) which can be computed by “maxing out”the s0 variable in Eq. (3). ThusH1(s1, t1) = maxs0= maxL1(s0, s1, t1) = maxs0K(t1; θ1) = maxt1; θ (cid:14)(s0, s1)K(t1; θ1),(cid:3)K(cid:4)(6)(7)θ1∈ (cid:5)Θ1(s1)θ1∈Θ1(s1)where (cid:5)Θ1(s1) = {θ (cid:14)(s0, s1): s0 ∈ S} and Θ1(s1) = Thin( (cid:5)Θ(s1)), where Thin(Θ) is thesmallest subset of Θ such thatK(t; θ ) = maxθ∈Θmaxθ∈Thin(Θ)K(t; θ ).(8)This computation is depicted in Fig. 3.We remark that it is a simple matter to identify Θ1(s1) = Thin( (cid:5)Θ(s1)) since we can“build” the maximum of Eq. (6) by incrementally adding components of (cid:5)Θ1(s1) whilediscarding those that leave the maximum unchanged. This algorithm is made more precisein Section 2.3.The computational feasibility of our dynamic programming algorithm follows becausethe form of Eq. (7)—a maximum of Gaussian kernels—is invariant under the operation ofEq. (5). That is, assumingHn(sn, tn) = maxK(tn; θn)(9)θn∈Θn(sn)we haveHn+1(sn+1, tn+1) = maxsn,tn= maxsn,tnHn(sn, tn)Cn+1(sn, sn+1, tn, tn+1)(cid:3)tn; θn)K(tn, tn+1; θ c(cid:3)tn, tn+1; θ c(cid:4)(cid:4)n+1(sn, sn+1)n+1(sn, sn+1)(cid:4)(cid:4)n+1(sn, sn+1)maxθn∈Θn(sn)K==maxsn,θn∈Θn(sn)maxsn,θn∈Θn(sn)K(tn; θn)K(cid:3)θn, θ cmaxtn(cid:3)tn+1; ˜θK(tn+1; θn+1)K(10)(11)==maxθn+1∈ (cid:5)Θn+1(sn+1)maxθn+1∈Θn+1(sn+1)where in going from Eq. (10) to (11), i.e., in computing ˜θ (θn, θ cEqs. (A.1), (A.6), and (A.4). (cid:5)Θn+1(sn+1) in the preceding is given byn(sn, sn+1): θn ∈ Θn(sn), sn ∈ S(cid:5)Θn+1(sn+1) =K(tn+1; θn+1),(cid:3)θn, θ c(cid:6)˜θ(cid:7)(cid:4)and Θn+1(sn+1) = Thin( (cid:5)Θn+1(sn+1)). This computation is depicted in Fig. 4.n(sn, sn+1)), we useC. Raphael / Artificial Intelligence 137 (2002) 217–238225Fig. 3. The construction of H1(s1, t1). Top (9-panel): The graph in the s0, s1 position gives L(s0, s1, t1). Middle(3-panel): Maxing out over s0 corresponds to superimposing the graphs in a column and taking the maximum(shown in bold). Bottom (3-panel): The thinning operation removes kernels from the representation withoutaffecting the maximum. Note that the middle plot in the bottom panel now is composed of only two kernelswhere before it had three.While the comparison of Eqs. (9) and (11) suggest |Θn(sn)| increases exponentiallywith n, this growth will be controlled by the “thinning” operation. In fact, the behavior226C. Raphael / Artificial Intelligence 137 (2002) 217–238Fig. 4. The construction of Hn+1(sn+1, tn+1). Top (9-panel): maxtn Hn(sn, tn)Cn+1(sn, sn+1) is depicted. Thecontinuous variable is tn+1. Middle (3-panel): Maxing out over sn gives Hn+1(sn+1, tn+1) (shown in bold)Bottom (3-panel): Hn+1(sn+1, tn+1) is represented with fewer kernels after thinning.observed in our experiments, which we anticipate is typical, was that |Θn(sn)| increased toa manageable number within a few dynamic programming iterations and fluctuated aroundthat number in the following iterations. Details are given in Section 3.C. Raphael / Artificial Intelligence 137 (2002) 217–2382272.2.1. Recovering the optimal parseThe maximal value of L = LN is easily computed as follows. Define(cid:8)Θn(S) =Θn(sn)sn∈S(12)for n = 1, . . . , N and let(ˆtN , ˆθN ) = arg maxtN ,θ∈ΘN (S)K(tN ; θ )which can be computed by letting(cid:3)ˆθN = arg maxθ∈ΘN (S)= arg maxmaxtNh(θ )θ∈ΘN (S)(cid:4)K(tN ; θ )and taking ˆtN = m( ˆθN ). ThenK(ˆtN ; ˆθN ) = maxtN= maxsN ,tN= maxsN ,tN= max0 ,t NsN1maxθ∈ΘN (S)maxθ∈ΘN (sN )HN (sN , tN )(cid:4).(cid:3)0 , t NsN1LNK(tN ; θ )K(tN ; θ )Thus K(ˆtN ; ˆθN ) is the maximal value of the likelihood function, L.We wish to recover the rhythmic parse ˆsN0 , ˆt N1 that attains this maximum. ConsideringEq. (11), we see that each element θn+1 ∈ Θn+1(sn+1) is generated by a unique“predecessor” or “parent” Pa(θn+1) ∈ Θn(S). That is, if θn ∈ Θn(S), and(cid:3)θn+1 = ˜θθn, θ cn+1(sn, sn+1)∈ Θn+1(sn+1)(cid:4)then Pa(θn+1) = θn. Thus we can trace back the optimizing sequence of parameter valuesby ˆθn = Pa( ˆθn+1) for n = 0, . . . , N − 1 as in Fig. 5. Then the optimizing sequence ofmeasure positions in S is given by ˆsn = s( ˆθn) for n = 0, . . . , N , where s(θn) = sn ifθn ∈ Θn(sn).Having identified ˆtN and ˆs0, . . . , ˆsN , we can recover the optimal ˆt1, . . . , ˆtN throughHn(ˆsn, tn)Cn(ˆsn, ˆsn+1, tn, ˆtn+1)K(tn; ˆθn)KK(tn; ¯θn)(cid:3)tn, ˆtn+1; θ cn+1(ˆsn, ˆsn+1)(cid:4)ˆtn = arg maxtn= arg maxtn= arg maxtn= m( ¯θn),where ¯θn can be computed by eliminating ˆtn+1 using Eq. (A.5) and multiplying the twokernels together using Eq. (A.1).228C. Raphael / Artificial Intelligence 137 (2002) 217–238Fig. 5. The figure corresponds to a situation in which |S| = 2, thus there are two parameter values at level 0and each parameter value has 2 child parameters where parameter values are depicted by nodes in the tree. Eachparameter θ ∈ Θn(S) has a unique parent, Pa(θ ), so the optimal sequence of parameter values θ0, . . . , θN (shownwith solid circles) can be traced back from leaf to root. ˆθN is marked as “optimal parameter” in the figure.Terminal nodes in the tree at levels other than N correspond to parameter values that have been pruned throughthe thinning algorithm.2.3. ThinningThe computational feasibility of our dynamic programming algorithm relies on thethinning operation of Section 2.2 since without this operation the complexity of therepresentation of Hn grows exponentially. Recall, Thin(Θ) is the smallest subset of Θ forwhich Eq. (8) holds. When Θ is composed of parameters for one-dimensional Gaussiankernels, as in Section 2.2, the algorithm for computing Thin(Θ) is straightforward, asfollows.Suppose Θ = {θ 1, . . . , θ I }. Defineˆθ i(t) = arg maxθ∈{θ 1,...,θ i }K(t; θ )for i = 1, . . . , I and note that ˆθ i (t) is piecewise constant and, hence, can be written asN(i)(cid:9)ˆθ i(t) =θ ik1(xik+1)(t),k,xik=1where −∞ = xi2 < · · · < xi1 < xiconcerned with the definition of ˆθ i(t) at points, t = xi(cid:10)Clearly then Thin(Θ) =N(i) < xN(i)+1 = ∞ and θ ikN(I )k=1 θ Ik .∈ {θ 1, . . . , θ i}. We need not bek, where the maximizer is not unique.Note that ˆθ i(t) can be computed iteratively by letting ˆθ 1(t) = θ 1 and notingˆθ i(t) = argmaxθ∈{ ˆθ i−1(t ),θ i}K(t; θ )(13)C. Raphael / Artificial Intelligence 137 (2002) 217–238229ˆθ i(t) can then be computed on each interval (xi−1θ i−1kk. To do this we simply find all solutions, t, to the quadratic equation, xi−1k+1) where it must be that ˆθ i−1(t) =(cid:4)(cid:4)= 0(cid:3)t; θ i−1k(cid:3)t; θ i− log Klog Kwhich lie in (xi−1k+1). These points partition the interval (xi−1, xi−1k+1) into subintervalswhere ˆθ i(t) must be constant so we need only identify ˆθ i(t) through Eq. (13) at any interiorpoint of these subintervals. Having done this for each interval (xi−1k+1) we may find thatˆθ i(t) is constant over neighboring subintervals. In such a case, the neighbors are simplymerged together to form a more compact representation of ˆθ i(t)., xi−1, xi−1(14)kkk2.3.1. Constrained optimal parseWith a minor variation on the thinning algorithm we can, in many cases, compute aconstrained optimal parse defined by Eq. (1) subject to tlow < tn < thigh for n = 1, . . . , Nand fixed constants tlow and thigh. This is a helpful restriction in our rhythmic parsingproblem since we know that the tempo must always be positive and can reasonably berestricted to be less than some maximum value as well. Such a constraint will also increasethe efficiency of our algorithm since it will decrease the number of kernels needed torepresent Hn in Eq. (9).We proceed as follows. Define the modified thinning procedure, Thinm(Θ), to be theminimal subset of Θ such thatK(t; θ ) = maxθ∈Θmaxθ∈Thinm(Θ)K(t; θ )for tlow < t < thigh. Thinm(Θ) can be constructed by using the thinning algorithm givenabove while retaining only those solutions, t, to Eq. (14) that satisfy tlow < t < thigh. Nextwe define H mn (sn, tn) to be the result of applying the dynamic programming iteration ofEq. (5) using Thinm in place of the original thinning operation. Then define H cn (sn, tn) tobe the result of constrained optimization in which we employ Eq. (5) but optimize onlyover tlow < tn < thigh. The computation of H cn is considerably more difficult than that ofH mn or Hn since the operation of “maxing out” is not so easily adapted to the constrainedcase, however H cn is still well-defined and will lead to the optimal constrained parse.We will show that we can, in many cases, obtain the constrained optimal parse withoutcomputing H cn .It is easily seen by induction on n thatH mn (sn, tn) (cid:1) H cn (sn, tn)for tlow < tn < thigh, since H cn(sn, tn) is achieved by optimizing over a subset of the realline, rather than the entire real line, and the modified thinning operation, Thinm, does0 , ˆt Nnot affect the values of H m1 )using the algorithm of Section 2.2 using the modified thinning procedure and find thattlow < ˆtn < thigh for n = 1, . . . , N , then we have(cid:3)N (ˆsN , ˆtN ).The first equality is immediate; the second inequality follows since, because tlow < ˆtn <thigh, the value LN (ˆsN1 ) can clearly be achieved by constrained optimization but notn (sn, tn) inside (tlow, thigh). However, if we construct (ˆsNN (ˆsN , ˆtN ) = LN0 , ˆt NˆsN0 , ˆt N= H cH m(15)(cid:4)1230C. Raphael / Artificial Intelligence 137 (2002) 217–238surpassed due to Eq. (15). Thus we see that (ˆsNsince0 , ˆt N1 ) is the constrained optimal solutionH cN (ˆsN , ˆtN ) = H mfor tlow < tN < thigh and any sN .N (ˆsN , ˆtN ) (cid:1) H mN (sN , tN ) (cid:1) H cN (sN , tN )While there is no guarantee that the condition tlow < ˆtn < thigh will hold for n =1, . . . , N , the condition was satisfied in nearly all of the experiments we have performed.Furthermore, it seems reasonable to expect that a solution (ˆsN1 ) constructed asabove that nearly satisfies tlow < tn < thigh is nearly a constrained optimal solution. Thecomputational advantage seeking constrained optima is demonstrated in the followingsection.0 , ˆt N3. ExperimentsWe performed several experiments using two different data sets, one derived from audiodata and the other taken from a MIDI performance.3.1. Schumann Romance dataThe first data set is derived from a performance of the first section of Schumann’s 2ndRomance for Oboe and Piano (oboe part only), an excerpt of which is depicted in Fig. 1.The original data, which can be heard at http://fafner.math.umass.edu/rhythmic_parsing, isa sampled audio signal, hence inappropriate for our experiments. Instead, we extracted asequence of 129 note onset times from the data using the HMM methodology describedin [22]. These data are also available at the above web page. In the performance of thisexcerpt, the tempo changes quite freely, thereby necessitating simultaneous estimation ofrhythm and tempo.Since the musical score for this excerpt was available, we extracted the complete set ofpossible measure positions,(cid:1)S =01,18,14,13,38,512,1532,12,58,34,78(cid:11).(The position 15/32 corresponds to a grace note which we have modeled as a 32nd notecoming before the 3rd beat in 4/4 time.) The most crucial parameters in our model arethose that compose the transition probability matrix R. The two most extreme choices forR are the uniform transition probability matrixRunif(s, s(cid:14)) = 1/|S|and the matrix ideally suited to our particular recognition experimentRideal(s, s(cid:14)) =|{n: Sn = s, Sn+1 = s(cid:14)}||{n: Sn = s}|.Rideal is unrealistically favorable to our experiments since this choice of R is optimalfor recognition purposes and incorporates information normally unavailable; Runif isC. Raphael / Artificial Intelligence 137 (2002) 217–238231unrealistically pessimistic in employing no prior information whatsoever. The actualtransition probability matrices used in our experiments were convex combinations of thesetwo extremesR = αRideal + (1 − α)Runiffor various constants 0 < α < 1. A more intuitive description of the effect of a particular αvalue is the perplexity of the matrix it produces: Perp(R) = 2H (R) where H (R) is the log2entropy of the corresponding Markov chain. Roughly speaking, if a transition probabilitymatrix has perplexity M, the corresponding Markov chain has the same amount of“indeterminacy” as one that chooses randomly from M equally likely possible successorsfor each state. The extreme transition probability matrices have(cid:3)Perp(cid:3)Perp(cid:4)Rideal(cid:4)Runif= 1.92,= 11 = |S|.In all experiments we chose our initial distribution, I (s0),to be uniform, therebyassuming that all starting measure positions are equally likely. The remaining constants,ν, φ2, τ 2(s, s(cid:14)), ρ2(s, s(cid:14)) were chosen through experimentation. In particular, we modeledτ 2(s, s(cid:14)) = β1l(s, s(cid:14)),ρ2(s, s),) = β2l(s, s(cid:14)(cid:14)so only four values, ν, φ2, β1, β2 were set by hand.The computational feasibility of our approach relies on the representation of Hn fromEq. (9) staying manageably small as n increases. The left panel of Fig. 6 shows theevolution of |Θn(S)| for n = 0, . . . , 128 with Perp(R) = 4. The figure shows results forboth the basic algorithm presented in Section 2.3 and for the constrained version discussedin Section 2.3.1. In the latter version we constrained the tempo variables to lie in (1, 5)corresponding to a range of 48–240 beats per minute (the composer’s tempo marking was104 beats per minute). Both versions show that the complexity of the representation of Hndoes not grow as n increases. The average number of kernels used in the representation ofFig. 6. Left: The number of Gaussian kernels necessary to represent Hn, |Θn(S)|, as a function of n. Right: Thenumber of errors produced by our system at different perplexities and with different numbers of errors alreadycorrected.232C. Raphael / Artificial Intelligence 137 (2002) 217–238(cid:2)Hn(sn, tn),unconstrained case.128n=0|Θn(S)|/(129 × |S|), was 4.22 in the constrained case and 9.59 in theThe rhythmic parsing problem we pose here is based solely on timing information.Even with the aid of pitch and interpretive nuance, trained musicians occasionally havedifficulty parsing rhythms. For this reason, it is not terribly surprising that our parsescontained errors. However, a virtue of our approach is that the parses can be incrementallyimproved by allowing the user to correct individual errors. These corrections are treatedas constrained variables in subsequent passes through the recognition algorithm. Due tothe global nature of our recognition strategy, correcting a single error often fixes othersparse errors automatically. Such a technique may well be useful in a more sophisticatedmusic recognition system in which it is unrealistic to hope to achieve the necessary degreeof accuracy without the aid of a human guide. In Fig. 6 we show the number of errorsproduced under various experimental conditions. The four traces in the plot correspondto perplexities 2, 4, 6, 8, while each individual trace gives the number of errors producedby the recognition after correcting 0, . . . , 7 errors. In each pass the first error found fromthe previous pass was corrected. In each case we were able to achieve a perfect parseafter correcting 7 or fewer errors. Fig. 6 also demonstrates that recognition accuracyimproves with decreasing perplexity, thus showing that significant benefit results fromusing a transition probability matrix well-suited to the actual test data.The experiments depicted in Fig. 6 were performed with the {tn} constrained to linein (1, 5) using the constrained thinning algorithm of Section 2.3.1. Over all experimentstwo of the 129 × 8 × 4 = 4128 estimated tempo variables were slightly outside this range.Thus, all but two of our parses are exact constrained MAP estimates, while the other twoare likely very good approximations.3.2. Chopin Mazurka dataIn our next, and considerably more ambitious, example we parsed a MIDI performanceof the Chopin Mazurka Op. 6, No. 3. for solo piano. Unlike the monophonic instrumentof the previous example, the piano can play several notes at a single score position. Thussimultaneous notes, corresponding to transitions of the form Sn = Sn+1, are possible (andoccur frequently).For this example, in 3/4 time, we took the possible measure positions from the actualscore, giving the set(cid:1)S =01,124,112,19,16,29,14,13,12,1324,712,23,56,1112,2324(cid:11).Again, several of the measure positions correspond to grace notes. Rather than fixingthe parameters of our model by hand, we instead estimated them from actual data.The transition probability matrix, R, was estimated from scores of several differentChopin Mazurkas by simply counting transitions in the data and smoothing the resultingconditional distributions. The result was a transition probability matrix having Perp(R) =2.02, thereby providing a model that has greatly improved predictive power over theuniform transition model having perplexity Perp(R) = |S| = 15.C. Raphael / Artificial Intelligence 137 (2002) 217–238233We also learned the variances of our model, τ 2(s, s(cid:14)) and ρ2(s, s(cid:14)) by using adifferent MIDI Mazurka performance with known score, thereby “clamping” the variablesS0, . . . , SN to known values. Once the discrete variables are fixed, the model consistsentirely of Gaussian variables and familiar techniques such as the Kalman Smoother ormethods from Bayesian networks can be used to estimate posterior distributions on the {δn}and {εn} variables given the observed data y1, . . . , yN . We used the EM algorithm to iterateback and forth between the computation of these posterior distribution and the reestimationof the desired variances. To smooth our estimates we used the modeling assumptions(cid:14)) = τ 2τ 2(s, sρ2(s, s(cid:14)) = ρ2(cid:14)(cid:3)(cid:3)Q(cid:3)l(s, s)(cid:3)l(s, s(cid:14))Q(cid:4)(cid:4),(cid:4)(cid:4),where Q is a function that quantizes the transition lengths into a small number ofcategories.In addition, we used a slight variation on the model presented in Section 2.1 thatincludes the MIDI pitch of each note as an observable variable. In particular, we assumethat, given the measure position, Sn, the MIDI pitch for the nth note is conditionallyindependent of all other variables in the model. In doing so our intention was to capitalizeon the relationship between measure position and pitch. For instance, in a Mazurka thebeginning of a measure is usually marked by a low note. The conditional distributions ofpitch given measure position were learned from several Mazurka scores by “binning” bothpitch and measure position into a small number of categories and smoothing empiricaldistributions.Fig. 7. Results of rhythmic parses of Chopin Mazurka Op. 6, No. 3.234C. Raphael / Artificial Intelligence 137 (2002) 217–238With this extended and automatically trained model we then iterated the procedure ofparsing the data and then fixing the error beginning the longest run of consecutive errors.The results of our experiments with this 1334-note data set are shown in Fig. 7. The actualMIDI performance can be heard at http://fafner.math.umass.edu/rhythmic_parsing. We seethat after only a couple of corrections our error rate is in the 2–3% range. We remark thatthis error rate is slightly misleading since the arbitrary rhythmic notation of grace notesrender the ground truth somewhat arbitrary. Many of the “errors” occurred with gracenotes.4. DiscussionWe have presented a method for simultaneous estimation of rhythm and tempo, given asequence of note onset times. Our method assumes that the collection of possible measurepositions is given in advance. We believe this assumption is a relatively simple wayof limiting the complexity of the recognized rhythm produced by the algorithm. Whenarbitrary rhythmic complexity is allowed without penalty, one can always find a rhythmwith an arbitrarily accurate match to the observed time sequence. Thus, we expect thatany approach to rhythm recognition will need some way to limit or penalize rhythmiccomplexity.Other than the collection of possible measure positions, all parameters of our modelcan, and should, be learned from actual data, as in Section 3.2. Our experience is thatthe R matrix, which represents our prior assumptions about rhythm sequences, containsthe most important parameters of our model. The estimation of this matrix requires aset of training data that “matches” the rhythmic content of the test data. For example,we would not expect successful results if we trained our model using various 4/4 timemovements from Beethoven’s piano sonatas and recognized on Madonna’s Material Girl.In our experiments with the Chopin Mazurka in Section 3.2, our training data was quitewell-matched to the test data, being examples of the same genre by the same composer. It islikely that a much less precise match between training and test would still prove workable.Another possibility is to estimate the model parameters “on-line”—however, our initialexperiments in this direction have not produced a significant benefit.In our experiments it was possible to perform the dynamic programming calculationswith a representation that remained bounded in complexity as described in left panel ofFig. 6. We note here that this behavior is not guaranteed; if the kernels have very smallvariance or are highly dispersed, the thinning procedure might not produce the decreasein the complexity of the {Hn} necessary to make the calculations feasible. However, weanticipate that the behavior we observed will be the rule rather than the exception.The experiments presented here deal with estimating the composite rhythm obtainedby superimposing the various parts on one another. A disadvantage of this approach isthat composite rhythms can be quite complicated even when the individual voices havesimple repetitive rhythmic structure. For instance, consider a case in which one voiceuses triple subdivisions while another use duple subdivisions. A possible extension isthe simultaneous estimation of rhythm, tempo and voicing. That is, one could modelthe observed data as a superposition of several independent rhythmic sources and seekC. Raphael / Artificial Intelligence 137 (2002) 217–238235to separate the sources, as well as recognize rhythm and tempo. Rhythm and voicingcollective constitute the “lion’s share” of what one needs for for automatic transcriptionof MIDI data.5. GeneralizationWhile the methodology in Section 2 was developed for the particular graphical model ofFig. 2, the ideas extend to arbitrary graphical models for conditional Gaussian distributions.While a complete description of this generalization is beyond the scope of this paper, wesketch here such an extension. A complete description of this work can be found in [23].A mixed collection of discrete and continuous variables, X, has a conditional Gaussian(CG) distribution if, for every configuration of the discrete variables, the conditionaldistribution on the continuous variables is multivariate Gaussian [18,19]. We assume wehave a representation of the CG distribution in terms of a DAG in which discrete nodeshave no continuous parents.If some of the components of X are observed, we can factor the conditional density onthe remaining unobserved variables, XU , as(cid:12)¯f (xU ) =φC(xC),C∈C(16)where C are the cliques of a junction tree and the potential functions, φC(xC), depend onlyon the indicated variables. When C contains continuous variables, φC can be shown to beof the formφC (xC) = K(cid:3)xΓ (C); θ (x∆(C))(cid:4),(17)where Γ (C) and ∆(C) index the continuous and discrete variables of C. Otherwise φC isthe usual discrete potential.The idea of dynamic programming can be extended beyond the linear graph structureencountered in Section 2, to maximize a function of the form of Eq. (16) with cliquepotentials as in Eq. (17). In this context, we define(cid:12)HC(xC) = maxxU\Cφ(cid:5)C (x(cid:5)C),(cid:5)C(cid:1)Cwhere (cid:5)C < C if C lies on the unique path between (cid:5)C and the root of the junctiontree (our tree grows downward). Then HC can be computed recursively by the dynamicprogramming iterationHC(xC) = φC (xC)(cid:12)S→(cid:5)CCmaxx(cid:5)C\SH(cid:5)C(x(cid:5)C),(18)where we take Cwith (cid:5)C < C.S→ (cid:5)C to mean that C and (cid:5)C are neighboring cliques separated by S236C. Raphael / Artificial Intelligence 137 (2002) 217–238As in Section 2, a specific functional form can be used to represent the HC functionsthroughout the dynamic programming recursion. Suppose C has continuous componentsand consider the formHC(xC) = maxθ∈Θ(x∆(C))K(xΓ (C); θ )(19)(HC is just a nonnegative function when xC has only discrete components). The terminalcliques clearly have HC of this form, where the maximum has a single Gaussian kernel.Furthermore one can show that if all child cliques of C have such a representation, thenthe Eq. (18) also leads to a similar representation for HC . Having computed the HCrfor the root clique, Cr we can easily trace back the calculations to find the optimalconfiguration ˆxU .While most of the methodology presented in Section 2 extends in a straightforwardmanner to the general domain of CG distributions, there is one notable exception. Thecomputation of Eq. (19) also involves the thinning operation of Eq. (8), however, thecollection of kernels we consider are not necessarily one-dimensional. Thus, the algorithmof Section 2.3, which is inherently one-dimensional, cannot be applied. We do anticipatethat a smarter algorithm can be used to compute, or at least approximate, the thinningoperation in higher dimensions. The development of such an algorithm is the only missinglink between our proposed methodology and a fully general approach to finding MAPestimates for unobserved variables in CG distributions.Appendix AA Gaussian kernel is a multivariate function of the form of Eq. (2). The followingidentities hold for such functions. The derivations of these results are quite straightforwardand are not included here.Multiplication.K(x; h1, m1, Q1)K(x; h2, m2, Q2) = K(x; h, m, Q),(A.1)where− 11Q1m1+mt2 (mth = h1h2em = Q−(Q1m1 + Q2m2),Q = Q1 + Q2,2Q2m2−mtQm),where Q− is the generalized inverse of Q [15,21]. We deal here only with nonnegativedefinite symmetric matrices; in this case Q− can be expressed asQ− = U D−U t,where Q = U DU t with U unitary and D diagonal, and D− is diagonal with(cid:1)D−ii=1/Dii Dii > 0,Dii = 0.0C. Raphael / Artificial Intelligence 137 (2002) 217–238237Maxing out. Let m and Q be partitioned asm =Q =(cid:14),(cid:13)(cid:13)m1m2Q11 Q12Q21 Q22(cid:14).Then(cid:13)(cid:13)(cid:14)(cid:14)Kmaxx2x1x2; h, m, Q= K(x1; h, m1, (cid:5)Q),where(cid:5)Q = Q11 − Q12Q−22Q21.(A.2)(A.3)(A.4)In the event that x1 has no components, we have maximized over all variables of the kerneland interpret K(x1; h, m1, (cid:5)Q) as the constant h.Fixing variables. Let m and Q be partitioned as in Eqs. (A.2) and (A.3). Regarding x2 asfixed(cid:13)(cid:13)(cid:14)(cid:14)Kx1x2; h, m, Q= K(x1; ˜h, ˜m, (cid:5)Q),where(A.5)−11Q12)(x2−m2)− 12 (x2−m2)t(Q22−Q21Q˜h = he−11Q12(x2 − m2),˜m = m1 − Q(cid:5)Q = Q11.Extension. The kernel K(x1; h, m, Q) can be viewed as a function of x1 and x2 by(cid:13)(cid:13)(cid:14)(cid:13)(cid:14)(cid:13)(cid:14)(cid:14)K(x1; h, m, Q) = Kx1x2; h,m0,Q 000.(A.6)Conditional Gaussian densities. If x2 = αtx1 + β + ξ where x2 is univariate and ξ ∼N(µ, σ 2), the conditional density of x2 given x1 is(cid:13)(cid:13)(cid:14)(cid:14)fv(x2 | x1) = Kx1x2; h, m, Q,(A.7)where(cid:3)2πσ 2(cid:4)−1/2,h =(cid:13)(cid:14),0β + µm =Q = 1σ 2(cid:13)(cid:14).ααt −α−αt1238C. Raphael / Artificial Intelligence 137 (2002) 217–238References[1] P. Allen, R. Dannenberg, Tracking musical beats in real time, in: Proceedings of the International ComputerMusic Conference, International Computer Music Association, San Francisco, CA, 1990, pp. 140–143.[2] A.T. Cemgil, Tempo tracking and rhythm quantization by sequential Monte Carlo, in: Advances in NeuralInformation Processing Systems, Vol. 14, MIT Press, Cambridge, MA, 2002.[3] A.T. Cemgil, B. Kappen, P. Desain, H. Honing, On Tempo Tracking: Tempogram representation and Kalmanfiltering, J. New Music Res. (2001), in press.[4] A.T. Cemgil, P. Desain, B. Kappen, Rhythm quantization for transcription, Computer Music J. 24 (2000)60–76.[5] R.G. Cowell, A.P. Dawid, S.L. Lauritzen, D.J. Spiegelhalter, Probabilistic Networks and Expert Systems,Springer, New York, 1999.[6] R. Dannenberg, B. Mont-Reynaud, Following an improvisation in real time, in: Proceedings of theInternational Computer Music Conference, International Computer Music Association, San Francisco, CA,1987, pp. 241–248.[7] A.P. Dawid, Applications of a general propagation algorithm for probabilistic expert systems, Statist.Comput. 2 (1992) 25–36.[8] P. Desain, H. Honing, The quantization of musical time: A connectionist approach, Computer Music J. 13 (3)(1989).[9] P. Desain, H. Honing, Foot tapping, A brief introduction to beat induction,in: Proceedings of theInternational Computer Music Conference, International Computer Music Association, San Francisco, CA,1994, pp. 78–79.[10] P. Desain, R. Aarts, A.T. Cemgil, B. Kappen, H. van Thienen, P. Trilsbeek, Robust time-quantization formusic from performance to score, in: Proceedings of 106th Audio Engineering Society Conference, Munich,1999.[11] S. Dixon, A beat tracking system for audio signals, in: Proceedings of the Diderot Forum on Mathematicsand Music, Austrian Computer Society, 1999.[12] S. Dixon, A lightweight multi-agent musical beat tracking system, in: Proceedings of the AAAI Workshopon Artificial Intelligence and Music: Towards Formal Models for Composition, Performance, and Analysis,Austin, TX, 2000.[13] M. Goto, Y. Muraoka, An audio-based real-time beat tracking system and its applications, in: Proceedings ofthe International Computer Music Conference, International Computer Music Association, San Francisco,CA, 1998, pp. 17–20.[14] M. Goto, Y. Muraoka, A real-time beat tracking system for audio signals, in: Proceedings of the InternationalComputer Music Conference, International Computer Music Association, San Francisco, CA, 1995,pp. 171–174.[15] F. Graybill, Matrices with Applications in Statistics, Wadsworth International Group, Belmont, CA, 1969.[16] S.L. Lauritzen, Propagation of probabilities, means, and variances in mixed graphical association models,J. Amer. Statist. Assoc. (Theory and Methods) 87 (420) (1992) 1098–1108.[17] S.L. Lauritzen, F. Jensen, Stable local computation with conditional Gaussian distributions, Technical ReportR-99-2014, Department of Mathematic Sciences, Aalborg University, 1999.[18] S.L. Lauritzen, N. Wermuth, Mixed interaction models, Technical Report R-84-8, Institute for ElectronicSystems, Aalborg University, 1984.[19] S.L. Lauritzen, N. Wermuth, Graphical models for associations between variables, some of which arequalitative and some quantitative, Ann. Statist. 17 (1989) 31–57.[20] U. Lerner, R. Parr, Inference in hybrid networks: Theoretical limits and practical algorithms, in: Proceedingsof the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI 2001), Seattle, WA, MorganKauffman, San Mateo, CA, 2001, pp. 310–318.[21] C.R. Rao, S.K. Mitra, Generalized Inverse of Matrices and its Applications, Wiley, New York, 1971.[22] C. Raphael, Automatic segmentation of acoustic musical signals using hidden Markov models, IEEE Trans.Pattern Anal. Machine Intelligence 21 (4) (1999) 360–370.[23] C. Raphael, Map estimation of unobserved variables in conditional Gaussian distributions, J. AmericanStatist. Assoc. (2001), submitted.[24] P. Trilsbeek, H. van Thienen, Quantization for notation: Methods used in commercial music software, in:106th Audio Engineering Society Conference, Munich, 1999.