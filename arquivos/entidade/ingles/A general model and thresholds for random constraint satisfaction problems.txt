Artificial Intelligence 193 (2012) 1–17Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA general model and thresholds for random constraintsatisfaction problemsYun Fan a, Jing Shen b,∗, Ke Xu ca Department of Mathematics, Central China Normal University, Wuhan, 430079, Chinab School of Science, Naval University of Engineering, Wuhan, 430033, Chinac State Key Lab of Software Development Environment, Beihang University, Beijing, 100191, Chinaa r t i c l ei n f oa b s t r a c tArticle history:Received 7 February 2012Received in revised form 23 July 2012Accepted 17 August 2012Available online 21 August 2012Keywords:Constraint satisfaction problemPhase transitionIn this paper, we study the relation among the parameters in their most general settingthat define a large class of random CSP models d-k-CSP where d is the domain size andk is the length of the constraint scopes. The model d-k-CSP unifies several related modelssuch as the model RB and the model k-CSP. We prove that the model d-k-CSP exhibitsexact phase transitions if k ln d increases no slower than the logarithm of the number ofvariables. A series of experimental studies with interesting observations are carried outto illustrate the solubility phase transition and the hardness of instances around phasetransitions.Crown Copyright © 2012 Published by Elsevier B.V. All rights reserved.1. IntroductionThe constraint satisfaction problem (CSP in short) is a central topic in the artificial intelligence community. It is inter-esting that many random CSPs exhibit phase transitions. Since the seminal work of Cheesman et al. [3], the link betweenthe threshold phenomenon of CSPs and the computational hardness of CSPs has attracted a great interest of mathemati-cians, physicists and theoretical computer scientists. For the investigation of the threshold phenomenon of CSPs, numerousexperimental studies [26,28,30] have been carried out, which results show strong evidences that the instances in the phasetransition region are hard to solve. Therefore these CSPs with hard instances play an important role as they are used asbenchmarks to test different CSP algorithms.Random k-SAT is a special case of random CSPs, where each formula has precisely k literals per clause. In the pasttwo decades, random k-SAT has been widely studied. The satisfiability threshold for random 2-SAT is obtained in [4,15].Friedgut made tremendous progress in [13] towards establishing the existence of a sharp threshold for random k-SAT. Itis theoretically attained in [21] and [9] that the updated lower bound and upper bound of the threshold point of random3-SAT are 3.53 and 4.4898, respectively. However, the exact location of the phase transition of random k-SAT for k (cid:2) 3 isstill under investigation.Much research has gone into the study of random models of CSPs with constant domain size at least 2, e.g. [1,6–8,17–19,24,28]. The initial standard models, named A, B, C and D [19,28], turn out to be flawed as they do not exhibit non-trivialsatisfiability threshold when the length of constraint scopes and the size of domains are all fixed, see [1]. To get non-trivialphase transitions, researchers have been going on three directions. Some CSP models [17–19] restrict particular “structures”on constraint relations, so that the phase transitions are attained at a cost of more expenditure on generating randominstances.* Corresponding author.E-mail addresses: yunfan02@yahoo.com.cn (Y. Fan), shendina@hotmail.com (J. Shen), kexu@nlsde.buaa.edu.cn (K. Xu).0004-3702/$ – see front matter Crown Copyright © 2012 Published by Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.08.0032Y. Fan et al. / Artificial Intelligence 193 (2012) 1–17Instead of incorporating more structures in the relations, some researchers left the constraint relations without anyrestrictions and focused on allowing the domain size to grow up with the number of variables, e.g. [10,27,31]. In 2000, Xuand Li [31] constructed the well-known model RB, where the length of constraint scopes is fixed but the domain size of theinstances is growing up about a power function of the number of variables. A few years later, Frieze and Molloy proposedtwo natural models of random binary CSPs with non-constant domain size, and determined how fast the domain size mustgrow with the number of variables to guarantee that the two models exhibit coarse threshold [14]. To ensure that thesemodels can generate hard instances, the complexity of CSPs had been extensively investigated, e.g. [2,5,18,20,22,25,29,30,32].For the practical side, benchmarks based on the model RB had been widely used in various kinds of algorithm competitionsand in many research papers on algorithms.From another point of view, inspired by the work of Frieze and Wormald [16], some CSP models, e.g. [11,12], allowthe length of constraint scopes to grow moderately and the exact locations of phase transitions are gained. In [11] theauthors proposed the so-called model k-CSP; differently from the model RB, the domain size of the model k-CSP is fixedwhile the length, denoted by k, of constrain scopes of the model k-CSP is growing up to a logarithm function of n (thenumber of variables); they located mathematically the exact phase transition point and demonstrated experimentally theperformance of the model k-CSP in [11]. Because no restriction is put on constraint relations and the length k is growingvery slowly while the domain size d is fixed, it is convenient to generate the instances of the model k-CSP; this is suitablefor algorithmic practice.In this paper we study a new random CSP model, d-k-CSP, where both the domain size d and the length of constrainscopes k are allowed to vary with the number of variables n. We show that the new model has a phase transition and theexact threshold point can be quantified precisely if k ln d (cid:2) (1 + ε) ln n for an arbitrarily small positive real number ε. Thatis the major result of this paper.The new model d-k-CSP covers obviously both the model RB and the model k-CSP as two specially extreme cases; andin the two extreme cases, i.e. either k is fixed or d is fixed, the above major result covers the well-known results in [31]and [11]. Moreover, the effective range of the model d-k-CSP is much more extensive than those of the model RB and themodel k-CSP, it works well whenever k ln d increases no slower than ln n. Thus it provides us a lot of various choices to dealwith phase transitions of random CSPs; the various choices could meet various theoretical and practical requirements. Forexample, we can deduce at once from our major result that the model d-k-CSP with d growing up to ln n and k growingup to ln nln ln n has a phase transition and the exact threshold point can be located precisely; further, a series of experimentalstudies are carried out for that case and the results are reported in this paper.The rest of the paper is organized as follows. In Section 2, we formulate the random model d-k-CSP precisely, and stateour major result on the phase transition of the model d-k-CSP. Section 3 is contributed to a complete proof of our majorresult. The previous methods for the similar questions, e.g. the arguments in [11], are far from enough for the new extensivemodel; a new key idea to prove our major result is to divide the variant area of parameters, which we consider to estimatethe satisfiability probability in the “second moment method” stage, by suitable curves into several subareas, so that in eachsubarea we can estimate the probability in different ways. In Section 4, we compare precisely the model d-k-CSP with someprevious models and deduce the corresponding corollaries, and illustrate the vast effective range of the model d-k-CSP. InSection 5, we present the results of a series of experiments about the model d-k-CSP for the case d = ln n. Finally we drawour conclusions in Section 6.2. Random model d-k-CSPIn this paper, ln x = loge x denotes the natural logarithm function, and exp x = ex denotes the natural exponential func-tion. Denote by |T | the cardinality of the set T .Any instance of a constraint satisfaction problem is a triple I = ( X, D, C), where X = (x1, . . . , xn) is a sequence of n vari-ables, D = (D1, . . . , Dn) is a sequence of finite sets which are called the domains, and C = (C1, . . . , Ct ) with Ci = ( Xi, R i)) are subsequences of X of length ki for i = 1, . . . , t,which are called the constraints; more precisely, Xi = (xi1 , . . . , xikicalled the constraint scopes, and correspondingly, R i are subsets of D i1, called the constraint relations. LetA = D1 × · · · × Dn. Any element (a1, . . . , an) ∈ A is viewed as an assignment to the variables X = (x1, . . . , xn) with val-ues in the domains D, i.e. an evaluation f ( X) = ( f (x1), . . . , f (xn)) = (a1, . . . , an). If an assignment (a1, . . . , an) ∈ A satisfies) ∈ R i for all i = 1, . . . , t, then we say that (a1, . . . , an) is a solution of the instance I ; and at that case wethat (ai1 , . . . , aikisay that the instance I is satisfiable.× · · · × D ikiDefinition 2.1. Let n be the number of variables and t be the number of constraints; let d = d(n) and k = k(n) be two integerfunctions of the natural number n such that d(n) > 1 and k(n) > 1; let p be a positive constant with 0 < p < 1. A randomCSP model is said to be d-k-CSP if the instances are generated as follows:• every cardinality |D i| = d for i = 1, . . . , n;• for i = 1, . . . , t, the constraints are generated as follows:– the constraint scopes Xi = (xi1 , . . . , xik ) are randomly selected with repetition allowed from the subsequences oflength k of the variable sequence (x1, . . . , xn);Y. Fan et al. / Artificial Intelligence 193 (2012) 1–173– the constraint relations R i are randomly selected with repetition allowed from the subsets of D i1with cardinality |R i| = pdk.× D i2× · · · × D ikLet Pr(SAT) denote the probability of a random instance of the model d-k-CSP being satisfiable. We have the followingasymptotic property of the model d-k-CSP.Theorem 2.1. Let the notations be as in Definition 2.1, and assume that t = r · n ln dconditions (i) and (ii) are satisfied:− ln p for a constant parameter r. If both the following(i) the limit limn→∞ 1(ii) k (cid:2) 1d(n) exists;p and there is a positive real number ε such that k ln d (cid:2) (1 + ε) ln n;thenPr(SAT) =limn→∞(cid:2)0,1,r > 1;r < 1.The theorem is stated in an extensive version so that it covers several known or guessed cases as consequences whichpeoples are concerned with, we’ll discuss them in Section 4. Here we just make some remarks on the statements of thetheorem. The proof of the theorem will be provided in Section 3.Remark 2.1. The condition (i) of the theorem makes a necessary restriction for the function d(n) as n → ∞, to avoidwild variances of d(n) which we could not control. Since it is an asymptotic result, and d(n) is positive integer-valued,“limn→∞ 1d(n) > 0” implies that there is an integer N such that d(n) is constant for n > N. Thus, asymptotically speaking, thecondition (i) implies that either limn→∞ d(n) = ∞ or d = d(n) is constant.The condition (ii) of the theorem is also understood in asymptotic sense. Precisely, the condition “k ln d (cid:2) (1 + ε) ln n”p ” implies thatimplies that there is an integer N such that k ln d (cid:2) (1 + ε) ln n for all n > N. Similarly, the condition “k (cid:2) 1there is an integer N such that k(n) (cid:2) 1p for all n > N.Remark 2.2. The positive real number ε in the condition (ii) of the theorem could be arbitrarily small; but, at the theoreticallevel, it should be fixed in the asymptotic process once it was given. In practice, however, we can ignore it completely toassume that k ln d > ln n, which is enough to guarantee the appearance of the phase transition; because: the number of theinstances generated by the actual operations in Definition 2.1 is always finite.Remark 2.3. In accordance with our present approach, the convergence speed of the limits in Theorem 2.1 has to be con-sidered case by case, thus we do not mention it in the statements of the theorem. Tracking the proof in Section 3 below,though a bit complicated, we can see that:(a) for the case where r > 1, the probability Pr(SAT) → 0 exponentially with n → ∞;(b) for the case where r < 1, there are two subcases:(b.1) if d(n) is constant, then the probability Pr(SAT) → 1 exponentially with n → ∞;(b.2) if limn→∞ 1d(n)= 0, then the probability Pr(SAT) → 1 at a rate not slower than that of a power function of 1dconverging to 0.3. A proof of Theorem 2.1Keep the notations and assumptions of Theorem 2.1. We prove Theorem 2.1 in seven subsections. First we deal with thecase “r > 1” in Section 3.1; and then we turn to the case “r < 1”, i.e. the stage of the so-called “second moment method”.After a preparation in Section 3.2 which reduces it to estimations of some functions of variables (n, x) ∈ (1, ∞) × [0, 1], weare faced with a complicated situation; the previous methods for the similar questions, e.g. the arguments in [11] wherethe area (1, ∞) × [0, 1] was divided by lines into subareas and the estimations were made in each subarea, are not powerfulenough for the present case. The key idea to carry it forward is to divide the area by curves. We’ll outline the further proofsin Section 3.3.3.1. Expectation of the number of solutions and the case r > 1Given any n, let G denote the set of all the instances generated by the model d-k-CSP with X = (x1, . . . , xn) and D =(D1, . . . , Dn). Then G is a probability space with equal probability for all samples. For I ∈ G, let Sol(I) denote the set ofsolutions of the instance I .4Y. Fan et al. / Artificial Intelligence 193 (2012) 1–17For any a = (a1, . . . , an) ∈ A, let(cid:2)Sa =if a ∈ Sol(I);1,0, otherwise;which is a 0–1 random variable over the probability space G. ThenS =(cid:3)Saa∈ Ais a non-negative integer random variable over the probability space G.The random variable S is obviously the number of solutions of the random instance I ∈ G; in particular, Pr(S > 0) =Pr(SAT), the probability for the random instance I being satisfiable.It is easy to see that the expectation E(Sa) = pt , and(cid:3)E(S) =E(Sa) = ptdn = dτ n, where τ = 1 − r;(1)a∈ AsoE(S) =limn→∞(cid:2)0,∞,r > 1;r < 1.By Markov’s inequality, we get thatPr(SAT) = 0 if r > 1.limn→∞3.2. Preparation for the case r < 1In the rest of Section 3 we always assume that r < 1, and prove thatlimn→∞ 1Pr(S>0)= 1.limn→∞ Pr(SAT) = 1 by proving thatAs a preparation of the proof, in this subsection we apply the following conditional expectation inequality to upperbound1Pr(S>0) :Pr(S > 0) (cid:2)(cid:3)a∈ AE(Sa)E(S|Sa = 1),where E(S|Sa = 1) denotes the conditional expectation of S assuming that Sa = 1 occurs, see [23, Theorem 6.10]. In fact, inour case this inequality is equivalent to the so-called second moment method, see [11, Appendix A].Let a = (a1, . . . , an) ∈ A. For any b = (b1, . . . , bn) ∈ A, considering the number, denoted by m, of such indices i thatai = bi , we have that(cid:4)(cid:6)(cid:4) (cid:5)m(cid:6) +(cid:5)knk1 −(cid:7)(cid:6)(cid:5)m(cid:6)(cid:5)knk(cid:7)t,pdk − 1dk − 1E(Sb|Sa = 1) =(cid:5)mk(cid:6)(cid:3)E(S|Sa = 1) =E(Sb|Sa = 1)wherestands for binomial coefficient. Denote σm,n =(cid:6)(cid:5)mk(cid:6)(cid:5)nk/. Sob∈ An(cid:3)(cid:4)(cid:7)nm=0n(cid:3)m(cid:7)(cid:4)nmm=0==(cid:4)(d − 1)n−mσm,n + (1 − σm,n)(cid:4)(d − 1)n−mp + (1 − p)(cid:7)tpdk − 1dk − 1(cid:7)tdkσm,n − 1dk − 1,which is independent of the choice of a; hencePr(S > 0) (cid:2)(cid:8)nm=0(cid:6)(cid:5)nm(d − 1)n−mp + (1 − p) dkσm,n−1dk−1.(cid:6)t(cid:8)a(cid:6)∈ A E(Sa(cid:6) )(cid:5)Combining it with the formula (1), we have(cid:5)1Pr(S > 0)(cid:3)n(cid:3)m=0Rm, with Rm =nm(cid:6)(d − 1)n−m(cid:5)p + (1 − p) dkσm,n−1dk−1dn pt(cid:6)t.(2)Y. Fan et al. / Artificial Intelligence 193 (2012) 1–175Define a function for (n, x) ∈ (1, ∞) × [0, 1] as followsFig. 1. The idea of the proof for the case r < 1.R(n, x) = B(n, x)W (n, x) =where(cid:5)(cid:6)nnx(d − 1)n−nx(cid:5)p + (1 − p) (dx)k−1dk−1dn pt(cid:6)t,(3)B(n, x) =(cid:7)nx(cid:4)(cid:7)(cid:4)(cid:4)nnx1d1 − 1d(cid:7)n(1−x),W (n, x) =(cid:4)1 + 1 − pp· (dx)k − 1dk − 1(cid:7)t.Lemma 3.1.(i) Rm (cid:3) R(n, m(ii) Rm (cid:3) B(n, mn ) for all m = 0, 1, . . . , n.n ) for all m = 0, 1, . . . , k − 1.Proof. (i) For m (cid:2) k, since n (cid:2) m, and σm,n = m(m−1)···(m−k+1)the other hand, for m = 0, 1, . . . , k − 1, we have σm,n = 0 (cid:3) ( m(ii) For m = 0, 1, . . . , k − 1, from that σm,n = 0 we have thatn(n−1)···(n−k+1) , we have m−in−in )k.p + (1 − p)dkσm,n − 1dk − 1= pdk − 1dk − 1(cid:3) p.(cid:2)3.3. Outline of the proof for the case r < 1(cid:3) mn for i = 0, 1, . . . , k − 1; so σm,n (cid:3) ( mn )k. Ongiven positive real number θ there is an integer N such thatTo prove that limn→∞ Pr(SAT) = 1, by the inequality (2) it is enough to show that limn→∞(cid:8)nFrom now on till the end of this section, we always assume that δ = θ/3, and consider the area (1, ∞) × [0, 1] of them=0 Rm < 1 + θ , ∀n > N.m=0 Rm = 1, i.e. for any(cid:8)nn–x plane. The key idea to achieve the above objective is to design an integer N and two curves(cid:2)x = η(n),x = ζ (n),where ζ (n), η(n) are two functions of n satisfying that0 < η(n) (cid:3) ζ (n) < 1, ∀n > N,such that for any n > N we have that0(cid:2) mn <η(n) Rm < 1 + δ;(cid:8)⎧⎨⎩n(ζ (n) − η(n))R(n, x) < δ, ∀x with η(n) (cid:3) x < ζ (n);nR(n, x) < δ, ∀x with ζ (n) (cid:3) x (cid:3) 1.(4)In other words, we divide the area (1, ∞) × [0, 1] of the n–x plane by the curves into three areas, as illustrated in Fig. 1, sothat we can deal with R(n, x) for each area in different ways. But note that it may happen that η(n) = ζ (n) somewhere, forthat n there is no x satisfying η(n) (cid:3) x < ζ (n).The inequalities of (4) are enough to complete the proof of Theorem 2.1 for the case r < 1, because they imply that forany n > N we have6Y. Fan et al. / Artificial Intelligence 193 (2012) 1–17n(cid:3)m=0Rm =(cid:3)Rm +(cid:3)0(cid:2) mn <η(n)η(n)(cid:2) mn <ζ (n)(cid:3)Rm +(cid:3)ζ (n)(cid:2) mnRm(cid:2)1(cid:3)< 1 + δ +δn(ζ (n) − η(n))+δn(cid:2)1η(n)(cid:2) mn <ζ (n)(cid:3) 1 + δ + n(ζ (n) − η(n)) · δn(ζ (n) − η(n))(cid:3) 1 + 3δ = 1 + θ.ζ (n)(cid:2) mn+ n(1 − ζ (n)) · δnWe will construct the function η(n) in the next subsection such that the first inequality of (4) holds. Then, by Remark 2.1,we consider two cases: either d(n) is a constant or limn→∞ d(n) = ∞. To proceed with the proof for the two cases, we needsome auxiliary results to explore the function ζ (n), which will be provided in Section 3.5. After that, we complete the prooffor the two cases in Sections 3.6 and 3.7 respectively.3.4. Construction of η(n)Proposition 3.1. Let λ be a real number such that 0 < λ < ε1+ε , and setη(n) = max(cid:2)k(n)n,1d(n)1−λ(cid:12).Then there exists an integer Nη such thatRm < 1 + δ, ∀n > Nη.(cid:3)0(cid:2) mn <η(n)Proof. Given any integer n > 0, assume that mB(n, mn ).n < η(n). If k(n)n(cid:2) 1d(n)1−λ , then m < k and, by Lemma 3.1, we have Rm (cid:3)Otherwise, 0 < k(n)n < 1d(n)1−λ and η(n) = 1d(n)1−λ . For 0 (cid:3) x (cid:3) 1d1−λ , recalling that t = rn ln d− ln p and noting that (dx)k−1dk−1(cid:3) xk,we can bound the function W (n, x) in (3) as follows:(cid:4)(cid:4)W (n, x) = exp(cid:3) exprn ln d− ln prn ln d− ln p(cid:4)(cid:4)ln1 + 1 − pp(cid:7)(cid:7)· (dx)k − 1dk − 1(cid:7)· (dx)k − 1dk − 1(cid:7)(cid:4)· 1 − pp· 1 − pp(cid:7);nxk ln dr(1 − p)−p ln p(cid:3) exprn ln d− ln pand, by the condition that k ln d (cid:2) (1 + ε) ln n, we have(cid:6)−(1−λ)(1+ε)= exp· xk(cid:5)dln n/ ln dsince kn < 1nxk ln d (cid:3) d−(1−λ)kn ln d (cid:3)d1−λ , i.e. ln d < ln n−ln k1−λ(cid:4)r(1 − p)−(1 − λ)p ln p, we getW (n, x) < exp· ln n − ln knε−λ−ελ(cid:7).n ln d = n−(ε−λ−ελ) ln d;Note that the right hand side of the inequality is a real number larger than 1.Summarizing the above, for any n > 0 and any m with 0 (cid:3) mn < η(n), by Lemma 3.1 we have(cid:4)(cid:7)(cid:4)(cid:7)Rm (cid:3) Bmn,· expr(1 − p)−(1 − λ)p ln p1+ε , we have ε − λ − ελ > 0, hence limn→∞ ln n−ln knε−λ−ελ· ln n − ln knε−λ−ελn.As 0 < λ < ε(cid:4)(cid:7)Rm < Bn,mn· (1 + δ), ∀n > Nη, ∀m with 0 (cid:3) mn< η(n).Hence, for any n > Nη we have= 0. Thus there is an integer Nη such that(cid:3)(cid:3)Rm <0(cid:2) mn <η(n)0(cid:2) mn <η(n)Y. Fan et al. / Artificial Intelligence 193 (2012) 1–177(cid:4)Bn,(cid:7)mn· (1 + δ)< (1 + δ) ·= (1 + δ) ·(cid:3)(cid:7)(cid:4)(cid:4)n0(cid:2) mn(cid:4)(cid:2)1(cid:4)+1dm1 − 1d(cid:7)n−m(cid:7)m(cid:4)1 − 1d1d(cid:7)(cid:7)n= 1 + δ.(cid:2)Remark 3.1. As Proposition 3.1 handles the first inequality of (4), to complete the proof of Theorem 2.1, we need to constructthe function ζ (n) satisfying the last two inequalities of (4); by Remark 2.1 we continue the proof in two cases.• d is a constant.• limn→∞ d(n) = ∞.3.5. Auxiliary results for the case r < 1Before going on to construct the function ζ (n) for the two cases, we show some results about the areas of (1, ∞) × [0, 1]of the n–x plane divided by straight lines.(cid:5)nRecall that the binomial coefficientsm−x ln x − (1 − x) ln(1 − x) for x ∈ [0, 1] as follows:(cid:6)(cid:4)(cid:7)nm(cid:5)(cid:6)nH(m/n);< expcan be bounded from above by the so-called natural entropy function H(x) =the entropy function H(x) is differentiable and satisfies that H(x) = H(1 − x), 0 (cid:3) H(x) (cid:3) ln 2 and H(0) = H(1) = 0.Proposition 3.2. For the function R(n, x) defined in (3), there are a real number ρ with 0 < ρ < 1 and an integer Nρ such thatnR(n, x) < δ, ∀n > Nρ , ∀x ∈ [ρ, 1].Proof. Note that p + (1 − p) (dx)k−1dk−1as in the formula (1). By the formula (3) we have< 1, (d − 1)n(1−x) < dn(1−x) and enH(x) = dnH(1−x)/ ln d. Denote ptdn = dτ n with τ = 1 − rnR(n, x) (cid:3)n(cid:5)(cid:6)nnx(d − 1)n(1−x)dn pt(cid:5)<(cid:6)H(1−x)ln d+(1−x)nenH(x)(d − 1)n(1−x)dτ n.· dn<nτ n2dτ n2dAs limx→1 H(1 − x) = 0, there is a real number ρ with 0 < ρ < 1 such thatH(1 − x)ln d+ (1 − x) <τ2, ∀x ∈ [ρ, 1];so there is an integer Nρ such thatnR(n, x) < δ, ∀n > Nρ , ∀x ∈ [ρ, 1].(cid:2)In the rest of this section we always assume that ρ and Nρ satisfy the condition in Proposition 3.2.Lemma 3.2. Let μ be any real number with 0 < μ < 1. If k (cid:2) 1/p, thenln(1 + 1−pxp xk)(cid:3) − ln p, ∀x ∈ [μ, 1].Proof. Letg(x) =ln(1 + 1−pxp xk),8and(cid:6)g(x) =We haveY. Fan et al. / Artificial Intelligence 193 (2012) 1–17(1−p)kxkp+(1−p)xk− ln(1 + 1−pp xk)x2.(cid:4)1 + 1 − pp(cid:7)xkln− (1 − p)kxkp + (1 − p)xk<1 − ppxk − (1 − p)kxkp + (1 − p)xkk1p + (1 − p)xk−(cid:4)p= (1 − p)xk(cid:7)(cid:3) (1 − p)xk(cid:7)− k.(cid:4)1pWhen k (cid:2) 1the lemma is proved. (cid:2)p , then g(cid:6)(x) (cid:2) 0, i.e. g(x) is an increasing function. The maximum value of g(x) in [μ, 1] is g(1) = − 1ln p . ThusRemark 3.2. For the function R(n, x) in (3), since (dx)k−1dk−1(cid:4)(cid:4)(cid:4)(cid:7)(cid:7)t1 + 1 − pxkp= rn ln d− ln pln1 + 1 − pp(cid:7)xk.ln W (n, x) < ln< xk and t = rn ln d− ln p , we have thatAlso, ln B(n, x) < nH(x) + nx ln d−1 + n(1 − x) ln(1 − d−1). So we get an estimation for R(n, x) as follows:(cid:4)ln R(n, x) < nH(x) + x ln+ (1 − x) ln1d(cid:7)(cid:4)1 − 1d+ r ln d− ln pln(cid:4)1 + 1 − pp(cid:7)(cid:7)xk.(5)Proposition 3.3. Let μ be any real number with 0 < μ < 1. If limn→∞ d(n) = ∞, then there is an integer Nμ such thatnR(n, x) < δ, ∀n > Nμ, ∀x ∈ [μ, 1].Proof. By the estimation (5) we have(cid:5)(cid:6)nR(n, x)ln< n(cid:4)ln nn+ H(x) + (1 − x) ln(cid:7)(cid:4)1 − 1d+ x ln1d+ r ln d− ln pln(cid:4)1 + 1 − pp(cid:7)(cid:7)xk,where ln nnand ln d → ∞, we have+ H(x) + (1 − x) ln(1 − 1d ) is bounded from above for all x ∈ [μ, 1]; further, because r − 1 is a negative constantx ln1d+ r− ln p(cid:4)ln1 + 1 − pp(cid:7)xkln d =(cid:3)(cid:4)(cid:4)−1 + r− ln p−1 + r− ln p·p xk)ln(1 + 1−px(cid:7)· (− ln p)x ln d(cid:7)x ln d= (r − 1)x ln d (cid:3) (r − 1)μ ln d −→n→∞−∞,where Lemma 3.2 is used for the second line. Noting that x does not appear in (r − 1)μ ln d, we have a uniform convergencelimn→∞ ln(nR(n, x)) = 0 for all x ∈ [μ, 1], which completes the proof. (cid:2)Lemma 3.3. Assume that a ∈ (0, 1). Leth(x, a) = H(x) + x ln a + (1 − x) ln(1 − a),x ∈ [0, 1].Then h(x, a) is a non-positive differentiable function which is strictly increasing in (0, a), and is strictly decreasing in (a, 1).Proof. See, for example, [11, Lemma 3.1]. (cid:2)Remark 3.3. Since ln(1 + 1−pof R(n, x) as follows:p xk) < 1−pp xk, with the function h(x, a) defined as above, from (5) we can get another estimation(cid:5)(cid:6)nR(n, x)ln< n(cid:4)ln nn(cid:4)+ hx,(cid:7)1d+ r(1 − p)−p ln p(cid:7)xk ln d.(6)Y. Fan et al. / Artificial Intelligence 193 (2012) 1–1793.6. The case “d is a constant”In this subsection we assume that r < 1 and d is a constant. As pointed out in Remark 3.1, the following propositioncompletes the proof of Theorem 2.1 for this case.Proposition 3.4. Let λ, η(n) and Nη be as in Proposition 3.1, ρ and Nρ be as in Proposition 3.2. Assume that d is a constant. Setζ (n) = η(n) for all n. Then there exists an integer N (cid:2) Nη such thatnR(n, x) < δ, ∀n > N, ∀x with ζ (n) (cid:3) x (cid:3) 1.Proof. If1d1−λ(cid:2) ρ, then ζ (n) = η(n) (cid:2) 1d1−λ1d1−λ < ρ. Consider the estimation (6) of R(n, x). Let −b = h( 1(cid:2) ρ and, by taking N = max{Nη, Nρ }, we are done by Proposition 3.2. In thed1−λ <d ). Note that 0 < 1d < 1d1−λ , 1following we assume thatρ < 1. By Lemma 3.3, the number b is positive and(cid:4)(cid:7)1dhx,(cid:3) −b, ∀x ∈(cid:13)(cid:14).1d1−λ , ρSince k ln d (cid:2) (1 + ε) ln n and d is a constant, we have k → ∞; so for x (cid:3) ρ < 1 we can take an integer Nk such thatln nn<b3andr(1 − p)−p ln pxk ln d <b3, ∀n > Nk.By the inequality (6), there is an integer N(cid:5)(cid:6)nR(n, x)ln< −bn/3, ∀n > N(cid:6)k, ∀x ∈(cid:6)k > Nk such that(cid:14)(cid:13)1d1−λ , ρ.Since −bn/3 −→ −∞, there is an N(cid:6)(cid:6)knR(n, x) < δ, ∀n > N(cid:6)(cid:6)k , ∀x ∈(cid:2) N(cid:13)(cid:6)k such that(cid:14)Take N = max{N(cid:6)(cid:6)k , Nρ , Nη}. Since1d1−λ.1d1−λ , ρ(cid:3) ζ (n), combining the above inequality with Proposition 3.2, we havenR(n, x) < δ, ∀ n > N, ∀x with ζ (n) (cid:3) x (cid:3) 1.(cid:2)3.7. The case “limn→∞ d(n) = ∞”In this subsection we always assume that r < 1 and limn→∞ d(n) = ∞, and we complete the proof of Theorem 2.1 forthis case.Keep λ, η(n), Nη as in Proposition 3.1, and ρ, Nρ as in Proposition 3.2.Reviewing the inequality (5), since (1 − x) ln(1 − d−1) < 0, we can relax it as: ln R(n, x) < n(H(x) + x ln 1d+ r(1−p)−p ln p xk ln d);i.e.ln R(n, x) < nx ln d(cid:4)H(x)x ln d− 1 + r(1 − p)−p ln p(cid:7)xk−1.First we have an estimation for the term H(x)x ln d , which will be used to construct the function ζ (n).Lemma 3.4. There is an integer Nα such that< 1 − λ + λ6H(x)x ln d, ∀n > Nα, ∀x with η(n) (cid:3) x (cid:3) 1.Proof. By the assumption we have 0 (cid:3) 1 − x < 1, hence(7)(8)=H(x)x ln dAs η(n) (cid:2) 1− ln xln d+−(1 − x) ln(1 − x)x ln d(cid:3)− ln xln d+− ln(1 − x)x ln d.d1−λ , for x (cid:2) η(n) we have x (cid:2) 1d1−λ , i.e. − ln x (cid:3) (1 − λ) ln d; soH(x)x ln d(cid:3) 1 − λ +− ln(1 − x)x ln d.10Y. Fan et al. / Artificial Intelligence 193 (2012) 1–17Further, since limx→0case, we have a uniform convergence:− ln(1−x)x= 1, the function− ln(1−x)xfor x ∈ (0, 1] is bounded from above; and d → ∞ in the present− ln(1 − x)x ln dn→∞−−−→ 0, ∀x ∈ (0, 1],from which the conclusion follows. (cid:2)Recall that k ln d (cid:2) (1 + ε) ln n. As η(n) (cid:2) kn , for x (cid:2) η(n) we have x (cid:2) k/n, i.e. k (cid:3) xn; henceln n < k ln d < xn ln d, ∀n > Nη, ∀x with η(n) (cid:3) x (cid:3) 1.Take a real number β such that 1 > β > 1 − λ6 , then(1 − β) ln nxn ln d<λ6, ∀x with η(n) (cid:3) x (cid:3) 1.Proposition 3.5. Let Nα be as in (8), and β be as in (9) above. Set(cid:12)(cid:2)ζ (n) = max1nβThen ζ (n) (cid:2) η(n) and there is an integer Nζ (cid:2) Nη such thatη(n),.(cid:5)(cid:6)ζ (n) − η(n)nR(n, x) < δ, ∀n > Nζ , ∀x with η(n) (cid:3) x < ζ (n).λk ln d2= ∞, there is an integer Ne such thatProof. Since limn→∞(cid:4)(cid:7)exp− λk ln d2< δ, ∀n > Ne.Therefore, there is an integer Nζ (cid:2) max{Nη, Nα, Ne} such that(cid:7)k−1r(1 − p)−p ln p(cid:4)1nβ<λ6, ∀n > Nζ .Now we show that Nζ fits the requirement.nβ , then ζ (n) = η(n) and there is no x satisfying η(n) (cid:3) x < ζ (n).Assume that n > Nζ . If η(n) (cid:2) 1Otherwise, 0 < η(n) < 1(cid:5)(cid:6)n1−β R(n, x)ln(cid:4)nβ hence ζ (n) = 1(1 − β) ln nxn ln d< xn ln dnβ . By the inequality (7) we have(cid:7)+ H(x)x ln d− 1 + r(1 − p)−p ln pxk−1.But, for x < ζ (n) = 1nβ we get from (11) that(cid:4)(cid:7)k−1r(1 − p)−p ln pxk−1 <r(1 − p)−p ln p1nβ<λ6, ∀n > Nζ , ∀x < ζ (n).(9)(10)(11)(12)(13)Combining (12) with (9), (8) and (13), for n > Nζ and η(n) (cid:3) x < ζ (n) we have< (xn ln d) · (−λ/2) = − xnλ ln d(cid:5)(cid:6)n1−β R(n, x)ln;2however, x (cid:2) η(n) (cid:2) kn , i.e. xn (cid:2) k, so ln(n1−β R(n, x)) < − λk ln d; further, it is clear that ζ (n) − η(n) (cid:3) 1(cid:7)(cid:6)− λk ln dζ (n) − η(n)2R(n, x) (cid:3) n1−β R(n, x) < exp(cid:4).2(cid:5)nnβ ; so we get thatBy (10), we reach the desired inequality:(cid:6)ζ (n) − η(n)n(cid:5)R(n, x) < δ, ∀n > Nζ , ∀x with η(n) (cid:3) x < ζ (n).(cid:2)As pointed out in Remark 3.1, Proposition 3.5 and the following proposition complete the proof of Theorem 2.1 for thepresent case.Y. Fan et al. / Artificial Intelligence 193 (2012) 1–1711Proposition 3.6. Keep the notations in Proposition 3.5. Then there is an integer N (cid:2) Nζ such thatnR(n, x) < δ, ∀n > N, ∀x with ζ (n) (cid:3) x (cid:3) 1.Proof. From the inequality (7), we have(cid:5)(cid:6)nR(n, x)ln< xn ln d(cid:4)ln nxn ln d+ H(x)x ln d− 1 + r(1 − p)−p ln p(cid:7)xk−1.As ζ (n) (cid:2) 1/nβ , for x (cid:2) ζ (n) we have x (cid:2) 1/nβ , henceln n(cid:3) nβ ln nn ln dxn ln dn1−β ln dso there is an integer Nβ such that= ln nn→∞−−−→ 0;ln n<λ6, ∀n > Nβ , ∀x with ζ (n) (cid:3) x (cid:3) 1.xn ln dSince r(1−p)xk−1 <−p ln p is a constant, we have a real number μ with 0 < μ < 1 such thatr(1 − p)−p ln p(cid:6) = max{Nζ , Nβ }; by (14), (15), (8) and (16), for n > N< (xn ln d)(−λ/2) (cid:3) − λk ln d(cid:5)(cid:6)nR(n, x); ∀x (cid:3) μ.λ6ln;(cid:6)Let Nand ζ (n) (cid:3) x (cid:3) μ we have2which implies that nR(n, x) < exp(− λk ln d2). Citing (10) again, we getnR(n, x) < δ, ∀n > N(cid:6), ∀x with ζ (n) (cid:3) x (cid:3) μ.Finally, since d → ∞ in the present case, we can cite Proposition 3.3 to have an integer N (cid:2) N(cid:6)such thatnR(n, x) < δ, ∀n > N, ∀x with μ (cid:3) x (cid:3) 1.The proposition follows from (17) and (18). (cid:2)4. Related CSP models(14)(15)(16)(17)(18)Keep the notations in Theorem 2.1. In this section, we consider some special cases of the size d of domains and thelength k of constraint scopes, and get the related CSP models which were studied before; as a comparison of Theorem 2.1with those obtained before, we further describe the activity range of the theorem with illustrations.First, fixing the domain size d, from Theorem2.1 we have the following corollary at once.Corollary 4.1. If d is a constant and there is a positive real number ε such that k ln d (cid:2) (1 + ε) ln n, thenPr(SAT) =limn→∞(cid:2)0,1,r > 1;r < 1.(cid:2)Obviously, the model d-k-CSP with constant domain size is just the model k-CSP studied in [11]. In fact, Corollary 4.1is an improvement to the main result of [11], since its assumption “k ln d (cid:2) (1 + ε) ln n” is weaker than the correspondingassumption of the main theorem in [11].On the other hand, if we fix the length k of constraint scopes, then we get the following consequence at once.Corollary 4.2. If k is a constant such that k (cid:2) 1(cid:2)Pr(SAT) =limn→∞0,1,r > 1;r < 1.(cid:2)p and d (cid:2) n(1+ε)/k for a positive real number ε, thenParticularly, we can take k to be a constant such that k (cid:2) 1Corollary 4.2 is satisfied; and revise t = r · n ln d− ln p= r · αn ln n− ln p asp and d = nα for a number α > 1k , then the hypothesis ofln p , one can rewrite the above phase transition of= − rαtn ln n12Y. Fan et al. / Artificial Intelligence 193 (2012) 1–17Fig. 2. The range where d-k-CSP works.satisfiability as:Pr(SAT) =limn→∞(cid:15)0,1,;tn ln n > − αln pn ln n < − αln p .tTherefore, the model d-k-CSP with constant length k of constrain scopes is just the model RB studied in [31]. Xu andLi showed that the model RB had a lot of hard instances and all instances at the threshold point had exponential tree-resolution complexity, see [30,32]. By relating the hardness of the model RB, there are also a lot of hard instances to solvein the model d-k-CSP.However, Theorem 2.1 says much more than the two special cases, as it depends on a trade-off between the growing ofd and the growing of k to guarantee that the exact phase transition points can be mathematically determined. For a series{an}, as usual, an = O (1) stands for that limn→∞ an < ∞. For series {an} and {bn} such that limn→∞ an = limn→∞ bn = ∞,we denote Ω(an) < Ω(bn) if limn→∞< 1. In this notation Theorem 2.1 says that, roughly speaking, the model d-k-CSPhas a phase transition and the threshold point can be exactly quantified whenever Ω(k ln d) > Ω(ln n). We illustrate withFig. 2 the range where the model d-k-CSP has an exact phase transition.anbnIn Fig. 2, the horizontal axis is scaled by the growing speed of ln d while the vertical axis is scaled by the growing speedof k. Of course, both the axis start from O (1), i.e. the constants. Since k (cid:3) n, the vertical axis ends at n, i.e. Ω(n). And, ifΩ(d(n)) > Ω(en), then d(n) is an exponentially growing variable with n, which is too large for the CSPs; so it is reasonableto end the horizontal axis at n too. The dotted area depicts where we have Ω(k ln d) > Ω(ln n); in other words, it is justthe range where Theorem 2.1 works. In particular, the points of the horizontal axis correspond exactly the case that k isconstant; thus the line segment on the horizontal axis from ln n(for a fixed k) to n is just the range where the model RBkworks. Similarly, the line segment on the vertical axis from ln nln d (for a fixed d) to n is just the range where the model k-CSPworks.As an example other than Corollary 4.1 and Corollary 4.2, we take the function d(n) = ln n, then the model d-k-CSP hasln ln n ). It is just the following corollary anda phase transition which can be precisely quantified provided Ω(k(n)) > Ω( ln nillustrated in Fig. 2 also with a line segment.Corollary 4.3. If d = ln n and there is a positive real number ε such that k ln ln n (cid:2) (1 + ε) ln n, thenPr(SAT) =limn→∞(cid:2)0,1,r > 1;r < 1.(cid:2)According to Corollary 4.3, Table 1 gives the domain size and the corresponding minimal value of k satisfying the con-dition against n. It is noteworthy that the domain size and the length k of constraint scopes increase very slowly with thenumber of variables; please compare it with Tables 2 and 3. Therefore the domain size and the length k of constraint scopesare not big for the practically applied models.Y. Fan et al. / Artificial Intelligence 193 (2012) 1–1713Table 1The value d and the minimal value of k for Corollary 4.3.nd = (cid:8)ln n(cid:9)(cid:9)k = (cid:8) ln nln d10335043Table 2d = 3 and the minimal value of k for Corollary 4.1.nd = 3k = (cid:8) ln nln 3(cid:9)10335034Table 3k = 3 and the minimal value of d for Corollary 4.2.nd = (cid:8)n1/3(cid:9)k = 3103350435. Experimental results for the case d = ln n11005410035100535007450036500831000741000371000103500094500038500018310 00010510 0003910 000223Experiments have been done to study the behavior of the model d-k-CSP with fixed length k of constraint scopes, see[30], and experiments for the model k-CSP, i.e. the model d-k-CSP with fixed domain size d, are reported in [11].We have performed a series of experiments on the model d-k-CSP with d = ln n,In this sec-tion, we report the experimental results. The platform we have used for our experimentation is called Abscon (seehttp://www.cril.univ-artois.fr/~lecoutre).i.e. Corollary 4.3.Each random instance generated by the model d-k-CSP is characterized by a 5-tuple (k, n, d, r, p) of parameters, where kn ln d a measure of thedenotes the length of constraint scopes, n the number of variables, d the uniform domain size, r = −t ln pconstraint density, p a measure of the constraint tightness. At each setting of (k, n, d, r, p), 50 instances are generated.According to Corollary 4.3, the instances of the model d-k-CSP change from being soluble to insoluble when the con-straint density r is varied accordingly. Figs. 3 and 4 depict the solubility phase transition and the easy-hard-easy phasetransition for d = ln n, p = 0.6, n ∈ {40, 60, 80} and k = 3. In Fig. 3 it clearly appears that the solubility phase transitionhappens around the theoretical threshold point r = 1, which illustrates that the theoretical result is in close agreement withthe empirical result. On the other hand, the hard instances are found at the neighborhood of the phase transition pointr = 1, as shown in Fig. 4. We remark that the vertical axis for CPU time in Fig. 4 uses a log scale ln T where T is the searchtime (seconds), because the search cost of solving the instances in the model d-k-CSP grows too fast when n is growing up,so that we could not depict the three curves for n ∈ {40, 60, 80} in one and the same coordinate system if the CPU timeaxis was scaled by any multiple of seconds. Table 4 lists the correspondence between CPU time T (seconds) and ln T .Fig. 5 shows the computational complexity of solving the instances of the model d-k-CSP around the theoretical thresholdr = 1 when n is varied from 20 to 80 in steps of 5. According to Corollary 4.3, k satisfies the condition k (cid:2) (1 + ε)ln n/ln ln nfor an arbitrary positive real ε, so we take k = 3 when n is varied from 20 to 80 in steps of 5. Table 5 gives the correspondingvalues of d satisfying the condition of Corollary 4.3 against n. Note that the vertical scale uses a log scale in Fig. 5, cf. Table 4.We summarize experimental results for d = ln n, k = 3, p = 0.6 and r ∈ {0.98, 1, 1.02}, and observe that the curves in Fig. 5look like the polygonal lines, which illustrate that the complexity of solving the instances with values d and k around thephase transition point grows exponentially with n. By Corollary 4.3 we know that d and k increase very slowly with n,making it feasible to use this class of models to generate random CSP instances in practice.We also consider the effect of different length of constraint scopes to the solubility phase transition and the hardnessphase transition. Fig. 6 shows the solubility phase transition for d = ln n, p = 0.6, n = 30 and k ∈ {3, 4, 5}. Similarly, Fig. 7indicates the hardness phase transition for d = ln n, p = 0.6, n = 30 and k ∈ {3, 4, 5}. Figs. 6 and 7 present the solubilityphase transition and the hardness phase transition for the different values of k both happen around the theoretical thresholdpoint r = 1. It is observed that the solubility phase transition of the model d-k-CSP becomes sharper and solving theinstances needs more time when the length k of constraint scopes increases. Therefore the solubility phase transition andthe hardness phase transition are affected by the length of constraint scopes.1 In practice, ln n should be rounded to the nearest integer.14Y. Fan et al. / Artificial Intelligence 193 (2012) 1–17Fig. 3. The solubility phase transition for d-k-CSP(3, n, ln n, r, 0.6).Fig. 4. Mean search cost of solving instances in d-k-CSP(3, n, ln n, r, 0.6).Table 4The correspondence between CPU time T (seconds) and ln T .Tln T0.002−60.018−40.135−2107.389254.604403.4629818Table 5The corresponding value of d against n for Corollary 4.3.nd203254304354404454504555605655705755805Y. Fan et al. / Artificial Intelligence 193 (2012) 1–1715Fig. 5. Mean search cost of solving instances in d-k-CSP(3, n, ln n, r, 0.6).Fig. 6. The solubility phase transition for d-k-CSP(k, 30, ln n, r, 0.6).6. ConclusionsIn this paper, we consider such a type of random CSPs: for any given positive integer n, any instance with n variableshas n domains of the same size d, and has t constraints with all constraint scopes of the same length k and all constraintrelations of the same tightness p. We studied a general random model, called the model d-k-CSP, where the tightness p isfixed, but the domain size d and the length k of the constraint scopes are allowed to vary with the number n of variables.Various cases of the parameters which peoples in the artificial intelligence community are interested in are included, e.g. themodel RB (with growing d and fixed k) and the model k-CSP (with fixed d and growing k). The core concept of the general-model-building is that some certain relations among the parameters that define instances of the CSPs could guarantee thepresence of phase transitions.16Y. Fan et al. / Artificial Intelligence 193 (2012) 1–17Fig. 7. Mean search cost of solving instances in d-k-CSP(k, 30, ln n, r, 0.6).From the new research perspective, we proved that for the model d-k-CSP a satisfiability phase transition threshold canbe exactly quantified if k ln d increases no slower than ln n.And, to prove the theoretical result mathematically, in the stage of the “second moment method” we developed a newidea to divide the whole area, which we considered to estimate the satisfiability probability, by curves (instead of by linesas before), and in each divided area we made the estimations in different ways. As the success of the new methodology forthe model d-k-CSP, it may be interesting to explore any applicability of the new approach to solve related open problems inthe future.Moreover, for the model d-k-CSP of the case d = ln n, a series of experimental studies are carried out and the resultsare reported in this paper. The results not only illustrate the satisfiability phase transition which is consistent with thetheoretical result, but also demonstrate that the hardness phase transition appears at the satisfiability phase transitionpoint. Just like some well-known cases, we can get a lot of hard instances with the model d-k-CSP of the case d = ln n.Because of the extensively effective range of the model d-k-CSP, it provides a lot of choices to generate random CSPinstances with a phase transition in practice. Though the product k ln d should increase no slower than ln n, quite a part ofthe choices provided by the model d-k-CSP makes the domain size d and the length k of the constraint scopes growing upwith n very slowly. The case of d = ln n is a new example of such choices. Thus, the model d-k-CSP is useful for testing CSPsolvers, since such choices provided by the model can generate asymptotically non-trivial CSP instances with small domainsize and small length of constraint scopes in an easy and natural way.AcknowledgementsThanks are given to NSFC for the support through Grant Nos. 11171370 and 60973033, and the National 863 Program(Grant No. 2012AA011005). It is also the authors’ great pleasure to thank the anonymous referees for their many helpfulcomments.References[1] D. Achlioptas, L. Kirousis, E. Kranakis, D. Krizanc, M. Molloy, Y. Stamatiou, Random constraint satisfaction: A more accurate picture, in: Proceedings ofthe Third International Conference on Principles and Practice of Constraint Programming, in: LNCS, vol. 1330, Springer, 1997, pp. 107–120.[2] E. Ben-Sasson, A. Wigderson, Short proofs are narrow – resolution made simple, Journal of the ACM 48 (2001) 149–169.[3] P. Cheesman, B. Kanefsky, W. Taylor, Where the really hard problems are, in: Proceedings of IJCAI-91, IJCAII, 1991, pp. 331–337.[4] V. Chvátal, B. Reed, Miks gets some (the odds are on his side), in: Proceedings of the 33rd IEEE Symposium on Foundations of Computer Science, IEEEComputer Society Press, 1992, pp. 620–627.[5] V. Chvátal, E. Szemerédi, Many hard examples for resolution, Journal of the ACM 35 (1988) 208–759.[6] N. Creignou, H. Daudé, Random generalized satisfiability problems, in: Proceedings of SAT, Elsevier, 2002.[7] N. Creignou, H. Daudé, Generalized satisfiability problems: minimal elements and phase transitions, Theoretical Computer Science 302 (2003) 417–430.[8] N. Creignou, H. Daudé, Combinatorial sharpness criterion and phase transition classification for random CSPs, Information and Computation 190 (2004)220–238.[9] J. Díaz, L. Kirousis, D. Mitsche, X. Pérez-Giménez, On the satisfiability threshold of formulas with three literals per clause, Theoretical ComputerScience 410 (2009) 2920–2934.Y. Fan et al. / Artificial Intelligence 193 (2012) 1–1717[10] M. Dyer, A. Frieze, M. Molloy, A probabilistic analysis of randomly generated binary constraint satisfaction problems, Theoretical Computer Science 290(2003) 1815–1828.[11] Y. Fan, J. Shen, On the phase transitions of random k-constraint satisfaction problems, Artificial Intelligence 175 (2011) 914–927.[12] A. Flaxman, A sharp threshold for a random constraint satisfaction problem, Discrete Mathematics 285 (2004) 301–305.[13] E. Friedgut, Sharp thresholds of graph properties, and the k-sat problem, with an appendix by Jean Bourgain, Journal of the American MathematicalSociety 12 (1999) 1017–1054.[14] A. Frieze, M. Molloy, The satisfiability threshold for randomly generated binary constraint satisfaction problems, Random Structures & Algorithms 28(2006) 323–339.[15] A. Frieze, S. Suen, Analysis of two simple heuristics on a random instance of k-sat, Journal of Algorithms 20 (1996) 312–355.[16] A. Frieze, N. Wormald, Random k-sat: A tight threshold for moderately growing k, in: Proceedings of the Fifth International Symposium on Theory andApplications of Satisfiability Testing, Springer, 2002, pp. 1–6.[17] Y. Gao, J. Culberson, Consistency and random constraint satisfaction models with a high constraint tightness, in: Proceedings of the Tenth InternationalConference on Principles and Practice of Constraint Programming (CP-2004), Springer, 2004, pp. 17–31.[18] Y. Gao, J. Culberson, Consistency and random constraint satisfaction models, Journal of Artificial Intelligence Research 28 (2007) 517–557.[19] I. Gent, E. Macintyre, P. Prosser, B. Smith, Random constraint satisfaction: Flaws and structure, Constraints 6 (2001) 345–372.[20] W. Jiang, T. Liu, T. Ren, K. Xu, Two hardness results on feedback vertex sets, in: Frontiers in Algorithmics and Algorithmic Aspects in Information andManagement—Joint International Conference (FAW–AAIM), Springer, 2011, pp. 233–243.[21] A. Kaporis, L. Kirousis, E. Lalas, The probabilistic analysis of a greedy satisfiability algorithm, in: The 10th Annual European Symposium on Algorithms,Springer, 2002, pp. 574–585.[22] T. Liu, X. Lin, C. Wang, K. Su, K. Xu, Large hinge width on sparse random hypergraphs, in: Proceedings of the 22nd International Joint Conference onArtificial Intelligence, IJCAI/AAAI, 2011, pp. 611–616.[23] M. Mitzenmacher, E. Upfal, Probability and Computing: Randomized Algorithm and Probabilistic Analysis, Cambridge Univ. Press, Cambridge, 2005.[24] M. Molloy, Models and thresholds for random constraint satisfaction problems, in: Proceedings of the 34th ACM Symposium on Theory of Computing,ACM Press, 2001, pp. 209–217.[25] M. Molloy, M. Salavatipour, The resolution complexity of random constraint satisfaction problems, SIAM Journal on Computing 37 (2007) 895–922.[26] P. Prosser, An empirical study of phase transitions in binary constraint satisfaction problems, Artificial Intelligence 81 (1996) 81–109.[27] B. Smith, Constructing an asymptotic phase transition in random binary constraint satisfaction problems, Theoretical Computer Science 265 (2001)265–283.[28] B. Smith, M. Dyer, Locating the phase transition in binary constraint satisfaction problems, Artificial Intelligence 81 (1996) 155–181.[29] C. Wang, T. Liu, P. Cui, K. Xu, A note on treewidth in random graphs, in: 5th Annual International Conference on Combinatorial Optimization andApplications (COCOA), Springer, 2011, pp. 491–499.[30] K. Xu, F. Boussemart, F. Hemery, C. Lecoutre, Random constraint satisfaction: Easy generation of hard (satisfiable) instances, Artificial Intelligence 171(2007) 514–534.[31] K. Xu, W. Li, Exact phase transitions in random constraint satisfaction problems, Journal of Artificial Intelligence Research 12 (2000) 93–103.[32] K. Xu, W. Li, Many hard examples in exact phase transitions, Theoretical Computer Science 355 (2006) 291–302.