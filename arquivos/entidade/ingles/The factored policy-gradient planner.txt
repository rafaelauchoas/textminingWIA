Artificial Intelligence 173 (2009) 722–747Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintThe factored policy-gradient planner ✩Olivier Buffet a,∗, Douglas Aberdeen ba LORIA-INRIA, Nancy University, Nancy, Franceb Google Inc., Zurich, Switzerlanda r t i c l ei n f oa b s t r a c tArticle history:Received 8 October 2007Received in revised form 31 October 2008Accepted 9 November 2008Available online 27 November 2008Keywords:Concurrent probabilistic temporal planningReinforcement learningPolicy-gradientAI planningWe present an any-time concurrent probabilistic temporal planner (CPTP) that includescontinuous and discrete uncertainties and metric functions. Rather than relying on dynamicprogramming our approach builds on methods from stochastic local policy search. That is,we optimise a parameterised policy using gradient ascent. The flexibility of this policy-gradient approach, combined with its low memory use, the use of function approximationmethods and factorisation of the policy, allow us to tackle complex domains. This factoredpolicy gradient (FPG) planner can optimise steps to goal, the probability of success, orattempt a combination of both. We compare the FPG planner to other planners on CPTPdomains, and on simpler but better studied non-concurrent non-temporal probabilisticplanning (PP) domains. We present FPG-ipc, the PP version of the planner which has beensuccessful in the probabilistic track of the fifth international planning competition.© 2008 Elsevier B.V. All rights reserved.1. IntroductionOnly a few planners have attempted to handle concurrent probabilistic temporal planning (CPTP) domains in their mostgeneral form. These tools have been able to produce good or optimal policies for relatively small problems. We designedthe factored policy gradient (FPG) planner with the goal of creating tools that produce good policies in real-world domainswith complex features. Such features may include metric functions (resources for example), concurrent actions, uncertaintyin the outcomes of actions and uncertainty in the duration of actions.In a single paragraph, our approach is to: 1) use gradient ascent for local policy search; 2) factor the policy into simpleapproximate policies for starting each action; 3) base policies on important elements of state only (implicitly aggregatingsimilar states); 4) estimate gradients using Monte-Carlo style algorithms that allow arbitrary distributions; and 5) optionallyparallelising the planner.The AI planning community is familiar with the value-estimation class of reinforcement learning (RL) algorithms, suchas RTDP [1], and arguably AO* [2]. These algorithms represent probabilistic planning problems as a state space and estimatethe long-term value, utility, or cost of choosing each action from each state [3,4]. The fundamental disadvantage of suchalgorithms is the need to estimate the values of a huge number of state-action pairs. Even algorithms that prune most statesstill fail to scale due to the exponential increase of important states as the domains grow. There is a wealth of literatureon the use of function approximation for estimating state-action values (e.g., [5,6]), however this has been little adopted(see [7] for an example) in the planning community, perhaps due to the difficulty of interpreting such approximated policies.✩The majority of this work was performed while the authors were employed by National ICT Australia.* Corresponding author.E-mail addresses: olivier.buffet@loria.fr (O. Buffet), doug.aberdeen@google.com (D. Aberdeen).URLs: http://www.loria.fr/~buffet/ (O. Buffet), http://sml.nicta.com.au/~daa (D. Aberdeen).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.11.008O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747723On the other hand, the FPG planner borrows from policy-gradient (PG) reinforcement learning [8–11]. This class ofalgorithms does not estimate state-action values, and thus memory use is not directly related to the size of the state space.Instead, policy-gradient RL algorithms estimate the gradient of the long-term average reward of the process. In the contextof stochastic shortest path problems, which covers most probabilistic planning problems, we can view this as estimatingthe gradient of the long-term value of only the initial state. Gradients are computed with respect to a set of real-valuedparameters governing the choice of actions at each decision point. These parameters summarise the policy, or plan,1 of thesystem. As distinct from value-based approaches, even those using function approximation, the parameters do not encodethe absolute value of actions or plans. Instead they encode only relative merit of each action. Hence we hope to achievea compact policy representation. Stepping the parameters in the direction of the gradient increases the long-term averagereward, improving the policy. Also, PG algorithms are guaranteed to converge to at least a local maximum when usingapproximate policy representations, which is necessitated when the state space is continuous, otherwise infinite, or simplyvery large. Our setting permits an infinite state space when action durations are modelled by continuous distributions.The policy takes the form of a function that accepts an observation of the planning state as input, and returns a prob-ability distribution over currently legal actions. The policy parameters modify this function. In other words, the policyparameters tune the shape of the probability distributions over actions, given the current planning state observation. Inour temporal planning setting, an action is defined as a single grounded durative action (in the planning domain definitionlanguage (PDDL) 2.1 sense [12]). A command is defined as a decision to start 0 or more actions concurrently. The commandset is therefore at most the power set of actions that could be started at the current decision-point state.From this definition it is clear that the size of the policy, even without learning values, can grow exponentially with thenumber of actions. We combat this command space explosion by factoring the parameterised policy into a simple policyfor each action. This is essentially the same scheme explored in the multi-agent policy-gradient RL setting [13,14]. Eachaction has an independent agent/policy that implicitly learns to coordinate with other action policies via global rewards forachieving goals. By doing this, the number of policy parameters—and thus the total memory use—grows only linearly withthe length of the grounded input description.An advantage of using function approximators is the ability to generalise learned knowledge. If the starting state changes,or an exogenous event occurs, or we modify the current plan by direct interaction, FPG can return a new suggested actioneffectively instantly. The quality of the suggested action will depend on how different the current state is to one that it hasfrequently encountered in training. This is an important quality for mixed-initiative planning [15].At the same time, this policy-gradient has the advantage of not only ignoring irrelevant states—because of an implicitreachability analysis—but also of focusing its effort on states which are the most relevant to the best policy, which arenaturally encountered more frequently. This feature is reminiscent of real-time dynamic programming [1].Our first parameterised action policy is a simple linear function approximator that takes the truth value of the predicatesat the current planning state, and outputs the probability of starting the command. A criticism of policy-gradient RL methodscompared to search-based planners—or even to value-based RL methods—is the difficulty of translating vectors of parametersinto a human readable plan. Thus, the second parameterised policy we explore is a readable decision tree of high-levelplanning strategies. Our non-concurrent, non-temporal version of FPG for the International Planning Competition (IPC),could be considered a third form of policy that ensures that only one action out of many eligible actions are chosen. Finally,we will describe how elements of the Relational Online Policy Gradient (ROPG) planner [16], can be viewed as an FPG styleparameterised policy.We believe that the contribution of this paper is an exploration of how existing Monte-Carlo local optimisation methodscan feed into planning under uncertainty. In summary, the local optimisation and factored policies framework allow goodpolicies to be found for very rich domains. Sometimes this is at the cost of long optimisation times or local minima. How-ever, we demonstrate that the FPG approach can find optimal policies where other state-of-the-art planning methods, e.g.,replanning, can fail. There are many alternate local optimisation methods and parameterisations that could yield interestingresults, those presented here should be considered as useful examples.The paper starts with a background section introducing Markov decision processes (MDPs), policy-gradient algorithmsfor MDPs and related work. Section 3 describes FPG, including example function approximators and some implementationdetails. Experimental results from Section 4 show the overall quality of this approach, pinpointing its main strengths andweaknesses.2. BackgroundWe describe some relevant background in planning, Markov decision processes, policy-gradient algorithms and previousprobabilistic planning approaches.1 We will generally prefer the term policy over plan to mean the final output of the planning phase. In a probabilistic setting plans change, policies donot.724O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747Fig. 1. A snippet in our XML format of a racing car domain, showing a probabilistic effect with a discrete probability outcome and continuous probabilitydelay.2.1. Concurrent probabilistic temporal planning (CPTP)FPG’s input language is the temporal STRIPS fragment of PDDL 2.1 but extended with probabilistic outcomes and un-certain durations, as in PPDDL [17,18]. In particular, we support continuous uncertain durations, functions, at-start, at-end,over-all conditions, and finite probabilistic action outcomes. In addition, we allow effects (probabilistic or otherwise) tooccur at any time within an action’s duration. FPG’s input syntax is actually XML with a schema designed to map almostdirectly to PPDDL (see Fig. 1). Our PPDDL to XML translator grounds actions and flattens nested probabilistic statements toa discrete distribution of action outcomes with delayed effects.Grounded actions are the basic planning unit. An action is eligible to begin when its preconditions are satisfied. It is pos-sible that certain combinations of eligible actions may be mutually exclusive. We will return to this possibility later. Actionexecution may begin with at start effects. Execution then proceeds to the next probabilistic event, an outcome is sam-pled, and the outcome effects are queued for the appropriate times. We use a sampling process rather than enumeratingoutcomes because it will transpire that we only need to simulate executions of the plan in order to estimate the neces-sary gradients. A benefit of this approach is that we can sample from both continuous and discrete distributions, whereasenumerating continuous distributions is not possible.2With N eligible actions there are up to 2N possible commands. Current planners explore this command space system-atically, attempting to prune commands via search or heuristically. When combined with probabilistic outcomes the statespace explosion cripples existing planners with just a few tens of actions. We deal with this explosion by factorising theoverall policy into independent policies for each action. Each policy learns whether to start its associated action given thecurrent predicate values, independent of the decisions made by the other action policies. This idea alone does not simplifythe problem. Indeed, if the action policy approximations were sufficiently rich, and all receive the same state observation,they could learn to predict the decision of the other actions and still act optimally. The significant reduction in complexityarises from using approximate policies, which implicitly assumes similar states will have similar policies.The FPG planner aims to produce good plans in very rich and large domains. It is not, however, complete in the sense thatit will always return a solution if one exists. In particular, FPG shares the completeness problems found in other temporalplanners [19,20] that arise when decisions to start new actions are restricted to times where an event is already queuedto occur, that is, at happenings. Since such examples are not frequent, and existing solutions are either computationallyprohibitive or require significant restrictions on domains (e.g., Temporal Graphplan (TGP) style actions only [20]), we choosenot to attempt guaranteed completeness for FPG.2.2. Probabilistic planningAlthough FPG was initially developed with the objective of handling concurrent probabilistic temporal planning (CPTP)problems, a simplified version called FPG-ipc participated in the probabilistic track of the fifth International Planning Com-petition (IPC-5) in 2006. We detail the simplifications in Section 3.2.3.This probabilistic planning (PP) setting can be seen as a restriction of CPTP, the objective being to maximise the probabilityof reaching the goal. Candidate planners had to process PPDDL specifications using :adl requirements [17,21].2.3. Previous workPrevious probabilistic temporal planners include DUR [22], Prottle [4], and a Military Operations (MO) planner [23]. Allthese algorithms use some optimised form of dynamic programming (either RTDP [1] or AO* [2]) to associate values witheach state-action pair. However, this requires that values be stored for each encountered state. Even though these algorithms2 We sample integer times, and there is a maximum permitted makespan, so these distributions are in reality still finite, but extremely large.O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747725prune off most of the state space, their ability to scale is still limited by memory size. Tempastic [24] uses the generate,debug, and repair planning paradigm. It overcomes the state space problem by generating decision tree policies from sampletrajectories that follow good deterministic policies, and repairing the tree to cope with uncertainty. This method may sufferin highly non-deterministic domains, but is a rare example of an approach that also permits modelling continuous distribu-tions for durations. Prottle, DUR, and Tempastic minimise either plan duration or failure probability. The FPG planner allowsfor simple trade-offs of these metrics.The 2004 and 2006 probabilistic tracks of the International Planning Competition (IPC) represent a cross section of recentapproaches to non-temporal probabilistic planning. Along with the FPG planner, other entrants included FOALP, Paragraphand sfDP. FOALP [25] solves a first order logic representation of the underlying domain MDP, prior to producing plans forspecific problems drawn from that domain. Paragraph [26] is based on Graphplan extended to a probabilistic framework.sfDP [27] uses a symbolic form of dynamic programming based on Algebraic Decision Diagrams (ADDs). A surprisinglysuccessful approach to the competition domains was FF-rePlan [28], winning the 2004 competition. A subsequent versioncould have also achieved first place at the 2006 competition. FF-rePlan uses the FF heuristic [29] to quickly find a potentialshort path to the goal. It does so by creating a deterministic version of the domain. Thus, it does not attempt to directlyoptimise either the probability of reaching the goal or the cost-to-go. In practice though, for many domains the ability toreach the goal at all leads to good performance. However, any replanning approach can perform poorly when the cost offailure must be taken into account [30].Policy-gradient RL for multiple-agents MDPs is described by [13,14], providing a precedent for factoring policy-gradientRL policies into independent “agents” for each action. This paper also builds on earlier work presented by [31,32].The challenge of learning to generalise across problems within a particular domain is another use of function approxi-mation for generalisation. The classical view of learning for planning is to acquire knowledge about a given domain by (1)planning in small problem instances; and (2) reusing this knowledge—such as heuristic rules—to plan in larger domains [33].In this direction there have been attempts at using Relational Reinforcement Learning (RRL) to find generic policies for plan-ning problems expressed in first-order logic [7,16,34]. But this remains a very challenging task: the search space is muchlarger and abstract notions may be required, such as above(A,B) and numberofblockson(X,N) in the Blocksworld.The Approximate Policy Iteration algorithm used in Classy is similar to FPG in the sense that it avoids computing a valuefunction and heavily relies on Monte-Carlo simulations, but does not attempt to factorise the policy. The Relational On-line Policy-Gradient (ROPG) [16] uses exactly the same policy-gradient algorithm as FPG. ROPG learns which higher-ordercontrol strategy to follow in a given state. While the contribution of ROPG is to develop possible control strategies, thepolicy-gradient component is needed to learn which strategies work and when.2.4. Markov decision processes and policy-gradient algorithmsWe describe our Markov decision process (MDP) framework and then give an overview of gradient ascent and policy-gradient algorithms.2.4.1. Markov decision processesA finite partially observable Markov decision process consists of: a (possibly infinite) set of states s ∈ S; a finite setof actions c ∈ C, that correspond to our command concept; probabilities P[sundercommand c; a reward for each state r(s) : S → R;3 and a finite set of observation basis vectors o ∈ O used by action policiesin lieu of complete state descriptions.(cid:4) | s, c] of making state transition s → s(cid:4)We can trade off the complexity of the action policies with the amount of state information provided as an observation.As we provide more and more state information, the policies can become richer and richer. At the extreme end of thisspectrum we provide a unique command for every state (essentially a state → command lookup table). At the other end,the policy knows nothing about the current state and can only generalise across all states by estimating the best stationarypolicy.However, in the case of FPG with a linear approximator, we construct policy observation vectors from the state as follows.Each predicate value—and only the predicate values—becomes an observation bit. We set the bit to 1 for asserted predicate,and 0 otherwise. A constant 1 observation bit is also provided as a bias element to assist the linear approximator.Goal states occur when the predicates and functions match a PPDDL goal state specification. From failure states it isimpossible to reach a goal state, usually because time or resources have run out, but it may also be due to an at-end orover-all condition being invalid. These two classes of end state form the set of terminal states, ending plan simulation.Policies are stochastic, mapping the observation vector o, generated from the current planning state, to a probabilitydistribution over commands. Fundamentally, this is necessary to enable the exploration of the command space. Over thecourse of optimisation, we hope that the policy distribution becomes increasingly peaked over equally optimal commands.Let N be the number of grounded actions available to the planner. For FPG a command c is a binary vector of length N.An entry of 1 at index n ∈ {1, . . . , N} means ‘Yes’ begin action n, and a 0 entry means ‘No’ do not start action n. Theprobability of a command is P[c | o; θ ], where conditioning on θ reflects the fact that the policy is tuned by a set of real3 This work remains valid when the reward depends on a complete transition r(s, c, s(cid:4)). We consider a simpler setting for readability reasons.726O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–7471: Initialisation:2: t ← 03: repeatt ← t + 14:θ t ← θ t−1 + α∇ R(θ t−1)5:6: until stoppingCriterion((cid:3), . . .)7: Return θ tAlgorithm 1. Generic gradientAscent(R, θ 0, α, (cid:3)).valued parameters θ ∈ Rp . Commands with non-eligible actions are guaranteed to have probability 0. We assume that allstochastic policies (i.e., any values for θ ) reach terminal states in finite time when executed from s0. This is enforced bylimiting the maximum makespan of a plan.FPG’s optimisation criteria is very general: it maximises the long-term average rewardR(θ ) := limT →∞1TEθ(cid:2)T −1(cid:3)(cid:4)r(st),t=0(1)where the expectation Eθ is over the distribution of state trajectories {s0, s1, . . .} induced by the current joint policy. Inthe context of planning, the instantaneous reward provides the action policies with a measure of progress toward the goal.A simple reward scheme is to set r(s) = 1000 for all states s that represent the goal state, and 0 for all other states. Tomaximise R(θ ), goal states must be reached as frequently as possible. This has the desired property of simultaneouslyminimising steps to goal and maximising the probability of reaching the goal because failure states achieve no reward.There are some pitfalls to avoid when describing a reward scheme. For example, if we have a large negative reward forfailure, the optimisation may choose to extend the plan execution as long as possible to reduce the frequency of negativerewards.We also provide intermediate rewards for progress toward the goal. These additional shaping rewards provide an im-mediate reward of 1 for achieving a goal predicate, and −1 for every goal predicate that becomes unset. Shaping rewardsare provably “admissible” in the sense that they do not change the optimal policy [35]. The shaping assists convergence fordomains where long chains of actions are necessary to reach the goal and proved important in achieving good results in IPCdomains. The reward shaping helps reinforcements occur as soon as an action moves the system closer to (or more distantfrom) the goal, not just when the goal is reached. This helps solve the reward assignment problem.2.4.2. Gradient algorithmsWe want to maximise R(θ ) by gradient ascent. That is, repeatedly computing gradients ∇ R(θ ) and stepping the param-eters in that direction. In our setting, the gradient is a vector operator mapping any differentiable function R : Rp → R to∇ R : Rp → Rp defined as⎡⎤∇ R(θ1, . . . , θp) =⎢⎣⎥⎦ .(2)∂ R∂θ1∂ R∂θn(θ1, . . . , θp)...(θ1, . . . , θp)A gradient ascent is an iterative algorithm used to find a local maximum of R(θ ). The principle is to compute a sequence{θ t}t∈T by following, at each parameter values point θ t , the direction of the gradient at this point. Algorithm 1 gives a verygeneric overview of the process, which requires: a differentiable function R, a starting point θ 0, a step-size α > 0 and, often,a threshold (cid:3) > 0 used by some stopping criterion. The stopping criterion will typically be a function of the parameters, thegradients, time, and/or the number of steps.The details of how Algorithm 1 can be implemented constitute an entire complex field. FPG is actually an online gradientascent, where the function R has additional external inputs which effect the gradient from step to step. In FPG’s case theadditional inputs are the observation of the planning state ot . However, as it shall transpire, the Markov nature of theprocess means a weighted average of these direct “policy” gradients with specific observations, converges to an approximateestimate of the gradient of R(θ ), and thus can be safely used in Algorithm 1.The gradients are stochastic because the planning domains contain uncertainty and we are sampling from distributionsover actions. The combination of being online and stochastic means that many of the more advanced gradient ascent al-gorithms such as line searches, BFGS, and other conjugate methods cannot be immediately applied [36]. However, we willcomment on the use of some approximate second-order gradient ascent methods in the discussion.Gradient optimisation methods perform local search. That is, they greedily step in the direction of the gradient untilreaching a maximum which may not be the global maximum. We accept this possibility because gradient ascent is far moretractable than global optimisation methods, such as tabula rasa dynamic programming variants. In practice, gradient ascentmay achieve good results on a planning domain which a global optimiser cannot return any policy for, typically becausememory runs out.O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–7477272.4.3. Introduction to policy-gradient algorithmsComputing the gradient of the long-term average reward R(θ ) in a closed-loop Markov decision process is not entirelytrivial. However, several papers have solved this problem with practical algorithms [8,9,11,37]. The benefits of this approachcompared to other reinforcement learning methods are: 1) local convergence under function approximation and partialobservability; 2) memory usage that is linear in the number of parameters in the policy function; 3) a convenient (althoughnot optimal) implicit approach to the exploration versus exploitation trade-off.We will use Baxter and Bartlett’s approach to policy-gradient algorithms [37]. For historical reasons we begin with anintroduction based on Williams’ REINFORCE algorithm [8], which is credited with being the first policy-gradient method ina reinforcement learning context.Horizon 1 gradient estimate Let us first consider a planning problem with maximum horizon length 1. This means startingin a random state s, choosing a command c, executing it and transitioning to an end state s. The transition generates aninstant reward r. The criterion to optimise is:(cid:4)R(θ ) = Eθ(cid:11)r(s, c, s(cid:4)(cid:12))=(cid:3)s,o,c,s(cid:4)P(s) P(o | s) P[c | o; θ ] P(s(cid:13)(cid:14)(cid:15)P(s,o,c,s(cid:4))(cid:4) | s, c)(cid:16)r(s, c, s(cid:4)),which can be estimated with N iid samples of r as ˜R(θ ) = 1Similarly, taking the gradient of R(θ ) gives:(cid:18) (cid:3)∇ R(θ ) = ∇P(s) P(o | s) P[c | o; θ ] P(s(cid:4) | s, c)r(s, c, s(cid:4))s,o,c,s(cid:4)(cid:3)(cid:11)∇ P[c | o; θ]P(s) P(o | s)(cid:12)P(s(cid:4) | s, c)r(s, c, s(cid:4))N(cid:17)Nk=1 rk.(cid:19)===∇ P[c | o; θ ]P[c | o; θ ](cid:19)s,o,c,s(cid:4)(cid:3)(cid:18)P(s) P(o | s)s,o,c,s(cid:4)(cid:3)(cid:18)∇ P[c | o; θ ]P[c | o; θ ] r(s, c, s(cid:19)∇ P[c | o; θ ]P[c | o; θ ] r(s, c, s(cid:4)),s,o,c,s(cid:4)(cid:18)= Eθ(cid:19)P[c | o; θ ]P(s(cid:4) | s, c)r(s, c, s(cid:4))(cid:4))P(s) P(o | s) P[c | o; θ ] P(s(cid:13)(cid:14)(cid:15)P(s,o,c,s(cid:4))(cid:4) | s, c)(cid:16)which provides the following estimate (with N samples):˜∇ R(θ ) = 1NN(cid:3)k=1∇ ln P[ck | ok; θ ]rk(noting that∇ P[ck|ok;θ ]P[ck|ok;θ ] = ∇ ln P[ck | ok; θ ]).Horizon 1 REINFORCE algorithm In a horizon 1 online gradient ascent, parameter θi is incremented after each iteration withan instant estimate of the gradient (N = 1), i.e., using:(cid:6)θi = αrei,where α > 0 is a step size (or learning rate factor) and ei = ∂ ln P[c | o; θ ]/∂θi is called the characteristic eligibility of θi .Under the same conditions, let us consider the following increment:(cid:6)θi = αi(r − bi)ei,where αi is a specific learning rate for parameter θi and bi is a baseline. If bi is conditionally independent of o and αidepends at most on θ and k, then such a learning algorithm is called a REINFORCE algorithm. Such an algorithm has theinteresting property [8, Theorem 1] that E[(cid:6)θ | θ] is always an ascent direction. Its inner product with ∇θ E[r | θ ] is non-negative, and is zero only when ∇θ E[r | θ] = 0. REINFORCE has two parameters. Let us observe that:• an appropriately decreasing step size αi , chosen to satisfy the standard stochastic function approximation conditions[38], ensures that θi converges to some limit value without missing a local optimum;• the choice of the reinforcement baseline makes it possible to improve the convergence behavior as well: for example,bi = ˜r (an empirical value of E[r]) leads to less instabilities than bi = 0 because it compares the instant reward withwhat is usually received, equivalent to a variance reduction in gradient estimates [39].728O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747To show how the characteristic eligibility ei is computed, let us assume an example where:• a single action a is available, the two possible commands being do not execute a (c = 0) and execute a (c = 1);• the parameterised policy takes the following form:(cid:20)P[c | o; θ ] =f (o, θ)1 − f (o, θ )if c = 0,if c = 1,where ff (o, θ) =is differentiable with respect to parameters θ1, . . . , θp . One such f might be a simple logistic regression11−exp(o(cid:2)θ ) .Then, the characteristic eligibility for parameter θi is given by∂ ln P[c | o; θ ]∂θi==1P[c | o; θ ](cid:21)1f (o,θ)11− f (o,θ)∂ P[c | o; θ ]∂θi∂ f (o,θ)∂θi−∂ f (o,θ)∂θiif c = 0,if c = 1.Indefinite horizon REINFORCE algorithm For a true policy-gradient approach we need to extend this algorithm to problemswith an indefinite horizon. Each trial should have a finite, but possibly unknown, duration. A first approach is, for a giventrial of length N, to accumulate the reward in rtrial and then compute the update direction for parameter θi as(cid:6)θi = αi(rtrial − bi)N(cid:3)k=1ei(k),where ei(k) is the characteristic eligibility evaluated at time k. Another option is to make an update at each time step kusing(cid:6)θi = αi(cid:22)r(k) − bi(cid:23) k(cid:3)ei(t).t=1The latter approach has the advantage of providing an online algorithm that makes use of instant rewards as soon as theyare obtained.REINFORCE algorithms are not exactly gradient ascent algorithms. They follow an estimate of an ascent direction (within◦) but not an estimate of the gradient direction itself (due to the step sizes being different for each parameter). Another90issue is that REINFORCE’s updates give the same weight to old and recent decisions, although in some settings old decisionsshould be assigned little credit for the current reward. This is why we prefer using a policy-gradient algorithms designedfor infinite horizon problems. In the next section we present the reinforcement learning algorithms used by FPG, which arebased on a direct estimate of the gradient and are meant for infinite horizon POMDPs.2.4.4. Baxter and Bartlett’s policy-gradient algorithmsWe follow the presentation of Baxter and Bartlett [10,37], and note that while other derivations [9,11] differ substantially,the resulting algorithms vary only a little. We begin our overview of this policy-gradient approach with an expression forthe exact gradient ∇ R(θ ).Theorem 1 (Exact policy-gradients [37]). Suppose there are S possible planning states.4 Let P (θ) be an S × S ergodic stochastic. Note that P (θ) describes the Markov chain resultingtransition matrix that gives the probability of a transition from state s to state sfrom setting parameter vector θ . Let π (θ ) be a 1 × S column vector that gives the stationary probability of being in any particularstate, derived from the unique solution of π P (θ ) = π . Let r be the 1 × S vector with the reward for each state. The identity matrix isgiven by I , and e is a column vector of 1’s. With these definitions(cid:23)(cid:22)(cid:4)(cid:2)∇ R(θ ) = π (θ )(cid:22)∇ P (θ )(cid:2)I − P (θ) + eπ (θ )(cid:23)−1r.(3)The point we wish to make with this theorem is that policy-gradients could be used to do true model-based plan-ning [40]. The planning domain and problem specification contain all the information necessary to calculate π (θ ) and P (θ)for a given initial policy described by θ (such as uniformly randomly starting eligible actions). We can construct a rewardvector r as described in Section 2.4.1. Also, ∇ P (θ ) can be computed and the matrix inverse in (3) exists. Thus if the state4 See [37] for the details required to extend this theorem to continuous state spaces.O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747729space is finite (that is, no continuous durations and a makespan limit) we could perform an exact model-based policy-gradient optimisation by repeatedly solving (3) in the Algorithm 1 loop. This seems preferable to our suggested approachof using the model simply to create a plan execution simulator for use with reinforcement learning. However, any sizeableplanning problems involve millions of possible states, making the O (S 3) matrix inversion in (3) intractable. It is worthnoting that P (θ ) is typically very sparse. This fact, along with a truncated iterative solution to (3), was used to performmodel-based policy-gradient on systems with tens of thousands of states [40].As an aside, we attempted to exploit a structured ADD (algebraic decision diagram) representation of (3). ADDs cancompactly represent factored matrices and the standard matrix operations can be redefined in terms of ADD manipulations.This fact has previously been used in planning [41]. Our idea was to create a parameterised ADD that allowed an analyticsolution to (3), which could then be efficiently evaluated for any parameter values. However, the initially compact ADDrepresentations of P (θ) and π (θ ) explode in size during the solution of the matrix inverse [42].Because an exact computation of the gradient is intractable we use Monte-Carlo gradient estimates generated fromrepeatedly simulating plan executions as if they were one long Markov process.Theorem 2 (Estimated approximate policy-gradients [37]). Let rt be the scalar reward received at time, i.e., rt = r(st). Let β ∈ [0, 1)be a discount factor. Then∇ R(θ ) = limβ→1limT →∞1TT(cid:3)t=1∇θ P[ct | ot; θ ]P[ct | ot; θ ]T(cid:3)τ =t+1βτ −t−1rτ .(4)This quantity is an estimate that becomes exact as T → ∞, of an approximation that becomes exact as β → 1. Thedetailed derivation is out of the scope of this paper, but we do wish to provide some insight into how this expressionrelates to (3). In particular, the role of the discount factor β. The theorem is derived from (3) by first establishing that(cid:22)(cid:2)I − P (θ) + eπ (θ)(cid:23)−1r = limβ→1vβ (θ ),where vβ (θ) is a 1 × S vector of discounted state values under the current policy. For state s this is defined asvβ (θ , s) := Eθβtr( Xt) | X0 = s,(cid:4)(cid:2)∞(cid:3)t=0where Xt is the random variable denoting the state at time t steps into the future. Note that this is the usual definition ofthe discounted value in reinforcement learning [6]. Combining this with (3) gives∇ R(θ ) = limβ→1(cid:2)π (θ )(cid:22)(cid:23)∇ P (θ )vβ (θ).The Ergodic Theorem can then be applied to turn the summations (implicit in the matrix operations) over state, nextstate, actions and observations into a Monte-Carlo estimate over a single infinite trajectory of states, next states, actionsand observations. So why was β introduced? You can observe from the infinite summation that without β < 1 (4) maybe unbounded. That is, infinite variance arises during the Monte-Carlo estimate. This can be loosely thought of as tryingto assign the credit for a reward to a possibly infinite number of actions into the past. So β creates a decaying artificialhorizon on how long ago actions could occur and still be assigned some credit for achieving the current reward.However, β < 1 introduces a bias [37]. Thus we try and keep β as close to 1 as possible, while still achieving reasonableestimates of the gradient. Alternatively, if we can observe a point where the state is reset (such as at the end of a planexecution), we can impose this true horizon on the impact of actions on rewards [8], and leave β = 1 to achieve an unbiasedestimate. In practice we have found the variance reduction due to β < 1 can be useful, even for finite-horizon planning.We have not yet defined a form for the policy P[ct | ot; θ t]. We discuss this in Section 3.2, and for now only note thatit is a parameterised policy function that we construct to produce a correct probability distribution. Its log derivative mustexist and be bounded to satisfy the assumptions described in [37].Note that (4) requires looking forward in time to observe rewards, so we reverse the summations∇ R(θ ) = limβ→1limT →∞1TT −1(cid:3)t(cid:3)rtt=0τ =0βt−τ∇ P(cτ | oτ ; θ )P(cτ | oτ ; θ ).(5)This now becomes easy to implement by using an eligibility trace et in place of the second summation, as shown in Algo-rithm 2.The eligibility trace et contains the discounted sum of normalised policy gradients for recent commands (equivalent tolog-policy gradients). This can provide an intuition for how the algorithm works: stepping the parameters in the directionof the eligibility trace will increase the probability of choosing recent commands under similar observations, with recencyweighting determined by β. But it is the relative value of rewards that indicate if we should increase or decrease theprobability of recent command sequences. So the instant gradient at each time step is rt et .730O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–7471: t = 02: repeat3:4:5:ot = sim.getObservationsample ct from P(· | ot ; θ t )et = βet−1 + ∇θ P(ct |ot ;θ t )P(ct |ot ;θ t )sim.doAction(ct )rt = sim.getReward()if OlPomdp then6:7:8:9:10:11:12:13: until stoppingCriterion((cid:3), . . .)14: if not OlPomdp then15:t+1 (tgt−1 + rt et )gt = 1t ← t + 1θ t+1 = θ t + αrt etreturn gtelse// batch estimateAlgorithm 2. policyGrad(θ 0, Simulator sim, α).Fig. 2. A high level illustration of how to parallelise FPG. Each node contains a separate process implementing Algorithm 2.There are two offered optimisation methods using the instant gradients [10]. These correspond to the OlPomdp and batchcases distinguished in Algorithm 2. OlPomdp is the simple online stochastic gradient ascent θ t+1 = θ t + αrt et with scalargain α. Alternatively, ConjPomdp averages rt et over T steps to compute the batch gradient gt , approximating (4), followedby a line search for the best step size α in the search direction.5 OlPomdp can be considerably faster than ConjPomdp inhighly stochastic environments because it is tolerant of stochastic gradients and adjusts the policy at every step. We preferit for all our single processor experiments.However, the batch approach is used for parallelising FPG as shown in Fig. 2. Each processor runs independent simula-tions of the current policy with the same fixed parameters. Instant gradients are averaged over many simulations to obtaina per processor estimate of the gradient (4). A master process averages the gradients from each processor and broadcaststhe resulting search direction. All processors then take part in evaluating points along the search direction to establish thebest α. Once found, the master process then broadcasts the final step size. The process is repeated until the aggregatedgradient drops below a threshold.3. FPGWe now describe how policy-gradients can be used to optimise CPTP policies. We begin by describing the constructionof a plan simulator.3.1. State space simulatorFPG’s planning state is:• the makespan so far (the plan starts at time 0), i.e., absolute time;5 We do not apply the conjugation of gradients described in [10], which gives the algorithm its name, because conjugate directions collapse under noise.O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747731• the truth value of each predicate;• the function values;• a dynamic length queue of (at least) future events including:– outcome sampling events,– outcome end events,– effect implementation events,– exogenous events.In a particular state, only the eligible actions have satisfied all at-start preconditions for execution. Recall that a commandis the decision to start a set of eligible actions. While actions might be individually eligible, starting them concurrently mayrequire too many resources. Or starting could cause a precondition of an eligible action to be invalidated by the deterministicstart-effects of another. Both of these are examples of actions which we consider mutually exclusive (mutex). We do notdeal with any other type of conflict when determining mutexes for the purpose of deciding to start actions. For example,we do not consider mutexed outcomes. This is because probabilistic planning means such mutexes may, or may not, occur.If they do occur the plan execution enters a failure state, moving the optimisation away from this policy.The planner handles the execution of actions using a time-ordered event queue. When starting an action, at-start effectsare processed, adding effect events to the queue if there are any delayed at-start effects. Additionally, a sample-outcome event is scheduled for some point during the execution of the action (this duration possibly being sampledfrom a continuous distribution). The sample-outcome event indicates the point when chance decides which particulardiscrete outcome is triggered for a given action. This results in adding the corresponding effect events for this outcome,and any other at-end effects, to the event queue (again possibly with an additional sampled delay). An action ends whenall possible effects due to an action have occurred, although this is an arbitrary definition given our execution model. Thesedefinitions allow predicates to change at any time during an action.The only element of state that is presented to the policy is the truth value of each predicate. We could trivially presentadditional features except for the dynamically sized event queue. However, empirically we found that the state of thepredicates alone was a good summary of the overall planning state. This is intuitive for many domains because their humandesigners describe important state with predicates. Note that it is particularly dangerous to supply a representation of thefuture event queue to the policy. Doing this could allow decisions to be based on a particular probabilistic outcome that hasnot occurred yet, and which would not be foreseen in real life.Exogenous events, if any, are handled by inserting these events into the event queue as they occur. They can includemanually (de)scheduling an action, effect, or probabilistic outcome. Note that this permits a form of mixed initiative plan-ning, and is very useful to see how a policy would adjust to unexpected events.To estimate policy gradients we need a plan execution simulator to generate a trajectory through the planning statespace. It takes commands from the factored policy, checks for mutex constraints, implements at-start effects, and queuessample-outcome events. The state update then proceeds to process sample-outcome and effect events from thequeue until a new decision point is met. Decision points equate to happenings, which occur when: (1) time has increasedsince the last decision point; and (2) there are no more events for this time step. Under these conditions a new action canbe chosen, possibly a no-op if the best action is to simply proceed to the next event. The process of simulating the planexecution from one command choice to another is described by simulateTillHappening() (Algorithm 3).When processing events, the algorithm also ensures no running actions have violated over-all conditions. If this happens,the plan execution ends in a failure state. Note that making decisions at happenings results in FPG being incomplete indomains with some combinations of effects and at-end conditions [19,22]. A simple fix is to set a maximum delay d betweentwo consecutive decision points. In the discrete time case, d = 1 guarantees completeness, but a small d introduces toomany feasible plans. Another fix is to learn how long to wait until the next decision point.6 We have not yet pursued eitherapproach.Only the current parameters, the eligibility trace, initial and current states, and the current observation are kept inmemory at any point in time. The number of parameters depends on the choice of policy function approximation but istypically O (N × |o|), where N is the number of actions and |o| is the dimensionality of the state observations. This is instark contrast with dynamic programming based planners that expand in memory all states relevant to planning, and thenumber of states is generally exponential in the number of state-variables. We emphasise that low memory use is a keyadvantage of FPG’s approach. The planning problem can be orders of magnitude larger than other CPTP planners attempt.However, this is a time/space trade off. FPG may require more time than other approaches to compensate for not retainingdetailed state information.3.2. Choice of the function approximatorA benefit of the FPG approach, arising from the use of policy-gradients, is flexibility in the choice of function approxima-tor. It is possible to trade off the richness of a policy representation with its compactness and ability to be trained quickly.6 This is feasible in the continuous time case because policy-gradient algorithms can optimise controllers with continuous action spaces.732O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747if atn .isMutex() then return MUTEXs.addEvent(atn , sample-outcome, s.time + sample(atn .duration-distribution))for each f ∈ atStartEffects(atn) dofor each f ∈ delayedEffects(atn) dos.addEvent( f , effect, s.time + sample( f .delay-distribution))if s.time > maximum makespan thenif s.operationGoalsMet() thens.processEffect( f )s.goal = truereturns.failure = truereturn1: for each atn = ‘Yes’ in ct do2:3:4:5:6:7:8: repeat9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:28: until s.isDecisionPoint()s.failure = truereturns.processEffect( f )event = s.nextEvent()s.time = event.timeif type(event) = effect thens.processEffect(event.effect)if ¬s.anyEligibleActions() & s.noEvent() thenelse if type(event) = sample-outcome thensample outcome out from eventfor each f ∈ immediateEffects(out) dofor each f ∈ delayedEffects(out) dos.addEvent( f , effect, s.time + sample( f .delay-distribution))Algorithm 3. simulateTillHappening(State s, Command ct ).On the other hand, it can be difficult to choose a representation that achieves the best balance between these considerations.This section provides the generic approximator requirements, and then discusses four specific approximators.The command ct = {at1, at2, . . . , at N } at time t is a combination of independent ‘Yes’ or ‘No’ choices made by each of thegrounded action policies. Each policy has an independent set of parameters that make up θ ∈ Rp : θ 1, θ 2, . . . , θ N . With theindependence of parameters the command policy factors intoP[ct | ot, θ] = P[at1, . . . , at N | ot; θ 1, . . . , θ N ]= P[at1 | ot; θ 1] × · · · × P[at N | ot; θ N ].(6)The computation of the log-policy gradients also factorises trivially. It is not necessary that all action policies receive thesame observation, and it may be advantageous to have different observations for different actions, leading to a decentralisedparadigm.7 Similar factored policy-gradient approaches are adopted by [13] and [14]. The main requirement for each actionpolicy is that log P[atn | ot; θ n] be differentiable and bounded with respect to the parameters for each choice of action startatn = ‘Yes’ or ‘No’ [37]. The gradient must also be bounded.In the situation where all action policies have access to the same state information, and rewards, and their policyapproximations are sufficiently rich, the action policies will be able to coordinate optimally given the state informationavailable. This coordination is a natural consequence of the learning process: individual actions taking part in rewardingjoint actions are reinforced so that the joint actions are preferred [43]. We note that difficulties do arise if there are manyequivalent optimal policies, for example if all policies that pick any single action out of all the eligible actions are equallyoptimal. In this case a gradient based planner can become stuck in saddle regions. The usual, somewhat unsatisfactory,resolution is to rely on random initialisation to give slightly higher probability to one such equivalent policy from theoutset. If each action policy sees different state information, optimal coordination becomes NExp-hard [44]. Methods suchas coordinated RL [45] exist to efficiently solve such problems by performing belief propagation between agents, or policies.The policies become conditionally independent, where the structure of this conditional independence is exploited to makethe process efficient. In this context, the policies would become conditionally dependent on the mutex relationships betweenactions. However, we have so far not experimented with this approach. We ensure that all action policies see the sameobservation.3.2.1. Linear function approximatorsOne simple and effective action policy is a linear approximator mapped to probabilities using a logistic regression func-tion87 Although we note that the use of a single shared simulator for optimisation means this cannot be made a truly decentralised planning algorithm.8 In the RL this function is more commonly expressed as the soft-max, or Gibbs, or Boltzmann distributions.O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747733Fig. 3. High level overview of FPG’s decision making loop, showing linear function approximators.P[atn = Yes | ot; θ n] =1(cid:10)t θ n) + 1exp(o,P[atn = No | ot; θ n] = 1 − P[atn = Yes | ot; θ n].(7)Recall that the observation vector o is a vector representing the current predicate truth values plus a constant bias. Truepredicates get a 1 entry, and false predicates get 0. The bias entry is a constant 1 that allows the linear approximator torepresent a decision boundary that does not pass through the origin. If the dimension of the observation vector is |o| theneach set of parameters θ n can be thought of as an |o| vector that represents the approximator weights for action n. Therequired log-policy instant gradients over each parameter θ ∈ θ n are∇θ nP[atn = Yes | ot; θ n]P[atn = Yes | ot; θ n]∇θ nP[atn = No | ot; θ n]P[atn = No | ot; θ n](cid:22)(cid:10)= −ot expt θ no(cid:23)P[atn = Yes | ot; θ n],= ot P[atn = Yes | ot; θ n].(8)These log-policy gradients are added to the eligibility trace (Algorithm 2, line 5) based on the yes/no decisions for eachaction. Looping this calculation over all eligible actions computes the normalised gradient of the probability of the jointcommand (6). Fig. 3 illustrates this scheme.Initially, the parameters are usually set to 0 giving a uniformly random policy, encouraging exploration of the commandspace. To increase the long-term average reward the policy parameters must gradually adjust to prefer some commandsover others. Typically we see the policy start to converge to a function that gives high probability to commands that havebeen established to be good. If FPG maintains that two or more commands have similar probabilities given a particularobservation, then this is indicative of FPG being unable to determine which command is better. This might be because thereis insufficient state information encoded in o, the parameterisation is too simple, or because the two commands really areequivalent in the long run.3.2.2. Trees of expertsTo demonstrate the flexibility of the FPG approach we develop an alternative parameterised policy function. This policyhas the task of switching between a collection of known expert policy strategies. As described below, this is achieved usinga partly-defined decision tree whose decision nodes are tuned by stochastic gradient ascent.Rather than start with a uniform policy we may be given a selection of heuristic policies that work well across a range ofdomains. For example, in a probabilistic setting we may have access to a replanner, an optimal non-concurrent planner, anda naïve planner that attempts to run all eligible commands. Indeed, the best planner to invoke may depend on the currentstate as well as the overall domain. The decision tree policies described here are a simple mechanism to allow FPG toswitch between such high level expert policies. We assume a user declares an initial tree of all available policies. The leavesrepresent a policy to follow, and the branch nodes represent decision rules for which policy to follow. We show how to learn734O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747Fig. 4. Decision tree action policy.(cid:24)these rules. In the factored setting, each action has its own decision tree policy. All actions start with the same templatetree but adapt them independently. Whether to start an action is decided by starting at the root node and following apath down the tree, visiting a set of decision nodes D. At each node we either apply a hard-coded branch selection rule,or sample a stochastic branch rule from a parameterised policy for that node. Assuming the conditional independence ofdecisions at each node, the probability of reaching an action leaf l equals the product of branch probabilities at each nodeP[a = l | o; θ n] =P[d(cid:4) | o; θ n,d],(9)d∈Dwhere d represents the current decision node, and drepresents the next node visited in the tree. The probability of a branch(cid:4) | o, θ n,d] functions can be any differentiable function offollowed as a result of a hard-coded rule is 1. The individual P[dthe parameters, such as the linear approximator. Parameter adjustments have the simple effect of pruning parts of the treethat represent poor policies in that region of the state space.(cid:4)For example, nodes A, D, F, H (Fig. 4) represent hard-coded rules that switch deterministically between the Yes and Nobranches based on the truth of the statement in the node, for the current state. Nodes B, C, E, G are parameterised sothat they select branches as a result of learning. In our example, the probability of choosing the left or right branches is asingle parameter logistic function that is independent of the observations. For action n, and decision node C “action durationmatters?” we haveP[Yes | o; θn,C ] = P[Yes; θn,C ] =1exp(θn,C ) + 1.In general the policy pruning could also be a function of the current state. The log derivatives of the ‘Yes’ and ‘No’ decisionsare given by (8), noting that in this case o is a scalar constant 1. The normalised action probability gradient for each nodeis added to the eligibility trace independently. We can do this because the product terms in (9) cancel when taking thegradient of the log of the same expression.If the parameters converge in such a way that prunes Fig. 4 to just the dashed branches we would have the policy: ifthe action IS eligible, and probability of this action success does NOT matter, and the duration of this action DOES matter, and thisaction IS fast, then start, otherwise do not start. Thus we can encode highly expressive policies with only a few parameters(four parameters in the case of Fig. 4). This approach allows extensive use of control knowledge, using FPG simply to switchbetween experts. Even though it is ignored in the example above, state can be taken into account if that is useful.3.2.3. Approximators for non-temporal probabilistic planningWe discus the particular case of non-concurrent non-temporal probabilistic planning (PP) domains. The probabilistictrack of the international planning competition (IPC) was based on this class of domains. In PP, we choose one action fromall currently eligible actions. This is in contrast to choosing a set of actions to contribute to a command. Even though this issomewhat different problem to CPTP, a similar factorisation can be applied.The factored policy still uses a parameterised policy function per action. But, instead of turning the output into a{Yes, No} probability distribution per action, logistic regression is used to compute a single probability distribution overall eligible actions. The probability of choosing n at time t isO. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747P[at = n | ot; θ ] =(cid:17)(cid:10)t θ n)exp(o(cid:10)t θ k)k∈E(ot ) exp(o,where E(ot) is the set of eligible actions. The normalised derivative is∇ P[at = n | ot; θ ]P[at = n | ot; θ ](cid:22)= otU (n) − P[· | ot; θ ](cid:23)(cid:2),735(10)where U (n) is a unit vector with a 1 in element n, and P[· | ot; θ ] is treated as an N dimensional probability vector.Also, the simulateTillHappening() function becomes substantially simpler. We implement the effects of action at ,sample an outcome, and implement its effects as well. Time and the event queue are no longer elements of the state.3.2.4. The relational online policy gradientThe relational online policy gradient (ROPG) is described in [16]. It is not part of the FPG planner, but we show how theoptimisation component of ROPG can be cast as an example of an FPG policy parameterisation.ROPG is a relational reinforcement learning (RRL) approach to solving probabilistic non-Markovian decision processes.RRL computes policies, expressed in a relational language, that work across a range of problems drawn from a domain.The state and observation spaces may vary across problem instances, but higher-order representations can capture thecommonalities in a concise way. The aim is to learn policies from a small number of problems, and then generalise to otherproblems from the same domain. An initial reasoning phase discovers candidate relational control strategies. In the secondphase policy-gradient learning is used to discover when to use each strategy.ROPG uses exactly Algorithm 2 in order to optimise the choice of control strategy for the current observation. ROPGchooses one action for each step, and in that sense it resembles FPGs IPC policy from Section 3.2.3. However, the outputof the parameterised policy is the choice of strategy to evaluate, which is then resolved deterministically to produce agrounded action. In that sense, ROPG more closely resembles a single layer of the tree of experts policy from Section 3.2.2.Assume there are N control strategies to choose from that may or may not be available at the current time step. Theobservations ot provided by ROPG can be thought of as binary vectors giving the eligibility of each strategy according tothe relational representation of the current state. In fact, due to the non-Markovian nature of the domains, the observations,control rules and rewards are over histories instead of only the current state. The ROPG parameterised policy is expressedasP[at = n | ot; θ ] = κ(n, ot)(cid:17)exp(θ n)Nk=1 κ(k, ot ) exp(θ k),where κ(n, ot ) is 1 if element n of ot is non-zero, and 0 otherwise. The use of κ restricts the policy to choose from eligiblestrategies only, and is analogous the use of the function E(ot) in (10).The results shown in [16] demonstrate that this is an effective strategy for automatically evaluating generated controlstrategies.3.3. The policy-gradient algorithmAlgorithm 4 completes our description of FPG by showing how to implement the gradient estimate (4) for planningwith factored action policies (assuming the more complex CPTP case). The algorithm works by repeatedly simulating planexecutions:1. the initial state represents makespan 0 in the plan (not to be confused with the step number t in the algorithm);2. the policies all receive the observation ot of the current state st ;3. each policy representing an eligible action emits a probability of starting;4. each action policy samples ‘Yes’ or ‘No’ and these are issued as a joint command;5. the plan state transition is sampled (see Section 3.1);6. the planner receives the global reward for the new state;7. for OlPomdp all parameters are immediately updated by αrt et , or for parallel planning rt et is averaged over T stepsbefore being passed to an additional line search phase controlled by a master process.Note the link to the planning simulator on line 9. If the simulator indicates that the action is impossible due to a mutexconstraint, the planner successively disables one action in the command (according to an arbitrary lexical ordering) untilthe command is eligible. This mutex elimination process can be considered part of the system’s dynamics. Some actions areautomatically cancelled if they are mutexed with other actions so that it does not influence the policy-gradient algorithmitself. While this seems like a dangerously ad hoc scheme, if the wrong decision is made the long-term average reward willsuffer and the choice of actions that led to the mutex will be discouraged in the future. More studied approaches to dealingwith mutexes may permit faster learning. Examples include coordinated RL [45] that provide efficient methods to includedependencies between action policies.736O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–7471: Set s0 to initial state, t = 0, et = [0], init θ 0 randomly2: repeat3:4:5:6:7:et+1 = βetot = sim.getObservationfor each eligible action an doEvaluate action policy n P[atn = {Yes, No} | o; θ tn]Sample atn = Yes or atn = Noet+1 = et+1 + ∇θ P[atn |o;θ tn ]P[atn |o;θ tn ]8:while (st+1 = simulateTillHappening(st , ct )) == MUTEX doarbitrarily disable action in ct due to mutex9:10:11:12:13:14:15: until stoppingCriterion((cid:3), . . .)rt = sim.getReward()θ t+1 = θ t + αrt et+1if st+1.isTerminalState then st+1 = s0t ← t + 1Algorithm 4. FPG based on OlPomdp.Line 8 computes the normalised gradient of the sampled action probability and adds the gradient for the nth action’sparameters into the eligibility trace. Because planning is inherently episodic we could alternatively set β = 1 and resetet every time a terminal state is encountered. However, empirically, setting β = 0.95 performed better than resetting et ,probably due to the variance reduction associated with a lower β.The gradient for parameters not relating to action n is 0. We do not compute P[atn | ot; θ n] or gradients for actions withunsatisfied preconditions. If no actions are chosen to begin, we issue a no-op action. If the event queue is not empty, weprocess up to the next happening, otherwise time is simply incremented by 1 to ensure all possible policies will eventuallyreach a maximum makespan and hence a reset state.3.3.1. The Q -learning variantFPG’s approach raises the question of whether a value-based reinforcement learning algorithm with a factored approxi-mator could work just as well. FPG’s factorisation is easy to extend in the non-temporal non-concurrent case. The result isthe Factored Q -learning planner (FQL), which is identical to FPG except that:• the policy-gradient algorithm is replaced by standard Q (λ)-learning algorithm;• each linear approximator has to learn the Q -value associated with its action.Contrary to FPG, there is no simple way of trading off exploration and exploitation. Our choice is, while learning, to use an(cid:3)-greedy policy with (cid:3) following a sigmoid function going from 0.95 down to 0.05 during the allocated learning period.We also explored various settings for λ and the learning rate. FQL also has no convergence guarantees. Even with λ = 0,Q -learning with linear function approximation may diverge under some exploration strategies [46].3.4. Implementation detailsThis section is devoted to some of the engineering details needed to achieve good performance with FPG. Unless specifiedthese details are for FPG for CPTP and FPG-ipc for PP.3.4.1. Grounding actions and variablesBecause PPDDL uses first-order constructs, a preliminary step when creating the function approximators is to ground thedomain. That is, we build:• a set of relevant actions Ar , i.e., actions which are reachable from the initial state, and• a set of relevant predicates P r , i.e., predicates whose value can change because of relevant actions.Exact reachability analysis to determine precise sets Ar and P r is expensive. Instead, a simple iterative process is used tocompute supersets A and P by following a relaxed reachability analysis.The relaxation is similar to that employed by many planners [47]. It relies on the fact that PDDL is based on the notion ofatoms rather than variables (predicates). States are described by a list of currently true/positive atoms. Two sets of groundedactions are used but initially empty: the set of new eligible actions An and the set of processed actions A p . The reachabilityanalysis starts by putting all initially true atoms in a set P , then, as shown in Algorithm 5, it alternates between:• adding new eligible actions to An based on atoms in P , using positive preconditions only to determine if an action iseligible; and• searching for new atoms to add to P based on actions in An (which are moved to A p once processed), using onlypositive effects.O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–7477371: Initialisation:2: An ← ∅ /* {new eligible actions} */3: A p ← ∅ /* {processed eligible actions} */4: P ← atoms in s05: repeatAn ← An6:Pick a ∈ An7:An ← An\{a}8:P ← PatomsInEffects(a)9:(cid:25)A p ← A p10:11: until An = ∅eligibleActions(P )\ A p{a}(cid:25)(cid:25)Algorithm 5. ground().The process stops when An is empty.3.4.2. Progress estimatorIn FPG-ipc, the reward function is shaped with a simple progress estimator. The progress estimator counts how manyof the goal’s facts have been added or deleted during the last transition. This possibly negative number fis multiplied bya coefficient ρ to compute the additional reward rprogress. Provided the net progress reward per episode is 0, ensured byadjusting the final goal state reward, this shaping does not alter the overall objective function [35].This approach works well because goals are often specified as a conjunction of facts to satisfy. It could be improved totackle more complex goals involving metric comparisons, disjunctions, and so on.3.4.3. Saving computation time when rewards are rareDomains of the probabilistic track of IPC-5 only reward reaching the goal. So most of the time rt = 0, particularly if noprogress estimator is used. This makes it possible to avoid or delay various computations in OlPomdp as follows:• update the parameter vector θ only when rt (cid:12)= 0;• while an action a remains ineligible and r = 0, do not discount (with β) the corresponding part of the eligibility vectorea but increment a counter #a;• when an action a is eligible or r = 0, discount ea by β #a before performing the normal update, and reset #a to zero.This process is not compatible with the use of a baseline reward—i.e., subtracting a running average r of the reward to theinstant reward rt —commonly used in RL to reduce the variance of gradient estimates. However, we gain more from theadditional simulated plan executions that can be generated with the saved computation time.(cid:25)3.4.4. Saving computation time when few actions are usedSection 3.4.1 covers creating a superset of the relevant actions. Let us denote this set as A = ArAi , where r denotesthe relevant subset, and i the irrelevant subset. It is useless to perform computations for actions in Ai : they will never beeligible and their eligibility traces will remain null. But we do not know in advance which actions belong to Ai .In the same vein, one can observe that an action’s eligibility vector ea remains null during an episode as long as it hasnot been eligible. During an episode, many actions will remain ineligible for a long time. So many computations can beavoided by just storing the actions that have been eligible at least once. Irrelevant actions will never become eligible. Theresult is:• when restarting from the initial state, set hasBeenEligible(a) to false for all a ∈ A;• when action a is eligible, set hasBeenEligible(a) to true;• until a becomes eligible once, do not update the corresponding part of the eligibility vector (ea), and parameter vector(θ a): both remain 0.We have yet to exploit the same strategy to reduce memory usage. If one suspects that there are many irrelevant actionsin A, we could allocate memory for an action’s parameter vector and eligibility vector only when the action is encounteredfor the first time.3.4.5. SoftwareThe reinforcement learning routines were provided by LibPG, a C++ library written by the authors, and used in multipleresearch projects. It makes use of the Boost C++ libraries for matrix operations, and optionally the ATLAS basic linear algebraroutines library for faster mathematics.In the CPTP case Algorithm 3 was implemented from scratch in C++ for the class of domains described in 2.1. As dis-cussed, XML was used to describe the domains and problems, writing PPDDL to XML translations where necessary. Thisdecision was made because of the ease of parsing and transforming XML across a range of programming languages. In thePP case we tied LibPG to the MDPSim package, which is the official simulator for the IPC.738O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747Table 1Main parameter settings of FPG-ipc as used during IPC-5 and of FQL. Other TD discount factors have been experimented (λ = 0.8 and λ = 1) but withnear-identical results.FPGFQLstep sizeα = 0.00005α = 0.001discount factorβ = 0.85discount = 0.85λN/Aλ = 0success rewardrs = 1000rs = 1000progress rewardrprogress = 100rprogress = 100Table 2Summary of non-temporal domain results on IPC-5 benchmarks. Values are % of plan simulations that reach the goal (out of 30 runs). A dash indicates theplanner was not run, or failed to produce results.DomainBWExploding BWTireZenoDriveElevatorPitchcatchScheduleRandom4. ExperimentsFOALPsfDPFPGParagraphFF-replan1002482––100–––2931–7–––––634375276376235465–319179––1586528210071935451100FQL0711711105410All the domains and source code for the following experiments are available from http://fpg.loria.fr/.4.1. Probabilistic planningBecause the probabilistic planning setting of the International Planning Competition is simpler and widely known, wefirst present experiments performed with FPG-ipc. In this section, we refer to FPG-ipc simply as FPG, since no confusion ispossible.4.1.1. IPCOne benefit of the IPC is to offer a variety of benchmarks recognised by the AI planning community. This was the mainreason for developing FPG-ipc. Many of the approximations made in FPG were designed to cope with a combinatorial actionspace, thus there was little reason to believe FPG would be competitive in non-temporal domains.In the competition there was a time limit of 40 minutes for each problem instance including optimisation and evaluation.Table 2 shows the overall summary of results from the IPC-5, by domain and planner. These results include FF-replan andFQL which were run after the competition but following similar rules. FQL had a slight advantage because it was runon a more powerful machine that the competition server (Core2 DUO 3.2 GHz), and we only limited the optimisationtime (10 minutes), not the evaluation time. FPG-ipc’s and FQL’s parameters were manually tuned to give the best resultsacross various domains from IPC-4 and from preliminary benchmarks of IPC-5, however they were not further tuned to thecompetition problems. These parameters are given in Table 1.The results are based on 9 PPDDL specified domains, averaged over 15 instances from each domain, and tested on 30simulations of plans for each instance. This is too few simulations to reliably measure the performance of a planner, butcompetition time constraints required a small number. Many of these domains, such as Blocksworld (BW), are classicaldeterministic domains with noise added to the effects.FPG and FF-replan prove to be much more robust to a variety of domains than the other planners. They have been ableto run on all problem instances and return results on most of them. Software maturity explains some of this performance:FPG and FF-replan are both largely based on stable software (FPG←MDPSim + LibPG, FF-replan←FF), whereas two of thethree other planners were not bug-free at the time of the competition. On some domains Paragraph and FOALP—which areoptimal planners, unlike FPG and FF-replan—outperformed other planners on most problem instances. We defer to [48] fordetails.FQL turns out to be competitive only on some problems of the Drive, Random, Schedule and Tireworld domains.9 Wesurmise that FPG is more successful than FQL because FQL aims at approximating the Q -value of each state-action pairswhile it is generally sufficient for FPG to simply return a good action in each state (policies being often deterministic). Thisis consistent with experiments with the Tetris game which show that learning a ranking function is more efficient thanlearning a value function [49]. We note that this is empirical evidence only. There are situations in which the additionalinformation encoded in values might be advantageous to learning, allowing faster convergence than PG methods that are9 Problems Ex-blocksworld(p03) and Zenoworld(p01) are trivial since the initial state is a goal state.O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747739Table 3FPG’s (in)stability: mean and standard deviation of the success rate over N = 30 runs on various problems (α = 0.0001).DomainBlocksworld p10Zenoworld p05Schedule p09Mean ± StdDev67.0 ± 41.939.7 ± 15.044.9 ± 4.46Min02643Max1007051Note6/10 policies found goal > 90%known for their high variance. We have not yet come across such a domain in the planning context. The Schedule domainis one where FQL and FPG perform equally well on each problem instance.FQL is more difficult to tune than FPG because the exploration policy has to be set explicitly and a method found todecay exploration at the correct rate, which is tied to the step size used and the value of λ. For FQL we tried λ = {0, 0.8, 1}.The variance in the final results was not significant. We quote the results for λ = 0, the standard version of Q-learning. It isprobable that further tuning would result in significant improvements to FQL’s results. FPG also requires tuning a step sizeand discount factor. However, this process is simpler than FQL’s, amounting to tuning the discount factor for a small stepsize, and then increasing the step size until convergence becomes unstable on some domains.4.1.2. InstabilitiesBecause it is a randomised algorithm, FPG will find different results each time it is run with a different random seed.They may for instance:• fall in different local optima;• not reach a locally optimal policy because the learning had insufficient time to converge; or• diverge due to the use of an inappropriately large step size.In some domains of the IPC (Blocksworld, Zenotravel, Elevators, and to a lesser extent, Random and Exploding-Blocksworld)there is a clear divide between “easy” and “hard” problem instances. FPG achieves a 100% success rate on easy ones andnear 0% on hard ones. Table 3 presents experimental results on some problem instances identified as leading to FPG beingunstable. It gives, for each problem, the average success rate (±standard deviation), and the minimal and maximal successrates obtained on 10 repeats of the optimisation and evaluation cycle.We could implement automatic random restarts to get more robust results on such problems. Yet, this solution wasavoided due to the time constraints of the competition. On easy problems restarts find very similar solutions, and for veryhard problems all restarts fail. It was a more efficient strategy to make a single complete gradient ascent for each problemand move to the next problem in the case of failure.4.1.3. Validating FPG’s speed up tricksSection 3.4.3 presented two ideas for avoiding useless computations. To illustrate their benefit FPG was run several timeson the same problem. The same random seed was used each time so that the learning process was identical. Optimisationwas limited to 15 minutes but with different combinations of speed-up tricks. Fig. 5 shows in each case the computationtime as a function of the number of simulation steps since the start of optimisation.For an example of how many grounded actions might become eligible, we calculated some statistics for IPC-5Blocksworld p07:• between 120 and 200 actions are eligible at any single point in time during an episode (with a max episode length of750); and• 1770 out of 2310 actions turn out to be eligible at some point during the episode.4.1.4. Benefits of the progress estimatorThe efficiency of the progress estimator is illustrated with IPC-5 Blocksworld problem p07. Fig. 6 shows learning curves ofFPG with four different weights ρ for the progress estimator. While the reward in case of success is always set to rs = 1000,the progress reward is the number of goal facts added or subtracted multiplied by ρ = {0, 1, 10, 100}. Fig. 6(a) shows theaverage number of goals reached per simulation step. Fig. 6(b) shows the average reward per simulation step, that is, R(θ ).All curves are averages over 10 runs, with error bars showing the standard deviation.This problem is hard enough that a progress estimator is necessary in order for FPG to initially find the goal, and thenbootstrap into a good policy. The learning time is limited to 15 minutes. Because uniformly scaling rewards is equivalent toscaling the step-size bigger r p values lead to faster learning. But the resulting policy after 900 seconds is better with r p = 10than r p = 100 because the latter favours short-term rewards linked to progresses rather than long-term and non-guaranteedrewards linked to goal states. Plus, the error bars show a higher standard deviation when r p = 100, which is due to FPGlearning too fast.We present figures with different x-axes on purpose to show that the number of simulation steps performed in 900 sec-onds depends on the setting. The fastest setting is with r p = 0 because never receiving a reward is a good way to avoidexcessive computations.740O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747Fig. 5. Speed-up gained from avoiding useless parameter updates. +r and -r indicate whether the speed-up based on observing the reward was used ornot. +e and -e indicate whether the speed-up relying on whether an action has been eligible or not.4.1.5. When FF-replan failsA second set of domains (one instance each) demonstrates domains introduced by [30] that are more challenging forreplanners. They are characterised by short plans having a high failure probability. Deterministic planners like FF often lookfor the shortest path to the goal, without the ability to avoid dangerous paths. Replanners may also fail to optimise the cost-to-go, which was not measured in IPC-5. Results are shown in Table 4. The optimal Paragraph planner does very well untilthe problem size becomes too large. Triangle-tire-4 in particular shows a threshold for this domain where an approximateprobabilistic planning approach is required in order to find a good policy.To summarise the probabilistic planning results, FPG appears to be a good compromise between the scalability of re-planning approaches, and the capacity of optimal probabilistic planner to perform reasoning about uncertainty. Note thatwe shall describe FPG’s limitations after describing its performance on CPTP domains.4.2. CPTPThese experiments compare FPG to two earlier probabilistic temporal planners: Prottle [4], and a Military Operations(MO) planner [23]. The MO planner uses LRTDP, and Prottle uses a hybrid of AO* and LRTDP. They both require storage ofstate values but attempt to prune off large branches of the state space. The Prottle planner has the advantage of using goodheuristics to prune the state space. The MO planner did not use heuristics.We present results along three criteria: the probability of reaching a goal state, the average makespan (including ex-ecutions that end in failure), and the long-term average reward (FPG only). However, each planner uses subtly differentoptimisation criteria:• FPG — maximises the average reward per step R = 1000 (1−Pr(fail))stepsin a plan execution, which is related to the makespan;, where steps is the average number of decision points• Prottle — minimises the probability of failure;• MO — minimises the cost-per-trial, here based on a weighted combination of P(failure), makespan, and resource con-sumption.It may not be clear why FPG’s minimisation of steps also helps to minimise makespan. Steps occur at decision points. Theway to minimise decision points is to start as many actions as possible in a single command, maximising concurrency. It isalso very easy to adapt the reward function to emphasise other criteria such as resource consumption.The first three domains are:Probabilistic Machine Shop (MS) [3]: Multiple sub-actions such as shape, paint, and polish need to be performed ondifferent objects using different machines, possibly in parallel. Not all machines are capable of every action and they cannotwork on the same object concurrently. Objects need to be transported from one machine to another for different sub-actions. The version we used was based on Mach6, the largest variant used in [3], but was subsequently modified exactlyO. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747741(a) Number of successes per simulation step (as a function of wall clock time).Fig. 6. FPG’s learning curve on Blocksworld problem p07 for several settings of the progress estimator (the success reward is always rs = 1000).(b) Average reward per simulation step (as a function of simulation time).Table 4Summary of results on non-replanner-friendly domains. Values are % of plan simulations that reach the goal (minimum 30 runs). A dash indicates theplanner was run, but failed to produce results, typically due to memory constraints. A starred result indicates a theoretical upper limit for FF-replan that itfailed to reach in these experiments.DomainClimberBus fareTri-tire 1Tri-tire 2Tri-tire 3Tri-tire 4Paragraph1001001001001003FPG10022100929168FF-replan62150∗13∗3∗0.8Prottle10010––––as in the original Prottle experiments.10 It has 9 durative actions and 13 predicates that expand to 38 grounded actions and28 grounded predicates. The maximum makespan is 20 (Prottle used 15).Maze (MZ) [4]: Maze is based on the idea of moving between connected rooms and finding the keys necessary to unlockclosed doors. There are doors and keys of different colours, and it is possible to try unlocking several doors at the same time10 Some actions have nested probabilistic events and outcome-dependant durations.742O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747Table 5Results on 3 benchmark domains. The experiments for MO and FPG were repeated 100 times. Success% = percentage of successful executions, MS =makespan, R is the final long-term average reward, and Time is the optimisation time in seconds.FPG-L = FPG + linear network. FPG-T = FPG + tree of experts.ProblemAlgorithmSuccess%MSMSMSMSMSMSMSMSMSMZMZMZMZMZMZMZMZMZTPTPTPTPTPTPTPTPPitStopPitStopPitStop500500500FPG-LFPG-LFPG-TFPG-TProttleMOrandomnaïveFPG-LFPG-LFPG-TFPG-TProttleMOMOrandomnaïveFPG-LFPG-LFPG-TFPG-TProttleMOrandomnaïveFPG-LrandomnaïveFPG-Trandomnaïve98.699.930.035.097.10.70.019.185.380.384.782.292.192.823.59.234.466.765.666.720.20.40.0100.071.00.097.523.430.5Out of memoryOut of memory6.65.5131318205.56.95.55.78.08.21316181818181519201801264966776158765736R11816620.921.40.10.013413013611516.48.62983053023011.00.014241.00.01.560.2310.100Time5326004396002723714402917107172340600258181442413345(or grab keys). Actions all have a duration of 1 or 2, and many have nested probabilistic outcomes. There are 6 durativeactions and 7 predicates that expand to 165 grounded actions and 207 grounded predicates. Plans fail if the makespanreaches 20 units (Prottle used 10). Despite the large number of actions, this problem is easy to solve compared to MachineShop.Teleport (TP) [4]: Objects can teleport between locations, if their objects have been successfully ‘linked’ to their destina-tion. There are ‘fast’ and ‘slow’ forms of teleporting, but ‘slow’ has a higher probability of success. There is a fixed numberof links, each with a fixed source location. One can change the destination of multiple links at the same time, and also tryto teleport oneself along a stable link. The problem has actions with outcome-dependent durations. It has 3 durative actionsand 3 predicates that expand to 63 grounded actions and 24 grounded predicates. Plans fail if the makespan reaches 25units (Prottle used 20).We additionally introduce two novel domains to illustrate particular strengths of FPG.PitStop: A proof-of-concept continuous duration uncertainty domain representing alternative pit stop strategies in a carrace, a 2-stop strategy versus a 3-stop. For each strategy a pit-stop and a racing action are defined. The 3-stop strategyhas shorter racing and pitting time, but the pit stop only injects 20 laps worth of fuel. The 2-stop strategy has longer pittimes, but injects 30 laps worth of fuel. The goal is to complete 80 laps. The pit-stop actions are modelled with Gaussiandurations. The racing actions take a fixed minimum time but there are two discrete outcomes (with probability 0.5 each):a clear track adds an exponentially distributed delay, or encountering backmarkers adds a normally distributed delay. Thusthis domain includes continuous durations, discrete outcomes, and metric functions (fuel counter and lap counters).500: To provide a demonstration of scalability and parallelisation we generated a 500 grounded actions, 250 predicatesdomain as follows: the goal state required 18 predicates to be made true. Each action has two outcomes, with up to 6effects and a 10% chance of each effect being negative. Two independent sequences of actions are generated that potentiallylead to the goal state with makespan of less than 1000 (but there may be many more than two possible routes the goal).There are 40 types of resource, with 200 units each. Each action requires a maximum of 10 units from 5 types, potentiallyconsuming half of the occupied resources permanently. Resources limit how many actions can start.Our experiments used a combination of: (1) FPG with the linear network (FPG-L) action policies; (2) FPG with the tree(FPG-T) action policy shown in Fig. 4; (3) the MO planner; (4) Prottle; (5) a random policy that starts eligible actions witha coin toss; (6) a naïve policy that attempts to start all eligible actions in each command.O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747743Fig. 7. Three decision-tree action sub-policies extracted the final Maze policy. Returning ’Yes’ means start the action.Fig. 8. Relative convergence of long-term average reward R, failure probability, and makespan over a single linear network FPG optimisation of MachineShop. The y-axis has a common scale for all three units.All of these experiments were limited to 10 minutes. Other parameters are described in Table 7. In particular, the singlegradient step size α was selected as the single highest value that ensured reliable convergence over 100 runs over alldomains. Experiments in this section were conducted on a dedicated 2.4 GHz Pentium IV processor with 1 GB of ram.The results are summarised in Table 5. Reported success percentage and makespan was estimated from 10,000 simulatedexecutions of the optimised plan. Prottle results were taken directly from [4], quoting the highest probability of successresult. FPG and MO optimisations/evaluations were repeated 100 times to investigate the impact of local minima. The FPGand MO results show the mean result over 100 runs, and the unbarred results show the single best run out of the 100,measured by probability of success. The small differences between the mean and best results indicate that local minimawere not severe.The random and naïve experiments are designed to demonstrate that optimisation is necessary to get good results. Ingeneral, Table 5 shows that FPG is at least competitive with Prottle and the MO planner. FPG achieves the best performancein Machine Shop. The poor performance of Prottle in the Teleport domain—20.2% success compared to FPG’s 65.6%—is dueto Prottle’s short maximum permitted makespan of 20 time units. At least 25 units are required to achieve a higher successprobability. We also observe that FPG’s linear action policy generally performs slightly better than the tree, but takes longerto optimise. This is expected given that the linear action-policy can represent a much richer class of policies at the expenseof more parameters. In fact, it is surprising that the decision tree does so well on all domains except Machine Shop, whereit only reduces the success rate to 30% compared to the 99% the linear policy achieves.We explored the types of policy that the decision tree structure in Fig. 4 produces. The pruned decision tree policy forthree grounded Maze actions is shown in Fig. 7. The first action is pruned such that it never runs. The third action alwaysruns when it is eligible to do so. The second action is more interesting: FPG has decided this is a useful action if there issome predicate it can set faster than any other eligible action. Over the 165 Maze actions, the majority had been optimisedto never start, or always start, or had not been optimised at all. The latter case indicates that the actions were never partof the active plan for long. The good performance of the simplistic tree-policy indicates that for our test domains—andpossibly many others—a large part of the planning effort is deciding which grounded actions are useful in the final plan.Machine shop exhibited a significant difference between the linear and decision tree action policy, indicating that it is amore complex domain. FPG with a simple decision tree like this could be used to generate control knowledge for reasoningbased planners. For example, a pre-processing stage FPG could determine which actions can be immediately discarded.744O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747Table 6FPG’s results when optimisation is terminated at 75% of the mean R achieved in Table 5.ProblemAlgorithmSuccess%MSMSMSMSMZMZMZMZTPTPTPTPFPG-LFPG-LFPG-TFPG-TFPG-LFPG-LFPG-TFPG-TFPG-LFPG-LFPG-TFPG-T95.399.929.634.180.785.380.384.265.467.065.366.9MS7.16.513145.55.65.57.018191818R89891616100100102102224224226226Time373210141322223.542.31Fig. 9. Convergence times for the 500 action experiment run in parallel mode on a varying number of processors. The numbers on the curve show totalaggregated simulation steps taken.Table 5 shows that Prottle achieves good results faster on Maze and Machine Shop. The apparently faster Prottle op-timisation is due to the asymptotic convergence of FPG using the criterion optimise until the long-term average reward failsto increase for 5 R(θ ) estimations of 10,000 steps each. In reality, good policies are achieved long before convergence to thiscriterion. To demonstrate this we plotted the progression of a single optimisation run of FPG-L on the Machine Shop domainin Fig. 8. The failure probability and makespan settle near their final values at a reward of approximately R = 85, however,the mean long-term average reward obtainable for this domain is R = 118. In other words, the tail end of FPG optimisationis removing unnecessary no-ops. To further demonstrate this, and the any-time nature of FPG, optimisation was stoppedat 75% (chosen arbitrarily) of the average reward obtained with the stopping criterion used for Table 5. The new results inTable 6 show a reduction in optimisation times by orders of magnitude, with very little drop in the performance of the finalpolicies.The experimental results for the continuous time PitStop domain show FPG’s ability to optimise under mixtures ofdiscrete and continuous uncertainties.Results for the 500 action domain are shown for running the parallel version of FPG algorithm with 16 processors. Noother CPTP planner we know of is capable of running domains on this scale. As expected, we observed that optimisationtimes dropped inversely proportional to the number of processors for up to 16 processors. This is shown in Fig. 9. However,on a single processor the parallel version requires double the time of OlPomdp. This is due to the less efficient use ofstep-by-step gradients, and somewhat due to the communication overheads of parallelisation. As the number of processorsincreases, we would observe these overheads grow for a fixed problem size, until the point where adding processors woulddecrease performance.O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747745Table 7Parameter settings not discussed in the text.Paramθααβ(cid:3)(cid:3)TTVal.−5−501 × 105 × 100.9510.0 to 0.66 × 1061 × 106Table 8Success probability on the XOR problem.DomainXORFPG-linear(cid:13)74 ± 5%, 1.0(cid:14)4.3. When FPG failsOpt.All FPGFPG-LFPG-TFPGMOProttleParallel FPG-TParallel FPG-TFPG-MLP(cid:13)75 ± 5%, 0.72(cid:14)(cid:13)100%, 0.28(cid:14)NotesInitial θBoth L&TLRTDP ParamProttle ParamFor search dir.For line searchFF-replan(cid:13)100%, 1.0(cid:14)As with any reinforcement learning algorithm relying on function approximation, FPG may fail if the function approxi-mator is not capable of representing a good policy. This is easy to demonstrate with FPG-ipc using the XOR problem definedby:• x and y are randomly initialised boolean predicates;• action a leads to plan success if and only if x (cid:12)= y (otherwise the plan fails); and• action b leads to plan success if and only if ¬(x (cid:12)= y) (otherwise the plan fails).Using linear function approximators, the best policies always opt for one action in 1 out of 4 of the possible states,and the other action in the remaining states. This results in a 75% probability of success. But if we make the functionapproximation richer by adding a hidden layer—a standard multi-layer perceptron (MLP)—with two squashed hidden unitsthen we can represent a policy that achieves 100% success.Experiments have been conducted for FPG-linear, FPG-MLP and FF-replan. We observed in this problem that FPG con-verges to various local optima. When hidden layers are used the second layer parameters must be initialised randomly.This is why FPG-linear and FPG-MLP have been run 100 times each on this problem, and the resulting scores have beenautomatically clustered to identify local optima (the number of clusters was initialised to 2).Table 8 lists identified clusters for each algorithm. A cluster is described by a tuple (cid:13)mean ± standard deviation, weight(cid:14).As expected, FF-replan always finds the goal. FPG-linear happens to give stable results confirming the theoretical valueof 75% success. This 75% value policy is also, unsurprisingly, a local optimum for FPG-MLP, which reaches 100% successprobability after only 28% of the optimisations.This toy problem not only illustrates the fact that a linear network may not be able to represent an optimal solution, butalso exhibits a situation where the gradient ascent tends to fall into local optima. Greater tuning of the number of hiddenunits and step size would probably overcome this problem, but such tuning would also likely be highly domain specific,which is undesirable for automated planning. Another solution is enriching the observation space, perhaps with the classicalstrategy used when such complex preconditions or conditional-effects appear in a planning problem: duplicate each actionso that each copy has its own simple preconditions [50].On the other hand we conducted many experiments with MLPs on the IPC problems, and never achieved better perfor-mance with them. We suspect that with appropriate encoding of the observation, a linear approximator will produce goodpolicies in most domains.There are also domains for which, even if FPG could easily represent the optimal policy, it is too difficult to learn. As westudied in Section 4.1.4, the progress estimator is crucial for achieving good results in domains such as Blocksworld. How-ever, the larger blocks world domains still proved too difficult for FPG. This hints at domains that are generally challengingfor FPG. They are characterised by requiring long chains of correct actions to obtain rewards. A random policy essentiallywalks around the state space for a long time without achieving a reward. In these cases the failure mode of FPG is thatit will optimise forever without achieving a gain in long-term reward. Essentially it stays perpetually in a flat region ofgradient space. However, such domains are challenging for many planners, and are a great motivation for reasoning aboutthe structure that exists in domains such as Blocksworld.746O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–7475. Discussion and future workFPG diverges from traditional planning approaches in two key ways: we search for plans directly, using local optimisationprocedures; and we approximate the policy representation with a factored collection of parameterised policies. Apart fromreducing the representation complexity, this approach allows policies to generalise to states not encountered during training.This is an important feature of FPG’s “learning” approach. We obtain a similar effect by restricting the input of the policiesto only part of the state. That is, we concentrate on the predicate values. If an action a is useful given observation o, it maybe generally useful for similar observations and therefore similar states.FPG scales in the sense that its memory use and per step computation times grow linearly with the domain. However,the total number of gradient steps needed for convergence is a function of the mixing time of the underlying POMDP, whichcan grow exponentially with the number of state variables. How to compute the mixing time of an arbitrary MDP is anopen problem, which in turn hints at the difficulty of assessing in advance the hardness of an arbitrary planning problem.In recent years, policy-gradient algorithms have been developed that make a better use of the samples they get byemploying approximate second order gradient ascent, and/or the use of critic value estimates [51,52]. They are of particularinterest when obtaining samples has a high cost. However, these algorithms have a time complexity cost of at least O (k2)per gradient step, where k is the number of parameters plus observation dimensions. In our setting, samples are obtainedusing a fast simulator. Using the available time to generate more samples was better than performing O(10002) matrixoperations for every step taken by the simulator.It is strange to use the planning model information only to build a plan execution simulator. We did this to avoidintractable computations on large models. However, we believe that a hybrid of dynamic programming/RL algorithms ishighly desirable to achieve the best of both approaches. This might be achieved through a Rao-Blackwellisation of thegradient estimates, reducing their variance by using the known per-step transition probabilities.We also believe that fast and efficient methods from non-probabilistic planning can be used to bootstrap probabilisticplanners. This can be done in several ways, including replanning. One method we have experimented with uses FF-replanto suggest actions to help FPG reach the goal in domains where random actions tend not to reach the goal [53]. Over time,FPG takes over from FF-replan in the choice of actions, optimising for the probabilistic structure.6. ConclusionTo conclude, FPG is a demonstration that Monte-Carlo local optimisation methods can supplement AI planning methods.This is particularly true in domains that involve large or infinite state spaces, uncertainty, and continuous quantities such astime. Ultimately, we believe that hybrid planning/learning approaches will become the state-of-the art for complex domains.AcknowledgementsThis work was supported by National ICT Australia, funded by the Australian Government’s Backing Australia’s Abilityprogram and the Centre of Excellence program. This project was also funded by the Australian Defence Science and Tech-nology Organisation. Thank you to Sylvie Thiébaux and Iain Little for many helpful insights. We also wish to thank theorganisers of the IPC-5 probabilistic track for opportunity to test FPG and for the subsequent feedback.References[1] A. Barto, S. Bradtke, S. Singh, Learning to act using real-time dynamic programming, Artificial Intelligence 72 (1995) 81–138.∗[2] E. Hansen, S. Zilberstein, LAO[3] Mausam, D.S. Weld, Concurrent probabilistic temporal planning, in: Proceedings of the Fifteenth International Conference on Automated Planning and: A heuristic search algorithm that finds solutions with loops, Artificial Intelligence 129 (2001) 35–62.Scheduling (ICAPS’05), Monterey, CA, 2005.[4] I. Little, D. Aberdeen, S. Thiébaux, Prottle: A probabilistic temporal planner, in: Proceedings of the Twentieth American National Conference on ArtificialIntelligence (AAAI’05), 2005.[5] D.P. Bertsekas, J.N. Tsitsiklis, Neuro-Dynamic Programming, Athena Scientific, 1996.[6] R.S. Sutton, A.G. Barto, Reinforcement Learning: An Introduction, MIT Press, Cambridge MA, ISBN 0-262-19398-1, 1998.[7] A. Fern, S. Yoon, R. Givan, Approximate policy iteration with a policy language bias: Solving relational Markov decision processes, Journal of ArtificialIntelligence Research 25 (2006) 85–118.[8] R.J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning, Machine Learning 8 (1992) 229–256.[9] R.S. Sutton, D. McAllester, S. Singh, Y. Mansour, Policy gradient methods for reinforcement learning with function approximation, in: Advances inNeural Information Processing Systems (NIPS’99), vol. 12, MIT Press, 2000, pp. 1057–1063.[10] J. Baxter, P. Bartlett, L. Weaver, Experiments with infinite-horizon, policy-gradient estimation, Journal of Artificial Intelligence Research 15 (2001)351–381.[11] H. Kimura, K. Miyazaki, S. Kobayashi, Reinforcement learning in POMDPs with function approximation, in: Proceedings of the Fourteenth InternationalConference on Machine Learning (ICML’97), Morgan Kaufmann, 1997, pp. 152–160.[12] M. Fox, D. Long, PDDL2.1: An extension to PDDL for expressing temporal planning domains, Journal of Artificial Intelligence Research 20 (2003) 61–124.[13] L. Peshkin, K.-E. Kim, N. Meuleau, L.P. Kaelbling, Learning to cooperate via policy search, in: Proceedings of the Sixteenth Conference on Uncertainty inArtificial Intelligence (UAI’00), 2000.[14] N. Tao, J. Baxter, L. Weaver, A multi-agent, policy-gradient approach to network routing, in: Proceedings of the Eighteenth International Conference onMachine Learning (ICML’01), Morgan Kaufmann, 2001.[15] M. Ai-Chang, J. Bresina, L. Charest, A. Chase, J.C. jung Hsu, A. Jonsson, B. Kanefsky, P. Morris, K. Rajan, J. Yglesias, B. Chafin, W. Dias, P.F. Maldague,MAPGEN: Mixed-initiative planning and scheduling for the Mars exploration rover mission, IEEE Intelligent Systems 19 (1) (2004) 8–12.O. Buffet, D. Aberdeen / Artificial Intelligence 173 (2009) 722–747747[16] C. Gretton, Gradient-based relational reinforcement-learning of temporally extended policies, in: Proceedings of the Seventeenth International Confer-ence on Automated Planning and Scheduling (ICAPS’07), 2007.[17] H.L.S. Younes, M.L. Littman, PPDDL1.0: An extension to PDDL for expressing planning domains with probabilistic effects, Tech. Rep. CMU-CS-04-167,Carnegie Mellon University, October 2004.[18] H.L.S. Younes, Extending PDDL to model stochastic decision processes, in: Proceedings of the ICAPS’03 Workshop on PDDL, 2003.[19] W. Cushing, S. Kambhampati, Mausam, D. Weld, When is temporal planning really temporal? in: Proceedings of the International Joint Conference onArtificial Intelligence (IJCAI’07), Hyderabad, India, 2007.[20] Mausam, P. Bertoli, D. Weld, Planning with durative actions in stochastic domains, Journal of Artificial Intelligence Research.[21] B. Bonet, B. Givan, Results of probabilistic track in the 5th international planning competition, in: Not in the Proceedings of the Fifth InternationalPlanning Competition (IPC-5), 2006.[22] Mausam, D.S. Weld, Probabilistic temporal planning with uncertain durations, in: Proceedings of the Sixteenth International Conference on AutomatedPlanning and Scheduling (ICAPS’06), 2006.[23] D. Aberdeen, S. Thiébaux, L. Zhang, Decision-theoretic military operations planning, in: Proceedings of the Fourteenth International Conference onAutomated Planning and Scheduling (ICAPS’04), 2004, pp. 402–411.[24] H.L.S. Younes, R.G. Simmons, Policy generation for continuous-time stochastic domains with concurrency, in: Proceedings of the Fourteenth Interna-tional Conference on Automated Planning and Scheduling (ICAPS’04), 2004.[25] S. Sanner, C. Boutilier, Practical linear value-approximation techniques for first-order MDPs, in: Proceedings of the Twenty-Second Conference onUncertainty in Artificial Intelligence (UAI’06), 2006.[26] I. Little, S. Thiébaux, Concurrent probabilistic planning in the graphplan framework, in: Proceedings of the Sixteenth International Conference onAutomated Planning and Scheduling (ICAPS’06), 2006.[27] P. Fabiani, F. Teichteil-Königsbuch, Symbolic focused dynamic programming for planning under uncertainty, in: Proceedings of the IJCAI’05 Workshopon Reasoning with Uncertainty in Robotics (RUR’05), 2005.[28] S. Yoon, A. Fern, B. Givan, FF-Replan: a baseline for probabilistic planning, in: Proceedings of the Seventeenth International Conference on AutomatedPlanning and Scheduling (ICAPS’07), 2007.[29] J. Hoffmann, B. Nebel, The FF planning system: Fast plan generation through heuristic search, Journal of Artificial Intelligence Research 14 (2001)253–302.[30] I. Little, S. Thiébaux, Probabilistic planning vs replanning, in: Proceedings of the ICAPS’07 Workshop on the International Planning Competition: Past,Present and Future, 2007.[31] D. Aberdeen, Policy-gradient methods for planning, in: Advances in Neural Information Processing Systems (NIPS’05), vol. 18, MIT Press, 2006.[32] D. Aberdeen, O. Buffet, Concurrent probabilistic temporal planning with policy-gradients, in: Proceedings of the Seventeenth International Conferenceon Automated Planning and Scheduling (ICAPS’07), Providence, USA, 2007.[33] Y. Xu, A. Fern, S. Yoon, Discriminative learning of beam-search heuristics for planning, in: Proceedings of the Twentieth International Joint Conferenceon Artificial Intelligence (IJCAI’07), 2007.[34] S. Dzeroski, L.D. Raedt, K. Driessens, Relational reinforcement learning, Machine Learning 43 (2001) 7–52.[35] A. Ng, D. Harada, S. Russell, Policy invariance under reward transformations: Theory and application to reward shaping, in: Proceedings of the SixteenthInternational Conference on Machine Learning (ICML’99), 1999.[36] T.G. Nicol, N. Schraudolph, Conjugate directions for stochastic gradient descent, in: Proceedings of the International Conference on Artificial NeuralNetworks (ICANN’02), in: Lecture Notes in Computer Science, vol. 2415, Springer-Verlag, 2002, pp. 1351–1356.[37] J. Baxter, P.L. Bartlett, Infinite-horizon policy-gradient estimation, Journal of Artificial Intelligence Research 15 (2001) 319–350.[38] A. Benveniste, M. Metivier, P. Priouret, Adaptive Algorithms and Stochastic Approximation, Springer-Verlag, 1990.[39] E. Greensmith, P. Bartlett, J. Baxter, Variance reduction techniques for gradient estimates in reinforcement learning, Journal of Machine LearningResearch 5 (2004) 1471–1530.[40] D. Aberdeen, J. Baxter, Scaling internal-state policy-gradient methods for POMDPs, in: Proceedings of the Nineteenth International Conference onMachine Learning (ICML’02), Morgan Kaufmann, Sydney, Australia, 2002.[41] J. Hoey, R. St-Aubin, A. Hu, C. Boutilier, SPUDD: Stochastic planning using decision diagrams, in: Proceedings of the Fifteenth Conference on Uncertaintyin Artificial Intelligence (UAI’99), 1999, pp. 279–288.[42] J. Nicholls, Algebraic decision diagrams for reinforcement learning, Tech. rep., Australian National University, honours thesis, September 2005.[43] C. Boutilier, Sequential optimality and coordination in multiagent systems, in: Proceedings of the 16th International Joint Conference on ArtificialIntelligence (IJCAI’99), 1999.[44] D. Bernstein, S. Zilberstein, N. Immerman, The complexity of decentralized control of Markov decision processes, in: Proceedings of the SixteenthConference on Uncertainty in Artificial Intelligence (UAI’00), 2000.[45] C. Guestrin, M.G. Lagoudakis, R. Parr, Coordinated reinforcement learning, in: Proceedings of the Nineteenth International Conference on MachineLearning (ICML’02), Morgan Kaufmann Publishers Inc., 2002, pp. 227–234.[46] G.J. Gordon, Reinforcement learning with function approximation converges to a region, in: Advances in Neural Information Processing Systems(NIPS’00), vol. 13, 2001, pp. 1040–1046.[47] A. Blum, M. Furst, Fast planning through planning graph analysis, Artificial Intelligence 90 (1997) 281–300.[48] O. Buffet, D. Aberdeen, The factored policy gradient planner, in: Proceedings of the Fifth International Planning Competition (IPC-5), 2006, seehttp://www.ldc.usb.ve/~bonet/ipc5 for all results and proceedings.[49] B. Scherrer, A. Boumaza, C. Thiery, Personal communication, 2008.[50] C. Anderson, D. Smith, D. Weld, Conditional effects in graphplan, in: Proceedings of the International Conference on Artificial Intelligence Planning andScheduling (AIPS’98), 1998.[51] J. Peters, S. Vijayakumar, S. Schaal, Natural actor-critic, in: Proceedings of the Sixteenth European Conference on Machine Learning (ECML’05), in:Lecture Notes in Computer Science, vol. 3720, Springer-Verlag, 2005.[52] S. Kakade, A natural policy gradient, in: Advances in Neural Information Processing Systems (NIPS’03), vol. 14, 2003.[53] O. Buffet, D. Aberdeen, FF+FPG: Guiding a policy-gradient planner, in: Proceedings of the Seventeenth International Conference on Automated Planningand Scheduling (ICAPS’07), Providence, USA, 2007.