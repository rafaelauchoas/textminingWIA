Artificial Intelligence 172 (2008) 1731–1751Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintThe combination of multiple classifiers using an evidential reasoningapproachYaxin Bi a,∗, Jiwen Guan b, David Bell ba School of Computing and Mathematics, University of Ulster at Jordanstown, Co Antrim, BT37 0QB, UKb School of Computer Science, The Queen’s University of Belfast, Belfast, BT7 1NN, UKa r t i c l ei n f oa b s t r a c tArticle history:Received 4 June 2007Received in revised form 12 June 2008Accepted 12 June 2008Available online 19 June 2008Keywords:Ensemble methodsDempster’s rule of combinationEvidential reasoningEvidential structuresCombination functions1. IntroductionIn many domains when we have several competing classifiers available we want tosynthesize them or some of them to get a more accurate classifier by a combinationfunction. In this paper we propose a ‘class-indifferent’ method for combining classifierdecisions represented by evidential structures called triplet and quartet, using Dempster’srule of combination. This method is unique in that it distinguishes important elementsfrom the trivial ones in representing classifier decisions, makes use of more informationthan others in calculating the support for class labels and provides a practical way toapply the theoretically appealing Dempster–Shafer theory of evidence to the problem ofensemble learning. We present a formalism for modelling classifier decisions as tripletmass functions and we establish a range of formulae for combining these mass functionsin order to arrive at a consensus decision.In addition we carry out a comparativesimplet and dichotomous structure and also compare twostudy with the alternatives ofcombination methods, Dempster’s rule and majority voting, over the UCI benchmark data,to demonstrate the advantage our approach offers.© 2008 Elsevier B.V. All rights reserved.The idea characterizing ensemble learning is to learn and retain multiple classifiers and combine their decisions in someway in order to classify new instances [36]. The attraction of this approach in supervised machine learning is based onthe premise that a combination of classifiers is often more accurate than an individual classifier. A theoretical explanationof its success is that different classifiers offer complementary information about instances to be classified which could beharnessed to improve the performance of the individual classifiers [27].Generally speaking a successful ensemble method depends on two components: a set of appropriate classifiers and acombination method, function or scheme [30]. Classifiers assign single classes or sets of classes to a new instance alongwith respective numeric values as decisions, and a combination function merges these decisions in some way to determinea final decision—usually by voting among the decisions.Ensemble classifiers can be generated in different ways. A typical approach is to use a single learning algorithm tooperate on different subsets of attributes or instances of the training data, as done in bagging [9] and boosting [11,21],and in derivatives such as random forests [10] or the random subspace method for constructing decision tree forests [26].Another approach is to use different learning algorithms to operate on a single data set [4,8,32]. Among any set of individualclassifiers, some are more accurate for a given task and others are less accurate. However there is often not a dominant* Corresponding author.E-mail addresses: y.bi@ulster.ac.uk (Y. Bi), j.guan@qub.ac.uk (J. Guan), da.bell@qub.ac.uk (D. Bell).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.06.0021732Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–1751one for the complete data distribution. By taking account of the strengths of classifiers through combination functions, theperformance of the best individual classifier can be improved [12].Kuncheva [28] roughly characterizes combination methods, based on the forms of classifier outputs, into two categories.In the first category, the combination of decisions is performed on single class labels, including majority voting and Bayesianprobability, which have been extensively examined in the ensemble literature [18,27,40,49]. The second category is con-cerned with the utilization of continuous values corresponding to class labels. One set of methods, often called class-alignedmethods, is based on using the same class labels from different classifiers in calculating the support for class labels, regard-less of what the support for the other classes is. This method includes linear sum and order statistics, to which considerableeffort has been devoted [25,27,47,48,51]. Another method, called stacked generalization or meta-learning, is to use continu-ous values of class labels as a set of features to learn a combination function in addition to a set of classifiers [19,46,54]. Analternative group of methods, which are called class-indifferent methods, is to make use of as much information as possibleobtained from both single classes and sets of classes in calculating the support for each class [28].Class-aligned methods and class-indifferent methods are both based on continuous values of class labels in calculatingthe support for class labels. A distinction between them is, however, that the latter takes impacts from different classesinto account in determining the support for a class that permits the presence of uncertainty information—as happens whenan instance is classified into different classes by different classifiers. Several related studies are presented in the literature,where class-indifferent methods utilize single classes and sets of classes [16,39,49]. Class-indifferent methods for combiningdecisions in the form of a list of ordered decisions have not been intensively studied and are poorly understood. In particular,little is known about the value of evidential reasoning methods for combining truncated lists of ordered decisions [5,8].In this study we consider a class-indifferent approach to combining classifiers using Dempster’s rule of combination. Ourfocus is on generating classifiers using different learning methods to manipulate a single data set, and the combinationof classifiers is modeled as a process of reasoning under uncertainty. We model each output given by classifiers on newinstances as a list of contender decisions and reduce it to subsets of 2 and 3 decisions, respectively, which are then repre-sented by the evidential structures oftriplet and quartet [5–8]. We first establish a formalism for combining triplets andquartets using Dempster’s rule of combination to constrain the final decision, and then we empirically and analytically ex-amine the effect of different sizes of decision lists on the combination of classifiers. Furthermore we justify the assumptionwe make that modelling classifier results as independent bodies of evidence is sensible.The advantages of our approach are summarized as follows. The first advantage is that our method makes use of a widerange of evidence items in classification to make the final decision. The idea is inspired by the observation that if onlythe ‘best’ single class labels are selected on the basis of their corresponding values, valuable information contained in thediscarded labels may be lost. Arguably, the potential loss of support from the other classes should be avoided by utilizingthis support information in the decision making process. The evidence structures, such as the triplet, are able to distinguishthe important classes from the trivial ones and incorporate the best-supported class, the second best-supported class, andthe rest of the classes which are treated in terms of ignorance within the process of decision making. The second advantageis that these evidence structures provide an efficient way for combining many pieces of evidence since they break downa large list of contender decisions into smaller, more tractable subsets. Like the simplet and dichotomous structures [2,44],our method deals well with a long-standing criticism saying that the evidence theory does not translate easily into practicalapplications due to the computational complexity of combining multiple pieces of evidence.To validate our method and illustrate its power, we have carried out numerous comparative experiments over the UCIdata sets [3]. We experimentally compare the triplet and quartet with the alternatives of simplet, dichotomous structureand the full list of decisions. We also make a comparison between Dempster’s rule and majority voting in combiningclassifiers. During the course of classifier combination, another important issue, namely the extent of agreement reachedon classification decisions among classifiers, is assessed by means of κ statistics, and the associative property of combiningtriplets is also experimentally examined. Finally to explain our empirical findings, we present an investigation into thecalculation process of Dempster’s rule which provides an insight into the reason for superiority of our method.The rest of the paper is organized as follows. Section 2 presents the representation of classifier outputs and the ideaof class-independent methods. Section 3 reviews the Dempster–Shafer theory of evidence. Section 4 presents a review ofseveral related studies with a focus on the alternative structures of simplet and dichotomous structure previously used incombining classifiers. The rationale of the evidential structure of the triplet, and the associated formulae, are presentedin Section 5. The combination functions of Dempster’s rule with different evidential structures and majority voting forcombining classifiers are evaluated and the experimental settings and results are detailed in Section 6. Section 7 presentsa discussion about the advantage of the triplet and the quartet over the alternatives. Section 8 gives a justification for theindependence of evidence derived from classifier outputs. The concluding summary is given in Section 9.2. Representation of classifier outputs and combination methodsIn supervised machine learning, a learning algorithm is provided with training instances of the form {(cid:3)d1, c1(cid:4), . . . ,(cid:3)d|D|, cq(cid:4)} (di ∈ D, ci ∈ C, 1 (cid:2) q (cid:2) |C|) for inducing some unknown function f such that f (d) = c. D is the space of at-tribute vectors and each vector di is in the form (w i1 , . . . , w in ) whose components are symbolic or numeric values; C isa set of categorical classes and each class ci is in the form of class label. Given a set of training data, a learning algorithmY. Bi et al. / Artificial Intelligence 172 (2008) 1731–17511733Fig. 1. A decision profile for instance d generated by ϕ1(d), ϕ2(d), . . . , ϕM (d).is aimed at learning a function ϕ—a classifier from the training data. The classifier ϕ is an approximation to the unknownfunction f .Given a new instance d, a classification task is to decide, using ϕ, whether instance d belongs to class ci . In a multi-classassignment, we denote such a process as a mapping:ϕ : D → C × [0, 1],(1)where C × [0, 1] = {(ci, si) | ci ∈ C, 0 (cid:2) si (cid:2) 1}, si is a numeric value that can be in different forms, such as a similarity score,a class-conditional probability (prior posterior probability) or other measure, depending on the types of learning algorithmsused. It represents the degree of support or confidence in the proposition that instance d is assigned to class ci . The greaterthe value for class ci , the greater the confidence we have that the instance belongs to that class. Without loss of generality,we denote the classifier output by ϕ(d) = {s1, . . . , s|C|}—a general representation of classifier outputs.From the representation ϕ(d), alternative forms of outputs can be derived. For example, we could rank all class labelsaccording to their continuous values in descending order. By choosing a single label at the top of the ranked list—the singlelabel with maximal value in the classifier output ϕ(d), we can assign a unique label or a label subset to instance d as aclassification decision. Alternatively, we rearrange the process of assigning a unique label to instance d as a mapping of eachpair (cid:3)d, ci(cid:4) to a Boolean value true (T ) or false (F ) in terms of the oracle output [29]. If value T is assigned to (cid:3)d, ci(cid:4), thatmeans a decision is made that the proposition of instance d belonging to class ci is true, whereas value F indicates theproposition is false. These alternatives can be seen as the output information at the final stage of classification.Given an ensemble of classifiers, ϕ1, ϕ2, . . . , ϕM , if each classifier outputs only a single class label for instance d, theresults of classifiers can be combined using majority voting (weighted) [31] or Naive Bayes [49], for example. More generally,if each classifier produces multiple classes as output for instance d—a numeric score vector (list) that is represented as ϕi(d)below, all these vectors can then be organized into a matrix called a decision profile (DP) as depicted in Fig. 1 [28]. Thereare several different ways that the combination of classifier outputs can be carried out.ϕi(d) =si j(cid:2)(cid:3)(cid:3) 1 (cid:2) j (cid:2) |C|(cid:4), 1 (cid:2) i (cid:2) M.(2)One of the most commonly used combination methods is to calculate the support for class c j using only the DP’s jthcolumn, i.e. s1 j, s2 j, . . . , sM j , regardless of what the support for the other classes is. We call such a method a class-alignedmethod. Some examples of this are linear sum [51], order statistics: minimum, maximum and median [48], and probabilisticproduct and sum rules [27]. Alternatively, the combination of classifier outputs can be performed on an entire decisionprofile, or on the selected information in order to constrain a class decision. We refer to this alternative group of methodsas class-indifferent methods.There exist several related contributions to this subject, including the combination of neural network classifiers usingDempster’s rule [41] and the combination of neural network classifiers derived from different feature sets using Dempster’srule [1]. In a broad sense, these studies take a similar approach to calculating the support for classes. That is, for each clas-sifier against each class, a mean vector (called a reference vector) is generated and organized into a matrix called a decisiontemplate, denoted by DT i . For M classifiers and |C| classes, |C| decision templates with M × |C| dimensions are formed.Given a decision profile DP(d), the closeness between DP(d) and DT i , 1 (cid:2) i (cid:2) |C|, is computed using different proximitymeasures such as Euclidean distance and cosine function, the class with the largest support is assigned to instance d.In our work, the concept of class-indifferent methods is slightly different from the one aforementioned. We neithergenerate decision templates nor use an entire decision profile to compute the degrees of support for every class. Instead weselect 2 and 3 classes from each ϕ(d) according to their numeric values, and restructure these into a new list composed ofthree and four subsets of classes of C respectively, which are represented by the evidential structures of triplet and quartet.With a triplet, for example, the first subset contains the class with the largest value of confidence, and the second containsthe second largest valued class, and the third one is the whole set of classes C . In this way, the decision profile as illustratedin Fig. 1 will be restructured into a triplet or quartet decision profile and each column no longer corresponds to the sameclass. The degree of support for each class is computed through combining all triplets or quartets in a decision profile. Wewill detail our method in later sections.1734Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–17513. Dempster–Shafer (DS) theory of evidenceFig. 2. Representation of belief interval.The DS theory of evidence has been recognized as an effective method for coping with uncertainty or imprecisionembedded in evidence used in a reasoning process. It is often viewed as a generalization of Bayesian probability theory, byproviding a coherent representation for ignorance (lack of evidence) and also by discarding the insufficient reasoning principle.The DS theory is well suited to a range of decision making activities. It formulates a reasoning process as pieces of evidenceand hypotheses, and subjects these to a strict formal process in order to infer conclusions from the given uncertain evidence,avoiding human subjective intervention to some extent.In the DS theory, which is also referred to as evidence theory, evidence is represented in terms of evidential functionsand ignorance. These functions include mass functions, belief functions, and plausibility functions [44]. Any one of these conveysthe same information as any of the others.Definition 1. Let Θ be a finite nonempty set, called the frame of discernment. Let [0, 1] be an interval of numeric values.A mapping function m : 2Θ → [0, 1] is called a mass function if it satisfies:1) m(∅) = 0,2)(cid:5)X⊆Θm( X) = 1.A mass function is a basic probability assignment (bpa) to all subsets X of Θ . A subset A of a frame Θ is called a focalelement or focus of a mass function m over Θ if m( A) > 0 and A is called a singleton if it is a one-element subset.Definition 2. A function bel : 2Θ → [0, 1] is called a belief function if it satisfies:1) bel(∅) = 0,2) bel(Θ) = 1and for any collection A1, A2, . . . , An (n (cid:3) 1) of subsets of Θ ,(cid:6) (cid:7)(cid:5)bel( A1 ∪ A2 ∪ · · · ∪ An) (cid:3)(−1)|I|+1bel(cid:8)Ai.I⊆{1,2,...,n},I(cid:10)=∅i∈IOne of the attractive features of the DS theory is that the belief function is in contrast to conventional probability, wherethe inequality is replaced by an equality. With this function, a plausibility function can be defined as pls( A) = 1 − bel( A).Notice that a belief function gathers all of the support that a subset A gets from all of the mass functions of its subsets,whereas a plausibility function is the difference between 1 and all of the support of A’s complement subsets. The differencepls( A) − bel( A) represents the residual ignorance, denoted by ignorance( A) (or ign( A)). This gives another important featureof DS—the representation of what is precisely known and what remains unknown. Fig. 2 presents an intuitive representationof supporting a subset A by its belief function and plausibility function along with ignorance.Definition 3. Let m1 and m2 be two mass functions on the frame of discernment Θ , and for any subset A ⊆ Θ , the orthogonalsum ⊕ of two mass functions on A is defined asm( A) = (1/N)(cid:5)m1( X)m2(Y ),X,Y ⊆Θ, X∩Y = A(3)(cid:9)where N = 1 −X∩Y =∅ m1( X)m2(Y ) and K = 1/N is called the normalization constant of the orthogonal sum m1 ⊕ m2. Theorthogonal sum is a fundamental operation of evidential reasoning and it is often called Dempster’s rule of combination(Dempster’s rule, for short). There are two conditions governing when it is used to combine mass functions.The first condition is that N (cid:10)= 0, i.e. if N = 0, the orthogonal sum does not exist.The second is that two mass functions must be independent of each other; that is they must represent independentopinions or evidence sources relative to the same frame of discernment. Notice that Dempster’s rule simply involves twomass functions, it does not tell us whether one mass function is independent of another, nor is it able to rule out thedependence between two mass functions. Therefore, the context of applying Dempster’s rule and the effect of combiningtwo mass functions depend very much on how evidence sources are treated or modelled as mass functions, rather than onaccounting for their relation alone [24,43].Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–175117353.1. Simple support functionIn some situations, when the structure of evidence is taken into account mass functions can be significantly simplified.The simplest form of a mass function is called a simple support function which is limited to providing a degree of supportfor a single proposition A ⊆ Θ , which is referred to as a simplet structure or simplet. This structure provides no support atall for any other propositions discerned by a frame of discernment Θ [44].Formally, a mass function is said to be simple if there is a non-empty subset A ⊆ Θ such thatm( A) = s,m(Θ) = 1 − s,m(Θ − A) = 0,where 0 < s (cid:2) 1, the subset A is a focus of m and s is called the degree of support and the mass function in this form iscalled a simple support function focused on A.A simple support function is the belief function bel of a simple mass function m. Thus,(cid:5)bel( A) =m( X) = m( A)X⊆ Afor all A ⊆ Θ .Let m1, m2, . . . , mn be simple mass functions with the common focus A and respective degrees of support s1, s2, . . . , sn.Then m1 ⊕ m2 ⊕ · · · ⊕ mn is still a simple mass function with the same focus A, and support degrees for A and others areas follows:(m1 ⊕ m2 ⊕ · · · ⊕ mn)( A) = 1 −(cid:10)(1 − si),(m1 ⊕ m2 ⊕ · · · ⊕ mn)(Θ) =i=1,2,...,n(cid:10)(1 − si),i=1,2,...,n(m1 ⊕ m2 ⊕ · · · ⊕ mn)(Θ − A) = 0.Separable mass functions include both simple mass functions and orthogonal sums of simple mass functions. They arebased on both homogeneous (with the same focuses) and heterogeneous evidence, where different subsets of the frame ofdiscernment can be referenced from different evidence sources.3.2. Dichotomous functionA mass function m is said to be a dichotomous function if the only possible focal elements of m are A, Θ − A, Θ forsome A ⊆ Θ . A special case occurs when A is a singleton {x} ⊆ Θ . In such a situation, a dichotomous mass function m hasno focuses other than {x}, Θ − {x}, Θ for some x which is referred to as a dichotomous structure [2].Let Θ = {x1, x2, . . . , x|Θ|}. Suppose thatfor every i = 1, 2, . . . , |Θ|,there is a dichotomous mass function mi :mi({xi}), mi(Θ − {xi}), mi(Θ) and mi({xi}) + mi(Θ − {xi}) + mi(Θ) = 1. We view these quantities as follows:• mi({xi}) is the degree of support for {xi };• mi(Θ − {xi}) is the degree of support for the refutation of {xi }; and• mi(Θ) is the degree of the support not assigned for or against the proposition {xi }.Barnett has proposed a technique based on dichotomous mass functions instead of general mass functions. It meansthat instead of potentially exponential time complexity function in deriving evidence combination, the computation ofdichotomous mass functions involves only the 3 particular subsets {x}, Θ − {x}, Θ for each x ∈ Θ ; the general mass functionshave to enumerate all 2subsets of Θ .Barnett’s approach is to consider the entire orthogonal sum for evidence bodies which have the structure m1 ⊕ m2 ⊕· · · ⊕ m|Θ|. These are precisely those evidence spaces which are separable into exactly |Θ| dichotomous mass functionsm1, m2, . . . , m|Θ|, and the time complexity of combining these mass functions is linear. Guan and Bell [22,23] generalizedthis to consider the general orthogonal sum as explained below.|Θ|Let Θ = {x1, . . . , x|Θ|}. Suppose that for each xi ∈ Θ , there are li dichotomous mass functions of repeated focus:i (Θ) = 1; where i = 1, 2, . . . , |Θ|; j = 1, 2, . . . , ls; s = l1, . . . , lk;i ({xi}), mm1 (cid:2) k (cid:2) |Θ|; 0 (cid:2) l1, . . . , lk. The task now is to calculate quantities associated withi (Θ − {xi}) + mi (Θ − {xi}), mi ({xi}) + mi (Θ); mjjjjjjm = m11(cid:11)⊕ · · · ⊕ ml1(cid:14)1(cid:12)(cid:13)· · · ⊕ m1k(cid:11)lk⊕ · · · ⊕ m,(cid:14)(cid:12)(cid:13)kl1 itemslk items(4)1736Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–1751where l1 + · · · + lk = n, and n is the number of masses to be summed which may be greater than |Θ|. In Eq. (4), the calcu-lation of combining n dichotomous mass functions is divided into two parts. The first part is to combine the mass functionswith repeated focal elements. The second part is to combine the mass functions for all focuses. A method of combiningthese mass functions was studied in [22,23], and it has been shown that the computational complexity of combining suchmass functions is linear.4. Existing methodsIn this section we first develop a model to unify the tasks of combining the outputs of multiple classifiers in the con-ceptual framework of Dempster–Shafer theory and then look at several DS-based studies.In the supervised learning domain, classifiers can assign one or more classes to each instance. The key assumptionunderlying our approach here is a single class assignment where each instance belongs to one and only one class. Thisassumption suggests that only the one-element subsets in 2C will be of semantic interest and they will be used to representpropositions. By using the DS terminology, given a frame of discernment C = {c1, . . . , c|C|}, evidence derived from classifiersconcerns specific individual classes, tending to support singleton classes of C , and the other subsets of 2C are not particularlymeaningful. Therefore the powerset 2C for all the propositions could be reduced to the subsets that contain only individualclasses making up a frame of discernment C itself.Formally let M be the number of classifiers ϕ1, . . . , ϕM and let C = {c1, . . . , c|C|} be a set of classes. For any instanced ∈ D, each classifier produces an output vector ϕi(d). Classifying d means assigning it into one class in C , i.e., decidingamong a set of |C| hypotheses: d belongs to ck, k = 1, . . . , |C|, according to ϕi(d). In DS terms, C is referred to as a frame ofdiscernment, and the classifying process is regarded as one which decides the true value the proposition of that instance dbelongs to ck according to the knowledge ϕ(d). ϕ(d) can be regarded as a piece of evidence that represents the degrees ofour support or belief for the proposition. Instead of 100% certainty, it only expresses some part of our belief committed to{ck} ∈ 2C and the rest of our belief which cannot be directly derived from ϕ(d) and the negation of the proposition remainsunknown or indiscernible. In the DS formalism, such a situation is regarded as ignorance, and belief (mass) functions provideus with an effective way to express it. This is one of the attractive features of DS-based methods.The merit of assuming the single class assignment is that it significantly eases the application of DS theory to practicalevidential functions in a general context are reduced to |C|. The following sectionsproblems since the computations of 2review four DS-based methods for representing evidence and defining mass (belief) functions based on ϕ(d).|C|4.1. Xu’s methodXu et al. [49] discussed several approaches for combining multiple classifiers, including majority voting (weighted) andthe Bayesian formalism, and proposed a DS model for combining the results of multiple classifiers. Their method treatedclassifier outputs as single class labels and defined the sources of evidence for the propositions of interest on the basis of theperformance of classifiers in terms of recognition, substitution and rejection rates. The items of evidence were representedby dichotomous mass functions.Let D be a training data set, and suppose that the recognition and substitution rates of classifiers are denoted bys. For a new instance d, a piece of evidence ϕi(d) iss(ϕi(D)), and the rejection rates are given by 1 − (cid:5)i− (cid:5)ir(cid:5)ir(ϕi(D)) and (cid:5)irepresented by the following mass function:(cid:16)(cid:16)(cid:15){ck}(cid:15){¯ck}mikmik= (cid:5)ir= (cid:5)ismik(C) = 1 − mi(cid:16)(cid:15)ϕi(d)(cid:15), 1 (cid:2) i (cid:2) M, ∃k ∈(cid:16)ϕi(d)(cid:15){ck}, 1 (cid:2) i (cid:2) M, ∃k ∈(cid:16)(cid:15){¯ck}− mik(cid:16),k(cid:4)(cid:2)1, . . . , |C|(cid:2)1, . . . , |C|,(cid:4),(5)(6)(7)where {¯ck} = C − {ck}. With M pieces of evidence existing, represented by M dichotomous mass functions, the degrees ofsupport for classes can be calculated through combining these mass functions by formula (3). A final class decision fora given instance is made on selecting the class with the largest degree of support. Notice that such a combination is anclass-indifferent method since the calculation of support for a class ck is not only based on the mass functions which havethe same focus ck, but the support is also impacted by the mass values of the second subset {¯ck} = C − {ck} and the wholeset C , i.e. the ignorance.In the above definition, however, the dichotomous mass functions are not directly defined on the basis of the numericvalues of ϕi(d) (see formula (2)). Instead they are defined based on the overall performance of classifiers. Therefore fordifferent instances, the basic probability assignments derived from the classifier outputs—dichotomous mass functions—are the same. This method ignores the fact that normally a classifier does not have the same performance on differentclasses, this might consequently degrade the combined performance of classifiers. Nevertheless the proposed method laysgroundwork for formalizing the problem of combining classifier outputs using the dichotomous evidence structure and itachieved a considerable performance improvement when applied to handwriting recognition.4.2. Rogova’s methodY. Bi et al. / Artificial Intelligence 172 (2008) 1731–17511737Rogova [41] proposed a model for combining the results of neural network classifiers using the DS theory. This workaccounted for the relation between classifier outputs and reference vectors as pieces of evidence and developed a generalway to measure the relation using proximity measures. The pieces of evidence are represented by simple support functions.Formally, let Dk be a subset of a training data set, in which all instances belong to class ck ∈ C and let {ϕi(x)} (x ∈ Dk) be aset of classifier outputs. A mean vector of {ϕi(x)} is denoted by E ik called a reference vector for class k. For any instance d,a general proximity measure used for E ik and ϕi(d) is defined as follows:i (cid:2) M, k (cid:2) |C|.(8)μik= φ(cid:15)(cid:16)E ik, ϕi(d),The measure φ can be in different forms—cosine function, Euclidean distance, and so forth. It provides us with evidenceabout how likely it is that instance d belongs to class ck. If the output ϕi(d) is far from E ik, ϕi(d) is considered as providingvery little information regarding the proposition of d being under ck; in that case, φ must therefore take on a ‘small’ value.On the contrary, if ϕi(d) is close to E ik belong to the same class.Simple support functions are given below:k, one will be much more inclined to believe that d and E i(cid:16)(cid:15)= μi{ck}k,(cid:16)(cid:15){¯ck}= 1 −j( j(cid:10)=k)mikmimik(C) = 1 − μik,(cid:10)(cid:16)(cid:15)1 − μi,rr=1,r(cid:10)=kmij( j(cid:10)=k)(C) = 1 − mij(cid:16)(cid:15){¯ck}=(cid:10)(cid:15)1 − μir(cid:16).r=1,r(cid:10)=k(9)(10)The interpretation of the above formulation is that formulae (9) and (10) quantify the relation of ϕi(d) with each classof reference vector E ik—pieces of evidence. However, each piece of evidence represents only some part of our belief which iscommitted to {ck} and it does not point to any other particular hypotheses. Thus the rest of our belief cannot be distributedto anything else other than C —the whole frame of discernment. Based on this formulation, for a unseen instance, a decisionprofile is remodeled by formula (11) and the kth (1 (cid:2) k (cid:2) |C|) column of DP corresponds to M simple support functionswith two focal elements, viz. {ck} and the whole set C .DP(d) =(cid:2)mij(cid:3)(cid:3) 1 (cid:2) j (cid:2) |C|(cid:4), 1 (cid:2) i (cid:2) M.For such a decision profile, the combination will be performed on each column k using Dempster’s rule, resulting in adegree of support for class ck as follows:(cid:16)(cid:15){ck}=(cid:15)m1k⊕ · · · ⊕ mMk(cid:16)(cid:15)(cid:16).{ck}m(12)The final class c = argmax{m({ck}) | 1 (cid:2) k (cid:2) |C|} is assigned to instance d. The combination performed in this way usingDempster’s rule can be seen as a class-aligned method.Rogova’s work is based on an original idea proposed by Mandler and Schurmann [35] and it extended that work in twoaspects regardless of how reference vectors are generated. The first aspect was to introduce a generic form of proximitymeasure φ, allowing different distance measures to be applied to compute class-conditional probabilities—basic probabilityassignments. The second was to obtain more support for ck by combining two simple mass functions given by formulae (9)and (10). However, the issue with this extension is the use of the opponent of ck, i.e., ¯ck, which was not explicitly specified.If {¯ck} = C − {ck}, then formula (10) seems not to make sense and the combination mij (Rogova used the notation ofkmk ⊕ m¯k) is questionable.⊕ mi4.3. Al-Ani’s methodA similar attempt to apply the DS theory of evidence to combining neural network outputs was carried out by Al-Aniet al. [1]. This method also treated the distance between a reference vector and a classifier output as a piece of evidence. Butthe difference from the previous work is in the way it obtains reference vectors. It first initializes reference vectors for eachclass, and then iteratively uses training instances to optimize reference vectors through minimizing the mean square errorsbetween combined classifier outputs and the target outputs, ensuring the optimized reference vectors can be achieved.Finally the distance between the optimized reference vectors and classifier outputs is defined as a piece of evidence andis represented by a simple support function. Let E ik be an optimized reference vector. For any instance d, each classifierproduces an output vector ϕi(d) (1 (cid:2) i (cid:2) M), a simple support function is defined below:mik(cid:15){ck}j(cid:16)=μik(cid:9)|C|j=1 μigij=1 μi= exp(−(cid:14)E ik(cid:9)|C|mik(C) =,+ gi,+ gij− ϕi(d)(cid:14)2) and gi is a coefficient to be tuned. The combination method is the same as Rogova’s,where μikwhich is static. However, the way of obtaining reference vectors through minimizing the overall mean square error makes(11)(13)(14)1738Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–1751the process of combining classifiers trainable, which may lead to better performance than a static combination scheme, butwith the additional cost for training as well as additional training data.4.4. Denoeux’s methodDenoeux [17] proposed an evidence theoretic k-nearest neighbors (kNN) method for classification problems based onthe DS theory. Unlike the methods above which were designed for combining classifiers in ensemble learning, this methodfocuses on a single classifier ϕ in classifying new instances, by accounting for distances from their neighbors to determineclass labels. Formally, let D be a training data set, for instance, d ∈ D and let Φ be a set of the k-nearest neighbors ofd according to some distance measures (e.g., Euclidean distance). Classifying d means assigning it to one of the classesck ∈ C based on the weights of representative classes of its neighbors. Thus the distance between d and neighbor di ∈ Φ isconsidered as a piece of evidence to support a proposition about the class membership of d. The evidence is represented bya simple support function as follows:(cid:16)(cid:15){ck}= φ(d, di),mimi(C) = 1 − φ(d, di),mi( A) = 0, ∀ A ∈ 2C \di ∈ Φ,(cid:2)(cid:4),{ck}, C(15)(16)(17)where φ was suggested to be exp(−γ (μi)2) and μi = (cid:14)d − di(cid:14). This formulation appears to be similar to Rogova’s methodwithout considering what specific distance measures are used. However, Denoeux’s method is quite different from Rogova’sin the sense that the former computes distances in feature space, whereas the latter works in the decision (classifier output)space. Therefore the decision profile of this method is slightly different from the one given in Fig. 1. For the decision matrixeach neighbor is treated as a row and the column corresponds to classes, and the combination using Dempster’s rule iscarried out on the column. In order to improve the classification accuracy, an effective decision procedure was proposed todetermine the optimal or near-optimal parameter values from the data by minimizing an error function [50].Denoeux’s method shows the advantage of permitting a clear distinction between the presence of conflictinginformation—as happens when an instance is close to several neighbors from different classes—and incomplete information—when an instance is far away from any instances in its neighborhood. It proves to be very competitive with the standardkNN methods. A similar idea has been adapted to a neural network classifier by Denoeux in [16].5. Triplet mass functionIn this section we describe the development of a key evidence structure—the triplet and its formulation.Starting by analyzing the computational complexity of combining multiple pieces of evidence, we consider how a moreefficient method for combining evidence can be established. Given M pieces of evidence represented by formula (2), thecomputational complexity of combining these pieces of evidence using Eq. (3) is dominated by the number of elements in Cand the number of classifiers M. In the worst case, the time complexity of combining M pieces of evidence is O (|C|M−1).One way of reducing the computational complexity is to reduce the number of pieces of evidence being combined, so thatthe combination of evidence is carried by a partition of the frame of discernment C , with fewer focal elements than C , butincluding possible answers to the question of interest. The partition can thus be used in place of C when the computationsof the orthogonal sum are carried out [42]. For example, a dichotomous structure can be used to partition the frameof discernment C into two subsets ϑ1 and ϑ2, where there are a number of mass functions that represent evidence infavor of ϑ1 and against ϑ2, along with the lack of evidence— ignorance. It has been shown that Dempster’s rule can beimplemented in such a way that the number of computations increases only linearly with the number of elements in Cif the mass functions being combined are focused on the subsets where ϑ1 is singleton and ϑ2 is the complement of ϑ1,i.e., O (|C|) [22]. Another approach to reducing the computational complexity of Dempster’s rule is to approximate thecalculation of belief functions in a coarsened frame of discernment, which is detailed in [15].The partitioning technique enables a large problem to be broken up into several smaller and more tractable problems.However, a fundamental issue in applying this technique is how to select elements that contain the possibly correct answersto the propositions corresponding to C .An intuitive way is to select the element with the highest degree of confidence. Indeed, since the classifier outputsapproximate class posteriori probabilities, selecting the maximum probability reduces to selecting the output that is themost ‘certain’ of the decisions. This could be justified from two perspectives. First, the probability assignments given in for-mula (2) give quantitative representation of judgments made by classifiers on the propositions; the greater their values, themore likely these decisions are correct. Thus selecting the maximum distinguishes the trivial decisions from the importantones. Second, the combination of decisions with the lower degrees of confidence may not contribute to the performance in-crease of combined classifiers, but only make the combination of classifier’s decisions more complicated [48]. The drawbackof selecting the maximum, however, is that the combined performance can be reduced by a single dominant classifier thatrepeatedly provides high confidence values. Contenders with the higher values are always chosen as the final classificationdecisions, but some of these may not be correct.Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–17511739To cope with the deficiency resulting from the maximal selection, we propose to take the second maximum decision intoaccount in combining classifiers. Its inclusion not only provides valuable information contained in the discarded class labelsby the maximal selection for combining classifiers, but this also to some extent avoids the deterioration of the combinedperformance caused by the errors resulting from a single dominant classifier that repeatedly produces high confidencevalues. We propose a novel structure—a triplet—partitioning a list of decisions ϕ(d) into three subsets.Definition 4. Let C be a frame of discernment, where each choice ci ∈ C is a proposition that instance d is classified incategory ci . Let ϕ(d) = {s1, s2, . . . , s|C|} be a list of scores, a localized mass function is defined by a mapping function,m : 2C → [0, 1], i.e. a bpa to ci ∈ C for 1 (cid:2) i (cid:2) |C| as follows:(cid:16)(cid:15){ci}m=si(cid:9)|C|j=1 s j,where 1 (cid:2) i (cid:2) |C|.(18)This mass function expresses the degrees of belief with regard to the choices of classes to which a given instance couldbelong. With this definition, we rewrite formula (2) as ϕ(d) = m({c1}), m({c2}), . . . , m({c|C|}).Definition 5. Let C be a frame of discernment, and let {u}, {v} be focal singletons and m be a respective mass function. Anexpression of the form Y = (cid:3){u}, {v}, C(cid:4) is defined as a triplet, it satisfies(cid:16)(cid:15){u}m(cid:16)(cid:15){v}+ m+ m(C) = 1and mass function m is called a triplet mass function.To obtain triplet mass functions, we define an outstanding rule below.Definition 6. Let C be a frame of discernment and ϕ(d) = {m({x1}), m({x2}), . . . , m({xn})}, where |n| (cid:3) 2, an outstanding ruleis a focusing operator, denoted by mσ , which breaks up ϕ(d) and makes(cid:16), . . . , m{u} = argmax m(cid:16), m(cid:16)(cid:4)(cid:16),(19)(cid:15)(cid:2)(cid:15)(cid:15)(cid:2)(cid:15){x2}(cid:15){xn}{x1}(cid:3)(cid:4)(cid:16)(cid:3) x ∈ {x1, . . . , xn} − {u}{x},(cid:16)+ mσ (C) = 1.{v} = argmax m(cid:16)(cid:15)mσ{v}(cid:15){u}+ mσ(20)(21)Clearly mσ is a triplet mass function and it is also referred as a two-point mass function. Based on this notation, for-mula (2) is simply rewritten as formula (22)(cid:16), mσ(cid:4)(cid:16), mσ (C)(cid:15){v}(cid:15){u}(cid:2)mσϕi(d) =1 (cid:2) i (cid:2) M.(22)For simplicity, we write ϕi(d) = {m({u}), m({v}), m(C)}.With the above definition of a triplet, it is easy to illustrate that it meets the two conditions given in Definition 1. Wenow show that the mass m(C) indeed represents ignorance.Given a triplet (cid:3){u}, {v}, C(cid:4), and m({u}) + m({v}) + m(C) = 1, we let A = {u} and A = {v}, then we havebel( A) =(cid:5)m( X) = m(cid:15){u}(cid:16),pls( A) =X⊆{u}(cid:5)X∩{u}(cid:10)=∅m( X) = m(cid:16)(cid:15){u}+ m(C),ignorance( A) = pls( A) − bel( A) = m(cid:16)(cid:15){u}+ m(C) − m(cid:16)(cid:15){u}= m(C).From the above formulation it can be seen that u gives the maximum of our quantitative judgments, representing theclass with the highest degree of confidence (support) in a list of decisions. It implies that the decision has a strong possibilityof being correct. v represents the class with the second highest degree of confidence in the decision list. This decision isless likely to be correct than u. However, its support is important in combining decisions since making a maximal selectionmay lose valuable information contained in the discarded class labels. Moreover, including v could avoid deterioration ofthe combined performance caused by a single error of a classifier. Apart from the support for u and v, a certain amountof confidence still remains unassigned, which is assigned to the entire set of classes—it is committed to the frame ofdiscernment C . The triplet evidence structure can be intuitively interpreted as follows. If a classifier cannot successfullyassign an instance to the correct class in two occasions, then it is not likely for the classifier to classify this instancecorrectly at all. We use the ignorance concept to capture important information in such a situation.1740Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–1751To develop formulae for combining two triplet mass functions, we need to consider the relation between two pairs ofsingletons in any two triplets.Suppose we are given two triplets (cid:3){x1}, { y1}, C(cid:4) and (cid:3){x2}, { y2}, C(cid:4) where xi, yi ∈ C (i = 1, 2), and the associated tripletmass functions m1 and m2. The enumerative relations between any two pairs of focal points {x1}, { y1} and {x2}, { y2} areillustrated below:1. Two focal points equal: 1) if {x1} = {x2} and { y1} = { y2}, then {x1} ∩ { y2} = ∅ and { y1} ∩ {x2} = ∅; 2) {x1} = { y2} and{ y1} = {x2}, then {x1} ∩ {x2} = ∅ and { y1} ∩ { y2} = ∅. In this case, the combination of two triplet functions involves threedifferent focal elements.2. One focal point equal: 1) if {x1} = {x2} and { y1} (cid:10)= { y2} then {x1} ∩ { y2} = ∅, { y1} ∩ {x2} = ∅ and { y1} ∩ { y2} = ∅; 2) if{x1} (cid:10)= {x2} and { y1} = { y2}, then {x1} ∩ { y2} = ∅, {x2} ∩ { y1} = ∅ and {x1} ∩ {x2} = ∅; 3) if {x1} = { y2} and { y1} (cid:10)={x2} then {x1} ∩ {x2} = ∅, { y1} ∩ {x2} = ∅ and { y1} ∩ { y2} = ∅; 4) if { y1} = {x2} and {x1} (cid:10)= { y2} then {x1} ∩ {x2} = ∅,{x1} ∩ { y2} = ∅ and { y1} ∩ { y2} = ∅. In this case, the combination of two triplet functions involves four different focalelements.3. Totally different focal points: if {x1} (cid:10)= {x2}, { y1} (cid:10)= { y2}, {x1} (cid:10)= { y2} and { y1} (cid:10)= {x2}, then {x1} ∩ {x2} = ∅, { y1} ∩ { y2} = ∅,{x1} ∩ { y2} = ∅ and { y1} ∩ {x2} = ∅, so the combination involves five different focal elements.The above three different cases require different formulae for combination. In cases 2) and 3), the combinations ofmultiple triplet functions cannot be iteratively performed since we are interested only in combining two-point focuses.However, by applying the outstanding rule (see Definition 6), the combined results of any two triplets can be transformedto a triplet mass function. In the following sections, we seek general formulae for combining triplet mass functions basedon the three different cases and present our combination algorithm.5.1. Two focal points equalConsidering the case where two focal singletons {x1}, { y1} in one triplet are equal to {x2}, { y2} in another triplet, i.e.,x1 = x2, y1 = y2 (x1 (cid:10)= y1) and xi, yi ∈ C (i = 1, 2), we have two triplet mass function m1 and m2 along with(cid:16)(cid:16)(cid:15){x}(cid:15){x}m1m2(cid:16)(cid:16)(cid:15){ y}(cid:15){ y}+ m1+ m2+ m1(C) = 1,+ m2(C) = 1.First, we need to show the combination of m1 ⊕ m2 does exist and then establish formulae to compute their combination. Toensure the combinability of triplet mass functions, we need only show under what conditions two triplet mass functions m1−1 (cid:10)= 0. By using formula (3) to combine m1 and m2, we canand m2 are not in conflict, i.e., the normalization factor K−1 must be greater than zero, i.e.,obtain K0 < 1 − m1({x})m2({ y}) − m1({ y})m2({x}), thus m1({x})m2({ y}) + m1({ y})m2({x}) < 1. Under this condition, we establish theformulae for computing the combination of two triplet mass functions below:−1 = 1 − m1({x})m2({ y}) − m1({ y})m2({x}), to make K−1 (cid:10)= 0 to be true, K(cid:16){x}= K(cid:15)(cid:16)(m1 ⊕ m2)m2(cid:15)(cid:16)(m1 ⊕ m2)m2(m1 ⊕ m2)(C) = K m1(C)m2(C),(cid:15){x}(cid:15){ y}(cid:17)m1(cid:17)m1= K{ y}(cid:16)(cid:16)(cid:15){x}(cid:15){ y}(cid:15){x}(cid:15){ y}(cid:16)m2(C) + m1(C)m2(cid:16)m2(C) + m1(C)m2(cid:15){x}(cid:15){ y}(cid:16)(cid:18)+ m1(cid:16)+ m1,(cid:16)(cid:18),where−1 = 1 −K(cid:5)X∩Y =∅m1( X)m2(Y ) = 1 − m1(cid:15){x}(cid:16)m2(cid:15){ y}(cid:16)(cid:15){ y}(cid:16)m2(cid:15){x}(cid:16).− m15.2. One focal point equal(23)(24)(25)(26)We now consider the case where given two triplet mass functions m1 and m2 and two pairs of singletons {x}, { y} and{x}, {z} ( y (cid:10)= z and x, y, z ∈ C), a focal element in one triplet is equal to one in another triplet. Following the proceduregiven in Section 5.1, we can show that m1, m2 are combinable if and only if the following condition is held:(cid:15){x}(cid:15){ y}(cid:15){ y}(cid:15){x}(cid:15){z}(cid:15){z}< 1.(cid:16)(cid:16)(cid:16)(cid:16)m2(cid:16)m2(cid:16)m2+ m1+ m1m1Thus by the orthogonal sum operation, the general formulae for computing the combination of any two triplet mass(cid:16){x}functions are given below:(cid:17)= Km1(cid:15)= K m1{ y}= K m1(C)m2(cid:15)(cid:15)(m1 ⊕ m2){x}(cid:15)(cid:16)(m1 ⊕ m2)m2(C),(cid:15)(cid:15){z}(m1 ⊕ m2)(m1 ⊕ m2)(C) = K m1(C)m2(C),(cid:16)m2(cid:15){x}{ y}(cid:16)(cid:16),{z}(cid:16)(cid:16)(cid:15){x}(cid:16)m2(C) + m1(C)m2(cid:15){x}(cid:16)(cid:18),+ m1(27)(28)(29)(30)Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–17511741where(cid:16)(cid:15)(cid:16)K− m1(cid:15){z}(cid:15){x}(cid:16)m2(cid:16)m2(cid:15){ y}−1 = 1 − m1(cid:16)m2(31)It is noted that the new mass function m1 ⊕ m2 is no longer a triplet mass function. It now involves four different focalelements {x}, { y}, {z} and C . For more than two triplet functions, the combining process cannot proceed iteratively since weare interested only in two-point focuses. However, by applying the outstanding rule, the combined result can be transformedto a new triplet mass function. We detail the computational steps below.By Definition 6, we have a new function (m1 ⊕ m2)σ as follows:(cid:15){ y}(cid:15){x}− m1{z}(cid:16).(m1 ⊕ m2)σ+ (m1 ⊕ m2)σ+ (m1 ⊕ m2)σ (C) = 1.(cid:16)(cid:15){x(cid:16)}(cid:16)(cid:15){ y(cid:16)}(cid:15){ y}(cid:16)= f ( y); m1 ⊕ m2(cid:16)(cid:15){z}= f (x).(cid:16)m1 ⊕ m2To obtain (m1 ⊕ m2)σ , we assume(cid:15){x}Then for focal element {x(cid:15){x= f (x); m1 ⊕ m2(cid:16)} we have(cid:16)= f (x(cid:16)} = argmax( f (x), f ( y), f (z)).m1 ⊕ m2where {x(cid:16)}),(cid:16)For focal element { y(cid:16)} we have(cid:16)(cid:16)(cid:15){ y(cid:16)}m1 ⊕ m2= f ( y(cid:16) = argmax( f (t) | t ∈ ({x, y, z} − {x),(cid:16)})).where yFor focal element C we have(cid:16)(m1 ⊕ m2)σ (C) = 1 − f (x) − f ( y(cid:16)).5.3. Completely different focal points(32)(33)(34)Finally, let us examine the case where no focal element is common to two triplets. As indicated previously, the combi-nation of two such triplet mass functions will involve five different focal elements. Let m1, m2 be two triplet functions, and{x}, { y} and {u}, {v} (x (cid:10)= y, x (cid:10)= u and y (cid:10)= v, and x, y, u, v, y ∈ C) be two pairs of focal elements along with the followingconditions:(cid:15){x}(cid:15){u}+ m1(C) = 1,+ m2(C) = 1.(cid:15){ y}(cid:15){v}+ m1+ m2m1m2(cid:16)(cid:16)(cid:16)(cid:16)Following the patten of the previous sections, it can be shown that m1, m2 are combinable if and only if the followingconstraint holds:(cid:15){x}(cid:16)m2(cid:16)(cid:15){u}(cid:15){x}(cid:16)m2(cid:15){v}(cid:16)+ m1(cid:15){ y}(cid:16)m2(cid:15){u}(cid:16)(cid:15){ y}(cid:16)m2(cid:15){v}(cid:16)+ m1+ m1< 1.m1Given the above condition we derive the formulae for computing each focal element below:(cid:16)(cid:16)(cid:15)(m1 ⊕ m2){x}(cid:15){ y}(m1 ⊕ m2)(cid:16)(cid:15)(m1 ⊕ m2){u}(cid:15){v}(m1 ⊕ m2)(cid:16)(cid:15){ y}(cid:15){ y}= K m1= K m1= K m1(C)m2= K m1(C)m2(cid:16)m2(C) = f (x),(cid:16)m2(C) = f ( y),(cid:15)= f (u),{z}(cid:15)= f (v),{z}(cid:16)(cid:16)where(cid:5)−1 = 1 −Km1( X)m2(Y )X∩Y =∅(cid:15){x}= 1 − m1(cid:16)m2(cid:15){u}(cid:16)(cid:15)(cid:16)m2(cid:15){v}(cid:16){x}− m1(cid:15){ y}(cid:16)m2(cid:15){u}(cid:16)(cid:15){ y}(cid:16)m2(cid:15){v}(cid:16),− m1− m1(35)(36)(37)(38)(39)However, the same situation occurs as in Section 5.2, the combination of m1, m2 is no longer a triplet mass function, itnow involves five focal elements {x}, { y}, {u}, {v}, C , therefore further combinations with more triplet functions are invalidin this context. To obtain a new triplet mass function, there is a need to apply the outstanding rule to the combined results.More specifically, by Definition 6, we can obtain a new function (m1 ⊕ m2)σ as follows:(m1 ⊕ m2)σ+ (m1 ⊕ m2)σ+ (m1 ⊕ m2)σ (C) = 1.(cid:16)(cid:15){x(cid:16)}(cid:16)(cid:15){ y(cid:16)}Then for focal element {x(cid:15){xm1 ⊕ m2(cid:16)= f (x(cid:16)}(cid:16)),(cid:16)} = argmax( f (x), f ( y), f (u), f (v)).where {x(cid:16)} we have(40)1742Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–1751//combine them by formulae (23)–(26)(cid:16) ∈ T(cid:16)} doct ← ct ⊕ tif (two focuses equal in t and ct) then {endifelse if (one focus equal in t and ct) then1 set T a set of triplet mass functions2 ct: holds the combined results of triplet mass functions3 set ct ← t4 for each t ∈ T \ {t5678910111213141516 endfor17 return ctct ← ct ⊕ tct ← ctσct ← ct ⊕ tct ← ctσendelseifelseendelse//combine them by formulae (27)–(31)//transform it a new triplet by formulae (32)–(34)//combine them by formulae (35)–(39)//transform the combined results to a new triplet by formulae (40)–(42)Algorithm 1. Combine multiple triplet mass functions.For focal element { y(cid:16)} we have(cid:16)(cid:16)(cid:15){ y(cid:16)}m1 ⊕ m2= f ( y),(cid:16) = argmax({ f (t) | t ∈ ({x, y, u, v} − {x(cid:16)})}).where yFinally, for focal element C we have) − f ( y(cid:16)(m1 ⊕ m2)σ (C) = 1 − f (x(cid:16)).(41)(42)We have shown that any two triplet mass functions are combinable if the conditions hold and we have established theformulae for computing the combination of two triplet mass functions. These formulae provide a way to not only computethe combinations of two triplet mass functions efficiently but also help us develop a general algorithm for combiningmultiple triplet mass functions in a complex situation where the evidence sources of triplets are independent of each other.Suppose we are given M triplet mass functions m1, m2, . . . , mM , by using the algorithm below they can be combined inany order due to Dempster’s rule being both commutative and associative. That means we can arrange these triplet massfunctions into a certain order based on three cases mentioned above. By repeatedly applying the outstanding rule at eachcomputational step of combining two triplet mass functions, the results can be transformed to a new triplet mass function.Formula (43) is a pairwise orthogonal sum calculation for combining any number of triplets which can be performed by theabove algorithm. Its time complexity is approximately O (2 × |C|) and the final decision is a class with the largest degree ofsupport among all the classes.m = m1 ⊕ m2 ⊕ · · · ⊕ mM =[m1 ⊕ m2] ⊕ · · · ⊕ mM.(43)(cid:17)(cid:17)· · ·(cid:18)(cid:18)5.4. Focusing on more points—quartetIn the previous sections we have presented the formulation of triplet functions and developed formulae for combiningthem. Similarly, we can consider three-point focuses, four-point focuses, or more focuses. In general, when a mass functionhas more than four focal singletons, we can use the outstanding rule σ to focus on 3 points in terms of quartet. Let m be amass function with focal singletons {x1}, {x2}, . . . , {xn}, n (cid:3) 4, and n (cid:2) |C|. Then the focusing operator σ is used to obtain mas follows:(cid:15)mσ{u}+ mσ (C) = 1,(cid:15){v}(cid:15){z}+ mσ+ mσ(cid:16)(cid:16)(cid:16)where{u} = argmax m(cid:15)(cid:2)(cid:15)(cid:15)(cid:2)(cid:15)(cid:2)(cid:16)(cid:4)(cid:16),(cid:16), m(cid:16), . . . , m(cid:15)(cid:15){x1}{xn}{x2}(cid:4)(cid:16){x} | x ∈ {x1, . . . , xn} − {u},(cid:4)(cid:16){x} | x ∈ {x1, . . . , xn} − {u, v},(cid:16)(cid:15)(cid:15){v}{u}(cid:15){z}− mσ− mσ(cid:16)(cid:16){v} = argmax m{z} = argmax mmσ (C) = 1 − mσ(47)and {u}, {v}, {z} are three-point focuses. The structure of the quartet is conceptually simple, but it has the added advantagethat it can be used to handle the case where the classifier results are diverse. For this case, the theoretical analysis isclearly more complicated than that of the triplet structure. In addition to the properties of mass function and ignorance,the corresponding analysis also needs to address, in each case, the situation where the focuses are ordered by their massesfor each of the two pieces of evidence being combined. For example, we need to consider the cases where all three focusesare the same, two focuses are identical, where just one focus is shared, and where there are no focuses in common. Moredetails about the extension from triplet to quartet can be found in [6].(44)(45)(46)Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–17511743Table 1General description of the datasetsDatasetInstanceClassannealaudiologybalancecarglassautosirisletterheartsegmentsoybeanwineZoo798200625172821420515020 00030315006831781016233476326571937AttributeCategoricalNumerical32694601000803501560009154165190132Table 2General description of the thirteen learning algorithmsNoClassifierDescription0123456789101112AODNaiveBayesSMOIBkIB1KStarDecisionStumpJ48RandomForestDecisionTableJRipNNgePARTPerform classification by averaging over all of a small space of alternative naive-Bayes-like models that have weaker(and hence less detrimental) independence assumptions than naive BayesThe Naive Bayes classifier using kernel density estimation over multiple values for continuous attributes, instead ofassuming a simple normal distributionSequential minimal optimization algorithm for training a support vector classifier using polynomial or RBF kernelsA instance-based learning algorithm. It uses a simple distance measure to find the training instance closest to thegiven test instance, and predicts the same class as this training instanceThe IBk instance-based learner with K = 1 nearest neighbors, in order to offset KStar with a maximally local learnerThe K instance-based learner using all nearest neighbors and an entropy-based distanceBuilding and using a decision stump, but it is not used in conjunction with a boosting algorithmDecision tree induction, a Java implementation of C4.5Constructing random forests for classificationA decision table learnerA propositional rule learner—a Java implementation of Ripper. It repeats incremental pruning to produce error re-ductionNearest neighbor-like algorithm using non-nested generalized exemplarsGenerating a PART decision list for classification6. Experimental evaluation6.1. Experimental settingsIn our experiments, we used thirteen data sets downloaded from the UCI machine learning repository [3]. All the selecteddata sets have at least three or more classes as required by the evidential structures. The details about these data sets canbe found in Table 1.For generating individual (base) classifiers, we used thirteen learning algorithms which are taken from the WaikatoEnvironment for Knowledge Analysis (Weka) version 3.4 (see Table 2). These algorithms were simply chosen on the basisof the performance over three data sets which were randomly picked. They can make up various ensembles of classifiers.Parameters used for each algorithm were set at the default settings. Detailed description of these algorithms can be foundin [55].To reflect the ensemble performance faithfully and to avoid overfitting to some extent, the experiments were performedon a three partition scheme using a ten-fold cross validation. We therefore divided each of the data sets into 10 mutuallyexclusive sets. For each fold, after excluding the test set, the training set was further subdivided into 70% for a new trainingset and 30% for a validation set. Apart from evaluating the performance of individual classifiers, the validation set wasused to select the best combination of classifiers. The performance of combining selected classifiers using the DS and MV(majority voting) combining schemes was evaluated on the testing set.Seven groups of experiments are reported here, which were carried out individually and in combination across all thethirteen data sets. These are:• evaluating the performance of the 13 algorithms shown in Table 2,• experimenting with various combinations of individual classifiers using DS, in which the classifier outputs are repre-sented by triplet and quartet functions, respectively,1744Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–1751Fig. 3. An example: the combination of two individual classifiers ϕ1 and ϕ2 with the triplet structure in a fold of a ten-fold cross validation, T1 is a numberof instances within a fold of training data.• examining the performance of combining 13 individual classifiers in the three different orders of decreasing, corre-sponding increasing and increasing using DS,• experimenting with combinations of individual classifiers represented by the dichotomous structure using DS, wherethe dichotomous mass functions were defined on the basis of the performance of classifiers in terms of recognition,substitution and rejection rates [49],• conducting experiments on combining the individual classifiers represented by the simplet structure using DS. Thesimplet mass functions were defined based on Rogova’s method (see Section 4.2; here classifiers were generated by thelearning algorithms shown in Table 2 rather than neural networks),• experimenting with combinations of classifiers using DS, where classifier outputs are represented by the full list ofdecisions (contenders),• experimenting with combinations of individual classifiers using MV, in which the individual outputs are single classlabels.It is noted that the ensemble construction involves 213 combinations of 13 individual classifiers for one evidential struc-ture with one data set in one fold. The computational cost for all the structures using a ten-fold cross validation requires213 × 13 × 5 × 10 combinations in total. Instead of exhausting all the combinations of classifiers, we ranked all the individualclassifiers based on their performance and then combined them in decreasing order as suggested in [1,8]. For example, wetook the best individual classifier, and then combined it with the second best, the third best, and so forth, until we achievedthe best accuracy of the combined classifiers. During the course of combination, a hybrid order (and non consecutive or-dering) was also involved in combining classifiers in order to find out the best combination of classifiers. Fig. 3 presents anexample of combining two individual classifiers where the classifier outputs are represented by triplets.To compare the classification accuracies between the individual classifiers and the ensemble classifiers across all the datasets, we employed ranking statistics in terms of the win/ draw/ loss (W/D/L) record used by Webb [53]. The win/draw/lossrecord presents three values, the number of data sets for which classifier A obtained better, equal, or worse than classifier Bwith respect to classification accuracy. All collected classification accuracies were measured by the averaged F -measure[55]. A paired t-test across all these domains was also carried out to determine whether the differences between the baseclassifiers and combined classifiers (ensembles of classifiers) are statistically significant at the 0.05 level.6.2. The basics of Kappa (κ ) statisticsTo examine the reliability of the ensemble performance, we performed a κ (Kappa) statistic analysis on the extent (level)of agreement between the combined classifiers under the different evidential structures using DS, and also the extent of theagreement between majority voting and Dempster’s rule in combining the base classifiers.The κ statistic is the most widely used pairwise method to measure the level of agreement (or disagreement) betweenraters/classifiers [29]. It can be thought of as chance-corrected proportional agreement [20]. Formally, given two classi-fiers ϕ1 and ϕ2 and a testing data set T , we can construct a global contingency table where entry E i j contains the numberof instances x ∈ T for which ϕ1 = i and ϕ2 = j. If ϕ1 and ϕ2 are identical on the data set, then all non-zero counts willappear along the diagonal of the table, otherwise there will be a number of counts off the diagonal. Now we defineμ1 =μ2 =(cid:9)Li=1 E ii|T |(cid:19)L(cid:5)L(cid:5)i=1j=1,(cid:20),E i j|T |×L(cid:5)j=1E ji|T |Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–17511745Fig. 4. Comparison of running time among simplet, triplet quartet and dichotomous structures.where μ1 is the probability that two classifiers agree and μ2 is a correction term for μ1, estimating the probability thatthe two classifiers agree simply by chance. Then the κ statistic is defined as follows:κ = μ1 − μ21 − μ2κ = 0 when the agreement of two classifiers equals that expected by chance, κ = 1 when two classifiers agree on all thetesting instances, and negative values of κ mean that an agreement is less than expected by chance.6.3. Special cases for triplets and quartetsIn our experiments, to ensure that Dempster’s rule is properly applied to combine triplet mass functions (and quartetmass functions), i.e., making sure N (cid:10)= 0 as required in Definition 3, adjustments have been made for the following threespecial cases.The first one is that given two mass functions m1 and m2 along with two pairs of singletons {x1}, { y1} and {x2}, { y2}, andthe whole set C , if x1 (cid:10)= x2, y1 (cid:10)= y2, xi (cid:10)= y j (i, j = 1, 2), and m1({x1}) = 1 or m1({ y1}) = 1 and m2({x2}) = 1 or m2({ y2}) = 1,then the intersection of m1 and m2 is committed to the empty set ∅, and consequently the condition of N (cid:10)= 0 does nothold. Thus it is necessary to redistribute masses over the triplets where the uncertainties lie. In this situation, we discountthe masses of xi and y j (i, j = 1, 2) to mi(C) (i = 1, 2) by a small value α which has been defaulted as 0.001 in ourexperiments.The addition of α represents the uncertainty involved in a classification process, which can be justified in two ways:1) generating classifiers is an approximation process, so that the class conditional probabilities estimated by classifiers isnot of 100% certainty; 2) α will play a role in improving the combined performance (or at least, in not deteriorating thecombined effect).The second case is where, as usual, m is a mass function derived from a classifier output, {x} and { y} are a pair ofsingletons and C is the whole set of classes. If the classifier output for an instance is a nil one, this means the classifieris not able to assign a class label to the instance, thus m({x}) = m({ y}) = 0. For such a situation, we reallocate 1 to m(C).Making m(C) = 1 can be regarded as the representation of uncertainty in classifying the instance by the classifier; in fact, itonly ensures N (cid:10)= 0 when m is combined with another mass function mand it does not affect the value of m ⊕ mThe last case is where we are given a resulting triplet mass function m together with singletons {x}, { y}, {z}, and m({x}) =m({ y}) = m({z}). To approximate m as a new triplet mass function or determine the best focus, we have no criterion foridentifying the best choice. All we can do is pick two focuses up at random for constructing a triplet, or randomly take oneas a final decision..(cid:16)(cid:16)Similar treatments have also been administered for the special cases of quartet mass functions.6.4. Experimental results6.4.1. Run timeThe first experimental result is the comparison of running time required in combining different evidential structuresusing DS. As expected, combining simplets was more efficient than the others as there is less computation involved. Theempirical results show the time required for combining triplet functions is 3.5% longer than that of combining simplets.The time required for combining dichotomous functions is 119.6% longer than that of simplets on average, and the time forcombining quartets is 169.1% longer than that for simplets on average. The comparative results are illustrated in Fig. 4.From the above results, it can be seen that although the triplet and dichotomous structures both have three focal ele-ments, the computational time required for combining triplet functions is significantly less than that required for combining1746Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–1751Table 3The classification accuracy of the best base classifier, the best combined classifiers based on the structures of simplet, triplet, quartet, dichotomous structureand fullist using DS and MV over 13 data setsDatasetsAnnealAudiologyBalanceCarGlassAutosIrisLetterHeartSegmentSoybeanWineZooAverageW/D/LSig winIndividualSimplet80.2348.6765.6789.6265.3677.5995.3392.0535.4896.6995.8998.9090.6279.3981.5753.6763.1795.4064.9178.4696.6792.4136.0297.4896.92100.0093.6180.7911/0/27Triplet81.5757.4463.1794.2966.8179.1796.6792.9137.0997.2896.69100.0093.6181.3012/0/17QuartetDichotomous79.7757.4464.4496.9664.9178.0896.6792.5437.0396.6896.88100.0093.6181.1510/0/3580.6851.9765.6791.9266.2678.7896.6793.4135.3797.7496.8598.9093.6180.6010/2/15MV81.1454.3062.7291.7566.6977.9496.6792.7734.3796.5596.1798.9793.6180.2810/0/34Fullist69.8849.3221.6890.0362.7566.8060.7168.3834.2688.4091.1196.7064.3966.49––Table 4The extent of agreement among the best combined classifiers under the four structures along with the agree-ment between combination methods DS and MVTriplet (DS)MVSimplet0.93250.9427Quartet0.93960.9273Dichotomous0.93490.9432Triplet–0.9437Average0.93570.9392dichotomous functions. This is mainly due to the second element of the dichotomous structure, i.e., it is a subset of thewhole set C which contains only one class less than the whole set of classes C . Examining the calculation process ofDempster’s rule, it is not difficult to find that for the optimized case, combining two dichotomous mass functions requires(|C| − 2)2 more computations than combining two triplet mass functions using DS.6.4.2. Performance summaryThe seven groups of experimental results are summarized in Table 3. The first column lists all the data sets used, thesecond column gives the classification accuracies of the best individual classifiers and the rest of the columns represent theaccuracies of the best combined classifiers using DS or MV over the data sets. If the difference between the best combinedclassifier and the best individual or base classifier on the same data set is statistically significant, then the larger of the twois shown in bold.The bottom of the table provides summary statistics comparing the performance of the best base classifiers with thebest combined classifiers across the data sets. From this summary, it can be observed that the accuracy of the combinedclassifiers based on the triplet structure using DS is better than any of the five others on average. It has more wins thanloses over the simplet, quartet, dichotomous structure and the best combined classifiers using MV compared to the bestindividual classifiers. This conspicuous superiority is further supported by the number of statistically significant wins—thetriplet has two more than the quartet and dichotomous structure, and three more than MV.6.4.3. κ statisticsFor examining the reliability of the ensemble performance, we selected six data sets (Anneal, Audio, Car, Iris, Wine,Zoo) where the ensemble performance is statistically significant and better than that of the best individuals, and carriedout a pairwise analysis on the level of agreement between the best combined classifiers along with the level of agreementbetween DS and MV on the testing data. Table 4 shows the statistical results averaged on the six data sets. Within the table,the first row consists of the four best combined classifiers, denoted by Simplet, Quartet, Dichotomous structure and Triplet,each of which corresponds to the six data sets. The first column names the two best combined classifiers with DS and MV,simply denoted by Triplet (DS) and MV, respectively. They are associated with the six data sets as well. Each remaining cellof the table is an averaged κ value, which represents the level of agreement between a pair of the classifiers on classifyingthe instances of the six data sets. For example, in the cell of (Triplet (DS), Simplet), 0.9325 is the averaged κ value of thebest combined classifiers of Triplet and Simplet which agree on classifying the instances across the six data sets. The κvalues presented in Table 4 indicate that the agreement on classifying the instances by the pairs of classifiers is substantialaccording to the rough guide provided in [20]. This establishes that the performance of these combined classifiers is reliable.Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–17511747Fig. 5. Performance of different numbers of classifiers in combination with different orders: decreasing, corresponding increasing and increasing.6.4.4. Order of combining classifiers under the tripletAs explained in Sections 5.2 and 5.3, combining two triplet mass functions does not in general result in a triplet massfunction. The combined results may need to be approximated to a new triplet using the focusing operator. However suchan approximation may make the combination of triplets no longer meet the associative property of Dempster’s rule. Thusdifferent orders of combination of classifiers may lead to the different combined performance.To better understand how the chosen order of classifiers affects the combined performance in ensemble construction,we have conducted a further experiment on combining classifiers with three different orders of decreasing, correspondingincreasing ( c-increasing) and increasing. The combination process is as follows. We ranked the 13 best individual classifiersin decreasing and increasing orders, and then consecutively combined them one by one using Dempster’s rule. With respectto the c-increasing order, it is a reverse process of each of the corresponding decreasing combinations. For instance, the firstdecreasing combination is to combine the best classifier with the second best, the second is to combine the best classifierwith the second best and then with the third best; this process will be repeated until all decreasing combinations arecompleted. The reverse process of the decreasing combination is, correspondingly, to combine the second best classifierwith the first, to combine the third best classifier with the second and then with the best, and so forth.Fig. 5 illustrates an effect comparison of three orders in combining classifiers under the triplet. It can be observed thatthe combined classifiers with the c-increasing order outperforms the other two, and the performance of the increasingcombination is worst among the three orders of combination. This suggests that combining the better classifiers would out-perform combining the worse classifiers. Consider the c-increasing order. It should be noted that although its performanceis better that of the decreasing combination, this mainly occurs on the combination of 9 classifiers and more; for the com-bination of 3 to 8 classifiers, the decreasing performance tends to be the same as that of the c-increasing. This experimentalresult leads to the conclusion that the order of the classifiers in combination has an impact on the performance of thecombined classifiers, but it is not dramatic. For both the decreasing and the c-increasing orders, the smaller the number ofclassifiers to be combined, the less the impact of the combination order.6.4.5. Ensemble sizeFig. 6 presents the sizes of the best combined classifiers across all the data sets. With the different structures, theconstruction of these ensembles involve 2–7 classifiers, and most of the ensembles are composed of only two classifiers.This result is consistent with some previous studies [1,8,19], but different from others [36,38] where experiments showedthat ensemble accuracy increased with ensemble size and the performance levels out with ensemble sizes of 10–25. Thesizes of ensembles of classifiers which are generated using different learning algorithms to operate on a single data set arenot necessarily the same as those of the combined classifiers constructed using a single learning method to manipulatedifferent portions of features or instances.6.4.6. Contribution to the ensemblesFig. 7 presents the contribution to the ensemble construction of each individual classifier with the structures of simplettriplet, quartet and dichotomous structure across all the domains. There are eleven aggregated columns with four differentcolors in the figure. Each column represents the number of a classifier which contributes to the construction of the thirteenbest combined classifiers (corresponding to thirteen data sets), and each color presents an evidential structure. For example,the first column indicates that AOD occurs six times over the thirteen ensembles (thirteen data sets) with the simplet andtriplet structures, and seven times over the thirteen ensembles with the quartet and dichotomous structure. It is observedthat the classifiers that contribute to four or more ensembles with four structures are AOD, SMO, IBk and KStar. Of these,AOD is not the best performing individually, but it plays a more important role than the others in constructing the effectiveensembles.1748Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–1751Fig. 6. Ensemble size: number of individual classifiers involved in the best combined classifiers across all the data sets with the evidential structures ofsimplet, triplet, quartet and dichotomous structure.Fig. 7. Number of the individual classifiers (0: AOD; 1: NaiveBayes; etc. see Table 2) occurring in all thirteen combined classifiers across all the thirteendata sets with different structures of simplet, triplet, quartet and dichotomous structure.7. DiscussionThe structure of simplet consists of two elements. The first is a decision in a list of decisions and the simplet massrepresents the degree of support for that decision, which is derived from a decision profile using a cosine function. Thesecond is the whole list of decisions C , its mass representing the uncertainty of the singleton element. With this structure,a list of decisions is associated with a list of simplet mass functions where each of the simplet functions corresponds toone of the decisions, and both the lists share the same order after ranking. It is easy to illustrate that the combined effectof two simplet functions will be highly affected by the larger of the simplet masses.Consider the combination of two simplet mass functions m1 and m2 along with two singletons {x} and { y} in thecase where x and y are not equal. By using formula (3) to combine m1 and m2, x is the better supported decision ifm1({x}) > m2({ y}), otherwise y will be better, and m1(Θ) and m2(Θ) do not affect the combination effect. Thus, we findthat which of {x} and { y} becomes the better supported decision depends merely on the mass values of {x} and { y}. Thiseffect can be generalized to combining two simplet classifiers ϕ1 ⊕ ϕ2; that is, the performance of ϕ1 ⊕ ϕ2 is dominated bya single simplet classifier ϕ1 or ϕ2 which repeatedly provides larger mass functions m1 or m2.Although the way of deriving mass values for simplets is different from that for triplets and quartets, in the broadestsense, the triplet and quartet can be regarded as extensions of the simplet structure. The key difference between both thetriplet and the quartet and the simplet is that the former makes use of a wide range of information that is contained in thesecond and third best decisions in combining classifiers. The performance of combining two triplet classifiers, for example,will be determined by the first and second elements along with ignorance. We conjecture that the use of more informationplays an important role in overcoming the problem we identified earlier—that a single error produced by a single classifierwhich repeatedly provides high confidence values of classes can occur when combining simplets.Now we look at a theoretical justification of our claim. We state formally the conditions for the first or second decisionin either of two triplets to become the better supported decision. Assume that two triplet functions m1 and m2 fall into theY. Bi et al. / Artificial Intelligence 172 (2008) 1731–17511749category where a pair of singletons {x1}, { y1} is equal to a pair of {x2}, { y2}, i.e., x1 = x2 and y1 = y2 (see Section 5.1). Byusing formulae (23)–(26), we have the following inequality when x is the better choice:(cid:15){x}(cid:16)m2(cid:15){x}(cid:16)m1(cid:15){x}(cid:16)m2(C) + m1(C)m2(cid:15){x}(cid:16)+ m1(cid:15){ y}(cid:16)m2(cid:15){ y}(cid:16)(cid:15){ y}(cid:16)m2(C) + m1(C)m2(cid:15){ y}(cid:16).+ m1> m1Substituting for C in formula (48) and rearranging it, we have the condition for the better support for x:(cid:16)(cid:15){x}m1> 1 −[1 − m1({ y})][1 − m2({ y})][1 − m2({x})].Likewise we can derive the condition for y being the better supported decision:(cid:16)(cid:15){ y}m2> 1 −[1 − m1({x})][1 − m2({x})][1 − m1({ y})].(48)(49)(50)We can obtain the conditions for the other two cases in two triplets in a similar manner.Formulae (49) and (50) present two conditions for determining which of x and y is the better choice. The first conditionindicates that, for example, when x is in the second position of a list of decisions, it can be ranked as the first decision whentwo triplets are combined as long as this condition is met. This effect provides an insight into the process of combiningtriplet classifiers where a single classifier cannot dominate the combined performance and the ignorance derived also playsan important role in deciding the best supported decisions. This is one explanation of the superior performance of thetriplet over the simplet. A similar analysis for the case of combining quartets can be carried out in the same way, obtaininga possible account of when the quartet outperforms the simplet.With respect to the dichotomous structure, its drawback is in its way of measuring evidence, where it ignores the factthat classifiers normally do not have the same performance on different classes, which could cause a deterioration in theperformance of the combined classifiers (see Section 4.1).It is not a surprise that the performance of combining classifiers in the form of triplets and quartets is better than thatof combining classifiers with a full list of decisions. The reason for this is that different individual classifiers produce variousdistributions of class-conditional probabilities for all the classes. The classes can have various predicted values in the rangebetween zero and one. When two lists are combined using the orthogonal sum operation, if ϕ1 and ϕ2 produce two lists ofdecisions with a similar distribution, then many non-zero values will appear along the diagonal of an intersection table (forthe orthogonal sum calculation). Otherwise there will be a large number of values off the diagonal. In the latter situation,the combination of the two lists is committed to more conflicting class labels. Examining the calculation of orthogonalsums closely, we find that the conflict incurred in combining two classifiers in the form of a full list is larger than that incombining two triplet classifiers. Such conflict could thus result in poor combined performance of classifiers, using a fulllist. This finding is also consistent with the result illustrated in Fig. 5 and the previous studies where the combination ofdecisions with the lower degrees of confidence may not contribute to an increase of combined performance of classifiers,but only make the combination of decisions more complicated [48].8. Independence assumptionMaking an independence assumption for a set of variables or a set of attribute values is a common practice in manylearning tasks, such as Bayesian belief networks and naive Bayes classifier [37]. Such an assumption dramatically reduces thecomplexity of learning classifiers and makes the computational process more tractable. This is also true in other applications,for instance, in the present context of modelling classifier outputs as independent bodies of evidence. The independenceassumption is of practical value, but there is little information available about whether it has substantially deteriorated theeffectiveness of classifiers in many applications.As mentioned in Definition 3, one condition of using Dempster’s rule is that pieces of evidence to be combined areindependent. However the precise meaning of independence in practice is difficult to specify. In an effort to clarify this,Dempster [13] explained that “opinions of different people based on overlapping experiences could not be regarded as inde-pendent sources”. This was subsequently complemented by a statement by Denoeux [14], who said that, “non-overlappingrandom samples from a population are clearly distinct items of evidence”. In the present context, a possible argument couldbe used against independence of two pieces of evidence derived from two classifier outputs due to the fact that the twoclassifier outputs arise from the same sample instance. However, at the same time it can also be argued that two classifieroutputs are distinct, because it is not clear that two classifiers have ‘overlapping experiences’ in determining class labels foran instance.Somewhat less philosophically, the argument in favor of the independence assumption is that the classifiers involved incombination are generated by the distinct learning algorithms as shown in Table 2. These algorithms are built on differenttheories and they use their own mechanisms to search for a characterization of the data. So they do not share “experiences”in generating classifiers, nor is there correlative dependence between the internal structures of the classifiers. The inherentprocesses of producing outputs by classifiers are entirely distinct. This distinctness can be formally interpreted as follows.Let ϕ1 and ϕ2 be two distinct classifiers, for any instance d, ϕ1(d) does not logically imply ϕ2(d), and vice versa, hencethey are mutually unrelated. Consequently as a natural case, denoting e1 as the proposition corresponding to ϕ1(d) and e21750Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–1751to ϕ2(d) (see Section 4), then e1 is independent of e2 and the probability P (e1 ∧ e2) = P (e1) · P (e2) [52]. Therefore theassumption made for modelling classifier outputs as independent pieces of evidence is sensible.There is a great deal of debate about the conditions for validly applying Dempster’s rule and the precise meaning ofdistinct bodies of evidence in the literature [13,14,33,43,52], and so far there is no conclusive study to these issues. Shafer[43] pointed out that the task of sorting evidence into independent items is not always easy, and “a theory that directsus to this task is grappling with the real problems in the assessment of evidence”. Over the past decades progress inseveral applications has been sufficient to show that although the independent condition is not explicitly specified in theapplications, with an independence assumption Dempster’s rule still demonstrates its effectiveness [1,6,17,39,49].It should be emphasized that this study is focused on developing a more efficient and effective computational methodfor more convincing practical applications based on the DS theory, rather than on investigating alternative combination rulesfor dependent evidence sources and conflict management. More detailed discussions of these aspects can be found in recentstudies [14,34].9. Conclusion and future workWe have described an effective framework built on the Dempster–Shafer theory of evidence for combining multipleclassifiers and expert opinions in classification and decision making systems. We have also developed a formalism fortriplets and quartets and the formulae for combining classifiers represented inmodelling classifier outputs in terms ofthe form of triplets that can be extended to the quartet. The distinguishing aspect of our class-indifferent method fromclass-aligned methods is in selecting the prioritized class decisions to be combined and in making use of ignorance inrepresenting unknown and uncertain class decisions.A range of experiments have been carried out over the thirteen UCI benchmark data sets. Our results show that theperformance of the best combined classifiers is better than that of the best individuals on most of the data sets and thecorresponding ensemble sizes are 2–7 where the ensemble sizes of 2 and 3 take 61.5% of the thirteen best ensembles.The comparative analysis among the structures of simplet, triplet, quartet and dichotomous structure identifies the triplet asbest, and the comparison made between DS and MV shows that DS is better than MV in combining the individual classifiers.We have used the κ statistic to examine the extent of agreement of the different combination methods in decidingclasses for testing instances. The statistics reveal that the classification performance achieved by using our DS combinationmethod under our evidential structures is reliable. However, in this work we did not touch the issue of classifier diversity.Although most successful ensemble methods encourage diversity to some extent, a recent study on ten diversity measuresraised the question of the what role diversity plays in constructing effective ensembles of classifiers and how it could bemeasured [29]. To our knowledge there has not been a conclusive study showing which measure of diversity is best for usein evaluating ensembles. We are working on this and we will discuss our findings in a future paper. Moreover, we wouldlike to carry out a comparative study with alternative combination rules in the future. These include a new combinationrule for combining non distinct items of evidence more recently introduced in [14] and the unnormalized combination ruleintroduced in [45].AcknowledgementsThe authors would like thank the anonymous reviewers for their detailed constructive comments which have helped usimprove the paper considerably.References[1] A. Al-Ani, M. Deriche, A new technique for combining multiple classifiers using the Dempster–Shafer theory of evidence, J. Artif. Intell. Res. 17 (2002)333–361.[2] J.A. Barnett, Computational methods for a mathematical theory of evidence, in: Proc. of 17th Joint Conference of Artificial Intelligence, 1981, pp. 868–875.[3] C.L. Blake, C.J.E. Keogh, UCI repository of machine learning databases, http://www.ics.uci.edu/mlearn/MLRepository.html.[4] Y. Bi, D. Bell, J.W. Guan, Combining evidence from classifiers in text categorization, in: Proc. of KES04, 2004, pp. 521–528.[5] Y. Bi, Combining multiple classifiers for text categorization using the Dempster–Shafer theory of evidence, PhD thesis, University of Ulster, UK, 2004.[6] D. Bell, J.W. Guan, Y. Bi, On combining classifiers mass functions for text categorization, IEEE Trans. Knowledge Data Engrg. 17 (10) (2005) 1307–1319.[7] Y. Bi, J.W. Guan, An efficient triplet-based algorithm for evidential reasoning, in: Proc. of the 22nd Conference on Uncertainty in Artificial Intelligence,2006, pp. 31–38.[8] Y. Bi, S.I. McClean, T. Anderson, On combining multiple classifiers using an evidential approach, in: Proc. of the Twenty-First National Conference onArtificial Intelligence (AAAI’06), 2006, pp. 324–329.[9] L. Breiman, Bagging predictors, Machine Learning 24 (2) (1996) 123–140.[10] L. Breiman, Random forests, Machine Learning 45 (1) (2001) 5–32;T. Dietterich, Machine learning research: Four current directions, AI Magazine 18 (4) (1997) 97–136.[11] T. Dietterich, An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization,Machine Learning 1 (22) (1998).[12] T. Dietterich, Ensemble methods in machine learning, in: Proc. 2nd Int. Workshop on Multiple Classifier Systems MCS2000, LNCS, vol. 1857, 2000,pp. 1–15.[13] A.P. Dempster, Upper and lower probabilities induced by a multivalued mapping, Ann. Math. Stat. 38 (1967) 325–339.Y. Bi et al. / Artificial Intelligence 172 (2008) 1731–17511751[14] T. Denoeux, Conjunctive and disjunctive combination of belief functions induced by nondistinct bodies of evidence, Artif. Intell. 172 (2–3) (2008)234–264.[15] T. Denoeux, A. Ben Yaghlane, Approximating the combination of belief functions using the fast Moebius transform in a coarsened frame, Internat. J.Approx. Reason. 31 (1–2) (2002) 77–101.[16] T. Denoeux, A neural network classifier based on Dempster–Shafer theory, IEEE Trans. Systems Man Cybernet. A 30 (2) (2000) 131–150.[17] T. Denoeux, A k-nearest neighbor classification rule based on Dempster–Shafer theory, IEEE Trans. Systems Man Cybern. 25 (5) (1995) 804–813.[18] R.P.W. Duin, D.M.J. Tax, Experiments with classifier combining rules, in: J. Kittler, F. Roli (Eds.), Multiple Classifier Systems, 2000, pp. 16–29.[19] S. Dzeroski, B. Zenko, Is combining classifiers with stacking better than selecting the best one? Machine Learning 54 (3) (2004) 255–273.[20] J.L. Fleiss, J. Cuzick, The reliability of dichotomous judgments: unequal numbers of judgments per subject, Appl. Psycholog. Meas. 3 (1979) 537–542.[21] Y. Freund, R. Schapire, Experiments with a new boosting algorithm, in: Machine Learning: Proceedings of the Thirteenth International Conference,1996, pp. 148–156.[22] J.W. Guan, D.A. Bell, Evidence Theory and Its Applications, vols. 1–2, Studies in Computer Science and Artificial Intelligence, vols. 7–8, Elsevier, North-Holland, 1991–1992.[23] J.W. Guan, D.A. Bell, Efficient algorithms for automated reasoning in expert systems, in: The 3rd IASTED International Conference on Robotics andManufacturing, 1995, pp. 336–339.[24] R. Haenni, Are alternatives to Dempster’s rule of combination real alternatives?: Comments on “about the belief function combination and the conflictmanagement problem” Lefèvre et al., Information Fusion 3 (3) (2002) 237–239.[25] L.K. Hansen, P. Salamon, Neural network ensembles, IEEE Trans. Pattern Anal. Machine Intell. 12 (10) (1990) 993–1001.[26] T.K. Ho, The random subspace method for constructing decision forests, IEEE Trans. Pattern Anal. Machine Intell. 20 (8) (1998) 832–844.[27] J. Kittler, M. Hatef, R.P.W. Duin, J. Matas, On combining classifiers, IEEE Trans. Pattern Anal. Machine Intell. 20 (3) (1998) 226–239.[28] L. Kuncheva, Combining classifiers: Soft computing solutions, in: S.K. Pal, A. Pal (Eds.), Pattern Recognition: From Classical to Modern Approaches, 2001,pp. 427–451.[29] L. Kuncheva, C.J. Whitaker, Measures of diversity in classifier ensembles, Machine Learning 51 (2003) 181–207.[30] A.K. Jain, R.P.W. Duin, J. Mao, Statistical pattern recognition: A review, IEEE Trans. Pattern Anal. Machine Intell. 22 (1) (2000) 4–37.[31] L. Lam, C.Y. Suen, Application of majority voting to pattern recognition: An analysis of its behavior and performance, IEEE Trans. Systems Man Cyber-net. 27 (5) (1997) 553–568.[32] L.S. Larkey, W.B. Croft, Combining classifiers in text categorization, in: Proceedings of SIGIR-96, 19th ACM International Conference on Research andDevelopment in Information Retrieval, 1996, pp. 289–297.[33] W. Liu, J. Hong, Reinvestigating Dempster’s idea on evidence combination, Knowledge Inform. Syst. 2 (2) (2000) 223–241.[34] W. Liu, Analyzing the degree of conflict among belief functions, Artif. Intell. 170 (11) (2006) 909–924.[35] E.J. Mandler, J. Schurmann, Combining the classification results of independent classifiers based on Dempster–Shafer theory of evidence, Pattern Recogn.Artif. Intell. X (1988) 381–393.[36] P. Melville, R.J. Mooney, Constructing diverse classifier ensembles using artificial training examples, in: Proc. of IJCAI-03, 2003, pp. 505–510.[37] T. Mitchell, Machine Learning, McGraw Hill, 1997.[38] D. Opitz, Feature selection for ensembles, in: Proc. of AAAI-99, AAAI Press, 1999, pp. 379–384.[39] B. Quost, T. Denoeux, M.-H. Masson, Pairwise classifier combination using belief functions, Pattern Recogn. Lett. 28 (5) (2007) 644–653.[40] F. Sebastiani, Machine learning in automated text categorization, ACM Comput. Surv. 34 (1) (2002) 1–47.[41] G. Rogova, Combining the results of several neural network classifiers, Neural Networks 7 (5) (1994) 777–781.[42] G. Shafer, R. Logan, Implementing Dempster’s rule for hierarchical evidence, Artif. Intell. 33 (3) (1987) 271–298.[43] G. Shafer, Belief functions and possibility measures, in: J.C. Bezdek (Ed.), The Analysis of Fuzzy Information, vol. 1: Mathematics and Logic, CRC Press,1987, pp. 51–84.[44] G. Shafer, A Mathematical Theory of Evidence, Princeton University Press, Princeton, NJ, 1976.[45] Ph. Smets, The combination of evidence in the Transferable Belief Model, IEEE Trans. Pattern Anal. Machine Intell. 12 (5) 447–458.[46] K.M. Ting, I.H. Witten, Issues in stacked generalization, J. Artif. Intell. Res. (JAIR) 10 (1999) 271–289.[47] D.M.J. Tax, M. van Breukelen, R.P.W. Duin, J. Kittler, Combining multiple classifiers by averaging or by multiplying, Pattern Recognition 33 (9) (2000)1475–1485.[48] K. Tumer, G.J. Robust, Combining of disparate classifiers through order statistics, Pattern Anal. Appl. 6 (1) (2002) 41–46.[49] L. Xu, A. Krzyzak, C.Y. Suen, Several methods for combining multiple classifiers and their applications in handwritten character recognition, IEEE Trans.System Man Cybernet. 2 (3) (1992) 418–435.[50] L.M. Zouhal, T. Denoeux, An evidence-theoretic k-NN rule with parameter optimization, IEEE Trans. Systems Man Cybernet. C 28 (2) (1998) 263–271.[51] Y. Yang, T. Ault, T. Pierce, Combining multiple learning strategies for effective cross validation, in: Proc. of ICML’00, 2000, pp. 1167–1182.[52] F. Voorbraak, On the justification of Dempster’s rule of combination, Artif. Intell. 48 (2) (1991) 171–197.[53] G.I. Webb, MultiBoosting: A technique for combining boosting and wagging, Machine Learning 40 (2) (2000) 159–196.[54] D. Wolpert, Stacked generalization, Neural Networks 5 (2) (1992) 241–259.[55] I.H. Witten, E. Frank, Data Mining: Practical Machine Learning Tools and Techniques, second ed., Morgan Kaufmann, San Francisco, 2005.