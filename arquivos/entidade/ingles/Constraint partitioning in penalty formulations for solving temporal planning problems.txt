Artificial Intelligence 170 (2006) 187–231www.elsevier.com/locate/artintConstraint partitioning in penalty formulationsfor solving temporal planning problems ✩Benjamin W. Wah a,∗, Yixin Chen ba Department of Electrical and Computer Engineering and the Coordinated Science Laboratory,University of Illinois, Urbana-Champaign, Urbana, IL 61801, USAb Department of Computer Science, Washington University, St. Louis, MO 63130, USAReceived 25 August 2003; accepted 7 July 2005Available online 24 August 2005AbstractIn this paper, we study the partitioning of constraints in temporal planning problems formulatedas mixed-integer nonlinear programming (MINLP) problems. Constraint partitioning is attractivebecause it leads to much easier subproblems, where each is a significant relaxation of the originalproblem. Moreover, each subproblem is very similar to the original problem and can be solved byany existing solver with little or no modification. Constraint partitioning, however, introduces globalconstraints that may be violated when subproblems are evaluated independently. To reduce the over-head in resolving such global constraints, we develop in this paper new conditions and algorithmsfor limiting the search space to be backtracked in each subproblem. Using a penalty formulation ofa MINLP where the constraint functions of the MINLP are transformed into non-negative functions,we present a necessary and sufficient extended saddle-point condition (ESPC) for constrained lo-cal minimization. When the penalties are larger than some thresholds, our theory shows a one-to-onecorrespondence between a constrained local minimum of the MINLP and an extended saddle point ofthe penalty function. Hence, one way to find a constrained local minimum is to increase gradually thepenalties of those violated constraints and to look for a local minimum of the penalty function usingany existing algorithm until a solution to the constrained model is found. Next, we extend the ESPCto constraint-partitioned MINLPs and propose a partition-and-resolve strategy for resolving violated✩ Research supported by National Science Foundation Grant IIS 03-12084 and National Aeronautics and SpaceAdministration Grant NCC 2-1230.* Corresponding author.E-mail address: wah@uiuc.edu (B.W. Wah).URL: http://manip.crhc.uiuc.edu.0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.07.001188B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231global constraints across subproblems. Using the discrete-space ASPEN and the mixed-space MIPSplanners to solve subproblems, we show significant improvements on some planning benchmarks,both in terms of the quality of the plans generated and the execution times to find them. 2005 Elsevier B.V. All rights reserved.Keywords: Constraint partitioning; Extended saddle-point condition; Penalty function; Local search; Mixedspace planning; Nonlinear constraints; Temporal planning1. IntroductionA temporal planning problem involves arranging actions and assigning resources inorder to accomplish given tasks and objectives over a period of time. It can be defined bya state space with discrete, continuous, or mixed variables; a discrete or continuous timehorizon; a set of actions defining valid state transitions; a set of effects associated with eachaction; a set of constraints to be satisfied in each state or throughout an action; and a set ofgoals to be achieved.In this paper, we formulate a planning problem as a mixed-integer nonlinear program-ming (MINLP) problem. Such a formulation allows us to develop a formal mathematicalfoundation when partitioning a large planning problem by its constraints into subproblems(stages). The MINLP formulation of the problem when partitioned into N + 1 subproblemsis as follows:J (z)(cid:2)(cid:1)z(t)(Pt ): minzsubject to h(t)and H (z) = 0, G(z) (cid:1) 0 (global constraints).(cid:2)(cid:1)z(t)= 0, g(t)(cid:1) 0, t = 0, . . . , N (local constraints),(1)1 , . . . , h(t)1 , . . . , g(t)Here, Stage t, t = 0, . . . , N , has local state vector z(t) = (z1(t), . . . , zut (t))T of ut mixedvariables; h(t) = (h(t)mt )T is a vector of mt local equality-constraint functions thatinvolve z(t); g(t) = (g(t)rt )T is a vector of rt local inequality-constraint functionsof z(t); H = (H1, . . . , Hp)T is a vector of p global equality-constraint functions that in-(cid:3)Nvolve z =i=0 z(i); and G = (G1, . . . , Gq )T is a vector of q global inequality-constraintfunctions of z. Note that z(t) includes all variables that appear in one or more of the lo-cal constraints in Stage t, and that z(0), . . . , z(N ) may overlap with each other since thepartitioning is by constraints.We assume that J is continuous and differentiable with respect to its continuous vari-ables and is lower bounded. Further, g and h can be unbounded, discontinuous, non-differentiable, and not in closed form. These assumptions are reasonable for AI planningproblems, whose constraint functions may be discontinuous and not in closed form andwhose objective functions are continuous and differentiable in the continuous subspace.A solution to Pt is a plan that involves an assignment of z and that satisfies all the con-straints.To illustrate the constrained formulation of a planning problem, consider the toy prob-lem in Fig. 1 solved by ASPEN [9]. The problem involves scheduling four activities: act_1B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231189Fig. 1. A toy example from ASPEN [9] whose goal is to find a valid schedule that completes 4 activities, act_1and act_2 that are instances of type A1 and act_3 and act_4 that are instances of type A2, while minimizingthe total power_resource used. Based on the initial infeasible schedule, the 19 constraints are partitioned into3 stages, {E1, E5, E9, E11}, {E2, E3, E6, E7, E10, E12, E13, E15}, and {E4, E8, E14}, and 4 global constraints{E16, E17, E18, E19}. A local constraint remains associated with a stage even when activities are rescheduled.The number of iterations to solve the problem is reduced from 16 taken by ASPEN to 12 after partitioning.and act_2 of type A1 and act_3 and act_4 of type A2, over a discrete horizon of 60 seconds.Its goal is to satisfy the nineteen constraints, E1 through E19, on positive and negative factsand preconditions and effects of actions, while minimizing the total power_resource used.Among the 19 constraints in the initial schedule in Fig. 1, E1, E2, E3, E4, E6, and E15 arenot satisfied.In a MINLP formulation of the toy example, each of the nineteen constraints in Fig. 1 istransformed into one or more equivalent constraints. We use two variables s(a) and e(a) todenote, respectively, the starting and ending times of activity a. For each state, we assign avector of state variables to denote their values indexed by time. For example, we use c(t)to denote the color_state at time t, which can be set to 0 (red), 1 (blue), or 2 (green)(c(t) = 2 means that the color_state at time t is green); p(t) to denote the power_supplyat t; and w(t) to denote the power_usage at t. The following illustrates a small portion ofthe resulting constraints encoded:(c1) w(t) (cid:1) p(t) (cid:1) 25, ∀t = 0, . . . , 60; // power_resource capacity constraint(c2) 0 (cid:1) s(act_3) − e(act_1) (cid:1) 30; // act_1 ends_before start of act_3 by [0, 30](c3) s(act_1) = t (cid:3)⇒ c(t) = 2; ∀t = 0, . . . , 60; // color_state constraint for act_1(c4) s(cc_b) = t (cid:3)⇒ c(t) = 1; ∀t = 0, . . . , 60; // color_changer cc_b effect constraintThe constraints are either equality or inequality constraints (such as (c1) and (c2)), ordeduction constraints (such as (c3) and (c4)). A deduction constraint A ⇒ B, where A and190B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231Fig. 2. An illustration of constraint partitioning that decomposes P into a conjunction (∧) of three subproblemsand a set of global constraints to be resolved, where the complexity of each subproblem is substantially smallerthan that of P . The set of global constraints G includes constraints in P that span across variables in multi-ple subproblems and new constraints added to maintain the consistency of shared entities and variables acrosssubproblems.B are equality or inequality constraints, can be encoded as an equivalent equality constraintH (A ⇒ B) = 0:(cid:4)H (A ⇒ B) =0numerical violation of B if A is true but B is false.if A is false, or A and B are both trueFor example, the equivalent equality constraint encoding (c3) returns 0 if s(act_1) = t isfalse; otherwise, it returns the value of (c(t) − 2).A general approach for solving a large constrained optimization problem is to se-lect iteratively a set of its variables to which values can be assigned according to thestructural characteristics of the domain, and to partition the problem into subspaces bysetting the variables selected to specific values. Systematic-search methods may set thevalues of variables in some predefined order or in an order independent of the interactionsamong variables. By considering variable interactions, intelligent backtracking employsvariable/value ordering to order the subproblems generated, pre-filters partial inconsistentassignments to eliminate infeasible subproblems, and prunes subproblems with inferiorbounds computed by relaxation or approximation. Alternatively, iterative-repair methodsoperate on the full dimensionality of the starting problem and consider the interaction ofeach assignment with all its variables. In general, it is difficult to make full use of the in-teractions among variables and subproblems in the selection and assignment of variables.In this paper, we propose a new approach called constraint partitioning that decomposesthe constraints of a problem into a conjunction (∧) of subproblems, each with local con-straints and related to others by global constraints (Fig. 2). Constraints that relate to onlyvariables in one subproblem are local constraints, whereas constraints that relate to localvariables as well as shared entities and variables across subproblems are global constraints.Since shared entities across subproblems must be consistent, additional global constraintsmay be added to enforce their consistency. This approach is attractive for solving temporalplanning problems because many of their constraints and objectives are related to activitieswith temporal locality, and the constraints can be partitioned into independent subproblemsrelated by only a small number of global constraints.B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231191(a)(b)Fig. 3. Two extreme configurations when partitioning the constraints of a problem. (a) Totally overlapped variablesets. (b) Totally disjoint variable sets.A constraint-partitioned problem can be solved by first evaluating the subproblems in-dependently, using possibly existing methods and disregarding some or all of the globalconstraints, and by resolving the violated global constraints through systematic backtrack-ing of subproblem evaluations. The advantage of evaluating subproblems independentlyis that each is much more relaxed than the original problem and requires significantlyless time to solve. The difficulty, however, lies in the resolution of violated global con-straints because they are defined in an exponentially large space across the subproblems.Even though the subproblems may be organized into stages, dynamic programming andthe Principle of Optimality [3] cannot be applied because a partial feasible plan that dom-inates another partial feasible plan in one stage will fail to hold when the dominating planviolates a global constraint in a later stage. Without dominance, resolving a violated globalconstraint may invalidate the solutions of subproblems found already and require back-tracking of their evaluations. The complexity due to backtracking is hard to characterizeprecisely because it depends on the aggregate search space across all the stages in whichglobal constraints can be satisfied. In the following, we illustrate this complexity in twoextreme cases.Fig. 3 illustrates two extreme configurations when partitioning the constraints of a prob-lem: one whose stages have totally overlapped variable sets and the other with totally192B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231Fig. 4. The 3687 constraints of an initial infeasible schedule generated by ASPEN in solving CX1-PREF with 16orbits. Each constraint is shown as a line that relates two activities (labeled in the y-axis) scheduled at two timeinstances in the horizon (x-axis). The partitioning of the constraints into four stages (separated by bold verticallines) leads to 3580 local constraints and 107 global constraints. A local constraint remains associated with astage even when activities are rescheduled. The number of iterations to find a better solution to the problem isreduced from 12,043 taken by ASPEN to 1102 after partitioning.disjoint variable sets. In each stage, the outermost box denotes the search space of thestage; the first inner box denotes all feasible solutions that satisfy the local constraints inthat stage; and the innermost shaded box denotes points that satisfy our proposed ESPC(which also satisfy the local constraints). In this paper, we show that ESPC is necessaryand sufficient for all constrained local minima, which means that only those points thatsatisfy ESPC in each stage should be considered in resolving violated global constraints.Let si be the set of points that satisfy ESPC in stage i, i = 0, . . . , N .In Fig. 3(a), when the stages have totally overlapped variable sets, each constraint inthe original problem can be assigned to exactly one stage. Because every variable is sharedacross all the stages, new global constraints must be introduced to maintain its consis-tency across the subproblems. These global constraints are defined in a search space whoseworst-case complexity is bounded by |s0 ∩ s1 ∩ · · · ∩ sN |. Since the number of such globalconstraints as well as si , i = 1, . . . , N , can be large, the complexity for resolving the globalconstraints can be very high. In contrast, in Fig. 3(b), when the stages have totally disjointvariable sets, it is likely that most constraints cannot be assigned as local constraints andwill remain as global. These global constraints will need to be resolved in a search spacewhose worst-case complexity is bounded by |s0| · |s1| · · · |sN |. Due to the large numberof such global constraints, the overhead for resolving them will likely to be high as well.In short, the minimization of the overhead in resolving violated global constraints entailstrade-offs among the number of shared entities and variables across the subproblems, thenumber of global constraints involved, and the search space where the global constraintsare defined.In this paper, we analyze the constraints of temporal planning problems in order to par-tition them into a small number of simpler subproblems (stages). In general, it is hard todevelop a good partitioning algorithm that minimizes the time to solve a planning problembecause the relation between the time to solve a subproblem and that to resolve violatedglobal constraints is complex and unknown. In this paper, we exploit the temporal localityof constraints in planning problems when partitioning them into stages. Starting from aninitial (possibly infeasible) schedule, we partition the constraints along the horizon into asmall number of stages, each with an approximately equal number of constraints. For ex-ample, Fig. 1 (respectively Fig. 4) shows the nineteen (respectively 3687) constraints of aninitial infeasible schedule generated by ASPEN [9] in solving the toy example (respectivelyB.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231193CX1-PREF with sixteen orbits). After partitioning the constraints into three (respectivelyfour) stages, the resulting problem has fifteen (respectively 3580) local constraints andfour (respectively 107) global constraints. Since some violated global constraints may be-come satisfied or new constraints corresponding to new actions may be added as planningprogresses, we also study algorithms to determine a suitable number of stages and to repar-tition the constraints periodically in order to balance the number of violated constraints ineach stage.Our major goal in this paper is to develop the theory and the corresponding algorithmsfor resolving violated global constraints when temporal planning problems are partitionedby their constraints into stages. Specifically, there are three contributions in this paper.(a) We show in Section 3 the necessary and sufficient extended saddle-point condition(ESPC) that governs all constrained local minima, when a MINLP problem is formulatedin a penalty function with non-negative (transformed) constraint functions. This paper isthe first to show that each constrained local minimum of the MINLP has a one-to-onecorrespondence to an extended saddle point of the penalty function when its penalties aresufficiently large. Using this result, one way to look for a constrained local minimum of theMINLP is to increase gradually the penalties of violated constraints in the penalty functionand to search repeatedly local minima of the penalty function by an existing algorithm untila feasible solution to the constrained model is found.(b) We present in Section 4 that the ESPC can be decomposed for constraint-partitionedMINLPs. Each decomposed ESPC is defined with respect to a subproblem consisting of itslocal constraints and an objective function that is made up of the objective of the originalproblem and biased by a weighted sum of the violated global constraints. As such, eachsubproblem is very similar to the original problem and can be solved by the same plannerwith little or no modification.(c) We describe a partition-and-resolve procedure in Section 4.2. The procedure iter-ates between calling a planner to solve the constraint-partitioned subproblems, and usinga constraint-reweighting strategy to resolve the violated global constraints across the sub-problems. In Section 5, we demonstrate significant improvements in using the discrete-space ASPEN and the mixed-space MIPS as basic planners to solve some large-scalebenchmarks. For example, the problem in Fig. 1 (respectively 4) can be solved by AS-PEN in 16 (respectively 12,043) iterations and by our implementation in 12 (respectively1102) iterations with the same (respectively better) quality.2. Previous workIn this section, we summarize some existing work related to AI planning and nonlinearoptimization. Our survey shows that existing approaches solve a problem directly whiletaking all its constraints into consideration.2.1. Existing temporal planning methodsFig. 5 classifies existing AI planning and scheduling methods based on their state andtemporal representations and the search techniques used.194B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231Fig. 5. A classification of existing planning and scheduling approaches.Discrete-time discrete-state methods consist of systematic searches, heuristic searches,local searches, and transformation methods. Systematic searches that explore the entirestate space are complete solvers. After decomposing a search space into subspaces, theyevaluate each as a complete planning problem. Examples include the generic A* algorithm,UCPOP [32], Graphplan [5], STAN [28], PropPLAN [13], and System R [27].Heuristic solvers explore the search space by a tree search guided by heuristics in or-der to estimate the distance from a state to the goal state. They do not have means toresolve violated global constraints when the original planning problem is partitioned byits constraints into subproblems. They are not guaranteed to find feasible plans becausetheir success depends on the guidance heuristics used. Examples include HSP [6], FF [18],AltAlt [31], GRT [37] (and its extension to MO-GRT [38]), and ASPEN [9]. Last, transfor-mation methods convert a problem into a constrained optimization or satisfaction problem,before solving it by an existing solver. Examples in this class include SATPLAN [22]Blackbox [23], and ILP-PLAN [24].Discrete-time mixed-state methods consist of systematic searches, heuristic searches,and transformation methods. Similar to discrete-time discrete-state methods, methods inthis class do not partition the constraints of a planning problem. Examples include SIPE-2 [52], O-Plan2 [46], Metric-FF [18], GRT-R [37], and LPSAT [54].Continuous-time mixed-state methods can be classified into systematic, heuristic, andlocal searches. Again, constraints are not partitioned in these methods. Examples includeLPG [16], MIPS [12], Sapa [45], ZENO [33], SHOP2 [30], TALplanner [10], and Eu-ropa [21].In summary, existing planners solve a problem as a whole without partitioning its con-straints, or transform it into another form before solving it by existing methods. In thispaper, we propose to augment existing approaches by constraint partitioning and decom-pose the constraints of a large problem into subproblems of a similar form before solvingthem by existing planners. Instead of developing a new planner based on ESPC to solvethe small subproblems, using an existing planner is more effective because it performswell in solving small problems, besides saving a lot of development efforts. We demon-strate our approach in Section 5 after formulating the objectives and the constraints of thesubproblems solved by ASPEN and MIPS.B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–2311952.2. Existing mathematical programming methodsIn this section, we survey existing methods on continuous and mixed-integer optimiza-tion. Although many of these methods cannot be applied to solve planning problemsbecause they have requirements, such as continuity, differentiability, and convexity, thatare not satisfied in planning problems, it is necessary to understand their limitations. Theconcepts of saddle points and penalty formulations are important and form the basis of ourtheory presented in Section 3.Continuous nonlinear programming (CNLP) methods. Consider the following CNLP:(Pc): minxsubject to h(x) =f (x) where x = (x1, . . . , xv)T ∈ Rv(2)(cid:1)(cid:2)T = 0 and g(x) =h1(x), . . . , hm(x)(cid:2)(cid:1)g1(x), . . . , gr (x)T (cid:1) 0,where f is continuous and differentiable, and g and h can be discontinuous, non-differentiable, and not in closed form. The goal of solving Pc is to find a constrained localminimum x∗ with respect to Nc(x∗) = {x(cid:8): (cid:9)x(cid:8) − x∗(cid:9) (cid:1) ε and ε → 0}, the continuousneighborhood of x∗, where ε → 0 means that ε is arbitrarily close to 0.Definition 1. Point x∗ is a CLMc, a constrained local minimum of Pc with respect to pointsin Nc(x∗), if x∗ is feasible and f (x∗) (cid:1) f (x) for all feasible x ∈ Nc(x∗).Traditional Lagrangian theory for continuous optimization works for Pc with continu-ous and differentiable constraint functions g and h. The Lagrangian function of Pc withLagrange-multiplier vectors λ = (λ1, . . . , λm)T ∈ Rm and µ = (µ1, . . . , µr )T ∈ Rr , is de-fined as:L(x, λ, µ) = f (x) + λTh(x) + µTg(x).(3)Under the continuity and differentiability assumptions, a CLMc satisfies the followingnecessary KKT condition and sufficient saddle-point condition.(a) Necessary Karush–Kuhn–Tucker (KKT) condition [4]. Assuming x∗ is a CLMc anda regular point,1 then there exist unique λ∗ ∈ Rm and µ∗ ∈ Rr such that:∇xL(x∗∗∗) = 0, λ, µ(4)where µj = 0 ∀j /∈ A(x∗) = {i | gi(x∗) = 0} (the set of active constraints), and µj > 0otherwise.The unique x, λ and µ that satisfy (4) can be found by solving (4) as a system ofnonlinear equations. For instance, for Pc with only equality constraints, the KKT conditionin (4) can be expressed as a system of v + m equations in v + m unknowns:(cid:5)(cid:6)F (x, λ) =∇xf (x) + λT∇xh(x)h(x)= 0,(5)1 Point x is a regular point [29] if gradient vectors of equality constraints ∇x h1(x), . . . , ∇x hm(x) and activeinequality constraints ∇x ga1 (x), . . . , ∇x gal (x), ai ∈ A(x) (the set of active inequality constraints) are linearlyindependent. An inequality constraint gi (x) (cid:1) 0 is active when gi (x) = 0. It will affect the search direction onlywhen it is active and can be ignored otherwise.196B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231where ∇xh(x)T = [∇xh1(x), . . . , ∇xhm(x)] is the Jacobian of the constraints. The v + munknowns are solvable when the matrix in (5) is nonsingular.Iterative procedures have been developed to find the unique λ, µ and x that sat-isfy (4). For example, existing sequential quadratic-programming solvers like SNOPT andLANCELOT solve (4) iteratively by forming a quadratic approximation, evaluating thequadratic model, and updating estimates of x, λ, and µ until a solution to (4) has beenfound.In short, existing CNLP solvers have continuity and differentiability requirements andcannot be applied to solve the type of planning problems studied in this paper.2(b) Sufficient saddle-point condition [2,26]. The concept of saddle points has been stud-ied extensively in the past. For continuous and differentiable constraint functions, x∗ is aCLMc of Pc if there exist unique λ∗ ∈ Rm and µ∗ ∈ Rr that satisfy the following saddle-point condition at x∗:∗, µ(6)for all x ∈ Nc(x∗) and all λ ∈ Rm and µ ∈ Rr . This condition is only sufficient but notnecessary because there may not exist λ∗ and µ∗ that satisfy (6) at each CLMc x∗ of Pc., λ, µ) (cid:1) L(x) (cid:1) L(x, λL(x, µ, λTo illustrate the concept, consider the following CNLP with CLMc at x∗ = 5:)∗∗∗∗∗f (x) = −x2subject to h(x) = x − 5 = 0.minx(7)By applying the KKT condition, we differentiate the Lagrangian function L(x, λ) = −x2 +λ(x − 5) with respect to x and evaluate it at x∗ = 5. We have ∇xL(x, λ)|x∗ = −10 + λ = 0,which implies λ∗ = 10. However, since ∇2x L(x, λ)|x∗,λ∗ = −2 < 0, we know that L(x, λ)is at a local maximum with respect to x at (x∗, λ∗) instead of a local minimum. Hence,there exists no λ∗ that will allow the second inequality in (6) to be satisfied at x∗ = 5.In practice, it is difficult to use (6) for finding the unique x∗, λ∗, and µ∗ that satisfy (4)because it is expressed as a system of nonlinear inequalities that are more difficult to solvethan nonlinear equalities. It is mainly used for verifying the solutions found by solving (4).A recent local optimal method for solving Pc with continuous and differentiable con-straint functions is the interior-point (cid:3)1-penalty method based on the following (cid:3)1-penaltyfunction [17]:(cid:1)(cid:3)1(z, c) = f (z) + c · max0,(cid:7)(cid:7)(cid:7), . . . ,(cid:7)h1(z)(cid:7)(cid:7)(cid:2)(cid:7)hm(z)(cid:7), g1(z), . . . , gq (z).(8)Its theory shows that there is a one-to-one correspondence between a CLMc and an un-constrained local minimum of (8) when c is larger than a finite threshold c∗. Althoughit appears that c is not unique, it can be proved that c∗ is the maximum of all Lagrangemultipliers of the corresponding Lagrangian formulation that satisfies the KKT condition.The approach cannot support constraint partitioning because it is difficult to partition (8)2 Constraint partitioning studied in this paper can be applied to solve problems solvable by existing CNLPand MINLP solvers. This is done by decomposing the constraints of a large CNLP or MINLP problem intosubproblems, calling an existing CNLP or MINLP solver to solve the subproblems, and applying constraint-reweighting to resolve violated global constraints. Results on this approach are beyond the scope here and isreported elsewhere [49].B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231197by its constraints and to reach a consistent value of a single penalty term c across thesubproblems.Another partitioning approach called separable partitioning [4] has similar advantagesas our proposed constraint partitioning. By exploiting some separable properties in theoriginal problem, these methods decompose the dual problem of Pc with continuous anddifferentiable constraint functions into multiple much simpler subproblems, each involvingonly a subset of the constraints and variables. They are limited in their applications becausethey have restricted assumptions, such as linearity or convexity of the functions.Mixed-integer NLP (MINLP) methods generally solve a MINLP problem by partition-ing its search space into subspaces (subproblems) in such a way that, after fixing a subsetof the variables, each subproblem is convex and is easily solvable, or can be relaxed andbe approximated. There are several approaches.(a) Generalized Benders decomposition (GBD) [15] computes in each iteration an upperbound on the solution sought by solving a primal problem and a lower bound on a masterproblem. Here, the primal problem corresponds to the original problem with fixed discretevariables, and the master problem is derived through nonlinear duality theory. It generallyrequires the original problem to have special decomposable structures and the subproblemsto have some special properties, such as nonempty and continuous subspaces with convexobjective and constraint functions.(b) Outer approximation (OA) [11] is similar to GBD except that the master problem isformulated using primal information and outer linearization. It requires the continuous sub-space to be a nonempty, compact and convex set, and the objective and constraint functionsto be convex.(c) Generalized cross decomposition (GCD) [19,20,39] iterates between a phase solvingthe primal and dual subproblems and a phase solving the master problem. Similar to OAand GBD, GCD requires the objective and constraint functions of subproblems to be properconvex functions.(d) Branch and reduce methods [40,41] solve MINLPs and CNLPs by a branch-and-bound algorithm and exploit factorable programming to construct relaxed problems as wellas range reduction to improve the performance of their bounding procedures. Many of therange-reduction techniques are applicable only when the relaxed problems are convex.(e) Direct-solution methods attack a problem without any transformation. They are verylimited in handling problems with nonlinear constraints and disconnected feasible regions.In summary, existing MINLP methods solve a problem either as a whole or by partition-ing its variables into subspaces. They are not applicable to solve planning problems due totheir convexity or factorability requirement on the decomposed subproblems.Penalty methods. A penalty function of a constrained optimization problem is a sum-mation of its objective function and its constraint functions weighted by penalties. Usingpenalty vectors α ∈ Rm and β ∈ Rr , the general penalty function for Pc is:Lp(x, α, β) = f (x) + αTP(cid:2)(cid:1)h(x)(cid:2)(cid:1)+ βTQg(x),(9)where P and Q are possible transformation functions. The goal of a penalty method is tofind suitable α∗ and β∗ in such a way that x∗ that minimizes (9) corresponds to a CLMc ofPc. Penalty methods belong to a general approach that can solve continuous, discrete, andmixed constrained optimization problems, including planning problems, with no continu-198B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231ity, differentiability, and convexity requirements. The Lagrangian function (3) used in theKKT condition is a special case of (9) when g(x) and h(x) are continuous differentiablefunctions that satisfy the regularity condition and are not transformed by P and Q.When P (g(x)) and Q(h(x)) are general functions that take positive and negative values,a local minimum of (9) at x∗ will require finding unique values of α∗ and β∗ (proof notshown). However, these unique penalty vectors may either not exist at x∗, or exist but (9)is not at a local minimum at x∗. For instance, for the problem in (7), there is no finite αthat will lead to a local minimum of the penalty function Lp(x, α) = f (x) + α · h(x) atx∗ = 5. Hence, it will not be possible to find x∗ by minimizing Lp(x, α) with respect to xfor any given α.Next, we survey some general results on penalty methods that associate the constrainedglobal minimum of a constrained minimization problem to the global minimum of (9) withsufficiently large penalties. Although we describe the results with respect to continuousproblems, they apply to discrete and mixed problems as well.A static-penalty method [29,36] formulates Pc as the minimization of (9) when theconstraints of Pc are transformed into non-negative functions that satisfy the followingproperties: (a) P (h(x)) (cid:2) 0 and Q(g(x)) (cid:2) 0; and (b) P (h(x)) = 0 if and only if h(x) = 0,and Q(g(x)) = 0 if and only if g(x) (cid:1) 0.For any finite penalty vectors α∗∗ and β∗∗ larger than some thresholds, α∗∗ > α∗3 andβ∗∗ > β∗, a global minimum x∗ of Lp(x, α∗∗, β∗∗) has a one-to-one correspondence to aconstrained global minimum (CGMc) of Pc. To show this result, we know that α and β in(9) must be greater than zero in order to penalize the violated constraints because P (h(x))and Q(g(x)) are non-negative with a minimum of zero. Since (9) is to be minimized withrespect to x, increasing the penalty of a violated constraint to a large enough value willforce the corresponding transformed constraint function to achieve the minimum of zero,and such penalties always exist if a feasible solution to Pc exists. At those points whereall the constraints are satisfied, every term on the right of (9) except the first is zero, and aglobal minimum of (9) corresponds to a CGMc of Pc.Continuing on the example in (7), if we use a penalty function that takes the absolutevalue of the constraint function, namely, Lp(x, α) = f (x) + α · |h(x)|, and assume that−100 (cid:1) x (cid:1) 100, then there will be a global minimum of Lp(x, α∗∗) at x∗ = 5 for anyα∗∗ > α∗ = 105. It is interesting to note that α∗ depends on the range of x. For example,Fig. 6 show that, if −1000 (cid:1) x (cid:1) 1000, then there will be a global minimum of Lp(x, α∗∗)at x∗ = 5 for any α∗∗ > α∗ = 1005. This example shows that α∗ can be exceedingly largein order to ensure global optimality in a given range of x.One of the difficulties of the static-penalty method is that its penalties have to be foundby trial and error. Moreover, each trial is computationally expensive, if not impossible,because it involves finding a global minimum of a nonlinear function. Techniques likesimulated annealing [25] can be used, although they only achieve global optimality withasymptotic convergence.3 α∗∗ > α∗means that each element of α∗∗means that every element of α∗∗is larger than or equal to the corresponding element of α∗.is larger than the corresponding element of α∗. Further, α∗∗ (cid:2) α∗B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231199Fig. 6. An illustration of a global minimum of Lp(x, α∗∗) at x∗ = 5 for any α∗∗ > α∗ = 1005 but not one whenα∗∗ (cid:1) α∗for the CNLP problem in (7), where −1000 (cid:1) x (cid:1) 1000.Instead of finding α∗∗ and β∗∗ by trial and error, a dynamic-penalty method [29,36]increases the penalties in (9) gradually, finds the global minimum x∗ of (9) with respect tox for each unconstrained problem in the sequence, and stops when x∗ is a feasible solutionto Pc. To show that x∗ is a CGMc when the algorithm stops, we know that the penaltiesneed to be increased when x∗ is a global minimum of (9) but not a feasible solution to Pc.The first time x∗ is a feasible solution to Pc, the solution must also be a CGMc. Hence, themethod leads to the smallest α∗∗ and β∗∗ that allows a CGMc to be found. However, it hasthe same limitation as the static-penalty method because it requires finding global minimaof nonlinear functions.The practice of re-weighting violated constraints during a local search of penalty func-tions has been popular and highly successful in the AI community. For example, plannerssuch as SATPLAN [22], Blackbox [23], and ILP-PLAN [24] first transform a planningproblem into a SAT or an ILP (integer linear programming) formulation. They then find asolution to the SAT or ILP problem using an existing solver that minimizes a penalty func-tion of the form in (9) with dynamically adjusted penalties [24,42–44]. The key feature inthese applications is that they deal with discrete constraint functions that are non-negativeto start with, such as the number of violated clauses in a problem and binary constraintson whether a clause is violated. Hence, they work well without the need to transform theconstraint functions. Moreover, the objective function is usually chosen in such a way thatfinding a constrained local minimum amounts to finding a constrained global minimum ofthe constrained SAT model. As a result, the theory of existing static and dynamic penaltymethods applies.In short, using (9) when P (h(x)) and Q(g(x)) can take positive and negative values,a CLMc x∗ of Pc does not imply a local minimum of (9) at x∗ because there may notexist feasible penalties there. This means that a CLMc whose penalties do not exist in (9)cannot be found by looking for a local minimum of (9). On the other hand, using (9) whenP (h(x)) and Q(g(x)) are non-negative functions, a CGMc of Pc always corresponds to anunconstrained global minimum of (9) when its penalties are larger than some thresholds.Unfortunately, this result is impractical because finding a global minimum of an uncon-strained nonlinear function is computationally expensive. A similar observation can bemade on discrete and mixed optimization problems.To cope with these shortcomings, we prove in the next section the one-to-one corre-spondence between a constrained local minimum of a MINLP and an extended saddlepoint of its penalty function with non-negative (transformed) constraint functions, when200B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231the penalties larger than some thresholds. This result extends our previous work that provesa special-case condition for discrete optimization problems [51]. A constrained local min-imum of a MINLP can, therefore, be found by increasing gradually the penalties of thoseviolated constraints and by looking for a local minimum of the penalty function using anyexisting algorithm until a solution to the constrained model is found. By showing a generaltheory that covers continuous, discrete and mixed-integer optimization, this paper providesa complete foundation on penalty methods.3. The theory of extended saddle pointsWe describe in this section our necessary and sufficient saddle-point condition (ESPC)in mixed space based on a penalty function with non-negative (transformed) constraintfunctions and under a relaxed range of penalties.3.1. ESPC for continuous, discrete, and mixed optimizationWe first state the necessary and sufficient ESPC on CLMc of Pc, based on the followingpenalty function.Definition 2. The penalty function for Pc in (2) is defined as in (9) by transforming theconstraint functions of Pc into non-negative functions:(cid:7)(cid:7)(cid:2)(cid:1)(cid:7) + βT max(cid:7)h(x),0, g(x)Lc(x, α, β) = f (x) + αT(10)and max(0, g(x)) = (max(0, g1(x)), . . . ,wheremax(0, gr (x)))T; and α ∈ Rm and β ∈ Rr are penalty vectors. Note that (10) is a specialcase of the penalty function used in the static-penalty method.|h(x)| = (|h1(x)|, . . . , |hm(x)|)TIn continuous space, we need the following constraint-qualification condition in orderto establish the existence of a local minimum of (10) at x∗.Definition 3. The subdifferential Dx(φ(x(cid:8)), (cid:12)p) of function φ at x(cid:8) ∈ X along direction(cid:12)p ∈ X represents the rate of change of φ(x(cid:8)) under an infinitely small perturbation along (cid:12)p.That is,(cid:1)φ(x(cid:8)), (cid:12)p(cid:2)Dx= limε→0φ(x(cid:8) + ε (cid:12)p) − φ(x(cid:8))ε.(11)Note that a function whose subdifferential exists along (cid:12)p at x(cid:8) does not imply that φ(x)is differentiable at x(cid:8) with respect to (cid:12)p.Definition 4 (Constraint-qualification condition). The solution x∗ ∈ X of Pc meets thecondition if there exists no direction (cid:12)p ∈ X along which the subdifferentials of continuousequality and active continuous inequality constraints are all zero. That is,∗(cid:13) ∃ (cid:12)p ∈ X such that Dx), (cid:12)pDx(cid:1)hi(x= 0 ∀i ∈ Ch and j ∈ Cg,(cid:1)gj (x), (cid:12)p(cid:2)(cid:2)∗= 0 and(12)B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231201where Ch and Cg are, respectively, the sets of indexes of continuous equality and activecontinuous inequality constraints. Constraint qualification is always satisfied if Ch and Cgare empty sets.Our constraint-qualification condition requires the subdifferential of at least one con-tinuous equality constraint or active continuous inequality constraint at x∗ to be non-zeroalong each and every direction (cid:12)p. It rules out the case in which there is a direction (cid:12)p at x∗along which all equality constraints and active inequality constraints have zero subdiffer-entials. Intuitively, constraint qualification at x∗ ensures the existence of finite α and β thatlead to a local minimum of (10) at x∗. Consider a neighboring point x∗ + (cid:12)p infinitely closeto x∗, where the objective function f at x∗ decreases along (cid:12)p and all active constraints atx∗ have zero subdifferentials along (cid:12)p. In this case, since all the active constraints at x∗ + (cid:12)pare also satisfied, it will be impossible to find finite α and β in order to establish a localminimum of (10) at x∗ with respect to x∗ + (cid:12)p. To ensure a local minimum of (10) at x∗,the above scenario must not be true for any (cid:12)p at x∗.Note that our condition is less restricted than the regularity condition in KKT, whichrequires the linear independence of the gradients of the equality and active inequality con-straint functions.The following theorem states the ESPC when the constraint qualification is satisfied.Theorem 1 (Necessary and sufficient ESPC on CLMc of Pc). Suppose x∗ ∈ Rv is a pointin the continuous search space of Pc and satisfies the constraint-qualification condition in(12), then x∗ is a CLMc of Pc if and only if there exist finite α∗ (cid:2) 0 and β∗ (cid:2) 0 such thatthe following is satisfied:∗Lc(x, α, β) (cid:1) Lc(x∗∗∗, α, β∗∗) (cid:1) Lc(x, α∗∗∗∗), β(13)for any α∗∗ > α∗ and β∗∗ > β∗, and for all x ∈ Nc(x∗), α ∈ Rm, and β ∈ Rr .The proof of the theorem is shown in Appendix A.Theorem 1 shows that x∗, a local minimum of (10) with respect to x, corresponds toa CLMc of Pc (second inequality of (13)) when α∗∗ > α∗ and β∗∗ > β∗ such that allthe constraints of Pc are forced to be satisfied (first inequality of (13)). Hence, instead oflooking for CLMc’s directly, it suffices to look for extended saddle points in the penaltyformulation.According to (13), an extended saddle point is a local minimum of Lc with respectto x and a local maximum of Lc with respect to α and β. One approach to look for anextended saddle point of Lc is to increase gradually α∗∗ and β∗∗, while minimizing Lcwith respect to x using an existing local-search method, until α∗∗ > α∗ and β∗∗ > β∗.Because there are many existing local-search algorithms, our approach improves over thestatic-penalty approach, which is defined with respect to difficult-to-find global minimaof (9). However, as presented in Theorem 4 later, our approach only generates fixed pointsthat are necessary, but not sufficient, to be CLMc. Additional steps presented in Section 3.2are needed to allow our approach to find CLMc.It is interesting to note that α∗ and β∗ that satisfy Theorem 1 can be much smallerthan the corresponding α∗ and β∗ found by the dynamic-penalty method in Section 2.2.202B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231Fig. 7. An illustration that (13) is satisfied when α∗∗ > α∗ = 10 for the CNLP problem in (7). Lc(x, α∗∗) is astrict local minimum around x∗ = 5 when α∗∗ > α∗but is not one when α∗∗ = α∗.Continuing on the example in (7), instead of α∗∗ > α∗ = 1005 in order to have a globalminimum of Lp(x, α∗∗) at x∗ = 5 for −1000 (cid:1) x (cid:1) 1000 in the dynamic-penalty method(Fig. 6), it suffices to have α∗∗ > α∗ = 10 by applying Theorem 1 in order to have a localminimum of Lc(x, α∗∗) = −x2 + α∗∗|x − 5| at x∗ = 5, irrespective of the range of x. Fig. 7illustrates that Lc(x, α∗∗) is at a local minimum around x∗ = 5 when α∗∗ = 20 but is notone when α∗∗ = 10. A small α∗∗ leads to a less rugged Lc(x, α∗∗) function and makes iteasier for global-search algorithms to locate local minima.Next, we present the ESPC of discrete nonlinear programming (DNLP) problems. Con-sider the DNLP whose f , g and h are not necessarily continuous and differentiable withrespect to y.(Pd ): minyf (y) where y = (y1, . . . , yw)T ∈ Dwsubject to h(y) = 0 and g(y) (cid:1) 0.(14)The goal of solving Pd is to find a constrained local minimum y∗ with respect toNd (y∗), the discrete neighborhood of y∗. Since the discrete neighborhood of a point isnot well defined in the literature, it is up to the user to define the concept. Intuitively,Nd (y) represents points that are perturbed from y, with no requirement that there be validstate transitions from y.Definition 5. Nd (y) [1], the discrete neighborhood of y ∈ Dw in discrete space, is a finiteuser-defined set of points {y(cid:8) ∈ Dw} such that y(cid:8) is reachable from y in one step, thaty(cid:8) ∈ Nd (y) ⇔ y ∈ Nd (y(cid:8)), and that it is possible to reach every y(cid:8)(cid:8) from any y in one ormore steps through neighboring points.Definition 6. Point y∗ is a CLMd , a constrained local minimum of Pd with respect to pointsin Nd (y∗), if y∗ is feasible and f (y∗) (cid:1) f (y) for all feasible y ∈ Nd (y∗).There are two distinct features of CLMd . First, the set of CLMd of Pd is neighborhooddependent, and a point may be a CLMd under one definition of neighborhood but maynot be one under another. However, all CLMd ’s are guaranteed to be feasible, even in theextreme case in which the neighborhood of each point includes only itself. The fact thatCLMd ’s are neighborhood dependent is not critical in constrained searches, because ourB.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231203goal is to find feasible solutions that are better than their neighboring points. As long as aconsistent neighborhood is used throughout a search, a CLMd found will be a local mini-mum with respect to its neighborhood. Second, a discrete neighborhood has a finite numberof points. Hence, the verification of a point to be a CLMd can be done by comparing itsobjective value against that of its finite number of neighbors. This feature allows the searchof a descent direction in discrete space to be done by enumeration or by greedy search.Definition 7. The penalty function for Pd is defined as in (9) by transforming the constraintfunctions of Pd into non-negative functions:Ld (y, α, β) = f (y) + αT(cid:7)(cid:7)(cid:2)(cid:1)(cid:7) + βT max(cid:7)h(y)0, g(y)where α ∈ Rm and β ∈ Rr .(15)Theorem 2 (Necessary and sufficient ESPC on CLMd of Pd [51,55]). Suppose y∗ ∈ Dw isa point in the discrete search space of Pd . Then y∗ is a CLMd of Pd if and only if thereexist finite α∗ (cid:2) 0 and β∗ (cid:2) 0 such that the following condition is satisfied:∗∗∗∗∗∗∗∗∗∗Ld (y, α, β) (cid:1) Ld (y, α, β) (cid:1) Ld (y, α, β)(16)for any α∗∗ > α∗ and β∗∗ > β∗, and for all y ∈ Nd (y∗), α ∈ Rm, and β ∈ Rr .The proof of the theorem is shown in Appendix B.Note that the constraint-qualification condition in Theorem 1 is not needed in Theorem 2because constraint functions are not changing continuously in discrete problems.Last, we present the ESPC of MINLP problems. Consider a MINLP problem whoseobjective function f is continuous and differentiable with respect to the continuous sub-space x:(Pm): minx,yf (x, y) where x = (x1, . . . , xv)T ∈ Rv and(17)y = (y1, . . . , xw)T ∈ Dwsubject to h(x, y) = 0 and g(x, y) (cid:1) 0.The goal of solving Pm is to find a constrained local minimum (x∗, y∗) with respectto Nm(x∗, y∗), the mixed neighborhood of (x∗, y∗). In this paper, we construct our mixedneighborhood as the union of points perturbed in either the discrete or the continuoussubspace, but not both. Such a definition allows the ESPC for the two subspaces to bedecomposable into that for each subspace. Note that a mixed neighborhood is also a user-defined concept because a discrete neighborhood is user-defined and a mixed neighborhoodis a union of discrete and continuous neighborhoods.Definition 8. Nm(x, y), the mixed neighborhood of (x, y) ∈ Rv × Dw in mixed space,is made up of the union of the continuous neighborhood and the user-defined discreteneighborhood:Nm(x, y) = Nc(x)|y ∪ Nd (y)|x(cid:9)(cid:8) ∈ Nc(x)(cid:8)(x, y) | x=(cid:8)(cid:8)(x, y(cid:8)∪) | y(cid:8) ∈ Nd (y)(cid:9).(18)204B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231Definition 9. Point (x∗, y∗) is a CLMm, a constrained local minimum of Pm with respectto points in Nm(x∗, y∗), if (x∗, y∗) is feasible and f (x∗, y∗) (cid:1) f (x, y) for all feasible(x, y) ∈ Nm(x∗, y∗).Definition 10. The penalty function of Pm is defined as in (9) by transforming the con-straint functions of Pm into non-negative functions:Lm(x, y, α, β) = f (x, y) + αTwhere α ∈ Rm and β ∈ Rr .(cid:7)(cid:7)(cid:2)(cid:1)(cid:7) + βT max(cid:7)h(x, y)0, g(x, y)(19)Theorem 3 (Necessary and sufficient ESPC on CLMm of Pm). Suppose (x∗, y∗) ∈ Rv ×Dwis a point in the mixed search space of Pm, and x∗ satisfies the constraint qualificationcondition in (12) for given y∗, then (x∗, y∗) is a CLMm of Pm if and only if there existfinite α∗ (cid:2) 0 and β∗ (cid:2) 0 such that the following condition is satisfied:∗∗, yLm(x, βfor any α∗∗ > α∗ and β∗∗ > β∗, and for all (x, y) ∈ Nm(x∗, y∗), α ∈ Rm, and β ∈ Rr ., α, β, y, α, β) (cid:1) Lm(x) (cid:1) Lm(x, y, α)∗∗∗∗∗∗∗∗∗∗(20)The proof of the theorem is shown in Appendix C.The following corollary facilitates the search of points that satisfy (20) by decompos-ing the condition into two independent necessary conditions. It follows directly from (18),which defines Nm(x, y) to be the union of points perturbed in either the discrete or the con-tinuous subspace. Such decomposition cannot be accomplished if a mixed neighborhoodlike Nc(x) × Nd (y) were used.Corollary 1. Given the definition of Nm(x, y) in (18), the ESPC in (20) can be rewritteninto two necessary conditions that, collectively, are sufficient:∗∗Lm(x∗∗, y, α, β) (cid:1) Lm(x∗)|x∗ ,) (cid:1) Lm(x, ywhere y ∈ Nd (y∗∗∗, β, α, y, y∗∗∗, αLm(x∗∗, β∗∗) (cid:1) Lm(x∗, y, α∗∗∗∗), β∗∗∗, α, β∗∗) where x ∈ Nc(x∗)|y∗ .(21)(22)In summary, we have presented in this section a set of necessary and sufficient condi-tions that govern all constrained local minima in nonlinear continuous, discrete, and mixedoptimization problems. In contrast to general penalty approaches, α∗∗ and β∗∗ always ex-ist in ESPC for any constrained local minimum, provided that the constraint qualificationcondition is satisfied in the continuous subspace. The similarity of these three conditionsallows problems in these three classes to be solved in a unified fashion.3.2. Search procedures for finding extended saddle pointsAs is discussed in the last section, a CLMc of Pc can be found by gradually increasingα∗∗ and β∗∗, while minimizing Lc(x, α∗∗, β∗∗), until α∗∗ > α∗ and β∗∗ > β∗. This obser-vation allows us to solve Pc by an iterative search in Fig. 8(a). (The algorithm for solvingB.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231205procedure ESP_search_continuous(Pc , x, αmax, βmax);α ← 0; β ← 0;repeatfor (i = 1, . . . , m) if (hi (x) (cid:13)= 0 and αi < αmaxfor (j = 1, . . . , r) if (gj (x) (cid:1) 0 and βj < βmaxrepeatij) then increase αi by δ;) then increase βj by δ;perform descent of Lc(x, α, β) with respect to x;until a local minimum of Lc(x, α, β) is found;for all hi (x) (cid:13)= 0 and βj > βmaxuntil (αi > αmaxjifor all gj (x) (cid:1) 0)or a CLMc of Pc is found;return CLMc if found;end_procedure(a)procedure ESP_search_mixed(Pm, z, αmax, βmax);α ← 0; β ← 0;repeatfor (i = 1, . . . , m) if (hi (z) (cid:13)= 0 and αi < αmaxfor (j = 1, . . . , r) if (gj (z) (cid:1) 0 and βj < βmaxrepeatij) then increase αi by δ;) then increase βj by δ;perform descent of Lm(z, α, β) with respect to x for given y;until a local minimum of Lm(z, α, β) with respect to x for given y is found;repeatperform descent of Lm(z, α, β) with respect to y for given x;until a local minimum of Lm(z, α, β) with respect to y for given x is found;until (αi > αmaxfor all hi (z) (cid:13)= 0 and βj > βmaxfor all gj (z) (cid:1) 0)jior a CLMm of Pm is found;return CLMm if found;end_procedure(b)Fig. 8. Iterative procedures to look for CLMc of Pc and CLMm of Pm. The bounds on α and β, αmax and βmax,are user-provided. (a) Direct implementation of (13) to look for CLMc of Pc for given starting point x. (b) Directimplementation of (21) and (22) to look for CLMm of Pm for given starting point z = (x, y).Pd is similar and is not shown.) Assuming α∗∗ and β∗∗ have been found in the outer loopand according to the second inequality in (13), the inner loop looks for a local minimumof Lc(x, α∗∗, β∗∗) in order to find x∗. If a feasible solution to Pc is not found at the localminimum x of Lc(x, α∗∗, β∗∗), the penalties corresponding to the violated constraints areincreased. The process is repeated until a CLMc is found or when α∗∗ (respectively β∗∗)is larger than the user-provided maximum bound αmax (respectively βmax), where αmax(respectively βmax) is chosen to be so large that it exceeds α∗ (respectively β∗).Fig. 8(b) shows the pseudo code which solves Pm by looking for x∗, y∗, α∗∗, and β∗∗that satisfy Corollary 1. By performing descents of Lm(x, y, α, β) in the continuous anddiscrete neighborhoods in the two inner loops, it looks for a local minimum (x∗, y∗) ofLm(x, y, α, β) with respect to points in Nm(x, y). The outer loop increases the penaltiesof violated constraints and stops when a CLMm is found or when α∗∗ (respectively β∗∗)exceeds its maximum bound αmax (respectively βmax).206B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231Because Lc(x, α∗∗, β∗∗) and Lm(x, y, α∗∗, β∗∗) may have many local minima andsome of them do not correspond to constrained local minima even when α∗∗ > α∗ andβ∗∗ > β∗, it is possible for the iterative procedures in Fig. 8 to terminate without finding aconstrained local minimum. The following theorem summarizes this observation.Theorem 4. When αmax > α∗ and βmax > β∗, the iterative procedure in Fig. 8(a) (re-spectively 8(b)) generates fixed points that are necessary but not sufficient to satisfy (13)(respectively (21) and (22)).To cope with this issue, we discuss three additional strategies to augment the procedurein Fig. 8(b). These strategies are general and are applicable when looking for CLMc andCLMd .First, when α∗∗ and β∗∗ reach their upper bounds during a search but a local minimumof Lm(x, y, α∗∗, β∗∗) does not correspond to a CLMm of Pm, then a different local min-imum of the function will need to be found. Instead of restarting the search from a newstarting point, reducing α∗∗ and β∗∗ will change the terrain and “lower” the barrier of thepenalty function, thereby allowing a local search to continue on the same trajectory andmove to another local minimum of the penalty function. By repeatedly increasing α∗∗ andβ∗∗ to their upper bounds and reducing them to some lower bounds, a local search algo-rithm will be able to visit multiple local minima of the penalty function. Alternatively, it ispossible to escape from a local minimum of the penalty function by using a global-searchalgorithm in the inner loops. Since these two strategies offset each other in their effects,only one of them will need to be applied.Second, the ease of finding a CLMm depends on the number of CLMm’s in the searchspace of Pm, which in turn depends on the neighborhood function chosen. If the neighbor-hood of each point is the entire search space, then finding a CLMm amounts to finding aconstrained global minimum. On the other hand, if the neighborhood of each point is onlythe point itself, then any feasible point in the search space is a CLMm. In this case, sincethe neighborhood is limited, only random probing can be applied, and finding a CLMmamounts to feasibility search. In practice, we choose the neighborhood of each point to berich enough in order to achieve a balance between the number of neighbors of each pointand the number of CLMm’s in the search space.Last, because functions in planning problems may not be in closed form and their gra-dients are unavailable, it is hard to locate local minima of the penalty function in thiscase. One way to address this issue is to generate probes based on deterministic, proba-bilistic, or genetic mechanisms and accept them based on some deterministic or stochasticcriteria. For example, in our experiments in Section 5.1 on using ASPEN to solve sub-problems, new probes generated using ASPEN’s built-in mechanism during the descent ofthe penalty function are accepted based on the Metropolis probability when Ld increases.This mechanism allows descents as well as occasional ascents of the penalty function. Inmore general cases, as is illustrated in the stochastic constrained simulated annealing algo-rithm [50], new probes generated are accepted based on the Metropolis probability whenLm increases along one of the x or y dimension and decreases along the α or β dimension.B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–2312074. Partitioning of ESPC for temporal planning problemsIn this section, we solve Pt in (1) by finding plan z that is a CLMm with respect tofeasible plans in its mixed neighborhood Nm(z). After showing that z satisfies the ESPCin (20), we decompose the ESPC into a set of necessary conditions that collectively aresufficient. Problem Pt is then solved by iteratively finding an extended saddle point in eachstage and by resolving those violated global constraints using appropriate penalties.4.1. Necessary and sufficient ESPC for partitioned subproblemsTo simplify our discussion, we do not partition plan z into discrete and continuous partsin the following derivation, although it is understood that each partition will need to befurther decomposed in the same way as in Corollary 1. To enable the partitioning of theESPC into independent necessary conditions, we define a mixed neighborhood of plan zas follows:Definition 11. Np(z), the mixed neighborhood of z for partitioned problem Pt , is definedas:Np(z) =N(cid:10)t=0N (t)p (z) =N(cid:10)(cid:8)zt=0(cid:8)(cid:8) | z(t) ∈ Nm(cid:1)z(t)(cid:2), and z∀zi(s) /∈ z(t), s (cid:13)= t, i = 1, . . . , us(cid:8)i(s) = zi(s)(cid:9),where Nm(z(t)) is the mixed-space neighborhood of z(t) in Stage t.Intuitively, Np(z) is decomposed into N + 1 neighborhoods, each perturbing z in onlyone of the stages of Pt , while keeping the overlapped variables consistent in the otherstages. The size of Np(z) defined in (23) is smaller than the Cartesian product of theneighborhoods across all stages.By considering Pt as a MINLP and by defining the corresponding penalty function, wecan apply Theorem 3 as follows.Definition 12. Let Φ(z, γ , η) = γ T|H (z)| + ηT max(0, G(z)) be the sum of the trans-formed global constraint functions weighted by their penalties, where γ = (γ1, . . . , γp)T ∈Rpare the penalty vectors for the global constraints. Then thepenalty function for Pt and the corresponding penalty function in Stage t are defined as in(9) by transforming the constraint functions of Pt into non-negative functions:and η = (η1, . . . , ηq )T ∈ RqLm(z, α, β, γ , η) = J (z) +N(cid:11)(cid:8)α(t)T(cid:7)(cid:7)h(t)(cid:1)z(t)(cid:2)(cid:7)(cid:1)(cid:7) + β(t)T max0, g(t)(cid:1)z(t)(cid:12)(cid:2)(cid:2)t=0+ Φ(z, γ , η),(cid:1)z, α(t), β(t), γ , η(cid:2)Γm(cid:7)(cid:7)h(t)= J (z) + α(t)T(cid:1)0, g(t)+ β(t)T max(cid:1)z(t)(cid:1)z(t)(cid:2)(cid:7)(cid:7)(cid:2)(cid:2)(23)(24)+ Φ(z, γ , η),208B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231where α(t) = (α1(t), . . . , αmt (t))T ∈ Rmt and β(t) = (β1(t), . . . , βrt (t))T ∈ Rrt are thepenalty vectors for the local constraints in Stage t.Lemma 1. Plan z is a CLMm of Pt with respect to Np(z) if and only if there exist finiteα∗ (cid:2) 0, β∗ (cid:2) 0, γ ∗ (cid:2) 0, and η∗ (cid:2) 0 such that the following ESPC is satisfied:∗Lm(z, α, β, γ , η) (cid:1) Lm(z∗∗∗, α, β∗∗, γ∗∗, η∗∗) (cid:1) Lm(z, α∗∗∗∗, γ, β∗∗for any α∗∗ > α∗, β∗∗ > β∗, γ ∗∗ > γ ∗ and η∗∗ > η∗, and for all α ∈ RR(cid:13), and z ∈ Np(z∗).i=0 ri , γ ∈ Rp, η ∈ RqN∗∗, η(cid:13))(25)Ni=0 mi , β ∈Based on Lemma 1, we next show the partitioning of (25) into multiple conditions.Theorem 5 (Partitioned necessary and sufficient ESPC on CLMm of Pt ). Given Np(z),the ESPC in (25) can be rewritten into N + 2 necessary conditions that, collectively, aresufficient:(cid:1)z, α(t), β(t), γ, γ∗∗∗∗∗∗∗∗∗∗(cid:2)(cid:2)∗Γm, η, α(t)∗∗(cid:1) Γm(cid:1) Γm∗, α, αp (z∗), α(t) ∈ Rmt , β(t) ∈ Rrt , and t = 0, . . . , N ., γ , η) (cid:1) Lm(z, β(t)∗∗, β(t)∗∗, β, β, η, γ, γ∗∗∗∗∗∗∗∗)∗(cid:1)z(cid:1)z, α(t)∗∗∗∗, η∗∗∗∗(cid:2),, η∗Lm(zfor all z ∈ N (t)(26)(27)The proof is shown in Appendix D.Theorem 5 shows that the ESPC in (25) can be partitioned into N + 1 necessary con-ditions in (26) on the local constraints and an overall necessary condition in (27) on theglobal constraints across the subproblems. A close examination shows that the local ex-tended saddle points in Stage t that satisfy (26) are the local minima of (24) with respect toz (the second inequality of (26)), when α(t)∗∗ and β(t)∗∗ are larger than some thresholdsα(t)∗ and β(t)∗ such that all the constraints in Stage t are forced to be satisfied (the firstinequality of (26)). In essence, a point that satisfies (26) in Stage t is a solution to the fol-lowing MINLP P (t), where the original objective function J (z) is biased by the violatedtglobal constraints:(P (t)t): minz(t)subject to h(t)(cid:7)(cid:7)(cid:2)(cid:1)(cid:7) + ηT max(cid:7)H (z)J (t)(z) = J (z) + γ T0, G(z)(cid:2)(cid:1)(cid:2)(cid:1)z(t)z(t)= 0 and g(t)(cid:1) 0.(28)The bias on the violated global constraints when solving P (t)is important because it leadsthe search towards points that minimize this bias. When the penalties on the violated globalconstraints are large enough, solving P (t)t will lead to points, if they exist, that satisfy theglobal constraints.tIn short, finding points that satisfy (25) can be reduced to solving multiple MINLPsin (28), and to the reweighting of violated global constraints defined in (27).defined by P (t)tB.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231209(a)procedure ESP_partitioned_search_mixed(Pt , z, αmax, βmax, γ max, ηmax);γ ← 0; η ← 0;repeat// increase the penalties on violated global constraints //for (i = 1, . . . , p) if (Hi (z) (cid:13)= 0 and γi < γ maxfor (j = 1, . . . , q) if (Gj (z) (cid:1) 0 and ηj < ηmaxfor t = 0 to N // iterate over all N + 1 stages to solve P (t)apply an existing solver or call ESP_search_ mixed(P (t)jit) then increase γi by δ;) then increase ηj by δ;in each stage //, z, αmax, βmax) to solve P (t)ttend_for;until (γi > γ maxireturn CLMm if found;for all Hi (z) (cid:13)= 0 and ηj > ηmaxfor all Gj (z) (cid:1) 0) or a CLMm of Pt is found.jend_procedure(b)Fig. 9. The partition-and-resolve procedure to look for CLMm of Pt . The bounds αmax, βmax, γ max, and ηmaxare user-provided. (a) Partitioned search to look for points that satisfy (26) and (27). (b) Implementation forfinding CLMm of Pt that satisfies (26) and (27) for given starting point z.4.2. The partition-and-resolve proceduretFig. 9 presents the partition-and-resolve procedure, which looks for points that satisfythe conditions in Theorem 5. The inner loop of Stage t in Fig. 9(b) solves P (t)by look-ing for an extended saddle point that satisfies (26). This can be done by the procedure inFig. 8(b), using fixed γ and η specified in the outer loop, or by an existing solver. Thelatter is possible because P (t)is a well-defined MINLP. This is illustrated in Section 5where we use the ASPEN and the MIPS planners to solve the partitioned planning sub-problems. After solving the subproblems, the penalties on those violated global constraintsare increased in the outer loop. The process is repeated until a CLMm to Pt has been foundor when γ and η exceed their maximum bounds. Similar to the result in Theorem 4, theprocedure in Fig. 9 generates fixed points that are necessary but not sufficient to satisfy(26) and (27). Hence, additional steps described in Section 3.2 are needed to help escapefrom local minima of the penalty function that are not feasible points to Pt .t5. Experimental resultsIn this section, we describe our experimental results on using the discrete-space AS-PEN [9] and the mixed-space MIPS [12] planners to solve partitioned planning bench-marks. We show significant improvements in their solutions, both in terms of the qualityof the plans generated and the execution times to find them.210B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–2315.1. SGPlant(ASPEN): A planner using ASPEN to solve partitioned problemsWe describe the ASPEN (Automated Scheduling and Planning Environment) system [9]developed at the Jet Propulsion Laboratory and its available benchmarks on spacecraftoperation planning. We then present our prototype planner SGPlantt (ASPEN, N, reparti-tioning_strategy) that partitions a problem along its temporal horizon into N subproblemsof the form in P (t), that calls ASPEN to solve the subproblems, that resolves the violatedglobal constraints, and that repartitions the problem if necessary. Finally, we compare theperformance between ASPEN and SGPlant(ASPEN, N, repartitioning_strategy).tASPEN [9] is an objective-based planning system for the automated planning andscheduling of complex spacecraft operations. It involves generating a sequence of parallellow-level spacecraft commands from a set of high-level science and engineering goals.Using a discrete time horizon and a discrete state space, an ASPEN model encodesspacecraft operability constraints, flight rules, spacecraft hardware models, science exper-iment goals, and operations procedures. It defines various types of schedule constraintsthat may be in procedural form among or within the parallel activities to be scheduled.Such constraints include temporal, decomposition, resource, state-dependency, and goalconstraints. In addition, the quality of a schedule is defined by a preference score, which isa weighted sum of multiple preferences (that may also be procedural) to be optimized bythe planner. Preferences can be related to the number of conflicts, the number of actions,the value of a resource state, or the value of an activity parameter.Since ASPEN cannot search for feasible plans and optimize plan quality at the sametime, it alternates between a repair phase and an optimization phase. In the repairphase [35], ASPEN generates an initial schedule that may have conflicts and searchesfor a feasible plan from this initial plan, using iterative repairs to resolve conflicts indi-vidually. In a repair iteration, the planner must decide at each choice point a conflict tobe resolved and a conflict-resolution method from a rich collection of repair heuristics. Inthe optimization phase, ASPEN uses a preference-driven, incremental, local-optimizationmethod to optimize plan quality defined by the preference score. It decides the best searchdirection at each choice point, based on information from multiple choice points. In our ex-periments, we allow ASPEN to alternate between a repair phase with an unlimited numberof iterations and an optimization phase with 200 iterations (both defaults in ASPEN).The ASPEN software can be tested on several publicly available benchmarks onscheduling parallel spacecraft operations. In this paper, we have tested all the four availablebenchmarks in the public domain. (a) The CX1-PREF benchmark [53] models the planningof operations of the Citizen Explorer-1 (CX-1) satellite that involve taking data related toozone and downloading the data to ground for scientific analysis. Its problem generatorcan generate problem instances with a user-specified number of satellite orbits. In our ex-periments, we have studied CX1-PREF with 8 and 16 orbits, respectively. (b) The DCAPSbenchmark [34] models the operation of DATA-CHASER shuttle payload that is managedby the University of Colorado at Boulder. (c) OPTIMIZE and PREF are two benchmarksdeveloped at JPL that come with the licensed release of ASPEN.Implementation of the partition-and-resolve search. Based on Fig. 9, we have imple-mented SGPlant(ASPEN, N, repartitioning_strategy) [8]. In our implementation, we setB.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231211for k = 1 to num_descentscall ASPEN to solve P (t)evaluate Γm(z, α(t), β(t), γ , η) and the Metropolis probability controlled by T ;if Γm(z, α(t), β(t), γ , η) is accepted thenin a child process and to generate a new schedule;tcall ASPEN to apply the action in the main process;update penalties α(t) and β(t) on violated local constraints;num_descents ← 1;for t = 1 to N1. procedure SGPlant(ASPEN, N, repartitioning_strategy)generate initial plan and set initial temperature T ;2.partition time horizon into N stages;3.4.repeat5.6.7.8.9.10.11.12.13.14.15.16.17.18.19.20.21.22. end_procedureuntil no change in z, α, β, γ , η in an iteration;return the best plan found;end_ifend_forend_forupdate penalties γ and η on violated global constraints;num_descents ← min(100, num_descents∗2);reduce temperature T ← T · c where c ∈ (0, 1);if (repartitioning_strategy is DYNP) then repartition the stages end_if;Fig. 10. SGPlant(ASPEN, N, repartitioning_strategy): The partition-and-resolve procedure used in SGPlan thatpartitions a planning problem along its temporal horizon into N subproblems, that calls ASPEN to solve the sub-problems, that resolves the violated global constraints, and that repartitions the problem if necessary. Annealing(lines 9–10) is used to probabilistically accept a probe with worse penalty-function value during descents of Γm.the weight of J (z) in P (t)to 100 (since the preference score is between 0 to 1), initial-ize all penalties to zeros, and increase the penalties of violated global constraints in eachiteration by 0.1.tIn generating a new schedule from the current schedule during descents of Γm (line 8of Fig. 10), ASPEN chooses probabilistically among its repair and optimization actions,selects a random feasible action at each choice point, and applies the selected actionsto the current schedule. Since many of the objectives and constraints in complex space-craft applications are not differentiable, the new schedule generated does not likely fol-low descent directions, and a local search may get stuck easily in local minima of thepenalty function that are not feasible solutions to the original problem. To this end,SGPlant(ASPEN, N, repartitioning_strategy) employs annealing to determine whether toaccept the new schedule (lines 9–10). Using a parameter called temperature, it acceptsthe new schedule with larger Γm based on the Metropolis probability, with the acceptanceprobability decreasing as the temperature decreases (c ∈ (0, 1)). In our implementation, wefix the initial temperature to 1000 and reduce it in every iteration by a factor c = 0.8.Two other important issues that must be addressed in our partition-and-resolve imple-mentation are the number of stages used and the duration of each. In ASPEN, a conflict hasan active window bounded by a start time and an end time called the time points. Adjacenttime points can be collapsed into a stage, since ASPEN has discrete time horizons.We have studied both the static and the dynamic partitioning of stages. In static parti-tioning, SGPlant(ASPEN, N, STATICP) partitions the horizon statically and evenly into N212B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231Fig. 11. Number of iterations taken by static and dynamic partitioning to find a feasible plan in the 8-orbitCX1-PREF problem.stages. This simple strategy often leads to an unbalanced number of time points in differentstages. During a search, some stages may contain no conflicts to be resolved, while othersmay contain a lot of conflicts. Such an imbalance leads to search spaces of different sizesacross different stages and search times that may be dominated by those in a few stages.To achieve a better balance of activities across stages, SGPlant(ASPEN, N, DYNP) ad-justs the boundary of stages dynamically. This is accomplished by finding M, the numberof time points in the horizon related to conflicts, at the end of the outer loop (line 15) andby partitioning the horizon into N stages in such a way that each stage contains approx-imately the same number (M/N ) of such time points (line 19). To determine the best N ,Fig. 11 plots the number of iterations taken by static and dynamic partitioning in finding afeasible schedule of the 8-orbit CX1-PREF problem. The results show that N = 100 is agood choice. Since other benchmarks lead to similar conclusions, we set N = 100 in ourexperiments. Note that although N is relatively large, some stages will have all their localconstraints satisfied as planning progresses. To avoid managing such defunct stages, ourimplementation collapses automatically adjacent defunct stages in such a way that eachresulting stage contains at least one unsatisfied local constraint. Consequently, the actualnumber of stages used during planning can be much smaller than the value of N shownhere.Experimental results. Fig. 12 compares the performance of ASPEN, SGPlantt(ASPEN,100, STATICP), SGPlant(ASPEN, 1, STATICP) (a version of our planner without parti-tioning), and SGPlant(ASPEN, 100, DYNP) on the four benchmarks described earlier inthis section. In each graph, we plot the quality of the best feasible schedule found withrespect to the number of search iterations. Although SGPlant is not guaranteed to findoptimal schedules, it can generate multiple locally optimal feasible schedules and keepimproving on the best schedule found. In our experiments, we maintain the best sched-ule found as more search time is spent. The results show that descents using annealingin SGPlant(ASPEN, 1, STATICP) have little improvements over the original ASPEN: theylead to better solutions in PREF but worse solutions in CX1-PREF with 16 orbits, DCAPS,and OPTIMIZE. Our results also show that SGPlant(ASPEN, 100, STATICP) is able tofind schedules of the same quality one to two orders of magnitude faster than ASPENand SGPlant(ASPEN, 1, STATICP), as well as much better schedules when they converge.B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231213(a)(c)(b)(d)(e)Fig. 12. Quality-time comparisons of SGPlant(ASPEN, 1, STATICP), SGPlant(ASPEN, 100, STATICP), AS-PEN, and SGPlant(ASPEN, 100, DYNP). (All runs involving SGPlant were terminated at 24,000 iterations.)(a) CX1-PREF with 8 orbits. (b) CX1-PREF with 16 orbits. (c) DCAPS. (d) PREF. (e) OPTIMIZE.Further, dynamic partitioning can lead to better schedules in shorter times than those ofstatic partitioning. Hence, we conclude that improvements in SGPlant are mainly due topartitioning and not to annealing.5.2. SGPlant(MIPS): A planner using MIPS to solve partitioned problemsIn this section, we describe our results on partitioning PDDL2.1 benchmarks along theirtemporal horizons and on using the mixed-space MIPS planner [12] to solve the partitionedsubproblems.214B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231MIPS [12] is a heuristic planner that performs static analysis of a problem instance inmixed space and continuous time, searches for an optimized sequential plan, and performsa critical path analysis called PERT to generate optimal parallel plans from a sequence ofoperators and their precedence relations. Using a weighted A∗ algorithm, it finds an optimal∈ G in a state space of propositional factsfeasible path from initial state sI to goal state sGand numeric variables. It can also optimize an arbitrary objective by incorporating theobjective in its heuristic function.By generating approximate relaxed plans for each encountered state, MIPS uses the re-laxed planning heuristic (RPH) [18] for guidance. RPH builds a relaxed plan by using thewell-known planning graph proposed in Graphplan [5] but by ignoring the delete effects ofactions. It then extracts a relaxed plan from the planning graph and computes an estimateddistance from the current state to the goal state. MIPS extends RPH with numeric infor-mation by using a combined propositional and numeric forward/backward approximationscheme. It can also integrate PERT scheduling in its heuristic estimate in order to favorstates with a smaller parallel plan length.MIPS can handle the STRIPS subset of the PDDL language and can cope with numericquantities and durations in PDDL 2.1 (level 2-3 in PDDL+) [14]. In PDDL2.1, actions arerepresented by parameters, durations, conditions, and effects. A condition may be definedin terms of logical or functional expressions of ground atoms, and a conditional effect canbe evaluated either at the start, the end, or during the interval of an action. MIPS can alsohandle some additional features from ADL, namely, negative preconditions and (universal)conditional effects.MIPS competed in the second and the third International Planning Competitions andwas awarded “Distinguished Performance” in the fully automated track in both. We useMIPS in our experiments because it performs well and its source code is readily available.Implementation of the partition-and-resolve search. Fig. 13 shows the pseudo code ofSGPlant(MIPS, N ). It generates an initial (possibly infeasible) plan of a planning prob-lem, formulates the problem in a penalty function, decomposes the states into N + 1 stages,solves each subproblem independently, and resolves the violated global constraints by in-creasing their penalties.MIPS specifies the state of a problem as s = (sf , sr ), where sf contains the set of nftrue facts at s, and sr is an nr -vector of instantiated values of the numeric variables at s. Itfurther partitions the set of grounded facts into symmetry groups [12] in the static-analysisphase in such a way that each element of sf is a fact from a unique symmetry group. Forexample, a small problem may have three symmetry groups:Group 1 = (at person1 city0, at person1 city1, at person1 city2, in person1 plane1);Group 2 = (at person2 city0, at person2 city1, at person2 city2, in person2 plane1);Group 3 = (at plane1 city0, at plane1 city1, at plane1 city2).A valid state can have sf = (at person1 city0, at person2 city1, at plane1 city2).Based on the definition of symmetry groups, we define the neighborhood Nm(s) of s toinclude s and all states s(cid:8) that differ from s by exactly one fact, where the facts that differB.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231215procedure SGPlant(MIPS, N )generate initial plan using relaxed operators;repeatiter ← 0;for t = 0 to N // initial state si (t) in Stage t from initial plan //num_trials ← 0;repeatnum_trials ← num_trials + 1;generate a new initial state in Nm(si (t)) for Stage t ;call MIPS to solve P (t)evaluate J (t)(z) in (28) of the solution plan generated by MIPS;in Stage t ;tuntil J (t)(z) is improved or num_trials > max_trials;end_forupdate penalty vector γ on violated global constraints;iter ← iter + 1;if (iter mod τ is 0) then repartition the stages end_if;until no change in z and γ in an iteration;return the best plan found;1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16.17.18.19. end_procedureFig. 13. SGPlant(MIPS, N ): The partition-and-resolve procedure in SGPlan that partitions a PDDL2.1 planningproblem along its temporal horizon into N + 1 subproblems, that calls MIPS to solve the subproblems, and thatresolves the violated global constraints.between s and s(cid:8) are in the same symmetry group. That is, s and s(cid:8) are neighboring stateswhen s and s(cid:8) differ by only two facts f ∈ s and f (cid:8) ∈ s(cid:8) in the same symmetry group.To generate a neighboring state s(cid:8) from s, we randomly pick a fact in s and perturb it toa different fact in the same symmetry group. Note that, since sr , the numeric part of s, isnot changed in the process, there may not exist an action for a valid transition from s to s(cid:8).To quantify the notion of a valid transition, we measure the distance D(s, q) betweentwo states s = (sf , sr ) and q = (qf , qr ) as the number of different facts between s and qplus the normalized difference between their numerical parts if s (cid:13)= q:D(s, q) = (number of different facts between s and q)|nr(cid:11)− qri|srimax(sri , qri ).s(cid:13)=q(cid:7)(cid:7)(cid:7)(cid:7)+i=1(29)Hence, D(s, q) = 0 if and only if s and q are identical states. We further define S(s), theset of successor (different from neighborhood) states of s, in such a way that there existsa valid action that brings s to q for all q ∈ S(s). Last, we define the transition distanceT (s, q) to be the minimum distance between s and q over all successors of s:T (s, q) = minv∈S(s)D(v, q).(30)According to this definition, T (s, q) = 0 when there exists a valid action to bring s to q.Based on the concepts on neighborhood, state transition, and transition distance, wecan now specify the local planning subproblem P (t)hasinitial state si(t) and goal state si(t +1). (See Fig. 14 for the states defined in Stage t.) Sincethis initial local plan may be infeasible, we need to formulate P (t)that, when solved, willin Stage t. After partitioning, P (t)ttt216B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231Fig. 14. Formulation of the planning subproblem P (t)tin Stage t to be solved by MIPS.hopefully make the overall planning problem feasible (line 10 of Fig. 13). This subproblemhas the same domain specification as the original problem, initial state s(cid:8)i(t) ∈ Nm(si(t)),and goal state si(t + 1). In addition, there are two global constraints at the boundariesbetween Stage t and the predecessor and successor stages:(cid:2)(cid:1)sg(t), si(t + 1)(cid:1)sg(t − 1), sHt−1(z) = THt (z) = T= 0;= 0.(31)(cid:2)(cid:8)i(t)Hence, Ht−1(z) = 0 (respectively Ht (z) = 0) is satisfied if and only if there is a validaction to bring sg(t − 1) (respectively sg(t)) to si(t) (respectively si(t + 1)). These globalconstraints are then added as biases in the objective of P (t)as follows:tJ (t)(z) = J (z) + γt−1Ht−1(z) + γt Ht (z),(32)where γt−1 and γt are the fixed penalties associated with the two global constraints whenP (t)is solved. The other constraints of the subproblem in Stage t remain unchanged.tAfter solving P (t), MIPS returns a locally optimal feasible plan from s(cid:8)i(t) to si(t + 1)if one exists; otherwise, it returns a feasible plan from s(cid:8)i(t) to sg(t) that minimizes (32).We accept this plan if it improves J (t)(z); otherwise, we repeat the process by using a newinitial state in Nm(si(t)) until we find a better plan, or when the maximum number of trialsis exceeded (line 12 of Fig. 13). In our experiments, we set max_trials to 5.tAfter completing the N + 1 subproblems in an iteration (line 14), we update the penal-ties of all violated global constraints, using ω > 0 to control the rate of increase:γt ← γt + ω · Ht (z),t = 0, 1, . . . , N.(33)We set heuristically ω = 0.01Ja, where Ja is the average value of J (z) in the last threeiterations.Similar to the partition-and-resolve implementation of ASPEN, we repartition the stagesdynamically by adjusting the boundary of stages every certain number (τ in Fig. 13) ofiterations. This is accomplished by counting the number of state transitions from sI to sGat the end of the outer loop (line 16) and by redefining the stage boundaries in order foreach stage to have approximately the same number of state transitions. After repartitioning,the number of violated global constraints in a stage may be different from one. In ourexperiments, we set N = 20 and τ = 5.B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231217Experimental results. We show that SGPlant(MIPS, 20) improves significantly over theoriginal MIPS planner on a set of PDDL2.1 planning benchmarks used in the Third In-ternational Planning Competition. The problems studied belong to a number of domains,including DepotNumeric, DepotSim, DepotTime, DriveLogNumeric, DriveLogSim, Drive-LogTime, ZenoTravelNumeric, ZenoTravelSim, and ZenoTravelTime.As a reference, we also show the performance of LPG, the best automated planner inthe competition. Because we did not have access to the source code of LPG at the timeof our experiments, we were not able to report the performance of using LPG as a basicsolver in SGPlant.4We conducted our experiments on an AMD Athlon MP2000 PC with Redhat Linux7.2. In our experiments on MIPS and LPG, we used the August-2003 version of theirexecutables with default parameters downloaded from their Web sites. In accordance tothe way that planners were run in the International Planning Competitions, we used a fixedrandom seed of 1000 in LPG, MIPS, and SGPlant(MIPS, 20), making them behave likedeterministic planners. We also used the same parameters specified for the original MIPSin the version of MIPS embedded in SGPlant(MIPS, 20). We then ran each planner onceon each problem instance for a maximum time limit of 1000 sec.For the 120 (out of a total of 160) instances solvable by MIPS, Fig. 15(a) compares thequality of the solution of each instance found by MIPS and by SGPlant(MIPS, 20) when itwas given the same amount of time taken by MIPS to solve that instance. It measures thefraction of instances that SGPlant(MIPS, 20) found a better solution using the same amountof time taken by MIPS. In contrast, Fig. 15(b) compares the time taken by MIPS to solve aninstance and that by SGPlant(MIPS, 20) when it found a solution of the same or better qual-ity as MIPS for that instance. It measures the fraction of instances that SGPlant(MIPS, 20)found a solution faster and of the same or better quality as that of MIPS. The graphs do notinclude the results on the 30 instances for which SGPlant(MIPS, 20) could solve but MIPScould not find any feasible plan in 1000 sec. The results show that SGPlant(MIPS, 20)is able to improve over MIPS in 81.7% of the cases in quality or 83.2% of the cases intime on the PDDL2.1 instances solvable by MIPS. In comparison, an implementation ofSGPlang(MIPS) that partitions a problem by its subgoals leads to comparable performanceand is able to improve over MIPS in 80.5% of the cases in quality or 80.1% of the cases intime on the PDDL2.1 instances solvable by MIPS [48].Of the 150 of the 160 instances solvable by SGPlant(MIPS, 20), SGPlant(MIPS, 20)can find a feasible solution with better time (respectively quality) than MIPS in 94.4%(respectively 93.8%).As a reference, Fig. 16 shows that SGPlant(MIPS, 20) has comparable normalized per-formance with respect to that of LPG. The results show that SGPlant(MIPS, 20) is ableto improve over LPG in 49.6% of the cases in quality or 54.2% of the cases in time. The4 At the time of this revision, we have finished implementing SGPlang(FF/LPG, N ), a planner that partitionsthe constraints of a problem instance according to its subgoals into N + 1 subproblems and that calls eitherFF or LPG to solve the subproblems [7]. SGPlang(FF/LPG, N ) participated in the Fourth International PlanningCompetition and won the first prize in the suboptimal temporal metric track and the second prize in the suboptimalpropositional track. It ranked better than a new version of LPG in both tracks. Due to the extensive redesigninvolved, we plan to report its features and performance in a future paper.218B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231(a)(b)Fig. 15. Normalized time and quality of SGPlant(MIPS, 20) with respect to MIPS on the 120 instances solvableby MIPS (out of a total of 160 instances). The time and quality of MIPS are normalized to (1, 1). (a) Distributionof the quality of solutions found by SGPlant(MIPS, 20) normalized with respect to that of the correspondingsolutions of MIPS, each using the same amount of time taken by MIPS to find the solution. (b) Distribution of thetimes taken by SGPlant(MIPS, 20) to find a solution of the same or better quality as that of MIPS, normalizedwith respect to the time taken by MIPS to find the solution.improvements over LPG are not substantial because SGPlant(MIPS, 20) inherits MIPS’limitations in its performance and may not be able to improve over LPG when MIPS per-forms worse than LPG to start with.Table 1 presents the complete results on the 160 instances tested. Since MIPS wasnot designed to work in an anytime mode and can find only one solution, whereasSGPlant(MIPS, 20) and LPG can generate multiple solutions with improving quality,we list for each instance the solution time and quality of MIPS, and those of thefirst and the final solutions found by SGPlant(MIPS, 20) and LPG. The results showthat SGPlant(MIPS, 20) outperforms MIPS in most of the instances tested, and thatSGPlant(MIPS, 20) has comparable performance as LPG.6. ConclusionsIn this paper, we have presented a new theory of penalty methods for continuous, dis-crete, and mixed-integer optimization and its application in solving temporal planningproblems partitioned by constraints. Our theory shows that a constrained local minimum ofa general MINLP problem has a one-to-one correspondence to an extended saddle point ofB.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231219(a)(b)Fig. 16. Normalized time and quality of SGPlant(MIPS, 20) with respect to LPG on the 154 instances solvableby LPG (out of 160). The time and quality of LPG are normalized to (1, 1). (a) Distribution of the quality ofsolutions found by SGPlant(MIPS, 20) normalized with respect to that of the corresponding solutions of LPG,each using the same amount of time taken by LPG to find the solution. (b) Distribution of the times taken bySGPlant(MIPS, 20) to find a solution of the same or better quality as that of LPG, normalized with respect to thetime taken by LPG to find the solution.a penalty function with non-negative (transformed) constraint functions, when its penaltiesare larger than some thresholds. Hence, one way to find a constrained local minimum ofthe MINLP is to increase gradually the penalties of those violated constraints and to lookfor a local minimum of the penalty function using any existing algorithm until a solution tothe constrained model is found. Next, by defining a proper neighborhood for MINLPs, weshow the extension of the method to constraint-partitioned MINLPs. Finally, by partition-ing along the time horizon and by using the discrete-space ASPEN and the mixed-spaceMIPS planners to solve partitioned planning subproblems, we have demonstrated signifi-cant improvements on some benchmark problems, both in terms of the quality of the plansgenerated and the execution times to find them. Results on partitioning planning problemsalong the subgoal dimension using the MIPS planner have been reported elsewhere [47,48].Our constraint-partitioning approach is important for reducing the complexity of non-linear constrained planning problems. It leads to subproblems that are much easier to solvebecause each has a significantly smaller number of constraints. It also results in subprob-lems that are very similar to the original problem and, therefore, can be evaluated byexisting planners with little or no modification. Partitioning, however, introduces violatedglobal constraints that may have to be resolved after solving the subproblems. To reduce theTable 1Results on MIPS, SGPlant(MIPS, 20) and LPG in solving some PDDL2.1 benchmark instances. All timing results are in milliseconds, and all solvers were run with amaximum time limit of 1000 sec. “–” means that no solution was found in the time limit. For MIPS, Time and Sol list the solution time and quality (lower are better).For SGPlant(MIPS, 20) and LPG, Time1 and Sol1 list the time and quality of the first solution found, and Timef and Solf list the time and quality of the last solutionfound in the time limit. For each instance, a boxed number indicates the best quality between MIPS and SGPlant(MIPS, 20) on the last solution found in the time limit,whereas the result of LPG is underlined when LPG has better quality than SGPlant(MIPS, 20) (irrespective of time) on the last solution found in the time limit, or whenthey have the same final quality and LPG requires a smaller CPU time220Problem IDDepotNumericlDepotNumeric2DepotNumericSDepotNumeric4DepotNumeric5DepotNumeric6DepotNumeric7DepotNumeric8DepotNumeric9DepotNumeric10DepotSim1DepotSim2DepotSim3DepotSim4DepotSim5DepotSim6DepotSim7DepotSim8DepotSim9DepotSim10DepotSim11DepotSim12DepotSim13DepotSim14MIPSTime1493982543–––15221605–1078503851476135485––169302327–164389––450–Sol324331–––3828–1646.1273.2103.39130.03––67.27111.039–91.04––84.025–SGPlant(MIPS, 20)Sol1Time122.62037.4402310054.61930129.530340––96.1160352180––15140303062029024400–1080530396801302780–350470455212967157–467025482194–17556Timef20901008037540144300–380390350–776604002010402789601507024400–6245019001038440067520259880–182076550Solf22.633.9113075.2–36.314–1024314028157–27431393884–4243LPGTime1304037010807310–250410264980250405018501480127033010100870329701606790425800190530Sol132.653.941214.8331.401–186.225413.719284811015319229263932037126728082114Timef402507682032980239360–3190364602649801787407085031788063630567290177590261304022403490101136202280104258002751090190Solf22.633.9123094.6–36.314413.71024314028142225294617238972804142(continued on next page)..BWWah,Y.Chen/ArtificialIntelligence170(2006)187–231Table 1 (continued)Problem IDDepotSim15DepotSim16DepotTime1DepotTime2DepotTime3DepotTime4DepotTime5DepotTime6DepotTime7DepotTime8DepotTime9DepotTime10DepotTime11DepotTime12DepotTime13DepotTime14DepotTime15DepotTime16DepotTime17DepotTime18DepotTime19DepotTime20DriveLogNumeric1DriveLogNumeric2DriveLogNumeric3DriveLogNumeric4DriveLogNumeric5DriveLogNumeric6DriveLogNumeric7DriveLogNumeric8MIPSTime–4398304054623143242––134754–172322––1203––429824––––908992112124130.1123.119680Sol–73.03259.481192.3111255.082199.456––149.791136.313–245.57––104.526––84.8653––––1099149790771587816678663273SGPlant(MIPS, 20)Sol1Time1262159490592004040212018058520–28033001137113031090334500430870242110230172074090580175940203020303030303045.62550548.90878556.905–95.71476.2861083.33159.416535.334194.096190.55370.199268.59644.88997.05320.292330.286814.294777.22006.51213.61038.91335.5968.4978.72135.6Timef576510595040122105489109321058520–280505390292220437720432840334500324603305202421105587060827028951017244026147042920324250634045902475705215013163031380Solf1982945.62536.66756.50874.25556.905–95.71437.7091038.5109.749437.443194.09650.042122268.59612.00139.125244.276152.867507.68777.198979.998641.9706.9581.3968.396870.6981430.2LPGTime1200370240301002301920150720–600620473901407600–210640153770310203015470500961602040404030304040Sol13467144.63563.667156.9147.4992049.384–691.57773.9051294.999207.417630.139–88.095297.032760.9555.501141.2379.016240.838682.233777.21472.21175.7933797.11000.511913493.5Timef359850306040590174260384170204330–13610372910591380241320575110–567390263450153770678502734501547039894031006042730398013378015210636403796088960176900Solf1702844.63537.66759.68374677.595–94.71435.905914.001111447.028–47.961122.2760.9513.55536.125379.016150.4424.705777.199979.999637.998704581.8965.498866.7991430.299(continued on next page)..BWWah,Y.Chen/ArtificialIntelligence170(2006)187–231221Table 1 (continued)Problem IDDriveLogNumeric9DriveLogNumeric10DriveLogNumeric11DriveLogNumeric12DriveLogNumeric13DriveLogNumeric14DriveLogNumeric15DriveLogNumeric16DriveLogNumeric17DriveLogNumeric18DriveLogNumeric19DriveLogNumeric20DriveLogSim1DriveLogSim2DriveLogSim3DriveLogSim4DriveLogSim5DriveLogSim6DriveLogSim7DriveLogSim8DriveLogSim9DriveLogSim10DriveLogSim11DriveLogSim12DriveLogSim13DriveLogSim14DriveLogSim15DriveLogSim16DriveLogSim17DriveLogSim18MIPSTime629278725014320252134433.112421–––––90909899112117122279.1202269.135117721734240313620–549119–Sol30024026163227214833471753–––––92.0792.2140.0789.1651.1964.1340.09111.26264.3161.2199.21252.41104.29226.44265.43–223.94–SGPlant(MIPS, 20)Sol1Time1323940346.350573.2604972.62301969.416010692.51230156857014398.37489320583.41951012480.11116025219.660500019323.248412030302030303030404040504001301910700–435065920921034798119945113419250855782581562311–867672Timef3836403468001518707430500690893604895807489358045025085060500056459010030404021801074031030702393108184068044235047747062250315350333100–346820208150Solf1816.5144.3340.52033.61161.31752.41157.114398.37384.7905525219.615084919240.15251524052923867168102106125–245327LPGTime150509028017030072010973044407980204360865302030303030403040406050410140170610–82704330Sol12376.4403.41234.15067.4012575.35092.5023254.215932.38911671.29711240.89427835.06615059.186911304798101101113130222113105748462290665–389747Timef8051018224052837049672014898057572023936010973014652038773053556054732020805049070703301724065760339043349022844070980130570422000–384210308990Solf1834.2143.4354.42119.61223.31677.91227.115932.3898816.4978402.80324096.86913163.9929192405251524052923865156102109113–238327(continued on next page)222..BWWah,Y.Chen/ArtificialIntelligence170(2006)187–231Table 1 (continued)Problem IDDriveLogSim19DriveLogSim20DriveLogTime1DriveLogTime2DriveLogTime3DriveLogTime4DriveLogTime5DriveLogTime6DriveLogTime7DriveLogTime8DriveLogTime9DriveLogTime10DriveLogTime11DriveLogTime12DriveLogTime13DriveLogTime14DriveLogTime15DriveLogTime16DriveLogTime17DriveLogTime18DriveLogTime19DriveLogTime20ZenoTravelNumeric1ZenoTravelNumeric2ZenoTravelNumeric3ZenoTravelNumeric4ZenoTravelNumeric5ZenoTravelNumeric6ZenoTravelNumeric7ZenoTravelNumeric8MIPSTime––658075751031241232352332873431530125623039853–236244–––72709291100112103.1183Sol––3033101733921122602683139803403916115581049893–954.94–––13564678675051696419916352821647233543SGPlant(MIPS, 20)Sol1Time11030580807711785503030303030403050405050260120260970–359032470–94730165030303040305030332121333933024232134669544250120726511066671–25081752–17211356418130.711014.619968.513013.91225951443553577.9Timef580803112501002066340407170145703105021952092001315062070210603297022410286970153350–247890540870–4422201245100110050295510342970107230471810433040Solf103029330224517323010216820020632093232319388287242–9831026–152813563.96786.194505.3816960.53974.79152068257.918682.9LPGTime1599250734002040303030304040405050250140260420–194015050135920355902030303030404040Sol1346071330340917349826816842126171419327115551052564661–875361347053264135649770.3996756.537037.80526043.69530475.09814434.99833613Timef599250522800409770030604802790301708605494042940424021046359054607800486120–496970322340439960445890150306840438010136020523460482710590740103100Solf346037530224617323010216820020231893232327388328265–56310611888143613563.9576786.2834505.47916960.5533973.87915204.9967276.89718682.852(continued on next page)..BWWah,Y.Chen/ArtificialIntelligence170(2006)187–231223Table 1 (continued)Problem IDZenoTravelNumeric9ZenoTravelNumeric10ZenoTravelNumeric11ZenoTravelNumeric12ZenoTravelNumeric13ZenoTravelNumeric14ZenoTravelNumeric15ZenoTravelNumeric16ZenoTravelNumeric17ZenoTravelNumeric18ZenoTravelNumeric19ZenoTravelNumeric20ZenoTravelSim1ZenoTravelSim2ZenoTravelSim3ZenoTravelSim4ZenoTravelSim5ZenoTravelSim6ZenoTravelSim7ZenoTravelSim8ZenoTravelSim9ZenoTravelSim10ZenoTravelSim11ZenoTravelSim12ZenoTravelSim13ZenoTravelSim14ZenoTravelSim15ZenoTravelSim16ZenoTravelSim17ZenoTravelSim18MIPSTime192214.12523064136247158903165264438123543.413593524533580781431124234330.12131243137615233734.135513603124518.4233530–––Sol28047795645548041310822302333811476181432821825587079421299789937180.01643.06683.09936.11690.13480.12716.16846.131256.241432.291219.191179.29913.311099.361758.4–––SGPlant(MIPS, 20)Sol1Time120806.1701000027013484425065678.412013259414023061712001527903090129253129202133253024401618214987103282545946006025221054220120303080409013026026024041030057902973022870772460103426180.0899810521272268682615019392005225317682834251017221898220753303401.4Timef376820532000248740240580532050224180160850227350582570498710717330105422010174720303407864014210844020867047584098803138606803052397017415052140400300284310874770103426Solf4787.840341.816694.921793.620435.314747868264.980313.6204608161821205481602522173592280622400323692549556643430643643883989147648013401.4LPGTime17010033011014012505110972027490540107066021466020404050505080160270150200380159032801168029170126230198170Sol114469.096146918.59413761072017.11791871.719273628.688216508.531146591.344250293.875148765.719185678.062612788.87518064364988265693514821173248688210391442556515352108306738745693Timef324990293390232520175780569060256040554260164940786704859101074604340002609970180981406101441607155054120077490548370130330133280110650511800390190679820253060675610Solf4743.60240337.89113389.09620665.69523555.893136158.40679056.13367332.422186907.7583159.25168739.938410776.6561735922805224003236795295364904235766367561042123338434544(continued on next page)224..BWWah,Y.Chen/ArtificialIntelligence170(2006)187–231Table 1 (continued)Problem IDZenoTravelSim19ZenoTravelSim20ZenoTravelTime1ZenoTravelTime2ZenoTravelTimeSZenoTravelTime4ZenoTravelTimeSZenoTravelTime6ZenoTravelTime7ZenoTravelTime8ZenoTravelTime9ZenoTravelTime10ZenoTravelTime11ZenoTravelTime12ZenoTravelTime13ZenoTravelTime14MIPSTime––5050788299931122012232212763534557823Sol––27.25730.210418.1527153.29437.747351.7826142.179160.639119.82181.68155.308126.00790.28375.056SGPlant(MIPS, 20)Time1––Sol1––102030403040401109090110130110188027.25630.409632.4213230.3321.86465.982114.203237.676113.862244.51171.481218.589128.758739.127Timef––1020210302109040808015941044135026422049009069320545780245110412240Solf––27.25630.409617.5674.318.27140.586.695132.73854.887126.756111.51775.89556.9374394.88LPGTime1270380–2020203030403050807090801101290Sol14055–27.25630.5125.080162.09624.29180.84178.96167.735126.458426.288172.094189.528153.593588.987Timef270380–202097802640601740701572601618045566051056044588018290081300112650Solf4055–27.25630.5117.4437519.07141.17385.295125.31758.089121.712106.8784.89556.994342.651..BWWah,Y.Chen/ArtificialIntelligence170(2006)187–231225226B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231amount of backtracking in resolving such global constraints, we have developed new nec-essary conditions that are much stronger than the local constraints for limiting the searchspace to be backtracked in each subproblem.The results presented can be generalized to solve nonlinear constrained optimizationproblems in many engineering applications [49]. Our new theory will allow nonlinearproblems in continuous, discrete, and mixed spaces to be solved in a unified and efficientfashion.AcknowledgementsThe authors would like to thank the two reviewers who provided comprehensive anddetailed comments that help improve this paper.Appendix A. Proof of Theorem 1The proof consists of two parts.“(⇒)” part: Given x∗, we need to prove that there exist finite α∗∗ > α∗ (cid:2) 0 and β∗∗ >β∗ (cid:2) 0 that satisfy (13). The first inequality in (13) is true for all α and β because x∗ is aCLMc, which implies |h(x∗)| = 0 and max(0, g(x∗)) = 0.To prove the second inequality in (13), we prove for any x ∈ Nc(x∗) that there existfinite α∗ (cid:2) 0 and β∗ (cid:2) 0 such that the inequality is satisfied for any α∗∗ > α∗ and β∗∗ >β∗. Let x = x∗ + ε (cid:12)p, where (cid:9) (cid:12)p(cid:9) = 1 is a unit directional vector and ε is an infinitely smallpositive scalar. We consider the following four cases.(1) If all the constraints are inactive inequality constraints, then x ∈ Nc(x∗) is also afeasible point. Hence, (13) implies f (x) (cid:2) f (x∗) and, regardless the choice of the penal-ties,∗∗∗∗, βLc(x, α(A.1)(2) If there exists an equality constraint function hk that is discontinuous along (cid:12)p, then, α, β).) = f (x) (cid:2) f (x) = Lc(x∗∗∗∗∗∗for a small enough ε, there exists a finite positive ξ such that:(cid:7)(cid:7)(cid:7)hk(x)(cid:7) > ξ > 0 = hk(x∗).(A.2)The above must be true because hk(x) would be continuous along (cid:12)p if (A.2) were false.If we set α∗∗k > α∗k= 1 and when ε is small enough, then from (A.2):Lc(x, α∗∗, β∗∗) = f (x) +m(cid:11)∗∗αi(cid:7)(cid:7)(cid:7) +(cid:7)hi(x)(cid:7)(cid:7)(cid:7) > f (x(cid:7)hk(x)∗∗∗∗)., β, α∗i=1∗∗(cid:2) f (x) + αk∗) = Lc(x> f (xr(cid:11)(cid:2)(cid:1)∗∗0, gj (x)βj maxj =1∗) + ε∇xf (x∗)T (cid:12)p + o(ε2) + α∗k ξ(A.3)(3) If there exists an active inequality constraint function gk that is discontinuous along(cid:12)p, then for a small enough ε, there exists a finite positive ξ such that max(0, gk(x)) > ξ >0. If we set β∗∗= 1 and when ε is small enough, this condition implies that:k > β∗kB.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231227Lc(x, α∗∗, β∗∗) = f (x) +(cid:2)(cid:1)∗∗0, gj (x)βj maxm(cid:11)r(cid:11)∗∗αi(cid:7)(cid:7)(cid:7) +(cid:7)hi(x)i=1j =1(cid:2)(cid:1)∗∗0, gk(x)k max∗)T (cid:12)p + o(ε2) + β) + ε∇xf (x∗∗) = Lc(x, α, β).∗∗∗(cid:2) f (x) + β∗> f (x∗> f (x∗k ξ(A.4)(4) Other than inactive inequality constraints, if there are equality and active inequal-ity constraint functions that are continuous along (cid:12)p, then according to the constraint-qualification condition, there must exist an equality or an active inequality constraintfunction that has non-zero subdifferential along (cid:12)p. Suppose there exists an equality con-straint function hk that has non-zero subdifferential along (cid:12)p (the case with an activeinequality constraint function is similar), which means |Dx(hk(x∗), (cid:12)p)| > 0. If we setα∗∗k >|∇x f (x∗)T (cid:12)p||Dx (hk(x∗), (cid:12)p)| and when ε is small enough, then:r(cid:11)m(cid:11)(cid:2)(cid:1)∗∗0, gj (x)βj maxLc(x, α∗∗∗∗, βj =1) = f (x) +∗∗αi(cid:7)(cid:7)(cid:7) +(cid:7)hi(x)i=1(cid:7)(cid:7)(cid:7)(cid:7)hk(x)∗∗(cid:2) f (x) + αk∗(cid:2) f (x) + ε∇xf (x(cid:7)(cid:1)(cid:7)Dx∗∗(cid:2) f (x) + εαk) = Lc(x, α> f (x, β∗∗∗∗∗∗).∗∗)T (cid:12)p + o(ε2) + αk ε(cid:7)(cid:2)(cid:7)(cid:1)(cid:7)∇xf (x(cid:7) −∗), (cid:12)phk(x∗∗(cid:7)(cid:7)Dx(hk(x∗(cid:7)(cid:2)(cid:7))T (cid:12)p∗(cid:7)(cid:7)), (cid:12)p)+ o(ε2)(A.5)The second inequality in (13) is proved after combining cases (1) to (4).“(⇐)” part: Assuming (13) is satisfied, we need to prove that x∗ is a CLMc. Point x∗is feasible because the first inequality in (13) can only be satisfied when h(x∗) = 0 andg(x∗) (cid:1) 0. Since |h(x∗)| = 0 and max(0, g(x∗)) = 0, the second inequality in (13) ensuresthat x∗ is a local minimum when compared to all feasible points in Nc(x∗). Therefore, x∗is a CLMc.Appendix B. Proof of Theorem 2An earlier proof [51,55] is rewritten in terms of our penalty formulation. It consists oftwo parts:“(⇒)” part: Given y∗, we need to prove that there exist finite α∗∗ > α∗ (cid:2) 0 and β∗∗ >β∗ (cid:2) 0 that satisfy (16). In order for α∗ and β∗ to exist for every CLMd y∗, α∗ and β∗must be bounded and be found in finite time. Given y∗, consider all y ∈ Nd (y∗), and letthe initial α∗ = β∗ = 0. For every y such that |h(y)| > 0 (respectively max(0, g(y)) > 0),there is at least one constraint that is not satisfied. For each such constraint, we update itspenalty as follows:(cid:14)(cid:12)∗αi← max∗i ,αf (y∗) − f (y)|hi(y)|(cid:7)(cid:7)(cid:7)hi(y)(cid:7) > 0,if(B.1)228B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231(cid:14)∗βj← max∗j ,β(cid:12)f (y∗) − f (y)max(0, gj (y))(cid:1)(cid:2)0, gj (y)> 0.if max(B.2)This update is repeated for every violated constraint of Pd and every y ∈ Nd (y∗) until nofurther update is possible. The key of the proof is that, since Nd (y∗) has a finite numberof elements in discrete space, the update will terminate in finite time and result in finite α∗and β∗ values.Next, we prove that the (y∗, α∗∗, β∗∗) found satisfies (16). The proof of the first in-equality in (16) is trivial because Ld (y∗, α, β) = f (y∗) = Ld (y∗, α∗∗, β∗∗).For the second inequality in (16), since y∗ is a CLMd , it is clear for all y ∈ Nd (y∗)where h(y) = 0 and g(y) (cid:1) 0 that:∗∗∗∗∗∗, β, αLd (y) = f (y(B.3)For all y ∈ Nd (y∗) such that h(y) (cid:13)= 0 (respectively g(y) (cid:1) 0), there must exist at least oneconstraint that is not satisfied. From (B.1) and (B.2), we know for this constraint that:) (cid:1) f (y) = Ld (y, α, β).∗∗∗∗∗∗∗αi > αi⇒ Ld (y∗∗∗βj > βj∗) = f (y(cid:2) f (y∗) − f (y)|hi(y)|∗∗∗∗, α, β(cid:2) f (y∗) − f (y)max(0, gj (y))∗∗∗) = f (y, α(cid:1)(cid:2)0, gj (y)if max(cid:13)k=1,k(cid:13)=i α∗> 0., β∗∗mk⇒ Ld (yFurther, sinceclear that:∗) < f (y) + α(cid:7)(cid:7)(cid:7)(cid:7)hi(y)∗∗i(cid:7)(cid:7)(cid:7)hi(y)(cid:7) > 0,if(B.4)∗) < f (y) + β(cid:2)(cid:1)∗∗0, gj (y)j max|hk(y)| (cid:2) 0 (respectively(cid:13)rk=1,k(cid:13)=j β∗(B.5)k max(0, gj (y)) (cid:2) 0), it isLd (y∗∗∗, α, β∗∗) = f (y∗) (cid:1) f (y) +(cid:7)(cid:7)(cid:7) +(cid:7)hi(y)∗∗αim(cid:11)i=1r(cid:11)j =1(cid:2)(cid:1)∗∗0, gj (y)βi max= Ld (y, α∗∗∗∗)., βHence, (y∗, α∗∗, β∗∗) satisfies (16).“(⇐)” part: Assuming (16) is satisfied, we need to prove that y∗ is a CLMd . The proofis straightforward and is similar to that of Theorem 1.Appendix C. Proof of Theorem 3The proof consists of two parts.“(⇒)” part: Given (x∗, y∗), we need to prove that there exist finite α∗ (cid:2) 0 and β∗ (cid:2) 0so that (x∗, y∗, α∗∗, β∗∗) satisfies (20). The first inequality in (20) is true for all α and β,since (x∗, y∗) is a CLMm and |h(x∗, y∗)| = max(0, g(x∗, y∗)) = 0.To prove the second inequality in (20), we know that fixing y at y∗ converts Pm into Pc.Further, from Theorem 1, there exist finite α∗c and β∗c such that:B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231229∗∗Lm(x, y, α∀x ∈ Nc(x∗∗∗∗∗∗∗∗, β)|y∗ , α) (cid:1) Lm(x, y∗∗∗> αc, α, β(cid:2) 0, and β∗∗),∗∗∗> βc(cid:2) 0.∗∗∗∗∗Lm(xd and β∗, α, y∀y ∈ Nd (y(C.1)Similarly, fixing x at x∗ converts Pm into Pd . Hence, from Theorem 2, we know that thereexist finite α∗d such that, for the same α∗∗ and β∗∗ in (C.1):∗, β)|x∗ , α, β, y, α(cid:2) 0, and βSince all (x, y) ∈ Nm(x∗, y∗) perturb either x∗ or y∗ but not both, by setting:∗d1∗d1∗ = max(α∗ = max(β(cid:1)max(α(cid:1)max(β) (cid:1) Lm(x∗∗∗> αd∗∗cm , αdm∗∗cr , βdr(cid:2)T,)(cid:2)T,)∗d ) =∗d ) =(C.3)(C.4)∗c , α∗c , β), . . . , max(α), . . . , max(β∗> βd∗c1∗c1(cid:2) 0.(C.2), α, β),∗∗∗∗∗∗αβ∗∗we conclude, based on (C.1) and (C.2), that the second inequality in (20) is satisfied for all(x, y) ∈ Nm(x∗, y∗) and for any α∗∗ > α∗ (cid:2) 0 and β∗∗ > β∗ (cid:2) 0.“(⇐)” part: Assuming (20) is satisfied, we need to prove that (x∗, y∗) is a CLMm. Theproof is straightforward and is similar to that of Theorem 1.Appendix D. Proof of Theorem 5We prove the theorem by showing the equivalence of (25) and the combined (26) and(27).“(⇒)” part: Given z∗ that satisfies (25), we show that it also satisfies (26) and (27).p (z∗) is also a point in Np(z∗); hence, the secondSince for all t = 0, . . . , N , any z ∈ N (t)inequality in (26) is implied by the second inequality in (25). The first inequality in (26)and the inequality in (27) are obvious, as all the constraints are satisfied at z∗.“(⇐)” part: We prove this part by contradiction. Assuming that z∗ satisfies (26) and(27) but not (25), the first inequality in (25) cannot be violated because the first inequalityin (26) and the inequality in (27) imply that all the local and global constraints are satisfied.Therefore, it must be the second inequality in (25) that is not satisfied at z∗. That is, thereexist z ∈ Np(z∗) and a unique t (cid:8) where z ∈ N (t (cid:8))(z∗) (according to the definition of Np(z)in (23)) such that:b∗∗∗∗∗∗∗∗∗) (cid:2) Lm(z, α, γ, η, β, αLm(z(D.1)This implies that the second inequality in (26) is not satisfied at t = t (cid:8), which contradictsour assumption that z∗ satisfies (26) and (27). Our argument proves that any z∗ that satisfies(26) and (27) must also satisfy (25)., β, η, γ).∗∗∗∗∗∗∗∗References[1] E. Aarts, J. Korst, Simulated Annealing and Boltzmann Machines, J. Wiley and Sons, New York, 1989.[2] M. Avriel, Nonlinear Programming: Analysis and Methods, Prentice-Hall, Englewood Cliffs, NJ, 1976.[3] R. Bellman, S. Dreyfus, Applied Dynamic Programming, Princeton Univ. Press, Princeton, NJ, 1962.230B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231[4] D.P. Bertsekas, Nonlinear Programming, Athena Scientific, Belmont, MA, 1999.[5] A.L. Blum, M.L. Furst, Fast planning through planning graph analysis, Artificial Intelligence 90 (1997)281–300.[6] B. Bonet, H. Geffner, Planning as heuristic search, Artificial Intelligence 129 (1) (2001).[7] Y.X. Chen, C.-W. Hsu, B.W. Wah, SGPlan: Subgoal partitioning and resolution in planning, in: Proc. FourthInternat. Planning Competition, Internat. Conf. on Automated Planning and Scheduling, 2004.[8] Y.X. Chen, B.W. Wah, Automated planning and scheduling using calculus of variations in discrete space, in:Proc. Internat. Conf. on Automated Planning and Scheduling, 2003, pp. 2–11.[9] S. Chien, et al., ASPEN—Automating space mission operations using automated planning and scheduling,in: Proc. SpaceOps, Space Operations Organization, 2000.[10] P. Doherty, J. Kvarnstrom, TALplanner: An empirical investigation of a temporal logic-based forward chain-ing planner, in: Proc. Sixth Internat. Workshop on Temporal Logic-based Forward Chaining Planner, AIPS,1999, pp. 47–54.[11] M.A. Duran, I.E. Grossmann, An outer approximation algorithm for a class of mixed-integer nonlinearprograms, Math. Programming 36 (1986) 306–307.[12] S. Edelkamp, Mixed propositional and numerical planning in the model checking integrated planning sys-tem, in: Proc. Workshop on Planning in Temporal Domains, AIPS, 2002, pp. 47–55.[13] M.P. Fourman, Propositional planning, in: Proc. Workshop on Model Theoretic Approaches to Planning,AIPS, 2000.[14] M. Fox, D. Long, PDDL2.1: An extension to PDDL for expressing temporal planning domains, J. ArtificialIntelligence Res. 20 (2003) 61–124.[15] A.M. Geoffrion, Generalized Benders decomposition, J. Optim. Theory Appl. 10 (4) (1972) 237–241.[16] A. Gerevini, I. Serina, LPG: A planner based on local search for planning graphs with action costs, in: Proc.of the Sixth Internat. Conf. on AI Planning and Scheduling, Morgan Kaufman, Santa Mateo, CA, 2002,pp. 12–22.[17] N.I.M. Gould, D. Orban, Ph.L. Toint, An interior-point L1-penalty method for nonlinear optimization, Tech-nical Report, RAL-TR-2003-022, Rutherford Appleton Laboratory Chilton, Oxfordshire, UK, 2003.[18] J. Hoffmann, B. Nebel, The FF planning system: Fast plan generation through heuristic search, J. ArtificialIntelligence Res. 14 (2001) 253–302.[19] K. Holmberg, On the convergence of the cross decomposition, Math. Programming 47 (1990) 269–316.[20] K. Holmberg, Generalized cross decomposition applied to nonlinear integer programming problems: dualitygaps and convexification in parts, Optimization 23 (1992) 341–364.[21] A.K. Jónsson, P.H. Morris, N. Muscettola, K. Rajan, Planning in interplanetary space: Theory and practice,in: Proc. 2nd Internat. NASA Workshop on Planning and Scheduling for Space, NASA, 2000.[22] H. Kautz, B. Selman, Pushing the envelope: planning, propositional logic, and stochastic search, in: Proc.13th National Conference on Artificial Intelligence, AAAI, 1996, pp. 1194–1201.[23] H. Kautz, B. Selman, Unifying SAT-based and graph-based planning, in: Proc. Internat. Joint Conf. onArtificial Intelligence, IJCAI, 1999.[24] H. Kautz, J.P. Walser, Integer optimization models of AI planning problems, Knowledge Engrg. Rev. 15 (1)(2000) 101–117.[25] S. Kirkpatrick, C.D. Gelatt Jr., M.P. Vecchi, Optimization by simulated annealing, Science 220 (4598) (1983)671–680.[26] H.W. Kuhn, A.W. Tucker, Nonlinear programming, in: Proc. Second Berkeley Symp. Math. Stat. Prob.,University of California Press, 1951, pp. 481–492.[27] F. Lin, A planner called R, AI Magazine (2001) 73–76.[28] D. Long, M. Fox, Efficient implementation of the plan graph in STAN, J. Artificial Intelligence Res. 10(1999) 87–115.[29] D.G. Luenberger, Linear and Nonlinear Programming, Addison-Wesley, Reading, MA, 1984.[30] D. Nau, H. Muoz-Avila, Y. Cao, A. Lotem, S. Mitchell, Total-order planning with partially ordered subtasks,in: Proc. Internat. Joint Conf. on Artificial Intelligence, IJCAI, 2001, pp. 425–430.[31] R.S. Nigenda, X. Nguyen, S. Kambhampati, AltAlt: Combining the advantages of Graphplan and heuristicstate search, Technical Report, Arizona State University, 2000.[32] J. Penberethy, D. Weld, UCPOP: A sound, complete, partial order planner for ADL, in: Proc. 3rd Internat.Conf. on Principles of Knowledge Representation and Reasoning, KR Inc., 1992, pp. 103–114.B.W. Wah, Y. Chen / Artificial Intelligence 170 (2006) 187–231231[33] J. Penberethy, D. Weld, Temporal planning with continuous change, in: Proc. 12th National Conf. on AI,AAAI, 1994, pp. 1010–1015.[34] G. Rabideau, S. Chien, C. Eggemeyer, T. Mann, J. Willis, S. Siewert, P. Stone, Interactive, repair-basedplanning and scheduling for shuttle payload operations, in: Proc. Aerospace Conf., IEEE, 1997, pp. 325–341.[35] G. Rabideau, R. Knight, S. Chien, A. Fukunaga, A. Govindjee, Iterative repair planning for spacecraft op-erations in the ASPEN system, in: Proc. Internat. Symp. on Artificial Intelligence Robotics and Automationin Space, European Space Agency, 1999.[36] R.L. Rardin, Optimization in Operations Research, Prentice-Hall, Englewood Cliffs, NJ, 1998.[37] I. Refanidis, I. Vlahavas, The GRT planner, AI Magazine (2001) 63–66.[38] I. Refanidis, I. Vlahavas, The MO-GRT system: Heuristic planning with multiple criteria, in: Proc. Workshopon Planning and Scheduling with Multiple Criteria, AIPS, 2002.[39] T.J. Van Roy, Cross decomposition for mixed integer programming, Math. Programming 25 (1983) 46–63.[40] H.S. Ryoo, N.V. Sahinidis, A branch-and-reduce approach to global optimization, J. Global Optim. 8 (2)(1996) 107–139.[41] N.V. Sahinidis, BARON: A general purpose global optimization software package, J. Global Optim. 8 (2)(1996) 201–205.[42] D. Schuurmans, F. Southey, Local search characteristics of incomplete sat procedures, Artificial Intelli-gence 132 (2) (2001) 121–150.[43] B. Selman, H.A. Kautz, An empirical study of greedy local search for satisfiability testing, in: Proc. of 11thNational Conf. on Artificial Intelligence, AAAI, 1993, pp. 46–51.[44] Y. Shang, B.W. Wah, A discrete Lagrangian based global search method for solving satisfiability problems,J. Global Optim. 12 (1) (1998) 61–99.[45] M.B.D. Subbarao, S. Kambhampati, Sapa: A domain-independent heuristic metric temporal planner, Tech-nical Report, Arizona State University, 2002.[46] A. Tate, B. Drabble, R. Kirby, O-Plan2: an open architecture for command, planning and control, in: Intel-ligent Scheduling, Morgan Kaufmann, 1994, pp. 213–239.[47] B.W. Wah, Y.X. Chen, Partitioning of temporal planning problems in mixed space using the theory of ex-tended saddle points, in: Proc. IEEE Internat. Conf. on Tools with Artificial Intelligence, 2003, pp. 266–273.[48] B.W. Wah, Y.X. Chen, Subgoal partitioning and global search for solving temporal planning problems inmixed space, Internat. J. Artificial Intelligence Tools 13 (4) (2004) 767–790.[49] B.W. Wah, Y.X. Chen, Solving large-scale nonlinear programming problems by constraint partitioning, in:Proc. Principles and Practice of Constraint Programming, Springer-Verlag, Berlin, 2005.[50] B.W. Wah, T. Wang, Simulated annealing with asymptotic convergence for nonlinear constrained globaloptimization, in: Proc. Principles and Practice of Constraint Programming, Springer-Verlag, Berlin, 1999,pp. 461–475.[51] B.W. Wah, Z. Wu, The theory of discrete Lagrange multipliers for nonlinear discrete optimization, in: Proc.Principles and Practice of Constraint Programming, Springer-Verlag, Berlin, 1999, pp. 28–42.[52] D. Wilkins, Can AI planners solve practical problems?, Computational Intelligence (1990) 232–246.[53] J. Willis, G. Rabideau, C. Wilklow, The citizen explorer scheduling system, in: Proc. Aerospace Conf.,IEEE, 1999.[54] S. Wolfman, D. Weld, Combining linear programming and satisfiability solving for resource planning,Knowledge Engrg. Rev. 15 (1) (2000).[55] Z. Wu, The theory and applications of nonlinear constrained optimization using Lagrange multipliers, Ph.D.Thesis, Department of Computer Science, Univ. of Illinois, Urbana, IL, 2001.