Artificial Intelligence 173 (2009) 240–257Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA comparative runtime analysis of heuristic algorithms forsatisfiability problemsYuren Zhou a,c,∗, Jun He b, Qing Nie ca School of Computer Science and Engineering, South China University of Technology, Guangzhou 510640, Chinab Department of Computer Science, University of Wales, Aberystwyth, Ceredigion, SY23 3DB, UKc Department of Mathematics, University of California, Irvine, CA 92697-3875, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 18 March 2008Received in revised form 23 July 2008Accepted 1 November 2008Available online 7 November 2008Keywords:Boolean satisfiabilityHeuristic algorithmsRandom walk(1 + 1) EAHybrid algorithmExpected first hitting timeRuntime analysis1. IntroductionThe satisfiability problem is a basic core NP-complete problem. In recent years, a lot ofheuristic algorithms have been developed to solve this problem, and many experimentshave evaluated and compared the performance of different heuristic algorithms. However,rigorous theoretical analysis and comparison are rare. This paper analyzes and comparesthe expected runtime of three basic heuristic algorithms: RandomWalk, (1 + 1) EA,and hybrid algorithm. The runtime analysis of these heuristic algorithms on two 2-SATinstances shows that the expected runtime of these heuristic algorithms can be exponentialtime or polynomial time. Furthermore, these heuristic algorithms have their own advan-tages and disadvantages in solving different SAT instances. It also demonstrates that theexpected runtime upper bound of RandomWalk on arbitrary k-SAT (k (cid:2) 3) is O ((k − 1)n),and presents a k-SAT instance that has (cid:2)((k − 1)n) expected runtime bound.© 2008 Elsevier B.V. All rights reserved.The satisfiability problem (SAT) of a propositional formula plays a central role in computer science and artificial intelli-gence. It is the first proposed NP-complete problem [5,21] and one of the basic core NP-complete problems [10]. In additionto its theoretical importance, the SAT problem is also directly applied in VLSI formal verification, software automation, andso on.Researchers have been trying to look for an effective algorithm for the SAT problem. Since the SAT problem is an NP-complete problem in nature, a polynomial algorithm is not currently available to solve it, although we cannot prove thatsuch an algorithm does not exist. In fact, a basic conjecture of modern computer science and mathematics is that nopolynomial algorithm exists for NP-complete problems. At present, the main methods for solving the SAT problems arecomplete algorithms [3,6,34] and incomplete algorithms [7,12,13,15,20,25,27,29,31,32]. There are several very successfulcomplete algorithms (e.g., SATO [34]). A complete algorithm often explores the whole search space and can always de-termine whether a given propositional formula is satisfiable or not; however, its time complexity is usually exponential.An incomplete algorithm does not carry out a complete search on the search space; instead, it often explores some partof the search space using heuristic information within a limited time; however it does not give the correct answer withcertainty.Since the 1990s, the use of incomplete algorithm for solving the SAT problem has grown quickly. The basic incom-plete heuristic methods are RandomWalk algorithm [25], GSAT algorithm [13,31], WalkSat algorithm [32], UnitWalk [15],* Corresponding author at: School of Computer Science and Engineering, South China University of Technology, Guangzhou 510640, China.E-mail addresses: yrzhou@scut.edu.cn (Y. Zhou), jun.he@ieee.org (J. He), qnie@math.uci.edu (Q. Nie).0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2008.11.002Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257241population-search-based evolutionary algorithms [7,12,20] and so on. In recent years, some powerful concepts and tech-niques of statistical physics have been applied to the SAT problem. One of these incomplete algorithms, known as “surveypropagation” [4,22], which is based on statistical physics methods, shows good performance on some difficult randomlygenerated SAT instances. It is well known that one of the earliest applications of statistical physics in the optimization prob-lem is the simulated annealing algorithm [19]. WalkSat [32] used a probability selection mechanism similar to that of thesimulated annealing algorithm.For some heuristic algorithms for the SAT problem, theoretical results about computational complexities have been ob-tained to some extent. Papadimitiou [25] was the first to prove that the average time upper bound of RandomWalk for2-SAT is O (n2). Schöning [29] presented a restarting local-search algorithm to show that, for any satisfiable k-CNF formulawith n variables, the algorithm has to repeat O ((2(1 − 1k ))n) times, on average, to find a satisfying assignment. Specially ifk = 3, the average time is O (1.334n) (the upper bound of an exhaustive search is O (2n)). There have been several improve-ments on the upper bound by hybrid algorithms based on randomized algorithms by Paturi et al. [27] and Schöning [29],e.g. O (1.324n) [18] and O (1.322n) [28]. Alekhnovich et al. [2] proved that, when the clause density is less than 1.63, theaverage time complexity of RandomWalk for 3-SAT is linear.Since there are many incomplete heuristic algorithms for SAT problems, comparing and understanding the workingprincipals of these heuristic algorithms is useful. The first thing we have to accept is that no one algorithm beats all otheralgorithms on all problems. There have been many numerical experiments that compared various heuristic algorithms onSAT problems, but theoretical study has been rare. This paper analyzes and compares the expected running time of threebasic heuristic algorithms: RandomWalk, (1 + 1) EA, and hybrid algorithm. We use absorbing Markov chains to model searchprocesses of these heuristic algorithms, and use explicit expressions of the first hitting time of a Markov chain to analyzeand estimate their expected runtime. Through runtime analysis of three SAT instances, we show that the expected runtimeof these heuristic algorithms can be exponential or polynomial. We also find that these heuristic algorithms have their owncomparative advantage under different circumstances.The rest of this paper is organized as follows. Section 2 introduces the concepts of the SAT problem, some heuristicalgorithms for the SAT problem, and the first hitting time of an absorbing Markov chain. Section 3 discusses the worst-case bound and the worst-case example on RandomWalk. Section 4 analyzes and compares the expected runtime boundsof three heuristic algorithms on two 2-SAT instances. Section 5 presents our conclusions and suggestions for further re-search.2. Heuristic algorithms for satisfiability and the first hitting time of the Markov chain2.1. The SAT problemWe begin by stating some definitions and notations that will be used throughout the paper.In Boolean logic, a literal is a variable or its negation, and a clause is a disjunction of literals. The formula f = c1 ∧ c2 ∧· · · ∧ cm is in k conjunctive normal form (k-CNF) if it is a conjunction of clauses with each clause as a disjunction of at mostk literals. We view a CNF Boolean formula as both a Boolean function and a set of clauses. Satisfiability is the problem ofdetermining whether the variables of a given Boolean formula can be assigned truth values in such a way as to make theformula evaluate to true.SAT is originally stated as a decision problem. In this paper we consider the more general MaxSAT, so, our goal is to lookfor an assignment that satisfies the maximum number of clauses.Evolutionary algorithms (EAs) are the heuristic algorithms that have been applied to SAT and to many other NP-completeproblems. EAs usually use a fitness value to guide the search process. In the MaxSAT formulation, the fitness value is definedas the number of satisfied clauses, i.e.fit(x) = c1(x) + c2(x) + · · · + cm(x)(1)where ci(x) (1 (cid:2) i (cid:2) m) represents the true value of the ith clause. This fitness function is used in most EAs for SATproblems.Throughout this paper, for x = (x1 · · · xn), y = ( y1 · · · yn) ∈ {0, 1}n, we denote by H(x, y) the Hamming distance between|xi − yi|. We also denote |x| = x1 + · · · + xn, and let S i = {x | x ∈ S = {0, 1}n, |x| = i}(cid:2)ni=1two points x and y, i.e. H(x, y) =(i = 0, 1, . . . , n) be a partition of search space S = {0, 1}n.2.2. Heuristic algorithms for the SAT problemRandomWalk, first introduced by Papadimitiou [25], is one of the most basic incomplete algorithms, and many otherheuristics have been developed based on the improvement of this algorithm, e.g. the Walk-SAT [32], combines RandomWalkwith a greed bias towards assignments that satisfy more clauses. RandomWalk algorithm first randomly selects a clause thatis not satisfied with the CNF, then randomly selects a flip in the clause (see Algorithm 1).242Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257Algorithm 1 (The RandomWalk algorithm).begininitialization: Select an initial bit string x at random;while (termination-condition does not hold) doSelect c := an unsatisfied clause chosen at random;Select xi := a variable in c chosen at random;Flip the value of xi ;odendEvolutionary algorithms are inspired from modeling the processes of natural selection and genetic evolution. Here weconsider a simple EA using mutation and selection approaches with population size of 1 denoted as (1 + 1) EA [9]. (1 + 1)EA is a simple but effective random hill-climbing EA. Its general description is:Algorithm 2 ((1 + 1) EA).begininitialization: Choose randomly an initial bit string x;while (termination-condition does not hold) doMutation: y := mutate(x);Selection: If fitness( y) >fitness(x), x := y;odend(1 + 1) EA generally uses two kinds of mutation, called local mutation and global mutation:(1) Local mutation randomly chooses a bit xi (1 (cid:2) i (cid:2) n) from the individual x = (x1 · · · xn) ∈ {0, 1}n and flips it.(2) Global mutation flips each bit of individual x = (x1 · · · xn) ∈ {0, 1}n independently with the probability of 1n . The expectednumber of bit flips for the global mutation is 1.The hill-climbing algorithm is usually trapped in a region which is a local optimum and needs to be restarted with arandom new assignment. Another widely-used mechanism for escaping such a local optimum of the maximization problemis to permit the search to make occasional downhill moves. The following hybrid strategy, which combines (1 + 1) EA andRandomWalk, is closely related to WalkSat [32] in that it allows for the possibility of downhill moves.Algorithm 3 (The hybrid algorithm of local (1 + 1) EA and RandomWalk).begininitialization: Set parameters, choose randomly an initial bit string;while (termination-condition does not hold) doWith probability p, follow the RandomWalk scheme;With probability 1 − p, follow the Local (1 + 1) EA scheme;odend2.3. The absorbing Markov chainMost heuristic algorithms are memory-less in the sense that the processes of selecting the next point in the searchspace depend only on the current point. This allows us to model these search processes as absorbing Markov chains whoseabsorbing set is the optimal solution (s). Such models are widely used in two heuristic algorithms: Simulated Annealing [1]and Genetic Algorithms [14,24]. Basic knowledge of absorbing Markov chains can be found in any literature regardingrandom processes, e.g. [17].Let ( Xt; t = 0, 1, . . .) denote a discrete homogeneous absorbing Markov chain in a finite state space S. T is the transientstate set, H = S − T is the absorbing set. Assume there are r absorbing states and t transient states, i.e. |T | = t and |H| = rwhere | · | denotes the cardinality of a set, then the transition matrix can be written in the canonical form as(cid:3)(cid:4)P =I OR Qwhere I is an r-by-r identity matrix, O is an r-by-t zero matrix, R is a nonzero t-by-r matrix, and Q is a t-by-t matrix. Forthe power of P, a standard matrix argument shows that the region I remains I. This corresponds to the fact that once theMarkov chain reaches an absorbing state, it will never leave that absorbing state.Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257243Definition 1. Let ( Xt; t = 0, 1, . . .) be an absorbing Markov chain. The first hitting time from status i (i ∈ S) to the absorbingstatus set H is:τi = min{t: t (cid:3) 0, Xt ∈ H | X0 = i}if the right-hand side involves the empty set, let τi = ∞.We are interested in the question: Given that the chain starts in state i, what is the expected number of steps beforethe chain is absorbed? Theorem 1 provides an answer.Theorem 1. Given that the absorbing Markov chain Xt starts in transient state i, let mi be the expected number of steps before thechain is absorbed, i.e. mi = E[τi]. Denote m = [mi]i∈T . Thenm = (I − T)−11where 1 represents the column vector all of whose entries are 1.Proof. See Ref. [17]. (cid:2)Several corollaries can be derived directly from Theorem 1.(2)Corollary 1. Let { Xt | t = 0, 1, . . .} be an absorbing Markov chain with finite state space S = {0, 1, . . . , n, n + 1} and absorbing stateset {0, n + 1}, and its transition probabilities are defined as follows:(1) For i = 0 or n + 1,(cid:5)pi j =1,j = i,0, otherwise.(2) For 1 (cid:2) i (cid:2) n,⎧⎪⎪⎪⎨⎪⎪⎪⎩pi j =ai,bi,1 − ai − bi,0,j = i − 1,j = i + 1,j = i,otherwise.Then for this absorbing Markov chain, its mean first hitting time to the absorbing state is given by⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩mn+1 = 0,(cid:2)+1a1mn =n−1h=11+(cid:2)h(j=11ah+1(cid:2)(cid:10)nnj=1i= j) − 1an+1)biai,(cid:10)hi= jbiai,mn−1 = mn(1 + bnan(cid:10)mk−1 = mk + mnni=km0 = 0.(cid:2)n−(k+1)j=01a j+k+1(cid:10)ji=0bk+iak+i− 1ak, k = n − 1, . . . , 2,−biaiCorollary 2. Let { Xt | t = 0, 1, . . .} be an absorbing Markov chain with finite state space S = {0, 1, . . . , n} and absorbing state set 0,and its transition probabilities are defined as follows:(1) For i = 0 or n + 1,(cid:5)pi j =1,j = i,0, otherwise.(2) For 1 (cid:2) i < n,⎧⎪⎪⎪⎨⎪⎪⎪⎩pi j =ai,bi,1 − ai − bi,0,j = i − 1,j = i + 1,j = i,otherwise.244Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257(3) For i = n,pi j =⎧⎨⎩ai,1 − ai,0,j = i − 1,j = i,otherwise.Then for this absorbing Markov chain, its mean first hitting time to the absorbing state is given by⎧⎪⎪⎨⎪⎪⎩m0 = 0,mi = mi−1 + 1aimn = mn−1 + 1an+.(cid:2)n−(i+1)j=01a j+i+1(cid:10)jh=0bi+hai+h,i = 1, . . . , n − 1,The difference between the two corollaries lies in the fact that the Markov chain of Corollary 1 has two absorbing stateswhile the Markov chain of Corollary 2 has only one. He et al. [14] used Corollary 2 to estimate the expected running timeof evolutionary algorithms.For our time complexity discussion and analysis, Theorem 1 and its corollaries play a key role. These methods are closeto the Markov chain analysis of the stochastic local search algorithms by Schöning [29,30]. The main difference is: We uselinear system (2) to estimate the absorbing time of the Markov chain while Schöning calculated the “success probability”.Now we introduce two vector norms, the average vector norm and the maximum vector norm, both of which are oftenused in the vector analysis. For vector m = [mi]i∈S , let μ0(i) = P ( X0 = i) be the initial distribution, the average vector norm(cid:6)m(cid:6)1 and the maximum vector norm (cid:6)m(cid:6)∞ are defined as(cid:11)μ(i)mii∈S(cid:6)m(cid:6)1 =and(cid:6)m(cid:6)∞ = maxi∈S{mi}.Specially, if the initial distribution is the uniform distribution on S, i.e. μ0(i) = 1|S| (i ∈ S), then we have(cid:6)m(cid:6)1 = 1|S|(cid:11)i∈Smi.Norms (cid:6)m(cid:6)1 and (cid:6)m(cid:6)∞ present average case and worst case performance measures respectively in the time complexityanalysis.3. Bounds on RandomWalkIt is well known that the most simple algorithm, complete enumeration, needs (cid:2)(2n) steps to find the satisfying assign-ment of the SAT problem with n variables. In the following, we shall show that the general upper bound of the averageiteration number of RandomWalk for k-SAT is O ((k − 1)n). We also construct a SAT instance for which this bound is tight,i.e. for which the expected runtime of RandomWalk is (cid:2)((k − 1)n).Proposition 1. The expected runtime of RandomWalk for any k-SAT (k (cid:3) 3) instance is at most O ((k − 1)n).∗S =∗ {H(x, y)} denote the distance between a point x ∈ S and the set SProof. Let S = {0, 1}n be the search space, and Smin y∈SDefine D i to be D i = {x ∈ S | d(x) = i}, i = 0, 1, . . . , n. Then the search space S is partitioned into n + 1 subspaces:(cid:12)ni=0 D i .the satisfying assignment set for given k-CNF formula ω. Let d(x) =Suppose we are given a string x ∈ D i (1 (cid:2) i (cid:2) n), for any clause σ that is not satisfied, there exists at least one of the kbits to be flipped to decrease d(x) by 1. Since RandomWalk picks a variable in some unsatisfied clause and flips its truthassignment, the probability that RandomWalk transfers x to some string y ∈ D i−1 is at least 1k , and the probability that ittransfers x to some string y ∈ D i+1 is at most 1 − 1k .Construct an auxiliary homogeneous Markov chain which is defined on the state space {D0, D1, . . . , Dn} with the transi-∗.⎛tion matrix11k⎜⎜⎜⎜⎜⎜⎜⎜⎝0· · ·00001k· · ·000k−1k0· · ·0000k−1k· · ·00. . .. . .. . .· · ·. . .. . .00000· · ·0· · ·1k001⎞⎟⎟⎟⎟⎟⎟⎟⎟⎠000· · ·k−1k0Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257245According to Corollary 2, its mean first hitting time to the absorbing state D0 is⎧⎪⎨⎪⎩m0 = 0,mi = mi−1 + k + kmn = mn−1 + 1.(cid:2)n−(i+1)j=0(k − 1) j+1,i = 1, . . . , n − 1,By induction, we havemn = mn−1 + 1= (n − 1)k + k(k − 1)k − 2= (n − 1)k + k(k − 1)k − 2(cid:19)n−1(cid:11)(cid:20)(k − 1)n−l−1 − (n − 1)(cid:3)l=1(k − 1)n−1 − 1k − 2(cid:4)− (n − 1).Hence(cid:6)m(cid:6)∞ = mn = OThis completes the proof. (cid:2)(cid:21)(k − 1)n(cid:22).We have shown that the expected running time of RandomWalk for any k-SAT instance is O ((k − 1)n), but this does notmean that RandomWalk needs, on average, (cid:2)((k − 1)n) steps to find a satisfying assignment in every k-SAT instance. In thefollowing we present a k-SAT instance ϕ(k)(x) (k > 2), where the average case expected time complexity is (cid:2)((k − 1)n).Definition 2. The SAT instance ϕ(k)(x) (k < n) has the following clauses:xi,and xi1∨ (¯xi2∨ · · · ∨ ¯xik )where 1 (cid:2) i (cid:2) n, and (i1, . . . , ik) ranges over all k-element subsets of {1, . . . , n}.It is evident that the SAT instance ϕ(k)(x) has the unique satisfying assignment xPapadimitiou [26] first proposed the special cases k = 3 of ϕ(k)(x) and claimed that it was a difficult instance for Ran-domWalk. Here we discuss the general situation and derive its worst case and average case bounds of the expected runtimeon RandomWalk.∗ = (1 · · · 1).Proposition 2. Given an integer k (cid:3) 3, the expected runtime of RandomWalk for SAT instance ϕ(k)(x) is(1) (cid:6)m(cid:6)∞ = O ((k − 1)n),(2) (cid:6)m(cid:6)1 = (cid:2)((k − 1)n).Proof.(1) It follows immediately from Proposition 1.(2) From the consequence of (1), it is sufficient to show (cid:6)m(cid:6)1 = (cid:7)((k − 1)n).Let T i = {x | x ∈ S = {0, 1}n, |x| = n − i} (i = 0, 1, . . . , n) be the partition of search space S. We denote by Xt (t = 0, 1, . . .)the random string describing at which point RandomWalk is during iteration t. Then Xt is a homogeneous Markov chainwith the absorbing state set T 0. The transition probabilities among the subspaces can be described as follows.When i = 0,P ( Xt+1 ∈ T 0 | Xt ∈ T 0) = 1.When 1 (cid:2) i (cid:2) n − (k − 1),P ( Xt+1 ∈ T i+1 | Xt ∈ T i) = k − 11 +P ( Xt+1 ∈ T i−1 | Xt ∈ T i) = 1 − k − 1k(cid:22)(cid:22) ,(cid:21)n−ik−1(cid:21)n−ik−1(cid:21)n−ik−1(cid:21)n−ik−11 +(cid:22)(cid:22) .kWhen n − (k − 1) < i (cid:2) n,P ( Xt+1 ∈ T i−1 | Xt ∈ T i) = 1.246Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257Denote⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩ai = 1 − k−1k(n−ik−1)1+(n−ik−1)(n−ik−1)1+(n−ik−1),bi = k−1kai = 1,bi = 0,, 1 (cid:2) i (cid:2) n − (k − 1),1 (cid:2) i (cid:2) n − (k − 1),n − (k − 1) < i (cid:2) n,n − (k − 1) < i (cid:2) n.Construct an auxiliary homogeneous Markov chain Zt (t = 0, 1, . . .) which is defined on the state set S = {0, 1, . . . , n}with the transition matrix⎞1a10· · ·0001 − a1 − b1a2· · ·0b11 − a2 − b2· · ·000000b2· · ·00. . .. . .00000· · ·0· · ·. . .· · ·. . . an−1 1 − an−1 − bn−1. . .0an000· · ·bn−11 − an⎟⎟⎟⎟⎟⎟⎟⎟⎠(3)For this absorbing Markov chain, according to Corollary 2, its mean first hitting time to the absorbing state 0 is givenby⎛⎜⎜⎜⎜⎜⎜⎜⎜⎝⎧⎪⎨m0 = 0,mi = mi−1 + 1aimi = mi−1 + 1,Note that for 1 (cid:2) i (cid:2) n − (k − 1),(cid:2)⎪⎩+n−(i+k−1)j=01a j+i+1(cid:10)jh=0bi+hai+h, 1 (cid:2) i (cid:2) n − (k − 1),n − (k − 1) < i (cid:2) n.(4)(5)(6)1ai(cid:3) 1kandbiai= (k − 1)1(cid:21)n−i1 + k/k−1Here we use the inequality 11+xFrom Eqs. (4), (5) and (6), we get(cid:22) (cid:3) (k − 1)e−k/(n−ik−1).(cid:3) e−x (x (cid:3) 0).m1 = 1a1+bh+1ah+1n−k(cid:11)j(cid:23)1a j+2h=0j=0n−k(cid:11)(cid:21)(k − 1) j+1(cid:2)j+1h=1−(cid:22)ek(n−hk−1) .(cid:3) 1k+ 1kj=0Becausej+1(cid:11)h=1k(cid:21)n−hk−1(cid:22) = k!(cid:2) k!= k!j+1(cid:11)h=1j+1(cid:11)h=1j+1(cid:11)h=11(cid:22)(k − 1)!(cid:21)n−hk−11(n − h)(n − (h + 1))(cid:3)1(n − (h + 1))− 1n − h(k > 2)(cid:4)(cid:2) k!,we havem1 (cid:3) 1k+ 1kn−k(cid:11)(k − 1) j+1−k!ej=0−k! k − 1k − 2(cid:22).e+ 1= 1kk(cid:21)= (cid:7)(k − 1)n(cid:22)(cid:21)(k − 1)n−k+1 − 1Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257247Finally, by the monotonicity of the sequence mi we obtain(cid:3)(cid:3)(cid:4)m0 +(cid:4)m1 + · · · +(cid:3)n1(cid:4)(cid:4)mn(cid:3)nn(cid:21)(cid:7)(k − 1)n(cid:22)(cid:21)(k − 1)n= (cid:7)(cid:22).n(cid:6)m(cid:6)1 = 12n0(cid:3) 2n − 12nThis completes the proof. (cid:2)From Proposition 2, it should be mentioned that even the exhaustive search (with upper bound of 2n) behaves muchbetter than RandomWalk on SAT instance ϕ(k)(x) (k > 3).Droste et al. [8] studied the expected running time of (1 + 1) EA on the special case ϕ(3)(x) and demonstrated that theaverage time complexity is a exponential time. Wei et al. [33] presented a class of formulas involving a so-called ternarychain which is similar to ϕ(3)(x). They also showed the expected runtime of RandomWalk on the ternary chain formula isexponential and proposed the “accelerating random walk” on this problem.For ϕ(k)(x), according to Eq. (1), the fitness function used in evolutionary algorithms isfitϕ(k) (x) =(cid:11)xi +(cid:11)(cid:11)(cid:11). . .(cid:21)1 − (1 − xi1 )xi2· · · xik(cid:22)1(cid:3)i(cid:3)n1(cid:3)i1(cid:3)n= s + k!(cid:4)(cid:3)nk1(cid:3)i2(cid:3)ni2(cid:8)=i11(cid:3)ik(cid:3)nik(cid:8)=i1,...,ik(cid:8)=ik−1(cid:4)(cid:3)− (n − s)(k − 1)!sk − 1(cid:21)|x| = s(cid:22).The fitness function fitϕ(k) (x) induces the MaxSAT problem ϕ(k)(x) into a polynomial in s (the number of 1 in x) of degree k.We might expect that the heuristic algorithms using fitness function have difficulty finding the all-one string because thenon-monotone polynomial fitness function might give misleading hints regarding the all-one string.4. Behavior of three heuristic algorithms on SAT instancesIn this section, in order to obtain a theoretical understanding of the behavior of different heuristic algorithms, we con-struct two SAT instances and analyze the average time complexity of RandomWalk, (1 + 1) EA and hybrid algorithm onthese SAT instances.Definition 3. For x = (x1 · · · xn) ∈ {0, 1}n, the SAT instance ψ1(x) is defined asψ1(x) = (x1 ∨ ¯x2) ∧ (x1 ∨ ¯x3) ∧ · · · (x1 ∨ ¯xn) ∧ (¯x1 ∨ x2) ∧ (¯x1 ∨ x3) ∧ · · · (¯x1 ∨ xn).The satisfying assignments of ψ1(x) are (0 · · · 0) and (1 · · · 1).We start from (1 + 1) EA for solving the MaxSAT instance ψ1(x). According to Eq. (1), for |x| = k, the fitness function ofψ1(x) is given as(cid:5)fitψ1 (x) =2(n − 1) − k,(n − 1) + (k − 1),x = (0 ∗ · · · ∗),x = (1 ∗ · · · ∗).When x = (0 ∗ · · · ∗) (or x = (1 ∗ · · · ∗)), the fitness function fitψ1 (x) decreases (or increases) monotonously with an increaseof the number of ones. The fitness function fitψ1 (x) with n = 20 is shown in Fig. 1.In the following, we consider local (1 + 1) EA and global (1 + 1) EA on ψ1(x) respectively. We shall show that they bothfind the satisfying assignment in time (cid:2)(n ln n).For simplicity, in Proposition 3 below we assume that n is even.We further divide the search space S into 2n subspaces:(cid:24)(cid:24)S0,k =S1,k =x | x = (0 ∗ · · · ∗) ∈ S, |x| = kx | x = (1 ∗ · · · ∗) ∈ S, |x| = k(cid:25)(cid:25)(k = 0, . . . , n − 1),(k = 1, . . . , n).248Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257Fig. 1. The fitness function of ψ1(x) with n = 20.Proposition 3. For SAT instance ψ1(x), for any x ∈ S u,k (u = 0, 1; k = 0, . . . , n), denote by mu,k the mean first hitting time of local(1 + 1) EA starting from state x, then⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩k ),m0,0 = 0,m0,k = n(1 + · · · + 1m1,n = 0,m1,k = n(1 + · · · + 1m0,k = 1m1,k = 1n−k ),k+1 (n + km0,k−1 + m1,k+1),n−k+1 (n + (n − k)m1,k+1 + m0,k−1), 1 (cid:2) k (cid:2) n2 ,n2(cid:2) k (cid:2) n − 1,+ 1 (cid:2) k (cid:2) n − 1,n21 (cid:2) k (cid:2) n2− 1,i.e. the expected runtime of the local (1 + 1) EA is (cid:6)m(cid:6)∞ = (cid:2)(n ln n).Proof. Let Xt ∈ {0, 1}n (t = 0, 1, . . .) be the random variable describing the state of local (1 + 1) EA solving SAT instanceψ1(x) at time t, then the transition probabilities can be described as follows.When k = 0,P ( Xt+1 ∈ S0,k | Xt ∈ S0,k) = 1.When 1 (cid:2) k (cid:2) n2− 1,P ( Xt+1 ∈ S0,k−1 | Xt ∈ S0,k) = knP ( Xt+1 ∈ S0,k | Xt ∈ S0,k) = 1 − kn,.When n2(cid:2) k (cid:2) n − 1,,P ( Xt+1 ∈ S0,k−1 | Xt ∈ S0,k) = knP ( Xt+1 ∈ S1,k+1 | Xt ∈ S0,k) = 1nP ( Xt+1 ∈ S0,k | Xt ∈ S0,k) = 1 − k + 1,n.Similarly, when k = n,P ( Xt+1 ∈ S1,k | Xt ∈ S1,k) = 1.When n2+ 1 (cid:2) k (cid:2) n − 1,Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257249P ( Xt+1 ∈ S1,k+1 | Xt ∈ S1,k) = 1 − knP ( Xt+1 ∈ S1,k | Xt ∈ S1,k) = kn.,When 1 (cid:2) k (cid:2) n2 ,,P ( Xt+1 ∈ S1,k+1 | Xt ∈ S1,k) = 1 − knP ( Xt+1 ∈ S0,k−1 | Xt ∈ S1,k) = 1nP ( Xt+1 ∈ S1,k | Xt ∈ S1,k) = k − 1,.nIntroduce an auxiliary homogeneous Markov chain Ztz1,2, . . . , z1,n}, the transition probabilities are defined byP (Zt+1 = zv,h | Zt = zu,k) = P ( Xt+1 ∈ S v,h | Xt ∈ S u,k)(t = 0, 1, . . .) with the state space {z0,0, z0,1, . . . , z0,n−1; z1,1,where u, v ∈ {0, 1}, and h, k ∈ {0, . . . , n}.Then Ztis an absorbing Markov chain with the absorbing state z0,0 and z1,n, and for any x ∈ S u,k (u ∈ {0, 1}, k ∈{0, . . . , n}), the mean first hitting time mx equals mzu,k .According to Theorem 1, the mean first hitting time of stochastic process Zt is given by⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩n m0,k−1 + kn m0,k+1 + k+1n m0,k = 1,n m0,k − 1n m1,k+1 = 1,m0,0 = 0,− k− km1,n = 0,− n−kn m1,k+1 + n−kn m1,k+1 + n−k+1The above linear equations can be solved asn m1,k = 1,n m1,k − 1− n−kn m0,k−1 = 1, 1 (cid:2) k (cid:2) n2 .− 1,1 (cid:2) k (cid:2) n2(cid:2) k (cid:2) n − 1,n2+ 1 (cid:2) k (cid:2) n − 1,n21 (cid:2) k (cid:2) n2− 1,k ),m0,0 = 0,m0,k = n(1 + · · · + 1m1,n = 0,m1,k = n(1 + · · · + 1m0,k = 1m1,k = 1n−k ),k+1 (n + km0,k−1 + m1,k+1),n−k+1 (n + (n − k)m1,k+1 + m0,k−1), 1 (cid:2) k (cid:2) n2 .n2(cid:2) k (cid:2) n − 1,+ 1 (cid:2) k (cid:2) n − 1,n2In the following, we prove1 + · · · + 1km0,k (cid:2) n(cid:3)(cid:4) (cid:3)n2(cid:4)(cid:2) k (cid:2) n − 1by induction.When k = nm0,n/2 =(cid:3)1n/2 + 1(cid:3)2 , from (7), we obtain(cid:3)n + n21 + · · · + 1n/2.(cid:2) n1 + · · · +(cid:4)(cid:4)(cid:3)+ n1 + · · · +1n/2 − 11n − (n/2 + 1)(cid:4)(cid:4)Thus we have that (8) holds for k = n2 .Assume it is true for some k (cid:3) nm0,k+1 = 1k + 2(cid:21)n + (k + 1)m0,k + m1,k+22 , i.e. m0,k (cid:2) n(1 + · · · + 1(cid:22)k ), from (7), we have(cid:4)(cid:3)+ n1 + · · · + 1k(cid:2) nk + 2(cid:3)(cid:2) n1 + · · · + 1k + 1(cid:4).Therefore (8) holds for all k.(7)(8)250Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257Similarly, we have m1,k (cid:2) n(1 + · · · + 1This proves the claim. (cid:2)n−k ) (1 (cid:2) k (cid:2) n2 ).Proposition 4. For SAT instance ψ1(x), the expected runtime of the global (1 + 1) EA is(1) (cid:6)m(cid:6)∞ = O (n ln n),(2) (cid:6)m(cid:6)1 = (cid:2)((n ln n).Proof.(1) We decompose the state space S = {0, 1}n into n subspaces by the Hamming distance between the string in S and thesatisfying assignment: S =Uk = T 0,k ∪ T 1,k(cid:12)n−1k=0 Uk where(k = 0, 1, . . . , n − 1),and(cid:24)(cid:24)T 0,k =T 1,k =x | x = (0 ∗ · · · ∗) ∈ S, Hx | x = (1 ∗ · · · ∗) ∈ S, H(cid:21)(cid:21)(cid:22)x, (0 · · · 0)(cid:22)x, (1 · · · 1)(cid:25),(cid:25).= k= kIn contrast to the notations S0,k and S1,k, both of which are used in the proof of Proposition 3, we see that T 0,k = S0,kand T 1,k (cid:8)= S1,k. For x = (1 ∗ · · · ∗) ∈ S, x ∈ S1,k means that the Hamming distance between x and (0 · · · 0) is k whilex ∈ T 1,k means that the Hamming distance between x and (1 · · · 1) is k.For x ∈ Uk(k = 1 · · · n − 1), note that fitψ1 (x) = 2n − k. Thus the probability that the global (1 + 1) EA leads x to somey ∈ Uk−1 ∪ · · · ∪ U 0 is greater than kn .n−1 ) = O (n ln n).Therefore we get (cid:6)m(cid:6)∞ (cid:2) (1 + · · · + 1(2) According to the result of (1), it is sufficient to prove that (cid:6)m(cid:6)1 = (cid:7)((n ln n). The proof below is similar to that of thelinear functions with nonzero weights by Droste et al. [9]. The main difference is that the linear function has only oneoptimum (0 · · · 0) while SAT instance ψ1(x) has two satisfying assignments (0 · · · 0) and (1 · · · 1).By Chernoff bounds [23], for any 0 < (cid:9) < 12 , the probability that the initial string x satisfies ( 1(for the simplicity of analysis, we assume that ( 12equivalent that with probability 1 − eIn order to reach the satisfying assignment, each of these strings needs to flip its all zeros (or all ones) at least once.Let X be a random variable defined to be the number of generations required to flip the all zeros (or ones) of the aboveinitialized string at least once, then its expectation is−(cid:7)(n) the randomly initialized string has at least ( 12− (cid:9))n is an integer) is exponentially close to one, i.e. 1 − e+ (cid:9))n−(cid:7)(n). It is− (cid:9))n ones.− (cid:9))n zeros and ( 12− (cid:9))n (cid:2) |x| (cid:2) ( 122E( X) =∞(cid:11)t=1t P ( X = t) =∞(cid:11)t=1P ( X (cid:3) t).Since the probability that one bit does not flip at all in t − 1 steps is (1 − 1least one of the ( 12Hence we have− (cid:9))n bits never flips in t − 1 steps is 1 − (1 − (1 − 1n )t−1)( 12−(cid:9))n.n )t−1, the probability for the event that at(cid:4)t−1(cid:4)( 12(cid:4)−(cid:9))nE( X) (cid:3)(cid:3)∞(cid:11)t=1(cid:3)(cid:3)1 −1 −1 − 1n(cid:3)(cid:3)(cid:3) (n − 1) ln n1 −1 −(cid:3) (n − 1) ln n(cid:21)1 − e−( 12−(cid:9))= (cid:7)(n ln n).(cid:4)(n−1) ln n(cid:4)( 12(cid:4)−(cid:9))n(cid:3)1 − 1n(cid:22)In the above, we use (1 − 1Therefore(cid:6)m(cid:6)1 (cid:3)(cid:21)1 − e−(cid:7)(n)−1(n > 1).n )n (cid:2) e(cid:22)(cid:7)(n ln n) = (cid:7)(n ln n).This completes the proof. (cid:2)Papadimitiou [25] proved that RandomWalk on any satisfiable 2-SAT will reach a satisfying assignment in time O (n2) bythe theory of random walks. In the following Proposition 5, we shall demonstrate that RandomWalk has the (cid:2)(n2) worstcase and average case expected runtime bound on 2-SAT instance ψ1(x).Now we introduce a lemma which provides the upper bound for the tail of the binomial distribution function. It will beused in Proposition 5 for estimating the average case expected runtime of RandomWalk.Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257251Lemma 1. Let X be a random variable following the binomial distribution with parameters n and p. Given a integer 0 (cid:2) k (cid:2) n, thecumulative distribution is expressed asF (k) = P ( X (cid:2) k) =k(cid:11)(cid:4)(cid:3)nii=0pi(1 − p)n−i.Then F (k) (cid:2) e− 2(np−k)2n.Proof. It follows immediately from Hoeffding’s inequality [16]. (cid:2)Proposition 5. For SAT instance ψ1(x), the average case expected runtime of RandomWalk is (cid:6)m(cid:6)1 = (cid:2)(n2).Proof. According to the above discussion, it is sufficient to show that (cid:6)m(cid:6)1 = (cid:7)(n2).We denote by Xt (t = 0, 1, . . .) the stochastic process of RandomWalk for SAT instance ψ1(x). Then Xt is a homogeneousMarkov chain with two absorbing states (0 · · · 0) and (1 · · · 1). The transition probabilities among the subspaces can bedescribed as follows.When i = 0,P ( Xt+1 ∈ S i | Xt ∈ S i) = 1.When 1 (cid:2) i (cid:2) n − 1,P ( Xt+1 ∈ S i+1 | Xt ∈ S i) = 1/2,P ( Xt+1 ∈ S i−1 | Xt ∈ S i) = 1/2.When i = n,P ( Xt+1 ∈ S i | Xt ∈ S i) = 1.Construct an auxiliary homogeneous Markov chain Zt (t = 0, 1, . . .) which is defined on the state space {0, 1, . . . , n} withthe transition probabilitiesP (Zt+1 = j | Zt = i) = P ( Xt+1 ∈ S j | Xt ∈ S i)where i, j = 0, . . . , n.According to Corollary 1, the mean first hitting time of absorbing chain Zt is given bymi = i(n − i)(i = 0, 1, . . . , n).Hence(cid:6)m(cid:6)1 = 12n= 12nm1 + · · · +(cid:3)(cid:3)n0n(cid:11)i=1(cid:4)m0 +i(n − i)(cid:4)(cid:3)n1(cid:4)(cid:3)ni(cid:4)(cid:4)mn(cid:3)nn(cid:3) 3n21612n3n/4(cid:11)(cid:4)(cid:3)nii=n/4(cid:22)− n38(by Lemma 1)(cid:21)(cid:3) 3n216(cid:21)= (cid:7)n21 − 2e(cid:22).Finally, we reach the conclusion. (cid:2)For SAT instance ψ1(x), from Propositions 3–5, we see that the (cid:2)(n ln n) expected runtime bound of (1 + 1) EA is betterthan the (cid:2)(n2) expected runtime bound of RandomWalk. In the following, we construct another SAT instance ψ2(x), forwhich we shall have an opposite situation under some conditions.Definition 4. The SAT instance ψ2(x) has the following clauses:xi, 1 (cid:2) i (cid:2) n,and xi ∨ ¯x j,(i, j) ∈ {1, . . . , n}2,i (cid:8)= j.252Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257Fig. 2. The fitness function of ψ2(x) with n = 21.The only satisfying assignment of SAT instance ψ2(x) is the all True assignment (1 · · · 1).According to Eq. (1), for |x| = k, the fitness function of ψ2(x) is given asfitψ2 (x) = n(n − 1) + k(k − n + 1),which is a polynomial in k of degree 2. The fitness function fitψ2 (x) with n = 21 is shown in Fig. 2. We note that this fitnessfunction has its local minimum at |x| = 10 while all one string is the only global optimum. If the local hill-climbing (1 + 1)EA starts from x (|x| < 10), it will never reach the global optimum.Proposition 6. Let Tk = {x | x ∈ S = {0, 1}n, |x| = n − k} (k = 0, 1, . . . , n) be the partition of search space {0, 1}n, and n an oddnumber. For SAT instance ψ2(x), for any i ∈ Tk (k = 1, . . . , n) the expected runtime of the local (1 + 1) EA is(cid:26)mi =k < n−1+∞2 ,O (n ln n) k (cid:3) n−12 .Proof. When k < n−1from any initial string i ∈ Tk will never reach the satisfying assignment (1 · · · 1).2 , since the fitness function fitψ2 (x) decrease monotonously as |x| increases, the local (1 + 1) EA starting2 , similar to that of OneMAX[11], the local (1 + 1) EA starting from any initial string i ∈ Tk will find theWhen k (cid:3) n−1satisfying assignment in O (n ln n) on average. (cid:2)RandomWalk on ψ2(x) has the same worst case expected runtime bound as that of ψ1(x):Proposition 7. For SAT instance ψ2(x), the expected runtime of RandomWalk is (cid:6)m(cid:6)∞ = (cid:2)(n2).Proof. It is sufficient to show that (cid:6)m(cid:6)∞ = (cid:7)(n2).The following proof is similar to that of Proposition 2.Let Xt (t = 0, 1, . . .) be the random variable describing at which point RandomWalk is during iteration t. Then Xtisan homogeneous Markov chain with the absorbing state set T 0. The transition probabilities among the subspace can bedescribed as follows.When k = 0,P ( Xt+1 ∈ T 0 | Xt ∈ T 0) = 1.When 1 (cid:2) k (cid:2) n − 1,P ( Xt+1 ∈ Tk−1 | Xt ∈ Tk) = 1 − 12× n − kP ( Xt+1 ∈ Tk+1 | Xt ∈ Tk) = 121 + (n − k)× n − k.1 + (n − k),Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257253When k = n,P ( Xt+1 ∈ Tk−1 | Xt ∈ Tk) = 1.Denote⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩× n−kak = 1 − 12× n−kbk = 11+(n−k) ,2ak = 1,bk = 0,1+(n−k) , 1 (cid:2) k (cid:2) n − 1,1 (cid:2) k (cid:2) n − 1,k = n,k = n.Construct an auxiliary homogeneous Markov chain Zt (t = 0, 1, . . .) which is defined on the state set {0, 1, . . . , n} with thetransition matrix (3) (see the proof of Proposition 2).For this absorbing Markov chain, according to Corollary 2, its mean first hitting time to the absorbing state 0 is given by(9)⎧⎪⎪⎨⎪⎪⎩m0 = 0,mk = mk−1 + 1akmn = mn−1 + 1an(cid:2)+n−(k+1)j=0(cid:2)1a j+k+1n−1k=11ak+1= 1a1+(cid:10)ji=0(1 +bi+kai+k,1 (cid:2) k (cid:2) n − 1,(cid:2)kj=1(cid:10)ki= jbiai).When n (cid:3) 2 and k (cid:2) n2 , we have= 1 + n − kn − k + 2+ (n − k)(n − k + 1)(n − k + 2)(n + 1)(k − 1) (cid:3) 1 + 14biai(k − 1).1 +k(cid:11)k(cid:23)j=1i= jIt follows thatmn (cid:3) 1a1+n/2−1(cid:11)k(cid:11)k(cid:23)(cid:19)1 +1ak+1biai(cid:20)(cid:22).i= jj=1(cid:4)(cid:21)= (cid:7)n2k=1(cid:3)n/2−1(cid:11)(cid:3) 12k=11 + 14(k − 1)This completes the proof. (cid:2)Contrary to the prior result that the expected runtime of (1 + 1) EA for SAT instance ψ1(x) is better than that ofRandomWalk, here we demonstrate that for SAT instance ψ2(x), the worst case expected runtime of RandomWalk is betterthan that of local (1 + 1) EA.In the following, we analyze the behavior of the hybrid algorithm of (1 + 1) EA and RandomWalk on SAT instance ψ2(x).We first set the selection probability p = 0.5.Proposition 8. If the selection probability p = 0.5, then for SAT instance ψ2(x), the expected runtime of the hybrid algorithm is(cid:4)n/2(cid:4)(cid:3)(cid:3)(cid:7)1n24e(cid:2) (cid:6)m(cid:6)∞ (cid:2) O(cid:3)(cid:3)(cid:4)n/2(cid:4).4eProof. For simplicity, we assume that n is odd.The proof is similar to that of Proposition 7 and the notations are the same as specified in the proof of Proposition 7.The transition probability of the stochastic process Xt (t = 0, 1, . . .) introduced by the hybrid algorithm of local (1 + 1)EA and RandomWalk with a probability p = 0.5 can be described as follows.When k = 0,P ( Xt+1 ∈ T 0 | Xt ∈ T 0) = 1.When 1 (cid:2) k (cid:2) (n + 1)/2,P ( Xt+1 ∈ Tk−1 | Xt ∈ Tk) = p(cid:3)(cid:4)+ (1 − p)kn,1 −(cid:3)n − k2(n − k + 1)(cid:4)1 − kn,P ( Xt+1 ∈ Tk | Xt ∈ Tk) = (1 − p)P ( Xt+1 ∈ Tk+1 | Xt ∈ Tk) = pn − k2(n − k + 1).254Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257When (n + 1)/2 < k (cid:2) n,(cid:3)P ( Xt+1 ∈ Tk−1 | Xt ∈ Tk) = p1 −n − k2(n − k + 1)(cid:4),P ( Xt+1 ∈ Tk | Xt ∈ Tk) = (1 − p)P ( Xt+1 ∈ Tk+1 | Xt ∈ Tk) = pk,nn − k2(n − k + 1)+ (1 − p)n − kn.(cid:26)(cid:26)Denoteak =bk =p(1 − n−kp(1 − n−kn−k2(n−k+1) ,n−k2(n−k+1)pp2(n−k+1) ) + (1 − p) k2(n−k+1) ),n , 1 (cid:2) k (cid:2) (n + 1)/2,(n + 1)/2 < k (cid:2) n.+ (1 − p) n−kn ,1 (cid:2) k (cid:2) (n + 1)/2,(n + 1)/2 < k (cid:2) n.(10)(11)Construct an auxiliary homogeneous Markov chain Zt (t = 0, 1, . . .) which is defined on the state {0, 1, . . . , n} space withthe transition matrix (3). Its mean first hitting time to the absorbing state 0 is given by Eqs. (9).n2 ( 4Now we show that the lower bound is (cid:7)( 1By assumption p = 0.5, according to Eqs. (9), we havee )n/2).(cid:19)1 +(cid:20)bkaki(cid:11)i(cid:23)j=1k= jmn = 1a1+n−1(cid:11)1ai+1i=1n−1(cid:23)bkak(cid:3) 1an= 2k=(n+1)/2+1n−1(cid:23)k=(n+1)/2+1(cid:3)n − kn − k + 21 + 2(n − k + 1)n(cid:4)(cid:21)from (10) and (11)(cid:22)=16(n − 1)(n + 1)(n−3)/2(cid:23)(cid:3)k=11 + 2(k + 1)n(cid:4).Note that(cid:19)(n−3)/2(cid:11)(cid:3)I :=(cid:4)(cid:20)21 + 2(k + 1)n(cid:4)2(cid:3)k=1n + 4n=· · · 2n − 1nn + 6n(cid:3) (n + 3)(n + 4) · · · (2n − 1)nn−3= 12n2(n + 1)(n + 2)(2n)!nnn!.Using Stirling formula√2πn(cid:3)(cid:4)nne< n!and(cid:3)n! <1 +112n − 1(cid:4)√(cid:3)(cid:4)n,ne2πnwe haveI (cid:3) 12= 12n2(n + 1)(n + 2)n2(n + 1)(n + 2)(cid:3)(cid:4)2n2n√1nnn!√2π 2n2π 2n(cid:4)(cid:3)e2n nnn!2e(12)(13)(cid:21)by Stirling formula (12)(cid:22)Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257255n2(cid:3) 1(n + 1)(n + 2)2(cid:21)(cid:22)= (cid:7)(4/e)n.√2π 2n(cid:4)2n(cid:3)2e1√2πn12n − 112nen(cid:21)by Stirling formula (13)(cid:22)Therefore mn (cid:3) (cid:7)( 1n2 ( 4e )n/2). That proves the lower bound.The upper bound can be proved as follows.Similar to the above equations, we have(cid:3)(cid:4)2I =n + 4nn + 6· · · 2n − 1nn(cid:2) (n + 4)(n + 5) · · · (2n)nn−3n3(n + 1)(n + 2)(n + 3)=(2n)!n!nn(cid:3)1 +√2π 2n(cid:3)(cid:4)(cid:3)(cid:4)2n2n124n − 1(cid:4)(cid:3)e(cid:4)2n124n − 12e(cid:2) 1n!nn√(cid:2)2π 2n(cid:21)(4/e)n1 +(cid:22).= O(cid:21)by Stirling formula (13)(cid:22)1√2πnen(cid:21)by Stirling formula (12)(cid:22)(14)Note that 1ak(cid:2) 4 (1 (cid:2) k (cid:2) n) and bkak(cid:2) 1 (1 (cid:2) k (cid:2) (n + 1)/2), we obtainmn = 1a1+n−1(cid:11)1ai+1i=1(cid:2) 4n + 4(n − 1)2(cid:19)1 +i(cid:11)i(cid:23)(cid:20)bkakk= jj=1n−1(cid:23)bkakk=(n+1)/2+1(cid:3)(n−3)/2(cid:23)(cid:4)1 + 2(k + 1)n= 4n + 32(cid:21)(4/e)n= On − 1n + 1(cid:21)(cid:22)by (14)k=1(cid:22).This completes the proof. (cid:2)In the above Proposition 8, we fixed the selection probability p = 0.5. In fact, when p (cid:2) 0.5, the result of the upperbound still holds.Proposition 9. If the selection probability p (cid:2) 0.5, then for SAT instance ψ2(x), the expected runtime of the hybrid algorithmis (cid:7)( 1n2 ( 4e )n/2).Proof. The proof is essentially the same that of Proposition 8. (cid:2)Proposition 10. If the selection probability p (cid:3) n−1is O (n2).n , then for SAT instance ψ2(x), the expected runtime of the hybrid algorithmProof. The proof is similar to that of Proposition 8.2 , according to Eqs. (10) and (11), notice that p (cid:3) n−1n , we have bkak(cid:2) 1.For 1 (cid:2) k (cid:2) n+1For n+12 < k < n, we have= n − k(cid:3)bkak(cid:3)n − k + 2(cid:2) n − kn − k + 2(cid:2) 1 + 1n.1 + 2(1 + (n − k))n1 + 2(1 + (n − k))n(cid:4)(cid:4)1 − pp1n − 1256Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257Table 1The expected runtime bounds of RandomWalk and (1 + 1) EA on SAT instance ψ1(x).Global (1 + 1) EA (Prop. 4)Local (1 + 1) EA (Prop. 3)RandomWalk (Prop. 5)(cid:2)(n ln n)(cid:2)(n ln n)(cid:2)(n2)Table 2The expected runtime bounds of three heuristic algorithms on SAT instance ψ2(x).Local (1 + 1) EA (Prop. 6)+∞ (|x| < n−12 )O (n ln n) ( n−1(cid:3) |x| (cid:3) n)2RandomWalk (Prop. 7)(cid:2)(n2)Hybrid algorithm (Prop. 9, 10)e )n/2) (p (cid:3) 0.5)(cid:3) p (cid:3) 1)(cid:7)( 1n2 ( 4O (n2) ( n−1nThen, for 1 (cid:2) j (cid:2) k < n, we get(cid:3)(cid:4)k− j+1k(cid:23)i= jbiai(cid:2)1 + 1n(cid:4)n(cid:3)1 + 1n(cid:2) e.(cid:2)Note that 1ak(cid:2) 2p (1 (cid:2) k (cid:2) n), we have(cid:19)mn = 1a1+(cid:19)n−1(cid:11)1ai+1i=11 +i(cid:11)i(cid:23)k= jj=1(cid:20)(cid:2) 2p1 +n−1(cid:11)i=1(e × i + 1)= O(cid:20)bkak(cid:21)n2(cid:22).This completes the proof. (cid:2)Remark 1. For the selecting probability p (cid:2) 0.5 or p (cid:3) n−1the hybrid algorithm on ψ2(x). Can we do better for it, i.e. can we get the tight bounds?n , we obtain lower and upper bounds of the expected runtime forRemark 2. For 0.5 < p < n−1However, we conjecture that the expected runtime changes from an exponential time to a polynomial time.n , the time complexity analysis of the hybrid algorithm on ψ2(x) is much more complicated.Tables 1 and 2 summarize the expected runtime bounds of three heuristic algorithms solving two SAT instances ψ1(x)and ψ2(x). From Table 1, we see that for instance ψ1(x), (1 + 1) EA is faster than RandomWalk. From Table 2, for instanceψ2(x), starting from the initial string x satisfying |x| (cid:2) (n − 1)/2, (1 + 1) EA can never reach the satisfying assignment andRandomWalk finds the satisfying assignment in (cid:2)(n2) on average. But (1 + 1) EA is still faster than RandomWalk when theinitial string x satisfies |x| (cid:2) (n − 1)/2.For instance ψ2(x), our analysis demonstrates the hybrid algorithm may help local (1 + 1) EA to escape from the localoptimum. It also shows that the expected runtime of the hybrid algorithm changes from a exponential time bound to apolynomial time bound as the selection probability varies. However, it remains unclear how this phase transition graduallyhappens, and it is worth further investigating.5. ConclusionIncomplete heuristic algorithms are now among the most prominent and frequently applied techniques for SAT problems.Many experimental comparisons with different heuristic algorithms have been reported, although theoretic comparisonsare rare. This paper contributes to the theory of heuristic algorithms for SAT problems. We derive the expected runtimebounds of RandomWalk on k-SAT problem. We construct two 2-SAT instances and provide analytic comparisons amongRandomWalk, (1 + 1) EA, and hybrid algorithm on these instances. It is shown that these heuristic algorithms have theirown advantages and disadvantages in solving two SAT instances and their expected runtime ranges from a polynomialtime to an exponential time. Our analysis provides an insight into the runtime behavior among these heuristic algorithms.Admittedly, two SAT instances ψ1(x) and ψ2(x) considered in this paper are relatively simple. Future investigation shouldbe extended to a broader class of SAT problems and more heuristic algorithms, such as UnitWalk [15] and PPSZ [27].Theoretic runtime analysis and comparison for heuristic algorithms on the SAT problem lag far behind experimentalcomparisons. Effort should be made to fill in the gap between theoretical studies on SAT and the design and application ofpractical algorithms, even though such an investigation will be challenging.Y. Zhou et al. / Artificial Intelligence 173 (2009) 240–257257AcknowledgementsThe authors are very grateful to the anonymous reviewers for their valuable comments and helpful suggestions, whichimproved the paper’s quality greatly. This work is partially supported by National Natural Science Foundation of China (Nos.60673062, 60873078) and Natural Science Foundation of Guangdong Province of China (No. 06025686) to YZ; and NIHgrants P50GM76516 and R01GM75309, and NSF grants DMS0511169 to QN.References[1] E.H.L. Aarts, J.H.M. Korst, Simulated Annealing and Boltzmann Machines, Wiley, 1989.[2] M. Alekhnovich, E. Ben-Sasson, Linear upper bounds for random walk on small density random 3-CNF, SIAM Journal on Computing 36 (5) (2006)1248–1263.[3] P. Beame, H. Kautz, A. Sabharwal, Towards understanding and harnessing the potential of clause learning, Journal of Artificial Intelligence Research 22(2004) 319–335.[4] A. Braunstein, M. Mezard, R. Zecchina, Survey propagation: An algorithm for satisfiability, Random Structures & Algorithms 27 (2) (2005) 201–226.[5] S. Cook, The complexity of theorem-proving procedures, in: Proc. 3rd Ann. ACM Symp. on Theory of Computing, Assoc. Comput. Mach., New York,1971, p. 151.[6] M. Davis, H. Putnam, A computer procedure for quantification theory, Journal of the ACM 7 (3) (1960) 202–215.[7] K.A. DeJong, W.M. Spears, Using genetic algorithm to solve NP-complete problems, in: Proceedings of the 3rd International Conference on GeneticAlgorithms, Virginia, USA, 1989, pp. 124–132.[8] S. Droste, T. Jansen, I. Wegener, A natural and simple function which is hard for all evolutionary algorithms, in: Proceedings of Third Asia–PacificConference on Simulated Evolution and Learning, Nagaya, Japan, 2000, pp. 2704–2709.[9] S. Droste, T. Jansen, I. Wegener, On the analysis of the (1 + 1)-evolutionary algorithm, Theoretical Computer Science 276 (1–2) (2002) 51–81.[10] M.R. Garey, D.S. Johnson, Computers and Intractability—A Guide to the Theory of NP-completeness, Freeman, New York, 1979.[11] J. Garnier, L. Kallel, M. Schoenauer, Rigorous hitting times for binary mutations, Evolutionary Computation 7 (2) (1999) 167–203.[12] J. Gottlieb, E. Marchiori, C. Rossi, Evolutionary algorithms for the satisfiability problem, Evolutionary Computation 10 (1) (2002) 35–50.[13] J. Gu, Efficient local search for large-scale satisfiability problems, ACM SIGART Bulletin 3 (1) (1992) 8–12.[14] J. He, X. Yao, Towards an analytic framework for analyzing the computation time of evolutionary algorithms, Artificial Intelligence 145 (1–2) (2003)59–97.[15] E.A. Hirsch, A. Kojevnikov, UnitWalk: A new SAT solver that uses local search guided by unit clause elimination, Annals of Mathematics and ArtificialIntelligence 43 (2005) 91–111.[16] W. Hoeffding, Probability inequalities for sums of bounded random variables, Journal of the American Statistical Association 58 (301) (1963) 13–30.[17] M. Iosifescu, Finite Markov Processes and Their Applications, John Wiley & Sons, Chichester, 1980.[18] K. Iwama, S. Tamaki, Improved bounds for 3-SAT, in: Proceedings of the Fifteenth Annual ACM–SIAM Symposium on Discrete Algorithms, 2004, pp.321–322.[19] S. Kirkpatrick, C.D. Gelatt, M.P. Vecchi, Optimization by simulated annealing, Science 220 (1983) 671–680.[20] F. Lardeux, F. Saubion, J.K. Hao, GASAT: A genetic local search algorithm for the satisfiability problem, Evolutionary Computation 14 (2) (2006) 223–253.[21] L.A. Levin, Universal search problems, Problemy Peredachi Informatsii 9 (1973) 115–116 (in Russian), translation: Problems of Information Transmis-sion 9 (3) (1975) 265–266.[22] M. Mezard, G. Parisi, R. Zecchina, Analytic and algorithmic solution of random satisfiability problems, Science 297 (2002) 812–815.[23] R. Motwani, P. Raghavan, Randomized Algorithms, Cambridge Univ. Press, Cambridge, UK, 1995.[24] A.E. Nix, M.D. Vose, Modeling genetic algorithms with Markov chain, Annals of Mathematics and Artificial Intelligence 5 (1) (1992) 79–88.[25] C.H. Papadimitiou, On selecting a satisfying truth assignment, in: Proceedings of the 32nd Annual IEEE Symposium on Foundations of ComputerScience, 1991, pp. 163–169.[26] C.H. Papadimitiou, Computational Complexity, Addison Wesley, 1994.[27] P. Paturi, P. Pudlak, M.E. Saks, F. Zane, An improved exponential-time algorithm for k-SAT, Journal of the ACM 52 (3) (2005) 337–364.[28] D. Rolf, Improved bound for the PPSZ/Schöning-algorithm for 3-SAT, Electronic Colloquium on Computational Complexity (ECCC), Report No. 159, 2005.[29] U. Schöning, A probabilistic algorithm for k-SAT and constraint satisfaction problems, in: Proceedings of the 40th Annual Symposium on Foundation ofComputer Science, 1999, pp. 410–414.[30] U. Schöning, Principles of stochastic local search, in: Proceedings of Sixth Conference on Unconventional Computation, Kingston, Canada, 2007, in:Lecture Notes in Computer Science, vol. 4618, 2007, pp. 178–187.[31] B. Selman, H. Levesque, D. Mitchell, A new method for solving hard satisfiability problems, in: Proceedings of Tenth National Conference on ArtificialIntelligence (AAAI-92), San Jose, 1992, pp. 440–446.[32] B. Selman, H.A. Kautz, B. Cohen, Noise strategies for improving local search, in: Proceedings of Twelfth National Conference on Artificial Intelligence(AAAI-94), vol. 1, 1994, pp. 337–343.[33] W. Wei, B. Selman, Accelerating random walks, in: Proceedings of 8th Intl. Conference on the Principles and Practice of Constraint Programming(CP-2002), in: Lecture Notes in Computer Science, vol. 2470, 2002, pp. 216–232.[34] H. Zhang, M. Stickel, Implementing the Davis Putnam method, Journal of Automated Reasoning 24 (12) (2000) 277–296.