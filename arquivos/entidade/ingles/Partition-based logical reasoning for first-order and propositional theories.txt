Artificial Intelligence 162 (2005) 49–88www.elsevier.com/locate/artintPartition-based logical reasoning for first-order andpropositional theoriesEyal Amir a, Sheila McIlraith b,∗a Computer Science Department, University of Illinois, Urbana-Champaign, Urbana, IL 61801, USAb Department of Computer Science, University of Toronto, Toronto, Ontario M5S 3H5, CanadaReceived 20 November 2001; accepted 5 November 2004Available online 15 December 2004AbstractIn this paper we show how tree decomposition can be applied to reasoning with first-order andpropositional logic theories. Our motivation is two-fold. First, we are concerned with how to reasoneffectively with multiple knowledge bases that have overlap in content. Second, we are concernedwith improving the efficiency of reasoning over a set of logical axioms by partitioning the set withrespect to some detectable structure, and reasoning over individual partitions either locally or in adistributed fashion. To this end, we provide algorithms for partitioning and reasoning with relatedlogical axioms in propositional and first-order logic.Many of the reasoning algorithms we present are based on the idea of passing messages betweenpartitions. We present algorithms for both forward (data-driven) and backward (query-driven) mes-sage passing. Different partitions may have different associated reasoning procedures. We character-ize a class of reasoning procedures that ensures completeness and soundness of our message-passingalgorithms. We further provide a specialized algorithm for propositional satisfiability checking withpartitions. Craig’s interpolation theorem serves as a key to proving soundness and completeness ofall of these algorithms. An analysis of these algorithms emphasizes parameters of the partitioningsthat influence the efficiency of computation. We provide a greedy algorithm that automatically de-composes a set of logical axioms into partitions, following this analysis. 2004 Published by Elsevier B.V.* Corresponding author.E-mail addresses: eyal@cs.uiuc.edu (E. Amir), sheila@cs.toronto.edu (S. McIlraith).0004-3702/$ – see front matter  2004 Published by Elsevier B.V.doi:10.1016/j.artint.2004.11.00450E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88Keywords: Reasoning with structure; Theorem proving; First-order logic; SAT; Tree decomposition; Graphicalmodels; Parallel computation; Distributed computation1. IntroductionThere is growing interest in building large knowledge bases (KBs) of everyday knowl-edge about the world, teamed with theorem provers or other reasoners to perform inference.Three such systems are Cycorp’s Cyc, and the High Performance Knowledge Base (HPKB)systems developed by Stanford’s Knowledge Systems Lab (KSL) (e.g., [51]) and by SRI(e.g., [24]). These KBs comprise tens/hundreds of thousands of logical axioms. One ap-proach to dealing with the size and complexity of these KBs is to structure the content insome way, such as into multiple domain- or task-specific KBs, or into microtheories. Inthis paper, we investigate how to reason effectively with partitioned sets of logical axiomsthat have overlap in content, and that may even have different reasoning engines. Moregenerally, we investigate the problem of how to exploit structure inherent in a set of logicalaxioms to induce a partitioning of the axioms that will improve the efficiency of reasoning.To this end, we propose partition-based logical reasoning algorithms, for reasoningwith logical theories1 that are decomposed into related partitions of axioms. Our algo-rithms exploit the idea of tree decomposition (e.g., [7]), extending it to propositional andfirst-order logic (FOL) theorem proving. We provide forward (data-driven) and backward(query-driven) message-passing algorithms over theories that are partitioned into a struc-ture very similar to a join-tree [66], specializing them for resolution theorem proving.We also provide an algorithm for partition-based propositional satisfiability (SAT). Ourmessage-passing algorithms are designed so that, without loss of generality, reasoningwithin a partition can be realized by an arbitrary consequence-finding engine [67]. Wecharacterize a class of reasoning procedures that ensures completeness and soundness ofour algorithms. We use Craig’s interpolation theorem [30] to prove the soundness and com-pleteness of all our message-passing algorithms with respect to this class of procedures. Itis also used to prove the soundness and completeness of our propositional satisfiability al-gorithm. We investigate the impact of these algorithms on resolution-based inference, andanalyze the computational complexity for our partition-based SAT algorithm.A critical aspect of partition-based logical reasoning is the selection of a good partition-ing of the theory. The computational analysis of our partition-based reasoning algorithmssuggests parameters of partitionings that influence the computation of our algorithms: thenumber of nonlogical symbols included in the communication between partitions, the sizeof each partition, and the topology of the partitions graph. This observation guides us topropose a generic algorithm for decomposing logical theories into partitions, and a greedyalgorithm that tries to optimize these parameters.Surprisingly, there has been little work on the specific problem of exploiting structure inFOL theorem proving in the manner we propose. We speculate that this might be attributed1 In this paper, every set of axioms is a theory (and vice versa). Also, unless stated otherwise, theories, axiomsand KBs are in first-order logic.E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8851to the fact that FOL theorem proving has traditionally examined mathematics domains, thatdo not necessarily have structure that supports decomposition. Nevertheless, tree decom-position methods similar to those we apply here have been used successfully in reasoningwith Bayes networks (e.g., [66,83]), constraint satisfaction problems (e.g., [37]), proposi-tional reasoning (e.g., [38,86]) and originally in dynamic programming [12]. The commoninsight is that when knowledge can be partitioned into clusters that interact in a tree-likemanner, reasoning can be accomplished in time that is exponential in a graph parameterknown as tree-width that captures the size of the clusters relative to the original problemgraph [7]. Where possible, we adopt this common terminology in our paper, to relate ourwork to previous contributions.The rest of the paper is organized as follows. Section 2 describes our message-passingalgorithms and sufficient conditions for their soundness and completeness. In Section 3 wespecialize these algorithms to theorem proving using resolution and discuss the efficiencyof message-passing. Section 4 offers an algorithm for propositional satisfiability and an-alyzes its computational complexity. Section 5 presents an algorithm for decomposing alogical theory. Finally, Section 6 discusses some related work. Some of the results in thispaper appeared previously in [5,78].2. Partition-based theorem provingIn this section we address the problem of how to reason with an already partitionedpropositional or FOL theory using theorem proving. In particular, we propose forwardand backwards message-passing algorithms, in the spirit of Pearl [83]. We further identifyconditions underwhich partition-specific theorem proving results in sound and completepartition-based logical reasoning.(cid:1)iWe define the following terminology. {Ai }i(cid:1)n is a partitioning of a logical theory Aif A =Ai . Each individual Ai is called a partition, L(Ai ) is its signature (the set ofnon-logical symbols), and L(Ai ) is its language (the set of formulae built with L(Ai )).Each partitioning defines a labeled graph G = (V , E, l), which we call the intersectiongraph. In the intersection graph, each node i corresponds to an individual partition Ai ,(V = {1, . . . , n}), two nodes i, j are linked by an edge if L(Ai ) and L(Aj ) have a sym-bol in common (E = {(i, j ) | L(Ai ) ∩ L(Aj ) (cid:3)= ∅}). The edges are labeled with the set ofsymbols that the associated partitions share (l(i, j ) = L(Ai ) ∩ L(Aj )). We refer to l(i, j )as the communication language between partitions Ai and Aj . We ensure that the inter-section graph is connected by adding a minimal number of edges to E with empty labels,l(i, j ) = ∅.We illustrate the notion of a partitioning in terms of the simple propositional theory A,depicted on the left of Fig. 1 (this is the clausal form of the theory presented with materialimplication in Fig. 2). These axioms capture the functioning of aspects of an espressomachine. The first four axioms denote that if the machine pump is OK and the pump is on,then the machine has a water supply. Alternately, the machine can be filled manually, butit is never the case that the machine is filled manually while the pump is on. The next fouraxioms denote that there is steam if and only if the boiler is OK and is on, and there is a52E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88Fig. 1. A partitioning of A and its intersection graph.ok_pump ∧ on_pump ⇒ waterman_fill ⇒ ¬on_pumpwater ∧ ok_boiler ∧ on_boiler ⇒ steam ¬water ⇒ ¬steam¬ok_boiler ⇒ ¬steamsteam ∧ coffee ⇒ hot_drinksteam ∧ teabag ⇒ hot_drink¬on_boiler ⇒ ¬steamcoffee ∨ teabagman_fill ⇒ water¬man_fill ⇒ on_pumpFig. 2. Axiomatization of a simplified espresso machine.supply of water. The final three axioms denote that there is always either coffee or tea, andthat steam and coffee (or tea) result in a hot drink.The right-hand side of Fig. 1 depicts a decomposition of A into three partitions A1, A2,A3 and its intersection graph. The labels for the edges (1, 2) and (2, 3) are {water} and{steam}, respectively.2.1. Forward message passingIn this section, we propose a forward message-passing algorithm for reasoning withpartitions of logical axioms. Fig. 3 describes our forward message-passing algorithm,FORWARD-M-P (MP), for finding the truth value of query formula Q ∈ L(Ak), k (cid:1) n,given partitioned theory A and graph G = (V , E, l). G may be the intersection graph of A,but is not always so.To determine the direction in which messages should be sent in the graph G, step (1) inMP computes a strict partial order over nodes in the graph using the partitioning togetherwith a query, Q.E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8853PROCEDURE FORWARD-M-P (MP)({Ai }i(cid:1)n, G, Q){Ai }i(cid:1)n a partitioning of the theory A, G = (V , E, l) a graph describingthe connections between the partitions, Q a query in L(Ak) (k (cid:1) n).(1) Determine ≺ as in Definition 2.1.(2) Concurrently,(a) Perform consequence finding in each of the partitions Ai , i (cid:1) n.(b) For every (i, j ) ∈ E such that i ≺ j , for every consequence ϕ of Ajfound (or ϕ in Aj ), if ϕ ∈ L(l(i, j )), then add ϕ to the set of axiomsof Ai .(c) If Q is proven in Ak (we derive a subsuming formula or initiallyadd ¬Q to Ak and derive inconsistency), return YES.Fig. 3. A forward message-passing algorithm.Definition 2.1 (≺). Given partitioned theory A =Ai , associated graph G = (V , E, l)and query Q ∈ L(Ak), let dist(i, j ) (i, j ∈ V ) be the length of the shortest path betweennodes i, j in G. Then i ≺ j iff dist(i, k) < dist(j, k).i(cid:1)n(cid:1)This algorithm exploits consequence finding (step (2a)) to perform reasoning in theindividual partitions. Consequence finding was defined by Lee [67] to be the problem offinding all the logical consequences of a theory or sentences that subsume them. Recall,in clausal FOL, ϕ subsumes ψ if there is a substitution θ such that ϕθ ⊂ ψ. ϕ strictlysubsumes ψ if ϕ subsumes ψ and ψ does not subsume ϕ.Theorem 2.4 proves the soundness and completeness of our MP algorithm. It requireseach of the reasoners in step (2) to be sound and complete.Definition 2.2 (Completeness for consequence finding). Given a set of formulae A and areasoning procedure R, R is complete for consequence finding iff for every clause ϕ, thatis a non-tautologous logical consequence of A, R derives a clause ψ from A such that ψsubsumes ϕ.Furthermore, we say that R is complete for consequence finding in FOL (as opposedto clausal FOL) iff for every non-tautologous logical consequence ϕ of A, R derives alogical consequence ψ of A such that ψ |= ϕ and ψ ∈ L(ϕ).In Section 3.1 we show that every reasoning procedure that is complete for consequencefinding in clausal FOL can be converted to a reasoning procedure that is complete forconsequence finding in FOL. In propositional logic the two conditions are identical.Consequently, we can use any sound and complete consequence-finding algorithm forreasoning within an individual partition in MP. This is not restricted to variants of reso-lution. Nevertheless, the resolution rule is complete for clausal consequence finding (e.g.,[67,98]) and the same is true for several variants of linear resolution such as the ones thatare described by Inoue [61] and Minicozzi and Reiter [80]. A weaker version of complete-54E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88Using FORWARD-M-P to prove hot_drinkPartitionA1A1A1A2A2A2A3A3A3Resolve(2),(4)(m1), (1)(m2), (12)(cid:11)⇒(m3), (5)(m4), (13)(m5), (14)(cid:11)⇒(10)(9),(m7), (11)(m8), (m6)Generatingon_pump ∨ water¬ok_pump ∨ waterwaterclause water passed from A1 to A2ok_boiler ∧ on_boiler ⊃ steam¬on_boiler ∨ steamsteamclause steam passed from A2 to A3¬steam ∨ teabag ∨ hot_drink¬steam ∨ hot_drinkhot_drink(m1)(m2)(m3)(m4)(m5)(m6)(m7)(m8)(m9)Fig. 4. A proof of hot_drink from A in Fig. 1 after asserting ok_pump (12) in A1 and ok_boiler (13), on_boiler(14) in A2.ness for consequence finding is also true for semantic resolution [99] and set-of-supportresolution. We discuss the case of using resolution further in Section 3.In addition, there are reasoning methods that focus on a given sublanguage as discussedin [18,52,61], and also [38,39,64,70,75]. An example of such restricted consequence find-ers is a prime implicate generator over a sublanguage. (Recall, a clause, ϕ, is a primeimplicate of a theory T if T |= ϕ and no formula that strictly subsumes ϕ is entailedfrom T .) Such consequence finders are commonly used for prime implicate generation inapplications such as diagnosis and abduction [77]. Consequence finders that focus on asublanguage can be directly used in MP for reasoning within partitions. Alternatively, theycan be used in a batch mode to generate select consequences in the sublanguage and thensend the messages in batch. In Fig. 4 we illustrate an execution of MP using resolution.Given a partitioning whose intersection graph forms an undirected tree, our MP al-gorithm is a sound and complete proof procedure. The completeness relies on Craig’sinterpolation theorem.Theorem 2.3 (Craig’s interpolation theorem [30]). If α (cid:13) β, then there is a formula γinvolving only symbols common to both α and β, such that α (cid:13) γ and γ (cid:13) β.Craig’s interpolation theorem is true even if we take α, β to be infinite sets of sentences[98] and use resolution theorem proving [60,98] with or without equality [30,31] (all afterproper reformulation of the theorem).Ai be a partitioned theoryTheorem 2.4 (Soundness and completeness). Let A =with the intersection graph G being a tree (i.e., no cycles). Let k (cid:1) n and ϕ a sentence inL(Ak). If the reasoning procedure in each partition is sound and complete for consequencefinding in FOL (as defined in Definition 2.2), then A |= ϕ iff MP outputs YES.i(cid:1)n(cid:1)Proof. See Appendix A.1.E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8855Fig. 5. An intersection graph before (left) and after (right) applying BREAK-CYCLES.Note that Theorem 2.4 requires the intersection graph of A to be a tree. If the intersec-tion graph of A is not a tree, and MP uses it as input, then MP may fail to be a completeproof procedure. Fig. 5 illustrates the problem. The left-hand side of Fig. 5 illustrates theintersection graph of partitioning A1, A2, A3, A4 of a theory A. If we try to prove s (whichfollows from A) from this partitioning and graph using MP, nothing will be transmitted be-tween the partitions. For example, we cannot send p ⇒ s from A2 to A4 because the graphonly allows transmission of sentences containing s.Thus, using MP with the left-hand side graph will fail to prove s. In such a case, we canfirst syntactically transform the intersection graph into a tree with enlarged labels, (i.e., anenlarged communication language) and apply MP to the resultant tree. In particular, wewould like the resultant tree to have a proper labeling for the given partitioning.Ai , we say that associatedDefinition 2.5 (Proper labeling). For a partitioning A =tree G = (V , E, l) has a proper labeling, if for all (i, j ) ∈ E and B1, B2, the two subtheoriesof A on the two sides of the edge (i, j ) in G, it is true that l(i, j ) ⊇ L(B1) ∩ L(B2).i(cid:1)n(cid:1)Note that this property is analogous to the running intersection property used in join-treealgorithms for inference in Bayes networks (e.g., [11,33,96]) and constraint satisfactionproblems (e.g., [37,58]). The running intersection property is defined with respect to par-titions or cluster whose contents are individual logical proposition, rather than a set oflogical axioms formed from such logical propositions. In our context, the running intersec-tion property requires that if a symbol s appears in Ai and Aj , then s appears in all thepartitions on the tree-path between Ai and Aj . In contrast, the proper labeling property isdefined with respect to the language L(Ai ) of a partition Ai , the partition itself being a setof logical axioms. As such, proper labeling is a condition that is applied only to the linkson a path and not the partitions themselves.The following lemma provides the main argument behind most of the completenessproofs in this paper.Lemma 2.6. Let A =Ai be a partitioned theory and assume that the associatedgraph G is a tree that has a proper labeling for the partitioning {Ai}i(cid:1)n. Also assumei(cid:1)n(cid:1)56E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88PROCEDURE BREAK-CYCLES(G = (V , E, l))(1) Find a minimal-length cycle of nodes v1, . . . , vc (v1 = vc) in G. If there areno cycles, return G.(cid:2)(2) Select index a s.t. a < c and|l(vj , vj +1) ∪ l(va, va+1)| is minimal(the label of (va, va+1) adds a minimal number of symbols to the rest of thecycle).a(cid:3)=j <c(3) For all j < c, j (cid:3)= a, set l(vj , vj +1) ← l(vj , vj +1) ∪ l(va, va+1).(4) Set E ← E \ {(va, va+1)}, l(va, va+1) ← ∅ and go to (1).Fig. 6. An algorithm to transform an intersection graph G into a tree.that each of the reasoning procedures used in MP is complete for consequence finding (asdefined in Definition 2.2). Let k (cid:1) n and let Q ∈ L(Ak ∪(k,i)∈E l(k, i)) be a sentence. IfA |= Q, then MP outputs YES.(cid:1)Proof. See Appendix A.1.Observe that Theorem 2.4 is overly narrow. Even when the intersection graph is not atree, MP can be sound and complete.Algorithm BREAK-CYCLES, shown in Fig. 6, performs a transformation that producesa tree with a proper labeling from any labeled graph. (|X| denotes the cardinality of aset X.) It is of particular importance when we are given a set of KBs that we cannot mergeor restructure. Note that Section 5 gives a more general algorithm and treats the case wheresuch restructuring is possible.Using BREAK-CYCLES, we can transform the graph depicted on the left-hand side ofFig. 5, into the tree on its right. First, we identify the minimal cycle (cid:17)(1, 3), (3, 4), (4, 1)(cid:18),remove (4, 1) from E and add r to the labels of (1, 3), (3, 4). Then, we find the minimalcycle (cid:17)(2, 3), (3, 4), (4, 2)(cid:18) and remove (2, 3) from E (s already appears in the labels of(4, 2), (3, 4)). Finally, we identify the minimal cycle (cid:17)(1, 3), (3, 4), (4, 2), (2, 1)(cid:18), remove(4, 2) and add s to the rest of the cycle. The proof of s by MP now follows by sendingp ⇒ s from A2 to A1, sending q ∨ r ∨ s from A1 to A3, sending r ∨ s from A3 to A4 andconcluding s in A4.Notice that when executing BREAK-CYCLES, we may remove an edge that partici-pates in more than one minimal cycle (as is the case when removing the edge (4, 1)), butits removal influences the labels of only one cycle.Theorem 2.7 (Soundness and completeness). Let A =Ai be a partitioned theorywith intersection graph G. Let k (cid:1) n and ϕ a sentence in L(Ak). If the reasoning pro-cedure in each partition is sound and complete for consequence finding (as defined inDefinition 2.2), then A |= ϕ iff applying BREAK-CYCLES and then MP outputs YES.i(cid:1)n(cid:1)Proof. See Appendix A.2.E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8857BREAK-CYCLES is a greedy algorithm that has a worst-case complexity of O(|E|2 ·m)(where m is the number of symbols in L(A)). The rationale is roughly as follows: There areat most |E| cycles that can be broken, step (1) takes O(|E|) time, step (3) takes O(|E| · m)time, and step (2) can be implemented to take O(|E|·m) time using dynamic programming.Other algorithms that we may use in this context are variants on the cutset method forreasoning with graphs [9,10]. Darwiche [33] used an algorithm that is similar to BREAK-CYCLES for the problem of creating a join tree. Our algorithm differs from Darwiche’sin treating an already formed partition and creating the tree in a greedy way (Darwiche’smethod randomly selects a tree).Finally, from Theorems 2.4 and 2.7 we observe that if partitioned theory A =Aiis a tree decomposition as defined by Arnborg and others (e.g., [7]), then MP is sound andcomplete.i(cid:1)n(cid:1)2.2. Backward message passingOur MP algorithm uses the query Q to induce an ordering on the partitions, whichin turn may guide selective consequence finding for reasoning forward. Many theoremproving strategies exploit the query more aggressively by reasoning backwards from thequery. Such strategies have proven effective for a variety of reasoning problems, such asplanning. Indeed, many theorem provers (e.g., PTTP [101]) are built as backward reasonersand must have a query or goal in order to run.One way to use MP for an analogous backward message-passing scheme is to assert¬Q in Ak, choose a partition Aj that is most distant from Ak in G (where the distancebetween 2 nodes in graph G is the number of nodes comprising the shortest path betweenthe two nodes), and try to prove {} in Aj using MP. If we wish to follow the spirit ofbackward-reasoning more closely, we can transform G into a chain in a similar way to ourtransformation of G into a tree using BREAK-CYCLES. The resultant chain graph maythen be used for query-driven backward message passing, from Ak. We present such analgorithm, called BACKWARD-M-P (BMP), in Fig. 7. BMP takes as input a partitionedtheory A, a graph G0, and a query, Q, and returns YES if it can prove Q.Procedure CHAINIFY is outlined in Fig. 8. It accepts a labeled graph and returns atransformation of the graph into a chain (changing the labels appropriately). Alternately,we can create a chain directly from the partitions and a total order over them. CHAINIFYensures that the resulting graph has a proper labeling. BMP is sound and complete if thereasoning procedure used in every partition is complete for consequence finding.Theorem 2.8 (Soundness and completeness). Let A =Ai be a partitioned theory.Let k (cid:1) n and ϕ ∈ L(Ak) a sentence. If the reasoning procedure used in each partition issound and complete for consequence finding, then A |= ϕ iff applying BMP outputs YES.i(cid:1)n(cid:1)Proof. See Appendix A.3.Algorithm BMP is presented for the case of subgoal-disjunctive systems, i.e., a proofof any subgoal yields a proof of the entire query. This is the case with resolution and its58E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88PROCEDURE BACKWARD-M-P(BMP)({Ai}i(cid:1)n, G0, Q){Ai }i(cid:1)n a partitioned theory, G0 = (V , E, l) a graph, Q a query in L(Ak ) (k (cid:1) n).(1) G ← CHAINIFY(G0, k).(2) For all i (cid:1) n, i (cid:3)= k set goaliSet goalk ← Q.(3) Concurrently,← FALSE (the goal of Ai is to prove FALSE).(a) For each partition Ai , i (cid:1) n, attempt to prove goali .(b) For every (i, j ) ∈ E such that i ≺ j , if we generate a subgoala ϕ in Ajand ϕ ∈ L(l(i, j )), then setb goali(c) If goali is proved in any Ai , return YES.← goali∨ ϕ.a In resolution every generated clause can be considered the negation of a subgoal.b In resolution refutation the goal is negated, so this step essentially adds ¬ϕ to Ai .Fig. 7. A backward message-passing algorithm.PROCEDURE CHAINIFY(G, k)G = (V , E, l) a graph describing connections between partitions, k (cid:1) |V |.(1) Let dist(i, j ) (i, j ∈ V ) be the length of the shortest path between i, j in G.Let i ≺0 j iff dist(i, k) < dist(j, k) (≺0 is a strict partial order).(2) Impose a total order ≺ on V that agree with ≺0 (i.e., i ≺0 j ⇒ i ≺ j ).(3) Let {va}a(cid:1)n = V such that v1 = k, ∀a (cid:1) n va ≺ va+1.(4) Let E(cid:20) = {(va, va+1)}i<n.(5) Set l(cid:20)(i, j ) ← ∅ for all i, j ∈ V .(6) For all (i, j ) ∈ E, for all a < n, if i (cid:21) va ≺ j (i.e., va is between i and j ),then set l(cid:20)(va, va+1) ← l(cid:20)(va, va+1) ∪ l(i, j ).(7) Return G(cid:20) = (V , E(cid:20), l(cid:20)).Fig. 8. A procedure that transforms a graph G into a chain G(cid:20).variants. The intuition behind the algorithm is that when a partition is supplied a subgoalsentence ϕ from another partition, ϕ is added to (OR-ed with) the partition’s goal.We make G a chain because otherwise subgoals may have to split between partitions.Splitting subgoals requires accounting for different preconditions (as in natural deduction),which we wish to avoid here, for simplicity of inference.2.3. Queries drawn from multiple partitionsMP and its variants require that query Q be in the language of a single partition, L(Ak),for some k (cid:1) n. One way to answer a query Q that comprises symbols drawn from multiplepartitions is to add a new partition AQ, with language L(AQ) = L(Q), the language ofthe query. AQ may contain ¬Q or no axioms. Following addition of this new partition,BREAK-CYCLES must be run on the new intersection graph to ensure a proper labelingE. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8859of G for the partitioned theory (as discussed in Section 2.1). To prove Q in AQ, we runMP on the resulting graph.Alternately, we can decompose the query into the appropriate partitions, following themethods of [81] or [97]. Since the issue of decomposing a query is not simple, we describeonly the simple case of a propositional query and leave the first-order case (with literalsthat contain symbols from multiple partitions) for future work.∨ · · · ∨ Qlrl), where each QiGiven a propositional query Q, we transform it into the form (Q11) ∧ · · · ∧j is a formula in the language of a single partition L(Akij )(Ql1(kij is the index of a partition that includes the vocabulary of Qij ). For example, if Q is inby asserting ¬QiCNF, it is already in this form. We check a disjunct Qij in1Akij for all j (cid:1) ri , and proving FALSE in one of the partitions. To prove Q we check eachof the disjunct in its transformed form. It is a valid consequence of A iff all the disjunctsare valid consequences of A. We discuss this special topic no further here, and assume Qis drawn from L(Ak), for some k (cid:1) n.∨ · · · ∨ Q1r1∨ · · · ∨ Qiri3. Resolution and message-passingThe previous section presented message-passing algorithms with an arbitrary soundand complete consequence finder. In this section, we specialize our message-passing al-gorithms with consequence finders that specifically employ resolution. We focus on thefirst-order case of resolution. We also analyze the effect message passing has on the com-putational efficiency of resolution-based inference.The presentation in this section makes explicit reference to the forward message-passingalgorithm, MP, but we wish to stress that the results in this section are equally applicableto other message-passing algorithms introduced in the previous section. For backgroundmaterial on resolution, the reader is referred to [45,54] as well as to [22,72].3.1. Resolution message-passingResolution [88] is one of the most widely used reasoning methods for automated deduc-tion, and more specifically for consequence finding. As noted in Section 2, the resolutionrule is complete for clausal consequence finding. It requires the input formula to be inclausal form, i.e., a conjunction of disjunctions of unquantified literals. For general first-order formulae, a transformation to clausal form (e.g., [71]) includes Skolemization, whicheliminates quantifiers and possibly introduces new constant symbols and new functionsymbols.We present algorithm RESOLUTION-M-P (RES-MP), which uses resolution (or reso-lution strategies), in Fig. 9. The rest of this section is devoted to explaining four differentimplementations for subroutine RES-SEND(ϕ, j , i), used by this procedure to send ap-propriate messages across partitions: the first implementation is for clausal propositionaltheories; the second is for clausal FOL theories, with associated graph G, which is a prop-erly labeled tree and whose labels include all the function and constant symbols of thelanguage; the third is also for clausal FOL theories, but it uses unskolemization and subse-60E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88PROCEDURE RESOLUTION-M-P(RES-MP)({Ai}i(cid:1)n, G, Q){Ai }i(cid:1)n a partitioned theory, G = (V , E, l) a graph, Q a query formula in thelanguage of L(Ak) (k (cid:1) n).(1) Determine ≺ as in Definition 2.1.(2) Add the clausal form of ¬Q to Ak .(3) Concurrently,(a) Perform resolution in each of the partitions Ai , i (cid:1) n.(b) For every (i, j ) ∈ E such that i ≺ j , if partition Aj includes the clauseϕ (as input or resolvent) and the predicates of ϕ are in L(l(i, j )), thenperform RES-SEND(ϕ, j , i).(c) If Q is proven in Ak , return YES.Fig. 9. A resolution forward message-passing algorithm.quent Skolemization to generate the messages to be passed across partitions; the fourth isa refinement of the third for the same class of theories that avoids unskolemization.In the propositional case, subroutine RES-SEND(ϕ, j , i) (Implementation 1) simplyadds ϕ to Ai , as done in MP. MP is then sound and complete.In the FOL case, implementing RES-SEND requires more care. To illustrate, considerthe case where resolution generates the clause P (B, x) (B a constant symbol and x a vari-able). It also implicitly proves that ∃b P (b, x). RES-MP may need to send ∃b P (b, x) fromone partition to another, but it cannot send P (B, x) if B is not in the communication lan-guage between partitions (for ground theories there is no such problem (see [98])). In thefirst-order case, completeness for consequence finding for a clausal first-order logic lan-guage (e.g., Lee’s result for resolution) does not guarantee completeness for consequencefinding for the corresponding full FOL language. This problem is also reflected in a slightlydifferent statement of Craig’s interpolation theorem [30] that applies for resolution [98].A simple way of addressing this problem is to add all constant and function symbolsto the communication language between every connected set of partitions. This has theadvantage of preserving soundness and completeness, and is simple to implement. In thiscase, subroutine RES-SEND(ϕ, j , i) (Implementation 2) simply adds ϕ to Ai , as done inMP.In large systems that consist of many partitions, the addition of so many constant andfunction symbols to each of the other partitions has the potential to be computationallyinefficient, leading to many unnecessary and irrelevant deduction steps. Arguably, a morecompelling way of addressing the problems associated with resolution for first-order the-ories is to infer the existential formula ∃b P (b, x) from P (B, x), send this formula to theproper partition and Skolemize it there. For example, if ϕ = P (f (g(B)), x) is the clausethat RES-SEND gets, replacing it with ∃b P (b, x) eliminates unnecessary work of thereceiving partition.The process of conservatively replacing function and constant symbols by existentiallyquantified variables is called unskolemization or reverse Skolemization and is discussedE. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8861PROCEDURE RES-SEND(ϕ, j , i)ϕ a formula, j, i (cid:1) n.(Implementation 3)(1) Unskolemize ϕ into a set of formulae, Φ in L(l(i, j )), treating every symbolof L(ϕ) \ l(i, j ) as a Skolem symbol.(2) Let Φ2 be the Skolemized version of Φ.(3) For every term t that was unskolemized in step 1, if t appeared in a previ-ously sent message, ψ , and s was t ’s Skolemization there, then replace theSkolemization of t in Φ2 with s.(4) For every ϕ2 ∈ Φ2, if ϕ2 is not subsumed by a clause that is in Ai , then addϕ2 to the set of axioms of Ai .Fig. 10. Subroutine RES-SEND using unskolemization.in [13,29], as well as [21]. Chadha and Plaisted in [21] present an algorithm U that iscomplete for our purposes and generalizes and simplifies an algorithm of [29].Theorem 3.1 [21]. Let V be a vocabulary and ϕ, ψ be formulae such that ψ ∈ L(V ) andϕ |= ψ. There exists F ∈ L(V ) that is generated by algorithm U such that F |= ψ.Thus, for every reasoning procedure that is complete for clausal consequence find-ing, unskolemizing ϕ using procedure U for V = l(i, j ) and then Skolemizing the resultgives us a combined procedure for message generation that is complete for FOL conse-quence finding. This procedure can then be used readily in RES-MP (or in MP), upholdingthe soundness and completeness to that supplied by Lemma 2.6. The subroutine RES-SEND(ϕ, j , i) (Implementation 3) that implements this approach is presented in Fig. 10.It replaces ϕ with a set of formulae in L(l(i, j )) that follows from ϕ. It then Skolemizesthe resulting formulae for inclusion in Ai . The procedure makes sure that terms and func-tions that appear in more than one message are replaced by the same Skolem constants andfunctions in all those instances (we discuss the reason for this at the end of this subsection).Procedure U may generate more than one formula for any given clause ϕ. Forexample, if ϕ = P (x, f (x), u, g(u)), for l(i, j ) = {P }, then we must generate both∀x∃y∀u∃vP (x, y, u, v) and ∀u∃v∀x∃yP (x, y, u, v) (ϕ entails both quantified formulae,and there is no one quantified formula that entails both of them). In our case we can avoidsome of these quantified formulae by replacing the unskolemize and then Skolemize processof RES-SEND (Implementation 3) with a procedure that produces a set of formulae directly(Implementation 4). It is presented in Fig. 11.Steps (3) and (4) of procedure RES-SEND(ϕ, j , i) (Implementation 4) correspond tosimilar steps in procedure U presented in [21], simplifying where appropriate for our setup.Our procedure differs from unskolemizing procedures in step (5), where it stops short ofreplacing the Skolem functions and constants with new, existentially quantified variables.Instead, it replaces them with new functions and constant symbols. The nondeterminismof step (4) is used to add all the possible combinations of unified terms, which is requiredto ensure completeness.62E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88PROCEDURE RES-SEND(ϕ, j , i)ϕ a formula, j, i (cid:1) n.(Implementation 4)(1) T is a static table (i.e., keeps its value between invocations of RES-SEND)that is initialized to ∅ when RES-MP is called.(2) Set S ← L(ϕ) \ l(i, j ) (S is the set of symbols of ϕ that we cannot send).(3) For every term instance, t = f (t1, . . . , tk), in ϕ, if f ∈ S and t is not asubexpression of another term t (cid:20) = f (cid:20)(t (cid:20)k(cid:20) ) of ϕ with f (cid:20) ∈ S, thenreplace t with “x ← t ” for some new variable, x (if k = 0, t is a constantsymbol).1, . . . , t (cid:20)(4) Nondeterministicallya, for every pair of marked arguments “x ← α”, “y ←β” in ϕ, if α, β are unifiable, then unify all occurrences of x, y (i.e., unifyαi , βi for all markings x ← αi , y ← βi ).(5) For every marked argument “x ← α” in ϕ,(a) Collect all marked arguments with the same variable on the left-handside of the “←” sign. Suppose these are x ← α1, . . . , x ← αl.(b) Let y1, . . . , yr be all the variables occurring in α1, . . . , αl. For everyi (cid:1) l, replace “x ← αi ” with f (y1, . . . , yr ) in ϕ, for a function symbolf (if r = 0, f is a constant symbol) such that• If αi appears in table T , then f is the symbols that is in the αi entryin T . Else, f is a fresh symbol; add the entry (cid:17)αi , f (cid:18) to T .(6) Add ϕ to Ai , if it is not subsumed by a clause in Ai .a Nondeterministically select the set of pairs for which to unify all occurrences of x, y.From here forth we continue with one such set of unifications. The end result is the set ofclauses that includes all these possibilities.Fig. 11. Subroutine RES-SEND without unskolemization.For example, if ϕ = P (f (g(B)), x) and l(i, j ) = {P }, then RES-SEND (Implementa-tion 4) adds P (A, x) to Ai , for a new constant symbol, A. If ϕ = P (x, f (x), u, g(u)), forl(i, j ) = {P }, then RES-SEND adds P (x, h1(x), u, h2(u)) to Ai , for new function symbolsh1, h2. Finally, if ϕ = P (x, f (x), u, f (g(u))), then RES-SEND adds P (x, f (x), u, h(u))and P (h1(u), h2(u), u, h2(u)) to Ai , for h, h1, h2 new function symbols.i(cid:1)nTheorem 3.2 (Soundness & completeness of RES-MP). Let A be a partitioned theory(cid:1)Ai of propositional or first-order clauses, G a tree that is properly labeled withrespect to A, and Q ∈ L(Ak), k (cid:1) n, be a sentence that is the query. A |= Q iff applyingRES-MP({Ai}i(cid:1)n, G, Q) (with Implementation 4 of RES-SEND) outputs YES.Proof. See Appendix A.4.In Implementations 3 and 4 of RES-SEND we carefully chose and repeated Skolemconstants in the sent clauses. The reason for this is the following. Craig’s interpolation the-orem guarantees that we need to send only a single sentence that includes only symbols thatare in the link language. However, this guaranteed sentence may include several clauses,E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8863and the Skolem constants that we need to apply to those must be the same. For example,consider the two partitions A1, A2 with A1 = {P (s), Q(s)} for a constant symbol s, andA2 = {¬Q(x) ∨ ¬P (x)}, for a variable x, and l(1, 2) = {P , Q}. In two separate messagesfrom A1 to A2 we need to send P (s) and Q(s). If we unskolemize and Skolemize P (s)into P (Skolem1) and Q(s) into Q(Skolem2), then we will not be able to reach the emptyclause in A2, a conclusion that does follow if we send P (Skolem1), Q(Skolem1).We suspect that the procedure that we outlined in Implementation 4 of RES-SEND fordeciding which symbols should be repeated can be significantly improved. Consider the ex-ample from the previous paragraph, with the change that A1 = {P (s), Q(s), P (r), Q(r)}.There are four messages that should be sent from A1 to A2, but in fact we have groundsto restrict ourselves and send only two messages, namely, the messages that contain s (orthe unskolemization and Skolemization of s). Since the only messages that mention r haveisomorphic messages that mention s, then the messages containing r can be dropped. Thisobservation promises to cut the number of constant symbols in a partition significantly,but its efficient and optimal application in the general case raises a set of graph-theoreticalproblems that are outside the scope of this article and we leave it for future work.3.2. Analysis and comparison of resolution-based inferenceIn this final subsection relating to resolution, we analyze the effect of MP on the com-putational efficiency of resolution-based inference, and identify some of the parameters ofinfluence. Current measures for comparing automated deduction strategies are insufficientfor our purposes. Proof length (e.g., [59,105,107]) (and see the survey article [26]) is onlymarginally relevant. More relevant is comparing the sizes of search spaces induced by dif-ferent strategies (e.g., resolution of propositional Horn clauses [84], and contraction rulesfor FOL [15]). These measures do not precisely address our needs, but we use them here,leaving the development of better measures for comparison to future work.In a resolution search space, each node in the search space includes a set of clauses, andproperties relevant to the utilized resolution strategy (e.g., clause parenthood information).Each arc in the search space is a resolution step allowed by the strategy. In contrast, inan MP resolution search space the nodes also include partition membership information.Further, each arc is a resolution step allowed by the utilized resolution strategy that satisfieseither of: (1) the two axioms are in the same partition, or (2) one of the axioms is inpartition Aj , the second axiom is drawn from its communication language l(i, j ), and thequery-based ordering allows the second axiom to be sent from Ai to Aj . Legal sequenceof resolutions correspond to paths in these spaces.(cid:1)Ai be a partitioned theory. Any path in the MP resolutionProposition 3.3. Let A =search space of {Ai}i(cid:1)n is also a path in the resolution search space of the unpartitionedtheory A.i(cid:1)nEvaluating MP with respect to proof length, it follows that the longest proof withoutusing MP is as long or longer than the longest MP proof. Unfortunately, the shortest MPproof may be longer than the shortest possible proof without MP. This observation can bequantified most easily in the simple case of only two partitions A1, A2. The set of messages64E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88that need to be sent from A1 to A2 to prove Q is exactly the interpolant γ promised byTheorem 2.3 for α = A1, β = A2 ⇒ Q. The MP proof has to prove α (cid:13) γ and γ (cid:13) β.For the propositional case there are several results relating shortest proofs and proofsusing the interpolant. Carbone [20] showed that, if γ is a minimal interpolant, then formany important cases the proof length of α (cid:13) γ together with the proof length of γ (cid:13) βis in O(k2) (for sequent calculus with cuts (with cuts (the case that is the most similar toresolution) the bound is k)), where k is the length of the minimal proof of α (cid:13) β. In someof these cases, the minimal interpolant is shown to be of size O(a2), where a is the sum oflengths of α, β.In general, the size of γ itself may be large. In fact, in the propositional case it is an openquestion whether or not the size of the smallest interpolant can be polynomially boundedby the size of the two formulae α, β. A positive answer to this question would implyan important consequence in complexity theory, namely that NP ∩ coNP ⊆ P/poly [17].Nevertheless, there is a good upper bound on the length of the interpolation formula as afunction of the length of the minimal proof [65]: If α, β share l symbols, and the resolutionproof of α (cid:13) β is of length k, then there is an interpolant γ of length min(klO(1), 2l).[86] presented an analysis of ordered resolution in the propositional case. They used theconcepts of induced width and treewidth to analyze the performance of this algorithm. Webring their analysis here as its results generalize to ours as well.(cid:1)Definition 3.4 [87]. A tree-decomposition of a graph G(V , E) is a pair D = (S, T ) withS = {Xi | i ∈ I } a collection of subsets of vertices of G and T = (I, F ) a tree, withone node for each subset of S, such that the following three conditions are satisfied:i∈I Xi = V . (2) For all edges (v, w) ∈ E there is a subset Xi ∈ S such that both v, w(1)are contained in Xi . (3) For each vertex x, the set of nodes {i | x ∈ Xi } forms a subtreeof T .The width of a tree-decomposition ({Xi | i ∈ I }, T = (I, F )) is maxi∈I (|Xi| − 1). Thetreewidth of a graph G equals the minimum width over all tree-decompositions of G. Everyordering on symbols induces a tree decomposition (we do not present details here; they canbe found in [63] and others). The width of that tree decomposition is sometimes called theinduced width of that ordering.In our context, the width of the decomposition A =Ai is the largest number(minus 1) of nonlogical symbols appearing in a single partition (including the symbols onits links to other partitions). The treewidth is the smallest width achievable for a theory(i.e., the best decomposition that is possible while still maintaining the proper-labelingproperty). The results of [86] show that ordered resolution cannot resolve more than 2knclause pairs when k is the width of the decomposition. The same result can be shown forany instantiation of MP for propositional theories.i(cid:1)n(cid:1)i(cid:1)nTheorem 3.5. Let A be a partitioned propositional theory with n, and let G = (V , E, l),(cid:1)Ai be a tree decomposition of width k of A. Then, the time taken by RES-MP(with any sound resolution strategy for in-partition computation) to compute SAT for Ais O(2kn). The space needed for this computation is O(2k).E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8865For the first-order case there are very few results. The best known is by Meyer [79] whoshowed that for the first-order predicate calculus with equality there is no recursive boundon the length of the smallest interpolant as a function of the length of the input axioms.However, there is no result relating the size of the interpolant with the length of the minimalproof (in resolution or any other proof system).The results above suggest that we can guarantee a small interpolant, if we make surethe communication language is minimal. Unfortunately, we do not always have controlover the communication language. Take, for example, the case of multiple KBs that haveextensive overlap. In such cases, the communication language between KBs may be large,possibly resulting in a large interpolant. In Section 5 we provide an algorithm for partition-ing theories that attempts to minimize the communication language between partitions.Finally, we bring a pair of results that appear in [6] for completion of our current ex-position. These results relate reasoning with partitioned theories and different orderingstrategies of resolution (look at [19,22,38] for more information on these strategies).Theorem 3.6 (MP simulates orderings). The following relationships hold between the MPalgorithm and the ordering strategies of directional resolution, A-ordering and lock reso-lution:(1) Let A be a propositional theory and (cid:1)A a total order on its n propositional symbols.Then, there is a partitioning {Ai }i(cid:1)n of A, a graph G and partition reasoners that arebased on ordered resolution such that running MP does not perform more resolutionsthan directional resolution (alternatively, A-ordering) of A with order (cid:1)A.(2) Let A be a FOL theory and (cid:1)A a total order on its n predicate symbols. Then, thereis a partitioning of A into {Ai }i(cid:1)n, a graph G and partition reasoners that are basedon ordered resolution such that running MP does not perform more resolutions thanA-ordered resolution of A with order (cid:1)A.(3) Let A be a FOL theory and I an indexing of its literal instances. Let n = maxlliteral I (l).Assume that I (l1) = I (l2) if l1, l2 have the same predicate symbol. Then, there is apartitioning A =Ai and partition reasoners that are generation-set complete,such that running MP does not perform more resolutions than lock resolution of Awith index I .i(cid:1)n(cid:1)Theorem 3.7 (Orders simulate MP). The following relationships hold between the MPalgorithm and the ordering-based resolution strategies of directional resolution and lockresolution:(cid:1)(1) Let A =Ai be a partitioned propositional theory and G(V , E, l) be a tree thati(cid:1)nis properly labeled for A.Then, there is a total order, (cid:1)A, on A’s propositional symbols such that if a clause C isa consequence of directional resolution of A with order (cid:1)A, then C is a consequenceof running MP on this partitioning using unrestricted resolution in each partition.(2) Let A =Ai be a partitioned FOL theory and G(V , E, l) be a tree that is prop-(cid:1)i(cid:1)nerly labeled for A.66E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88(3) Let A =(cid:1)Then, there is a total order, (cid:1)A, on A’s predicate symbols such that if a clause C is aconsequence of A-ordered resolution of A with order (cid:1)A, then C is a consequence ofrunning MP on this partitioning using unrestricted resolution in each partition.Ai be a partitioned propositional theory and G(V , E) be a tree that isi(cid:1)nproperly labeled for A.Then, there is an index, I , on A’s literal instances such that if a clause C is a conse-quence of lock resolution of A with index I , then C is a consequence of running MPon this partitioning using unrestricted resolution in each partition.4. Partition-based propositional satisfiabilityIn this section we present an algorithm for partition-based logical reasoning that takesadvantage of propositional satisfiability (SAT) search subroutines (e.g., DPLL [34], GSAT[92] and WALKSAT [91]).This algorithm is very similar to the algorithm of [36,37] for constraint satisfactionproblems and we contrast it here with our MP algorithms. We also bring a correctnessproof that follows from our soundness and completeness proof for MP. The algorithm alsoallows us to examine the complexity of computation and show that here too the complexityis directly related to the size of the labels in the intersection graph, i.e., the width and linksize of the graph.4.1. A partition-based SAT procedureThe algorithm we propose is presented in Fig. 12. It uses a SAT procedure as a sub-routine and is backtrack-free. We describe the algorithm using database notation [106].πp1,...,pk T is the projection operation on a relation T . It produces a relation that includesall the rows of T , but only the columns named p1, . . . , pk (suppressing duplicate rows).S (cid:1) R is the natural join operation on the relations S and R. It produces the cross prod-uct of S, R, selecting only those entries that are equal between identically named fieldsPROCEDURE LINEAR-PART-SAT({Ai}i(cid:1)n){Ai }i(cid:1)n a partitioning of the theory A.(1) G0 ← the intersection graph of {Ai }i(cid:1)n. G ← BREAK-CYCLES(G0).(2) For each i (cid:1) n, let L(i) =(i,j )∈E l(i, j ).(3) For each i (cid:1) n, for every truth assignment A to L(i), perform SAT-search(cid:1)on Ai ∪ A, storing the result in a table Ti (A).(4) Determine ≺ as in Definition 2.1.(5) Iterate over i (cid:1) n in reverse ≺-order (the last i is 1). For each j (cid:1) n thatsatisfies (i, j ) ∈ E and i ≺ j , perform:• Ti ← Ti (cid:1) (πL(i)Tj ) (Join Ti with those columns of Tj that correspondto L(i)). If Ti = ∅, return FALSE.(6) If FALSE has not be returned, return TRUE.Fig. 12. An algorithm for SAT of a partitioned propositional theory.E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8867(checking S.A = R.A), and discarding those columns that are now duplicated (e.g., R.Awill be discarded).The proposed algorithm shares some intuition with prime-implicate generation (e.g.,[61,74]). Step (1) of the algorithm converts the intersection graph of A into a tree. Step (2)computes L(i), the set of symbols on all of partition Ai ’s links, i.e., the union of allthe communication languages connected to partition Ai . Step (3) determines which truthvalues of L(i) are satisfiable (akin to computing the implicates of each partition in the lan-guage L(L(i)). Finally, the algorithm uses (cid:1) to combine those values to find out if thereare any models for A.This algorithm resembles finding all the models of each partition and then joining theconsistent interpretation fragments into models for A (as done in [37]). The iterated jointhat we perform takes time proportional to the size of the tables involved. Furthermore, wekeep table sizes below 2|L(i)| by keeping only the consistent truth assignments for L(i) andprojecting every table before joining it with another table. This is similar to an approachthat was presented in [36,94] that trades space for time in CSPs and Bayes Network. Thecomputation is done via search in each partition, yielding a method that takes time expo-nential in the partition size and space exponential in the separator (label) size.Fig. 13(a) displays the result of applying LINEAR-PART-SAT up to step (3) to thepartitioned theory and input of Fig. 4. Fig. 13(b) and 13(c) show the progression of step (5)of LINEAR-PART-SAT.Soundness and completeness follow by an argument similar to that given for MP.Theorem 4.1 (Soundness and completeness). Given a sound and complete SAT-searchprocedure, LINEAR-PART-SAT is sound and complete for SAT of partitioned propositionaltheories.Fig. 13. Iteratively projecting and joining tables to check satisfiability.68E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88Proof. See Appendix A.5.4.2. Analyzing satisfiability in LINEAR-PART-SATLet A be a partitioned propositional theory with n partitions. Let m = |L(A)|, L(i)be the set of propositional symbols calculated in step (2) of LINEAR-PART-SAT, andmi = |L(Ai ) \ L(i)| (i (cid:1) n). Let a = |A| and k be the length of each axiom.Lemma 4.2. The time taken by LINEAR-PART-SAT to compute SAT for A is(cid:3)Timen, m, m1, . . . , mn, a, k,(cid:6)(cid:4)(cid:4), . . . ,(cid:4)(cid:4)L(n)(cid:4)(cid:4)(cid:5)= Oa · k2 + n4 · m +|L(i)| · fSAT (mi)(cid:4)(cid:4)L(1)(cid:3)n(cid:7)2i=1(cid:8)(cid:5),where fSAT is the time to compute SAT. If the intersection graph G0 is a tree, the secondargument in the summation can be reduced from n4 · m to n · m.Proof. See Appendix A.6.Corollary 4.3. Let A be a partitioned propositional theory with n partitions, m proposi-tional symbols and intersection graph G = (V , E, l). Let d(v) be the degree of node v inthe graph G(V , E, l), let d = maxv∈V d(v) and let l = maxi,j (cid:1)n |l(i, j )|. Assume P (cid:3)= NP.If intersection graph G of A is a tree and all the partitions Ai have the same number ofpropositional symbols, then the time taken by the LINEAR-PART-SAT procedure to com-pute SAT for A isTime(m, n, l, d) = O(cid:3)n · 2d·l · fSAT(m/n)(cid:5).The space taken for this computation is O(2d·l).For example, if we partition a given theory A into only two partitions (n = 2) sharing lpropositional symbols, the algorithm will take time O(2l · fSAT (m/2)). Assuming P (cid:3)= NP,this is a significant improvement over a simple SAT procedure for every l that is smallenough (l < αm/2, and α (cid:1) 0.582 [26,89]).Corollary 4.4. Let A be a partitioned propositional theory with n partitions, m propo-sitional symbols and intersection graph G = (V , E, l) of width k. Then, the time takenby the LINEAR-PART-SAT procedure to compute SAT for A is O(2kn). Taking ld =maxi(cid:1)n |j (cid:1)n l(i, j )|, the space needed for this computation is O(2ld ).(cid:1)E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88695. Decomposing a logical theoryThe algorithms presented in previous sections assumed a given partitioning of theory A.In this section we address the critical problem of automatically decomposing a set of propo-sitional or FOL clauses into a partitioned theory. Guided by the results of previous sections,we propose guidelines for achieving a good partitioning and present a greedy algorithm thatdecomposes a theory following these guidelines.5.1. What is a good partitioning?The analysis done in Section 4.2 does not assume any particular time complexity forfSAT(m) (aside from P (cid:3)= NP in the corollary). If we assume that fSAT(m) = (cid:9)(2m), thenwe can conclude that the time for our reasoning algorithm is dominated by the largestpartition (including its links). If the largest partition is of size s (i.e., it has s proposi-tional symbols in its language, link languages included), then the time for the algorithm isO(n · 2s).This analysis is exactly the one that is done for CSPs and Bayes networks (e.g., [37,38,86], and [11]), where the utilized algorithms do in fact use time (cid:9)(2m) for a problem withm variables. For satisfiability the situation is slightly different. There are known stochasticalgorithms (e.g., [91,92]) that perform much better than this pessimistic forecast. Thesealgorithms are not complete, but they can be used in our algorithm, if we are willing to giveup completeness. Efficient complete algorithms also typically exhibit better than worst-case behavior (see the analysis of [93]).In the general FOL case we have no clear-cut measure of computational complexity,but the results for propositional logic suggest similar relationship between partitioningand computational behavior. All of this suggests that emphasizing link sizes together withpartition sizes is more accurate for the satisfiability problem.Thus, given a theory, we wish to find a partitioning that minimizes the formula derived inLemma 4.2. To that end, assuming P (cid:3)= NP, we want to minimize the following parametersin roughly the following order. For all i (cid:1) n:(1) |L(i)|—the total number of symbols contained in all links to/from node i. If G0 isalready a tree, this is the number of symbols shared between the partition Ai and therest of the theory A \ Ai .(2) mi —the number of symbols in a partition, less those in the links, i.e., in Ai but notin L(i). This number is mostly influenced by the size of the original partition Ai ,which in turn is influenced by the number of partitions of A, namely, n. Having morepartitions will cause mi to become smaller.(3) n—the number of partitions.Also, a simple analysis shows that given fixed values for l, d in Corollary 4.3, the maximaln that maintains l, d such that also n (cid:1) ln 2 · α · m (α = 0.582 [26,89]) yields an optimalbound for LINEAR-PART-SAT. In Section 3.2 we saw that the same parameters influencethe number of derivations we can perform in MP: |L(i)| influences the interpolant size andthus the proof length, and mi influences the number of deductions/resolutions we can per-70E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88form. Thus, we would like to minimize the number of symbols shared between partitionsand the number of symbols in each partition less those in the links.The question is, how often do we get large n (many partitions), small mi ’s (small par-titions) and small |L(i)|’s (weak interactions) in practice. We believe that in domains thatdeal with engineered physical systems, many of the domain axiomatizations have thesestructural properties. Indeed, design of engineering artifacts encourages modularization,with minimal interconnectivity (see [3,24,69]). More generally, we believe axiomatizersof large corpora of real-world knowledge tend to try to provide structured representationsfollowing some of these principles. Recent experiments with the HPKB knowledge baseof SRI and a part of the Cyc knowledge base support this belief (those experiments arereported elsewhere).5.2. An approach to partitioning logical theoriesTo exploit the partitioning guidelines proposed in Section 5.1, we represent our theoryA using a symbols graph that captures the features we wish to minimize. G = (V , E) is asymbols graph for theory A such that each vertex v ∈ V is a symbol in L(A), and there isan edge between two vertices if their associated symbols occur in the same axiom of A,i.e., E = {(a, b) | ∃α ∈ A s.t. a, b appear in α}.Fig. 14 (top) illustrates the symbols graph of theory A from Fig. 1 and the connectedsymbols graphs (bottom) of the individual partitions A1, A2, A3. Notice that each axiomcreates a clique among its constituent symbols. To minimize the number of symbols sharedbetween partitions (i.e., |L(i)|), we must find partitions whose symbols have minimal ver-tex separators in the symbols graph.Fig. 14. Decomposing A’s symbols graph.E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8871Generally speaking, we decompose theory A by first creating the symbols graph of A,then partition this graph into partitions (similar to the bottom part of Fig. 1), and finallyuse the partitioning of the graph to define a partitioning of the axioms. The first part ofFig. 16 (Procedure Split-Thy) is a generic algorithm that does just that. Gstr is manipulatedby the subroutine of this procedure such that at the end Gstr is a tree decomposition (seeDefinition 3.4) of G. Section 5.3 describes one complete instantiation of this algorithm forthis task. Examples of the recursive procedure that we are going to use are presented inFig. 17.The relationship between computing minimum vertex separators and computing treedecompositions is well known and has been studied in the literature for some time (e.g.,[7]). Here we present a particular greedy algorithm for partitioning propositional and first-order logical theories that is also based on the computation of minimum vertex separators.In more recent work, we use this algorithm as one of two algorithms for evaluating theperformance of our partition-based logical reasoning work [73].5.3. Split: greedy vertex min-cut in the graph of symbols5.3.1. Minimum vertex separatorsIn this section, we briefly describe the notion of a vertex separator. Let G = (V , E) be anundirected graph. A set S of vertices is called an (a, b)-vertex-separator if {a, b} ⊂ V \ Sand every path connecting a and b in G passes through at least one vertex contained in S.Let N(a, b) be the least cardinality of an (a, b)-vertex-separator. The connectivity ofthe graph G is the minimum N(a, b) for any a, b ∈ V that are not connected by an edge.An (a, b)-vertex-separator of minimum cardinality is said to be a minimum (a, b)-vertex-separator. The weaker property of a vertex separator being minimal requires that no subsetof the (a, b)-vertex-separator is an (a, b)-vertex-separator.We briefly review an algorithm by Even and Tarjan for finding minimum vertex separa-tors [48,49]. This algorithm builds on [32]. It is shown in Fig. 15. The algorithm is giventwo vertices, a, b, and an undirected graph, G. It transforms G into a directed graph, (cid:24)G,that has two vertices (corresponding to input and output) for each original vertex of G,directed edges connecting the corresponding input and output vertices, and edges corre-sponding to those of G, but only from output to input vertices. It then runs a max-flowalgorithm on (cid:24)G (steps (1)–(3)). The produced flow, f , has a throughput of N(a, b). Toextract a minimum separator, it produces a layered network (see [48, p. 97]) from (cid:24)G andthe flow found, f , in step (5). The layered network includes a subset of the vertices of (cid:24)G.The set of edges between this set of vertices and the rest of (cid:24)G corresponds to the separator.Algorithms for finding maximal flow are abundant in the graph algorithms literature.Prominent algorithms for max-flow include the Simplex method, Ford and Fulkerson’s[53], the push-relabel method of Goldberg and Tarjan [57] (time bound of O(|V | · |E| ·log (|V |2/|E|)) and several implementations [23]), and Dinitz’s algorithm [44]. WhenDinitz’s algorithm is used to solve the network problem, Even and Tarjan’s algorithm hastime complexity O(|V |1/2|E|) [48]. The unit-capacity network-flow algorithm of [1] canalso be used here, giving Even and Tarjan’s algorithm time complexity of O(|V |1/2|E|) aswell.72E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88PROCEDURE MIN-V-SEP-A-B(G = (V , E), a, b)(1) Construct a digraph (cid:24)G((cid:24)V , (cid:24)E) as follows. For every v ∈ V put two vertices−−−−−→(v(cid:20), v(cid:20)(cid:20)) (internal−−−−−→(u(cid:20)(cid:20), v(cid:20)) and(input/output vertices) in (cid:24)V with an edge ev =v(cid:20), v(cid:20)(cid:20)edge). For every edge e = (u, v) in G, put two edges e(cid:20) =−−−−−→(cid:20)(cid:20)(cid:20) =, u(ve) in (cid:24)G (external edges).(cid:20)(cid:20)(2) Define a network, with digraph (cid:24)G, source a(cid:20)(cid:20), sink b(cid:20)and unit capacitiesfor all the edges.(3) Compute the maximum flow f in the network.(4) Set the capacities of all the external edges in (cid:24)G to infinity.(5) Construct the layered network {Vi}i(cid:1)l from (cid:24)G using f . Let S =i(cid:1)l Vi .(6) Let R = {v ∈ V | v(cid:20) ∈ S, v(cid:20)(cid:20) /∈ S}. R is a minimum (a, b) vertex-separator(cid:1)in G.Fig. 15. An algorithm for finding a minimum separator between a and b in G.Another possibility is to use the Ford–Fulkerson flow algorithm as described in [53](alternatively, see [27]), for computing maximum flow. For an original graph of tree-width2< k this involves finding at most k augmenting paths of capacity 1. Thus, the combinedalgorithm using the Ford–Fulkerson maximum flow algorithm finds a minimum (a, b)-vertex-separator in time O(k(|V | + |E|)).Finally, to compute the vertex connectivity of a graph and a minimum separator, with-out being given a pair (a, b), we check the connectivity of any c vertices (c being theconnectivity of the graph) to all other vertices. When Dinitz’s algorithm is used as above,this procedure takes time O(c · |V |3/2 · |E|), where c (cid:2) 1 is the connectivity of G. Whenwe use Ford–Fulkerson’s algorithm for a graph of tree-width k, this procedure takes timeO(c · k · |V | · (|V | + |E|)), where c (cid:2) 1 is the connectivity of G. For the cases of c = 0, 1there are well-known linear time algorithms. [47] also showed a way to test for k connec-tivity of a graph using only n + k2 pairs of vertices.5.3.2. Procedure SplitProcedure Split-Thy, presented in Fig. 16, uses procedure Split to decompose a theoryinto a tree of partitions. It is given a theory, A, and limitations on the partition size (alower limit, M) and the separators between partitions (an upper limit, l). Split initiallyconsiders the theory as one big partition, and at every recursive iteration it breaks one ofthe partitions in two. It represents the tree structure of the partitions in a global variable,Gstr. This tree structure and the set of partitions, {Ai}i(cid:1)p, is returned as the result of Split-Thy. An example of the input and the output is shown in Fig. 1.Split partitions the theory A by taking as input its symbols graph, G = (V , E), the twolimiting parameters, M and l, and nodes a, b ∈ V that are initially set to nil. Split updatesthe global variable Gstr to represent the progressing decomposition. In each recursive call,Split finds a minimum vertex separator of a, b in G (i.e., a minimum-size set of vertices2 The tree-width of a graph plus one is the minimum, over all triangulations of this graph, of the size of thelargest clique in the triangulation (see [63]).E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8873PROCEDURE Split-Thy(A, M, l)A is a theory. M limits the number of symbols in a partition from below. l limitsthe number of symbols shared between partitions.(1) Let G(V , E) be an undirected graph with V = L(A) and E = {(l1, l2) |∃C ∈ A l1, l2 ∈ L(C)}.(2) Let Gstr(Vstr, Estr) be an undirected graph with Vstr = {{V }} and Estr = ∅.(3) Run Split(G, M, l, nil, nil).(4) For every v ∈ Vstr, let Av = {C ∈ A | L(C) ⊂ v}. Return {Av}v∈Vstr andGstr.PROCEDURE Split(G, M, l, a, b)G = (V , E) is an undirected graph. M, l as above. a, b are in V or are nil.(1) If |V | < M, then return V .(2) (a) If a, b = nil, find a minimum vertex separator, R, in G. (b) Otherwise,if b = nil, find a minimum vertex separator, R, of a in G. (c) Otherwise,find minimum vertex separators, Ra of a in G, and Rb of b in G. Let R bethe smaller of Ra, Rb.(3) If R = V or |R| > l then return V .(4) Let G1, G2 be the two subgraphs of G separated by R, with R included inboth subgraphs.(5) Let Vstr ← Vstr \ {{V }} ∪ {{V1}, {V2}} and Estr ← Estr ∪ {({V1}, {V2})}.Change the edges that connected to {V } to connect to one of {V1}, {V2}.(cid:20)(cid:20)2 from G1, G2, respectively, by aggregating the vertices in R1, G(6) Create Ginto a single vertex r, removing all self edges and connecting r with edgesto all the vertices connected by edges to some vertices in R.(7) Run Split2(G(cid:20)1, M, l, r, a), Split2(G(cid:20)Vstr by the members of R.2, M, l, r, b). Replace r in the nodes ofFig. 16. An algorithm for generating partitions of axioms.that crosses every path between a, b). If one of a, b or both are nil, it finds the overallminimum vertex separator between all vertices and the non-nil vertex (or all other vertices).This separator splits G into two graphs, G1, G2, and the process continues recursively. Anexample of the progress made on the input graph G is shown in Fig. 17.Different variants of the algorithm yield different structures for the intersection graphof the resulting partitioning. As is, Split returns sets of symbols that result in a chain ofpartitions. We obtain arbitrary trees, if we change step 3(c) to find a minimum separatorthat does not include a, b (not required to separate a, b). We obtain arbitrary graphs, if inaddition we do not aggregate R into r in step 6.Proposition 5.1. Procedure Split takes time O(|V |5/2 · |E|).Proof. See Appendix A.7.74E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88Fig. 17. Recursive use of Split by aggregating minimal separators into single nodes. Only the larger side of theleftover graph is shown after each split.E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88755.3.3. Fine-tuning SplitSince fSAT(m) is not known, and the time for reasoning with FOL theories in MP isnot bounded, it is not clear what is an optimal decomposition. Nevertheless, the analy-ses done throughout this paper suggests minimizing the parameters mentioned in the lastsection. If we assume that fSAT(m) = (cid:9)(2αm), then the problem of finding an optimal par-tition for LINEAR-PART-SAT is equivalent to finding triangulations of minimum cliquenumber (a.k.a. finding treewidth). With this assumption, M should be chosen to be 1, lshould be chosen to be m, and the algorithm will stop the recursive decomposition onlywhen reaching a graph that is a clique. This is justified by the observation that any furtherdecomposition can only decrease the size of the maximum partition (including the links).Thus, Assuming fSAT(m) = (cid:9)(2αm), further decompositions only decrease the asymptotictime function of LINEAR-PART-SAT.For reasoning with FOL theories it may sometimes be useful to choose M (the limiton the number of symbols in a partition) to be large, so that sentences are aggregatedmore closely to topics. This can be useful for managing large axiom sets as well as forapplying specialized reasoning algorithms for each partition. This can be combined withreplacing our vertex separator algorithm with a balanced separator algorithm. A balancedseparator is a vertex separator that separates the graph such that the separated subgraphsare of comparable sizes (typically, they are chosen to be no larger than a constant timesthe size of the original graph). The problem of finding balanced separators is NP-hard, butseveral approximations exist (e.g., [50,62,68]).Our time bound for Split is lower than (cid:9)(2αm) when l (cid:1) (αm − αmi − lg n)/d (i =argmaxj mj ). In particular, if l > m/2, a standard deterministic SAT procedure will bebetter (compared to the best time bound known for SAT procedures [26,89]).All the observations above are predicated on the assumption the A is propositional andthat fSAT(n) = O(2αn), for some α > 0 constant. If our theory is in FOL, or we drop theassumption on fSAT , then there is no clear good way to choose M, l. In those circumstances,l and M are perhaps best determined experimentally.5.3.4. Other decomposition approachesThere are many possible approaches to decomposing a set of logical axioms. Onecomplementary approach that we have briefly experimented with is a normalized cut al-gorithm [95], using the dual graph of the theory. The dual graph represents each axiom,rather than each symbol, as a node in the graph to be split. The possible advantage of thisapproach is that it preserves the distinction of an axiom. Also, since the min-cut algorithmis normalized, it helps preclude the creation of small isolated partitions by both maximizingthe similarity within partitions and minimizing the similarity between partitions.A different decomposition is conceived from a semantic approach. Our reasoning algo-rithms and our computational analysis suggested a syntactic approach to decomposition.Semantic approaches are also possible along lines similar to [100] or to [22] (chapter onsemantic resolution). Such decomposition approaches may require different reasoning al-gorithms to be computationally useful.76E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–886. Related workThe work related to ours is vast. We divide it into three parts. First is the work onautomated decomposition; second is the use of decompositions for propositional reasoning;and third is the work that relates to FOL theorem proving.6.1. Automated decompositionDecomposition techniques for CSPs, Bayes nets and other NP-hard problems are mostrelevant to our work on automated decomposition. These typically look for a separationvertex [35], use various heuristics to order symbols (an ordering that translates to a de-composition of the graph) [37,38,90], and use approximations for tree decomposition ofminimum width (equivalent to finding triangulations of minimum clique number, comput-ing treewidth, and finding optimal clique trees) [4,11,63,87,96].The last approach is applicable to our setup, if we assume that fSAT(m) = (cid:9)(2αm).In contrast to our SPLIT, these algorithms find weak approximations (factor O(log OPT))to the optimal in polynomial time and constant-factor approximations or optimal results inquasi-polynomial time (polynomial time, assuming the treewidth is bounded by a constant,and exponential time otherwise). Furthermore, work on implementing SAT and automateddeduction strongly suggests that the assumption of fSAT(m) = (cid:9)(2αm) is overly pes-simistic. For this reason we prefer to minimize the links first, and then look at minimizingthe partitions, leading to our proposed algorithm.Cut-set conditioning (e.g., [9,10,35,83]) and hypertree decompositions of CSPs (suchas the work of [58]) are other methods for using decompositions, that are fairly differentfrom the one we use in this paper.6.2. Use of decompositions in propositional SATWith respect to propositional SAT problems, perhaps the most relevant previous workis that of Dechter and Pearl [37], which presented algorithms for reasoning with decom-posed CSPs. These can be used for SAT under a given decomposition. In comparison, thealgorithm we presented for partition-based SAT does not produce all the models possiblein each partition, as proposed in [37]. Instead, it finds the truth values for propositions onthe links that are extendible to a satisfying truth assignment for the whole partition. Thisreduces our computation time and makes it more dependent on the links’ sizes rather thanon partition sizes.Other SAT uses of SAT decomposition include [82] which proposed a decompositionprocedure that represents the theory as a hypergraph of clauses and divides the proposi-tional theory into two partitions (heuristically minimizing the number of hyperedges). Itfinds the set of possible truth-value assignments to the propositions associated with thehyper-edges and tests them recursively for the two partitions. Cowen and Wyatt [28] devel-oped an algorithm that partitions a propositional CNF theory into connected componentsthat can be tested for satisfiability individually. Their partitioning algorithm is an adap-tation of a best first search as used to find components in a graph, or strongly connectedcomponents in a digraph. The authors tested their decomposition algorithm together withE. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8877a SAT solver, demonstrating a dramatic decrease in the runtime of the SAT solver on thedecomposed theories described in the paper.Prior to [5] there has been no work on using decompositions for automated deduc-tion in propositional logic in the manner we propose. Concurrently to this work, Rish andDechter [86] proposed an algorithm similar to our MP algorithm for the case of proposi-tional ordered resolution. However, their work looks at a limited case (ordered resolution,propositional logic), and they allow excessive computation by performing all possible res-olutions in each partition, twice. Our MP algorithm can be used with different reasoningprocedures for every partition, maintaining completeness as long as those algorithms sat-isfy some natural conditions. It is opportunistic in the sense that it does not wait for eachpartition to perform all of the possible resolutions. (In FOL waiting is not even a possibil-ity.) Thus, Rish and Dechter’s algorithm may use exponential amounts of space and timeover and above MP in the same settings. Also, MP can be considered a generalization oftheirs in the propositional case.6.3. Use of decompositions in FOL theorem proversSurprisingly, there has been little work on the specific problem of exploiting structure intheorem proving in the manner we propose in this paper. We conjecture that this can largelybe attributed to the fact that theorem proving has traditionally examined mathematics do-mains, that do not necessarily have structure that supports decomposition. Nevertheless,there is related work both in the parallel theorem proving community, and in the work oncombining logical systems.The majority of work on parallel theorem proving implementations followed decompo-sition of the search space [15,25,28,46,102,103], or allowed messages to be sent betweenthe different provers working in parallel, using heuristics to decide on what messages arerelevant to each prover [40,41,43] (surveys can be found in [14,42]). Both approachestypically look at decompositions into very few sub-problems (typically less than ten). Inaddition, the first approach typically requires complete independence of the sub-spaces orthe search is repeated on much of the space by several reasoners. The second approachis more similar to ours, but there are some major differences still. First, there is no clearmethodology for deciding what messages should be sent from one partition to another, orwhich partitions should receive messages from which other partitions. Second, there areno clear criteria for decomposing a theory into sub-problems.Another related line of work focuses on combining logical systems, including the workof [8,81,85,97,104]. Here, the computational focus has been on treating combinations ofsignature-disjoint theories (allowing the queries to include symbols from all signatures),e.g., [8]. Recent work introduced sharing function symbols between two theories (e.g.,[85]), but no algorithm allowed any sharing of relation symbols. All approaches eithernondeterministically instantiate the (newly created) variables connecting the theories (e.g.,[104]), or restrict the theories to be convex (disjunctions are intuitionistic) and have infor-mation flowing back and forth between the theories. In contrast, we focus on the structureof interactions between theories with signatures that share symbols and the efficiency ofreasoning with consequence finders, theorem provers and SAT procedures. We do not haveany restrictions on the language besides finiteness.78E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88Finally, work on formalizing and reasoning with context, including the work of [2,76]),can be related to partition-based logical reasoning by viewing the contextual theories asinteracting sets of theories. Unfortunately, to introduce explicit contexts, a language that ismore expressive than FOL is needed. Consequently, a number of researchers have focusedon context for propositional logic, while much of the work on reasoning has focused onproof checking. Examples include GETFOL [55,56]. Automated reasoning has few suc-cesses; [16] presents one example.7. ConclusionsIn this paper we have shown that decomposing theories into partitions and reasoningover those partitions has potential computational advantages for theorem provers and SATsolvers. Theorem proving strategies, such as resolution, can use such decompositions toconstrain search. Partition-based reasoning will improve the efficiency of propositionalSAT solvers if the theory is decomposable into partitions that share only small numbers ofsymbols.We have provided sound and complete algorithms for reasoning with partitions ofrelated logical axioms, both in propositional logic and in FOL. Different reasoning algo-rithms can be plugged-in for different partitions in these algorithms. We gave conditions onthose reasoners that ensure that the combined reasoning procedure is sound and complete.Specialized versions of these algorithms for resolution strategies in FOL were provided.We showed that some of these algorithms simulate some order-based resolution strategies,while order-based strategies may simulate some of our algorithms in restricted cases. Allour reasoning algorithms are suited for parallel and distributed processing.We examined the efficiency of our theorem-proving algorithms and our SAT algorithm.Guided by both analyses, we suggested guidelines for achieving a good partitioning andproposed an algorithm for the automatic decomposition of theories that tries to minimizeidentified parameters.Our work was motivated in part by the problem of reasoning with large multiple KBsthat have overlap in content. The results in this paper address some of the theoretical prin-ciples that underly such partition-based reasoning. More recently, we have integrated ourpartition-based reasoning algorithms into SRI’s SNARK theorem prover and tested the ef-fectiveness of our automated partitioning reasoning algorithms. Our experimental results[73] indicate that decomposing FOL theories automatically and using MP with resolutionto answer queries reduces the number of resolution steps significantly, sometimes by ordersof magnitude.AcknowledgementsWe particularly wish to thank Rina Dechter for many helpful and clarifying commentson a draft of this paper. We also wish to thank Mike Genesereth, John McCarthy, LeoraMorgenstern, Nils Nilsson, and Mark Stickel for their comments on this work and for inter-esting discussions on general topics related to this work. Finally, we thank Rada Chirkova,E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8879Tom Costello and Jörg Denzinger for comments on earlier papers describing this work.This research was supported in part by DARPA grant N66001-97-C-8554-P00004 and byAFOSR grant AF F49620-97-1-0207.Appendix A. ProofsA.1. FORWARD-M-P (MP) is sound and completeFirst, notice that soundness is immediate because the only rules used in deriving conse-quences are those used in our chosen consequence-finding procedure (of which rules aresound). In all that follows, we assume A is finite. The infinite case follows by the compact-ness of FOL.Lemma A.1. Let A = A1 ∪ A2 be a partitioned theory. Let ϕ ∈ L(A2). If A (cid:13) ϕ, thenA2 (cid:13) ϕ or there is a sentence ψ ∈ L(A1) ∩ L(A2) such that A1 (cid:13) ψ and A2 (cid:13) ψ ⇒ ϕ.Proof of Lemma A.1. We use Craig’s interpolation theorem (Theorem 2.3), taking α =A1 and β = A2 ⇒ ϕ. Since α (cid:13) β (by the deduction theorem for FOL), there is a formulaψ ∈ L(α) ∩ L(β) such that α (cid:13) ψ and ψ (cid:13) β. By the deduction theorem for FOL, we getthat A1 (cid:13) ψ and ψ ∧ A2 (cid:13) ϕ. Since ψ ∈ L(A1) ∩ L(A2) by the way we constructed α, β,we are done. (cid:2)Definition A.2 (Definition 2.5). For a partitioning A =Ai , we say that a tree G =(V , E, l) has a proper labeling, if for all (i, j ) ∈ E and B1, B2 the two subtheories of A onthe two sides of the edge (i, j ) in G, it is true that L(l(i, j )) ⊇ L(B1) ∩ L(B2).i(cid:1)n(cid:1)We will show that all intersection graphs have a proper labeling. First, the followinglemma provides the main argument behind all of the completeness proofs in this paper.(cid:1)Lemma A.3 (Lemma 2.6). Let A =Ai be a partitioned theory and assume that thegraph G is a tree that has a proper labeling for the partitioning {Ai }i(cid:1)n. Let k (cid:1) n and letQ ∈ L(Ak ∪(k,i)∈E l(k, i)) be a sentence. If A |= Q, then MP will find a consequence ofAk that subsumes Q.i(cid:1)n(cid:1)Proof of Lemma 2.6. We prove the lemma by induction on the number of partitions in thelogical theory. For |V | = 1 (a single partition), A = A1 and the proof is immediate, as thereasoning procedure for A1 is complete for consequence finding. Assume that we provedthe lemma for |V | (cid:1) n − 1 and we prove the lemma for |V | = n.In G, k has c neighbors, i1, . . . , ic. (k, i1) separates two parts of the tree G: G1 (includesi1) and G2 (includes k). Let B1, B2 be the subtheories of A that correspond to G1, G2,respectively.Notice that Q ∈ L(B2). By Lemma A.1, either B2 (cid:13) Q or there is ψ ∈ L(B1) ∩ L(B2)such that B1 (cid:13) ψ and B2 (cid:13) ψ ⇒ Q. If B2 (cid:13) Q, then we are done, by the induction hy-pothesis applied to the partitioning {Ai | i ∈ V2} (V2 includes the vertices of G2) and G2(notice that ≺(cid:20) used for G2, Q agrees with ≺ used for G).80E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88(cid:1)∪(cid:1)Otherwise, let ψ be a sentence as above.(i1,j )∈E,j (cid:3)=k l(i1, j ) ⊇ L(B2 ∪ Ai1 ) ∩ L(B1 \Ai1 ) because the set of edges (i1, j ) separates two subgraphs corresponding to the theoriesB1 \ Ai1 and B2 ∪ Ai1 , and G has a proper labeling our partitioning. Thus, since ψ ∈ L(B1)we get that ψ ∈ L(Ai1(i1,j )∈E,j (cid:3)=k l(i1, j )). By the induction hypothesis for G1, B1,at some point a sentence ψ (cid:20) that subsumes ψ will be proved in Ai1 (after some formulaewere sent to it from the other partitions in G1, B1).At this point, our algorithm will send ψ (cid:20) to Ak because ψ (cid:20) ∈ L(l(k, i1)) because G hasa proper labeling for A, G. Since B2 (cid:13) ψ (cid:20) ⇒ Q, then by the induction hypothesis appliedto G2, B2 (ψ (cid:20) ⇒ Q ∈ L(Ak ∪(k,i)∈E l(k, i))) at some point a sentence subsuming ψ (cid:20) ⇒ Qwill be generated in Ak (after some message passing). Thus, at some point a sentencesubsuming Q will be generated in Ak. (cid:2)Proof of Theorem 2.4. All we are left to prove is that the intersection graph G has aproper labeling. But if G is the intersection graph of the partitioning {Ai}i(cid:1)n then l(i, j ) =L(Ai ) ∩ L(Aj ). If for (i, j ) ∈ E L(l(i, j )) (cid:3)⊇ L(B1) ∩ L(B2), with B1, B2 the theories onthe two sides of (i, j ) in the tree G, then there are Ax , Ay in B1, B2, respectively, such that(x, y) ∈ E and x (cid:3)= i or y (cid:3)= j . Since G is connected (it is a single tree), this means thereis a cycle in G, contradicting G being a tree. Thus L(l(i, j )) ⊇ L(B1) ∩ L(B2) and G hasa proper labeling. The proof follows from Lemma 2.6. (cid:2)A.2. FORWARD-M-P with BREAK-CYCLES is sound and completeSoundness is immediate, using the same argument as for Theorem 2.4. For complete-ness, first notice that the graph output by BREAK-CYCLES is always a tree, becauseBREAK-CYCLES will not terminate if there is still a cycle in G. Now, we need the fol-lowing lemma.Lemma A.4. Let G(cid:20) = (V , E(cid:20), l(cid:20)) be a tree resulting from applying BREAK-CYCLES toG = (V , E, l) and {Ai}i(cid:1)n. Then G(cid:20) has a proper labeling for this partitioning.Proof of Lemma A.4. Assume there is a symbol p in L(B1) ∩ L(B2) that is not in l(i, j ),and let Ax , Ay be partitions on the two sides of (i, j ) that include sentences with thesymbol p. We will prove that throughout the run of the BREAK-CYCLES algorithm thereis always a path in G(cid:20) (we start with G(cid:20) = G) between Ax , Ay that has p showing on allthe edge labels. Call such a path a good path.Obviously we have a good path in G, because we have (x, y) ∈ E and p ∈ l(x, y) (be-cause G is the intersection graph of A1, . . . , An). Let us stop the algorithm at the firststep in which G(cid:20) does not have a good path (assuming there is no such path, or oth-erwise we are done). In the last step we must have removed an arc (a, b) (which wason a good path) to cause G(cid:20) to not have a good path. Since p ∈ l(a, b) and (a, b) is ina cycle (cid:17)(b, a1), (a1, a2), . . . , (ac, a), (a, b)(cid:18) (this is the only reason we removed (a, b)),we added l(a, b) to the labels of the rest of this cycle. In particular, now the labels of(b, a1), (a1, a2), . . . , (ac, a) include p. Replacing (a, b) in the previous good path by thissequence, we find a path in the new G(cid:20) that satisfies our required property. This is a con-E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8881tradiction to having assumed that there is no such path at this step. Thus, there is no suchp as mentioned above and L(l(i, j )) ⊇ L(B1) ∩ L(B2). (cid:2)Proof of Theorem 2.7. The proof of Theorem 2.7 follows immediately from Lemmas 2.6and A.4. (cid:2)A.3. BACKWARD-M-P (BMP) is sound and completeProof of Theorem 2.8. Notice that for a prover i to have a goal Qi means that it needs toprove that the theory Ai ∪ {¬Qi} is inconsistent. ϕ is a subgoal in a subgoal-disjunctivesystem if {ϕ} ∪ Ai (cid:13) Qi . For a series of subgoals ψ1, . . . , ψr in partition Ai , {ψ1 ∨ · · · ∨ψr } ∪ Ai (cid:13) Qi . Also, if Aj is the partition to whom Ai sends its subgoals, then Qj , thegoal of partition Aj , is ψ1 ∨ · · · ∨ ψr at this point in time.Let ϕ be a subgoal of Ai . This means that Ai ∪ {¬Qi } (cid:13) ¬ϕ. Thus, our BMP algorithmreadily reduces to MP, as ¬ϕ would be sent from Ai to Aj in MP while ϕ would bedisjoined with the goal of Aj in BMP, and both need to prove inconsistency of Aj ∪{¬Qj ∧ ¬ϕ}, when Qj is the goal of Aj before the arrival of ϕ. From the soundnessand completeness of MP for graphs that are trees, we get soundness and completeness forBMP. (cid:2)A.4. Theorem 3.2: RESOLUTION-M-P (RES-MP) is sound and completeTheorem A.5 [67]. For every non-tautologous clause D following from a given clauseset A, a clause C is derivable by the resolution rule such that D is obtained from C byinstantiation and addition of further literals (i.e., C ⊂-subsumes D).Proof of Theorem 3.2. Soundness and completeness of the algorithm follow from thatof MP, if we show that RES-SEND (Implementation 4) adds enough sentences (implyingcompleteness) to Ai that are implied by ϕ (thus sound) in the restricted language L(l(i, j )).If we add all sentences ϕ that are submitted to RES-SEND to Ai without any translation,then our soundness and completeness result for MP applies (this is the case where we addall the constant and function symbols to all l(i, j )).We use Theorem 3.1 to prove that we add enough sentences to Ai . Let ϕ2 be a quantifiedformula that is the result of applying algorithm U to ϕ. Then, ϕ2 results from a clause Cgenerated in step 4 of algorithm U (respectively, step 4 in RES-SEND). In algorithm U,for each variable x, the markings “x ← αi ” in C are converted to a new variable thatis existentially quantified immediately to the right of the quantification of the variablesy1, . . . , yr . ϕ2 is a result of ordering the quantifiers in a consistent manner to this rule (thisprocess is done in steps 5–6 of algorithm U).Step 5 of RES-SEND performs the same kind of replacement that algorithm U per-forms, but uses new function symbols instead of new quantified variables. Since each newquantified variable in ϕ2 is to the right of the variables on which it depends, and our newfunction uses exactly those variables as arguments, then step 5 generates a clause C(cid:20) fromC that entails ϕ2. Thus, the clauses added to Ai by RES-SEND entail all the clauses gener-82E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88ated by unskolemizing ϕ using U. From Theorem 3.1, these clauses entail all the sentencesin L(l(i, j )) that are implied by ϕ.To see that the result is still sound, notice that the set of clauses that we add to Ai hasthe same consequences as ϕ in L(l(i, j )) (i.e., if we add those clauses to Aj we get aconservative extension of Aj ). (cid:2)A.5. LINEAR-PART-SAT is sound and completeProof of Theorem 4.1. For each partition Ai , i (cid:1) n, lines 1–3 perform the equivalent offinding all the models of Ai and storing their truth assignments to the symbols of L(L(i))in Ti . (L(i) specifies the columns, thus each row corresponds to a truth assignment.) Thisis equivalent to finding the implicates of the theory Ai in the sublanguage L(L(i)). Thus, ifAi is the DNF of the set of implicates (α1(pj1 , . . . , pjli)), then Tiinitially includes the set of models of Ai in the sublanguage L(L(i)), namely, [[Ai]]L(L(i)).The natural join operation ((cid:1)) then creates all the consistent combinations of modelsfrom [[Ai]]L(L(i)) and [[Aj ]]L(L(j )). This set of consistent combinations is the set of modelsof Ai ∪ Aj . Thus, Ti (cid:1) Tj ≡ [[(Ai ∪ Aj )]]L(L(i)∪L(j )).) ∨ · · · ∨ αai (pj1 , . . . , pjliFinally, the projection operation restricts the models to the sublanguage L(L(i)), gettingrid of duplicates in the sublanguage. This is equivalent to finding all the implicates ofAi ∧ Aj in the sublanguage L(L(i)). Thus, πL(i)(Ti (cid:1) Tj ) ≡ [[{ϕ ∈ L(L(i)) | Ai ∪ Aj |=ϕ}]]L(L(i)).To see that the algorithm is sound and complete, notice that it does the analogousoperations to our forward message-passing algorithm MP (Fig. 3). We break the cyclesin G0 (creating G) and perform forward reasoning as in MP, using the set of impli-cates instead of online reasoning in each partition: instruction 2b in MP is our projec-tion (“Ai |= ϕ and ϕ ∈ L(l(i, j ))”) and then join (“add ϕ to the set of axioms of Aj ”).Since Ti (cid:1) Tj ≡ [[(Ai ∪ Aj )]]L(L(i)∪L(j )), joining corresponds to sending all the messagestogether. Since πL(i)(Ti (cid:1) Tj ) ≡ [[{ϕ ∈ L(L(i)) | Ai ∪ Aj |= ϕ}]]L(L(i)), projection corre-sponds to sending only those sentences that are allowed by the labels.By Theorem 2.7, LINEAR-PART-SAT is sound and complete for satisfiability of A. (cid:2)A.6. Time complexity of LINEAR-PART-SATProof of Lemma 4.2. Let A be a partitioned propositional theory with n partitions. Let mbe the total number of propositional symbols in A, L(i) the set of propositional symbolscalculated in step 4 of LINEAR-PART-SAT, and mi the number of propositional symbolsmentioned in Ai \ L(i) (i (cid:1) n). Let us examine procedure LINEAR-PART-SAT (Fig. 12)step by step, computing the time needed for computation.Computing the intersection graph takes O(a · k2) time, where k is the number of propo-sitional symbol in each axiom (for 3SAT, that is 3), because we check and add k2 edges toG0 for each axiom.BREAK-CYCLES’ loop starts by finding a minimal-length cycle, which takes timeO(n) (BFS traversal of n vertices). Finding the optimal a in line 2 takes time O((c · m) · c),where c is the length of the cycle found (union of two labels takes at most O(m) time).Finally, since a tree always satisfies |E| = |V | − 1, breaking all the cycles will require usE. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8883to remove |E| − (|V | − 1) edges. Thus, the loop will run |E| − (|V | − 1) times (assumingthe graph G0 is connected). An upper bound on this algorithm’s performance is then O(n2 ·(n2 · m)) = O(n4 · m) (because c (cid:1) n and |E| (cid:1) |V |2 = n2).Step 2 of LINEAR-PART-SAT takes time O(n · m), since there are a total of n − 1 edgesin the graph G (G is a tree with n vertices) and every label is of length at most m.Checking the truth assignments in step 3 takes timei=1 2|L(i)| per satisfiability checkof Ai ∪ A, because there are 2|L(i)| truth assignments for each i (cid:1) n. Since Ai ∪ A has onlymi free propositional variables, (A is an assignment of truth values to |L(i)| variables),Ai ∪ A is reducible (in time O(|A|)) to a theory of smaller size with only mi proposi-tional variables. If the time needed for a satisfiability check of a theory with m variables isO(fSAT(m)), then the time for step 3 is(cid:2)n(cid:6)O(cid:8)|L(i)| · fSAT(mi)2(cid:5)n(cid:7)(cid:3)i=1Finding the relation ≺ takes O(n) as it is easily generated by a BFS through the tree.Instruction 5 performs a projection and join, which takes time O(2|L(i)|) (the maximalsize of the table). Since the number of iterations over i (cid:1) n and j being a child of i is n − 1(there are only n − 1 edges), we get that the total time for this step is O((cid:2)ni=1 2|L(i)|).Summing up, the worst-case time used by the algorithm isTime(cid:3)(cid:5)n, m, m1, . . . , mn, a, k, L(1), . . . , L(n)(cid:6)= Oa · k2 + n4 · m + n · m +(cid:6)= Oa · k2 + n4 · m +n(cid:7)(cid:3)2i=1n(cid:7)(cid:3)2i=1|L(i)| · fSAT(mi)(cid:8)(cid:5)|L(i)| · fSAT(mi ).(cid:8)|L(i)|2(cid:5)+ n +n(cid:7)i=1We can reduce the second argument (in the last formula) from n4 · m to n · m, if the inter-section graph G0 is already a tree. (cid:2)A.7. Time complexity for SPLITProof of Proposition 5.1. Finding a minimum vertex separator R in G takes time O(c ·|V |3/2 · |E|). Finding a minimum separator that does not include s is equivalent to havings be the only source with which we check connectivity (in Even’s algorithm). Thus, thiscan be done in time O(|V |3/2 · |E|). Finding a minimum separator that separates s from ttakes time O(|V |1/2 · |E|). In the worst case, each time we look for a minimum s-separator(t = nil), we get a very small partition, and a very large one. Thus, we can apply thisprocedure O(|V |) times. Summing up the time taken for each application of the procedureyields O(|V | · |V |3/2 · |E| + c · |V |3/2 · |E|) = O(|V |5/2 · |E|). (cid:2)References[1] R.K. Ahuja, J.B. Orlin, Distance directed augmenting path algorithms for maximum flow and parametricmaximum flow problems, Naval Research Logistics Quarterly 38 (1991) 413–430.84E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88[2] V. Akman, M. Surav, Steps toward formalizing context, AI Magazine 17 (3) (1996) 55–72.[3] E. Amir, (De)Composition of situation calculus theories, in: Proc. National Conference on Artificial Intel-ligence (AAAI ’00), Austin, TX, AAAI Press/MIT Press, 2000, pp. 456–463.[4] E. Amir, Efficient approximation for triangulation of minimum treewidth, in: Proc. Seventeenth Conferenceon Uncertainty in Artificial Intelligence (UAI ’01), Morgan Kaufmann, San Mateo, CA, 2001, pp. 7–15.[5] E. Amir, S. McIlraith, Partition-based logical reasoning, in: Proc. Seventh International Conference onPrinciples of Knowledge Representation and Reasoning (KR ’2000), Morgan Kaufmann, San Mateo, CA,2000, pp. 389–400.[6] E. Amir, S. McIlraith, Strategies for focusing structure-based theorem proving, Artificial Intelligence, inpress.[7] S. Arnborg, Efficient algorithms for combinatorial problems on graphs with bounded decomposability—a survey, BIT 25 (1985) 2–23.[8] F. Baader, K.U. Schulz, Unification in the union of disjoint equational theories: combining decision pro-cedures, in: Proceedings of the Eleventh International Conference on Automated Deduction, in: LectureNotes in Artificial Intelligence, vol. 607, Springer, Berlin, 1992, pp. 50–65.[9] A. Becker, R. Bar-Yehuda, D. Geiger, Randomized algorithms for the loop cutset problem, J. ArtificialIntelligence Res. 12 (2000) 219–234.[10] A. Becker, D. Geiger, Approximate algorithms for the loop cutset problem, in: Proc. Tenth Conference onUncertainty in Artificial Intelligence (UAI ’94), Morgan Kaufmann, San Mateo, CA, 1994, pp. 60–68.[11] A. Becker, D. Geiger, A sufficiently fast algorithm for finding close to optimal junction trees, in: Proc.Twelfth Conference on Uncertainty in Artificial Intelligence (UAI ’96), Morgan Kaufmann, San Mateo,CA, 1996, pp. 81–89.[12] U. Bertele, F. Brioschi, Nonserial Dynamic Programming, Academic Press, New York, 1972.[13] W.W. Bledsoe, A.M. Ballantyne, Unskolemizing, Technical Report Memo ATP-41, Mathematics Depart-ment, University of Texas, Austin, 1978.[14] M.P. Bonacina, J. Hsiang, Parallelization of deduction strategies: an analytical study, J. Automat. Rea-son. 13 (1994) 1–33.[15] M.P. Bonacina, J. Hsiang, On the representation of dynamic search spaces in theorem proving, in:C.-S. Yang (Ed.), Proceedings of the International Computer Symposium, 1996, pp. 85–94.[16] P.E. Bonzon, A reflective proof system for reasoning in contexts, in: Proc. National Conference on ArtificialIntelligence (AAAI ’97) Providence, RI, 1997, pp. 398–403.[17] R.B. Boppana, M. Sipser, The complexity of finite functions, in: J. van Leeuwen (Ed.), Handbookof Theoretical Computer Science, vol. 1, in: Algorithms and Complexity, Elsevier/MIT Press, Amster-dam/Cambridge, MA, 1990, pp. 757–804.[18] G. Bossu, P. Siegel, Saturation, nonmonotonic reasoning and the closed world assumption, Artificial Intel-ligence 25 (1) (1985) 13–65.[19] R.S. Boyer, Locking: a restriction of resolution, PhD thesis, Mathematics Department, University of Texas,Austin, 1971.[20] A. Carbone, Interpolants, cut elimination and flow graphs for the propositional calculus, Ann. Pure Appl.Logic 83 (3) (1997) 249–299.[21] R. Chadha, D.A. Plaisted, Finding logical consequences using unskolemization, in: Proceedings of the7th International Symposium on Methodologies for Intelligent Systems (ISMIS ’93), in: Lecture Notes inArtificial Intelligence, vol. 689, Springer, Berlin, 1993, pp. 255–264.[22] C.-L. Chang, R.C.-T. Lee, Symbolic Logic and Mechanical Theorem Proving, Academic Press, New York,1973.[23] B.V. Chekassky, A.V. Goldberg, On implementing the push-relabel method for the maximum flow problem,Algorithmica 19 (4) (1997) 390–410.[24] P. Cohen, R. Schrag, E. Jones, A. Pease, A. Lin, B. Starr, D. Gunning, M. Burke, The DARPA high-performance knowledge bases project, AI Magazine 19 (4) (1998) 25–49.[25] S.E. Conry, D.J. McIntosh, R.A. Meyer, DARES: a distributed automated reasoning system, in: Proc.National Conference on Artificial Intelligence (AAAI ’90), Boston, MA, AAAI Press/MIT Press, 1990,pp. 78–85.[26] S.A. Cook, D.G. Mitchell, Finding hard instances of the satisfiability problem: a survey, in: DIMACS Seriesin Discrete Mathematics and Theoretical Computer Science, vol. 35, American Mathematical Society,Providence, RI, 1997.E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8885[27] T.H. Cormen, C.E. Leiserson, R.L. Rivest, Introduction to Algorithms, McGraw Hill, New York, 1989.[28] R. Cowen, K. Wyatt, BREAKUP: a preprocessing algorithm for satisfiability testing of CNF formulas,Notre Dame J. Formal Logic 34 (4) (1993) 602–606.[29] P. Cox, T. Pietrzykowski, A complete nonredundant algorithm for reversed skolemization, Theoret. Com-put. Sci. 28 (1984) 239–261.[30] W. Craig, Linear reasoning. A new form of the Herbrand–Gentzen theorem, J. Symbolic Logic 22 (1957)250–268.[31] W. Craig, Three uses of the Herbrand–Gentzen theorem in relating model theory and proof theory, J. Sym-bolic Logic 22 (1957) 269–285.[32] G.B. Dantzig, D.R. Fulkerson, On the max-flow min-cut theorem of networks, in: H.W. Kuhn, A.W. Tucker(Eds.), Linear Inequalities and Related Systems, in: Annals of Mathematics Study, vol. 38, Princeton Univ.Press, Princeton, NJ, 1956, pp. 215–221.[33] A. Darwiche, Utilizing knowledge-based semantics in graph-based algorithms, in: Proc. National Con-ference on Artificial Intelligence (AAAI ’96), Portland, OR, Morgan Kaufmann, San Mateo, CA, 1996,pp. 607–613.[34] M. Davis, G. Logemann, D. Loveland, A machine program for theorem proving, Comm. ACM 5 (1962)394–397.[35] R. Dechter, Enhancement schemes for constraint processing: backjumping, learning, and cutset decompo-sition, Artificial Intelligence 41 (3) (1990) 273–312.[36] R. Dechter, Y. El Fattah, Topological parameters for time-space tradeoff, Artificial Intelligence 125 (1)(2001) 93.[37] R. Dechter, J. Pearl, Tree clustering for constraint networks, Artificial Intelligence 38 (1989) 353–366.[38] R. Dechter, I. Rish, Directional resolution: the Davis–Putnam procedure, revisited, in: Proc. Fourth In-ternational Conference on Principles of Knowledge Representation and Reasoning (KR ’94), MorganKaufmann, San Mateo, CA, 1994, pp. 134–145.[39] A. del Val, A new method for consequence finding and compilation in restricted language, in: Proc. NationalConference on Artificial Intelligence (AAAI ’99), Orlando, FL, AAAI Press/MIT Press, 1999, pp. 259–264.[40] J. Denzinger, D. Fuchs, Cooperation of heterogeneous provers, in: D. Thomas (Ed.), Proc. Sixteenth Inter-national Joint Conference on Artificial Intelligence (IJCAI ’99), Stockholm, Sweden, Morgan Kaufmann,San Mateo, CA, 1999, pp. 10–15.[41] J. Denzinger, Knowledge-based distributed search using teamwork, in: V. Lesser (Ed.), Proceedings of theFirst International Conference on Multi-Agent Systems, San Francisco, CA, MIT Press, Cambridge, MA,1995, pp. 81–88.[42] J. Denzinger, I. Dahn, Cooperating theorem provers, in: W. Bibel, P.H. Schmitt (Eds.), in: AutomatedDeduction. A Basis for Applications, vol. 2, Kluwer, Dordrecht, 1998, pp. 383–416.[43] J. Denzinger, M. Fuchs, M. Fuchs, High performance ATP systems by combining several AI methods,in: Proc. Fifteenth International Joint Conference on Artificial Intelligence (IJCAI ’97), Nagoya, Japan,Morgan Kaufmann, San Mateo, CA, 1997, pp. 102–107.[44] E.A. Dinic, Algorithm for solution of a problem of maximum flow in networks with power estimation,Soviet Math. Dokl. 11 (1970) 1277–1280.[45] N. Eisinger, H.J. Ohlbach, Deduction systems based on resolution, in: D.M. Gabbay, C.J. Hogger, J.A.Robinson (Eds.), Handbook of Logic in Artificial Intelligence and Logic Programming, vol 1: LogicalFoundations, Oxford University Press, Oxford, 1993.[46] W. Ertel, OR-parallel theorem proving with random competition, in: Proc. International Conference onLogic Programming and Automated Reasoning (LPAR ’92), in: Lecture Notes in Artificial Intelligence,vol. 624, 1992, pp. 226–237.[47] S. Even, An algorithm for determining whether the connectivity of a graph is at least k, SIAM J. Com-put. 4 (3) (1975) 393–396.[48] S. Even, Graph Algorithms, Computer Science Press, 1979.[49] S. Even, R. Endre Tarjan, Network flow and testing graph connectivity, SIAM J. Comput. 4 (4) (1975)507–518.[50] U. Feige, R. Krauthgamer, A polylogarithmic approximation of the minimum bisection, in: Proc. 41st IEEESymp. on Foundations of Computer Science (FOCS ’00), IEEE Press, 2000.86E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88[51] R. Fikes, A. Farquhar, Large-scale repositories of highly expressive reusable knowledge, IEEE IntelligentSystems 14 (2) (1999).[52] J.J. Finger, M.R. Genesereth, Residue: a deductive approach to design synthesis, Technical Report STAN-CS-85-1035, Stanford University, Computer Science Department, Stanford, CA, 1985.[53] L.R. Ford Jr., D.R. Fulkerson, Flows in Networks, Princeton University Press, Princeton, NJ, 1962.[54] M.R. Genesereth, N.J. Nilsson, Logical Foundations of Artificial Intelligence, Morgan Kaufmann, SanMateo, CA, 1987.[55] E. Giunchiglia, P. Traverso, A multi-context architecture for formalizing complex reasoning, Internat. J.Intelligent Systems 10 (1995) 501–539. Also, IRST Tech. Report #9307-26.[56] F. Giunchiglia, GETFOL manual—GETFOL version 2.0, Technical Report DIST-TR-92-0010, DIST—University of Genoa, 1994. Available at http://ftp.mrg.dist.unige.it/pub/mrg-ftp/92-0010.ps.gz, and websiteat http://www-formal.stanford.edu/clt/ARS/Entries/getfol.[57] A.V. Goldberg, R.E. Tarjan, A new approach to the maximum-flow problem, J. ACM 35 (4) (1988) 921–940.[58] G. Gottlob, N. Leone, F. Scarcello, A comparison of structural CSP decomposition methods, in: Proc.Sixteenth International Joint Conference on Artificial Intelligence (IJCAI ’99), Orlando, FL, Morgan Kauf-mann, San Mateo, CA, 1999, pp. 394–399.[59] A. Haken, The intractability of resolution, Theoret. Comput. Sci. 39 (1985) 297–308.[60] G. Huang, Constructing Craig interpolation formulas, in: First Annual International Conference on Com-puting and Combinatorics (COCOON ’95), 1995, pp. 181–190.[61] K. Inoue, Linear resolution for consequence finding, Artificial Intelligence 56 (2–3) (1992) 301–353.[62] P. Klein, S.A. Plotkin, S. Rao, Excluded minors, network decomposition, and multicommodity flow, in:Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, 1993, pp. 682–690.[63] T. Kloks, Treewidth: Computations and Approximations, Lecture Notes in Computer Science, vol. 842,Springer, Berlin, 1994.[64] J. Kohlas, R. Haenni, S. Moral, Propositional information systems, J. Logic Comput. 9 (5) (1999) 651–681.[65] J. Krajiˇcek, Interpolation theorems, lower bounds for proof systems, and independence results for boundedarithmetic, J. Symbolic Logic 62 (2) (1997) 457–486.[66] S.L. Lauritzen, D.J. Spiegelhalter, Local computations with probabilities on graphical structures and theirapplication to expert systems, J. Roy. Statist. Soc. B 50 (2) (1988) 157–224.[67] R.C.-T. Lee, A completeness theorem and a computer program for finding theorems derivable from givenaxioms, PhD Thesis, University of California, Berkeley, 1967.[68] T. Leighton, S. Rao, An approximate max-flow min-cut theorem for uniform multicommodity flow prob-lems with applications to approximation algorithms, in: Proc. 29th IEEE Symp. on Foundations of Com-puter Science (FOCS’88), 1988, pp. 422–431.[69] D.B. Lenat, Cyc: a large-scale investment in knowledge infrastructure, Comm. ACM 38 (11) (1995) 33–38.[70] F. Lin, On strongest necessary and weakest sufficient conditions, in: Proc. Seventh International Conferenceon Principles of Knowledge Representation and Reasoning (KR ’2000), 2000, pp. 167–175.[71] J.W. Lloyd, R.W. Topor, A basis for deductive database systems, J. Logic Programming 2 (1985) 93–109.[72] D.W. Loveland, Automated Theorem Proving: A Logical Basis. Fundamental Studies in Computer Science,North-Holland, Amsterdam, 1978.[73] B. MacCartney, Sh. McIlraith, E. Amir, T. Uribe, Practical partition-based theorem proving for largeknowledge bases, in: Proc. Eighteenth International Joint Conference on Artificial Intelligence (IJCAI ’03),Acapulco, Mexico, Morgan Kaufmann, San Mateo, CA, 2003, pp. 89–96.[74] P. Marquis, Knowledge compilation using theory prime implicates, in: Proc. Fourteenth International JointConference on Artificial Intelligence (IJCAI ’95), Montreal, Quebec, 1995, pp. 837–843.[75] P. Marquis, Consequence finding algorithms, in: D. Gabbay, Ph. Smets (Eds.), Handbook of DefeasibleReasoning and Uncertainty Management Systems, vol 5: Algorithms for Defeasible and Uncertain Rea-soning, Kluwer, Dordrecht, 2000.[76] J. McCarthy, S. Buvaˇc, Formalizing context (expanded notes), in: A. Aliseda, R.J. van Glabbeek, D. West-erståhl (Eds.), Computing Natural Language, in: CSLI Lecture Notes, vol. 81, Center for the Study ofLanguage and Information, Stanford University, Stanford, CA, 1998, pp. 13–50.E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–8887[77] S. McIlraith, Logic-based abductive inference, Technical Report KSL-98-19, Knowledge Systems Lab,Department of Computer Science, Stanford University, 1998.[78] S. McIlraith, E. Amir, Theorem proving with structured theories, in: Proc. Seventeenth International JointConference on Artificial Intelligence (IJCAI ’01), Seattle, WA, Morgan Kaufmann, San Mateo, CA, 2001,pp. 624–631.[79] A.R. Meyer, A note on the length of Craig’s interpolants Technical Memo MIT/LCS/TM-183, Massa-chusetts Institute of Technology, Laboratory for Computer Science, 1980.[80] E. Minicozzi, R. Reiter, A note on linear resolution strategies in consequence-finding, Artificial Intelli-gence 3 (1972) 175–180.[81] G. Nelson, D.C. Oppen, Simplification by cooperating decision procedures, ACM Trans. ProgrammingLang. Syst. 1 (2) (1979) 245–257.[82] T.J. Park, A. Van Gelder, Partitioning methods for satisfiability testing on large formulas, in: Proceedingsof the Thirteenth International Conference on Automated Deduction (CADE-13), Springer, Berlin, 1996,pp. 748–762.[83] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kauf-mann, San Mateo, CA, 1988.[84] D.A. Plaisted, The search efficiency of theorem proving strategies, in: Proceedings of the Twelfth Interna-tional Conference on Automated Deduction (CADE-12), 1994, pp. 57–71.[85] C. Ringeissen, Cooperation of decision procedures for the satisfiability problem, in: F. Baader, K.U. Schulz(Eds.), Frontiers of Combining Systems: Proceedings of the 1st International Workshop, Munich (Ger-many), Applied Logic, Kluwer Academic, Dordrecht, 1996, pp. 121–140.[86] I. Rish, R. Dechter, Resolution versus search: two strategies for SAT, J. Automat. Reason. 24 (1–2) (2000)225–275.[87] N. Robertson, P.D. Seymour, Graph minors. II: algorithmic aspects of treewidth, J. Algorithms 7 (1986)309–322.[88] J.A. Robinson, A machine-oriented logic based on the resolution principle, J. ACM 12 (1) (1965) 23–41.[89] I. Schiermeyer, Pure literal look ahead: an O(1, 497n) 3-satisfiability algorithm (extended abstract), Tech-nical Report, University of Köln, 1996. Workshop on the Satisfiability Problem, Siena, April 29–May 3.[90] B. Selman, H. Kautz, Domain-independent extensions to GSAT: solving large structured satisfiability prob-lems, in: Proc. Eighteenth International Joint Conference on Artificial Intelligence (IJCAI ’03), Acapulco,Mexico, 1993.[91] B. Selman, H.A. Kautz, B. Cohen, Noise strategies for local search, in: Proc. National Conference onArtificial Intelligence (AAAI ’94), Seattle, WA, 1994, pp. 337–343.[92] B. Selman, H.J. Levesque, D. Mitchell, A new method for solving hard satisfiability problems, in: P. Rosen-bloom, P. Szolovits (Eds.), Proceedings of the Tenth National Conference on Artificial Intelligence, SanJose, CA, American Association for Artificial Intelligence, AAAI Press, Menlo Park, CA, 1992, pp. 440–446.[93] B. Selman, D. Mitchell, H. Levesque, Generating hard satisfiability problems, in: B. Selman, D. Mitchell,H. Levesque (Eds.), Artificial Intelligence, 1997, pp. 17–29.[94] G. Shafer, P. Shenoy, Probability propagation, Ann. Math. Artificial Intelligence 2 (1990) 327–352.[95] J. Shi, J. Malik, Normalized cuts and image segmentation, in: Proc. of IEEE Conference on ComputerVision and Pattern Recognition, 1997, pp. 731–737.[96] K. Shoikhet, D. Geiger, A practical algorithm for finding optimal triangulations, in: Proc. National Con-ference on Artificial Intelligence (AAAI ’97), Providence, RI, Morgan Kaufmann, San Mateo, CA, 1997,pp. 185–190.[97] R.E. Shostak, Deciding combinations of theories, J. ACM 31 (1984) 1–12.[98] J.R. Slagle, Interpolation theorems for resolution in lower predicate calculus, J. ACM 17 (3) (1970) 535–542.[99] J.R. Slagle, C.-L. Chang, R.C.T. Lee, Completeness theorems for semantic resolution in consequence-finding, in: Proc. First International Joint Conference on Artificial Intelligence (IJCAI ’69), Washington,DC, 1969, pp. 281–285.[100] J. Slaney, T.J. Surendonk, Combining finite model generation with theorem proving: problems andprospects, in: F. Baader, K.U. Schulz (Eds.), Frontiers of Combining Systems: Proceedings of the 1st Inter-national Workshop, Munich (Germany), Applied Logic, Kluwer Academic, Dordrecht, 1996, pp. 141–156.88E. Amir, S. McIlraith / Artificial Intelligence 162 (2005) 49–88[101] M.E. Stickel, A Prolog technology theorem prover: a new exposition and implementation in Prolog, Theo-ret. Comput. Sci. 104 (1992) 109–128.[102] G.C.J. Sutcliffe, A heterogeneous parallel deduction system, in: FGCS’92 Workshop on Automated De-duction: Logic Programming and Parallel Computing Approaches, 1992.[103] C.B. Suttner, SPTHEO, J. Automat. Reason. 18 (1997) 253–258.[104] C. Tinelli, M.T. Harandi, A new correctness proof of the Nelson–Oppen combination procedure, in: F.Baader, K.U. Schulz (Eds.), Frontiers of Combining Systems: Proceedings of the 1st International Work-shop, Munich (Germany), Applied Logic, Kluwer Academic, Dordrecht, 1996, pp. 103–120.[105] G.S. Tseitin, On the complexity of proofs in propositional logics, Seminars in Mathematics 8 (1970).[106] J.D. Ullman, Principles of Database and Knowledge-base Systems, vol. 1, Computer Science Press, 1988.[107] A. Urquhart, Hard examples for resolution, J. ACM 34 (1) (1987) 209–219.