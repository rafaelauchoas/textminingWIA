Artificial Intelligence 94 (1997) 7-56 Artificial Intelligence The independent choice logic for modelling multiple agents under uncertainty David Poole * Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver; BC, Canada V6T I24 Abstract Inspired by game theory representations, Bayesian networks, influence diagrams, structured Markov decision process models, logic programming, and work in dynamical systems, the inde- pendent choice logic (ICL) is a semantic framework that allows for independent choices (made by various agents, including nature) and a logic program that gives the consequence of choices. This representation can be used as a specification for agents that act in a world, make observa- tions of that world and have memory, as well as a modelling tool for dynamic environments with uncertainty The rules specify the consequences of an action, what can be sensed and the utility of outcomes. This paper presents a possible-worlds semantics for ICL, and shows how to embed in- fluence diagrams, structured Markov decision processes, and both the strategic (normal) form and extensive (game-tree) form of games within the ICL. It is argued that the ICL provides a natural and concise representation for multi-agent decision-making under uncertainty that allows for the representation of structured probability tables, the dynamic construction of networks (through the use of logical variables) and a way to handle uncertainty and decisions in a logical representation, @ 1997 Elsevier Science B.V. 1. Introduction This paper presents the Independent Choice Logic (ICL), agents under uncertainty. [ 7,351, representations multiple networks tured agent ma,delling and dynamical and change influence diagrams of Bayesian [ 27,45,50]. It is inspired by game theory a logic for modelling [ 17,32,53], Bayesian networks [ 22,231, probabilistic Horn abduction [ 361, struc- [ 5,7,8], [ 29,48,5 1,571 and logical modelling of action and Markov decision processes systems * Email: poole@cs.ubc.ca. 0004-3702/97/$17.00 @ 1997 Elsevier Science B.V. All rights reserved. PZI s0004~-3702(97)00027-1 8 D. Poole/Artijcial Intelligence 94 (1997) 7-56 First we motivate ICL from a number of different perspectives, (Section of knowledge 1.3), representation the paradigms the foundations it fits within subsections we present theory (Section then build this paper presents examples of the use of the logic, including diagrams, Markov decision processes, can be represented. diagrams of the representation influence the formal definition (Section of ICL based on agents 1.4) and logic and the strategic and extensive then show how In separate 1.2), game 1.5). We (Section in Section 2. The majority of 1.1). (Section showing how influence forms of games than tests), amongst rule-based framework its parents represented probabilities representation is exponentially of independence of variables given for the conditional probability in the network. The conditional as tables, but can often be specified more compactly the rule representation, that there agents, so that not only can the knowledge base be expressed compactly [ 351 provide a useful representation are a representation for reasoning under Bayesian or belief networks uncertainty. Bayesian networks random variables. The Bayesian network model does not constrain how a variable depends on its of a variable parents, nor does it specify a representation their given parents are typically in than trees (unless terms of trees [ 71 or rules [ 361. Rules are more compact the trees can in the sense have shared structure and redundant that there are some functions larger the tree representation where the converse does not hold.’ Rules have the added advantage extension [ 361, a first-order as failure and fewer restriction on the rules paper extends the probabilistic multiple rules, but agents’ policies can also be expressed by rules. the primary but is a natural to the first-order case [ 361. This paper builds on probabilistic Horn abduction for Bayesian networks, allowing negation than in probabilistic Horn abduction. This and decisions made by to include utilities by is in AI. This to specify what because It has often been you want argued scheme must be at least as rich as the first-order predicate calculus. One of the problems with the first order predicate calculus a rather blunt reasoning under uncertainty. Rather calculus theory, uncertainty, In the ICL, we start with a logic does instead of disjunction, not include any uncertainty theory entails exactly one of p or up for all propositions p). Agents own alternatives, which are sets that it owns. of propositions. An agent gets to choose one value from each alternative Nature is a special agent; the alternatives owned by nature have a probability distribution over them. The logic gives is in to the first-order predicate and probabilistic of how it is computed representation representation and a way [42]. which would entail having both disjunctive of the choices made by nature and the and is definitive on all propositions is the way it handles uncertainty; that we should use probability and does not do justice [33] ) that any general than adding uncertainty to all of the subtleties to handle uncertainty. This involved focus of knowledge this paper proposes Logic has become all it has available the consequences to give meaning is disjunction. and decision independently [ 3,19,20,24], to compute to symbols it provides instrument a way (every (e.g., ’ As we allow negation as failure in the roles, the rules can be seen as a DNF definition of a concept Clark’s completion when converted leaf in the decision [ 111). It is known that DNF formulae to decision trees [ 461. Decision tree whose body corresponds trees can be converted to the path to the leaf. (using sometimes entail an exponential blow up in size to mles, with a role for each simply D. Poole/Artificial Intelligence 94 (1997) 7-56 9 agents. This allows us to have the advantages of logic, with symbols denotations, specifications lets us use the normative agent should do. of valid consequences tools of decision/game theory notions and first-order representations, that can be given but also to determine what an in systems [ 29,441) referring functions the state (see e.g., and describe a state space terms of Bayesian Models, of dynamical [ 121 or even more concisely in terms of matrices. in terms of propositions, in terms of these propositions. The state transition networks been described have traditionally treating state spaces in terms of vectors of states It is often much more convenient transition function can be stated con- [ $81 is loga- local, the in the size of the state space. The effects of actions are typically the poten- only on a few other variables. This provides of the propositional In the in terms of rules. One advantage of rules cal- to also and dynamic program- in terms of state spaces, for example and state transition to describe function cisely or rules 11401, never rithmic value of a variable depending tial ICL we specify state is that they are closer [ 3 1 ] (see culus the stage or situation make helps clarify ming. to take advantage of the compactness functions transition to the traditional AI representations [ 401) . The first-order nature of the rules, with explicit the rules perspicuous. The rule based representation state. The number of variables such as the situation the close relationship regression planning to the explicit representation. reference by trees between 1.1. Knowledge representation representation should be. There axe two different views of what a knowledge l The first is that a knowledge representation should let users state whatever knowl- this view it is not appropriate to specify how a piece of knowl- follows from the facts in a common sense manner. An example of can conclude what logically representation represen- to let us state many facts about the world, but logics have been for knowledge this tradition, edge they have in a reasonably natural way. Under for the designer of a knowledge edge should be encoded. Reasoning stated facts or can fill in missing this view is in the use of the first-order predicate calculus tation. It is a rich enough with primitive means deve:loped 21, ;!4]. Missing maximum the user can add whatever should be able to make appropriate l The second view is that a knowledge entropy or random worlds assumptions to handle uncertainty. Within to handle uncertainty facts can be inferred using default reasoning [ 41. What inferences. language they like to the knowledge base, and the representation and multiple agents making decisions [3,15,19- [ 301, or by making is that is important language representation symlbolic modelling view a knowledge guide users as to how they should and once some choices have been made, be specified. An example tool for representing specify the random variables of interest, independence representation that makes some things easier should provide a high-level this It should think about the domain, what they should say; to should specify how to model a domain. it prescribes what information to state. Under needs is Bayesian networks [ 351, which provide a modelling amongst random variables. The user needs to these variables can take, and the values 10 D. Poole/Artificial Intelligence 94 (1997) 7-56 the dependency amongst these variables. Once these are specified the Bayesian network model prescribes what probabilities need to be specified. It is important not to confuse these, as judging a knowledge representation by the inappropriate criteria will lead to an unfair judgment. The knowledge representation in this paper should be seen as an instance of the second. We do not expect that people will be able to just throw any knowledge in. For example, missing rules have a particular meaning; if you want to assert ignorance there are specific ways to do it. 1.2. Agents An agent is something that acts in the world. An agent can, for example, be a person, a robot, a worm, the wind, gravity, a lamp, or anything else. Purposive agents have act in order to prefer some states of the world to other states-and preferences-they (try to) achieve worlds they prefer. The non-purposive agents are grouped together and called “nature”. Whether an agent is purposive or not is a modelling assumption that may or may not be appropriate. For example, for some applications it may be appropriate to model a dog as purposive, and for others it may suffice to model a dog as non-purposive. Agents can have sensors, (possibly limited) memory, computational capabilities and effecters. Agents reason and act in time. An agent should react to the world; it has to condition its actions on what is received by its sensors. These sensors may or may not reflect what is true in the world;2 sensors can be noisy, unreliable or broken; and even when sensors are reliable there is still ambiguity about the world from sensors’ readings. An agent can only condition its actions on what it knows, even if it is very weak such as “sensor a appears to be outputting value u”. Similarly actuators may be noisy, unreliable, slow or broken. What an agent can control is what message (command) it sends to its actuators. An agent can be seen as an implementation of a transduction [ 39,47,48,57], a function from input (sensor readings) history into outputs (action attempts or actuator settings) at each time point. These are causal in the sense that the output can only depend on current inputs and previous inputs and outputs; they cannot be conditional on future inputs or outputs. A policy or strategy is a specification of what an agent will do under various contin- gencies. That is, it is a representation of a transduction. A plan is a policy that includes either time or the stage as part of the contingencies conditioned on. Our aim is to provide a representation in which we can define perception, actions and preferences for agents. This can be used to define a policy, the notion of when to that agent’s preferences), and so an one policy is better than another (according appropriate notion of an optimal policy for an agent. Once we have defined what an optimal policy is, we can use exact and approximation algorithms to build policies for agents. * Of course if there is no correlation between what a sensor reading tells us and what is true in the world, and the preferences of the agent depend on what is true in the world (as they usually do), it may as well ignore the sensor. D. Poole/Arti~cial Intelligence 94 (1997) 7-56 11 We want to model agents and their environments with the same language. The lan- guage should provide a decision theoretic, or game theoretic (for more than one agent) framework that can be used to build agents that can be shown to be optimal (as in [ 481) or at least to have a specification of the expected utility of an agent. A planner in this framework is a program that generates a (possibly stochastic) transduction for an agent to execute. The output of the planner should be suitable for actually controlling an agent. It has to be more than a sequence of steps that is the output of traditional planners. Here we consider reactive agents that have internal state. This paper does not consider the problem of building a planner. Even for the single-agent propositional case, the problem of finding an optimal policy is computationally prohibitive [25], but this is more a property of the problem than of the representation. By having a rich representation we can discuss the complexity of various restrictions and build approximation algorithms. Under this view, beliefs, desires, intentions, and commitments [51] are not essential to agenthood. It may, however, be the case that agents with beliefs, desires, intentions, and commitments that, for example, communicate by way of speech acts [ 5 11, perform better by some measure than those that do not. We do not want to define agenthood to exclude the possibility of formulating and testing this empirical claim. In this paper we provide a representation that can be used to model the world, agents (including available sensors and actuators) and goals (in terms of the agents utilities in different situations) that will allow us to design optimal (or approximately optimal) agents. 1.3. Game theory Game theory [ 17,32,53] is a theory of multi-agent reasoning under uncertainty. The general idea is that there is a set of players (agents) who make moves (take actions) based on what they observe. The agents each try to do as well as they can (maximize their utility). Game theory is designed to be a general theory of economic behaviour [53] that is a generalization of decision theory. The use of the term “game” here is much richer than typically studied in AI text books for parlour games such as chess. These could be described as deterministic (there are no chance moves by nature), perfect information (each plalyer knows the previous moves of the other players), zero-sum (one player can only win by making the other player lose), two-person games. Each of these assumptions can be lifted [ 531. A game is a sequence of moves taken sequentially or concurrently by a finite set of agents. Nature is usually treated as a special agent. There are two main (essentially equivalent in power [ 171) representations of games, namely the extensive form and the normalized [ 531 (or strategic [ 17,321) form of a game. The extensive form of a game is specified in terms of a tree; each node belongs to an agent, and the arcs from a node correspond to all of the possible moves (actions) of that agent. A branch from the root to a leaf corresponds to a possible play of the game. Information availability is represented in terms of information sets which are sets of nodes that an agent cannot distinguish. The aim is for each agent to choose a move at each of the information sets. 12 D. Poole/Artificial Intelligence 94 (1997) 7-56 kicker left right goalie left (0.9,O.l) (0.2,0.8) right (0. I, 0.9) (0.9,O.l) Fig. 1. Expected pay-off matrix for the game of Example 1. Payoff (~1, ~2) indicates that the kicker has an expected payoff of UI (where a goal is worth one, and a block is worth zero for the kicker) and that the goalie has an expected payoff of u2 (where a goal is worth zero, and a block is worth one for the goalie). In the strategic form of a game each player adopts a strategy, where a strategy is “a . . .which specifies what choices plan [53, p. 791. This is represented move. [an agent] will make in every possible situation” to the agent’s from information available as a function The initial framework developed here should be seen as a representation form of a game, with a possible world corresponding normalized of a game. In the ICL we add a logic program This allows us to use a logical representation presents a representation that is closer to the extensive form of a game. based on the to a complete play of the play. for the world and for agents. Section 5.5 to give the consequences Where ized strategy. some probability In these cases distribution. there are agents with competing interests, the best strategy the agent decides to randomly is often a random- choose actions based on a problem to prevent in designing for this example, to either jumping soccer playing robots. in some soccer games they will go). Suppose if they both go (to their own) left and the kicker kicks (to its) the goal. The goalie must commit that are tied at the end of regulation In particular we to decide a there are two agents: a kicker who is trying the problem of a penalty kick. Penalty kicks are used Example 1. Consider want to consider winner kick, is trying before they know whether let the other know which direction goalie jumps (to its) goal. Similarly the kicker kicks left and the goalie jumps kicker kicks right and the goalie jumps happen if the goalie The goalie should not reason right”. For then the kicker think should kick left. This is a never ending in game theory its choice. Similarly example goalie time. In a penalty to score a goal, and a goalie who left or right the kicker will kick right or left (of course, neither want to that if the left, there is a 90% chance of a right, there is a 90% chance of a goal. If right there is a 10% chance of a goal. If the left, there is a 20% chance of a goal (this could for this example. jump this) will always kick right. The goalie could then left”. In which case the kicker regress. Such problems have been well studied is to randomize for the goalie its choice. In this 8/15, and best for the from this (realizing that “as the kicker will kick right I should jump is right handed). See Fig. 1 for a payoff matrix [ 531. It turns out that the best strategy in the sense that if either deviate that “I am better when I jump to kick right with probability it is best for the kicker right with probability right so I should the best strategy is to randomize for the kicker to jump 7/15 D. Poole/Artificial Intelligence 94 (1997) 7-56 13 randomized strategy, the other can exploit the deviation to have a higher chance of either scoring a goal or stopping a goal (see Example 22 for a derivation of these numbers). 1.4. Injutrnce diagrams Influence diagrams 1231 (see the papers in [ 341) are a graphical representation of decision problems that extend Bayesian networks to include decision nodes and value nodes. Infuence diagrams provide a perspicuous representation for decision problems making explicit the probabilistic dependencies and the information available when a decision is made (see e.g., [22]) The propositional version of the logic presented here can be seen as a representation for influence diagrams (see Section 3.1), where we can use rules to specify conditional probabilities [ 361, rules to specify utility and the policies of agents are specified as logic programs that imply what an agent will do based on its observations. The ICL allows specification of the influence diagram in a logic that lets us axiomatise the dynamics of the world, derive implicit information from explicit knowledge, and has a formal and natural semantics. The independent choice logic (ICL) preserves the repr~entation~ clarity of influence diagrams and extends them in four ways: The first advance is for representation of structured probability tables. The use of rules, allows for the compact representation of probability tables (similar to the use of decision trees for specifying probability and utility tables [5,52]). For example, although some variable d may depend on variables a, b and c, it may only depend on b when a has one value and on c when a has another value. This asymmetric dependency can be easily expressed in rules, forming a much more compact representation than the traditional tables. The rule structure can be exploited for efficiency [ 5,38,41]. The same rule-based representation can be used to express the policies. The use of logical variables allows for a form of first-order influence diagrams. These form a method for the dynamic construction of influence diagrams [ 9,361. The use of the rule base means we do not have to specify in one step how a variable depends on its parents; we can use arbitrary computation. For example, we can axiomatise the dynamics of a domain and use the ~iomatisation to specify how the position at one time depends on the position at a previous time in a compact way. This features will be exploited for many of the examples. The ICL can also handle multiple agents making decisions, permitting a form of multiple-agent influence diagrams. We are thus importing the representational advantages of influence diagrams into game-theory representations. Note that extending the representation to logical variables and multiple agents increases the worst case computational complexity of deriving optimal plans;3 this is because the problems that can be represented are more complex. It is not a problem with the representation per se. 3 The use of variables makes it undecidable, but even without variables, multi-agent reasoning is exponentially harder than modelling a singte no-forgetting agent 1251. 14 D. Poole/Art$icial Intelligence 94 (1997) 7-56 1.5. Logic Our aim is to define a logic where all uncertainty is resolved by decision/game theory rather has no uncertainty; how uncertainty probability than using disjunction to encode uncertainty. We start with a logic that it is definitive on the value of every proposition. We then show that are chosen by agents or have a as alternatives can be modelled distribution over them. The logic tells us the consequences of the choices. We are treating logic as the modelling an object [ 561, we are using logic at the meta-level describing [ 281 or constraint nets is no other representation structure of the world and the causal structure of agents terms of propositions). 4 language of the world. Rather level in another the logic to represent language than having such as GOLOG the object level. There the causal in (all of which are defined apart from the logic. We use the logic to axiomatise than using disjunction to use probability Rather we want normative under uncertainty decision or game theory arguments to handle uncertainty, and decision to handle as to why we should use probability theory for reasoning [ 491. The aim here is to get as much as we can from logic, but using as in the predicate calculus, the uncertainty. There are and utilities to handle all of the uncertainty. [ 27,45,50] it seems as though Starting a logic without uncertainty potentially in the situation calculus actions lets us sidestep many traditional prob- the frame [ 311 is solved for the case with complete knowledge and nonde- (see e.g., [6] ). This lems, or at least adopt simple solutions. For example, problem and deterministic terministic paper takes quite a different view to other proposals, where we resolve nondeterminism and uncertainty “who gets to resolve the uncertainty”. We consider all these actions as deterministic and have probabilities variables or have them chosen by different agents. This become a very powerful and arguably natural way to model non-deterministic ; when there is incomplete knowledge there are still many problems (see Section 5.4 and [ 401) . but with hidden variables, to be resolved by considering actions, action over 2. Independent choice logic In this section we formalize the independent abstract definition of how an independent logic. In order to make programs under the stable model semantics as the base logic. the paper and examples more concrete we adopt acyclic choice choice logic can be constructed logic. We first give a general from a base logic An independent tion. We assume construction choosing propositions used to determine choice logic (ICL) that we are given a base logic that conforms is a logic built with a specific semantic construc- to some restrictions. The below specifies how to build possible worlds. Possible worlds are built by is choice alternatives. The base logic from sets of independent truth in the possible worlds. 4 All of the logical statements in this paper arc at the object level (i.e., are about the domain being axiomatised than being axioms about the formalism). This was done in order to reduce confusion: we do not need statements here are and the problems of quoting one language. All of the me&level languages rather two different given in English or normal mathematical notation. D. Poole/Art#cial Intelligence 94 (1997) 7-56 15 The base logic is defined on two languages, the language CF of facts, and the language relation k between elements of &Z and elements of in infix notation. We languages which share the same atomic the definition of the semantic construction we discuss what properties is a relation on LF x LQ. It is usually written that languages 13~ and LQ are logical is, k ,LQ of qtreries, and a consequence CQ. That assume formulae. After we want of CF and LQ. Definition 2. A base logic is a triple and b is a consequence relation. (LF, k, CQ) such that LF and LQ are languages Definition 3. An independent choice logic theory on base (LF, b, CQ) is a pair (C, F), where l C, called the choice space, is a set of sets of ground atomic formulae from language LF, such that if x1 E C, x2 E C and x1 # x2 then x1 n x2 = {}. An element of is called an atomic choice. C is called an alternative. An element of an alternative l 3, called the facts or the rule base, is a set of formulae in logic CF. The base logic is often omitted when it can be understood from context. The semantics of an ICL is defined world for each selection of one element using the consequence world. relation in terms of possible worlds. There is a possible from each alternative. The atoms which follow from these atoms together with 3 are true in this possible Definition 4. Given mapping 7, written R(r) called a total choice. independent choice logic theory (C, F), a selector function is a r : C -+ IJ C such that r(x) E x for all x E C. The range of selector function function will be 1 x E C}. The range of a selector is the set {r(x) The basic semantic construction we want for the ICL is that each selector function to one possible world, where every element of the range of the selector :IS true. The facts 3 specify what else is true in the possible world. corresponds function First we define restrictions on the base logic to ensure that the semantic construction gives a well defined semantics: Definition 5. Base logic selector function 7, (LF, k, CQ) and ICL theory (C, F) are dejkitive if for every l if -la is the negation of a in language CQ 5 then for each ground atom a of CQ, either FUR(T) and 3 U R(T) k a or FUR( 7) b -a, and it is not the case that FUR(r) b a f- TZ, and l if o is an atomic choice then 3 U R(T) j- (Y if and only if cr E R(r). 5 If LQ does not contain a negation then the property we need is that the set of atomic formulae /-) (using for example from .FUR( T) completely determines the other formulae that follow from FUR(T). that if F U ‘R(T) b a V b then FUR(T) /-- n or F U R(T) b b. that follow This means, 16 D. Poole/Artificial Intelligence 94 (1997) 7-56 Definition 6. Suppose we are given definitive base logic (C, 3). For each selector language ,Co, and w, is a possible world, we write w, k(c,3) function (&, k, ,Co) and ICL theory in f, read “f is true in world r there is a possible world w,. If f is a formula w, based on (C,.F)“, omitted as a subscript of k. iff F U R(r) b f. When understood from context, the (C, J=‘) is The fact that every proposition is either true or false in a possible world follows from the definitiveness of the base logic. Note that, for each alternative x E C and for each world w7, there is exactly one and w, # LY for all a E that is true in w,. In particular, w, b element of x r(x), x - {r(x)}. 2.1. The languages LF and LQ Languages LF and LQ are logical languages which share the same propositions. The is that we want to impose restrictions on each so that they are reason appropriate they are different for their task. For the rest of this paper we assume corresponding logic with atoms to the set of ground atoms of CF. In this paper we will that LQ is the propositional relating to variables in LQ. We will allow arbitrary logical connectives disjunction, to use the independent negation, etc.) choice in LQ. framework we have that has the property to choose a logic that it gives us a (language LF plus consequence unique model for each total choice. This means relation k) two things: (propositions) ignore issues (e.g., conjunction, If we want l Each selection of an element the logic cannot allow a selection of choices any restrictions the logic from being integrity constraints on choices from each alternative that to impose from other alternatives. This, for example, disallows the arbitrary predicate calculus or even Horn clauses with is consistent. This means from some alternatives [ 271. l Each that we cannot have total choice cannot be extended excludes us from having explicit disjunctions example, tics which may have none or more cluding (whether tion) . extending our semantics three valued models of logic programs into more than one possible world. This for seman- than one stable model.7 We are also ex- the stable model It also means, in our logic.6 logic programs under (e.g., [43] ) from consideration in this way is useful or not is an open ques- 6 Disjunction can be seen as a form of uncertainty. to the choice space, that all uncertainty can be relegated choices. This should be contrasted with other approaches end up with a much simpler alternatives. Whether currently under study. ’ The program a + this is a good (both computationally (1 - -a has no stable models. language, but handle uncertainty by considering different agents getting the logic leaving (e.g., In some sense what we are pursuing here is that idea of the [ 31) that allow both sorts of uncertainty. We to choose idea is an empirical question to give the consequences and ergonomically) lb, b + ya has two stable models, one with a true and one with b true. The program D. Poole/Artificial Inrelligence 94 (1997) 7-56 17 2.2. Acyclic logic programs In order to use an ICL we must commit the language 13~ to consist of logic programs with a unique stable model to be truth in the stable model consequence if q is true in the unique stable model of P. One way to ensure model to a base logic. In this paper, we consider [ 181, and the [ 181. That is, logic program P t- q there is a unique stable to be acyclic the programs is to restrict relation [2]. In this section we give the language and the semantics of acyclic logic programs. The language follows Prolog’s conventions. Definition 7. A variable with an upper case letter. is an alphanumeric string (possibly including “_“) starting A constant or a function symbol or a predicate symbol letter. starting with an uppercase is an alphanumeric string not A term is either a variable, a constant, or has the form f( tl, . . . , t,,) where f is a function symbol and tl , . . . , t,,, are terms. An atom is either a predicate symbol, or has the form p( tl , . . . , t,,,) where p is a predicate symbol and tl, . . . , tm are terms. A literal is either an atom or has the form lcr where a is an atom. (the conjunction A body is either a literal or a conjunction of bodies of PI and & is written as: PI A &). A cZause is either an atom or has the form (Y + p where LY is an atom (called the head of the clause) and p is a body. The latter form is called a rule. A program A groltnd is a set of clauses. term, atom or clause instance of a clause c is a clause obtained by uniformly variables in c. is one that does not contain any variables. A ground terms for the replacing ground Definition 8. The Herbrand base of program P is the set of ground atoms formed function constant if P does not contain any constants). symbols and constants from predicates, instances of the a new in P (inventing integer Definition 9 (see [2]). A logic program P is acyclic positive of ground instances of clauses the atom in the head of the rule is greater appears of a to each element of the Herbrand base of P such that, if P’ is the set to to each atom that then for every rule in P’ the number assigned than the number assigned if there is an assignment in the body. in P, Acyclic programs are surprisingly It just means recursive definitions. general [ 21. Note that acyclicity does not preclude that all such definitions have to be well founded. Definition 10. An interpretation the Herbrand base. Interpretation M every ground atom h, h is true in M is an assignment of true or false to each member of is a stable model [ 181 of logic program P if for if and only if h is in P or there is a rule h +- b 18 D. Pde/Artijicial Intelligence 94 (1997) 7-56 in P’ such that b is true in M. Conjunction in M. A negation Ta is true in M if and only if a is not true in M. a A b is true in M if both a and b are true Note that the negation here is the so-called negation-as-failure [ 111. We can use negation-as-failure does not necessarily in our knowledge [ 371. hold base, although the standard procedural intuition Theorem 11 (see [ 21) . An acyclic logic program has a unique stable model. Acyclicity an agent cannot Section 5.5). is also important condition for the physical realization of our game theory strategies; to do (see it is going that depends on what on a value In some sense the possible world w, is the stable model of 3 U R(T) ; they assign exactly the same truth values to propositions. Example 12. Suppose we have ICL theory with C = { {ui, ~2, Q}, {bl , bz}}, and with 3 = {c +- al A bl, c + a3 A b2, d + al, d +- la2 A bl, e + c, e +- Td}. There are 6 truth assignments: possible worlds with the following WI,, %bl) + ai t= -al w{uz,b,} W{a3,b,} t= Tal W{,,.bz) I= al la2 Ta3 bl Tb:! c d a:! Ta3 61 Tb2 TC -d e e la2 a3 bl Tb2 -c d Te -a2 la3 lb, b2 lc d Te W{azrbz} I= la1 W{asbz) I= la1 Ta2 ag a2 Tas Tbl b2 ~c -d lb, b:! c Td e e (c, d, e). The atomic choices Note that there are two sorts of atoms; atomic choices atoms function selector derived atoms bl, b2) and derived (al,az,a3, that are true in the world are given by the selector the range of the the worlds with truth of the is defined by the rules and the range of the selector function. The function. for the world function), is a world for each selector (here we have subscripted and there 2.3. The multi-agent independent choice logic The independent to model multi-agent different agents choice logic (ICL) specifies a way to build possible worlds. In order In particular we need situations, we need to have more structure. to be able to control different choices. Definition 13. A multi-agent controller, PO), where independent choice logic theory is a tuple (C,3, A, l C, the choice space, is as in Definition 3. . 3, the facts, is an acyclic logic program such that no atomic choice unifies with the head of any rule. . A is a finite set of agents. There is a distinguished agent 0 called “nature”. D. Poole/Arti&ial Intelligence 94 (1997) 7-56 19 from C + A. If controller the set of alternatives = a then agent a is said to controlled by a = a}. Note that C = UaEA C,. suchthatVxECo,~,,,Po(a)=l.* Thatis,for controlled by nature, Pa is a probability measure over the atomic is a function con,troller control alternative x. If a E A is an agent, is C, = {x EC 1 controller(x) Pa is a function UC0 -+[O,l] each alternative choices Often, when theory simply as an independent in the alternative. the context is clear we refer choice logic theory. to a multi-agent independent choice logic The idea is that an agent gets to choose one element it controlled by nature have a probability distribution over them. from each of the alternatives controls. The facts give the consequences ‘The alternatives of the choices by the agents. 2.3.1. Rules for utility Game theory and decision theory are based on the notion of utility, a cardinal value the worth to an agent of an outcome or possible world.9 Higher utilities utility. Finding optimal agents with competing objectives, representing reflect preferred worlds. Agents act to increase strategies becomes but the idea of each agent there are multiple to maximise trickier when trying their (expected) its utility remains. is a function of both an agent and a world. Different and so different utilities in the possible worlds. Note that nature agents have different (agent 0) Utility preferences does not have a utility. The logic program can have rules for utiZity( a, u), where utiZity( a, u) is true in a possible world if u is the utility for agent a # 0 in that world. Definition 14. ICL theory l utility consistent (C, 3, A, controller, PO) is for agent a E A where a # 0 if, for each possible world w,, w, I= utiZity(a, ~1) A utility(a, 242) implies ui = ~2. The theory is utility consistent (other if it is utility consistent than agent 0). for all agents l utility complete for agent a E A where a # 0 if, for each possible world w,, there is utility complete is a unique number u such that w, k utiZity( a, u) . The theory if it is utility complete (other than agent 0). for all agents Thus an ICL theory is utility consistent function for each possible world. and complete means that the utility relation is a We assume that all of the theories are utility consistent and complete. 8 When x is not discrete, we may need to use an integration in this paper that all sets are discrete and finite, although rather than summation. To avoid measurability the framework and integrability is not neces,sarily issues, we assume restricted g The existence of a utility to this case. function, of intuitive axioms about rational preferences, Like most decision and game represent and use to derive optimal actions utilities can be acquired (see e.g., papers in [ 341). and the existence of a probability such that agents distribution is implied try to maximise expected utilities from a set [49,53]. theory practitioners we take the notion of utility as something that we want to is a large body of literature about how these for agents. There 20 D. Poole/Artificial Intelligence 94 (1997) 7-56 Example 15. Continuing Example 12 suppose the rules for utility are: utilizy( agent,, 5) t Te utility(agent, , 0) +-- e A c utility(agent, ,9) +- e A lc utility( agent2, 7) + d utiZity( agent2, 2) + -d that, Note complete if these are all the rules for utility then the ICL is utility consistent and for agent1 and agent2 independently of the choice space and the other rules. The values for the possible worlds (omitting the false atomic choices) are: W{,,,b,} != a1 h c d e utility( agent,, 0) utiZity( ugent2, 7) W{a*,h} k a2 h TC -d e utility( agent,, 9) utility( agent2, 2) W{Uj,b,} I= a3 h TC d le utility(agentI ,5) utility(agent2, 7) W{a,,bz} k a~ b2 yC d le utility(agentl ,5) utility(agent2, 7) W{az.bz} k a2 b:! -C W{aS,b2} k a3 b2 C ld ld e utility( agent,, 9) utility(agent2, 2) e utility( agent, , 0) utility( agent2, 2) 2.3.2. Strategies Given an ICL the single-agent from for chooses the alternatives stochastic where the agent adopts a probability trols. theory, agents adopt strategies. These are also often called policies an agent a strategy can be it con- specify which atomic choices over the alternatives by the agent. case. These In general, distribution controlled strategies Definition 16. strategy for agent a is a function P, : UC, + [ 0, 1 ] such that If (C, 3, A, controller, PO) is an ICL theory and a E A, a f 0, then a VXECn CP,(a)=l. aEx In other words, for each alternative over the atomic choices in the alternative. controlled by agent a, P, is a probability measure Definition 17. A pure strategy for agent a is a strategy for agent a such that the range is (0, 1). In other words, P, selects a member of each element of C, to have of P, probability 0. A pure strategy for agent thus have probability a thus corresponds to a selector function on C,. 1, and the other members Definition 18. A strategy pro$Ze is a function strategies than nature) If (+ is a strategy profile and a E A, a # 0 then a(a) for the agents. from agents (other into is a D. Poole/Arlijcial Intelligence 94 (1997) 7-56 21 strategy for agent a. We write a(a) over the alternatives controlled by agent a. (We also define P&’ = PO.) as P,” to emphasize that CT induces a probability Thus a strategy profile specifies what each agent will do in the sense of specifying a probability distribution over their alternatives. Given the probability distribution over alternatives, we can derive the expected utility, which is the weighted sum of tbe utilities of the worlds (worlds weighted by their probability) : 19. If ICL theory (C, 3, A, controller, PO) is utility consistent and complete, Definition and CT is a strategy profile, then the expected utility for agent a # 0, under strategy profile v is (summing over all selector functions r), where (this is well defined as the theory is utility consistent and complete), and is the probability of world T under strategy profile g, and u( T, a) is the utility ~(cT, T) of world w, for agent a. Note that the expected utility is undefined unless there is a probability distribution over every alternative. In particular, for the multi-agent case, there is no such thing as the expected utility for an agent of a strategy for that agent; the utility for that agent depends on the strategies of the other agents as well. Each agent wants to choose a strategy that maximise its (expected) utility. For the single-agent, finite choice (i.e., a finite number of finite alternatives} case, this definition is str~glitforw~d. Each of their (finite number of) strategies has an expected utility, and so they can choose a strategy with a maximal expected utility. For the multiple agent case, an agent has to consider what other agents will choose, and their choice depends on the first agent’s choice. How to choose strategies has been well studied in game theory [ 17,32,53]. We can mirror the definitions of game theory; for example, we can define the Nash ~uilibrium and Pareto optimal (both of which reduce to maximum expected utility in the single-agent case) as follows: 20. Given utility consistent and complete ICL theory (C, 3, d, controller, Definitim PO), strategy profile (+ is a Nash Equilibrium if no agent can increase its utility by unilaterally deviating from cr. Formally, CT is a Nash equilibrium if for all agents a f A, for all a’ + a then s(a, a,) < if cr, is a strategy profile such that a&a’) = rr(a’) 44 a). 22 D. Poole/Artijicial Intelligence 94 (I 997) 7-56 In other words, no strategy profile crO that is the same as strategy profile cr for all agents other than a is better for a than U. That is, a cannot be better off by unilaterally deviating from (+. One of the fundamental results of game theory one Nash equilibrium for the equilibrium equilibrium to exist. For a single agent is an optimal decision theoretic strategy. [ 17,321. In general you need non-pure is that every finite game has at least strategies (randomised) a Nash environment, in an uncertain Definition 21. Given utility consistent PO), strategy profile u is Pareto optimal if no agent can do better without some other agents doing worse. Formally, CT is Pareto optimal (+‘, if there exists an agent a E A such that E(U, u’) > E(U, (+) then there exists an agent u’ E A such that ~(a’,&) if for all strategies and complete < &(~‘,a). theory (C, 3, A, controller, ICL from game to game theory Other definitions What we are adding to provide the environment, and way that probabilistic Horn abduction assumptions of Bayesian networks). and theory can also be given is the use of a logic program to express a way [ 36 ] can be used to represent in the logic of this paper. the agents the same to model (in independence the independence Example 22. Here we show how to represent Example 1. In the facts we axiomatise utility for both the kicker and the goalie) : and complete axiomatisation is a utility consistent (this utility( kicker, 1) +- goal. utility( kicker, 0) + -goal. utility(goulie, 1) +- goal. utility( goalie, 0) + ygoul. In the facts we axiomatise when a goal is scored: lo goal c kicks(D) A jumps( D) A goulifsamedir. goal +-- kicks( left) A jumps( right) A goul_z~_kl_jr. goal +-- kicks( right) A jumps( left) A goulifkr-jl. owned by goalie, namely Gjumps( right), jumps( left)}, and In C, we have one alternative owned by kicker, namely alternative tives owned by nature, namely: no-goulifkl_jr}, PO (goulifkl-jr) and {goalif-kr_jl, no-goulifkr-jl} = 0.1 and PO (goulif_kr_jl) = 0.2. {goulifsamedir, {kicks(right), kicks(lef)}, one three alterna- (goalifAl_jr, no-goal&same_dir}, with Po(goalifsume_dir) = 0.9, lo The atoms goal_if_samedir, goal_$kl_jr and goaljfkrjl are introduced so that we can have normal logical rules, and independent alternatives. are independent causal hypotheses [361. These D. Poole/Artificial Intelligence 94 (1997) 7-56 23 Suppose that the goalie is to choose a strategy with ps = Pgon/&ump( and the kicker is to choose a strategy with m( = Pkicker( kick( ~~g~~~ ) . In this setup, there are four cases wh:ere goal is true; these cases are exclusive, and so we can sum the probabilities. Thus, right)) ~(@Xd) =pkpgo.9f(1 -pk)(l -pg)o.9+ (l-pk)pgo.l+pk(l -pg)o.2. for each agent The problem utility. So the kicker has to choose pk to maximise goalie has to choose ps to minimize the probability is to choose In a Nash equilibrium, neither agent can improve their probability to maximise the probability their expected of a goal and the of a goal. its expected utility by unilaterally strategy, of the payoffs of the pure the kicker can (otherwise the same values the pure strategy with the higher value). is a linear combination In a randomized its strategies. Take the kicker’s point of view. If there is a randomised the pure strategies must have strategy changing then, as the randomised strategies, improve equilibrium, kicking with pk == 0. These are equal when: the payoff for kicking its utility by choosing right is the above formula with pk = 1, the payoff for kicking right and kicking left must be equal. The payoff for left is the formula p,o.9+ (1 -ps)O.2= (1 -p,)O.9+psO.l. for pg we can derive pg = 7/15. Thus a mixed strategy is when the goalie jumps the only the only time right with probability randomis~ that the kicker would 7f15. Us- the for equilib~~m reasoning, we can show It is easy that to show is when pk = S/15. is a unique Nash equilibrium with pg = 7/15, pk = 8/15. Under there are no pure strategy equilibria. this equi- the kicker has a slight it jumps is slightly worse when thus the probability (which of a goal is 79/150 = 0.52666; should be expected, as the goalie Solving consider ing similar goalie There librium advantage left). 3. Embedding other formalisms in the ICL In this section we show how influence diagrams, Markov decision problems (MDPs) in the ICL. We will show rather and the strategic direct embeddings form of games can be represented of these formalisms. Another embedding [ 361, a -restriction of ICL more resirictions embedding on the rules), of influence diagrams should be noted, and that is that probabilistic Horn abduction as failure and [ 361. The (with only choices by nature, no negation represent Bayesian networks can directly is based on this embedding. 3.1. Representing influence diagrams An influence diagram or decision network [23] is a graphical representation of a decision problem. into a (single-agent) (See Section 1.4.) We show how to translate an influence diagram the policies ICL theory such that there is an isomorphism between 24 D. Poole/Art@cial Intelligence 94 (1997) 7-56 diagram and of the influence utilities equal. We only consider influence diagram can be mapped onto this representation). the strategies of the ICL, with corresponding expected influence diagrams with a single value node (any other Definition 23. An influence diagram is a tuple (N, A, 0, P, U) such that: l N is a finite set of nodes, partitioned of decision nodes and the singleton set {V} containing nodes are drawn as ovals, decision nodes as rectangles diamond. into the set R of random nodes, the set D the value node. Random and the value node as a l A c N x N is the set of arcs such that (N, A) forms an acyclic directed graph If (ni, nj) E A then ni is said to be a parent of nj and nj is a child of ni. is the set of parents of node n. (DAG). Define r(n) = {m 1 (m, n) E A}. That is, r(n) We assume l 0 is a function that the value node does not have any children. from RU D into sets of variable values, O(n), called the frame of that the variable associated with node n can take. We node n, is the set of values extend 0 to cover sets of nodes by O({na,. . .,n,}> = f&no) x ... x f2(n,). l P is a probability function over the random nodes given their parents. That is, for each x E R, P( x = u 1 n-(x) = w) is a non-negative number such that VW c P(x=/J &O(X) 1 T(X) =w> = 1 The probability and w E a( r( x) ) are derived from context. is often written simply as P(x 1 n-(x)) where the values u E O(x) l U : L~(TT( V)) + Iw. U is the utility function that gives the utility for different values of the parents of V. The parents of a random node represent probabilistic dependence (as is a Bayesian [ 351). The parents of the value node represent network the utility only depends on the values of the parents of the value node. The parents of a decision node represent node will be known when the decision available; one value for each parent of the decision dependence; information functional is made. If di E D, a decision function for di is a function Si : fl( r( di)) -+ O(di). If the . . ,8k) where Si is a decision decision nodes are (dl, . . . , dk), a policy is a tuple (St,. function for di. Policy S induces a conditional probability Pa on the decision variables defined by Ps(di I~(411 = 1, if si(r(di)) = di, Suppose R U D = {XI,. . . , x,}. The joint distribution given policy 6 is: PS(Xl9. .*,Xn> = n P(Xi I r(Xi)) X n PS(Xj 1 T(Xj)) X&R X,ED (what is meant by Pa should be clear from context). D. Poole/Art@cial Intelligence 94 (1997) 7-56 25 ta d Fig. 2. An influence diagram. The expected utility of policy 8 is given by X,.....X” X&R XjED where we are summing over all of the values of variables XI,. . . , xn. ta and d, Example 24. Fig. 2 shows an influence diagram with two decision nodes four randlom nodes a, as, b, bs, and one value node utility. The intuition for this diagram is that there is one decision d to be made that depends on a and b. bs is a noisy sensor for b and as is a sensor for a that can be controlled by ta. Associated with the influence diagram and the conditional probability (not shown in the diagram) is a frame table for each random variable given for its tables for the random nodes are a major source of complexity in the number of parents of the node. each variable, parents. These probability as their size is exponential Suppose the frames are as follows: 26 D. Poole/Artijicial Intelligence 94 (1997) 7-56 fl( tu) = {high, low} f2( a) = {low, medium, high} O(as) = (PO& neg} Q(b) = {PO& neg} iI = (pas, neg} fin(d) = {dl,dz,d3} There are ten probability assignment and two for bs. of values distributions to be assigned; one for a, six for as (one for each to a and tu, such as P(us = pos 1 a = low A ta = high)), one for b The mapping of an influence diagram l Random variable xi has value vi is represented l Random variable Xi with ki parents xi,, . . . , xi4 is represented into a (single-agent) ICL theory is as follows: l1 as the proposition xi( ui). l2 as a rule and expo- nentially (in ki) many alternatives. There is one rule: Xi(K) +Xi,(Xl) A”‘Axi,,(Kkl) ACi(XtK~9...,Q). For each assignment . . . x fl( Xi,, ) there is an alternative controlled by nature: of values to the Xij, that is for each (Ui],. . . , uik,) E L?( Xi1 ) x {Ci(ul*uig>. . .,uit,),...,Ci(u~,uil,...,ui~,)) where n(xi) = {ut , . . . , u,.}. The probability of each atomic choice is the value of the corresponding probability: conditional PO(Ci(~lv uil T. . .,Uiti))=P(Xi=lll (Xi, =Ui~,.s.,Xik, =Uiz+). on the right-hand that under probabilities diagram. Note as there are rows in the probability The conditional influence alternatives are provided. of probabilities In many cases the probability this occurs when some parents are irrelevant variables [ 71. this mapping side are provided as part of the there are the same number of tables for Xi, and the same number can be represented more compactly. in the context of values In particular to other l Value node with parents Xi,, . . . , Xiti is represented as a rule of the form: Utikty(Ugent, U) t Xi, (LJ~,) A . . . A Xiti(Uik,) for each with chance nodes, than this. compactly (Oil, . . e,U&) E fl(Xi,) X "' x f2(Xiki), where u = U(Ui,,. in many cases the value function AS can be represented more . . Toi,). ” The mapping Horn abduction ‘* We have not used the standard probabilistic different by equality, namely for random nodes [ 361. that two terms denote the same object. is the same as the representation of Bayesian networks in probabilistic notation of xi = U; because logicians usually mean something D. Poole/Art$cial Intelligence 94 (1997) 7-56 21 l Decision variable xi with ki parents xi,, . . . , xiii is represented as a rule and exp~~nenti~ly (in ki) many alternatives. There is one rule: Xi(F) +-Xi,(Q) A***nx,i(K*i) ACi(ti,K,,...,vl,). For each (ui, , . . . 9 Uix,) by the agent: E fJ(XiL > x ’ . - x L?(xj+) there is an alternative controlled {Ci(Ul,Uilt-. * ,uik,),...,Ci(uT,uil’***‘ui~~)) where L!(xi) = (~1,. . . , u,}. Just as the influence diagram policy has to choose a value for each value of the parents we have to choose a value for each alternative. There is a one to one mapping between the alternatives and the values of the parents of a node. Example 25. Continuing Example 24, with the influence diagram of Fig. 2, variable tu has no parents, therefore there is one value to be chosen. This can be represented as having {t,z( hi), ta( low)} E Ct. There are 8 independent choices to be made for d (one for each assignment of values to its parents). This can be represented as the rule: d(DV) +-- ta(7-V) A as(AV) A bs(BV) A d~#es(D~~A~BV) with {d~7es(dt,~AYBV),d~~es(dz,~AYBV),dd~es(ds,~AYBV)) E Cl for each value of TV, AV, BV. Theorem 26. Given an influence diagram ID and the corresponding ZCL theory, dejined by the matpping above, there is a correspondence between the policies of the injuence diagram and the pure strategies of the ICL theory. The corresponding policies and strategies have the same expected ~ti~i~. Proof. A policy of an influence diagram specifies a decision function for each decision node. Each decision function is a function from the values of the parent to the values of the nodes. A decision function, S; corresponds to the selection of from the corresponding alternative. It is easy to see that different policies correspond to different s.elections, and that different selections correspond to different policies. The expected utility of the influence diagram policy S is: e(S)= x xt,...,x,, n .riER P(Xi 1 d&)1 x n pS(xj IQdXjl) X u(4V)) XjED = x n pO(Ci(Xiv~(Xi))) X r]: f?(Cj(X,j,71.(xj))) x u(~(V))r x,....,x. xiER .TjED where ci(.Xi, V( Xi) ) has the obvious mining, policy. This is the expected utility of the PHA theory for the same policy. and Pt is the probability induced by the 28 D. Poole/Artificial Inrelligence 94 (1997) 7-56 3.2. Markov decision processes Markov decision processes [44] are models of single-agent decision problems where a notion of state conveys all of the information history. stochastic sequential about the past A Markov decision process is defined function P(sl 1 so, a) which specifies in terms of a set S of states, a set A of actions, that s1 is the the probability from carrying out action a in state s, and a reward function R( so, a, q ) the reward obtained when action a is carried out in so and the resulting a state transition state resulting that specifies state is ~1. A stationary policy is a selection of an action for each state; what the agent does at any time depends on the state. We can represent the choice function for an agent, assuming we want stationary policies, as: is where do( A, S) is true if the agent will do action A in state S and A = {al,. the set of available actions. The agent gets to choose what it does for each state. If we want a non-stationary the policy depends on the time or stage), we add a time parameter to do. policy . . , a,} (i.e., We also axiomatise the state transition function, which specifies how states transform under actions: state( S’, s(T) ) +- state( S, T) A do( A, S) A st_trans( S, A, S’) , where state( S, T) is true if the system true if action A transforms state S into state S’. This is a stochastic transition: is in state S at time T, and st_trans( S, A, S’) is VSVA {st_trans(S,A,sg),...,st_trans(S,A,s,)}ECo, where {SO, . . . , s,} is the set of all states. Note that these are alternatives nature. PO (st_trans( S, A, S’) ) is the probability out action A in state S. controlled by that state S’ will be the result of carrying A reward function can be defined in terms of rules of the form reward( ri, T) c state(si, T) for each state Si and for some number ri. We typically do not want to write Markov decision processes by explicitly the size of the probabilistic to the states, but instead want to divide the state into propositions [ 121. This can reduce reduced distributions gain; Boutilier computational iomatisation further by the use of rules; concisely. This concise et al. [5] exploit gain for MDPs. The ICL representation in logic, with a well defined semantics, the rule these allows us to express structured probability for computational specification can be exploited (or tree) structure of probability for also allows for the concise ax- of the dynamics of the system. tables assessment necessary. This can be referring (or random variables) D. Poole/Art$cial Intelligence 94 (1997) 7-56 29 to Kanazawa [24], but incorporates a particular, and we claim useful, is similar This probability independence. the structured MDP example of Boutilier logic. Essentially we can convert rules for each action (which is exactly the trees et al. [5] into rules, but the frame prob- Example 27. Let us axiomatise choice in the independent we do not need separate lem [31]). In this example, is at the office (as opposed umbrellaiT), robot has coffee; and uhc(T), the robot there are six state propositions: Zoc_ofs(T), the location of the robot is wet; the to being at the caf6) at time T; wet(T), is carrying an umbrella; it is raining; &c(T), ruining(T), the robot the user has coffee. There are four actions: go(T), go to opposite location; buyC( T), buy coffee; delC( T), deliver coffee; and getU(T), get coffee. We can specify the dynamics using logic, for example, the following clauses define wet and hcu: wet(T+ 1) t wet(T) wet(T + 1) + go(T) A ruining(T) A ~umbreZZu(T) hcu(T + 1) +-- h(T) hcu( T + 1) c deZC( T) A 4cu( T) A loc_o#( T) A hcr( T) A deECsucceeds( T) where VT {deZCsucceeds(T), delCfuiZs( T)} E Co, and VT Po(delCsucceeds(T)) We can also define the reward function using rules: = 0.8. rewurd( 1.0, T) t /KU(T) A lwet(T) rewurd(0.9, T) t /KU(T) A wet(T) rewurd(0.1, T) c +zcu(T) A Twet(T) rewurd(O.O, T) +- -&cu(T) A wet(T) For finite horizon problems, the value can be specified in terms of rules. For example, vulueto( R + U, T + 1) +- rewurd( R, T) A vulueto( U, T). vuzutrto( 0,O). where vul’ueto( VT) is true if V is the sum of the rewards up to time T. For infiinite horizon problems, it is not so simple. You could imagine writing, for the discounted reward function [ 441: vuluc?( R + U x y, T) +- reward( R, T) A vulue(U, T + 1)) where y is the discount does not terminate. the specifcation specification, atic. factor. However, such rules are problematic It is probably better to define the value external of the value as is traditionally function done separate from in MDPs, does not seem as the recursion to the logic. Having the other parts of the problem to be too problem- 30 D. Poole/ArtQ5cial Intelligence 94 (1997) 7-56 3.3. The strategic form of a game The most direct connection of the ICL is to the strategic form of a game (see Section 5.5 for a comparison to the extensive form of a game). The strategic form of a game [ 17,321 is a tuple (A, .X’, U) where l A is a non-empty set of players (agents), 0 2 is a function from agents into non-empty sets (of pure strategies). Thus _Z( a) is the set of all pure strategies for agent a. 0 u is a function @:A--+ (G2A2(a) --+lR). Suppose A = (~1,. . . a,,} and a E A. u(a) is a function that given an n-tuple of strategies, one for each agent in A, returns the utility for agent a under this strategy profile. Thus 24 (a) ( (go,, . . . , CT,,)) where gai E .X( ai) is the von Neumann-Morgenstern utility for agent a when each player ai chooses strategy ohi. The general idea is that each player chooses a strategy which specifies what it will do under all contingencies. Following a complete play (specified by each player’s strategy) each player receives a utility. Note that there are two different forms which we treat as the same here. One is where nature is not an agent, and all of the payoffs are expectations (averaging over nature’s choices). The second is where nature is a player, and has a probability distribution over its strategies; this is called the Bayesian form of a game [ 171, where the players have partial information about nature’s choice. The private information about nature’s move is called the player’s type. The Bayesian form of a game assumes that each player chooses its strategy after it learns its type. Such a distinction is beyond the scope of this paper, as we do not consider how or when strategies are computed (for example, whether they are computed online or offline). The ICL can be seen as a particular representation for the strategic or the Bayesian form of a game. The set of agents is the same. We divide the space of strategies for players into independent choices (i.e, we allow more structure in the strategy space) and use a logic program to axiomatise the u function. There is a direct mapping of the strategic or Bayesian form of a game into an ICL theory: strategic game (A, z‘, U) is mapped into the multi-agent ICL theory (C, F, d, controller, PO), where A is the same set, C is the set {{ffo(ai,a,i) 1 ani is a strategy for aj} 1 ad E A}, where do( ai, ohi) is an atom that says that agent ai is adopting strategy a;, (all we need is a name for each strategy), controller is the function {do( ai, a,,), . . .) I-+ ai, PO is the probability distribution over types (for the Bayesian model of a game) and 3 is is set of rules of the form utiZity(a, util) +- cio( at, oh, ) A . . . A do( a,, CT+), where util = u(a) ((a,,, . . . , a,,,)). D. Poole/Art@cial Intelligence 94 (1997) 7-56 31 This mapping has trivialised the fact that there is a lot hidden in the structure of the strategies. We have assumed we can name the strategies and say what follows from them. For simple games we may be able to, but for most realistic situations we want to be able: to specify the choices at a lower level of detail, and be able to control the selection of components of strategies. The consequence of a number of different agents choosing strategies should involve reasoning about the building blocks of the strategies (what is done under what circumstances), and reasoning about the dynamics of the domain to determine the consequences of the actions. 4. The dynamic ICL The ICI, presented so far is only good at representing problems where the the decision problem c.an be statically expressed (even if the problem to be solved involves dynamics and change). Like the strategic form of a game, the building blocks of the strategies have to be constructed ahead of time. For example, for the influence diagram mapping we had to create an alternative corresponding to each assignment of values to parents, rather than creating the appropriate rules for defining the policy on the fly. There are a number of problems with this: l What the agent will do (or attempt to do) is buried within the representation. The alternatives are at a lower level than the choices faced by the agent; they spec:ify what the agent will do under each contingency. While we can represent the dynamic structure structure of reasoning and acting, the fo~alism presented so far gives us no help in doing so. l We have to create an alternative for each independent choice that the agent could make; that is, we have to a priori divide up information states for the decision. The problem is that the a priori division needs to be at the finest level of detail. For available when example, al~ough some decision d may have much info~ation the decision is made (in the influence diagram d may have many parents), the specification of what the agent should do may not require all of the distinctions of the information state. There may be a more concise encoding of the policy. Just as we have used rules as a concise specification of probability distributions, we may like to express the policy for an agent as rules. o We may want to create alternatives on the fly; what options are available to an agent and what information it knows may depend on the context, and it may be more economical to create alternatives as needed, rather than having to anticipate all of them as part of a strategy. l We want to reason about the program the agent used to compute an action rather than just the action itself [48]. We want to build a representation upon a more natural specification of dynamic systems. We will extend the ICL to the dynamic ICL logic that is slightly more complicated, but arguably more natural. We model the dynamics of the world rather than the structure of the choices. The dynamic ICL is more like a representation for the extensive form of a game than a representation for the strategic form of a game; it will tell us how to construct 32 D. Poole/Artificial Intelligence 94 (1997) 7-56 the appropriate game/decision the advantages networks, variables. the ability of the ICL, namely, to represent tree (see Section 5.5). This will be done without the embedding structured decision of the independence tables, and losing of Bayesian the use of logical We build the theory upon a general model of agents in an environment. as it places the ICL within a wider theoretical context, and introduces interacting This is important the notions of traces, transductions, state and sensing. 4.1. Dynamical systems [ 13,291 is common in many areas of science, from Modelling dynamical engineering systems to economics mechanical to ecology. We assume a time structure 7, that is totally ordered and has a metric over intervals. time. in this framework.) A trace is a for this paper we will consider discrete for a development of continuous or discrete; time 7 can either be continuous (See function from 7 [39] into some domain A. A transduction is a function the output at some time in the sense that the output at time t can only depend (i.e., agent will be a specification of a transduction. Transductions of dynamic adequately handle the case of nondeterministic agents and dynamic agents. l3 in general systems from (input) traces into (output) traces that is “causal” in inputs at times t’ where t’ < t is a function of the input history up to that time). An form a general abstraction [39,47,57], although they do not The state of an agent that needs is that information to be a function of the state and the current to be remembered in order for inputs. At one extreme a state the entire history of the agent. At the other extreme an agent can have no the output can contain state and just react to its current inputs. 4.2. Agent structure So far we have modelled agents by naming them and specifying which choices that makes they it of agents that It helps to do more than this; we want to provide some structure control. easy to model actual agents. Not every logic program and set of assignments to choices will make sense. Agents have input and outputs; they have no access to, and some internal values that only they can access. there are some values We will model agents as a logic program that specifies how the outputs are entailed [ 391. This logic program can use the internal values and sense values the agent has no access to (i.e., cannot sense or otherwise by certain inputs but cannot use those values determine). l3 With deterministic nondeterministic memory), agent with no inputs composition, step. agents, only the input history inhabit an environment with other if they is needed. Nondeterministic need to be able to recall to implement agents, or if they have (competing) their inputs as well as choice commitments made. For example, (a; b)((c;d) where “I” is nondeterministic agents (agents need to be limited for an choice and “;” is sequential in the first time it chose at the second time step the agent has to be able to recall what D. Poole/ArttjMd Intelligence 94 (1997) 7-56 33 In modelling agents we have to be careful about a number of things: What are the inputs and what are the outputs? When we have-noisy sensors and actuators with slop and failure, we cannot condition on the values in the world, but only on what our sensors tell us. We have to be able to model what an agent can observe (and how it relates to the world), and what an agent controls (and how it relates to what the agent actually does). We have to make sure that the agent can actually carry out the policy specified for the agent. An executable policy cannot depend on events the agent cannot observe, or are not under the agent’s control. The logic programs are models of the agents. They are not the agents themselves. We want to be able to model many different sorts of agents, both natural and artificial. We want to be able to model agents that are implements as simple language, for example. We want logic circuits or in some traditional programing to use the same language to model agents we design, agents that our agent may encounter, and the environment. We also want to be able to model how long the agent will take to execute an action (including the time to execute the program to choose the action) [483. This does not mean we could not run this specification to make an agent (but it will be a different agent, for example, if we forward chain on the axioms than if we backward chain on the axioms; they will have very different timing properties). We distinguish the “controller” and the “plant” of an agent [ 131. (See Fig. 3.) The controEZer is that part that we have to optimize; it receives digital signals (“‘observa- tions”) and outputs digital “controls” or “actuator commands”. The plant or body is the physical embodiment of the agent that includes input devices such as cameras, micro- phones, radio receivers as well as wheels, limbs and transmitters. The plant receives “percepts” from the environment (e.g., sound, light, radio signals), and sends observa- tions to the controller. The observations are usually correlated with the percepts received, but are typically not identical as sensors are noisy. The plant also receives controls from the controller and makes actions in the environment (e.g., actually moving, sending messages). Multiple agents will all interact through the environment; the only way for agents to communicate is through the environment I4 and they a11 act in the environment (as in Fig. 3). There may be many agents, all of which carry out actions in the environment and receive percepts from the environment. As far as8 the outside world (including other agents) is concerned, an agent receives percepts such as messages, light, sounds, etc., and performs actions such as moving limbs, sending messages. Thus other agents will tend to group the controller and the plant together as “the agent” (or “the robot” for physical implementations). As far as the controller is concerned, it can group the plant and the environment together. It receives observations, and outputs controls. The distinction between the plant and the environment is essentially arbitrary; we usually make the distinction because we often build controllers for particular plants, but for more general environments. While I4 If there is some form of direct communi~tion This makes modelling more uniform and allows us to model noise and failure channel, then this is also modelbd as part of the environment. in communication. 34 D. Poole/Artificial Intelligence 94 (1997) 7-56 agent ,,,,,,,I act uat oTTlervat ions 14 IA agent2 agent3 Fig. 3. An agent acting in an environment. between the distinction outside observer, when building order to construct a controller. [ 1, 10,55], but this is beyond the controller and the plant may seem an agent we have to commit to be arbitrary to a particular division to an in and plants (Typically we want a hierarchy of controllers the scope of this paper.) the controller’s the rest of this paper we take receives definitive observations a For to the plant. The plant controller will be modelled in observations (observations may not always reflect what is true in the environment) will be modelled in terms of rules that depend on the percepts as well as nature’s choices. point of view; we assume as part of the environment. In particular, uncertainty and can issue controls 4.3. Agent specification module The agent specification module the inputs will and the outputs will be controls. The agent will be able to condition on and will need to choose values for the controls. takes the controller’s point of view; be observations the observations specification modules will allow us to modularise computer Agent science common modular program design. We will generalise the use of agent specification modules; decision problems. like information techniques our knowledge, and use hiding, abstract data types and ICL” through of representation this will allow a more concise the ICL to the “dynamic D. Poole/Artificial Intelligence 94 (1997) 7-56 35 Definition 28. An agent speci$cation module for agent a # 0, written ASM,, (C,, 0,) T) where is a tuple l C, i:s a set of alternatives ator commands commands for the agent; in each element of C,. controlled by a. This will be the set of possible actu- to “do” one of the actuator the agent can attempt l c3,, the observables, are called observation alternatives; elements of observation atomic observations. is a set of sets of ground atomic formulae. Elements of 0, are called alternatives l T, the observable function, agent decides which element of alternative x E C, to choose, one atomic observation r(x) of /1’. r(x) diagram. is a function T : C, -+ 20a. The idea is that when the it will have observed Elements of to the agent when it has to choose an element in an influence available to the parents of a decision node x are the information corresponds from each observation alternative in r(x). The following definition mirrors the analogous definition from game theory: Definition 29. Agent a has pelfect its previous observations of C, are totally ordered and if XI E C,, x2 E C,, x1 < ~2 then XI E 74x2) all of that the element and recall (or is no-forgetting) actions. Formally if it remembers and previous this means T(XI) c rTT(x2). A dynamic ICL theory consists of an agent specification module a logic program, plus stochastic choices: choices to axiomatise what follows for each agent, and the agent’s from Definition 30. A dynamic PO, ASM) , where independent choice logic theory is a tuple (A,&, 30, l A is a finite set of agents containing l CO, nature’s choice space, is a choice space with alternatives l &, controlled by nature, is a logic program such that no atomic choice unifies with the agent 0 called “nature”, a distinguished .nature’s facts, head of any rule, 0 PO is a function UC0 -+ [0, l] such that V,y E CO CcrEX Po(cr) = 1, l ASM is a function on A - (0) such that ASM, is an agent specification module for a.gent a, that & such VO E r(,y), V’a’ E 0, a’ < a in the acyclic ordering. That ordering where the actions are after their corresponding is acyclic with an acyclic ordering where Vu E A, Vx E C,, Vcz E x, is an acyclic observables. is, there Note that (& CO, PO) will correspond strategy for nature (see Definition 37). It is a specification of what choices nature will make. This specifies the stocha:stic dynamics of the system. to a particular (stochastic) We have to make sure that the observables for each agent really cover the possibilities and really are alternatives. 36 D. Poole/Art$cial Intelligence 94 (1997) 7-56 ICL theory (d,Co,Fo, Po,ASM), set r of ground is non-exclusive if there exists a choice function (~1 # LYE such that R(r) UFii b a1 A (~2. Otherwise if for every choice function r on UaEA C,, there for- r on lJaEA C, and there exists is exclusive. that is cx E r such r Definition 31. Given dynamic mulae at E r, cry2 E r, r is covering R(7) u & t_ cy. For example, every choice alternative is exclusive and covering. A less trivial example is exclusive and covering. Every set of the form is: given C = {{a, b}, {c, d}, {a, la} {e, f}}, and F = {g c a, h c b A c, i c b A d}, {g, h, i} is an exclusive and covering set. If g c e were added to F then {g, h, i} would no longer be exclusive. Definition 32. A dynamic every element of 0, ICL theory ing. ICL theory is exclusive with respect is observation consistent if for every a E A, ICL theory. A dynamic is cover- to the dynamic is observation complete if for every a E A, every element of 0, require a theory The above definitions are to make sure that we can treat the elements of 0 as random variables. Unlike elements of C, they are not exclusive and covering by definition. We consistent, but, when we have negation will always as failure complete (there may be an extra, unnamed element of each element of 0). Note that observation consistency but then we cannot exploit is not a severe restriction; we can always make 0 a set of singleton the structure of observations. [ 371, we will not require to be observation to be observation in the logic the theory sets, suppose we have {high, medium, low} as an observation to never allow choices of the agents that a strategy can be specified as a function to entail both high from this set into Example 33. For example, alternative. We want the theory and medium. This means actuator settings. (medium, lmedium}, exclusive and covering), cross product of these into actuator settings. also have to cover the case Thigh A Tmedium A 4ow and {low, llow} If these values were not exclusive, we could make sets {high, digh}, into observation (which but this would mean a strategy would be a function are each from the If {high, medium, low} is not covering, we alternatives in defining a strategy. The general idea is that the agent will always observe one element of each member and the observa- is of 7r( x) before choosing one element of x. The acyclicity tion completeness possible. this temporal ordering and consistency above ensure requirements restrictions In this paper we assume all our theories are observation consistent and complete. 4.4. Pure strategies A pure strategy is represented is a specification of what an agent will do based on what it observes. on the logic it does not have as a logic program. There are restrictions on values to which condition to ensure an agent cannot This strategy program access. D. Poole/Arti$cial Intelligence 94 (1997) 7-56 31 Definition 34. pure strategy for agent a is a logic program 30 such that: If (C,, O,, 7r0) is an agent specification module for agent a E A, then a there function is observed, the decision. rrcX) on r(x), the logic program specifies what the agent will do. is a (Y E x that is true in the unique stable model of 3a U R( 7,~~)). That is, 30 is acyclic with an acyclic ordering such that, for every x E C,, every element of each element of rrO( x) is before every element of x. That is, the agent can observe before making For every x E C,, and for every selection unique whatever The heads of rules in 30 only unify with either local atoms (that do not unify with atoms in the agent specification module of any other agent or in 30) or members of choice alternatives in x0. Thus 3Q can only be used to imply alternatives owned by agent a, perhaps with some local atoms as intermediaries. For each x E C,, the only formulae prove an element of x are: elements of members of V(X), whose definition built from to compute that the agent cannot observe or otherwise compute. that can appear in the bodies of rules in 3a to local atoms and atoms formulae (and these). While we do not want to restrict the complexity of programs the choice from an element of x, the choice cannot depend on values does not depend on the choices of other agents Thus a strategy for an agent is just a program to specify what the agent will do based on what information it receives. Definition 35. Given a dynamic one (pure) program J ‘F = UaEA 30 that specifies what each agent will attempt is a selection of strategy for each agent (other than nature). Thus a strategy profile is a logic ICL theory, a (pure) strategy profile to do. from each alternative There are two (equivalent) ways to define the semantics. One it to have a possible world for each selection of an element controlled by nature, and to have 3 specify what is true in each world. In this case, the probability of a world is of the strategy, but a strategy profile specifies what is true in each world. independent is to have a possible world for each selection of one element The second from each In this case, what is true in a world does not depend on the strategy profile alternative. of a world does. The second has many possible worlds with chosen, but the probability the first zero probability method in the first scenario. We will define that were not created formally here. Definition 36. is a pure :;trategy profile, If dynamic ICL theory (A, Co, 30, Po,ASM) is utility complete, and 3 then the expected utility of strategy profile 3 for agent a is e(u9.3) = Cp(7) x U(7,U,3) (summing over all selector functions r on Cc) where u(7,6r,3) =u if R(7) U3kutility(a,u) (this is well defined as the theory is utility complete), and 38 D. Pde/Art@cial Intelligence 94 (1997) 7-56 P(7) = J-J Po(dx)) xeo ~(7, a, 3) probability is the utility of world w, for agent a under strategy profile of world 7. .7=. p(7) is the 4.5. Stochastic strategies As in the goal kick example above, it is often desirable for agents strategies. that a random strategy In this section we define random (stochastic) strategies. The general is a probability distribution over pure strategies. to adopt random idea is If (C,, Oa, TV) is an agent specification module for agent a E A, then a Definition 37. strategy for agent a is a tuple (3,) CL, Pa) where CL is a choice space whose (stochastic) atomic choices do not appear outside of this strategy, P, is a probability distribution over each element of CL (i.e., ‘dx E CA, V’a E x, 0 6 P,(a) < 1 and CaEX P(a) = l), and F is a pure strategy (the element of ‘R( ra) will be local atoms in the strategy). is a logic program such that for all selector functions r, on CA, .F U R( 7,) ({{q,down, r( {up, Example 38. Suppose down, left}) = {{high,medium, for agent a. That is, a can do one of {up, down, left}, and when it has to act, it will know which of {high, medium, low} left)}, {{high, medium, low}}, T), where is an agent specification module strategy could have facts: is true. One stochastic low}} up +-- high A uh, down +- high A dh, left t high A Zh, up t medium A urn, down + medium A dm, left t low, choice space CA = { {uh, dh, Zh}, { um,dm}}, 0.2, P,(Zh) =O.l, PO(um) = 0.6, and P,(mh) =0.4. and P, given by P,(uh) = 0.7, P,(dh) = strategy for each (i.e., If u is a strategy profile, .Fz to be the first component of ~(a), Cz to be the second component of a( a), Definition 39. A strategy profile agent for each a E A, a(a) define and P,” to be the third component of c+(a). (+ is an assignment is a stochastic of a stochastic strategy). It remains to define the expected value of a strategy profile. and (A,&, Definition 40. complete, and u is a stochastic strategy profile, then the expected utility (+ for agent a is is utility consistent .Fa, Po,ASM) If dynamic theory ICL c(a,g) = CP( 7,gl x 47,a,(+) D. Poole/Art@cial Intelligence 94 (1997) 7-56 39 (summing over all selector functions T on Co U lJaEA C,“) where ~(7,CZ,G) =u ifR(7)U3ff~Vt~ility(a,u), where 3” = UaEA 3: and (u is well defined as the theory is utility consistent and complete), u (7, a, 3) probability of world 7 under strategy profile (7. is the utility of world w, for agent a under strategy profile 3. p( T, a) is the 5. Discw&on In this section we discuss some modelling issues for the dynamic ICL. We first discuss how to model information-producing actions, how to model noisy sensors and actuators, what it means to execute a stochastic strategy, and finally the relationship to the extensive form of a game. Section 6 presents some detailed examples. 5.1. Passive sensors and information seeking actions The observations represent passive sensors that (at each time) receive values from the environment (one value from each observation alte~ative). We also do not distinguish between information-producing actions and actions that “change the world”; there is only one t.ype of action. The nature module will specify the consequences of doing an action. We can model “information-producing actions” by having actions whose effect to make a sensor have a value that correlates with some value in the world. For example, the information producing action “look” may affect what is sensed by the eyes; if the agent does not “look” they will sense the value “nothing”, if they do look (in a certain direction) they may sense what is in that direction. Of course the “look” action may be unreliable (the lights may be out), and it may take an arbitrary amount of time to achieve its effect (as in a medical test). What is also important is that the agent can only condition on its sense values or on values derived from these. The agent cannot condition on what it has no access to (e.g., the true state of the world). Similarly, the agent can only control what message is sent to its actuators; what it actually does may be quite different. Section 6.1 gives a detailed example and discussion of modelling passive sensors and info~ation seeking actions. 5.2. Noisq sensors and actuators There is a straightforward way to model noisy sensors and actuators. This follows the distinction between the plant and the environment depicted in Fig. 3. The general idea is to axiomatise how the observations are a function of the percepts plus noise. Similarly 40 D. Poole/ArtQicial Intelligence 94 (1997) 7-56 we can axiomatise how the actions of the agent are a function of the controls plus noise (for slop, errors, slippage, etc.). We can divide up the noise that the sensor into systematic errors (e.g., broken, and always makes the same error) and intermittent noise (the independent for each reading), checking and a continuum road speeds on a highway in between. For example, consider [ 161: is actually error for a sensor Example 41. In this example we show how to axiomatise a noisy sensor that can break there is some “normal” error down. The sensor from the true reading. it produces some reading at random (independent If the sensor of the actual velocity). is either working or not. If it is working is not working sense_velocity( V + DYT) + velocitysensor_OK(T) A velocity( YT) A normal_error( DYT). sense_velocity( EYT) +- ~velocitysensor_OK(T) A error_reading( DVT) . We need errors are discrete to have a probability in lOkm/h distribution over the normal errors. For example if the steps, we can specify something like: ‘dT {normah-ror( DYT) 1 DV E { -30, -20, -10, 0, 10,20,30}} E Co Po(normaZ_error( -30,T)) = 0.01 Pa (normal_error( -20, T) ) = 0.03 Po( normal_error( - 10, T) ) = 0.06 PO (normaZ_error( 0, T) ) = 0.8 Po(normal-error( 10,T)) = 0.06 Po( normal-error( 30, T) ) = 0.03 Po( normal_error( 30, T) ) = 0.01 Similarly we can define error_reuding which provides error readings. There is nothing distribution, that here as the mathematics sets of speeds rather than the speed themselves. such as a normal that prevents us from having a non-discrete distribution over errors. We have not presented is more complicated; we would need to consider measurable a probability distribution over in principle (Gaussian) Whether the sensor is working time working at some other time. We need to axiomatise we need to specify time; suppose it is (i.e., distribution over time). The sensor can break at any it had been working of whether the dynamics of sensor failure it has a 2% chance of breaking at any time when the probability independent at some is not D. Poole/Artijkinl Intelligence 94 (1997) 7-56 41 and a 5% chance of being fixed up when it was broken (we could also axiomatise a more complicated dyn~ics of how the sensor can get fixed, but this will show the main point). ver’ocitysensor_OK can be axiomatised as: veloc,itysensor_OK( T f 1) +- velocitysensor_OK( T) A ~veloci~~e~sor~~a~( T) . velocitysensor_OK( T + 1) +. melocitysensor_OK( T) A ve~oci~~ensor_~ed( T) . VT {velocity_sensor_breaks( T) , velocitysensor_remains_OK( T) } f Co VT { velocity-sensor-jixed ( T) , velocitysensor_remains-broken ( T) } E CO = 0.05. In with PO (veloci~~ensor~reu~ addition, we need to define the initial value of ve~oci~~ensor_OK, for example as a member of an alternative controlled by nature. (T) ) = 0.02 and PO (ve~oci~~ensor-bed) 5.3. Executing a strategy What does it mean for an agent to execute a s~ategy? If the strategy for the agent is pure, then it will tell the agent what to do based on its sensor values. The agent will “do” .the unambiguous actions that are entailed. If the strategy for the agent is not pure, then there may be a number of things that the agent could attempt to do based on its inputs. To follow a strategy it should pick the actions randomly according to the dis~ibution specified in the strategy. Picking strategies at random does not mean that there must be a random number generator in the robot (or access to some really random quantum phenomenon), although it could. For example, for the soccer playing robots we could compile in the randomness, by choosing offline whether it should go right or left according to the random strategy, If we did this then we would have to hide our design from our opponent designer and replace our robot after a single penalty kick. An alternative would be to have two non-random robots, where we choose one at random for each penalty kick. These two robots, together with the choosing mechanism, can be seen as one randomizing agent. The most important property is that the other agent is not able to predict what our agent will do. It is silly to think of an agent cheating, by not choosing from the random distribution. This is particularly the case when we consider that the agent gets to choose whichever strategy it wants. Cheating with one strategy is the same as choosing a different strategy. We will then consider it to be that strategy that the agent is carrying out. This does not mean that an agent cannot lie about what strategy it is carrying out. What an agent says about what it will do will be another action of the agent, and it can do whatever it wants. 42 D. Poole/Artijcial Intelligence 94 (1997) 7-56 5.4. Non-deterministic actions and the frame problem There has been much work on logical specifications of actions and change. The speci- that solves the frame problem fication of actions case with complete knowledge of the axiomatisation change with acyclic programs. [ 27,45,50]. for change. See [2] is well understood These axiomatisations for a detailed description for the deterministic the closure assume of axiomatising What is added in this work is a way to handle non-deterministic and complete knowledge idea is that determinism knowledge. The central in negation as failure) occurs for each world. We can have a distribution When we have uncertainty, value”. Often axiomatiser it is random, but often it is another agent. What we require this ambiguity. it is useful to consider resolves the question of “who chooses actions and partial (as assumed over worlds. the is that the Example 42. We can axiomatise heads 50% of the time and tails 50% of the time; when the state (heads or tails) it was before: a coin toss, where, when a coin is tossed, it is not tossed, it remains it lands in heads(C,T+ I) t tossed( C, T) A heads_tums_up( C, T) . heads(C,T+ 1) t +ossed( C, T) A heads( C, T) . taiZs( C, T) +- lheads( C, T) where heads(C, T) is true at time T if heads would turn up on coin C is it were tossed at time T. It is defined as: is true if coin C has heads up at time T. heads_tumsap(C, T) VC VT { headxtums-up ( C, T) , tailsfurnsup ( C, T) } E Co with VC VT Po( heads-turns-up( C, T) ) = 0.5. Many of the papers use the situation calculus time model used here. See [40] for a description of how the situation calculus can be combined with the independent to the frame problem rather than the discrete that present a solution change, for representing [2,27,45,50] choice logic. 5.5. The extensive form of a game The extensive form of a game [ 17,32,53] is a representation of a game in terms of a game tree, a generalisation at each node, and having of a decision information tree to include different agents making decisions sets of nodes that agents cannot distinguish. D. Poole/Artificial Intelligence 94 (1997) 7-56 43 the following form of a game contains “The extensive ( 1) the set of players, the order of the moves, i.e., who moves when, (2) (3) the players’ payoffs as a function of the moves (4) what the players’ choices are when they move, (5) what each player knows when he makes his choices, (6) t.he probability distribution over any exogenous events.” information: that were made, [ 17, p. 771 There is a direct mapping between the dynamic (actions) game. d is the set of players. The logic program specifies the moves move. The v function a probability provides random variables specifies what each player knows when function over exogenous of the players. The set C, specifies in CO. events represented ICL and the extensive form of a the payoffs as a function of the players’ choices when they its move. Pa it makes as the independent The ord.er of the moves is defined by the acyclicity of the knowledge base. The moves is made. If there such that ~1 is before x2 then xi can be made before ~2; in for each choice must be ordered so that the information is some acyclicity ordering if there is another acyclicity ordering where x2 is before xi, which ord.er the choices are made (as all of the information cannot depend on the other choice). then it does not matter available is available before the decision While ple semantic games. the acyclicity of the rule base was chosen framework [36], it can be justified in order by appealing to allow for a sim- to the structure of 6. Examples in detail In this section we present “information strates so-called second prlesents a decision-theoretic imperfect information of the representation, formalism. seeking actions” three different examples of using the ICL. The first demon- and noisy sensors and actuators. The example. The third defines a two-player, to show the details for the planning tic-tat-toe. There are intended game of blind and were not chosen because they are elegant examples 6.1. Information seeking actions and noisy sensors In this section we give an ideal&d single agent in an environment example, showing how to model l Information the following: producing in robotics, or asking a question plans l Comlitional l How a passive sensor can be used to model an active sensor l How noisy sensors and actuators can be modelled (conditioning (Section 6.1.4). that “looks”. in a user modelling on sense values). situation. actions, such as tests in diagnosis and positioning a camera 44 D. Poole/Ar?$cial htdigence 94 (1997) 7-56 ’ _r__-_____\\ ,’ z j isTrue j Fig. 4. Influence diagram for our idealised example. 6.1. I. Informution producing actions The look? decision of Fig. 4 can be seen as an information information with it. about isTrue be available to the next action. producing action. It lets It also has a cost associated The agents action can either be look or dontlook. The action can be modelled by having {look, dontlook} E C with our agent controlling this alternative. In the rule base we model how actions by the agent and truths in the world affect for the case where there is no sense values for the agent. Here is an idealised example noise. the agent (Section 6.1.4 considers noise.) Suppose can sense either pos, neg or nothing. For the case with no noise, the environment model has the following that following the looking, axioms in 3: sense(pos) + look A is-true. sense( neg) + look A +-true. sense ( nothing) +- dent-look. Thus “looK’ provides information about “is_true” to “do-it”. 6.1.2. Conditioning on sense values The agent can sense the world, and then decide what to do based on the sense values. Continuing that the agent has possible actions doit, dontdoit, our example, suppose and has the sense values above. There are three independent namely whether or not to doit for each of the three contingencies. choices the agent can make, Within the ICL this can be modelled by having the axioms: D. Poole/Ar@iciuf Intelkgence 94 (1997) 7-56 45 doit +- sense(pos) A do-ifgos. dont_doit c sense(pos) A dontifpos. do-it +- sense{ neg) A doijlneg. dent_doit +- sense( neg) A dontifneg. doit c sense (nothing) A doifnothing. dent_doit + sensefnothing) A dont$nothing. and by having the following alternatives in C, controlled by our agent: {do_ifpos, dont_ifqos) {do_ifneg, dontifneg} {do-&nothing, dontif_nothing} Within ithe dynamic ICL we specify: (doit, dontdo-it} E C,,,t 0 ngenr = { (sense(pos) , sense( neg) , sense(not~ing) )) T( {doit, dont-doit}) = { { sense(pos) , sense( neg) , sense( nothing)}) The rules that need to be provided as part defining the ICL are created as part of the agent’s strategy in the dynamic ICL. 6.1.3. Utility model Finally, a utility model (part of the environment model) specifies how the utility varies depending on what is true and what the agent does. Here is an example of such an an axiomatisation (given that our agent is agent, ): ~ti~~~(agent, , Fnie - K) +- tesr_cost( TC) A prize{ Prize). test_ct?st( 5) * look. test-cost(O) c dent-look. prize{ 10) c doit A is-true. prize( 0) c dont_doit A is-true. prize( 8) c dontdoit A -&true. prize{ 1) +-- doit A -+true. 6.1.4. Noisy actuators and sensors In Section 6.1.1 we assume that there were no noise in either actuator settings or in sense values. We can model actuator noise, for example in the “looking” actuator, by something like: 46 D. Poole/Artificial Intelligence 94 (1997) 7-56 see +- look A looking-works see t dontlook A not_looking_doesnt_work with following in C, controlled by nature: {looking-works, lookingdoesnt-work} {not-looking-works, not-lookingdoesnt-work} Po(looking_works) Pu( notlooking-doesnt-work) look. is the probability that the agent succeeds in seeing when is the probability that that agent sees when it looks. it does not Noisy sensors can be modelled similarly. Assume on whether are two alternatives controlled by nature: the agent sees, but that there is no noise with respect that the value sensed only depends to not seeing. There Gfalsepositive, truenegative} Gfalsenegative, true-positive} and the following facts: sense(pos) +- see A is-true A true-positive. sense ( -) + see A isfrue A falsenegative. sense( -) + see A G-true A truenegative. sense(pos) c see A +s_true A falsegositive. sense( nothing) c 7see. Po(falsepositive) when the value false-negative; when the sensor reports negative when the value in the world is true. is the probability of a false-positive; when the sensor reports positive of a true. PoCfalsenegative) is the probability the world is not in 6.2. Shipping widgets In this section we present an example of Draper et al. [ 141. The example is that of a that must process a widget. Its goal is to have the widget painted and processed flawed that it is done. Processing to see if it is the widget robot and then to notify widgets and shipping untlawed widgets. The robot can inspect blemished, which usually in the widget being painted but removes blemishes. correlates with the widget being flawed. Painting consists of rejecting its supervisor the widget initially results Agent module. We first represent blemishes. at time T. sense(blemished,T) the robot. The robot has one sensor that the widget is true if the robot senses for detecting is blemished The robot has 6 actions (exactly one of which is possible at any time), namely reject, ship, notify, paint or inspect robot does action A at time T. the widget or do nothing. do(A,T) to is true if the D. Poole/Art$cial Intelligence 94 (1997) 7-56 47 The robot specification module for robot is the tuple (Crobot, Orobor, r) where C robor = { {do( reject, T) , do( ship, T) , do(notify, T) , do(paint, T) , do( inspect, T), do(nothing, T)} 1 T is a time}. 0 rob,,t = { { lsense( blemished, T) , sense ( blemished, T) } 1 T is a time}. T( {do( reject, T) , do(ship, T) , do(notify, T) , do(paint, T) , do( inspect, T) , do(nothing, T)}) = { { lsense( blemished, T’) , sense( blemished, T’) } 1 T’ < T}. In other words, at each time, the robot gets to choose which of the six actions out. When in the past. it carries it knows whether or not it has sensed blemishes this decision, it is making Nature module. The remaining by nature. This specifies actions affect the world, how the world affects the senses of the robot. the dynamics of the world. We axiomatise to define is the rules and alternatives thing controlled how the robot’s The widget being painted persists the widget can result in the widget being painted that whether painting works does not depend on the time (a second painting will not make the widget more likely to be painted). Painting only works if it has not already been shipped or rejected. Once painted, a widget remains painted. in the world. Painting 0.95). We assume (with probability painted(T + 1) + do(paint, T) A paint-works A vhipped( T) A -vejected( T) . pain,ted(T + 1) +- painted(T) . Painting succeeds 95% of the time: (pai8~t_works,paintfils} E Co P0(paint_works) = 0.95, Po(paintfails) = 0.05 Note that we have not parametrized paint-works by the time. This is lets us model fact that repainting will not help when painting world where paintfails if it is false, painting always results the the first time. In any possible in the widget being painted, and is true, painting always results in the widget being painted. failed The widget is blemished if and only if it is flawed and not painted: 48 D. Poole/Artificial Intelligence 94 (1997) 7-56 btemished( T) +- ~uwed(T) A lpuinted( T) . Note that the use of logic programs, assuming the stable model semantics entails that the rules mean “if and only if” (in the same way Clark’s completion [ 111 does). Whether the widget is flawed or not persists: Jlawed( T + 1) +- @wed{ T) . The widget is processed if it is rejected and flawed or shipped and not flawed: processed(T) +- rejected(T) A &wed( 1”). processed(T) +- ~hipped( T) A +uwed( T) . The widget is shipped if the robot ships it, and being shipped persists: s~ipped( T) +-- dofship, T) . shipped( T + 1) + shipped(T) . The widget is rejected if the robot rejects it, and being rejected persists: rejected(T) c do( reject, T). rejected{ T + 1) + Ejected(T) . We axiomatise how what the robot senses is affected by the robot’s actions and the world: sease( blemished, T + 1) +- da{ inspect, T) A blemished(T) A +zlsepos( T) . The sensor gives a false positive with probability 0.1. Unlike whether punting succeeds, suppose the probability of the sensor giving a false positive at each time is independent of what happens at other times: Gfulsepos( T) , not$uZsepos( T) } E Co PO ~u~~e~~~( T) ) = 0.1 Po(notfuZse_pos(T)) = 0.9 D. Poole/Artijicial Intelligence 94 (1997) 7-56 49 30% of widgets are initially flawed: cfzawed( 0)) ~~~ff~e~( 0) ) E Co Pe (j!awerl( 0) ) = 0.3 PO ( ufzJEawed( 0) ) = 0.7 Finally we specify how the utility is dependent on the world and actions of the robot. The utility is one if the widget is painted and processed the first time the robot notifies, and is zero otherwise. utility( robot, 1) +- dc( notify, T) A ~~zoti~ed~efore( T) I\ painted(T) A processed(T). ~tility( robot, 0) + ~~ti~i~( robot, 1) . notifed_before( T) c TI < T A do( notify, TI ) . One (pure) policy for our robot is the logic program: do( inspect, 0). do(paint, 1). do ( reject, 2) t sense( blemished, 1) . do( ship, 2) t lsense( blemished, 1) . do( notify, 3). This has expected utility 0.925. Note that in the problem formulation, we need to paint blemished widgets. This policy is not optimal. Policy: do( inspect, 0). do ( inspect, 1) . do(pzint, 2). do( reject, 3) c sense( blemished, I ) . do(reject, 3) +- sense(blemished, 2). do( ship, 3) c vense( blemished, 1) A lsense( blemished, 2). do(notify,4). has expected utility 0.94715. There is no optimal policy for this example (it is not a finite game so Nash’s theorem does not apply here), we can add more “inspect”s to keep raising the expected utility. 50 D. Poole/Artijicial Intelligence 94 (1997) 7-56 The best policy without has expected utility 0.665. inspecting, namely {&Quint, 0)) do(ship, 1) , do(noti;fy, 2)) Of course, we can always define the utility so that the robot is penalized for taking too much time, e.g., by defining utility by: utility( robot, 1 - T/ 10) +- rewarded(T) utility( robot, 0) +- vvwarded_atsome_time. rewarded_atsome-time +- rewarded(T) . rewarded(T) c do( notify, T) A lnoti$edbefore( T) A painted(T) A processed ( T) . the first policy above (with a single inspect) is optimal, with the revised utility, Under expected utility 0,625. 6.3. Blind tic-tat-toe Koller and Pfeffer [26] present a game description tic-tat-toe”. We represent example and to enable us to compare the representation language Gala. In that language the same game in the ICL in they represent the game “blind order to present a parlour-game with Gala. Blind tic-tat-toe is an imperfect information version of standard tic-tat-toe: tic-tat-toe, the players “As in regular take turns placing marks in squares. How- ever, on his turn, each player can choose to mark either an x or an o; he reveals in which he makes to his opponent the mark, but not what type of the square the goal is to complete a line of three squares with make he makes. As usual, the same mark.” [26] The basic idea in defining such a game is to axiomatise the dynamics of the game in the logic. The rules should imply the consequences for the state of the game. In this game, of the choices made by agents. First we need a representation to make three in a row wins). We can represent the order of is important as well as who put what where (as the last player who places a the state of the game as a list that the player “Who” (either a or b) (X, Y) . The first element of the list was the last the moves marker of the form put( X, Y Who, What) which means put “What” (an o or an x) at position there. element placed The first rule defines the first move. The second rule for the state progression defines subsequent moves: sture( [put( X, KAgent, What) 1, s( 0) ) +- starts(Agent) A chooses(Agent,place( X, E: What), 0). D. Poole/Art@cial Intelligence 94 (1997) 7-56 51 stutc?( [put(X, ZAgent, What) ,put(Xp, Yp, Ap, Wp) /Rest], s(T)) c stat4 [put(Xp, Yp, AP, Wp) IRest], T) A $nished( [put( Xp, Yp, Ap, Wp) IRest] ) A opponent(Ap, Agent) A chooses(Agent,place(X, I: What), T). We can define auxiliary when the game is finished. relations such as who starts, and how the moves alternate, and stana( a). opponent( a, b) . opponent( b, a). $nished( S) + draw(S) . jinisized( S) +- wins( A, S). draw(S) c length( S, 9). We can axiomatise any particular the utility functions. Note that we are relying on the fact that for set of choices by agents, there is only one win state or one draw state. utiZi;y( a, 1) t wins( a, S). utili[y( a, 0) +- wi,rzs( b, S) . utili[v( a, 0.5) c dmw( S). uti&u( b, 1) t wirzs( b, S) . utiZity( b, 0) c wins( u, S). utiZiQ( a, 0.5) c dmw( S). 52 D. Poole/Art@cial Intelligence 94 (1997) 7-56 We can axiomatise the choices by the agents: t5 chooses (Agent, place( X, I: What), T) +- chooseX(Agent, X, T) A chooseY(Agent, X, T) A chooseWhat(Agent, What, T). The agents get to choose the X position, the Y position, and what mark they make. ThUS, VT {chooseX(Agent, 1, T) , chooseX(Agent, 2, T) , chooseX(Agent, 3, T)} E C,+,r VT {chooseY(Agent, 1, T), chooseY(Agent, 2, T), chooseY(Agent, 3, T)} E C.+enr VT {chooseWhat(Agent, o, T), chooseWhat(Agent, x, T)} E C,+,ent Now we have to decide what an agent gets to observe when making their decision. that for each of these choices We assume the state, which consists of a list of poswho( X, I: Who) for the squares pos_what( X, I: What) they occupy: the agent gets to observe a filtered version of for each square and a list of sense(Agent, Pas_WhoList, Pas_WhatList, T) + stute( S, T) A extractpos_who-Zist( S, Pos_Who_List) A extract.pos_what_list(Agent, S, Pas_WhatList) . extruct_pos_who_list( [ ] , [ ] > . extructposwho_list( [put( X, XAgent, What) IS11 , [pos_who( X, x Who) IP 11) t extract_pos_who_list( Sl , P 1) . Similarly for extruct_pos_whatlist. The observable function can be given by: l6 z-( {chooseX(Agent, 1, T) , chooseX(Agent, 2, T) , chooseX(Agent, 3, T)}) = {{sense(Agent, Pos_WhoList, Pos_WhatList, T)} 1 Pos_WhoList, PosWhatList are appropriate lists}. The other two choices have similar (i.e., the agent knows which X it chose when choosing a Y). sets, but include information the previous decisions space (presumably I5 This is one simple way an occupied method from this list. an element I6 We have neither presented a syntax to present object this set notation gets the general level rules, idea across. is that the agent can derive a list of free spaces from the sensed information, to axiomatise the choices. It means they will be penalized by losing that agents can choose if they choose an occupied to place a mark in spot). Another and then can only choose for r, nor a syntax in order to not have to present for the choices. This is because we only wanted It is hoped that two different formalisms. logical D. P~l~~ie/Ar~~ciai Intelligence 94 (1997) 7-56 53 For those who do not like to read declarative logical formulae, the best way to understand these rules is to think about building a game tree by forward chaining on the rules. a starts and so must make a choice of the X position, the Y position, and what mark they are making. This forms an l&way split in the game tree (there are 18 different Ichoices available to a). Then the state evolves by b making a move. There are 9 different information states for b, and they have to choose one of 18 choices (or I6 if the alternative ~iomatisation is made). And so on building the game tree. It is envisioned that such a representation could be used to build the same game tree as for Gala [ 261, and can use the same efficient algorithms. The representation proposed in this paper is more declarative (in that we can give a declarative possible worlds semantics for the whole framework, and all of the logical rules can be interpreted as statements about the domain), and more general in that it is not tuned specifically to 2-person alternating games. In fact the ICL is not tuned for any particular application; there are no built in predicates, and no syntax beyond that of the logic. This may mean that Gala is more natural for those games it is designed for, but we believe that the more general language will be more useful for general specification of decision problems under uncertainty. 7. Conclusion This paper has presented a logic that allows for what is arguably a natural specification of multi-agent decision problems. There is a simple semantic framework in terms of possible worlds semantics. It lets us use logic to specify the dynamics of the world, while retaining the elegance and generality of game theory. What we are adding to game theory is an object-level representation of the domain. We can axiomatise how actions (moves) affect the world, how the utility is derived from simpler components, and how sensors work. All of these axioms can be interpreted within the simple logic. It allows us to represent the probabilistic dependencies in a domain, in much the same way that influence diagrams provide a more intuitive representation for many problems than decision trees [ 22,231. We also allow for a form of parametrized rules by the use of logical variables that allow us to construct large game trees from smaller components. We are adding to influence diagrams, the ability to represent multiple agents, the ability to represent I7 structured probability and decision tables, and a way to have a dynamic construction of influence diagrams (with a similar motivation to Breese [ 91, but having logic programs as object-level statements about the world rather than at the meta-level as does Breese). We also allow for the designer to axiomatise the dynamics of the sydem, instead of having to summarize it in a single step as a probability distribution. We are adding to logic a new way to handle and think about non-determinism and uncertainty. Rather than just using disjunction which does not seem to be subtle enough for the range of forms of uncertainty that we need to handle, we provide a mechanism in I7 In other papers we show how the structure can be exploited for computational gain [ 38,411. 54 D. Poole/Arti$cial Intelligence 94 (1997) 7-56 different in a logical formalism. choices independence to handle uncertainty. We argue that considering is the right way to think about uncertainty terms of independent agents making choices The ICL is weaker than other mixes of logic and decision 19,20,24] which have added probability general and have to cope with many different (e.g., disjunction choices by agents). The goals of this paper are different: we are investigating way of viewing uncertainty representations question. agents. We are looking of the world simpler. Whether we have succeeded they can state independence forms of uncertainty for modelling and decisions assumptions (although assumptions), as well as a different for ways to make in this is an open theory for modelling agents [ 3, to a rich logic. They do not have Conspicuous by its absence in this paper is a discussion In this ( 1) building a situated agent that embodies on computation. even harder (2) simulating the propositional, can mean three things: a policy and environment; the author’s web page. l8 It should be noted of the second that finds expected utilities of strategies context, computation a strategy; Prolog implementation from of finding Nash equilibria, case is exponentially is because dynamic programming cannot a problem with the representation; the representation whether There easier a decision problem [5,38,41] structure. There for the problems algorithms that the computational single-agent without perfect or (3) finding an optimal strategy. A is available complexity recall this does not work when we have a forgetful agent; we [ 541. This is not It is not clear to solve. presented here makes solving than the rules is much more work to be done on exact and approximate represented that the representation in an influence diagram, in this formalism makes the problems more difficult is some, however, evidence than an influence diagram of the earlier decisions as we can exploit it is the problems that are difficult. the last decision independently in the ICL. Intuitively to solve solve [25]. Acknowledgements Thanks to Craig Boutilier This work was supported by Institute and Natural Sciences OGP0044 12 1. and Holger Hoos for detailed comments on this paper. for Robotics and Intelligent Systems, Project IC-7 and Engineering Research Council of Canada Operating Grant References [ l] J.S. Albus, Brains. Behavior and Robotics (BYTE Publications, Peterborough, NH, 1981). [2] K.R. Apt and M. Berem, Acyclic programs, New Generafion Comput. 9 (1991) 335-363. [3] E Bacchus, Representing and Reasoning with Uncertain Knowledge (MIT Press, Cambridge, MA, 1990). ]4] F. Bacchus, A.J. Grove, J.Y. Halpem and D. Keller, From statistical knowledge bases to degrees of belief, ArtiJcial InteZZigence 87 ( 1996) 75-143; ftp://logos.uwaterloo.ca/pub/bacchus. [ 51 C. Boutilier, R. Dearden and M. Goldszmidt, Exploiting structure in policy construction, in: Proceedings IJCAI-95, Montreal, Que. ( 1995) 1104-l 111. lx http://www.cs.ubc.ca/spider/poole D. Poole/Artificial Intelligence 94 (1997) 7-56 55 [6] C. Boutilier and N. Friedman, Nondeterministic in: Working Notes AAAI Theories of Actions: Formal Theory and Practical Applications actions and the frame problem, Spring Symposium 1995-Extending (199?)). [ 71 C. Boutilier, N. Friedman, M. Goldszmidt and D. Keller, Context-specific independence in Bayesian in: E. Horvitz and E Jensen, eds., Proceedings 12th Conference on Uncertainty in Arti#cial networks, Intelligence (CIAI-96), Portland, OR (1996) 115-123. [ 81 C. Boutilier and D. Poole, Computing optimal policies for partially observable decision processes using compact representations, [9] J.S. B,mese, Construction [ lo] R.A. .Brooks, A robust layered control system for a mobile robot, IEEE I. Robotics Automation 2 (1986) in: Proceedings AAAI-96, Portland, OR ( 1996) 1168-l 174. of belief and decision networks, Comput. Intell. 8 ( 1992) 624-647. 14-23. [ 111 K.L. Clark, Negation as failure, New York, 1978) 293-322. in: H. Gallaire and J. Minker, eds., Logic and Databases (Plenum Press, [ 121 T.L. Dean and K. Kanazawa, A model for reasoning about persistence and causation, Comput. Intell. 5 (198!)) 142-150. [ 131 T.L. Dean and MI? Wellman, Planning and Control (Morgan Kaufmann, San Mateo, CA, 1991). [ 141 D. Draper, S. Hanks and D.S. Weld, Probabilistic and contingent in: Proceedings 2nd International Conference on AI Planning Systems, Menlo Park, CA planning with information gathering execution, (1994) 31-36. [ 15 ] R.Y. Fagin, J.Y. Halpem, Y. Moses and M.Y. Vardi, Reasoning about Knowledge (MIT Press, Cambridge, MA, 1994). [ 161 J. Forbes, T. Huang, K. Kanazawa and S. Russell, The BATmobile: in: P,Foceedings IJCAI-95, Montreal, Que. (1995) 1878-1885. towards a Bayesian automated taxi, [ 171 D. Fudenbetg [ 181 M. Gelfond and V. Lifschitz, The stable model semantics and J. Tiiole, Game Theory (MIT Press, Cambridge, MA, 1992). for logic programming, in: R. Kowalski and K. Bowen, eds., Proceedings 5th Logic Programming Symposium, Cambridge, MA (1988) 1070-1080. [ 191 P Haddawy, Representing Plans under Uncertainty: A Logic of Time. Chance, and Action, Lecture Notes in Artificial Intelligence, Vol. 770 (Springer, Berlin, 1994). [20] J.Y. Halpem, An analysis of first-order 1211 J.Y. Halpem and M.R. Tuttle, Knowledge, probability [22] R.A. Howard, From influence to relevance to knowledge, logics of probability, Artificial Intelligence 46 ( 1990) 31 l-350. and adversaries, J. ACM 40 (1993) 917-962. in: R.M. Oliver and J.Q. Smith, eds., Influence Diagrams, Belief Nets and Decision Analysis (Wiley, New York, 1990) Chapter 1, 3-23. Influence diagrams, and J.E. Matheson, in: R.A. Howard and J. Matheson, [23] R.A. Howard eds., The Principles and Applications of Decision Analysis (Strategic Decisions Group, CA, 198 1) 720-762. [ 241 K. Kanazawa, A logic and time nets for probabilistic inference, in: Proceedings AAAI-91, Anaheim, CA (1991) 360-365. [ 251 D. Koller and N. Megiddo, The complexity and Economic Behavior 4 (1992) 528-552. of two-person zero-sum games in extensive form, Games [ 261 D. Koller and A.J. Pfeffer, Generating and solving imperfect information games, in: Proceedings IJCAI- 95, Montreal, Que. (1995) 1185-l 192. [27] R. Kowalski, Logicfor Problem Solving, Artificial [28] H.J. Levesque, R. Reiter, Y. Lesperance, E Lin and R.B. Scherl, GOLOG: a logic programming Intelligence Series (North-Holland, New York, 1979). language about Action and Change J. Logic Programming, Special Issue on Reasoning for dynamic domains, (1996). [ 291 D.G. Luenberger, Introduction to Dynamic Systems: Theory, Models and Applications (Wiley, New York, 1979). [ 301 J. McCarthy, Applications of circumscription to formalizing common-sense knowledge, Art@%1 Intelligence 28 (1986) 89-l 16. [ 3 11 J. McCarthy and PJ. Hayes, Some philosophical intelligence, in: B. Meltzer and D. Michie, eds., Machine Intelligence, Vol. 4 (Edinburgh University Press, Edinburgh, 1969) 463-502. from the standpoint of artificial problems [ 321 R.B. Myerson, Game Theory: Analysis of Conflict (Harvard University Press, Cambridge, MA, 1991). [ 331 N.J. Nilsson, Logic and artificial intelligence, Artificial Intelligence 47 ( 1991) 3 l-56. 56 D. Pooie/Art@cial Intelligence 94 (1991) 7-X [ 341 R.M. Oliver and J.Q. Smith, eds., Influence Diagrams, Belief Nets and Decision Analysis, Series in Probability and Mathematical Statistics (Wiley, Chichester, 1990). [35] J. Pearl, Probabilistic Reasoning in Inielligent Systems: Networks of Plausible Inference (Morgan Kaufmann, San Mateo, CA, 1988). 1361 D. Poole, probabilistic Horn abduction and Bayesian networks, Arri$cial rnrelligence 64 (1993) 81-129. [37] D. Poole, Abducing through negation as failure: stable models within the independent choice logic, Tech. Rem., Department of Computer Science, University of British Columbia, Vancouver, BC ( 1997); ftp://ftp.cs.ubc.ca/ftp/local/poole/paperslabnaf.ps.gz. 138 J D. Poole, Exploiting the rule structure for decision making within the independent choice logic, in: P. Besnard and S. Hanks, eds., Proceedings 11th Conference on Uncertainty in Artificial Intelligence (I/AI-95), Montreal, Que. (1995) 454-463. 1391 D. Poole, Logic programming for robot control, in: Proceedings IJCAI-95, Montreal, Que. (1995) 150-157. ]40] D. Poole, A framework for decision-theo~tic pIanning 1: combining the situation calculus, conditional plans, prob~ility and utility, in: E. Horvitz and I? Jensen, eds., Proceedings 12th Conference on Uncertainty in Artijiciul Intelligence (UAI-96), Portland, OR (1996) 436-445. [41] D. Poole, Probabilistic partial evaluation: exploiting rule structure in probabilistic inference, in: Proceedings IJCAI-97, Nagoya, Japan ( 1997); ftp://ftp.cs.ubc.ca/ftp/local/poole/pape-pa.ps.gz. 1421 D. Poole, A.K. Mackworth and R.G. Goebel, Computational Intelligence: A Logical Approach (Oxford University Press, New York, 1997.) [43] T.C. Przymusinski, Three-valued nonmonotonic formalisms and semantics of logic programs, Art$iciul lnrelljgence 49 ( 1991) 309-343. [44] M.L. Puterman. Markov decision nrocesses. in: D.P Hevman and M.J. Sobel, eds., ~ffndboo~ in Operations Research and tenement Science, Vol. 2 (Nigh-Holland, Amsterdam, 1990) Chapter 8, 331-434. R. Reiter, The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression, in: V. Lifschitz, ed., Artijcial Intelligence and the Mathematical Theory of Computation: Papers in Honor of John McCarthy (Academic Press, San Diego, CA, 1991) 359-380. R.L. Rivest, Learning decision lists, Machine Learning 2 (1987) 229-246. S.J. Rosenschein and L.P Kaelbhng, A situated view of representation and control, Artificial Intelligence 73 (1995) 149-173. S.J. Russell and D. Subr~anian, Provably bounded-optimal agents, J. Art$ Zntelf. Res. 2 (1995) 5X-609. L.J. Savage, The Found&on of Statistics (Dover, New York, 2nd ed., 19720. L.K. Schubert, Monotonic solutions to the frame problem in the situation caJculus: an efficient method for worlds with fully specified actions, in: H.E. Kyburg, R.P. Loui and G.N. Carlson, eds., Knowledge Representation and Defeasible Reasoning (Kluwer Academic Press, Boston, MA, 1990) 23-67. Y. Shoham, Agent-oriented programming, Artijicial Intelligence 60 (1993) 51-92. J.E. Smith, S. Holtzman and J.E. Matheson, Structuring conditional relationships in influence diagrams, Oper. Res. 41 (1993) 280-297. J. von Neumann and 0. Morgenstem, i%eory of Games and Economic Behavior (Princeton University Press, Princeton, NJ, 3rd ed., 1953). L. Zhang, R. Qi and D. Poole, A computational theory of decision networks, International J. Approximate Reasoning 11 (1994) 83-158. Y. Zhang, A foundation for the design and analysis of robotic systems and behaviours, Ph.D. Thesis, Department of Computer Science, University of British Columbia, Vancouver, BC (1994). Y. Zhang and A.K. Mackworth. Will the robot do the right thing?, in: Proceedings 10th Biennial Conference of the Canadian Society for Computfltion~f Studies of fnteliigence, Banff, Aha. ( 1994) 255-262. Y. Zhang and A.K. Mackworth, Constraint nets: a semantic model for hybrid dynamic systems, Theoret. Comput. Sci. 138 (1995) 211-239. 1451 1461 1471 ]481 t491 l501 l511 f521 t531 I541 1551 L561 1571 