Artificial Intelligence 197 (2013) 1–24Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintInconsistency measures for probabilistic logicsMatthias ThimmInstitute for Web Science and Technologies, Universität Koblenz–Landau, Germanya r t i c l ei n f oa b s t r a c tArticle history:Received 23 February 2012Received in revised form 27 July 2012Accepted 4 February 2013Available online 7 February 2013Keywords:Inconsistency measuresInconsistency managementProbabilistic reasoningProbabilistic conditional logic1. IntroductionInconsistencies in knowledge bases are of major concern in knowledge representation andreasoning. In formalisms that employ model-based reasoning mechanisms inconsistenciesrender a knowledge base useless due to the non-existence of a model.In order toinconsistencies are mandatory.restore consistency an analysis and understanding ofRecently, the field of inconsistency measurement has gained some attention for knowledgerepresentation formalisms based on classical logic. An inconsistency measure is a toolthat helps the knowledge engineer in obtaining insights into inconsistencies by assessingtheir severity. In this paper, we investigate inconsistency measurement in probabilisticincorporates uncertainty and focuses on the role ofconditionalconditionals, i.e. if–then rules. We do so by extending inconsistency measures for classicallogic to the probabilistic setting. Further, we propose novel inconsistency measures that arespecifically tailored for the probabilistic case. These novel measures use distance measuresto assess the distance of a knowledge base to a consistent one and therefore takes thecrucial role of probabilities into account. We analyze the properties of the discussedmeasures and compare them using a series of rationality postulates.logic, a logic that© 2013 Elsevier B.V. All rights reserved.The field of knowledge representation and reasoning [4] is concerned with formal representations of knowledge andhow these formalizations can be used for reasoning, i.e., how new information can be automatically inferred using a formalsystem. One of the big issues in knowledge representation is accuracy. Usually, the term “knowledge” is used to describestrict or objective information that is considered to be absolutely true in the given frame of reference, i.e. the real world.The counterpart, denoted by “subjective knowledge” or “beliefs”, is used to describe information that is assumed to be true bythe individual under consideration. While strict knowledge describes—by definition—a consistent state, subjective knowledgemight be flawed in several aspects. Besides being incorrect with respect to the real world, subjective knowledge can beincomplete, uncertain, or inconsistent. That is, for some piece of information I it might be unknown whether I is true orfalse (incompleteness), I might be believed only to a certain degree (uncertainty), or I might be in conflict with anotherpiece of information Iimplies that at least(inconsistency). Note that inconsistency of two pieces of information I and Iwith the state of the real world, anone of them is incorrect. However, even without the possibility to compare I and Iinconsistency can be detected by a being capable of reasoning, which is not necessarily true for incorrect information ingeneral. In this paper, we do not consider the general problem of incorrect information and always assume that representedpieces of information are subjective. However, as some terms like knowledge base have been established in the literature weadapt those conventions.(cid:2)(cid:2)(cid:2)Within the field of knowledge representation and reasoning there are several subfields that deal with incomplete, un-certain, and/or inconsistent knowledge such as default [27] and defeasible reasoning [19], argumentation [2,26], or possibilisticE-mail address: thimm@uni-koblenz.de.0004-3702/$ – see front matter © 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2013.02.0012M. Thimm / Artificial Intelligence 197 (2013) 1–24and fuzzy reasoning [33]. Among the most established logical frameworks for dealing with uncertainty is probability theory[23,25]. There have been numerous works on combining probability theory with knowledge representation. For example,Bayesian networks and Markov nets allow for derivation of uncertain beliefs from other uncertain beliefs. Especially inapplication areas such as medical diagnosis, where the user has to rely crucially on the certainty of individual recommen-dations, reasoning using probabilistic models of knowledge serves well [24].In this paper we employ probabilistic conditional logic [28] for representing uncertain knowledge. In probabilistic condi-tional logic, knowledge is represented using probabilistic conditionals (ψ | φ)[p] with the intuitive meaning “if φ is truethen ψ is true with probability p”. Probabilistic conditional logic has been studied extensively under several aspects, e.g.effective reasoning mechanisms [9], default reasoning [20], or extensions with first-order logic fragments [15,16]. Moreover,the field of information theory provides a nice solution to the problem of incomplete information in probabilistic conditionallogic. Using the principle of maximum entropy [23] one can complete uncertain and incomplete information in order togain new information that was unspecified before, see also [28,14]. The expert system SPIRIT [30] is a working system thatemploys reasoning based on the principle of maximum entropy. It has been applied to various fields of operations researchsuch as project risk management [1] and portfolio selection [29]. Though reasoning with maximum entropy can deal withincomplete and uncertain information, it is not suitable for reasoning with inconsistent information. But inconsistency is aubiquitous matter and human beings have to deal with it all the time. In knowledge engineering and expert system design itbecomes most apparent when multiple experts try to build up a common knowledge base. However, the issue of extendingreasoning with maximum entropy to inconsistent knowledge bases has been dealt with in the literature only little so far,cf. [31,8,6].In this paper, we investigate inconsistencies in probabilistic conditional logic from an analytical perspective. One wayto analyze inconsistencies is by measuring them. An inconsistency measure is a function that quantifies the severity ofinconsistencies in knowledge bases. An inconsistency value of zero indicates no inconsistency (and therefore consistency)while the larger the inconsistency value, the more severe the inconsistency. Thus, an inconsistency measure can be seen asthe counterpart to an information measure [5] for the case of inconsistent information. Recently, there has been a gain inattention to approaches for measuring inconsistency in classical logics, see e.g. [13,11]. In general, an inconsistency measurecan be used to support the knowledge engineer in building a consistent knowledge base or repairing an inconsistent one. Forexample, Grant and Hunter [11] develop an approach for stepwise inconsistency resolution of inconsistent knowledge basesthat makes use of inconsistency measures. In their approach, a knowledge base is repaired by e.g. deleting or weakeningformulas. There, inconsistency measures serve as heuristics for selecting the right formula that has to be modified, i.e. byselecting that one that maximizes consistency gain. Inconsistency measures can also be used to determine which pieces ofinformation are most responsible for producing the inconsistency. In [13,36] the Shapley value [32] is used to distributethe inconsistency value of a knowledge base among the individual formulas. In a setting where knowledge is merged fromdifferent sources this information can help in identifying the responsible contributors.However, classical approaches for inconsistency measurement do not grasp the nuances of probabilistic knowledge andallow only for a very coarse assessment of the severity of inconsistencies. In particular, those approaches do not take thecrucial role of probabilities into account and exhibit a discontinuous behavior in measuring inconsistency. That is, a slightmodification of the probability of a conditional in a knowledge base may yield a discontinuous change in the value ofthe inconsistency. Consequently, we develop novel inconsistency measures that are more apt for the probabilistic setting.We do so by continuing and largely extending previous work [34–36]. In particular, the contributions of this paper are asfollows. First, we propose and discuss a series of rationality postulates for inconsistency measures in probabilistic conditionallogic. Many of those postulates are inspired by similar properties for the classical case—see e.g. [13]—and others specificallyaddress demands arising from the use of a probabilistic logic, such as the demand for a continuous behavior with respectto changes in the knowledge base. Second, we extend several inconsistency measures that were proposed for the classicalcase to the more expressive framework of probabilistic conditional logic and investigate their properties with respect to therationality postulates. Third, we pick up an extended logical formalization [21] of the inconsistency measure proposed in[34] for probabilistic conditional logic, generalize it, and define a family of inconsistency measures based on minimizing thedistance of a knowledge base to a consistent one. We also propose a novel compound measure that solves an issue with theprevious measure. We thoroughly investigate the properties of all measures with respect to the rationality postulates anddiscuss their advantages and disadvantages with the use of examples.The rest of this paper is organized as follows. We continue in Section 2 with an overview on probabilistic conditionallogic and introduce further notation. In Section 3 we approach the problem of inconsistency measurement in probabilisticconditional logic by developing a series of rationality postulates. We continue with an overview on the technical resultsof the paper in Section 4. We extend inconsistency measures for classical logic to the probabilistic setting in Section 5and present novel inconsistency measures that are more apt for the probabilistic setting in Section 6. In Section 7 wereview related work and in Section 8 we conclude with some final remarks. All proofs of technical results can be found inAppendix A.2. Probabilistic conditional logicLet At be a propositional signature, i.e. a finite set of propositional atoms. Let L(At) be the corresponding propositionallanguage generated by the atoms in At and the connectives ∧ (and), ∨ (or), and ¬ (negation). For φ, ψ ∈ L(At) we abbreviateM. Thimm / Artificial Intelligence 197 (2013) 1–243φ ∧ ψ by φψ and ¬φ by φ. The symbols (cid:6) and ⊥ denote tautology and contradiction, respectively. We use possible worlds,i.e. syntactical representations of truth assignments, for interpreting formulas in L(At). A possible world ω is a completeconjunction, i.e. a conjunction that contains for each a ∈ At either a or ¬a. Let Ω(At) denote the set of all possible worlds.A possible world ω ∈ Ω(At) satisfies an atom a ∈ At, denoted by ω |(cid:8) a if and only if a positively appears in ω. The entailmentrelation |(cid:8) is extended to arbitrary formulas in L(At) in the usual way. Formulas ψ, φ ∈ L(At) are equivalent, denoted byφ ≡ ψ , if and only if ω |(cid:8) φ whenever ω |(cid:8) ψ for every ω ∈ Ω(At).The central notion of probabilistic conditional logic [28] is that of a probabilistic conditional.Definition 1 (Probabilistic conditional). If φ, ψ ∈ L(At) with p ∈ [0, 1] then (ψ | φ)[p] is called a probabilistic conditional.A probabilistic conditional c = (ψ | φ)[p] is meant to describe a probabilistic if–then rule, i.e., the informal interpretationof c is that “if φ is true then ψ is true with probability p”. If φ ≡ (cid:6) we abbreviate (ψ | φ)[p] by (ψ)[p]. Further, forc = (ψ | φ)[p] we denote with head(c) = ψ the head of c, with body(c) = φ the body of c, and with prob(c) = p theprobability of c. Let C(At) denote the set of all probabilistic conditionals with respect to At.Definition 2 (Knowledge base). A knowledge base K is a finite sequence of probabilistic conditionals, i.e. it holds that K =(cid:10)c1, . . . , cn(cid:11) for some c1, . . . , cn ∈ C(At).We impose an ordering on the conditionals in a knowledge base K only for technical convenience. The order can bearbitrary and has no further meaning other than to enumerate the conditionals of a knowledge base in an unambiguousway. For similar reasons we allow a knowledge base to contain the same probabilistic conditional more than once. Wecome back to the reasons for these design choices later. However, for all practical purposes a knowledge base can beused as a set of probabilistic conditionals, as it is usually defined for knowledge representation issues. In particular, forknowledge bases K = (cid:10)c1, . . . , cn(cid:11), K(cid:2) = (cid:10)c(cid:11) and a probabilistic conditional c we define c ∈ K via c ∈ {c1, . . . , cn},K ⊆ K(cid:2)}. The union of knowledge bases is definedvia {c1, . . . , cn} = {cvia concatenation.(cid:2)(cid:2)1, . . . , cm}, and K = K(cid:2)via {c1, . . . , cn} ⊆ {c(cid:2)1, . . . , c(cid:2)1, . . . , c(cid:2)m(cid:2)mSemantics are given to probabilistic conditionals by probability functions on Ω(At). Let F (At) denote the set of all proba-bility functions P : Ω(At) → [0, 1]. A probability function P ∈ F (At) is extended to formulas φ ∈ L(At) viaP (φ) =(cid:2)P (ω).ω∈Ω(At), ω|(cid:8)φ(1)That is, the probability of a formula is the sum of the probabilities of the possible worlds that satisfy that formula. IfP ∈ F (At) then P satisfies a probabilistic conditional (ψ | φ)[p], denoted by P |(cid:8)pr (ψ | φ)[p], if and only if P (ψφ) = p P (φ).Note that we do not define probabilistic satisfaction via P (ψ | φ) = P (ψφ)/P (φ) = p in order to avoid a case differentiationfor P (φ) = 0, see [23] for further justification. Note, that P |(cid:8)pr (ψ)[p] if and only if P (ψ) = p as (ψ)[p] is the abbreviationfor (ψ | (cid:6))[p] and P |(cid:8)pr (ψ | (cid:6))[p], if and only if P (ψ ∧ (cid:6)) = p P ((cid:6)) which is equivalent to P (ψ) = p. A probabilityfunction P satisfies a knowledge base K (or is a model of K), denoted by P |(cid:8)pr K, if and only if P |(cid:8)pr c for every c ∈ K.Let Mod(K) be the set of models of K. If Mod(K) (cid:14)= ∅ then K is consistent and if Mod(K) = ∅ then K is inconsistent.Example 1. Consider the knowledge baseK =(cid:4)(cid:3)( f | b)[0.9], (b | p)[1], ( f | p)[0.01]with the intuitive meaning that birds (b) usually (with probability 0.9) fly ( f ), that penguins (p) are always birds, and thatpenguins usually do not fly (only with probability 0.01). The knowledge base K is consistent as for e.g. P ∈ F (At) withP (bf p) = 0.005P (b f p) = 0.0P (bf p) = 0.49P (b f p) = 0.2P (b f p) = 0.045P (b f p) = 0.0P (b f p) = 0.01P (b f p) = 0.25it holds that P |(cid:8)pr K as e.g.P (b) = P (bf p) + P (bf p) + P (b f p) + P (b f p) = 0.55 andP (bf ) = P (bf p) + P (bf p) = 0.495and therefore P ( f | b) = P (bf )/P (b) = 0.9.Example 2. The knowledge base {(a)[0.9], (a)[0.4]} is inconsistent as there is no P ∈ F (At) with P (a) = 0.9 and P (a) = 0.4.Furthermore, observe that {(b | a)[0.8], (a)[0.6], (b)[0.4]} is inconsistent as P |(cid:8)pr {(b | a)[0.8], (a)[0.6]} implies P (b) (cid:2) 0.48which cannot simultaneously be satisfied with P (b) = 0.4.4M. Thimm / Artificial Intelligence 197 (2013) 1–24A probabilistic conditional (ψ | φ)[p] is normal if and only if there are ω, ω(cid:2) ∈ Ω(At) with ω |(cid:8) ψφ and ω(cid:2) |(cid:8) ψφ. Inother words, a probabilistic conditional c is normal if it is satisfiable but not tautological.Example 3. The probabilistic conditionals c1 = ((cid:6) | a)[1] and c2 = (a | a)[0.1] are not normal as c1 is tautological (there isno ω ∈ Ω(At) with ω |(cid:8) (cid:6)a as (cid:6)a ≡⊥) and c2 is not satisfiable (there is no ω ∈ Ω(At) with ω |(cid:8) aa as aa ≡⊥).As a technical convenience, for the rest of this paper we consider only normal probabilistic conditionals, so let K be theset of all non-empty knowledge bases of C(At) that contain only normal probabilistic conditionals.Proposition 1. If (ψ | φ)[p] is normal then (ψ | φ)[x] is normal for every x ∈ [0, 1].The proof of the above proposition is easy to see as the definition of normality does not depend on the probability of aconditional.Knowledge bases K1, K2 are extensionally equivalent, denoted by K1 ≡e K2, if and only if Mod(K1) = Mod(K2). Note thatthe notion of extensional equivalence does not distinguish between inconsistent knowledge bases, i.e. for inconsistent K1and K2 it always holds that K1 ≡e K2. As we are interested particularly in inconsistent knowledge bases we require anothermeans for comparing those. Knowledge bases K1, K2 are semi-extensionally equivalent, denoted by K1 ≡s K2, if and only ifthere is a bijection ρK1,K2 : K1 → K2 such that c ≡e ρK1,K2 (c) for every c ∈ K1. This means that two knowledge bases K1and K2 are semi-extensionally equivalent if we find a mapping between the conditionals of both knowledge bases such thateach conditional of K1 is extensionally equivalent to its image in K2. The following relationship is easy to see and givenwithout proof.Proposition 2. It holds that K1 ≡s K2 implies K1 ≡e K2.However, note that the other direction is not true in general.Example 4. Consider the two knowledge bases K1 = (cid:10)(a)[0.7], (a)[0.4](cid:11) and K2 = (cid:10)(b)[0.8], (b)[0.3](cid:11). Both K1 and K2 areinconsistent and therefore K1 ≡e K2. But it holds that K1 (cid:14)≡s K2 as both (a)[0.7] (cid:14)≡e (b)[0.8] and (a)[0.7] (cid:14)≡e (b)[0.3].One way for reasoning with knowledge bases is by using model-based inductive reasoning techniques [23]. For example,reasoning based on the principle of maximum entropy selects among the models of a knowledge base K the one uniqueprobability function with maximum entropy. Reasoning with this model satisfies several commonsense properties, see e.g.[23,14]. However, a necessary requirement for the application of model-based inductive reasoning techniques is the exis-tence of at least one model of a knowledge base. In order to reason with inconsistent knowledge bases the inconsistencyhas to be resolved first. In the following, we discuss the topic of inconsistency measurement for probabilistic conditional logicas inconsistency measures can support the knowledge engineer in the task of resolving inconsistency.3. Principles for inconsistency measurementInconsistency measurement is a research topic that has been mainly investigated in the field of classical theories only,see e.g. [11] for some recent work. In the following, we investigate inconsistency measurement for probabilistic conditionallogic in a principled fashion but borrow some notation from classical inconsistency measurement like from [13]. An incon-sistency measure I is a function that maps a (possibly inconsistent) knowledge base onto a non-negative real value, i.e., aninconsistency measure I is a function I : K → [0, ∞). The value I(K) for a knowledge base K is called the inconsistencyvalue of K with respect to I. In order to formalize the intuition behind inconsistency measures we give a list of principlesthat should be satisfied by any reasonable inconsistency measure. For that we need some further notation.Definition 3 (Minimal inconsistent set). A set M of probabilistic conditionals is minimal inconsistent if M is inconsistent andevery M(cid:2) (cid:2) M is consistent.Let MI(K) be the set of the minimal inconsistent subsets of K ∈ K.Example 5. Consider the knowledge base K = (cid:10)(a)[0.3], (b)[0.5], (a ∧ b)[0.7](cid:11). Then the set of minimal inconsistent subsetsof K is given via(cid:5)(cid:5)(cid:5)MI(K) =(cid:6)(a)[0.3], (a ∧ b)[0.7],(b)[0.5], (a ∧ b)[0.7](cid:6)(cid:6).The notion of minimal inconsistent subsets captures those conditionals that are responsible for causing inconsistencies.Conditionals that do not take part in creating an inconsistency are free.M. Thimm / Artificial Intelligence 197 (2013) 1–245Definition 4 (Free conditional). A probabilistic conditional c ∈ K is free in K if and only if c /∈ M for all M ∈ MI(K).For a conditional or a knowledge base C let At(C) denote the set of atoms appearing in C .Definition 5 (Safe conditional). A probabilistic conditional c ∈ K is safe in K if and only if At(c) ∩ At(K \ {c}) = ∅.Note that the notion of safeness is due to Hunter and Konieczny [13]. The notion of a free conditional is clearly moregeneral then the notion of a safe conditional.Proposition 3. If c is safe in K then c is free in K.Consider now the following properties, cf. [13,34]. Let K, K(cid:2)be knowledge bases and c a probabilistic conditional.Consistency K is consistent if and only if I(K) = 0.Monotonicity I(K) (cid:3) I(K ∪ {c}).Super-additivity If K ∩ K(cid:2) = ∅ then I(K ∪ K(cid:2)) (cid:2) I(K) + I(K(cid:2)).Weak independence If c ∈ K is safe in K then I(K) = I(K \ {c}).Independence If c ∈ K is free in K then I(K) = I(K \ {c}).Penalty If c ∈ K is not free in K then I(K) > I(K \ {c}).Irrelevance of syntax If K1 ≡s K2 then I(K1) = I(K2).MI-separability If MI(K1 ∪ K2) = MI(K1) ∪ MI(K2) and MI(K1) ∩ MI(K2) = ∅ then I(K1 ∪ K2) = I(K1) + I(K2).The property consistency demands that I(K) is minimal for consistent K. The properties monotonicity and super-additivitydemand that I is non-decreasing under the addition of new information. The properties weak independence and independencesay that the inconsistency value should stay the same when adding “harmless” information. The property penalty is thecounterpart of independence and demands that adding inconsistent information increases the inconsistency value. We definethe property irrelevance of syntax in terms of the equivalence relation ≡s as all inconsistent knowledge bases are equivalentwith respect to ≡e . For an inconsistency measure I, imposing irrelevance of syntax to hold in terms of ≡e would yieldI(K) = I(K(cid:2)) for every two inconsistent knowledge bases K, K(cid:2). The property MI-separability—which has been adaptedfrom [13]—states that determining the value of I(K1 ∪ K2) can be split into determining the values of I(K1) and I(K2) ifthe minimal inconsistent subsets of K1 ∪ K2 are partitioned by K1 and K2.The above properties do not take the crucial role of probabilities into account. In order to account for those we needsome further notation. Let K be a knowledge base. For (cid:19)x ∈ [0, 1]|K|we denote by K[(cid:19)x] the knowledge base that is obtainedfrom K by replacing the probabilities of the conditionals in K by the values in (cid:19)x, respectively. More precisely, if K =(cid:10)(ψ1 | φ1)[p1], . . . , (ψn | φn)[pn](cid:11) then K[(cid:19)x] = (cid:10)(ψ1 | φ1)[x1], . . . , (ψn | φn)[xn](cid:11) for (cid:19)x = (cid:10)x1, . . . , xn(cid:11) ∈ [0, 1]n. Similarly, for asingle probabilistic conditional c = (ψ | φ)[p] and x ∈ [0, 1] we abbreviate c[x] = (ψ | φ)[x].Definition 6 (Characteristic function). Let K ∈ K be a knowledge base. The function ΛK : [0, 1]|K| → K with ΛK((cid:19)x) = K[(cid:19)x] iscalled the characteristic function of K.Due to Proposition 1 the function ΛK is well-defined. The above definition is also the justification for imposing an orderon the probabilistic conditionals of a knowledge base.Definition 7 (Characteristic inconsistency function). Let I be an inconsistency measure and let K ∈ K be a knowledge base.The functionθI,K : [0, 1]|K| → [0, ∞)with θI,K = I ◦ ΛK is called the characteristic inconsistency function of I and K.The following property continuity describes our main demand for continuous inconsistency measurement, i.e., a “slight”change in the knowledge base should not result in a “vast” change of the inconsistency value.Continuity θI,K is continuous.The above property demands a certain smoothness of the behavior of I. Given a fixed set of probabilistic conditionals thisproperty demands that changes in the quantitative part of the conditionals trigger a continuous change in the inconsistencyvalue. Note that we require the qualitative part of the conditionals, i.e. premises and conclusions of the conditionals, tobe fixed. This makes this property not applicable for the classical setting. In the probabilistic setting satisfaction of thisproperty is helpful for the knowledge engineer in restoring consistency. Observe that for every knowledge base K ∈ K there6M. Thimm / Artificial Intelligence 197 (2013) 1–24is always a (cid:19)x ∈ [0, 1]|K|such that K[(cid:19)x] is consistent, cf. [34]. While in the classical setting, consistency of knowledge basescan only be restored by either removing or weakening formulas, in the probabilistic setting every knowledge base can alsobe made consistent by changing probabilities, see [8] for a heuristic approach utilizing this observation. Given that we havea continuous inconsistency measure the search for a “close” consistent solution can be better guided, see [36] for approachesthat utilize continuous inconsistency measures in order to implement a search procedure similar to gradient descent searchin optimization [3].Some relationships between the above properties are as follows.Proposition 4. Let I be an inconsistency measure and let K, K(cid:2)be some knowledge bases.1. If I satisfies super-additivity then I satisfies monotonicity.2. If I satisfies independence then I satisfies weak independence.3. If I satisfies MI-separability then I satisfies independence.4. K ⊆ K(cid:2)5. If I satisfies independence then MI(K) = MI(K(cid:2)) implies I(K) = I(K(cid:2)).6. If I satisfies independence and penalty then MI(K) (cid:2) MI(K(cid:2)) implies I(K) < I(K(cid:2)).implies MI(K) ⊆ MI(K(cid:2)).In [13] two further properties are discussed for classical inconsistency measurement: normalization and dominance. Theproperty normalization can be phrased as follows (note that the term normalization is not the be confused with our notionof normal conditionals).Normalization I(K) ∈ [0, 1].The above property states that inconsistency values should be bounded from above by one. On the one hand, this demandmakes perfect sense as this allows for comparing inconsistency values of different knowledge bases in a unified way. On theother hand, this demand is—in general—in conflict with the demand for super-additivity as the following example shows.Example 6. Let i, k ∈ N with k (cid:3) i. Consider the conditionalsck1= (ak)[0.6]ck2= (ak)[0.4]on a propositional signature Ati = {a1, . . . , ai}. Obviously, the knowledge base (cid:10)c1some inconsistency measure I satisfying consistency assigns some non-zero inconsistency value to (cid:10)c1x > 0. Furthermore, any knowledge base (cid:10)civalue, i.e. I((cid:10)cibase into account then there is a natural number n ∈ N such that for Kn = (cid:10)c1(cid:11) on At1 is inconsistent and therefore(cid:11)) =1, c1(cid:11) on Ati is inconsistent as well and should be assigned the same inconsistency(cid:11)) = x. It follows that, if I satisfies super-additivity and does not take the size of signature of a knowledge(cid:11) it holds that(cid:11), i.e. I((cid:10)c11, c11, c11, ci1, ci222221, c12, . . . , cn1, cn2I(Kn) (cid:2) I(cid:7)(cid:3)1, c1c12(cid:4)(cid:8)+ · · · + I(cid:7)(cid:3)1, cncn2(cid:4)(cid:8)(cid:2) nx > 1.Thus, I cannot satisfy normalization.The previous example showed that an inconsistency measure that does not take (the size of) the signature into accountcannot satisfy consistency, super-additivity, and normalization at the same time. Furthermore, taking (the size of) the signatureinto account may become unintuitive. As for the case of Example 6, in order to allow I to satisfy consistency, super-(cid:11) defined on At1 and K(cid:2) = (cid:10)c1additivity, and normalization it has to hold that for K = (cid:10)c1(cid:11) defined on At2 it follows1, c1that I(K) (cid:14)= I(K(cid:2)). As K = K(cid:2)this result may be unintuitive. However, one has to note that for K the whole languageis affected by the inconsistency while for K(cid:2)only “half” of the language is affected. In particular, for the proposition a2 ∈At2 there is no conditional c ∈ K(cid:2)such that c ∈ M for some M ∈ MI(K(cid:2)) and a2 ∈ At(c). Provided that we employ aparaconsistent reasoning mechanism for probabilistic knowledge—like the one proposed in [6]—information about a2 canconsistently be derived, maybe only by inferring that there is no information on a2, i.e., by deriving the probability 0.5for a2. This observation distinguishes K(cid:2)from K as for the latter a2 does not belong to the signature and therefore noinformation is derivable for a2 at all. Although this distinction is marginal, observe that there is a difference in inferringthat we have no information on a2 and that inference on a2 is not defined.We now turn to the property dominance [13] which can be phrased as follows. Let c1 |(cid:8)pr c2 be defined via Mod({c1}) ⊆1, c122Mod({c2}) for conditionals c1, c2.Dominance If c1 |(cid:8)pr c2 then I(K ∪ {c1}) (cid:2) I(K ∪ {c2}).The motivation of the property dominance in the classical setting is that logically stronger formulas have the potentialto bring more conflicts [13]. In the context of probabilistic conditional logic this property is vacuous as entailment byprobabilistic conditionals is trivial.M. Thimm / Artificial Intelligence 197 (2013) 1–247Table 1Comparison of inconsistency measures.PropertyConsistencyMonotonicitySuper-additivityIrrelevance of syntaxWeak independenceIndependenceMI-separabilityPenaltyNormalizationContinuityI0XX–XXX––X–IMII CMIXXXXXXXX––XXXXXXXX––IηXX–XXX––X–IpXXp = 1XXX(p = 1)––XIIpΣXXXXXXXX–XIhμXX–XX?––XXProposition 5. Let c1 = (ψ1 | φ1)[p1] and c2 = (ψ2 | φ2)[p2] be normal. If c1 |(cid:8)pr c2 then c2 ≡e c1.Applying this observation to the property dominance we obtainDominance If c1 ≡e c2 then I(K ∪ {c1}) = I(K ∪ {c2}),which is a weakening of the property irrelevance of syntax. For this reason, we will not consider the property dominance inwhat follows.4. Overview of resultsIn the following sections we investigate different inconsistency measures with respect to the properties defined above.We review inconsistency measures for classical logics and adapt them to the probabilistic case in Section 5. In particular,we investigate the drastic inconsistency measure I0, the MI inconsistency measure IMI, the MIC inconsistency measure I CMI,and the η-inconsistency measure Iη. Afterwards, we develop novel inconsistency measures for the probabilistic case inSection 6. More specifically, we develop the family of d-inconsistency measures ID that are based on distance measures Dand the family of Σ -inconsistency measures IIΣ that utilize other inconsistency measures. Finally, in Section 7 we alsoinvestigate another inconsistency measure Ihμ from related work [6].Table 1 summarizes the properties of the inconsistency measures discussed in this paper. Note that we only showthe properties of the p-norm distance inconsistency measure Ip as a particularly good representative for d-inconsistencymeasures ID . The properties of other d-inconsistency measures may vary, cf. Theorem 2. For the same reasons we only showthe properties of the Σ -inconsistency measure instantiated with the p-norm distance inconsistency measure. In Table 1 theentry “X” means that the inconsistency measure satisfies the given property, the entry p = 1 means that the property issatisfied if the condition is satisfied, an entry in parentheses means that satisfaction of the property is conjectured, and aquestion mark means that it is unclear whether the property is satisfied.In the following, we continue with providing the formal definitions of the inconsistency measures and the elaboration ofthe technical results.5. Classical inconsistency measuresWe start with a survey on existing approaches to inconsistency measurement for classical logic and adapt those tothe probabilistic case. In particular, we have a look at the drastic inconsistency measure, the MI inconsistency measure,the MIC inconsistency measure, and the η-inconsistency measure, see e.g. [12,17] for the classical definitions. What theseapproaches have in common, due to their origin, is that they concentrate on the qualitative part of inconsistency ratherthan the quantitative part, i.e. the probabilities.5.1. Drastic inconsistency measureThe simplest approach to define an inconsistency measure is by just differentiating whether a knowledge base is consis-tent or inconsistent.Definition 8 (Drastic inconsistency measure). Let I0 : K → [0, ∞) be the function defined as(cid:9)I0(K) =0 if K is consistent1 if K is inconsistentfor K ∈ K. The function I0 is called the drastic inconsistency measure.8M. Thimm / Artificial Intelligence 197 (2013) 1–24The drastic inconsistency measure allows only for a binary decision on inconsistencies and does not quantify the severityof inconsistencies. Although being a very simple inconsistency measure, I0 still satisfies several basic properties as the nextproposition shows.Proposition 6. The function I0 satisfies consistency, irrelevance of syntax, monotonicity, weak independence, independence, andnormalization.Notice, that I0 satisfies neither super-additivity, penalty, MI-separability, nor continuity.Example 7. Consider the knowledge bases K1 = (cid:10)c1, c2(cid:11) and K2 = (cid:10)c3, c4(cid:11) given viac1 = (a)[0.4]c2 = (a)[0.6]c3 = (b)[0.4]c4 = (b)[0.6].It follows that I0(K1) = I0(K2) = 1 butI0(K1 ∪ K2) = 1 (cid:14)= I0(K1) + I0(K2),therefore violating both super-additivity and MI-separability. Furthermore, c4 is not a free conditional in K1 ∪ K2 butI0(K1 ∪ K2 \ {c4}) = I0(K1 ∪ K2) violating penalty. Also, I0 fails to satisfy continuity as Im I0 = {0, 1} (Im f denotes theimage of the function f ).One thing to note is that I0 is the upper bound for any inconsistency measure that satisfies consistency and normalization,i.e., if I satisfies consistency and normalization then I(K) (cid:3) I0(K) for every K ∈ K [36].5.2. MIinconsistency measureThe next inconsistency measure quantifies inconsistency by the number of minimal inconsistent subsets of a knowledgebase.Definition 9 (MIinconsistency measure). Let IMI : K → [0, ∞) be the function defined asIMI(K) =(cid:10)(cid:10)(cid:10)(cid:10)MI(K)for K ∈ K. The function IMI is called the MIinconsistency measure.The definition of the MI inconsistency measure is motivated by the intuition that the more minimal inconsistent subsetsthe greater the inconsistency.Proposition 7. The function IMI satisfies consistency, monotonicity, super-additivity, weak independence, independence, irrele-vance of syntax, MI-separability, and penalty.Notice, that IMI satisfies neither normalization nor continuity.Example 8. Consider again K1 and K2 from Example 7. It holds that IMI(K1 ∪ K2) = 2 violating normalization. Also, IMI failsto satisfy continuity as Im IMI = N0 (the non-negative natural numbers).For a further discussion of the MI inconsistency measure we refer to [36].5.3. MIC inconsistency measureOnly considering the number of minimal inconsistent subsets may be too simple for assessing inconsistencies in general.Another indicator for the severity of inconsistencies is the size of minimal inconsistent subsets. A large minimal inconsistentsubset means that the inconsistency is distributed over a large number of conditionals. The more conditionals involved inan inconsistency the less severe the inconsistency is. Furthermore, a small minimal inconsistent subset means that theparticipating conditionals strongly represent contradictory information. Consider the following example for classical logicthat can be found in e.g. [12].Example 9. In a lottery there are n lottery tickets and only one of them is the winning ticket. If w i denotes the propositionthat ticket i will win the lottery then the (classical) formula φ = w 1 ∨ · · · ∨ wn can be regarded as true. Furthermore,the belief of each ticket buyer i is that he will not win the lottery, i.e., the formula φi = ¬w i is regarded to be true foreach i = 1, . . . , n. Obviously the set {φ, φ1, . . . , φn} is inconsistent as φ demands that one ticket has to win and, hence, oneticket owner k is wrong in assuming ¬wk. However, with increasing number of available tickets the inconsistency becomesnegligible and each ticket owner is justified in believing that he will not win.M. Thimm / Artificial Intelligence 197 (2013) 1–249Although the previous example has been formulated for classical logic the argument stands for probabilistic logics aswell.The following inconsistency measure is inspired by [12] and aims at differentiating between minimal inconsistent sets ofdifferent size.Definition 10 (MIC inconsistency measure). Let I CMI : K → [0, ∞) be the function defined asMI(K) =I C(cid:2)M∈MI(K)1|M|for K ∈ K. The function I CMI is called the MIC inconsistency measure.MI(K) = 0 if MI(K) = ∅.Note that I CThe MIC inconsistency measure sums over the reciprocal of the sizes of all minimal inconsistent subsets. In that way,a large minimal inconsistent subset contributes less to the inconsistency value than a small minimal inconsistent subset. Asthe MI inconsistency measure the MIC inconsistency measure behaves well with respect to many desirable properties.Proposition 8. The function I Cvance of syntax, MI-separability, and penalty.MI satisfies consistency, monotonicity, super-additivity, weak independence, independence, irrele-Note that I CMI satisfies neither normalization nor continuity.Example 10. Consider the knowledge base K = (cid:10)c1, . . . , c6(cid:11) given viac1 = (a)[0.4]c4 = (b)[0.6]c2 = (a)[0.6]c5 = (c)[0.4]c3 = (b)[0.4]c6 = (c)[0.6].It follows that I Crational numbers).MI(K) = 1.5 thus violating normalization. I CMI also fails to satisfy continuity as Im I CMI= Q+0 (the non-negativeFor a further discussion of the MIC inconsistency measure we refer to [36].5.4. η-Inconsistency measureThe work [17] employs probability theory itself to measure inconsistency in classical theories by considering probabilityfunctions on classical interpretations. Those ideas can be extended for measuring inconsistency in probabilistic logics byconsidering probability functions on probability functions. Let ˆP : F (At) → [0, 1] be a probability function on F (At) suchthat ˆP (P ) > 0 only for finitely many P ∈ F (At). Let F 2(At) be the set of those probability functions. Then defineˆP (c) =(cid:2)ˆP (P )P ∈F(At), P |(cid:8)pr c(2)for a conditional c. This means that the probability (in terms of ˆP ) of a conditional is the sum of the probabilities ofprobability functions that satisfy c. Note that this definition is similar in spirit to the definition of the probability of formulasin (1). The main difference is that in (1) formulas of the object level are propositional formulas and in (2) formulas of theobject level are probabilistic conditionals. Note also that by restricting ˆP to assign a non-zero value only to finitely manyP ∈ F (At), the sum in (2) is well-defined.Now consider the following definition of the η-inconsistency measure.Definition 11 (η-Inconsistency measure). Let Iη : K → [0, ∞) be the function defined as(cid:10)(cid:10) ∃ ˆP ∈ F 2(At): ∀c ∈ K: ˆP (c) (cid:2) ηIη(K) = 1 − maxη(cid:6)(cid:5)for K ∈ K. The function Iη is called the η-inconsistency measure.The idea of the η-inconsistency measure is that it looks for the largest probability that can be consistently assigned tothe conditionals of a knowledge base and defines the inconsistency value inversely proportional to this probability.Example 11. Let K be a knowledge base with K = (cid:10)(b | a)[0.9], (a)[0.9], (b)[0.1](cid:11). Note that K is inconsistent. AsA1 = {(b | a)[0.9], (a)[0.9]} is consistent,letlet P 1 ∈ F (At) be a probability function with P 1 |(cid:8)pr A1. Similarly,10M. Thimm / Artificial Intelligence 197 (2013) 1–24A2 = {(b | a)[0.9], (b)[0.1]}, A3 = {(a)[0.9], (b)[0.1]} and P 2, P 3 ∈ F (At) such that P 2 |(cid:8)pr A2 and P 3 |(cid:8)pr A3. Then defineˆP ∈ F 2(At) viaˆP (P 1) = ˆP (P 2) = ˆP (P 3) = 1/3ˆP (P ) = 0 for P ∈ F(At) \ {P 1, P 2, P 3}.It follows(cid:7)(cid:8)(b | a)[0.9]ˆP(cid:2)=P ∈F(At), P |(cid:8)pr cˆP (P ) = ˆP (P 1) + ˆP (P 2) = 2/3and similarly ˆP ((a)[0.9]) = ˆP ((b)[0.1]) = 2/3. It is also easy to see that there is no ˆPc ∈ K. Therefore, it follows Iη(K) = 1 − 2/3 = 1/3.(cid:2) ∈ F 2(At) such that ˆP(cid:2)(c) > 2/3 for allSeveral properties for the η-inconsistency measure can be directly derived from properties of its classical counterpart.For example, the following proposition is a direct extension of Theorem 2.12 in [18].Proposition 9. If MI(K) = {K} then Iη(K) = 1/|K|.As for the properties proposed in the previous section consider the following proposition.Proposition 10. The function Iη satisfies consistency, monotonicity, weak independence, independence, irrelevance of syntaxand normalization.Note that Iη does not satisfy super-additivity, penalty, MI-separability and continuity.Example 12. Consider again the knowledge bases K1 = (cid:10)c1, c2(cid:11) and K2 = (cid:10)c3, c4(cid:11) from Example 7 given viac1 = (a)[0.4]c2 = (a)[0.6]By Proposition 9 it follows that Iη(K1) = Iη(K2) = 1/2 butc3 = (b)[0.4]c4 = (b)[0.6].Iη(K1 ∪ K2) = 1/2as well, therefore violating both super-additivity and MI-separability (observe that there are P 1, P 2 such that P 1 |(cid:8)pr c1, c3and P 2 |(cid:8) c2, c4). Consider now the knowledge base K3 = (cid:10)(a)[0.4], (a)[0.6], (¬a)[0.4](cid:11). Note that (¬a)[0.4] ∈ K3 is not afree conditional in K3. However, it holds thatIη(K3) = Iη(cid:5)(cid:7)K3 \(cid:6)(cid:8)(¬a)[0.4]= 1/2as there are P 1, P 2 ∈ F (At) with P 1 |(cid:8)pr (a)[0.4], (¬a)[0.6] and P 2 |(cid:8)pr (a)[0.6]. Consider now the knowledge base Kx =(cid:10)(a)[0.2], (a)[x](cid:11). It holds that Iη(Kx) = 1/2 for x (cid:14)= 0.2 and Iη(Kx) = 0 for x = 0.2. Therefore, Iη fails to satisfy continuity.5.5. Classical inconsistency measures and continuityThe inconsistency measures discussed above were initially developed for inconsistency measurement in classical theoriesand therefore allow only for a “discrete” measurement. Hence, all of the above discussed inconsistency measures do notsatisfy continuity. But satisfaction of continuity is crucial for an inconsistency measure in probabilistic logics in order toassess inconsistencies in a meaningful manner, cf. also Section 3.Example 13. Consider the knowledge base K = (cid:10)c1, c2, c3(cid:11) given viac1 = (b | a)[1]c2 = (a)[1]c3 = (b)[0].The knowledge base K is inconsistent and the set of minimal inconsistent subsets is given by MI(K) = {{c1, c2, c3}}. Itfollows thatI0(K) = 1IMI(K) = 1Consider a modification K(cid:2) = (cid:10)c(cid:2)1, cMI(K) = 1I C3(cid:11) of K given via(cid:2)2, c(cid:2)3Iη(K) = 13.(cid:2)1c= (b | a)[1](cid:2)2c= (a)[1](cid:2)3c= (b)[0.999].The knowledge base K(cid:2)knowledge base K(cid:2)(cid:2) = (cid:10)c(cid:2)(cid:2)1c= (b | a)[1]M. Thimm / Artificial Intelligence 197 (2013) 1–2411is still inconsistent and it holds that I(K(cid:2)) = I(K) for I ∈ {I0, IMI, I C(cid:2)(cid:2)1, c(cid:11) given via(cid:2)(cid:2)2, c(cid:2)(cid:2)3MI, Iη}. Now consider thec= (a)[1](cid:2)2is consistent and it follows that I0(K(cid:2)(cid:2)) = IMI(K(cid:2)(cid:2)) = I C= (b)[1].(cid:2)(cid:2)3cone can discover only a minor difference of the modeled knowledge. Whereas in K(cid:2)(cid:2)The knowledge base K(cid:2)(cid:2)K(cid:2)(cid:2)probability of 1 in K(cid:2)relevance. Still, a knowledge engineer may not grasp the harmlessness of the inconsistency in K(cid:2)of inconsistency with respect to those classical measures.andthe proposition b is assigned ait is assigned a probability of 0.999. From a practical point of view this difference may be of noas K has the same degreeMI(K(cid:2)(cid:2)) = Iη(K(cid:2)(cid:2)) = 0. By comparing K(cid:2)The above example motivates the need for a more graded approach to measure the inconsistencies in K, K(cid:2), and K(cid:2)(cid:2).This measure should assign K(cid:2)a much smaller inconsistency value than to K in order to distinguish their severities. In thenext section, we continue with an investigation of inconsistency measures that take the probabilities of conditionals intoaccount and therefore satisfy those needs.6. Inconsistency measures based on distance minimizationAs can be seen in Example 13 the probabilities of conditionals play a crucial role in creating inconsistencies. In order torespect this role we propose a family of inconsistency measures that is based on the distance to consistency. To this end weemploy the notion of a distance measure.6.1. The d-inconsistency measureThe obvious difference between classical knowledge bases—i.e. sets of classical formulas—and probabilistic knowledgebases is that the latter are parametrized by probabilities. Therefore, given a knowledge base of a fixed qualitative structurethe different instantiations of probabilities can be represented within the vector space [0, 1]|K|. In a vector space, thetraditional means of measuring differences are distance measures.Definition 12 (Distance measure). Let n ∈ N+following properties:. A function dn : Rn × Rn → [0, ∞) is called a distance measure if it satisfies the1. dn((cid:19)x, (cid:19)y) = 0 if and only if (cid:19)x = (cid:19)y (reflexivity).2. dn((cid:19)x, (cid:19)y) = dn((cid:19)y, (cid:19)x) (symmetry).3. dn((cid:19)x, (cid:19)y) (cid:3) dn((cid:19)x, (cid:19)z) + dn((cid:19)z, (cid:19)y) (triangle inequality).For n ∈ N+let Dn denote the set of all distance measures dn : Rn × Rn → [0, ∞). Let D =(cid:11)n∈N+ Dn.The simplest form of a distance measure is the drastic distance measure d0n defined as d0n((cid:19)x, (cid:19)y) = 0 for (cid:19)x = (cid:19)y andn((cid:19)x, (cid:19)y) = 1 for (cid:19)x (cid:14)= (cid:19)y (for (cid:19)x, (cid:19)y ∈ Rn and n ∈ N+d0). A more interesting distance measure is the p-norm distance.Definition 13 (p-Norm distance). Let n, p ∈ N+p. The function dn: Rn × Rn → [0, ∞) defined via(cid:12)n ((cid:19)x, (cid:19)y) = pdp|x1 − y1|p + · · · + |xn − yn|pfor (cid:19)x = (cid:10)x1, . . . , xn(cid:11), (cid:19)y = (cid:10) y1, . . . , yn(cid:11) ∈ Rn is called the p-norm distance.Special cases of the p-norm distance include the Manhattan distance (for p = 1) and the Euclidean distance (for p = 2).In order to deal with vector spaces of different dimensions we also consider distance generators which map a dimensionn ∈ N+to a corresponding distance function.Definition 14 (Distance generator). A distance generator D is a function D : N+ → D such that D(n) ∈ Dn for all n ∈ N+Let D be a distance generator..1. D is monotonously generating if(cid:7)D(n)(cid:10)x1, . . . , xn(cid:11), (cid:10) y1, . . . , yn(cid:11)(cid:8)(cid:7)(cid:3) D(n + 1)(cid:10)x1, . . . , xn+1(cid:11), (cid:10) y1, . . . , yn+1(cid:11)(cid:8)(3)for every n ∈ N+and x1, . . . , xn+1, y1, . . . , yn+1 ∈ R.12M. Thimm / Artificial Intelligence 197 (2013) 1–242. D is super-additively generating if(cid:10)x1, . . . , xn(cid:11), (cid:10) y1, . . . , yn(cid:11)(cid:10)xn+1, . . . , xn+m(cid:11), (cid:10) yn+1, . . . , yn+m(cid:11)(cid:7)D(n)(cid:7)(cid:3) D(n + m)for every n, m ∈ N+(cid:10)x1, . . . , xn+m(cid:11), (cid:10) y1, . . . , yn+m(cid:11)and x1, . . . , xn+m, y1, . . . , yn+m ∈ R.(cid:8)(cid:7)+ D(m)(cid:8)(cid:8)3. D is symmetric generating if(cid:7)D(n)(cid:10)x1, . . . , xn(cid:11), (cid:10) y1, . . . , yn(cid:11)(cid:8)(cid:7)= D(n)(cid:10)x1, . . . , 1 − xi, . . . , xn(cid:11), (cid:10) y1, . . . , 1 − yi, . . . , yn(cid:11)(cid:8)for every i = 1, . . . , n and n ∈ N+.4. D is continuously generating if D(n) is continuous for every n ∈ N+.(4)(5)Although distance generators may be defined quite arbitrarily we consider the drastic distance generator D 0 defined viaD0(n) = d0n (for every n ∈ N+) and the p-norm distance generator D p defined via D p(n) = dpn (for every n, p ∈ N+).Coming back to the issue of measuring inconsistency one can define the “severity of inconsistency” in a knowledge baseby the minimal distance of the knowledge base to a consistent one. As we are able to identify knowledge bases of the samequalitative structure in a vector space, we can employ distance measures for measuring inconsistency.Definition 15 (d-Inconsistency measure). Let D be a distance generator. Then the function ID : K → [0, ∞) defined via(cid:5)(cid:7)|K|(cid:8)((cid:19)x, (cid:19)y)(cid:10)(cid:10) K = K[(cid:19)x] and K[(cid:19)y] is consistent(cid:6)(6)ID (K) = minDfor K ∈ K is called the d-inconsistency measure.The idea behind the d-inconsistency measure is that we look for a consistent knowledge base that both (1) has thesame qualitative structure as the input knowledge base and (2) is as close as possible to the input knowledge base (thatthere always exists such a knowledge base see the proof of Theorem 1 below). That is, if the input knowledge base is K[(cid:19)x]we look at all (cid:19)y ∈ [0, 1]|K|such that K[(cid:19)y] is consistent and (cid:19)x and (cid:19)y are as close as possible with respect to the distancemeasure D(|K|).As we are mainly working with the p-norm distance we abbreviate ID p simply by Ip . Looking at Eq. (6) it is not obviousthat ID is well-defined as e.g. the minimum might not be defined on the set of distance values. However, as the followingtheorem shows the d-inconsistency measure is well-defined for every reasonable distance measure.Theorem 1. If D is continuously generating the function ID is well-defined.As the p-norm distance is a continuous function it also follows that Ip is well-defined for every p ∈ N. In [34,36]the measure I1 has been investigated in a preliminary fashion while [21,35] contain some first discussions of the generalp-norm distance inconsistency measure. In particular, in [21] it has been shown that for every p, pthetwo measures Ip and Ip(cid:2) are not equivalent, i.e., there are knowledge bases K1 and K2 such that Ip(K1) > Ip(K2) butIp(cid:2) (K1) < Ip(cid:2) (K2).Note that D being continuously generating is a sufficient but not necessary requirement for ID being well-defined.with p (cid:14)= p(cid:2) ∈ N+(cid:2)Consider also the following observation.Proposition 11. The function ID0 is well-defined and it holds that ID0 = I0.Before we investigate the formal properties of the above measure we first have a look at an example.Example 14. We continue Example 13 with the knowledge base K = (cid:10)c1, c2, c3(cid:11) given viac1 = (b | a)[1]c2 = (a)[1]c3 = (b)[0]and consider the p-norm distance inconsistency measure. In particular, observe that for K∗ = (cid:10)(b | a)[1], (a)[0.5], (b)[0.5](cid:11) itholds that(cid:5)K∗ ∈ arg min3 ((cid:19)x, (cid:19)y)d. That is, K∗p(cid:10)(cid:10) K = K[(cid:19)x] and K[(cid:19)y] is consistentis a consistent knowledge base that has minimal p-norm distance to K for all p ∈ N+(cid:6). Infor all p ∈ N+√particular, it holds that Ip(K) = p2 · 0.5p . For example, it holds thatI1(K) = 1 and I2(K) ≈ 0.707.Furthermore, it holds that I1(K(cid:2)) = 0.001 and I2(K(cid:2)) ≈ 0.00071, and clearly I1(K(cid:2)(cid:2)) = I2(K(cid:2)(cid:2)) = 0.As with respect to the properties proposed in the previous section consider the following results.M. Thimm / Artificial Intelligence 197 (2013) 1–2413Theorem 2. Let D be a distance generator such that ID is well-defined.1. The function ID satisfies consistency.2. If D is monotonously generating then ID satisfies monotonicity.3. If D is super-additively generating then ID satisfies super-additivity.4. If D is symmetric generating then ID satisfies irrelevance of syntax.5. If D is continuously generating then ID satisfies continuity.As for the specific case of the p-norm distance inconsistency measure consider the following theorem.Theorem 3. Let p ∈ N+.1. The function Ip satisfies consistency, monotonicity, weak independence, independence, irrelevance of syntax, and continu-ity.2. If p = 1 then Ip satisfies super-additivity.The property MI-separability is not, in general, satisfied by ID as the following example shows.Example 15. Let K = (cid:10)(a)[0.3], (a)[0.7], (b)[0.3], (b)[0.7](cid:11). It is easy to see thatI1(K) = 0.4 + 0.4 = 0.8I2(K) =(cid:12)0.22 + 0.22 + 0.22 + 0.22 = 0.4.It also holds that(cid:7)(cid:3)(a)[0.3], (a)[0.7](b)[0.3], (b)[0.7](a)[0.3], (a)[0.7](b)[0.3], (b)[0.7](cid:4)(cid:8)(cid:4)(cid:8)(cid:7)(cid:3)(cid:7)(cid:3)= I1= I2(cid:4)(cid:8)(cid:4)(cid:8)= 0.4 and(cid:12)=0.22 + 0.22 ≈ 0.283.I1I2(cid:7)(cid:3)For p = 1 it follows that(cid:7)(cid:3)I1(K) = I1(a)[0.3], (a)[0.7](cid:4)(cid:8)+ I1(cid:7)(cid:3)(b)[0.3], (b)[0.7],therefore satisfying MI-separability. However, for p = 2 it follows that(cid:7)(cid:3)I2(K) < I2(a)[0.3], (a)[0.7](cid:4)(cid:8)+ I2(cid:7)(cid:3)(b)[0.3], (b)[0.7](cid:4)(cid:8)(cid:4)(cid:8)violating MI-separability—and also super-additivity—as (cid:10)(a)[0.3], (a)[0.7](cid:11) and (cid:10)(b)[0.3], (b)[0.7](cid:11) partition the set of minimalinconsistent subsets of K.As the above example suggests MI-separability seems to be satisfied for Ip with p = 1. However, neither a counterexam-ple nor a formal proof has been found yet.Conjecture 1. If p = 1 then Ip satisfies MI-separability.Observe that Ip does not satisfy penalty which has been mistakenly claimed in [34]. Consider the following counterex-ample.Example 16. Consider the knowledge base K = (cid:10)(a)[0.7], (a)[0.3](cid:11) and the probabilistic conditional (a)[0.5]. Then (a)[0.5]it holds that I1(K) = I1(K(cid:2)) = 0.4—asis not free in K(cid:2) = K ∪ {(a)[0.5]} as {(a)[0.3], (a)[0.5]} ∈ MI(K(cid:2)). However,(cid:10)(a)[0.5], (a)[0.5](cid:11) has minimal distance to K and (cid:10)(a)[0.5], (a)[0.5], (a)[0.5](cid:11) has minimal distance to K(cid:2)—which violatespenalty.As for normalization consider the following counterexample.Example 17. Consider the knowledge base K = (cid:10)(a)[0], (a)[1], (b)[0], (b)[1](cid:11). It is easy to see that I1(K) = 2 violating nor-malization.However, a normalized variant of Ip can be defined by exploiting Ip(K) (cid:3) |K| for all K ∈ K, cf. [36].14M. Thimm / Artificial Intelligence 197 (2013) 1–246.2. The Σ -inconsistency measureThe main drawback of the inconsistency measure discussed above is that it does not satisfy penalty. However, this issuecan be solved by the following compound measure.Definition 16. Let K be a knowledge base and let I be an inconsistency measure. Then define the Σ -inconsistency measureIIΣ (K) of K and I via(cid:2)IIΣ (K) =I(M).M∈MI(K)The Σ -inconsistency measure is defined as the sum of the inconsistency values of all minimal inconsistent subsets ofthe knowledge base under consideration. The following property is easy to see and given without proof.Proposition 12. Let I be an inconsistency measure. If MI(K) = {K} then IIΣ (K) = I(K).The above proposition states that IIbases consider the following example.Σ (K) is the same as I(K) if K is minimally inconsistent. For general knowledgeExample 18. We continue Example 16 and consider the knowledge base K = (cid:10)(a)[0.7], (a)[0.3](cid:11) and the probabilistic condi-tional (a)[0.5]. Observe thatII1Σ (K) = I1(K) = 0.4II1Σ(a)[0.5]K ∪(cid:6)(cid:8)(cid:5)(cid:7)= I1(K) + I1(cid:7)(cid:3)(a)[0.7], (a)[0.5](cid:4)(cid:8)+ I1(cid:7)(cid:3)(a)[0.3], (a)[0.5]= 0.4 + 0.2 + 0.2 = 0.8.(cid:4)(cid:8)Therefore, the addition of the non-free conditional (a)[0.5] to K has been penalized by II1Σ .As hinted above, the Σ -inconsistency measure IIΣ (K) behaves well with respect to the property penalty, provided thatthe inner measure is a reasonable inconsistency measure. Consider the following theorem.Theorem 4. Let I be an inconsistency measure.Σ satisfies monotonicity, super-additivity, weak independence, independence, and MI-separability.1. II2. If I satisfies consistency then II3. If I satisfies irrelevance of syntax then II4. If I satisfies continuity then IIΣ satisfies consistency and penalty.Σ satisfies irrelevance of syntax.Σ satisfies continuity.The following corollary is a direct application of Theorems 3 and 4.then IIpCorollary 1. If p ∈ N+separability, penalty, irrelevance of syntax, and continuity.Σ satisfies consistency, monotonicity, super-additivity, weak independence, independence, MI-As one can see, the Σ -inconsistency measure performs well with respect to all properties except normalization.7. Related workThe problem of measuring inconsistency in probabilistic knowledge bases is relatively novel and has—to our knowledge—only been addressed before in [31,6] and [21]. We have a more closer look on the works [21] and [6] below.Further related work is concerned with measuring inconsistency in classical theories, see e.g. the works by Hunter et al.[12,10,13]. While[12,13] deal with measuring inconsistency in propositional logic, the work [10] considers first-order logic.Those works also take a principled approach to measuring inconsistency and many of our properties have been adapted frome.g. [12]. Furthermore, the inconsistency measures presented in Section 6 are straightforward translations of inconsistencymeasures from those works. However, Hunter et al. are working with classical theories and as such do not have to deal withprobabilities as a means for knowledge representation. In order to adhere for the presence of probabilities we introducedcontinuous inconsistency measures which have no correspondent in the classical setting.Besides the inconsistency measures discussed here another form of measuring inconsistency can be realized using culpa-bility measures [6,34], also used under the term inconsistency values in [13]. A culpability measure does not assign a degreeof inconsistency to the whole knowledge base but to each individual element of the knowledge base. The interpretationM. Thimm / Artificial Intelligence 197 (2013) 1–2415of culpability measures is that they assign a degree of “blame” for creating an inconsistency to an element. In [13] such ameasure has been defined in terms of some ordinary inconsistency measure and the Shapley value, a well-known solutionfor solving coalition games in game theory [32]. This approach can also be applied for inconsistency measures for proba-bilistic logics as has been done for the measure I1 in [34]. Furthermore, in [6] the measure Ihμ (see below) has also beenextended to a culpability measure.We go on by taking a closer look on the works by Muiño [21] and Daniel [6], for some analysis on [31] see [36].7.1. Infinitesimal inconsistency valuesThe research presented in this paper is complementary to the work in [21]. The paper [21] also discusses the Ip measurebut focuses on (1) the problem of infinitesimal inconsistency values and (2) the application of Ip on the medical knowledgebase CADIAG-2. In particular, it is not investigated how Ip behaves with respect to the principles above.The problem of infinitesimal inconsistency values appears when one defines probabilistic satisfaction via P |(cid:8)pralt (ψ | φ)[p]if and only ifP (ψ | φ) = p and P (φ) > 0.A knowledge base K is |(cid:8)prmeasure I(cid:2)p from [21] can be defined via(cid:7)(cid:5)I(cid:2)p(K) = minD p|K|alt-consistent if there exists P ∈ F (At) with P |(cid:8)praltK. Using our notation, the inconsistency(cid:8)((cid:19)x, (cid:19)y)(cid:10)(cid:10) K = K[(cid:19)x] and K[(cid:19)y] is |(cid:8)pralt-consistent(cid:6).(7)Theorem 1 does not apply for I(cid:2)always defined.p as the set {(cid:19)y | K[(cid:19)y] is |(cid:8)pralt-consistent} is not closed. Therefore, the minimum in (7) is notExample 19. Consider the knowledge base K = (cid:10)(a)[0], (b | a)[0.7](cid:11). Note that K is consistent (using our notion of proba-alt-consistent as there is no probability function P with P |(cid:8)prbilistic satisfaction) but not |(cid:8)pralt (b | a)[0.7].However, one can easily construct a sequence of probability functions P 1, P 2, . . . such that P i(a) > 0 and P i(b | a) = 0.7 fori ∈ N andalt (a)[0] and P |(cid:8)prP (a) = 0.limn→∞In [21] knowledge bases like K above are assigned an infinitesimal inconsistency value. The motivation for introducinginfinitesimal inconsistency values stems from the application of Ip on the medical knowledge base, a collection of expertrules relating symptoms and diseases. In [21] it is shown that CADIAG-2 has an infinitesimal inconsistency value.7.2. Candidacy degrees of best candidatesAmong others, one contribution of [6] is an inconsistency measure on knowledge bases of probabilistic constraints. Inparticular, the work [6] focuses on linear probabilistic knowledge bases but also considers generalizations such as polynomialprobabilistic knowledge bases. However, in order to compare it to our work we simplify several notations and present theinconsistency measure Ihμ of [6] only for probabilistic conditional logic.The central notion of [6] is the candidacy function. A candidacy function is similar to a fuzzy set as it assigns a degree ofmembership of a probability function belonging to the models of a knowledge base. More specifically, a candidacy functionC is a function C : F (At) → [0, 1]. A uniquely determined candidacy function CK can be assigned to a (consistent or incon-sistent) knowledge base K as follows. For a probability function P ∈ F (At) and a set of probability functions S ⊆ F (At) letdE (P , S) denote the distance of P to S with respect to the Euclidean norm, i.e., dE (P , S) is defined via(cid:10)(cid:10)(cid:10)(cid:10) P(cid:8)P (ω) − P (cid:2)(ω)dE (P , S) = inf(cid:13)(cid:14) (cid:2)(cid:2) ∈ S(cid:15)(cid:7).2ω∈Ω(At)Let h : R+ → (0, 1] be a strictly decreasing, positive, and continuous log-concave function with h(0) = 1. Then the candidacyfunction ChK for a knowledge base K is defined as(cid:7)(cid:7)(cid:12)(cid:16)(cid:8)(cid:8)(cid:8)2|At|dE(cid:7)P , Mod{c}K(P ) =Chhc∈Kfor every P ∈ F (At). Note that the definition of the candidacy function ChK depends on the size of the signature At. Theintuition behind this definition is that a probability function P that is near to the models of each probabilistic conditional16M. Thimm / Artificial Intelligence 197 (2013) 1–24in K gets a high candidacy degree in Chcandidacy function ChK the inconsistency measure Ihμ can be defined viaK(P ). It is easy to see that it holds that ChK(P ) = 1 if and only if P |(cid:8)pr K. Using theμ(K) = 1 − maxIhP ∈F(At)ChK(P )for a knowledge base K. In [6] it is shown that Ihμ satisfies (among others) the following properties.Proposition 13. (See [6].) Ihμ satisfies consistency, monotonicity, continuity, and normalization.Furthermore, it can also be shown that the function Ihμ satisfies the following properties.Theorem 5. Ihμ satisfies irrelevance of syntax and weak independence.In Example 6 we talked about the issue of an inconsistency measure satisfying all three of consistency, super-additivity,and normalization. We showed that an inconsistency measure that does not take the cardinality of the signature into accountcannot satisfy all these properties at once. As one can see above, the function Ihμ takes the cardinality of the signature intoaccount and it may be possible that Ihμ satisfies super-additivity. However, this is not the case as the following exampleshows.Example 20. Let At = {a1, a2} be a propositional signature and let K1 = (cid:10)c1, c2(cid:11) and K2 = (cid:10)c3, c4(cid:11) be knowledge bases withc2 = (a1)[0]c4 = (a2)[0]c1 = (a1)[1]c3 = (a2)[1]and let K = K1 ∪ K2. Note that both K1 and K2 are inconsistent and K1 ∩ K2 = ∅. As Ihμ is defined on the semantic levelμ(K2). As the situations in K1 and K2and does not take the names of propositions into account it follows that Ihare symmetric and Ki is symmetric with respect to c1 and c2 and with respect to c3 and c4 there are probability functionsμ(Ki) = 1 − C hP i with IhKi(cid:7)(cid:7)(cid:8)(cid:8)P 1, Mod(P i) for i = 1, 2 and(cid:8)(cid:8)μ(K1) = Ih(cid:7){c2}P 1, ModP 2, ModP 2, Mod= dE= dE= dE(cid:8)(cid:8).{c1}{c3}{c4}dE(cid:8)(cid:8)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)Let x = dE (P 1, Mod({c1})) and let hwith h∗Ihμ must satisfy∗(0) = 1 and h∗(√∗2|At|x) = 0.5. Then it follows C hK1∗ : R+ → (0, 1] be a strictly decreasing, positive, and continuous log-concave functionμ (K1) = 0.75. In order to satisfy super-additivity(P 1) = 0.25 and Ih∗∗∗μ (K) (cid:2) IhIhμ (K1) + Ihμ (K2) = 1.5∗which is a contradiction since Ih∗μ satisfies normalization.The above example is also a counterexample for MI-separability as K1 and K2 partition the set of minimal inconsistentμ also fails to satisfy penalty for similar reasons as Id fails to satisfysubsets. Furthermore, it can be easily seen that Ihpenalty. For the knowledge base K = (cid:10)(b | a)[1], (a)[1], (b)[0](cid:11) let P(cid:2)be such that(8)maxP ∈F(At)K(P ) = ChChK(cid:7)(cid:8)(cid:2).P(cid:2)In other words, P(cid:2)fails to satisfy at least one of the probabilistic conditionals of K. Assume that it holds that Pit follows that P(cid:2)(a) > 0. Consider the knowledge base K(cid:2) = K ∪ {cwhich implies P√follows Ihis a probability function that has the maximal candidacy degree with respect to K. As K is inconsistent,(cid:2) (cid:14)|(cid:8)pr (b | a)[1]μ satisfies monotonicity italso satisfies(cid:2)} with c(cid:2)}))) = 1, as dE (P(cid:2)})) = 0, it follows that P(cid:2) = (b | a)[P(cid:2), Mod({c(cid:2)(b | a)]. As Ih(cid:2), Mod({c2|At|dE (P(cid:2)μ(K(cid:2)) (cid:2) Ihμ(K) and due to h((cid:7)(cid:8)K(cid:2) (P ) = ChChK(cid:2)(cid:2)P.maxP ∈F(At)Therefore, P(cid:2)tent with Pinconsistent subset of K(cid:2)(cid:2)has also maximal candidacy degree with respect to K(cid:2)(otherwise Pwould have violated (8)). It follows Ih(cid:2)μ(K(cid:2)) (cid:3) Ihwhich is clear as we only added information consis-(cid:2)} is a minimal(cid:2) (cid:14)|(cid:8)pr (b)[0].μ(K) and as {(b | a)[1], (a)[1], c(cid:2) (cid:14)|(cid:8)pr (a)[1] or Pthis contradicts penalty. Similar observations can be made when PIn [6] it is shown that Ihsee [36] for a discussion. It is also still an open issue whether Ihμ satisfies several other properties that cannot be related directly to our properties of Section 3,μ satisfies independence.M. Thimm / Artificial Intelligence 197 (2013) 1–24178. Summary and discussionAnalyzing inconsistencies is of major concern in the area of knowledge representation as consistency is a necessaryprerequisite for many knowledge representation formalisms. In particular, the task of inference bases mostly on the con-sistency of the underlying information. In this paper, we investigated inconsistency measures for probabilistic conditionallogic. For that, we developed a series of rationality postulates for inconsistency measures motivated by both inconsistencymeasurement for classical logics and the peculiarities of probabilistic knowledge representation. We adapted several classicalinconsistency measures and showed that they lack a particularly important property for the probabilistic domain, namely,a continuous behavior with respect to modifications of the knowledge base. Consequently, we investigated inconsistencymeasures based on distance measures and showed that these measures are more apt for the probabilistic domain. Wecompared these measures with related work, in particular with the approach of [6].In this paper, we used probabilistic conditional logic for knowledge representation which suffices for many application ar-eas that need to represent rule-like information. However, probabilistic conditional logic is not capable of expressing generallinear relationships such as “a is twice as probable as b” or polynomial relationships such as “a and b are probabilisticallyindependent”. Furthermore, using point probabilities can be seen as too restricting as well and one may want to representconditionals of the form (ψ | φ)[u, l] with the intended meaning that P (ψ | φ) ∈ [u, l], cf. [20]. The motivation of using thesimple framework of probabilistic conditional logic here merely stems from reasons of presentation rather than inadequacyof the ideas to more complex frameworks. In [36] the inconsistency measure I1 has also been defined for the more gen-eral frameworks of linear probabilistic knowledge bases and probabilistic conditional logic with intervals. Generalizing thoseideas to the discussion from this paper is straightforward.The focus of the discussion in this paper was mainly on properties of inconsistency measures and not on their algorithmiccomputation and complexity. However, Eq. (6) already induces a straightforward method to compute the value ID (K) for aspecific knowledge base K by representing (6) as an optimization problem, see [34,21] for formalizations. Note that theseoptimization problems are, in general, non-convex. Furthermore, a subproblem of determining the inconsistency value for aknowledge base K is checking consistency of probabilistic conditional knowledge bases which is an NP-hard problem [23].Therefore, determining ID (K) is in general a hard task and future work comprises of investigating scalable approaches.Some first steps have already been conducted in [36] by approximating ID (K) by “similar” convex optimization problems.Future work comprises of developing optimized algorithms by utilizing e.g. more sophisticated methods for probabilisticconsistency checking [7]. The basic approach for computing ID (K) using non-convex optimization methods has also beenimplemented in the Tweety library for artificial intelligence.1Appendix A. Proofs of technical resultsProposition 3. If c is safe in K then c is free in K.Proof. Assume that c is not free in K ∪ {c}. Then there is a set M ∈ MI(K) with c ∈ M. As M \ {c} is consistent andAt(M \ {c}) ∩ At({c}) = ∅ (as c is safe in K) let P 1 be a probability function in F (At \ At({c})) with P 1 |(cid:8)pr M \ {c}. As c isnormal let P 2 be a probability function in F (At({c})) with P 2 |(cid:8)pr c. Let ω ∈ Ω(At) and define ωA with A ⊆ At to be theprojection of ω on A, i.e. ωA ={a | a ∈ A, ω |(cid:8) a} ∪ {¬a | a ∈ A, ω |(cid:8) ¬a}. Define a probability function P in F (At) via(cid:17)P (ω) = P 1(ωAt\At({c})) · P 2(ωAt({c}))for all ω ∈ Ω(At). Note that f : Ω(At) → Ω(At \ At({c})) × Ω(At({c})) withf (ω) = (ωAt\At({c}), ωAt({c}))is a bijection. It follows that P is indeed a probability function as(cid:2)(cid:2)P (ω) =ω∈Ω(At)ω∈Ω(At)P 1(ωAt\At({c})) · P 2(ωAt({c}))(cid:2)P 1(ω1)P 2(ω2)(ω1,ω2)∈Ω(At\At({r}))×Ω(At({c}))(cid:2)(cid:2)ω1∈Ω(At\At({c}))(cid:2)ω2∈Ω(At({c}))(cid:18)P 1(ω1) ·P 1(ω1)P 2(ω2)(cid:2)(cid:19)P 2(ω2)===ω1∈Ω(At\At({c}))ω2∈Ω(At({c}))= 1.1 http://sourceforge.net/projects/tweety/.18M. Thimm / Artificial Intelligence 197 (2013) 1–24Furthermore, for ω ∈ Ω(At \ At({c})) it holds that(cid:2)(cid:2)(cid:8)(cid:7)P (ω) =ω ∧ ω(cid:2)P=P 1(ω)P 2(cid:8)(cid:7)ω(cid:2)= P 1(ω)(cid:2)(cid:8)(cid:7)ω(cid:2)P 2= P 1(ω)ω(cid:2)∈Ω(At({c}))ω(cid:2)∈Ω(At({c}))ω(cid:2)∈Ω(At({c}))and similarly P (ω(cid:2)) = P 2(ω(cid:2)). It follows that P |(cid:8)pr M \ {c} and P |(cid:8)pr c contradicting the assumption that M is a minimalinconsistent subset. (cid:2)Proposition 4. Let I be an inconsistency measure and let K, K(cid:2)be some knowledge bases.1. If I satisfies super-additivity then I satisfies monotonicity.2. If I satisfies independence then I satisfies weak independence.3. If I satisfies MI-separability then I satisfies independence.4. K ⊆ K(cid:2)5. If I satisfies independence then MI(K) = MI(K(cid:2)) implies I(K) = I(K(cid:2)).6. If I satisfies independence and penalty then MI(K) (cid:2) MI(K(cid:2)) implies I(K) < I(K(cid:2)).implies MI(K) ⊆ MI(K(cid:2)).Proof.1. Let I satisfy super-additivity. If c ∈ K then I(K) = I(K ∪ {c}). If c /∈ K then I(K ∪ {c}) (cid:2) I(K) + I({c}) (cid:2) I(K) due tosuper-additivity.2. Let I satisfy independence and let c be safe in K. By Proposition 3, c is also free in K and it follows I(K \ {c}) = I(K)by independence and, hence, I satisfies weak independence.3. Let I satisfy MI-separability and let c be free in K. Observe that MI({c}) = ∅ as c is normal. Then it also holds thatMI(K) = MI(K \ {c}) = MI(K \ {c}) ∪ MI({c}) and MI(K \ {c}) ∩ MI({c}) = ∅. By MI-separability it follows that I(K) =I(K \ {c}) + I({c}) = I(K \ {c}).4. Let M ∈ MI(K) be a minimal inconsistent subset of K. Then it holds that M ⊆ K ⊆ K(cid:2). Suppose M /∈ MI(K(cid:2)) which isequivalent to stating that either M is not minimal or not inconsistent. Both cases contradict the assumption M ∈ MI(K).only contains free conditionals of KM. It holds that I(K) = I(K(cid:2)(cid:2)) due to the facts that K \ K(cid:2)(cid:2)5. Let K(cid:2)(cid:2) =(cid:11)and that I satisfies independence. As the same is true for K(cid:2)it follows I(K) = I(K(cid:2)).6. Let K(cid:2)(cid:2) =and that I satisfies independence. As K(cid:2)(cid:2) (cid:2) K(cid:2)is not free in K(cid:2)M. It holds that I(K) = I(K(cid:2)(cid:2)) due to the facts that K \ K(cid:2)(cid:2)only contains free conditionals of Kdue to MI(K) (cid:2) MI(K(cid:2)) and K(cid:2) \ K contains at least one conditional c that—otherwise it would be MI(K) = MI(K(cid:2))—it follows I(K(cid:2)) > I(K(cid:2)(cid:2)) = I(K) as I satisfies penalty. (cid:2)M∈MI(K)M∈MI(K)(cid:11)Proposition 5. Let c1 = (ψ1 | φ1)[p1] and c2 = (ψ2 | φ2)[p2] be normal. If c1 |(cid:8)pr c2 then c2 ≡e c1.|At| − 1 (note that F (At) can be embedded in a space of dimension 2Proof. Observe that the set of models Mod({c}) of a probabilistic conditional c = (ψ | φ)[p] can be described via Mod({c}) ={P ∈ F (At) | P (φ) = x, P (ψφ) = px, x ∈ [0, 1]}. That is, Mod({c}) is the intersection of F (At) with a hyperplane, i.e. a linearspace of dimension 2, one dimension for eachprobability of an interpretation), see also [23]. In general, there are three different possible relationships between any two|At| − 2, orhyperplanes in a space of dimension 2coincide. For example, two planes in a 3-dimensional space are either parallel, intersect in a line, or are the same. Thenc1 |(cid:8)pr c2 implies that the hyperplanes corresponding to c1 and c2 coincide, otherwise there would be a model of c1 that isnot a model of c2. It follows Mod({c1}) = Mod({c2}) and the claim. (cid:2): they may either be parallel, intersect in a linear space of dimension 2|At||At|Proposition 6. The function I0 satisfies consistency, irrelevance of syntax, monotonicity, weak independence, independence, andnormalization.Proof. We only show that I0 satisfies consistency, irrelevance of syntax, monotonicity, independence, and normalization as weakindependence follows from independence due to Proposition 4.Consistency A knowledge base K is consistent if and only if I0(K) = 0 by definition.Irrelevance of syntax From K1 ≡s K2 follows K1 ≡e K2 by Proposition 2. Therefore, K1 is inconsistent if and only if K2 isinconsistent. It follows I0(K1) = I0(K2).I0(K ∪ {c}) (cid:2) I0(K) = 0 by definition.Monotonicity If K is inconsistent so is any superset of K. It follows I0(K) = 1 = I0(K ∪ {c}). If K is consistent thenIndependence Assume that K is consistent and c is free in K ∪ {c}. If K ∪ {c} would be inconsistent then for everyminimal inconsistent subset M of K ∪ {c} it holds that c /∈ M. Hence, M is also a minimal inconsistent subsetof K rendering K inconsistent. As K is consistent it follows that K ∪ {c} is consistent and therefore I0(K ∪ {c}) =0 = I0(K). If K is inconsistent so is any superset of K and hence I0(K ∪ {c}) = 1 = I0(K).M. Thimm / Artificial Intelligence 197 (2013) 1–2419Normalization For every K it holds that either I0(K) = 0 or I0(K) = 1 and therefore I0(K) ∈ [0, 1]. (cid:2)Proposition 7. The function IMI satisfies consistency, monotonicity, super-additivity, weak independence, independence, irrele-vance of syntax, MI-separability, and penalty.Proof. We only show that IMI satisfies consistency, super-additivity, irrelevance of syntax, MI-separability, and penalty, asmonotonicity follows from super-additivity, weak independence follows from independence, and independence follows fromMI-separability, cf. Proposition 4.Consistency If K is consistent it follows that MI(K) = ∅ and therefore IMI(K) = 0. If K is inconsistent then MI(K) (cid:14)= ∅ andIMI(K) > 0.Super-additivity Let K ∩ K(cid:2) = ∅. Due to Proposition 4 it holds that MI(K) ⊆ MI(K ∪ K(cid:2)) and MI(K(cid:2)) ⊆ MI(K ∪ K(cid:2)). Dueto K ∩ K(cid:2) = ∅ it follows that MI(K) ∩ MI(K(cid:2)) = ∅ and therefore IMI(K ∪ K(cid:2)) = |MI(K ∪ K(cid:2))| (cid:2) |MI(K) ∪ MI(K(cid:2))| =|MI(K)| + |MI(K(cid:2))| = IMI(K(cid:2)) + IMI(K(cid:2)).Irrelevance of syntax Let K1 and K2 be knowledge bases with K1 ≡s K2 and let ρK1,K2: K1 → K2 be a bijection withc ≡e ρK1,K2 (c) for all c ∈ K1. Let C ⊆ K1 and letρK1,K2 (C) =(cid:5)ρK1,K2 (c)(cid:6)(cid:10)(cid:10) c ∈ C.(A.1)As Mod(c) = Mod(ρK1,K2 (c)) for every c ∈ K1 and due to the fact that ρK1,K2 is a bijection it follows that M isa minimal inconsistent subset of K1 if and only if ρK1,K2 (M) is a minimal inconsistent subset of K2. Hence, itfollows IMI(K1) = IMI(K2).MI-separability Let K1, K2 be knowledge bases with MI(K1 ∪ K2) = MI(K1) ∪ MI(K2) and MI(K1) ∩ MI(K2) = ∅. It followsdirectly that IMI(K1 ∪ K2) = |MI(K1 ∪ K2)| = |MI(K1)| + |MI(K2)| = IMI(K1) + IMI(K2).Penalty Let c /∈ K be a conditional that is not free in K ∪ {c}. By the facts that MI(K) ⊆ MI(K ∪ {c}) and that there is anM ∈ MI(K ∪ {c}) with c ∈ M it follows that |MI(K)| < |MI(K ∪ {c})| and therefore IMI(K) < IMI(K ∪ {c}). (cid:2)Proposition 8. The function I Cvance of syntax, MI-separability, and penalty.MI satisfies consistency, monotonicity, super-additivity, weak independence, independence, irrele-Proof. We only show that I CMI satisfies consistency, super-additivity, irrelevance of syntax, MI-separability, and penalty asmonotonicity follows from super-additivity, weak independence follows from independence, and independence follows fromMI-separability, cf. Proposition 4.Consistency If K is consistent it follows that MI(K) = ∅ and therefore I Cthen MI(K) (cid:14)= ∅ with M ∈ MI(K) and |M| > 0. It follows that I CMI(K) = 0 (the empty sum). If K is inconsistentMI(K) > 0.Super-additivity Let K ∩ K(cid:2) = ∅. Due to Proposition 4 it holds that MI(K) ⊆ MI(K ∪ K(cid:2)) and MI(K(cid:2)) ⊆ MI(K ∪ K(cid:2)). Due toK ∩ K(cid:2) = ∅ it follows that MI(K) ∩ MI(K(cid:2)) = ∅ and therefore(cid:7)I CMIK ∪ K(cid:2)(cid:8)=(cid:2)M∈MI(K∪K(cid:2))1|M|(cid:2)(cid:2)M∈MI(K)1|M|+(cid:2)M∈MI(K(cid:2))1|M|= I CMI(K) + I CMI(cid:7)(cid:8).K(cid:2)Irrelevance of syntax Let K1 and K2 be knowledge bases with K1 ≡s K2 and let ρK1,K2: K1 → K2 be a bijection withc ≡e ρK1,K2 (c) for all c ∈ K1. In the proof of Proposition 7 it has already been shown that M is a minimalinconsistent subset of K1 if and only if ρK1,K2 (M) is a minimal inconsistent subset of K2, cf. the definition ofρK1,K2 (M) in Eq. (A.1). As ρK1,K2 is a bijection it also follows that |M| = |ρK1,K2 (M)| and henceMI(K2) =I C(cid:2)M∈MI(K2)1|M|=(cid:2)M∈MI(K1)1|ρK1,K2 (M)|=(cid:2)M∈MI(K1)1|M|= I CMI(K1).MI-separability Let K1, K2 be knowledge bases with MI(K1 ∪ K2) = MI(K1) ∪ MI(K2) and MI(K1) ∩ MI(K2) = ∅. It followsdirectly thatMI(K1 ∪ K2) =I C(cid:2)M∈MI(K1∪K2)1|M|=(cid:2)M∈MI(K1)1|M|+(cid:2)M∈MI(K2)1|M|= I CMI(K1) + I CMI(K2).Penalty Let c /∈ K be a conditional that is not free in K ∪ {c}. By the facts that MI(K) ⊆ MI(K ∪ {c}) and that there is anM ∈ MI(K ∪ {c}) with c ∈ M it follows that |MI(K)| < |MI(K ∪ {c})| and therefore I CMI(K) < I CMI(K ∪ {c}). (cid:2)Proposition 9. If MI(K) = {K} then Iη(K) = 1/|K|.20M. Thimm / Artificial Intelligence 197 (2013) 1–24Proof. Let K = (cid:10)c1, . . . , cn(cid:11) and let K1, . . . , Kn be defined via Ki = K \ {ci} for i = 1, . . . , n. Each Ki for i = 1, . . . , n isconsistent as K is minimally inconsistent. Therefore, let P 1, . . . , Pn be probability functions with P i |(cid:8)pr Ki for i = 1, . . . , n.Define ˆP through ˆP (P i) = 1/n and ˆP (P ) = 0 for all P ∈ F (At) with P /∈ {P 1, . . . , Pn}. Note that every ci is contained in everyK j with j (cid:14)= i. Therefore, all probability functions P j with j (cid:14)= i satisfy ci and it followsˆP (ci) = ˆP (P 1) + · · · + ˆP (P i−1) + ˆP (P i+1) + · · · + ˆP (P n) = n − 1n= 1 − 1n.It follows that ˆP (ci) = 1 − 1/n for every i = 1, . . . , n and, hence, Iη(K) (cid:2) (1 − 1/n). It is also easy to see that there can beno ˆP(cid:2)(ci) > 1 − 1/n for all i = 1, . . . , n, see [18] for details. It follows Iη(K) = (1 − 1/n). (cid:2)with ˆP(cid:2)Proposition 10. The function Iη satisfies consistency, monotonicity, weak independence, independence, irrelevance of syntaxand normalization.Proof. We only show that Iη satisfies consistency, monotonicity, independence, irrelevance of syntax and normalization as weakindependence follows from independence, cf. Proposition 4.Consistency Let K be consistent and P be a probability function with P |(cid:8)pr K. Define ˆP P via ˆP P (P ) = 1 and ˆP P (P(cid:2)) = 0(cid:2) (cid:14)= P . It follows that ˆP P (c) = 1 for every c ∈ K and due to normalization it followsfor all PIη(K) = 1 − 1 = 0. If K is inconsistent there can be no ˆP with ˆP (c) = 1 for every c ∈ K because otherwise everyP with ˆP (P ) > 0 would obey P |(cid:8)pr K. Therefore max{η | ∃ ˆP : ∀c ∈ K: ˆP (c) (cid:2) η} < 1 and Iη(K) > 0.(cid:2) ∈ F (At) with PMonotonicity Let K be a knowledge base, c a conditional and K(cid:2) = K ∪ {c}. Let ˆP ∈ F 2(At) be a probability function andη(cid:2) ∈ [0, 1] be such that(cid:8)(cid:7)K(cid:2)Iη= 1 − η(cid:2)In particular, it holds that ˆP (c) (cid:2) η(cid:2)and η(cid:2) = max(cid:5)η(cid:10)(cid:10) ∀c ∈ K(cid:2): ˆP (c) (cid:2) η(cid:6).Iη(K) = 1 − max(cid:5)η(cid:10)(cid:10) ∀c ∈ K: ˆP (c) (cid:2) ηfor all c ∈ K and therefore(cid:7)(cid:6)(cid:3) 1 − η(cid:2) = Iη(cid:8).K(cid:2)Independence Let K be a knowledge base and let c ∈ K be free in K. Due to monotonicity it follows Iη(K) (cid:2) Iη(K \ {c}).The proof of Iη(K) (cid:3) Iη(K \ {c}) is analogous to the proof of Corollary 2.20 in [18].Irrelevance of syntax Let K1 and K2 be knowledge bases with K1 ≡s K2 and let ρK1,K2 : K1 → K2 be a bijection withc ≡e ρK1,K2 (c) for all c ∈ K1. As Mod(c) = Mod(ρK1,K2 (c)) for all c ∈ K1 it follows that ˆP (c) = ˆP (ρK1,K2 (c)) forevery ˆP ∈ F 2(At) and therefore Iη(K1) = Iη(K2).Normalization For every ˆP : F (At) → [0, 1] and probabilistic conditional c it holds that ˆP (c) ∈ [0, 1] as ˆP is a probabilityfunction. It follows that max{η | ∃ ˆP : ∀c ∈ K: ˆP (c) (cid:2) η} ∈ [0, 1] and therefore Iη(K) ∈ [0, 1]. (cid:2)Theorem 1. If D is continuously generating the function ID is well-defined.Proof. Let K = (cid:10)c1, . . . , cn(cid:11) be a knowledge base with ci = (ψi | φi)[pi] for i = 1, . . . , n and d = D(n). Consider the setPK ⊆ F (At) × [0, 1]n defined via(cid:5)(cid:3)PK =P , (cid:10)x1, . . . , xn(cid:11)(cid:4)∈ F(At) × [0, 1]n(cid:10)(cid:6)(cid:10) P |(cid:8)pr ΛK(x1, . . . , xn).We show now that PK is a closed set. Let (cid:10)P i, (cid:10)xidefine1, . . . , xin(cid:11)(cid:11) ∈ PK for i ∈ N be such that limi→∞(P i, (xi1, . . . , xin)) exists and(cid:3)Q , (cid:10) y1, . . . , yn(cid:11)(cid:4)(cid:3)= limi→∞(cid:3)P i,1, . . . , xixin(cid:4)(cid:4).In particular, it holds that limi→∞ P i = Q with Q ∈ F (At).2 For j = 1, . . . , n, if Q (φ j) > 0 then there is some k ∈ N suchthat for all i > k it holds that P i(φ j) > 0 as well. Therefore, for i > k it holds that P i(ψ j | φ j) = xij andQ (ψ j | φ j) = Q (ψ jφ j)Q (φ j)= limi→∞ P i(ψ jφ j)limi→∞ P i(φ j)= limi→∞P i(ψ jφ j)P i(φ j)= limi→∞P i(ψ j | φ j) = limi→∞xij= y jwhich implies Q |(cid:8)pr (ψ j | φ j)[ y j]. Furthermore, for j = 1, . . . , n, if Q (φ j) = 0 then trivially Q |(cid:8)pr (ψ j | φ j)[ y j] due toour definition of probabilistic satisfaction. It follows that Q |(cid:8)pr ΛK( y1, . . . , yn) and therefore Q ∈ PK, i.e., PK is closed.Consider now the projection ρ : PK → [0, 1]n defined via ρ((cid:10)P , (cid:10)x1, . . . , xn(cid:11)(cid:11)) = (cid:10)x1, . . . , xn(cid:11) for (cid:10)P , (cid:10)x1, . . . , xn(cid:11)(cid:11) ∈ PK. As2 Note that the set F(At) is a closed set, see e.g. [36].M. Thimm / Artificial Intelligence 197 (2013) 1–2421F (At) is compact—see e.g. [36]—it follows that ρ is a closed map, cf. the Tube Lemma3 [22]. Therefore, ρ maps closed setsto closed sets and it follows that(cid:5)(cid:6)(cid:5)(cid:6)(cid:3)(cid:4)ρ(PK) =(cid:10)x1, . . . , xn(cid:11) ∈ [0, 1]nP , (cid:10)x1, . . . , xn(cid:11)∈ PK=(cid:19)x ∈ [0, 1]n(cid:10)(cid:10) K[(cid:19)x] is consistent(cid:10)(cid:10) ∃P :(cid:5)ID (K) = mindis a closed set. Observe that we can write ID (K) as(cid:8) (cid:10)(cid:6)(cid:10) (cid:10)x1, . . . , xn(cid:11) ∈ ρ(PK)As ρ(PK) is a closed set—and also compact as it is bounded due to ρ(PK) ⊆ [0, 1]n—and the mapping (cid:10)x1, . . . , xn(cid:11) (cid:27)→d((cid:10)p1, . . . , pn(cid:11), (cid:10)x1, . . . , xn(cid:11)) is continuous—as d is a continuous function and d1, . . . , dn are constants—the set(cid:10)p1, . . . , pn(cid:11), (cid:10)x1, . . . , xn(cid:11)(cid:7).(cid:7)(cid:5)dNdK =(cid:10)p1, . . . , pn(cid:11), (cid:10)x1, . . . , xn(cid:11)(cid:8) (cid:10)(cid:6)(cid:10) (cid:10)x1, . . . , xn(cid:11) ∈ ρ(PK)K are non-empty as for every K there is always an (cid:19)x such that K[(cid:19)x]is closed as well. Note that ρ(PK) and therefore Ndis consistent (take an arbitrary positive probability function P and define xi = P (ψi | φi), see also [34]). It follows thatID (K) = min NdK is well-defined. (cid:2)Proposition 11. The function ID0 is well-defined and it holds that ID0 = I0.Proof. Let K = K[(cid:19)x] for some (cid:19)x ∈ [0, 1]n be a consistent knowledge base and let d0 = D0(n). Then clearly ID0 (K) = 0 =I0(K) as d0((cid:19)x, (cid:19)x) = 0 is minimal and K[(cid:19)x] is consistent. Let K = K[(cid:19)x] for some (cid:19)x ∈ [0, 1]n be an inconsistent knowledgebase. As noted in the proof of Theorem 1 there is a (cid:19)y ∈ [0, 1]n such that K[(cid:19)y] is consistent. It follows that ID0 (K) (cid:3)d0((cid:19)x, (cid:19)y) = 1 and ID0 (K) = 1 = I0(K) andtherefore ID0 (K) > d0((cid:19)x, (cid:19)x) = 0 as K[(cid:19)x] is inconsistent. Due to Im d0 = {0, 1} it follows ID0 = I0. As I0 is well-defined so is ID0 . (cid:2)Theorem 2. Let D be a distance generator such that ID is well-defined.1. The function ID satisfies consistency.2. If D is monotonously generating then ID satisfies monotonicity.3. If D is super-additively generating then ID satisfies super-additivity.4. If D is symmetric generating then ID satisfies irrelevance of syntax.5. If D is continuously generating then ID satisfies continuity.Proof. Let K = (cid:10)c1, . . . , cn(cid:11) ∈ K be a knowledge base with ci = (ψi | φi)[pi] for i = 1, . . . , n, d = D(n), and define ΘK ={(cid:19)y | K[(cid:19)y] is consistent}.1. If K = K[(cid:19)x] is consistent then due to d((cid:19)x, (cid:19)x) = 0 and d((cid:19)x, (cid:19)y) (cid:2) 0 for all (cid:19)y ∈ [0, 1]|K|2. Without loss of generality we only show that ID (K) (cid:2) ID (K \ {cn}). First, note that if K(cid:2) = (cid:10)c1, . . . , cn−1(cid:11)[(cid:10) y1, . . . , yn−1(cid:11)]is consistent there is a yn ∈ [0, 1] such that (cid:10)c1, . . . , cn(cid:11)[(cid:10) y1, . . . , yn(cid:11)] is consistent (by taking some model P of K(cid:2)anddefining yn = P (ψn | φn); the latter is defined as cn is normal). Furthermore, if (cid:10)c1, . . . , cn(cid:11)[(cid:10) y1, . . . , yn(cid:11)] is consistentso is (cid:10)c1, . . . , cn−1(cid:11)[(cid:10) y1, . . . , yn−1(cid:11)]. It follows that (cid:10) y1, . . . , yn−1(cid:11) ∈Θ K(cid:2)if and only if there is a yn ∈ [0, 1] such that(cid:10) y1, . . . , yn(cid:11) ∈Θ K. Let now K = K[(cid:19)x] for some (cid:19)x = (cid:10)x1, . . . , xn(cid:11) ∈ [0, 1] and (cid:10) y1, . . . , yn−1(cid:11) ∈ ΘK(cid:2) . Then for every yn ∈[0, 1] such that (cid:10) y1, . . . , yn(cid:11) ∈ ΘK it holds thatit follows ID (K) = 0.(cid:7)(cid:10)x1, . . . , xn−1(cid:11), (cid:10) y1, . . . , yn−1(cid:11)(cid:8)d(cid:7)(cid:3) d(cid:10)x1, . . . , xn(cid:11), (cid:10) y1, . . . , yn(cid:11)(cid:8)as D is monotonously generating. It follows that every element of M1 = {d((cid:19)x, (cid:19)y) | (cid:19)y ∈ ΘK} is greater or equal to an ele-ment in M2 = {d((cid:10)x1, . . . , xn−1(cid:11), (cid:19)y) | (cid:19)y ∈ ΘK(cid:2) }. Consequently, ID (K(cid:2)) = min M2 (cid:3) min M1 = ID (K) proving monotonicity.3. This proof is analogous to the proof of (2).4. Let K1 = K1[ (cid:19)x1] and K2 = K2[ (cid:19)x2] be knowledge bases with K1 ≡s K2 and (cid:19)x1 = (cid:10)x11= K1[ (cid:19)y1] be consistent such that ID (K1) = d((cid:19)x1, (cid:19)y1) for some (cid:19)y1 = (cid:10) y1|K1| = n = |K2|). Let K(cid:2)tion 5 it has been shown that for normal c = (ψ | φ)[p] and c(a) φ ≡ φ(cid:2)(b) φ ≡ φ(cid:2)Define now (cid:19)y2 = (cid:10) y11 (for i = 1, . . . , n). As D is1 and via yisymmetric generating, by iteratively applying (5) it follows that d( (cid:19)x2, (cid:19)y2) = d( (cid:19)x1, (cid:19)y1). Note also that, by construction,and ψ ∧ φ ≡ ψ (cid:2) ∧ φ(cid:2)and ψ ∧ φ ≡ ψ (cid:2) ∧ φ(cid:2)and p = porand p = 1 − p(cid:2) = (ψ (cid:2) | φ(cid:2))[p(cid:2)] with c ≡e c(cid:2).1 if xiit holds that either(cid:11) via yi22, . . . , yn= 1 − yi= 1 − xi1 if xi= yi= xi22221(cid:2)(cid:2)1, . . . , xn1(cid:11) and (cid:19)x2 = (cid:10)x11, . . . , yn2, . . . , xn(cid:11) (with2(cid:11). In Proposi-3 An equivalent formalization of the Tube Lemma is “If X is Hausdorff and Y is Hausdorff and compact then p : X × Y → X with p(x, y) = x is a closedmap”. Note, that all spaces above are Hausdorff as they are subsets of Euclidean spaces (or can be characterized as such).22M. Thimm / Artificial Intelligence 197 (2013) 1–245. Let (cid:19)x ∈ [0, 1]|K|K2[(cid:19)y2] is consistent as K2[(cid:19)y2] ≡s K1[(cid:19)y1]. It follows that ID (K2) (cid:3) d( (cid:19)x2, (cid:19)y2) = d( (cid:19)x1, (cid:19)y1) = ID (K1). Similarly we obtainID (K1) (cid:3) ID (K2) and therefore the claim.. For every (cid:19)y ∈ ΘK the mapping (cid:19)x (cid:27)→ d((cid:19)x, (cid:19)y) is continuous as D is continuously generating. As theminimum of a set of continuous functions is continuous it follows that the mapping (cid:19)x (cid:27)→ Id(K[(cid:19)x]) is continuous aswell. (cid:2)Theorem 3. Let p ∈ N+.1. The function Ip satisfies consistency, monotonicity, weak independence, independence, irrelevance of syntax, and continu-ity.2. If p = 1 then Ip satisfies super-additivity.Proof.1. It has already been noted that D p is continuously generating and therefore Ip is well-defined. By Theorem 2 it alsofollows that Ip satisfies consistency and continuity. We continue with showing that D p is also monotonously and sym-metric generating. Let x1, . . . , xn+1, y1, . . . , yn+1 ∈ R+for some n ∈ N+.(cid:7)D p(n)(cid:10)x1, . . . , xn(cid:11), (cid:10) y1, . . . , yn(cid:11)(cid:8)(cid:12)= p(cid:12)(cid:3) p|x1 − y1|p + · · · + |xn − yn|p|x1 − y1|p + · · · + |xn − yn|p + |xn+1 − yn+1|p(cid:10)x1, . . . , xn+1(cid:11), (cid:10) y1, . . . , yn+1(cid:11)(cid:7)= D p(n + 1)(cid:8)as |xn+1 − yn+1|p (cid:2) 0 and the root function is monotonous. Let i ∈ {1, . . . , n}.(cid:8)(cid:7)D p(n)(cid:10)x1, . . . , xn(cid:11), (cid:10) y1, . . . , yn(cid:11)(cid:12)= p(cid:12)= p|x1 − y1|p + · · · + |xi − yi|p + · · · + |xn − yn|p|x1 − y1|p + · · · + |1 − xi − (1 − yi)|p + · · · + |xn − yn|p(cid:10)x1, . . . , 1 − xi, . . . , xn(cid:11), (cid:10) y1, . . . , 1 − yi, . . . , yn(cid:11)(cid:8).(cid:7)= D p(n)By Theorem 2 it follows that Ip satisfies monotonicity and irrelevance of syntax. It remains to show that Ip satisfiesweak independence and independence. Before proving independence we first show that from both K ∪ {(ψ | φ)[p1]} andK ∪ {(ψ | φ)[p2]} being consistent for some knowledge base K and p1 (cid:3) p2 it follows that K ∪ {(ψ | φ)[ y]} is consistentfor every y ∈ [0, 1] that satisfies p1 (cid:3) y (cid:3) p2. Let P 1 |(cid:8)pr K ∪ {(ψ | φ)[p1]} and let P 2 |(cid:8)pr K ∪ {(ψ | φ)[p2]}. If P 1(φ) = 0then clearly P 1 |(cid:8)pr K ∪ {(ψ | φ)[ y]} for every y ∈ [0, 1] due to our definition of probabilistic satisfaction. If P 2(φ) = 0then P 2 |(cid:8)pr K ∪ {(ψ | φ)[ y]} for every y ∈ [0, 1] accordingly. So assume P 1(φ) > 0 and P 2(φ) > 0. Let δ ∈ [0, 1] andconsider the probability function P δ defined via P δ(ω) = δ P 1(ω) + (1 − δ)P 2(ω) for all ω ∈ Ω(At). Then P δ |(cid:8)pr K forall δ ∈ [0, 1] as the set of models of a knowledge base is a convex set, cf. [23]. Furthermore, note that P δ(φ) > 0 forevery δ ∈ [0, 1] as both P 1(φ) > 0 and P 2(φ) > 0. Then P δ(ψ | φ) is continuous in δ and for every y ∈ [p1, p2] there|(cid:8)pr K ∪ {(ψ | φ)[ y]} for every y ∈ [p1, p2] and thereforeis a δ y ∈ [0, 1] such that P δ y (ψ | φ) = y. It follows that P δ yK ∪ {(ψ | φ)[ y]} is consistent for every y ∈ [p1, p2].Let now K = (cid:10)c1, . . . , cn(cid:11) and ci = (ψi | φi)[pi] for i = 1, . . . , n be a knowledge base and let c = (ψ | φ)[p] be free inK ∪ {c}. Assume that K is also a minimal inconsistent set, i.e. MI(K) = {K}. Let Ip(K) = x and let (cid:10)x1, . . . , xn(cid:11) ∈ [0, 1]nbe such that ΛK(x1, . . . , xn) is consistent and |p1 − x1| + · · · + |pn − xn| = x. Consider now K(cid:2) = (cid:10)(ψ1 | φ1)[p1], . . . ,(ψn | φn)[pn], (ψ | φ)[p](cid:11). It suffices to show that ΛK(cid:2) (x1, . . . , xn, p) is consistent. Define C j = K \ {(ψ j | φ j)[p j]} forevery j = 1, . . . , n. Then both C j and C j ∪ {c} are consistent. Let p j be such that there is a P with P |(cid:8)pr C j ∪ {c},(cid:2)(cid:2)| (cid:2) x (otherwise this would contradict Ip(K) = x).] and |p j − pP |(cid:8)pr (ψ j | φ j)[pjj(cid:2)j > p j . As {c, c j} is consistent as well (as c is free) it follows that {c, (ψ j | φ j)[ y]} is consistent forAssume w.l.o.g. p(cid:2)(cid:2)every y ∈ [p j, pj, p j] if] due to our elaboration above. As |p j − x j| (cid:3) x it follows x j ∈ [p j, pj(cid:2)j ). Hence, {c, (ψ j | φ j)[x j]} is consistent for every j = 1, . . . , n. As ΛK(x1, . . . , xn) is consistent and c is consistentp j > pwith every combination of conditionals in ΛK(x1, . . . , xn) it follows that ΛK(cid:2) (x1, . . . , xn, p) is consistent. The above canbe generalized if K contains multiple minimal inconsistent subsets by iteratively considering each minimal inconsistentsubset of K. By Proposition 4 it also follows that Ip satisfies weak independence.| is minimal. It follows that |p j − p] as well (or x j ∈ [p(cid:2)j(cid:2)j2. Due to Theorem 2 it suffices to show that D1 is super-additively generating. Let n, m ∈ N+and x1, . . . , xn+m, y1, . . . ,yn+m ∈ R.(cid:8)(cid:7)D1(n)(cid:10)x1, . . . , xn(cid:11), (cid:10) y1, . . . , yn(cid:11)(cid:7)+ D1(m)= |x1 − y1| + · · · + |xn − yn| + |xn+1 − yn+1| + · · · + |xn+m − yn+m|(cid:7)= D1(n + m)(cid:10)x1, . . . , xn+m(cid:11), (cid:10) y1, . . . , yn+m(cid:11)(cid:10)xn+1, . . . , xn+m(cid:11), (cid:10) yn+1, . . . , yn+m(cid:11)(cid:2)(cid:8).(cid:8)Theorem 4. Let I be an inconsistency measure.M. Thimm / Artificial Intelligence 197 (2013) 1–2423Σ satisfies monotonicity, super-additivity, weak independence, independence, and MI-separability.1. II2. If I satisfies consistency then II3. If I satisfies irrelevance of syntax then II4. If I satisfies continuity then IIΣ satisfies consistency and penalty.Σ satisfies irrelevance of syntax.Σ satisfies continuity.Proof.1. We first show that IIΣ satisfies super-additivity. If K ∩ K(cid:2) = ∅ then it holds that MI(K) ∩ MI(K(cid:2)) = ∅ as well. Due toProposition 4 it follows that MI(K) ∪ MI(K(cid:2)) ⊆ MI(K ∪ K(cid:2)). It follows(cid:2)(cid:2)(cid:2)I(M) (cid:2)I(M) +(cid:7)IIΣK ∪ K(cid:2)=I(M) = IIΣ (K) + IIΣ(cid:7)(cid:8).K(cid:2)M∈MI(K∪K(cid:2))M∈MI(K(cid:2))M∈MI(K)Due Proposition 4 it also follows that IIΣ satisfies monotonicity. We now show that IIMI(K ∪ K(cid:2)) = MI(K) ∪ MI(K(cid:2)) and MI(K) ∩ MI(K(cid:2)) = ∅. Then clearly(cid:2)(cid:2)(cid:2)I(M) =I(M) +I(M) = IIΣ (K) + IIΣ(cid:7)(cid:8).K(cid:2)(cid:7)IIΣK ∪ K(cid:2)=Σ satisfies MI-separability. Let(cid:8)(cid:8)M∈MI(K∪K(cid:2))M∈MI(K)M∈MI(K(cid:2))Due to Proposition 4 it also follows that II2. We first show that IIΣ satisfies independence and weak independence.Σ satisfies consistency. If K is consistent then MI(K) = ∅ and IIΣ (K) = 0. If K is inconsistentthen there is an M ∈ MI(K) and as I satisfies consistency it follows that I(M) > 0. Hence, IIΣ (K) > 0 as well. Wenow show that IIΣ satisfies penalty. Let c ∈ K be a probabilistic conditional that is not free in K. Due to Proposition 4it follows that MI(K \ {c}) ⊆ MI(K). As c /∈ K \ {c} and there is at least one M ∈ MI(K) with c ∈ M it follows thatMI(K \ {c}) (cid:2) MI(K). As I satisfies consistency it follows that I(M) > 0 and therefore IIΣ (K).3. Let it hold that K1 ≡s K2. It follows that for every M ∈ MI(K1) there is M(cid:2) ∈ MI(K2) with M ≡s M(cid:2), and viceversa. As I satisfies irrelevance of syntax it follows that I(M) = I(M(cid:2)) for every M ∈ MI(K1). Hence, it holds thatIIΣ (K2).Σ (K1) =Σ (K \ {c}) < III(M(cid:2)) = II(cid:20)M∈MI(K1)(cid:20)(cid:20)4. It is easy to see that θIIIt follows directly, that θIIM∈MI(K) θI,M (given an adequate ordering of the conditionals in K).Σ ,K is continuous if θI,M is continuous for every M ∈ MI(K), i.e., if I satisfies continuity. (cid:2)Σ ,K =I(M(cid:2)) =M(cid:2)∈MI(K2)Σ ,K is given via θIITheorem 5. Ihμ satisfies irrelevance of syntax and weak independence.Proof.Irrelevance of syntax Let K1 and K2 be knowledge bases with K1 ≡s K2 and let ρK1,K2 : K1 → K2 be a bijectionwith c ≡e ρK1,K2 (c) for all c ∈ K1. Due to Mod({c}) = Mod({ρK1,K2 (c)}) it follows that dE (P , Mod({c})) =μ(K1) =dE (P , Mod({ρK1,K2 (c)})) and therefore ChK1μ(K2).Ih(P ) for every c ∈ K1 and P ∈ F (At). It follows IhWeak independence Let K = (cid:10)c1, . . . , cn(cid:11) be a knowledge base with ci = (ψi | φi)[pi] for i = 1, . . . , n and assume w.l.o.g.(P ) = ChK2(cid:5)ˆΩ B(cid:10)(cid:10) IhP ∈ F(B)h (K) =(cid:6)K(P )that cn is safe in K, i.e. At(cn) ∩ At(K \ {cn}) = ∅. For B with At(K) ⊆ B letμ(K) = 1 − Ch∗ ∈ ˆΩ At(K)(K \ {cn}) with P |(cid:8)pr cn as this implies Chμ(K \ {cn}). Together with monotonicity it follows IhIt suffices to show that there is a Pto h(0) = 1) and therefore IhLet ω ∈ Ω(At) and define ωA ∈ Ω(A) with A ⊆ At to be the projection of ω onto A, e.g. for At = {a, b, c} andω = a ∧ ¬b ∧ c it is ω{a,b} = a ∧ ¬b. Furthermore, if P ∈ F (At) let P |A ∈ F (At) denote the projection of P ontoA ⊆ At, that is(cid:8)(cid:7)ω(cid:2)∗) = ChK(Pμ(K) = IhK\{cn}(Pμ(K \ {cn}).μ(K) (cid:3) Ih∗) (dueP (ω)P |A(cid:2)=.hω∈Ω(At), ω|(cid:8)ω(cid:2)for all ω(cid:2) ∈ Ω(A). In [6] it has been shown that ChF (At \ At(cn)) and every Pas no atom of At(cn) is mentioned in K \ {cn} it holds that Chprojection of P(cid:7)ˆΩ At(K)(cid:10)(cid:10) P |At(K)\At(cn) ∈ ˆΩ At(K)\At(cn)onto At \ At(cn). In particular, it follows(cid:2) ∈ F (At) such that P = PK \ {cn}=(cid:5)P(cid:7)(cid:8)(cid:2)hhK \ {cn}(cid:8)(cid:6).(cid:2)|At\At(cn) it holds that ChK is language invariant, that is in particular, for every P ∈(cid:2)). In other words,K\{cn}(P ) = Ch(cid:2)) is the same as ChK\{cn}(P ) if P is theK\{cn}(PK\{cn}(P24M. Thimm / Artificial Intelligence 197 (2013) 1–24Let now P(cid:2)(cid:2) ∈ F (At(cn)) with P(cid:2)(cid:2) |(cid:8)pr cn and P(cid:2) ∈ ˆΩ At(K)\At(cn)h(K \ {cn}). Define P(cid:2)(cid:2)(cid:2) ∈ F (At(K)) via(cid:2)(cid:2)(cid:2)P(ω) = P(cid:2)(cid:2)(ωAt(cn))P(cid:2)By construction it holds that P(cid:2)(cid:2)(cid:2) |(cid:8)pr cn the claim follows. (cid:2)P(ωAt(K)\At(cn)).(cid:2)(cid:2)(cid:2)|At(K)\At(cn) = P(cid:2) ∈ ˆΩ At(K)\At(cn)h(K \ {cn}) and therefore P(cid:2)(cid:2)(cid:2) ∈ ˆΩ At(K)h(K \ {cn}). AsReferences[1] A. Ahuja, W. Rödder, Project risk management by a probabilistic expert system, in: Proceedings of the International Conference on Operations Research,2002, pp. 329–334.[2] T.J.M. Bench-Capon, P.E. Dunne, Argumentation in artificial intelligence, Artificial Intelligence 171 (10–15) (2007) 619–641.[3] S.P. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.[4] R.J. Brachman, H.J. Levesque, Knowledge Representation and Reasoning, The Morgan Kaufmann Series in Artificial Intelligence, Morgan KaufmannPublishers, 2004.[5] T.M. Cover, Elements of Information Theory, 2nd edition, Wiley–Interscience, New York, 2001.[6] L. Daniel, Paraconsistent probabilistic reasoning, Ph.D. thesis, L’École Nationale Supérieure des Mines de Paris, 2009.[7] M. Finger, G.D. Bona, Probabilistic satisfiability: Logic-based algorithms and phase transition, in: Proceedings of the 22nd International Joint Conferenceon Artificial Intelligence (IJCAI’11), 2011, pp. 528–533.[8] M. Finthammer, G. Kern-Isberner, M. Ritterskamp, Resolving inconsistencies in probabilistic knowledge bases, in: KI 2007: Advances in Artificial Intelli-gence – 30th Annual German Conference on Artificial Intelligence, in: Lecture Notes in Computer Science, vol. 4667, Springer-Verlag, 2007, pp. 114–128.[9] A.M. Frisch, P. Haddawy, Anytime deduction for probabilistic logic, Artificial Intelligence 69 (1–2) (1994) 93–122.[10] J. Grant, A. Hunter, Analysing inconsistent first-order knowledge bases, Artificial Intelligence 172 (8–9) (2008) 1064–1093.[11] J. Grant, A. Hunter, Measuring the good and bad in inconsistent information, in: Proceedings of the 22nd International Joint Conference on ArtificialIntelligence (IJCAI’11), 2011, pp. 2632–2637.[12] A. Hunter, S. Konieczny, Measuring inconsistency through minimal inconsistent sets, in: Proceedings of the 11th International Conference on Principlesof Knowledge Representation and Reasoning, AAAI Press, 2008, pp. 358–366.[13] A. Hunter, S. Konieczny, On the measure of conflicts: Shapley inconsistency values, Artificial Intelligence 174 (14) (2010) 1007–1026.[14] G. Kern-Isberner, Conditionals in Nonmonotonic Reasoning and Belief Revision, Lecture Notes in Computer Science, vol. 2087, Springer-Verlag, 2001.[15] G. Kern-Isberner, T. Lukasiewicz, Combining probabilistic logic programming with the power of maximum entropy, Artificial Intelligence 157 (1–2)(2004) 139–202.[16] G. Kern-Isberner, M. Thimm, Novel semantical approaches to relational probabilistic conditionals, in: F. Lin, U. Sattler, M. Truszczy ´nski (Eds.), Proceed-ings of the Twelfth International Conference on the Principles of Knowledge Representation and Reasoning (KR’10), AAAI Press, 2010, pp. 382–392.[17] K.M. Knight, Measuring inconsistency, Journal of Philosophical Logic 31 (2001) 77–98.[18] K.M. Knight, A theory of inconsistency, Ph.D. thesis, University of Manchester, 2002.[19] H.E. Kyburg, R.P. Loui, G.N. Carlson (Eds.), Knowledge Representation and Defeasible Reasoning, Studies in Cognitive Systems, Springer-Verlag, 1990.[20] T. Lukasiewicz, Probabilistic default reasoning with conditional constraints, Annals of Mathematics and Artificial Intelligence 34 (2002) 35–88.[21] D.P. Muiño, Measuring and repairing inconsistency in probabilistic knowledge bases, International Journal of Approximate Reasoning 52 (6) (2011)828–840.[22] J. Munkres, Topology, 2nd edition, Prentice Hall, 1999.[23] J.B. Paris, The Uncertain Reasoner’s Companion – A Mathematical Perspective, Cambridge University Press, 2006.[24] G. Parmigiani, Modeling in Medical Decision Making: A Bayesian Approach, Statistics in Practice, John Wiley and Sons, 2002.[25] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann Publishers, 1998.[26] I. Rahwan, G.R. Simari (Eds.), Argumentation in Artificial Intelligence, Springer-Verlag, 2009.[27] R. Reiter, A logic for default reasoning, Artificial Intelligence 13 (1–2) (1980) 81–132.[28] W. Rödder, Conditional logic and the principle of entropy, Artificial Intelligence 117 (2000) 83–106.[29] W. Rödder, I.R. Gartner, S. Rudolph, Entropy-driven portfolio selection – A downside and upside risk framework, Tech. rep., Diskussionsbeitrag derFakultät für Wirtschaftswissenschaft, FernUniversität in Hagen, 2009.[30] W. Rödder, C.-H. Meyer, Coherent knowledge processing at maximum entropy by SPIRIT, in: Proceedings of the Twelfth Conference on Uncertainty inArtificial Intelligence (UAI’96), 1996, pp. 470–476.[31] W. Rödder, L. Xu, Elimination of inconsistent knowledge in the probabilistic expert system-shell SPIRIT (in German), in: B. Fleischmann, R. Lasch,K. Derigs, W. Domschke, K. Riedler (Eds.), Operations Research Proceedings: Selected Papers of the Symposium on Operations Research 2000, Springer-Verlag, 2001, pp. 260–265.[32] L.S. Shapley, A value for n-person games, in: H. Kuhn, A. Tucker (Eds.), Contributions to the Theory of Games II, in: Annals of Mathematics Studies,vol. 28, Princeton University Press, 1953, pp. 307–317.[33] W. Siler, J.J. Buckley, Fuzzy Expert Systems and Fuzzy Reasoning, John Wiley and Sons, 2005.[34] M. Thimm, Measuring inconsistency in probabilistic knowledge bases, in: J. Bilmes, A. Ng (Eds.), Proceedings of the Twenty-Fifth Conference on Uncer-tainty in Artificial Intelligence (UAI’09), AUAI Press, 2009, pp. 530–537.[35] M. Thimm, Analyzing inconsistencies in probabilistic conditional knowledge bases using continuous inconsistency measures, in: Proceedings of theThird Workshop on Dynamics of Knowledge and Belief (DKB’11), 2011, pp. 31–45.[36] M. Thimm, Probabilistic reasoning with incomplete and inconsistent beliefs, Ph.D. thesis, Technische Universität Dortmund, Germany, 2011.