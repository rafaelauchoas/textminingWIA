Artificial Intelligence 105 (1998) 209-261 Artificial Intelligence Multiple perspective dynamic decision making Tze Yun Leong ’ Medical Computing Laboratory, Department of Computer Science, School qf Computing, National University of Singapore, Lower Kent Ridge Road, Singapore 119260, Singapore Received 15 September 1997; received in revised form 17 April 1998 Abstract Decision making often involves deliberations acquisition in the same discourse. This work presents a general paradigm views support knowledge inference decision making over time and under uncertainty. Baaed on a unifying vocabulary model transparency and solution efficiency for the relevant decision problems, in current decision frameworks. and representation this new paradigm balances for multiple perspective task definition and a common the trade-off between in different perspectives. Distinct perspectives or types or stages of suitable for different language the design of DynaMoL support for the modeling The new paradigm motivates for modeling and solving dynamic decision problems. The DynaMoL inferential and representational (Dynamic decision Modeling Language), frame- a general task from the solution work differentiates or computation task. The dynamic decision grammar defines an extensible decision ontology and supports complex problem specification with multiple interfaces. The graphical presentation conven- tion governs parameter visualization as formal model analysis and admits multiple solution meth- semi-Markov decision process facilitates ods. A set of general translation is devised to manage the different perspectives and rep- resentations of the decision parameters and constraints. DynaMoL has been evaluated on a prototype implementation, via some comprehensive case studies in medicine. The results demonstrate practical promise of the framework. 0 1998 Elsevier Science B.V. All rights reserved. in multiple perspectives. The mathematical representation techniques Keywords: Decision making; Knowledge representation; Multiple perspective reasoning; Probabilistic reasoning; Temporal reasoning; Semi-Markov decision processes 1. Introduction Dynamic decision making concerns problems in which explicitly considered. For example, a common medical decision time and uncertainty are is to choose an optimal ’ Email: leongty@comp.nus.edu.sg. 0004-3702/98/$ - see front matter 0 1998 Elsevier Science B.V. All rights reserved. PII: SOOO4-3702(98)00082-4 210 7: K Leong /Artijicial Intelligence 105 (1998) 209-261 course of treatment common investment decision to fluctuating market factors over time. financial for a patient whose physical is to determine conditions may vary over time; a an optimal portfolio with respect Reasoning about dynamic decision problems often information from different perspectives or viewpoints. For instance, at one stage of problem delibera- states that a patient would tion, it might be essential go through; at another stage, it would be illuminating the uncertain effects of a treatment that would lead to different physiological the possible physiological to consider to estimate integrating involves states. intelligence solving, and support different Research in control theory, operations research, decision analysis, artificial techniques for formulating, has led to various (AI) and other disciplines analyzing dynamic decision problems. They adopt different assumptions, ontologies, and have different strengths and weaknesses. One major difficulty of dynamic decision making into simple, parameterized models. Many assumptions resulting models. The limited decision vocabulary contributes but obscures model transparency. Moreover, all the current perspective presentation influence diagrams [37] and decision decision problem. The respective strengths of the different different stages of decision addressed however, integrates and supports different representational factors is to fit the complex decision are implicit in the toward solution efficiency, support single frameworks to the fixed vocabulary or graphical for instance, analysis, In decision to the same frameworks are best suited for In AI, various efforts have solution, and analysis. levels of abstraction. None of these frameworks, the decision models conform framework. trees [60] provide alternate perspectives issues in reasoning at multiple of the specific and constraints formulation, convention reasoning; views. 1.1. Research objectives and approaches introduces to reason about a general for addressing a new paradigm of dynamic decision making. We present a and a that a methodology extension. Multiple in This work uniform way common vocabulary supports multiple perspective perspective different ways; it facilitates effective modeling and analysis of dynamic decision problems. through the Incremental to be use of translators; gradually expanded. class of dynamic such problems. We describe language it allows the scope of the dynamic decision problems addressed reasoning allows the modeler to visualize and examine language extension provides a framework that can be customized the same information decision problems and incremental reasoning The proposed methodology motivates a language design, called DynaMoL the trade-off between model transparency (Dynamic and for the modeling decision Modeling Language). To balance solution efficiency, requirements The relevant support for the modeling correspondences relevant support action in a well-formed model. and relationships for the solution among We evaluate DynaMoL via a prototype studies in medicine.We demonstrate the DynaMoL design differentiates task from those for the solution or computation representational and inferential task facilitates specification the decision parameters computation task accelerates task. and derivation of the and constraints; the of the optimal course of implementation with some comprehensive is expressive enough case to handle a class that DynaMoL I: E hong /ArtiJicial Intelligence 105 (1998) 209-261 211 of real-life dynamic decision problems. We claim that the proposed methodology general and more effective techniques. The exercise also illuminates how the proposed methodology motivates a new generation of dynamic decision making tools and techniques and how it can be put into practical use. than most existing is more 1.2. Guide to the paper This paper is organized as follows: Section 2 describes the class of dynamic decision in multiple decision dynamic reasoning addressed the main concepts and the desiderata of a multiple perspective and the decision problems the design of the DynaMoL in this work. Section 3 introduces language. Section 4 presents the domain background framework. Section 5 for a case study. Based on it examines the syntax and the semantics of the language. Section 7 discusses how the problems perspective modeling sketches the case study, Section 6 describes decision model formulation in detail in DynaMoL are translated among each other and how consistency different perspectives the solution is maintained process. Sections 8 and 9 briefly examine the design of methods and the analyses supported by the language. Section 10 illustrates a prototype in applying and documents the system to different dynamic decision problems compares It also examines research. Finally, Section 12 summarizes the methodology with related work and proposes some ideas for future research. in general AI and decision making some of our experiences in some practical domains. Section 11 and limitations of this work. of the methodology in the translation the achievements the implications for DynaMoL in DynaMoL; system 2. Dynamic decision making under uncertainty: an overview 2. I. The dynamic decision problem The general dynamic decision problem in an environment. The main distinguishing some objectives problem the action or decision points, and the objective measures are specified with respect time horizon. Fig. 1 depicts the factors involved from a static one is the explicit reference in a dynamic decision problem. to time. The environment to a that satisfies feature of a dynamic decision description, is to select a course of action This work addresses dynamic decision problems with the following properties: l The time horizon l The environment is defined as a set of discrete time points. comprises to be discrete, phenomena. A patient is either “well” or “sick” on the third day after being treated; a can that a robot is about to grasp is “full”, “half-full”, or “empty”. a finite set of discrete, or reasonably assumed l There is a finite set of discrete actions. These actions are context-dependent; have varying preconditions, A patient can only go through be carried out if the patient’s physical conditions permit. usually with respect to the environment they or time or both. three open heart surgeries, and each surgery can only l Each action has a finite set of discrete, or reasonably to be discrete, effects. is “well” again; moving from its current position, a robot “gets closer to the target position”. The a patient who was previously After a treatment, forward assumed “sick’ 212 T E hong /Artificial Zntelligence 10.5 (1998) 209-261 ... & & & ... t ,< Objective measure at t-6 j . . . . . . . . . . . . . . . . . . . . . . . 4 \ .\.Objective measure at t ,: t ,,..................... i:: Objective measure at t+6 \ ..” Fig. 1. A dynamic decision problem. nature of the effects are often uncertain. A treatment either cures a disease or leads to some undesirable the time at which the effects may occur are also uncertain. A patient who is cured of peptic ulcer may have a relapse sooner than another patient. side-effects. Moreover, l The effects of an action have measurable desirability. Such a measure can be multi- dimensional, in a hospital and being well, versus of staying staying at home and being sick, but it must be time separable, i.e., the total desirability can be calculated by summing functions over time. e.g., the desirability the desirability Given an initial environment, optimizes stages; the stages may vary in duration. the expected desirability the problem is solved by choosing a course of action that of their potential effects. The decisions are made in 2.2. Current approaches The major approaches to addressing the class of dynamic decision problems described above include the following: 2.2.1. Markov and semi-Ma&v decision processes Markov decision processes (MDPs) are mathematical models of sequential optimization [64] and Bellman and state structure. In the 1950s research in MDPs by Howard decision processes than MDPs. Much progress has occurred since, both in [3], and the formalization to semi-Markov these results formulation problems with stochastic began with the ideas of Shapley [34] and others. Jewel1 [39] extended (SMDPs), which are more general extending [59]. The small and simple vocabulary many assumptions quite formidable and constraints to directly formulate the basic mathematical definitions and in improving involved algorithms in the methodology, however, necessitates the optimization to be implicitly incorporated, and hence complex dynamic decision problems renders it in practice. 2: Y; Leong /Art$cial Intelligence 105 (1998) 209-261 213 for instance, while Markov and semi-Markov In medicine, survival and prognosis analyses, MDPs and SMDPs are seldom applied directly decision making processes are often used in in clinical [38]. 2.2.2. Dynamic decision analysis Emerging in the 1960s from operations theory, the normative, analytical an optimal course of action, but also gain constructing research and game theory [60], decision analysis technique under risk and uncertainty. Based on probability is a useful decision making framework not only helps the decision theory and utility into the complex maker determine and analyzing a graphical decision model decision situation by systematically [36]. In recent years, some new decision analytic to deal with dynamic decision problems. These include dynamic influence diagrams [70], Markov trees [32]; they are based on structural and semantical cycEe trees [2,33] and stochastic extensions of conventional trees, with the mathematical decision models such as influence diagrams and decision definitions of stochastic processes. formalisms have been devised insights Dynamic decision analysis is used widely in real world applications. For instance, it is a common is difficult intensive, and the graphical structures tool in clinical decision making to apply. In particular, model formulation [40,54,56,57]. Nevertheless, is knowledge-intensive restrict the admissible solution methods the methodology and labor- [44]. 2.2.3. Planning in artijicial intelligence theorem proving, AI planning Motivated by the studies of human problem emerged [26]. Early research logical et al. [51] and Fikes and Nilsson and reasoning with complete and deterministic imperfect and plan-execution the various planning gaining attention extends algorithms frameworks, information, [ 12,25,47]. [52], operations solving research, and in the 1960s with the works of Newell in this area focuses on representing introduces information. Recent progress the planning ontology, and improves [69]. Analytical studies on the theoretical in terms of their expressiveness the plan-generation of and complexity, are also foundations planning algorithms to be applied of AI planners [75] and O-Plan [13], they are difficult [48]. While there are some general-purpose in the fixed planning vocabularies. As compared The AI planning works that have modest impact on realistic applications, however, are systems special-purpose in practice in use, e.g., SIPE is partly due to the complexity of the problems [20]. The impracticality planning problems to encode domain-dependent addressed, and partly due to the difficulty to, say, an MDP, where the and heuristics solution algorithms operate directly on the fully specified set of actions, states, and the among them, an AI planner allows more flexible and expressive probabilistic action and problem descriptions. for instance. their effects, and the relevant partial plans with hierarchical [20]. Such expressiveness constraints facilitates complicate search control for the optimal solutions. representation are represented with various syntactic abbreviations formulation of highly complex problems, task network (HTN) planning, but may significantly of the actions, In hierarchical relationships 214 TY Leong /Artificial Intelligence IO5 (1998) 209-261 planning 2.2.4. Recent development in decision-theoretic planning In the late 1980s research efforts in decision-theoretic the different approaches described in a stochastic domain, where techniques from involves planning works in DTP are based on the semantic and computational The major focus in DTP research solutions with approximation to evaluate potential plans in achieving goals [23,43]; restricting of the state space and allowing existing MDPs solution reduced problems 221 or the action space or both [3 1] to reduce the search space for optimal plans. (DTP) began to integrate above. The general DTP problem the action effects are uncertain. Most framework of MDPs [16]. is, however, on speeding up computation of the optimal include using probabilities search to local regions to be applied on the [1,19,67]; and using abstraction or aggregation of the state space [7,17, techniques. Relevant approaches techniques DTP frameworks usually employ more compact and structured representations of the a common than MDPs. For instance, problem components action effects adopted in such frameworks [43]. More general or expressive definitions of the state space [30] and the utility functions or adopted goal descriptions [6,30,74] may also be involved. The expressive they in these frameworks mainly aim to support search control for the optimal solutions; sometimes of the planning problems, but this is a usually a secondary concern. is the probabilistic STRIPS-like operators representations also facilitate representation of uncertain formulation 2.3. A uniform task dejinition The three major tasks are problem in dynamic decision making under uncertainty solution, and analysis. These tasks may be iterated many times for a given formulation, problem. 2.3.1. Problem formulation involved. Formulating and the constraints a dynamic decision problem begins with specifying type, the problems, different decision parameters, In optimization problems, actions or strategies are compared an optimal course of action include alternative actions and strategies, possible state transitions, uncertain events that constitute and relevant probability distributions with respect to time. Examples the state transitions, of decision constraints conditions of the actions, validity conditions of the states, and logical relationships is determined. Examples of decision parameters among the states and uncertain events. in terms of their effectiveness. include applicability In discriminatory the problem The formulation task is guided by the decision ontology or vocabulary with the following and constraints of a decision ontology defines how accurately we can specify situation; defining characteristics: l Expressiveness parameters organization decision ontology may include only actions and states as basic concepts and temporal as basic relations, while another may also include uncertain events and dependences probabilistic a decision ontology may represent actions as dependences. individual concepts, while another may support reasoning with classes of actions. the the types and a in a decision factors and interrelationships. of the relevant For example, it determines Similarly, their T.Y Leong /Artificial Intelligence 105 (1998) 209-261 215 of a decision ontology determines l Succinctness parameters in a decision states in a dynamic decision problem have to be explicitly enumerated; the state descriptions situation. For example, and constraints can be factored into the relevant conditions or attributes. how easily we can specify the in SMDPs, all the in AI planning, information of a decision ontology involved l Transparency on the comprehend not necessarily be needed to specify a small piece of knowledge. and how easily we can such information. An ontology with a simple syntax or semantics does imply that it is transparent; unless it is succinct, many expressions may reflects how efficiently we can draw inferences in a decision situation, 2.3.2. Problem solution The solution is conditional to a well-formed some objectives. A closed-loop dynamic decision problem solution it solves the problem with respect to all possible that of initial points and in an SMDP An open-loop solution does to it solves the problem with respect initial point and a specific end point, e.g., a plan generated by a classical AI is a computational model in the well-formed model; it does not need to know how optimizes the decision situation; all possible end points, e.g., a policy determined not employ a specific planner that manipulates the information organization for a starting state and a goal state. A solution method is a course of action on the observation arises in the first place. from the decision the information feedback situation; 2.3.3. Problem analysis Both the formulation and the solution of a dynamic decision problem may require to ensure accuracy, sensitivity involved analysis sensitivity. Structural and clarity. Sensitivity sensitivity analysis of the decision parameters and constraints relevance, and numerical qualitative relevant alternatives, sensitivity includes affect the choice and the value of the decision. can be divided is reflected that the problem threshold analysis and stochastic analysis it ensures and uncertain involved; objectives, through adjustments is reflected information sensitivity of the quantitative into structural through adjustments of the is framed correctly, and the events are specified properly. Numerical that reveal how the uncertain information involved; it events 2.4. The application domain test and therapy planning in this work are general, tests and treatments, or combinations the issues we address is diagnostic involve domain we the application While in medicine. The relevant dynamic decision examine the risk of some adverse events that may continue or vary over time; the problems are uncertain. The relevant actions events may recur and the timing of such recurrences comprises are diagnostic include the the physical conditions physiological that would lead to the states. For instance, a patient can be in a state with a stroke, and a cerebral hemorrhage may have caused the stroke. Some of these events are the effects of the actions. The decision objectives usually life expectancy and include both prolonging maximizing states of the patient, or any observable or unobservable of a patient or a class of patients. These conditions of them. The environment the cost-effectiveness of the actions. the patient’s events 216 T Y Leong /Art&id Intelligence 105 (1998) 209-261 3. Multiple perspective reasoning Decision making often involves deliberations tives or views support knowledge acquisition and representation or stages of inference support for visualizing spectives. Different ways. frameworks, however, may display the decision parameters and constraints in the same discourse. Many decision making in different perspectives. Distinct perspec- suitable for different types frameworks provide in specific graphical per- in different the same information 3.1. Advantages and disadvantages of different perspectives To illustrate tives, consider the strengths and weaknesses of visualizing the following example: in different graphical perspec- Example. A patient is in one of three states: WELL, SICK or DEAD. A doctor must decide treatments A and B, each of which will have a different effect on the patient between the WELL or the SICK state. The patient can only stick with one course of in either treatment, which may be repeated many initial success rate in treating once the treatment fails, i.e., does not improve the patient’s condition within the fixed period before the next treatment decision. Treatment B, on the other hand, may cause a serious complication C in an already sick patient. the disease, but the patient will die almost immediately times. Treatment A has a higher Fig. 2 illustrates the structural representations transition diagram of an MDP; (b) a dynamic and (c) a Markov cycle perspective; each perspective explicates from a different point of view. Problem conducted with respect to the chosen perspective. tree. Each representation of the problem in (a) a Markov state influence diagram over a finite time period; in a specific the problem projects level of detail, the problem structure at a different are and analysis in each framework solution 3. I. 1. State transition diagram In a state transition diagram, associated function transition functions, denote is defined for each state in the diagram. the nodes denote the possible the states, and the arcs, with their transitions given the action. A value As shown in Fig. 2(a), each state transition diagram explicitly transitions given au action. The diagram does not reflect, however, e.g., complication C for treatment B, that may occur during state transitions. shows the possible state events, the uncertain 3.1.2. Dynamic In a dynamic injuence diagram influence diagram, the squares represent the circles the value nodes. The number at the end of each the chance nodes, and the diamonds is considered. The arcs node indicates and the arcs leading leading in each chance node or value node is a list of the possible values or outcomes of the node, e.g., into the chance and value nodes indicate probabilistic into the decision nodes dependences, dependences. Embedded the decision stage in which the decision/event/value the decision nodes, informational indicate T. Y Leong /Artijicial Intelligence 105 (I 998) 2OS261 217 State transition diagram for Treatment A State transition diagram for Treatment EI Keys: q Decision node @ ~$tevda;a;ode ‘n (Complications) 0 Chance node __) Pmbablllstk lnftuence 0 v, (wdttlonal dependence) Va’ue node Markov cycle tree for Treatment A Markov cycle tree for Treatment 8 Keys: Markov node (Branches emanating are states) -1 State node 0 Chance node -- Probabilistic or conditional dependence Fig. 2. Representation influence diagram; and (c) Markov cycle trees. of a simple decision problem in: (a) Markov state transition diagrams; (b) a dynamic conditional on its probabilistic predecessors; “Well”, “Sick’ and “Dead” for S, and “Complication CT” and “No complication C” for C, , and a table of probabilities in each decision node is a list of the alternate predecessors. shows the sequential structure of the decision problem, and may capture the varying uncertain events in-between in particular, and the relations state transitions. But the possible patterns of state transitions in general, are kept only in among the chance outcomes and/or the decision alternatives treatments and a list of its informational influence diagram explicitly in Fig. 2(b), a dynamic As shown embedded 218 ZYS Leong /Artij’icial Intelligence 105 (1998) 209-261 the table entries embedded reflect that only treatment B may lead to complication C. in the nodes. For instance, the structure of the diagram does not 3.1.3. Markov cycle tree In a Markov cycle tree, the branches emanating stage, given an action. The intermediate the possible outcomes of as well as the probabilistic represent the states at the beginning from the root node, called the Markov and at the end nodes are chance nodes and the among events that could happen between the root and the leaves of a cycle tree. is dependencies of uncertain in between is associated with each branch of the tree. A value function node, and the leaf nodes, respectively, of a decision arcs indicate the nodes. All the possible combinations any two state transitions are represented A conditional defined for each state in the diagram. probability As shown in Fig. 2(c), a Markov cycle tree explicitly shows the possible state transitions it also depicts given an action, and reflects the uncertain events in-between asymmetric relations among the chance outcomes and decision alternatives. For instance, complication C may occur only in an already sick patient if treatment B is prescribed. The and the uncertain events tree, however, becomes extremely complex vary with time; it also cannot represent a mixture of sequential actions effectively. if the state transitions state transitions; 3.2. Desiderata of an integrated decision language Based on the above observations, we postulate decision making should provide representational tasks involved, while balancing efficiency. In particular, such a language should satisfy the following desiderata: the trade-off between model transparency that an effective and inferential language for dynamic support for the different and solution l The decision ontology must address a wide range of issues in typical dynamic decision should be expressive, and their organization components problems. The language succinct, and transparent. l The language should have a formal theoretical basis with simple and rigorous syntax the formal and semantics. These qualities would facilitate examining properties of the problems and their solutions. and analyzing l The language must support at multiple user can deal mainly with the relevant ontological mathematical definitions. reasoning levels of abstraction, concepts, so that a instead of the specific l The language must support graphical in multiple perspectives and constraints Visualization to viewing making. providing a perspective most natural in multiple perspectives It is analogous of the decision visualization and across different reflects a common pattern parameters levels of abstraction. in human decision the world with different pairs of lenses, each to a particular task. should be incrementally additional l The language incorporating and constraints. Adaptation constructs; l The language A practical it does not necessarily should support language implementations is modular, comprehensive language constructs involves changes extensible and adaptable. Extensions involve for new types of decision parameters of the language in the organization affect the expressiveness of the language. that can be put into practical use. and explicit. lYK L_.eong /Artificial Intelligence 105 (1998) 209-261 219 4. DynaMoL: the language The DynaMoL design is motivated by an in-depth analysis of existing frameworks [44] in the previous section. In this section, we explain the features. Detailed definitions of the design the language and based on the desiderata outlined design approach and summarize will be presented in Sections 6-9. 4. I. Design approach To address the transparency-efficiency trade-off, the representation problems. a high-level and inference support for modeling It incorporates the general language for modeling idea of programming into an object language for execution. the DynaMoL design aims to separate and solving dynamic decision translating language design: The language comprises four components: the graphical presentation the mathematical extensible decision ontology and supports complex problem interfaces; multiple perspectives; analysis and admits multiple devised to manage the different perspectives and representations and constraints. the dynamic decision grammar defines an specification with multiple in formal model is techniques convention governs parameter representation as SMDP facilitates translation of the decision parameters solution methods; a set of general visualization language is defined by convention. The semantics the dynamic decision grammar and that bridges the grammar, is defined by the mathematical the presentation, of an SMDP and the translations The syntax of the the graphical presentation representation and the mathematical In DynaMoL, representation. therefore, we formulate and examine a dynamic decision model in terms this for solution of the high-level decision grammar with different graphical visualization high level model is then translated and analysis. into a formal mathematical representation perspectives; 4.2. Basic decision ontology We adopt a knowledge formalization approach similar in [27]. In DynaMoL, a conceptualization the relevant decision parameters and the interrelationships we define the expressions (FOPC), as presented includes the conceptualization, components: basic concepts, variable concepts,functions, in DynaMoL to first-order predicate calculus of a decision situation among them. Based on in terms of the following relations and constraints. 4.2.1. Basic concepts A basic concept is a partial description of the decision situation at a single point in time. l An event describes a property of a phenomenon. There are three types of basic concepts: events, actions and states. It corresponds to a proposition of the t. In the clinical context, “presence form: P(r) with unary predicate P and constant of complication C”, “ absence of complication C”, “ number of stroke (that a patient has suffered) is 2” are all is I”, and “number of stroke (that a patient has suffered) examples of events, which can be expressed as the propositions: 220 lY:E Leong /Art$cial Intelligence 105 (1998) 209-261 Complication-C(Present), Complication-C(Absent), Number-of-stroke(l), Number-of-stroke(2), respectively. a An action is an externally include different of events which controlled phenomenon. involves tests and treatments, to a single or relevant a conjunction actions e.g., “Test A”, “Treatment B”, “Test X followed by Treatment Y.” The effects of an action are in terms of events, e.g., a side-effect of prescribing Treatment B is the event expressed c”. These actions and their effects can be expressed as the “presence of complication propositions and conjunctive propositional an actor. In the clinical and their combinations, It corresponds sentences: context, Test(A), Treatment(B), Test(X)&Treatment(Y), Complication-C(Present), respectively. l A state describes a phenomenon with properties relevant to the overall decision ob- jectives, brought about by performing certain actions. It corresponds of special events called the state attributes. For instance, clinical decision problem can be expressed as the conjunctive propositional to a conjunction the state WELL in a typical sentence: Vital-status(Alive)&Disease-X(Absent)& Disease-Y(Absent). Every state has an associated utility or value, indicating that state. the desirability of being in 4.2.2. Variable concepts A variable concept represents uncertainty in a partial description of the decision function of the form: in probability situation at a single point in time. It corresponds P(x) with unary predicate P and variable x in FOPC or a random variable theory. There are two types of variable concepts: chance variables and action variables. We only consider discrete variables to a propositional (a) An instantiation in this work. of a chance variable in the FOPC and an outcome is an event or a state as defined earlier. Each or outcome occurs only by chance. A chance variable also to a chance node in a decision model. There are three types of chance interpretation interpretation of it in the probabilistic possible corresponds variables: instantiation (i) Event variable. The possible are events. For instance, instantiations “number of stroke (that a patient has suffered)” or outcomes of an event variable is Z Y; Leong /Artificial Intelligence 105 (1998) 209-261 221 that can be denoted as an atomic sentence: Number-of - Number-of-stroke(l) and an event variable stroke(x), with possible instantiations Number-of -stroke (2)) or a random variable: Number-of-stroke with possible outcomes x = 1 or 2. = x, (ii) A state attribute variable is a special kind of event variables; it represents a the states. The possible characteristic or property directly relevant to describing instantiations or outcomes of a state attribute variable are the state attributes, which are special events. Given a set of such variables, a state is usually defined in terms of the Cartesian product of their outcomes. Some heuristics, however, may be used to reduce the number of the resulting states. For instance, the state of a patient, described as WELL, SICK, or DEAD, can be defined in terms of the “vital status (alive or dead)“, “disease X (present or state attribute variables: the absent)“, and “disease Y (present or absent)“. state WELL canbeexpressedas theconjunctionofthe al- state attributes:Vit status(Alive)&Disease-X(Absent)&Disease-Y(Absent). the uncertainty is to a state space in an SMDP influence diagram at that time point. the possible or outcomes of the state variable for a clinical decision problem reachable at a single point in time. It corresponds and a state variable node in a dynamic All the outcomes of a state variable are states. For example, instantiations are the states that a patient can be in: WELL, SICK and DEAD. In particular, as mentioned, the actual state about that (iii) A state variubZe represents of an action variable interpretation (b) An action variable denotes a decision or choice point at a single point in time. An in the FOPC interpretation and an alternative of it is an action as defined earlier. An action variable to part or all of the action space in an SMDP or a decision node in instantiation in the probabilistic also corresponds decision model. For example, for an action variable can be denoted as an atomic sentence: Treatment with possible (A) and Treatment variable: Treatment = x, with possible outcomes x = A or B. in a clinical decision problem, the possible alternatives (B) , or a random instantiations Treatment (x ) , 4.23. Functions There are two types of basicfunctions (a) A probabilityfunction function among the events and/or the transition characteristics is either a probability mass function (CDF). The probability functions express (PMF) or a cumulative the conditional among the states, defined over the basic and variable concepts: distribution probabilities given an action. (b) A valuefunction measures the desirability of being a state in the decision situation. in the cost and life expectancy e.g., monetary It may have different dimensions, clinical context, and is usually conditional on an action. 4.2.4. Relations There are two types of basic relations defined among the basic and variable concepts, from the domain relation constants used in the descriptions which are to be distinguished of the basic concepts in Section 4.2.1: 222 L E bong /Artificial Intelligence 105 (1998) 209-261 (a) A probabilistic dependence notion is conditionally ditional dependence concept rection of a probabilistic dependence conditional dependence; complication C” is probabilistically state “(the patient is) SICK”. relation between in probability to the con- that a theory. Such a relation dependent on another, given the decision situation. The di- two concepts corresponds indicates relation reflects the definition of the underlying it has no temporal implication. For example, “presence of dependent on the action “Treatment B” and the (b) A temporal dependence relation between two concepts indicates precedes another; it has no causal implication. For example, t is temporally dependent on the state variable at time t - 1. that one temporally the state variable at time 4.2.5. Constraints A constraint is a meta-level concepts, quantification assumed a quantification sentence to be applicable sentence: the relations, and the functions descriptive or prescriptive imposed on the to a logical or that all the actions a E A are in all the states s E S at all time points t E T can be expressed as as defined above. It correspond the constraint in FOPC. For instance, condition VaVsVt (Applicable(a, s, t)). 4.3. Dynamic decision grammar for DynaMoL The dynamic decision grammar defines the structure of a dynamic decision model in terms of its components; of these components support multiple defining appropriate is an abstract grammar. The grammar the structures are recursively defined in a similar manner. The abstract grammar can the grammar can be extended by implementations. Moreover, for the new constructs. translators interface The complete dynamic decision grammar for DynaMoL is documented in [45]; it contains the following components: l A finite set of names of constructs; l A finite set of productions, each associated with a construct. that defines the construct “model” An example of a production is as follows: Model -+ name: Identifier; contexts: Context-list; de3nitions: Dejinition-list; constraints: Constraint-list; solution: Optimality-policy. Each construct describes the structure of a set of objects, called the specimens of the construct. The construct appearing on the right-hand productions. productions, “choice” productions, is the (syntactic) type of its specimens. The constructs/types side of the above definition are similarly defined by different The structure of a construct terms of “aggregate” i.e., the constructs have specimens comprising a fixed number of components, can be specified in “list” productions, etc. 7: E Lmng /Artificial Intelligence 105 (1998) 209-261 223 4.4. Graphical presentation The presentation convention constraints be visualized graphical representations. in the grammar are displayed; in DynaMoL prescribes how the decision parameters and can it determines how the same information in multiple perspectives. The current convention includes three established 4.4.1. Transition view The transition view for a given action depicts to the Markov state transition diagram corresponds Fig. 3, each node represents a state or condition with an associated utility or value function. Conditional on the action, each arc represents a possible one-step is defined on each arc to govern the transition characteristics. in the state of origin; a transition transition function the possible state transition patterns. It in the MDP framework. As shown in 4.4.2. Influence view An influence view for a given action shows the nature of uncertainties into network; for a specific decision to be factored to a Bayesian or probabilistic the state transitions state attribute variables. corresponds conditional only on one alternative. to a slice of a dynamic the state descriptions It corresponds involved in the relevant it also stage, and allows influence diagram stage is represented for Disease-X? (disease X present) and “No-X” As shown in Fig. 4, each node denotes an event variable with a few possible outcomes, (disease as a state variable. state stage n) and stage n + 1) are the four states and Disease-Y? state definitions: e.g., the outcomes are “x” X absent). The state space at each decision The possible outcomes of the state variable are all the states in the corresponding space, e.g., the outcomes of the state variable node S, (state at decision the next-state variable node &+I the event variables Disease-X? depicted in Fig. 3. In this example, are also the state attribute variables to the following that contribute WELL = {No-X, No-Y], DISCOMFORT = (No-X, Y), SICK = (X) and DEAD. Each arc dependence. A conditional probability in the figure indicates conditional or probabilistic distribution table is associated with each node. The diagram can be interpreted as follows: given that the action is taken at any of the state of origin at decision stage n, the state of the patient at decision stage n + 1 is conditionally dependent on the presence or absence of disease X and the presence or absence of disease Y, as characterized by the underlying conditional probability distributions. (state at decision Fig. 3. A transition view for an action. 224 ?: k: Leong /ArtQicial Intelligence 105 (1998) 209-261 Fig. 4. An influence view for an action. Sick _ No- X 0.7 Discomfort 1 .o Dead No-Y Well 0.3 1.0 B 4 Fig. 5. A tree view for an action. 4.4.3. Tree view The tree view for a given action is a decision tree expansion of the influence view. It to a Markov cycle tree for a specific action. It explicitly in-between possible state transitions in a chronological manner; shows the uncertain it also depicts corresponds events asymmetric relations among the chance outcomes. As shown in Fig. 5, the nodes correspond to the event variables in the influence views, and the arcs capture both the possible outcomes of the event variables and probabilistic dependences. the root node S, and the leaf nodes &+l In particular, represent the state I: k: Leong /Artificial Intelligence 105 (1998) 209-261 225 and the branches of these nodes variables at decision denote is the possible associated with each outcome branch of an event variable, conditional on the given action and all the states and/or events to the left of the event variable. stages n and n + 1, respectively, states at the corresponding stages. A probability decision entry 4.5. Mathematical representation The central basis among theoretical basis of a dynamic decision model is an SMDP idea in supporting multiple perspective the different views or aspects of a decision reasoning is to establish a common the In DynaMoL, situation. Briefly, an SMDP is a mathematical model of a sequential decision process. An SMDP a time index set T, an action space A, a state space S, has the following basic components: a set of one-step transitionfunctions with probability mass functions (PMFs) q$)(m. t) and functions (CDFs) Q$‘(m, cumulative distribution where i, j E S, a E A, t E T, m = [O, 1,2,3,. are reflected duration m) before the transition, both governed by the one-step MDP is an SMDP with constant in both the destination inter-transition A solution of an SMDP is an optimal policy; (i.e., state j) of the transition and the time lapsed (i.e., functions. An transition times at one time unit (i.e., m = 1). it is a guideline for choosing the optimal for all possible evolutions of the states, that maximize t), and a set of valuefunctions v,‘“‘(m), . .]. The stochastic nature of the transitions actions over the decision horizon, the expected value or reward. Many solution methods are available for SMDPs. 4.6. Translation convention Two types of translation are involved in co-ordinating multiple perspective reasoning in a model inter-level inter-level translation, DynaMoL: translation and inter-perspective translation. in the dynamic into an SMDP The model may involve more constructs rules are employed In translated SMDP. A set of translators or correspondence correspondence. The general idea is to map a construct or a set of constructs to an entity or relation Inter-perspective on the other hand, establishes in the mathematical representation. translation, specified decision grammar is than those defining an the proper to establish in the grammar translation representational formats. Since the different perspective constructs among the different perspectives. For example, view corresponds corresponds to the state space of the transition view. to the root node with all its branches can be defined the same in terms of any covering correspondence information is involved, among inter- set of representation the state variable in the influence in the tree view, which also 4.7. Dynamic decision making in DynaMoL Dynamic decision making in DynaMoL is an iterative process; it involves and repeating the problem l The dynamic decision problem formulation, problem solution, and problem analysis is formulated as a dynamic decision model. The model specifies the relevant decision parameters and constraints, and the preferences involved. including the uncertainties interleaving tasks: 226 T:Z Leong /Artificial Intelligence 105 (1998) 209-261 l The dynamic decision model is solved with respect to an evaluation or optimal criterion, to derive an optimal policy or course of action. l The model and the solution are analyzed to ensure correctness of the formulation and robustness of the results. In the next few sections, we discuss the modeling, solution, and analysis processes in the DynaMoL framework with a case study in medicine. 5. A case study Atrial fibrillation two major undesirable formation of blood clots. AF can occur Both the frequency fibrillation often develops eventually. (AF) is a kind of cardiac arrhythmia or abnormal heartbeat. It has i.e., i.e., sudden, periodic episodes. and the length of the AF episodes may increase with time; constant and embolization, in paroxysms, hemodynamic deterioration side-effects: AF management involves using ant&rhythmic restore normal sinus rhythm. Because of the risk of embolization, as warfarin and aspirin are often used to prevent blood clot formation treatments, however, have corresponding antiarrhythmic cause excessive bleeding, which in turn may lead to strokes or death. to control heart rates and to such in the atria. The In particular, a common the risk of sudden death; an anticoagulant may agent, Quinidine, anticoagulants side-effects. undesirable increases agents The case in question is based on an actual clinical consult at the Tufts-New England Medical Center in Boston, Massachusetts. The patient history of paroxysmal AF. He has been on warfarin and on Quinidine. The clinical decision problems are as follows: is a 52-year old white male with a Question 1. Quinidine decreases the decreased risk of embolic complications the proportion of time that the patient spends in AF. Does justify the increased risk of sudden death? Question 2. What is the optimal course of treatment account treatment? relative risk of bleeding the varying for warfarin with respect in the next five years, taking into to the duration of the above, theoretical capabilities to illuminate and practical of the DynaMoL The case study aims is modeled and solved can handle an actual dynamic decision problem and framework. The original clinical consult, which addresses in the Markov cycle tree in the first clinical the exact qualitative and quantitative parameters of the model whenever to existing frameworks, we the adopted in is less important. For ease of exposition, we formulate the two dynamic in terms of a single dynamic decision model with two sets of slightly limitations the first clinical question only framework. To show that DynaMoL medicine, we adhere closely question, reusing appropriate. To show that DynaMoL offers additional focus on illustrating second clinical question; this latter problem decision problems different numerical parameters. the clinical significance of the data and assumptions to the original clinical consult and computational features involved the ontological in addressing in addressing flexibility We assess the effectiveness of DynaMoL problem with respect to three criteria. First, we demonstrate for modeling and solving the dynamic decision how the relevant decision ZI: Leong /Art@ial Intelligence 105 (1998) 209-261 227 the solutions and constraints can be expressed parameters compare of domain experts or both. Third, we conduct behavior of the parameters ensure interpretation, and analysis reasonable for the case study are documented analysis on the solutions sensitivity involved. The detailed discussion, in [45]. to data to the results of the actual clinical consult or the clinical judgment in the DynaMoL ontology. Second, we 6. Model formulation in DynaMoL Model formulation (I) Specify a problem (2) Define the alternative actions and the states involved; (3) Conditional typically comprises on each action, type, its duration, and its optimality and evaluation criteria; the following tasks: identify possible progression patterns of significant states or final outcomes and the values achievable and values are difficult such transitions (4) When in the states; to be directly assessed, uncertain relations effects of actions by specifying the possible event variables that would constitute the transitions; identify and their (5) Identify special assumptions on the actions, effects, and states and impose relevant constraints among the decision parameters when appropriate. in performing There is no strict order definitions may and often change modeling In DynaMoL, we formulate a dynamic decision model via interfaces through multiple perspective visualization. dynamic decision grammar, and for the is to provide adequate and direct support for the changes. the in the modeling process. A major challenge involved. The specifications that implement the tasks therefore, language, 6. I. Definition of a dynamic decision model A dynamic decision model in DynaMoL is a well-formed or complete model when it corresponds directly correspond The default optimality criterion to a well-formed SMDP with an optimality criterion. Some model constructs to the underlying mathematical definitions, others need to be translated. discounted total expected value. is finite-horizon Definition 1 (Dynamic decision mode& A dynamic decision model M in DynaMoL an lo-tuple criterion, where: together with an evaluation or optima& (T, A, S, E, QE, Z, q, {, K, r) is l The decision horizon or time index set T = {0, 1,2,3, . .) denotes the time frame for the decision problem. l The action space A = (al, a2,. . . , a(AI] denotes the set of alternative actions to be considered. l The state space S = (SI, ~2, . . . , slsl} denotes the set of states or conditions that would affect the value functions and in which the actions can take place. l For each action a E A, its action-specific event variable space E, = (el , e2, . . , e(E, I} its effects. The event vari- the set of event or chance variables that constitute denotes able space E = UucA E, denotes the set of all event variables for all the actions. 228 I: Y hong /Art$icial Intelligence 105 (1998) 209-261 l Each event variable e has an associated event or outcome space L?, = [WI, O_Q, . . . , Wk}; k > 1, which denotes the set of possible outcomes for e. The action-specz$ic event space DE, and the overall event space DE are Simihrly defined. l Conditional on each action, a set of possible transitions 8 = {t(a, i, j, t) 1 i, j E S, a E A, t E T} is defined among the states. A transition action a E A at time t E T, denotes nature of this accessibility relation 6 & A x S x S x T between any two states i, j E S, given an of j from i given a at t. The with PMF is characterized by a one-step transitionfinction the accessibility q$)( .) and CDF Q$‘( .) as defined for an SMDP. on each action, a set of probabiEistic l Conditional influences @ = {+(a, x, y, t) 1 a E influence A, x, y E Ear t E T} is defined among the event variables. A probabilistic variables X, y E Ea, given an action a E A at time t E T, reflects dependences, variables at time t . the action, among conditional on the probabilistic the outcomes of the two event relation q G A x E, x E, x T between any two event l Conditional on each action, and with respect to the evaluation criterion, a set of vaEue functions < = {v?‘(m) 1 i E S; a E A, m = 0, 1,2, . . .) is defined for the states. A value function u : A x S x K + IR determines i E S state over time duration m E K, conditional on action a E A. the objective value achievable in l The above components can be subjected to a set of general or domain-specific sentence or well-formed constraints K = {K 1 K C {S U fiE U 3 U !P U 0”; n b 2). A constraint to a logical or quantification formula K corresponds (wff) in FOPC. is the set of translators, l f i.e., the set of correspondence rules or functions that establish equivalence relations among the language constructs. 6.2. Basic problem speci$cation The basic characteristics of a dynamic decision problem are determined as follows: 6.2.1. Problem type Both optimization An optimization problem problems and discrimination is solved by constructing problems are supported by DynaMoL. an optimal policy where puf : S + A; t E T is an optimizing space A at time t with respect to the time index set T. function from the state space S to the action An optimization problem compares the independent effects of actions; stage, what is the best alternative the problem action solution answers to take? the question: at each decision Contrarily, a discrimination problem is solved by choosing the best policy: 75* = na* l? E Leong /Art$cial intelligence 105 (1998) 209-261 229 from a predetermined set of single-action involve a single action over the entire decision horizon, such that stationary policies {n”}, i.e., policies which na* = { ~a*JLa*&a*&a* ,... ] where kLn* : S --+ a*; a* E A, is a function a*. from the state space S to an optimizing action The main type of discrimination problems delineates strategies or combinations of effects. This formulation in a strategy, and incorporates the embedded actions, actions with dependent controlled actions, called explicit enumeration combinations question: designates a single action as externally the application pattern of some other relevant in the state descriptions. To solve the problem, an action and all possible the solution answers for the entire decision horizon, what is the best alternative strategy to take? is necessary. The problem of the policies associating of the embedded each controlled actions The first dynamic decision problem in the case study is formulated as a discrimination problem. Four strategies are to be discriminated: (1) start without any drug, with warfarin administered (2) start with warfarin, which can be stopped when necessary; (3) start with Quinidine, with war-farm administered (4) start with both Quinidine The second dynamic decision problem and warfarin, with warfarin stopped when necessary. in the case study is formulated as an optimization and stopped as necessary; and and stopped as necessary; problem. The problem without warfarin or Quinidine or both, with strategic implications is to determine a course of optimal initial action, e.g., start with or as defined above. decision modeling problems support frameworks to different extents. Dynamic of optimization Other dynamic support and discrimination problems. Without augmenting direct formulation of both optimization on the solution computational methods, Markov cycle trees only support direct formulation of discrimination problems. They do not support direct formulation of optimization problems because they do not allow decisions the values for the alternatives over the entire decision horizon. formulation influence diagrams is made only at the final stage by comparing to be made in stages; a decision or flags, and assumptions and discrimination such as bindings structures 6.2.2. Decision horizon The decision horizon can be finite or infinite. A finite horizon with a long duration and small time units may be approximated as an infinite horizon. In the case study, the decision horizon T is 600 months or 50 years for the first dynamic decision problem, and 60 months or 5 years for the second. 6.2.3. Evaluation criterion The evaluation cost, and cost-benefit one evaluation and additive. The evaluation criterion (QAW. criterion can be in any unit of interest, e.g., life expectancy, monetary ratio; it is defined by one or more sets of value functions. More than to be linear life expectancy for the case study is quality-adjusted criterion may be involved. The value functions are assumed 230 I: I! Leong / ArtQicial Intelligence 105 (1998) 209-261 6.3. Structure speci$cation DynaMoL currently remain unchanged behavior; activities over the entire decision horizon. An action assumes a static action space and a static state space, i.e., they indicates a single unit of two drugs. The constituent it may involve more than one activity, e.g., administer in an action are assumed to be simultaneous controlled In the case study, the action space A = {NoDrug, Quinidine} actions and constant at every decision stage. consists of the two involved. The effects of war-farm are the states state attribute variables: vital status indicates whether indicates heartbeat pattern; cerebral vascular for the strategies externally modeled as embedded actions in the state descriptions. The state space S includes defined the patient accident indicates history of obstruction of blood flow to the brain, which may result in a stroke or temporary weakness; warfarin eligibility affects the applicability of the action warfarin; warfarin status reflects the status of the embedded action warfarin. is alive or dead; sinus rhythm in terms of the following Conditional on each controlled action, a set of value functions C is defined for the states. the QALE achievable A value function v,‘“‘(.) indicates given the action. as chance In dynamic nodes directly therefore, are the strict Cartesian product of the outcomes of these chance nodes. In Markov cycle trees, state attribute variables are captured in these frameworks often involves influence diagrams, influencing the value nodes; the implicit state descriptions, the state attribute variables are represented in a state over a time duration, of the relevant bindings. state space definition logical combinations in the bindings; in the case study. is in atria1 63.1. Modeling in the transition view Based on the state attribute variables, a total of 20 states are defined For instance, a relevant state is: AF-STR-WNE, which indicates fibrillation (AF), has a history of stroke (STR), and is not eligible for warfarin (WNE). that the patient Conditional on each controlled action, a set of possible the states. Each transition C; is associated with a one-step transitions transition 8 is defined among function with PMF q,!,a)(.) and CDF Q$‘(.), governing before the transition. the destination of the transition and the time remaining functions Transition the transitions including only the state attributes Fig. 6. in a transition view. A simplified for an action can be specified directly on the links representing transition view for the action Quinidine, action warfarin, to the embedded related is shown in The transition view above expresses, among others, the following facts for a patient on Quinidine: l If a patient determined l If a patient determined is not on warfarin, he may either be put on warfarin to be ineligible for warfarin. later or may be is on warfarin, he will never be put off warfarin unless he is later to be ineligible. l Once a patient is determined treatment again. for warfarin to be ineligible for warfarin, he will never be considered ZI: kong /Artificial Intelligence 105 (1998) 209-261 231 WA@ARlN-OPF 6 WARfAR1NQN Fig. 6. Transition view for action Quinidine. 6.3.2. Modeling in the influence view In the transition view, all the states are assumed they can be expressed can then be specified directly. At times to be fully observable. in terms of the state attributes; to specify it is difficult the action effects are partially observable, or interact in a complex manner. Such effects are modeled as event variables effects are fully observable, functions transition transition functions directly, especially when unobservable, and the probabilistic If the action the the influences among them. In the case study, the event variable space E includes event variables indicating the cerebral hemorrhage, gastro-intestinal presence or absence of thromboembolization, bleeding and whether the patient survives action, these events probabilistically influence eligibility, cerebral vascular accident, etc., which in turn define the physiological of the patient. Conditional event variables; variables the events should they occur. Conditional on an the state attribute variables such as warfarin states on each action, a set of probabilistic the absence of a probabilistic the relation @ between any two event influence between can be defined directly on the event variable in the influence view. Fig. 7 shows an influence in the case study. The outcomes of the two state variables, in nodes connected by probabilistic view for the action Quinidine represented by the nodes named state-n and state-n+l, are the different states defined the model. The state attribute variables are represented by the nodes directly the state variable node state-n+l. While influence view, their incorporation probabilistic dependencies independence Given an action, conditional probabilities influences among the event variables and the state attribute variables. in the specifications of the facilities separate and independent them, given the decision context. they need not be explicitly ly is defined among indicates conditional represented influencing injuences 6.3.3. Modeling in the tree view Operating on the same set of event variables and probabilistic the tree view imposes a chronological relations. Conditional probabilities both the outcomes of the event variables and the probabilistic view for an action, asymmetric representing Fig. 8 shows a simplified version of a tree view for the action Quinidine showing only the simplified branches with the terminating in Sri+++ All the subtrees grouped by the brackets can be attached states and one subtree for each branch. triangles are assumed to lead directly in the case study, In the figure, all the to the DEAD state in to all the branches influences in the influence the can be defined directly on the tree branches dependences. order and explicit display 232 Z K hong /Artificial Intelligence 105 (1998) 2OS261 Fig. 7. Influence view for action Quinidine. Wartarin-OFF AFib W&ON TIA Warf-OK Fig. 8. Tree view for action Quinidine. the directly preceding group of subtrees in any order. Asymmetries in the tree are reflected through the subtrees that can only be attached to selected groups of preceding branches, and along the branches, e.g., P(Die-TE 1 No-Te) = 0. some of the zero conditional probabilities imposed ordering of the event variable nodes in the tree view may obscure to manipulate. DynaMoL also too complex the expansion The strictly the actual relations and render incorporates of a particular predecessor nodes filling the influence view. in the conditional a partial tree view that supports partial expansion and focused specification event variable node; the expansion in the influence view. In other words, a partial involves distribution table for the corresponding the chosen node and all its tree view facilitates event variable node in Z E Leong /Artificial Intelligence 105 (1998) 209-26I 233 Conditional distribution for ?Die-TE event variable: (distribution ?die-te/NoDrug) #<Distribution-Table #:G721 Type: DISCRETE Matrix: #:G721 "P(clr)" PEW-STR TRANS-STR TIA NO-TE DIE-TE PDIE-PERM-STR PDIE-TRANS-STR 0.00000 0.00000 NOT-DIE-TE (- 1 PDIE-PERWSTR) (- 1 PDIE-TRANS-STR) 1.00000 1.00000 Fig. 9. Asymmetric relations represented in an influence view. Perm-STR Die-TE PDIE-PERM-STR - Trans-STR TIA No-TE Not-Die-TE (- 1 PDIE-PERM-STR) Die-TE PDIE-TRANS-STR (- 1 PDIE-TRANS-STR) Not-Die-TE 1.0 Not-Die-TE 1.0 Fig. 10. A partial tree view for the event variable ?Die-TE for the action NoDrug. Consider the piece of knowledge: “a patient may have a chance of thromboembolization, which may be a stroke or a transient ischemic attack; if he has a stroke, it may be permanent or transient, and there is a chance that he will die from the stroke”. Fig. 9 shows the partial is embedded influence view that represents in the conditional distribution the above knowledge. The asymmetry table. involved from the tree structure. The tree structure reflects the logical flow of represented The same piece of knowledge can be expressed relevant event variables, as shown in Fig. 10. In the partial obvious knowledge more directly. Numerical or functional probabilities terminating on the terminating the partial tree view. branches. Only the probabilities in a partial tree view that expands tree view, the asymmetry the is can be specified on the in branches are relevant 234 I: Y: Leong /Art$cial Intelligence 105 (1998) 209-261 6.4. Numerical parameters assessment The numbers, value functions and probability distributions associated with the variables and relations are usually assessed after the structure of the model is in place. 6.4.1. Value function dejinitions v?‘(m) The value function is defined in terms of the associated transition value functions v:‘(m) for each possible transition from state i given action a. A transition value function v,ji”‘(m) determines i to state j over time duration m, conditional on action a. It has two components: the objective value achievable in a transition from state a yieZd rate y?)(l) entering when the transition independent of the absolute time t . and a bonus b!;‘. The yield rate indicates state i over time interval (I, I + 1); the bonus the value achievable at time 1 after is a one-time value achievable takes place from state i [35]. The value functions are assumed to be Formally v(“)(m) = C qg)( m, t)vt’(m) j a E A, i, j E S, l,m 3 0. = Cq(f’( j IJ m, t) 1 ([:I: y?)(l) ’ ] + b?) iJ j ; bleeding. and for all actions. for all states and transitions, they are associated only with transitions the bonuses are currently specified with respect to the transitions, In the case study, we adopt a special case for the value function: states, those for the absorbing short-term morbidities; the yield rates and the bonus are constant In other words, v!“‘(m) = vij (m) = myi + bij, for all a E A, i, j E S, m > 0. The yields are specified states being zero. Bonuses are negative for the individual in in this case, indicating which some adverse events may occur, e.g., the presence of stroke, cerebral hemorrhage, or gastro-intestinal instead In DynaMoL, influence diagrams and Markov cycle of their constituent the value influence diagrams, trees allow more detailed bonus specifications. the bonuses can be functions usually contain only the yield rates for fixed time intervals; links from the relevant chance nodes to the value nodes. incorporated by adding influence to be associated with Similarly, to an outcome of a chance node, indicating a positive the chance nodes. A toll is a binding life months or negative value associated with that outcome, e.g., -0.25 quality-adjusted for a stroke. Tolls can be added or subtracted as the branches of the tree structure are traversed. This feature, however, is feasible only with forward induction based solution algorithms. from the total expected value accumulated in Markov cycle trees allow bonuses event variables. Both dynamic the “toll” structures In dynamic 6.4.2. Transition functions andprobabilistic specifying Some guidelines for directly in$uences one-step to the transition view in DynaMoL, corresponding estimation of the one-step in the influence views and/or their constituent event variables transition functions the tree views, instead. functions transition are documented in an SMDP, in [45]. When direct the parameters can be specified of is difficult, in terms of the conditional distributions ZY Lung /Artificial Intelligence IO5 (1998) 209-261 235 For distribution the Markov case, the influence table or matrix C associated with an event variable in view, each entry C[x, y] in a conditional POJ I x, a, tl subject to c PIY I x, a, tl = 1 is: (1) (2) for an event variable y, y E Sz, is a column a combination of outcome events of all the probabilistic where x is a row index representing an outcome predecessors of y, a E A, and t E T. The entry, which is usually a function the probability of event y in the next decision stage, given that event x occurs and action a taken at the decision stage at time t. f(t) of time, indicates index representing Similarly, in the tree view, (1) and (2) hold for the conditional probability associated with a that lie to the left of, each branch or outcome y E QY of an event variable y, where x is an index representing combination and leading of outcome events of all the probabilistic to y in the tree structure, given a E A and t E T. predecessors For the semi-Markov in terms of the constituent case, there are two ways to assess the one-step event variables. Consider specifying transition functions the PMFs of one-step transition functions, qz’(r;l, t). The PMFs of one-step transition functions can be defined either in terms of the transition probabilities such that p,:“‘(t) and holding time PMFs h~~)(m, t), ,!a)(m lJ ’ t) = P(P)(t)hj”)(m ‘J lJ t)’ ’ ’ aEA, i,jES, tET, m30 (3) or the conditional such that transition probabilities p$‘(m, t) and waiting time PMFs wf)(m, t), qp)(m ‘J ’ t) = p!“‘(m (J t)wp’(m I ’ t)- ” a E A 1 i j E S, t E T m j> 0. t H > (4) The first approach, corresponding to (3), is to assess the probabilities of the transition times with respect to those transitions. transition probability, first, then decide on the holding the probability given in (1) which is a function In destinations f(t) of time, as an this case, we interpret eventual i.e., the probability of event y over those of other outcomes of y, given that event x occurs and action a is taken at the decision stage at time t. The collection of conditional distributions the eventual in the influence view will then constitute F’,‘?‘(t), where i, j E S, a E A, t E T for the underlying SMDP. transition probabilities In the We then estimate the transition destination of a patient, and clinical context, the time duration spent in the current state then depending on this destination, there is a probability of 0.3 that a patient who before making has undergone for such a patient to is 0.2 in the first month after surgery, 0.5 in the second month, develop and 0.3 in the third month. the transition. For instance, surgery A will develop complication B; the probability . first determines estimates mg time PMFs directly for the corresponding the ho1 d! this approach the complication transitions. The second approach, corresponding to (4), is to first decide on the waiting then assess the probabilities of transition destinations with respect to those waiting times, and times. 236 Z K Leong /Art@zl Intelligence IO5 (1998) 209-261 the waiting the probability given in (l), which is now a function time PMFs for each state s E S. We then In this case, we directly estimate f (m, t) of both duration and interpret time, as a conditional i.e., the probability of event y over those of other outcomes of y, given that event x occurs, action a is taken, and the waiting time is m at the decision stage at time t. The collection of conditional distributions transition probability, in the influence the conditional transition probabilities view will then constitute a E A, t E T, m 3 0 for the underlying SMDP In the clinical context, estimates and then depending on the this duration, determines the probability that a patient who has undergone month after surgery, 0.5 for two months and 0.3 for three months; probability the second month, and 0.3 in the third month. the time duration a patient spends in the current state before making a transition, the transition destination. For instance, surgery A will remain well is 0.2 for a if the patient is well, the for him to develop complication B is 0.6 in the first month after surgery, 0.5 in p$’ (m, t), where i, j E S, first this approach Validity, established DynaMoL ontology in future. adequacy, techniques when appropriate. Such techniques of the numeric relevance and parameters are determined could be incorporated by into the In the case study, the distributions derived from statistical of an event are usually reported in terms of yearly or monthly the following equation is used to convert a constant involved are estimated from the medical tables such as life tables. Since the relevant data for the occurrence rates instead of probabilities, rate r into a probability literature and where t is the unit of the rate with respect to the time unit of the decision horizon, e.g., to derive a monthly probability, rate, and t = 1 / 12 for a yearly rate. t = 1 for a monthly In the case study, the first dynamic decision problem is formulated as a Markov decision the second as a semi-Markov decision problem. Most of the numerical parameters in both problems. For the second problem, we assume that the relative rate of to a for warfarin varies with the duration m since entering a state s according problem, are identical bleeding function of the form A + BepCm (6) where A, B, C E I@. With respect distributions conditional We assume the waiting bleeding This means that a patient on warfarin sooner than later. the corresponding transition probabilities. relative for warfarin are functions of duration m with the same form as indicated to this varying of the event variables as the conditional time PMFs for the states affected by the varying risk of in (6). is more likely to make a transition out of that state risk, we assess 6.5. Constraint declaration Definitions of the model components constraints K. Many constraints domain-specific by DynaMoL. For instance, conditional on an action a E A, an absorbing a zero value function example is the relevance of a particular set of event variables and has no outgoing are inherent transitions in the definitions to a set of general or supported state i E S has to other states j E S. A similar in describing a state transition as described can be subjected l? Y; Leong /Artificial Intelligence 105 (1998) 209-261 237 conditional effects are explicitly encoded the dynamic decision model. on an action. These constraints are imposed during model construction; in the definitions of the structural or numerical parameters their in There are some constraints whose effects need to be explicitly inter- interpreted during is the disjunctive definition, or translations. A major example for specifying Bayesian networks. Canonical models are default strategies in a Bayesian network; level or inter-perspective “partial OR-gate”, for event combinations. The partial OR-gate is analogous model specifying are used when detailed elicit, or too complex to reduce interaction, or “noisy-OR-gate”. interactions to be determined precisely probabilities [55]. A common canonical that need to be assessed to a canonical for they to form used is disjunctive the conditional distributions of the chance variables the variables are unavailable, the number of conditional too numerous among is devised to facilitate conditional distribution specifications of the constraint has the general form: The partial OR-gate event variables. Formally, et Ve2V...Vek+e, where el,ez,..., constraint specification The constraining event is an outcome of x. ek are the constraining events and e, is read as: “if et or e2 or . . . or ek, then e,“. The constraint and interpretation of the conditional distribution is the consequent event. The is imposed on the table of an event variable x. events are outcomes of the probabilistic predecessors of x; the consequent With respect to the generalized noisy OR model developed by Srinivas [65], of which the in [55] is a special case, the partial OR-gate is different binary noisy OR-gate as described in the following manners. First, in the DynaMoL vocabulary, the former is a function on the event variable space E, while the latter is a function on the event space DE. Second, in the former, the combination constraint while in the latter, the combination subset of its predecessors, hence the name “partial”. is imposed on an event variable and all its predecessors, constraint can be imposed on an event variable and any In DynaMoL, a user can explicitly specify the partial OR-gate constraints in terms of a set of logical statements different event variables conditional distribution created. The numerical instance, we want to express: “a patient from a stroke or dies from cerebral hemorrhage or dies from gastro-intestinal This can be expressed as the partial OR-gate constraint: tables with the correct dimensions involved For is dead if he dies from natural causes or dies bleeding”. and labels are automatically then be specified accordingly. parameters can for the as shown above. The If Die or Die-TE or Die-CHM or Die-GIB then Dead-A event Dead-A where the consequent status, and the constraining status in the influence view as shown in Fig. 7. restriction and order Numerical is an outcome of the state attribute variable vital events are outcomes of the probabilistic predecessors of vital restriction expressed in terms of extra state attribute variables. Additional grammar constructs and their in on the actions are currently translators may be similarly to manage such constraints incorporated directly corresponding DynaMoL. 238 TZ Leong /Artificial Intelligence 105 (1998) 209-261 7. Model translation DynaMoL employs a set of translators extension. Structural language are automatically incremental perspective modeler can work with the most convenient of the modeling process. translated and numerical to support multiple perspective and in one so that the and effective perspective or view at any stage parameters into other perspectives, reasoning specified and updated 7.1. Translating transition view into SMDP and consistently When completely correspond directly in this case. Details of the model, however, usually cannot be easily specified directly the transition views; inter-perspective translations constructs in the transition views to the definitions of an SMDP Hence no special translators are needed in in other views into those of the transition views. are almost always involved the components in translating specified, Once translated into a transition view, reversed translation back to the influence view or information of the in the the structural and numerical into the transition functions tree view is not supported. This is because influence view or tree view is “compiled” or aggregated transition view. If the transition cannot be recovered with respect to the original functions are subsequently changed, the loss of information influence view or tree view. 7.2. Translating injbence view into transition view The mathematical probabilities The conditional influences transitions among the states. should be translated definitions of SMDP do not include event variables and influences. the corresponding characterizing in the event variables and functions into the one-step transition captured the The injuence translator view it is analogous probabilities; [62]. Given random variables x, y, and z, conditional to y means, variable e: for x E a,, algorithm is based on the expectation to the chance node reduction algorithm of conditional in influence diagrams expectation of z given x with respect y E J2,, z E 52,, where Q2, is the outcome space of the event to Fig. 4, the overall idea of the algorithm With reference variable nodes between a direct influence between state space, the conditional event the two state variable nodes, so that the final diagram contains only static action space and static table associated with the next-state variable on the two state variables. Assuming is to reduce the intermediate distribution Fig. 11. Final influence view after reduction ?: k: Leong /Art&ial Intelligence IO5 (1998) 209-261 239 the set of PMFs or CDFs for the one-step transition functions or their the right contains components, conditional on an action. the influence view translator In essence, identifies an event variable node to be reduced, updates the conditional distributions of other event variables affected by it, and removes it. The main algorithm is as follows: iteratively INFLUENCE-VIEW-TRANSLATOR (ID) 0 ID is a sequence of event-variable-nodes x t FIND-REMOVABLE-EVENT-VARIABLE while x (ID) do ID t ABSORB-EVENT-VARIABLE (ID, x) x t FIND-REMOVABLE-EVENT-VARIABLE (ID) return ID An event variable node A simple heuristic distribution distribution dimensions tables to be updated as small as possible. first. This heuristic is employed is removable only if it has a single probabilistic successor. to remove event variable nodes with smaller conditional the size of the conditional helps keep FIND-REMOVABLE-EVENT-VARIABLE (ID) 0 ID is a sequence of event-variable-nodes E-list t 0 for each x E ID unless x = state-variable or x = next-state-variable = 1 do if length[successors[x]] then E-list t E-list U {x) 0 Sort elements of E-list according and the increasing return first[E-list] size of (lone) successor. to the increasing number of predecessors nodes To remove an event variable node X, its lone successor y must inherit and the conditional distribution of y must be updated accordingly. Consequently, x will no longer be a successor to all its predecessors. its predecessors, ABSORB-EVENT-VARIABLE (ID, X) x is an event-variable-node. 0 Assume x has only one successor. first[successors[x]] 0 ID is a sequence of event-variable-nodes, D, t distribution-table[x] y t D, t distribution-table[y] predecessors[y] distribution[y] t new-distribution UPDATE-DISTRIBUTION-TABLE for each p E predecessors[x] () do successors[p] t successors[p] U {y}\x return ID\x t predecessors[x] U predecessors[y] (distribution-table[x], Dx, DY) 240 Z Y Leong /ArtiJicial Intelligence 105 (1998) 209-261 Conditional distribution table for b: "P(CIT) n A NOT-A Bl 0.70 0.30 B2 0.20 0.50 83 0.10 0.20 Conditional distribution table for c: "P(clr) Ir (A Bl) (NOT-A al) (A 82) (NOT-A BZ) (A B3) (NOT-A B3) C 0.70 0.60 0.45 0.10 0.20 0.30 NOT-C 0.30 0.40 0.55 0.90 0.80 0.70 Updated conditional distribution table for c, after b is removed: "P(clr)" A NOT-A C 0.60 0.29 NOT-C 0.40 0.71 Calculation methods: P(CIA) = (0.70)(0.70) + (0.45)(0.20) + (O.ZO)(O.lO) = 0.60 P(NOT-CIA) = 1 - P(CIA) = (0.30)(0.70) + (0.55)(0.20) + (0.80) (0.10) = 0.40 P(CINCl!-A) = (0.60) (0.30) + (0.10)(0.50) + (0.30) (0.20) = 0.29 P(NOT-CINOT-A) = 1 - P(ClNOT-A) = (0.40)(0.30) + (0.90) (0.50) + (0.70)(0.20) = 0.71 Fig. 12. An example to show conditional distribution table updating. Updating the conditional distribution table of the successor event variable begins with of conditioning row and column events with respect indices in the conditional events; each event in the sequence constituting of the conditioning to its new predecessors. distribution table an events from the appropriately. The to (7). Fig. 12 shows a simple is an outcome of a predecessor event variable. The conditioning to be removed must be filtered from each combination that the proper combination the default Recall are conjunctions index event variable conditional probability example of the calculations entries are then updated according involved. Assuming that each event variable average, the number of conditional distribution involved has m outcomes and n predecessors table entries to be updated is O(m”+‘). on 7.3. Translating tree view into transition view from the tree view into the transition view is again based on the expectation Translation of conditional probabilities to reduce the intermediate that the final diagram contains only direct probabilistic dependences between variable nodes as shown in Fig. 13. Assuming conditional idea is the two state variable nodes, so the two state static action space and static state space, the entries associated with the state variable outcomes on the right event variable nodes between in (7). With reference to Fig. 5, the main distribution as shown Z E: Leong /Artificial Intelligence 105 (1998) 209-261 241 Well d I 0.12 Discomfort 0.28 0 ‘n+l Sick 0.48 Sick Dead Fig. 13. Final tree view after reduction. the PMFs or CDFs for the one-step transition functions or their components, indicate conditional on an action. tree view The and calculating the corresponding translator updates entries conditional along the expected conditional to each of the next-state variable outcomes concerned. The path into the outcomes of the depth-first leading are simply propagated event variable nodes in a breadth-first manner. The final path probabilities by traversing the branches probabilities down-stream are then summed up to established of the next-state variable node. The main algorithm the conditional distributions distribution probabilities for the outcomes or states forward by multiplication is as follows: liX!e-VIEW-TRANSLATOR (TD) 0 TD is a sequence of event-variable-nodes for each x E TD do slist t successors[x] (x, slist) PROPAGATE 0 Assume SUM-PATHS olist t outcomes[state-variable] for each o E olist sums up the final path probabilities for each ns E next-state-variable[o] do SUM-PATHS (ns) return TD 242 Z Z Lang /Artificial Intelligence 105 (1998) 209-261 The propagation procedure simply multiplies the conditional probability of each outcome of an event variable into those of its successors. PROPAGATE (x, Elist) 0 x is an event variable node, Elist is a sequence of event-variable-nodes olist t outcomes[x] 0 olist is a sequence of ordered pairs ~1, prob>, where 1 is the label of an outcome, and prob is the conditional probability for each o E olist associated with the outcome, for each e E Elist do solist t outcomes[e] for each s E solist do prob[s] = prob[s]*prob[o] Assuming that each event variable average, the number of conditional distribution involved has m outcomes and n predecessors entries to be updated is again O(m”+‘). on 7.3.1. Translating between injuence view and tree view The influence view can be regarded as a special form of influence diagram, and the tree view a special form of decision tree. Both the influence view and the tree view do not contain any decision nodes; the values are associated with the outcomes of the special chance nodes, namely, the next-state variable nodes. Moreover, both the influence view and the tree view are always oriented with the state variable node as the source, and the next- the influence diagram state variable node as the sink. Many theoretical to the and the decision influence view and the tree view. tree, therefore, can be applied directly or in a simplified manner results concerning the representations The main idea in direct translation between of any two event variable nodes in the influence view and the tree view is shown in Fig. 14. The number of entries in in the the conditional distribution tree view can be generalized in Section 6.3.3, relations can be captured similarly with the proper zero-valued entries reflected asymmetric in the conditional in the influence view, and the proper omission of branches tables in the influence view and the number of branches event variables. As mentioned in the tree view. to non-binary distribution tables Influence view Tree view +gzy Y Not-Y 0.85 Fig. 14. Direct translation between an influence view and a tree view for an action. 7: K Leong /Art$cial Intelligence IO5 (1998) 209-261 243 In translating an influence view into a tree view, the event variable nodes in the influence view are first ordered from the state variable node, i.e., the source; the next-state variable node, i.e., the sink, is always at the end of the list. are then These nodes and their respective outcomes and associated conditional probabilities is as follows: expanded in terms of their maximal distances into the nodes and branches tree. The main algorithm in a connected INFLUENCE-VIEW-TO-TREE-VIEW-TRANSLATOR (ID) 0 ID is a sequence of event-variable-nodes ID t ORDER (ID) TDtIzr 0 Assume EXPAND properly attaches a node and all its outcomes and the corresponding in the tree for each e E ID conditional probabilities to each of the leaves as branches do EXPAND (e, TD) return TD two functions: The ordering procedure performs and maximum distances of the event variable nodes from the source node; second, it sorts the event variable nodes in increasing order, first with respect to their maximum distances and then, for those nodes of equal maximum distances, with respect to their minimum distances from the source. This heuristic ensures that all the predecessors of any event variable node are positioned to the left side of that node in the resulting first, it determines the minimum tree. ORDER (Elist) 0 Elist is a sequence of event-variable-nodes 0 dis[e] for an event variable node e is an ordered pair tmin, max>, where min is the minimum distance and max is the maximum distance from the state-variable node. for each e E Elist do dis[e] t for each e E Elist tO,O> unless e = state-variable do slist t for each s E slist successors[e] do if min[dis[s]] = 0 then min[dis[s]] = max[dis[e]] + 1 max[dis[s]] = MAX(max[dis[s]], max[dis[e]] + 1) else max[dis[s]] = MAX(max[dis[s]], max[dis[e]] + 1) 0 Assume DIS-SORT properly sorts the marked event-variable to their distances from the state-variable node. DIS-SORT return Elist (Elist) nodes according The complexity for the marking part of the ordering algorithm is the set of event variables and p is the set of probabilistic influences is 0( 1 E I+ 1 ly I), where E in the influence view, 244 ZK L.eong /Arhjicial Intelligence 105 (1998) 209-261 and that for the sorting part is 0( 1 E I2 I) (or 0( 1 E 1 log I E I) on average) using the quicksort algorithm. In translating a tree view into an influence view, the event variable nodes in the tree view the same event variable are at the same are first aligned so that all the nodes representing depth in the tree from the state variable node, i.e., the root. These nodes and their respective outcomes or branches and associated conditional probabilities into the nodes and tables in the influence view. The main algorithm are then extracted is as follows: TREE-VIEW-TO-INFLUENCE-VIEW-TRANSLATOR (TD) 0 TD is a sequence of event-variable-nodes TD t ALIGN (TD) ID t 0 0 Assume EXTRACT collects all the information pertaining event-variable the corresponding for each x E TD do EXTRACT conditional probabilities (x, ID) in the influence view. to a single at a particular depth in the tree view and updates the outcomes and return ID The aligning procedure simply assigns a depth number the tree view. The asymmetric cases are automatically of the nodes in the tree. The event variable nodes are then sorted according from the root node in the tree view. to all the event variable nodes in taken care of by the relative positions to their depths ALIGN (Elist) 0 Elist is a sequence of event-variable-nodes 0 depth[e] for an event variable node e is the distance from the root node for each e E Elist do depth[e] for each e E Eli& t 0 unless e = state-variable do slist t successors[e] for each s E slist do depth[s] = depth[e] + 1 properly sorts the marked event-variable nodes to their depths from the state-variable node. 0 Assume DEPTH-SORT according DEPTH-SORT return Elist (Elist) The complexity for the marking part of the aligning algorithm is 0( I E In), where E is the set of event variables and m is the average number of outcomes, and that for the sorting part is O() E 12”) (or O() E Jm log I E I”) on average) using the quicksort algorithm. The inherent exponential nature of the tree representation these algorithms. in the complexity of is reflected T. Z Leong /Arti$cial Intelligence 105 (1998) 209-261 245 The translations l Translating as described above are complicated by the following situations: an influence view into a tree view loses the information on conditional independence among the event variables. l Translating a tree view into an influence view loses the information on chronological ordering among the event variables. The current “memorize” specifications of conditional framework translators the relevant information in the DynaMoL framework in each view, and prompt technique and chronological interface heuristics to the user for the required for automated detection and conversion ordering will be incorporated employ into the if necessary. A general independence in the near future [78]. 7.4. Handling constraints in translations Most constraints have direct correspondence for instance, corresponds variable, in the SMDP. No explicit translator currently defined in the influence view. It is encoded as a special version of the update distribution algorithm described earlier. Each state time or decision stage for such constraints. The only explicit for the partial OR-gate constraint table translators in DynaMoL to the state-space at a particular in the SMDP representation. is a translator are devised In translating the influence view into the transition view, a new algorithm update the conditional distribution lone successor y, or both contain conditional distribution constraints. The algorithm simply propagates the successor of the event variable to be removed is used to entries when the event variable x to be removed, or its tables with the partial OR-gate into the disjunctive outcome labels properly in the influence-view-translator. In translating the influence view into the tree view, the disjunctive outcome table with the partial OR-gate constraint is properly a conditional distribution into the corresponding asymmetric branches The number of table entries or branches in the tree view. to be updated is of about the same order as l)n+l) where m is the average number of outcomes and n is the average before: O((m - number of predecessors of an event variable. For the influence view of the case study, the state variable and the next-state variable have 18 and 19 outcomes each, and the other event the average number of predecessors variables have an average number of 3 outcomes; for an event variable from an influence view, the number of is of the order of 8700; the addition of one more table entries predecessor would make it to the order of 26000! The smaller dimensions of the conditional distribution of the intermediate tables rendered by the partial OR-gate, however, drastically cuts down the sizes tables in practice. is 3. To reduce an event variable that need to be calculated conditional distribution in labels interpreted 7.5. Maintaining consistency Decision modeling in one view may render the constructs same time. Modifying view obsolete. There are two types of consistency management involves working with different graphical views at the in another in DynaMoL the information involved and numerical in DynaMoL: parameters add up to 1, the number in a l Zntra-view consistency ensures that the structural particular view are specified correctly, e.g., the probabilities 246 T k: Leong /Artificial Intelligence IO5 (1998) 209-261 l is in the interfaces. simple checking mechanisms of states displayed are in accordance with the state-space, etc. Such consistency maintained by incorporating Inter-view consistency ensures that updated information views when the translators are invoked. For unidirectional event variables and influences into transition transition target view is updated, e.g., new transition functions however, in the transition information of the translation origin may become obsolete when the target is modified, view. the influence directly e.g., updating A consistency invoked during a modeling session, and a flag is signalled functions, we have to make sure that the arcs with associated one-step this convention, view. Under a transition table can keep track of the translations is reflected in all the relevant translators, e.g., translating are displayed translating function after target information if the relevant e.g., updating parameters with direct correspondence are reflected inconvenience is modified. the model; throughout is that changes are reflected only when they are saved or confirmed For bi-directional translations, in different views, modifications minimized by requiring in the interfaces. 8. Model solution Once the dynamic decision model the model can be solved by any solution methods for SMDPs. A variety of solution methods are available of the dynamic decision problem concerned, we can choose the most efficient solution method available. The major solution methods in the current version of DynaMoL are based on value iteration. for SMDPs. Based on the characteristics into an SMDP, and translated is formulated incorporated 8.1. Value iteration A dynamic decision problem can be expressed as the dynamic programming Bellman optimality equation, of an SMDP. The main idea in solving the equation solving approach, working backward pruning sub-optimal branches along the way. equation, or is through “divide and conquer” from the final outcomes at the end of the decision horizon, stage via an optimizing at each decision the subproblems In the specialized case of an MDP, all the state transitions out of any state i E S occur at the unit time interval or duration of 1. For a decision horizon of n stages, with a discount factor 0 < j3 < 1, the optimal value achievable in any state, i E S, VT, given an initial value Vi (0) and current time t E T, is: Vi*@, B, t) = maxa v!“)(l)+BC~~‘(t)V;l(n--1,B,t+l) ( n>O, i,jES, UEA, teT i d ; 1 (8) where Pi?) (t) is the transition probability from state i to j, conditional on action a at time t, and VP’ (1) is the value achievable time. in state i , conditional on action a, for one unit of In the general case of an SMDP, the state transitions out of any state i E S may occur at the state. For a decision horizon of n stages, with a time durations after entering different T. Z Leong / Artijicial Intelligence 105 (I 998) 209-261 241 factor 0 < /? < 1, the dynamic programming the optimal in any state, i E S, Vi*, given an initial value Vi (0) and current time t E T, for calculating equation discount value achievable consists of two parts: l The first part indicates if the next transition out of state i occurs after duration n, where the values achievable beyond the decision horizon are ignored; and the expected value achievable l The second part indicates before duration state and the values achievable account. IZ, where the values achievable the expected value achievable if the next transition occurs in the for transitions out of the state i are both taken into in the state i for the duration The equation, which incorporates the first part above as the first addend and the second part as the second addend, is as follows [35]: =maxa C C qc)(m,t) C?+yf)(Z)+j?"Vj(O) n-1 [ l=O 1 j m=n+l 1 03 j m=l [ I=0 +y+&ym,t) ~~B1gl”)(z)+B~h:;)+iH”‘v~~(n-m.~.t+m) ; II n>O, i,jcS, atzA, tcT (9) where s$‘(m, is the transition value function such that t) is the one-step transition function and v/T’(m) = [cr=<’ yi(a)(l)] + bl,“’ v!“‘(m) = ~q,lia)olp(m) foralli,jES,aEA,tETandm>O. The value iteration solution method is to solve the optimality equation shown in (8) or (9). The solution to such an equation is an optimal policy jr* = {CL;;, CL:, /1;&;>. . . I j3, 0), the optimal expected value where & : S -+ A and t E T, that maximizes Viz,(N, or reward for an initial state over duration N, at time t = 0. Recall that n = N - I is the remaining duration for the decision horizon. For a decision horizon of duration N, the complexity of the value iteration algorithm for SMDP is 0( N2 . 1 A 1 . 1 S 1 2), since in each decision stage, we generally need to consider all time points t’ such that t 6 t’ < t + n. The complexity of the corresponding for . IA 1 . ]S12); in this case there is a one-to-one MDP is O(N the correspondence duration n and the current time t : n = N - t algorithm between All the default solution methods in existing dynamic decision modeling inference frameworks are techniques can also be employed for based on value iteration. While probabilistic solving dynamic directly influence diagrams to Markov value iteration. Cohort analysis and Monte Carlo simulation [63], the graph reduction algorithm [70] corresponds in Markov 248 Z E Leong /Art$cial Intelligence 105 (1998) 209-261 cycle complexity of these algorithms memorized optimal substructures The first clinical question trees, however, are based on conditional the is O(]S] . (IAl . ISl)N) since they do not make use of the in a forward manner; expectation inherent for the case study times. The solution produced by Markov value iteration are assessed to the four strategies considered; the numerical parameters holding corresponding achievable that the patient has a probability thromboembolism, results long-term decision horizon of 50 years or 600 months. that administering for all possible only warfarin in dynamic programming techniques. problem; is posed as a discrimination in accordance with an MDP, i.e., with constant is a set of two policies, the expected value stages. We assume has no history of of the decision horizon. The is the preferred strategy over a each policy includes starting states and all possible decision of 0.25 to be in atria1 fibrillation, and is not on warfarin at the beginning indicate initially are assessed The second clinical question for the case study is posed as an optimization problem; the in accordance with an SMDP The solution produced numerical parameters is an optimal policy for all possible starting states and all by semi-Markov value iteration possible decision stages. We adopt the same assumptions about the condition of the patient as mentioned. For a short-term decision horizon of 5 years or 60 months, the results indicate that administering months, is preferred. is the preferred strategy up to 8 months. After 8 in the same condition, not administering if the patient remains any drug initially only warfarin initially 8.2. Other methods reported Solution methods matrix solution, policy directly applicable stationary policies, constant discount in the MDPs and SMDPs literature such as fundamental iteration [34], adaptive aggregation [4], or linearprogramming are if certain assumptions or conditions are met. These conditions include factors, homogeneous transition functions, etc. As mentioned in Section 2.2.4, recent research for MDPs [1,7,17-19,22,23,29-31,43,67]. methods between solution optimality domains. Some of them, however, may required formats specific to each framework. and computation in DTP has also led to new solution the trade-off These methods address decision in special cost in complex and time-critical that the MDPs be formulated Besides the solution methods for MDPs and SMDPs, other solution methods that take advantage of the high-level DynaMoL ontology can also be employed. 8.3. Separating modeling and solution support In stunmary, by separating the modeling task supported by the decision grammar and task supported by the mathematical and the solution graphical presentation, a large collection of solution methods can be employed a new solution method does not involve any change itself; all solution methods Similarly, extending or adapting affect the solution methods already applicable; methods that make use of the additional constructs. reference only to the high-level modeling representation, in DynaMoL. Moreover, employing language of a model. do not these may, however, admit other solution representation the decision grammar and graphical presentation the mathematical T Z L.eong /Artijicial intelligence 105 (1998) 209-261 249 9. Model analysis Model analysis analysis techniques e.g., clarity to ensure “correctness” is performed on a dynamic decision model of the solution, Many sensitivity test [36], risk profiles, In DynaMoL, some event variables; of the model and robustness decision analysis, way analyses are directly applicable. done by removing or adding changing some of the transition and value functions methods. The influence view supports structural parameter manipulation numerical parameters by explicitly displaying all such information structural organization.The and numerical parameters. For instance, war-farm is ineligible in tornado diagrams, one- or multi- is usually analysis by the solution analysis by allowing direct of the at the same level as the specified structural from a state where transition view helps to detect improperly there should not be a transition of the event variables. The tree view supports careful examination parameter in the SMDP and invoking structural parameter analysis to one where warfarin is administered. numerical The quality of the model can be assessed in terms of its “requisiteness”, which can in turn if it is reflects the decision and clarity. A model is accurate that sufficiently and contains all the relevant in terms of its accuracy, conciseness be interpreted well-formed situation. A model is concise if it contains no redundant it contains “meaningful” meaningful, formulation but also indicate how easily and problem analysis answers. Therefore, the quality metrics in practice. information information that can be easily accessed by an inference process information. A model is clear if to produce should not only be theoretically the model can be “debugged” during problem include for determining Some relevant metrics decision, which compares for assessing model accuracy relevant chance events [58], the value of information, class analysis the “confidence” measure of the recommended of subjective probabilities with objective While most of these metrics are applicable implementations of the metrics, and hence types. For instance, equivalence dynamic renders such updating much more difficult. specific model types in mind, e.g., structural controllability diagrams the equivalence decision and the fidelity [50,76]. relative to any dynamic decision model type, actual their ease of use, vary with different model in updating is usually employed the structural complexity of the tree-based decision models some metrics are designed with in influence decision class analysis influence diagrams; and observability In addition, frequencies the model in of state attributes with either formal canonical forms or domain-specific in formulating and analyzing an SMDP is the combinatorially of the state space with each relevant of the state space description state attribute variable. One the is to explicitly manipulate increasing dimension way to ensure conciseness combinations heuristics. Another way is to develop quantities be of smaller dimension information effective guidelines solution for developing for problem The quality of the solution statistics, which would called suficient than the original state space, and yet summarize all the essential [4]. Currently, there are no in general. decision model and analysis such techniques is usually determined, however, to a dynamic knowledge, using standard statistical to domain-specific such as with respect one-way of SMDP facilitates analysis of the solution nature. Much insights can be gained about the solution analysis. The mathematical and two-way representation techniques sensitivity [ 111. The main difficulty 250 Z E Leong /At@cial Intelligence 105 (1998) 209-261 Graphical Interface Fig. 15. The DYNAMO system architecture. in value iteration. The accuracy of the SMDP can also be assessed the nature of the state space, action space, and other decision parameters. if absorbing states are present in a well-formed is in terms that can be proved about it, e.g., existence and convergence that facilitate such proofs are the certainty equivalence SMDP, convergence by examining For instance, guaranteed of the mathematical properties of optimal policies. Some methods principle, contraction mapping principle such techniques may be difficult to apply and to understand discrimination For the long-term the controllability problem and observability and [4]. Due to the advanced mathematical definitions, however, the monotonicity conditions, and in the case study, in practice. indicate that administering strategy for all reasonable optimization the preferred strategy shifts in a reasonable manner to no drug, depending on the desirable and the undesirable only warfarin but not Quinidine ranges of numerical parameters results demonstrate the sensitivity problem, the sensitivity results to the patient is the dominant involved. For the short-term that over the decision horizon, from initially administering warfarin effects of warfarin. 10. The DYNAMO system The system architecture of a prototype in Fig. 15. The system is implemented is shown graphics package indicate specification. The base-language the correspondence implements information [49]. In the figure, the blocks inflows. The graphical defines several solution methods for SMDPs. implementation of DynaMoL, called DYNAMO, in Common Lisp with the GARNET the arrows interactive model the model components. The translator contains the SMDP The solver indicate system components; user interface allows and rules among the model components 10.1. System implementation All in the preceding the components described of DynaMoL in the current version of DYNAMO. implemented dynamic decision ontology as described of transition view, influence view, and the corresponding version of the tree view, partial influence view and to the transition view. 2 The solution methods value tree view, and the corresponding for solving discrimination In particular, problems iteration and optimization in Section 4, the complete graphical presentation and a functioning translators, translators implemented from/to the include problems, policy sections have been the system supports the full ’ The current version of implemented algorithms and preliminary evaluation results for the tree views and their translators are documented in [78]. ?: Y. Lmng /Art$cial Intelligence 105 (1998) 209-261 251 iteration, adaptive aggregation, semi-Markov interfaces for the decision parameters and constraints, data visualizing analysis and linear programming3 set of practical cases. A sophisticated support tools are also being developed. Figs. 6 and 7 are actual screen snapshots from the DYNAMO for both the Markov and the editors and tools, and sensitivity tools including is a copy of the software on restricted system. The system to be commercialized; use is available upon request. licensing negotiations under release for non-commercial 10.2. System evaluation The DYNAMO system, and hence the DynaMoL framework, has been evaluated in several practical domains as follows: 10.2.1. Atria1 fibrillation management rate through reasoning of multiple As illustrated is demonstrated for the different The effectiveness is shown actions to specify the transition support of the transition value iterations. Numerical functions directly, the corresponding include both Markov and semi-Markov significant parameters, e.g., rate of embolism, in Section 6, we have conducted a comprehensive in Fig. 7, are created. include case study based on an in atria1 fibrillation management with the graphical actual clinical decision analysis consult views and the influence views. The dynamic and translation decision problems involve 4 different strategies, 20 states, 10 event variables, over time horizons of 600 months or 50 years and 60 months or 5 years. The solution methods employed sensitivity analyses are done on various clinically of cerebral hemorrhage, etc., to ensure robustness of the optimal policies derived. perspective the the 20 states in the transition views. modeling process. Modeling begins with specifying influence Since it is difficult In this case study, the influence views, one of which the same set of event variables with different views they can also be structurally different. Although not numerical parameters, but in general the event variables performed and the underlying numerical parameters can be facilitated by the partial tree views which support from the influence views into the corresponding tree views, one of which is shown in Fig. 8, order, if any, of the various event variables, as well as would illuminate in the specified. Visualizing the numerical parameters explicitly the structure of the model with the original tree views would also allow us to compare model influence views are transition views, a simplified version of which translated back specified are further verified by making is shown is sure that the transition being administered If discrepancies tables of the influence views, or the tree views and the partial tree views, are examined links are correctly defined, e.g., the states in which warfarin is ineligible. from those in which warfarin distribution should not be reachable the contribution relations, as shown in Fig. 10. Translations the specification of asymmetric in this exercise, deliberations to help rectify the errors. in Fig. 6, the numerical into the corresponding in the Markov cycle tree format. When on the relationships the fully specified the chronological the parameters are detected, parameters showing among 3 The implemented programming methods algorithms included and evaluation in the DYNAMO solver are documented in [53]. results for the policy iteration, adaptive aggregation, and linear 252 T.E Leong /A@icial Intelligence IO5 (1998) 209-261 As compared to the original solutions to expert judgment in terms of expressiveness correct answers according capabilities problem. For instance, DynaMoL bleeding dependent allows direct accounting for warfarin with respect to the duration of treatment; expression of such duration- relative factors are difficult in most existing decision modeling frameworks. for the clinical consult, our framework achieves and sensitivity analyses, and provides extra and efficiency for modeling and solving the decision risk of of varying 10.2.2. Colorectal cancerfollow-up We have also conducted a comprehensive case study on deciding the optimal follow-up surgery. The decision context in the Singapore General test and treatment i.e., spreading of the optimal course of diagnostic of cancer recurrence, metastasis, schedule for colorectal cancer patients who have undergone is based on a group of Dukes Stage 3 colorectal cancer patients Hospital. The objective is to determine for early detection and management the cancer, or both recurrence and metastasis. The dynamic decision model is constructed with the graphical and translation the tree views. In this case, the tree views and the partial the modeling process as described earlier. The chronological metastasis, and both recurrence and metastasis solution method employed that the implemented produced are reasonable as judged by clinical experts [78]. support of the transition views, the influence views, and facilitate order of cancer recurrence, in the tree views. The results show algorithms work correctly, and that the optimal policies is Markov value iteration. Preliminary is explicitly captured tree views directly translation evaluation 10.2.3. Dynamic decision problems in other domains Finally, we have also evaluated all the implemented test suite [53]. The test suite includes a benchmark problems space and the state space range problems are considered, with or without discounting. in general domains, e.g., the automobile problem solution methods in DYNAMO with examples and decision randomized [34]. The sizes of the action from 5 to 25. Both finite-horizon and infinite-horizon the correctness of the implemented the suitable problem characteristics linear programming it has solution in infinite- for such problems with large state space and/or action space, however, and adaptive state aggregation are preferred. On the other hand, for finite- i.e., the same for applying sensitivity the different analyses only stationary policies, algorithms; facilitates involving The exercise has demonstrated also illuminated methods. For instance, horizon problems; policy-iteration horizon problems, strategies in all the decision optimization problems with time-dependent suitable method in this case. solution methods to be applied stages, are not appropriate transition functions; value iteration for heterogeneous is the only 11. Discussion 11.1. Related work The DynaMoL framework dynamic decision making Section 2.2. is mainly motivated by the ideas of and the approaches in SMDPs, dynamic decision analysis, and DTP as described to in T:E Leong /ArtiJicial Intelligence 105 (1998) 209-261 253 Egar and Musen [24] have examined to specifying decision model variables graphical level abstraction grammar, however, has not been extended structure of influence for modeling diagrams; the grammar or higher-level and constraints. This grammar it captures prototypical language approach is based on the patterns at a high- in clinical decision problems. The trade-offs or dilemmas to handle dynamic decision models. Graphical representation of continuous-time by Berzuini et al. [5] and Dean et al. [21]. Similarly, models by Dagum et al. [14,15] captures the general however, frameworks, networks. These in Bayesian presentation of the relevant decision and probabilistic influence diagrams. semi-Markov processes has been explored recent work on dynamic network techniques focus only on single perspective variables as Bayesian networks or time-series analysis dynamic techniques integrating that automatically On developed data of the underlying SMDPs, but they only employ algorithms influence decision to the dynamic however, has to be explicitly done for each decision stage. decision models and SMDPs, Provan et al. [57,58] have from the for evaluating dynamic in each the overall structure corresponds of an SMDP. Tailoring of the parameters, influence diagram representation the state attribute variables as a Bayesian network; stage are represented In this framework, influence diagrams construct dynamic diagrams. involved Magni and Bellazzi [46] have recently adopted the influence view representation naMoL to develop the DT-Planner environment fluence view supports parsimonious for representing specification of an MDP in this interactive in Dy- and solving MDPs. The in- framework. 11.2. Future work and extension Besides addressing features and system capabilities for general practical applications. On the theoretical transparency the trade-off between model and solution efficiency, adapted, the that would allow the system to model and solve a larger class of that would make the side, we propose some research can be extended, side, we discuss of DynaMoL issues the language and developed language enhancement problems. On the practical system more effective and convenient to use. 11.2.1. Supporting language extension and adaptation Incremental language enhancement is supported in DynaMoL by introducing a set of new grammar constructs or presentation for them. formats or both, and devising a set of translators (a) Static versus dynamic spaces. The DynaMoL decision grammar can be extended action space. New productions for each state and action. The graphical presentation dynamic constructs state space and dynamic can be written to incorporate for the corresponding duration mostly unchanged; only the valid entities are displayed decisions stages. Solution methods are readily available SMDPs. to incorporate the valid convention time or remains for particular time points or for such general classes of (b) Automatic state augmentation. State augmentation for incorpo- rating extra information in SMDPs. Most of the declaratory and strategic constraints, for instance, can be represented as state attribute variables or additional dimensions is a basic technique 254 TZ Leong /Artijcial Intelligence 105 (1998) 209-261 to automatically indicates whether it is more convenient if a state attribute variable separately, and have a set of translators of the state space. Many constraints, however, affect only part of the original state space. For example, the patient has a stroke before, the constraint on the number of stroke only affects those states that in- to specify such constraints and their volve a stroke. Therefore, associated parameters in- the information. This involves working with an abstract state space, which corporate will eventually be translated into the full state space for solution and analysis. This approach would allow us to incrementally without directly dealing with the resulting This idea of modeling to the idea of execution or solution idea is adopted as aggregation methods in AI [41,42,61,66,68] methods Automatic state augmentation grammar translating may be needed to facilitate modeling at multiple is directly opposite in an abstract state space for efficiency. The latter in control theory [4], and as HTN planning in DynaMoL by adding a set of for and a set of translators and separately large and complex state space. and DTP [7,17,22]. can be achieved them to form a larger state space. Another in an abstract state space for transparency impose various constraints, levels of abstraction. set of reversed the constraints, for specifying translators constructs is important in a dynamic decision problem. For example, (c) Limited memory. In a semi-Markov process, there is a limited memory of the time since entry into a state. In some cases, memory about previous states or if a patient had to a second heart attack during into a semi-Markov or Markov duration actions a heart attack before, he would be more susceptible surgery. Such limited memory can be incorporated process by state augmentation. lead to an explosion of the state Repeated applications of state augmentation to keep space size. To avoid such explosion, we can relegate track of limited memory. Only forward induction based solution methods, however, In this approach, a new counter or binding called memory or history are applicable. to keep track of the relevant states that a process has visited. can be introduced to the model parameters are now Calculations conditional on the history accumulated A set of history constructs limited memory, and a set of translators specification are needed. The solution methods can then operate on both the SMDP and the history constructs. (d) Numerical in DynaMoL for the conditional parametric so far. can be introduced of the expected value with respect to keep track of the the solution methods types of numerical usually and ordering constraints. There are several event or state numerical specify constraints constraints: ordering number of events or states will lead to a specific consequence, constraints assumed dead if he has had three strokes. Action numerical number of times an action may be applied. Action ordering constraints an action must always or never follow another action. The first method is to augment decision solution methods are needed to incorporate the state space factors. The second method into a dynamic decision model to keep track of the number or the order of the is to introduce a set of counters and let the take care of the constraints. Again new constructs and translators To facilitate proper such constraints the constraints. e.g., a patient to incorporate translations, an and that a certain is the restrict specify that ?: Y: Lmng /Artijcial Intelligence 10.5 (1998) 209-261 25s the nature of the constraints external knowledge based system can be employed by examining domain specific constraints or canonical models can be introduced a similar manner described earlier. to choose the right translators and solution methods. Other general or in into DynaMoL to the definition of the partial-OR constraint on event combination (e) Presentation convention. In addition in DynaMoL, other useful presentation perspectives, of parametric by conventions evolution, and translators. can be incorporated to the three graphical presentation perspectives e.g., 2-D or 3-D visualization new presentation introducing 11.2.2. Supporting automated dynamic decision making The DynaMoL decision making and analysis of the relevant problems. that effectively framework can also serve as the anchor of a new paradigm of dynamic solution integrates automatic and interactive formulation, (a) Automatic derivation of numerical parameters. One of the most daunting task in the issues and the processing the numerical parameters is to estimate and specify assumptions, such derivations. to support decision modeling; errors dynamic decision modeling In general, given a state-space of size 1 S 1, an action-space of size 1 A I, and a involved. to be assessed decision horizon of duration n, the number of probabilistic parameters from expert physicians may is of the order of 0( IA 11 S12n). Subjective assessments in some cases. When the decision situations are complex or the decision be adequate is limited are large, however, the practicality of the modeling approach dimensions by the lack of realistic estimations. On the other hand, given a large set of data, objective probabilities may not be easily calculated the recording the measurement formats, associated with the data may complicate We wish to investigate transition feasibility of such a task for supporting dynamic decision modeling. the limiting illuminate learned will contribute decision modeler and the database builder. This will in turn encourage and advancement of the techniques and facilities provided by both fields. By incorporating mented a parameter butions in DynaMoL [93. We have reported some encouraging sults from experimenting with large medical databases dependent diabetes mellitus surgery patients learning methods, we have imple- construct probability distri- in the influence views and the transition views of a dynamic decision model insights and re- insulin- [77] and the follow-up management of colorectal cancer of one-step into the It will also lessons the dynamic integration from databases. This exercise will provide preliminary in the management constraints toward bridging a set of statistical and Bayesian in available databases. The the expectation gap between in automatic construction system that automatically functions involved inherent learning insights [9,10]. (b) Supporting knowledge based model construction. Knowledge-based sys- tems employing knowledge-based model construction (KBMC), e.g., ALTERID [8], that the decision FRAIL [28], SUDO-PLANNER [72], and DYNASTY [56], advocate models for different problems should be constructed on demand from a knowledge base [73]. Currently, each KBMC system synthesizes only one type of decision mod- con- els, e.g., influence diagrams. In dynamic decision models, the time-dependent decision 256 ZZ hong /Artificial Intelligence IO5 (1998) 209-261 project translated substructures if not impossible, into numbers, equations, or complicated into specific decision models; subsequent from these models. [71], we explore how the high straints are usually hardwired quite difficult, level decision ontology In an on-going can facilitate KBMC. DynaMoL provides an expressive and explicit in DynaMoL the knowledge language base representation from being restricted by the graphical structure of the target model. The resulting models will also support more detailed analysis and more efficient solution methods as argued before. for formulating dynamic decision problems. This relieves retrieval of the constraints and organization is (c) Automated knowledge acquisition tasks is essential sources, their imprecision, from multiple knowledge and possible disagreements in biomedical knowledge and electronic should address such sources. Automating for effective dynamic decision making. delivery, information issues as the wide variety of among various model formulation With rapid advances for model building support available information the different sources. In another on-going project, we investigate how to acquire and integrate information from different sources such as domain experts, knowledge bases, electronic medical and test and treatment protocols on the Internet. The records, on-line from different sources major research for model formulation, to support model model formulation incompleteness experts. among groups of experts, including dealing with issues such as among the the integrated and facilitating and organizing analysis, issues include selecting of knowledge, possible and disagreements and structural inconsistency, collaborative representing information information refinement registries, relevant 12. Conclusions We conclude this paper by summarizing the achievements and limitations of this work. 12.1. A unifying view The analysis of three major approaches their similarities and differences, to dynamic decision making under uncertainty as well as their strengths and weaknesses. the representational task definition the same role in dynamic decision making under uncertainty and inferential for dynamic decision making under uncertainty, we have that SMDPs support involved. We propose as first-order representation. Recently, Dean [ 161 (FOPC) in deterministic knowledge come up with the same idea to regard MDPs as the basis for DTP We illustrated the motivation for our proposal by devising a new methodology highlighted Based on a uniform explicated be regarded predicate calculus has independently have further based on the idea. 12.2. A general paradigm Building methodology the common on a new general for dynamic decision making under uncertainty. We propose a novel language basis of SMDPs, we have introduced 7: Z Leong /Artijcial Intelligence 105 (1998) 20%261 251 that integrates the desirable design new paradigm of multiple perspective perspective languages. We have also established methods ontology. This is in contrast features of current techniques. By introducing a the mold of single in all existing graphical dynamic decision modeling the language this design breaks to systematically in most existing techniques. to the fixed vocabularies reasoning, supported reasoning extend than that in influence Model specification in DynaMoL is in terms of a higher-level language languages presentation frameworks. the declaratory structure of the model; and strategic constraints In frameworks such as dynamic existing dynamic decision modeling diagrams or Markov cycle trees, the model parameters need to be explicitly are explicitly detail. In particular, into the graphical are explicitly encoded for each time slice or period considered. The dynamic decision grammar in DynaMoL, on the other hand, supports abstract statements about the decision situation, related to each other. These e.g., statements about how the events and states are logically abstract statements to the macro constructs languages. By focusing on the decision problem ontology components, DynaMoL provides a more concise and yet more transparent platform supporting model construction. in conventional instead of the decision model for specified incorporated and temporal parameters the probabilistic are analogous programming in The advantages of the graphical nature of existing dynamic decision modeling and extended are preserved convention, most model components different presentation perspectives be visualized. The ease and clarity. Theoretically, SMDPs can approximate most stochastic processes by state augmentation can potentially to the visualization further contribute in DynaMoL. By extending and constraints the graphical or other mechanisms. The resulting manipulation for more general stochastic models. By distinguishing underlying mathematical model, DynaMoL preserves model structure, while at the same time admits a spectrum of solution methods. for direct or visualization. On the other hand, efficient solution methods may not exist the specification grammar and the the clarity and expressiveness of the state space, however, may be too complex While illuminate the different perspectives information may occur when the information Unless all the perspective may be difficult. Therefore, an overhead Moreover, in translation the SMDP framework has some inherent time. information is kept around, there is an extra burden of information the model in different ways, loss of is stored in a normal form, e.g., as SMDPs. later retrieval of the information limitations. Explosion of the state storage. There is also space size seems unavoidable when we introduce more problem attributes or constraints. Choosing an appropriate a daunting task. We have proposed some ideas on how such issues can be addressed. solution method and adapting it to handle the constraints can be 12.3. A prototype system To evaluate the feasibility and effectiveness of the new paradigm, we have developed a prototype system that can handle a general class of dynamic decision problems. We have conducted implementation. gave us confidence problems results produced that the methodology works well for a large class of dynamic decision in practical domains. Besides providing superior modeling support and a variety some detailed case studies to evaluate the DynaMoL design and the DYNAMO involved and the solution The modeling experiences 258 ZE hong /Artificial Intelligence 105 (1998) 209-261 the performance of solution methods, is at least on par with existing programs, The system is being used to model various real-life decision problems in clinical and pharmaceutical in other domains are also being planned. decision analysis. Applications of the DYNAMO system We are interested in putting documented these lessons have illuminated the experiences of performing the system into practical use. Towards this end, we have some detailed case studies in complex domains; some relevant research issues and the required support tools. Acknowledgements in part by the National This paper reports work done partly at the MIT Laboratory for Computer Science, Institutes of is supported Cambridge, MA, USA. This research Health Grant No. 5 ROl LM04.493 from the National Library of Medicine, USA. It is from the National Science also supported by a Strategic Research Grant No. RP960351 and Technology Board and the Ministry of Education, Singapore. like to thank the case study in Drs. Charles E. Ellis and Stephen G. Pauker for guidance I would also like to thank Peter Szolovits, Alvin Drake, atria1 fibrillation management. for useful discussions David Harmanec, Eric Horvits, Cungen Cao and Jianye Zheng and comments. version of the tree views a functioning into the DYNAMO architecture; Mun Cheong Ng has analyzed and and their translators in the system; David Harmanec, Suman Sundaresh implemented for non commercial use. and Jianye Zheng have worked on the release version of DYNAMO Jianye Zheng has incorporated several solution methods in conducting I would References [l] A.G. Barto, S.J. Bradtke, S.P. Singh, Learning to act using real-time dynamic programming, Artificial Intelligence 72 (l-2) (1995) 81-138. [2] J.R. Beck and S.G. Pauker, The Markov process in medical prognosis, Medical Decision Making 3 (1983) 419458. [3] R.A. Bellman, Dynamic Programming, [4] D.P. Bertsekas, Dynamic Programming: Deterministic Princeton University Press, Princeton, NJ, 1957. and Stochastic Models, Prentice-Hall, Englewood Cliffs, NJ, 1987. [5] C. Berzuini, R. Bellazzi, S. Quaglini, Temporal reasoning with probabilities, on Uncertainty 14-21. in Artificial Intelligence, Association for Uncertainty in: Proceedings 5th Workshop 1989, pp. Intelligence, in Artificial [6] C. Boutilier, RI. Brafman, C. Geib, Prioritized goal decomposition of Markov decision processes: a synthesis of classical and decision Intelligence on Artificial (IJCAI-97), Nagoya, Japan, 1997. theoretic planning, in: Proceedings 15th International toward Joint Conference [7] C. Boutilier, R. Dearden, M. Goldszmidt, Exploiting International 1111. Joint Conference on Artificial Intelligence structure in policy construction, 14th (IJCAI-95), Montreal, Quebec, 1995, pp. 1104- in: Proceedings (81 J.S. Breese, Construction of belief and decision networks, Computational [9] C. Cao, T.Y. Leong, Learning probabilities conditional for influence views, Intelligence 8 (1992) 624-647. in: Working Notes IJCAI Workshop on Intelligent Data Analysis (IDAMAP-97), [lo] C. Cao, T.Y. Leong, A. Peng Kiong Leong, F. Choen Seow, Dynamic decision analysis in Medicine and Pharmacology 1997, pp. 11-19. in medicine: a data- driven approach, Intemat. J. Medical Informatics, to appear. Z I’. Leong /Art&id Intelligence 105 (1998) 209-261 259 [l l] B.Y. Chan, R.D. Shachter, Structural controllability and observability in influence diagrams, M.P. Wellman, B.D. D’Ambrosio, P. Smets (Eds.), Proccedmgs 8th Conference on Uncertainty Intelligence, Morgan Kaufmann, San Mateo, CA, 1992, pp. 25-32. in: D. Dubois, in Artificial [12] D. Chapman, Planning [ 131 K. Currie, A. Tate, O-Plan: [ 141 P. Dagum, A. Galper, Forecasting (Eds.), Proceedings A. Mamdami Kaufmann, San Mateo, CA, 1993, pp. 64-71. for conjunctive goals, Artificial Intelligence 32 (1987) 333-377. the open planning architecture, Artificial Intelligence 5 1 (1) (1991) 49-86. sleep apnea with dynamic 9th Conference on Uncertainty network models, in Artificial in: D. Heckerman, Intelligence, Morgan [15J P. Dagum, A. Galper, E. Horvitz, Dynamic network models for forecasting, B.D. D’Ambrosio, P. Smets (Eds.), Proceedings Morgan Kaufmann, San Mateo, CA, 1992, pp. 41-48. 8th Conference on Uncertainty in: D. Dubois, M.P. Wellman, Intelligence. in Artificial [ 161 T. Dean, Decision-theoretic Institute on Probability and Artificial Intelligence, Corvalis, OR, 1994. planning and Markov decision processes, Tutorial presented at the Summer [17] T. Dean, R. Givan, S. Leach, Model reduction in: Proceedings for Markov decision processes, Morgan Kaufmann, San Mateo, CA, 1997, pp. 124-131. techniques for computing approximately 13th Conference on Uncertainty in Artificial optimal solutions Intelligence, [lS] T. Dean, L.P. Kaelbling, J. Kirman, A. Nicholson, Deliberation scheduling for time-critical sequential decision making, Artificial in: D. Heckerman, A. Mamdami (Eds.), Proceedings 9th Conference on Uncertainty m Intelligence, Morgan Kaufmann, San Mateo, CA, 1993, pp. 309316. [19] T. Dean, L.P. Kaelbling, J. Kirman, A. Nicholson, Planning with deadlines Proceedings pp. 574-579. 1 lth National Conference on Artificial Intelligence in: (AAAI-93). Washington, DC, 1993, in stochastic domains, [20] T. Dean, S. Kambhampati, Planning and scheduling, in: A.B. Tucker (Ed.), The Computer Science and Engineering Handbook, CRC Press, Rockville, MD, 1997, pp. 614-636. [21] T. Dean, JKriman, K. Kanazawa, Probabilistic network representations processes for applications Planning Systems, 1992. in planning and control, in: Proceedings of continuous-time 1st International Conference stochastic on AI [22] R. Dearden, C. Boutilier, Abstraction and approximate decision-theoretic planning. Artificial Intelligence 89 (l-2) (1997) 219-283. [23] M. Drummond, J. Bresina, Ayntime synthetic projection: maximizing the probability of goal satisfaction, in: Proceedings 8th National Conference on Artificial Intelligence (AAAI-90) Boston, MA, 1990, 1388144. [24] J.W. Egar, M.A. Musen, Graph-grammar assistance for automated generation of influence diagrams, in: D. Heckerman, A. Mamdami Morgan Kaufmann, San Mateo, CA, 1993, pp. 235-242. (Eds.), Proceedings 9th Conference on Uncertainty in Artificial Intelligence, [25] K. Erol. J. Hendler, D.S. Nau, HTN planning: complexity and expressitivity, in: Proceedings 12th National Conference on Artificial Intelligence (AAAI-94), Seattle, WA, 1994. [26] R.E. Fikes, N.J. Nilsson, STRIPS: a new approach to the application of theorem proving to problem solving, Artificial Intelligence 2 (1971) 189-208. 1271 M.R. Genesereth, N.J. Nilsson, Logical Foundations Mateo, CA, 1987. of Artificial Intelligence, Morgan Kaufmann, San [28] R.P. Goldman, E. Chamiak, Dynamic construction of belief networks, in: Proceedings 6th Conference on Uncertainty in Artificial Intelligence, 1990, pp. 9&97. 129) V. Ha, P. Haddawy, Theoretical Conference on Uncertainty 298. foundations of abstraction-baaed 12th probabilistic planning, Intelligence, Morgan Kaufmann, San Mateo, CA, 1996, pp. 29 I- in: Proceedings in Artificial [30] P. Haddawy, A.H. Doan, R. Goodwin, Efficient decision-theoretic planning: analysis, Intelligence, 1995, pp. 229-236. in: P. Besnard, S. Hanks (Eds.), Proceedings 11th Conference [3 11 M. Hauskrecht, N. Meuleau, C. Boutilier, L.P. Kaelbling, T. Dean, Hierarchical techniques on Uncertainty and empirical in Artificial 14th Conference on Uncertainty solution of markov decision Intelligence, in Artificial processes using macro-actions, Morgan Kaufmann, San Mateo, CA, 1998. in: Proceedings [32] G.B. Hazen, Stochastic trees: a new technique for temporal medical decision modeling, Medical Decision Making 12 (1992) 163-178. 260 ZK Leong /ArtiJicial Intelligence IO5 (1998) 209-261 [33] J.P. Hollenberg, Markov cycle trees: a new representation for complex Markov processes, Medical Decision Making 4 (4) (1984). Abstract [34] R.A. Howard, Dynamic Programming [35] R.A. Howard, Dynamic Probabilistic Systems, Vol. 1 & 2, Wiley, New York, 1971. [36] R.A. Howard, Decision analysis: practice and promise, Management Science 34 (1988) 679695. [37] R.A. Howard, J.E. Matheson, from the 6th Annual Meeting of the Society for Medical Decision Making. and Markov Processes, MIT Press, Cambridge, MA, 1960. in: R.A. Howard, J.E. Matheson Influence diagrams, (Eds.), The Principles and Applications of Decision Analysis, Vol. 2, Strategic Decisions Group, Menlo Park, CA, 1984, pp. 719-762. [38] J. Janssen (Ed.), Semi-Markov Models: Theory and Applications, Plenum Press, New York, 1986. [39] W.S. Jewell, Markov renewal programming: finite return models II. infinite return models I. formulations, example, Operations Research 11 (1963) 938-971. [40] J.P. Kassirer, A.J. Moskowitz, Medicine 106 (1987) 275-291. J. Lau, S.G. Pauker, Decision analysis: a progress report, Annals of Internal [41] C.A. Knoblock, Search reduction on Artificial Intelligence in hierarchical problem solving, (AAAI-91), Anaheim, CA, 199 1, pp. 68669 1. in: Proceedings 9th National Conference [42] R.E. Korf, Planning as search: a quantitative approach, Artificial Intelligence 33 (1987) 65-88. [43] N. Kushmerick, Proceedings 1078. 12th National Conference on Artificial S. Hanks, D. Weld, An algorithm in: (AAAI-94) Seattle, WA, 1994, pp. 1073- least-commitment for probabilistic Intelligence planning, [44] T.Y. Leong, Dynamic decision modeling in medicine: a critique of existing formalisms, in: Proceedings 17th Annual Symposium on Computer Applications in Medical Care, IEEE, November 1993, pp. 478484. [45] T.Y. Leong, An integrated approach to dynamic decision making under uncertainty, Technical Report TR 63 1, MIT Laboratory for Computer Science, Cambridge, MA, 1994. [46] P. Magni, R. Bellazzi, DT-Planner: an environment for managing dynamic decision problems, Computer Methods and Programs in Biomedicine 54 (1997) 183-200. [47] D. McAllester, D. Roseblitt, Systematic nonlinear planning, in: Proceedings 9th National Conference on Artificial [48] D. McDermott, Intelligence (AAAI-91), Anaheim, CA, 1991, pp. 634-639. J. Hendler, Planning: What it is, What it could be, An introduction to the Special Issue on Planning and Scheduling, Artificial Intelligence 76 (l-2) (1995) 1-16. [49] B.A. Myers, D.A. Giuse, B. Vander Zanden Dannenberg, D.S. Kosbie, E. Pervin, A. Mickish, P. Marchal, IEEE Computer 23 (11) for graphical, highly-interactive user interfaces, support Garnet: comprehensive (1990) 71-85. [50] R.E. Neapolitan, Computing in a medical decision obtained from an influence diagram, Artificial [51] A. Newell, Intelligence the confidence in Medicine 5 (1993) 341-363. J.C. Shaw, H.A. Simon, Report on a general problem-solving program, in: Proceedings International Conference on Information Processing, UNESCO, 1960, pp. 256264. [52] A. Newell, H.A. Simon, Human Problem Solving, Prentice-Hall, Englewood Cliffs, NJ, 1972. [53] MC. Ng, A solution suite for a dynamic decision modeling system, National University of Singapore, Department of Information Systems and Computer Science, Honours Year Project Report, 1997. [54] S.G. Pauker, J.P. Kassirer, Medical progress: decision analysis, New England Journal of Medicine 316 (1987) 250-258. [55] J. Pearl, Probabilistic Reasoning Kaufmann, San Mateo, CA, 1988. in Intelhgent Systems: Networks of Plausible Inference, Morgan [56] G.M. Provan, Modeling Report MIS-CS-92-69, Philadelphia, PA, 1992. the evolution of acute abdominal pain using temporal University of Pennsylvania, Department of Computer influence diagrams, Technical and Information Science, [57] G.M. Provan, J.R. Clarke, Dynamic network construction and updating techniques for the diagnosis of acute abdominal pain, lEEE Trans. Pattern Analysis and Machine Intelligence 15 (3) (1993) 299-307. [58] G.M. Provan, D. Poole, A utility-based analysis of consistency-based diagnosis, in: J. Allen, R. Fikes, E. Sandewall Conference (Eds.), Principles of Knowledge Representation and Reasoning: Proceedings 2nd International (KR-91), Morgan Kaufmann, San Mateo, CA, 1991, pp. 461472. [59] M.L. Puterman, Markov decision processes, in: D.P. Heyman, M.J. Sobel (Eds.), Handbooks in Operations Research and Management Science, Vol. 2, Elsevier, North-Holland, Amsterdam, 1990, pp. 331434. [60] H. Raiffa, Decision Analysis: Introductory Lectures on Choices Under Uncertainty, Addison-Wesley, Reading, MA, 1968. T. I: Leong /Artijicial Intelligence 105 (1998) 209-261 261 [61] E.D. Sacerdoti, Planning [62] R.D. Shachter, Evaluating [63] R.D. Shachter, M.A. Peot, Decision making using probabilistic in a hierarchy of abstraction spaces, Artificial influence diagrams, Operations Research 34 (1986) 871-882. inference methods, Intelligence 5 (1974) 115-135. M.P. Wellman, B.D. D’Ambrosio, P. Smets (Eds.), Proceedings 8th Conference on Uncertainty Intelligence, Morgan Kaufmann, San Mateo, CA, 1992, pp. 276283. in: D. Dubois, in Artificial [64] L.S. Shapley, Stochastic games, Proceedings National Academy of Science 39 (1953) 1095-I 100. [65] S. Srinivas, Generalizing to n-ary variables, Technical Memorandum the noisy or model 79, Rockwell International Science Center, Palo Alto Laboratory, Palo Alto, CA, April 1992. [66] M. Stefik, Planning with constraints [67] J. Tash, S. Russell, Control strategies Intelligence 1681 A. Tate, Generating Artificial (MOLGEN: Part l), Artificial Intelligence 16 (1981) 11 l-139. for a stochastic planner, in: Proceedings 12th National Conference on (AAAI-94), Seattle, WA, 1994, pp. 1079-1085. project networks, in: Proceedings 5th International Joint Conference on Artificial Intelligence (IJCAI-77), Cambridge, MA, 1977, pp. 888-893. [69] A. Tate, J. Hendler, M. Drummond, A review of ai planning techniques, in: J. Allen, J. Hendler, A. Tate, (Eds.), Readings in Planning, Morgan Kaufmann, San Mateo, CA, 1990, pp. 2&49. [70] J.A. Tatman, R.D. Shachter, Dynamic programming and influence diagrams, IEEE Trans. Systems Man Cybcmet. 20 (2) (1990) 365-379. [71] C. Wang, Knowledge-based formulation of dynamic decision models in medicine, M.Sc. Thesis, National University of Singapore, Department of Information Systems and Computer Science. [72] M.P. Wellman, Formulation of Tradeoffs in Planning under Uncertainty, Pitman, London/Morgan Kaufmann, San Mateo, CA, 1990. [73] M.P. Wellman, J.S. Breese, R.P. Goldman, From knowledge bases to decision models, The Knowledge Engineering Review 7 (1) (1992) 35-53. [74] M.P. Wellman, J. Doyle, Modular utility representation for decision-theoretic planning, in: Proceedings 1st International Conference on Al Planning Systems, 1992. [75] D.E. Wilkins, Practical Planning: Extending the Classical Planning Paradigm, Morgan Kaufmann, San Mateo, CA, 1988. [76] K.E. Willard, G.C. Critchfield, Probabilistic analysis of decision trees using symbolic algebra, Medical Decision Making 6 (1986). [77] S.S. Yeh, T.Y. Leong, Automatic generation of transition probabilities in dynamic decision modeling: a case study, in: Proceedings AAAl Spring Symposium on Artificial 1781 J. Zheng, Consistency management in multiple-perspective Intelligence in Medicine, 1994. dynamic decision modeling, M.Sc. Thesis, National University of Singapore, Department of Information Systems and Computer Science. 