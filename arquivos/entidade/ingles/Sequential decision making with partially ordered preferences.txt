Artificial Intelligence 175 (2011) 1346–1365Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintSequential decision making with partially ordered preferences ✩Daniel Kikuti, Fabio Gagliardi Cozman∗, Ricardo Shirota FilhoEscola Politécnica, Universidade de São Paulo, Av. Prof. Mello Moraes, 2231 São Paulo, SP, Brazila r t i c l ei n f oa b s t r a c tThis paper presents new insights and novel algorithms for strategy selection in sequentialdecision making with partially ordered preferences;is, where some strategiesmay be incomparable with respect to expected utility. We assume that incomparabilityamongst strategies is caused by indeterminacy/imprecision in probability values. Weinvestigate six criteria for consequentialist strategy selection: Γ -Maximin, Γ -Maximax,Γ -Maximix, Interval Dominance, Maximality and E-admissibility. We focus on the populardecision tree and influence diagram representations. Algorithms resort to linear/multilinearprogramming; we describe implementation and experiments.that© 2010 Elsevier B.V. All rights reserved.Article history:Received 28 February 2009Received in revised form 11 August 2010Accepted 11 August 2010Available online 2 December 2010Keywords:Sequential decision making underuncertaintyPartially ordered preferencesSets of probability measuresCriteria of choiceConsequentialist and resolute normsLinear and multilinear programming1. IntroductionIt is often possible, in a decision problem, to express preferences that are completely ordered; that is, for every twoalternatives, the decision maker either prefers one to the other, or is indifferent between them. In fact, expected utilitytheory is based on the assumption that revealed preferences are completely ordered. However, preferences are often partiallyordered; examples can be found in the theory of CP-nets and the theory of nondeterministic planning, as briefly discussedin Section 2. When preferences are partially ordered, two alternatives may be incomparable and incomparability may fail tobe transitive.In this paper we focus on preferences that can be represented by a single utility function and a set of probabilitymeasures. Whenever there are incomplete or partial beliefs, or disagreements amongst experts concerning chances, onemay fail to assign a precise probability value to every event, thus producing a partial order with respect to expected utility[3,46,73]. This is the situation we wish to focus on. Section 2 contains the necessary background on these topics.The literature describes many criteria of choice when preferences are partially ordered [71]. These criteria are coveredin Section 3 and can be roughly divided into two groups: (1) criteria that enforce a complete ordering amongst choices(Γ -Maximin, Γ -Maximax and Γ -Maximix); and (2) criteria that select a set of incomparable actions (Interval Dominance,Maximality and E-admissibility). Practical approaches to decision making with sets of probabilities have been mainly limitedto the first category; however, recent discussions [60] have highlighted theoretical and behavioral problems when usingthis group of criteria, and the second group of criteria has been advocated as a more adequate approach. Nevertheless,incomparability comes at a cost, and very little has been observed in the literature in terms of algorithmic progress, mainlydue to computational complexity and inability to deal with incomparable choices.✩This work has been financially supported by FAPESP, grants 2003/11165-9, 2004/09568-0, 2005/58090-9, 2008/03995-5.* Corresponding author.E-mail addresses: danielkikuti@yahoo.com.br (D. Kikuti), fgcozman@usp.br (F.G. Cozman), ricardo.shirota@poli.usp.br (R.S. Filho).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.11.017D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–13651347There are also distinct behavioral norms when it comes to sequential decision making with partially ordered preferences;that is, whenever a sequence of decisions must be made. For instance, a decision maker may be resolute in that she commitsherself to a complete strategy once and for all, or consequentialist in that she allows herself to change the current strategyin case another one is appropriate in view of the future possible choices.The interplay between criteria of choice, behavioral norms, and models such as decision trees and influence diagramshas not been explored in the literature; this paper aims at filling this gap to some extent. There are indeed insights to belearned from an organized discussion of criteria of choice and behavioral norms; for instance, we discuss in Section 5 thefact that the standard LIMID model clashes with a consequentialist stance.Another, more substantial, contribution of the paper is the development of algorithms for consequentialist sequentialdecision making expressed through decision trees [56] and influence diagrams [34]. Algorithms for decision making underΓ -Maximin and similar criteria have appeared in many settings [58,69,74], while algorithms for decision making underMaximality and E-admissibility have been suggested by Kyburg and Pittarelli [43] and proposed more recently by Kikutiet al. [42] and Utkin and Augustin [72].1 Section 3 presents algorithms and computational analysis for several criteria ofchoice. The most valuable contribution of Section 3 is the algorithm for E-admissibility. We also present a new algorithmfor strategy selection using linear programming in a family of decision trees where partial preferences have considerableregularity.Sections 4 and 5 respectively present algorithms for decision making in problems specified through decision trees andinfluence diagrams. We should note the scarcity of previous literature on influence diagrams under partially ordered prefer-ences, perhaps due to the fact that several criteria of choice require the manipulation of an exponential number of strategies.To reduce this complexity, we examine “ordered” LIMIDs, and we analyze both their conceptual foundation (in particulartheir clash with consequentialism) and their computational properties.In short, we present novel results and algorithms for sequential decision making with decision trees and influencediagrams, plus new insights for single-stage decision making under Interval Dominance and E-admissibility. The broadergoal of the paper is to combine both the philosophical underpinnings and the computational properties of partially orderedpreferences, a combination we feel is missing in the current literature.2. Partially ordered preferences, behavioral norms, and credal setsThroughout, our decision makers must select one or more actions within a finite set of possible alternatives A ={a1, . . . , am}. Performing action a yields a reward a(ω) for each state of nature ω; the set of states of nature is assumed tobe a finite set Ω = {ω1, . . . , ωn}. We assume that a(ω) is a real number expressed in utiles. Even though some theories ofpreference allow multiple utilities to be defined for a single decision problem [2], in this paper we assume that utilities areprecisely fixed in a given decision problem, and consequently every action is identified with a single real-valued functionover the states of nature. Note that a utility function is a function that returns a value in utiles for each possible outcome;so we are assuming that a single utility function is fixed.The connection between preference and expected utility, in decision making under risk [47], is based on the axiomati-zation of preference relations. Denote the strict preference of ai over a j by ai (cid:3) a j , and define indifference between twoactions as ai ∼ a j ⇔ ¬(ai (cid:3) a j) ∧ ¬(a j (cid:3) ai). Suppose (cid:3) satisfies (recall that actions are functions that can be multiplied andadded) [23]:Axiom 1 (completeness). The relation (cid:3) is complete and negatively transitive (recall that (cid:3) is negatively transitive if itsatisfies for all ai , a j , ak: (ai (cid:2) a j) ∧ (a j (cid:2) ak) ⇒ (ai (cid:2) ak)).Axiom 2 (independence). For α ∈ (0, 1], ai (cid:3) a j ⇒ αai + (1 − α)ak (cid:3) αa j + (1 − α)ak (this axiom says that whenever ai (cid:3) a j ,a compound action made of ai and ak will be preferred to a compound action made of a j and ak, where α denotes the ratioof mixture between the actions).Axiom 3 (continuity). If ai (cid:3) a j (cid:3) ak, then there exists α, β ∈ (0, 1) such that αai + (1 − α)ak (cid:3) a j (cid:3) βai + (1 − β)ak.Then there must exist a single probability measure P and a related expected utility representation for (cid:3); that is, thevalue of an action ai is given by E[ai] =j=1 P (ω j)ai(ω j), and ai (cid:3) a j if and only if E[ai] > E[a j].(cid:2)nSeveral theories relax these axioms, attempting to accommodate various observed decision making patterns [1,20,40]. Forinstance, lexicographic preferences violate Axiom 3 and are encoded through expected utility vectors, ordered with respectto a lexicographic hierarchy [5,23]. Other theories violate Axiom 2 and lead to non-additive functionals that representpreferences [49, Section 2.3]. Partially ordered preferences violate Axiom 1 by assuming that preferences are not completelyordered; this is exactly the situation we examine in the present paper. If we assume a single utility function and doif and only if E P [ai] > E P [a j] for all P innot require the preference relation (cid:3) to be complete, we have that ai (cid:3) a j1 Most results are based on material presented in Kikuti et al. [42] and Kikuti and Cozman [41]. The third author participated in developing the columngeneration method, and the experiments, reported in Section 4.3.1348D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–1365Fig. 1. A sequential decision problem represented through a decision tree.a set of probability measures [26,61,62]. That is, the preference relation (cid:3) can be completely represented by a set ofprobability measures K . Incomparability between two actions ai and a j appears when one probability measure P 1 ∈ Kproduces E P 1[a j] while another probability measure P 2 ∈ K fails to produce E P 2[ai] > E P 2[ai] > E P 1[a j].As alluded to in Section 1, several circumstances may preclude the assessment of a complete preference ordering, fromincomplete understanding of a decision scenario, or perhaps a desire to abstract elements of a complex decision situation,to disagreements amongst experts involved in decision making. Sometimes the very language in which preferences areexpressed allows for partial specification; this is particularly relevant in artificial intelligence applications. For instance, thesemantics of “nondeterministic” actions in planning [6] is that these actions have effects whose probabilities are unknown,and consequently it is not possible to completely order them with respect to expected utility [69]. Another suggestiveexample is the theory of CP-nets [7], in which a graph-theoretical language organizes preferences about features of outcomesrather than outcomes themselves. A CP-net may generate a single preference ordering for outcomes, but in general itspecifies a partial ordering. While a CP-net deals with outcomes (and thus reflects incomplete specification of utilities),a similar language for actions would be within the confines of the present paper. Hopefully the present paper will helpshorten the gap between the current theories of nondeterministic planning and CP-nets, and the foundational literature onpartially ordered planning.When preferences are partially ordered, there may be no single “best” action to select. Before we examine criteria ofchoice in Section 3, we review behavioral norms for sequential decision problems (Section 2.1) and some properties of setsof probability measures (Section 2.2).2.1. Sequential decision problems: strategies and behavior normsIn a sequential decision problem, a decision maker faces a sequence of decisions, and each decision may impact futuredecisions. A convenient language to introduce sequential decision problems is through decision trees [56]. A decision tree Tis a connected graph without cycles, where each node belongs to one of three categories. A decision node D ∈ D, typicallydrawn as a square, represents the place where the decision maker must choose an action. A chance node C ∈ C, typicallydrawn as a circle, represents an event out of control of the decision maker. A utility node U ∈ U is associated with a real-valued utility. In decision trees, a leaf node is a utility node and vice versa. Edges out of a decision node represent thepossible actions that the decision maker can choose and edges out of a chance node represent the possible outcomes ofthe event. A subtree of T is a tree T (cid:9)whose nodes and edges form subsets of T . We assume that any tree or subtree isrooted at a decision node. A strategy is a complete set of actions specifying how the decision maker should act when she isactually called to decide. A strategy for a subtree is called a substrategy. We are interested in selecting strategies. The nextexample clarifies this notion.Example 1. The decision tree in Fig. 1 is adapted from [40]. It has three strategies: s1 = (a1, a3), s2 = (a1, a4) and s3 = (a2);where (a1, a3) means that the decision maker will choose action a1 at A and a3 if she reaches the decision node B (if shedoes not reach B then she receives $0). (One might instead consider all conceivable combinations of decisions, as often donein game theory; in this case we would have strategies such as (a2, a3) and (a2, a4). We do not follow this route.)There are two widely debated behavioral norms for decision makers engaged in sequential decision problems [21]. A reso-lute decision maker commits herself to a complete strategy once and for all, comparing simultaneously all strategies rootedat the first decision node [50]. A sophisticated or consequentialist decision maker selects strategies to follow out of a decisionnode only looking at the subtree rooted at that decision node, and may actually change the strategy previously selected[28,29]. There are other possible norms that we do not investigate further; for instance, a myopic decision maker constructsher strategy by selecting actions, at each decision node, independently of future choices [28,67].The following example illustrates the behavioral norms in the light of a non-expected utility model of preference.Example 2. Consider again Example 1. Suppose the decision maker has adopted a rank-dependent utility model of preference[40], where strategies are ranked using a single utility/probability pair and the functionD. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–13651349Fig. 2. Decision tree for Example 3.V (a) = a(ω1) +n(cid:3)j=2(cid:4)(cid:4)−(cid:5)− ln(x j)0.5(cid:5)(cid:6)(cid:7)a(ω j) − a(ω j−1),exp(cid:2)nk= j P (ωk) and the inequalities a(ω1) (cid:2) · · · (cid:2) a(ωn) are assumed to hold. At node A we have V (s1) = 1124.86,where x j =V (s2) = 924.23 and V (s3) = 1000, so at node A, s1 (cid:3) s3 (cid:3) s2. However at node B, action a4 is preferred to a3 as V (a3) =2494.06 and V (a4) = 3000. If the decision maker is resolute, she selects s1 and implements it; at B she must choose a3 evenif a4 is locally better. If the decision maker is consequentialist, she would anticipate that in B she would prefer a4 to a3, sos1 is infeasible for her: comparing s2 and s3 at A, she must choose s3. If the decision maker is myopic, she would select s1at A, but when she reaches B she deviates from s1 by choosing a4 (the best option locally), thus actually implementing s2.In general, resolute behavior demands examination of all strategies at the beginning. Even though the resolute norm hasbeen forcefully defended when moral aspects of commitments are taken into account [9,51], resolution faces several prob-lems as to how preferences are to be elicited and modeled. By renouncing consequentialism, the decision maker does nothave well defined local preferences that can be revealed from her choices [57].2 A consequentialist norm makes even moresense if we consider a resource bounded decision maker who cannot possibly optimize over the space of all strategies [66].For this reason, we adopt the consequentialist norm throughout this paper.If a decision maker eliminates an action at a non-root node, and this action might be selected from the perspective ofthe root node, we have an episode of incoherent choice. Another situation is that of inconsistent choice, where the decisionmaker selects a strategy but subsequently deviates from it [28,49]. Incoherent and inconsistent choices do not occur whena decision maker ranks preferences through expected utility with a single utility function and a single non-zero probabilitymeasure (in fact, resolute and consequentialist norms are equivalent for such a decision maker). However, incoherence maybefall the decision maker when preferences are partially ordered:Example 3 (Adapted from Seidenfeld [60]). In the decision tree depicted in Fig. 2, p ∈ [0.25, 0.75], q = 1/2 and (cid:7) > 0. Thereis a charge of 0.4 utiles to take action a1, a2a or a2b, and a charge of 0.35 utiles to take action a3, thus E P [a1] = 0and E P [a2a] = E P [a2b] = 0.1 for every possible P . There are 9 additional strategies to consider, by combining actions outof D2 and D3. Suppose we are interested in strategies with maximum minimum expected utility (that is, we adopt the(cid:9)Γ -Maximin criterion to be detailed later). For small (cid:7), at D2 the Γ -Maximin action is a1, and at D3 the Γ -Maximin action(cid:9)(cid:9)2b) is a strategy with larger minimum expectedis autility. This is an episode of incoherent choice, since we throw away the actions that would lead to better strategy at theroot node.(cid:9)(cid:9)1) is not Γ -Maximin at D1, as (a3, a(cid:9)(cid:9)1; however the strategy (a3, a(cid:9)2a, a(cid:9)1, aWe will further discuss incoherent and inconsistent choices in Section 4.1, after detailing some criteria of choice.2.2. Partially ordered preferences through sets of probability measuresAs noted previously, partially ordered preferences can often be represented by sets of probability measures. We call aset of probability measures a credal set [46], and denote by K ( X) a credal set that contains distributions for a random2 Jaffray [38] combines consequentialist preferences and non-consequentialist behavior. Nielsen and Jaffray [52] use the rank-dependent utility modelwith preferences function revealed by the anticipated utility theory of Quiggin [55].1350D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–1365variable X . We assume throughout that credal sets are given by a finite set of linear constraints, thus being closed convexwith finitely many vertices (Example 4 shows one such credal set).Given a set of assessments containing constraints on probability values, any credal set that satisfies the constraints is anextension of the assessments. Given an event A, P ( A) = minP ∈K P ( A) and P ( A) = maxP ∈K P ( A) are respectively the lowerprobability and the upper probability of A. Given a random variable X , E[ X] = minP ∈K E[ X] and E[ X] = maxP ∈K E[ X] arerespectively the lower expectation and the upper expectation of X (we use expected value and expectation as synonyms).A conditional credal set K (·| A) is obtained by conditioning (Bayes’ rule) every measure in a credal set with respect to A(likewise, K (·|Y ) is produced by elementwise conditioning with respect to a random variable Y ). We assume that anyconditioning event has lower probability strictly larger than zero ( P ( A) > 0). If the (joint) credal set K ( X, Y ) is such thatevery one of its vertices satisfies stochastic independence of X and Y (that is, all vertices factorize as P ( X)P (Y )), then Xand Y are said to be strongly independent. There are several other concepts of independence for credal sets in the literature[11,12]; strong independence is perhaps the most popular, and we adopt it in this paper. Conditional strong independence isdefined in the obvious manner, by requiring conditional stochastic independence for every vertex of the conditional credalset of interest.We shall, when we deal with influence diagrams, use elements of the theory of credal networks. A credal network is agraph-theoretical representation for a joint credal set K ( X1, . . . , Xn) that mimics the structure of a Bayesian network [13].A credal network consists of a directed acyclic graph such that each node is identified with a random variable Xi . Thein the graph are denoted by pa( Xi). Each variable is associated with a conditional credal setparents of variable XiK ( Xi|pa( Xi) = πk) for each value πk of pa( Xi), and every variable is assumed strongly independent of its nondescen-dants in the graph given its parents in the graph. Thus the largest extension of all assessments in a credal network,called the strong extension, is a joint credal set K ( X1, . . . , Xn) given by the convex hull of the set of joint distributions:(cid:8)i=1 p( Xi|pa( Xi)): p( Xi|pa( Xi) = πk) ∈ K ( Xi|pa( Xi) = πk)} where this expression refers to densities induced by the ap-{npropriate probability distributions. An inference is then the computation of lower/upper probabilities for the values of somevariable. In general, inference with strong extensions is N P P P -complete; with constraints on the induced width of credalnetworks, the complexity of inferences is in N P [17]. The best available algorithms for inference with strong extensionsi=1 p( Xi|pa( Xi)) subject to assessments in the credal network [16]. Some variables mayoptimize a multilinear polynomialbe discarded when computing a particular inference, using the d-separation property that strong extensions inherit fromBayesian networks [12].3(cid:8)nWe will need to solve multilinear programs several times in this paper. The most refined algorithm for solution ofmultilinear programming problems arising from judgements of independence seems to be the adaptation of Sherali andTuncbilek [65]’s RL method by de Campos and Cozman [17]. We have used the adapted RL method in our implementation(Section 5.2), and we usually take the solution of a multilinear program to be a “unit” of computation, even though such asolution may require substantial effort in itself.3. Criteria of choice in single-stage decision makingIn this section we study several criteria of choice for partially ordered preferences; that is, criteria that select one ormore actions from a given set of actions. We present the basic computations that must be performed in a single-stagedecision making problem, and we present short code fragments that are used later. While several of these algorithmshave appeared in the literature [70,71], the discussion contributes with new analyses both for Interval Dominance andfor E-admissibility [42]. The computational cost of the algorithms is presented as a function of the number of auxiliaryoptimization programs that must be solved. The following example clarifies the nature of these programs (the exampledeals with linear constraints; later we face situations where auxiliary programs are multilinear).Example 4. Consider a decision problem with three states, x1, x2 and x3 that are values of a random variable X . Actionsand utilities are given in Fig. 3. Suppose the credal set K ( X) is specified through P ( X = x1) ∈ [1/10; 7/20], P ( X = x2) ∈[1/5; 2/5], and P ( X = x3) ∈ [7/20; 13/20], as depicted in Fig. 3. To obtain lower and upper expectations for actions ai (alsoin Fig. 3), we must solve linear programs of the form min / max E[ai] =j p j = 1 and P ( X = x j) (cid:2)p j (cid:2) P ( X = x j) for j = 1, 2, 3.j p jai(x j) subject to(cid:2)(cid:2)Existing criteria can be roughly divided into two groups. Indecision-resistant criteria force a single ordering of choices,and thus select either a single action or a set of equally ranked (with respect to choice) actions. Indecision-prone criteriamay return a set of actions that are deemed incomparable (with respect to preference).We start by briefly examining indecision-resistant criteria: Γ -Maximin, Γ -Maximax, and Γ -Maximix. The Γ -Maximincriterion selects an action with highest lower expectation, a “pessimistic” solution that focuses on worst case scenarios[3,25]. Algorithm 1 is an easy translation of the Γ -Maximin criterion. The Γ -Maximax criterion selects an action withhighest upper expectation, an “optimistic” solution that focuses on best case scenarios [58]. The Γ -Maximix criterion selects3 Given three collections of variables X, Y and Z, suppose that along every path between a variable in X and a variable in Y there is a variable W suchthat: either W has two converging arrows and is not in Z and none of its descendants are in Z, or W is in Z. Then X and Y are d-separated by Z [54].D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–13651351x1x2x3E[ai ]E[ai ]a1a2a3a4a59056275546185453.34.35.04.24.25.66.455.04.75.1Fig. 3. Left: Actions, utilities, and lower and upper expected utilities for Example 4. Right: Credal set K ( X) (hatched area) defined by probability intervalsin Example 4; axes denote probability of x1, x2 and x3.Algorithm 1: Criterion_Γ -MaximinInput: Set of actions A, and set of constraints K on probability values.Output: A Γ -Maximin action.∗ ← a1, x ← E[a1];aforeach ai ∈ A\a1 do if E[ai ] > x then {areturn a∗;123∗ ← ai , x ← E[ai ]};Algorithm 2: Criterion_MaximalityInput: Set of actions A containing #A actions (each action has attribute “admissible”, initially set to true), and set of constraints K on probabilityvalues.Output: The set of maximal actions.for i ← 1 to (#A − 1) dofor j ← i + 1 to (#A) doif E[ai − a j ] > 0 then a j .admissible ← false;else if E[ai − a j ] < 0 then ai .admissible ← false;12345return All actions with attribute “admissible” set to true;∗ = arg maxai ∈A(ηE[ai] + (1 − η)E[ai]), where η ∈ [0, 1] reflects the degree of ambiguity aversion [72], and is alreadyasketched by Hurwicz [36]. Algorithm 1 can be easily modified to deal with the Γ -Maximax and Γ -Maximix criteria. Forthese three criteria, Algorithm 1 returns the selected action by solving a number of optimization programs, each subjectto constraints in K . The number of optimization programs is clearly linear on the number of actions. In Example 4, theΓ -Maximin criterion selects a3, while the Γ -Maximax criterion selects a2 and the Γ -Maximix criterion also selects a2 forη = 0.5.Consider indecision-prone criteria, starting with Maximality. An action ai is maximal if there is no action a j such that,for each possible probability measure P ∈ K , E P [a j] > E P [ai]. The maximality criterion is based on pairwise comparisonsamongst actions, as indicated by Algorithm 2 (in Algorithm 2 we use the fact that E P [ai] > E P [a j] for all P is equivalent toE[ai − a j] > 0 [73]). To handle n actions, the algorithm must solve at most (n2 − n) optimization programs. In Example 4,to determine whether a4 can be maximal in the presence of a3, we must solve the linear program max(6p1 + 4p2 + 4p3 −5p1 − 5p2 − 5p3) subject toi=1 pi = 1, 1/10 (cid:2) p1 (cid:2) 7/20, 1/5 (cid:2) p2 (cid:2) 2/5, and 7/20 (cid:2) p3 (cid:2) 13/20. We find that themaximum is 3/10, hence a4 is not maximal when a3 is present.(cid:2)3We now examine Interval Dominance and E-admissibility in more detail.3.1. Interval DominanceInterval Dominance selects one or more I-admissible actions as follows [70]. Action a j is I-inadmissible when ai is presentif E[ai] > E[a j]. The I-admissible actions are the actions that are never I-inadmissible. Note that Interval Dominance doesnot identify dominance in the sense that, given two actions ai and a j , ai dominates a j if for all probability measures P ,1352D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–1365Fig. 4. Left: Interval Dominance does not capture “true” dominance (slanted lines denote the expectations of actions a1 and a2 as the probability of eventA varies). Right: Actions and their expected utilities in Example 5.Algorithm 3: Criterion_IntervalDominanceInput: Set of actions A (each action has attribute “admissible”, initially set to true), and set of constraints K on probability values.Output: The set of I-admissible actions.∗ ← Criterion_Γ -Maximin(A, K ); x ← E[aaforeach ai ∈ A do if x > E[ai ] then ai .admissible ← false;return All actions with attribute “admissible” set to true;∗];123E P [ai] (cid:3) E P [a j]: in Fig. 4, a2 has higher expectation than action a1 for every probability value P ( A) ∈ [0.3, 0.7], but IntervalDominance does not choose between these actions.∗A naive method to generate I-admissible actions would compare every pair of actions. Algorithm 3 avoids unneces-sary computation of lower and upper expectations by using the Γ -Maximin solution a. Action ais always I-admissible:suppose otherwise that ais I-inadmissible; then there is awith higher lower expectation, contradicting the hypothesis.The comparison of all actions with aiswith the maximum lower expectation, because E[a] (cid:3) E[aalso dominated by action awe must solve noptimization programs; to determine the set of admissible actions we must solve n − 1 additional optimization programs.Consequently, Algorithm 3 returns the I-admissible actions by solving a linear (on the number of actions) number of opti-mization programs.dominated by another action a∗generates the I-admissible actions (an action a(cid:9)(cid:9)]). To find a(cid:9)(cid:9)∗∗∗∗(cid:9)(cid:9)3.2. E-admissibilityThe criterion of E-admissibility, where E stands for “expectation” [45], focuses on actions that maximize expected utility.Given a set of actions A and a credal set K , the action ai ∈ A is E-admissible when, for at least one P ∈ K , ai maximizesexpected utility [59]:ai is E-admissible when ∃(P ∈ K ): ∀(a j ∈ A, j (cid:13)= i) : E P [ai − a j] (cid:3) 0.(1)A variant of E-admissibility has been explored for Markov decision processes with imprecise probabilities by Itoh andNakamura [37].(cid:9)1, a(cid:9)2a and a(cid:9)2a and aExample 5. Take ations aexpected utility for P (E) ∈ [0.5, 0.75]. Even though anot E-admissible.(cid:9)2b are E-admissible: action a(cid:9)2b as in Example 3, and assume (cid:7) = 0. Expected utilities are shown in Fig. 4 (right). Only ac-(cid:9)(cid:9)2b maximizes expected utility for P (E) ∈ [0.25, 0.5], while action a2a maximizes(cid:9)1 is the Γ -Maximin action, it never maximizes expected utility and isE-admissibility is qualitatively different from the previous criteria in that it does not depend on pairwise comparisons;rather, it is based on the existence of specific probability measures in the underlying credal set. Thus one might think thatE-admissibility is more difficult to handle computationally than the other criteria. This feeling transpires in the literatureon decision making with partially ordered preferences, as best expressed in Troffaes’ [70] excellent review. However, it ispossible to reduce the search for E-admissible actions to a linear sequence of optimization programs, using insights firstderived by Kyburg and Pittarelli [43]. The original discussion by Kyburg and Pittarelli did not focus on computational cost,and it lay dormant until the same techniques surfaced independently in work by Kikuti et al. [42] and Utkin and Augustin[72], in response to Troffaes’ [70] analysis.The basic idea is that an action ai is E-admissible if there is P ∈ K such that all constraints generated by Expression (1)are satisfied. If these constraints cannot be satisfied, then ai is not E-admissible. Algorithm 4 generates exactly these con-straints: the linear expression E P [ai − a j] (cid:3) 0 in line 3, stored in the set C, denotes a symbolic constraint on the free (notyet bound) values of P . This way the algorithm avoids the need to represent credal sets explicitly (that is, the need toenumerate vertices). As every action is verified only once:D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–13651353Algorithm 4: Criterion_E-admissibilityInput: Set of actions A containing #A actions (each action has attribute “admissible”, initially set to true), and set of constraints K on probabilityvalues.Output: The set of E-admissible actions.for i ← 1 to (#A) doC ← K ;for j ← 1 to (#A) do if i (cid:13)= j then C ← C ∪ {E P [ai − a j ] (cid:2) 0};if C is not feasible then ai .admissible ← false;return All actions with attribute “admissible” set to true;12345Proposition 1. Algorithm 4 returns the E-admissible actions by solving a linear (in the number of actions) number of optimizationprograms.In Example 4, a1, a2 and a3 are E-admissible actions. To verify whether a1 is E-admissible, we must verify whether thei=1 pi = 1 andfollowing linear constraints can be satisfied together:(cid:2)31/10 (cid:2) p1 (cid:2) 7/20, 1/5 (cid:2) p2 (cid:2) 2/5, 7/20 (cid:2) p3 (cid:2) 13/20,9p1 + 7p2 + p3 − 0p1 − 5p2 − 8p3 (cid:3) 0,9p1 + 7p2 + p3 − 6p1 − 4p2 − 4p3 (cid:3) 0,9p1 + 7p2 + p3 − 5p1 − 5p2 − 5p3 (cid:3) 0,9p1 + 7p2 + p3 − 2p1 − 6p2 − 5p3 (cid:3) 0.4. Algorithms for sequential decision making: decision treesIn this section we derive algorithms for sequential decision making with decision trees that display indetermi-nacy/imprecision in probability values.4.1. Preliminaries: the option for consequentialismIn a decision tree where chance nodes are associated with credal sets we may face differences between resolute andconsequentialist behaviors.We start by noting that E-admissibility and Maximality never lead to incoherent choice as both resolute and consequen-tialist norms produce identical sets of strategies [35,60].4 Thus with E-admissibility and Maximality it is possible to runbackward induction and produce a sequence of substrategies that satisfy consequentialism and that reach the strategiescomplying with the resolute norm. As a digression, note that such backward induction scheme is exactly given by Algo-rithm 5, detailed later, when this algorithm is specialized to the E-admissibility and Maximality criteria [42]; we also notethat recent work by Huntley and Troffaes [35] yields simplifications to Algorithm 5 when applied to Maximality.The remaining criteria in Section 3 may produce distinct consequentialist and resolute behaviors. The Γ -Maximin cri-terion was considered in Example 3. Clearly the same applies to the Γ -Maximix and Γ -Maximax criteria, since they donot guarantee that a discarded substrategy is not part of an optimal strategy. To illustrate this for Γ -Maximax criterion,(cid:9)(cid:9)(cid:9)consider again Example 3. At D2, the Γ -Maximax action is a2a, and at D3, the Γ -Maximax action is a2b. However at D1(cid:9)(cid:9)2b).the Γ -Maximax strategy is either (a3, aIncoherent choice can also happen with Interval Dominance, a fact that apparently has not been indicated before:(cid:9)(cid:9)2a) or (a3, a(cid:9)2b, a(cid:9)2a, aExample 6. In the decision tree depicted in Fig. 5, suppose p ∈ [1/10, 3/10], q1 ∈ [1/5, 2/5], q2 ∈ [2/5, 3/5] and q3 ∈[3/5, 4/5]. At D2, action a(cid:9)2, but the strategy (a1, a(cid:9)2) is I-admissible at D1.(cid:9)1 dominates action aIn this paper we adopt the consequentialist position that the best substrategy starting at a decision node can depend onlyon the subtree starting at that node, motivated by the fact that the sensible alternative, resolute behavior, is computationallyunfeasible in general for bounded agents (in a sequential decision problem, the resolute behavior can be viewed as a “brute-force” method, that demands the enumeration of all possible strategies).4.2. Selecting strategiesAlgorithm 5 presents a general framework for consequentialist sequential decision making in the presence of indeter-minacy/imprecision in probabilities — that is, under partially ordered preferences. The algorithm can be specialized byreplacing throughout criterion by the desired criterion of choice. The intuition behind the algorithm is that a strategy can4 Nevertheless, both E-admissibility and Maximality still may lead to inconsistent choice, where the decision maker plans for an action but then executesa different action.1354D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–1365Algorithm 5: DecisionTree_criterionInput: A decision tree T as described in Section 4.1, with a set T .K of constraints on probability values.Output: A set of strategies selected by criterion.Fig. 5. Decision tree for Example 6.123456789m ← Number of decision nodes;for i ← m until 1 dothat is a child of Dm doS ← ∅;foreach Dif D(cid:9)(cid:9) ∈ D thenforeach s j ∈ D(cid:9) ∈ C thenelse if D(cid:9).substrategies do S ← S ∪ {D(cid:9), s j };foreach s j ∈ Combination(D(cid:9)}else S ← S ∪ {D(cid:9)) do S ← S ∪ {D(cid:9), s j };1011K ← GenerateConstraints(S, Dm, T .K );D.substrategies ← Criterion_criterion(S, K);12return Content of attribute substrategies for root decision node;be constructed by visiting the nodes backwards, i.e. from last (leaf) to first (root), by selecting the optimal choice as decisionnodes are encountered. The optimal choice at a decision node is then combined to the array of optimal choices selected atpreviously visited nodes, until the root node is resolved. In order to accomplish this, Algorithm 5 assumes that each decisionnode keeps a list of admissible substrategies (according to criterion) rooted at that node; this list is kept in the “substrate-gies” attribute. The remainder of this section is dedicated to a detailed discussion of technical aspects of this algorithm. Inaddition, we propose a simple transformation that allows the algorithm to be solved using linear programming instead ofthe more computationally demanding multilinear program. We also show that this linear program can benefit from the useof the column generation technique in order to obtain solutions more efficiently.We assume that the decision nodes in D are topologically sorted, that is, they follow a linear (temporal) ordering ofdecisions such that if T contains a path from decision node D x to D y , then D x appears before the decision node D y in theordering. We assume that D1 is the root of T and Dm is the last decision node in such ordering.(cid:9)(cid:9)(cid:9)As mentioned, the construction of strategies in our algorithm starts backwards at a leaf of the tree, that is, at a utility(cid:9)node (line 9 of Algorithm 5). We visit each decision node Dm (from last to first) and examine its children. If a child Dof D is a decision node, the substrategies rooted at D(thisis a chance node, there are two cases to handle. Ifis indicated by storing Da chance node has no decision nodes as successors, the only substrategy rooted at D is a substrategy that simply moves. If a chance node has decision nodes as successors, it is then necessary to combine all substrategies that can branchto D. For instance, if there are three decision nodes out of a chance node, and each one of these decision nodes leadsout of Dto two substrategies, then eight substrategies must be produced. We assume that these substrategies are returned by thefunction Combination in line 8 of Algorithm 5.are combined with the action that prescribes a move to Dat the beginning of the substrategies). If DThe substrategies S generated in the loop from line 4 to line 9 are fed to the appropriate function Criterion_criterionin line 11. This function treats each substrategy as an action, and it must also receive the constraints and parametersof optimization programs that are run so as to select substrategies in Criterion_criterion. These constraints are basicallycontained in the input set T .K , and given in terms of local assignments.(cid:9)(cid:9)(cid:9)Once the constraints are generated, they are processed by Criterion_criterion in a sequence of optimization programs.Fix a decision node D and strategy s; the expectation of s is:(cid:9)(cid:9)(cid:5)(cid:5)(cid:9)an(X N )(cid:9)an(X1)X N = xNX1 = x1. . . P(cid:3)P(cid:4)(cid:4)U (s, x1, . . . , xN ),(2)x1,...,xN ∈{0,1}where Xi denotes indicator functions for each one of the events in the subtree rooted at D only with branches selectedby s, an( Xi) is the set of nodes in the path from D to Xi , and P ( Xi = xi|an( Xi)) is a local assignment on the probabilityD. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–13651355that the event Xi obtains xi . This expression is clearly in a multilinear form, and optimizing it subject to constraints onprobability values (the terms in the product) takes us to nonlinear programming, as has been pointed out before [14,15,42].Nonlinear programming is known to be a class of very difficult problems to solve, and specific optimization methods havebeen recently applied to the particular case of sequential decision making under the Γ -Maximax criteria by de Campos andJi [19].There are, however, interesting situations where all optimization programs in Criterion_criterion can be transformedinto linear programs, as discussed in the next section.4.3. Selecting strategies with linear programmingA linear formulation is obtained for strategy selection when assessments are linear constraints and are separately specifiedin the sense that probability constraints for the event at a particular chance node do not depend on probability values forany other event. For instance, assessments are separately specified if T .K contains only bounds on probabilities such asP ( A|B) ∈ [α, β] where A is an event at a chance node and B is a conjunction of events in the decision tree from the rootto that node (as illustrated by Example 6).To obtain linear programs, we first note that Expression (2) can be written as(cid:3)x1,...,xN ∈{0,1}P (X1 = x1, . . . , X N = xN )U (s, x1, . . . , xN ),(3)where P ( X1 = x1, . . . , X N = xN ) are the probability values to optimize over. For instance, consider again constraints such asP ( A|B) ∈ [α, β], where A and B are (conjunctions of) events in the decision tree. We can use Bayes’ rule of conditioning totransform these constraints into the form α P (B) (cid:2) P ( A ∩ B) (cid:2) β P (B) under the assumption that probabilities are strictlypositive. In Example 6, we have the constraints:1/10 (cid:2) P (C) (cid:2) 3/10,1/5P (C) (cid:2) P (F ∩ C) (cid:2) 2/5P (C),3/5P (C) (cid:2) P (E ∩ C) (cid:2) 4/5P (C),(cid:4)G ∩ C c2/5P(cid:2) PC c(cid:5)(cid:4)(cid:5)(cid:2) 3/5P(cid:4)(cid:5).C cIt is instructive to analyze the size of the linear programs that are generated by this method. One extreme (favorable)situation is represented by a symmetric decision tree where each decision node in the same slice branches into a constantnumber of chance nodes containing the same event. That is, the first decision node branches into several nodes containingevent A; then the decision nodes out of these two chance nodes branch into several chance nodes all labeled with event B,and so on, as depicted in Fig. 6 (left) for the case of branching factor equal to two. For a fixed strategy, we have a symmetricbinary tree, and if the problem deals with N events, this binary tree contains 2N − 1 nodes. Each complete path from theroot to a utility node corresponds to a complete conjunction of events and complements of events, and the linear programto be built has as many optimization variables as there are paths in this symmetric binary tree. We reach the satisfyingconclusion that, for symmetric decision trees, the linear programs that must be solved within Algorithm 5 are polynomialon the size of the decision tree (this is of course not entirely comforting as the size of these decision trees is exponential onthe number of events). Fig. 6 (right) shows running times for the computation of lower expected value for a given strategy,as this is the basic operation for all criteria of choice. Points in that graph have been produced by generating symmetricdecision trees with randomly generated utilities and lower/upper probabilities for events. The implementation is coded inAMPL and uses the CPLEX commercial package as linear programming solver; experiments were run in a microcomputerwith two dual-core processors and 4 GBytes of memory. One sees that running times are quite small for N (cid:2) 10; it is hardto imagine a symmetric decision tree with more than 10 chance nodes. Just to compare, we have also solved the multilinearformulation (2) using the programming package Multilin [17]. The multilinear programs took minutes even for N = 5 andoften failed to converge (Multilin produces successive approximations and typically reaches a vicinity of the solution quickly,but then converges very slowly).Now consider the other extreme situation, where for every fixed strategy we have a symmetric binary tree such that eachchance node contains a different event. Here the linear program that computes the lower/upper expectation of a strategyis exponentially larger than the decision tree: for a binary tree with height H , there are N = 2H − 1 chance nodes and2N = 2(2H −1) optimization variables. That is, we may have a relatively small decision tree that leads to very large linearprograms: for example, in a binary tree of height 5 (5 levels), there are N = 25 − 1 = 31 chance nodes and 2N = 231optimization variables. As indicated in Fig. 6 (right), running times grow substantially as N grows beyond 10. Here we facea situation that is similar to probabilistic logic; that is, we have a relatively small set of constraints on N events and we musthandle 2N configurations of these events [24,27,31]. Such problems have been tackled with column generation [39] andredundancy detection [32,48]. We are interested in minimizing/maximizing an objective function given by Expression (3).We can write this expression as a product of vectors u · p, where u contains the values of the utility nodes and p theprobability values over the 2N possible configuration of events. We also know from T .K the constraints p is subject to, thatcan also be easily written in matrix form by Ap (cid:3) b.It is clear that A has a very large number of columns (more precisely, 2N , one for each possible configuration of events).Storing and manipulating such an amount of columns is very inefficient and time consuming. However, by using column1356D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–1365Fig. 6. Linear programming solution for exponentially large, separately specified decision trees. Left: Symmetric decision tree. Right: Running times, inlogarithmic scale, for growing N (number of events).(cid:9)generation, it is possible to solve this linear program by manipulating only a smaller sub-matrix Aof A with as manyrows and columns as there are rows in A. The challenge is that, to run the simplex method, at each iteration we mustselect one of the previously discarded columns of A to replace an existing column in A. This is done by computing thereduced cost c − yA, where y is the dual cost of the current solution [4]. We can write every column of A as a vector ofmultilinear expressions [ A1 B1 − α1 B1, . . . , Am Bm − α Bm]T , where Ai is an event and B i is a conjunction of events, and αi isan assessment. Thus c − yA is a multilinear expression on optimization variables that are either 0 or 1; there are standardtechniques to reduce such an optimization problem to integer programming [18, Section 4.2]. To summarize: we run the, we run an auxiliary integer program. We notesimplex method with Athat problems with hundreds of variables in probabilistic logic have been solved using column generation with relativeease [32], so the exact solution of large decision trees can be obtained. Note also that these techniques can be extendedto chance nodes that are associated with random variables with finitely many values. The added effort is to binarize therandom variables into sets of binary variables, and to add Boolean constraints so that these binary variables only take onpossible values. The result is again a probabilistic logic problem that can be solved using linear programming, possibly withcolumn generation if N is large., and to decide which column of A to enter into A(cid:9)(cid:9)(cid:9)4.4. Consequentialist backward induction. . .?One might argue that constraints K should be generated only once before any other computation in Algorithm 5, asassessments T .K are available as input. However such an approach may miss significant simplifications, because there maybe chance nodes that are discarded during execution of the algorithm, given our consequentialist perspective. For instance,(cid:9)in Fig. 5 the event F can be discarded when one is at D1, because action a2 and the subsequent nodes are not admissiblefor any criteria of choice previously discussed. Thus it makes sense to generate constraints “inside” the loop (line 10) inAlgorithm 5. Nevertheless, in the worst case the function GenerateConstraints may build, if implemented as described in theprevious section, an exponentially large optimization program at the root node. This is somewhat unsatisfying, particularlywhen compared to backward induction in standard decision trees.In a standard decision tree, an already processed decision node is completely summarized by the unique expected valueof the selected substrategy from that node on. Instead in the function GenerateConstraints described in the previous section,the programs built at decision nodes grow in size. The natural question is: Can we have a function GenerateConstraints thatsummarizes the already processed decision nodes through an interval of expected utility? For instance, in Example 6 wewould like the choice between a1 and a2 to be resolved at D1 only by processing an expectation interval from D2. Alas,such an interval-based backward induction fails in general:Example 7. Consider the decision tree in Fig. 7, adapted from Hammond [30]. Here p ∈ [(cid:7), 1 − (cid:7)] and q = (cid:7), for somesmall (cid:7) > 0. Actions a and b are maximal and E-admissible at D2, and c and d are maximal and E-admissible at D3, but(cid:9), b}. If D2 and D3 were to return the expectation intervals forstrategy {atheir maximal/E-admissible actions, it would not be possible to detect that some strategies are dominated; in fact, all fourstrategies have overlapping expectation intervals.(cid:9)(cid:9), c} dominates {a(cid:9), a} dominates {a(cid:9)(cid:9), d}, and {aD. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–13651357Fig. 7. Decision tree for Example 7.Maximality and E-admissibility may fail in an interval-based backward induction because the necessary informationabout the constraints on probability values may be lost. We now wish to show that an interval-based backward inductioncan succeed for Γ -Maximin, Γ -Maximax, Γ -Maximix and Interval Dominance. That is, for these criteria and under theassumption that constraints are separately specified, we can evaluate actions at a decision node D as a one-step decisionproblem where each decision node reached from D is replaced by a single expectation interval. We start by rehearsing, inthe next paragraph, an argument by Danielson and Ekenberg [14], who derived algorithms for computation of lower/upperexpectations of a fixed strategy in the presence of local bounds on probabilities and expectations.The central idea by [14] is as follows. Fix a strategy s; using Expression (2),(cid:9)(cid:5)(cid:9)an(X1)(cid:9)(cid:5)(cid:9)an(X N ). . . P N(cid:3)(cid:3)· · ·X NX1P 1(cid:4)(cid:4)U (s, X1, . . . , X N ),E[s] = minP 1,...,P NX1X Nwhere we have subscripted the probability distributions so as to emphasize the scope of the minimization. We can movethe summations to the right, eliminating variables one by one from X N to X1, and we can place the minimization over aprobability distribution right before the probability distribution of interest; this is only possible because all constraints arelocal and independently specified for each path from root to the node of interest. Hence to compute the lower expectationof s, we can run a backward induction scheme where a reduced optimization program is built at each decision node D byencoding constraints in the subtree rooted at D, with action selected by s, and with branches cut at future decision nodes.These latter decision nodes are replaced by their lower and upper expected utilities; and as D produces its own lowerand upper expected utilities, it passes only these values back to its ancestors. Reduced programs are multilinear due to thepresence of probability constraints and expectation intervals; that is, in Expression (2) we have both values of P i and of Uas free variables to optimize for. Danielson and Ekenberg [14] present techniques that simplify the solution of these localmultilinear programs.Returning to our problem, consider the criterion of Interval Dominance. At decision node D we can build a completeoptimization program with all chance nodes in the subtree rooted at D, except those nodes in subtrees eliminated in previ-ous stages of the backward induction method. Alternatively, we can build a reduced optimization problem with the chancenodes between D and the decision nodes that are direct descendants, with the proviso that these descendants summarizethe content of their subtrees by intervals of expected utility. Recall that each decision node that is direct descendant of D isattached to a fixed set of substrategies, because our decision maker is consequentialist. Thus the only question is whetherthe set of I-admissible actions at D is the same regardless of whether we use the complete or the reduced program. Giventhat all that matters for Interval Dominance are lower/upper expectations, the argument in the previous paragraph leads toa positive answer: the two programs produce identical results, as the complete program can be divided into several smallerprograms that are all encoded in the reduced program and its expectation intervals.Similar arguments work for Γ -Maximin, Γ -Maximax and Γ -Maximix. Each descendant D(cid:9). Every expected utility in the expectation interval for sof D is attached to a singlecan be attained by selecting. Consequently, at D we lose nothing by restricting attention to theselected substrategy sprobability distributions in the subtree rooted at Dexpectation interval of s, and likewise for every descendant of D.rooted at D(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)To finish this section, we compare several kinds of assessments and criteria of choice in the context of Example 3:(cid:9)1, a(cid:9)2a and aExample 8. Consider Example 3, and suppose (cid:7) is small. Algorithm 5 starts at D2, where three substrategies, corresponding(cid:9)2b, are evaluated by the appropriate function Criterion_criterion. The same happens at node D3. Atto actions anode D1, the actions a1, a2a, a2b are evaluated, together with all combinations of selected strategies from D2 and D3. Wehave the implicit constraint that a single value p refers to several probability values ( P (E|C, D1), P (E|C c, D2), etc.); thatis, constraints are not separately specified. Running the complete backward induction algorithm, we obtain the following(cid:9)(cid:9)1 at D2 and D3, and the Γ -Maximin strategy is either (a2a) orselected strategies. The Γ -Maximin actions are a(cid:9)(cid:9)(a2b) at D1. (Note that action a2b is likewise inadmissible at D3, but their combination(cid:9)(cid:9)is identical to a2a, a Γ -Maximin action at D1!) The Γ -Maximax criterion prescribes a2b at D2 and D3, and then(cid:9)2a is inadmissible at D2, action a(cid:9)2a and a(cid:9)1 and a1358D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–1365(cid:9)2a, a(cid:9)(cid:9)2b) at D1. Interval Dominance selects all actions at nodes D2 and D3, so we have twelve strategies to evalu-(cid:9)(cid:9)(cid:9)(cid:9)2a)). E-admissibility discards only a1, a1(cid:9)(cid:9)1 at D2 and D3, while Maximality does not discard any of them. At D1 Maximality discards the same strategies as(cid:9)(cid:9)(cid:9)1). Finally, at D1 E-admissibility discards the same strate-2a, a(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)2a),1). There are three E-admissible strategies: (a3, a2a, a(a3, aate at D1, five of which are inadmissible ((a1), (a2a), (a2b), (a3, aand aInterval Dominance plus the strategies (a3, agies as Maximality plus the strategies (a3, a(cid:9)2b, a(a3, a(cid:9)(cid:9)2a) and (a3, a(cid:9)(cid:9)2b) and (a3, a(cid:9)(cid:9)2b), and (a3, a(cid:9)(cid:9)1) and (a3, a(cid:9)1, a(cid:9)1, a(cid:9)2b, a(cid:9)2a, a(cid:9)2a, a(cid:9)(cid:9)2b).Suppose we change Example 3 so that the probability of E depends on the path from root to the chance node labeledwith E; that is, we have eight probability values pi ∈ [0.25, 0.75] (constraints are separately specified). A linear programmingsolution is now possible. However, if we wish we can deal with reduced programs through multilinear programming. Notethat we face a decrease in selectivity by separating assessments: the only inadmissible strategy for Interval Dominance,Maximality and E-admissibility is (a1).To summarize the discussion on decision trees: the same strategies are selected under resolute and consequentialistnorms for Maximality and E-admissibility, and there, a backward induction procedure is fully justified; for the remainingcriteria, a backward induction method is only justified by a consequentialist position (as adopted in this paper). Anotherpoint is that the size of optimization programs generated by Algorithm 5 may grow exponentially; however for Γ -Maximin,Γ -Maximax, Γ -Maximix and Interval Dominance it is possible to run reduced programs when constraints are separatelyspecified. And finally, a linear programming formulation is possible in some cases but multilinear programming is requiredin general, and in particular when dealing with reduced programs.5. Influence diagrams with partially ordered preferencesDecision trees can hardly represent large, or even medium size, decision problems, as the number of nodes in a decisiontree increases exponentially with the number of chance and decision variables. A more compact way to represent sequentialdecision problems is through influence diagrams. A seminal work on influence diagrams with interval probabilities waspresented by Breese and Fertig [8,22]; no substantial advance seems to have appeared in the literature after that work. Wenow expand that analysis by considering several criteria of choice.The section is organized as follows. In Section 5.1 we introduce influence diagrams with partially ordered preferences, wedefine a class of problems that we are interested in, we present an algorithm to solve such class of problems and providean analysis of complexity for the algorithm. In Section 5.2 we discuss several examples and experiments.5.1. Preliminaries, strategy selection and algorithmAn influence diagram with imprecise probabilities is a directed acyclic graph over a set of decision nodes D (squareshaped), chance nodes C (circle shaped) and utility nodes U (diamond shaped). Edges into a chance node indicate stochas-tic dependence; edges into a decision node indicate the available information at the time of the decision; edges into autility node indicate functional dependence. Each decision node is associated with a finite set of actions conditional on itsparents. Each chance node is associated with a random variable C and with a set of credal sets: for each instantiation πiof the parents of C (pa(C)), we have a credal set K (C|pa(C) = πk) specified as in credal networks5; each utility node U isassociated with a function u(pa(U )) that depends only on the parents of U . If more than one utility node is specified, thenthe total utility is the sum of all functions in utility nodes [68]. The standard definition of influence diagrams [34] requiresa linear temporal order of all decisions (typically represented by a directed path comprising all decision nodes) and the noforgetting assumption, that is, at each decision node the decision maker knows all her previous decisions and past observa-tions. However, some past information may be irrelevant and should not be considered for computational reasons [53,64]. InLimited Memory Influence Diagrams (LIMIDs) [44], the no forgetting assumption is relaxed, that is, the decision maker knowsonly the past decisions and observations that are explicitly linked to the decision nodes. This allows the representation of abroad class of decision problems, including situations with many decision makers.A policy δD for decision node D is a mapping from the parents of D to the possible actions in D. A strategy s is anordered set of prescribed actions for all decision nodes, where each action depends on the parents of the decision node;}. The expression of expected utility for a strategy s, for a fixed probabilitythat is, an ordered set of policies s = {δD1 , . . . , δDndistribution P , is(cid:3)(cid:10)(cid:4)(cid:5) (cid:11)(cid:4)upa(U )PX(cid:9)(cid:5)(cid:9)pa(X)(cid:12),(4)U ∈U, X∈{C,D}Xwhere we note that if X ∈ D, then its value is fixed by strategy s (the variable is associated to zero/one probabilities given s).In standard influence diagrams and standard LIMIDs, an optimal strategy is a strategy with maximum expected utility (suchstrategies are called global maximum strategies in LIMIDs [44]).It is important to pause for a moment and consider the properties of LIMIDs. First, any influence diagram is a LIMIDwhere a decision node is informed about all previous decisions. However, if a LIMID contains a small number of arcs into5 In this paper all variables have finitely many values.D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–13651359Fig. 8. A simple influence diagram with too many strategies.decision nodes, the number of possible strategies is small when compared to the number of strategies in an influencediagram with identical graph. Hence the number of arcs into decision nodes is a critical parameter in a LIMID. Anotherimportant property of LIMIDs is that decision nodes are not necessarily ordered, so a decision maker contemplating aparticular decision node may have no clue as to which decisions are implemented already and which decisions are still tobe reached.The lack of ordering amongst decisions in LIMIDs brings about a point that seems to have been missed in the literature.Namely, that LIMIDs are intrinsically inappropriate for consequentialist behavior. Clearly a decision maker can enumerateall strategies in a LIMID and then select a strategy with maximum expected utility, presumably to follow it all the way(in a resolute manner). A consequentialist behavior is harder to describe in the context of a LIMID. Suppose a decisionmaker seats at a decision node, considering only future moves in an attempt to evaluate its current decision; that is, ina consequentialist position. But how is this decision maker to know what are the “future” moves in a LIMID? There maybe decisions that are not ordered with respect to the current decision, and the only way to examine the relative valueof strategies is to consider all possible orderings. This may only be possible by considering the set of strategies from theoutset, as a resolute decision maker would do. Indeed, the popular algorithm Single Policy Update (SPU) [44] finds non-optimal strategies in LIMIDs by updating policies in some given order; the resulting strategies are not guaranteed to beoptimal and, more importantly, the whole reasoning behind SPU cannot be given a consequentialist justification. For thisreason, we do not attempt in this paper to adapt SPU to LIMIDs with imprecise probabilities; quite the contrary, we use adirect multilinear formulation of the strategy-selection problem. Note that a version of SPU for indecision-resistant criteriais not so difficult to conceive (as every decision node must yield a single policy) but a version of SPU for indecision-pronecriteria seems not to be possible.For these reasons, in this paper we are interested only in those LIMIDs that have a temporal order for decisions, so thatwe can always consider consequentialist behavior. Such an assumption about LIMIDs clearly limits the scope of models wecan use, but it should be noted that the resulting class of LIMIDs is substantially larger than the class of influence diagrams;even though there is an ordering on decisions, the set of strategies that is allowed for a LIMID does not necessarily requireeach decision node to be aware of all previous decisions in the ordering. That is, the number of possible strategies in theLIMID may be substantially smaller than the number of strategies in an influence diagram of identical structure. In fact, thereason why we focus on “ordered” LIMIDs is exactly so that we can limit the number of possible strategies as compared toinfluence diagrams proper.Example 9. Consider Fig. 8. Suppose that A has four possible outcomes and D1 has four possible actions. A policy for D2specifies one action for each configuration of the parents. If D2 has two possible actions, then there are 216 policies. Ingeneral, if we have m configurations and n actions, we can generate nm policies. This example shows that the number ofpolicies grows quickly (exponentially) when decision nodes depend on several parents. In this case the search for optimalstrategies may easily become intractable. For instance, suppose we intend to apply the Maximality criterion, then we havea total of nm2optimization programs to evaluate.As noted, our approach to strategy selection is to use a (consequentialist) backward induction process, rather than toresort to a variant of SPU or any other approximate scheme. The algorithm proceeds from the last decision node up tothe first, building the strategies during execution by selecting admissible actions for each configuration of pa(D), and thencombining only the selected actions. The subtle point is that, for indecision-prone criteria, we may have to consider morethan one admissible policy for a decision node D as long as the algorithm iterates. This approach can save computationsin two ways: (1) the computations are done locally (we do not need to consider all variables on the graph, thus, theoptimization programs are smaller), and (2) if the number of selected actions is smaller than n, then we reduce the numberof possible policies to be considered. Algorithm 6 summarizes this idea. Once the fact that we adopt a consequentialistposition is understood, the algorithm can be viewed as a mix of the algorithm for policy selection in standard influencediagrams (where consequentialism is natural) and our previous algorithms for decision trees.In Algorithm 6, we keep track of a list Si associated to decision node D i . The list Siis used to hold the set of(sub)strategies suggested by indecision-prone criteria. It contains all admissible substrategies, evaluated by criterion, rootedat D i , Si = {δD i , . . . , δDm}. The initialization in line 2 indicates that previous to the first iteration there is no substrategy. As1360D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–1365Algorithm 6: InfluenceDiagram_criterionInput: An influence diagram D as described in Section 5.1, with a set D.K of constraints on probability values.Output: A set of strategies selected by criterion.123456789m ← Number of decision nodes;Sm+1 ← ∅;for D i ∈ D, i ← m until 1 doGi ← Required variables to evaluate actions in D i ;Ai ← CombineActions(D i , Si+1);foreach configuration πk of pa(D i ) doKi ← GenerateConstraints(D i , Ai , πk, Gi , D.K );Admissible[πk] ← Criterion_criterion(Ai, Ki );Si ← CombineSubstrategies( Admissible, Si+1);10return Criterion_criterion(S1, K1);we have pointed out before, to evaluate an action in a given decision node, we do not need to consider all variables in thegraph. This is exactly what we do in line 4 of Algorithm 6. First we note that a utility node U is relevant to a decision D ifthere exists a directed path connecting D and U . Then we use a standard d-separation algorithm [63] to obtain the neededvariables, that is, the set of variables that are not d-separated from the set Ui of utility nodes relevant to D i , given thatthe set {D i, pa(D i)} are “observed” (the decision node is clamped to the selected action).6 The function CombineActionstakes the list of substrategies Si+1 and attaches an action of D i to each s ∈ Si+1. Suppose that in Example 9 there are twoadmissible policies in D2, then at decision node D1 the function CombineActions returns to Ai eight possible combinations.These combined actions will be evaluated by the criteria of choice before building the policies for decision node D i . We relyon the criteria for reducing the number of admissible policies. This is done in the inner loop.The function GenerateConstraints is responsible for creating the constraints on probabilities that are passed to the func-tion Criterion_criterion. This function must encode the state space, as done for decision trees, and then encode constraintson probability values based on the input constraints D.K . Differently from decision trees, the constraints in Ki must takeinto account the fact that lower/upper expectations are now: (1) restricted to variables in Gi ; and (2) conditional on a setof “observed” nodes {D i, pa(D i)}. Thus the new element here is that we must minimize/maximize conditional expectations;this is done by introducing an auxiliary variable z and a new constraint,(cid:3)Ui ,Gi \EiP (Gi)u(Ui) − z P (Ei) = 0,(5)where Ei denotes the set of nodes “observed” at D i . This constraint forces z to be the desired conditional expectation.Hence the inner loop in Algorithm 6 builds the optimization programs as in decision trees, using Expression (5) in symbolicform whenever necessary.The function CombineSubstrategies is responsible for building Si . It receives the set of admissible actions and the set ofsubstrategies S i+1, builds the possible policies for D i , and appends them to the substrategies in S i+1.The complexity of the algorithm obviously depends on the criteria of choice. For indecision-resistant criteria, we al-ways consider one optimal policy at each decision node and, consequently, we have only one strategy (similar to influencediagrams with precise probabilities). This is also the best case for indecision-prone criteria (when the criterion is very se-lective). The worst case happens when the criterion does not discard any action. This implies that we need to consider allpossible combination of policies as long as we proceed in a backward fashion, and at the first decision node we have thesame amount of strategies as a resolute decision maker.5.2. Examples and experimentsThe algorithm presented in the previous section has been implemented and run in several well-known examples. Mostof the implementation was coded in the Java language, with calls to optimization packages (the multilinear programmingpackage Multilin [17] and the CPLEX and Minos commercial packages respectively for linear and nonlinear optimization).Tests were run in a microcomputer with two dual-core processors and 4 GBytes of memory. We report the character ofselected strategies and the computational effort spent to select these strategies.We start with the classic oil wildcatter problem [56]; this is a small influence diagram, so we can indicate the steps ofthe algorithm in some detail.Example 10. The oil wildcatter problem is depicted in Fig. 9. An oil wildcatter must decide whether to drill or not to drill(decision D). The cost of drilling is $70K . If the decision is to drill, the soil may be soaking, wet or dry (with a return6 Due to their independence relations, influence diagrams and LIMIDs can be viewed as extended Bayesian networks [10,63,64]. Our framework can beviewed as an extended credal network, so d-separation applies.D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–13651361Amount of oil (O )Test (S)drywetsoakingnsoscs[0.60, 0.65][0.30, 0.35][0.10, 0.10][0.25, 0.30][0.10, 0.10][0.40, 0.40][0.25, 0.30][0.40, 0.45][0.45, 0.50]Fig. 9. Influence diagram for the oil wildcatter problem and probability intervals for seismic test given amount of oil.of $270K , $120K or $0 respectively). Suppose probabilities for the amount of oil (O ) are: P (O = soaking) ∈ [0.2, 0.2],P (O = wet) ∈ [0.3, 0.35], P (O = dry) ∈ [0.45, 0.5]. At the cost of $10K , the oil wildcatter can take seismic soundings of thesite. The result of this test (S) may be ns (no oil), os (some oil), cs (abundance of oil), with interval probabilities in Fig. 9. Ifthe test is not conducted, then P (S = s|O ) = 1 if {S = nt} and P (S|O ) = 0 otherwise, where nt is a special value of S thatindicates absence of test.To select strategies, we start with the decision D; node U 2 is required and all nodes but U 1 are returned by GetdCon-nected. The expected utility of not drilling is 0.00 regardless of S. The expected utility of drilling depends on S. To computethe lower expected utility of drilling given that {S = ns}, we must minimize auxiliary variable z subject to constraints onprobabilities and to(cid:3)OP (O )P (S = ns|O , T )u(D = yes, O ) − z P (S = ns) = 0.Running this and similar multilinear programs, we obtain:SE[D = yes|S]E[D = yes|S]nt20.0026.00ns−32.76−21.27os32.8650.00cs82.6191.29Using the Γ -Maximix criterion with η = 0.5 we find that the action not to drill is not admissible, except when theseismic test indicates ns. Thus we have the policy δ∗D (S) = yes otherwise. As the Γ -Maximixcriterion specifies only one policy in D, we have only two policies to analyze at T (δT = yes and δT = no). The overall utilityis given by the sum of U 1 and U 2; we then obtain E[T = yes] ∈ [−10K , −10K ] + [31.75K , 37.23K ] = [21.75K , 27.23K ] andE[T = no] ∈ [0, 0] + [20K , 26K ] = [20K , 26K ]. The selected action at T is to take the seismic test (δ∗= yes). The selectedT}. This strategy is also selected by the Γ -Maximin and Γ -Maximax criteria. With E-admissibility,strategy is swe obtain at D the same policy suggested by the Γ -Maximix criterion. At node T , we have two E-admissible policies,δT = yes and δT = no. The combination of optimal policies leads to two strategies with expected utilities respectively in[21.75K , 27.23K ] and [20.0K , 26.0K ]. Interval Dominance and Maximality select the same strategies as E-admissibility.D (S) = ns if {S = no} and δ∗∗ = {δ∗T , δ∗DThe next influence diagram we examine is the “Breeding Pigs” problem described by Lauritzen and Nilsson [44], andrepresented by the influence diagram in Fig. 10. A pig breeder is growing pigs for a period of four months and subsequentlyselling them. During this period the pig may or may not develop a certain disease (hi represents the pig’s health, healthy orill, in the ith month). Once a month, a doctor makes a test for the presence of the disease (ti represents the test’s results,disease free or otherwise), and the doctor may or may not treat the pig for the disease by injecting a certain drug (decisionnode di ). The utility nodes u1, u2, u3 represent the cost of treating the pig, and u4 represents the payoff for selling thepig. Additionally, we have: the price of a pig with disease is 300DKK (Danish kroner) and of a disease-free pig is 1000DKK(utility node u4); the cost of an injection is 100DKK (utility nodes u1, u2, u3); the test is correct when the pig is ill withprobability 0.80, and correct when the pig is healthy with probability 0.90 (chance nodes ti ); a healthy pig develops thedisease in the subsequent month with probability 0.20 without injection, whereas a healthy and treated pig develops thedisease with probability 0.10; an untreated pig that is unhealthy will remain so in the subsequent month with probability0.90, whereas the similar probability is 0.5 for an unhealthy pig that is treated (chance nodes h2, h3, h4). The decision makeris uncertain about the pig’s health in the first month (h1). In our experiments we assume two intervals for h1: the first oneis a small interval defined near the probability of the original problem ( P (ill) = [0.1, 0.2]); the second one is a large interval( P (ill) = [0.0, 0.5]).In this example the full previous treatment and test history are available when decisions are made. At decision node d3,there are 5 conditioning nodes. A policy at d3 specifies an action for 32 configurations (all five conditioning nodes havejust two possible values). An indecision-resistant criterion must run 64 inferences to find out the best policy, while anindecision-prone criterion may have to keep track of many incomparable substrategies. It is indeed possible that the largenumber of incomparable substrategies makes it impossible to finish the algorithm, as we will see in a moment.Now, consider a LIMID for the Breeding Pigs problem, where the decision maker does not remember past decisionsand results of tests from previous months (the decision maker remembers only the result of the test taken in the currentmonth). The resulting LIMID is also depicted in Fig. 10.1362D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–1365Fig. 10. Left: Influence diagram (with complete history) for the Breeding Pigs problem. Right: LIMID version for the Breeding Pigs problem.Table 1Experiments with the Breeding Pigs problem.P (h1 = ill)Criteria# of admissiblestrategies LIMIDsElapsed time (s)LIMIDs# of admissiblestrategies IDsElapsed time (s)IDs[0.1, 0.2][0.0, 0.5]Γ -MaximinΓ -MaximaxΓ -MaximixI. DominanceMaximalityE-admissibilityΓ -MaximinΓ -MaximaxΓ -MaximixI. DominanceMaximalityE-admissibility1112111118212.412.633.595.351.842.393.745.277.5723.423.764.141111622111––710.2011.4516.0389.9517.5923.5816.4832.2343.39––584.64Table 1 presents results for all criteria we have discussed. The column labeled # of admissible strategies reports the numberof strategies selected by our implementation, and the column labeled Elapsed time shows the average time of execution overthirty runs for each criterion. An interesting fact is that the smaller probability interval leads to smaller execution times:the sharper the probabilities, the smaller the number of incomparable substrategies to process. Another point to note isthat Interval Dominance and Maximality could not be run to termination for the larger probability interval, due to the hugenumber of strategies that these criteria fail to discard during execution. A curious fact is that E-admissibility does not crashthe system, and indeed leads to a relatively small number of selected strategies: even though E-admissibility seems morecomplex computationally, the fact that it is more selective than Interval Dominance and Maximality is quite valuable inpractice.Another curious fact in Table 1 is that, with the LIMID, Maximality and E-admissibility seem to be faster than theindecision-resistant criteria. This result can be traced to a few computational aspects that are not apparent from a superficialanalysis. As we can define the set of maximal/E-admissible actions without computing exact probability values (just find adistribution satisfying the constraints), we use a fast approximated solver in Minos to produce a preliminary selection ofactions. After this, we use the exact solver Multilin to compute lower/upper expectations. The approximated solver quicklygets close to the exact solution, so the overall computing time is greatly benefited. It is also possible to explain the weakshowing of Interval Dominance, as it requires the use of the exact solver to compute lower and upper expectations so as tocompare actions.7 It is noteworthy that Interval Dominance is a simple criterion to apply in single-stage decision problemswhile in sequential decision problems it faces difficulties due to its low selectivity.Our final example deals with a relatively large LIMID that has been proposed by de Campos and Ji [19] as a model forEffects-Based Operation planning (EBO). The LIMID is shown in Fig. 11; all variables are binary, and the decision nodeshave two possible actions (yes/no). The cost of actions, given by {U i}11is yes, then cost is 50 for U 3, 150for U 8, 80 for U 10, 100 for U 11 and 20 for the all others. The reward for achieving the main goal is 1000, while notachieving it costs 500 (utility node U H ). The chance nodes Ci represent the rate of success with an interval probabilityP (Ci = 1|D i = yes) ∈ [0.9, 1.0]. The chance nodes B j and Ak have probability 1 if all parents are positive, probability 0.5if only one parent is positive, and probability 0 otherwise. The probability of chance node G is 1 given that its parentsi=1, is: if D i7 If we use the solver Minos to find I-admissible actions, Interval Dominance takes 2.68 and 56.70 seconds in the LIMID and influence diagram respec-tively, when P (h1 = ill) ∈ [0.1, 0.2]; and it takes 9.45 seconds in the LIMID, and crashes with the influence diagram, when P (h1 = ill) ∈ [0.0, 0.5].D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–13651363Fig. 11. Influence diagram for the EBO problem.are positive, 0.6 if only one parent is negative, 0.3 if two parents are negative, and 0 otherwise. Decision nodes have noparents (an extreme LIMID), hence an arbitrary ordering of decision nodes is adopted when selecting strategies. Using theΓ -Maximin criterion, the selected strategy is to take action yes in all decision nodes except D5, D6, D7 and D8 (approxi-mately 40 seconds were taken to select it). The lower expected utility of this strategy is −55.28; the elapsed time to selectit was approximately 40 seconds. The only E-admissible strategy is to take action yes in all decision nodes except D 8 (ap-proximately 120 seconds were taken to select it). The expected utility of this strategy belongs to [68.97, 330]; note that theconsequentialist E-admissible strategy is always better than the consequentialist Γ -Maximin strategy.86. ConclusionThis paper has examined the selection of strategies in sequential decision making when preferences are partially ordered.In particular, we have focused on preference patterns that are encoded through a single utility and a set of probabilitymeasures. A partial order over strategies introduces subtle ingredients into the decision problem, as we have several criteriaof choice and behavioral norms, episodes of incoherent/inconsistent choice, and varying degrees of computational gain.We have tried to shed some light into these matters from a consequentialist perspective, and to present algorithms thatselect strategies by solving sequences of optimization programs. Most algorithms employ multilinear programming, andsome particular cases can be tackled by linear programming. Clearly, algorithms based on multilinear programming canbe adapted to handle interval-valued utility, as we then have products between probabilities and utilities that must beoptimized over; we have refrained from discussing interval-valued utility so as to limit the length of the paper.The current literature on sequential decision making with partially ordered preferences can be roughly divided in twostreams. The philosophical debate tends to favor abstract comparisons amongst criteria and norms, with little considerationof computational costs. On the other hand, if one looks at techniques that do involve decision making with partially orderedpreferences, such as nondeterministic planning and CP nets, and even the theory of LIMIDs, one finds detailed study ofcomputational costs but little attention to criteria, norms, and consistency. We hope that this paper strikes some neededbalance between conceptual discussion and computational development, and helps shorten some of the gaps between theseviewpoints. In particular we believe that the effect of the consequentialist perspective is currently not appreciated in theartificial intelligence literature, exactly where this norm is most adequate as one must deal with bounded agents.We can summarize our contributions as follows. We have first derived new algorithmic techniques for Interval Dom-inance and E-admissibility (using insights by Kyburg and Pittarelli [43], as done independently by Utkin and Augustin[72]). We also presented a brief analysis of incoherent choice with Interval Dominance. More importantly, we have stud-ied decision trees with partially ordered preferences, by presenting a consequentialist backward induction framework withmultilinear and linear programming instantiations, and by noting that different criteria do affect the computational prop-erties of backward induction. We have then applied these insights to influence diagrams, and actually to ordered LIMIDs,where the technology of credal networks (d-separation and multilinear programming) is used as much as possible. We have8 By brute-force enumeration of strategies, we find that there is an E-admissible strategy with lower expected utility 156.41 (and upper expected utility480.00); the consequentialist approach misses this possibility.1364D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–1365also presented experiments with a complete implementation of the algorithms for influence diagrams. Given the lack ofliterature on influence diagrams with indeterminacy and imprecision in probability values, our results are a first step inunderstanding this powerful but intricate model. We have noted already that recently Huntley and Troffaes [35] and deCampos and Ji [19] have presented specialized algorithms respectively for Maximality in decision trees and for Γ -Maximaxin influence diagrams, that can be more efficient than the algorithms discussed in this paper in particular problems.An important subject this paper has not tackled is Markov Decision Processes (MDPs). MDPs are probably the mostprominent model for sequential decision making under uncertainty in use in artificial intelligence today, and could likewisebenefit from partial preference orderings. It should be noted that the initial translation into the framework of MDPs isnot difficult, in fact there is a considerable amount of publications that deal with MDPs with sets of probability (usuallyreferred to as Markov Decision Processes with Imprecise Probabilities) [33,58,74]. The topic deserves an investigation ofits own; additionally, there are many questions that must be answered before partial preferences can be extensively usedin MDPs, such as how to deal with act-state dependence and how to choose between the set of incomparable choicessuggested by criteria such as Maximality and E-admissibility.We certainly leave many avenues for future exploration. A necessary next step is a detailed empirical characterizationof computational effort in solving decision trees and influence diagrams, including a comparison between multilinear andlinear programming schemes whenever the latter are possible. It would also be important to characterize the class ofinfluence diagrams that can be solved through reduced programs (that is, programs that deal with small subsets of chanceand decision nodes, passing back interval-valued expected utility). There are also conceptual questions that deserve furtherscrutiny, as there are other criteria of choice and behavioral norms besides the ones investigated in this paper. Even withinthe scope of criteria discussed in this paper, there are questions to answer. For instance: we have produced algorithms thatcompute all E-admissible strategies; perhaps in practice one should be content with just one E-admissible strategy?References[1] M. Allais, O. Hagen, Expected Utility Hypotheses and the Allais Paradox, D. Reidel Publishing Company, Dordrecht, Holland, 1979.[2] R.J. Aumann, Utility theory without the completeness axiom, Econometrica 30 (3) (July 1962) 445–461.[3] J.O. Berger, Statistical Decision Theory and Bayesian Analysis, Springer, New York, 1985.[4] D. Bertsimas, J.N. Tsitsiklis, Introduction to Linear Optimization, Athena Scientific, Belmont, Massachusetts, 1997.[5] L. Blume, A. Brandeburger, E. Dekel, Lexicographic probabilities and choice under uncertainty, Econometrica 59 (1) (January 1991) 61–79.[6] B. Bonet, R. Givan, in: 5th International Planning Competition: Non-deterministic Track, call for participation, 2005.[7] C. Boutilier, R.I. Brafman, H.H. Hoos, D. Poole, CP-nets: A tool for representing and reasoning with conditional ceteris paribus preference statements,Journal of Artificial Intelligence Research 21 (2004) 135–191.[8] J. Breese, K. Fertig, Decision making with interval influence diagrams, in: Sixth Conference on Uncertainty in Artificial Intelligence, Elsevier Science,New York, 1990, pp. 122–129.[9] K. Bykvist, Time-partial morality and dynamic choice, in: W. Rabinowicz (Ed.), Value and Choice – Some Common Themes in Decision Theory andMoral Philosophy, Lund Philosophy Reports, 2000, pp. 53–64.[10] G.F. Cooper, A method for using belief networks as influence diagrams, in: Proceedings of the 4th Conference on Uncertainty in Artificial Intelligence,Minneapolis, 1988, pp. 55–63.[11] I. Couso, S. Moral, P. Walley, A survey of concepts of independence for imprecise probabilities, Risk, Decision and Policy 5 (2) (2000) 165–181.[12] F.G. Cozman, Separation properties of sets of probabilities, in: C. Boutilier, M. Goldszmidt (Eds.), Proceedings of the 16th Conference on Uncertainty inArtificial Intelligence, Morgan Kaufmann, San Francisco, July 2000, pp. 107–115.[13] F.G. Cozman, Graphical models for imprecise probabilities, International Journal of Approximate Reasoning 39 (2–3) (June 2005) 167–184.[14] M. Danielson, L. Ekenberg, Computing upper and lower bounds in interval decision trees, European Journal of Operational Research 181 (2) (September2007) 808–816.[15] M. Danielson, L. Ekenberg, J. Johansson, A. Larsson, The DecideIT decision tool, in: J.-M. Bernard, T. Seidenfeld, M. Zaffalon (Eds.), Proceedings of the3rd International Symposium on Imprecise Probabilities and Their Applications, Carleton Scientific, Lugano, Switzerland, July 2003, pp. 204–217.[16] C.P. de Campos, F.G. Cozman, Inference in credal networks using multilinear programming, in: Proceedings of the 2nd European Starting AI ResearcherSymposium, IOS Press, Valencia, Spain, August 2004, pp. 50–61.[17] C.P. de Campos, F.G. Cozman, The inferential complexity of Bayesian and credal networks, in: Proceedings of the 9th International Joint Conference onArtificial Intelligence, Edinburgh, Scotland, UK, July–August 2005, pp. 1313–1318.[18] C.P. de Campos, F.G. Cozman, Inference in credal networks through integer programming, in: International Symposium on Imprecise Probability: Theo-ries and Applications, Prague, 2007, pp. 145–154.[19] C.P. de Campos, Q. Ji, Strategy selection in influence diagrams using imprecise probabilities, in: Proceedings of the 24th Conference on Uncertainty inArtificial Intelligence, Helsinki, Finland, July 2008, pp. 121–128.[20] D. Ellsberg, Risk, ambiguity, and the Savage axioms, The Quarterly Journal of Economics 75 (4) (1961) 643–669.[21] N. Etchart, Adequate moods for Non-EU decision making in a sequential framework, Theory and Decision 52 (February 2002) 1–28.[22] K. Fertig, J. Breese, Probability intervals over influence diagrams, IEEE Transactions on Pattern Analysis and Machine Intelligence 15 (3) (1993) 280–286.[23] P.C. Fishburn, Utility Theory for Decision Making, Kriefer Publishing Company, New York, 1970.[24] G. Georgakopoulos, D. Kavvadias, C.H. Papadimitriou, Probabilistic satisfiability, Journal of Complexity 4 (1) (March 1988) 1–11.[25] I. Gilboa, D. Schmeidler, Maxmin expected utility with non-unique prior, Journal of Mathematical Economics 18 (2) (1989) 141–153.[26] F.J. Giron, S. Rios, Quasi-Bayesian Behaviour: A More Realistic Approach to Decision Making? University Press, Valencia, 1980.[27] T. Hailperin, Best possible inequalities for the probability of a logical function of events, American Mathematical Monthly 72 (1965) 343–359.[28] P.J. Hammond, Changing tastes and coherent dynamic choice, The Review of Economic Studies 43 (1) (1976) 159–173.[29] P.J. Hammond, Consequentialism and the independence axiom, in: B.R. Munier (Ed.), Risk, Decision and Rationality (Proceedings of the 3rd InternationalConference on the Foundations and Applications of Utility, Risk and Decision Theories), Dordrecht, Holland, 1988, pp. 503–516.[30] P.J. Hammond, Orderly decision theory: a comment on Professor Seidenfeld, Economics and Philosophy 4 (1988) 272–297.[31] P. Hansen, B. Jaumard, Probabilistic satisfiability, Tech. Rep. G-96-31, Les Cahiers du GERAD, École Polytechique de Montréal, 1996.[32] P. Hansen, S. Perron, Merging the local and global approaches to probabilistic satisfiability, International Journal of Approximate Reasoning 47 (2)(2008) 125–140.D. Kikuti et al. / Artificial Intelligence 175 (2011) 1346–13651365[33] D. Harmanec, Generalizing Markov decision processes to imprecise probabilities, Journal of Statistical Planning and Inference 105 (1) (June 2002)199–213.[34] R.A. Howard, J.E. Matheson, Influence diagrams, Decision Analysis 2 (3) (2005) 127–143.[35] N. Huntley, M. Troffaes, An efficient normal form solution to decision trees with lower previsions, in: International Workshop on Soft Methods inProbability and Statistics, 2008, pp. 419–426.[36] L. Hurwicz, A class of criteria for decision-making under ignorance, Cowles Comission Paper 356, 1951.[37] H. Itoh, K. Nakamura, Partially observable Markov decision processes with imprecise parameters, Artificial Intelligence 171 (8–9) (2007) 453–490.[38] J.-Y. Jaffray, Rational decision making with imprecise probabilities, in: G.D. Cooman, F.G. Cozman, S. Moral, P. Walley (Eds.), Proceedings of the 1stInternational Symposium on Imprecise Probabilities and Their Applications, Ghent, Belgium, June 1999, pp. 183–188.[39] B. Jaumard, P. Hansen, M.P. de Aragão, Column generation methods for probabilistic logic, ORSA Journal on Computing 3 (2) (1991) 135–148.[40] D. Kahneman, A. Tversky, Prospect theory: An analysis of decisions under risk, Econometrica 47 (1979) 262–291.[41] D. Kikuti, F.G. Cozman, Influence diagrams with partially ordered preferences, in: 3rd Multidisciplinary Workshop on Advances in Preference Handling,2007.[42] D. Kikuti, F.G. Cozman, C.P. de Campos, Partially ordered preferences in decision trees: Computing strategies with imprecision in probabilities, in:Workshop on Advances in Preference Handling, Edinburgh, United Kingdom, July 2005, pp. 118–123.[43] H.E. Kyburg Jr., M. Pittarelli, Set-based Bayesianism, IEEE Transactions on Systems, Man and Cybernetics, Part A 26 (3) (1996) 324–339.[44] S.L. Lauritzen, D. Nilsson, Representing and solving decision problems with limited information, Management Science 47 (9) (2001) 1235–1251.[45] I. Levi, On indeterminate probabilities, The Journal of Philosophy 71 (1974) 391–418.[46] I. Levi, The Enterprise of Knowledge, The MIT Press, Massachusetts, 1980.[47] R.D. Luce, H. Raiffa, Games and Decisions, Wiley, New York, 1957.[48] C. Luo, C. Yu, J. Lobo, G. Wang, T. Pham, Computation of best bounds of probabilities from uncertain data, Computational Intelligence 12 (4) (1996)541–566.[49] M.J. Machina, Dynamic consistency and non-expected utility models of choice under uncertainty, Journal of Economic Literature 27 (4) (December1989) 1622–1688.[50] E.F. McClennen, Rationality and Dynamic Choice: Foundational Explorations, Cambridge University Press, Cambridge, 1990.[51] E.F. McClennen, Pragmatic rationality and rules, Philosophy and Public Affairs 26 (3) (1997) 210–258.[52] T.D. Nielsen, J.-Y. Jaffray, An operational approach to rational decision making based on rank dependent utility, 2001, unpublished manuscript availableon http://www.cs.aau.dk/~tdn/papers/nielsen-jaffray-01.pdf.[53] T.D. Nielsen, F.V. Jensen, Welldefined decision scenarios, in: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, MorganKaufmann, Stockholm, Sweden, July 1999, pp. 502–511.[54] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann Publishers, Los Altos, CA, 1988.[55] J.C. Quiggin, A theory of anticipated utility, Journal of Economic Behavior & Organization 3 (4) (December 1982) 323–343.[56] H. Raiffa, Decision Analysis: Introductory Lectures on Choices under Uncertainty, Addison-Wesley, Massachusetts, 1968.[57] P.A. Samuelson, Consumption theory in terms of revealed preference, Econometrica 15 (1948) 243–253.[58] J.K. Satia, R.E. Lave Jr., Markovian decision processes with uncertain transition probabilities, Operations Research 21 (3) (May–June 1973) 728–740.[59] M.J. Schervish, T. Seidenfeld, J.B. Kadane, I. Levi, Extensions of expected utility theory and some limitations of pairwise comparisons, in: Proceedingsof the 3rd International Symposium on Imprecise Probabilities and Their Applications, Lugano, Switzerland, July 2003, pp. 496–510.[60] T. Seidenfeld, A contrast between two decision rules for use with (convex) sets of probabilities: Γ -Maximin versus E-Admissibility, Synthese 140 (1–2)(May 2004) 69–88.[61] T. Seidenfeld, M.J. Schervish, J.B. Kadane, Decisions without ordering, in: W. Sieg (Ed.), Acting and Reflecting, Kluwer Academic Publishers, Dordrecht,1990, pp. 143–170.[62] T. Seidenfeld, M.J. Schervish, J.B. Kadane, A representation of partially ordered preferences, Annals of Statistics 23 (6) (December 1995) 2168–2217.[63] R. Shachter, Bayes-Ball: the rational pastime (for determining irrelevance and requisite information in belief networks and influence diagrams), in:Proceedings of the 14th Annual Conference on Uncertainty in Artificial Intelligence (UAI-98), Morgan Kaufmann, San Francisco, CA, 1998, pp. 480–487.[64] R. Shachter, Efficient value of information computation, in: Proceedings of the 15th Annual Conference on Uncertainty in Artificial Intelligence (UAI-99),Morgan Kaufmann, San Francisco, CA, 1999, pp. 594–601.[65] H.D. Sherali, C.H. Tuncbilek, A global optimization algorithm for polynomial programming problems using a reformulation–linearization technique,Journal of Global Optimization 2 (1) (March 1992) 101–112.[66] H.A. Simon, A behavioral model of rational choice, The Quarterly Journal of Economics 69 (1) (1955) 99–118.[67] R. Strotz, Myopia and inconsistency in dynamic utility maximization, The Review of Economic Studies 23 (3) (1956) 165–180.[68] J.A. Tatman, R.D. Shachter, Dynamic programming and influence diagrams, IEEE Transactions on Systems, Man and Cybernetics 20 (2) (1990) 365–379.[69] F.W. Trevizan, F.G. Cozman, L.N. de Barros, Planning under risk and Knightian uncertainty, in: International Joint Conference on Artificial Intelligence,2007, pp. 2023–2028.[70] M.C.M. Troffaes, Decision making with imprecise probabilities: A short review, in: F. Cozman (Ed.), Society for Imprecise Probability Theory and Appli-cations Newsletter, Manno, Switzerland, December 2004, pp. 4–7.[71] M.C.M. Troffaes, Decision making under uncertainty using imprecise probabilities, International Journal of Approximate Reasoning 45 (1) (2007) 17–29.[72] L.V. Utkin, T. Augustin, Powerful algorithms for decision making under partial prior information and general ambiguity attitudes, in: Proceedings ofthe 4th International Symposium on Imprecise Probabilities and Their Applications, Pittsburgh, Pennsylvania, July 2005, pp. 349–358.[73] P. Walley, Statistical Reasoning with Imprecise Probabilities, Chapman and Hall, London, 1991.[74] C.C. White III, H.K. El-Deib, Markov decision processes with imprecise transition probabilities, Operations Research 42 (4) (July–August 1994) 739–749.