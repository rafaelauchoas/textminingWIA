Computers in Biology and Medicine 104 (2019) 339–351Contents lists available at ScienceDirectComputers in Biology and Medicinejournal homepage: www.elsevier.com/locate/compbiomedRethinking multiscale cardiac electrophysiology with machine learning andpredictive modellingChris D. Cantwella,b,∗Charles Houstona,c, Rasheda A. Chowdhurya,c, Fu Siong Nga,c, Anil A. Bharatha,d,Nicholas S. Petersa,ca ElectroCardioMaths Group, Imperial College Centre for Cardiac Engineering, Imperial College London, London, UKb Department of Aeronautics, Imperial College London, South Kensington Campus, London, UKc National Heart and Lung Institute, Imperial College London, South Kensington Campus, London, UKd Department of Bioengineering, Imperial College London, South Kensington Campus, London, UK, Yumnah Mohamieda,c, Konstantinos N. Tzortzisa,c, Stef Garastoa,d,TA R T I C L E I N F OA B S T R A C TKeywords:Cardiac electrophysiologyCardiac arrhythmiaElectrogramMachine learningPredictive modellingDeep learningWe review some of the latest approaches to analysing cardiac electrophysiology data using machine learning andpredictive modelling. Cardiac arrhythmias, particularly atrial fibrillation, are a major global healthcare chal-lenge. Treatment is often through catheter ablation, which involves the targeted localised destruction of regionsof the myocardium responsible for initiating or perpetuating the arrhythmia. Ablation targets are either ana-tomically defined, or identified based on their functional properties as determined through the analysis ofcontact intracardiac electrograms acquired with increasing spatial density by modern electroanatomic mappingsystems. While numerous quantitative approaches have been investigated over the past decades for identifyingthese critical curative sites, few have provided a reliable and reproducible advance in success rates. Machinelearning techniques, including recent deep-learning approaches, offer a potential route to gaining new insightfrom this wealth of highly complex spatio-temporal information that existing methods struggle to analyse.Coupled with predictive modelling, these techniques offer exciting opportunities to advance the field and pro-duce more accurate diagnoses and robust personalised treatment. We outline some of these methods and il-lustrate their use in making predictions from the contact electrogram and augmenting predictive modellingtools, both by more rapidly predicting future states of the system and by inferring the parameters of these modelsfrom experimental observations.1. IntroductionCardiac arrhythmias, particularly atrial fibrillation (AF), are a majorglobal healthcare challenge in the developed world. Arrhythmias de-scribe the abnormal and self-perpetuating propagation of action po-tentials (AP) within the myocardium. Their initiation and maintenanceare incompletely understood and this has hindered the development ofeffective and reliable therapy. Treatment for AF is often through ca-theter ablation, where the regions of myocardium determined to beresponsible for initiating or perpetuating the disturbance are targetedand made electrically inactive through the localised application ofradio-frequency energy or freezing. For paroxysmal AF, catheter abla-tion delivers relatively good outcomes, with success rates in the regionof 80–90 percent [1]. However, outcomes of catheter ablation in pa-tients with persistent AF remain disappointing, and is effective in onlyapproximately 50 percent of patients, despite all forms of adjunctiveablation strategies [2].Identifying the critical sites responsible for abnormal AF main-tenance has been a major focus of research, with a number of drivingmechanisms, including rotors [3], multiple wavelets [4] and epi-endodisassociation [5], being proposed. Recent clinical studies, such as theCONFIRM study [6], have tested the new approaches of catheter ab-lation by targeting the foci of rotational drivers, with initially promisingresults showing that 86% of 101 cases achieved AF termination orslowing. However, subsequent studies suggest more moderate outcomeswith Steinberg et al. [7] reporting only 4.7% of 47 cases achieved AFtermination, while 60% documented recurrence within 12 months. Theefficacy of this technique may be in part due to the poor spatial re-solution of the global mapping catheter used [8]. Techniques involvingthe targeting of complex fractionated atrial electrograms (CFAE) [9],∗Corresponding author. Department of Aeronautics, Imperial College London, South Kensington Campus, London, UK.E-mail address: c.cantwell@imperial.ac.uk (C.D. Cantwell).https://doi.org/10.1016/j.compbiomed.2018.10.015Received 29 June 2018; Received in revised form 4 October 2018; Accepted 14 October 20180010-4825/ © 2018 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/BY/4.0/).C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351high dominant frequency (DF) [10] and singularities identified duringphase mapping [11] have each been used as strategies for terminatingarrhythmias. However, none of these adjunctive ablation strategieshave been shown to add any value to the conventional approach ofelectrically isolating the pulmonary veins [2]. Part of the reason for thismay be that they each discard a large proportion of the informationcontent of the acquired electrogram signals or their spatio-temporalassociation during analysis. Additionally, not all identified sites may becritical to the perpetuation of the arrhythmia, leading to excessive ab-lation. The complexity of the underlying electro-architecture of myo-cardium therefore requires a more sophisticated, personalised andmulti-faceted approach to address the challenge of treating AF.The principle data modality used clinically for the treatment of AF isthe contact electrogram, which arises from the superposition of electricfields induced by charged ions moving across cell membranes in themyocardium. It is the electrical signature of action potential propaga-tion through tissue which implicitly encodes the functional and struc-tural characteristics of the local substrate. The electrogram thereforeprovides a wealth of information which is rarely fully utilised in currentclinical practice. Electrograms are normally only broadly categorised bybinary descriptors – such as simple or complex, early or late [12,13],fractionated or non-fractionated – with much of the signal content ef-fectively discarded. Despite a number of studies based on interpretingclinical electrogram data [14,15], these do not investigate how elec-trogram morphology is influenced by individual electro-architecturalfactors. Our knowledge about the direct effects of electrical remodellingon electrogram morphology is consequently poor, considering thenumber of these abnormalities related to cardiac diseases [16]. Lever-aging the electrogram to infer electroarchitectural properties of themyocardium may therefore provide new direct insight in locating cri-tical sites for ablation.Multiple concurrently recorded electrograms may be combined toevaluate the spatio-temporal propagation patterns occurring in thetissue. This activity can also be inferred from the surface of the body[17]. More recently, predictive modelling of action potential propaga-tion is emerging as a potential tool for personalised testing and opti-misation of interventions [18], but this technology is heavily dependenton the accuracy of the underlying calibration of parameters. This canonly be achieved by fully leveraging the huge wealth of informationnow available clinically. The data science revolution in the form ofsophisticated machine learning algorithms and increasing availabilityof computing power, opens up possibilities to manage this data over-load, both in terms of learning from the data, inferring model para-meters and consequently making increasingly accurate predictions.1.1. Machine learning in cardiac electrophysiologyMachine learning describes a class of algorithms which learn modelparameters from a set of training data (for which outcomes may, or maynot, be known) with the purpose of accurately predicting outcomes forpreviously unseen data. Training data that includes associated outcomelabels can be used for supervised learning in which the algorithm usesthis knowledge to directly improve its prediction. In contrast, un-supervised learning seeks patterns in the data with more limited gui-dance, of which clustering is a common example. Although there isconsiderable overlap, machine learning methods are considered todiffer from more conventional statistical modelling, such as regression,in that they are more concerned with the predictive accuracy of theresulting model rather than the ability to explain the reasoning behindits parameters and determining concrete relationships between thedata. The high accuracy of some of the more recent machine learningmethods – which are virtually impossible to analyse analytically – hasjustified this lack of transparency.All machine learning algorithms seek some form of mapping thatmodels the relationship between input data and outcome. In an abstractcontext, we suppose that we have a model f, governed by one or moreparameterslation, which maps an inputto some output, under the re-(1)The form and dimensions ofin Equation (1) are a functionandof the particular problem under consideration. For example, may be alarge one-dimensional vector (time-series) in the case of a music-clas-sification problem, or a two-dimensional image in the case of objectrecognition. For regressions, the output may be a prediction of thedependent quantity, while for classification problems,is usually alabel which assigns the corresponding input to a single class. The size ofdepends on both the problem and also on the chosen model. For ex-ample, for a linear regression between two variables would consist ofonly two values (namely the slope and intercept), while for a many-layered deep neural network with high-dimensional input data, the sizeof may be ofor more.Broadly speaking, the process of training a supervised machinesuch that, for some set oflearning algorithm is the notion of seekingtraining input data, a given lossfunction is minimised. While there are a number of loss functions, eachwith their own properties [19], a simple loss function might be theloss function which computes the differences between the predictedoutputs and the actual outputs and is given bywith corresponding outcomes(2)If sufficient (and suitable) training data are used with an appro-priate model, the expectation is that the model will then correctlypredict the outcomes for other inputs which did not form part of thesedata.Supervised machine learning is increasingly being used in medicine[20]. One area of cardiac electrophysiology in which machine learninghas become particularly prevalent to date is the analysis of the Elec-trocardiogram (ECG), in part due to its wide availability and its po-tential to conveniently provide important information about cardiacfunction without intervention. There now exists a substantial body ofliterature on the application of machine learning tools to classify ECGs.A review of some of the earlier work is given by Ref. [21]. The recentPhysioNet challenge to classify single-lead ECG segments into four ca-tegories (sinus rhythm, AF, other rhythm or too noisy) has catalyseddevelopments in this area [22]. Most approaches require some form ofpreprocessing of the signal, including de-noising and correcting forbaseline wander. While convolutional neural networks [23–30] andrecurrent neural networks [23,31] are gaining popularity, many studiesstill achieved accurate classification results using other algorithms suchas ensembles of decision trees (random forests) [32,33], multi-levelbinary classifiers [34] and least-squares support vector machine clas-sifiers [35]. The use of these approaches in combination also providesaccurate classification [24,31]. Recently, online real-time feature ex-traction and classification of ECGs using machine learning is beingexplored [36] and similar approaches are being used to diagnose morespecific cardiac abnormalities [37].In contrast, relatively little attention has been given to applyingmachine learning to make predictions from the contact intracardiacelectrogram, or to predict the spatio-temporal patterns of activation inmyocardium. Recently, there have been studies to characterise AF usingin silico or clinical contact electrograms [14,15], as well as for the au-tomated location of in silico re-entrant drivers using electrograms [38].1.2. Predictive numerical modellingNumerical modelling assumes the system under observation obeysparticular physical laws, known a priori and often represented in theform of partial differential equations, which are used to predict thefuture state of the system given an initial state. These equations often340xy=xyf(,).xyxyy(10)6Ox{}y{}1L==Nfxy||(,)||.iii11C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351contain a number of parameters, which are estimated from experi-mental observations or experience.While predictive modelling has advanced significantly in the field ofcardiac electrophysiology for the past decade, only recently have thenumerical methods and clinical imaging technologies improved suffi-ciently to allow viable predictions to be made on anatomically accurategeometries [39,40]. However, challenges still remain in how to accu-rately personalise and validate these models, as well as how to safelyincorporate them into clinical practice.1.3. OutlineIn this review, we describe some of the opportunities machinelearning can provide in the field of cardiac electrophysiology. We il-lustrate these through examples as well as discuss their potential impacton arrhythmia management. We begin with the contact electrogram thedata modality on which much of modern clinical electrophysiology isbased. We introduce machine learning approaches to analysing andclassifying these signals, contrasting both feature-based methods anddeep neural networks, and show how they can be used to potentiallyelucidate a wealth of electro-architectural information about the myo-cardial substrate. We then discuss recent advances by our group inmodelling action potential propagation and how machine learningmight supplement and extend these methods to improve our ability tocreate personalised models which can be used on clinically relevanttimescales.2. Feature-based classifiersFeatures describe characteristics of a process being observed. Theyare often represented in a numerical form and together form a featurevector. A feature-based machine learning algorithm then uses thesefeature vectors as input during both training and prediction. The se-lection of informative features is critical to the effectiveness of a ma-chine learning algorithm to predict the correct output label. When alarge number of features are available algorithms may struggle togeneralise due to redundancy of information between features. Featureselection algorithms can alleviate this issue by selecting a subset offeatures which promote learning and improve the ability of the algo-rithm to make accurate predictions. The identification of which featuresare important in specific situations may also generate hypotheses tomotivate further investigation of mechanistic links.When the output label is one of a finite set of possible discrete va-lues, the algorithm is termed a classifier. In the simplest case of binaryclassification, the accuracy of a learning algorithm may be char-acterised by a number of statistical measures. Sensitivity describes thepercentage of positive outcomes that are predicted as positive, whilespecificity captures the proportion of negative outcomes predicted asnegative. Positive (and negative) predictive value instead captures theproportion of positive (and negative) predictions, which are truly po-sitive (and negative).A broad range of feature-based classifiers for supervised machinelearning exist, and we refer the reader to previously published com-prehensive reviews for specific details [41,42]. Both linear and non-linear classifiers map the input features to a set of classes d using aweighted sum,Several approaches may be used to determine the weights of linearclassifiers. Linear discriminant analysis [44] seeks weights which bestof different classes. Support vector machines (SVMs)separate inputs[45] instead seek to maximise the margin between a hyperplane andthe two data classes it separates. The k-nearest neighbour classifier is anon-linear classifier which uses the classes of the nearest trainingsamples to predict the classification for unseen samples [46]. Decisiontrees [42] approach the classification problem feature-by-feature withbranches in the tree representing different values a feature can assume.The leaves of the tree denote the final classification.The performance of the above predictors can often be improvedusing the method of bootstrap aggregation, or bagging [47]. Rather thantraining a single predictor on a training dataset, a number of trainingdatasets are generated by drawing observations at random but withreplacement and predictors are trained on each of these bootstrap da-tasets. When making a prediction, the results of these predictors areaggregated – usually by voting when performing classification – to formthe final predictor. Random forests [48] extend the bootstrap ag-gregation of decision trees by additionally selecting random subsets offeatures when deciding how to split at each node. These approacheshelp to overcome the problem of over-fitting often present with deci-sion trees where they fail to generalise.2.1. Application to electrogram classificationWe explore the use of supervised machine learning to classify in-dividual electrograms based on the presence of cellular abnormalities.For initial proof of concept we use signals acquired from cell mono-layers in culture. While distinctive from clinical electrograms, theyenable us to assess the capabilities of these algorithms in a controlledcontext and ensure signals can be labelled accurately. We investigatedthe hypothesis that controlled functional modulations of the mono-layers can be accurately and precisely predicted from the recordedunipolar electrogram morphology using supervised machine learningmethods. In particular, we sought the classification of electrical signalsaccording to pharmacological gap junction uncoupling.2.1.1. Data acquisition and pre-processingElectrogram recordings were acquired as previously described [49].In brief, cell monolayers of neonatal rat ventricular myocytes (NRVMs)were seeded onto five microelectrode arrays (MEA), each consisting of60 electrodes (MultiChannel Systems, Reutlingen, Germany). Ten-second recordings were made at a sampling frequency of 25 kHz whilepacing from one edge, before and after administration of 40 μM car-benoxolone (CBX) to increase gap junction uncoupling. No signal fil-tering was applied during data acquisition. All animal procedures wereconducted according to the standards set by the EU Directive 2010/63/EU.Pacing artefacts were removed by approximating the exponentiallydecaying stimulus deflection with a rational polynomial and sub-tracting it from the recorded signals. The dataset was further curated toremove recordings from electrodes where no further deflections werepresent. This resulted in 485 control electrograms and 471 electrogramsafter treatment with CBX. The dataset was partitioned into a trainingdataset and a testing dataset. The testing dataset consisted of all elec-trograms recorded from one plate (90 control and 104 CBX electro-grams). The training dataset consisted of the remaining four plates (395control and 367 CBX electrograms). Examples of electrograms from thecontrol and CBX classes are shown in Fig. 1.learnt during training. In the linear casewith the weights,.The function f maps the result of the sum onto the different classes andmay be a simple threshold function in the case of a binary classification,or probability densities more generally. While in general not as accurateas non-linear classifiers, linear classifiers are typically faster and so maybe more effective in time-critical applications [43].2.1.2. Feature extractionThe high sampling rate of the electrograms would result in veryhigh-dimensional input data if used directly. We mapped each signalonto a pre-defined feature space of much lower dimension. Each elec-trogram recording was represented by a fixed set of 27 time-, fre-quency- and morphological-based features, extracted from the signal341=dfwxx()(())iiiwi=xx()xC.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351Fig. 2. 3D scatter plot of the most relevant features, normalised in the interval[0,1], as determined by SFS. No single feature clearly discriminates between thecontrol and carbenoxolone classes.Table 1Performance of classification training using the Bagging Ensemble method andevaluation of the subsequent prediction model on the test dataset.Classification training (734EGMs)Prediction model testing(194 EGMs)SensitivitySpecificityPositive predictivevalue98.1%98.3%98.4%Negative predictive97.7%valueError rate1.9%96.7%84.4%83.8%96.8%10%Fig. 3. Confusion matrix comparing the predictability of classes using thetraining dataset. Diagonal cells (green) show the percentage of electrogramsthat were correctly classified.electrograms. It achieved a 96.7% sensitivity, 84.4% specificity, 83.8%positive predictive value and 96.8% negative predictive value, in-dicating the model has generalised successfully.The computational time of the training process was also measured.Calculation of the electrogram features was the dominant cost, at 18.845.2 seconds per electrogram. The time taken for the bagging en-semble method to train a model using the feature vectors was 3.250.13 seconds.3. Convolutional neural networksOne of the limitations of conventional machine-learning techniques342Fig. 1. Examples of 20 ms segments of electrogram recordings from the Controland CBX groups, after removal of the stimulus artefact.using a custom-written algorithm (Matlab R2017b). Details of thesefeatures are provided in the Supplementary Material. SequentialForward Selection (SFS) [50] was used to select a subset of these fea-tures which were sufficient to differentiate the control and CBX classes.In brief, SFS is a bottom-up approach to choosing discriminatory fea-tures. Starting with an empty feature set, the algorithm sequentiallyadds features from the candidate set of features which maximises agiven objective function, until the addition of further features providesno improvement. Classification accuracy is used as the objective func-tion. Feature selection indicated that only three of the 27 featuresconsidered were sufficient to distinguish the control and CBX classes:electrogram amplitude, standard deviation of the autocorrelation functionand the scale with minimum energy in the continuous wavelet transformof the signal. The set of values from the selected electrogram char-acteristics form the feature vector for that electrogram. These featurevectors were subsequently used to train the classifier.The bootstrap aggregating (or bagging) ensemble tree method [47]was applied during both feature selection and classification training.2.1.3. ResultsFig. 2 shows the distribution of all electrogram feature vectors fromthe training dataset in the corresponding three-dimensional featurespace. It is evident that no single feature alone effectively discriminatesbetween the two classes.Validation was performed on the training data using ten-fold cross-validation. A total of 30 decision trees, with a leaf size of one, were usedfor the bagging ensemble method. The performance characteristics ofthe trained model are given in Table 1. A specificity of 98.1% wasachieved on the training data when using only the three features chosenby SFS. This is illustrated by the confusion matrix shown in Fig. 3, whichcompares predicted class against true class. This indicates the modelwas capable of accurately distinguishing the classes. The model per-formance was then measured using the unseen test dataset of 194±±C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351is their inability to be applied directly to high-dimensional data, ne-cessitating a transformation into a suitable feature-space representationthat captures characteristics of the data relevant for discrimination,while remaining invariant to irrelevant aspects. As illustrated in Section2.1.2, extracting these features often requires significant domain-spe-cific knowledge and expertise in order to hand-engineer suitable algo-rithms, and thus produce an informative representation that supportsdiscrimination. This process can, however, be circumvented if the fea-ture-extraction step is automated.Representation learning [51] encompasses a set of techniqueswithin the field of machine learning which enable a model to auto-matically learn and discover for itself discriminating features directlyfrom the raw observational data. Among the most popular of currenttechniques are layered artificial neural networks, which take inspira-tion from neuroscience. They are composed of artificial neurons (sim-plified versions of biological neurons) arranged in layers, where theneurons in one layer are connected to many, if not all, of the neurons inthe subsequent layer. The artificial neuron is a non-linear mapping froman input value to an output value. The output values from all theneurons in one layer are each multiplied by adjustable parameters,called weights, to form a weighted sum as input to one individualneuron in the subsequent layer. A neural network is, therefore, acomplex system of weighted non-linear functions nested within eachother and it is these weights that must be learnt in order for the networkto accurately map raw input data to a desired output.Neural networks learn their own weights during training: initiallythe weight values are randomly selected and thus when a model is giveninput it is initially highly unlikely to predict the correct output label.During training, the model is shown raw input data and the associatedlabel or output value. For each input example the model makes a pre-diction based on the current weight values, and the error between theprediction and the true desired output is measured. The weights arethen modified in order to minimise this error via a process called back-propagation, the details of which are provided in Ref. [52]. An epoch isdefined as a complete pass over the training data. Unlike the dis-criminant classifier of Section 2.1 which only needs one pass, neuralnetworks benefit from multiple passes over the training data. Withsufficient training data and sufficient iterations (epochs) over all thedata the weights converge onto values that enable the model to makeaccurate predictions for the training dataset. The trained model issubsequently tested on a validation dataset it has never before seen inorder to measure its predictive power.The weights of a neural network can be seen as the features of thelearnt representation, automatically discovered without the need formanually-designed feature detectors. When the network contains sev-eral layers in between the input layer and the final output layer –known as hidden layers – it is referred to as a deep network, from whichthe term deep-learning arises. These models are therefore representa-tion-learning methods with multiple non-linear layers, each trans-forming the representation, beginning with the raw input, into in-creasingly more discriminative representations.Today, deep-learning techniques provide state-of-the-art solutionsin the fields of object recognition [53,54] and detection [55], speechrecognition [56] and natural language processing [57,58], and are in-creasingly being used in other domains such as genomics [59] andchallenging segmentation problems required for geometric reconstruc-tion in biomedical imaging [60]. Recently, studies have applied deepneural networks to the ECG signal [23–30]. These studies have all madeuse of convolutional neural networks (or convnets).Fully-connected neural networks treat neighbouring data pointsidentically to those spaced far apart. In the case of time-series data, theydo not account for the temporal structure and autocorrelation that maybe present in the raw input data. As such, they may fail to recognise, forexample, a QRS complex in an ECG if it had been shifted in time by halfa beat compared to the training data examples. Convnets are deep-learning architectures that cater to this need for translation-invariance.Fig. 4. Schematic of the convolutional neural network.They exploit compositional hierarchies – whereby higher-level featuresare generated by accumulating a set of lower-level features – oftenexhibited in datasets derived from the natural world.3.1. Application to classifying electrogramsWe demonstrate the application of convolutional neural networks,using the same data as described in Section 2.1.1, to perform theclassification directly from the labelled time-series data. The network iscomposed of four repeated blocks, each itself consisting of a convolu-tional layer, a batch normalisation layer [61], a non-linear ReLU layer[62] and a max pooling layer [63]. Batch normalisation adjusts theinputs to a layer to have mean zero and standard deviation of one, andis a technique for improving the stability of the network. The ReLUlayer is a form of activation function, while max pooling layers are usedto down-sample the input as it progresses through the network. The lastblock is followed by one fully-connected layer to the two output classes.A schematic of the architecture is shown in Fig. 4.3.1.1. TrainingDuring training, a randomly selected one-second segment (therebyguaranteed to contain one deflection) was taken from the full ten-second recording and down-sampled to 5 kHz. Training was carried outfor 250 epochs using the Adam optimiser [64] with a variable learningrate starting at 0.0001 and a weight-decay of zero. The error wasevaluated using the cross-entropy loss function [19], and training wasrepeated five times each with different random initial weights toevaluate the performance (averages and standard deviations), giving ameasure of the robustness of the architecture and optimisation process.This was carried out using the PyTorch framework [65] on an GTX1080ti GPU (NVIDIA Corporation, USA).Cross-validation was carried out to measure the robustness of theprediction model as well as to aid tuning of the parameters of thelearning algorithms and design choices of the architecture (e.g. numberof layers). Once these hyper-parameters were sufficiently tuned, thevalidation and training data were combined (four plates) and the modelretrained. It was then evaluated using the test data (one plate). Trainedmodels were evaluated by splitting the ten-second recordings from thetest or validation datasets into ten segments corresponding to the tendeflections and classifying each segment. If six or more deflections wereclassified correctly, the recording was considered successfully classifiedand the uncertainty of this positive classification was computed usingthe binary entropy function. In practice, there was little variation be-tween the ten deflections within a signal, resulting in consistent clas-sification of each recording.3.1.2. ResultsTable 2 shows the classification accuracy using four-fold cross-343C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351Table 2Total classification accuracy results from the 4-fold cross-validation and finaltesting. For each model, the convolutional neural network was trained a total offive times to ensure the model was robust to differences in random initialisa-tions. Averages and standard deviations of the classification accuracy are pre-sented.coupled through gap junctions, which can be mathematically modelledas resistors. However, modelling the whole heart at a cellular scale iscomputationally intractable. Consequently, homogenisation of the dis-crete cell model leads to a bi-domain continuum model in the form oftwo partial differential equations (PDEs),FoldClassification accuracy (%)Cross-validationTesting123496.797.296.794.096.30.80.61.00.70.7Fig. 5. Confusion matrix from the binary classification of electrograms beforeand after administration of carbenoxolone to monolayers of cultured myocytes.validation after all design choices were made. The average accuracyacross the folds was 96.2% with a standard deviation of 1.5%, in-dicating a reliably robust model, and in general the entropy valuesremained low at approximately 0.05, indicating that each ten-secondsegment was consistently classified correctly or incorrectly. The stan-dard deviation of the five repeats for each fold showed the models wereconverging to a similar optimum state regardless of the initial weightvalues. The final evaluation of the model on the test data resulted in anoverall accuracy of 96.31.1% sensi-tivity, 95.80.7% positive prediction value1.2% negative predictive value. The confusion matrix forand 96.2this binary classification problem is shown in Fig. 5.0.7%, with results of 96.70.9% specificity, 96.44. Numerical modellingNumerical modelling predicts the future behaviour of a dynamicalsystem from a known state under an a priori belief in the physical lawsgoverning the system. In particular, it can allow observations to beextrapolated forward in time to help better understand the likely be-haviour of a physical system and, by modulating the parameters andinitial condition of the system appropriately, allow hypothetical sce-narios to be explored in silico. This makes it a potentially invaluable toolfor both improving our understanding of the mechanisms driving ar-rhythmogenesis and as a direct clinical tool for aiding diagnosis andplanning intervention.4.1. Tissue-scale continuum modellingThe physical processes responsible for the cardiac action potentialin a cardiac myocyte are a complex choreography of ion movementsacross the cell membrane. Mathematically, these are often described bysystems of ordinary differential equations (ODEs) which range incomplexity from two equations to more than twenty [66]. The para-meters of these equations have been chosen based on fitting the in-dividual equations to experimental measurements. Cells are electrically344(3)(4)is the extracellular potential,supplemented with appropriate boundary conditions [67]. Here v is thetransmembrane potential,is themembrane capacitance per unit area, χ is the cellular surface-to-volumeis the transmembrane current density from the coupledratio andaction potential ODE model. Anisotropy and heterogeneity of themyocardium is captured in the intracellular conductivity tensorandextracellular conductivity tensor. With the further assumption ofequal anisotropy ratios in these spaces, such that, this system ofequations can be further reduced to a single PDE, again with appro-priate boundary conditions, known as the monodomain model,(5)A comprehensive review of the mathematical models used in thecardiac electrophysiology domain is given by Clayton et al. [67]. Themonodomain model belongs to the class of reaction-diffusion PDEs. Thediffusion component can be considered to relate to the biophysicalprocess of ion propagation through gap junctions between cells, whilethe reaction component is the cumulative result of the action potentialmodel describing the opening and closing of ion channels in themembrane (gating variables) and related ion movements into, or out of,the cell.4.2. Numerical methods for action potential propagationTo solve equation (5) in all but the most trivial of scenarios requiresthe use of numerical approximations. The continuous PDE is trans-formed into a system of algebraic equations which are more amenableto solution on a computer. This transformation may use one of severaldiscretisation techniques, such as finite difference, finite element orspectral approximations and with sufficient spatial and temporal re-solution can provide very accurate approximations to the true solutionof the PDE. However, the wide range of time-scales on which the dif-ferent physical processes in the model occur makes the numerical so-lution of this system challenging and may lead to long simulation timeseven with considerable computational resources.One technique being explored within our group for modellingelectrophysiology is the spectral/element method [68]. This ap-proach combines the flexibility of the finite element method to model,for example, the complex geometry of the heart chambers, with thenumerical benefit of spectral methods, by enriching the polynomialspace of each element with higher-order basis functions. In particular,this allows an approximation of comparable accuracy to a conventionalfinite element discretisation to be achieved with a smaller algebraicsystem of equations, resulting in faster simulations and ultimately ashorter time to solution. We have also explored the simulation of actionpotential propagation in the left atrium using a surface representationof the chamber wall [69], further reducing the size of the numericalproblem to solve.Even with these advancements, the computational cost of usingnumerical methods to accurately perform predictive modelling is stillsubstantial, with the time-to-solution being orders of magnitude higherthan what might be required for an interactive clinicaltool.Furthermore, the inference of model parameters is highly challengingand even more computationally costly. In Section 5 and Section 6 weconsider opportunities for machine learning techniques to complement±±±±±+=+vvCvtI()(),iiemion++=vv()(())0,iieeveCmIionie=ie+=+vCvtI1()imionhp±±±±±C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351modelling and help address these difficulties.5. Recurrent neural networksDeep neural networks can be trained to predict the future behaviourof a dynamical system [70–72], and subsequently their internal re-presentation can be used to estimate the latent parameters of thesystem. Neural networks have been shown to be faster (at inferencetime) than commonly used numerical simulation approaches [70,71].Indeed, while they require large amounts of data to train, once theoptimal network weights have been found, obtaining predictions fromunseen inputs requires only a fraction of the time and computationalresources in comparison to conventional numerical methods. Despitethe drawback of generating only approximate solutions, leveraging thefast prediction performance of neural networks may enable largenumbers of what-if scenarios to be rapidly explored in a fraction of thetime of conventional numerical modelling. Clinically, this might allowthe viability of potential therapeutic strategies to be quickly tested andaccelerate the calibration of more precise numerical modelling whichcan be used to further optimise the treatment.Recurrent neural networks differ from purely feed-forward neuralnetworks, such as the convolution neural networks considered inSection 3, in that they are designed for processing sequences of inputs.They incorporate a feedback loop where the output of each step in thesequence is added to the input of the next step [63]. These types ofnetworks are therefore well suited to dealing with data sequences. Thisallows information to be propagated along the sequence: each outputwill then be conditioned not only by the current input in the sequence,but also by all previous inputs. However, in practice, vanilla recurrentneural networks can only store information for a short number of steps.Long short-term memory (LSTMs) networks are a particular variation ofrecurrent neural networks developed to alleviate this problem andallow the learning of longer term dependencies [63].5.1. Application to predicting two-dimensional diffusionHere, we present a proof of principle study where we apply thisapproach to a two-dimensional diffusion problem with a spatially het-erogeneous and anisotropic diffusion tensor. Diffusion is a key com-ponent of models of excitable media, such as cardiac tissues [67]. Oursystem is governed by the following equations:(6)(7)(8)is a diagonal 2 × 2 matrix with non-zero diagonal elements, governing diffusion on the horizontal and vertical axis, re-Here,andspectively. The computational domain.5.1.1. Training data generationNumerical simulations were performed using a regular mesh ofsquare elements, using a modified Legendre polynomial basis. The solution over time was sub-with polynomials up to ordersequently sampled on a regulargrid for input into the neuralnetwork. A total of 1600 numerical simulations were performed usingNektar++ [68] with initial condition and diffusion fields drawn atrandom from a predefined distribution. The initial condition consistedof spatially smoothed noise. This was created by first generating arandom spatial frequency spectrum in the Fourier domain. This had atwo-dimensional frequency profile drop-off as, withalpha randomly chosen to be eithertities. The quan-are measured in terms of cycles per domain length.andandor, withorFurthermore, there is a sharp cut-off at16 which is randomly drawn independently for each simulation. Thespatial frequency profile was multiplied by standard Gaussian noise andphases were drawn from a uniform distribution between 0 and 2π. Fi-nally, a symmetric version of the spectrum is inverted to obtain theinitial condition in the spatial domain and then normalised to achieve acertain level of contrast. All initial conditions are gradually smoothed tozero when approaching the boundary of the domain. Moreover, theywere first generated at a resolution of 128 × 128 pixels and then in-terpolated onto the mesh used for the simulation.The diffusion field was characterised by six parameters. A line withrandom orientation (θ) and location (β) is chosen to partition the do-main, with one part denoted as healthy and the other as scarred tissue.The former is given a higher diffusion coefficient with respect to thelatter. Diffusivity in the domain is therefore characterised by four dif-ferent diffusion parameters:, which are chosen to,satisfy the conditions,,,(9)(10)Here, the anisotropy ratio γ and heterogeneity ratio λ are randomlydrawn from a uniform distribution on the intervals [1,3] and [2,7],respectively. Finally, for each simulation we randomly selected whichofwas assigned to have the highest magnitude. This conse-quently determines the direction along which diffusion is fastest. Thedirection of fastest diffusion was assigned a value drawn from the in-terval [3.2, 3.8].and5.1.2. Network architectureTo predict the future behaviour of the system, we built a fullyconvolutional neural network consisting of three main blocks, similar tothe architecture used by Ehrhardt et al. [72]. First, a three-layer en-coding network extracts relevant features from the input frames, whileperforming dimensionality reduction. By compressing the informationthat passes through the layers, this encoder network thus acts as abottleneck that encourages the network to only extract a useful re-presentation of the system. While Ehrhardt et al. [72] used a portion ofa pre-trained VGG network [73], here we train the network end-to-endto tailor the features extracted to the specific physical system underconsideration. Next, a convolutional LSTM layer [74] progresses thefeatures in time for as many steps as necessary. After the recurrentlayer, the structure of our predictive network is completed by a three-layer decoder network that transforms the output of the LSTM back intoframes in the spatial domain. Transposed convolutions are used with astride of two [75] to return to the original resolution. Batch normal-isation [61] and ReLU non-linearities [63] were used after each con-volutional layer in both the encoder and the decoder. A schematic of theneural network can be found in Fig. 6, while Table 3 has a summary ofthe key network parameters. The hyper-parameters of the network werechosen manually, after a brief exploration of the hyper-parameterspace. Therefore, it is possible that a more thorough search wouldfurther improve prediction accuracy.5.1.3. Network trainingThe network received as input 2 or 3 sequential frames and wastrained to predict the next 11 frames, while only a smaller number oftarget frames (variable between 1 and 7) was used to back-propagatethe prediction error, and thereby constituting useful information for thenetwork training process. All networks considered in this study wereable to extrapolate for more time steps than that used during training.We used Mean Squared Error (MSE) – the average of the squared dif-ference between targets and predictions – as the loss function, andtrained the network for 1000 epochs using Adam update schemes [76],345=×vxyttxyvxytxytTD(,,)((,)(,,)),(,,)[0,],=vxyvxyxy(,,0)(,),(,),0=×vxytxytT(,,)0,(,,)[0,].Dd0d1=[2,2]2×8080=P5×6464++fffxyc22/212=f3cfxfy+<fffxyo222=f8,120d0d1dscar0,dscar1,=ddddddddmax(,)min(,)max(,)min(,),scarscarscarscar01010,1,0,1,=dddd.scarscar00,11,d0d1C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351Fig. 6. The architecture of the network used to predict the future behaviour of the 2D diffusion system.Table 3Details of the architecture for the prediction network. The number of channelsin the last layer of the decoder is variable because it depends on how manyframes are requested as output (K).Nb. of channelsFilter sizedown-/up-samplingEncoderLSTMDecoder(64,32,32)32(32,64,K)(5,5,5)5(4,4,4)Max-pooling (2 × 2)n.a.Stride = 2weight decay [61] and a batch size of 64. The learning rate started at0.0005 and was decreased by a factor of ten after 700 epochs. The si-mulations were split into training and testing using a 5-fold cross-va-lidation scheme. In addition, 20% of the training dataset was set asidefor validation, since the final model was chosen as that which exhibitedbest performance across epochs on this validation dataset. The trainingwas carried out using the PyTorch framework [65] and required be-tween 40 minutes and 60 minutes per network on a GTX 960 GPU(NVIDIA Corporation, USA), depending on the number of back-propa-gated frames.5.1.4. Results) and as the target (Our aim was to investigate the changes in prediction accuracy withvarying amounts of training data, quantified as the number of framesgiven as the input () to the network. The formerdetermines how much information the network can use to make pre-dictions while the latter, which corresponds to the number of back-propagated target frames, is used to calculate the error which informsthe update of the network weights. The more frames that are used forback-propagation, the more future time steps can be aggregated tobuild the error signal that guides the network learning process. Thedisadvantage is that longer simulations are required to train the net-work and there is an increased risk of over-fitting.Prediction errors on the test dataset are shown in Fig. 7, fornetworks trained with different numbers of input and target frames.The solid lines correspond to the MSE averaged over the test datasetand over the five-fold cross-validation, with the error bars extendingfrom the minimum to the maximum accuracy achieved across the fivefolds. The MSE is plotted against the number of back-propagated targetframes, but is computed as an average across the prediction of 11 futureframes, for all networks shown. The dashed lines represent the “lastinput” level, computed as the error that would be achieved if the lastinput frames were used as the predictions. This control test provides abaseline against which to judge the generalisation capability of thenetwork. Fig. 8 shows the full “last input” level distribution against the).performance achieved by one of the trained networks (It can be observed that the network is making effective use of the in-formation it receives as input, since the accuracy achieved is better thanthat obtained with no physical knowledge for each individual simula-tion in the test dataset.,To assess statistical significance, we compared the two distributionsusing a Wilcoxon signed rank test: we obtained a p-value smaller thanagainst the null hypothesis of equality of medians, as shown inFig. 8. Such a null hypothesis can be interpreted as the situation inwhich the neural network makes advantageous use of its inputs, but hasnot learned the physics behind it. Furthermore, the plot in Fig. 7 sug-gests that the learning capabilities of the network saturate after a cer-frames (potentially fromtain number of back-propagated target). From that point onwards, the network is able to maintain si-milar error levels when extrapolating the predictions further ahead intime. This could be indicative of the network having better assimilatedthe mechanics of diffusion. The benefit of using a smaller number ofback-propagated frames would be evident when there is a limited timeavailable to run the simulations necessary to build the training dataset:in this case, being able to train effectively with shorter, rather thanlonger, simulations is advantageous.It is possible, however, that the gain in prediction accuracy shownin Fig. 7 is influenced by the progressive decrease over time of the) and back-Fig. 7. Accuracy of next steps prediction versus number of input (propagated () frames. Mean Squared Error (MSE), averaged over the testdataset, first, and the 5 cross-validation folds, then, against.Error bars extend to the minimum and maximum MSE among the 5 folds.Dashed lines represent the last input level.and forFig. 8. Comparison between the full MSE distribution across the test dataset forthe predictions of a single trained network, compared with that given by usingthe last input frame as the prediction. The asterisks represent statistical sig-nificance with p-value(Wilcoxon signed rank test).346KbKt=K3b=K5t104=K4tpKbKtKt=K2,3b<104C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351Fig. 9. The logarithm of the Normalised Mean Squared Error (NMSE) againstthe prediction time for networks trained with various). Theblack dots represent thecorresponding to each line.(andand different values ofoverall energy of the system, caused by the diffusion dynamics. Tocontrol for this, we computed the normalised mean squared error(NMSE) – the MSE between predicted and target frames divided by the-norm of the target frames at each individual time point. Such ameasure is similar to the fraction of variance explained by a regressionalgorithm, and allows us to directly compare performance at differenttime steps. Fig. 9 shows the logarithm of NMSE as a function of time fornetworks trained with, averaged overthe test dataset and the cross-validation folds. It can be noted that, ateach time step, the NMSE is inversely proportional to the number ofback-propagated frames, suggesting that networks with a higherareable to explain a larger proportion of the variability of all future frames.Once trained, each prediction from the neural network took lessthan one second to calculate, compared to 40 seconds required by nu-merical modelling. Extending this to models of cardiac electro-physiology, such a gain in computational speed would allow for aquicker exploration of multiple scenarios of interest, such as the al-teration of functional and structural characteristics of different areas ofthe myocardium when planning intervention.5.2. Application to estimating diffusion parametersRobustly and accurately estimating the parameters for models iscritical for them to be useful in prediction. In this section we consideran example of how machine learning can meet this need. Extending thenext-step prediction of Section 5.1, we explored how accurately theparameters of the diffusion model can be estimated from one of thenetworks trained for predictions. We specifically considered the net-work with, as shown in Fig. 8.andWe first extracted the internal representation of the network for all11 predicted future frames. Specifically, this is the activity of the LSTMunits. This multidimensional vector was used as the input to a secondneural network with two convolutional, and one fully connected, layerswhich was trained to predict the six parameters,as well as the orientation (θ) and location (β) of the boundary betweenhealthy and scarred tissue. Details of this network are given in Table 4.and,,5.2.1. ResultsResults are shown in Fig. 10. The correlation coefficients betweentarget and predicted parameter values are 0.84, 0.80, 0.80, 0.70, 0.95,and 0.87 for, θ and β, respectively. Despite therebeing potential for improvements, the achieved accuracy indicates thatthe internal representation learnt by the network does contain in-formation about physically relevant quantities. This suggests that our,,,Table 4Details of the architecture for the parameters estimation network.Channels/unitsFilter sizedown-/up-samplingConvolutionalFully connected(128,64)(6)(6,6)n.a.Stride = 2n.a.347Fig. 10. Accuracy of parameter inference from the internal representation ofthe prediction network withfor (a) boundary angle θ, (b)and. Theboundary position β and (c–d) scar parameterspredicted parameter values are plotted against the target values for a subset ofthe test dataset.and,,deep learning model is able to assimilate at least some of the me-chanisms intrinsic to the physical system under consideration.6. Approximate Bayesian Computation (ABC)Approximate Bayesian Computation (ABC) is a statistical inferencetechnique that can be applied as a parameter fitting method to in-corporate uncertainty estimates into the fitting process [77–79]. Thealgorithm interrogates the range of outputs of a numerical model bydrawing different parameter choices from a defined prior probabilityspace, running the simulation, and comparing the output to experi-mental data. This probability space is then sequentially refined basedon a distance function which measures the closeness of the simulatedoutputto the experimental output. After multiple iterations (orreaching a chosen error threshold), the algorithm produces a discreteapproximation to the true posterior probability distribution of modelparameters given the observed experimental data. The distributionsgive more information to a modeller on the ability of the experimentaldata to constrain the parameter choice and the resulting credibility ofthe full model.6.1. Application to inferring cell model parametersTissue-scale cardiac electrophysiology simulations are built onmodels of the action potential of single myocytes. These cell models aresolved by calculating the opening and closing kinetics of transmem-brane and internal ion currents, and their effect on the membrane po-tential of the cell. Each parameter in ion current sub-models is chosenspecific to the particular cell type. The parameter values are based ondata from patch-clamp experiments which interrogate the dynamics ofspecific ion currents in isolated myocytes at a range of prescribed vol-tages [80].The standard approach is to fit ion channel parameters to these datausing a traditional method such as least squares regression. Thesemethods produce point estimates and thus do not take into accountuncertainties introduced through the fitting process itself when mul-tiple parameter choices can result in similar values of the fitting losscriterion. This has led to discrepancies between cell models whichL2=K3bKtKt=K3b=K4td0d1dscar0,dscar1,d0d1dscar0,dscar1,Kt=K3bKt=K4t=K3bd0d1dscar0,dscar1,C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351purport to represent the same cell type [81].6.1.2. ResultsDaly et al. previously investigated the use of ABC on parameterschosen in the original Hodgkin-Huxley action potential model [78,82].They found that parameters were generally well constrained by theexperimental data. These data were average current traces which in-form both voltaic and temporal behaviour of a channel. Modern ex-perimental studies predominantly report steady-state behaviour ofchannels in response to voltage steps.6.1.1. Neonatal rat ventricular sodium channelWe investigate the ability of modern patch-clamp data, which maycontain less information, to constrain parameters of a physiologicalmodel for neonatal rat ventricular myocytes (NRVMs) [83]. We presentthe result of fitting the fast sodium channel of the NRVM model usingABC. The fast sodium channel plays a crucial role in the generation of acellular action potential and the propagation of an electrical signalthrough cardiac tissue; consequently, it is critical to have confidence inany in silico model of the channel. We use the ABC Sequential MonteCarlo (ABCSMC) algorithm with a population adaptation strategy fromthe pyabc python library (http://pyabc.readthedocs.io/en/latest/) [79]and the myokit python library (http://myokit.org) for running simula-tions [84]. The equations for the fast sodium channel [83] include threegates: activation, fast inactivation, and slow inactivation, and are givenby,(11)(12)(13)(14)-andwhere the parametersare determined from experi-mental data, the original published values of which are given in thesecond column of Table 5. The gates govern the proportion of openchannels in the cells and thus affect the maximum current that is able toflow across the membrane. These equations were adapted for NRVMsfrom earlier cell models of different species by varying only the channelconductance [83]. We therefore use only the directly applicable data forobservations in the ABCSMC algorithm. These data are from patchclamp experiments on adult rat ventricular cells [85,86]. They includefive patch clamp protocols testing activation, inactivation and recoverycharacteristics of the channel. The protocols do not explicitly testtemporal characteristics of the current, and thus we retain the temporalparameters of the fast and slow inactivation processes to reduce thedimensionality of the problem. For each of the nine parameters that weconstrain using ABCSMC, initial priors were set to uniform distributionsroughly an order of magnitude larger than the parameter setting in theoriginal model.Table 5Results of ABCSMC inference for the parameters of the fast sodium channel.orig.priorposteriorp1p2p3p4p5p6p7q1q245−6.50.23547.1−0.10.058811.076.16.07(0, 100)(-50, 0)(0, 1)(0, 100)(-50, 0)(0, 1)(0, 1000)(0, 100)(0, 50)meanminmax43.4−11.60.071779.7−36.20.0034568272.59.5443.2−11.60.070779.3−50.00.0030232572.98.8243.6−11.50.072679.9−12.20.0037399873.110.0348Table 5 also shows the prior distribution ranges and statistics of theposterior distributions. Seven of the nine parameters appear well con-) are relatively close tostrained by the data, of which four (the original values. These parameters govern the steady-state activationand inactivation (both fast and slow) of the current, confirming that thepatch clamp protocols predominantly test the steady-state character-istics of this current.,,,Fig. 11(a) shows how the distributions of the two steady-stateparameters of activation are sequentially constrained through theiterations of ABCSMC. Fig. 11(b) shows the data used to fit the current,along with simulation results using original parameters, 100 prior dis-tribution samples and 100 posterior distribution samples. The outputfor the posterior distribution is close to both the original settings andthe experimental data. In some aspects, particularly the upper half ofinactivation behaviour in exp = 2, the ABCSMC fit is a noticeable im-provement over the original parameter choices. However, in other areassuch as the start of activation (seen in exp = 0 and exp = 1), theABCSMC fit is further from the observed data than the original settings.For exp = 3 which tests the normalised peak current of a regular trainof voltage pulses, the equations of the model appear unable to captureboth the initial exponential and then linear decay of the observed data,shown by the fact that both the ABC posterior and original parameterchoices end in a constant relationship after the initial decay portion.Despite the large variation present in two of the parameters, theposterior results in Fig. 11(b) show little variation. This indicates thatthe current patch-clamp protocols may not sufficiently interrogatetemporal aspects of the channel, as both unconstrained parameters (p5and p7) govern this aspect of the model equations. This highlights thevalue of the ABC approach; using traditional fitting methods, we wouldnot be aware of the unidentifiability in these parameters. More complexpatch-clamp protocols could be investigated in an attempt to improvethe ability of the data to constrain the model.7. DiscussionIn this review we have shown the potential benefits of a number ofmachine learning approaches and how they can enable us to extractmore information from the data we collect. The electrogram is theubiquitous data modality of the cardiac electrophysiology catheter la-boratory and yet the relative information content extracted from thesecomplex signals is currently poor. We have demonstrated that byquantifying and combining a range of features in the signal, some ofwhich are already used in isolation, and applying machine learningalgorithms to them we can learn more about the properties of the un-derlying myocardium and the substrate that sustains arrhythmias. Deeplearning allows us to further automate this process, removing the in-herent bias of manually choosing potentially sub-optimal features, andallowing the neural network to extract latent representations whichbest discriminate between classes directly from the signal.Numerical modelling is becoming increasingly established withinthe cardiac electrophysiology field, due to the increased availability ofcomputational power, and improved resolution of clinical imagingtechnologies. However, the numerical resolution requirements for ac-tion potential propagation and complexity of ion channel kinetics stillnecessitates high computational cost. Furthermore, the number ofparameters and difficulties associated with deriving appropriate valuesexperimentally or clinically means that great care is needed when in-corporating these into predictive modelling. We have shown how ma-chine learning can help in both inferring appropriate parameter valuesfrom data as well as quantifying how certain we can be in those para-meter values and therefore how confident we can be in the modeloutput.=IGmhjVE()NaNaNa3=++mpVp{1exp[()/]}121==++jhqVq{1exp[()/]}121=+++pVppVppVp()1exp[()]exp(/)m3454671p1p7qq,12p1p2q1q2C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351Fig. 11. (a) Kernel density estimates for steady-state activation parameters showing sequential constraining of distributions. (b) Data used to fit channel (blackcrosses) are plotted for each patch clamp experiment. Simulation results are also plotted for original parameter settings (green triangles), 100 samples from priordistribution (blue circles), and 100 samples from posterior distribution (orange squares). Shaded area indicates 95% confidence intervals around the median line.7.1. LimitationsAcknowledgementsIn analysing the micro-electrode array electrogram data in Section 2the large-amplitude stimulus artefact, created by pacing of the culture,was first removed. This was required for the feature-detection algo-rithms to reliably measure the electrogram characteristics used to formthe feature vector. Since the stimulus artefact dominates the signal, itwas also a necessary pre-processing step for the convolutional neuralnetwork approach. Without its removal the network was unable todistinguish specific morphological features of the response signal.For the robust training of deep neural networks, used in bothSection 3 and Section 5, the volume of data and computational cost oftraining is high. While data augmentation techniques are used to im-prove the generalisation of models, additional recorded data wouldfurther improve the quality of the predictions made by these methods.Graphics Processing Units (GPUs) are particularly effective at un-dertaking the learning process for deep neural networks and their use isessential to produce trained models in tractable timescales. In thiscontext, feature-based classifiers provide a performance advantage insituations where appropriate features are known and can be defined apriori to distinguish the classes. However, the computational time forprediction using the trained models is negligible for both feature-basedand deep-learning based methods.8. ConclusionsCardiac arrhythmias are a major global healthcare problem andthere is significant scope for improving their diagnosis and treatment.Improvements will be achieved from better understanding of the me-chanisms sustaining fibrillation, as well as increasingly personalisedtreatment. Modern machine learning techniques and numerical mod-elling, when applied appropriately, both have great potential to helpfulfil this role and their combination, in particular, offers a powerfulapproach to achieving personalisation of care.Conflict of interestNone declared.This work was generously supported by the Rosetrees Trust (grantM577), British Heart Foundation research grants (PG/15/59/31621,PG/16/17/32069 and RG/16/3/32175) and the British HeartFoundation Centre for Research Excellence (grant RE/13/4/30184).Appendix A. Supplementary dataSupplementary data to this article can be found online at https://doi.org/10.1016/j.compbiomed.2018.10.015.References[1] T. Phlips, P. Taghji, M. El Haddad, M. Wolf, S. Knecht, Y. Vandekerckhove,R. Tavernier, M. Duytschaever, Improving procedural and one-year outcome aftercontact force-guided pulmonary vein isolation: the role of interlesion distance,ablation index, and contact force variability in the ‘CLOSE’-protocol, EP Europace(2018) eux376doi, https://doi.org/10.1093/europace/eux376.[2] A. Verma, C.-y. Jiang, T.R. Betts, J. Chen, I. Deisenhofer, R. Mantovan, L. Macle,C.A. Morillo, W. Haverkamp, R. Weerasooriya, et al., Approaches to catheter ab-lation for persistent atrial fibrillation, N. Engl. J. Med. 372 (19) (2015) 1812–1822.[3] J.M. Davidenko, A.V. Pertsov, R. Salomonsz, W. Baxter, J. Jalife, Stationary anddrifting spiral waves of excitation in isolated cardiac muscle, Nature 355 (6358)(1992) 349.[4] G.K. Moe, W.C. Rheinboldt, J. Abildskov, A computer model of atrial fibrillation,Am. Heart J. 67 (2) (1964) 200–220.[5] S. Verheule, J. Eckstein, D. Linz, B. Maesen, E. Bidar, A. Gharaviri, U. Schotten, Roleof endo-epicardial dissociation of electrical activity and transmural conduction inthe development of persistent atrial fibrillation, Prog. Biophys. Mol. Biol. 115 (2–3)(2014) 173–185.[6] S.M. Narayan, D.E. Krummen, K. Shivkumar, P. Clopton, W.-J. Rappel, J.M. Miller,Treatment of atrial fibrillation by the ablation of localized sources: CONFIRM(conventional ablation for atrial fibrillation with or without focal impulse and rotormodulation) trial, J. Am. Coll. Cardiol. 60 (7) (2012) 628–636.[7] J.S. Steinberg, Y. Shah, A. Bhatt, T. Sichrovsky, A. Arshad, E. Hansinger, D. Musat,Focal impulse and rotor modulation: acute procedural observations and extendedclinical follow-up, Heart Rhythm 14 (2) (2017) 192–197.[8] C.H. Roney, C.D. Cantwell, J.D. Bayer, N.A. Qureshi, P.B. Lim, J.H. Tweedy,P. Kanagaratnam, N.S. Peters, E.J. Vigmond, F.S. Ng, Spatial resolution require-ments for accurate identification of drivers of atrial fibrillation, Circulation:Arrhythm. Electrophysiol. 10 (5) (2017) e004899.[9] K. Nademanee, E. Lockwood, N. Oketani, B. Gidney, Catheter ablation of atrial fi-brillation guided by complex fractionated atrial electrogram mapping of atrial fi-brillation substrate, J. Cardiol. 55 (1) (2010) 1–12.[10] R. Latchamsetty, A. G. Kocheril, Review of dominant frequency analysis in atrialfibrillation, J. Atr. Fibrillation 2 (3).[11] S. Nattel, F. Xiong, M. Aguilar, Demystifying rotors and their place in clinicaltranslation of atrial fibrillation mechanisms, Nat. Rev. Cardiol. 14 (9) (2017) 509.349C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351[12] P. Kanagaratnam, P. Kojodjojo, N.S. Peters, Electrophysiological abnormalitiesdisciplinary survey, Data Min. Knowl. Discov. 2 (4) (1998) 345–389.occur prior to the development of clinical episodes of atrial fibrillation: observa-tions from human epicardial mapping, Pacing Clin. Electrophysiol. 31 (4) (2008)443–453.[13] R.J. Schilling, D.W. Davies, N.S. Peters, Characteristics of sinus rhythm electro-grams at sites of ablation of ventricular tachycardia relative to all other sites: anoncontact mapping study of the entire left ventricle, J. Cardiovasc. Electrophysiol.9 (9) (1998) 921–933.[14] A. Orozco-Duque, J. Bustamante, G. Castellanos-Dominguez, Semi-supervisedclustering of fractionated electrograms for electroanatomical atrial mapping,Biomed. Eng. Online 15 (1) (2016) 44.[15] S. Duque, A. Orozco-Duque, V. Kremen, D. Novak, C. Tobón, J. Bustamante, Featuresubset selection and classification of intracardiac electrograms during atrial fi-brillation, Biomed. Signal Process. Control 38 (2017) 182–190.[16] A.G. Kléber, Y. Rudy, Basic mechanisms of cardiac impulse propagation and asso-ciated arrhythmias, Physiol. Rev. 84 (2) (2004) 431–488.[17] R.N. Ghanem, P. Jia, C. Ramanathan, K. Ryu, A. Markowitz, Y. Rudy, Noninvasiveelectrocardiographic imaging (ECGI): comparison to intraoperative mapping inpatients, Heart Rhythm 2 (4) (2005) 339–354.[18] P.M. Boyle, J.B. Hakim, S. Zahid, W.H. Franceschi, M.J. Murphy, E.J. Vigmond,R. Dubois, M. Haïssaguerre, M. Hocini, P. Jaïs, et al., Comparing reentrant driverspredicted by image-based computational modeling and mapped by electrocardio-graphic imaging in persistent atrial fibrillation, Front. Physiol. 9 (2018) 414.[19] K. Janocha, W. M. Czarnecki, On Loss Functions for Deep Neural Networks in[43] G.-X. Yuan, C.-H. Ho, C.-J. Lin, Recent advances of large-scale linear classification,Proc. IEEE 100 (9) (2012) 2584–2603.[44] R.H. Riffenburgh, Linear Discriminant Analysis, Ph.D. thesis Virginia PolytechnicInstitute, 1957.[45] C.J. Burges, A tutorial on support vector machines for pattern recognition, DataMin. Knowl. Discov. 2 (2) (1998) 121–167.[46] T. Cover, P. Hart, Nearest neighbor pattern classification, IEEE Trans. Inf. Theor. 13(1) (1967) 21–27.[47] L. Breiman, Bagging predictors, Mach. Learn. 24 (2) (1996) 123–140.[48] L. Breiman, Random forests, Mach. Learn. 45 (1) (2001) 5–32.[49] R.A. Chowdhury, K.N. Tzortzis, E. Dupont, S. Selvadurai, F. Perbellini,C.D. Cantwell, F.S. Ng, A.R. Simon, C.M. Terracciano, N.S. Peters, Concurrentmicro-to macro-cardiac electrophysiology in myocyte cultures and human heartslices, Sci. Rep. 8 (1) (2018) 6947.[50] A.W. Whitney, A direct method of nonparametric measurement selection, IEEETrans. Comp. C 20 (9) (1971) 1100–1103, https://doi.org/10.1109/T-C.1971.223410 ISSN 0018-9340.[51] Y. Bengio, A. Courville, P. Vincent, Representation learning: a review and newperspectives, IEEE Trans. Pattern Anal. Mach. Intell. 35 (8) (2013) 1798–1828.[52] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436.[53] A. Krizhevsky, I. Sutskever, G.E. Hinton, Imagenet classification with deep con-volutional neural networks, Advances in Neural Information Processing Systems,vols. 1097–1105, 2012.Classification, ArXiv Preprint arXiv:1702.05659 .[54] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,[20] R.C. Deo, Machine learning in medicine, Circulation 132 (20) (2015) 1920–1930.[21] S.H. Jambukia, V.K. Dabhi, H.B. Prajapati, Classification of ECG signals using ma-chine learning techniques: a survey, Computer Engineering and Applications(ICACEA), 2015 International Conference on Advances in, IEEE, 2015, pp. 714–721.V. Vanhoucke, A. Rabinovich, et al., Going Deeper with Convolutions, Cvpr, 2015.[55] S. Ren, K. He, R. Girshick, J. Sun, R.-C.N.N. Faster, Towards real-time object de-tection with region proposal networks, IEEE Trans. Pattern Anal. Mach. Intell. 39(6) (2017) 1137–1149.[22] G.D. Clifford, C. Liu, B. Moody, L.-w. H. Lehman, I. Silva, Q. Li, A. Johnson,[56] G. Hinton, L. Deng, D. Yu, G.E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,R.G. Mark, AF classification from a short single lead ECG recording: the PhysionetComputing in Cardiology Challenge 2017, Proc. Comput. Cardiol. 44 (2017) 1.[23] S. Hong, M. Wu, Y. Zhou, Q. Wang, J. Shang, H. Li, J. Xie, ENCASE: an ENsembleV. Vanhoucke, P. Nguyen, T.N. Sainath, et al., Deep neural networks for acousticmodeling in speech recognition: the shared views of four research groups, IEEESignal Process. Mag. 29 (6) (2012) 82–97.ClASsifiEr for ECG classification using expert features and deep neural networks,Computing in Cardiology (CinC), 2017, IEEE, vols. 1–4, 2017.[57] I. Sutskever, O. Vinyals, Q.V. Le, Sequence to sequence learning with neural net-works, Advances in Neural Information Processing Systems, vols. 3104–3112, 2014.[24] M. Zihlmann, D. Perekrestenko, M. Tschannen, Convolutional Recurrent NeuralNetworks for Electrocardiogram Classification, ArXiv Preprint arXiv:1710.06122 .[25] Z. Xiong, M.K. Stiles, J. Zhao, Robust ECG signal classification for detection of atrialfibrillation using a novel neural network, Computing 44 (2017) 1.[26] P. Rajpurkar, A. Y. Hannun, M. Haghpanahi, C. Bourn, A. Y. Ng, Cardiologist-levelArrhythmia Detection with Convolutional Neural Networks, ArXiv PreprintarXiv:1707.01836 .[27] J. Rubin, S. Parvaneh, A. Rahman, B. Conroy, S. Babaeizadeh, Densely ConnectedConvolutional Networks and Signal Quality Analysis to Detect Atrial FibrillationUsing Short Single-Lead ECG Recordings, ArXiv Preprint arXiv:1710.05817 .[28] P. Warrick, M.N. Homsi, Cardiac arrhythmia detection from ECG combining con-volutional and long short-term memory networks, Computing 44 (2017) 1.[29] R. Kamaleswaran, R. Mahajan, O. Akbilgic, A robust deep convolutional neuralnetwork for the classification of abnormal cardiac rhythm using single lead elec-trocardiograms of variable length, Physiol. Meas. 39 (3) (2018) 035006.[30] U.R. Acharya, S.L. Oh, Y. Hagiwara, J.H. Tan, M. Adam, A. Gertych, R. San Tan, Adeep convolutional neural network model to classify heartbeats, Comput. Biol. Med.89 (2017) 389–396.[31] T. Teijeiro, C.A. García, D. Castro, P. Félix, Arrhythmia classification from the ab-ductive interpretation of short single-lead ecg records, Comput. Cardiol. 44(2017) 1–4.[32] G. Bin, M. Shao, G. Bin, J. Huang, D. Zheng, S. Wu, Detection of atrial fibrillationusing decision tree ensemble, Computing 44 (2017) 1.[33] M. Zabihi, A.B. Rad, A.K. Katsaggelos, S. Kiranyaz, S. Narkilahti, M. Gabbouj,Detection of atrial fibrillation in ECG hand-held devices using a random forestclassifier, Computing 44 (2017) 1.[34] S. Datta, C. Puri, A. Mukherjee, R. Banerjee, A.D. Choudhury, R. Singh, A. Ukil,S. Bandyopadhyay, A. Pal, S. Khandelwal, Identifying Normal, AF and otherAbnormal ECG Rhythms using a cascaded binary classifier, Computing 44 (2017) 1.[35] L. Billeci, F. Chiarugi, M. Costi, D. Lombardi, M. Varanini, C. SpA, Detection of AFand other rhythms using RR variability and ECG spectral measures, Computing 44(2017) 1.[36] J. Sutton, R. Mahajan, O. Akbilgic, R. Kamaleswaran, PhysOnline: an online featureextraction and machine learning pipeline for real-time analysis of streaming phy-siological data, IEEE J. Biomed. Health Inform. (2018), https://doi.org/10.1109/JBHI.2018.2832610 1–1ISSN 2168-2194.[37] P.P. Sengupta, H. Kulkarni, J. Narula, Prediction of abnormal myocardial relaxationfrom signal processed surface ECG, J. Am. Coll. Cardiol. 71 (15) (2018) 1650–1660.[38] M.F. McGillivray, W. Cheng, N.S. Peters, K. Christensen, Machine learning methodsfor locating re-entrant drivers from electrograms in a model of atrial fibrillation,Roy. Soc. Open Sci. 5 (4) (2018) 172434.[58] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, P. Kuksa, Naturallanguage processing (almost) from scratch, J. Mach. Learn. Res. 12 (Aug) (2011)2493–2537.[59] M.K. Leung, H.Y. Xiong, L.J. Lee, B.J. Frey, Deep learning of the tissue-regulatedsplicing code, Bioinformatics 30 (12) (2014) i121–i129.[60] M. Helmstaedter, K.L. Briggman, S.C. Turaga, V. Jain, H.S. Seung, W. Denk,Connectomic reconstruction of the inner plexiform layer in the mouse retina,Nature 500 (7461) (2013) 168.[61] S. Ioffe, C. Szegedy, Batch Normalization: Accelerating Deep Network Training byReducing Internal Covariate Shift, ArXiv Preprint arXiv:1502.03167 .[62] V. Nair, G.E. Hinton, Rectified linear units improve restricted Boltzmann machines,Proceedings of the 27th International Conference on Machine Learning, vols.807–814, ICML-10), 2010.[63] I. Goodfellow, Y. Bengio, A. Courville, Y. Bengio, Deep Learning vol. 1, MIT pressCambridge, 2016.[64] D. P. Kingma, J. Ba, Adam: A Method for Stochastic Optimization, CoRR abs/1412.6980, URL http://arxiv.org/abs/1412.6980.[65] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison,L. Antiga, A. Lerer, Automatic Differentiation in PyTorch, NIPS-W, 2017.[66] J. Keener, J. Sneyd, S.S. Antman, J.E. Marsden, L. Sirovich (Eds.), MathematicalPhysiology, Vol. 8/2 of Interdisciplinary Applied Mathematics, Springer New York,New York, NY, 2009, , https://doi.org/10.1007/978-0-387-79388-7 ISBN 978-0-387-79387-0 http://link.springer.com/10.1007/978-0-387-79388-7.[67] R. Clayton, O. Bernus, E. Cherry, H. Dierckx, F. Fenton, L. Mirabella, A. Panfilov,F.B. Sachse, G. Seemann, H. Zhang, Models of cardiac tissue electrophysiology:progress, challenges and open questions, Prog. Biophys. Mol. Biol. 104 (1–3) (2011)22–48.[68] C.D. Cantwell, D. Moxey, A. Comerford, A. Bolis, G. Rocco, G. Mengaldo, D. DeGrazia, S. Yakovlev, J.-E. Lombard, D. Ekelschot, et al., Nektar++: an open-sourcespectral/hp element framework, Comput. Phys. Commun. 192 (2015) 205–219.[69] C.D. Cantwell, S. Yakovlev, R.M. Kirby, N.S. Peters, S.J. Sherwin, High-orderspectral/hp element discretisation for reaction–diffusion problems on surfaces:application to cardiac electrophysiology, J. Comput. Phys. 257 (2014) 813–829.[70] J. Tompson, K. Schlachter, P. Sprechmann, K. Perlin, Accelerating Eulerian FluidSimulation with Convolutional Networks, ArXiv Preprint arXiv:1607.03597 .[71] E. de Bezenac, A. Pajot, P. Gallinari, Deep Learning for Physical Processes:Incorporating Prior Scientific Knowledge, ArXiv Preprint arXiv:1711.07970 .[72] S. Ehrhardt, A. Monszpart, N. J. Mitra, A. Vedaldi, Learning a Physical Long-termPredictor, ArXiv Preprint arXiv:1703.00247 .[73] K. Simonyan, A. Zisserman, Very Deep Convolutional Networks for Large-scaleImage Recognition, ArXiv Preprint arXiv:1409.1556 .[39] H.J. Arevalo, F. Vadakkumpadan, E. Guallar, A. Jebb, P. Malamas, K.C. Wu,N.A. Trayanova, Arrhythmia risk stratification of patients after myocardial infarc-tion using personalized heart models, Nat. Commun. 7 (2016) 11437.[40] A. Prakosa, H.J. Arevalo, D. Deng, P.M. Boyle, P.P. Nikolov, H. Ashikaga,[74] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, W.-c. Woo, ConvolutionalLSTM network: a machine learning approach for precipitation nowcasting,Advances in Neural Information Processing Systems, 2015, pp. 802–810.[75] M.D. Zeiler, D. Krishnan, G.W. Taylor, R. Fergus, Deconvolutional networks,J.J. Blauer, E. Ghafoori, C.J. Park, R.C. Blake, et al., Personalized virtual-hearttechnology for guiding the ablation of infarct-related ventricular tachycardia, Nat.Biomed. Eng. (2018) 1.[41] S.B. Kotsiantis, I. Zaharakis, P. Pintelas, Supervised machine learning: a review ofclassification techniques, Emerg. Artif. Intell. Appl. Comput. Eng. 160 (2007) 3–24.Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE,vols. 2528–2535, 2010.[76] D. Kinga, J.B. Adam, A method for stochastic optimization, InternationalConference on Learning Representations, ICLR), 2015.[77] T. Toni, D. Welch, N. Strelkowa, A. Ipsen, M.P.H. Stumpf, Approximate Bayesian[42] S.K. Murthy, Automatic construction of decision trees from data: a multi-computation scheme for parameter inference and model selection in dynamical350C.D. Cantwell et al.Computers in Biology and Medicine 104 (2019) 339–351systems, J. R. Soc. Interface 6 (31) (2009) 187–202, https://doi.org/10.1098/rsif.2008.0172 https://doi.org/10.1098/rsif.2008.0172.[78] A.C. Daly, D.J. Gavaghan, C. Holmes, J. Cooper, Hodgkin-Huxley revisited: repar-ametrization and identifiability analysis of the classic action potential model withapproximate Bayesian methods, R. Soc. Open Sci. 2 (12) (2015) 150499, https://doi.org/10.1098/rsos.150499 https://doi.org/10.1098/rsos.150499.[79] E. Klinger, D. Rickert, J. Hasenauer, pyABC: distributed, likelihood-free inference,BioRxiv https://doi.org/https://doi.org/10.1101/162552, URL http://biorxiv.org/lookup/doi/10.1101/162552.[80] E. Neher, B. Sakmann, The patch clamp technique, Sci. Am. 266 (3) (1992) 44–51.[81] S.A. Niederer, M. Fink, D. Noble, N.P. Smith, A meta-analysis of cardiac electro-physiology computational models, Exp. Physiol. 94 (5) (2009) 486–495, https://doi.org/10.1113/expphysiol.2008.044610 https://doi.org/10.1113/expphysiol.2008.044610.[82] A.L. Hodgkin, A.F. Huxley, A quantitative description of membrane current and itsapplication to conduction and excitation in nerve, J. Physiol. (Lond.) 117 (4) (1952)500–544, https://doi.org/10.1113/jphysiol.1952.sp004764 https://doi.org/10.1113/jphysiol.1952.sp004764.[83] T. Korhonen, S.L. HÃ¤nninen, P. Tavi, Model of excitation-contraction coupling ofrat neonatal ventricular myocytes, Biophys. J. 96 (3) (2009) 1189–1209, https://doi.org/10.1016/j.bpj.2008.10.026 https://doi.org/10.1016/j.bpj.2008.10.026.[84] M. Clerx, P. Collins, E. de Lange, P.G.A. Volders, Myokit: a simple interface tocardiac cellular electrophysiology, Prog. Biophys. Mol. Biol. 120 (1–3) (2016)100–114, https://doi.org/10.1016/j.pbiomolbio.2015.12.008 https://doi.org/10.1016/j.pbiomolbio.2015.12.008.[85] S.V. Pandit, R.B. Clark, W.R. Giles, S.S. Demir, A mathematical model of actionpotential heterogeneity in adult rat left ventricular myocytes, Biophys. J. 81 (6)(2001) 3029–3051, https://doi.org/10.1016/S0006-3495(01)75943-7 https://doi.org/10.1016/S0006-3495(01)75943-7.[86] H.C. Lee, T. Lu, N.L. Weintraub, M. VanRollins, A.A. Spector, E.F. Shibata, Effects ofepoxyeicosatrienoic acids on the cardiac sodium channels in isolated rat ventricularmyocytes, J. Physiol. (Lond.) 519 (Pt 1) (1999) 153–168, https://doi.org/10.1111/j.1469-7793.1999.0153o.x https://doi.org/10.1111/j.1469-7793.1999.0153o.x.351