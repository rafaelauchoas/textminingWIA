Understanding common human driving semanticsfor autonomous vehiclesArticleHighlightsd Reveal human auditory cortex activation during drivingd Discover the hierarchical structure of human drivingunderstandingd Propose a neural-informed semantics-driven drivingunderstanding modeld Address long-term contextual dependency of drivingbehaviorsAuthorsYingji Xia, Maosi Geng, Yong Chen, ...,Bing Zhang, Ziyou Gao,Xiqun (Michael) ChenCorrespondencechenxiqun@zju.edu.cnIn briefAutonomous vehicles will share roadswith human-driven vehicles and bringwith them problems regardingbidirectional understanding of drivingbehavior. Based on cerebral neurologicalfindings from the human process forunderstanding driving, a novel neural-inspired semantics-driven drivingunderstanding model is proposed forautonomous vehicles. The model imitatesthe way humans understand driving andcan interpret long-term driving behaviorevolutions like human drivers.Xia et al., 2023, Patterns 4, 100730June 9, 2023 ª 2023 The Author(s).https://doi.org/10.1016/j.patter.2023.100730llPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSArticleUnderstanding common human drivingsemantics for autonomous vehiclesYingji Xia,1 Maosi Geng,1,2 Yong Chen,1 Sudan Sun,3 Chenlei Liao,1 Zheng Zhu,1,4,10 Zhihui Li,5Washington Yotto Ochieng,6 Panagiotis Angeloudis,6 Mireille Elhajj,6 Lei Zhang,7 Zhenyu Zeng,7 Bing Zhang,7 Ziyou Gao,8and Xiqun (Michael) Chen1,4,9,10,11,*1Institute of Intelligent Transportation Systems, College of Civil Engineering and Architecture, Zhejiang University, Hangzhou 310058, China2Polytechnic Institute & Institute of Intelligent Transportation Systems, Zhejiang University, Hangzhou 310015, China3School of Medicine, Zhejiang University, Hangzhou 310058, China4Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies, Hangzhou 310027, China5School of Transportation, Jilin University, Changchun 130022, China6Department of Civil and Environmental Engineering, Imperial College London, South Kensington Campus, London SW7 2AZ, UK7Alibaba Group, Hangzhou 310052, China8School of Traffic and Transportation, Beijing Jiaotong University, Beijing 100044, China9Zhejiang University/University of Illinois Urbana-Champaign (ZJU-UIUC) Institute, Zhejiang University, Haining 314400, China10Zhejiang Provincial Engineering Research Center for Intelligent Transportation, Hangzhou 310058, China11Lead contact*Correspondence: chenxiqun@zju.edu.cnhttps://doi.org/10.1016/j.patter.2023.100730THE BIGGER PICTURE ‘‘Driving like humans’’ is the ultimate goal of autonomous driving. Hence, human-like driving understanding ability is required for autonomous vehicles to better understand the driving be-haviors of surrounding human-driven vehicles. In this study, we investigated human driving neural responseand subsequently built a biologically plausible model to interpret driving behaviors like humans. This studypioneers the design of bio-inspired, human-like autonomous vehicles and can ultimately benefit futureresearch of human-machine interactions.Proof-of-Concept: Data science output has been formulated,implemented, and tested for one domain/problemSUMMARYAutonomous vehicles will share roads with human-driven vehicles until the transition to fully autonomoustransport systems is complete. The critical challenge of improving mutual understanding between bothvehicle types cannot be addressed only by feeding extensive driving data into data-driven models but byenabling autonomous vehicles to understand and apply common driving behaviors analogous to humandrivers. Therefore, we designed and conducted two electroencephalography experiments for comparingthe cerebral activities of human linguistics and driving understanding. The results showed that driving acti-vates hierarchical neural functions in the auditory cortex, which is analogous to abstraction in linguistic un-derstanding. Subsequently, we proposed a neural-informed, semantics-driven framework to understandcommon human driving behavior in a brain-inspired manner. This study highlights the pathway of fusingneuroscience into complex human behavior understanding tasks and provides a computational neural modelto understand human driving behaviors, which will enable autonomous vehicles to perceive and think like hu-man drivers.INTRODUCTIONAutonomous vehicles (AVs) continue to receive significant atten-tion worldwide because they have the potential to realize a safer,faster, and more efficient mode of transportation. Every day,almost 2,700 people are killed globally in traffic crashes6; fataland non-fatal crash injuries are estimated to cost approximately1.8 trillion US dollars between 2015 and 2030.7 By shiftingvehicle control from the human driver to machines via AVs,driver-related road crashes can be eliminated and thus savePatterns 4, 100730, June 9, 2023 ª 2023 The Author(s). 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).Please cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSACBDArticleEFigure 1. Four-stage development to understand the driving behavior of AVs(A) The surrounding vehicles are considered moving obstacles without self-consciousness.(B) The velocity of the surrounding vehicles is predicted using probability distribution outputs of discrete choice models.(C) The potential maneuvers of surrounding vehicles are surmised by recurrently applying short-time trajectory prediction models.(D) The intentions of the surrounding vehicles are understood from their contextual driving behaviors.(E) The mechanistic and biological requirements for AV development.lives.8 However, until the transition to fully autonomous transportis completed, AVs will inevitably share roads with human-drivenvehicles. During this transitory phase, AVs and human-driven ve-hicles need to share mutually interactive behaviors.9,10 Given thiscontext, it is impossible to expect every human driver to accom-modate certain traits and attributes of AVs, such as inconsistentor stilted driving behaviors (e.g., aggressive car following, jerk-ing, sudden braking, or unexpected mandatory lane changing).Existing studies have revealed that a lack of transparency inAV decision making creates a psychological barrier that affectshuman drivers’ trust in AVs11; human drivers expect AVs tomimic their driving behaviors to become trustworthy.12 A moreplausible approach is for AVs to acquire the ability to drive likehuman drivers, which would make it easier for other road usersto interpret their driving behaviors and react appropriately. Thiswould subsequently rebuild the driver’s trust and increase thesocial acceptability of AVs.13,14In recent years, various types of AVs were developed andtested in urban road scenarios, and they yielded promising re-sults and applications.15–17 Given that vehicular sensing andnavigation technologies are relatively mature,18 major concernswith AV adoption are related to whether AVs can interact appro-priately with the human-driven vehicles in the surrounding areas.However, research studies on understanding common drivingbehaviors and designing AVs to operate while following hu-man-like principles or brain-inspired mechanisms remain lack-ing. Therefore, we developed a method to understand AV drivingbehaviors as shown in Figure 1, where the red vehicle representsan AV and the blue vehicles represent the surrounding human-driven vehicles in a typical driving scenario.Unlike the classical vehicular trajectory prediction and routeplanning models19–21 widely accepted in the robotics field(Figure 1A) or the various discrete-choice driving models22,232 Patterns 4, 100730, June 9, 2023developed in the traffic engineering discipline (Figure 1B), recentresearch24–26 showed that subjective individual human drivingfactors are critical and cannot be neglected in the developmentof human-like AVs (Figure 1C). Unfortunately, as human drivingbehaviors evolve with an indefinite temporal dependency,state-of-the-art machine learning-based methods that partiallyimitate the nature of the human driving decision-making processmay lose the ability to adapt and generalize. For example, stand-alone data-driven intention recognizers (e.g., deep neural net-works) employed in machine learning-based methods are likelyto be trapped in the following dilemma: those with increasedtemporal inputs carry a more significant risk of overfitting localfeatures and the output confusing driving intentions, whereasthose with shorter temporal dependencies are too myopic to fullyunderstand the driving intentions of surrounding drivers, such ashuman drivers.Understanding common human driving behaviors that followthe human cerebral driving thinking mechanisms of humandrivers (Figure 1D) is necessary to address this dilemma.27–29As human driving behaviors are generated by humans ratherthan machines, AV development needs to be mechanisticallyand biologically plausible (Figure 1E).Our research is motivated by the fact that talking while drivingcan cause severe distractions because both behaviors or activ-ities share the same cerebral resources30,31 (right parietal re-sources32 and working memory in the prefrontal cortex33,34).Therefore, we attempt to fuse neuroscience and robotics in aneuroengineering manner35 in this study. To this end, we de-signed two separate electroencephalography (EEG) experi-ments to reveal the formation of cerebral driving thinking andevolution using well-studied linguistic analyses. We subse-quently present a semantics-driven method to understand com-mon driving behaviors for developing AVs in a brain-inspiredPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730Articlemanner. First, we compared the cerebral activities involved inhuman linguistic and driving thinking with EEG and reported bio-logical evidence that they share conspecific neural activations inthe auditory cortex. Second, we designed another EEG experi-ment by watching sequential and randomized driving videos ata 4 Hz frame rate by following similar research pipelines in aprevious study36 and verifying whether driving intentions canbe hierarchically and semantically understood. Proposed com-parisons from intention decoding to action formation revealedsimilar cortical activations and presented a sound research di-rection for semantically addressing human driving thinking.This idea was also supported by several cerebral and cognitivestudies,37–39 which indicated that understanding commondriving behavior using natural language processing (NLP) tech-niques, especially semantics-driven methods, was biologicallyplausible. We prototyped the approach to demonstrate thatthe proposed semantics-driven method for understanding com-mon human driving behaviors for AVs in a linguistic-inspiredmanner is mechanistic and biologically plausible.Neurological and cognitive research40 has revealed that thecognitive mechanism of humans requires both the perceptualencoding of the stimulus and its subsequent maintenance inthe working memory for further processing. Inspired by the hu-man cerebral function to process and understand speech,41the proposed prototypical implementation method imitates thecerebral pathway, which a human driver uses to understandthe contextual ‘‘driving language’’ of surrounding vehicles. Thishelps us to gain insights into employing grammatical or linguisticknowledge for addressing the nature of human driving thinking.First, detailed driving behaviors (namely, common driving sylla-bles) are abstractly represented by meaningful common drivinglexical units and phrasal units using a brain-inspired codebookby imitating the cortical encoding manner of human speechunits.42 Second, these phrasal units further form common drivingsentences based on the behavioral chain of each vehicle to dealwith long-term contextual dependency. Third, the sentences arecompared and analyzed at the document level using the latentDirichlet allocation (LDA) model43 to find language stimuli44,45;they are then projected into common driving topics in a high-dimensionalfeature space. Finally, a biologically plausiblespiking neural network (SNN)46 is used to understand contextualcommon driving semantics, such as those of human drivers, byutilizing the abstracted evolution characteristics of the commondriving topics in the projected feature space. Our experimentalresults provide computationally explicit evidence that the pro-posed demonstration can accurately capture and understandcommon human driving topic evolution in a flexible time andthat it can understand and predict potential stop-and-go orlane-changing behaviors semantically in a human-like manner.Our work includes the following key features.(1) We present biological findings that human cerebrallin-driving understanding activates similar hierarchicalguistic neural functions in the auditory cortex as thatwith linguistic understanding.(2) A semantics-driven framework is proposed to understandcommon driving behaviors in a human-like and linguistic-inspired manner contextually. This provides AVs with theability to perceive and think similar to human drivers.llOPEN ACCESS(3) A prototype is proposed to demonstrate the possiblecomputational neuralimplementation of the biologicalfindings. The evolution of human driving behavior is en-coded to highly abstracted common driving units andtopics in a brain-inspired manner, and it is understood us-ing an SNN with NLP techniques to tackle the contextualreasoning limitations of state-of-the-art human-likedriving behavior understanding methods and models.RESULTSEEG comparisons of driving and linguisticunderstandingWe collected EEG responses from 18 participants on three 5-mintasks—resting, driving, and listening—to investigate the relation-ship between human linguistics and driving-related thought pro-cesses. We compared the collected responses in the spatial andfrequency domains. Counterintuitively, as reflected in the topo-graphic maps (bottom half of Figure 2), the spatial domain anal-ysis shows that driving in a silent environment activates bothtemporal lobes, similar to that for a listening task. In addition,we plotted the cortical response power spectrum for each tem-poral lobe separately, as shown in the top half of Figure 2. Thevisual analysis showed that both activities had analogous powerintensities and trends. We further examined theta band re-sponses for the driving, listening, and resting tasks and plottedtheir mean value with respect to frequency in the middle linechart of the top half of Figure 2 because the phase patterns ofthe theta band (4–8 Hz) responses in the right human auditorycortex are closely related to the inherent brain rhythms of speechcomprehension.47 The results suggest that the driving task hassignificantly higher neural responses in each temporal lobe inthe theta band (4–8 Hz) compared with that in the resting state(p < 0.05, paired one-sided t test with false discovery rate[FDR] correction for multiple comparisons; see Table S1 for de-tails). This biological finding suggests that human driving under-standing probably activates neural functions in the auditory cor-tex, which are widely believed to occur during linguisticunderstanding tasks.Hierarchical structure of driving understandingThe most critical attribute of human linguistic understanding is itscombinatorial nature: the grammatical system relies on hierar-chical linguistic representations (e.g. words, phrases, and sen-tences) to understand intrinsic semantical knowledge.36 There-fore, we investigated the hierarchical structure of drivingunderstanding and designed a brain-inspired, semantics-drivenunderstanding framework for driving thinking, as shown in Fig-ure 3. First, we synthesized three isochronous, 4-Hz first-persondriving video clips—two with sequential frames and the otherswith random frames (Videos S1, S2, and S3). The EEG responsesof the participants when watching these low-frame-rate videoclips were recorded separately. A typical corresponding EEGtime-frequency map and frequency-power spectrum are shownin Figure 3A. For the stimuli involving sequentially driven videoframes, we observed significant rhythmic patterns in event-related spectral perturbation (ERSP) and distinctive spectralpeaks at 2 and 1 Hz (stronger power than neighbors, 0.5-HzPatterns 4, 100730, June 9, 2023 3Please cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSArticleFigure 2. EEG comparisons between driving and listening activitiesThe bottom half shows part of the topographic maps of the driving and listening tasks obtained using independent component correlation algorithm (ICA). Bothdriving and listening activities cause spatial activation of the T7 and T8 in the temporal lobes (auditory cortex). The top half shows the corresponding powercomparison in the frequency domain. The two lower line charts represent the power intensities of the temporal lobes from 2 to 25 Hz. The lines indicate theaverage values, and shaded areas indicate the corresponding value ranges. The middle line chart represents the power comparison of listening, driving, andresting with respect to the frequency in the theta band (4–8 Hz) in each temporal lobe.range), respectively (p < 0.05, paired one-sided t test with FDRcorrection for multiple comparisons; see Table S2 for details).This suggests that the driving understanding task activates neu-rallevels. Hence, weconclude that the phenomenon is consistent with the hierarchi-cal structure of listening to speech reported by Ding et al.36responses at different hierarchicalThis finding indicates that human drivers can employ similarintrinsic hierarchicallinguistic structures (i.e., word, phrase,and sentence levels) in the cortex to pre-process an abstractdynamic driving scenario instead of directly utilizing rawdriving information as part of the decision-making unit. In acorrelative experiment, the neural responses of random drivingvideo frames were relatively smooth, which eliminated thephrasal/sentential structure. The corresponding frequency-po-wer visualization suggested that participants merely obtainedany abstracted knowledge from these irrelevant frames; thisstrengthened the finding of hierarchical driving understandingabstraction in the human brain.Semantics-driven framework to understand humandriving thinkingInspired by the hierarchical structure of driving behavior under-standing, we proposed a three-level common driving semanticsstructure (corresponding to syllabic, lexical, and phrasal levels inlinguistic structure) to encode raw driving information into mean-ingful and comprehensible semantic units, as shown in Fig-ure 3B. After obtaining the raw environment observations duringdriving, we used the information from two adjacent observationsto infer the temporal dependency of the preceding vehicles.4 Patterns 4, 100730, June 9, 2023Then, we used sequential temporal dependencies to acquirethe spatial-temporal understanding of the driving environment.The proposed encoding method works in a pipeline similar tohow the cortex ensembles neurons to encode semantic knowl-edge.48 This bridges the raw information processing of humanlinguistic understanding and the AV’s driving behaviorunderstanding.Encoded common driving semantic units are temporallystored in the working memory and then used to understand com-mon driving semantics, as shown in Figure 3C. Then, thesesequential common driving semantic units in the episodic bufferare processed to retrieve common driving topics in a recurrentmanner. The retrieved common driving topic time series reflectsthe contextual evolution of the drivers’ understanding of driving.To make this evolution understandable and interpretable, weconverted the common driving topics into neural spikes (i.e., aset of firing neurons in the cortex) and introduced cerebralbehavioral recognizers to fully imitate the understanding anddecision-making functions of biological brains.49 Thus, humancerebral driving thinking is partially revealed and imitated,which can provide a biologically plausible, semantics-drivenframework and facilitate the design of next-generation human-like AVs.Understanding the evolution of common drivingsemanticsTo validate whether the proposed semantics-driven frameworkcan effectively understand human driving evolution, we de-implementation experiment using asigned a prototypicalPlease cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730ArticlellOPEN ACCESSFigure 3. Brain-inspired, semantics-driven framework for human driving understanding(A) Biological evidence of tracking hierarchical structures in human cerebral driving understanding. EEG time-frequency maps, frequency-power spectra, andhierarchical structure representations corresponding to different video stimuli are shown. The light curves in the frequency-power spectrum represent individual(legend continued on next page)Patterns 4, 100730, June 9, 2023 5Please cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSsemantics-driven approach to understand human driving inten-tions with ubiquitous traffic eye (UTE) naturalistic driving trajec-tory datasets4,5 (see experimental procedures for details). Weused the Shuangqiaomen Expressway, Nanjing, China (SQM1)dataset in this experiment, which contains both free-flow andcongested-flow traffic with a high proportion of lane-changingvehicles (see Figure S1 for the site layout and trajectory map).A pipeline of the proposed prototypical implementation methodis shown in Figure 4. As human drivers perceive the driving envi-ronment visually with discrete fuzzy thresholds rather than pre-cise measurements,50,51 we proposed a discrete, brain-inspiredcodebook to form common driving semantic units that imitatethe hierarchical abstraction process and acquire behavioral se-mantics.52,53 After encoding each vehicle trajectory into discretecommon driving semantic units, we translated the vehicular tra-jectory into a driving document per vehicle. The driving docu-ment was used to retrieve a common driving topic time series us-ing an LDA model recurrently in an unsupervised manner (seeNote S1, Figure S2, and Table S3 for details). The buffer lengthArticleFigure 4. Prototypical implementation methodfor understanding common driving semanticsThe trajectory of the surrounding vehicle is encodedby a bio-inspired hierarchical codebook and formedinto driving sentences. LDA and SNN then process thedriving sentence to obtain either the upcoming com-mon driving unit or the topic evolution.of the working memory was selected asseven based on previous biological evi-dence.54 Then, we employed an SNN asrecognizer andthe cerebral behaviortrained the SNN modelto predicttopic evolution recurrently in a self-super-vised and naturallanguage generationmanner55,56 (see Note S2, Figure S3, andTable S4 for details).Our experimental results indicate that theproposed method achieved high accuracyin recurrently predicting the most relevanttopic (1st second: 98.14%, 5th second:95.33%, and 10th second: 95%) while re-taining a low root-mean-square error(RMSE; 1st second: 0.06, 5th second: 0.09,and 10th second: 0.10; see Table S5 for de-tails). A visual assessment of Figure 5 sug-gests that the proposed method capturedstop-and-go driving behaviors and pre-dicted lane-changing maneuvers in thefew seconds (topic group duringnextthe 17th through the 21st second) without explicitly pre-definingthe potential lane-changing choice probability. This indicates theadvantage of the long-term contextual prediction stability of theproposed semantics-driven understanding framework.Semantics-driven multi-step vehicle velocity predictionWe used similar training pipelines to validate the effectiveness ofthe multi-step velocity prediction using the proposed method.We used the mean speed in each semantic unit to calculatethe RMSE of the speed-prediction task. We compared ourexperimental results with a deep neural network (DNN) andin Figure 6Amulti-output support vector regression (MSVR)and using the RMSE metric (see Tables S6 and S7 for details).The RMSE of the proposed method was relatively high becausethe discrete nature of the common driving unit encoding processmay induce measurement errors. However, as we predicted thevehicle velocity recurrently,the comparativemodels accumulated rapidly and surpassed the proposedmethod in the 4th second. In contrast, the RMSE of the proposedthe RMSE ofresponses, and the dark curve represents the grand average. Frequency bins with significantly stronger power than neighbors (0.5 Hz range) aremarked (**p < 0.01).(B) The three-level representation scheme of human driving understanding for forming common driving semantical units. Two-level abstractions are marked withred and green lines, respectively.(C) Semantics-driven driving understanding scheme. The sequential common driving semantical units are used to form common driving topics in the workingmemory, and the driving semantics are recognized by a cerebral behavior recognizer.6 Patterns 4, 100730, June 9, 2023Please cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730ArticlellOPEN ACCESSFigure 5. Visualization of multi-step driving topic evolution prediction(A) Observed topic evolution (1–23 s) for target vehicle #573 in the SQM1 dataset.(B) Predicted topic evolution (13–23 s) of the target vehicle.(C) Absolute difference between predicted and observed topic probability values.(D) Visualization of the corresponding vehicle trajectory. The yellow dotted line is the observed trajectory, and the red dotted line is the predicted trajectorycontaining congested and lane-changing driving behaviors. Each dot denotes a period corresponding to six video frames (0.33 s each).method remained consistent. This comparison explicitly con-firms that the proposed framework has excellent robustness inpredicting contextual driving behaviors in a flexible time andhas outstanding generalization ability, which helps overcomethe dilemma faced by standalone data-driven intentionrecognizers.To further validate the velocity prediction accuracy, we visual-ized the velocity prediction result of a typical vehicle inFigures 6B–6D. Although the proposed model generates multi-step discrete speed prediction according to the mean value ofeach bin in the brain-inspired codebook, the vehicle cannotadjust its longitudinal or lateral speed instantaneously as thestep function indicates. We employed a uniform filter to smooththe outputs and demonstrate the practical speed changes inboth directions. A visual assessment of the results showed thatfiltered speed in both directions was close to that of theobserved trajectory and could precisely reflect lane-changingand deceleration maneuvers. Therefore, we concluded fromthe common driving topic evolution understanding and vehiclevelocity prediction results that the proposed semantics-drivenunderstanding framework for human driving thought processwas biologically and computationally sound.DISCUSSIONThis article addressed the future coexistence challenge of auton-omous and human-driven vehicles and proposed a brain-inspired computational neural framework to understand the evo-lution of common driving topics and address this challenge. Wereported biological findings that driving and linguistic under-standing share conspecific hierarchical neural abstractions inthe auditory cortex and further proposed a semantics-drivenmethod to understand human driving thinking. The prototypicalimplementation experimental results showed that the proposedframework accurately predicted driving maneuvers in the nextfew seconds while retaining a lower error accumulation ratebenefiting from contextual driving semantics compared withother data-driven trajectory prediction baselines. Therefore,this work acts as a complementary component for current AVs,and it can provide AVs with the ability to perceive and thinksimilar to human drivers. We lay the biological and theoreticalfoundations to advance the development of human-like AVs.The goal of AVs is to drive similar to human drivers. AVs andthe surrounding human-driven vehicles should have a clear bidi-rectional understanding of each other’s driving intentions. State-of-the-art driving intention recognition models have endlesslyimproved the accuracy of trajectory prediction using highlystacked and complex neural networks. However, their modelinterpretability decreases quickly, which can create barriers tomutual human-machine understanding and result in traffic con-flicts or accidents. Our work pioneers the introduction of biolog-ical and cerebral research methods for analyzing human drivingunderstanding patterns related to driving behaviors and providesa prototypical demonstration to benefit future human-machineinteraction research.There are some limitations in our study that should be ad-dressed in future research. For example, the proposed methodfocuses on developing frameworks that would allow AVs tocomprehend and interpret the driving behaviors of surroundingvehicles, similar to human drivers. However, the method doesnot address the static driving environment quantification prob-lem. Weather, traffic signs, lane specifications, etc., stronglycorrelate with human driving behavior adaptation. Thus, anfuture direction would be to quantify cerebralimportantPatterns 4, 100730, June 9, 2023 7Please cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSArticleFigure 6. Visualization of multi-step vehicle velocity prediction(A) Overall accumulated RMSE value of the proposed method, DNN, and MSVR in multi-step prediction.(B) Observed, predicted, and filtered longitudinal speeds of vehicle #478 in the SQM1 dataset.(C) Observed, predicted, and filtered lateral speeds of the vehicle. The positive values refer to moving left and negative values to moving right.(D) Visualization of corresponding vehicle trajectory. The yellow dotted line is the observed trajectory, and the red dotted line is the predicted trajectory containinglane-changing and deceleration driving behaviors. Each dot denotes the duration of six video frames (0.33 s each).responses to various driving environment factors in a stimulus-based manner and to add them to the common driving seman-tics. Moreover, our work represents the first step toward devel-oping brain-inspired, human-like AVs and addressing vehicularinteractions. Therefore, future work should include a self-cor-recting mechanism that can actively consider the effect ofcontextual vehicular interactions in an attention-based manner.Autoencoders (AEs) that use machine learning algorithms,such as variational AEs, sparse AEs, and semantic AEs, can beembedded into the common semantic unit encoding processto achieve a higher-level abstracted semantic representation.Theoretically, these AEs are more biologically sound to imitatethe activation of cerebral neurons and fit continuous velocitychanges compared with that using the proposed discrete code-book. The employment of AEs neither improves method inter-pretability nor decreases computational complexity. Therefore,in this study, AEs were replaced by a more straightforward, yeteffective, codebook method following the model simplicity lawof Occam’s razor.57EXPERIMENTAL PROCEDURESResource availabilityLead contactFurther information and requests for resources and reagents should bedirected to and will be fulfilled by the lead contact, Xiqun (Michael) Chen(chenxiqun@zju.edu.cn).Materials availabilityThis study did not generate new unique materials.Data and code availabilitywebsite.2,3 The naturalistic vehicle trajectory dataset is publicly availablefrom the UTE Project by Southeast University and can be either downloadedfrom its website4 or requested from the authors.5Ethical approvalThe experimental procedures were approved by the Research Ethics Commit-tee of the School of Medicine of Zhejiang University (ZGL202204-7). This studywas conducted in accordance with the ethical standards of the 1964 Declara-tion of Helsinki. All participants provided written informed consent before theexperiment, and the possible consequences of the study were explained.ParticipantsEighteen normal-hearing, right-handed adult drivers (20–36 years old, mean24.72 years old; 8 female) participated in the EEG experiments. All participantswere qualified drivers with driving licenses, and their average driving experi-ence was 4.5 years. The sample size for previous linguistic experiments wasbetween 3 and12,36,41 and the basic phenomenon reported here was repli-cated in all EEG experiments in this study.EEG data processingThe EEG signals were recorded at 128 samples per second (1,024-Hz internal)with a 32-channel Emotiv Epoc Flex device and processed with EEGLAB soft-ware.58 The electrodes were positioned at Cz, Fz, Pz, Oz, FP1, FP2, F3, F4, F7,F8, FC1, FC2, FC5, FC6, C3, C4, FT9, FT10, T7, T8, CP5, CP1, CP2, CP6, P3,P4, P7, P8, PO9, PO10, O1, and O2, based on the standard 10–20 internationalEEG system (also known as IS). The raw signals were filtered by a band-passfinite impulse response (FIR) filter (0.5–50 Hz) to remove noise and segmentedin 2-s epochs for analysis. Second, interpolation was performed with adjacentelectrodes, and the data were rereferenced by computing the average refer-ence. Finally, the signals were decomposed using an independent componentcorrelation algorithm (ICA),59 and artifacts such as electrooculograms (EOGs)were removed.The processed EEG data used to generate the results and the prototypical im-plementation code to understand common human driving semantics of thisstudy are publicly available at Zenodo (https://doi.org/10.5281/zenodo.7714338).1 The Cityscapes driving video dataset is publicly available on itsEEG experiment 1Each participant was asked to listen to the news materials: ‘‘News 1 + 1: Howto disinfect correctly in work and life?’’ (Mandarin)60 at a comfortable loudnesslevel for 5 min. They then rested for 3 min, and the resting-state EEG was8 Patterns 4, 100730, June 9, 2023Please cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730Articlerecorded for 5 min. Finally, they rested for another 3 min and drove the pro-vided vehicle on a silent, closed road section for another 5 min. The EGG sig-nals were recorded simultaneously. The comparison of EEG responses be-tween the driving and resting tasks in the theta band (4–8 Hz)in eachtemporal lobe was analyzed using a paired one-sided t test (FDR corrected).The details are shown in Table S6.EEG experiment 2We synthesized three video clips using the Cityscapes driving video data-set2,3 (Movies S1–S3). The frame rate was 4 Hz for all video clips, and theframes were arranged in either a sequential (two video clips) or random(one video clip) order. The video clips were played on a desktop computerscreen without sound. Each participant was instructed to watch the videoclips randomly with two 3-min breaks. The EGG signals were recorded simul-taneously. The significance of distinctive EEG spectral peaks (at 4, 2, and1 Hz) was examined by testing if the neural response power in the targetbin was significantly stronger than the average of their corresponding neigh-boring 4 frequency bins (two bins on each side, 0.5-Hz range) using pairedone-sided t test (FDR corrected). The details of this power analysis areshown in Table S7.Driving trajectory datasetUTE naturalistic driving trajectory datasets4,5 were used in the experiments toevaluate the proposed semantics-driven model quantitatively. We used thevehicle trajectories captured from the SQM1 dataset for model evaluation.The SQM1 dataset was captured by an unmanned aerial vehicle (UAV) at analtitude of 310 m using aerial photography. It consisted of 822,712 trajectorypoints from 1,041 vehicles collected from a road section of 427 m. Thesedata provide precise vehicle position coordinates with a time resolution of0.1 s and contain the speed, acceleration, spacing, time distance, lane, etc.Brain-inspired codebookThe two-dimensional vehicle velocity (i.e., longitudinal and lateral speeds)was sampled every 0.25 s (4 Hz) and averaged every 0.5 s in a pairwise manner(2 Hz). Using a discrete codebook, the 2-Hz averaged velocity was divided into30 non-overlapping cells with different thresholds. The bin interval sets of thelateral and longitudinal speeds used in this study are shown in Figure 3. Theevenly spaced lateral speed interval is mapped non-linearly by v0 = tanhð2vÞ(v stands for the uniform speed threshold, and v’ denotes the mappedthreshold) to balance the data distribution because vehicles tend to maintaintheir lane most of the time. After obtaining the bin numbers in the first and later0.5 s, another codebook was constructed to represent the 1-Hz commondriving semantic units with 900 bin numbers, which imitates the three-levelabstraction process of human driving understanding.SUPPLEMENTAL INFORMATIONSupplementalpatter.2023.100730.information can be found online at https://doi.org/10.1016/j.ACKNOWLEDGMENTSThe work is partially supported by the National Natural Science Foundation ofChina (72171210), the China Postdoctoral Science Foundation (2021M702819),the Zhejiang Provincial Natural Science Foundation of China (LZ23E080002),and the National Key Research and Development Program of China(2020AAA0107400).AUTHOR CONTRIBUTIONSY.X., Z.L., Z.G., and X.(M.)C. proposed the question. S.S. and Z.Zh. designedand conducted the EEG experiments. M.G., Y.C., C.L., L.Z., Z.Ze., and B.Z.developed the algorithms. Y.X., W.Y.O., P.A., and M.E. wrote the paper.DECLARATION OF INTERESTSThe authors declare no competing interests.llOPEN ACCESSINCLUSION AND DIVERSITYWe support inclusive, diverse, and equitable conduct of research.Received: August 24, 2022Revised: December 5, 2022Accepted: March 20, 2023Published: April 18, 2023REFERENCES1. Gengmaosi (2022). Driving-Semantics-SNN. https://doi.org/10.5281/zen-odo.7714338.2. Cityscapes (2023). The Cityscapes Dataset. https://www.cityscapes-dataset.com/downloads/.3. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson,R., Franke, U., Roth, S., and Schiele, B. (2016). The Cityscapes dataset forsemantic urban scene understanding.the IEEEConference on Computer Vision and Pattern Recognition, pp. 3213–3223. https://doi.org/10.1109/CVPR.2016.350.In Proceedings of4. Ubiquitous Traffic Eyes (2023). Ubiquitous Traffic Eyes VehicleIdentification and Tracking Data. http://seutraffic.com/.5. Feng, R., Li, Z., Wu, Q., and Fan, C. (2021). Association of vehicle objectdetection and the time-space trajectory matching from aerial videos.Journal of Transport Information and Safety 39, 61–69+77. https://doi.org/10.3963/j.jssn.1674-4861.2021.02.008.6. Centers for Disease Control and Prevention (2020). Road Traffic Injuriesand Deaths—A Global Problem.7. Chen, S., Kuhn, M., Prettner, K., and Bloom, D.E. (2019). The global mac-roeconomic burden of road injuries: estimates and projections for 166countries. Lancet Planet. Health 3, e390–e398. https://doi.org/10.1016/S2542-5196(19)30170-6.8. Nunes, A., and Axhausen, K.W. (2021). Road safety, health inequity andthe imminence of autonomous vehicles. Nat. Mach. Intell. 3, 654–655.https://doi.org/10.1038/s42256-021-00382-3.9. Awad, E., Levine, S., Kleiman-Weiner, M., Dsouza, S., Tenenbaum, J.B.,Shariff, A., Bonnefon, J.F., and Rahwan, I. (2020). Drivers are blamedmore than their automated cars when both make mistakes. Nat. HumanBehav. 4, 134–143. https://doi.org/10.1038/s41562-019-0762-8.10. De Freitas, J., Censi, A., Walker Smith, B., Di Lillo, L., Anthony, S.E., andFrazzoli, E. (2021). From driverless dilemmas to more practical common-sense tests for automated vehicles. Proc. Natl. Acad. Sci. USA 118,e2010202118. https://doi.org/10.1073/pnas.2010202118.11. Shariff, A., Bonnefon, J.-F., and Rahwan, I. (2017). Psychological road-blocks to the adoption of self-driving vehicles. Nat. Human Behav. 1,694–696. https://doi.org/10.1038/s41562-017-0202-6.12. Craig, J., and Nojoumian, M. (2021). Should self-driving cars mimic humandriving behaviors? In HCI in Mobility, Transport, and Automotive Systems,pp. 213–225. https://doi.org/10.1007/978-3-030-78358-7_14.13. Park, C., and Nojoumian, M. (2022). Social acceptability of autonomousvehicles: unveiling correlation of passenger trust and emotional response.In HCI in Mobility, Transport, and Automotive Systems, pp. 402–415.https://doi.org/10.1007/978-3-031-04987-3_27.14. Shahrdar, S., Park, C., and Nojoumian, M. (2019). Human trust measure-ment using an immersive virtual reality autonomous vehicle simulator. InProceedings of the 2019 AAAI/ACM Conference on AI (Honolulu, USA:Ethics, and Society), pp. 515–520. https://doi.org/10.1145/3306618.3314264.15. Hancock, P.A., Nourbakhsh, I., and Stewart, J. (2019). On the future oftransportation in an era of automated and autonomous vehicles. Proc.Natl. Acad. Sci. USA 116, 7684–7691. https://doi.org/10.1073/pnas.1805770115.16. Harel, D., Marron, A., and Sifakis, J. (2020). Autonomics: in search of afoundation for next-generation autonomous systems. Proc. Natl. Acad.Sci. USA 117, 17491–17498. https://doi.org/10.1073/pnas.2003162117.Patterns 4, 100730, June 9, 2023 9Please cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730llOPEN ACCESSArticle17. Feng, S., Yan, X., Sun, H., Feng, Y., and Liu, H.X. (2021). Intelligent drivingintelligence test for autonomous vehicles with naturalistic and adversarialenvironment. Nat. Commun. 12, 748. https://doi.org/10.1038/s41467-021-21007-8.18. Almalioglu, Y., Turan, M., Trigoni, N., and Markham, A. (2022). Deeplearning-based robust positioning for all-weather autonomous driving.Nat. Mach.Intell. 4, 749–760. https://doi.org/10.1038/s42256-022-00520-5.19. Nguyen, A.T., Rath, J., Guerra, T.M., Palhares, R., and Zhang, H. (2021).Robust set-invariance based fuzzy output tracking control for vehicleautonomous driving under uncertain lateral forces and steering con-straints. IEEE Trans. Intell. Transport. Syst. 22, 5849–5860. https://doi.org/10.1109/TITS.2020.3021292.20. Spielberg, N.A., Brown, M., Kapania, N.R., Kegelman, J.C., and Gerdes,J.C. (2019). Neural network vehicle models for high-performance auto-mated driving. Sci. Robot. 4, eaaw1975. https://doi.org/10.1126/sciro-botics.aaw1975.35. Cheng, G., Ehrlich, S.K., Lebedev, M., and Nicolelis, M.A.L.(2020).Neuroengineering challenges of fusing robotics and neuroscience. Sci.Robot. 5, eabd1911. https://doi.org/10.1126/scirobotics.abd1911.36. Ding, N., Melloni, L., Zhang, H., Tian, X., and Poeppel, D. (2016). Corticallinguistic structures in connected speech. Nat.tracking of hierarchicalNeurosci. 19, 158–164. https://doi.org/10.1038/nn.4186.37. Lo¨ ffler, A., Sylaidi, A., Fountas, Z., and Haggard, P. (2021). A hierarchicalattractor network model of perceptual versus intentional decision up-dates. Nat. Commun. 12, 2020. https://doi.org/10.1038/s41467-021-22017-2.38. Ganesh, G., Nakamura, K., Saetia, S., Tobar, A.M., Yoshida, E., Ando, H.,Yoshimura, N., and Koike, Y. (2018). Utilizing sensory prediction errors forintention decoding: a new methodology. Sci. Adv. 4,movementeaaq0183. https://doi.org/10.1126/sciadv.aaq0183.39. Snyder, L.H., Batista, A.P., and Andersen, R.A. (1997). Coding of intentionin the posterior parietal cortex. Nature 386, 167–170. https://doi.org/10.1038/386167a0.21. Pek, C., Manzinger, S., Koschi, M., and Althoff, M. (2020). Using onlineverification to prevent autonomous vehicles from causing accidents.Nat. Mach.Intell. 2, 518–528. https://doi.org/10.1038/s42256-020-0225-y.40. Courtney, S.M., Ungerleider, L.G., Keil, K., and Haxby, J.V.(1997).Transient and sustained activity in a distributed neural system for humanworking memory. Nature 386, 608–611. https://doi.org/10.1038/386608a0.22. Tian, J., Zhu, C., Chen, D., Jiang, R., Wang, G., and Gao, Z. (2021). Carfollowing behavioral stochasticity analysis and modeling: perspectivefrom wave travel time. Transp. Res. Part B Methodol. 143, 160–176.https://doi.org/10.1016/j.trb.2020.11.008.23. Mehr, N., Li, R., and Horowitz, R. (2021). A game theoretic macroscopicmodel of lane choices at traffic diverges with applications to mixed–autonomy networks. Transp. Res. Part B Methodol. 144, 45–59. https://doi.org/10.1016/j.trb.2020.11.004.24. Kolekar, S., de Winter, J., and Abbink, D. (2020). Human-like drivingbehaviour emerges from a risk-based driver model. Nat. Commun. 11,4850. https://doi.org/10.1038/s41467-020-18353-4.25. Xia, Y., Qu, Z., Sun, Z., and Li, Z. (2021). A human-like model to understandsurrounding vehicles’ lane changing intentions for autonomous driving.IEEE Trans. Veh. Technol. 70, 4178–4189. https://doi.org/10.1109/TVT.2021.3073407.26. Russell, H.E.B., Harbott, L.K., Nisky, I., Pan, S., Okamura, A.M., andGerdes, J.C. (2016). Motor learning affects car-to-driver handover in auto-mated vehicles. Sci. Robot. 1, eaah5682. https://doi.org/10.1126/sciro-botics.aah5682.27. Milford, M. (2020). Elegans inspires self-driving cars. Nat. Mach. Intell. 2,661–662. https://doi.org/10.1038/s42256-020-00245-3.41. Ding, N., and Simon, J.Z. (2012). Emergence of neural encoding of audi-tory objects while listening to competing speakers. Proc. Natl. Acad.Sci. USA 109, 11854–11859. https://doi.org/10.1073/pnas.1205381109.42. Luo, C., and Ding, N. (2020). Cortical encoding of acoustic and linguisticrhythms in spoken narratives. Elife 9, e60433. https://doi.org/10.7554/eLife.60433.43. Blei, D.M., Ng, A.Y., and Jordan, M.I. (2003). Latent dirichlet allocation.J. Mach. Learn. Res. 3, 993–1022. https://doi.org/10.5555/944919.944937.44. Mollica, F., Bacon, G., Zaslavsky, N., Xu, Y., Regier, T., and Kemp, C.(2021). The forms and meanings of grammatical markers support efficientcommunication. Proc. Natl. Acad. Sci. USA 118, e2025993118. https://doi.org/10.1073/pnas.2025993118.45. Schrimpf, M., Blank, I.A., Tuckute, G., Kauf, C., Hosseini, E.A., Kanwisher,N., Tenenbaum, J.B., and Fedorenko, E. (2021). The neural architecture oflanguage: integrative modeling converges on predictive processing. Proc.Natl. Acad. Sci. USA 118, e2105646118. https://doi.org/10.1073/pnas.2105646118.46. Maass, W. (1997). Networks of spiking neurons: the third generation ofneural network models. Neural Network. 10, 1659–1671. https://doi.org/10.1016/S0893-6080(97)00011-7.28. Lechner, M., Hasani, R., Amini, A., Henzinger, T.A., Rus, D., and Grosu, R.(2020). Neural circuit policies enabling auditable autonomy. Nat. Mach.Intell. 2, 642–652. https://doi.org/10.1038/s42256-020-00237-3.47. Luo, H., and Poeppel, D. (2007). Phase patterns of neuronal responsesreliably discriminate speech in human auditory cortex. Neuron 54, 1001–1010. https://doi.org/10.1016/j.neuron.2007.06.004.29. (2022). Safe driving cars. Nat. Mach. Intell. 4, 95–96. https://doi.org/10.1038/s42256-022-00456-w.30. Ship, A.N. (2010). The most primary of care — talking about driving anddistraction. N. Engl. J. Med. 362, 2145–2147. https://doi.org/10.1056/NEJMp0910137.31. Klauer, S.G., Guo, F., Simons-Morton, B.G., Ouimet, M.C., Lee, S.E., andDingus, T.A. (2014). Distracted driving and risk of road crashes amongnovice and experienced drivers. N. Engl. J. Med. 370, 54–59. https://doi.org/10.1056/NEJMsa1204142.32. Atchley, P., Dressel, J., Jones, T.C., Burson, R.A., and Marshall, D. (2011).Talking and driving: applications of crossmodal action reveal a special rolefor spatial language. Psychol. Res. 75, 525–534. https://doi.org/10.1007/s00426-011-0342-7.33. Kim, J., Gulati, T., and Ganguly, K. (2019). Competing Roles of slow oscil-lations and delta waves in memory consolidation versus forgetting. Cell179, 514–526.e13. https://doi.org/10.1016/j.cell.2019.08.040.34. Kissler, M.J., Kissler, K., and Burden, M.(2021). Toward a medical‘‘Ecology of attention’’. N. Engl. J. Med. 384, 299–301. https://doi.org/10.1056/NEJMp2027190.48. Grewe, B.F., Gr€undemann, J., Kitch, L.J., Lecoq, J.A., Parker, J.G.,Marshall, J.D., Larkin, M.C., Jercog, P.E., Grenier, F., Li, J.Z., et al.(2017). Neural ensemble dynamics underlying a long-term associativememory. Nature 543, 670–675. https://doi.org/10.1038/nature21682.49. DeWolf, T. (2021). Spiking neural networks take control. Sci. Robot. 6,eabk3268. https://doi.org/10.1126/scirobotics.abk3268.50. Our Neurophysiology Correspondent (1970). Perception: detectors in hu-man visual system. Nature 226, 903. https://doi.org/10.1038/226903a0.51. Maddox, J. (1983). Fuzzy sets make fuzzy logic. Nature 306, 637. https://doi.org/10.1038/306637a0.52. Wang, X., Ma, X., and Grimson, W.E.L. (2009). Unsupervised activityperception in crowded and complicated scenes using hierarchicalBayesian models. IEEE Trans. Pattern Anal. Mach. Intell. 31, 539–555.https://doi.org/10.1109/TPAMI.2008.87.53. Hospedales, T., Gong, S., and Xiang, T. (2012). Video behaviour mining us-ing a dynamic topic model. Int. J. Comput. Vis. 98, 303–323. https://doi.org/10.1007/s11263-011-0510-7.54. Baddeley, A. (1992). Working memory. Science 255, 556–559. https://doi.org/10.1126/science.1736359.10 Patterns 4, 100730, June 9, 2023Please cite this article in press as: Xia et al., Understanding common human driving semantics for autonomous vehicles, Patterns (2023), https://doi.org/10.1016/j.patter.2023.100730ArticlellOPEN ACCESS55. Jing, L., and Tian, Y. (2021). Self-supervised visual feature learning withdeep neural networks: a survey. IEEE Trans. Pattern Anal. Mach. Intell.43, 4037–4058. https://doi.org/10.1109/TPAMI.2020.2992393.56. Kiros, R., Zhu, Y., Salakhutdinov, R.R., Zemel, R., Urtasun, R., Torralba, A.,and Fidler, S. (2015). Skip-thought vectors. Adv. Neural Inf. Process. Syst.28, 3294–3302. https://doi.org/10.5555/2969442.2969607.57. Smith, T.F. (1980). Occam’s razor. Nature 285, 620. https://doi.org/10.1038/285620a0.58. Delorme, A., and Makeig, S. (2004). EEGLAB: an open source toolbox foranalysis of single-trial EEG dynamics including independent componentanalysis. J. Neurosci. Methods 134, 9–21. https://doi.org/10.1016/j.jneu-meth.2003.10.009.59. Hyv€arinen, A., and Oja, E. (2000). Independent component analysis: algo-rithms and applications. Neural Network. 13, 411–430. https://doi.org/10.1016/S0893-6080(00)00026-5.60. CCTV13 (2022). News 1+1: How to Disinfect Correctly in Work and Life?.https://tv.cctv.com/2020/02/12/VIDEEF9vgUooUlYitRbj2wzO200212.shtml.Patterns 4, 100730, June 9, 2023 11