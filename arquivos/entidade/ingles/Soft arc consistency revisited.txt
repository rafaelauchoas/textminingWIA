Artificial Intelligence 174 (2010) 449–478Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintSoft arc consistency revisitedM.C. Cooper a, S. de Givry b, M. Sanchez b, T. Schiex b,∗a IRIT, University of Toulouse III, 31062 Toulouse, Franceb UBIA, UR-875, INRA, F-31320 Castanet Tolosan, Francec Center for Machine Perception, Czech Technical University, 12135 Praha 2, Czech Republica r t i c l ei n f oa b s t r a c t, M. Zytnicki b, T. Werner cArticle history:Received 31 January 2009Received in revised form 25 January 2010Accepted 28 January 2010Available online 2 February 2010Keywords:Valued constraint satisfaction problemWeighted constraint satisfaction problemSoft constraintsConstraint optimizationLocal consistencySoft arc consistencyGraphical modelSubmodularitylocal cost functions defined over discrete variables.The Valued Constraint Satisfaction Problem (VCSP) is a generic optimization problemIt hasdefined by a network ofapplications in Artificial Intelligence, Operations Research, Bioinformatics and has beenused to tackle optimization problems in other graphical models (including discrete MarkovRandom Fields and Bayesian Networks). The incremental lower bounds produced by localconsistency filtering are used for pruning inside Branch and Bound search.In this paper, we extend the notion of arc consistency by allowing fractional weights and byallowing several arc consistency operations to be applied simultaneously. Over the rationalsand allowing simultaneous operations, we show that an optimal arc consistency closure cantheoretically be determined in polynomial time by reduction to linear programming. Thisdefines Optimal Soft Arc Consistency (OSAC).To reach a more practical algorithm, we show that the existence of a sequence of arcconsistency operations which increases the lower bound can be detected by establishingarc consistency in a classical Constraint Satisfaction Problem (CSP) derived from the originalcost function network. This leads to a new soft arc consistency method, called, Virtual ArcConsistency which produces improved lower bounds compared with previous techniquesand which can solve submodular cost functions.These algorithms have been implemented and evaluated on a variety of problems, includingtwo difficult frequency assignment problems which are solved to optimality for the firsttime. Our implementation is available in the open source toulbar2 platform.© 2010 Elsevier B.V. All rights reserved.1. IntroductionGraphical model processing is a central problem in AI. The optimization of the combined cost of local cost functions,central in the valued CSP framework [52], captures problems such as weighted Max-SAT, Weighted CSP or Maximum Prob-ability Explanation in probabilistic networks. It also has applications in areas such as resource allocation [9], combinatorialauctions, optimal planning, and bioinformatics [50]. Valued constraints can be used to code both classical crisp constraintsand cost functions.Since valued constraint satisfaction is NP-hard, heuristics are required to speed up brute-force exhaustive search. Byshifting weights between cost functions, soft arc consistency allows us to transform a problem in an equivalent problem.This problem reformulation can provide strong, incrementally maintainable lower bounds which are crucial for Branch andBound search [44].* Corresponding author.E-mail addresses: cooper@irit.fr (M.C. Cooper), simon.degivry@toulouse.inra.fr (S. de Givry), marti.sanchez@toulouse.inra.fr (M. Sanchez),thomas.schiex@toulouse.inra.fr (T. Schiex), matthias.zytnicki@toulouse.inra.fr (M. Zytnicki), werner@cmp.felk.cvut.cz (T. Werner).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.02.001450M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478Similarly to classical arc consistency in CSPs (constraint satisfaction problems), previously-defined soft arc consistencyproperties are enforced by the chaotic application of local soft arc consistency operations shifting integer costs between differentscopes, until a fixpoint is reached [19,3]. Unlike the arc consistency closure in CSPs, this fixpoint is often not unique andmay lead to different lower bounds. In this paper, we instead consider local consistencies enforced by carefully plannedsequences of soft arc consistency operations which necessarily increase the lower bound. Since costs may need to be dividedinto several parts in order to be shifted in several directions, the resulting transformed problem may contain fractionalcosts. By allowing the introduction of rational multiples of costs, we both avoid the intractability of finding an optimal softarc consistency closure involving only integer costs [19] and produce a strictly stronger notion of soft arc consistency.The two new techniques presented in this paper aim at finding a reformulation of the original problem P with an op-timized constant cost term c∅. This constant cost provides an explicit lower bound provided that all costs are non-negative.Optimal soft arc consistency (OSAC) identifies a sequence of soft arc consistency operations (shifting of costs between costfunctions, of which at most one has arity greater than 1) which yields an optimal reformulation. Intermediate reformula-tions may contain negative costs provided all costs in the final version are non-negative. Such operations can be found inpolynomial time by solving a linear program [54]. We considerably extend this result by showing that a polynomial-timealgorithm exists even in the presence of crisp constraints coded by infinite costs and an upper bound coded by using anaddition-with-ceiling aggregation operator.Alternatively, we show that when a problem is not Virtual Arc Consistent (VAC), it is possible to find a sequence of softarc consistency operations which improve the lower bound and are such that all intermediate problems have non-negativecosts. Our iterative VAC algorithm is based on applying arc consistency in a classical CSP which has a solution if and onlyif P has a solution of cost c∅. We show that OSAC is strictly stronger than VAC. However, finding a lower bound using ourVAC algorithm is much faster than establishing OSAC, and hence has potentially many more practical applications.The idea of using classical local consistency to build lower bounds in Max-CSP or Max-SAT is not new. On Max-CSPproblems, [48] used independent arc inconsistent subproblems to build a lower bound. For Max-SAT, [45] used minimal UnitPropagation inconsistent subproblems to build a lower bound. These approaches do not use problem transformations butrely on the fact that the inconsistent subproblems identified are independent and costs can simply be summed. They lack theincrementality of soft consistency operations. In Max-SAT again, [31] used Unit Propagation inconsistency to build sequencesof integer problem transformations but possibly strictly above the arc level, generating higher-arity weighted clauses (costfunctions). OSAC and VAC remain at the arc level by allowing rational costs. It should be pointed out that our VAC algorithmis similar to the “Augmenting DAG” algorithm independently proposed by [39] for preprocessing 2-dimensional grammars,recently reviewed in [56]. Our approach is more general, in that we can treat cost functions of arbitrary arity, infinite costsand a finite upper bound.Note that the special case of real-valued binary VCSPs over Boolean domains has been extensively studied under thename of quadratic pseudo-Boolean function optimization [7]. In the case of Boolean domains, it is well known that findingan equivalent quadratic posiform representation (i.e. an equivalent binary VCSP) with an optimal value of c∅ can be formu-lated as a linear programming problem [30] and can even be solved by finding a maximum flow in an appropriately definednetwork [7]. It is also worth noting that in this special case of Boolean binary VCSPs, determining whether there exists azero-cost solution is an instance of 2SAT and hence can be completely solved in polynomial time.The two new notions presented in this paper (optimal soft arc consistency and virtual arc consistency) can be applied tooptimization problems over finite domains of arbitrary size, involving local cost functions of arbitrary arity. Crisp constraintscan be coded by infinite costs and an upper bound can be coded by using an addition-with-ceiling aggregation operator. Weshow that the resulting arc consistency properties have attractive theoretical properties, being capable of solving differentpolynomial classes of weighted CSP without detecting them a priori. We also show their strengths and limitations on variousrandom and real problem instances. Some of the problems considered are solved for the first time to optimality using theselocal consistencies.We begin in Section 2 with the definition of a valued constraint satisfaction problem. Section 3 introduces the notion ofan equivalence-preserving transformation and gives the three basic equivalence-preserving transformations that are requiredto establish all forms of soft arc consistency considered in this paper. In Section 4 we review previously defined notions ofsoft arc consistency. These definitions are necessary to define the soft arc consistency EDAC [43], with which we compareboth theoretically and experimentally the new notions of soft arc consistency defined in this paper. Section 5 defines OSAC(Optimal Soft Arc Consistency) and Section 6 reports the results of experimental trials which demonstrate the potentialutility of OSAC during preprocessing. The rest of the paper is devoted to Virtual Arc Consistency (VAC) which providesa practical alternative to OSAC which can be applied during search. Section 7 introduces VAC and shows formally theconnection between this definition and the existence of a sequence of soft arc consistency operations which increase thelower bound. Section 8 introduces our VAC algorithm through examples while Section 9 gives the necessary subroutines indetail. Section 10 shows that certain tractable classes, including permuted submodular functions, can be directly solved byVAC. As the detailed example in Appendix A shows, our VAC algorithm may enter an infinite loop. This justifies the useof a heuristic version called VACε . Section 11 reports the results of our experimental trials on VACε . Finally, an alternativealgorithm converging towards VAC and techniques for finding better bounds are discussed in Section 12.M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–4784512. Valued constraint satisfactionThe Constraint Satisfaction Problem (CSP) consists in finding an assignment to n finite-domain variables such that aset of constraints are satisfied. Crisp yes/no constraints in the CSP are replaced by cost functions in the Valued ConstraintSatisfaction Problem (VCSP) [52]. A cost function returns a valuation (a cost, a weight or a penalty) for each combinationof values for the variables in the scope of the function. Crisp constraints can still be expressed by, for example, assigningan infinite cost to inconsistent tuples. In the most general definition of a VCSP, costs lie in a valuation structure (a positivetotally-ordered monoid) (cid:3)E, ⊕, (cid:2)(cid:5) where E is the set of valuations totally ordered by (cid:2) and combined using the aggregationoperator ⊕. In this paper we only consider integer or rational costs.A Valued Constraint Satisfaction Problem can be seen as a set of valued constraints, which are simply cost functionsplaced on particular variables. Formally,Definition 2.1. (See Schiex [51].) A Valued Constraint Satisfaction Problem (VCSP) is a tuple (cid:3) X, D, C, Σ(cid:5) where X is a setof n variables X = {1, . . . , n}, each variable i ∈ X has a domain of possible values di ∈ D, C is a set of cost functions andΣ = (cid:3)E, ⊕, (cid:2)(cid:5) is a valuation structure. Each cost function (cid:3)S, c S (cid:5) ∈ C is defined over a tuple of variables S ⊆ X (its scope)as a function c S from the Cartesian product of the domains di(i ∈ S) to E.Purely for notational convenience, we suppose that no two cost functions have the same scope. This allows us to identifyC with the set of scopes S of cost functions c S in the VCSP. We write ci as a shorthand for c{i} and ci j as a shorthand forc{i, j}. Without loss of generality, we assume that C contains a cost function ci for every variable i ∈ X as well as a zero-arityconstant cost function c∅.Notation. For S ⊆ X we denote the Cartesian product of the domains di(i ∈ S) (i.e. the set of possible labellings for thevariables in S) by (cid:4)(S).Let Z ⊆ Y ⊆ X with Y = { y1, . . . , yq} and Z = {z1, . . . , zp}. Then, given an assignment t = (t y1 , . . . , t yq ) ∈ (cid:4)(Y ), t[ Z ]denotes the sub-assignment of t to the variables in Z , i.e. (tz1 , . . . , tzp ). If Z is a singleton {z1} then t[ Z ] will also bedenoted as tz1 for simplicity.The usual query on a VCSP is to find an assignment t whose valuation (i.e. total cost) is minimal.Definition 2.2. In a VCSP V = (cid:3) X, D, C, Σ(cid:5), the valuation of an assignment t ∈ (cid:4)( X) is defined byValV (t) =(cid:2)(cid:3)c S(cid:4)t[S](cid:5)(cid:6)S∈CTo solve a VCSP we have to find an assignment t ∈ (cid:4)( X) with a minimum valuation.2.1. Weighted CSPIn the VCSPs studied in this paper, the aggregation operator ⊕ is either the usual addition operator or the addition-with-ceiling operator +m defined as follows:∀a, b ∈ {0, 1, . . . , m} a +m b = min{a + b, m}A Weighted Constraint Satisfaction Problem (WCSP) [44] is a VCSP over the valuation structure Sm = (cid:3){0, 1, . . . , m}, +m, (cid:3)(cid:5)where m is a positive integer or infinity. It has been shown that the WCSP framework is sufficient to model all VCSPs overdiscrete valuation structures in which ⊕ has a partial inverse (a necessary condition for soft arc consistency operations tobe applicable) [14].When m is finite, all solutions with a cumulated cost reaching m are considered as equally and absolutely bad. This is asituation which applies at a node of a branch and bound search tree on a WCSP problem whenever the best known solutionhas cost m.The Boolean valuation structure S1 = (cid:3){0, 1}, +1, (cid:3)(cid:5) allows us to express only crisp constraints, with the valuation 0representing consistency and 1 representing inconsistency. In this paper, in order to express VCSPs and CSPs in a commonframework, we will often represent CSPs as VCSPs over the valuation structure S1.The valuation structure S∞ = (cid:3)N ∪ {∞}, +, (cid:3)(cid:5), where N is the set of non-negative integers, can be embedded in thevaluation structure Q+ = (cid:3)Q+ ∪ {∞}, +, (cid:3)(cid:5) where Q+represents the set of non-negative rational numbers. Similarly, thevaluation structure Sm can be embedded in the valuation structure Qm = (cid:3)Qm ∪ {∞}, +m, (cid:3)(cid:5) where Qm is the set of rationalnumbers α satisfying 0 (cid:4) α < m. For clarity of presentation, we use ∞ as a synonym of m in Qm, since this valuationrepresents complete inconsistency. We use ⊕ to represent the aggregation operator (which is + in Q+and +m in Qm). Thepartial inverse of the aggregation operator ⊕ is denoted by (cid:11) and is defined in both Q+and Qm by α (cid:11) β = α − β (for allvaluations α, β such that ∞ > α (cid:3) β) and ∞ (cid:11) β = ∞ (for all valuations β).452M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478In the remainder of the paper, we assume, unless stated otherwise, that the valuation structure Σ of the VCSP to beor Qm. These rational valuation structures enrich the set of available operations on costs, compared tosolved is either Q+the integer valuation structures S∞ and Sm, by allowing for the circulation of fractional weights.3. Soft arc consistency operationsIn this section we introduce the basic operations which allows us to reformulate a VCSP by shifting costs.Definition 3.1. Two VCSPs V 1 = (cid:3) X, D, C1, Σ(cid:5), V 2 = (cid:3) X, D, C2, Σ(cid:5) are equivalent if ∀t ∈ (cid:4)( X), ValV 1 (t) = ValV 2 (t).(cid:7)Definition 3.2. The subproblem of a VCSP (cid:3) X, D, C, Σ(cid:5) induced by F ⊆ C is the problem VCSP(F ) = (cid:3) X F , D F , F , Σ(cid:5), whereX F =c S ∈F S and D F = {di: i ∈ X F }.Definition 3.3. For a VCSP (cid:3) X, D, C, Σ(cid:5), an equivalence preserving transformation on F ⊆ C is an operation which transformsthe subproblem VCSP(F ) into an equivalent VCSP.When F contains at most one cost function c S such that |S| > 1, such an equivalence-preserving transformation is calleda Soft Arc Consistency (SAC) operation.Algorithm 1: The basic equivalence-preserving transformations required to establish different forms of soft arc consis-tency.12345678910111213141516(* Precondition: α (cid:2) min{c S (t): t ∈ (cid:4)(S) and ti = a} *);Procedure Project(S, i, a, α)ci (a) ← ci (a) ⊕ α;foreach (t ∈ (cid:4)(S) such that ti = a) doc S (t) ← c S (t) (cid:11) α;(* Precondition: α (cid:2) ci (a) and |S| > 1 *);Procedure Extend(i, a, S, α)foreach (t ∈ (cid:4)(S) such that ti = a) do(cid:8)β ← c∅ ⊕ (c S (t) ← ((c S (t) ⊕ β) (cid:11) β) ⊕ α;j∈S c j (t j ));ci (a) ← ci (a) (cid:11) α;(* Precondition: α (cid:2) min{ci (a) : a ∈ di } *);Procedure UnaryProject(i, α)foreach (a ∈ di ) doci (a) ← ((ci (a) ⊕ c∅) (cid:11) c∅) (cid:11) α;c∅ ← c∅ ⊕ α;Algorithm 1 gives three basic equivalence-preserving transformations which are also SAC operations [19]. Project projectsweights from a cost function (on two or more variables) to a unary cost function. Extend performs the inverse operation,sending weights from a unary cost function to a higher-order cost function. Finally UnaryProject projects weights from aunary cost function to the nullary cost function c∅ which is a lower bound on the value of any solution. For example,if ∀a ∈ di , ci(a) (cid:3) α, then a call UnaryProject(i, α) increases the constant term c∅ by α while decreasing by α each ci(a)(a ∈ di ). For each of the SAC operations given in Algorithm 1, a precondition is given which guarantees that the resultingcosts are non-negative.The addition and then subtraction of the same weight β in line 10 of Extend allows us to detect certain inconsistenttuples, since this sets c S (t) to ∞ when c S (t) ⊕ β = ∞. Similarly, the addition and then subtraction of the weight c∅ inline 15 of UnaryProject sets ci(a) to ∞ when ci(a) + c∅ = ∞. Extend and UnaryProject can thus modify cost functions evenwhen the argument α = 0. This happens, for example, for UnaryProject in the valuation structure Q10 if ci(a) = c∅ = 5 sinceci(a) becomes ((5 ⊕ 5) (cid:11) 5) (cid:11) 0 which is equal to 10 = ∞ in Q10.Of course, if ⊕ is the addition of real numbers and all costs are finite, then Extend and UnaryProject cannot modifycost functions when α = 0. Indeed, in the case of finite costs, Extend and UnaryProject can be considerably simplified bycanceling β and c∅ respectively.4. Soft arc consistency techniquesIn this section we briefly review previously-defined notions of soft arc consistency and, in particular, Existential Direc-tional Arc Consistency (EDAC) [43]. EDAC was the strongest known polynomial-time achievable form of soft arc consistencybefore the introduction of the two notions (OSAC and VAC) presented in this paper. Note that EDAC has only been definedin the special case of binary [43] and ternary [50] VCSPs. Recall that we assume that the valuation structure of the VCSP iseither Q+or Qm.Definition 4.1. (See Larrosa and Schiex [44].) A VCSP is node consistent if for any variable i ∈ {1, . . . , n},M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–4784531. ∀a ∈ di , ci(a) ⊕ c∅ < ∞,2. ∃a ∈ di such that ci(a) = 0.Node consistency can be established by repeated calls to UnaryProject until convergence. We assume, for simplicityof presentation, that values a such that ci(a) = ∞ are automatically deleted from di . Node consistency determines themaximum lower bound that can be deduced from the unary and nullary constraints; it transforms the VCSP accordingly sothat this lower bound is stored explicitly in the nullary constraint c∅.A VCSP is generalized arc consistent if all infinite weights have been propagated and no weights can be projected downto unary constraints. Formally,Definition 4.2. (See Cooper and Schiex [19].) A VCSP (cid:3) X, D, C, Σ(cid:5) is generalized arc consistent if for all S ∈ C such that |S| > 1we have:1. ∀t ∈ (cid:4)(S), c S (t) = ∞ if c∅ ⊕ (i∈S ci(ti)) ⊕ c S (t) = ∞,2. ∀i ∈ S, ∀a ∈ di , ∃t ∈ (cid:4)(S) such that ti = a and c S (t) = 0.(cid:8)If the VCSP is binary, then generalized arc consistency is known as (soft) arc consistency. Generalized arc consistency canbe established by repeated calls to Project, together with extensions of zero weights (i.e. calls of the form Extend(_,_,_,0)) topropagate inconsistencies, until convergence.Consider a VCSP which is node consistent and generalized arc consistent. Extending non-zero weights and re-establishinggeneralized arc consistency and node consistency may lead to an increase in c∅ [51]. One way to guarantee the convergenceof such a process is to restrict the direction in which non-zero weights can be extended by placing a total ordering on thevariables.Definition 4.3. (See Cooper [12].) A binary VCSP is directional arc consistent (DAC) with respect to an order < on the variablesif for all ci j such that i < j, ∀a ∈ di , ∃b ∈ d j such that ci j(a, b) = c j(b) = 0.If for all b ∈ d j either ci j(a, b) or c j(b) is non-zero, then it is possible to increase ci(a) by transferring the non-zero costsc j(b) to ci j by calls to Extend and then projecting costs from ci j to ci(a). Hence establishing Directional Arc Consistencynot only projects weights down to unary constraints, but also shifts weights towards variables which occur earlier in theorder <. This tends to concentrate weights on the same variables which, after applying node consistency, tends to lead toan increase in the lower bound c∅.Consider a binary VCSP with e binary cost functions and maximum domain size d. Then directional arc consistency canbe established in O(ed2) time [19,44]. As in classical CSP, DAC solves tree-structured VCSP if the variable order used is builtfrom a topological ordering of the tree.Definition 4.4. (See Cooper [12].) A binary VCSP is full directional arc consistent (FDAC) with respect to an order < on thevariables if it is arc consistent and directional arc consistent with respect to <.Full directional arc consistency can be established in O(ed2) time if the valuation structure is Q+time if the valuation structure is Sm [44].[12] and in O(end3)Existential arc consistency (EAC) is independent of a variable order. For each variable i in turn, EAC shifts costs to ci ifthis can lead to an immediate increase in c∅ via UnaryProject.Definition 4.5. (See Larrosa et al. [43].) A binary VCSP is existential arc consistent (EAC) if it is node consistent and if ∀i,∃a ∈ di such that ci(a) = 0 and for all cost functions ci j , ∃b ∈ d j such that ci j(a, b) = c j(b) = 0. Value a is called the EACsupport value of variable i.Definition 4.6. (See Larrosa et al. [43].) A binary VCSP is existential directional arc consistent (EDAC) with respect to an order< on the variables if it is existential arc consistent and full directional arc consistent with respect to <.Over the valuation structure Sm, existential directional arc consistency can be established in O(ed2 max{nd, m}) time [43].An important difference between local consistency in CSPs and local consistency in VCSPs is that the closure underthe corresponding local consistency operations is unique in CSPs but this is not, in general, the case for VCSPs [51]. Forexample, even for a 2-variable VCSP with domains of size 2, the arc consistency and existential arc consistency closures arenot necessarily unique. Similarly, for problems with more than two variables, in general, the FDAC closure is not unique.Fig. 1(a), (b) illustrates separately the two techniques FDAC and EAC (which together form the stronger notion EDAC). In) can be transformed into the equivalent VCSP on the rightboth cases, the VCSP on the left (over the valuation structure Q+454M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478Fig. 1. Examples of (a) full directional arc consistency (b) existential arc consistency.by establishing, respectively, FDAC and EAC. In both cases, the lower bound c∅ is increased from 0 to 1. Each oval representsa domain and each • a value. Names of values and the variable number are written outside the oval (names of values onthe side and the variable number underneath). A line joining (i, a) and ( j, b) represents a weight ci j(a, b) = 1 and a valueα written next to a ∈ di (and inside the oval) represents ci(a) = α. The absence of a line or the absence of a cost next to adomain value indicates a zero cost. In Fig. 1(a) the VCSP on the right is obtained by establishing FDAC with a lexicographicDAC ordering, via the following SAC operations:1. Project({1, 2},1,F,1), Project({2, 3},3,F,1): this moves unit costs from the binary cost functions c12 and c23 down to c1(F )and c3(F ) (which establishes arc consistency).2. Extend(3,F,{1, 3},1): we send a unit cost from c3(F ) up to the binary cost function c13, so that c13(T , F ) = c13(F , F ) = 1.3. Project({1, 3},1,T,1): this moves a unit cost from c13 to c1(T ) (which establishes directional arc consistency).4. UnaryProject(1,1): we increase the lower bound c∅ by replacing c1(T ) = c1(F ) = 1 by c∅ = 1 (which establishes nodeconsistency).In order to establish EAC, weights are shifted towards the same variable whenever this can lead to an immediate increasein c∅. In Fig. 1(b) the existential arc consistent VCSP on the right is obtained by shifting weights towards variable 3, via thefollowing SAC operations:1. Extend(2,T,{2, 3},1), Project({2, 3},3,F,1): we send a unit cost from c2(T ) up to c23 which allows us to project a unit costfrom c23 down to c3(F ).2. Extend(1,F,{1, 3},1), Project({1, 3},3,T,1): in an entirely similar manner, we send a unit cost from c1(F ) to c3(T ).3. UnaryProject(3,1): we increase the lower bound by replacing c3(F ) = c3(T ) = 1 by c∅ = 1.The VCSP on the left of Fig. 1(a) is EAC and the problem on the left of Fig. 1(b) is FDAC, which proves that these twoproperties are complementary. EDAC [43], which is simply the combination of FDAC and EAC, represents the state-of-the-artsoft arc consistency technique against which we must compare the new techniques defined in this paper.EDAC tries to find a set of SAC operations which increases c∅, but does not perform an exhaustive search over all suchsets. This is because FDAC can only extend non-zero weights in one direction, while EAC can only extend weights in theneighborhood of each variable. In the next section we will show, somewhat surprisingly, that it is possible to perform anexhaustive search over all sets of SAC operations in polynomial time.5. Optimal soft arc consistencyAn arc consistency closure of a VCSP P is any VCSP obtained from P by repeated calls to Project and UnaryProject untilconvergence. After each call of Project or UnaryProject, the resulting VCSP must be valid in the sense that the cost functionstake values lying in the valuation structure.Definition 5.1. An arc consistency closure of a VCSP P is optimal if it has the maximum lower bound c∅ among all arcconsistency closures of P .In a previous paper we proved that over a discrete valuation structure such as the non-negative integers together withinfinity, the problem of finding the optimal arc consistency closure is NP-hard [19]. However, we will show in this sectionthat extending the valuation structure to include all rationals and extending our notion of arc consistency closure allows usM.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478455Fig. 2. The linear program to establish optimal soft arc consistency (after propagation of infinite weights).to determine an optimal arc consistency closure in polynomial time by a simple reduction to linear programming. This isnot so much a practical proposition as a theoretical result to demonstrate that extending the valuation structure not onlyallows us to produce better lower bounds but also avoids intractability.We now relax the preconditions of the soft arc consistency (SAC) operations Extend, Project and UnaryProject so thatthese operations can introduce negative finite costs. Over the rationals, the only restriction on costs after application of arelaxed SAC operation is that they are not −∞.Definition 5.2. Over the valuation structure Q+UnaryProject such that the resulting cost functions take values in Q ∪ {∞} (respectively {α ∈ Q: α < m} ∪ {∞}).(respectively Qm), a relaxed SAC operation is a call to Extend, Project orIf we apply a sequence of relaxed SAC operations to produce a VCSP P , then in order to be able to use c∅ as a lowerbound, we must ensure that the costs in P are all non-negative (although intermediate problems may contain negativefinite costs).Definition 5.3. Given a VCSP P over the valuation structure Q+operations which transforms P into a valid VCSP (i.e. such that all cost functions take values in the valuation structure).or Qm, a SAC transformation is a sequence of relaxed SACDefinition 5.4. A VCSP P over the valuation structure Q+applied to P increases c∅.or Qm is optimal soft arc consistent (OSAC) if no SAC transformationOver the valuation structure Q+, a SAC transformation involving the shifting of only finite costs can be considered as a setof relaxed SAC operations: the order in which operations are applied is of no importance since, in this case, the operationsExtend, Project and UnaryProject all commute.Affane and Bennaceur [1] split integer costs by propagating a fraction w i j of the binary cost function ci j towards variablei and a fraction 1 − w i j towards variable j (where 0 (cid:4) w i j (cid:4) 1) and suggested determining the optimal values of theweights w i j . In a more recent paper, Bennaceur and Osmani [4] suggested introducing different weights w iajb for each pairof domain values (a, b) ∈ di × d j . As we show in this paper, it turns out that assigning a different weight to each triple(i, j, a), where a ∈ di , allows us to find optimal weights in polynomial time.Theorem 5.5. Let P be a VCSP over the valuation structure Q+such that the arity of cost functions in P is bounded by a constant. It ispossible to find in polynomial time a SAC transformation of P which maximizes the lower bound c∅ and hence establishes optimal softarc consistency.Proof. Firstly, as in [12], we can assume that all infinite costs have been propagated using a standard generalized arcconsistency algorithm [46]. Note that we assume that c S (t) has been set to ∞ if ci(ti) = ∞ for some i ∈ S. At this point nomore infinite costs can be propagated in the VCSP by the operations Extend, Project or UnaryProject.We then want to determine the set of finite SAC operations which when applied simultaneously maximizes the increasein c∅. For each S ∈ C such that |S| > 1 and for each i ∈ S, let p Si (a) be the sum of the weights projected from c S to ci(a)minus the sum of the weights extended from ci(a) to c S . Let ui be the sum of the weights projected (by UnaryProject)from ci to c∅. Thus the problem is to maximizei ui such that the resulting cost functions take on non-negative values.This is equivalent to the linear program given in Fig. 2. We can simply ignore the inequalities for which ci(a) = ∞ orc S (t) = ∞ since they are necessarily satisfied. The remaining inequalities define a standard linear programming problemwith O(ed + n) variables (if e is the number of cost functions, n the number of variables and d the maximum domain size)which can be solved in polynomial time [33]. Since no infinite weights can be propagated and no further propagation offinite weights can increase c∅, the resulting VCSP is optimal soft arc consistent. (cid:2)(cid:9)Karmarkar’s interior-point algorithm for linear programming has O(N 3.5 L) time complexity, where N is the number ofvariables and L the number of bits required to encode the problem [33]. Under the reasonable assumption that e (cid:3) n, the456M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478Fig. 3. No sequence of SAC operations can be applied to the VCSP in (a), but a set of simultaneous SAC operations transforms it into the VCSP in (b).number of variables N in the linear program in Fig. 2 is O(ed) and the number of bits L required to code it is O(edr log M),where r is the maximum arity of cost functions and M the maximum finite cost. Therefore this linear program can besolved in O(e4.5d(r+3.5) log M) time.A weaker version of Theorem 5.5, limited to 3-variable subproblems, is the basis of the algorithm to establish 3-cyclicconsistency [13]. Note that the linear program in Fig. 2 is the dual of the linear relaxation of the 01-integer program definedin thesis [38,36]. Both the primal and dual linear programs were first studied in [54].It is important to note that there is a difference between SAC transformations (which are sequences of relaxed SACoperations) and sequences of SAC operations: the former are stronger due to the fact that intermediate problems can containnegative costs. When only finite costs are shifted in Q+, a SAC transformation is equivalent to a set of SAC operations.Several SAC operations applied simultaneously can produce a valid VCSP even when no individual SAC operation can beapplied. As an example, consider the binary VCSP P over domains d1 = d3 = {a, b, c}, d2 = d4 = {a, c} and valuation structureQ+illustrated in Fig. 3(a). All unary costs are equal to zero. All edges represent a unit cost. c∅ is assumed to be zero. P isnode consistent and arc consistent, and hence no cost α > 0 can be projected (or unary-projected) without introducing anegative cost. Also, since all unary costs are equal to zero, no cost α > 0 can be extended without introducing a negativecost. It follows that no SAC operation (Extend, Project or UnaryProject) can transform P into a valid VCSP. This implies thatno sequence of SAC operations can modify P , and, in particular, that P is EDAC.However, we may perform the following relaxed SAC operations:1. Extend(2, c, {2, 3}, 1): we move a (virtual) cost of 1 from c2(c) to three pairs inside c23, namely c23(c, a), c23(c, b) andc23(c, c). This introduces a negative cost c2(c) = −1.2. Project({2, 3}, 3, a, 1), Project({2, 3}, 3, b, 1): this moves two unit costs to c3(a) and c3(b).3. Extend(3, a, {3, 4}, 1), Extend(3, b, {3, 1}, 1): these two unit costs are moved inside c34 and c31 respectively.4. Project({3, 4}, 4, c, 1): this moves a unit cost of 1 to c4(c).5. Project({3, 1}, 1, a, 1), Project({3, 1}, 1, c, 1): this moves two unit costs of 1 to c1(c) and c1(a).6. Extend(1, a, {1, 2}, 1), Project({1, 2}, 2, c, 1): we reimburse our initial loan on value c2(c).7. Extend(1, c, {1, 4}, 1), Project({1, 4}, 4, a, 1): we send a unit cost to value c4(a).8. Finally, the application of UnaryProject(4, 1) yields the problem on the right of Fig. 3 with a lower bound c∅ = 1.If the relaxed SAC operations are applied in the above order, then the intermediate problems between steps 1 and 6 havethe invalid negative weight c2(c) = −1, but in the final problem all weights are non-negative. Since all costs movementsare finite this sequence of relaxed SAC operations is equivalent to a set of simultaneous relaxed SAC operations. This set of=operations corresponds to a solution of the linear programming problem given in Fig. 2 in which p232c= p14p144a1c= −1 and p233aWe have seen that applying a set of SAC operations simultaneously leads to a stronger notion of consistency thanapplying a set of SAC operations sequentially. An obvious question is whether another even stronger form of consistencyexists which transforms a VCSP into an equivalent VCSP.= u4 = 1 (all other variables being equal to zero).= p313b= p233b= p121a= p343a= p344c= p122c= p311a= p311cDefinition 5.6. A VCSP P is in-scope c∅-irreducible if there is no equivalent VCSP Q with the same set of cost functionscopes as P and such that c Q∅ are the nullary cost functions in P , Q ).∅ (where c P∅ > c P∅, c QThe following theorem is a direct consequence of Lemma 5.2 in [13] (in which it was proved for any finitely-boundedstrictly monotonic valuation structure, hence in Q+).M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478457Theorem 5.7. Let P be a binary VCSP with all unary and binary cost functions and in which cost functions take values in Q+hence all costs are finite). If no SAC transformation applied to P produces a VCSP Q with c Q(and∅, then P is in-scope c∅-irreducible.∅ > c PThus, when all costs are finite rational numbers, the linear programming approach can be used to establish in-scopec∅-irreducibility in binary VCSPs. This is unfortunately not the case if infinite costs can occur. Consider, for example, thegraph-coloring problem on a triangle with two colors, expressed as a VCSP with costs in {0, ∞}. The problem is clearlyinconsistent and hence equivalent to a VCSP with a single cost function c∅ = ∞, but no SAC transformation can be appliedto this VCSP to increase c∅.We conclude this section by showing that optimal soft arc consistency can also be established in polynomial time overthe valuation structure Qm. In this case, however, we may have to solve many linear programs.Theorem 5.8. Let P = (cid:3) X, D, C, Qm(cid:5) be a VCSP such that the arity of cost functions in P is bounded by a constant r. Then it is possibleto find in polynomial time an optimal soft arc consistent VCSP equivalent to P .Proof. In the following, we use S to represent any constraint scope such that |S| (cid:3) 1. For each (cid:3)S, c S (cid:5) ∈ C and for eacht ∈ (cid:4)(S), let P S,t denote the VCSP which is identical to P except that the domain of each variable i ∈ S has been reducedis Q+to a singleton consisting of the value ti assigned by the tuple t to variable i and the valuation structure of P S,t.By performing operations in the valuation structure Q+, the upper bound m is temporarily ignored. If establishing OSACin P S,t produces a lower bound c∅ (cid:3) m, then in the original valuation structure Qm this represents an inconsistency. Thismeans that setting c S (t) = ∞ in P produces a VCSP which is equivalent to the original VCSP P . Denote by OSACm(S, t)the establishment of OSAC in P S,t and the setting of c S (t) to ∞ in P if the resulting lower bound in the transformedis greater than or equal to m. Now consider the algorithm OSACm which simply repeatedly applies OSACm(S, t) forP S,tall constraint scopes S and all tuples t ∈ (cid:4)(S) until convergence. Denote by Pthe VCSP which results when OSACm isapplied to P . The complexity of OSACm is bounded by the time complexity of (edr)2 times the time complexity of the linearprogram in Fig. 2, where r is the maximum arity of cost functions in P .∞We now only need to establish OSAC one more time in P∞, considered as a VCSP over the valuation structure Q+∗∞It remains to show that Pσ denote the corresponding sequence of relaxed SAC operations which establish OSAC in Pwhich results when this sequence of operations σ is applied to POSAC over Qm cannot introduce new infinite costs. Suppose, for a contradiction, that there exists a sequence σ (cid:14)SAC operations in Qm which when applied to Pσ (cid:14)a value ρ (cid:3) m. Preduced to a singleton and the valuation structure is Q+this cost ρ from c S (t) to c∅), the sequence σ , σ (cid:14)of Ptherefore, by the definition of P∗is optimal soft arc consistent over Qm. To prove this, it is sufficient to show that establishingof relaxedsets some cost c S (t) to ∞. Without loss of generality, we can assume that∞S,t sets c S (t) toexcept that the domain of each variable i ∈ S has been. By adding at most one Project and one UnaryProject (to transfer∞S,t . But, by the definition, andno such sequence can exist. Hence no sequence of relaxed SAC operations can introduce infinite costs in Pis minimal, so that c S (t) is the first cost set to ∞ by σ (cid:14)∞S,t represents the VCSP which is identical to Pcan be expanded so that it sets c∅ to ρ (cid:3) m in P, no sequence of relaxed SAC operations can increase c∅ in P. Then the combined sequence σ , σ (cid:14)is equivalent to P .applied to P, and let P. Clearly P. (cid:2)∞∞∗∗∗∗∗. Letdenote the VCSP∞6. Experimental trials of OSACIn this section, the linear programming problem defined by OSAC was solved using ILOG CPLEX version 9.1.3 (using thebarrier algorithm). We first evaluate the strength and the computational cost of the lower bounds produced after a directapplication of OSAC on different problems.6.1. Evaluation of OSAC lower boundsRandom MaxCSP. The first set of instances processed are random Max-CSP instances created by the random_vcsp generator1using the usual four parameter model (n: number of variables, d: size of domains, e: number of randomly-chosen binaryconstraints, and t: percentage of randomly-chosen forbidden tuples inside each constraint). The aim is to find an assign-ment that minimizes the number of violated constraints. Four different categories of problems with domain size 10 weregenerated following the same protocol as in [44]: sparse loose (SL, 40 variables), sparse tight (ST, 25 variables), dense loose(DL, 30 variables) and dense tight (DT, 25 variables). These instances are available in the Cost Function Library archive athttps://mulcyber.toulouse.inra.fr/projects/costfunctionlib.Samples have 50 instances. Table 1 shows respectively the average optimum value, the average values of the EDAClower bound and the average value of the OSAC lower bound. On loose problems, OSAC and EDAC leave the lower boundunchanged. This shows that higher level local consistencies are required here. However for tight problems, OSAC is extremelypowerful, providing lower bounds which are sometime three times better than EDAC bounds.1 http://www.inra.fr/mia/ftp/T/VCSP/src/random_vcsp.c.458M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478Table 1Results of preprocessing random WCSPs by OSAC and EDAC. For each category of problems (S: Sparse (e = 2.5n), D: Dense (e = n(n−1)T: Tight), the average cost of an optimal solution and the average lower bound c∅ produced by EDAC and OSAC is reported.8), L: Loose,OptimumEDAC lb.OSAC lb.SL2.8400ST19.684.2612.30DL2.2200DT29.629.9619.80Table 2Radio link frequency assignment problems: for each problem, the problem size (number of values), the best known upper bound, the best knownlower bound and the corresponding cpu-time needed to produce it. These cpu-times are taken from [49] using a 2.66 GHz Intel Xeon with 32 GB(scen06r , scen07r ), from [21] on a SUN UltraSparc 10 300 MHz workstation (scen08), and from [37] on a DEC 2100 A500MP workstation(graph11r , graph13r ). These are followed by the lower bounds (c∅) produced by EDAC and OSAC, as well as the cpu-time needed to enforceEDAC and OSAC using CPLEX on a 3 GHz Intel Xeon with 2 GB.Total # of valuesBest known ubBest known lbBest lb cpu-timeEDAC lbOSAC lbEDAC cpu-timeOSAC cpu-timescen06r319633893389(cid:14)(cid:14)22103.5(cid:14)(cid:14)< 1(cid:14)(cid:14)621scen07r4824343,592343,592(cid:14)(cid:14)386,03510,00031,453.1(cid:14)(cid:14)< 13530(cid:14)(cid:14)scen08r14,194262216(cid:14)(cid:14)13,452648(cid:14)(cid:14)< 1(cid:14)(cid:14)6718graph11r574730803016(cid:14)(cid:14)74,11327102957(cid:14)(cid:14)< 1(cid:14)(cid:14)492graph13r13,15310,1109925(cid:14)(cid:14)23,21187229797.5(cid:14)(cid:14)< 1(cid:14)(cid:14)6254Frequency assignment. The second set of benchmarks is defined by instances of the Radio Link Frequency Assignment Prob-lem of the CELAR [9].2 This problem consists in assigning frequencies to a set of radio links in such a way that all the linksmay operate together without noticeable interference. Some RLFAP instances can be naturally cast as binary WCSPs.These problems have been extensively studied and their current state is reported on the FAP web site at http://www.zib.de/fap/problems/CALMA. Despite extensive studies, the gap between the best upper bound (computed by localsearch methods) and the best lower bound (computed by exponential time algorithms) is not closed except for instancescen06, and more recently instance scen07 [49]. The problems considered here are the scen0{6,7,8}reduc.wcspand the graph1{1,3}reducmore.wcsp instances which have already been through different strong preprocessing (seethe Benchmarks section in [22]). In order to differentiate these from the equivalent full unprocessed instances, a subscript ris used to identify them in the following tables.As Table 2 shows, OSAC offers substantial improvements over EDAC, especially on the graph11 and graph13 instances.to 4% and 3% respectively. The polynomial time lower boundsFor these instances, OSAC reduces the optimality gap ub−lbubobtained by OSAC are actually close to the best known (exponential time) lower bounds.6.2. OSAC preprocessing before tree searchTo actually assess the practical interest of OSAC we tried to solve problems using a tree-search algorithm maintainingEDAC after OSAC preprocessing.Tight random MaxCSP. The first experiment was performed on problems where OSAC preprocessing seems effective: randomtight MaxCSPs. The difficulty here lies in the fact that CPLEX is a floating point solver while the open source WCSP solverused (toolbar version 3.0 in C language, section Algorithms in [22], extended with OSAC) deals with integer costs. Toaddress this issue, we use “fixed point” costs: for all WCSPs considered, we first multiply all costs by a large integer constantλ = 1000, and then solve the linear programming problem defined by OSAC using integer variables (instead of floatingpoint). The first integer solution found is used. The resulting problem has integer costs and can be tackled by toolbar.3This means that we shift from a polynomial problem to an NP-hard one. In practice, we found that the problems obtainedhave a very good linear continuous relaxation and are not too expensive to solve as integer problems (up to 3.5 slower thanLP relaxation in the following experiments). Using a polytime rational LP solver would allow to recover a polynomial timebound.Fig. 4 reports cpu-time (top) and size of the tree search (bottom) for dense tight problems of increasing size. The timelimit was set to 1800 seconds.Clearly, for small problems (with less than 29 variables), OSAC is more expensive than the resolution itself. As theproblem size increases, OSAC becomes effective and for 33 variables, it divides the total cpu-time by roughly 2. The number2 We would like to thank the French Centre Electronique de l’Armement for making these instances available.3 The code of toolbar has been modified accordingly: if a solution of cost 2λ is known for example and if the current lower bound is 1.1λ thenbacktrack occurs since all global costs in the original problem are integer and the first integer above 1.1 is 2, the upper bound.M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478459Fig. 4. Experimental evaluation of OSAC as a preprocessing technique on random dense tight problems. Three cpu-times are reported: (1) OSAC MIP: timetaken to get the first integer solution, (2) MEDAC: time taken to solve the original problem by maintaining EDAC [43] in toolbar with default parametersand a good initial upper bound, (3) OSAC+MEDAC is the sum of OSAC MIP with the time needed by MEDAC to solve the OSAC problem (with the samedefault parameters and upper bound).Fig. 5. Experimental evaluation of OSAC as a preprocessing technique on random problems with a binary clique tree structure. The figure uses a logarithmicscale for cpu-time for different constraint tightnesses (below 40%, problems are satisfiable).of nodes explored in both cases shows the strength of OSAC used as a preprocessing technique (remember that EDAC ismaintained during search).OSAC and DAC ordering. The strength of OSAC compared to local consistencies such as directional arc consistency (DAC)is that is does not require an initial variable ordering. Indeed, DAC directly solves tree-structured problems but only if thevariable ordering used for DAC enforcing is a topological ordering of the tree. To evaluate to what extent OSAC can overcome460M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478these limitations, we used random problems structured as binary clique trees as in [23]. Each clique contains 6 variableswith domain size 5, each sharing 2 variables with its parent clique. The overall tree height is 4, leading to a total numberof 62 variables, with a graph density of 11%.On these clique-tree problems, two DAC orderings were used. One is compatible with a topological ordering of the binarytree (and should give good lower bounds), the inverse order can be considered as pathological. The cpu-times for MEDACalone (default toolbar parameters and a good initial upper bound) and OSAC+MEDAC (as previously) are shown in eachcase in Fig. 5. Clearly, OSAC leads to drastic (up to 20 fold) improvements when a bad DAC ordering is used. Being usedjust during preprocessing, it does not totally compensate for the bad ordering. But, even when a good DAC ordering is used,OSAC gives impressive (up to 4 fold) speedups, especially on tight problems.Finally, we tried to solve the challenging open CELAR instances after OSAC preprocessing. Despite the strength of OSAC,all problems remained unsolvable.7. Virtual arc consistencyAlthough OSAC is optimal in terms of strength of the induced lower bound, the associated linear program is often toolarge for OSAC to be beneficial in terms of resolution speed. However, OSAC showed that instead of the chaotic application ofinteger equivalence-preserving transformations, the planning of a set of rational SAC operations may be extremely beneficial.In this section, we introduce Virtual Arc Consistency (VAC) which plans sequences of rational SAC operations which increasethe lower bound c∅. These sequences are found by means of classical (generalized) arc consistency in a CSP Bool(P ) derivedfrom the VCSP P . Over the valuation structures Q+or Qm (and under the reasonable assumption that c∅ (cid:15)= ∞), the relationsin Bool(P ) contain exactly those tuples which have zero cost in P . Bool(P ) is a CSP whose solutions are exactly those n-tuples x such that ValP (x) = c∅.Definition 7.1. If P = (cid:3) X, D, C, Σ(cid:5) is a VCSP over the valuation structure Q+or Qm, then Bool(P ) is the classical CSP(cid:3) X, D, C(cid:5) where, for all scopes S (cid:15)= ∅, (cid:3)S, R S (cid:5) ∈ C if and only if ∃(cid:3)S, c S (cid:5) ∈ C , where R S is the relation defined by ∀x ∈(cid:4)(S) (t ∈ R S ⇔ c S (t) = 0).We say that a CSP is empty if at least one of its domains is the empty set.Definition 7.2. A VCSP P is virtual arc consistent if the (generalized) arc consistency closure of the CSP Bool(P ) is non-empty.The following theorem shows that if establishing arc consistency in Bool(P ) detects an inconsistency, then it is possibleto increase c∅ by a sequence of soft arc consistency operations.Theorem 7.3. Let P be a VCSP over the valuation structure Q+or Qm such that c∅ < ∞. Then there exists a sequence of soft arcconsistency operations which when applied to P leads to an increase in c∅ if and only if the arc consistency closure of Bool(P ) isempty.Proof. Throughout this proof we consider Bool(P ) as a VCSP over the Boolean valuation structure S1 = (cid:3){0, 1}, +1, (cid:3)(cid:5). Todifferentiate the cost functions in Bool(P ) from those in P , we denote the cost functions of scope S in P and Bool(P ) by c Sand c S , respectively.⇒: Let O 1, . . . , O k be a sequence of soft arc consistency operations (Project, Extend or UnaryProject) in P which producean equivalent VCSP in which c∅ has increased. We assume, without loss of generality, that O k is the UnaryProject operation(cid:14)which increases c∅. For each i = 1, . . . , k, if O i projects or extends a weight α, let Oi be the corresponding operation inBool(P ) except that α is replaced by α where(cid:10)α =1 if α > 00 if α = 0(cid:14)i(cid:14)1, . . . , Ois Project (S, j, a, 0.5) in P , then O(cid:14)i is Extend ( j, a, S, 0). Let Bool(P )i represent the result of applying Ois Extend ( j, a, S, 0), thenFor example, if O i(cid:14)i to Bool(P ) and P i represent the result ofO(cid:14)k never decreases a cost function c S (since 1 (cid:11) 1 = 1 in S1). By a simpleapplying O 1, . . . , O i to P . The sequence Oinductive argument we can see that, for |S| (cid:3) 1 and i < k, c S (t) = 1 in Bool(P )i whenever c S (t) > 0 in P i (and hence the(cid:14)i+1 are satisfied). If O i is a projection which assigns a non-zero weight to c j(a), then c j(a) = 1 afterpreconditions of O(cid:14)i . Finally, sinceapplying OO t is a unary projection which increases c∅ by some weight α > 0, it follows that O(cid:14)i . If O i is an extension which assigns a non-zero weight to c S (t), then c S (t) = 1 after applying Ois Project (S, j, a, 1) in Bool(P ); if O i(cid:14)1, . . . , O⇐: Suppose that there exists a sequence of arc consistency operations which lead to a domain wipe-out in Bool(P ).We can assume, without loss of generality, that no two of these operations are identical since applying the same arcconsistency operation twice is redundant in CSPs. There is a corresponding sequence O 1, . . . , O k of soft arc consistencyoperations (Project, Extend or UnaryProject) in Bool(P ), viewed as a VCSP over the Boolean valuation structure S1, whichset c∅ to 1 in Bool(P ). We assume, without loss of generality, that O k is the UnaryProject operation which sets c∅ to 1 inBool(P ).(cid:14)k sets c∅ to α = 1.M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478461Let δ be the minimum non-zero weight occurring in P , i.e. δ = min{c S (t): ((cid:3)S, c S (cid:5) ∈ C) ∧ (t ∈ (cid:4)(S)) ∧ (c S (t) > 0)}. For(cid:14)i = 1, . . . , k, let Oi be the soft arc consistency operation in P which is identical to O i except that the weight being projected(cid:14)or extended is δ/2i . For example, if O i is Project (S, i, a, 1) in Bool(P ), then Oi is Project (S, i, a, δ/2i ) in P . We divide bytwo each time to ensure that strictly positive costs remain strictly positive. Let Bool(P )i represent the result of applying(cid:14)(cid:14)O 1, . . . , O i to Bool(P ) and P i represent the result of applying O1, . . . , Oi to P . By a simple inductive argument, the mini-(cid:14)mum non-zero cost in P i is at least δ/2i . Since the operations O i and Oi are identical except for the weight being projected(cid:14)or extended, Bool(P i) is identical to Bool(P )i for i < k (and hence the preconditions of Oi+1 are satisfied). It follows thatO(cid:14)k necessarily increases c∅ by δ/2k > 0 in P since O k sets c∅ to 1 in Bool(P ). (cid:2)It may not seem that increasing c∅ by a very small amount (such as the increase of δ/2k demonstrated in the proofof Theorem 7.3) is worthwhile. However, if the original weights in P were all integers, then c∅ > 0 actually implies thatValP (x) (cid:3) 1, for all x, thus allowing us to increase the lower bound used by branch and bound by 1. In this case the lowerbound is strictly greater than c∅.VAC is easily shown to be stronger than Existential Arc Consistency [43]. Indeed, EAC can be seen as applying virtual arcconsistency but limited to a single iteration of arc consistency in Bool(P ). In EAC, weights are transferred virtually to eachvariable from all its neighbors; if a unary projection with a non-zero weight is possible, then we trace back and actuallyperform the necessary soft arc consistency operations. Thus EAC avoids the problem of fractional weights by applying onlya weak form of virtual arc consistency.Corollary 7.4. If a VCSP P over the valuation structure Q+lower bound c∅ in P .or Qm is virtual arc consistent, then establishing EDAC cannot increase theProof. EDAC is established by applying a sequence of SAC operations [43], but by Theorem 7.3, no sequence of SAC opera-tions can increase c∅ in P . (cid:2)Corollary 7.5. If a VCSP P over the valuation structure Q+or Qm is optimal arc consistent, then P is also virtual arc consistent.Proof. Since P is optimal soft arc consistent, no sequence of relaxed SAC operations increases c∅. Hence no sequence ofSAC operations increases c∅ and therefore, by Theorem 7.3, P is virtual arc consistent. (cid:2)8. Increasing the lower bound using VACWe know by Theorem 5.5 and Theorem 5.8 that we can establish OSAC (and hence VAC) in polynomial time. Unfortu-nately, the time complexity of OSAC limits its use to preprocessing. In this section we introduce a low-order polynomial-timealgorithm which determines a sequence of SAC operations which necessarily increases c∅ if such a sequence exists. By The-orem 7.3, a VCSP is virtual arc consistent if and only if no such sequence exists. VAC is strictly weaker than OSAC due tothe fact that, in the case of VAC, intermediate problems must have non-negative cost functions.In soft arc consistency [19] we often have a choice as to which direction we project or extend weights. Note that thename virtual arc consistency comes from the fact that instead of making such choices, we effectively project or extendsimultaneously virtual weights in all possible directions, by establishing arc consistency in Bool(P ). One iteration of ourVAC algorithm consists of three phases:1. Establish arc consistency in Bool(P ), stopping if domain wipe-out occurs (i.e. as soon as the domain of some variable ibecomes empty). If Bool(P ) is arc consistent, then quit, since P is virtual arc consistent.2. Suppose that domain wipe-out occurred at variable i in Bool(P ), and that σ is the sequence of arc consistency opera-tions which led to this domain wipe-out. Find a minimal subsequence of σ which provokes this domain wipe-out bytracing back from variable i only retaining those arc consistency operations which are strictly necessary.Convert this minimal sequence of arc consistency operations in Bool(P ) into a corresponding sequence σ (cid:14)consistency operations in P which produces the maximum increase λ in c∅ while keeping all costs non-negative.of soft arc3. Apply the sequence σ (cid:14)of operations to P .Consider the following instance P of Max-SAT: ¬ X1; X1 ∨ ¬ X4; ¬ X3 ∨ X4; X2; ¬ X2 ∨ X3. This VCSP is illustrated inthe leftmost box of Fig. 6. A line joining (i, a) and ( j, b) represents a cost ci j(a, b) = 1. Unary costs ci(a) = 1 are notednext to the domain element (i, a). Note that P is existential directional arc consistent (EDAC). However, it is not virtual arcconsistent, since establishing arc consistency in Bool(P ) leads to an inconsistency. The leftmost box in Fig. 6 also representsBool(P ) where now weights are interpreted as being Boolean values. For ease of comparison with the corresponding VCSP P ,in figures we will always represent the CSP Bool(P ) as a VCSP over the Boolean valuation structure S 1 = (cid:3){0, 1}, +1, (cid:3)(cid:5) inwhich 0 < 1 and 1 +1 1 = 1 (i.e. 0 represents consistency, 1 inconsistency and +1 is the idempotent plus operator in theclassical 2-element Boolean algebra). In other words, in Bool(P ) a line between (i, a) and ( j, b) represents the fact that462M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478Fig. 6. A VCSP P (leftmost box) which is EDAC but not virtual arc consistent, as shown by establishing arc consistency in Bool(P ).Fig. 7. (a) Tracing back weights of λ from variable 4 until we arrive at non-zero weights in the original VCSP P of Fig. 6; (b) applying the correspondingsoft arc consistency operations to P (in the reverse order to which they were found in (a)).(a, b) is not a consistent assignment to variables (i, j) and a unary cost of 1 next to (i, a) represents the fact that a isnot a consistent assignment to variable i. In this representation of Bool(P ), propagating inconsistencies, as illustrated inthe middle and right-hand boxes of Fig. 6, means adding lines and setting unary costs to 1. For example, the inconsistencyc1(T ) = 1 is propagated to the binary cost function c12 (c12(T , T ) = c12(T , F ) = 1) and then to value T in d4 (c4(T ) = 1), asshown in the middle box in Fig. 6. A domain wipe-out occurs at variable 4 in the right-hand box of Fig. 6: c4(T ) = c4(F ) = 1meaning that both elements of d4 are inconsistent.During establishment of arc consistency in Bool(P ), the reason for each inconsistency (i.e. a cost which changes from0 to 1 in the valuation structure S1) is recorded. In this example, inconsistency in Bool(P ) is first detected at variable 4.By Theorem 7.3 this means that by soft arc consistency operations in P we can transform P into an equivalent VCSP inwhich ∀x ∈ d4, c4(x) (cid:3) λ for some λ > 0. We can associate λ with each x ∈ d4 and trace back these weights by, at each step,using the reason for inconsistency as recorded during the establishment of arc consistency in Bool(P ). This is illustrated inFig. 7(a). The weights of λ in each c4(x) (x ∈ d4) shown in the top left box can be obtained by projection from cost functionsc14 and c34 (as illustrated in the second box). If the corresponding cost in the original problem P is non-zero, which is thecase for c14(F , T ) and c34(T , F ), then these weights do not need to be traced back further. The remaining weights, namelyc14(T , T ) and c34(T , F ), can be obtained by projections from c1(T ) and c3(F ) as illustrated in the third box. The algorithmhalts when all weights have been traced back to a non-zero costs in the original VCSP P . All the weights of λ shown in thefinal box of Fig. 7(a) correspond to non-zero costs in the original problem P . The value of λ must not exceed any of theseoriginal costs. In this example, the maximal value we can assign to λ is clearly 1. Tracing back is equivalent to finding inM.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478463Fig. 8. An example of a VCSP where virtual arc consistency produces a better lower bound than EDAC by allowing fractional weights.reverse order a sequence of soft arc consistency operations which would produce a VCSP with ∀x ∈ d4, c4(x) (cid:3) λ. The softarc consistency operations can now be applied in the right order. This is illustrated in Fig. 7(b). In the resulting VCSP we(cid:14))have c∅ = 1. This VCSP Pis arc consistent., shown in the final box of Fig. 7(b) is virtual arc consistent since the corresponding CSP Bool(P(cid:14)Unfortunately, establishing virtual arc consistency may require the introduction of fractional weights, as the followingexample illustrates. Consider the instance P of Max-SAT given by: ¬ X1; X1 ∨ ¬ X2; X1 ∨ X3; X2 ∨ ¬ X3. This problem isillustrated in the leftmost box of Fig. 8(a). As usual, each line represents a cost of 1 and unary costs are noted next to thecorresponding domain element. Bool(P ) can also be represented by the same figure, where now the value 1 is understoodto be the element of the Boolean valuation structure S1 = (cid:3){0, 1}, +1, (cid:3)(cid:5) in which 0 < 1 and 1 +1 1 = 1 since 1 representscomplete inconsistency. Fig. 8(a) illustrates the process of establishing arc consistency in Bool(P ), where the detection ofan inconsistency means the addition of a line or a unary cost of 1 in the figure: arc consistency operations are performedon the pairs of variables (1, 2), (1, 3) and then on the pair (2, 3), which leads to a domain wipe-out at variable 3. We cantherefore already deduce a lower bound of the integer value 1 for the original problem P . However, in this example, no setof soft arc consistency operations with integer weights produces a non-zero lower bound.In order to determine a sequence of soft arc consistency operations in P which lead to an increase λ > 0 in c∅, we haveto retrace the steps made while establishing arc consistency in Bool(P ). We place a value of λ at each element of d3, asillustrated by the leftmost box in Fig. 8(b). Retracing our steps, we know that these weights can be obtained by projectionfrom the binary cost functions c13 and c23 (as illustrated in the next box in Fig. 8(b)). If the corresponding weight in theoriginal problem P was non-zero, such as c13(F , F ) and c23(F , T ), then such weights do not need to be traced back anyfurther. We know that the other weights can be obtained by extension from c1 and c2. A weight of λ has to be traced backfurther via c12 to c1. The algorithm halts when all remaining weights were non-zero in the original VCSP P (as shown inthe last box in Fig. 8(b)). We have traced a combined weight of 2λ back to c1(T ). Since c1(T ) = 1 in P , the maximum valuewe can assign to λ is 12 .To concretely collect this cost of 1Fig. 8(b), to the original VCSP P with λ = 1then projected onto c2(T ). We now have c2(T ) = 1amount of cost λ = 1virtual deletion of (1, F ) in Fig. 8(a). In the last step of Fig. 8(c), c2(T ) = 12 in c∅, we apply these soft arc consistency operations, found in reverse order in2 is extended from c1(T ) to c12 and2 , which matches the virtual deletion of value (2, T ) in Fig. 8(a). The same2 , which matches the2 is extended to c23 and projected onto c3(T ). The2 is extended from c1(T ) to c13 and projected onto c3(F ). We now have c3(F ) = 12 . This is shown in Fig. 8(c): a weight of λ = 1464M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478situation matches the virtual wipe-out previously obtained on the right of Fig. 8(a). We finally project c3 onto c∅ and getan equivalent VCSP with c∅ = 12 .The example of Fig. 8 shows that applying a sequence of SAC operations found by our virtual arc consistency algorithmmay lead to the introduction of fractional weights in the VCSP. We have to ensure that we avoid an infinite loop in whichwe make smaller and smaller increases to c∅ each time. We give a concrete example of such an infinite loop in Appendix A.A pragmatic solution to this problem is presented in Section 11.9. Virtual arc consistency subroutinesIn this section we give algorithms to trace back the value of λ from c∅ until we reach non-zero weights in P and topropagate forward in order to actually increase c∅. We assume that the valuation structure used is either Q+or Qm.We give these algorithms for non-binary cost functions. This means that we in fact apply generalized arc consistency[46] rather than arc consistency in Bool(P ). We assume that the generalized arc consistency algorithm applied in the firstphase to Bool(P ) is instrumented as follows: each time a value a ∈ di is eliminated from di in Bool(P ) because it has nosupport in the constraint relation R S , this is recorded by setting killer[i, a] ← S and by pushing the value (i, a) itself onto adedicated queue denoted by Q . A similar instrumentation is used in dynamic CSP algorithms such as [5]. For simplicity, wegive a formal description of this modification in the framework of an AC3-based algorithm. A time-optimal GAC algorithmis used to compute space and time complexities in our implementation.Algorithm 2: VAC iteration – Phase 1: Instrumented AC.123456789101112131415161718(* Revise variable i w.r.t. constraint R S *);Function Revise(i, S)change ← false;foreach a ∈ di doif (cid:3)t ∈ ((cid:4)(S) ∩ R S ) s.t. ti = a thendelete a from di ;killer[i, a] ← S;Q .Push(i, a);change ← true;return change;Function Instrumented-AC()P ← {(i, S) | c S ∈ C, i ∈ S};while P (cid:15)= ∅ do(i, S) ← P .Pop();if Revise(i, S) thenif di = ∅ then return i;else P ← P ∪ {( j, S(cid:14)) | c S(cid:14) ∈ C, S(cid:14) (cid:15)= S, {i, j} ⊂ S(cid:14), j (cid:15)= i};return 0;Compared to the traditional Revise() procedure, lines 7 and 8 have been added. The same modifications can be applied toan AC6 or AC2001 based algorithm. If no wipe-out occurs when AC is enforced on Bool(P ), the problem is already VAC andour Instrumented-AC algorithm returns 0. Otherwise, the wiped-out variable is returned. The stack Q has a space complexityin O(n.d) as each value can be deleted at most once. Implemented as pointers to cost functions, the killer data-structure isalso of O(nd). These complexities do not change the asymptotic space complexity of any GAC algorithms.The second phase is described in Algorithm 3. It exploits the queue Q and the killer data structure to rewind thepropagation history and collect an inclusion-minimal subset of value deletions that is sufficient to explain the domain wipe-out observed. For this, a Boolean M(i, a) is set to true whenever the deletion of (i, a) is needed to explain the wipe-outand needs to be traced back. This phase also computes the quantum of cost λ that we will ultimately add to c∅. Using theprevious killer structure, it is always possible to trace back the cause of deletions until a non-zero cost is reached: this willbe the source from which the cost of λ must be taken. However, in classical CSP, the same forbidden labeling or value maybe used multiple times, as has been shown in the example of Fig. 8. In order to compute the value of λ, we must know howmany quanta of costs are requested for each solicited source of cost in the original VCSP, at the unary or r-ary level. For alabeling t S of scope S, such that c S (t S ) (cid:15)= 0, we use an integer k(S, t S ) to store the number of requests of the quantum λ onc S (t S ). Using the queue Q guarantees that the deleted values are explored in anti-causal order: a deleted value is alwaysexplored before any of the deletions that caused its deletion. Thus, when the cost request for a given tuple is computed, it isc S (t S )based on already computed counts and it is correct. Ultimately, we will be able to compute λ as the minimum ofk(S,t S ) forall t S such that k(S, t S ) (cid:15)= 0. This ratio represents the cost the constraint c S can provide divided by the number of requestsfor this cost.Initially, all k are equal to 0 except at the variable i0 that has been wiped-out where one quantum is needed for eachvalue (line 5). In the simplest case, some cost is already available for some values of the wiped out variable: no backtracingM.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478465Algorithm 3: VAC iteration – Phase 2: Computing λ.12345678910111213141516171819202122Initialize all k, kS to 0, λ ← ∞;i0 ← Instrumented-AC();if (i0 = 0) then return;foreach a ∈ D i0 dok(i0, a) ← 1, M(i0, a) ← true;if (ci0 (a) (cid:15)= 0) then M(i0, a) ← false, λ ← min(λ, ci0 (a));while (Q (cid:15)= ∅) do(i, a) ← Q .Pop();if (M(i, a)) thenS ← killer[i, a];R.Push(i, a);foreach t ∈ (cid:4)(S) s.t. ti = a doif (c S (t) (cid:15)= 0) thenk(S, t) ← k(S, t) + k(i, a);λ ← min(λ, c S (t)k(S,t) );elseLet j ∈ S, j (cid:15)= i be a variable that invalidates t in Bool(P );if (k(i, a) > kS ( j, t j )) thenk( j, t j ) ← k( j, t j ) + k(i, a) − kS ( j, t j );kS ( j, t j ) ← k(i, a);if (c j (t j ) = 0) then M( j, t j ) ← true;else λ ← min(λ, c j (t j )k( j,t j ) );is required and the value of λ is updated accordingly (line 6). Otherwise, a value (i, a) extracted from Q (line 8) was deletedduring arc consistency in Bool(P ) by lack of support in the constraint relation R S associated with c S of scope killer[i, a] = S.If cost is needed at (i, a) (line 9), this lack of support on each tuple t ∈ (cid:4)(S) extending (i, a) can be due to the fact that:1. t is forbidden by R S in Bool(P ) which means that c S (t) (cid:15)= 0 (line 13). The traceback can stop as the number of quantarequested can directly be taken from c S (t). The counter k associated with labeling t (line 14) and λ (line 15) are updatedaccordingly.2. Otherwise, t is not valid because for one of the variables j ∈ S, j (cid:15)= i, the value ( j, t j) was deleted and k(i, a) quanta ofcosts are needed from it. Note that if different values of other variables in S request different numbers of quanta fromvalue ( j, t j) through c S , just the maximum amount is needed since one extension from ( j, t j) to c S provides cost to allc S (t) for t extending ( j, t j). To maintain this maximum, we use another data structure, kS ( j, t j) to store the number ofkS ( j, b), where we sum over all S ∈ C such thatquanta requested on ( j, t j) through c S . We therefore have k( j, b) =j ∈ S. Here, if the new request is higher than the known request (line 18), k( j, t j) (line 19) and kS ( j, t j) (line 20) mustbe increased accordingly. If there is no unary cost c j(t j) explaining the deletion, this means that the value ( j, t j) hasbeen deleted by GAC enforcing and we need to trace back the deletion of ( j, t j) inductively (line 21). Otherwise, thetraceback can stop at ( j, t j) and λ is updated (line 22).(cid:9)The last phase is described in Algorithm 4 and actually modifies the original VCSP by applying the sequence ofequivalence-preserving transformations identified in the previous phase in reverse order, thanks to the queue R. For eachvalue ( j, b) which has been deleted in Bool(P ) and which is needed to explain the wipe-out, we identify the cost functionc S that enabled this deletion in Bool(P ). We then move all the unary costs required in the scope S using Extend() (line 4)and move it to the deleted value ( j, b) using Project() (line 4). The amounts of cost extended and projected are alwaysequal to the cost quantum λ multiplied by the number of requests given by the k data-structure. Ultimately, we reach thewipe-out variable i0 and move the quantum cost to c∅. The new VCSP will have an improved c∅, as Theorem 7.3 shows.Algorithm 4: VAC iteration – Phase 3: Applying equivalence-preserving transformations.12345678while (R (cid:15)= ∅) do( j, b) ← R.Pop();S ← killer[ j, b];foreach i ∈ S, i (cid:15)= j, a ∈ D i s.t. kS (i, a) (cid:15)= 0 doExtend(i, a, S, λ × kS (i, a));kS (i, a) ← 0;Project(S, j, b, λ × k( j, b));UnaryProject(i0, λ);466M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478Because of the k(S, t) data structure, the algorithm has a O(edr) space complexity where r is the maximum arity ofcost functions in P . It is possible to get round this exponential number of counters by observing that quanta requests onc S (t) for |S| > 1 can come only from some variables i ∈ S. For every variable i ∈ S, k(i, ti) quanta are requested by i ifkiller[i, ti] = S and M(i, ti) is true. Thus, the k(S, t) need not to be maintained (removing line 14 of Algorithm 3). When thevalue of a k(S, t) is needed (line 15), it can be computed on the fly as:(cid:11)k(S, t) =kS (i, ti)(i∈S)(killer[i,ti ]=S)∧(M(i,ti))By implementing killer as pointers to cost functions, we get a time complexity of O(|S|) instead of constant time. Becauseof the kS counters, we ultimately get an O(erd) space complexity. As for time complexity, one iteration of the algorithmhas time complexity of O(edr). This is true for the first phase as long as an optimal GAC algorithm is used since theinstrumentation itself is O(nd). The 2nd phase is O(ndr) since there are at most nd values in P and the loop at line 12takes O(dr−1). An O(edr) complexity applies to the last phase.10. Problems solved by virtual arc consistencyWhen a problem P is virtual arc consistent, it is known that the problem Bool(P ) has a non-empty (generalized) arc-consistency closure. This allows VAC to inherit various tractable problem classes which are solved by (generalized) arc-consistency in CSP. For example, VAC can solve submodular minimization problems, a non-trivial polynomial language ofVCSP over the valuation structure Q+[11]. It is already known that OSAC solves VCSPs with submodular cost functions [15].In this section, we give a simpler proof that the weaker notion of VAC is sufficient to solve such problems.Definition 10.1. In the valuation structure Q+is submodular if ∀t, tapplications of max (resp. min) on the tuples t, t(cid:14) ∈ (cid:4)(S), c S (max(t, t(cid:14))) ⊕ c S (min(t, t(cid:14).or Qm, assuming a given total ordering on every domain, a cost function c S(cid:14)) where max and min represent component-wise(cid:14))) (cid:4) c S (t) ⊕ c S (tOver the valuation structure Q+, the class of submodular cost functions includes functions such as(cid:12)x2 + y2 or φr (forr (cid:3) 1) [11] where(cid:10)φr(x, y) =(x − y)r∞if x (cid:3) yotherwiseuseful in bioinformatics [57] and captures simple temporal CSP with linear preferences [34]. Other well-known examples ofsubmodular functions are the cut function of a graph [20] or of a hypergraph [28], and the rank function of a matroid. Thecomplexity of the fastest known fully-combinatorial algorithm for submodular function minimization in Q+is O(N 5γ + N 6)where N is the number of boolean variables and γ is the time to calculate the submodular function to be minimized [47].The standard coding of a VCSP with submodular cost functions and n variables of domains-size d as a submodular functionminimization problem requires N = n(d − 1) Boolean variables [11].Theorem 10.2. Over the valuation structure Q+or Qm, let P be a VCSP whose cost functions are all of arity bounded by a constantand are all submodular for a given domain ordering. If P is VAC, then an optimal solution to P can be found in polynomial time and itscost is given by c∅.Proof. If c∅ = ∞, then any assignment is trivially an optimal solution of cost c∅. Suppose now that c∅ is finite. It follows(cid:14))) =directly from Definition 10.1 that if c S is submodular then ∀t, t(cid:14))) = 0. Thus cost function submodularity implies the following property on relations in Bool(P ): if R S is a relationc S (min(t, twith scope S, then ∀t, t(cid:14) ∈ R S(t ∈ R S ) ∧(cid:14) ∈ (cid:4)(S) such that c S (t) = c S (t(cid:14)) = 0, we have c S (max(t, t(cid:14) ∈ (cid:4)(S),(cid:5)⇒(cid:4)t, t(cid:4)t, t∈ R S∈ R Smaxmin(cid:4)t∧(cid:4)(cid:5)(cid:5)(cid:5)(cid:5)(cid:4)(cid:14)(cid:14)where the operations max and min are applied component-wise.This means that all the relations of Bool(P ) are both min-closed and max-closed [32]. Since the VCSP P is VAC, Bool(P )has a non-empty (generalized) arc consistency closure. It follows that a solution x to Bool(P ) exists and can be found inpolynomial time by establishing (generalized) arc consistency and then taking maximum values in each domain [32]. Thecost of x in the VCSP P is equal to c∅ by definition of Bool(P ) and therefore optimal. (cid:2)The previous proof suggests a very useful and simple value ordering heuristic to use while maintaining VAC inside abranch and bound algorithm: after making Bool(P ) arc consistent, the first value which has not been deleted in the arcconsistent closure of Bool(P ) should be tried first (as submodular cost functions are both max-closed and min-closed). Thisspecific value ordering heuristic will be denoted as Hval in the experimental section.Submodularity is defined based on an order on each domain. It may be the case that all the cost functions of a VCSP aresubmodular but the orders on each domain that make all these cost functions explicitly submodular is unknown. FindingM.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478467the suitable domain orders for so-called permuted submodular cost functions is a polynomial problem that can be directlyreduced to 2-SAT [53]. Interestingly, VAC can directly solve VCSPs with permuted submodular cost functions, without deter-mining the permutations.Theorem 10.3. Over the valuation structure Q+or Qm, let P be a VCSP whose cost functions are all of arity bounded by a constantand are all submodular for unknown domain orders. If P is VAC, then an optimal solution to P can be found in polynomial time and itscost is given by c∅.Proof. The VCSP P can be transformed, by some unknown domain permutations, into a VCSP Pwith submodular costfunctions. The (generalized) arc consistency closure of a CSP being independent of domain orderings, the (generalized) arcconsistency closure of Bool(Pis also VAC. The existence of a solution of cost c∅ followsdirectly from Theorem 10.2.(cid:14)) is also non-empty and hence P(cid:14)(cid:14)Although a solution cannot be directly identified in this case (by taking maximum values in each domain), a solutioncan nevertheless be identified without backtrack by maintaining (generalized) arc consistency in Bool(P ) during search. Thissearch is backtrack-free provided we only accept an assignment if making this assignment and establishing (generalized) arcconsistency in Bool(P ) leads to a non-empty closure. Assigning a value to a variable preserves the max-closed nature of theconstraints, and a (generalized) arc consistent CSP with max-closed constraints necessarily has a solution [32]. (cid:2)Corollary 10.4. Over the valuation structure Q+known) domain orders. Then after establishing virtual arc consistency, the cost of an optimal solution to P is given by c∅., let P be a VCSP whose cost functions are all submodular for some (known or un-Proof. Because Project, Extend and UnaryProject preserve submodularity over Q+[15], establishing VAC on the submodularproblem P produces an equivalent submodular VCSP which is virtual arc consistent. Hence, by Theorem 10.2 or Theo-rem 10.3, establishing VAC solves P . (cid:2)The simplicity of these proofs highlights the fact that VAC solves all polynomial classes such that the corresponding CSPBool(P ) is solved by arc consistency, provided that the property defining the tractable class is preserved under establishingVAC. Very simple cases can become significant in the VCSP case. For example, tree-structured VCSP can be solved by DAC(directional arc consistency) but this requires the tree structure to be detected and a specific variable order to be specifiedfor DAC enforcing. A VAC tree-structured problem will be solved automatically, as arc consistency does in classical CSP. Wecan even give a more general result.Proposition 10.5. Over the valuation structure Q+decision procedure, then P has an optimal solution of cost c∅.or Qm, if P is VAC and Bool(P ) is in a class of CSPs for which arc consistency is aProof. By the definition of VAC, the arc-consistency closure of Bool(P ) is non-empty. Since arc consistency is a decisionprocedure for Bool(P ), this implies that Bool(P ) has a solution and hence, by definition of Bool(P ), that P has a solution ofcost c∅. This solution is necessarily optimal since c∅ is a lower bound on the cost of any solution. (cid:2)Tractable classes of CSP solved by arc consistency include max-closed CSPs [32] and CSP instances satisfying the broken-triangle property (a hybrid class which strictly generalizes tree-structured CSPs [18]). By Proposition 10.5, if after establish-ing VAC, Bool(P ) falls into one of these tractable classes, then the VCSP P is also solved. As an example of a very simplecase, one can observe that any VCSP problem P which is VAC and such that Bool(P ) has all domains reduced to singletonsis also solved. Note that for the VCSP P , this just means that there is no variable which has two (or more) values with unarycost 0. Note, however, that in general, these properties of Bool(P ) may be destroyed under soft arc consistency operationsand hence may not define a tractable class that can be recognized before establishing VAC.11. Experimental trials of our VAC algorithm11.1. Heuristic implementation of our VAC algorithmTo study the actual quality of the VAC bound for solving VCSP, we restricted ourselves to binary cost functions forsimplicity. Since the number of iterations of our VAC algorithm described in Section 9 can be unbounded (as shown onan example in Appendix A), we enforce an approximation of VAC using a threshold ε. If more than a given number ofiterations never improve c∅ by more than ε then VAC enforcing stops prematurely. This is called VACε . In Qm, the numberof iterations is thus O( mε ) and hence the total complexity of VACε is O(ed2m/ε). When one iteration does not increase thelower bound by more than ε, one bottleneck (a cost that fixed the value of λ) is identified and the unary and binary costscorresponding to one of the variables concerned by the bottleneck are ignored in Bool(P ) at following iterations.In order to rapidly collect large cost contributions, and similarly to what has previously been done in maximum flowalgorithms [2], we replaced Bool(P ) by a relaxed but increasingly strict variant Boolθ (P ). A tuple t is forbidden in Boolθ (P )468M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478Table 3A comparison of EDAC, VACε and OSAC for preprocessing random MaxCSP.preprocessingEDACVACεOSACSTlb162527time<0.01 s0.06 s10.5 sDTlb182832time<0.01 s0.09 s2.1 sCTlb404974time<0.01 s0.25 s631 siff its cost in P is larger than θ . After sorting the list of non-zero binary costs ci j(a, b) in a fixed number k of buckets, thedecreasing minimum costs observed in each bucket define a sequence of thresholds (θ1, . . . , θk). Starting from θ1, iterationsare performed at a fixed threshold until no wipe-out occurs. Then the next value θi+1 is used. After θk, a geometric scheduledefined by θi+1 = θi2 is used and stopped when θi (cid:4) ε.11.2. Value ordering heuristicWhen P is virtual arc consistent, values which have been deleted in the arc consistent closure of Bool(P ) imply a costlarger than c∅. This information can be used to direct search towards good solutions. Quickly finding a good (but notnecessarily optimal) solution is an essential ingredient of branch and bound, since it provides a tighter upper bound on theoptimal cost. Since the valuation structure used during branch and bound is Sm where m is the current upper bound, atighter upper bound will lead to more effective pruning during search.In this experimental section, we therefore consider a new value ordering heuristic which selects the minimum domainvalue which has not been deleted in Bool(P ). This value ordering heuristic is more informed than the value ordering heuris-tic that selects the EAC support values (see Definition 4.5) used in the toulbar2 solver. It also has the nice property (seeSection 10) that it will guide the solver towards an optimal solution for non-permuted submodular problems. The combi-nation of this value ordering heuristic with VACε maintenance in a branch and bound procedure is known as VAC+Hval inthe following experimental results.11.3. Experiment setupIn this section we present experimental results on VACε using toulbar2 version 0.8 written in C++ (section Algorithmsin [22]). Our implementation uses fixed point representation of costs. To achieve this, all initial costs in the problem aremultiplied by 1ε which is assumed to be an integer. To exploit the knowledge that the original problem had integer costs,(cid:3) m where m is the global upper bound (the cost of the best knownbranch and bound pruning occurs as soon assolution). As VACε is incapable of producing unary costs, VACε is always enforced together with FDAC.(cid:23)c∅×ε(cid:24)εExperiments were performed on a 3 GHz (2.66 GHz for submodular benchmarks) Intel Xeon with 16 GB. Our solverincludes a last conflict driven variable selection heuristic [8], elimination of variable with degree lower than two duringsearch [42] and binary branching.4 The default value of ε used in VACε was ε = 1Because of the overhead of each iteration of VACε , which implies a reconstruction of Boolθ (P ), the convergence of VACεis stopped prematurely during search (except for the random benchmark problems), using a final θ larger than duringpreprocessing. This enforces VACε only when it is capable of providing large improvements in the lower bound. No non-trivial initial upper bound was used on the random instances.10,000 .11.4. Evaluation of VACε lower boundsIn this first set of experiments, we analyse the strength of the lower bounds provided by VACε compared to other lowerbounds, including OSAC.Random MaxCSP. We report results on the problems described in Section 6.1. These are Sparse Tight, Dense Tight, CompleteTight (ST, DT, CT with 32 variables, 10 values, 50 instances per class) where VACε and OSAC preprocessing yield non-triviallower bounds. Table 3 shows the time and the quality of the lower bound (lb) after preprocessing by EDAC, VACε and OSAC(ILP formulation solved by CPLEX 11.0).As expected, OSAC always provides the strongest lower bound. VACε computes a lower bound which is 8% (ST) to 33%(CT) weaker than OSAC and is one to three orders of magnitude faster. These considerable speedups thus have only a fairlymoderate impact on the strengths of the lower bounds.Frequency assignment problems. The problems considered here were already described in Section 6.1. Considering just thelower bounds produced, Table 4 shows that VACε is again one to two orders of magnitude faster than OSAC and givesalmost the same lower bounds on the graph11r and graph13r instances.4 For small domains (d (cid:2) 10), a value is assigned or removed. Larger domains are split in two halves.M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478469Table 4A comparison of the lower bounds produced by OSAC and VACε on different RLFAP instances.preprocessinglbEDACVACεOSACtimeVACεOSACscen07r10,00029,49831,454211 s3530 sscen08rgraph11rgraph13r6354886 s6718 s2710295529573.5 s492 s87229798979829 s6254 sFig. 9. A comparison of the efficiency of algorithms maintaining EDAC and VACε (with or without the enhanced value ordering heuristic described inSection 11.2) on random permuted binary submodular problems.Overall, our unoptimized version of VACε seems capable of producing significantly stronger lower bounds than EDACalone and is also one to three orders of magnitude faster than a highly optimized linear programming solver which doesnot always produce a better lower bound. VACε is therefore an attractive component for a branch and bound search.11.5. SubmodularityIn this section, we try to evaluate the efficiency of VACε on submodular problems (or on problems with a large part ofsubmodular cost functions).Random binary submodular problems. The following procedure was used to generate random binary submodular problems:at the unary level, every value receives a 0/1/2/3 cost with identical probability. Binary submodular cost functions can bedecomposed into a sum of so-called generalized interval functions [10]. A generalized interval function ηa,b(x, y) is definedby a fixed cost (we used 3) and bounds a and b for the variables x and y:(cid:10)ηa,b(x, y) =0 if (x < a) ∨ ( y > b)3 otherwiseWe summed together p (with p a randomly-chosen integer value in [0, d[, where d is the size of each domain) suchgeneralized interval functions ηa,b(x, y), using uniformly-sampled random values a and b, to generate each submodularbinary cost function. The domains of all variables were then randomly permuted to “hide” submodularity.Problems have from n = 100 to n = 450 variables, 20 domain values, and (n − 1)n/8 binary constraints, and 50 instancesper class. The cpu-time to solve these problems, including the proof of optimality, is reported in Fig. 9 (with a time limitof 1 hour). Fig. 9 shows that maintaining VACε rapidly outperforms EDAC on these problems. Although OSAC can solvepermuted submodular problems in polynomial time [15], the degree of this polynomial is such that OSAC could not beapplied to problems of this size. Thus, even though VACε only establishes an approximation of virtual arc consistency,maintaining VACε proved to be much faster than OSAC on these submodular problems. Similarly, the state-of-the-art fullycombinatorial polynomial-time algorithm for submodular function minimization [47] could not be applied to problems ofthis size since its complexity is O((nd)5e).Notice the speed-up offered by the enhanced value ordering (VAC+Hval in Fig. 9) compared to the default value order-ing heuristic (VAC in Fig. 9).470M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478Fig. 10. A comparison of the efficiency of algorithms maintaining EDAC and VACε on random dense tight problems with a percentage of permuted binarysubmodular cost functions.Partly-submodular random problems. To evaluate the influence of the existence of a limited number of submodular costfunctions, we started from random dense tight problems as generated in [16], replacing a given percentage of cost functionsby permuted binary submodular cost functions (100% means a fully submodular instance). Problems have 100 variables, 10values, 1237 binary constraints, and 50 instances per class. The results are reported in Fig. 10 where a logarithmic scaleis used for the cpu-time axis. We set a time limit of 1 hour (the average being calculated assigning 1 hour to problemsthat were unsolved within this time limit). When 90% of the cost functions are submodular, VACε (VAC or VAC+Hval) istwo orders of magnitude faster than EDAC. For less than 75% submodular cost functions, both EDAC and VACε did not solvethe instances within the 1-hour time limit. As the percentage of submodular cost functions decreases, VAC+Hval becomesless efficient than VAC although it develops slightly less search nodes. This is due to the overhead in maintaining a morecomplex value ordering heuristic. In the rest of the experiments, we therefore used the enhanced VAC+Hval value orderingheuristic for submodular benchmarks only.Feedback arc set. Given a directed graph, the feedback arc set problem consists in removing a minimum subset of the arcs inorder to obtain an acyclic subgraph. An alternative formulation is to find a total order < on the vertices such that there is aminimum number of feedback arcs (i.e. an arc from i to j with i > j). This problems is NP-hard [25]. In order to experimentwith submodular problems, we modified the penalty function so that if there is a feedback arc (i, j), instead of having acost of 1 we have a cost proportional to the difference between the ordering positions of i and j. The resulting problemis similar to a simple temporal CSP with linear preferences [34]. We took instances with n = 50 vertices and from 100 to900 arcs from Resende’s home page.5 In our WCSP model there is a variable xi with domain [1, n] corresponding to eachvertex i. For each arc (i, j), there is a cost function max(0, xi − x j + 1). The results are reported in Fig. 11. The time limit wasalmost 2 days. When the number of arcs is less than 150, OSAC preprocessing solves the problem without search. However,it is much more expensive than EDAC or VACε . As the problem is submodular, VACε is quite efficient compared to EDAC.However, despite this submodularity, VACε was slower than EDAC on the densest instances. When the graph density is high,VACε tends to more frequently find cyclic arc-inconsistency proofs in Bool(P ), resulting in small rational cost incrementsthat may cause the premature termination of VACε , with a loose lower bound c∅. As shown in Fig. 11, lowering the valueof ε effectively improves the lower bound c∅ and reduces the search effort especially when the constraint graph densityincreases. Using a smaller threshold ε =1,000,000 , VACε was always significantly faster than EDAC.1Minimum cut problems. Our last submodular problem example is the (s, t) minimum cut problem which consists in findinga partition of the vertices of a weighted undirected graph G = (V , E, w) into two disjoint subsets, one containing thesource node and the other the terminal node, such that the weighted sum of edges whose end points are in differentsubsets of the partition is minimum. Our WCSP formulation associates one 0/1 variable with each vertex in V . For eachedge e = (i, j) ∈ E, there is a soft equality cost function which returns a cost of w(e) if xi (cid:15)= x j (and 0 otherwise). Wefix x1 = 0 and xn = 1 (since they correspond, respectively, to the source and terminal nodes). Instances were producedby the genrmf generator6 [29] used in the First DIMACS Challenge. The graph is a succession of b grids each of sizea × a in which each vertex is connected to its neighbors and to a randomly chosen vertex in the next grid. Capacities areselected uniformly at random in [c1..c2] for inter-grid arcs and are fixed to c2 × a2 for intra-grid arcs. Problems have from5 http://www.research.att.com/~mgcr/data/index.html.6 www.informatik.uni-trier.de/~naeher/Professur/research/generators/maxflow/genrmf/index.html.M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478471Fig. 11. A comparison of the efficiency of algorithms maintaining EDAC and VACε for different ε values and OSAC on submodular feedback arc set problems.Fig. 12. A comparison of the efficiency of algorithms maintaining EDAC and VACε on random minimum cut problems, compared to state-of-the-art maxi-mum flow and general submodular algorithms.16 (genrmf_long coefficients a = 2, b = a2 = 4 and c1 = 1, c2 = 100) to 20,736 variables (a = 12, b = 144), from 46 to96,626 binary constraints, and 50 instances per class. The results are reported in Fig. 12 where a logarithmic scale is usedfor the cpu-time axis.We compared EDAC and VACε (ε = 1) with a dedicated maximum flow algorithm (Goldberg–Tarjan push-relabel methodH_PRF, cpu-time interpolated from [27] by taking cpu clock frequency ratio 1.8/2.66) and a general submodular (not re-stricted to binary cost functions) minimum-norm point algorithm [26] (cpu-times from [26] with the same cpu ratio appliedand for a = 10 from [41] who have a faster implementation). The algorithms compared have widely different capabilities.The Goldberg–Tarjan algorithm is capable of solving Maxflow/Mincut problems and therefore arbitrary finite binary sub-modular WCSPs [7]. The general submodular algorithm is limited to submodular functions of arbitrary arities while theEDAC/VAC-based algorithms are not restricted to submodular functions or to boolean domains (although our present imple-mentation is only designed for binary cost functions).Not surprisingly, VACε is faster than a general submodular solver (7.6 times faster for a = 10, n = 10,000) and muchslower than the dedicated and finely tuned maximum flow algorithm. Although it develops two times less nodes thanEDAC, it is up to 30 times slower than EDAC due to its overhead during search. Interestingly, the arc inconsistency proofsfound by arc consistency on Bool(P ) were always acyclic, meaning that VACε (whatever the value of ε) solved this specificproblem in preprocessing. It is rather surprising that a relatively simple generic WCSP solver such as EDAC solves minimumcut problems with n ≈ 15, 000 vertices in only 5.5 seconds (even if this is considerably slower than the 0.04 secondsrequired by a specialized and optimized maximum flow algorithm).472M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478Table 5Maintaining VACε on hard RLFAP instances.gr11rgr11gr13rgr13sc06nb. var.23234045445882nb. val.574712,82013,15317,5883274nb. c.fEDAC cpuVACε nodes792142523144815327––––39 min.15362 × 105321142 × 106VACε cpu18.2 s217 min.62 s254 s155 min.lb/iternb. iter2.56.634.80.4969732.6 × 105189394863 × 106Table 6Comparison of EDAC, VACε and CPLEX 11.0 on different uncapacitated warehouse location problems.EDACVACεCPLEXmq125082279622mq2305033121022mq329532883415mq4705240241266mq5732381242357a617932433b–43434.5c–27511311.6. Solving general problemsOur final tests are dedicated to solving non-submodular problems using branch and bound search maintaining VACε +EDAC during search. Since it includes FDAC, EDAC can remove values that would not be deleted by VACε . It thereforeprovides additional information for variable and value ordering heuristics. In the experiments, the toulbar2 solver selectsthe variable with the smallest ratio of current domain size divided by current number of constraints involving the variable.Ties are broken by choosing a variable with maximum unary cost.Frequency assignment. Experiments were performed on the same CELAR instances as mentioned in Section 11.4. Duringsearch, VACε was stopped at θ = 1000ε. Table 5 reports the results on the open instances graph11 and graph13 (seefap.zib.de/problems/CALMA) which are solved to optimality for the first time both in their reduced and original formulation,given the best known upper bound. Table 5 also gives the results on the instance scen06. The table gives for each problemthe number of variables, total number of values, number of cost functions, cpu-time for EDAC alone (a dash for > 104seconds), number of nodes explored with VACε , cpu-time with VACε , mean increase of the lower bound observed afterone VACε iteration (lb/iter) and total number of VACε iterations (nb. iter). We observed that the value k (number of costrequests) at each VACε iteration can be high, reaching a mean value of 16 in some resolutions of graph instances.This shows that the stronger lower bound provided by VACε clearly pays off on sufficiently difficult problems where agood lower bound is essential to prune a large search tree. VACε is also capable of solving simpler problems, but becauseof the associated overheads, less computationally expensive techniques such as EDAC may outperform it.Uncapacitated warehouse location problem (UWLP).In the UWLP, the aim is to decide which facilities should be opened toprovide goods to all customers with maximum profit or, equivalently, minimum cost. The cost minimization variant of theUWLP is known to be supermodular (the opposite of a submodular cost function). Minimizing supermodular functions isknown to be NP-hard. The precise problem description and WCSP model are given in [40] and [24] respectively.We tested both EDAC and VACε preprocessing followed, in both cases, by maintaining EDAC during search on instancescapmq1-5 (600 variables, up to 300 values per variable and 90,000 cost functions) and instances capa, capb and capc (1100variables, around 90 values per variable and 101,100 cost functions). We report solving time to prove optimality (initialupper bound equal to optimum) in seconds in Table 6 (a dash for > 104 seconds). VACε outperforms EDAC on 6 out of the8 problems.Instances were also solved using the ILP solver CPLEX 11.0 and a direct formulation of the problem. On these problems,CPLEX is more efficient than VACε . Note, however, that given the floating point representation of CPLEX and the large rangeof costs in these problems, the proof of optimality of CPLEX is questionable here. OSAC results are not given because LPgeneration overflows on these instances.11.7. ConclusionThe lower bounds produced by VACε are stronger than those produced by EDAC but weaker than those produced byOSAC. Our experiments have conclusively demonstrated that there are some problems for which maintaining VACε duringsearch is the best strategy. This is particularly true of difficult problems (such as the two frequency assignment benchmarkproblems closed for the first time using VACε ). Clearly EDAC will outperform VACε whenever the time devoted by VACε tofinding a better lower bound is not compensated by sufficient pruning of the branch and bound search tree. This may occurfor various reasons: this phenomenon has been observed in (s, t)-mincut problems reported here, but also in the extractionof an optimal plan from a planning graph [17]. It is worth pointing out that our current implementation of VACε leavesroom for considerable optimization.M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478473Fig. 13. A VCSP corresponding to the 2-color graph-coloring optimization problem on a triangle.Our experiments have confirmed the theoretical relationship between VAC and submodularity. Although VACε is only anapproximation to VAC, it is nevertheless capable of taking advantage of the submodular nature of cost functions to providea good lower bound. It is also no doubt because EDAC can be considered as an approximation to VAC, that explains therapidity of EDAC on certain submodular problems. An interesting outcome of our experiments was that VACε performs wellon problems containing a high proportion of submodular cost functions (to which specialized submodular algorithms areinapplicable).12. Discussion12.1. Virtual arc consistency by diffusionA much simpler (but slower) algorithm, known as MIN-SUM diffusion, can also be used as an alternative to our VACalgorithm described in Section 9. MIN-SUM diffusion consists in iterating until convergence the following operation: foreach S ∈ C , i ∈ S and a ∈ di , call Project (i, a, S, α) where(cid:13)c S (t): t ∈ (cid:4)(S) such that ti = a(cid:14)min− ci(a)α = 12Rather than sending as much cost as possible towards the unary constraint ci , MIN-SUM diffusion equalizes costs betweenunary and higher-arity constraints, in the sense that after the above call of Project,(cid:13)c S (t): t ∈ (cid:4)(S) such that ti = aci(a) = min(cid:14)If after each iteration we establish node consistency, it is easy to see that whenever MIN-SUM diffusion converges, theresulting VCSP is VAC. MIN-SUM diffusion has been generalized to the tree-reweighting (TRW) algorithm which performsexact equalizations on trees rather than on single edges [55,35]. In trials on binary problems from low-level computervision, MIN-SUM diffusion was found to converge several times slower than both the TRW algorithm (where the treescorresponded to the rows and columns of the image) and the “Augmenting DAG” algorithm which is similar to our VACalgorithm described in Section 9 [39,56].12.2. Beyond arc consistencyIt should be mentioned that forms of higher-order consistency have been proposed for VCSPs [14] which can find abetter lower bound than any SAC transformation. This is at the cost of introducing higher-order cost functions. Considerthe optimization version of the graph coloring problem on a triangle with two colors, equivalent to the VCSP in Fig. 13,where a line represents a cost of 1. The aim is to assign a color to each node so as to minimize the number of pairs ofnodes joined by an edge and assigned the same color. No SAC transformation applied to this VCSP increases c∅, whereassoft 3-consistency produces a lower bound c∅ = 1 [14]. One disadvantage of establishing soft 3-consistency is that someweights are now stored in ternary cost functions.Bool(P ) is a classical CSP which has a solution if and only if the VCSP P has a solution of cost c∅. In the same way thatvirtual arc consistency uses inconsistencies detected when establishing arc consistency in Bool(P ) to determine a sequenceof soft arc consistency operations which increase the lower bound c∅ in P , other virtual consistency techniques could bedefined based on other notions of consistency in Bool(P ).13. ConclusionWe have presented new techniques for finding improved lower bounds in the finite-domain optimization problem VCSP,based on the notions of optimal and virtual arc consistency.In order to establish optimal soft arc consistency (OSAC), after the propagation of infinite costs, a linear program issolved to determine a set of soft arc consistency operations (shifting of costs between unary and non-unary cost functions)which produces an equivalent instance with a maximum value of the constant cost term. This constant cost term representsa natural lower bound and plays an essential role in branch and bound search. When all costs are finite, the resultingconstant cost term is optimal among all equivalent instances with the same set of constraint scopes. Experimental trialshave demonstrated the potential utility of establishing OSAC during preprocessing.474M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478Fig. 14. Example of a VCSP instance for which our VAC algorithm can enter an infinite loop.Virtual arc consistency (VAC) can be seen as an approximation to OSAC that can be applied either during preprocessingor at every node of a search tree. If a VCSP is virtual arc consistent, then this means that no sequence of soft arc consistencyoperations could increase the lower bound c∅. In particular, the previous state-of-the-art soft consistency technique EDAC(Existential Directional Arc Consistency) [43] cannot increase c∅ for any variable order.Virtual arc consistency can be tested in O(ed2) time in the case of a binary VCSP using an optimal arc consistencyalgorithm, such as AC-2001 [6] in the CSP Bool(P ). It can also be established in polynomial time by simply establishingOSAC. The main aim of soft consistency techniques is to rapidly find a good (but not necessarily optimal) lower bound.Therefore, in our experimental trials we used an algorithm with guaranteed low-order polynomial time complexity whichestablished a relaxed version VACε of virtual arc consistency, in order to avoid problems of convergence generated by theintroduction of smaller and smaller fractional weights. Applying VACε during branch and bound search allowed us to closetwo longstanding open frequency assignment problems.AcknowledgementsWe would like to thank Arie Koster and Achemi Bennaceur for discussions on the OSAC lower bound and its relation,by duality, to the linear relaxation of the ILP formulation of weighted CSP given in [37]. The presentation of the paperwas greatly improved thanks to the remarks of the anonymous reviewers. This research was partly funded by the AgenceNationale de la Recherche (STALDECOPT project).Appendix A. Infinite loops while trying to enforce VACIn this section we give an example of a VCSP instance over the valuation structure Q+for which our VAC algorithm canenter an infinite loop, increasing c∅ by a smaller and smaller amount at each iteration. We present this example to justifyour use of heuristics, described in Section 11, in our experimental trials. These heuristics guarantee a low-order polynomialtime complexity at the cost of not necessarily completely establishing VAC.Denote by P i (for all integers i (cid:3) 0) the 10-variable VCSP instance shown in Fig. 14 in which the values of α, β, γ , δ, εare given by−i(cid:5),(cid:4)1 − 4−i(cid:4)1 − 4α = 23α = 1γ = 123−iε = 4(cid:4)β = 1 − α = 13(cid:5)δ = 1 − γ = 131 + 2(cid:4),(cid:5)(cid:5)−i(cid:4)4(cid:5)2 + 4−iM.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478475A non-zero binary cost ci j(u, v) = ρ is represented by a line labeled ρ joining (i, u) and ( j, v). Non-zero unary costs aregiven explicitly. The original problem P 0 is somewhat simpler than the problem P i shown in Fig. 14 since, when i = 0,α = γ = 0 and hence many edges have zero weight. We will show that two iterations of our VAC algorithm can transformP i into P i+1 (for i (cid:3) 0). Hence, it is possible that our algorithm enters an infinite loop producing the sequence P 0, P 1,P 2 . . . , and hence never actually establishes virtual arc consistency.There are different sequences of SAC operations that can be applied to P i which would allow us to increase c∅. Inparticular, it is possible to increase c∅ by ε by shifting a weight of ε from c10(a) to variable 1 via variable 9, using thefollowing sequence of SAC operations:Extend(10,a,{10, 9},ε), Project({10, 9},9,b,ε),Extend(9,b,{9, 1},ε), Project({9, 1},1,b,ε),UnaryProject(1,ε)This sequence of SAC operations immediately produces a VCSP in which c∅ = 1. But our VAC algorithm can equally wellsuccessively transform P i into P i+1, P i+2, . . .; in this case c∅ never actually attains the value 1.Imagine that among the different c∅-increasing sequences of SAC operations that can be applied to P i , our algorithmdetermines that we can increase c∅ by an amount ε/2 by shifting weights through the cycle of variables 1, 2, 3, 4, 5, 8,9, 1. In this sequence σ1 of SAC operations a weight of ε which is extended from c1(a) towards variable 2 effectively comesback to c1(b) as a weight of ε/2 since it has to be split into two at variable 3, half being sent towards variable 4 andhalf towards variable 5. Weights are sent along these two paths (via variables 4 and 5) to variable 8 in order to increaseboth c8(a) and c8(b), which allows the propagation to continue to variable 9 and then variable 1. The sequence σ1 of SACoperations applied to P i is given below:σ1: Extend(1,a,{1, 2},ε), Project({1, 2},2,b,ε),Extend(2,b,{2, 3},ε), Project({2, 3},3,a,ε), Project({2, 3},3,b,ε),Extend(3,a,{3, 4},ε/2), Extend(3,b,{3, 4},ε/2), Project({3, 4},4,a,ε/2),Extend(3,a,{3, 5},ε/2), Extend(3,b,{3, 5},ε/2), Project({3, 5},5,a,ε/2),Extend(4,a,{4, 8},ε/2), Project({4, 8},8,a,ε/2),Extend(5,a,{5, 8},ε/2), Project({5, 8},8,b,ε/2),Extend(8,b,{8, 9},ε/2), Extend(8,a,{8, 9},ε/2), Project({8, 9},9,b,ε/2),Extend(9,b,{9, 1},ε/2), Project({9, 1},1,b,ε/2),UnaryProject(1,ε/2)Since the values of α, β and ε (defined above) satisfy the following inequalities2 − 2α (cid:3) εβ (cid:3) ε/21 (cid:3) ε2 − 3α (cid:3) 3ε/2Let Pthe above sequence σ1 of SAC operations produces a VCSP with non-negative costs. Furthermore, ε/2 is the largest increasein c∅ which we can produce by such a sequence due to the fact that c2,3(a, a) = c2,3(a, b) = ε in the instance P i . This showsthat σ1 may be the operations actually carried out in one iteration of our VAC algorithm.(cid:14)i denote the VCSP instance which results when the sequence σ1 of SAC operations is applied to P i . A sequence of(cid:14)i to increase c∅ by ε/4 by shifting weights through the cycle of variables 10, 9,SAC operations can then be applied to P8, 7, 6, 3, 2, 10. This is the rotational symmetry equivalent of the sequence σ1 of SAC operations with all weights dividedby two (and, by rotational symmetry, variable j replaced by variable 11 − j). For completeness, we list the sequence σ2 ofoperations below. Again, ε/4 is the largest increase in c∅ which we can produce by such a sequence due to the fact that(cid:14)c9,8(b, a) = c9,8(b, b) = ε in the instance Pi and hence σ2 may be the operations actually carried out by our VAC algorithm.σ2: Extend(10,a,{10, 9},ε/2), Project({10, 9},9,b,ε/2),Extend(9,b,{9, 8},ε/2), Project({9, 8},8,a,ε/2), Project({9, 8},8,b,ε/2),Extend(8,a,{8, 7},ε/4), Extend(8,b,{8, 7},ε/4), Project({8, 7},7,a,ε/4),Extend(8,a,{8, 6},ε/4), Extend(8,b,{8, 6},ε/4), Project({8, 6},6,a,ε/4),Extend(7,a,{7, 3},ε/4), Project({7, 3},3,a,ε/4),Extend(6,a,{6, 3},ε/4), Project({6, 3},3,b,ε/4),Extend(3,b,{3, 2},ε/4), Extend(3,a,{3, 2},ε/4), Project({3, 2},2,b,ε/4),Extend(2,b,{2, 10},ε/4), Project({2, 10},10,b,ε/4),UnaryProject(10,ε/4)476M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478(cid:14)(cid:14)(cid:14)(cid:14)We denote the resulting VCSP instance by Pjk, with c∅, c j , c jk denoting the costi and its cost functions by cfunctions in P i . After the two sequences of SAC operations σ1, σ2, the new values of the cost functions are given by thefollowing equations.(cid:14)(cid:14)∅, c(cid:14)(cid:14)j , c(cid:14)(cid:14)∅ = c∅ + ε/2 + ε/4(cid:14)(cid:14)1(a) = c1(a) − ε − ε/2(cid:14)(cid:14)1,2(a, a) = c1,2(a, a) + ε(cid:14)(cid:14)1,2(b, b) = c1,2(b, b) − ε(cid:14)(cid:14)2,3(a, a) = c2,3(a, a) − ε + ε/4(cid:14)(cid:14)2,3(b, c) = c2,3(b, c) + ε − ε/4(cid:14)(cid:14)3,4(a, b) = c3,4(a, b) + ε/2(cid:14)(cid:14)3,4(c, a) = c3,4(c, a) − ε/2(cid:14)(cid:14)10(a) = c10(a) − ε/2 − ε/4(cid:14)(cid:14)8,7(a, b) = c8,7(a, b) + ε/4(cid:14)(cid:14)8,7(c, a) = c8,7(c, a) − ε/4(cid:14)(cid:14)8,9(c, b) = c8,9(c, b) − ε/2 + ε/2cccccccccccc(cid:14)(cid:14)cccc(cid:4)(cid:5)34−i−i− 4−(i+1)−(i+1)1 − 41 − 4(cid:4)4(cid:4)3 (2 + 43 (1 − 4+ 4(cid:5)1 − 4(cid:4)−i) and ε = 4−i , we can deduce that−i)), γ = 1−(i+1)(cid:5)3 (1 − 4−i + 4(cid:5)−i− 4−i), β = 1−i/2 + 4−i − 4(cid:5)3 (1 + 2(4−i/4 = 1 − 4−i/2 = 2−i = 43−i = 2 − 43−(i+1)For example, c∅ is increased by ε/2 + ε/4 due to the combined effect of the operations UnaryProject(1,ε/2) and UnaryPro-ject(10,ε/4), and c1(a) is decreased by ε + ε/2 as a result of the operations Extend(1,a,{1, 2},ε) and UnaryProject(1,ε/2).Reading off the cost function values from Fig. 14 (that is c∅ = 1 − ε, c1(a) = 2ε, c1,2(a, a) = 2α, c1,2(b, b) = 2 − 2α,c2,3(a, a) = ε, c2,3(b, c) = 1 − ε, c3,4(a, b) = α, c3,4(c, a) = β, c10(a) = ε, c8,7(a, b) = γ , c8,7(c, a) = δ, c8,9(c, b) = 1) and−i), δ = 1given that α = 2(cid:14)(cid:14)∅ = 1 − 4(cid:4)(cid:14)(cid:14)1(a) = 21,2(a, a) = 4(cid:14)(cid:14)1,2(b, b) = 2 − 43−i − 4(cid:14)(cid:14)2,3(a, a) = 4(cid:14)(cid:14)2,3(b, c) = 1 − 43,4(a, b) = 2(cid:14)(cid:14)3,4(c, a) = 1 − 23−i − 4−i/2 − 4(cid:14)(cid:14)10(a) = 4(cid:4)8,7(a, b) = 1(cid:14)(cid:14)8,7(c, a) = 1(cid:14)(cid:14)8,9(c, b) = 1 − ε/2 + ε/2 = 1−i/2 = 23−i/2 = 13−(i+1)−i/4 = 13−i/4 = 13−i/4 = 1 − 4(cid:4)−i + 4(cid:5)1 − 4(cid:4)1 − 4(cid:4)−i/4 = 4−i/4 = 4+ 4(cid:5)−i + 4−i − 41 − 41 − 42 + 41 − 42 + 41 + 21 − 4−(i+1)−(i+1)−(i+1)−(i+1)−(i+1)−(i+1)+ 4− 4− 4(cid:5)(cid:5)−i−i−i−i4333(cid:4)(cid:4)(cid:4)(cid:5)(cid:5)(cid:4)(cid:5)(cid:5)(cid:5)(cid:4)(cid:5)(cid:4)cccccccc(cid:14)(cid:14)(cid:14)(cid:14)The remaining cost function values can be deduced from these values, since those edges which have identical labels in(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)2,3(a, a) (edges labeled ε in Fig. 14), c9,1(b, a) =9,10(a, a) = ci . In other words cFig. 14 are also identical in P(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)3,4(a, b) (edges labeled α), c3,5(b, b) = c9,10(b, b) =5,8(a, a) = c5,8(a, c) = c4,8(a, b) = c4,8(a, c) = cc(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)6,3(a, c) =3,4(c, a) (edges labeled β), c3,5(c, a) = c5,8(b, b) = c4,8(b, a) = c9,1(a, b) = c7,3(a, b) = cc(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)8,7(a, b) (edges labeled γ ) and c8,6(c, a)8,7(b, b) = c8,6(a, b) = c8,6(b, b) = c6,3(a, a) = cc(cid:14)(cid:14)8,7(c, a) (edges labeled δ). Furthermore, all cost function values which were 0 (represented by the absence of an edge in= cFig. 14) are also 0 in P(cid:14)(cid:14)2,3(a, b) = c(cid:14)(cid:14)3,5(a, b) = c(cid:14)(cid:14)3,4(b, b) = c(cid:14)(cid:14)2,10(b, a) = c(cid:14)(cid:14)2,10(a, b) = c(cid:14)(cid:14)7,3(a, c) = c(cid:14)(cid:14)6,3(b, b) = c(cid:14)(cid:14)7,3(b, a) = cThe above calculations of the cost functions c(cid:14)(cid:14)is, in fact, exactly the VCSP instance P i+1. It followsjk show that Pthat, starting from P 0, our algorithm may find the non-ending sequence of VCSP instances P 0, P 1, P 2, . . . and hence neverhalt.(cid:14)(cid:14)∅, c(cid:14)(cid:14)j , c(cid:14)(cid:14)i .(cid:14)(cid:14)iM.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478477References[1] M.S. Affane, H. Bennaceur, A weighted arc consistency technique for Max-CSP, in: Proc. of the 13th ECAI, Brighton, United Kingdom, 1998, pp. 209–213.[2] R.K. Ahuja, T. Magnanti, J. Orlin, Network Flows: Theory, Algorithms, and Applications, Prentice Hall, 1993.[3] K. Apt, The essence of constraint propagation, Theoretical Computer Science 221 (1–2) (1999) 179–210.[4] H. Bennaceur, A. Osmani, Computing lower bounds for Max-CSP problems, in: Developments in Applied Artificial Intelligence, vol. 22718, Springer,2003, pp. 217–240.[5] C. Bessière, Arc-consistency in dynamic constraint satisfaction problems, in: Proc. of AAAI’91, Anaheim, CA, 1991, pp. 221–226.[6] C. Bessière, J.-C. Régin, Refining the basic constraint propagation algorithm, in: Proc. IJCAI’2001, 2001, pp. 309–315.[7] E. Boros, P. Hammer, Pseudo-Boolean optimization, Discrete Appl. Math. 123 (2002) 155–225.[8] C. Lecoutre, L. Saïs, S. Tabary, V. Vidal, Reasoning from last conflict(s) in constraint programming, Artificial Intelligence 173 (18) (2009) 1592–1614.[9] B. Cabon, S. de Givry, L. Lobjois, T. Schiex, J. Warners, Radio link frequency assignment, Constraints 4 (1999) 79–89.[10] D.A. Cohen, M.C. Cooper, P.G. Jeavons, A.A. Krokhin, A maximal tractable class of soft constraints, Journal of Artificial Intelligence Research 22 (2004)1–22.[11] D.A. Cohen, M.C. Cooper, P.G. Jeavons, A.A. Krokhin, The complexity of soft constraint satisfaction, Artificial Intelligence 170 (11) (Aug. 2006) 983–1016.[12] M.C. Cooper, Reduction operations in fuzzy or valued constraint satisfaction, Fuzzy Sets and Systems 134 (3) (2003) 311–342.[13] M.C. Cooper, Cyclic consistency: a local reduction operation for binary valued constraints, Artificial Intelligence 155 (1–2) (2004) 69–92.[14] M.C. Cooper, High-order consistency in valued constraint satisfaction, Constraints 10 (2005) 283–305.[15] M.C. Cooper, Minimization of locally-defined submodular functions by Optimal Soft Arc Consistency, Constraints 13 (4) (2008).[16] M.C. Cooper, S. de Givry, T. Schiex, Optimal soft arc consistency, in: Proc. of IJCAI’2007, Hyderabad, India, Jan. 2007, pp. 68–73.[17] M.C. Cooper, M. de Roquemaurel, P. Régnier, A weighted CSP approach to cost-optimal planning. Tech. Rep. RR-2009-28-FR, IRIT, Toulouse, France, 2009.[18] M.C. Cooper, P. Jeavons, A. Salamon, Hybrid tractable CSPs which generalise tree structure, in: Proc. ECAI’08, 2008, pp. 530–534.[19] M.C. Cooper, T. Schiex, Arc consistency for soft constraints, Artificial Intelligence 154 (1–2) (2004) 199–227.[20] W. Cunningham, Minimum cuts, modular functions, and matroid polyhedra, Networks 15 (2) (1985) 205–215.[21] S. de Givry, Minorants de problèmes de minimisation de violation de contraintes : recherche de bonnes relaxations à l’aide de méthodes incomplètes,in: Proc. of JNPC-99, Lyon, France, 1999.[22] S. de Givry, F. Heras, J. Jarrosa, E. Rollon, T. Schiex, The SoftCSP and Max-SAT benchmarks and algorithms web site, 2003, carlit.toulouse.inra.fr/cgi-bin/awki.cgi/softcsp.[23] S. de Givry, T. Schiex, G. Verfaillie, Exploiting tree decomposition and soft local consistency in weighted CSP, in: Proc. of the National Conference onArtificial Intelligence, AAAI-2006, 2006, pp. 22–27.[24] S. de Givry, M. Zytnicki, F. Heras, J. Larrosa, Existential arc consistency: getting closer to full arc consistency in weighted CSPs, in: Proc. of IJCAI-05,Edinburgh, Scotland, 2005, pp. 84–89.[25] P. Festa, P. Pardalos, M. Resende, Feedback set problems, in: Handbook of Combinatorial Optimization, Kluwer Academic Publishers, 1999, pp. 209–258.[26] S. Fujishige, T. Hayashi, S. Isotani, The minimum-norm-point algorithm applied to submodular function minimization and linear programming, Tech.Rep., Research Institute for Mathematical Sciences, Kyoto, Japan, 2006.[27] S. Fujishige, S. Isotani, New maximum flow algorithms by ma orderings and scaling, Journal of the Operations Research Society of Japan 46 (2003)243–250.[28] S. Fujishige, S.B. Patkar, Realization of set functions as cut functions of graphs and hypergraphs, Discrete Math. 226 (2001) 199–210.[29] D. Goldfarb, M.D. Grigoriadis, A computational comparison of the dinic and network simplex methods for maximum flow, Annals of Oper. Res. 13(1988) 83–123.[30] P. Hammer, P. Hansen, B. Simeone, Roof duality, complementation and persistency in quadratic 0–1 optimization, Math. Programming 28 (1984) 121–155.[31] F. Heras, J. Larrosa, A. Oliveras, MiniMaxSat: A new weighted Max-SAT solver, in: Proc. of SAT’2007, Lisbon, Portugal, May 2007, in: LNCS, vol. 4501,pp. 41–55.[32] P. Jeavons, M. Cooper, Tractable constraints on ordered domains, Artificial Intelligence 79 (2) (Dec. 1995) 327–339.[33] N. Karmarkar, A new polynomial time algorithm for linear programming, Combinatorica 4 (4) (1984) 373–395.[34] L. Khatib, P. Morris, R. Morris, F. Rossi, Temporal constraint reasoning with preferences, in: Proc. of the 17th IJCAI, Seattle, WA, 2001, pp. 322–327.[35] V. Kolmogorov, Convergent tree-reweighted message passing for energy minimization, IEEE Trans. on Pattern Analysis and Machine Intelligence 28 (10)(2006) 1568–1583.[36] A. Koster, S. van Hoesel, A. Kolen, The partial constraint satisfaction problem: facets and lifting theorems, Oper. Res. Lett. 23 (3–5) (1998) 89–97.[37] A. Koster, S. van Hoesel, A. Kolen, Solving frequency assignment problems via tree-decomposition, Tech. Rep. RM/99/011, Universiteit Maastricht,Maastricht, The Netherlands, 1999.[38] A.M.C.A. Koster, Frequency assignment: Models and algorithms, Ph.D. thesis, University of Maastricht, The Netherlands, available at www.zib.de/koster/thesis.html, Nov. 1999.[39] V.K. Koval, M.I. Schlesinger, Dvumernoe programmirovanie v zadachakh analiza izobrazheniy (Two-dimensional programming in image analysis prob-lems), USSR Academy of Science Automatics and Telemechanics 8 (1976) 149–168 (in Russian).[40] J. Kratica, D. Tosic, V. Filipovic, I. Ljubic, Solving the simple plant location problems by genetic algorithm, RAIRO Operations Research 35 (2001) 127–142.[41] A. Krause, C. Guestrin, IJCAI tutorial on intelligent information gathering and submodular function optimization, Tech. Rep., Caltech/CMU, Pasadena,2009, www.submodularity.org.[42] J. Larrosa, Boosting search with variable elimination, in: Principles and Practice of Constraint Programming – CP 2000, Singapore, Sep. 2000, in: LNCS,vol. 1894, pp. 291–305.[43] J. Larrosa, S. de Givry, F. Heras, M. Zytnicki, Existential arc consistency: getting closer to full arc consistency in weighted CSPs, in: Proc. of the 19thIJCAI, Edinburgh, Scotland, Aug. 2005, pp. 84–89.[44] J. Larrosa, T. Schiex, In the quest of the best form of local consistency for weighted CSP, in: Proc. of the 18th IJCAI, Acapulco, Mexico, Aug. 2003,pp. 239–244.[45] C.M. Li, F. Manyà, J. Planes, Exploiting unit propagation to compute lower bounds in branch and bound Max-SAT solvers, in: Proc of CP’2005, Sitges,Spain, 2005, in: LNCS, vol. 3709, pp. 403–414.[46] R. Mohr, G. Masini, Good old discrete relaxation, in: Proc. of the 8th ECAI, Munchen, FRG, 1988, pp. 651–656.[47] J.B. Orlin, A faster strongly polynomial time algorithm for submodular function minimization, Mathematical Programming Ser. A 118 (2) (2009) 237–251.[48] J.-C. Régin, T. Petit, C. Bessière, J.-F. Puget, New lower bounds of constraint violations for over-constrained problems, in: Proc. of CP-01, Paphos, Cyprus,Dec. 2001, in: LNCS, vol. 2239, pp. 332–345.[49] M. Sanchez, D. Allouche, S. de Givry, T. Schiex, Russian doll search with tree decomposition, in: Proc. of IJCAI’09, Pasadena, CA, USA, 2009.478M.C. Cooper et al. / Artificial Intelligence 174 (2010) 449–478[50] M. Sanchez, S. de Givry, T. Schiex, Mendelian error detection in complex pedigrees using weighted constraint satisfaction techniques, Constraints 13 (1)(2008) 130–154.[51] T. Schiex, Arc consistency for soft constraints, in: Principles and Practice of Constraint Programming – CP 2000, Singapore, Sep. 2000, in: LNCS, vol.1894, pp. 411–424.[52] T. Schiex, H. Fargier, G. Verfaillie, Valued constraint satisfaction problems: hard and easy problems, in: Proc. of the 14th IJCAI, Montréal, Canada, Aug.1995, pp. 631–637.[53] D. Schlesinger, Exact solution of permuted submodular MinSum problems, in: Energy Minimization Methods in Computer Vision and Pattern Recogni-tion, in: LNCS, vol. 4679, Aug. 2007, pp. 28–38.[54] M. Schlesinger, Sintaksicheskiy analiz dvumernykh zritelnikh signalov v usloviyakh pomekh (Syntactic analysis of two-dimensional visual signals innoisy conditions), Kibernetika 4 (1976) 113–130.[55] M. Wainwright, T. Jaakkola, A. Willsky, MAP estimation via agreement on (hyper)trees: message passing and linear programming approaches, IEEETrans. on Information Theory 51 (11) (2005) 3697–3717.[56] T. Werner, A linear programming approach to max-sum problem: A review, IEEE Trans. on Pattern Recognition and Machine Intelligence 29 (7) (Jul.2007) 1165–1179.[57] M. Zytnicki, C. Gaspin, S. de Givry, T. Schiex, Bounds arc consistency for weighted CSPs, Journal of Artificial Intelligence Research 35 (2009) 593–621.