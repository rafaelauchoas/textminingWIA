Artificial Intelligence 194 (2013) 176–202Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintComputing text semantic relatedness using the contents and linksof a hypertext encyclopediaMajid Yazdani a,b,∗, Andrei Popescu-Belis aa Idiap Research Institute, 1920 Martigny, Switzerlandb EPFL, École Polytechnique Fédérale de Lausanne, 1015 Lausanne, Switzerlanda r t i c l ei n f oa b s t r a c tArticle history:Available online 19 June 2012Keywords:Text semantic relatednessDistance metric learningLearning to rankRandom walkText classificationText similarityDocument clusteringInformation retrievalWord similarity1. IntroductionWe propose a method for computing semantic relatedness between words or texts by usingknowledge from hypertext encyclopedias such as Wikipedia. A network of concepts is builtby filtering the encyclopedia’s articles, each concept corresponding to an article. Two typesof weighted links between concepts are considered: one based on hyperlinks between thetexts of the articles, and another one based on the lexical similarity between them. Wepropose and implement an efficient random walk algorithm that computes the distancebetween nodes, and then between sets of nodes, using the visiting probability from one(set of) node(s) to another. Moreover, to make the algorithm tractable, we propose andvalidate empirically two truncation methods, and then use an embedding space to learnan approximation of visiting probability. To evaluate the proposed distance, we apply ourmethod to four important tasks in natural language processing: word similarity, documentsimilarity, document clustering and classification, and ranking in information retrieval.The performance of the method is state-of-the-art or close to it for each task, thusdemonstrating the generality of the knowledge resource. Moreover, using both hyperlinksand lexical similarity links improves the scores with respect to a method using only oneof them, because hyperlinks bring additional real-world knowledge not captured by lexicalsimilarity.© 2012 Elsevier B.V. All rights reserved.Estimating the semantic relatedness of two text fragments – such as words, sentences, or entire documents – is importantfor many natural language processing or information retrieval applications. For instance, semantic relatedness has been usedfor spelling correction [1], word sense disambiguation [2,3], or coreference resolution [4]. It has also been shown to helpinducing information extraction patterns [5], performing semantic indexing for information retrieval [6], or assessing topiccoherence [7].Existing measures of semantic relatedness based on lexical overlap, though widely used, are of little help when textsimilarity is not based on identical words. Moreover, they assume that words are independent, which is generally not thecase. Other measures, such as PLSA or LDA, attempt to model in a probabilistic way the relations between words and topicsas they occur in texts, but do not make use of structured knowledge, now available on a large scale, to go beyond worddistribution properties. Therefore, computing text semantic relatedness based on concepts and their relations, which havelinguistic as well as extra-linguistic dimensions, remains a challenge especially in the general domain and/or over noisytexts.* Corresponding author at: Idiap Research Institute, 1920 Martigny, Switzerland.E-mail addresses: majid.yazdani@epfl.ch, majid.yazdani@idiap.ch (M. Yazdani), andrei.popescu-belis@idiap.ch (A. Popescu-Belis).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.06.004M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202177In this paper, we propose to compute semantic relatedness between sets of words using the knowledge enclosed in alarge hypertext encyclopedia, with specific reference to the English version of Wikipedia used in the experimental part.We propose a method to exploit this knowledge for estimating conceptual relatedness (defined in Section 2) following astatistical, unsupervised approach, which improves over past attempts (reviewed in Section 3) by making use of the large-scale, weakly structured knowledge embodied in the links between concepts. The method starts by building a network ofconcepts under the assumption that every encyclopedia article corresponds to a concept node in the network. Two typesof links between nodes are constructed: one by using the original hyperlinks between articles, and the other one by usinglexical similarity between the articles’ content (Section 4).This resource is used for estimating the semantic relatedness of two text fragments (or sets of words), as follows. Eachfragment is first projected onto the concept network (Section 5). Then, the two resulting weighted sets of concepts arecompared using a graph-based distance, which is computed based on the distance between two concepts. This is estimatedusing the visiting probability (VP) of a random walk over the network from one concept to another, following the twotypes of links (Section 6). Visiting probability integrates ‘density of connectivity’ and ‘length of path’ factors for computinga relevant measure of conceptual relatedness in the network. Several approximations based on truncation of paths areproposed (Section 7) and justified (Section 8) in order to make the computations tractable over a very large network, with1.2 million concepts and 35 million links. Moreover, a method to learn an approximation of visiting probability using anembedding space (Section 9) is shown to be another solution to the tractability problem.To demonstrate the practical relevance of the proposed resources – the network, the distance over it, and the approxi-mations – we apply them to several natural language processing problems that should benefit from an accurate semanticdistance: word similarity (Section 10), document similarity (Section 11), document clustering and classification (Sections 12and 13 respectively), and information retrieval (Section 14), including learning to rank (Section 15). The results of ourmethod are competitive on all tasks, and demonstrate that the method provides a unified and robust answer to measuringsemantic relatedness.2. Semantic relatedness: Definitions and issuesTwo samples of language are said to be semantically related if they are about things that are associated in the world, i.e.bearing some influence one upon the other, or being evoked together, in speech or thought, more often than other things.Semantic relatedness is a multi-faceted notion, as it depends on the scale of the language samples (words vs. texts) andon what exactly counts as a relation. In any case, the adjective ‘semantic’ indicates that we are concerned with relationbetween the senses or denotations, and not, e.g., surface forms or etymology.2.1. Nature of semantic relations for words and textsSemantic relations between words, or rather between their senses, have been well studied and categorized in linguistics.‘freedom’ and ‘liberty’), antonymy (oppositionThey include classical relations such as synonymy (identity of senses, e.g.of senses such as ‘increase’ vs. ‘decrease’), hypernymy or hyponymy (e.g. ‘vehicle’ and ‘car’), and meronymy or holonymy(part–whole relation such as ‘wheel’ and ‘car’). From this point of view, semantic similarity is more specific than semanticrelatedness. For instance, antonyms are related, but not similar. Or, following Resnik [8], ‘car’ and ‘bicycle’ are more similar(as hyponyms of ‘vehicle’) than ‘car’ and ‘gasoline’, though the latter pair may seem more related in the world. Classicalsemantic relations are listed in hand-crafted lexical or ontological resources, such as WordNet [9] or Cyc [10], or implicitlyin Roget’s Thesaurus (as used by Jarmasz [11]), or they can be inferred from distributional data as discussed below.Additional types of lexical relations have been described as ‘non-classical’ by Morris and Hirst [12], for instance basedon membership in similar classes (e.g. positive qualities), or on association by location, or due to stereotypes – but theserelations do not qualify as similarity ones, and are generally not listed in lexical resources. Budanitsky and Hirst [1] pointout that semantic ‘distance’ can be seen as the contrary of either similarity or relatedness. In this paper, our use of ‘distance’will refer to our measure of semantic relatedness as defined below.At the sentence level, semantic relatedness can subsume notions such as paraphrase or logical relations (e.g., entailmentor contradiction). More generally, two sentences can be related by a similarity of topic, a notion that applies to multi-sentence texts as well, even though the notion of ‘topic’ is difficult to define. Topicality is often expressed in terms of thecontinuity of themes, i.e. referents or entities about which something is predicated, which ensures the coherence of texts.Linguists have analyzed coherence as being maintained by cohesive devices [13,14], which include identity-of-reference,lexical cohesion, and similarity chains based on classical lexical relations [15,16].A key relation between the semantic relatedness of words and their occurrence in texts has long been exploited byresearchers in natural language processing (NLP) under the form of distributional measures [17], despite certain limitationspointed out by Budanitsky and Hirst [1, Section 6.2]. The assumption that sentences and texts form coherent units makes itindeed possible to infer word meanings and lexical relations from distributional similarity [18], using vector-based modelssuch as Latent Semantic Analysis [19], possibly enhanced with syntactic information [20]. In return, the hidden topicalparameters that govern the occurrences of words can be modeled probabilistically (e.g. using PLSA [21] or LDA [22]), thusproviding measures of text similarity.178M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–2022.2. Use of encyclopedic knowledge for semantic relatednessSemantic relatedness has been mainly considered from the perspective of repertoires of semantic relations (often hand-crafted), or from the perspective of relations inferred from the distributional properties of words in collections of texts.However, the computation and use of relations founded on real-world knowledge has been considerably less explored,as it was made possible only recently by the emergence of large-scale hypertext encyclopedias such as Wikipedia. Webelieve that the use of encyclopedic knowledge may significantly complement semantic relatedness measures based onword distributions only: in the remainder of this section we briefly frame encyclopedic knowledge, outline our proposal,and discuss its task-based validation.Encyclopedias are lists of general concepts and named entities, accompanied by descriptions in natural language. Theydiffer from dictionaries as they describe concepts or entities, rather than define words, and provide significant factualknowledge for grounding them in the real world, rather than linguistic information only. While printed encyclopedias al-ready include certain references from one entry to another, the linking mechanism is used much more extensively withinhypertext encyclopedias such as Wikipedia. As a result, hypertext encyclopedias seem quite adept at capturing semantic re-lations between concepts, which range from culture-specific to universal ones, including classical and non-classical relationsmentioned above.To measure the extent to which two text fragments are semantically related according to an encyclopedia, two mainoperations are necessary. First, the concepts related to the texts must be identified. These can be either concepts directlymentioned in the texts, or otherwise related to it, in a sense that will be specified in Section 5 below. Second, the relatednessor proximity of the two sets of concepts thus identified must be measured. This presupposes the capacity to measure therelatedness of two concepts in the first place, taking advantage of the contents of the corresponding articles, and of thehyperlinks between them (see Section 6).Empirical evidence should support the definition of relatedness and demonstrate its relevance to NLP applications suchas those cited at the beginning of this paper. The measure proposed here will be judged and compared to others based onits performance over a variety of tasks, following an empirical stance similar to those expressed in the introductions to theirarticles by Budanitsky and Hirst [1] and Padó and Lapata [20], to cite only two examples. Our proposal will be applied toword similarity, document similarity, document clustering and classification, and information retrieval (Sections 10 to 15).3. Related workThis paper puts forward a new method for computing semantic relatedness, which makes use of a graph structure overWikipedia to solve several NLP problems. Therefore, related work spans a large number of domains and approaches, andcan be divided mainly into: (1) previous methods for computing semantic relatedness, including uses of Wikipedia or othernetworked resources, for one or more tasks in common with this paper; (2) previous algorithms for computing distanceswithin graphs; and (3) state-of-the-art methods and scores for each of the targeted tasks. In fact, many combinations oftasks, methods and resources may share one or more elements with our proposal. This section will focus on the first twocategories of previous work, while for the third one, performance comparisons with state-of-the-art methods will be madein each of the application sections. A synthetic view of previous work appears in Table 1 at the end of this section, withresources, algorithms, tasks, and data sets used for testing.3.1. Word semantic relatedness: WordNet and WikipediaMany attempts have been made in the past to define word and text similarity distances based on word overlap, forvarious applications to language technology. One approach is to construct – manually or semi-automatically – a taxonomyof word senses or of concepts, with various types of relations, and to map the text fragments to be compared onto thetaxonomy. For instance, WordNet [9] and Cyc [10] are two well-known knowledge bases, respectively of word senses andconcepts, which can be used for overcoming the strong limitations of pure lexical matching. A thesaurus such as Roget’scan also be used for similar purposes [11,23]. This approach makes use of explicit senses or concepts that humans canunderstand and reason about, but the granularity of knowledge representation is limited by the taxonomy. Building andmaintaining these knowledge bases requires a lot of time and effort from experts. Moreover, they may cover only a fractionof the vocabulary of a language, and usually include few proper names, conversational words, or technical terms.Several methods for computing lexical semantic relatedness exploit the paths in semantic networks or in WordNet, assurveyed by Budanitsky and Hirst [1, Section 2]. Distance in the network is one of the obvious criteria for similarity, whichcan be modulated by the type of links [24] or by local context, when applied to word sense identification [25]. Resnik [8,26]improved over distance-based similarity by defining the information content of a concept as a measure of its specificity,and applied the measure to word sense disambiguation in short phrases. An information-theoretic definition of similarity,applicable to any entities that can be framed into a probabilistic model, was proposed by Lin [27] and was applied toword and concept similarity. This work and ours share a similar concern – the quest for a generic similarity or relatednessmeasure – albeit in different conceptual frameworks – probabilistic vs. hypertext encyclopedia.Other approaches make use of unsupervised methods to construct a semantic representation of words or of documentsby analyzing mainly co-occurrence relationships between words in a corpus (see e.g. Chappelier [28] for a review). LatentM. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202179Semantic Analysis [19] offers a vector-space representation of words, which is grounded statistically and is applied to doc-ument representation in terms of topics using Probabilistic LSA [21] or Latent Dirichlet Allocation [22]. These unsupervisedmethods construct a low-dimensional feature representation, or concept space, in which words are no longer supposedto be independent. The methods offer large vocabulary coverage, but the resulting “concepts” are difficult for humans tointerpret [29].Mihalcea et al. [30] compared several knowledge-based and corpus-based methods (including for instance [25]) andthen used word similarity and word specificity to define one general measure of text semantic similarity. Results of severalmethods and combinations are reported in their paper. Because it computes word similarity values between all word pairs,the proposed measure appears to be suitable mainly for computing similarity between short fragments – otherwise, thecomputation becomes quickly intractable.One of the first methods to use a graph-based approach to compute word relatedness was proposed by Hughes andRamage [31], using Personalized PageRank (PPR) [32] over a graph built from WordNet, with about 400 000 nodes and5 million links. Their goal (as ours) was to exploit all possible links between two words in the graph, and not only theshortest path. They illustrated the merits of this approach on three frequently-used data sets of word pairs – which will bealso used in this paper, see Section 10 – using several standard correlation metrics as well as an original one. Their methodreaches “the limit of human inter-annotator agreement and is one of the strongest measures of semantic relatedness thatuses only WordNet.”In recent years, Wikipedia has appeared as a promising conceptual network, in which the relative noise and incomplete-ness due to its collaborative origin is compensated for by its large size and a certain redundancy, along with availability andalignment in several languages. Several large semantic resources were derived from it, such as a relational knowledge base(DBpedia [33]), two concept networks (BabelNet [34] and WikiNet [35]) and an ontology derived from both Wikipedia andWordNet (Yago [36]).WikiRelate! [37] is a method for computing semantic relatedness between two words by using Wikipedia. Each word ismapped to the corresponding Wikipedia article by using the titles. To compute relatedness, several methods are proposed,namely, using paths in the Wikipedia category structure, or using the contents of the articles. Our method, by comparison,also uses the knowledge embedded in the hyperlinks between articles, along with the entire contents of articles. Recently,the category structure exploited by WikiRelate! was also applied to computing semantic similarity between words [38].Overall, however, WikiRelate! measures relatedness between two words and is not applicable to similarity of longer frag-ments, unlike our method. Another method to compute word similarity was proposed by Milne and Witten [39] usingsimilarity of hyperlinks between Wikipedia pages.3.2. Text semantic relatednessSeveral studies have measured relatedness of sentences or entire texts. In a study by Syed et al. [40], Wikipedia wasused as an ontology in three different ways to associate keywords or topic names to input documents: either (1) by cosinesimilarity retrieval of Wikipedia pages, or (2) by spreading activation through the Wikipedia categories of these pages, or(3) by spreading activation through the pages hyperlinked with them. The evaluation was first performed on three articlesfor which related Wikipedia pages could be validated by hand, and then on 100 Wikipedia pages, for which the task wasto restore links and categories (similarly to [41]). The use of a private test set makes comparisons with other work uneasy.In another text labeling task, Coursey et al. [42] have used the entire English Wikipedia as a graph (5.8 million nodes,65 million edges) with a version of Personalized PageRank [32] that was initialized with the Wikipedia pages found to berelated to the input text using Wikify![43] . The method was tested on a random selection of 150 Wikipedia pages, withthe goal of retrieving automatically their manually-assigned categories.Ramage et al. [44] have used Personalized PageRank over a WordNet-based graph to detect paraphrases and textualentailment. They formulated a theoretical assumption similar to ours: “the stationary distribution of the graph [random]walk forms a ‘semantic signature’ that can be compared to another such distribution to get a relatedness score for texts.” Ourproposal includes a novel method for comparing such distributions, and is applied to different tasks (we tested paraphrasesin a previous paper [45]).Explicit Semantic Analysis (ESA), proposed by Gabrilovich and Markovitch [46,47], instead of mapping a text to a node ora small group of nodes in a taxonomy, maps the text to the entire collection of available concepts, by computing the degreeof affinity of each concept to the input text. ESA uses Wikipedia articles as a collection of concepts, and maps texts to thiscollection of concepts using a term/document affinity matrix. Similarity is measured in the new concept space. Unlike ourmethod, ESA does not use the link structure or other structured knowledge from Wikipedia. Our method, by walking over acontent similarity graph, benefits in addition from a non-linear distance measure according to word co-occurrences.ESA has been used as a semantic representation (sometimes with modifications) in other studies of word similarity, suchas a cross-lingual experiment with several Wikipedias by Hassan and Mihalcea [48], evaluated over translated versions ofEnglish data sets (see Section 10 below). In a study by Zesch et al. [49], concept vectors akin to ESA and path length wereevaluated for WordNet, Wikipedia and the Wiktionary, showing that the Wiktionary improved over previous methods. ESAalso provided semantic representations for a higher-end application to cross-lingual question answering [50], and was usedby Yech et al. [51], to which we now turn.180M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202Table 1Comparison of the present proposal (last line) with previous work cited in this section, in terms of resources, algorithms, NLP tasks, and data sets. Theabbreviations for the data sets in the rightmost column are explained in Section 10 on page 190. The methods are abbreviated as follows: ESA for Ex-plicit Semantic Analysis [46,47], LSA for Latent Semantic Analysis [19], IC for Information Content [8,26], PMI-IR pointwise mutual information using datacollected by information retrieval [53,30], and PPR for the Personalized PageRank algorithm [32,54].ArticleJarmasz [11], Jarmasz andSzpakowicz [23]Mihalcea et al. [30],corpus-basedMihalcea et al. [30], sixknowledge-basedResourceRogetAlgorithmShortest pathTaskWord sim.Data setM&C, R&G, SynonymsWeb/BNCPMI-IR/LSAParaphraseMicrosoftWordNetShortest path, IC, etc.==Hughes and Ramage [31]WordNetPPRWord sim.M&C, R&G, WS-353Gabrilovich and Markovitch[46]Agirre and Soroa [52]Zesch et al. [49]WikipediaESA: TF-IDF + Cosine sim.Word sim., Doc. sim.WS-353, Lee∼WordNetWordNet,Wikipedia,WiktionaryPPRWSDPath length, concept vectorsWord sim.Senseval-2, 3M&C, R&G, WS-353 +GermanStrube and Ponzetto [37]WikipediaShortest path, categories,text overlapWord sim., coreferenceresolutionM&C, R&G, WS-353Milne and Witten [39]WikipediaSimilarity of hyperlinksWord sim.M&C, R&G, WS-353Hassan and Mihalcea [48]WikipediaModified ESACross-lingual word sim.Translated M&C, WS-353Syed et al. [40]WikipediaCoursey et al. [42]Ramage et al. [44]Gabrilovich and Markovitch[47]WikipediaWordNetWikipediaCosine sim., spreadingactivationPPRPPRDoc. classif.3–100 handpicked docsDoc. classif.150 WP articlesParaphrase, entailmentMicrosoft, RTEESA: TF-IDF + Cosine sim.Doc. clusteringReuters, 20NG, OHSUMED,short docsYeh et al. [51]WikipediaPPRWord sim., Doc. sim.M&C, WS-353, LeePresent proposalWikipediaVisiting Probability (VP)Word sim., Doc. sim. andclustering, IRSee Sections 10–14Probably the closest antecedent to our study is the WikiWalk approach [51]. A graph of documents and hyperlinks wasconstructed from Wikipedia, then the Personalized PageRank (PPR) [32] was computed for each text fragment, with theteleport vector being the one resulting from ESA. A dictionary-based initialization of the PPR algorithm was studied as well.To compute semantic similarity between two texts, Yeh et al. simply compared their PPR vectors. Their scores for wordsimilarity were slightly higher than those obtained by ESA [47], while the scores on document similarity (Lee data set,see Section 11 below) were “well below state of the art, and show that initializing the random walk with all words inthe document does not characterize the documents well”. By comparison, in our method, we also consider in addition tohyperlinks the effect of word co-occurrence between article contents, and use a different random walk and initializationmethods. In particular, we have previously shown [45] that visiting probability improves over PPR, likely because it capturesdifferent properties of the network.Mihalcea and Csomai [43] and Milne and Witten [41] discussed enriching a document with Wikipedia articles. Theirmethods can be used to add explanatory links to news stories or educational documents, and more generally to enrichany unstructured text fragment (or bag-of-words) with structured knowledge from Wikipedia. Both perform disambiguationfor all n-grams, which requires a time-consuming computation of relatedness of all senses to the context articles. The firstmethod detects linkable phrases and then associates them to the relevant article, using a probabilistic approach. The secondone learns the associations and then uses the results to search for linkable phrases.3.3. Distances between nodes in graphsWe now turn to abstract methods for measuring distances between vertices in a graph. Many graph-based methodshave been applied to NLP problems (see for instance the proceedings of the TextGraphs workshops) and were recentlysurveyed by Navigli and Lapata [55] with an application to word sense disambiguation. A similar attempt was made by Ionand ¸Stef˘anescu [56], while Navigli [57] defined a method for truncating a graph of WordNet senses built from input text.Navigli and Lapata [55] focused on measures of connectivity and centrality of a graph built on purpose from the sentences todisambiguate, and are therefore close in spirit to the ones used to analyze our large Wikipedia-based network in Section 4.3.M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202181Two measures of node distance which have a similar goal as the visiting probability (VP) proposed in this paper arehitting time, a standard notion in graph theory, and Personalized PageRank (PPR) [32], surveyed by Berkhin [54]. Hittingtime from vertex si to s j is the number of steps a random walker takes on average to visit s j for the first time when itstarts from si . The difference between hitting time and visiting probability will be discussed at the end of Section 6.2 below,once our proposal is properly introduced. Hitting time has been used in several studies as a distance measure in graphs,e.g. for dimensionality reduction [58] or for collaborative filtering in a recommender system [59]. Hitting time has also beenused for link prediction in social networks along with other graph-based distances [60], or for semantic query suggestionusing a query/URL bipartite graph [61]. A branch and bound approximation algorithm has been proposed to compute anode neighborhood for hitting time in large graphs [62]. PageRank has been used for word sense disambiguation over agraph derived from the candidate text by Navigli and Lapata [55]. As for PPR, the measure has been used for word sensedisambiguation by Agirre and Soroa [52] over a graph derived from WordNet, with up to 120 000 nodes and 650 000 edges.PPR has also been used for measuring lexical relatedness of words in a graph built from WordNet by Hughes and Ramage[31], as mentioned above.4. Wikipedia as a network of concepts4.1. Concepts = nodes = verticesWe built our concept network from Wikipedia by using the Freebase Wikipedia Extraction (WEX) dataset [63] (versiondated 2009-06-16). Not all Wikipedia articles were considered appropriate to include in the network of concepts, for reasonsrelated to their nature and reliability, but also to the tractability of the overall method, given the very large number of pagesin the English Wikipedia. Therefore, we removed all Wikipedia articles that belonged to the following name spaces: Talk,File, Image, Template, Category, Portal, and List, because these articles do not describe concepts, but contain auxiliary mediaand information that do not belong into the concept network. Also, disambiguation pages were removed as well, as theyonly point to different meanings of the title or of its variants.As noted by Yeh et al. [51], short articles are often not appropriate candidates to include in the concept network, forseveral reasons: they often describe very specific concepts which have little chances to occur in texts; they might correspondto incomplete articles (stubs); they contain an unreliable selection of hyperlinks; and their number considerably slows downcomputation in the network. In previous work, Yeh et al. [51] set a size limit of 2000 non-stop words below which entrieswere pruned, and this limit decreased considerably the size of their network. As our goal is to minimize the risk of removingpotentially useful concepts, and to respect as much as possible the original contents of Wikipedia, we set a cut-off limit of100 non-stop words, thus pruning only very minor articles. This value is thus much lower than the value used by Yeh et al.[51], and is similar to the one used by Gabrilovich and Markovitch [47]. Out of an initial set of 4 327 482 articles in WEX,filtering removed about 70% of all articles based on namespaces and length cut-off, yielding a resulting set of 1 264 611concepts.Each concept has a main Wikipedia name, which is the title of the main page describing the concept. However, in manycases, other words or phrases can be used as well to refer to the concept. One such type of words can be determined byexamining Wikipedia redirects, i.e. articles that have no content but point the user to a proper article with an alternativetitle. The titles of redirect pages were added as secondary titles to the titles of the articles they redirect to. In addition,for every hyperlink from one article to another, we extracted the corresponding anchor text and considered it as anotherpossible secondary title for the linked article, thus capturing a significant part of the terminological variation of conceptnames (with some noise due to variability in linking practice). Therefore, each concept has three types of titles (see summaryof data structure in Table 2): the original one, the anchor texts of the hyperlinks targeting it, and the variants provided byredirect pages, each specific title being listed only once.4.2. Relations = links = edgesRelations between concepts can be determined in several ways. In a previous study [45], we considered four types oflinks between concepts: hyperlinks and links computed from similarity of content, of category, and of template. While eachtype of links captures some form of relatedness, we focus in the present study on the first two types, which are the mostcomplementary. However, the proposed computational framework is general enough to accommodate more than two typesof links, in particular if an optimal combination can be learned from training data.The use of hyperlinks between Wikipedia articles embodies the somewhat evident observation that every hyperlink fromthe content of an article towards another one indicates a certain relation between the two articles. These are encyclopedicor pragmatic relations, i.e. between concepts in the world, and subsume semantic relatedness. In other words, if article Acontains a hyperlink towards article B, then B helps to understand A, and B is considered to be related to A. Such linksrepresent a substantial amount of human knowledge that is embodied in the Wikipedia structure. It must be noted thatthese links are essentially asymmetric, and we decided to keep them as such, i.e. to list for a given page only its outgoinglinks and not the incoming ones. Indeed, observations showed that while the target page of a link helps understanding thesource one, the contrary is not always true or the relation is not specific enough. For each article, the XML text from WEX182M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202Table 2Logical structure of each node in the network resulting from the English Wikipedia.Concept IDNames of the conceptIntegerName of article in the encyclopediaAlternative names redirecting to the articleAnchor texts of incoming hyperlinksDescription of the conceptTextRelations to other concepts (outgoing links)Hyperlinks from the description towards other concepts (no weights)Lexical similarity links to the ten closest concepts (weights from cosine similarity)was parsed to extract hyperlinks, resulting in a total of 35 214 537 hyperlinks – a time-consuming operation that requiredalso the ability to handle instances of ill-formed XML input.The second type of links is based on similarity of lexical content between articles of Wikipedia, computed from word co-occurrence. If two articles have many words in common, then a topic-similarity relation holds between them. To capturecontent similarity, we computed the lexical similarity between articles as the cosine similarity between the vectors derivedfrom the articles’ texts, after stopword removal and stemming using Snowball.1 We then linked every article to its k mostsimilar articles, with a weight according to the normalized lexical similarity score (for non-zero weights). In the experimentsdescribed below, k was set to 10, so that each node has ten outgoing links to other nodes, based on lexical similarity.2The value of k = 10 was chosen to ensure computational tractability, and is slightly lower than the average number ofhyperlinks per concept, which is about 28. As the Wikipedia articles are scattered in the space of words, tuning k does notseem to bring crucial changes. If k is very small then the neighborhood contains little information, whereas a large k makescomputation time-consuming.4.3. Properties of the resulting networkThe processing of the English Wikipedia resulted in a very large network of concepts, each of them having the logicalstructure represented in Table 2. The network has more than 1.2 million nodes (i.e. vertices), with an average of 28 outgoinghyperlinks per node and 10 outgoing content links per node.A natural question arising at this point is: how can the structure of the network be characterized, apart from putting it towork? It is not possible to visualize the entire network due to its size, and displaying only a small part, as done for instanceby Coursey et al. [42, Fig. 1], might not be representative of the entire network.3 A number of quantitative parameters havebeen proposed in graph theory and social network analysis, and some have for instance been used to analyze WordNet (andan enhanced version of it) by Navigli and Lapata [55]. We compute below some well-known parameters for our network,and add a new, more informative characterization.A first characteristic of graphs is their degree distribution, i.e. the distribution of the number of direct neighbors pernode. For the original Wikipedia with hyperlinks, a representation [64] suggests that the distribution follows a power law.A more relevant property here is the network clustering coefficient, which is the average of clustering coefficients per node,defined as the size of the immediate neighborhood of the node divided by the maximum number of links that could connectall pairs of neighbors [65]. For our hyperlink graph, the value of this coefficient is 0.16, while for the content link graph itis 0.26.4 This shows that the hyperlink graph is less clustered than the content link one, i.e. the distribution of nodes andlinks is more homogeneous, and that overall the two graphs have rather low clustering.5We propose an ad-hoc measure offering a better illustration of the network’s topology, aimed at finding out whetherthe graph is clustered or not – i.e., whether the communities of nodes based on neighborhoods have a preferred size, orare uniformly distributed. We consider a sample of 1000 nodes. For each node of the graph, the Personalized PageRankalgorithm [32] is initialized from that node and run, thus resulting into a proximity coefficient for each node in the graphto the initial node. This is first done using hyperlinks, and then using the content links. The community size for the node iscomputed by sorting all nodes with respect to their proximity and counting how many nodes contribute to 99% of the mass.The results shown in Fig. 1 show that the distribution is neither flat nor uniformly decreasing, but has a peak, whichprovides an indication of the average size of clusters. This size is around 150–400 nodes for the hyperlink graph, withouta sharp maximum, showing less clustering than for content links, for which this average is around 7–14 nodes. The lattervalue is partly related to the 10-link limit set for content links, but is not entirely due to it, as the limit concerns only1 From the Apache Lucene indexing system available at http://lucene.apache.org/.2 Therefore, the outdegree of all nodes is 10, but as the indegree may vary (the number of incoming links), the graph is not strictly speaking a 10-regulargraph.3 An example of neighborhood according to our relatedness measure is however shown in Section 6.3.4 For the content links, the coefficient was computed regardless of their weights. A recent proposal for computing it for weighted graphs could be appliedtoo [66].5 The observed values, together with the power law degree distribution, suggest that our graph is a scale-free network – characterized by the presenceof “hub” nodes – or a small-world network [65]. However, it is not clear what impact these properties defined mainly for social networks would have here.M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202183Fig. 1. Distribution of community sizes for a sample of 1000 nodes (see text for the definition of a community). For each community size (x-axis)the graphs show the number of nodes ( y-axis) having a community of that size, for each of the two graphs built from Wikipedia. Both graphs havea tendency towards clustering, i.e. a non-uniform distribution of links, with an average cluster size of 150–400 for hyperlinks and 7–14 for contentlinks.outgoing links. The use of hyperlinks thus avoids local clusters and extends considerably the connectivity of the network incomparison to content similarity ones.5. Mapping text fragments to concepts in the networkIn order to use the concept network for similarity judgments between two text fragments, two operations are nec-essary, as explained at the end of Section 2.2. The first one, mapping each text fragment to a set of vertices of thenetwork, is explained in this section, while Sections 6 and 7 define the distance between two weighted sets of vertices.For mapping, two cases must be considered, according to whether the text matches exactly the title of a Wikipedia pageor not. Exact matching is likely to occur with individual words or short phrases, but not with entire sentences or longertexts.If a text fragment consists of a single word or a phrase that matches exactly the title of a Wikipedia page, then it issimply mapped to that concept. In the case of words or phrases that may refer to several concepts in Wikipedia, wesimply assign to them the same page as the one assigned by the Wikipedia contributors as the most salient or preferredsense or denotation. For instance,‘mouse’ directs to the page about the animal, which contains an indication that the‘mouse_(computing)’ page describes the pointing device, and that other senses are listed on the ‘mouse_(disambiguation)’page. So, here, we simply map ‘mouse’ to the animal concept. However, for other words, no sense or denotation is preferredby the Wikipedia contributors, e.g. for the word ‘plate’. In such cases, a disambiguation page is associated to that wordor phrase. We chose not to include such pages in our network, as they do not correspond to individual concepts. So,in order to select the referent page for such words, we simply use the lexical similarity approach described in the nextparagraph.When a fragment (a word, phrase, sentence, or text) does not match exactly the Wikipedia title of a vertex in our network, it ismapped to the network by computing its lexical similarity with the text content of the vertices in the network, using cosinedistance over stemmed words, stopwords being removed. Concept names (principal and secondary ones) are given twiceas much weight as the words found in the description of the concept. The text fragment is mapped to the k most similararticles according to this similarity score, resulting in a set of at most k weighted concepts. The weights are normalized,summing up to one, therefore the text representation in the network is a probability distribution over at most k concepts.Finding the k closest articles according to lexical similarity can be done efficiently using the Apache Lucene search engine(see footnote 1).For example, consider the following text fragment to be mapped to our network: “Facebook: you have some seriousprivacy and security problems.” When this fragment is mapped to the k = 10 most similar Wikipedia articles, the result-ing probability distribution is the following one: ‘Facebook’ (0.180),‘Facebook history’ (0.116),‘Criticism of Facebook’ (0.116), ‘Facebook features’ (0.116), ‘Privacy’ (0.084), ‘Privacy International’ (0.080), ‘Internet privacy’(0.080), ‘Privacy policy’ (0.054), ‘Privacy Lost’ (0.054).‘Facebook Beacon’ (0.119),This mapping algorithm has an important role in the performance of the final system, in combination with the networkdistance described hereafter. It must however be noted that the effects of wrong mappings at this stage are countered lateron. For instance, when large sets of concepts related to two text fragments are compared, a few individual mistakes arenot likely to alter the overall relatedness scores. Alternatively, when comparing individual words, wrong mappings are less184M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202likely to occur because the test sets for word similarity described in Section 10, page 190, also consider implicitly the mostsalient sense of each word, just as described above for Wikipedia.6. Semantic relatedness using visiting probabilityIn this section, we describe our framework for computing relatedness between two texts, represented as sets of conceptsin a network. Although this is applied here to Wikipedia, the model is general enough to be applied to other networkstoo. The goal is to estimate a distance between two concepts or two sets of concepts that captures their relatedness bytaking into account the global connectivity of the network, and without being biased by local properties. Indeed, the use ofindividual links and paths, e.g. when estimating conceptual relatedness as the length of shortest path, does not take intoaccount their relative importance with respect to the overall properties of the network, such as the number and length ofall possible paths between two nodes. Moreover, the length of the shortest path is quite sensitive to spurious links, whichare frequent in Wikipedia. Therefore, a number of aggregated proximity measures have been proposed, including PageRankand hitting time, as reviewed in Section 3 above.Our proposal uses a random walk approach, and defines the relatedness of concept sets as the visiting probability, in thelong run, of a random walker going from one set of nodes to the other one. The walker can use either type of directed linksbetween concepts described in Section 4.2, i.e. hyperlinks or lexical similarity ones. This algorithm is independent from themapping algorithm introduced in the previous section.6.1. NotationsLet S = {si | 1 (cid:2) i (cid:2) n} be the set of n concepts or vertices in the network. Any two concepts si and s j can be connectedby one or more directed and weighted links, which can be of L different types (L = 2 in our case: hyperlinks and lexicalsimilarity links). The structure of links of type l (1 (cid:2) l (cid:2) L) is fully described by the matrix Al of size n × n, where Al(i, j) isthe weight of the link of type l between si and s j , with possible weights being 0 or 1 for the hyperlink matrix, and actuallexical similarity scores (or 0) for the content similarity matrix. The transition matrix Cl gives the probability of a direct(one step) transition between concepts si and s j , using only links of type l. This matrix can be built from the Al matrix asfollows:Cl(i, j) =Al(i, j)(cid:2)nk=1 Al(i, k).In the random walk process using all link types (1 (cid:2) l (cid:2) L), let the weight wl denote the importance of link type l. Then,the overall transition matrix C which gives the transition probability Ci, j between any concepts si and s j is C =Ll=1 wlCl.Finally, let (cid:4)r be the n-dimensional vector resulting from mapping a text fragment to the concepts of the network (Sec-tion 5), which indicates the probabilities of concepts in the network given a text, or the relevance of concepts to the text’swords. The sum of its elements is 1.(cid:2)6.2. Definition of visiting probabilityGiven a probability distribution (cid:4)r over concepts (i.e. a weighted set of concepts), and a concept s j in the network, wefirst compute the probability of visiting s j for the first time when a random walker starts from (cid:4)r in the network, i.e. fromany of the concepts in (cid:4)r that have a non-zero probability.6 To compute visiting probability, the following procedure providesa model of the state St of the random walker, i.e. the concept in which the random walker is positioned. Executing theprocedure until termination gives the visiting probability VP.Step 0:Step t:Choose the initial state of the walker with probability P (S 0 = si | (cid:4)r) = ri . In other words, position the randomwalker on any of the concepts of (cid:4)r with the probability stated in (cid:4)r.Assuming St−1 is determined, if St−1 = s j then return ‘success’ and finish the procedure. Otherwise, with proba-bility α, choose a value for the next concept St according to the transition matrix C , and with probability 1 − α,return ‘fail’. The possibility of ‘failing’, or absorption probability, introduces a penalty over long paths that makesthem less probable.(cid:5)We introduce Cas being equal to the transition matrix C , except that in row j, C(cid:5)( j, k) = 0 for all k. This indicatesthe fact that when the random walker visits s j for the first time, it cannot exit from it and its probability mass drops tozero in the next step. This modified transition matrix was defined to account for the definition of visiting probability as theprobability of first visit of s j (a similar idea has been proposed by Sarkar and Moore [62]).6 A simpler derivation could first be given for the visiting probability to s j starting from another concept si , but to avoid duplication of equations, weprovide directly the formula for a weighted set of concepts.M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202185To compute the probability of success in the above process, we introduce the probability of success at step t, pt(success),(cid:5) t is the power t of. Then, the probability of success in the process, i.e. the probability of visiting s j starting from (cid:4)r, is the sum over(cid:5) t) j is the jth element of the vector (cid:4)rC(cid:5) t) j . In this formula, ((cid:4)rCwhich is pt(success) = αt((cid:4)rCmatrix Call probabilities of success with different lengths:(cid:5) t , and C(cid:5)p(success) =∞(cid:3)t=0pt(success) =(cid:4)(cid:4)rC(cid:5)(cid:5) tαtj.∞(cid:3)t=0The visiting probability of s j starting from the distribution (cid:4)r, as computed above, is different from the probability as-signed to s j after running Personalized PageRank (PPR) with a teleport vector equal to (cid:4)r. In the computation of VP, the loopsstarting from s j and ending to s j do not have any effect on the final score, unlike the computation of PPR, for which suchloops boost the probability of s j . If some pages have this type of loops (typically “popular” pages), then after using PPR theywill have high probability although they might not be very close to the teleport vector (cid:4)r.(cid:2)∞The visiting probability of s jis also different from the hitting time to s j , defined as the average number of steps arandom walker would take to visit s j for the first time in the graph. If we use the same notations, the hitting time from (cid:4)r(cid:5) t) j . Hitting time is more sensitive to long paths in comparison to VP (t in comparison with αtto s j is H((cid:4)r, s j) =in the formula), which might introduce more noise, while VP reduces the effect of long paths sooner in the walk. We havecompared the performance of these three algorithms in a previous paper [45] and concluded that VP outperformed PPR andhitting time, which will not be used here.t=0 t((cid:4)rC6.3. Visiting probability to weighted sets of concepts and to textsTo compute the VP from a weighted set of concepts to another set, i.e. from distribution (cid:4)r1 to distribution (cid:4)r2, weconstruct a virtual node representing (cid:4)r2 in the network, noted sR ((cid:4)r2). We then connect all concepts si to sR ((cid:4)r2) according tothe weights in (cid:4)r2. We now create the transition matrix Cby adding a new row (numbered nR ) to the transition matrix Cwith all elements zero to indicate sR ((cid:4)r2), then adding a new column with the weights in sR ((cid:4)r2), and updating all the otherrows of C as follows (Ci j is an element of C ):(cid:5)(cid:4)(cid:5)CCC(cid:5)i j(cid:5)inR(cid:5)nR j1 − ((cid:4)r2)i= Ci j= ((cid:4)r2)i= 0 for all j.for all i,for i, j (cid:7)= nR ,These modifications to the graph are local and can be done at run time, with the possibility to undo them for subsequenttext fragments to be compared.To compute relatedness between two texts, we average between the visiting probability of (cid:4)r1 given (cid:4)r2 (noted VP((cid:4)r2, (cid:4)r1))and the visiting probability of (cid:4)r2 given (cid:4)r1 (noted VP((cid:4)r1, (cid:4)r2)). A larger value of the measure indicates closer relatedness. It isworth mentioning that 2 − (VP((cid:4)r1, (cid:4)r2) + VP((cid:4)r2, (cid:4)r1)) is a metric distance, as it satisfies four properties: non-negativity, identityof indiscernible, symmetry, and triangle inequality, as it can be shown using the definition of VP.6.4. Illustration of visiting probability ‘to’ vs. ‘from’ a textTo illustrate the difference between VP to and from a text fragment, we consider the following example. Though any textfragment could be used, we took the definition of ‘jaguar’ (animal) from the corresponding Wikipedia article. Although theword ‘jaguar’ is polysemous, the topic of this particular text fragment is not ambiguous to a human reader. The text wasfirst mapped to Wikipedia as described above. Then, using VP with equal weights on hyperlinks and content links, the tenclosest concepts (i.e. Wikipedia pages) in terms of VP from the text, and respectively to it, were found to be the followingones:• Original text: “The jaguar is a big cat, a feline in the Panthera genus, and is the only Panthera species found in theAmericas. The jaguar is the third-largest feline after the tiger and the lion, and the largest in the Western Hemisphere.”• Ten closest concepts according to their VP to the text: ‘John Varty’, ‘European Jaguar’, ‘Congolese Spotted Lion’, ‘Lionheadrabbit’, ‘Panthera leo fossilis’, ‘Tigon’, ‘Panthera hybrid’, ‘Parc des Félins’, ‘Marozi’, ‘Craig Busch’.• Ten closest concepts according to their VP from the text: ‘Felidae’, ‘Kodkod’, ‘North Africa’, ‘Jaguar’, ‘Panthera’, ‘Algeria’,‘Tiger’, ‘Lion’, ‘Panthera hybrid’, ‘Djémila’.The closest concepts according to VP from a text tend to be more general than closest concepts according to VP to thetext. For instance, the second list above includes the genus and family of the jaguar species (Panthera and Felidae), while thefirst one includes six hybrid or extinct species related to the jaguar. Concepts close to a text thus bring detailed informationrelated to the topics of the text, while concepts close from a text are more popular and more general Wikipedia articles.Note also that none of the closest concepts above is related to the Jaguar car brand, as found by examining each page,186M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202including the lesser-known ones (both persons cited are wildlife film makers and park founders). However, not all conceptsare related in an obvious way (e.g. ‘Algeria’ is likely retrieved through ‘Western Hemisphere’).The behavior illustrated on this example is expected given the definition of VP. A concept that is close to a text hasmore paths in the network towards the text with respect to other concepts in the network, which means that the conceptis quite specific in relation to the topics in the text, as in the above example. Conversely, a concept that is close from atext (in terms of VP from a text) is typically related by many paths from the text to the concept, in comparison to otherconcepts. This generally means that it is likely that the article is a general and popular article around the topics of the text.If we consider the hierarchy between concepts from more general to more specific ones, then concepts close to a text aregenerally lower in the hierarchy with respect to the text, and concepts close from a text are generally higher.7. Approximations: T -truncated and ε-truncated visiting probabilityThe above definition of the random walk procedure has one direct consequence: computation can be done iteratively andcan be truncated after T steps when needed, especially to maintain computation time within acceptable limits. Truncationmakes sense as higher order terms (for longer paths) get smaller with larger values of t because of the αt factor. Moreover,besides making computation tractable, truncation reduces the effect of longer (hence less reliable) paths on the computedvalue of p(success). Indeed, VP to popular vertices (i.e. to which many links point) might be quite high even when theyare not really close to the starting distribution (cid:4)r, due to the high number of long paths toward them, and truncationconveniently reduces the importance of such vertices. We propose in this section two methods for truncating VP, and justifyempirically the level of approximation chosen for our tasks.T -truncated visiting probability. The simplest method, called T -truncated VP, truncates the computation for all the pathslonger than T . To compute an upper bound on the error of this truncation, we start by considering the possible state ofthe random walk at time T : either a success, or a failure, or a particular node. The sum of the probabilities of these threestates equals 1, while the third value, i.e. the probability of returning neither success nor failure in the first t steps, can(cid:5) t)i – this is in fact the probability mass at time t at all nodes except s j , the targeted node. Ifbe computed asp T (success) denotes the probability of success considering paths of length at most T , and εT the error made by truncatingafter step T , then by replacing p T (success) with the value we just computed, and noting that p(success) + p T (failure) (cid:2) 1,we obtain the following upper bound for εT :i(cid:7)= j αt((cid:4)rC(cid:2)nεT = p(success) − p T (success) (cid:2)(cid:4)(cid:4)rC(cid:5)(cid:5) Tα Ti.n(cid:3)i(cid:7)= jSo, if p T (success) is used as an approximation for p(success) then an upper bound for this approximation error εT is the(cid:5) T )i are both decreasing overright term of the above inequality. This term decreases over time because α T andtime, therefore εT decreases when T increases.(cid:2)ni(cid:7)= j(rCε-truncated visiting probability. A second approach, referred to as ε-truncated VP, truncates paths with lower probabilitiesin earlier steps and lets paths with higher probabilities continue more steps. Given that the probability of being at si at time(cid:5) t )i . Setting this(cid:5) t )i , if this is neglected and set to zero, then the error caused by this approximation is αt(rCstep t is αt(rCterm to zero means exactly that paths that are at si at time step t are no longer followed afterwards. So, in ε-truncation,(cid:5) t )i (cid:2) ε. This approach is faster to compute than thepaths with a probability less than ε are not followed, i.e. when αt(rCprevious one, but no upper bound of the error could be established. We use the ε-truncated VP in some of our experimentsbelow, leading to competitive results in an acceptable computation time.T -truncated visiting probability from all vertices to a vertex. Given a dataset of N documents, some tasks such as documentclustering (Section 12) require to compute the average VP between all N documents. Similarly, for information retrieval(Section 14), it is necessary to sort all the documents in a repository according to their relatedness to a given query. It isnot tractable to compute exact VP values for all concepts, but we provide here a way to compute T -truncated VP from/to adistribution (cid:4)r for all concepts in the network, which is faster than repeating the computation for each concept in part.As in Section 6.3 above, we consider the virtual concept sR ((cid:4)r) representing (cid:4)r in the network (noted simply sR ). It is thenpossible to compute VP from all concepts towards sR at the same time, using the following recursive procedure to computethe T -truncated visiting probability VPT . This procedure follows from the recursive definition of VP given in Section 6.2,which states that VP(si, sR ) = α(cid:5)(si, sk)VP(sk, sR ). Therefore,(cid:2)k CVPT (si, sR ) = α(cid:5)(si, sk)VPT −1(sk, sR )Cfor i (cid:7)= nR ,(cid:3)kVPT (si, sR ) = 1 for i = nR ,VP0(si, sR ) = 0.Using dynamic programming, it is possible to compute T -truncated VP from all concepts to sR in O (E T ) steps, where E isthe number of edges of the network.M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202187Fig. 2. VPsum(T ) as an “average VP” for content links (a) and hyperlinks (b) over the Wikipedia graph, depending on T , for α varying from 0.4 to 0.9 (by0.1, bottom to top). The shape of the curves indicates the values of T leading to an acceptable approximation of the exact, non-truncated value: α = 0.8and T = 10 were chosen in the subsequent experiments.T -truncated visiting probability from a vertex to all vertices. Conversely, to compute VPT from (cid:4)r to all concepts in the network,the total computation time is O (N E T ), where N is the number of concepts and E the number of edges, because VPT mustbe computed for each concept. With our current network, this is very time consuming: therefore, we propose the followingsampling method to approximate T -truncated VP. The sampling involves running M independent T -length random walksfrom (cid:4)r. To approximate VPT to concept s j from (cid:4)r, if s j has been visited for the first time at {tk1 , . . . , tkm} time steps in the(cid:2)(cid:6)VPT ((cid:4)r, s j) = (l αtkl )/M.M samples, then the T -truncated VP to s j can be approximated by the following average:According to the proposed method, it is possible to approximate truncated VP from (cid:4)r to all concepts in the networkin O (M T ) time, where M is number of samples. It remains to find out how many samples should be used to obtain thedesired approximation level, a question that is answered by the following theorem. For any vertex, the estimated truncatedVP approximates the exact truncated VPT to that vertex within ε with a probability larger than 1 − δ if the number ofsamples M is larger than. The proof of this result is given in Appendix A.α2 ln(2n/δ)2ε28. Empirical analyses of VP and approximationsWe now analyze the convergence of the two approximation methods proposed above, i.e. the variation of the margin oferror with T or ε. In the case of T -truncated approximation, when T increases to infinity, the T -truncated approximatedVP converges towards the exact value, but computation time increases linearly with T . In the case of ε-truncation, when εtends to zero the approximated value converges towards the real one, but again computation time increases. Therefore, weneed to find the proper values for T and ε as a compromise between the estimated error and the computing time.8.1. Convergence of the T -truncated VP over WikipediaThe value of T required for a certain level of convergence (upper bound on the approximation error) depends on thetransition matrix of the graph. It was not possible to find a convenient theoretical upper bound, so we analyzed the relationbetween T and the approximation error empirically, by a sampling method. We chose randomly a set S of 1000 nodes inthe graph, and computed the T -truncated VP from all nodes in the graph to each of the nodes in S. Then we computed theaverage of the values of T -truncated VP from all nodes to the nodes in S: VPsum(T ) =i(cid:7)= j VPT (i, j)/|S|.Given that S is a large random sample, we consider that the evolution of VPsum(T ) with T is representative of theevolution of an “average” VPT with T . Fig. 2(a) shows the values of VPsum(T ) depending on T for the Wikipedia graphwith the lexical similarity (content) links, for various values of α. Fig. 2(b) shows the same curves for the Wikipedia graphwith hyperlinks. Both analyses show, as expected, that larger values of α correspond to a slower convergence in terms of T ,because a larger α requires the random walker to explore longer paths for the same level of approximation. (The exact valuetoward which VPsum(T ) converges is not important here.) The figures give some indication, for each given α, of the extentto which the paths should be explored for an acceptable approximation of the non-truncated VP value. In our experiments,we chose α = 0.8 and T = 10 as a compromise between computation time and accuracy – the approximation error is lessthan 10% in Figs. 2(a) and 2(b).(cid:2)(cid:2)j∈S8.2. Convergence of ε-truncated VP over WikipediaTo analyze the error induced by ε-truncation when computing VP over the Wikipedia graph, we proceeded in a similarway. We chose 5000 random pairs of nodes and computed the sum of ε-truncated VP between each pair. Fig. 3(a) shows the188M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202Fig. 3. Sum of ε-truncated VP depending on 1/ε for the Wikipedia graph with content links (a) or hyperlinks (b), for α from 0.6 to 0.9 (bottom to top).−5 and α = 0.8 were chosenThe shape of the curves indicates the values of ε leading to an acceptable approximation of the non-truncated value: ε = 10for subsequent experiments.Fig. 4. Average proportion of identical nodes among K closest neighbors using cosine similarity vs. average T -truncated VP, for varying sizes K of theneighborhood and various values of α (which have little influence on the result). The proportion decreases for larger neighborhoods, i.e. the two metricsgive quite different results for smaller relatedness values.values of the sum depending on 1/ε for the Wikipedia graph with content links, and Fig. 3(b) for the Wikipedia graph withthe hyperlinks. Again, it appears from the curves that for a larger α, smaller values of ε are needed to reach the same levelof approximation error, because for a larger α longer paths must be explored to reach the same approximation level. The−5, and a value of α = 0.8ε-truncation is used for word similarity and document similarity below, with a value of ε = 10as above.8.3. Differences between the random walk model over content links and direct lexical similarity between articlesPerforming a random walk over the Wikipedia graph leads to a relatedness measure that is different from direct lexicalsimilarity (i.e. cosine similarity in the space of words), as we empirically show here through the following experiment. Werandomly chose 1000 nodes and sorted all other nodes (Wikipedia articles) based on the cosine similarity with the randomlyselected nodes, measured using TF-IDF vectors. In parallel, for each of the randomly selected nodes, all other nodes werealso sorted based on the average T -truncated VP with T = 10.The comparison of the two resulting sorted lists for each node shows the difference between the two measures. Toperform this comparison, for each node we look at the intersection of the heads of the two sorted lists (neighborhoods ofthe node), varying the size of these subsets. Fig. 4 shows the percentage of nodes in common depending on neighborhoodsizes, for different values of α (which changes little to the results). It appears that for the closest neighbors, and in particularfor the closest one, both lexical similarity and VP over content links return the same nodes. However, when expanding thesize of the neighborhood, the size of the intersection decreases rapidly. For example, when looking at the ten closest nodes,M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202189only about 50% of the nodes on average are the same in the two lists. This is an empirical argument showing that walkingover content links leads to a different relatedness measure than simply using lexical similarity between articles.9. Learning embeddings from visiting probabilitiesIn our approach to computing semantic relatedness, each text is mapped to a fixed number of concepts, and the distancebetween them is computed using VP at runtime. To make computation tractable, we have introduced a number of approxi-mations in Section 7. In this section, we put forward a more principled approach, which learns from VP values a similaritymeasure between text vectors, then applies it with a much lower cost at run time. This approach does not require a fixednumber of nodes to project a document to. Moreover, it can be integrated as prior knowledge to other learning algorithmsfor NLP and can be applied to very large scale problems.At training time, given a series of samples – that is, pairs of texts with VP values from the first text to the second one– the goal is to learn a transformation from the space of words to a latent space, so that the similarity between the latentrepresentation of the texts is as close as possible to the VP similarity. In other words, the goal is to approximate VP between(cid:5)two texts i and j by the matrix product xi A Bj , where xi and x j are the TF-IDF vectors of the two texts constructed fromxtheir words using a fixed dictionary. The size of matrices A and B is n × m, with n being the size of the dictionary (numberof words) and m the size of the latent space (akin to the number of topics in topic models). Two different matrices A andB are needed because VP values are not symmetric in i and j.(cid:5)In principle, all pairs of Wikipedia articles (i.e., texts) corresponding to nodes in our network can be used for training, butthis set is extremely large (ca. 1.4 × 1012). Therefore, we formulate the following constraints for training: (1) training shouldfocus on neighboring articles (articles with high VP values), and (2) the exact values of VP are replaced with the ranking ofpairs of articles by decreasing VP. We show here that under these constraints valuable embeddings can be learned.Let VPtok(i) be the set of the k closest articles to the article i according to VP similarity. We define a hinge loss functionL as follows, so that the similarity between i and its k closest articles is larger than the similarity to all other articles by afixed margin M(cid:3)(cid:3)(cid:3)(cid:4)L =i∈WPj∈VPtok(i)z /∈VPtok(i)max0, M − xi A B(cid:5)(cid:5)xj+ xi A B(cid:5)(cid:5)xz(cid:5).We optimize L with stochastic gradient descent: in each iteration we randomly choose one article i, then randomly chooseone of the k closest articles to i (noted j) and one other article from the rest of documents (noted z).In our experiments, we set k = 10 and M = 0.2. We computed the VPtok(i) values for all i using the approximationsin Section 7 above. We built the dictionary by using the Snowball tokenizer from Lucene and removing the highest andlowest frequency words, keeping around 60 000 words. We set the number of topics to m = 50, because a larger m offers ahigher learning capability, but also increases linearly the number of parameters to learn. Given the size of the training set,we chose a rather small number m to make the training possible in a reasonable time, and found a satisfactory predictionerror.Moreover, to perform regularization over matrices A and B when optimizing L, we impose the constraint that A andB are orthonormal. In order to apply this constraint, we project at every 1000 iterations both A and B to their nearestorthogonal matrix found by using SVD decomposition. The rationale for the constraint is the following: if we assume thateach latent dimension corresponds to a possible topic or theme, then these should be as orthogonal as possible.Figs. 5(a) and 5(b) show the average training error at every 1000 iterations, with and without regularization, respectivelyfor the hyperlink graph and for the content link one. The training error is computed as the number of text triples (i, j, z)(cid:5)(cid:5)xfor which the test in the hinge loss function is false, i.e. xi A BjTo test the predictive power of the embeddings as a replacement for the computation of VP at runtime, we excluded50 000 articles from the training set, and then built 50 000 triples of articles by choosing randomly, for each of the excludedarticles, one article from the k closest ones and one randomly from the rest of the articles. The test set error, which is thenumber of triples from this set that do not respect the order given by VP similarities, is shown in Table 3.(cid:5)z < M.x− xi A B(cid:5)The two main findings are the following. First, VP over the hyperlinks graph is harder to learn, which may be due tothe fact that hyperlinks are defined by users in a manner that is not totally predictable. Second, regularization decreasesthe prediction ability. However, if regularization traded prediction power for more generality, in other words if it reducedoverfitting to this problem and made the distance more general, then it would still constitute a useful operation. This willbe checked in the experiments in Sections 12, 13 and 15. Moreover, these experiments will show that the embeddings canbe plugged into state-of-the-art learning algorithms as prior knowledge, improving their performance.10. Word similarityIn the following sections, we assess the effectiveness of visiting probability by applying it to four language processingtasks. In particular, for each task, we examine each type of link separately and then compare the results with those obtainedfor combinations of links, in the attempt to single out the optimal combinations.The word similarity task has been heavily researched using a variety of methods and resources – such as WordNet,Roget’s Thesaurus, the English Wikipedia or Wiktionary – starting for instance with Resnik’s seminal paper [8] and even190M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202Fig. 5. Average training error for learning embeddings, measured every 1000 iterations, with and without using regularization.Table 3Accuracy of embeddings learned over four different training sets. The er-ror (percentage) is computed for 50 000 triples of articles from a separatetest set, as the number of triples that do not respect the ordering of VPsimilarities.Training set for embeddingHyperlinksHyperlinks with RegularizationContent LinksContent Links with RegularizationError (%)9.813.55.45.9earlier. We have reviewed in Section 3 some of the recent work on word similarity, focusing mainly on graph-based methodsover Wikipedia but also WordNet. Recent studies, including also references to previous scores and several baselines, havebeen made by Bollegala et al. [67], Gabrilovich and Markovitch [46] (see also [47, Tables 2 and 3]), Zesch et al. [49], Agirreet al. [68], and Ramage et al. [44], among others.Three test sets for the English word similarity task have been extensively used in the past. They consist of pairs ofwords accompanied by average similarity scores assigned by human subjects to each pair. Depending on the instructionsgiven to the subjects, the notion of ‘similarity’ was sometimes rather interpreted as ‘relatedness’, as discussed for instanceby Hughes and Ramage [31, Section 5]. The three sets, all of them reproduced in Jarmasz’s thesis [11] for instance, weredesigned respectively by Rubenstein and Goodenough [69] (henceforth, R&G, 65 pairs, 51 judges), by Miller and Charles [70](M&C, 30 pairs), and by Finkelstein et al. [71] (WordSimilarity-353, with 353 pairs).We estimate the relatedness between words by mapping them to concepts and computing the ε-truncated VP distance−5. We set the value of α = 0.8 as explained in Section 8.2 above. The correlation with humanbetween them with ε = 10judgments of relatedness is measured using the Spearman rank correlation coefficient ρ as well as the Pearson correlationcoefficient r between the VP values for each pair and the human judgments.The values of the Spearman rank correlation coefficient ρ on the WordSimilarity-353 data set, for varying relative weightsof content links vs. hyperlinks in the random walk, are shown in Fig. 6. The best scores reach ρ = 0.70 for a combination ofhyperlinks and content links, weighted between 0.3/0.7 and 0.2/0.8. As shown in the figure, results improve by combininglinks in comparison with using a single type. Results improve rapidly when hyperlinks are added to content ones (right endof the curve), even with a small weight, which shows that adding encyclopedic knowledge represented by hyperlinks toword co-occurrence information can improve significantly the performance. Conversely, adding content links to hyperlinksalso improves the results, likely because the effect of spurious hyperlinks is reduced after adding content links.To find out whether the two types of links encode similar relations or not, we examined to what extent results using VPwith hyperlinks only are correlated with results using content links only. The Spearman rank correlation coefficient betweenthese scores is ρ = 0.71, which shows that there is some independence between scores.We also tested our method on the R&G and M&C data sets. The values of the Pearson correlation coefficient r forprevious algorithms using lexical resources are given by Jarmasz [11, Section 4.3.2], by Gabrilovich and Markovitch [47,Table 3] and by Agirre et al. [68, Table 7], showing again that for word similarity, using lexical resources is very successful,reaching a correlation of 0.70–0.85 with human judgments. For our own algorithm, correlation r with human judgments onR&G and M&C is, respectively, 0.38–0.42 and 0.46–0.50, depending on the combination of links that is used. Lexically-basedM. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202191Fig. 6. Spearman rank correlation ρ between automatic and human judgments of word similarity on the WordSimilarity-353 data set, depending on theweight of content links in the random walk (the weight of hyperlinks is the complement to 1). The best scores, ρ = 0.70, are reached when contentlinks have more weight than hyperlinks (0.7–0.8 vs. 0.3–0.2). The result of LSA, ρ = 0.56, is quoted from [46], and is outperformed by other scores in theliterature.Table 4A comparison of several word similarity scores, in terms of Spearman rank correlation ρ and Pearson correlation r, on two data sets: WordSimilarity-353 [71] and M&C [70]. Our scores for VP appear in the last line, and are for WordSimilarity-353 in the upper range, though not above state-of-the-artones.WordSimilarity-353StudyFinkelstein et al. [71]Jarmasz [11]Strube and Ponzetto [37]Hughes and Ramage [31]Gabrilovich and Markovitch [46]Agirre et al. [68]ρ0.560.550.480.550.750.78M&C data setStudyWu and Palmer [72]Resnik [8]Leacock and Chodorow [25]Lin [27]Jarmasz [11]Patwardhan and Pedersen [73]Bollegala et al. [67]Alvarez and Lim [74]Hughes and Ramage [31]Agirre et al. [68]VP0.70VPρ0.780.810.790.820.87N/A0.82N/A0.900.920.69r0.780.800.820.830.870.910.830.91N/A0.930.50techniques are thus more successful on these data sets, for which only the lexical similarity relation is important, while ourmethod, which considers linguistic as well as extra-linguistic relations, is less efficient.However, if we examine the Spearman rank correlation ρ on the R&G and M&C data sets, considering therefore only theranking between pairs of words and not the exact values of the relatedness scores, then our method reaches 0.67–0.69. Ourscores are thus still lower than the best lexically-based techniques, which have a ρ between 0.74–0.81 and 0.69–0.86 onR&G and M&C, but the difference is now smaller. Our method is thus more suitable for capturing the ranking of the pairsinstead of their exact scores, an observation that was also made by Gabrilovich and Markovitch [47].Gabrilovich and Markovitch [46, Table 4] (see also [47, Table 3]) and Agirre et al. [68, Table 9] provide the values ofthe Spearman rank correlation coefficient ρ of previous methods on the WordSimilarity-353 data set. The best results isobtained by Explicit Semantic Analysis with ρ = 0.75 and by a system combination of distributional and WordNet-basedmethods (Agirre et al. [68]) with ρ = 0.78. Apart from these methods, the best reported scores on this data are the onesreached by LSA with ρ = 0.56 oly. Our method outperforms this score by a margin that is similar to that of ESA or systemcombination [68], though our method does not quite reach their scores. Some authors have attempted to reproduce the ESAscores: Zesch et al. [49] reached only ρ = 0.46, while Ramage et al. [44] and Hassan and Mihalcea [48] reported resultsclose to the original ones [46,47]. A key factor that ensures high performance seems to be the cleaning procedure appliedto concept vectors [47, 3.2.3]. Overall, to facilitate comparison of our scores to important scores in the literature, Table 4provides a synthetic view.To improve our understanding of the types of links, Fig. 7 shows the average frequency of the path lengths traveled forcomputing ε-truncated VP on the word pairs from WordSimilarity-353, for three different combinations of links. The resultsshow that using hyperlinks shortens the length of the average path that is used for the computation of VP. Conversely, byusing hyperlinks, the number of paths between words is increasing dramatically in comparison to using content links only,192M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202Fig. 7. Average frequency of the path lengths contributing to VP on the WordSimilarity-353 data set, in three configurations (left to right for each integervalue): hyperlinks only, equal weights, content links only.Fig. 8. Pearson correlation coefficient r between VP results and human judgments on document similarity, depending on the weight of the hyperlinks inthe combination (the weight of the content links is the complement to 1). The best score of r = 0.676 is reached when hyperlinks have less weight thancontent links, but are still used. The result of LSA, r = 0.60, is quoted from Lee et al. [75].a fact that explains why adding hyperlinks, even with a small weight, to content links improves the results so rapidly inFig. 6.11. Document similarityThe estimation of document similarity is another task on which our proposal was assessed. The document similarity dataset used in this experiment was gathered by Lee et al. [75], and contains average human similarity scores for all pairs of aset of 50 documents.As in the experiment with word similarity (using the same parameters α = 0.8 and ε = 10−5), we tested our methodusing various combinations of weights for the two types of links, with results shown in Fig. 8. Following Gabrilovich andM. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202193Fig. 9. Frequencies of the path lengths contributing to VP, averaged over the document similarity data set, in three configurations of the system (left toright for each integer value): hyperlinks only, content links only, and equal weights for both types.Markovitch [46] – and because the averaged human judgments cover only a small range of possible values – we use thePearson correlation coefficient r to evaluate how close our method approaches the human judgments. For these experiments,each document from the set was mapped to the 1000 closest concepts in the network. Otherwise, the same random walkparameters as for the word similarity task were used.Our findings are that the behavior of the system and its best results are similar to the previous experiment on wordsimilarity. Adding hyperlinks to content links improves the correlation sharply, but adding content links to hyperlinks alsoimproves the results (after an initial decrease), so that the best performance (r = 0.676) is reached with a combination oflinks, which appears to be very similar to the word similarity experiment.The authors of the data set (Lee et al. [75]) report the results of several other methods on the document similarity task,the best one reaching r = 0.60 using LSA. However, in this case (also mentioned by Gabrilovich and Markovitch [46]), LSAwas trained only a small document corpus of 314 news articles. When trained over a much larger corpus, the performanceof LSA increases to r = 0.69, a value reported by Hassan and Mihalcea [76] after training LSA over the entire Wikipediacorpus. ESA [46] reaches a score of r = 0.72 on this task, which outperforms all other methods (as in the case of wordsimilarity) including ours, although by a small margin. Indeed, with our method, the best observed combination reachedr = 0.676. Therefore, although our method is below some of the best results in the literature, it reaches nevertheless highscores with a general semantic relatedness measure.As we did for word similarity, we show in Fig. 9 the frequency of the path lengths traveled for computing VP, averagedon the document similarity data set, for three different combination of links. The figure shows that using mostly hyperlinksshortens the average path length that is used for the computation, while using more content links lengthens the pathlengths.12. Document clusteringThis section describes the experimental setting and the results of applying the text relatedness measure defined aboveto the problem of document clustering over the 20 Newsgroups dataset.7 The dataset contains about 20 000 postings to7 The dataset is distributed at http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html, see also [77, Chapter 6].194M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202Table 5Rand Index (RI), precision, recall and F-score of different clustering methods. VPComb stands for VP over a combination of content links (weighted 0.6) andhyperlinks (0.4). Scores in bold are significantly higher than LSA, the one in italics significantly lower (t-test, p < 0.001).Distance metricPrecisionCosine similarity of TF-IDF vectorsLSAVP on content linksVP on hyperlinksVPComb (0.6 content links)Embedding from content links (ECL)Embedding from hyperlinks (EHL)ECL with regularizationEHL with regularization9.2919.1024.1318.3624.2621.3022.6322.7424.08Recall19.0420.6437.1539.2236.9126.1328.4027.6729.64F-score12.4919.8429.1324.7229.2323.4725.1724.9526.56RI86.6791.6790.9487.7891.0391.4891.5691.6891.8020 news groups, hence 20 document classes with about 1000 documents per class. We aim here at finding these classesautomatically, using for testing the entire data set without using any part of it as a training set. The knowledge of oursystem comes entirely from the Wikipedia network and the techniques described in Sections 6–7 for computing distancesbetween two texts projected onto the network. We also experimented with the embeddings trained over VP similaritiesdescribed in Section 9.12.1. Setup of experiment on 20 NewsgroupsWe first compute a similarity matrix for the entire 20 Newsgroups data set, with the relatedness score between anytwo documents being VPT . For tractability, we fixed T = 10, a value that gives sufficient precision, as studied empiricallyin Section 8 above. Similarly, we empirically set the absorption probability of the random walk at 1 − α = 0.2. Based on αand T , the results in Section 7 allowed us to compute the error bound of the truncation. So, the choice of α and T was alsoguided by the fact that for a smaller α, fewer steps (T ) are needed to achieve the same approximation precision because ofthe penalty set to longer paths. In this application, instead of computing VPT between all possible pairs separately, we filledone row of the matrix at a time using the approximations we proposed. To use the embeddings we simply transform allthe documents by the embeddings matrices and then run the clustering algorithm over the transformed documents in theprojection space. We trained two matrices, A and B, for each Wikipedia graph. The qualitative behavior of the results byusing A and B is very similar. Here we report only the results of the transformation by A to help readability of the results.Clustering is performed using a k-means algorithm over each of the similarity matrices and feature representations.The similarity metric between two representation vectors is cosine similarity. Given the randomized initialization of thealgorithm, the final clusterings are different in each run of the algorithm.The quality of the clustering is measured using the Rand Index (RI), which counts the proportion of pairs of documentsthat are similarly grouped, i.e. either in the same, or in different clusters, in the reference vs. candidate clusterings. RI tendsto penalize the smaller clusters, for example if the algorithm clusters together two classes and splits another class, it willreceive a high penalty from RI. This happens often in 20 Newsgroups data set, given that some classes are very similarand some other classes are more general and potentially suitable for splitting. As a consequence, the RI values vary largelyfrom one run to another run, making significance testing quite difficult. To gain more information about the quality of theclustering, we consider precision, recall and F-score.Table 5 shows average results (over ten runs) of the clustering algorithm using various relatedness measures. The randomwalk model significantly outperforms the baseline cosine similarity between TF-IDF vectors for document clustering, and italso outperforms LSA in terms of F-score, with most of the relatedness measures. The RI scores do not allow conclusionsas their variation does not allow significance judgments. The comparison with previous work is uneasy, as no systematiccomparison of clustering methods on the 20 Newsgroups datasets was known to us. In a recent paper, Hu et al. [78] foundan F-score of 14.8% for a baseline word vector method, improved to 19.6% by their use of Wikipedia, for agglomerative clus-tering. However, for partitional clustering with K-means, both scores increased more than twice (to 38.2% for the baselineand 41.8% for their best method), a result that could not be confirmed independently.For our approach, the combination of hyperlinks and content links improves the results over using either of them alone.Using the embeddings of the content links reduces the performance in comparison to the computation of the VP valuesover the graph. On the contrary, using embeddings of the hyperlinks improves the results in comparison to using the VPvalues over the hyperlinks graph.The embedding learned on VP similarities over the hyperlinks appears to provide a more general similarity measure,with does not overfit to the Hyperlinks graph of Wikipedia. The high recall it obtains is related to the larger extension ofpaths computed with hyperlinks, which can connect many documents together and attach them to the same class, whilethe high precision obtained using content links is due to their tendency to cluster into smaller neighborhoods.Although the regularization imposed on the embeddings reduced their predictive power for the VP similarities, but itimproves the performance on this task.Computation time using embeddings is (as expected) greatly reduced, as computations are performed in the low-dimensional latent space. Moreover, other unsupervised clustering algorithms can be applied to the documents transformedby the embeddings, e.g. the state-of-the-art clustering algorithm proposed by Bordogna and Pasi [79].M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202195Fig. 10. Values of k-purity (vertical axis) averaged over all documents, for neighborhoods of different sizes k. The horizontal axis indicates the weight w ofvisiting probability vs. cosine lexical similarity in the formula: w × VPComb + (1 − w) × LS.12.2. Comparison of VP and cosine similarityTo find out in which cases the proposed method improves over a simple cosine similarity measure, we considered alinear combination of the cosine similarity and VPComb (VP over content links weighed 0.6 and hyperlinks weighed 0.4),namely w × VPComb + (1 − w) × LS, and varied the weight w from 0 to 1. Considering the k-nearest neighbors of everydocument according to this combined similarity, we define k-purity as the number of documents with the correct labelover the total number of documents k in the computed neighborhood. The variation of k-purity with w, for several samplevalues of k, is shown in Fig. 10.The best purity appears to be obtained for a combination of the two methods, for all values of k that were tested.This shows that VPComb brings valuable additional information about document relatedness that cannot be found in LSonly. Furthermore, when the size of the examined neighborhood k increases (lower curves in Fig. 10), the effect of VPCombbecomes more important, i.e. its weight in the optimal combination increases. For very small neighborhoods, LS is almostsufficient to ensure optimal purity, but for larger ones (k = 10 or 15), VPComb used alone (w = 1) outperforms LS used alone(w = 0). Their optimal combination leads to scores that are higher than those obtained for each of them used separately,and, as noted, the weight of VPComb in the optimal combination increases for larger neighborhoods.These results can be explained as follows. For very small neighborhoods, the cosine lexical similarity score with thenearest 1–5 documents is very high, as they have many words in common, so LS is a good measure of text relatedness.However, when looking at larger neighborhoods, for which relatedness is less based on identical words, then VPComb be-comes more effective, and LS performs poorly. Therefore, we can predict that VPComb will be most relevant when lookingfor larger neighborhoods, or in order to increase recall. VPComb should also be relevant when there is low diversity amongdocument words, for instance when all documents are very short.13. Text classificationWe showed in the previous section that using the VP similarities over Wikipedia hyperlinks and content links graphsimproved text clustering. Although text clustering is an important application, there have been more studies on text clas-sification, i.e. when labeled examples are available for learning. In this section, we investigate this problem by using theembeddings that were learned over VP similarities in Section 9 above. Embeddings can easily be integrated with any dis-tance learning algorithm as prior knowledge or as an initial state. We thus designed a distance learning algorithm forthis purpose, which we compared (using various embeddings) to an SVM text classifier, outperforming its score when fewtraining examples are available.13.1. Distance learning classifierWe built a distance learning classifier which learns, given a training set, a similarity measure so that for each data pointin the training set, its similarity to data points with the same label (or class) is higher than data points with different labels.This classifier is essentially very similar to a large margin nearest neighbor classifier [80], with some changes that make itapplicable to large scale text classification problems with a large number of features (here, words).196M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202Table 6Classification accuracy for different sizes of the training set over the 20 Newsgroups data set. The accuracy is the average of 10 times run on a randomlydivided data set.‘CL’ is the embedding learned over the content links graph and ‘HL’ the embedding learned overhyperlinks, ‘REG’ stands for regularization of the matrix. The numbers in italics are significantly better than the accuracy of the SVM (t-test, p < 0.001).‘DL’ is distance learning classifier,MethodSize of the training setDL + CL embeddingDL + CL emb. + REGDL + HL embeddingDL + HL emb. + REGSVM4021.1322.5421.4022.507.9010032.9334.1434.3135.2915.9320042.7944.3244.0946.1728.4850056.5757.1257.1858.6452.4080062.5463.8263.0964.0861.76100065.9066.4366.3166.8465.67150070.5771.1070.6571.1470.83We define the similarity between two documents i and j represented by TF-IDF vectors xi and x j as xi A A, where Ais a matrix n × m, n being the size of the feature dictionary and m size of the latent space. If C(i) denotes the class (label)of document i, then we define the following loss function L over the training set:(cid:5)(cid:5)xj(cid:3)(cid:3)(cid:3)L =(cid:4)max0, M − xi A A(cid:5)(cid:5)xj+ xi A A(cid:5)(cid:5)xz(cid:5).iC( j)=C(i)C(z)(cid:7)=C(i)M is a margin which is set to 0.2 in our experiments. We performed the optimization of L by stochastic gradient descent.At testing time, we proceeded similarly to the k-nearest neighbors algorithm: we chose the k closest documents from thetraining set according to the learned distance and then returned the class (label) resulting from the majority vote.Our goal here is to show that starting from the prior knowledge obtained from VP similarities over Wikipedia graphs inthe forms of the embeddings can improve the performance of the classification, especially when a small number of trainingsamples is available.13.2. 20 Newsgroups classificationWe applied the above method to classify texts from the 20 Newsgroups data set. We compared the results of the distancelearning algorithm, with various initial points, to a linear SVM method (LIBSVM implementation [81]), which is a state-of-the-art text classifier. The classification accuracy is given in Table 6 for various sizes of the training set. We have trained twomatrices over VP similarities, A and B, because VP similarity is not symmetric. We have experimented with initializationby either A or B, which gives similar results, therefore we show only the results using matrix A. The distance learningclassifier with random initialization of the parameters performed poorly, so it is not reported here.The first important observation is that, when the training set is small, the distance learning classifier initialized withthe embeddings from VP similarities over Wikipedia graphs outperforms the baseline SVM classifier significantly. By addingmore and more labeled data, the importance of prior knowledge appears to decrease, presumably because the distancelearning algorithm can infer reliable decisions based only on the training data. A similar effect was shown for a methodbased on deep learning by Ranzato and Szummer [82], with their method and a TF-IDF/SVM method both reaching 65%accuracy for more than 50 samples per class (corresponding to 1000 total samples) in the training data.The second observation is that the orthonormality regularization that we imposed on the embeddings again improved theperformance. The generalization ability was improved at the price of decreasing slightly the precision in the approximationof the VP values. A third observation is that the accuracy using hyperlinks was slightly higher than using content links.14. Information retrievalIn this section, we apply the proposed distance to information retrieval data from TREC-7 and TREC-8 [83]. The appli-cation of our method to a large scale information retrieval task requires the computation of VP between a query and therepresentation of all the documents in the repository. By using the approximations proposed in Section 7, we can computeT -truncated VP between every query and documents in an acceptable amount of time. Firstly, we map all documents in thedata set to the Wikipedia graph; then, at query time, each query is mapped to the graph; finally, for each query, VPT iscomputed to and from all documents in the data set by using the proposed approximations. In this section, we use α = 0.8,T = 10 and a combination of hyperlinks and content links in VP weighted 0.2/0.8, following observations from previoussections. The time-consuming operation is the first one – viz., mapping a large collection of documents to the Wikipediagraph – but this is done only once, before query time.We used the TREC-7 and TREC-8 Ad-hoc Test Collections.8 The data set includes a repository of 530 000 documents andtwo sets of 50 queries. For each query, the data set also provides relevance judgments for a subset of documents consideredto be related to the query, as they were retrieved by a pool of search methods, a method that is intended to maximize8 These are available at http://trec.nist.gov, and we used more specifically the test queries (topics) numbers 351–400 and 401–450, with the associatedrelevance judgments. The documents are available from the Linguistic Data Consortium.M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202197recall. To study the effect of VP in comparison to lexical similarity and TF-IDF scoring in a single retrieval operation, wecompute for each document and query a linear combination of VPT with weight w and of lexical similarity with weight1 − w. The weight w thus sets the relative importance of conceptual relatedness vs. lexical similarity, and will serve toillustrate their respective contributions. We will refer to this as the Combined similarity. We measure the IR performancethrough the average precision of the first 10 returned documents (10 is the typical size of the first result page of Websearch engines).We computed all the scores for w = 0 to w = 1 with increments of 0.1, and found out that the highest score was reachedfor w = 0.1 for the TREC-7 data set. This optimized value was then tested over the TREC-8 query set (50 topics), leading toan average precision for Combined of 0.515, which improves (+15.4%) over the precision of lexical similarity alone (w = 0),which is 0.446. Egozi et al. [84] reported precision at 10 on TREC-8 data for various bag-of-words information retrievalsystems (Xapian, Okapi, LM-KL-DIR) along with improved results (up to 14%) obtained by using a new retrieval algorithm,which integrates ESA [47] semantic similarity to the previous systems.9 Here, we reported the mean average precision at10, which is the lower bound of precision at 10, but is more informative than it. Direct comparison between our scores andthe scores reported in [84] is not possible, as they used different base retrieval systems.We also examined the precision score for every query separately. In particular, we counted the proportion of queriesfor which the Combined similarity returned more relevant documents than lexical similarity alone. Combined similarityoutperformed the lexical one on 14 queries, while the reverse was true for 5 queries only, and the scores were identical onthe remaining 31 queries. The average score difference between Combined and lexical similarity is 0.018 (maximum is 1).This value is not significantly above 0 at the usual levels (e.g. p < 0.01), but we found using a normal distribution that theprobability of the true difference being zero, given the observations, is only p = 0.11. While this does not ensure that theCombined similarity is “significantly” better than the lexical one, it still provide encouraging evidence for the utility of VPon the TREC-8 test set.The precision scores of both methods vary considerably across queries. We therefore examined separately the “difficult”queries, defined here as the queries on which lexical similarity had a score of 0.3 or less (meaning it returned between 0and 3 relevant documents). There are 21 such queries out of 50 on the TREC-8 test set. Over these queries, the Combinedsimilarity outperforms the lexical one on 7, while the reverse is true for only one, and 13 queries are a tie. Of course, itmight seem unsurprising to see this difference as these queries are “difficult” for the lexical similarity by definition. How-ever, when examining the 20 queries that are “difficult” for the Combined similarity, this measure still outperforms thelexical one on 5, while the reverse is true for only two, and 13 are a tie. These results show that VP provides complemen-tary information that can improve the results of lexical similarity for IR, especially for queries on which lexical similarityperforms less well.15. Learning to rank by using VP similaritiesWe have seen in the previous section that the linear combination of the VP and lexical similarities improved onlyslightly the retrieval results, although VP provides additional information. This motivated us to integrate the VP similarity toa learning to rank system [85], instead of a linear combination with lexical similarity. We have chosen an approach similarto the discriminative projection learning algorithm introduced by Yih et al. [86] which exhibits good performance. We havereimplemented the algorithm for the experiments in this section, and we first describe it briefly, then discuss the results.Assume that for a query q, a document dr is relevant and dnr is not relevant. The algorithm learns a projection A fromTF-IDF vectors of the articles to a latent space such that the similarity between the projections of q and dr is higher than thesimilarity between those of q and dr . Given a training set consisting of (qi, dri , dnri ), the algorithm minimizes the followingloss function L over the training set:(cid:4)maxL =(cid:3)0, M − qi A A+ qi A Add(cid:5).(cid:5)(cid:5)(cid:5)ri(cid:5)nriiWe will show that using the embeddings learned from VP as a starting point for minimizing L can help to improve theranking performance.To build the training and test sets, we used the 50 queries from TREC-8 and considered for each query the documentslabeled as relevant (4728 documents), while unlabeled documents were considered as irrelevant. We divided evenly andrandomly the pairs of queries and relevant documents into a training and a test set. The possible number of triples in thetraining set is very large due to the large number of irrelevant documents. We perform stochastic gradient descent over thetraining set by choosing at each iteration a query and a relevant document, with an irrelevant document chosen randomlyfrom the rest of the documents. We stop when the training error is lower than a fixed threshold. To test the performance,we report average precision at 10. This is computed over the number of relevant documents that are not used for training.Therefore, when using a larger training set, fewer documents are left for testing and the precision at 10 scores necessarilydecrease; however, our interest is in comparing scores for different methods on the same training set.9 Precision at 10 was improved as follows: for Xapian from 0.472 to 0.478 (+1.3%), for Okapi from 0.488 to 0.522 (+7.0%), and for LM-KL-DIR from 0.442to 0.506 (+14.4%).198M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202Table 7Precision at 10 for different ranking algorithms when training size varies. As in Table 6, ‘RL’ is the ranking learner, ‘CL’ is the embedding learned over thecontent links graph and ‘HL’ the embedding learned over hyperlinks, and ‘REG’ stands for regularization of the matrix.MethodSize of the training setRL + CL embeddingsRL + CL emb. + REGRL + HL embeddingsRL + HL emb. + REGRL + random startCosine between TF-IDF vectors50013.6217.4815.7419.4416.321.16100014.4017.8415.9818.6015.1614.74150012.6214.0213.1014.3413.569.62200011.2412.0811.2812.4612.487.9830005.545.565.365.705.342.70Table 8Spearman rank correlation coefficient ρ between automatic and human wordsimilarity when two-stage random walk is used for VP. In parentheses therespective weights of hyperlinks and content links. The best result, ρ = 0.714is found when both hyperlinks and content links are used for the first threestages (with weights 0.7 vs. 0.3), but only content links are used for latterstages.(Hyperlink weight, Content link weight)(1.0, 0.0) for 3 steps, then (0.0, 1.0)(0.0, 1.0) for 3 steps, then (1.0, 0.0)(0.7, 0.3) for 3 steps, then (0.0, 1.0)ρ0.6840.6520.714Table 7 shows the precision at 10 of various algorithms by using different initial states and different number of trainingsamples. The first observation is that only when the training size is very small, 10 documents on average for each query, thecosine similarity outperforms the learning to rank algorithms. Otherwise, the performance of the learning to rank algorithmsis always better than the cosine similarity.The second result is that when the number of training examples is small, the learning algorithm initialized with theregularized embeddings outperforms the random initialization. Gradually, when adding more training examples, it becomesless useful to leverage the prior knowledge, as the learning algorithm can solve the problem better simply by looking attraining samples. Similarly to the classification experiments, the embeddings learned over the hyperlinks are more usefulthan the ones learned over the content links.16. PerspectivesIn addition to the above experiments, we examined two-stage random walks, in which at the first stage, the network isbuilt using one set of weights on the links, and in the second stage using a different set. The hypothesis here is that somelinks might be more useful when explored first, while some others might be more useful when explored later, as discussedby Collins-Thompson and Callan [87].For the word similarity task, we focused on ε-truncated two-stage walks in which one combination of links is used forthe first three steps of the random walker, and a different combination from the fourth to the last steps. The choice of threesteps was empirical, by looking at the average length of single-stage ε-truncated walks. We report the ρ scores of threesignificant combinations of weights for this scenario in Table 8, including the best we found, which reached ρ = 0.714,higher than the one-stage walks (see Section 10). As the optimization took place on the test data, this is not a competitivescore, but intends to show that two-stage walks are a promising approach, in particular exploring the hyperlinks first andthen the content links.A similar analysis to the one shown in Fig. 7 explains why scores improve in the two-stage random walk in Table 8, whichtravels hyperlinks first (thus expanding the possible paths), and then content links (following precise neighborhoods). In thecase of exploring hyperlinks first and content links second, there are longer paths in comparison to using only hyperlinks.In the case of exploring hyperlinks in the second stage, there are many long paths in comparison to other scenarios.The results of VP following two-stage random walks with several meaningful combinations of links are given in Table 9for document similarity data set. The scores on document similarity can be slightly improved to r = 0.680 if hyperlinks aremostly explored in the first steps, and then only content links are followed which is congruent with our finding about twostage random walk for word similarities.17. ConclusionWe have proposed a general framework for text semantic similarity based on knowledge extracted from Wikipedia. Wehave constructed a graph including Wikipedia articles and two different link structures between them. Our hypothesis wasM. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202199Table 9Pearson correlation coefficient r between two-stage VP results and humanjudgments on document similarity. The best result (0.680) is found when forthe first three stages both hyperlinks (0.8) and content links (0.2) are used,but only content links are used for latter stages.(Hyperlink weight, Content link weight)(1.0, 0.0) for 3 steps, then (0.0, 1.0)(0.0, 1.0) for 3 steps, then (1.0, 0.0)(0.8, 0.2) for 3 steps, then (0.0, 1.0)r0.6670.6350.680that using both word co-occurrence information and user-defined hyperlinks between articles could improve the result-ing textual distance for application to a variety of tasks: word similarity; document similarity, clustering, and classification;information retrieval, and learning to rank.We tested our approach on four different benchmark tasks, and found that results were often competitive compared tostate-of-the-art results obtained by task-specific methods. Our experimental results supported the hypothesis that both typesof links are useful, as the improvement of performance was higher when both were used together rather than separately.We have introduced visiting probability (VP) to measure proximity between weighted sets of concepts, and proposedapproximation algorithms to compute it efficiently for large graphs. One advantage of our approach is that the trainingphase (building the network of concepts) is not computationally demanding, as all the computation is done at query time.Therefore, the update of the data set and related network does not imply an additional cost of re-computation. At run time,we have shown how to make computation possible, using a k-nearest-neighbors graph in the random walk framework, fora large resource such as the English Wikipedia. The truncation of VP did not only speed up computation, but also improvedthe accuracy of results by reducing the importance of very popular vertices. To speed up computation at run time evenmore, we showed that it was possible to train embeddings to learn the proposed similarity measures. In addition to thegain in speed, we were able to integrate the proposed distance as prior knowledge, in the form of embeddings, to severallearning algorithms: document clustering, document classification, and learning to rank.AcknowledgementsThis work has been supported by the Swiss National Science Foundation through the National Center of Competence inResearch (NCCR) on Interactive Multimodal Information Management (IM2), http://www.im2.ch. We are very much indebtedto the AI Journal reviewers and special issue editors for their numerous and insightful suggestions on previous versions ofthe paper. We would like to thank Michael D. Lee for access to the textual similarity dataset, as well as Ronan Collobert andJean-Cédric Chappelier for helpful discussions on several issues related to the paper.Appendix A. Estimate of truncated VP to any vertexIn Section 7, we have announced a formal probabilistic bound for the differences between the estimated truncated VP,(cid:6)VPT , and the exact truncated VPT to any vertex. The complete proof of the result, inspired by the proof in [88], isnotedgiven below.Let us note the estimation of a variable X by ˆX , and suppose that concept s j has been visited for the first time at} time steps in the M samples. We define the random variable Xl by αtkl /M, where tkl indicates the time step{tk1 , . . . , tkMat which s j was visited for the first time in lth sampling. If s j was not visited at all, then Xl = 0 by convention. The l randoml αtkl )/Mvariables Xl (k1 (cid:2) l (cid:2) kM ) are independent and bounded by 0 and 1 (0 (cid:2) Xl (cid:2) 1). We haveand E((cid:6)VPT ((cid:4)r, s j)) = VPT ((cid:4)r, s j). So, by applying Hoeffding’s inequality, we have:(cid:6)VPT ((cid:4)r, s j) =l Xl = ((cid:2)(cid:2)(cid:4)(cid:7)(cid:7)(cid:6)VPT ((cid:4)r, s j) − E(cid:4)(cid:6)VPT ((cid:4)r, s j)(cid:5)(cid:7)(cid:7) (cid:3) ε(cid:5)P(cid:2) 2 exp(cid:8)− 2Mε2α2(cid:9).If the probability of error must be at most δ, then setting the right side lower than δ gives the bound for M that is statedin our theorem.As a consequence, we have the following lower bound for M if we look for an ε-approximation for all possible s j withprobability at least 1 − δ. We use the union bound and Hoeffding’s inequality to prove that:(cid:4)P∃ j ∈ {1, . . . , n},(cid:7)(cid:7)(cid:6)VPT (r, s j) − E(cid:4)(cid:6)VPT (r, s j)(cid:5)(cid:7)(cid:7) (cid:3) ε(cid:5)(cid:2) 2n exp(cid:9)(cid:8)− 2Mε2α2which gives the desired lower bound M (cid:3) α2 ln(2n/δ)2ε2.200M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202References[1] A. Budanitsky, G. Hirst, Evaluating WordNet-based measures of semantic distance, Computational Linguistics 32 (2006) 13–47.[2] S. Patwardhan, S. Banerjee, T. Pedersen, Using measures of semantic relatedness for word sense disambiguation, in: Proceedings of CICLing 2003 (4thInternational Conference on Computational Linguistics and Intelligent Text Processing), in: LNCS, vol. 2588, Mexico City, Mexico, pp. 241–257.[3] U.S. Kohomban, W.S. Lee, Learning semantic classes for word sense disambiguation, in: Proceedings of ACL 2005 (43rd Annual Meeting of the Associa-tion for Computational Linguistics), Ann Arbor, MI, pp. 34–41.[4] S.P. Ponzetto, M. Strube, Knowledge derived from Wikipedia for computing semantic relatedness, Journal of Artificial Intelligence Research 30 (2007)181–212.[5] M. Stevenson, M. Greenwood, A semantic approach to IE pattern induction, in: Proceedings of ACL 2005 (43rd Annual Meeting of the Association forComputational Linguistics), Ann Arbor, MI, pp. 379–386.[6] M. Baziz, M. Boughanem, N. Aussenac-Gilles, C. Chrisment, Semantic cores for representing documents in IR, in: Proceedings of SAC 2005 (ACMSymposium on Applied Computing), Santa Fe, NM, pp. 1011–1017.[7] D. Newman, J.H. Lau, K. Grieser, T. Baldwin, Automatic evaluation of topic coherence, in: Proceedings of HLT-NAACL 2010 (Annual Conference of theNorth American Chapter of the Association for Computational Linguistics), Los Angeles, CA, pp. 100–108,.[8] P. Resnik, Using information content to evaluate semantic similarity in a taxonomy, in: Proceedings of IJCAI 1995 (14th International Joint Conferenceon Artificial Intelligence), Montreal, pp. 448–453.[9] C. Fellbaum (Ed.), WordNet: An Electronic Lexical Database, The MIT Press, Cambridge, MA, 1998.[10] D.B. Lenat, CYC: A large-scale investment in knowledge infrastructure, Communications of the ACM 38 (1995) 33–38.[11] M. Jarmasz, Roget’s Thesaurus as a lexical resource for natural language processing, Master’s thesis, University of Ottawa, 2003.[12] J. Morris, G. Hirst, Non-classical lexical semantic relations, in: Proceedings of the HLT-NAACL 2006 Workshop on Computational Lexical Semantics,Boston, MA, pp. 46–51.[13] M. Halliday, R. Hasan, Cohesion in English, Longman, London, 1976.[14] J. Hobbs, Why is discourse coherent? in: F. Neubauer (Ed.), Coherence in Natural Language Texts, Buske, Hamburg, 1983, pp. 29–70.[15] R. Hasan, Coherence and cohesive harmony, in: J. Flood (Ed.), Understanding Reading Comprehension: Cognition, Language and the Structure of Prose,International Reading Association, Newark, DE, 1984, pp. 181–219.[16] M. Halliday, R. Hasan, Language, Context, and Text: Aspects of Language in a Social-Semiotic Perspective, 2nd edition, Oxford University Press, London,1989.[17] S. Mohammad, G. Hirst, Distributional measures as proxies for semantic distance: A survey, available online (consulted on April 15, 2012) athttp://www.cs.toronto.edu/pub/gh/Mohammad+Hirst-2005.pdf, 2005.[18] J.E. Weeds, Measures and applications of lexical distributional similarity, PhD thesis, University of Sussex, 2003.[19] S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, R. Harshman, Indexing by latent semantic analysis, Journal of the American Society for Infor-mation Science 41 (1990) 391–407.[20] S. Padó, M. Lapata, Dependency-based construction of semantic space models, Computational Linguistics 33 (2007) 161–199.[21] T. Hofmann, Probabilistic latent semantic indexing, in: Proceedings of SIGIR 1999 (22nd ACM SIGIR Conference on Research and Development inInformation Retrieval), Berkeley, CA, pp. 50–57.[22] D.M. Blei, A.Y. Ng, M.I. Jordan, Latent Dirichlet allocation, Journal of Machine Learning Research 3 (2003) 993–1022.[23] M. Jarmasz, S. Szpakowicz, Roget’s thesaurus and semantic similarity, in: Proceedings of RANLP 2003 (Conference on Recent Advances in NaturalLanguage Processing), Borovetz, Bulgaria, pp. 111–120.[24] R. Rada, H. Mili, E. Bicknell, M. Blettner, Development and application of a metric to semantic nets, IEEE Transactions on Systems, Man and Cybernet-ics 19 (1989) 17–30.[25] C. Leacock, M. Chodorow, Combining local context and WordNet similarity for word sense identification, in: C. Fellbaum (Ed.), WordNet: An ElectronicLexical Database, The MIT Press, Cambridge, MA, 1998, pp. 265–283.[26] P. Resnik, Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language, Journalof Artificial Intelligence Research 11 (1999) 95–130.[27] D. Lin, An information-theoretic definition of similarity, in: Proceedings of ICML 1998 (15th International Conference on Machine Learning), Madison,WI, pp. 296–304.[28] J.-C. Chappelier, Topic-based generative models for text information access, in: E. Gaussier, F. Yvon (Eds.), Textual Information Access: Statistical Models,ISTE/Wiley, London, UK, 2012, pp. 129–178.[29] J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, D.M. Blei, Reading tea leaves: How humans interpret topic models, in: Y. Bengio, D. Schuurmans, J. Lafferty,C.K.I. Williams, A. Culotta (Eds.), Advances in Neural Information Processing Systems, vol. 22, The MIT Press, Cambridge, MA, 2009, pp. 288–296.[30] R. Mihalcea, C. Corley, C. Strapparava, Corpus-based and knowledge-based measures of text semantic similarity, in: Proceedings of AAAI 2006 (21stNational Conference on Artificial Intelligence), Boston, MA, pp. 775–782.[31] T. Hughes, D. Ramage, Lexical semantic relatedness with random graph walks, in: Proceedings of EMNLP-CoNLL 2007 (Conference on Empirical Methodsin Natural Language Processing and Conference on Computational Natural Language Learning), Prague, Czech Republic, pp. 581–589.[32] T.H. Haveliwala, Topic-sensitive PageRank: A context-sensitive ranking algorithm for web search, IEEE Transactions on Knowledge and Data Engineer-ing 15 (2003) 784–796.[33] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, S. Hellmann, DBpedia – A crystallization point for the web of data, Journal of WebSemantics 7 (2009) 154–165.[34] R. Navigli, S.P. Ponzetto, BabelNet: Building a very large multilingual semantic network, in: Proceedings of ACL 2010 (48th Annual Meeting of theAssociation for Computational Linguistics), Uppsala, Sweden, pp. 216–225.[35] V. Nastase, M. Strube, B. Boerschinger, C. Zirn, A. Elghafari, WikiNet: A very large scale multi-lingual concept network, in: Proceedings of LREC 2010(7th International Conference on Language Resources and Evaluation), Valletta, Malta.[36] F. Suchanek, G. Kasneci, G. Weikum, Yago: A large ontology from Wikipedia and WordNet, Journal of Web Semantics 6 (2008) 203–217.[37] M. Strube, S.P. Ponzetto, WikiRelate! Computing semantic relatedness using Wikipedia, in: Proceedings of AAAI 2006 (21st National Conference onArtificial Intelligence), Boston, MA, pp. 1419–1424.[38] S.P. Ponzetto, M. Strube, Taxonomy induction based on a collaboratively built knowledge repository, Artificial Intelligence 175 (2011) 1737–1756.[39] D. Milne, I.H. Witten, An effective, low-cost measure of semantic relatedness obtained from Wikipedia links, in: Proceedings of WIKIAI 2008 (1st AAAIWorkshop on Wikipedia and Artificial Intelligence), Chicago, IL, pp. 25–30.[40] Z.S. Syed, T. Finin, A. Joshi, Wikipedia as an ontology for describing documents, in: Proceedings of the Second International Conference on Weblogsand Social, Media, Seattle, WA, pp. 136–144.[41] D. Milne, I.H. Witten, Learning to link with Wikipedia, in: Proceedings of CIKM 2008 (17th ACM Conference on Information and Knowledge Manage-ment), Napa Valley, CA, pp. 509–518.[42] K. Coursey, R. Mihalcea, W. Moen, Using encyclopedic knowledge for automatic topic identification, in: Proceedings of CoNLL 2009 (13th Conferenceon Computational Natural Language Learning), Boulder, CO, pp. 210–218.M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202201[43] R. Mihalcea, A. Csomai, Wikify!: Linking documents to encyclopedic knowledge, in: Proceedings of ACM CIKM 2007 (16th ACM Conference on Infor-mation and Knowledge Management), Lisbon, Portugal, pp. 233–242.[44] D. Ramage, A.N. Rafferty, C.D. Manning, Random walks for text semantic similarity, in: Proceedings of TextGraphs-4 (4th Workshop on Graph-BasedMethods for Natural Language Processing), Singapore, pp. 23–31.[45] M. Yazdani, A. Popescu-Belis, A random walk framework to compute textual semantic similarity: a unified model for three benchmark tasks, in:Proceedings of IEEE ICSC 2010 (4th IEEE International Conference on Semantic Computing), Pittsburgh, PA, pp. 424–429.[46] E. Gabrilovich, S. Markovitch, Computing semantic relatedness using Wikipedia-based explicit semantic analysis, in: Proceedings of IJCAI 2007 (20thInternational Joint Conference on Artificial Intelligence), Hyderabad, India, pp. 6–12.[47] E. Gabrilovich, S. Markovitch, Wikipedia-based semantic interpretation for natural language processing, Journal of Artificial Intelligence Research 34(2009) 443–498.[48] S. Hassan, R. Mihalcea, Cross-lingual semantic relatedness using encyclopedic knowledge, in: Proceedings of EMNLP 2009 (Conference on EmpiricalMethods in Natural Language Processing), Singapore, pp. 1192–1201.[49] T. Zesch, C. Müller, I. Gurevych, Using Wiktionary for computing semantic relatedness, Proceedings of AAAI 2008 (23rd National Conference on ArtificialIntelligence), vol. 2, Chicago, IL, pp. 861–866.[50] P. Cimiano, A. Schultz, S. Sizov, P. Sorg, S. Staab, Explicit vs. latent concept models for cross-language information retrieval, in: Proceedings of IJCAI2009 (21st International Joint Conference on Artificial Intelligence), Pasadena, CA, pp. 1513–1518.[51] E. Yeh, D. Ramage, C.D. Manning, E. Agirre, A. Soroa, WikiWalk: random walks on Wikipedia for semantic relatedness, in: Proceedings of TextGraphs-4(4th Workshop on Graph-Based Methods for Natural Language Processing), Singapore, pp. 41–49.[52] E. Agirre, A. Soroa, Personalizing PageRank for word sense disambiguation, in: Proceedings of EACL 2009 (12th Conference of the European Chapter ofthe Association for Computational Linguistics), Athens, Greece, pp. 33–41.[53] P. Turney, Mining the web for synonyms: PMI-IR versus LSA on TOEFL, in: Proceedings of ECML 2001 (12th European Conference on Machine Learning),Freiburg, Germany, pp. 491–502.[54] P. Berkhin, A survey on PageRank computing, Internet Mathematics 2 (2005) 73–120.[55] R. Navigli, M. Lapata, An experimental study of graph connectivity for unsupervised word sense disambiguation, IEEE Transactions on Pattern Analysisand Machine Intelligence 32 (2010) 678–692.[56] R. Ion, D.¸Stef˘anescu, Unsupervised word sense disambiguation with lexical chains and graph-based context formalization, in: Proceedings of LTC 2009(4th Language and Technology Conference), in: LNAI, vol. 6562, Poznan, Poland, pp. 435–443.[57] R. Navigli, A structural approach to the automatic adjudication of word sense disagreements, Natural Language Engineering 14 (2008) 547–573.[58] M. Saerens, F. Fouss, L. Yen, P. Dupont, The principal components analysis of a graph, and its relationships to spectral clustering, in: Proceedings ofECML 2004 (15th European Conference on Machine Learning), Pisa, Italy, pp. 371–383.[59] M. Brand, A random walks perspective on maximizing satisfaction and profit, in: Proceedings of the 2005 SIAM International Conference on DataMining, Newport Beach, CA, pp. 12–19.[60] D. Liben-Nowell, J. Kleinberg, The link prediction problem for social networks, in: Proceedings of CIKM 2003 (12th ACM International Conference onInformation and Knowledge Management), New Orleans, LA, pp. 556–559.[61] Q. Mei, D. Zhou, K. Church, Query suggestion using hitting time, in: Proceeding of CIKM 2008 (17th ACM International Conference on Information andKnowledge Management), Napa Valley, CA, pp. 469–478.[62] P. Sarkar, A. Moore, A tractable approach to finding closest truncated-commute-time neighbors in large graphs, in: Proceedings of UAI 2007 (23rdConference on Uncertainty in Artificial Intelligence), Vancouver, BC, pp. 335–343.[63] Metaweb Technologies, Freebase Wikipedia Extraction (WEX), http://download.freebase.com/wex/, 2010.[64] V. Grishchenko, Wikipedia as an ant-hill, web page consulted on April 15, 2012, http://no-gritzko-here.livejournal.com/22900.html, 2008.[65] D.J. Watts, S.H. Strogatz, Collective dynamics of ‘small-world’ networks, Nature 393 (1998) 440–442.[66] T. Opsahl, P. Panzarasa, Clustering in weighted networks, Social Networks 31 (2009) 155–163.[67] D. Bollegala, Y. Matsuo, M. Ishizuka, Measuring semantic similarity between words using web search engines, in: Proceedings of WWW 2007 (16thInternational Conference on World Wide Web), Banff, Alberta, pp. 757–766.[68] E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pa ¸sca, A. Soroa, A study on similarity and relatedness using distributional and WordNet-based ap-proaches, in: Proceedings of HLT-NAACL 2009 (Human Language Technologies: The 2009 Annual Conference of the North American Chapter of theAssociation for Computational Linguistics), Boulder, CO, pp. 19–27.[69] H. Rubenstein, J.B. Goodenough, Contextual correlates of synonymy, Communications of the ACM 8 (1965) 627–633.[70] G.A. Miller, W.G. Charles, Contextual correlates of semantic similarity, Language and Cognitive Processes 6 (1991) 1–28.[71] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, E. Ruppin, Placing search in context: The concept revisited, ACM Transactionson Information Systems (TOIS) 20 (2002) 116–131.[72] Z. Wu, M. Palmer, Verb semantics and lexical selection, in: Proceedings of ACL 1994 (32nd Annual Meeting of Association for Computational Linguis-tics), Las Cruces, NM, pp. 133–138.[73] S. Patwardhan, T. Pedersen, Using WordNet-based context vectors to estimate the semantic relatedness of concepts, in: Proceedings of the EACL 2006Workshop on Making Sense of Sense, Trento, Italy, pp. 1–8.[74] M.A. Alvarez, S.J. Lim, A graph modeling of semantic similarity between words, in: Proceedings of ICSC 2007 (1st International Conference on SemanticComputing), Irvine, CA, pp. 355–362.[75] M.D. Lee, B. Pincombe, M. Welsh, An empirical evaluation of models of text document similarity, in: Proceedings of CogSci 2005 (27th Annual Confer-ence of the Cognitive Science Society), Stresa, Italy, pp. 1254–1259.[76] S. Hassan, R. Mihalcea, Semantic relatedness using salient semantic analysis, in: Proceedings of AAAI 2011 (25th AAAI Conference on Artificial Intelli-gence), San Francisco, CA, pp. 884–889.[77] T.M. Mitchell, Machine Learning, McGraw–Hill, New York, NY, 1997.[78] X. Hu, X. Zhang, C. Lu, E. Park, X. Zhou, Exploiting Wikipedia as external knowledge for document clustering, in: Proceedings of KDD 2009 (15th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining), Paris, France, pp. 389–396.[79] G. Bordogna, G. Pasi, Hierarchical-hyperspherical divisive fuzzy C-means (H2D-FCM) clustering for information retrieval, in: Proceedings of WI-IAT 2009(IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology), vol. 1, Milan, Italy, pp. 614–621.[80] K. Weinberger, J. Blitzer, L. Saul, Distance metric learning for large margin nearest neighbor classification, in: Y. Weiss, B. Schölkopf, J. Platt (Eds.),Advances in Neural Information Processing Systems, vol. 18, The MIT Press, Cambridge, MA, 2006, pp. 1473–1480.[81] C.-C. Chang, C.-J. Lin, LIBSVM: A library for support vector machines, ACM Transactions on Intelligent Systems and Technology 2 (2011), article n. 27.[82] M. Ranzato, M. Szummer, Semi-supervised learning of compact document representations with deep networks, in: Proceedings of ICML 2008 (25thInternational Conference on Machine Learning), Helsinki, Finland, pp. 792–799.[83] E.M. Voorhees, D. Harman, Overview of the Eighth Text REtrieval Conference (TREC-8), in: Proceedings of TREC-8, Gaithersburg, MD, pp. 1–24.[84] O. Egozi, S. Markovitch, E. Gabrilovich, Concept-based information retrieval using explicit semantic analysis, ACM Transactions on Information Systems(TOIS) 29 (2011), article n. 8.202M. Yazdani, A. Popescu-Belis / Artificial Intelligence 194 (2013) 176–202[85] H. Li, Learning to Rank for Information Retrieval and Natural Language Processing, Morgan and Claypool, San Rafael, CA, 2011.[86] W.-T. Yih, K. Toutanova, J.C. Platt, C. Meek, Learning discriminative projections for text similarity measures, in: Proceedings of CoNLL 2011 (15thConference on Computational Natural Language Learning), Portland, OR, pp. 247–256.[87] K. Collins-Thompson, J. Callan, Query expansion using random walk models, in: Proceedings of CIKM 2005 (14th ACM Conference on Information andKnowledge Management), Bremen, Germany, pp. 704–711.[88] P. Sarkar, A.W. Moore, A. Prakash, Fast incremental proximity search in large graphs, in: Proceedings of ICML 2008 (25th International Conference onMachine Learning), Helsinki, Finland, pp. 896–903.