Artificial Intelligence 173 (2009) 1133–1149Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintThe learnability of voting rules ✩Ariel D. Procaccia a,∗,1, Aviv Zohar b,a, Yoni Peleg b, Jeffrey S. Rosenschein ba Microsoft Israel R&D Center, 13 Shenkar Street, Herzeliya 46725, Israelb School of Computer Science and Engineering, The Hebrew University of Jerusalem, Jerusalem 91904, Israela r t i c l ei n f oa b s t r a c tArticle history:Received 16 May 2008Received in revised form 25 March 2009Accepted 27 March 2009Available online 9 April 2009Keywords:Computational social choiceComputational learning theoryMultiagent systemsScoring rules and voting trees are two broad and concisely-representable classes ofvoting rules; scoring rules award points to alternatives according to their position inthe preferences of the voters, while voting trees are iterative procedures that selectan alternative based on pairwise comparisons. In this paper, we investigate the PAC-learnability of these classes of rules. We demonstrate that the class of scoring rules, asfunctions from preferences into alternatives, is efficiently learnable in the PAC model. Withrespect to voting trees, while in general a learning algorithm would require an exponentialnumber of samples, we show that if the number of leaves is polynomial in the size of theset of alternatives, then a polynomial training set suffices. We apply these results in anemerging theory: automated design of voting rules by learning.© 2009 Elsevier B.V. All rights reserved.1. IntroductionVoting is a well-studied method of preference aggregation, in terms of its theoretical properties, as well as its computa-tional aspects [6,21]; various practical, implemented applications that use voting exist [9,12,13].In an election, n voters express their preferences over a set of m alternatives. To be precise, each voter is assumed toreveal linear preferences—a ranking of the alternatives. The outcome of the election is determined according to a voting rule.In this paper we will consider two families of voting rules: scoring rules and voting trees.Scoring rules. The predominant—ubiquitous, even—voting rule in real-life elections is the Plurality rule. Under Plurality, eachvoter awards one point to the alternative it ranks first, i.e., its most preferred alternative. The alternative that accumulatedthe most points, summed over all voters, wins the election. Another example of a voting rule is the Veto rule: each voter“vetoes” a single alternative; the alternative that was vetoed by the fewest voters wins the election. Yet a third exampleis the Borda rule: every voter awards m − 1 points to its top-ranked alternative, m − 2 points to its second choice, and soforth—the least preferred alternative is not awarded any points. Once again, the alternative with the most points is elected.The above-mentioned three voting rules all belong to an important family of voting rules known as scoring rules. A scoringrule can be expressed by a vector of parameters (cid:3)α = (cid:4)α1, α2, . . . , αm(cid:5), where each αlis a real number and α1 (cid:2) α2 (cid:2)· · · (cid:2) αm. Each voter awards α1 points to its most-preferred alternative, α2 to its second-most-preferred alternative, etc.Predictably, the alternative with the most points wins. Under this unified framework, we can express our three rules as:✩This paper subsumes two earlier conference papers [A.D. Procaccia, A. Zohar, Y. Peleg, J.S. Rosenschein, Learning voting trees, in: Proceedings of the 22ndAAAI Conference on AI (AAAI), 2007, pp. 110–115; A.D. Procaccia, A. Zohar, J.S. Rosenschein, Automated design of scoring rules by learning from examples,in: Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2008, pp. 951–958].* Corresponding author.E-mail addresses: arielpro@gmail.com (A.D. Procaccia), avivz@cs.huji.ac.il (A. Zohar), jonip@cs.huji.ac.il (Y. Peleg), jeff@cs.huji.ac.il (J.S. Rosenschein).1 The author was supported in this work by the Adams Fellowship Program of the Israel Academy of Sciences and Humanities.0004-3702/$ – see front matter © 2009 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2009.03.0031134A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–1149Fig. 1. A binary voting tree.• Plurality: (cid:3)α = (cid:4)1, 0, . . . , 0(cid:5).• Borda: (cid:3)α = (cid:4)m − 1, m − 2, . . . , 0(cid:5).• Veto: (cid:3)α = (cid:4)1, . . . , 1, 0(cid:5).A good indication of the importance of scoring rules is given by the fact that they are exactly the family of voting rulesthat are anonymous (indifferent to the identities of the voters), neutral (indifferent to the identities of the alternatives), andconsistent (an alternative that is elected by two separate sets of voters is elected overall) [26].Voting trees. Some voting rules rely on the concept of pairwise elections: alternative a beats alternative b in the pairwiseelection between a and b if the majority2 of voters prefers a to b. Ideally, we would like to select an alternative that beatsevery other alternative in a pairwise election, but such an alternative (called a Condorcet winner) does not always exist.However, there are other prominent voting rules that rely on the concept of pairwise elections, which select an alterna-tive in a sense “close” to the Condorcet winner. In the Copeland rule, for example, the score of an alternative is the numberof alternatives it beats in a pairwise election; the alternative with the highest score wins. In the Maximin rule, the score ofan alternative is the number of votes it gets in its worst pairwise election (the least number of voters that prefer it to somealternative), and, predictably, the winner is the alternative that scores highest.When discussing such voting rules, it is possible to consider a more abstract setting. A tournament T over A is a com-plete binary asymmetric relation over A (that is, for any two alternatives a and b, aT b or bT a, but not both). Clearly, theaforementioned majority relation induces a tournament (a beats b in the pairwise election iff aT b). More generally, thisrelation can reflect a reality that goes beyond a strict voting scenario. For example, the tournament can represent a basket-ball league, where aT b if team a is expected to beat team b in a game. We denote the set of all tournaments over A byT = T ( A).So, for the moment let us look at (pairwise) voting rules as simply functions f : T → A. The most prominent class ofsuch functions is the class of binary voting trees. Each function in the class is represented by a binary tree, with the leaveslabeled by alternatives. At each node, the alternatives at the two children compete; the winner ascends to the node (soif a and b compete and aT b, a ascends). The winner-determination procedure starts at the leaves and proceeds upwardstowards the root; the alternative that survives to the root is the winner of the election.For example, assume that the alternatives are a, b, and c, and bT a, cT b, and aT c. In the tree given in Fig. 1, b beats aand is subsequently beaten by c in the right subtree, while a beats c in the left subtree. a and c ultimately compete at theroot, making a the winner of the election.Notice that we allow an alternative to appear in multiple leaves; further, some alternatives may not appear at all (so, forexample, a singleton tree is a constant function).Motivation and setting. We consider the following setting: an entity, which we refer to as the designer, has in mind a votingrule (which may reflect the ethics of a society). We assume that the designer is able, for each constellation of voters’preferences with which it is presented, to designate a winning alternative (perhaps with considerable computational effort).In particular, one can think of the designer’s representation of the voting rule as a black box that matches preference profilesto winning alternatives. This setting is relevant, for example, when a designer has in mind different properties it wants itsrule to satisfy; in this case, given a preference profile, the designer can specify a winning alternative that is compatible withthese properties.We would like to find a concise and easily understandable representation of the voting rule the designer has in mind. Werefer to this process as automated design of voting rules: given a specification of properties, or, indeed, of societal ethics, findan elegant voting rule that implements the specification. In this paper, we do so by learning from examples. The designer ispresented with different preference profiles, drawn according to a fixed distribution. For each profile, the designer answerswith the winning alternative. The number of queries presented to the designer must intuitively be as small as possible: thecomputations the designer has to carry out in order to handle each query might be complex, and communication might becostly.2 We will assume, for simplicity, an odd number of voters.A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–11491135Now, we further assume that the “target” voting rule the designer has in mind, i.e., the one given as a black box, isknown to belong to some family R of voting rules. We would like to produce a voting rule from R that is as “close” aspossible to the target rule.By “close,” we mean close with respect to the fixed distribution over preference profiles. More precisely, we would liketo construct an algorithm that receives pairs of the form (preferences, winner) drawn according to a fixed distribution Dover preferences, and outputs a rule from R, such that the probability according to D that our rule and the target rule agreeis as high as possible. We wish, in fact, to learn rules from R in the framework of the formal PAC (Probably ApproximatelyCorrect) learning model; a concise introduction to this model is given in Section 2.In this paper, we look at two options for the choice of R: the family of scoring rules, and the family of voting trees.These are natural choices, since both are broad classes of rules, and both have concise representations. Choosing R as above,the designer could in principle translate the possibly cumbersome, unknown representation of a voting rule into a succinctone that can be easily understood and computed.Further justification for our agenda is given by noting that it might be difficult to compute a voting rule on all instances,but it might be sufficient to simply calculate the election’s result on typical instances. The distribution D can be chosen, bythe designer, to concentrate on such instances.Our results. The dimension of a function class is a combinatorial measure of the richness of the class; this dimension isclosely related to the number of examples needed to learn the class. We give almost tight bounds on the dimension of theclass of scoring rules, providing an upper bound of m, and a lower bound of m − 3, where m is the number of alternatives inan election. In addition, we show that, given a set of examples, one can efficiently construct a scoring rule that is consistentwith the examples, if one exists. Combined, these results imply the following theorem:Theorem 3.1. The class of scoring rules over n voters and m alternatives is efficiently learnable for all values of n and m.In other words, given a combination of properties that is satisfied by some scoring rule, it is possible to construct a“close” scoring rule in polynomial time.The situation with respect to the learnability of voting trees is two-fold: in general, due to the expressiveness andpossible complexity of binary trees, the number of examples required is exponential in m. However, if we assume thatthe number of leaves is polynomial in m, then the required number of examples is also polynomial in m. In addition, weinvestigate the computational complexity of problems associated with the learning process.It is also worthwhile to ask whether it is possible to extend this approach. Specifically, we pose the question: given aclass of voting rules R, if the designer has some general voting rule in mind (rather than a voting rule that is known tobelong to R), is it possible to learn a “close” rule from R? We prove, for a natural definition of approximation:Theorem 5.3. Let Rnleast a (1 − δ)-fraction of the voting rules f : Ln → {x1, . . . , xm} satisfy the following property: no voting rule in Rnapproximation of f .m be a family of voting rules of size exponential in n and m. Let (cid:3), δ > 0. For large enough values of n and m, atm is a (1/2 + (cid:3))-In particular, we show that the theorem holds for scoring rules and small voting trees, thus answering the questionposed above in the negative with respect to these classes.Related work. Currently there exists a small body of work on learning in economic settings. Kalai [16] explores the learn-ability (in the PAC model) of rationalizable choice functions. These are functions which, given a set of alternatives, choosethe element that is maximal with respect to some linear order. Similarly, PAC learning has very recently been applied tocomputing utility functions that are rationalizations of given sequences of prices and demands [2].Another prominent example is the paper by Lahaie and Parkes [17], which considers preference elicitation in combina-torial auctions. The authors show that preference elicitation algorithms can be constructed on the basis of existing learningalgorithms. The learning model used, exact learning, differs from ours (PAC learning).Conitzer and Sandholm [3] have studied automated mechanism design, in the more restricted setting where agentshave numerical valuations for different alternatives. They propose automatically designing a truthful mechanism for everypreference aggregation setting. However, they find that, under two solution concepts, even determining whether there existsa deterministic mechanism that guarantees a certain social welfare is an N P -complete problem. The authors also showthat the problem is tractable when designing a randomized mechanism. In more recent work [5], Conitzer and Sandholmput forward an efficient algorithm for designing deterministic mechanisms, which works only in very limited scenarios. Inshort, our setting, goals, and methods are completely different—in the general voting context, even framing computationalcomplexity questions is problematic, since the goal cannot be specified with reference to expected social welfare.Some authors have studied the computational properties of scoring rules. For instance, Conitzer et al. [6] have in-vestigated the computational complexity of the coalitional manipulation problem in several scoring rules; Procaccia andRosenschein [21] generalized their results, and finally, Hemaspaandra and Hemaspaandra [14] gave a full characterization.Many other papers deal with the complexity of manipulation and control in elections, and, inter alia, discuss scoring rules(see, e.g., [1,4,8,15,22,27]).1136A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–1149The computational properties of voting trees have also been investigated. One prominent example is the work of Lang etal. [18], which studied the computational complexity of selecting different types of winners in elections governed by votingtrees. Fischer et al. [10] investigated the power of voting trees in approximating the maximum degree in a tournament.Structure of the paper.In Section 2 we give an introduction to the PAC model. In Section 3, we present our results withrespect to scoring rules. In Section 4, we investigate voting trees. In Section 5, we discuss a possible extension of ourapproach. We conclude in Section 6.2. PreliminariesIn this section we give a very short introduction to the PAC model and the generalized dimension of a function class.A more comprehensive (and slightly more formal) overview of the model, and results concerning the dimension, can befound in [20].In the PAC model, the learner is attempting to learn a function f : Z → Y , which belongs to a class F of functionsfrom Z to Y . The learner is given a training set—a set {z1, z2, . . . , zt} of points in Z , which are sampled i.i.d. (independentlyand identically distributed) according to a distribution D over the sample space Z . D is unknown, but is fixed throughout∗(z) exists, and the giventhe learning process. In this paper, we assume the “realizable” case, where a target function ftraining examples are in fact labeled by the target function: {(zk, f(cid:3)(z)k=1. The error of a function f ∈ F is defined as∗(zk))}tf (z) (cid:9)= f(1)(cid:2)∗.err( f ) = Prz∼D(cid:3) > 0 is a parameter given to the learner that defines the accuracy of the learning process: we would like to achieve∗) = 0. The learner is also given a confidence parameter δ > 0, that provides an upper bound onerr(h) (cid:3) (cid:3). Notice that err( fthe probability that err(h) > (cid:3):(cid:2)Prerr(h) > (cid:3)(cid:3)< δ.(2)We now formalize the discussion above:Definition 2.1. (See [20].)1. A learning algorithm L is a function from the set of all training examples to F with the following property: given(cid:3), δ ∈ (0, 1) there exists an integer s((cid:3), δ)—the sample complexity—such that for any distribution D on X , if Z is asample of size at least s where the samples are drawn i.i.d. according to D, then with probability at least 1 − δ it holdsthat err(L(Z )) (cid:3) (cid:3).2. L is an efficient learning algorithm if it always runs in time polynomial in 1/(cid:3), 1/δ, and the size of the representationsof the target function, of elements in X , and of elements in Y .3. A function class F is (efficiently) PAC-learnable if there is an (efficient) learning algorithm for F .The sample complexity of a learning algorithm for F is closely related to a measure of the class’s combinatorial richnessknown as the generalized dimension.Definition 2.2. (See [20].) Let F be a class of functions from Z to Y . We say F shatters S ⊆ Z if there exist two functionsf , g ∈ F such that1. For all z ∈ S, f (z) (cid:9)= g(z).2. For all S1 ⊆ S, there exists h ∈ F such that for all z ∈ S1, h(z) = f (z), and for all z ∈ S \ S1, h(z) = g(z).Definition 2.3. (See [20].) Let F be a class of functions from a set Z to a set Y . The generalized dimension of F , denoted byD G (F ), is the greatest integer d such that there exists a set of cardinality d that is shattered by F .Lemma 2.4. (See [20, Lemma 5.1].) Let Z and Y be two finite sets and let F be a set of total functions from Z to Y . If d = D G (F ), then2d (cid:3) |F |.A function’s generalized dimension provides both upper and lower bounds on the sample complexity of algorithms.Theorem 2.5. (See [20, Theorem 5.1].) Let F be a class of functions from Z to Y of generalized dimension d. Let L be an algorithm∗ ∈ F , sampled i.i.d. according to some fixed but unknown∗(zk))}k of some fsuch that, when given a set of t labeled examples {(zk, fdistribution over the instance space X , produces an output f ∈ F that is consistent with the training set. Then L is an ((cid:3), δ)-learningalgorithm for F provided that the sample size obeys:(cid:4)s (cid:2) 1(cid:3)(σ1 + σ2 + 3)d ln 2 + ln(cid:4)(cid:5)(cid:5)1δwhere σ1 and σ2 are the sizes of the representation of elements in Z and Y , respectively.(3)A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–11491137Theorem 2.6. (See [20, Theorem 5.2].) Let F be a function class of generalized dimension d (cid:2) 8. Then any ((cid:3), δ)-learning algorithmfor F , where (cid:3) (cid:3) 1/8 and δ < 1/4, must use sample size s (cid:2) d16(cid:3) .3. Learnability of scoring rulesBefore diving in, we introduce some notation. Let N = {1, 2, . . . , n} be the set of voters, and let A = {x1, x2, . . . , xm} bethe set of alternatives; we also denote alternatives by {a, b, c, . . .}. Let L = L( A) be the set of linear preferences3 over A;each voter has preferences (cid:11)i ∈ L. We denote the preference profile, consisting of the voters’ preferences, by (cid:11)N = (cid:4)(cid:11)1, (cid:11)2, . . . , (cid:11)n(cid:5). A voting rule is a function f : LN → A, that maps preference profiles to winning alternatives.Let (cid:3)α be a vector of m nonnegative real numbers such that αl (cid:2) αl+1 for all l = 1, . . . , m − 1. Let f (cid:3)α : LN → C be thescoring rule defined by the vector (cid:3)α, i.e., each voter awards αl points to the alternative it ranks in the lth place, and therule elects the alternative with the most points.Since several alternatives may have maximal scores in an election, we must adopt some method of tie-breaking. Ourmethod works as follows. Ties are broken in favor of the alternative that was ranked first by more voters; if several alterna-tives have maximal scores and were ranked first by the same number of voters, the tie is broken in favor of the alternativethat was ranked second by more voters; and so on.4Let Snfunction f (cid:3)α∗ ∈ Sndistribution over LN ; let x jkk , we denote by π kx j in place l. Notice that alternative x j ’s score under the preference profile (cid:11)Nm be the class of scoring rules with n voters and m alternatives. Our goal is to learn, in the PAC model, some targetk is drawn from a fixedj,l the number of voters that ranked alternativek ism. To this end, the learner receives a training set {((cid:11)Nk ). For the profile (cid:11)Nk )}k, where each (cid:11)Nk , f (cid:3)α∗ ((cid:11)N= f (cid:3)α∗ ((cid:11)Nl π kj,lαl.(cid:6)3.1. Efficient learnability of SnmOur main goal in this section is to prove the following theorem.Theorem 3.1. For all n, m ∈ N, the class Snm is efficiently PAC-learnable.By Theorem 2.5, in order to prove Theorem 3.1 it is sufficient to validate the following two claims: 1) that there existsan algorithm which, for any training set, runs in time polynomial in n, m, and the size of the training set, and outputs ascoring rule which is consistent with the training set (assuming one exists); and 2) that the generalized dimension of theclass Snm is polynomial in n and m.Remark 3.2. It is possible to prove Theorem 3.1 by using a transformation between scoring rules and sets of linear thresholdfunctions. Indeed, it is well known that the VC dimension (the restriction of the generalized dimension to boolean-valuedfunctions) of linear threshold functions over Rd is d + 1. In principle, it is possible to transform a scoring rule into a linearthreshold function that receives (generally speaking) vectors of rankings of alternatives as input. Given a training set ofprofiles, we could transform it into a training set of rankings and use a learning algorithm.However, we are interested in producing an accurate scoring rule according to a distribution D on preference profiles,which represents typical profiles. It is possible to consider a many-to-one mapping between distributions over profiles anddistributions over the above-mentioned vectors of rankings. Unfortunately, when this procedure is used, it is nontrivial toguarantee that the learned voting rule succeeds according to the original distribution D. Moreover, this procedure seemsto require an increase in sample complexity compared to the analysis given below. Therefore, we proceed with the more“direct” agenda outlined above and detailed below.It is rather straightforward to construct an efficient algorithm that outputs consistent scoring rules. Given a training set,we must choose the parameters of our scoring rule in a way that, for any example, the score of the designated winner is atleast as large as the scores of other alternatives. Moreover, if ties between the winner and a loser would be broken in favorof the loser, then the winner’s score must be strictly higher than the loser’s. Our algorithm, given as Algorithm 1, simplyformulates all the constraints as linear inequalities, and solves the resulting linear program. The first part of the algorithmis meant to handle tie-breaking. Recall that x jk= f (cid:3)α∗ ((cid:11)Nk ).A linear program can be solved in time that is polynomial in the number of variables and inequalities [24]; it followsthat Algorithm 1’s running time is polynomial in n, m, and the size of the training set.Remark 3.3. Notice that any vector (cid:3)α with a “standard” representation, that is with rational coordinates such that bothnumerator and denominator are integers represented by a polynomial number of bits, can be scaled to an equivalent vector3 A binary relation which is antisymmetric, transitive, and total.4 In case several alternatives have maximal scores and identical rankings everywhere, break ties arbitrarily—say, in favor of the alternative with thesmallest index.1138A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–1149for k ← 1 . . . s doXk ← ∅for all x j (cid:9)= x jk do− (cid:3)π k(cid:3)π (cid:7) ← (cid:3)π kjjkl0 ← min{l: π (cid:7)lif π (cid:7)< 0 thenl0Xk ← Xk ∪ {x j }(cid:9)= 0}end ifend for(cid:2) x jk is the winner in example k(cid:2) Ties are broken in favor of x jend forreturn a feasible solution (cid:3)α to the following linear program:(cid:6)(cid:6)(cid:6)l π kl π k∀k, ∀x j ∈ Xk,∀k, ∀x j /∈ Xk,∀l = 1, . . . , m − 1 αl (cid:3) αl+1∀l, αl (cid:3) 0jk ,lαl (cid:3)jk ,lαl (cid:3)(cid:6)l π kl π kj,lαl + 1j,lαlAlgorithm 1. Given a training set of size s, the algorithm returns a scoring rule which is consistent withthe given examples, if one exists.of integers which is also polynomially representable. In this case, the scores are always integral. Thus, instead of using astrict inequality in the LP’s first set of constraints, we can use a weak inequality with an additive term of 1.Remark 3.4. Although the transformation between learning scoring rules and learning linear threshold functions mentionedin Remark 3.2 has some drawbacks as a learning method, we conjecture that results on the computational complexity oflearning linear threshold functions can be leveraged to obtain computational efficiency. Indeed, well-known algorithms suchas Winnow [19] might suit this purpose.Remark 3.5. Algorithm 1 can also be used to check, with high probability, if the voting rule the designer has in mind isindeed a scoring rule, as described (in a different context) by Kalai [16] (we omit the details here). This further justifies thesetting in which the voting rule the designer has in mind is known to be a scoring rule.So, it remains to demonstrate that the generalized dimension of Snm is polynomial in n and m. The following lemmashows this.Lemma 3.6. The generalized dimension of the class Snm is at most m:(cid:8)(cid:7)SnmD G(cid:3) m.Proof. According to Definition 2.3, we need to show that any set of cardinality m + 1 cannot be shattered by Snm. Let}m+1S = {(cid:11)Nk=1 be such a set, and let h, g be the two social choice functions that disagree on all preference profiles in S. Wekshall construct a subset S1 ⊆ S such that there is no scoring rule f (cid:3)α that agrees with h on S1 and agrees with g on S \ S1.1 ) = x1,1 ties between x1 and x2 are broken in favor of x1. Let (cid:3)α be some parameter vector. If we1 . We shall assume without loss of generality that h((cid:11)NLet us look at the first preference profile from our set, (cid:11)Nwhile g((cid:11)Nare to have h((cid:11)Nm(cid:9)1 ) = x2, and that in (cid:11)N1 ) = f (cid:3)α((cid:11)Nm(cid:9)π 11,l· αl (cid:2)π 12,l· αl,1 ), it must hold thatl=1l=1whereas if we wanted f (cid:3)α to agree with g we would want the opposite:m(cid:9)l=1π 11,l· αl <m(cid:9)l=1π 12,l· αl.(4)(5)More generally, we define, with respect to the profile (cid:11)Nk , the vector (cid:3)π k(cid:7) as the vector whose lth coordinate is thedifference between the number of times the winner under h and the winner under g were ranked in the lth place:5(cid:3)π k(cid:7)= (cid:3)π kh((cid:11)k)− (cid:3)π kg((cid:11)k).(6)Now we can concisely write necessary conditions for f (cid:3)α agreeing on (cid:11)Nk with h or g, respectively, by writing:65 There is some abuse of notation here; if h((cid:11)N6 In all profiles except (cid:11)Nk ) = xl then by (cid:3)π k1 , we are indifferent to the direction in which ties are broken.h((cid:11)k ) we mean (cid:3)π kl .A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–1149(cid:3)π k(cid:7)(cid:3)π k(cid:7)· (cid:3)α (cid:2) 0,· (cid:3)α (cid:3) 0.1139(7)(8)Notice that each vector (cid:3)π k(cid:7) has exactly m coordinates. Since we have m + 1 such vectors (corresponding to the m + 1profiles in S), there must be a subset of vectors that is linearly dependent. We can therefore express one of the vectors asa linear combination of the others. Without loss of generality, we assume that the first profile’s vector can be written as acombination of the others with parameters βk, not all 0:(cid:3)π 1(cid:7)=m+1(cid:9)k=2βk · (cid:3)π k(cid:7).Now, we shall construct our subset S1 of preference profiles as follows:(cid:11)(cid:10)k ∈ {2, . . . , m + 1}: βk (cid:2) 0.S1 =(9)(10)Suppose, by way of contradiction, that f (cid:3)α agrees with h on (cid:11)Nk for k ∈ S1, and with g on the rest. We shall examine thevalue of (cid:3)π 1(cid:7)· (cid:3)α:(cid:3)π 1(cid:7)· (cid:3)α =m+1(cid:9)k=2βk · (cid:3)π k(cid:7)· (cid:3)α =(cid:9)k∈S1βk · (cid:3)π k(cid:7)· (cid:3)α +(cid:9)k /∈S1∪{1}βk · (cid:3)π k(cid:7)· (cid:3)α (cid:2) 0.(11)· (cid:3)α is nonpositive ( f (cid:3)α agreesThe last inequality is due to the construction of S1—whenever βk is negative, the sign of (cid:3)π k(cid:7)with g), and whenever βk is positive, the sign of (cid:3)π k(cid:7)1 ) (cid:9)= x2 = g((cid:11)NTherefore, by Eq. (5), we have that f ((cid:11)Nwith g outside S1—this is a contradiction. (cid:2)· (cid:3)α is nonnegative (agreement with h).1 ). However, it holds that 1 /∈ S1, and we assumed that f (cid:3)α agreesTheorem 3.1 is thus proven. The upper bound on the generalized dimension of Snm is quite tight: in the next subsectionwe show a lower bound of m − 3.3.2. Lower bound for the generalized dimension of SnmTheorem 2.6 implies that a lower bound on the generalized dimension of a function class is directly connected to thecomplexity of learning it. In particular, a tight bound on the dimension gives us an almost exact idea of the number ofexamples required to learn a scoring rule. Therefore, we wish to bound D G (Snm) from below as well.Theorem 3.7. For all n (cid:2) 4, m (cid:2) 4, D G (Snm) (cid:2) m − 3.Proof. We shall produce an example set of size m − 3 which is shattered by Sn, forl = 3, . . . , m − 1, as follows. For all l, the voters 1, . . . , n − 1 rank alternative x j in place j, i.e., they vote x1 (cid:11)il xm.The preferences (cid:11)nl ) are defined as follows: alternative x2 is ranked in place l,lalternative x1 is ranked in place l + 1; the other alternatives are ranked arbitrarily by voter n. For example, if m = 5, n = 6,the preference profile (cid:11)Nm. Define a preference profile (cid:11)Nl· · · (cid:11)i(the preferences of voter n in profile (cid:11)Nl x2 (cid:11)il3 is:(cid:11)13x1x2x3x4x5(cid:11)23x1x2x3x4x5(cid:11)33x1x2x3x4x5(cid:11)43x1x2x3x4x5(cid:11)53x1x2x3x4x5(cid:11)63x3x4x2x1x5Lemma 3.8. For any scoring rule f (cid:3)α with α1 = α2 (cid:2) 2α3 it holds that:(cid:12)(cid:8)(cid:7)(cid:11)Nlf (cid:3)α=x1 αl = αl+1,x2 αl > αl+1.Proof. We shall first verify that x2 has maximal score. x2’s score is at least (n − 1)α2 = (n − 1)α1. Let j (cid:2) 3; x j ’s scoreis at most (n − 1)α3 + α1. Thus, the difference is at least (n − 1)(α1 − α3) − α1. Since α1 = α2 (cid:2) 2α3, this is at least(n − 1)(α1/2) − α1 > 0, where the last inequality holds for n (cid:2) 4.1140A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–1149Now, under preference profile (cid:11)Nl, x1’s score is (n − 1)α1 + αl+1 and x2’s score is (n − 1)α1 + αl. If αl = αl+1, the twoalternatives have identical scores, but x1 was ranked first by more voters (in fact, by n − 1 voters), and thus the winneris x1. If αl > αl+1, then x2’s score is strictly higher—hence in this case x2 is the winner. (cid:2)Armed with Lemma 3.8, we will now prove that the set {(cid:11)Nl= 2α1= · · · = 2α14l ) = x1, and f (cid:3)α2 ((cid:11)Nm, and (cid:3)α2 be such that α1l ) = x2.(cid:2) 2α1= α122α13f (cid:3)α1 ((cid:11)N1}m−1l=33 > 2α1is shattered by Snm. Let (cid:3)α1 be such that α1(cid:2)m. By the lemma, for all l = 3, . . . , m − 1,= α1214 > · · · > 2α1Let T ⊆ {3, 4, . . . , m − 1}. We must show that there exists (cid:3)α such that f (cid:3)α((cid:11)Nl ) = x2 forall l /∈ T . Indeed, configure the parameters such that α1 = α2 > 2α3, and αl = αl+1 iff l ∈ T . The result follows directly fromLemma 3.8. (cid:2)l ) = x1 for all l ∈ T , and f (cid:3)α((cid:11)N4. Learnability of voting treesRecall that we are dealing with a set of alternatives A = {x1, . . . , xm}; as before, we will also denote alternatives bya, b, c ∈ A. A tournament is a complete binary irreflexive relation T over A; we denote the set of all possible tournamentsby T = T ( A).A binary voting tree is a binary tree with leaves labeled by alternatives. To determine the winner of the election withrespect to a tournament T , one must iteratively select two siblings, label their parent by the winner according to T , andremove the siblings from the tree. This process is repeated until the root is labeled, and its label is the winner of theelection.A preference profile (cid:11)N of a set of voters N induces a tournament T ∈ T ( A) as follows: aT b (i.e., a dominates b) if andonly if a majority of voters prefer a to b. Thus, a voting tree is in particular a voting rule, as defined in Section 3. However,for the purposes of this section it is sufficient to regard voting trees as functions f : T ( A) → A, that is, we will disregardthe set of voters and simply consider the dominance relation T on A. We shall hereinafter refer to functions f : T ( A) → Aas pairwise voting rules.Let us therefore denote the class of voting trees over m alternatives by Vm; we emphasize the class depends only on m.We would like to know what the sample complexity of learning functions in Vm is. To elaborate a bit, since we think ofvoting trees as functions from T to A, the sample space is T .4.1. Large voting treesIn this section, we will show that in general, the answer to the above question is that the complexity is exponentialin m. We will prove this by relying on Theorem 2.6; the theorem implies that in order to prove such a claim, it is sufficientto demonstrate that the generalized dimension of Vm is at least exponential in m. This is the task we presently turn to.Theorem 4.1. D G (Vm) is exponential in m.Proof. Without loss of generality, we let m = 2k + 2. We will associate every distinct binary vector v = (cid:4)v 1, . . . , vk(cid:5) ∈ {0, 1}kwith a distinct example in our set of tournaments S ⊆ T . To prove the theorem, we will show that Vm shatters this set Sof size 2k.Let the set of alternatives be:(cid:10)a, b, x0A =1, x11, x02, x12, . . . , x0k , x1k(cid:11).i T v bT v x0For every vector v ∈ {0, 1}k, define a tournament T v as follows: for i = 1, . . . , k, if v i = 0, we let x0i T v bT v x1i ; otherwise, ifjj = 0, 1, a beats xi . In addition, for all tournaments T v , and all i = 1, . . . , k,v i = 1, then x1i , but a loses to b. Wedenote by S the set of these 2k tournaments.7 Let f be the constant function b, i.e., a voting tree which consists of onlythe node b; let g be the constant function a. We must prove that for every S 1 ⊆ S, there is a voting tree such that b winsfor every tournament in S1 (in other words, the tree agrees with f ), and a wins for every tournament in S \ S 1 (the treeagrees with g). Consider the tree in Fig. 2, which we refer to as the ith 2-gadget.7 The relations described above are not complete, but the way they are completed is of no consequence.Fig. 2. 2-gadget.A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–11491141Fig. 3. v-gadget.Fig. 4. v-gadget∗.With respect to this tree, b wins a tournament T v ∈ S iff v i = j. Indeed, if v i = j, then xji , so b loses to x; if v i (cid:9)= j, then xT v bT v x1− ji1− ji1− ji.beats xji T v bT v x1− ji, and in particular bLet v ∈ {0, 1}k. We will now use the 2-gadget to build a tree where b wins only the tournament T v ∈ S, and loses everyother tournament in S. Consider a balanced tree such that the deepest nodes in the tree are in fact 2-gadgets (as in Fig. 3).As before, b wins in the ith 2-gadget iff v i = j. We will refer to this tree as a v-gadget.of the entire election. On the other hand, let vit holds that x0Now, notice that if b wins in each of the 2-gadgets (and this is the case in the tournament T v ), then b is the winner(cid:9)= v i = 1. Theni proceeds to win the entire election, unless it isjl —but this must be also an alternative that beats b, as it survived the lth(cid:16) (cid:9)= v, i.e., there exists i ∈ {1, . . . , k} such that w.l.o.g. 0 = vi wins in the ith 2-gadget. x0i ; this implies that x0i T v(cid:16) bT v(cid:16) x1(cid:16)ibeaten in some stage by some other alternative x2-gadget. In any case, b cannot win the election.Consider the small extension, in Fig. 4, of the v-gadget, which (for lack of a better name) we call the v-gadgetRecall that, in every tournament in S, a beats any alternative xij but loses to b. Therefore, by our discussion regardingonly in the tournament T v ; for any other tournament in S,the v-gadget, b wins the election described by the v-gadgetalternative a wins the election.∗∗.We now present a tree and prove that it is as required, i.e., in any tournament in S 1, b is the winner, and in anytournament in S \ S1, a prevails. Let us enumerate the tournaments in S1:S1 = {T v1 , . . . , T vr}.We construct a balanced tree, as in Fig. 5, where the bottom levels consist of the vl-gadgets*, for l = 1, . . . , r.Let T vl∗vl-gadgetb proceeds to win the election. Conversely, let T v ∈ S \ S1. Then a survives in every vl-gadgetproceeds to win the entire election.∈ S1. What is the result of this tournament in the election described by this tree? First, note that b prevails in the. The only alternatives that can reach any level above the gadgets are a and b, and b always beats a. Therefore,, for l = 1, . . . , r. a surely∗We have shown that Vm shatters S, thus completing the proof. (cid:2)Remark 4.2. Even if we restrict our attention to the class of balanced voting trees (corresponding to a playoff schedule),the dimension of the class is still exponential in m. Indeed, any unbalanced tree can be transformed to an identical (as avoting rule) balanced tree. If the tree’s height is h, this can be done by replacing every leaf at depth d < h, labeled by analternative a, by a balanced subtree of height d − h in which all the leaves are labeled by a. This implies that the class ofbalanced trees can shatter any sample which is shattered by Vm.Remark 4.3. The proof we have just completed, along with Lemma 2.4, imply that the number of different pairwise votingrules that can be represented by trees is double exponential in m, which highlights the high expressiveness of voting trees.1142A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–1149Theorem 4.1, coupled with Theorem 2.6, implies that the sample complexity of learning arbitrary voting trees is expo-Fig. 5. The constructed tree.nential in n and m.4.2. Small voting treesIn the previous section, we have seen that in general, a large number of examples is needed in order to learn voting treesin the PAC model. This result relied on the number of leaves in the trees being exponential in the number of alternatives.However, in many realistic settings one can expect the voting tree to be compactly represented, and in particular one canusually expect the number of leaves to be at most polynomial in m. Let us denote by V (k)m the class of voting trees over malternatives, with at most k leaves. Our goal in this section is to prove the following theorem.Theorem 4.4. D G (V (k)m ) = O(k log m).This theorem implies, in particular, that if the number of leaves k is polynomial in m, then the dimension of V (k)m ispolynomial in m. In turn, this implies by Lemma 2.5 that the sample complexity of V (k)m is only polynomial in m. In otherwords, there is a polynomial p(m, 1/(cid:3), 1/δ) such that, given a training set of size p(m, 1/(cid:3), 1/δ), any algorithm that returnssome tree consistent with the training set is an ((cid:3), δ)-learning algorithm for V (k)m .To prove the theorem, we require the following straightforward lemma.Lemma 4.5. |V (k)m| (cid:3) k · mk · Ck−1, where Ck is the kth Catalan number, given by(cid:4)(cid:5)Ck = 1k + 12kk.Proof. The number of voting trees with exactly k leaves is at most the number of binary tree structures multiplied by thenumber of possible assignments of alternatives to leaves. The number of assignments is clearly bounded by mk. Moreover,it is well known that the number of rooted ordered binary trees with k leaves is the (k − 1) Catalan number. So, the totalnumber of voting trees with exactly k leaves is bounded by mk · Ck−1, and the number of voting trees with at most k leavesis at most k · mk · Ck−1. (cid:2)We are now ready to prove Theorem 4.4.Proof of Theorem 4.4. By Lemma 4.5, we have that(cid:13)(cid:13)V (k)m(cid:13)(cid:13) (cid:3) k · mk · Ck−1.Therefore, by Lemma 2.4:(cid:8)(cid:7)V (k)mD G(cid:3) log(cid:13)(cid:13)V (k)m(cid:13)(cid:13) = O(k log m).(cid:2)4.3. Computational complexityIn the previous section, we restricted our attention to voting trees where the number of leaves is polynomial in k. Wehave demonstrated that the dimension of this class is polynomial in m, which implies that the sample complexity of theA.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–11491143class is polynomial in m. Therefore, any algorithm that is consistent with a training set of polynomial size is a suitablelearning algorithm (Theorem 2.5).It seems that the significant bottleneck, especially in the setting of automated voting rule design (finding a compactrepresentation for a voting rule that the designer has in mind), is the number of queries posed to the designer, so in thisregard we are satisfied that realistic voting trees are learnable. Nonetheless, in some contexts we may also be interestedin computational complexity: given a training set of polynomial size, how computationally hard is it to find a voting treewhich is consistent with the training set?In this section we explore the above question. We will assume hereinafter that the structure of the voting tree is knowna priori. This is an assumption that we did not make before, but observe that, at least for balanced trees, Theorems 4.1and 4.4 hold regardless. We shall try to determine how hard it is to find an assignment to the leaves which is consistentwith the training set. We will refer to the computational problem as Tree-SAT (pun intended).Definition 4.6. In the Tree-SAT problem, we are given a binary tree, where some of the leaves are already labeled byalternatives, and a training set that consists of pairs (T j ,xi j ), where T j ∈ T and xi j∈ A. We are asked whether there existsan assignment of alternatives to the rest of the leaves which is consistent with the training set, i.e., for all j, the winner inT j with respect to the tree is xi j .Notice that in our formulation of the problem, some of the leaves are already labeled. However, it is reasonable to expectany efficient algorithm that finds a consistent tree, given that one exists, to be able to solve the Tree-SAT problem. Hence,an N P -hardness result implies that such an algorithm is not likely to exist.Theorem 4.7. Tree-Sat is N P -complete.Proof. It is obvious that Tree-SAT is in N P . In order to show N P -hardness, we present a reduction from 3SAT. In thisproblem, one is given a conjunction of clauses, where each clause is a disjunction of three literals. One is asked whetherthe given formula has a satisfying assignment. It is known that 3SAT is N P -complete [11].Given an instance of 3SAT with variables {x1, . . . , xm}, and clauses {lj1∨ lj2∨ lj3}kj=1, we construct an instance of Tree-Satas follows: the set of alternatives isA = {a, b, x1, ¬x1, c1, x2, ¬x2, c2, . . . , xm, ¬xm, cm}.For each clause j, we define a tournament T j as some tournament that satisfies the following restrictions:j1. lj1, lj2 and l2. a loses to l3 beat any other alternative among the alternatives xi, ¬xi , possibly excluding their own negations.j1, l3, but beats any other alternative among the alternatives xi, ¬xi .j2 and ljIn addition, all tournaments in our instance of Tree-SAT satisfy the following conditions:1. b beats any alternative which corresponds to a literal, but loses to a.2. For all i = 1, . . . , m, ¬xi beats xi .3. ci loses to xi and ¬xi , and beats any other literal and the alternatives a and b. The tournaments are arbitrarily definedwith respect to competitions between ci and ck, i (cid:9)= k.Finally, for each tournament, we require the winner to be alternative b. We now proceed to construct the given (partiallyassigned) tree. We start, as in the proof of Theorem 4.1, by defining a gadget which we call an i-gadget, illustrated in Fig. 6.In this subtree, two leaves are already assigned with xi and ci . Now, with respect to any of the tournaments we defined,if we assign ¬xi to the last leaf, then ¬xi proceeds to beat ci , and subsequently beats xi . If we assign xi to the third leaf,then xi beats ci and wins the election. If we assign any other alternative that is not ck for some k = 1, . . . , m, then thatalternative is defeated by ci , which in turn is beaten by xi . Finally, if ck is assigned, it either loses to ci and then xi is thewinner, or it beats ci and proceeds to beat xi . To conclude the point, either xi , ¬xi , or ck for some k (cid:9)= i survive the i-gadget.Using the i-gadgets, we design a tree that will complete the construction of our Tree-SAT instance; the tree is describedin Fig. 7.Fig. 6. i-gadget.1144A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–1149Fig. 7. The reduction.We now prove that this is indeed a reduction. We first have to show that if the given 3SAT instance is satisfiable, thereis an assignment to the leaves of our tree (in particular, choices of xi or ¬xi ) such that, for each of the m tournaments,the winner is b. Consider some satisfying assignment to the 3SAT instance. For every literal li that is assigned a truthvalue, we assign the label li to the unlabeled leaf of the i-gadget, i.e., we make li survive the i-gadget. Now, consider somejtournament T j . At least one of the literals l3 must be true; as these three literals beat all other literals in thetournament T j , one of these three literals reaches the competition versus a, and wins; subsequently, this literal loses toalternative b. Therefore, b is the winner of the election. Since this is true for any j = 1, . . . , m, we have that the assignmentis consistent with the given tournaments.j2 or lj1, lIn the other direction, consider an instance of 3SAT which is not satisfiable, and fix some assignment to the leaves of thetree. A first case that we consider is that under this assignment, ck survives some i-gadget, i (cid:9)= k. ck cannot be beaten on theway to the root of the tree, except by another c alternative. Hence, b does not win in any of the constructed tournaments.A second case to consider is that for each i-gadget, either xi or ¬xi survives. The corresponding assignment to the 3SATjj1, l3 are all false. This implies that in T j some otherinstance is not satisfying. Therefore, there is some j such that lliteral other than these three reaches the top of the tree to compete against a, and loses. Subsequently, a competes againstb and wins, making a the winner of the election with respect to tournament T j . Hence, this is not an assignment which isconsistent with all tournaments—but this is true with respect to any such assignment. (cid:2)j2, and lDespite Theorem 4.7, it seems that in practice, solving the Tree-Sat problem is sometimes possible; we shall empiricallydemonstrate this.Our simulations were carried out as follows. Given a fixed tree structure, we randomly assigned alternatives (out of a poolof 32 alternatives) to the leaves of the tree. We then used this tree to determine the winners in 20 random tournaments overour 32 alternatives. Next, we measured the time it took to find some assignment to the leaves of the tree (not necessarilythe original one) which is consistent with the training set of 20 tournaments. We repeated this procedure 10 times for eachnumber of leaves in {4, 8, 16, 32, 64}, and took the average of all ten runs.The problem of finding a consistent tree can easily be represented as a constraint satisfaction problem, or in particularas a SAT problem. Indeed, for every node, one simply has to add one constraint per tournament which involves the nodeand its two children. To find a satisfying assignment, we used the SAT solver zChaff. The simulations were carried out on aPC with a Pentium D (dual core) CPU, running Linux, with 2 GB of RAM and a 2.8 GHz clock speed.We experimented with two different tree structures. The first is seemingly the simplest—a binary tree which is as closeto a chain as possible, i.e., every node is either a leaf, or the parent of a leaf; we refer to these trees as caterpillars. Thesecond is intuitively the most complicated: a balanced tree. Notice that, given that the number of leaves is k, the numberof nodes in both cases is 2k − 1. The simulation results are shown in Fig. 8.In the case of balanced trees, it is indeed hard to find a consistent tree. Adding more sample tournaments would addeven more constraints and make the task harder. However, in most elections the number of alternatives is usually not aboveseveral dozen, and the problem may still be solvable. Furthermore, the problem is far easier with respect to caterpillars(even though the reduction in Theorem 4.7 builds trees that are “almost caterpillars”). Therefore, we surmise that for manytree structures, it may be practically possible (in terms of the computational effort) to find a consistent assignment, evenwhen the input is relatively large, while for others the problem is quite computationally hard even in practice.A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–114911455. On learning voting rules “close” to target rulesFig. 8. Time to find a satisfying assignment.Heretofore, we have concentrated on learning voting rules that are known to be either scoring rules or voting trees. Inparticular, we have assumed that there is a scoring rule or a voting tree that is consistent with the given training set.In this section, we push the envelope by asking the following question: given examples that are consistent with someMathematically we are actually asking whether there exist target voting rules fgeneral voting rule, is it possible to learn a scoring rule or a small voting tree that is “close” to the target rule?such that min f (cid:3)α ∈Snerr( f (cid:3)α), orerr( f ), is large. This of course depends on the underlying distribution D. In the rest of this section, the implicitmin f ∈V ∗massumption is that D is the simplest nontrivial distribution over profiles, namely the uniform distribution. Nevertheless, theuniform distribution usually does not reflect real preferences of voters; this is an assumption we are making for the sake ofanalysis. In light of this discussion, the definition of distance between voting rules is going to be the fraction of preferenceprofiles on which the two rules disagree.∗mDefinition 5.1. A voting rule f : LN → A is a c-approximation of a voting rule g iffpossible preference profiles:(cid:10)(cid:13)(cid:13)(cid:11)N ∈ LN :f(cid:8)(cid:7)(cid:11)N(cid:8)(cid:11)(cid:13)(cid:7)(cid:11)N= g(cid:13) (cid:2) c · (m!)n.f and g agree on a c-fraction of theIn other words, the question is: given a training set {((cid:11)Nk , f ((cid:11)Nj )}k, where f : LN → A is some voting rule, how hard isit to learn a scoring rule or a voting tree that c-approximates f , for c that is close to 1?It turns out that the answer is: it is impossible. We shall first give an extreme example for the case of scoring rules.Indeed, there are voting rules that disagree with any scoring rule on almost all of the preference profiles; if the target rulefis such a rule, it is impossible to find, and of course impossible to learn, a scoring rule that is “close” to f .In order to see this, consider the following voting rule that we call flipped veto: each voter awards one point to thealternative it ranks last; the winner is the alternative with the most points. In addition, ties are broken according to thelexicographic order on alternative names. This rule is of course not reasonable as a preference aggregation method, butstill—it is a valid voting rule.Proposition 5.2. Let f (cid:3)α be a scoring rule that is a c-approximation of flipped veto. Then c (cid:3) 1/m.∗ ∈ A. Define a set B(cid:11)N ⊆ LN∗Proof. Let (cid:11)N be a preference profile such that f (cid:3)α((cid:11)N ) = flipped veto((cid:11)N ) = x∗as follows: each profile in the set is obtained by switching the place of an alternative x ∈ A, x (cid:9)= x,, with the place of x∗∈ B(cid:11)N that was obtained by switchingin the ordering of each voter that did not rank x∗x with x, since its score did not decrease as a result of the switches,while its situation in terms of tie-breaking remained the same (that is, its name did not change). In addition, under f (cid:3)α thesituation of x in (cid:11)Nin (cid:11)N (voters that havelast, and the score of the other alternatives remains unchanged).not switched the two alternatives are ones that rank x1 , with respect to score and tie-breaking, is at least as good as the situation of xlast.8 For a preference profile (cid:11)N1, it holds that the winner under flipped veto is x, for some x∗∗∗∗∗8 It cannot be the case that all voters ranked xlast, by our tie-breaking assumption with respect to f (cid:3)α .1146A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–1149∗Note that it might be the case that x and x∗that xfollows that f (cid:3)α((cid:11)Nis ranked last by at least one voter in (cid:11)N , and hence in f (cid:3)α((cid:11)Nhave the same score under (cid:11)N1 ) = x. Therefore, for any preference profile in B(cid:11)N , f (cid:3)α and flipped veto do not agree.∗1 ; however, since flipped veto((cid:11)N ) = x∗1 ) ties between x and xit holdsare broken in favor of x. ItB(cid:11)N2We claim that for any two preference profiles (cid:11)N= ∅. Indeed, assume that there exists (cid:11)N ∈ B(cid:11)Nthe case that the same alternative was switched with x(cid:11)N1 and (cid:11)N∩ B(cid:11)N∗12∗2 are identical. Therefore, assume w.l.o.g. that x1 was switched with x2 on which f (cid:3)α and flipped veto agree, it holds that B(cid:11)N∗. Assume first that the winner in both profiles is xin order to obtain (cid:11)N from both (cid:11)N2 —that would imply1 (only in the rankings of voters that2 . But this means that both x1 and x2 are winners in (cid:11)N under f (cid:3)α1 and (cid:11)N. It cannot bein (cid:11)N∩11 and (cid:11)N∗did not rank x∗(by the fact that x∗last), and x2 was switched with x1 and (cid:11)Nwas a winner in both (cid:11)Nin (cid:11)N2 )—a contradiction.2 such that1 and (cid:11)NIn addition, in any two preference profiles (cid:11)N(cid:8)(cid:7)(cid:11)N1(cid:7)(cid:11)N= flipped veto1(cid:8)f (cid:3)α∗= x,and(cid:8)(cid:7)(cid:11)N2f (cid:3)α(cid:7)= flipped veto(cid:11)N2∩ B(cid:11)N(cid:8)∗∗= x,it holds that B(cid:11)N1in all profiles in B(cid:11)N1It follows that for every preference profile on which f (cid:3)α and flipped veto agree, there are at least m − 1 distinct profilesin all profiles in B(cid:11)N2, but elects x.2∗= ∅, as flipped veto elects x∗∗on which the two voting rules disagree; this proves the proposition. (cid:2)We shall now formulate our main result for this section. The theorem states that almost every voting rule cannot be2 by any small family of voting rules. We shall subsequently see that the theoremapproximated by a factor better than 1holds for small voting trees as well as scoring rules.Theorem 5.3. Let Rnleast a (1 − δ)-fraction of the voting rules f : Ln → {x1, . . . , xm} satisfy the following property: no voting rule in Rnapproximation of f .m be a family of voting rules of size exponential in n and m, and let (cid:3), δ > 0. For large enough values of n and m, atm is a (1/2 + (cid:3))-Proof. We will surround each voting rule f ∈ Rnis a(1/2 + (cid:3))-approximation. We will then show that the union of all these balls covers at most a δ-fraction of the set of thespace of voting rules. This implies that for at least a (1 − δ)-fraction of the voting rules, no scoring rule is a (1/2 + (cid:3))-approximation.m with a “ball” B( f ), which contains all the voting rules for which fFor a given f , what is the size of B( f )? As there are (m!)n possible preference profiles, the ball contains rules that donot agree with f on at most (1/2 − (cid:3))(m!)n preference profiles. For a profile on which there is disagreement, there are moptions to set the image under the disagreeing rule.9 Therefore,(cid:5)m(1/2−(cid:3))(m!)n(cid:13)(cid:13)(cid:13) (cid:3)(cid:13)B( f )(12)(cid:4).(m!)n(1/2 − (cid:3))(m!)nHow large is this expression? Let B(cid:16)( f ) be the set of all voting rules that disagree with f on exactly (1/2 + (cid:3))(m!)npreference profiles. It holds that(cid:4)(cid:13)(cid:13)B(cid:16)(cid:13)(cid:13) =( f )(m!)n(1/2 + (cid:3))(m!)n(m!)n(1/2 − (cid:3))(m!)n(m!)n(1/2 − (cid:3))(m!)n(cid:4)(cid:4)=(cid:2)(cid:5)(m − 1)(1/2+(cid:3))(m!)n(cid:5)(cid:7)(m − 1)1+2(cid:3)(cid:8)1/2(m!)n(cid:5)m1/2(m!)n,where the last inequality holds for a large enough m. But since the total number of voting rules, m(m!)nnumber of rules in B(cid:16)( f ), we have:(cid:7)(cid:8)(m!)nm1/2(m!)n(1/2−(cid:3))(m!)n(cid:8)m(1/2−(cid:3))(m!)n(cid:7)(m!)n(1/2−(cid:3))(m!)n(cid:2)(cid:2) B(cid:16)( f )B( f )= m(cid:3)(m!)n.m(m!)nB( f )ThereforeB( f ) (cid:3) m(m!)nm(cid:3)(m!)n= m(1−(cid:3))(m!)n.9 This reasoning also takes into account voting rules that agree with f on more than (1/2 + (cid:3))(m!)n profiles.(13), is greater than the(14)(15)A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–11491147If the union of balls is to cover at least a δ-fraction of the set of voting rules, we must have |Rnmequivalently, it must hold that |Rnmthan double exponential), so for large enough values of n and m, the above condition does not hold. (cid:2). However, by the assumption |Rnm| (cid:2) δ · m(cid:3)(m!)n| · m(1−(cid:3))(m!)n (cid:2) δ · m(m!)n;| is only exponential in n and m (ratherNotice that the number of distinct voting trees with at most k leaves, as voting rules f : LN → A where | A| = m, isbounded from above for any number of voters n by the expression given in Lemma 4.5, namely k · mk · Ck−1. Therefore, wehave as a corollary from Theorem 5.3:Corollary 5.4. For large enough values of n and m, almost all voting rules cannot be approximated by V (k)factor better than 12 .m , k polynomial in m, to aIn order to obtain a similar corollary regarding scoring rules, we require the following lemma, which may be of indepen-dent interest.Lemma 5.5. There exists a polynomial p(n, m) such that for all n, m ∈ N, |Snm| (cid:3) 2p(n,m).Proof. It is true that there are an infinite number of ways to choose the vector (cid:3)α that defines a scoring rule. Nevertheless,what we are really interested in is the number of distinct scoring rules. For instance, if (cid:3)α1 = 2 (cid:3)α2, then f (cid:3)α1 ≡ f (cid:3)α2 , i.e., thetwo vectors define the same voting rule.It is clear that two scoring rules f (cid:3)α1 and f (cid:3)α2 are distinct only if the following condition holds: there exist two alternatives∈ C , and a preference profile (cid:11)N , such that f (cid:3)α1 ((cid:11)N ) = x j1 and f (cid:3)α2 ((cid:11)N ) = x j2 . This holds only if there exist twox j1 , x j2alternatives x j1 and x j2 and a preference profile (cid:11)N such that under (cid:3)α1, x j1 ’s score is strictly greater than x j2 ’s, andunder (cid:3)α2, either x j2 ’s score is greater or the two alternatives are tied, and the tie is broken in favor of x j2 .Now, assume (cid:11)N induces rankings (cid:3)π j1 and (cid:3)π j2 . The conditions above can be written as(cid:9)l(cid:9)lπ j1,lα1l >π j1,lα2l(cid:3)(cid:9)l(cid:9)lπ j2,lα1l ,π j2,lα2l ,(16)(17)where the inequality is an equality only if ties are broken in favor of x j2 , i.e., if l0 = min{l: π j1,l (cid:9)= π j2,l}, then π j1,l < π j2,l.10Let (cid:3)π(cid:7) = (cid:3)π j1− (cid:3)π j2 . As in the proof of Lemma 3.6, Eqs. (16) and (17) can be concisely rewritten as(cid:3)π(cid:7) · (cid:3)α1 > 0 (cid:2) (cid:3)π(cid:7) · (cid:3)α2,(18)where the inequality is an equality only if the first nonzero position in (cid:3)π(cid:7) is negative.In order to continue, we opt to reinterpret the above discussion geometrically. Each point in Rm corresponds to a possiblechoice of parameters (cid:3)α. Now, each possible choice of (cid:3)π(cid:7) is the normal to a hyperplane. These hyperplanes partition thespace into cells: the vectors in the interior of each cell agree on the signs of dot products with all vectors (cid:3)π(cid:7). More formally,if (cid:3)α1 and (cid:3)α2 are two points in the interior of a cell, then for any vector (cid:3)π(cid:7), (cid:3)π(cid:7) · (cid:3)α1 > 0 ⇔ (cid:3)π(cid:7) · (cid:3)α2 > 0. By Eq. (18), thisimplies that any two scoring rules f (cid:3)α1 and f (cid:3)α2 , where (cid:3)α1 and (cid:3)α2 are in the interior of the same cell, are identical.What about points residing in the intersection of several cells? These vectors always agree with the vectors in one ofthe cells, as ties are broken according to rankings induced by the preference profile, i.e., according to the parameters thatdefine our hyperplanes. Therefore, the points in the intersection can be conceptually annexed to one of the cells.So, we have reached the conclusion that the number of distinct scoring rules is at most the number of cells. Hence, it isenough to bound the number of cells; we claim this number is exponential in n and m. Indeed, each (cid:3)π(cid:7) is an m-vector, inwhich every coordinate is an integer in the set {−n, −n + 1, . . . , n − 1, n}. It follows that there are at most (2n + 1)m possiblehyperplanes. It is known [7] that given k hyperplanes in d-dimensional space, the number of cells is at most O (kd). In ourcase, k (cid:3) (2n + 1)m and d = m, so we have obtained a bound of:(cid:7)2log 3n(cid:8)m (cid:3) (3n)m2 =(cid:7)(2n + 1)m= 2m2 log 3n.(cid:8)m2(19)(cid:2)Remark 5.6. This lemma implies, according to Lemma 2.4, that there exists a polynomial p(n, m) such that for all n, m ∈ N,D G (Snm) (cid:3) p(n, m). However, we have already obtained a tighter upper bound of m.Finally, using Theorem 5.3 and Lemma 5.5 we obtain:10 W.l.o.g. we disregard the case where (cid:3)π j1factor at most.= (cid:3)π j2 ; the reader can verify that taking this case into account multiplies the final result by an exponential1148A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–1149Corollary 5.7. For large enough values of n and m, almost all voting rules cannot be approximated by Snm to a factor better than 12 .Remark 5.8. Proposition 5.2 can seemingly be circumvented by removing the requirement that in a scoring rule defined by avector (cid:3)α, αl (cid:2) αl+1 for all l. Indeed, flipped veto is essentially a scoring rule with αm = 1 and αl = 0 for all l (cid:9)= m. However,the constant voting rule which always elects the same alternative has the same inapproximability ratio, even when thisproperty of scoring rules is not taken into account. Moreover, Corollary 5.7 also holds when scoring rules are not assumedto satisfy this property.6. DiscussionWe have demonstrated the possibility of learning scoring rules and small voting trees. We have argued that, given a blackbox specification of the choice criteria of the society, learning from examples allows one to efficiently (albeit approximately)design such rules. The black box reflects some ideal voting rule the designer has in mind, which satisfies, for instance,different desirable properties. The designer thus essentially translates a cumbersome representation of a voting rule (hiddenwithin the black box) to a concisely represented voting rule which is easy to understand and apply.In Section 5 we have explored the possibility of extending our approach to the setting where the designer has in mindsome general voting rule, rather than a scoring rule or a voting tree, and we would like to find a scoring rule or voting treethat is as close as possible. Technically, our learning-theoretic results basically hold (up to polynomial factors in the samplecomplexity) in this setting, although the situation is more difficult in terms of computational complexity.Unfortunately, it turns out (Corollaries 5.4 and 5.7) that many voting rules cannot be approximated, neither by usingscoring rules nor by small voting trees. However, this negative result relied implicitly on assuming a uniform distributionover profiles. More importantly, it might be the case that some of the important families of voting rules can be approximatedby scoring rules or small voting trees. Therefore, we do not rule out at this point the application of our approach todesigning general voting rules by directly learning scoring rules or small voting trees that approximate them.Criticisms of our approach. A possible concern, given Corollaries 5.4 and 5.7, is with our general motivation. Indeed, if weassume that the designer has in mind, say, a scoring rule, it can be argued that the designer must be aware of this fact, andmust have knowledge of the parameters of the rule. However, recall that the class of scoring rules is exactly the class ofanonymous, neutral, and consistent voting rules [26]. Hence, if the designer selects winners in any way that satisfies thesethree desiderata, a scoring rule with unknown parameters would be obtained.A similar case can be made for voting trees. The underlying assumption behind the literature on implementation byvoting trees (see, e.g., [10] and the references therein) is that voting trees are an abstract model of decision making, andthat many voting rules can in fact be represented as voting trees, even if this transformation is not obvious. For example,the Copeland rule, that selects an alternative that beats the largest number of alternatives in pairwise elections, can berepresented as an elaborate voting tree if there are up to seven alternatives [23]. Hence, the designer might be using avoting rule that can be represented as a voting tree, but might be unaware of the exact representation.Let us discuss two additional possible criticisms regarding our general setting. First, notice that in multiagent environ-ments, the number of alternatives m can be large; for example, if the agents are voting on joint plans [9], then the numberof alternatives might be significantly larger than the number of agents. Hence, complexity results that depend on the num-ber of alternatives are meaningful.Second, it has been suggested that the designer might find it easier to express the ethical properties that are consideredmandatory, rather than express the voting rule by examples. We argue that this is rarely the case. Indeed, it is very difficultto concisely represent properties in computational settings; a universal, agreed-upon language would have to be used, andit is hard to imagine how one would go about creating such a language. On the other hand, specifying a voting rule by(a polynomial number of) examples provides a concise description of the voting rule, and, as we have shown, can lead to aclose approximation.Future work. We mention two directions for future research. First, imagine the following scenario: the designer has in minda huge voting tree, and would like to know whether there exists a smaller voting tree that implements the same votingrule. The same goes for scoring rules, e.g., the designer might have in mind a scoring rule with huge values for componentsof the vector (cid:3)α. This is a setting closely related to ours, but our results do not hold in the alternative setting.Second, it might prove interesting to study the learnability of larger families of voting rules that have a concise repre-sentation. One compelling example is the class of generalized scoring rules recently proposed by Xia and Conitzer [25].AcknowledgementsThe authors wish to thank anonymous AIJ, AAAI, and AAMAS reviewers, for their very helpful comments. This work waspartially supported by Israel Science Foundation grant #898/05.References[1] J. Bartholdi, C.A. Tovey, M.A. Trick, How hard is it to control an election, Mathematical and Computer Modelling 16 (1992) 27–40.A.D. Procaccia et al. / Artificial Intelligence 173 (2009) 1133–11491149[2] E. Beigman, R. Vohra, Learning from revealed preference, in: Proceedings of the 7th ACM Conference on Electronic Commerce (ACM-EC), 2006,pp. 36–42.[3] V. Conitzer, T. Sandholm, Complexity of mechanism design, in: Proceedings of the 18th Annual Conference on Uncertainty in Artificial Intelligence(UAI), 2002, pp. 103–110.[4] V. Conitzer, T. Sandholm, Universal voting protocol tweaks to make manipulation hard, in: Proceedings of the 18th International Joint Conference onArtificial Intelligence (IJCAI), 2003, pp. 781–788.[5] V. Conitzer, T. Sandholm, An algorithm for automatically designing deterministic mechanisms without payments, in: Proceedings of the 3rd Interna-tional Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2004, pp. 128–135.[6] V. Conitzer, T. Sandholm, J. Lang, When are elections with few candidates hard to manipulate? Journal of the ACM 54 (2007) 1–33.[7] H. Edelsbrunner, Algorithms in Combinatorial Geometry, EATCS Monographs on Theoretical Computer Science, vol. 10, Springer, 1987.[8] E. Elkind, H. Lipmaa, Small coalitions cannot manipulate voting, in: Proceedings of the Annual Conference on Financial Cryptography (FC), in: LectureNotes in Computer Science, Springer-Verlag, 2005.[9] E. Ephrati, J.S. Rosenschein, A heuristic technique for multiagent planning, Annals of Mathematics and Artificial Intelligence 20 (1997) 13–67.[10] F. Fischer, A.D. Procaccia, A. Samorodnitsky, A new perspective on implementation by voting trees, Manuscript, 2008.[11] M.R. Garey, D.S. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness, W.H. Freeman and Company, 1979.[12] S. Ghosh, M. Mundhe, K. Hernandez, S. Sen, Voting for movies: The anatomy of a recommender system, in: Proceedings of the 3rd Annual Conferenceon Autonomous Agents (AGENTS), 1999, pp. 434–435.[13] T. Haynes, S. Sen, N. Arora, R. Nadella, An automated meeting scheduling system that utilizes user preferences, in: Proceedings of the 1st AnnualConference on Autonomous Agents (AGENTS), 1997, pp. 308–315.[14] E. Hemaspaandra, L. Hemaspaandra, Dichotomy for voting systems, Journal of Computer and System Sciences 73 (1) (2007) 73–83.[15] E. Hemaspaandra, L.A. Hemaspaandra, J. Rothe, Anyone but him: The complexity of precluding an alternative, Artificial Intelligence 171 (5–6) (2007)255–285.[16] G. Kalai, Learnability and rationality of choice, Journal of Economic Theory 113 (1) (2003) 104–117.[17] S. Lahaie, D.C. Parkes, Applying learning algorithms to preference elicitation, in: Proceedings of the 5th ACM Conference on Electronic Commerce(ACM-EC), 2004, pp. 180–188.[18] J. Lang, M.-S. Pini, F. Rossi, K.B. Venable, T. Walsh, Winner determination in sequential majority voting, in: Proceedings of the 20th International JointConference on Artificial Intelligence (IJCAI), 2007, pp. 1372–1377.[19] N. Littlestone, Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm, Machine Learning 2 (1988) 285–318.[20] B.K. Natarajan, Machine Learning: A Theoretical Approach, Morgan Kaufmann, 1991.[21] A.D. Procaccia, J.S. Rosenschein, Junta distributions and the average-case complexity of manipulating elections, Journal of Artificial Intelligence Re-search 28 (2007) 157–181.[22] A.D. Procaccia, J.S. Rosenschein, A. Zohar, Multi-winner elections: Complexity of manipulation, control and winner-determination, in: Proceedings ofthe 20th International Joint Conference on Artificial Intelligence (IJCAI), 2007, pp. 1476–1481.[23] S. Srivastava, M.A. Trick, Sophisticated voting rules: The case of two tournaments, Social Choice and Welfare 13 (1996) 275–289.[24] R.J. Vanderbei, Linear Programming: Foundations and Extensions, second ed., Springer, 2001.[25] L. Xia, V. Conitzer, Generalized scoring rules and the frequency of coalitional manipulability, in: Proceedings of the 9th ACM Conference on ElectronicCommerce (ACM-EC), 2008, pp. 109–118.[26] H.P. Young, Social choice scoring functions, SIAM Journal of Applied Mathematics 28 (4) (1975).[27] M. Zuckerman, A.D. Procaccia, J.S. Rosenschein, Algorithms for the coalitional manipulation problem, in: The 19th ACM–SIAM Symposium on DiscreteAlgorithms (SODA), 2008, pp. 277–286.