Artificial Intelligence 191–192 (2012) 61–95Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintGAMoN: Discovering M-of-Nlattice-based Genetic Algorithm{¬,∨}hypotheses for text classification by aVeronica L. Policicchio∗, Adriana Pietramala, Pasquale RulloDept. of Mathematics, University of Calabria, Italya r t i c l ei n f oa b s t r a c tArticle history:Received 16 November 2011Received in revised form 4 July 2012Accepted 11 July 2012Available online 20 July 2012{¬,∨}While there has been a long history of rule-based text classifiers, to the best of ourknowledge no M-of-N-based approach for text categorization has so far been proposed.In this paper we argue that M-of-N hypotheses are particularly suitable to model the textclassification task because of the so-called “family resemblance” metaphor: “the members(i.e., documents) of a family (i.e., category) share some small number of features, yet thereis no common feature among all of them. Nevertheless, they resemble each other”. Startingfrom this conjecture, we provide a sound extension of the M-of-N approach with negation, which enables to best fit the true structure of the data.and disjunction, called M-of-Nhypothesis space hasBased on a thorough theoretical study, we show that the M-of-Ntwo partial orders that form complete lattices.GAMoN is the task-specific Genetic Algorithm (GA) which, by exploiting the lattice-basedstructure of the hypothesis space, efficiently induces accurate M-of-NBenchmarking was performed over 13 real-world text data sets, by using four ruleinduction algorithms: two GAs, namely, BioHEL and OlexGA, and two non-evolutionaryalgorithms, namely, C4.5 and Ripper. Further, we included in our study linear SVM, as itis reported to be among the best methods for text categorization. Experimental resultsdemonstrate that GAMoN delivers state-of-the-art classification performance, providinga good balance between accuracy and model complexity. Further, they show that GAMoNcan scale up to large and realistic real-world domains better than both C4.5 and Ripper.hypotheses.{¬,∨}{¬,∨}© 2012 Elsevier B.V. All rights reserved.1. IntroductionAn M-of-N hypothesis, also called Boolean threshold function, may be thought of intuitively as follows. Given a set of Nfeatures, whenever an example satisfies at least M of such features, it is a positive example; otherwise, it is a negative one.That is, an M-of-N hypothesis is a description that involves “counting properties”. There is quite a literature on methodsfor building M-of-N hypotheses. For instance, in [1] algorithms for extracting M-of-N hypotheses from neural networks arereported. M-of-N concepts are also constructed as tests for the induction of decision trees [2–5,7].However, to the best of our knowledge, no M-of-N-based approach for text classification has been so far proposed.Despite this, we conjecture that M-of-N hypotheses are well suited to model the text classification task.Text categorization (TC) is aimed at assigning natural language texts to one or more thematic categories on the basis oftheir contents. It is a difficult task essentially because of two main factors: on one hand, TC has to do with the complexityand richness of the natural language, which allows a concept to be expressed by a variety of constructs and words. Thisaspect is often amplified by the presence in a category of documents which are not about a single narrow subject with* Corresponding author.E-mail address: policicchio@mat.unical.it (V.L. Policicchio).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.07.00362V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95limited vocabulary. On the other hand, the TC task deals with highly dimensional data sets (i.e., with many features).Both such factors concur to make quite unlikely the existence of a set of features, or even a single feature, that occur inall documents of a given category. It may even happen that documents that belong to the same category do not shareany content words. However, as argued in [6], the relationship of “family resemblance” holds. That is, documents underthe same category share a (usually small) set of N features, yet this set is not present in every document. Instead, eachdocument contains (at least) M (cid:2) N of such features, and different documents may not share features at all. That is, the textclassification task deals with the kind of data that M-of-N hypotheses are able to explain.A shortcoming of the M-of-N approach, however, is that its propositions handle positive information only, whereasnegative evidence is deemed to play a crucial role in text categorization. This is mainly because natural languages areintrinsically ambiguous, and negation helps to disambiguate concepts – e.g., the word “ball” may ambiguously refer toeither the concept “sport” or “dance”, whereas the conjunction “ball and not ballroom” much likely refers to “sport”.To overcome this drawback, we extend classical M-of-N hypotheses by negation. In addition, to best fit the true structureof the data, we allow disjunctions of hypotheses. That is, we define a new hypothesis language for text classification, called, which generalizes the classical M-of-N language through negation and disjunction (a preliminary descriptionM-of-Nof the proposed approach can be found in [8]).{¬,∨}In our approach, a classifier is a propositional formula of the form Hc = H1c= pi-of-Pos ∧¬ni-of-Neg is an atom (note that all atoms forming Hc share the same sets Pos and Neg). Here, Pos is the set of positiveterms, Neg the set of negative terms, and pi (cid:3) 0 and ni > 0 are integers called thresholds. The meaning of an atom Hic is:classify document d under category c if at least pi positive terms occur in d and (strictly) less than ni negative terms occurprovides support for explicitly modeling the interactions between positive and negative features.in d. That is, M-of-NOf course, Hc classifies document d under c if any of H1c classifies d under c.c , where each Hic , . . . , Hr∨ · · · ∨ Hr{¬,∨}c{¬,∨}where a hypothesis is an atom with thresholds p = n = 1 is OlexGA [10].The special case of M-of-NThere is a natural ordering in the space of M-of-N{¬,∨}hypotheses determined by two kinds of subsumption relation-ships: the feature and the threshold relationships. The feature relationship is determined by the feature sets Pos and Negappearing in a classifier Hc . As an example, assume that Hc is the atomic classifier p-of-Pos ∧ ¬n-of-Neg, with p = 2 andn = 1. Clearly, the larger Pos, the higher the probability that the condition “at least two positive features occur in a docu-ment” is satisfied. Dually, the smaller Neg, the more likely a document will contain no negative feature in Neg. In summary,the larger Pos, the smaller Neg, the more general Hc . The threshold relationship, in turn, is determined by the thresh-olds appearing in Hc . For an instance, if in the above classifier we replace p = 2 by p = 1, we get a new classifier whichis more general than the previous one – intuitively, only one instead of two positive features is necessary for classifyinga document. These relationships define two hierarchies of hypotheses (more precisely, complete lattices) exploitable for aneffective exploration of the hypothesis space. To this end, we provide suitable refinement operators whereby “navigating”the hypothesis lattices.As argued in [7], the evolutionary approach seems to be particularly suited for the M-of-N learning task, as the globalsearch style of GAs (as opposed to the “one-attribute-at-a-time” of the greedy approach) makes them capable of catching thehidden interactions among attributes that strongly characterize the induction of M-of-N hypotheses. However, the purelynon-deterministic nature of conventional genetic operators does not enable the search strategy to benefit of the structure ofthe hypothesis space. To overcome this drawback, we define a task-specific Genetic Algorithm (GA), called GAMoN, relyingon specialized evolutionary operators representing a stochastic implementation of the refinement operators defined over thesubsumption lattices. At a glance, the following are the main characteristics of GAMoN:• It relies on a variable-length individual representation, where each individual encodes a candidate classifier (Pittsburghapproach [9]).• It combines the standard search strategy of GAs with ad hoc generalizing/specializing (GS) reproduction operators whichexploit the structure of the hypothesis space.• It dynamically adapts the probability of selecting the GS operators over the standard ones.• It maintains a number of competing subpopulations.• It uses the F -measure to assess the fitness of an individual.Unlike in the classical approach, where the feature space is simply a subset of terms from the vocabulary, the one on whichGAMoN builds its hypotheses consists of both a set of positive and a set of negative candidate features. One main issue thatin general arises when inducing a classifier is that of selecting the appropriate dimensionality of the feature space, i.e., howmany features the classifier can access during the learning process. This is a very important design choice, as the qualityof the selected features strongly determines the quality of the learned classifier, especially in text classification, where datasets are usually highly dimensional, noisy and ambiguous. For most systems, the size of the feature space is managed asa tuning parameter, that is, the learning process is rerun over feature spaces of different dimensions and the best resultsare eventually taken. Unfortunately, this may require very long training times, especially over large data sets. To get overthis inconvenience, GAMoN was provided with techniques to automatically detect convenient dimensionality of the featurespace. This way, no manual feature selection is preliminarily needed.V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9563GAMoN was designed as a binary classification system. We use the “one-vs-all” approach to produce one (independent)model for each class in a multi-class classification task (this technique is frequently used in multi-label classification, whereeach example may have more than one label – as it is the case in text classification).We performed an extensive empirical analysis aimed at comparing the proposed approach with other well known learn-ing algorithms. The experimental results show that GAMoN provides an appealing combination of strengths:• First, it provides state-of-the-art predictive accuracy in a wide class of problem domains.• Second, it constructs simple and compact models, thereby facilitating human comprehension of what it has learned.• Third, it can scale up to large data sets better than other state-of-the-art rule-based classifiers.GAMoN was implemented in Java as a plug-in of the Weka platform [11].This paper is organized as follows. In Section 2 we discuss different current learning techniques. In Section 3 we providean overview of the proposed hypothesis language. In Section 4 we develop a thorough theoretical investigation of its prop-erties. In Section 5 we define suitable refinement operators exploiting the structure of the hypothesis space. In Section 6we state the learning problem and show its complexity. In Section 7 we describe the GA, paying particular attention to thetask-specific operators. In Section 8 we describe the experimental framework applied in our empirical analysis. In Section 9we report the results of a comparative study of GAMoN against two GA-based rule induction systems, namely, OlexGA [10]and BioHEL [12,13], and three non-GA systems, namely, Ripper [14], C4.5[15] and the Platt’s Sequential Minimal Optimiza-tion (SMO) method for linear SVM training [16]. In Section 10 we provide a discussion on the proposed method and relateit to other learning algorithms. Finally, Section 11 concludes the paper.2. BackgroundVarious supervised machine learning techniques have been applied to document classification. An excellent overview canbe found in [17].SVMs are a class of learning algorithms that showed to be highly accurate in many data mining tasks. In [19,6], Joachimshas investigated their application to text classification. The results of the empirical study showed that SVMs are moreeffective than other learning algorithms, namely, Naive Bayes, Rocchio, C4.5 and k-Nearest Neighbor. Further, linear SVMshowed to perform as well as non-linear kernels, but substantially more efficiently.Naive Bayes (NB) has been a very popular technique to classify texts due to its computational efficiency and simplicity.McCallum and Nigam [20] investigated the two main document representations for NB text classification, the Bernoulli andmultinomial. They concluded that the latter is superior in accuracy in most cases. However, one problem with MultinomialNB (MNB) is that, when one class has more training examples than another, it selects poor weights for the decision bound-ary. One additional problem is that MNB does not model text well. To improve the performance of MNB, Rennie et al. [21]proposed Complement Naive Bayes (CNB). While learning the conditional probability of one class, CNB uses the frequencyinformation pertaining to all other classes (that is, uses negative information).In a different view, rule learning algorithms have become a successful strategy for classifier induction. Direct methodsextract rules directly from data, while indirect methods extract rules from other classification models, such as decision trees(e.g., C4.5 [15]). Representative examples of direct methods include Inductive Rule Learning (IRL) systems, such as FOIL [22]and Ripper [14], and Associative Rule Learning (ARL) systems, such as CMAR [23], CPAR[24] and TFPC [25]. A sub-class of theinductive rule learners is that of Genetics-Based Machine Learning algorithms (GBML) [26], which rely on the EvolutionaryAlgorithms as search mechanisms. Examples of such systems are XCS [27], SIA [28], GAssist [29] and BioHEL [12,13]. ManyGBML systems have explicit generalization/specialization operators [30–35].The most well-known rule-based classifiers used to learn from texts, notably, Ripper and C4.5, actually originate fromnon-text data mining (see, e.g., [14,19,36]). Among the few examples of rule-based systems specifically designed to classifytexts, we mention the associative classifier NeW [37] and the IRL systems Olex [38] and OlexGA [10]. Olex induces rulesconsisting of one positive conjunction and (zero or) more negative conjunctions. It relies on a search technique that greedilyselects at each step the conjunct, either positive or negative, that maximizes the F -measure over the training set. OlexGAatom with thresholds p = 1 and n = 1.1is a GBML which is a special case of GAMoN, where a classifier is an M-of-NA peculiarity of such systems is that of explicitly dealing with negated features.{¬,∨}Even if prior studies found SVMs and Complement Naive Bayes to be particularly effective for text categorization, rule-based text classifiers are often preferred in real-world applications as they provide interpretable models. Readability isindeed a very desirable property of classification models, which allows a human being to understand and possibly modifythem based on his a priori knowledge.However, one drawback with most rule-based systems is the high computational cost, especially on high dimensionaldata sets. In ARL systems, the time cost for frequent pattern mining may increase very sharply when the size of data setgrows. In addition, the high number of rules generated usually requires an additional pruning step where redundant rulesare discarded. Also IRL systems typically rely on a two-stage process: a greedy heuristics constructs an initial rule set and,1 The Olex and OlexGA suite is downloadable from http://www.mat.unical.it/OlexGA.64V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95then, one or more optimization phases improve compactness and accuracy of the rule set (a similar approach is used fordecision tree as well). All this makes it difficult for most rule induction methods to scale up to large and realistic real-worlddata sets.3. Language overview{¬,∨}The M-of-N{¬,∨}classifier for category c is a propositional formula of the form Hc = H1crepresentation generalizes the classical notion of M-of-N concepts by allowing negation and disjunction.=An M-of-Npi-of-Pos ∧ ¬ni-of-Neg is an atom expressing the following condition: classify document d under category c if at least pipositive features in Pos and less than ni negative features in Neg occur in d. Integers pi (cid:3) 0 and ni > 0 are called thresholds.Of course, Hc classifies document d under c if any among H1c , where each HiSince all atoms forming Hc = H1cc share the same sets of features Pos and Neg, a convenient notationis (cid:5)Pos, Neg, T (cid:6), where T = {(p1, n1), . . . , (pr, nr)} is the set of threshold pairs appearing in the atoms of Hcfor Hc(T is called threshold set). For example, (1-of-Pos ∧ ¬2-of-Neg) ∨ (2-of-Pos ∧ ¬3-of-Neg) can be simpler represented as(cid:5)Pos, Neg, {(1, 2), (2, 3)}(cid:6).c classifies d under c.c , . . . , Hr∨ · · · ∨ Hr∨ · · · ∨ HrcAs a concrete example, consider the classifier constructed by GAMoN for category “grain” from the Reuters data set:Hgrain =(cid:2)Pos = {barley, cereals, corn, grain, maize, rice, sorghum, wheat},Neg = {acquisition, bank, earning, pay, profit, tax, york}, T =(cid:3)(1, 1), (2, 2)(cid:4)(cid:5).This is a classifier of order 2 (as its threshold set has two elements, i.e., (1, 1) and (2, 2)), with 8 positive features (barley,cereals, etc.) and 7 negative ones (acquisition, bank, etc.). The meaning of Hgrain is the following: classify document d undercategory “grain” if either one of the following conditions holds: (1) d contains (exactly) one positive feature and no negativefeatures, or (2) d contains more than one positive feature and less than two negative ones. That is to say, one single positivefeature has no effect on predicting the category “grain” if any negative feature occurs in d, while one single negative featurehas no effect in denying the classification of d if more positive features occur in d.As the above example shows, one beneficial aspect of the M-of-Nrepresentation is readability. This is a very impor-{¬,∨}tant feature, as it makes possible for people to visually inspecting and understanding the induced model.{¬,∨}The M-of-Nhypothesis space has a structure determined by two kinds of subsumption relationships: the featureand the threshold subsumptions.Intuitively, positive features are indicative of membership for a category, contrary to negative ones that are indicativeof non-membership. Thus, the more elements are in Pos, the less are in Neg, the more general a classifier (cid:5)Pos, Neg, T (cid:6) is(i.e., it classifies more documents). Feature subsumption encodes this intuition. As an example, (cid:5){t0, t1}, {t3, t4}, T (cid:6) subsumes(cid:5){t0, }, {t3, t4}, T (cid:6) and is subsumed by (cid:5){t0, t1}, {t3}, T (cid:6).The threshold subsumption relationship is in turn determined by the threshold sets appearing in the classifiers. For an in-stance, the hypothesis (cid:5)Pos, Neg, {(1, 1)}(cid:6) subsumes (cid:5)Pos, Neg, {(2, 1)}(cid:6) as only one, instead of two positive features, isnecessary for it to classify a document.Thus, both the above hierarchies capture the intuitive notion of general-to-specific ordering, that is, if Hc subsumes H(cid:7)either hierarchy, then whatever is classified by H(cid:7)is that they form complete lattices in the hypothesis space (thus, any hypothesis can be reached in the search space).c inc is classified by Hc as well. One interesting property of such relationshipsWe can take advantage of this general-to-specific ordering in order to selectively search the hypothesis space. For an in-stance, if the classifier (cid:5)Pos, Neg, {(2, 1)}(cid:6) is too specific (i.e., it covers too few positive examples) it can be generalized either(i) through the threshold subsumption, by replacing the threshold set {(2, 1)} by one less restrictive, say, {(1, 1)}, or (ii) bythe feature subsumption, i.e., by adding some term to Pos or removing some term from Neg.Another way of generalizing or specializing a hypothesis is by “interaction” with another one. To this end, we exploitthe lattice structure of the hypothesis space. That is, the least upper bound (resp. greatest lower bound) of two hypothesescan be taken, in any of the two lattices, in order to get a more general (resp. specific) one. As an example, given twohypotheses sharing the same threshold sets, say, (cid:5){t0, t1}, {t3}, {(2, 1)}(cid:6) and (cid:5){t0, t4}, {t5}, {(2, 1)}(cid:6), we can specialize bothby taking the greatest lower bound in the feature subsumption lattice, that is, (cid:5){t0}, {t3, t5}, {(2, 1)}(cid:6) – a classifier whosesets of positive and negative features are {t0} = {t0, t1} ∩ {t0, t4} and {t3, t5} = {t3} ∪ {t5}, respectively. Likewise, given twohypotheses sharing the same feature sets, say, (cid:5)Pos, Neg, {(1, 1)}(cid:6) and (cid:5)Pos, Neg, {(2, 2)}(cid:6), we can specialize both by takingthe greatest lower bound in the threshold subsumption lattice, that is, (cid:5)Pos, Neg, {(2, 1)}(cid:6) – a classifier whose threshold setis {max(1, 2), min(1, 2)} (see Fig. 1). It can be easily verified that both greatest lower bounds are more specific than therespective parents.As we will see later on this paper, the above concepts are at the basis of the definition of the refinement operators. Theseare the abstract tools for searching the hypothesis space, that find concrete application in the definition of the reproductionoperators of GAMoN.4. Language definition and hypothesis spaceNow that we have an intuitive view of the basic ideas, in the next subsections we will provide formal definitions ofthem. In particular, we will start from the notion of feature space, i.e., the set of features which provide the lexicon fromV.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9565Fig. 1. τ -subsumption lattice with threshold bounds P = 2 and N = 3.language and define the feature (cid:4)φ and the threshold (cid:4)τwhich hypotheses are built. Then we formalize the M-of-Nsubsumption relationships, showing a number of interesting properties. In particular, we will prove that they form completelattices in the hypothesis space and provide a constructive definition of the meet and the join operators in both lattices.Finally, we will give a deep insight into the structure of the hypothesis space, and show the notion of decision boundary forM-of-Nclassifiers.{¬,∨}{¬,∨}Note. For the proofs of the propositions reported in this section, the reader is referred to Appendix A.4.1. Feature spaceWe are given a set T of training documents (also called “examples”) and a set C of categories (also called “concepts”).A document is a set of features (also called “terms”), a feature being a sequence of one or more words (or word stems).Each document in T is associated with a category in C. We denote by T c ⊆ T the training set of c, i.e., the set of trainingdocuments associated with category c. We call vocabulary the set of features occurring in the documents of T .Unlike in the classical definitions, where the feature space is simply a subset of the vocabulary, in our definition thehypothesesfeature space consists of both a set of positive and a set of negative features. This is because M-of-Nexplicitly models the interaction between positive and negative features, the latter being regarded as “first class citizens”.{¬,∨}Definition 4.1 (Feature space). We are given a vocabulary V , a non-negative integer k and a scoring function σ which assignsa score to every feature in V based on its correlation with category c (e.g., CHI Square [39]). Define the feature space Fc(k)∗(of size k) for category c as the pair (cid:5)Posc (k), Neg∗∗c (k) ⊆ V and Negc (k)(cid:6), where Pos∗c (k) ⊆ V are as follows:∗c (k) is the set of the k highest scoring features in V for category c, according to σ ; we say that t ∈ Pos• Pos∗c (k) isa candidate positive feature of c.• given Pos(cid:6)∗N = {t ∈ V | t /∈ PosΘ + =t∈Pos∗(k), consider the set N of terms co-occurring with positive candidate features within negative examples, i.e.,c (k) and (Θ + ∩ Θ(t) \ T c) (cid:12)= ∅} where Θ(t) ⊆ T is the set of training documents containing feature t,c (k) Θ(t) and T c is the training set of c. With each feature t ∈ N we assign a score η(t) as follows:∗η(t) =|Θ + ∩ Θ(t) \ T c||Θ + \ T c| + |Θ(t) ∩ T c|.It can be easily seen that 0 < η(t) (cid:2) 1. In particular, a term t occurring in all negative examples and in no positive onecontaining any positive feature has score η(t) = 1. On the other hand, η(t) > 0, ∀t ∈ N as, by definition, t co-occurs∗c (k) as the set of the best k elementswith a candidate positive feature in some negative example. Then, we define Neg∗of N according to η; we say that t ∈ Negc (k) is a candidate negative feature of c.The rationale behind the above definition is rather intuitive: candidate positive features are supposed to capture mostof the positive examples, as they are characterized by high scoring values. On the contrary, candidate negative features,defined as terms co-occurring with positive candidate terms within negative examples, are supposed to discard most of the(potentially) false positive examples. For an instance, if the feature “ball” is a candidate positive and “ballroom” co-occurs66V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95with “ball” within some negative examples, “ballroom” becomes a negative candidate feature. Clearly, the higher the scoringσ (t) (resp. η(t)) of a term t, the higher its value as a candidate positive (resp. negative) feature.4.2. Hypothesis spaceA hypothesis is a propositional formula used to describe the examples of a given concept. The hypothesis language wepropose in this section is an extension of the M-of-N language, called M-of-N{¬,∨}.Definition 4.2 (Hypothesis language). We are given the feature space Fc(k) = (cid:5)Posand N, called threshold bounds. An M-of-Nfollows:∗∗c (k)(cid:6), along with two integers, Pc (k), Neghypothesis (or “classifier”) for category c over Fc(k) is inductively defined as{¬,∨}• Basis: p-of-Pos ∧ ¬n-of-Neg is an atom (or 1-order classifier), where 0 (cid:2) p (cid:2) P and 0 < n (cid:2) N are integers called∗∗positive and negative thresholds, respectively, and Pos ⊆ Posc (k) and Neg ⊆ Negc (k) are (possibly empty) sets of features.In particular, Pos is the set of positive features and Neg the set of negative features. This classifier classifies a documentd under category c if at least p positive features occur in d and less than n negative features occur in d. A convenientnotation for p-of-Pos ∧ ¬n-of-Neg is (cid:5)Pos, Neg, {(p, n)}(cid:6).• Induction: let H1c= (cid:5)Pos, Neg, T1(cid:6) be an r-order classifier and H2cc share the same sets of features). Then Hc = H1c∨ H2= (cid:5)Pos, Neg, T2(cid:6) an s-order classifier (note that H1cc is a classifier of order q (cid:2) r + s. Hc classifies documentc or H2c classifies d under c. A convenient notation for Hc is (cid:5)Pos, Neg, T (cid:6), where T = T1 ∪ T2.and H2d under c if either H1As already noticed, all atoms forming an r-order classifier hold the same sets Pos and Neg. That is why we can denotec by using the compact notation (cid:5)Pos, Neg, T (cid:6), where T = {(p1, n1), . . . , (pr, nr)} is the set of all thresholdHc = H1cpairs appearing in the atoms of Hc . Clearly, the size of T is the order of the classifier.∨ · · · ∨ HrExample 4.1. The 2-order classifier (1-of-Pos ∧ ¬2-of-Neg) ∨ (2-of-Pos ∧ ¬3-of-Neg) can be represented as (cid:5)Pos, Neg, {(1, 2),(2, 3)}(cid:6).An atom with p = 0 and n > |Neg| acts as an acceptor, while one with p > |Pos| is to be understood as a rejector. An atom(cid:5)Pos, Neg, {(1, 1)}(cid:6) coincides with an OlexGA classifier [10]. In general, a (non-acceptor, non-rejector) atom (cid:5)Pos, Neg, {(p, n)}(cid:6)is logically equivalent to the following propositional formula:c ← (T 1 ∨ · · · ∨ Tk) ∧ ¬(Tk+1 ∨ · · · ∨ Tk+m)where T 1 · · · Tk are all possible conjunctions made of p positive terms in Pos, and Tk+1 · · · Tk+m are all possible conjunctionsmade of n negative terms in Neg. The rule-based semantics of an r-order classifier Hc = H1c is the obviouscgeneralization of the base case: Hc is equivalent to the union of the rule sets of all Hic , 1 (cid:2) i (cid:2) r.∨ · · · ∨ HrWe finally provide the definition of hypothesis space.Definition 4.3. The hypothesis space H(Fc(k), P , N) is the set of all hypotheses constructible over a feature space Fc(k) andfor given thresholds bounds P and N.4.3. Ordering the hypothesis spaceThere is a natural ordering in the hypothesis space determined by two kinds of subsumption relationships, namely,the feature and the threshold subsumption.4.3.1. Ordering along the feature dimensionLet HT (Fc(k)) ⊆ H(Fc(k), P , N) be the hypothesis subspace consisting of all hypotheses in H(Fc(k), P , N) having thesame given threshold set T . Hypotheses in HT (Fc(k)) are said τ -homogeneous. On HT (Fc(k)) there exists a binary relationthat we call feature-subsumption (φ-subsumption, for short).Definition 4.4 (Feature-subsumption). We are given two classifiers in HT (Fc(k)), say, H1c(cid:5)Pos2, Neg2, T (cid:6). H1H1c φ-subsumes H2c is called a φ-generalization of H2c is φ-subsumed by H1c a φ-specialization of H1c ).c ) if both Pos2 ⊆ Pos1 and Neg1c (and H2c (and H2= (cid:5)Pos1, Neg1, T (cid:6) and H2=c(cid:4)φ H2⊆ Neg2 (write H1c ).cExample 4.2. According to the above definition, the classifier Hc = (cid:5){t0, t1}, {t3, t4}, {(1, 1)}(cid:6) φ-subsumes H(cid:7)= (cid:5){t0}, {t3, t4},{(1, 1)}(cid:6) and is φ-subsumed by H(cid:7)(cid:7)= (cid:5){t0, t1}, {t4}, {(1, 1)}(cid:6). Intuitively, the former subsumption holds as the classificationccondition on the positive features “at least one of t0 and t1 must occur in d” is clearly weaker than “t0 must occur in d”,so as (ceteris paribus) more documents will be classifier by Hc than by H(cid:7)c as the conditionc . Dually, Hc is φ-subsumed by H(cid:7)(cid:7)cV.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9567on the negative features expressed by the latter “t4 must not occur in d” is weaker than that expressed by the former“neither t3 nor t4 can occur in d”.Next we show that if H1c(cid:4)φ H2c then H1c classifies all documents classified by H2D(Hc) ⊆ D the set of documents classified by Hc , for a document set D.c . In the following, we will denote byProposition 4.1. Let H1c and H2c be two classifiers in HT (Fc(k)). Then H1c(cid:4)φ H2c implies D(H1c ) ⊇ D(H2c ).The following proposition shows that (HT (Fc(k)), (cid:4)φ) is a complete lattice.Proposition 4.2. (HT (Fc(k)), (cid:4)φ) is a complete lattice. Indeed, for any H1c ) and the least upper bound lubφ(H1bound glbφ(H1c ) as follows:c , H2c , H2c , H2c∈ HT (Fc(k)), there are both the greatest lower(a) lubφ(H1(b) glbφ(H1c , H2c , H2c ) = (cid:5)Pos1 ∪ Pos2, Neg1c ) = (cid:5)Pos1 ∩ Pos2, Neg1∩ Neg2, T (cid:6).∪ Neg2, T (cid:6).It is easy to recognize that the bottom element of HT (Fc(k)) is (cid:5)∅, Neg∗(k), T (cid:6) and the top (cid:5)Pos∗(k), ∅, T (cid:6).4.3.2. Ordering along the threshold dimensionLet HΦ (P , N) ⊆ H(Fc(k), P , N) be the hypothesis subspace consisting of all hypotheses having the same Φ = (cid:5)Pos, Neg(cid:6),with Pos and Neg over Fc(k). We say that two classifiers in HΦ (P , N) are φ-homogeneous. Next we show that a subsumptionhierarchy there exists in HΦ (P , N). We call it threshold-subsumption, or τ -subsumption, for short.Notation. Since φ-homogeneous hypotheses share all the same feature sets, in the following, whenever no ambiguity arises,we shall represent a classifier (cid:5)Pos, Neg, T (cid:6) simply by T .Example 4.3. The 2-order classifier (cid:5)Pos, Neg, {(1, 2), (2, 3)}(cid:6) may be represented simply as {(1, 2), (2, 3)}.Definition 4.5 (Threshold-subsumption). Let T1 = {(p1, n1)} and T2 = {(p2, n2)} be two threshold sets of size 1. Then T1τ -subsumes T2, denoted T1 (cid:4)τ T2, if both p1 (cid:2) p2 and n1 (cid:3) n2 hold. More in general, given two threshold sets (of any size),(cid:7))} (cid:4)τwe say that T1 τ -subsumes T2 if, for each element (p, n) ∈ T2, there exists an element (p{(p, n)}.(cid:7)) ∈ T1 such that {(p(cid:7), n(cid:7), nThe relation (cid:4)τHΦ (P , N), H1c (and H2of H2induces a relation on HΦ (P , N) as follows. Given H1cc (and H2c τ -subsumes H2c is τ -subsumed by H1c a τ -specialization of H1c ).c ), if T1 (cid:4)τ T2 (write H1c= (cid:5)Pos, Neg, T1(cid:6) and H2c= (cid:5)Pos, Neg, T2(cid:6) inc is called a τ -generalization(cid:4)τ H2c ). H1Example 4.4. Given the φ-homogeneous atoms H1cof H1{(1, 2), (2, 1)} and H2cc is smaller than that of H2= {(2, 1)}, H1cc , whereas the vice versa holds for the negative thresholds. As a more general case, let H1c holds as the positive threshold== {(1, 2)} and H2c(cid:4)τ H2c= {(1, 1)} be φ-homogeneous classifiers. Since {(1, 2)} (cid:4)τ {(1, 1)} holds, H1c(cid:4)τ H2c follows.Next we show that if H1c(cid:4)τ H2c then any document classified by H2c is classified by H1c as well.Proposition 4.3. Let H1c and H2c be two classifiers in HΦ (P , N). Then H1c(cid:4)τ H2c implies D(H1c ) ⊇ D(H2c ).Unlike (cid:4)φ , the binary relation (cid:4)τ is not a partial order.Example 4.5. Classifiers H1c= {(1, 2)} and H2c= {(1, 2), (2, 2)} are such that both H1c(cid:4)τ H2c and H2c(cid:4)τ H1c hold.(cid:4)τ H2Definition 4.6 (Equivalence, minimality). Two φ-homogeneous classifiers H1H1∨ H2c . If a classifier Hc can be expressed as H1cis redundant. Otherwise Hc is minimal. If Hc = (cid:5)Pos, Neg, T (cid:6) is minimal, T is minimal. H1H1c and H2c such that either H1c are equivalent, denoted H1(cid:4)τ H2c(cid:4)τ H1c or H2c strictly τ -subsumes H2c and H2(cid:4)τ H1ccccc >τ H2c , if H1c(cid:4)τ H2c and not H1c≡ H2c .≡ H2c , if bothc , then Hcc , denotedExample 4.6. Classifiers H1Hc = H1c , where H1cIt can be easily recognized that Hc ≡ H1∨ H2cc holds.c and H2= {(1, 1), (2, 2)} and H2cc of Example 4.5 are equivalent. Classifier Hc = {(1, 1), (3, 1), (2, 2)} is redundant as(cid:4)τ H2c is minimal.c holds. On the contrary, H1= {(3, 1)} and, further, H1c68V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95The notion of equivalence encodes the intuition that equivalent hypotheses provide the same classification behavior.In fact, from Proposition 4.3 it immediately follows that equivalent classifiers do classify the same documents. The nextlemma and proposition show that the vice versa holds as well (i.e., classifiers that classify the same documents are equiva-lent).Lemma 4.1. Let Hc = H1cand (D(H1c ) ⊇ D( ˆH2∨ H2c ) or D(H2c and ˆHc = ˆH1c ) ⊇ D( ˆH2c )).c∨ ˆH2c be given. Then, D(Hc) ⊇ D( ˆHc) only if (D(H1c ) ⊇ D( ˆH1c ) or D(H2c ) ⊇ D( ˆH1c ))Proposition 4.4. Let Hc and H(cid:7)c be two classifiers in HΦ (P , N). Then D(Hc) ⊇ D(H(cid:7)c) implies Hc (cid:4)τ H(cid:7)c .From the above proposition and Proposition 4.3 it immediately follows the following statement.Corollary 4.1. Given classifiers H1c and H2c , H1c≡ H2c iff D(H1c ) = D(H2c ).Next we show a number of further interesting properties of classifiers.Proposition 4.5. Let Hc = (cid:5)Pos, Neg, T (cid:6) be given. Then:1. Hc is redundant iff there exist (pi, ni), (p j, n j) ∈ T such that {(pi, ni)} (cid:4)τ {(p j, n j)}.2. Hc is minimal iff T = {(p1, n1), . . . , (pr, nr)} is such that pi < p j and ni < n j , or vice versa, for each i, j ∈ [1, r].3. If Hc = H1cc , then Hc ≡ H1c .c and H1(cid:4)τ H2∨ H2cExample 4.7. According to Part 1 of Proposition 4.5, the classifier Hc = {(1, 1), (2, 1), (2, 2)} is redundant, as {(1, 1)} (cid:4)τ{(2, 1)}, while H1c satisfies the condition p1 < p2 and n1 < n2cof Part 2 of Proposition 4.5. Since Hc = H1c follows from Part 3 ofcProposition 4.5.= {(1, 1), (2, 2)} is minimal. It is easily verified that H1∨ H2= {(2, 1)}, and H1cc , where H2c , Hc ≡ H1(cid:4)τ H2cAnother interesting property of HΦ (P , N) is that, for any two classifiers H1fier Hc in it which is a τ -specialization of both H1H2c (i.e., Hc is equivalent to the logical AND of H1the (constructive) definition of and(H1that (cid:4)τ forms a complete lattice over the set of minimal classifiers.c and H2c and H2c , H2c in it, there exists another classi-c andc ). Next we providec ). As we will see shortly after, this definition is a preliminary step for showingc that classifies exactly the documents classified by both H1c ). We denote such a classifier by and(H1c , H2c and H2Definition 4.7 (AND of classifiers). Given H1c , H2c∈ HΦ (P , N), and(H1c , H2c ) is the classifier inductively defined as follows:• Basis: if H1cMin{n1, n2}.= {(p1, n1)} and H2c= {(p2, n2)} are atoms, then and(H1c , H2c ) = {(p, n)}, where p = Max{p1, p2} and n =• Inductive step: if H1cand(H1,1c, H2,1c), H2 = and(H1,1= H1,1c ∨ H1,2c, H2,2cc= H2,1and H2c), H3 = and(H1,2c ∨ H2,2c, H2,1cc, then and(H1) and H4 = and(H1,2c , H2c, H2,2c).c ) = H1 ∨ H2 ∨ H3 ∨ H4, where H1 =Example 4.8. If H1cinition). Intuitively, and(H1negative one, the more specific an atom is. As another example, if H3cand(H3= {(1, 2)} and H2cc , H2c ) is more specific than H1= {(2, 3)}, then and(H1c , H2c and H2c ) = {(1, 1), (2, 1), (2, 2)} (inductive step of the definition). Notice that and(H3c ) = {max{1, 2}, min{2, 3}} = {2, 2} (base step of the def-c as the higher the positive threshold, the lower the= {(0, 1), (2, 2)}, then= {(1, 1), (2, 3)} and H4cc ) is not minimal.c , H4c , H4Proposition 4.6. Given Hc, ˆHc ∈ HΦ (P , N), the classifier and(Hc, ˆHc) is such that (1) and(Hc, ˆHc) ∈ HΦ (P , N), (2) D(and(Hc,ˆHc)) = D(Hc) ∩ D( ˆHc), and (3) Hc (cid:4)τ and(Hc, ˆHc) and ˆHc (cid:4)τ and(Hc, ˆHc).c , H2Example 4.9. In Example 4.8 we have seen that and(H1and(H1recognized that a document satisfying such a condition is classified by both H1by both H1by and(H1= {(2, 3)}. Hence,c ) classifies a document d if d contains x (cid:3) 2 positive features and y < 3 negative features; it is immediatelyc . On the other hand, d is classifiedc if it contains x (cid:3) max(1, 2) positive features and y < min(2, 3) negative features, that is, if d is classifiedc ) = {(2, 2)}, for H1= {(1, 2)} and H2cc and H2c , H2c and H2c , H2c ).cThe above result shows that the inclusion of the “∧” operator in the definition of classifier would not increase theexpressivity of the language (i.e., it would be redundant).Now we turn our attention to minimal classifiers. The following proposition shows a key result, that is, the uniquenessof the minimal classifier for an equivalence class.V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9569Functions (cid:18)(T1, T2) and (cid:19)(T1, T2)1. function Minimize(T )2.4. return T .drop from T each (p, n) s.t. ∃(p5. function (cid:19)(T1, T2)6.return Minimize(T1 ∪ T2);(cid:7), n(cid:7)) ∈ T s.t. (p(cid:7), n(cid:7)) (cid:2)τ (p, n).T = ∅;for each (p, n) ∈ T18. function (cid:18)(T1, T2)9.10.11.12.13. return Minimize(T );(cid:7)) ∈ T2(cid:7), nT = T ∪ {(Max{p, pfor each (p(cid:7)}, Min{n, n(cid:7)})};Fig. 2. Computation of (cid:19)(T1, T2) and glbτ (T1, T2).Proposition 4.7. Any equivalence class into which is partitioned the hypothesis subspace HΦ (P , N) by the relation ≡ has a uniqueminimal classifier.We denote by Min(Hc) the minimal classifier of the equivalence class of Hc . From now on, we will restrict our attentionto the set of minimal classifiers MΦ (P , N) ⊆ HΦ (P , N). It is immediate to recognize that the restriction of the binaryrelation (cid:4)τ to MΦ (P , N) is a partial order. More precisely, it is a complete lattice.Proposition 4.8. The poset (MΦ (P , N), (cid:4)τ ), where MΦ (P , N) is the set of the minimal classifiers in HΦ (P , N), is a complete lattice.Indeed, for any two elements H1c ) and the least upperc , H2bound lubτ (H1c ) as follows:c of MΦ (P , N), there are both the greatest lower bound glbτ (H1c and H2c , H2(a) lubτ (H1c , H2∨ H2H1c .cc , H2(b) glbτ (H1of and(H1c , H2c ).c ) = Min(H1c∨ H2c ), that is, the least upper bound of H1c , H2c is the minimal classifier of the equivalence class ofc ) = Min(and(H1c , H2c )), that is, the greatest lower bound of H1c , H2c is the minimal classifier of the equivalence classExample 4.10. The τ -subsumption lattice, for threshold bounds P = 2 and N = 3, is depicted in Fig. 1. How we can see,there are 19 classifiers; the most general one is {(0, 3)} and the most specific one is {(2, 1)}. Further, the maximum orderof a classifier is 3 (the order of {(0, 1), (1, 2), (2, 3)}).We conclude this section by providing a constructive definition of both lubτ and glbτ . Let H1= (cid:5)Pos, Neg, T2(cid:6). Now, by Proposition 4.8, lubτ (H1H2c(by Definition 4.2), that is, lubτ (H1by discarding every (p, n) such that there exists (ption 4.5, Part 1). We denote Min(T1 ∪ T2) by (cid:19)(T1, T2), so that lubτ (H1tion 4.8, we have that glbτ (H1c , H2Definition 4.7 and then minimized as shown above, so as glbτ (H1= (cid:5)Pos, Neg, T1(cid:6) andc ) = Min((cid:5)Pos, Neg, T1 ∪ T2(cid:6))c ), that is, lubτ (H1c ) = (cid:5)Pos, Neg, Min(T1 ∪ T2)(cid:6), where Min(T1 ∪ T2) is obtained from T1 ∪ T2 simply(cid:7))} (cid:4)τ {(p, n)} (immediate from Proposi-(cid:7), nc ) = (cid:5)Pos, Neg, (cid:19)(T1, T2)(cid:6). Likewise, by Proposi-c )). We denote by (cid:18)(T1, T2) the threshold set constructed by using(cid:7)) ∈ T1 ∪ T2 such that {(pc ) = (cid:5)Pos, Neg, (cid:18)(T1, T2)(cid:6).c ) = Min(and(H1c ) = Min(H1c , H2c , H2c , H2c , H2c , H2c , H2∨ H2(cid:7), ncc(cid:7)= (cid:5)Pos, Neg, T1(cid:6) and H2Proposition 4.9. Let H1cc(cid:2)(cid:5)Pos, Neg, (cid:19)(T1, T2)(cid:2)(cid:5)Pos, Neg, (cid:18)(T1, T2)c , H2(cid:8)lubτ(cid:7)H1H1==c , H2(cid:8),ccglbτ= (cid:5)Pos, Neg, T2(cid:6) be two (minimal) classifiers in MΦ (P , N). Thenwhere (cid:19)(T1, T2) and (cid:18)(T1, T2) are constructively defined as shown in Fig. 2.A proof of correctness of the algorithms of Fig. 2 is reported in Appendix A.4.4. The minimal hypothesis spaceIn the previous subsection we defined the notion of minimal classifier as the representative hypothesis of an equiva-lence class. Minimality is a desirable property of classifiers as, by guaranteeing the uniqueness of representation, imposesan ordered structure within the hypothesis space. For this reason, we restrict ourselves to minimal classifiers.70V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95∗(k), NegDefinition 4.8. Let the feature space Fc(k) = (cid:5)Poshypothesis space constructible over Fc(k), for the given P and N values, is (M(Fc(k), P , N), (cid:4)τ , (cid:4)φ), where∗(k)(cid:6) and the threshold bounds P and N be given. The minimal(cid:7)MFc(k), P , N(cid:8)=(cid:9)MΦ (P , N)s.t. Φ ∈(cid:3)(cid:5)Pos, Neg(cid:6)(cid:10)(cid:10) Pos ⊆ Pos∗(k), Neg ⊆ Neg∗(cid:4)(k).ΦThus, a minimal hypothesis space is uniquely determined by k, P and N.Using the previously defined notational convention, in the following we will denote by Mτ (Fc(k)) the set of (minimal)classifiers in M(Fc(k), P , N) with threshold set T . It is immediate to recognize that, given the minimal threshold set T ,Mτ (Fc(k)) and Hτ (Fc(k)) coincide, as both consist of all (minimal) classifiers with threshold set T constructible over Fc(k).It turns out that (Mτ (Fc(k)), (cid:4)φ) and (Hτ (Fc(k)), (cid:4)φ) coincide as well.Next we discuss on the structure of (M(Fc(k), P , N) (cid:4)τ , (cid:4)φ), as determined by the two subsumption relations. Since theφ-subsumption and the τ -subsumption lattices are the basic building blocks of a minimal hypothesis space, we start ourdiscussion by preliminarily showing the size of such lattices.4.4.1. The size of the two types of latticeA φ-subsumption lattice MT (Fc(k)) consists, for a given T , of all hypotheses that can be built over a given featurespace Fc(k), each hypothesis corresponding to a particular choice of the sets Pos and Neg over Fc(k). It is immediate torecognize the following fact.Fact 4.1. The size of MT (Fc(k)) is equal to the number of sets Pos and Neg constructible over the feature space Fc(k) =(cid:5)Pos∗(k)(cid:6), that is, |HT (Fc(k))| = 22k.∗(k), NegA τ -subsumption lattice MΦ (P , N) consists, for a given Φ = (cid:5)Pos, Neg(cid:6), of all hypotheses that can be built for the giventhreshold bounds P and N, each hypothesis corresponding to a particular threshold set satisfying P and N. The next lemmaand proposition show both the size of MΦ (P , N) and the maximum order of a classifier.Lemma 4.2. Given threshold bounds P and N, along with k (cid:2) Min(P + 1, N), let Tintegers, where ∀i (cid:2) k, 0 (cid:2) pi (cid:2) P and 0 < ni (cid:2) N. Then there exists a unique subset S ⊆ Tthreshold set.+k= {p1, . . . , pk} and T× T= {n1, . . . , nk} be sets of−k having size k which is a minimal+k−kProposition 4.10. Given the threshold bounds P and N, (1) the maximum order of a classifier in MΦ (P , N) is Min(P + 1, N),and (2) the number of minimal threshold sets that can be constructed for the given bounds is(cid:13)(cid:13) (cid:12)(cid:12)Nj.(1)λ(P , N) =Min{P +1,N}(cid:11)j=1P + 1j4.4.2. The landscape from the τ -subsumption perspective(cid:7), NegGiven threshold bounds P and N, let us consider two lattices (MΦ (P , N), (cid:4)τ ) and (MΦ(cid:7) (P , N), (cid:4)τ ), where Φ = (cid:5)Pos, Neg(cid:6)(cid:7)(cid:6). By Proposition 4.10, they have the same number λ(P , N) of classifiers, i.e., all those constructibleand Φ(cid:7) = (cid:5)Posfor the given P and N. Hence, there is a one to one correspondence g between the classifiers of MΦ (P , N) and thoseof MΦ(cid:7) (P , N), two related classifiers having the same threshold sets. Since the τ -subsumption relation among classifiersis determined by the τ -subsumption relation among the respective threshold sets (see Definition 4.5), clearly H1(cid:4)τ H2ccholds in (MΦ (P , N), (cid:4)τ ) iff g(H1c ) holds in (MΦ(cid:7) (P , N), (cid:4)τ ). That is, the two lattices are isomorphic. Further,(cid:7)(cid:6),all classifiers in MΦ (P , N) share the feature sets (cid:5)Pos, Neg(cid:6), while those in MΦ(cid:7) (P , N) share the feature sets (cid:5)Posso as MΦ (P , N) and MΦ(cid:7) (P , N) are disjoint. Since the number of different Φs (i.e., pairs of sets Pos and Neg) constructibleover a given feature space Fc(k) is 22k, we may conclude that (M(Fc(k), P , N), (cid:4)τ ) has a structure made of 22k isomorphic,disjoint lattices (MΦ (P , N), (cid:4)τ ), each of size λ(P , N). For an instance, given P = 2 and N = 3, (M(Fc(k), 2, 3), (cid:4)τ ) willconsists of 22k lattices whose structure is that depicted in Fig. 1.c ) (cid:4)τ g(H2(cid:7), NegFact 4.2. The partial order (M(Fc(k), P , N), (cid:4)τ ) consists of 22k isomorphic, disjoint lattices (MΦ (P , N), (cid:4)τ ), each of size λ(P , N).4.4.3. The landscape from the φ-subsumption perspectiveThe φ-subsumption perspective is of course dual to the τ -subsumption one. Consider two lattices (MT (Fc(k)), (cid:4)φ)and MT (cid:7) (Fc(k), (cid:4)φ), for any T , T (cid:7). As stated by Fact 4.1, their size is 22k. Since the relationship (cid:4)φ among classifiersis determined only by the inclusion relationship among the respective sets of features (see Definition 4.4), the structureof the above lattices does not depend on T . Hence, MT (Fc(k)) and MT (cid:7) (Fc(k)) are isomorphic under (cid:4)φ . Since anyhypothesis in MT (Fc(k)) has threshold set T and any hypothesis in MT (cid:7) (Fc(k)) has threshold set T (cid:7), MT (Fc(k)) andMT (cid:7) (Fc(k)) are disjoint. Therefore, in the hypothesis space (M(Fc(k), P , N), (cid:4)φ) there exist λ(P , N) isomorphic, disjointlattices (MT (Fc(k)), (cid:4)φ), each of size 22k.V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9571Fig. 3. Decision boundaries of {(1, 2)} (left side) and {(1, 2), (2, 3)} (right side).Fact 4.3. The partial order (M(Fc(k), P , N), (cid:4)φ) consists of λ(P , N) isomorphic, disjoint lattices (Mτ (Fc(k)), (cid:4)τ ), each of size 22k.4.5. Decision boundariesThere is an interesting graphical representation of a classifier Hc = (cid:5)Pos, Neg, T (cid:6) on the 2-dimensional space N 2(see Fig. 3). Here, each point (x, y), with x and y non-negative integers, is labeled by a pair of integers (cid:5)π (x, y), ν(x, y)(cid:6),where π (x, y) is the number of positive examples (documents) and ν(x, y) the number of negative ones containing exactly xfeatures from Pos and y features from Neg. Intuitively, we may think of a point (x, y) as identifying the set of (both positiveand negative) examples with x positive features and y negative ones. Hence, the region of the plane(cid:3)=RHc(x, y)(cid:10)(cid:10) x (cid:2) |Pos|, y (cid:2) |Neg|, ∃(pi, ni) ∈ T s.t. x (cid:3) pi, y < ni(cid:14)(cid:4),whose points satisfy the threshold conditions, identifies the documents that are classified by Hc (we call RHc classificationregion). It turns out that the number of documents classified by Hc is(π (x, y) + ν(x, y)). The border of theregion RHc is the decision boundary of Hc .As an example, the classification regions of the (φ-homogeneous) classifiers Hc = {(1, 2)} and H(cid:7)= {(1, 2), (2, 3)} are(x, y)∈RHccthose depicted in Fig. 3. Here, the following should be noted:1. the decision boundary of the atom Hc is a rectangle (left side of Fig. 3), while that of the 2-order classifier H(cid:7)c is theoverlapping of two rectangles, one for each atom (right side of Fig. 3), and2. the classification region of Hc , which is a τ -specialization of H(cid:7)c , is contained in the classification region of Hc .c , Hc such that H(cid:7)The above two statements can be generalized. In particular, concerning point (2), it can be easily verified that, for any twoclassifiers H(cid:7)⊇ RHc holds, and vice versa (it suffices to use the above definitionof classification region along with Definition 4.5). As for point (1), we can state that the decision boundary of a classifierH1c is a step-wise non-decreasing polyline in N 2 consisting alternately of vertical and horizontal segments. To seecwhy, it suffices to observe the following:(cid:4)τ Hc , the condition RH(cid:7)∨ · · · ∨ Hrcca. the decision boundary of each single atom Hicsatisfy the test conditions pi (cid:2) x (cid:2) |Pos| and 0 (cid:2) y < ni , and= {(pi, ni)}, 1 (cid:2) i (cid:2) r, is a rectangle subtending the points (x, y) whichb. the r atoms H1c· · · Hrc are such that pi−1 < pi and ni−1 < ni , for each i ∈ [1, r] (see Proposition 4.5, Part 2).Intuitively, the non-decreasingness of decision boundaries implies that documents which are less likely to belong toa category c (that is, documents with few positive features and many negative ones) are also less likely to be classifiedby Hc . For an instance, consider two documents d(x, y) and dpositive features, respectively, with(cid:7) (cid:3) x, and both containing the same number y of negative features. Intuitively, d(x, y) is less likely to be a positive examplex(cid:7), y) (as it holds less positive features, which are indicative of membership, for the same number of negativefor c than dones). On the other hand, since the boundary is non-decreasing, it also happens that d(x, y) is less likely to fall within RHcthan d(cid:7), y), that is, d(x, y) is less likely to be classified by Hc .(cid:7)(cid:7), y), having x and x(cid:7)(x(cid:7)(x(cid:7)(x4.6. Remarks on the proposed languageThe “family resemblance” metaphor. In a binary classification task there are two families (classes): the positive, call it P ,and the negative, call it N. Let us assume that the atom p-of-Pos ∧ ¬n-of-Neg is used to characterize the members of P .72V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95{¬,∨}Here, Pos is the set of features that such members share, while the threshold p states how many of such features eachmember must hold. Symmetrically, Neg is the set of features shared by the members of the other family N. Actually, notall members, but more specifically only those that are most similar to the members of P (recall that, by Definition 4.1,the features in Neg are those that characterize members of N holding some features of the family P ). Thus, an examplethat exhibits p positive features is a member of P provided that it holds less than n negative features. To use an analogy,imagine that the members of the Brown family hold at least two of the following features: green eyes, black hair andtallness. However, also in the White family there are members that are tall and have green eyes, but they also hold at leasttwo of the following features: fair hair, long nose and high forehead. Thus, an individual that is tall and has green eyesbelongs to the Brown family provided that he possesses less than two of such features (that would play the role of negativefeatures for the Brown family with threshold n = 2).On the expressivity of M-of-N. M-of-N hypotheses can be regarded as M-of-Natoms with only positive features,i.e., atoms of the form p-of-Pos. Simple M-of-N hypotheses are often sufficient for the classification of new and unseendata, but it is well known that there are cases where the need for negative features cannot be avoided. A simple exampleis the following: document {t0, t1} belongs to c, document {t0, t1, t2} belongs to c. It iseasy to recognize that this scenario can be modeled by the atoms Hc = (cid:5){t0}, {t2}, {(1, 1)}(cid:6), Hc(cid:7) = (cid:5){t0, t2}, ∅, {(2, 1)}(cid:6), andHc(cid:7)(cid:7) = (cid:5){t1}, {t0}, {(1, 1)}(cid:6), where the negative features are needed to discriminate among classes.and document {t1, t2} belongs to c(cid:7)(cid:7)(cid:7)Although M-of-Natoms surpass classical M-of-N hypotheses in expressive power, there are data sets that cannotbe represented simply by atoms. As an example, assume that documents d1 = {t0}, d2 = {t0, t1} and d3 = {t0, t1, t2} areassociated with category c, while d4 = {t0, t2} is not. Intuitively, to correctly classify such data we need a hypothesis Hcstating the following: the occurrence of either t0 or t1 is sufficient in order for a document d be classified under c, providedthat t2 does not appear in d; but, if t2 does appear in d, a stronger condition is needed, that is, both t0 and t1 must occurin d. We can easily recognize that Hc is the 2-order classifier (cid:5){t0, t1}, {t2}, {(1, 1), (2, 2)}(cid:6), and that no atomic equivalentclassifier there exists.{¬,∨}{¬,∨}{¬,∨}However, though the proposed language improves the expressive power of M-of-N concepts, M-of-Ndoes not ac-hypothesis capable of explaining thetually reach the full expressiveness of DNF. For an instance, there is no M-of-Nfollowing data: d1 = {t0}, d2 = {t1, t2} and d3 = {t0, t2}, with d1 and d2 belonging to class c and d3 to its complement. It iseasy to recognize that the reason for this limitation is that all atoms forming a hypothesis share the same sets of positiveand negative features. As we will see in the next sections, the rationale for this choice is that it drastically restricts thesearch space. That is, effectiveness is traded-of against efficiency.{¬,∨}Why subsumption relations are important. As we have seen, the two relations (cid:4)τ and (cid:4)φ codify the intuitive notion ofc and H2c isc , any example covered by H 2c such that H1“more-general-than” between hypotheses. That is, given H1covered by H1(cid:4)τ H2cc as well.The idea of ordering the concept space by a “more-general-than” relation is not new in Inductive Logic Programming(see, e.g., [40]). What is actually original in our approach is the ordering along two dimensions, the feature and the thresholddimensions.The ordering relations are important because they provide the learning algorithm with a means to selectively searchthe hypothesis space. For an instance, the search strategy can move towards a more general hypothesis whenever too fewpositive examples are covered by the current one or, vice versa, towards a more specific hypothesis if too negative examplesare covered.The implementation of a selective search requires the definition of suitable operators which, by exploiting the subsump-tion relations, enable the generalization/specialization of a hypothesis. This is what we will do in the next section.5. Refinement operatorsInformally, a refinement operator is a function which enables to “navigate” the space of the minimal classifiers throughthe partial order relations. We next provide two classes of refinement operators: unary and binary refinement operators.Notation. For the sake of simplicity, in the following definitions we will often denote the set of minimal hypothesesM(Fc(k), P , N, ) simply by M.5.1. Unary refinement operatorsA unary refinement operator is a non-deterministic function which returns a “neighbor” of Hc either in the φ-subsumptionor in the τ -subsumption relationship. It is used to move a classifier “one step” upward or downward in either one of thetwo hierarchies.Definition 5.1 (Unary refinement operators). A unary refinement operator is a non-deterministic function from M to M. In par-ticular, the unary x-generalization operator, denoted by ↑x, with x ∈ {φ, τ },if(cid:2)H(cid:7)c suchcthat H(cid:7)c >x Hc (i.e., Hc is the top element); otherwise, ↑x (Hc) = H(cid:7)c >x Hc . Theunary x -specialization operator ↓x is defined accordingly.is a function such that: ↑x (Hc) = Hcc >x Hc and (cid:2)H(cid:7)(cid:7)∈ M such that H(cid:7)c where H(cid:7)c >x H(cid:7)(cid:7)V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9573We first provide a constructive definition of both ↑φ (Hc) and ↓φ (Hc) in the φ-subsumption lattice. Informally, a directancestor of Hc in the φ-subsumption hierarchy is obtained from Hc either by adding to Pos a candidate positive term or byremoving any term from Neg (a direct descendant is obtained in a dual way).(cid:23) COMPUTATION of ↑φ (Hc) and ↓φ (Hc). Given Hc = (cid:5)Pos, Neg, T (cid:6), compute:↑φ (Hc) = Hc if Pos = Pos∗(k) and Neg = ∅ (i.e., if Hc is the top element in the φ-subsumption lattice), otherwise(cid:15)↑φ (Hc) =(cid:5)Pos ∪ {t}, Neg, T (cid:6) where t ∈ Pos(cid:5)Pos, Neg \ {t}, T (cid:6) where t ∈ Neg.∗(k), or↓φ (Hc) = Hc if Pos = ∅ and Neg = Neg(cid:15)∗(k), otherwise↓φ (Hc) =(cid:5)Pos \ {t}, Neg, T (cid:6) where t ∈ Pos, or∗(k).(cid:5)Pos, Neg ∪ {t}, T (cid:6) where t ∈ Neg(cid:2)A proof of correctness of the above computation is reported in Appendix A.Example 5.1. Given Hc = (cid:5){t0, t1}, {t2}, T (cid:6), let t ∈ Poshypotheses are “neighbors” of Hc in the φ-subsumption hierarchy:(cid:5),(cid:5)∗(k) and t(cid:4)(cid:5)(cid:5),↓φ (Hc) =↑φ (Hc) =(cid:2){t0, t1},(cid:2){t0, t1}, ∅, T, T(cid:3)t2, t(cid:7)(cid:2){t0, t1, t}, {t2}, T(cid:2){t1}, {t2}, T,↑φ (Hc) =↓φ (Hc) =.(cid:7) ∈ Neg∗(k) be two candidate features. Then, the followingLet us now see how a neighbor ↑τ (Hc) or ↓τ (Hc) of Hc in the τ -subsumption lattice is computed. Clearly, to obtain,say, ↑τ (Hc), we have to replace in Hc the threshold set T by an immediate ancestor ↑ T in the τ -subsumption lattice.So as the problem reduces to the computation of ↑ T .(cid:23) COMPUTATION of ↑τ (Hc) and ↓τ (Hc). Given Hc = (cid:5)Pos, Neg, T (cid:6), compute ↑τ (Hc) and ↓τ (Hc) as follows↑τ (Hc) = (cid:5)Pos, Neg, ↑ T (cid:6) and ↓τ (Hc) = (cid:5)Pos, Neg, ↓ T (cid:6)where the non-deterministic operator ↑ (resp. ↓) applied to T returns an immediate ancestor (resp. descendant) of Tin the τ -subsumption hierarchy. ↑ T is constructed from T by the algorithm of Fig. 4 (we do not report the dualalgorithm for ↓ T for space reason). (cid:2)For a description of the algorithm of Fig. 4 the reader is referred to Appendix B.5.2. Binary refinement operatorsBinary refinement operators are aimed at exploiting the lattice structure of both the φ-subsumption and theτ -subsumption hierarchies. In particular, given two classifiers, they return a classifier which is either the lub or the glbof the two classifiers in any of the two subsumption lattices, depending on whether a generalization or a specialization isneeded, respectively.Definition 5.2 (Binary refinement operators). A binary refinement operator is a function from M × M to M. Let classifiers= (cid:5)Pos2, Neg2, T2(cid:6) be given. There are two binary generalization operators, the τ -generalization= (cid:5)Pos1, Neg1, T1(cid:6) and H2H1(cid:16)(cid:16)ccτ and the φ-generalizationφ , defined as follows:(cid:17)τ(cid:17)φ(cid:7)H1(cid:7)c , H2cH1c , H2c(cid:8)(cid:8)(cid:2)(cid:5)Pos1, Neg1, (cid:19)(T1, T2),== (cid:5)Pos1 ∪ Pos2, Neg1(cid:18)∩ Neg2, T1(cid:6)(cid:18)and two binary specialization operators(cid:8)τ and(cid:2)(cid:5)Pos1, Neg1, (cid:18)(T1, T2)c , H2(cid:19)=(cid:7)c,φ defined as follows:H1(cid:7)τ(cid:19)φ(cid:8)H1c , H2c= (cid:5)Pos1 ∩ Pos2, Neg1∪ Neg2, T1(cid:6).It should be noted that all the above operators are not commutative. In fact,x(H1ization of H1c ) yields a specialization of H1c (through H2c ), andc , H2(cid:18)c , H2x(H1c ), with x ∈ {τ , φ}, yields a general-c (through H2c ).(cid:16)74V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95Non-deterministic function ↑ TInput: threshold bounds P and N; a minimal threshold set T = {τ1, . . . , τk}, whereτi = (pi, ni) for each i ∈ [1, k] and pi < pi+1, ni < ni+1, for each i ∈ [1, k) (see Proposition 4.5)Output: a direct ancestor ↑ T of T ;px < p < p y and nx < n < n y , i.e., (p, n) = (p y − 1, nx + 1)function NewElement( X, Y )1. if (px > p y) then swap X = (px, nx) and Y = (p y, n y)2. δ+ = |p y − px|, δ− = |n y − nx|;3. if (δ+ > 1 and δ− > 1) then compute the most specific threshold pair (p, n) such that4.5. else compute the most specific threshold pair (p, n) such that px (cid:3) p (cid:3) p y6.7.8.9.10. return {(p, n)}.and nx (cid:3) n (cid:3) n y and:if δ+ > 1 then (p, n) (cid:2)τ Y ; set (p, n) = (p y − 1, n y);else if δ− > 1 then (p, n) (cid:2)τ X ; set (p, n) = (px, nx + 1)else (p, n) (cid:2)τ X and (p, n) (cid:2)τ Y ; set (p, n) = (px, n y)if T = {(0, N)} (i.e., T is the top of the lattice) return ∅;τ0 = (p0, n0) = (−1, 0); τk+1 = (pk+1, nk+1) = (P + 1, N + 1);randomly select i ∈ [1, k];if i = 1 and pi = 0 then adj = τi+1 // right adjacentelse if i = k and ni = N then adj = τi−1; // left adjacent11. begin12.13.14.15.16.17.18. return ↑ T = Minimize(T ∪ NewElement(τi, adj));else randomly select adj ∈ {τi−1, τi+1}; ;Fig. 4. Pseudo code for the random selection of a direct ancestor of a threshold set in the τ -subsumption lattice.Fig. 5. Given H1cclassifier (cid:5)Pos1, Neg1, T2(cid:6), i.e.,= (cid:5)Pos1, Neg1, T1(cid:6) and H2(cid:16)cτ (H1c , H2c ) = (cid:5)Pos1, Neg1, (cid:19)(T1, T2)(cid:6).= (cid:5)Pos2, Neg2, T2(cid:6), the hypothesis(cid:16)τ (H1c , H2c ) is the least upper bound of H1c and the φ-homogeneous(cid:16)Intuitively,c , H2c (see Fig. 5). Dually,τ (H1of H2feature sets of H2c ) is the least upper bound of H1(cid:16)c ) is the least upper bound of H1c (the specialization operators are defined accordingly).φ(H1c , H2c and the φ-homogeneous classifier having the same threshold setc and the τ -homogeneous classifier having the sameExample 5.2. Consider H1cDefinition 5.2, we have that= (cid:5)Pos1, Neg1, T1(cid:6) and H2c= (cid:5)Pos2, Neg2, T2(cid:6), where T1 = {(2, 2)} and T2 = {(1, 1)}. According to(cid:17)τ(cid:19)τ(cid:7)H1(cid:7)H1c , H2cc , H2c(cid:8)(cid:8)==(cid:2)(cid:5)Pos1, Neg1, (cid:19)(T1, T2)(cid:2)(cid:5)Pos1, Neg1, (cid:18)(T1, T2)(cid:16)(cid:2)(cid:2)==Pos1, Neg1,(cid:3)(cid:3)(cid:4)(cid:5),(1, 1), (2, 2)(cid:4)(cid:5)Pos1, Neg1,(2, 1).(cid:18)It is easily verified thatτ (H1c , H2c ) is a generalization of H1c , whileNow, assume that Pos1 = {t1, t2}, Neg1(cid:16)= {t3, t4}, Pos2 = {t2, t5}, Neg2theφ operator as follows:(cid:16)(cid:8)(cid:7)H1c , H2cφ= (cid:5)Pos1 ∪ Pos2, Neg1∩ Neg2, T1(cid:6) =(cid:2){t1, t2, t5}, {t3}, T1(cid:5)τ is a specialization of H1c .c (through H2= {t3}. We generalize H1c ) by usingand specialize H1c (through H2c ) byV.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9575(cid:18)φ as follows:(cid:18)(cid:7)φH1c , H2c(cid:8)= (cid:5)Pos1 ∩ Pos2, Neg1∪ Neg2, T1(cid:6) =(cid:2){t2}, {t3, t4}, T1(cid:18)(cid:5).It is easy to recognize that both(cid:16)φ(H1c , H2c ) (cid:4)φ H1c and H1c(cid:4)φφ(H1c , H2c ) hold.Proposition 5.1. Given classifiers H1c and H2c , the following hold:(cid:16)•• H1cx(H1(cid:4)xc , H2(cid:18)c ) (cid:4)x H1c ,c , H2x(H1c ),where x ∈ {τ , φ}.6. Learning problem and complexityBefore providing an effective algorithm for the learning of classifiers, in this section we give a definition of the learningproblem and show its complexity.The goal is to find, for each category c ∈ C, a (minimal) hypothesis Hc ∈ M(Fc(k), P , N) that best fits the trainingdata. To this end, we assume that categories in C are mutually independent, so as the whole learning task consists of |C|independent binary sub-tasks, one for each category.To assess Hc we use the F -measure. This is a measure that trades off precision Pr versus recall Re and is defined as theharmonic mean of Pr and Re as follows2:F = 2 Pr RePr + Re.(2)Let us denote by F (Hc, T ) the F -measure obtained by Hc when it is applied to the documents of the training set T . Now,the learning problem can be formulated as the following optimization problem.Definition 6.1 (Learning problem). Let the feature space Fc(k) and the threshold bounds P , N be given. The learning problemis to find a (minimal) classifier Hc ∈ M(Fc(k), P , N) that maximizes the F -measure F (Hc, T ) of Hc over the training set T .The above learning problem is essentially an instance of Inductive Logic Programming (ILP) [41], which deals with thegeneral problem of inducing logic programs from examples in the presence of background knowledge. It is well known thatILP problems are computationally intractable.Proposition 6.1. The decision version of the learning problem is NP-complete.The reader is referred to Appendix A for a proof of the above statement.The theory of PAC-learnability, first proposed by Valiant in [42], provides a model of approximated polynomial learningwhere the polynomially bound amount of resources (both number of examples and computational time) is traded-off againstthe accuracy of the induced hypothesis. However, as shown by the above proposition, there is no algorithm that producesa consistent M-of-Nhypotheses are not PAC-hypothesis on p examples in time polynomial in p, so as M-of-Nlearnable (this should not be surprising, given that M-of-N concepts are not PAC-learnable – see Pitt and Valiant [43]).{¬,∨}{¬,∨}7. Learning a classifier: a GA-based approachSo far, we have seen the structural properties of the M-of-Nhypothesis space and designed a set of refinementoperators that are the search abstract tools. Further, we have defined the learning problem and showed that it is compu-tationally difficult. In this section we provide an effective algorithm for learning classifiers in the M-of-Nhypothesisspace. In particular, we propose a heuristic approach based on a Genetic Algorithm (GA).{¬,∨}{¬,∨}A GA represents a well known and powerful domain-independent search technique based on natural evolutionary opera-tors. A standard GA can be regarded as composed of three basic elements: (1) A population, i.e., a set of candidate solutions(classifiers), called individuals or chromosomes, that will evolve during a number of iterations (generations); (2) a fitness2 This is also known as the F 1-measure, because recall and precision are evenly weighted.76V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95function used to assign a score to each individual of the population; (3) an evolution mechanism based on operators suchas elitism, selection, crossover and mutation. A comprehensive description of GAs can be found in [44].{¬,∨}GAs showed to be well suited for learning classification rules (see, e.g., [29,28,27]) as well as M-of-N hypotheses [7],as they perform a thorough search of the hypothesis space, not limited by any greedy search bias. However, GAs also havesome disadvantages for rule discovery. For instance, conventional genetic operators, such as crossover and mutation, are nor-mally applied without directly trying to optimize the quality of the new candidate solution by exploiting the structure ofthe hypothesis space. A recent research trend aimed at overcoming this drawback is that of combining the standard searchstrategy of GAs with that of task-specific genetic operators which incorporate the knowledge about the specific applica-tion [30–33] (here, by “application” we mean the task of inducing classification rules).Next we present GAMoN, the task-specific GA designed to induce M-of-Nhypotheses. As we will see, GAMoN relieson a search strategy where ad hoc, selective reproduction operators, aimed at exploiting the structure of the hypothesisspace, are combined with standard ones.Detecting the “best” hypothesis space M(Fc(k), P , N) to be explored by GAMoN is a fundamental task which stronglyaffects the quality of the learning process. In principle, we might either (1) manage the model parameters k, P and N(which uniquely determines the hypothesis space) as parameters to be manually tuned, or (2) embed them in the evolutivedynamics of the GA, letting it to adaptively evolve the best values. GAMoN incorporates this latter approach. To this end,evolution relies on a number of competing subpopulations S(k1, P 1, N1), . . . , S(kn, Pn, Nn), where each S(ki, P i, Ni) consistsof individuals encoding classifiers in the same hypothesis space Mi(Fc(ki), P i, Ni), 1 (cid:2) i (cid:2) n.A preliminary step for the creation of the subpopulations S(k1, P 1, N1), . . . , S(kn, Pn, Nn) is the detection of a suitablerange [kmin, kmax] for the feature space dimensionality ki of each subpopulation. This is the subject of the next subsection.Afterward, we will discuss on individual encoding and reproduction operators. Then, we report a detailed description of thegenetic algorithm GAMoN and, finally, we provide some remarks on the proposed GA.7.1. Detecting the feature space dimensionalityThe feature space Fc(k) provides the basic symbols from which the classifiers of a given hypothesis space M(Fc(k), P , N)are constructed. Behind its definition there is the implicit assumption that only the selected terms are representative of thecategory being learned, while the rest are redundant. Thus, predicting the right value of the dimensionality k is a crucialstep. On one hand, a reduced feature space is desirable as redundant or noisy features may “deceive” the learning algorithmand have detrimental effect on classification results (this is particularly true in the text classification task, where data setsare usually noisy and ambiguous). Further, reducing the number of features makes the learning process more efficient(especially in the evolutionary approach, where large feature spaces may entail large individuals and, thus, more matchoperations). On the other hand, an aggressive feature selection might discard features that carry essential information.Next we provide a criterion, inspired to the one proposed in [36], for detecting a range of dimensionality values basedon the statistical characteristics of the data set at hand.Definition 7.1 (Dimensionality range). We are given a vocabulary V c and a scoring function σ . We define the dimensionalityrange [kmin, kmax] for category c as follows:(cid:10)(cid:4)(cid:10)(cid:10) σ (t, c) (cid:3) m + s(cid:10),(cid:10)(cid:4)(cid:10)(cid:10) σ (t, c) (cid:3) m + 3s(cid:10)(cid:10)(cid:3)(cid:10)t ∈ V c(cid:10)(cid:3)(cid:10)t ∈ V ckmin =kmax =where σ (t, c) is the score of feature t ∈ V c w.r.t. category c, and m and s are the average and standard deviation of thescoring values, respectively.∗c (k) of candidate positive featuresWe notice that the above definition is essentially aimed at selecting a good set Pos∗∗c (k) – see Definition 4.1). Indeed, to determine kminc (k) consists of terms co-occurring with terms in Pos(recall that Neg(resp. kmax) we compute the scoring function σ for all features, and then count the number of features whose score ishigher than 1 (resp. 3) standard deviations above the average, i.e., features with high discriminating power.7.2. Individual encodingGiven a hypothesis space M(Fc(k), P , N), a candidate (minimal) classifier Hc = (cid:5)Pos, Neg, T (cid:6) ∈ M(Fc(k), P , N) is encodedby a bit string I = (cid:5)I+, I−, I(cid:2)τ (cid:6), where:1. the positive component I∗ti ∈ Posc (k). A ‘1’ or ‘0’ in the gene I+∗is used to encode Pos ⊆ Posc (k). It is made of k bits, each associated with a candidate featureis used to encode Neg ⊆ Neg∗+[ti], 1 (cid:2) i (cid:2) k, indicates whether or not ti ∈ Posc (k) belongs to Pos.∗c (k). It is made of k bits, each associated with a candidate−[ti], 1 (cid:2) i (cid:2) k, indicates whether or not the i-th candidate feature ti−∗c (k). A ‘1’ or ‘0’ in the gene I2. The negative component Ifeature ti ∈ Negbelongs to Neg.V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9577(cid:2)τ3. The threshold component Iis used to encode the threshold set T . The encoding of T relies on a straightforwardbinary representation of all pairs (p, n) ∈ T , with 0 (cid:2) p (cid:2) P and 0 < n (cid:2) N. One additional bit for each element (p, n)is Min(P + 1, N)((cid:24)log(N(P + 1))(cid:25) + 1),is used to represent presence/absence of that element. Thus, the length of Iwhere Min(P + 1, N) is the maximum order of a classifier with threshold bounds P and N (see Proposition 4.10). In thefollowing we will denote by enc the encoding function, i.e., I(cid:2)τ = enc(T ).(cid:2)τIt turns out that the length L(I) of I is the following function of k, P and N(cid:8)(cid:7)(cid:20)(cid:8)(cid:7)(cid:7)(cid:8)(cid:7)(cid:7)L(I) = L+I+ L−I+ L(cid:2)τI= 2k + Min(P + 1, N)logN(P + 1)(cid:8)(cid:21)(cid:8).+ 1Clearly, individuals encoding classifiers in the same hypothesis space M(Fc(k), P , N) are of equal length.Example 7.1. Let the hypothesis space M(Fc(k), P , N) be given, where k = 50, P = 2 and N = 3. According to Proposi-tion 4.10, the maximum order of a classifier is Min(P + 1, N) = Min(3, 3) = 3 (see also Example 4.10). Thus, an individualencoding a classifier (cid:5)Pos, Neg, T (cid:6) ∈ M(Fc(k), P , N) consists of 2k = 100 bits needed to represent sets Pos and Neg, and fur-ther Min(P + 1, N)((cid:24)log(N(P + 1))(cid:25) + 1) = 15 bits to encode the threshold set T .7.3. FitnessThe performance measure used for evaluating the fitness of an individual is the objective function of the learning prob-lem (see Definition 6.1).Definition 7.2 (Fitness). We are given a chromosome I , encoding classifier Hc , and the training set T . The fitness of I isF (Hc, T ).7.4. Task-specific GA operators and stochastic refinementNext we propose some application-specific reproduction operators as an implementation of the refinement operatorsdefined in Section 5. Such operators provide a concrete means whereby the learning algorithm selectively searches thehypothesis space. In particular, we next define two classes of Generalizing/Specializing (GS) operators: GS Crossover and GSMutation.7.4.1. Generalizing/specializing crossoverCrossover is the operation of swapping genetical material between two individuals (parents). GS crossover (GSX) isa special kind of crossover aimed at making a classifier more general or more specific.The GSX operators we are defining are an application of the binary refinement operators given by Definition 5.2. As wehave seen, they combine two classifiers of the same hypothesis space and provide a new classifier in the same space. Thus,GSX operators combine two parents belonging to the same subpopulation (i.e., encoding classifiers in the same hypothesisspace) and yields an individual in the same subpopulation. Therefore, they operate on individuals of equal length (and iso-morphic).Notation. With a small abuse of notation, in the following we will denote byencoding the classifiers(1 (cid:2) i (cid:2) 2). Further, we write I1 (cid:4)x I2 if H1cc , H2x(H1(cid:4)x H2c .x(I1, I2) the individualsc ), respectively, where x ∈ {τ , φ} and I i is the binary encoding of Hicx(I1, I2) andc ) andc , H2x(H1(cid:18)(cid:16)(cid:16)(cid:18)Definition 7.3 (GSX operators). We are given individuals I1 and I2 encoding classifiers H1The generalization crossover GX(I1, I2) of I1 and I2 is the individual encoding either the binary φ-generalizationc and H2or the binary τ -generalizationτ (I1, I2) with probability p = 0.5,φ(I1, I2) otherwise.c . More precisely, using the above agreed notationGX(I1, I2) =c ) of H1(cid:15) (cid:16)(cid:16)τ (H1c , H2c , H2(cid:16)c∈ M(Fc(k), P , N), respectively.c , H2c )φ(H1(cid:16)The specialization crossover operator SX(I1, I2) is defined accordingly (i.e., using(cid:18)(cid:16)x in place ofx, with x ∈ {τ , φ}).Based on Definition 5.2, the implementation ofoperations (OR and AND) on I1 and I2 as follows:(cid:16)φ(I1, I2) and(cid:18)φ(I1, I2) can be achieved by simple bitwise logical(cid:16)(cid:18)••Here Iφ(I1, I2) = I s.t. Iφ(I1, I2) = I s.t. I++ = OR(I1 , I+ = OR(I+ = AND(I+1 , I+1 , I+2 ), I+2 ), I− = AND(I− = OR(I−1 , I−1 , I+[ti] = OR(I−2 ), I−2 ), I+1(cid:2)τ = I(cid:2)τ = I(cid:2)τ1 .(cid:2)τ1 .+2 ) stands for ∀i ∈ [1, k], I[ti], I+2[ti]) (AND is defined accordingly).78V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95Fig. 6. Relevance ρ(t) and irrelevance ¯ρ(t) functions. Terms t0, . . . , tn are in increasing order of scoring value.However, there is a problem with the above implementation. In fact, by performing a “blind” OR or AND, the twoindividuals exchange 0’s and 1’s with no regard for the relevance of the features they represent. This may be detrimental,as a surplus of low-quality features may increase the risk of overfitting the training data. To overcome this drawback,we introduce the probabilistic OR (pOR) and the probabilistic AND (pAND), which are logical operators biased towards high-quality features. They both rely on the notion of relevance of a candidate feature.Definition 7.4. Given the feature space (cid:5)Posative candidate terms, respectively (see Definition 4.1). With each t ∈ Posas follows:∗c (k), Neg∗c (k)(cid:6), let σ and η be the scoring functions for the positive and the neg-∗(k) we assign the relevance measure ρ(t)∗(k) ∪ Negρ(t) =f (t)Max{ f (ti)|ti ∈ S} ,∗where f (t) = σ (t) and S = Posc (k) if t is candidate positive feature,3 or f (t) = η(t) and S = Negwe define the irrelevance ¯ρ(t) of t as¯ρ(t) = Min{ f (ti)|ti ∈ S}f (t)∗(t) otherwise. Dually,where f and S are as above. Clearly, 0 < ρ(t) (cid:2) 1 takes on the value 1 for the highest scoring term t, while 0 < ¯ρ(t) (cid:2) 1takes on the value 1 for the lowest scoring term t (see Fig. 6).Definition 7.5 (Probabilistic logical operators). Given two individuals I1 and I2, and a feature t, define the probabilistic OR asfollows:(cid:7)pOR(cid:8)I1[t], I2[t]=(cid:15)OR(I1[t], I2[t]) with probability p = ρ(t),I1[t]alternatively,where ρ(t) is the relevance of t (see Definition 7.4). The pAND operator is defined accordingly, using ¯ρ(t) in place of ρ(t).We note that neither operators are commutative. An important property of pOR is that pOR(I1[t], I2[t]) (cid:2) OR(I1[t], I2[t])i.e., pOR(I1[t], I2[t]) may be 0 while OR(I1[t], I2[t]) is not, but not vice versa. In particular, pOR(I1[t], I2[t]) =holds,OR(I1[t], I2[t]) when either I1[t] = 1 or I1[t] = I2[t] = 0. Otherwise,I1[t] = 0 and I2[t] = 1, pOR(I1[t], I2[t]) =OR(I1[t], I2[t]) = 1 with probability p = ρ(t). Clearly, the higher the relevance ρ(t) (recall that ρ(t) = 1 when t is thehighest scoring term), the higher the probability that a 1 is “moved” from I2 to I1 (at phenotypic level, this means that theclassifier encoded by I1 acquires a new feature t from the classifier encoded by I2). Thus, the overall effect of pOR is thatof “moving” preferably the most relevant features from I2 to I1. The pAND operator works in a dual way. That is, the effectof pAND is that of “discarding” from I1 (by moving zeroes from I2 to I1) preferably the least relevant features.i.e.,Now, by using the above probabilistic logical operators in place of the standard ones, we compute an “approximation” of(cid:16)(cid:18)bothφ andφ as follows.(cid:16)(cid:18)(cid:23) COMPUTATION of ≈φ(I1, I2) as follows:φ(I1, I2) and ≈(cid:18)φ(I1, I2). Given the individuals I1 and I2, compute ≈(cid:16)φ(I1, I2) and ≈(cid:16)(cid:18)• ≈• ≈φ(I1, I2) = I s.t. Iφ(I1, I2) = I s.t. I+ = pOR(I+ = pAND(I+1 , I+1 , I+2 ), I+2 ), I− = pAND(I− = pOR(I−1 , I−1 , I−2 ), I−2 ), I(cid:2)τ = I(cid:2)τ = I(cid:2)τ1 .(cid:2)τ1 . (cid:2)(cid:18)It can be easily verified that ≈φ(I1, I2) is a generalization of I1 and ≈φ(I1, I2) a specialization of I1.(cid:16)3 We assume that σ (t) > 0 for any t ∈ Pos∗(t).V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9579(cid:16)(cid:18)Unlike the implementation of the φ-subsumption primitivesat the genotype level, the implementation of the τ -subsumption primitivesTo see this point, we preliminarily recall that(cid:16)φ , which relies on bit-wise operations performedτ is performed at phenotype level.c ) = (cid:5)Pos1, Neg1, (cid:19)(T1, T2)(cid:6) (see Definition 5.2). Thus, to implementτ (I1, I2), we first extract from the individuals I1 and I2 the threshold sets T1 and T2 by using the inverse of the en-) (1 (cid:2) i (cid:2) 2). Then, we compute (cid:19)(T1, T2) by using the algorithm of Fig. 2 and, finally,τ (I1, I2) is the individual having the same positive and negativecoding function, i.e., Ti = encwe apply the encoding function enc((cid:19)(T1, T2)). Therefore,components of I1 and threshold component enc((cid:19)(T1, T2)).τ (H1c , H2τ andφ and−1(I(cid:2)τi(cid:16)(cid:16)(cid:18)(cid:16)(cid:23) COMPUTATION of(cid:16)(cid:18)τ (I1, I2) andτ (I1, I2). Let I1 and I2 be two individuals, and let Ti = enc−1(I(cid:2)τi) be the thresh-(cid:16)(cid:18)old set of the classifier encoded by I i (1 (cid:2) i (cid:2) 2). Then compute− = I+ = I•− = I+ = I•where (cid:19)(T1, T2) and (cid:18)(T1, T2) are constructed by the algorithm of Fig. 2. (cid:2)τ (I1, I2) = I such that Iτ (I1, I2) = I such that I(cid:2)τ = enc((cid:19)(T1, T2)),(cid:2)τ = enc((cid:18)(T1, T2))−1 , I−1 , I+1 , I+1 , IThe correctness of the above computation directly follows from Definition 5.2.7.4.2. Generalizing/specializing (GS) mutationThe GS mutation (GSM) operators are an implementation of the unary refinement operators defined in Definition 5.1.Therefore, the GSM applied to an individual encoding Hc returns another individual encoding a neighbor (generalization orspecialization) of Hc in either one of the two hierarchies.Notation. In the following we will denote, with a small abuse of notation, by ↑x (I) (resp. ↓x (I)) the individual encodingthe classifiers in ↑x (Hc) (resp. ↓x (Hc)), where x ∈ {φ, τ } and I is the encoding of Hc (see Definition 5.1).Definition 7.6 (GSM operators). Let I be an individual encoding Hc ∈ M(Fc(k), P , N). Thegeneralization mutation GM (I) of Iis an individual encoding a direct ancestor of Hc in M(Fc(k), P , N) either in the τ - or in the φ-generalization hierarchy, i.e.,either GM(I) =↑τ (I) or GM(I) =↑φ (I). More precisely,(cid:15)GM(I) =↑φ (I) with probability p = 0.5,↑τ (I) otherwise.The specialization mutation SM(I) is defined accordingly.That is, the GS mutation of I yields an individual encoding, with equal probability, a neighbor of the classifier encodedby I either in the φ- or the τ -hierarchy (this is exactly what in our model is a “small” change in a hypothesis).The computation of the non-deterministic primitives ↑φ (I) and ↓φ (I) is clearly the transposition at genotype level ofthe computation of the unary refinement operators ↑φ (Hc) and ↓φ (Hc) shown in Section 5.1. Hence, we obtain the binaryencoding ↑φ (I) of a classifier ↑φ (Hc) simply by flipping either one 0 into 1 in I(i.e., add a positive feature to Hc ) or a 1(i.e., remove a negative feature from Hc ). Dually, we get the binary encoding ↓φ (I) of a classifier ↓φ (Hc) byinto 0 in Iflipping a 1 into 0 in Ior a 0 into 1 in I−++−.However, like in the case of the GS crossover, we bias the GM mutation towards high-relevance features. To this end,we introduce the notions of insertion probability ip(t) and removal probability rp(t) of a candidate feature t as followsip(t) =ρ(t)(cid:14)i=1,k ρ(ti),rp(t) =¯ρ(t)(cid:14)i=1,k¯ρ(ti)where ρ(t) and ¯ρ(t) are the relevance and the irrelevance measures of t, respectively (see Definition 7.4). Intuitively, theprobability ip(t) represents the chance that I[t] is flipped from 0 to 1, that is, the chance that the candidate feature t isselected as a term (either positive or negative) for the classifier encoded by individual I . The meaning of rp(t) is dual.We notice that, since ρ(ti) (cid:2) 1, the conditioni=1,k ρ(ti) (cid:2) k holds (recall that k is the number of both positive andi=1,k ρ(ti) (cid:3) 1/k,negative features). Therefore, for the highest scoring feature t (for which ρ(t) = 1) we have that ip(t) = 1/i.e., the maximum insertion probability is not smaller than 1/k (defined in [45] as the lower bound of the optimal mutationrate). Dually, the removal probability rp(t) is maximum for the lowest scoring feature t, for which the relation rp(t) (cid:3) 1/kholds as well. We are now ready to provide the computation of ↑φ (I) and ↓φ (I).(cid:14)(cid:14)(cid:23) COMPUTATION of ↑φ (I) and ↓φ (I). Let the individual I be given. Compute ↑φ (I) and ↓φ (I) as follows:(a) ↑φ (I): select randomly (with probability 0.5) either one of the options below:1. probabilistically select a bit I+[t] = 0 according to the insertion probability distribution ip(t); mutate it from 0to 1, or2. probabilistically select a bit I−[t] = 1 according to the removal probability distribution rp(t); mutate it from 1 to 0.80V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95(b) ↓φ (I): select randomly (with probability 0.5) either one of the options below:+[t] = 1 according to the removal probability distribution rp(t); mutate it from 1−[t] = 0 according to the insertion probability distribution ip(t); mutate it from 01. probabilistically select a bit Ito 0, or2. probabilistically select a bit Ito 1. (cid:2)(cid:18)(cid:16)τ andNow let us consider the τ -subsumption primitives ↑τ and ↓τ . Like in the case previously seen of the τ -subsumptionτ , also the implementation of ↑τ and ↓τ is performed at phenotype level. To this end, we firstprimitives(cid:2)τ ). Then,extract from the individual I the threshold set T by using the inverse of the encoding function, i.e., T = encwe compute ↑ T by using the algorithm of Fig. 4 and, finally, we apply the encoding function enc(↑ T ). Therefore, ↑ T isthe individual having the same positive and negative components of I and threshold component enc(↑ T ).−1(I(cid:23) COMPUTATION of ↑τ (I) and ↓τ (I). Let the individualI be given, and let T = enc(cid:2)τ . Now, ↑τ (I) is implemented simply by replacing the encoding Iencoded by Iany direct ancestor ↑ T computed by the algorithm of Fig. 4. ↓τ (I) is implemented accordingly. That is:(cid:7) (cid:2)τ = enc(↑ T ).• ↑τ (I) = I(cid:7) (cid:2)τ = enc(↓ T ). (cid:2)• ↓τ (I) = I, where I, where I(cid:7) − = I(cid:7) − = I(cid:7) + = I(cid:7) + = I, and I, and I, I, I+−+−(cid:7)(cid:7)−1(I(cid:2)τ ) be the threshold set(cid:2)τ of T by the encoding enc(↑ T ) of7.5. The genetic algorithmFirst, the dimensionality range [kmin, kmax] is computed from the input vocabulary by applying Definition 7.1. Then,given the (user-defined) input values P max and Nmax, for each randomly generated triple (k, P , N), with k ∈ [kmin, kmax],0 (cid:2) P (cid:2) P max and 0 < N (cid:2) Nmax, a random number (> 1) of individuals of length 2k + Min(P + 1, N)((cid:24)log(N(P + 1))(cid:25) + 1) iscreated. Each of such individuals encodes a classifier in the hypothesis space M(Fc(k), P , N). The set of individuals createdfor the same triple (k, P , N) form a subpopulation S(k, P , N). Each individual I ∈ S(k, P , N) is initialized as follows: the k bits(cid:2)τ is randomly set to a (minimal) threshold set {(p1, n1), . . . , (pr, nr)}in Isuch that 0 (cid:2) pi (cid:2) P and 0 < ni (cid:2) N, for each i = 1, r (see Definition 4.2). Afterwards, evolution takes place by iteratingelitism, selection, crossover and mutation, until a pre-defined number of generations is created. Finally, the phenotype ofthe best generated chromosome is returned.are set to 1 with probability 0.5, while Iand I+−Next we give some details about selection, crossover and mutation.Selection. We want to be able to preserve subpopulations under the pressure of selection, in order to guarantee a certaindegree of population diversity (niching methods are often used for this purpose [46,47]). At the same time, we want to avoidpremature convergence within subpopulations that consist of a small number of individuals. At these aims, we maintain aset of mating pools, each being the union of all subpopulations with the same threshold bounds P and N, i.e., M(P , N) =(cid:6)i S(ki, P , N). So, individuals in M(P , N) may have different length, while belonging to isomorphic τ -subsumption lattices.In particular, the lengths of two individuals in M(P , N) may differ only as far as the feature components are concerned, thethreshold components being of equal length (as they have the same threshold bounds P and N – see Section 7.2). Now,selection is performed as follows: a mating pool is randomly selected, and tournament selection is then applied over itsindividuals.Crossover. We are given individuals I1 ∈ S(k1, P , N) and I2 ∈ S(k2, P , N), thus belonging to the same mating poolM(P , N). The GAMoN crossover of I1, I2 combines a slightly modified version of the uniform crossover (called MUX) withthe GS crossover operators defined in the previous sections. A sketch of the proposed method is shown in Fig. 7. It basicallyrelies on two steps:Step 1: Decide probabilistically whether or not MUX(I1, I2) takes place (line 18). This decision is made positively with(user-defined) probability px. MUX is an adaptation of the uniform crossover UX to deal with (i) the different lengths ofthe feature components of two mating individuals, and (ii) the presence of threshold sets. Informally, MUX(I1, I2) can beregarded as UX(I1, I2) where (1) only the first min(k1, k2) bits of the positive and negative components of I1 and I2 are(cid:2)probabilistically exchanged (lines 2–4), and (2) I2 are swapped as if they were single bits (line 5). Note that theoffspring J 1 and J 2 are such that J 1 ∈ S(k1, P , N) and J 2 ∈ S(k2, P , N) (i.e., they belong to the same subpopulations of theirparents).(cid:2)1 and IStep 2: If the decision for MUX has not been made positively in Step 1, then perform the GS crossover by invokingfunction GSX(I1, I2) (line 20). This is executed with a probability equal to the F -measure of the classifier Hc encoded by I1(line 13). This way, we give fitter individuals a higher chance to generalize or specialize so as to allow them for furtherrefinement – see discussion in Section 7.7. Whether a generalization or a specialization of I1 is to be performed, dependson whether Hc is too specific or too general (line 14). However, to carry out GSX(I1, I2), individual I2 has preliminarilyto be “promoted” citizen of the subpopulation S(k1, P , N) of I1 as, by Definition 7.3, the GS crossover can be applied onlyto members of the same subpopulation. To this end, I2 is made of the same length of I1 by invoking function promote(line 15). The following two cases may arise (recall that the threshold set components of I1 and I2 are of equal length):• k1 (cid:2) k2. Only the first k1 bits of I+2 and I−2 are picked up (lines 7–8).V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9581/* copy the first min(k1, k2) positive and negative features from I2 to J */[i];+−and Jso that their length becomes k1 */JJJ−2[i],(cid:2)τ2+2−2(cid:2)τ = IJ 1 = I1,+[i] = I[i], J[i], J(cid:2)τ2) with probability 0.5;/* copy the threshold component of I2 into J */[i]) with probability 0.5;[i]) with probability 0.5;GAMoN Xover(I1, I2)Input: individuals I1 ∈ S(k1, P , N) and I2 ∈ S(k2, P , N); MUX probability px;Output: offspring J 1 ∈ S(k1, P , N) and J 2 ∈ S(k2, P , N)function MUX(I1, I2)J 2 = I2;1.2. for i = 1 to min(k1, k2) do+3.Swap( J1−Swap( J4.1(cid:2)τ, J5. Swap( J1return J 1,J 2.function promote(I2, generalize)6.;7. for i = 1, min(k1, k2) do+−[i] = I8.29. for i = k2 + 1, k1 doif generalize then10.+[i] = 0,11.12.return J .function GSX(I1, I2)13. with probability equal to Fmeasure(Hc) dogeneralize = (precision(Hc) > recall(Hc));14.15. (cid:22)I2 = promote(I2, generalize);16.17.return j.begin18. with probability px set (cid:5) J 1, J 2(cid:6) = MUX(I1, I2);19.20.return J 1,if generalize then J = GX(I1,(cid:22)I2)else J = SX(I1,(cid:22)I2)if MUX has not been performed thenJ 1 = GSX(I1, I2); J 2 = GSX(I2, I1)/* when k1 > k2 add further k1 − k2 bits to both J/* I2 becomes a citizen of S(k1, P , N) */J+[i] = 1,Jelse J−[i] = 0;−[i] = 1/* pad J/* pad JJ 2.J++−J,,with k1 − k2 0’s and 1’s, resp., */−Jwith k1 − k2 1’s and 0’s, resp. *//* GX or SX are performed according to Definition 7.3 *//* Hc is the classifier encoded by I1 */Fig. 7. Pseudocode for the GAMoN Xover.• k1 > k2. Both I+2 and I−2 are extended with further n = k1 − k2 bits. In particular, I+2 is padded with n 1’s (resp. 0’s) and−2 with n 0’s (resp. 1’s) if a generalization (resp. specialization) is to be performed (lines 9–12).IAt this point, either GX(I1, I2) or SX(I1, I2) is computed according to Definition 7.3 (lines 16–17). Once GSX(I1, I2) has beencarried out, GSX(I2, I1) is performed likewise (line 20). Again, the offspring J 1 and J 2 belong to subpopulations S(k1, P , N)and S(k2, P , N), respectively.Mutation. Mutation is performed by using a similar framework. This is a combination of a modified version of stan-dard mutation (denoted MSM), which takes into account threshold sets, with the GS mutation (GSM) operators previouslydefined. In particular, MSM(I) works as follows: first, it randomly decides (with probability 0.5) whether to operate overthe feature or the threshold component. In the former case, MSM(I) randomly flips the bits of Iwith prob-ability 1/(2k). In the latter case, MSM(I) replaces Iby the encoding of a randomly chosen neighbor of the threshold(contrary to GSM which selectively chooses either a direct ancestor or a direct descendant dependingset encoded by Ion whether generalization or specialization is to be performed, respectively). Note that, in all cases, MSM(I) causes smallchanges of position in the subsumption lattices. Now, GAMoN mutation works as follows (for each offspring):and I(cid:2)(cid:2)+−• Step 1: decide probabilistically whether or not GSM(I) takes place. This decision is made positively with probabilityF -measure(Hc), Hc being the phenotype of I .• Step 2: If the decision for GSM(I) has not been made positively in step 1, execute MSM(I).7.6. GAMoN time complexityIt is immediate to recognize that the cost of the task-specific reproduction operators is O (k), while the cost of thefitness computation is O (km), where k is the size of the feature space and m the number of examples in the training set(in fact, the evaluation of the fitness of an individual requires the evaluation of the number of candidate (both positiveand negative) features occurring in each document of the training set). Now, since the number of different features (words)occurring in the training set is asymptotically independent of m (as the lexicon is finite), irrespective of feature selection,k is (asymptotically) independent of m. Thus, technically, we have that O (km) = O (m), that is, the asymptotic behavior ofGAMoN is linear in the size of the training set. Quite obviously, for relatively small values of m (like those that characterizereal-life data sets), the practical complexity is O (km).82V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–957.7. Remarks on the proposed GAIndividual encoding. There are two basic approaches, according to whether a chromosome of the population is used torepresent a single rule or a rule set [9]. Within the former approach (i.e., “chromosome = one rule”) there are rule inductionGAs like XCS [27], SIA [28], COGIN [48]. In the second approach (i.e., the “chromosome = set of rules”), called Pittsburghapproach, a rule is used to code an entire classifier. GAssist [29], OlexGA [10] and BioHEL [12,13] fall in this category.From one side, the “chromosome = one rule” approach makes the individual encoding simpler, but the fitness of a geno-type may not be a meaningful indicator of the quality of the phenotype [49]. Further, under the competitive style of GA,there may be a conflict between individual and collective interests of the rules forming a classifier [9]. On the other side,the “chromosome = set of rules” approach requires a more sophisticated encoding of individuals, but the fitness providesa more reliable indicator [49]. Moreover, no conflict of interests can happen in this case, as competition occurs amongclassifiers (and not single rules).In our approach, an individual encodes a candidate classifier – so it falls in the class of Pittsburgh methods. Despite this,individual encoding is very simple and compact – 2k bits for the encoding of Pos and Neg (a few tens of bits altogether)and a handful of bits to encode the threshold set. Thus, GAMoN combines the advantages of both the above mentionedapproaches, i.e., the individual simplicity and compactness of the “chromosome = one rule” approach, along with the effec-tiveness of both the reproductive competition and the fitness function of the “chromosome = set of rules” approach.Search strategy. It is well known that standard reproduction operators are rather disruptive, in the sense that the offspringmay be very different from the parents. On one hand, this has the advantage of making unlikely the GA getting stuck intolocal optima but, on the other hand, the high degree of unpredictability in the generation of new candidate classifiers maymake the GA converge very slowly. In contrast, GS operators move hypotheses from one position to another in either oneof the two hierarchies in a controlled way, depending on the “state” of the current hypothesis. Such a search bias, however,forces a search strategy which may quickly converge to local optima.As we have seen, GAMoN combines the space search of a standard GA with that based on GS operators. The rationalebehind this choice is that of exploiting the latter to perform a selective search, and to compensate the selectiveness ofthis search by introducing a certain degree of diversity through the standard operators. In particular, GAMoN runs the GSoperators (both crossover and mutation) with increasing probability, this being defined as the F -measure achieved by anindividual I over the training set. This way, as the generations pass and the algorithm more and more approaches theoptimal solution, a more controlled search of the space is performed.8. Empirical investigation framework8.1. Machine learning algorithmsTo evaluate the GAMoN approach proposed in this paper, we focused on comparisons with other rule learning algorithms.To this end, we selected two rule induction GAs, namely, BioHEL and OlexGA, and two non-evolutionary algorithms, namely,C4.5 and Ripper. Further, we included in our study Platt’s Sequential Minimal Optimization (SMO) method for linear SVMtraining [16], as it is reported to be one of the best methods for text categorization.Our interest for OlexGA was that of assessing to what extent GAMoN is an effective extension (see Section 2). BioHELwas chosen as it is one of best performing GA-based methods. We are not aware of experimental results of BioHEL ontextual data sets. Finally, C4.5 and Ripper were selected as they are standard decision tree/rule learners widely used for textclassification.All the selected learning algorithms are implemented in Java, but not all are available on the same platform. In particular,GAMoN runs only on the Weka platform, while BioHEL is available only on the KEEL platform (KEEL – Knowledge Extractionbased on Evolutionary Learning – is a suite of machine learning software tools – [50]). C4.5, Ripper, SMO and OlexGA runon both platforms. For the purpose of our work, we used Weka (version 3.5.8) for all algorithms, but BioHEL.8.2. Data setsWe carried out our empirical work on 13 real-world data sets whose properties are summarized in Table 1. As we cansee, they span over a wide range of sizes, from a minimum of around 900 (Oh15) to a maximum of nearly 204,000 (market)documents. The rarest category has 51 documents (Oh0), while the most frequent has 85,440 documents (Market). Most ofthese datasets have been widely used in large scale text classification tasks, and are publicly available.Market is a data set of 203,926 documents extracted from Reuters Corpus Volume I (RCV1) [63]. R10 is the standardsubset of the Reuters-21578 Distribution 1.0 which consists of 12,897 documents and uses the 10 most frequent Top-ics categories [51]. Ohsumed is from the Ohsumed-233445 collection subset of MEDLINE database [52] and is made ofall 34,389 cardiovascular diseases abstracts out of 50,216 medical abstracts contained in the year 1991. The classificationscheme consists of the 23 cardiovascular diseases MeSH categories. Ohscale, Oh0, Oh5, Oh10 and Oh15 are other subsetsof Ohsumed-233445 [53]. Data sets SRAA and 20NG (20-newsgroups) are articles from newsgroups. In particular, 20NG isV.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9583Table 1Data set description.NameSourceOh15Oh5Oh0Oh10BlogsGenderOhscaleR1020NGOhsumedCade12SRAAODP-S22MarketOhsumed-233445Ohsumed-233445Ohsumed-233445Ohsumed-233445Blog author genderOhsumed-233445Reuters-2157820 newsgroupsOhsumed-233445Gerindo Proj.UseNetODPRcv1Originalformatarffarffarffarfftextarfftextcsvtextcsvtexttexttext#Doc#Feat#CatCat size91391810031050323211,16212,89718,84634,38940,98373,218107,262203,926310030123182323815,02611,46521,36359,90334,35969,47063,96625,06868,60410101010210102023124224Min53595152154870923762842762547968826,036Max1571491941651684162139649999611847341,35128,28685,440a collection of 18,846 newsgroup documents organized into 20 different categories. We used the version sorted by date,which does not include newsgroup-identifying headers. The SRAA [54] data set contains 73,218 articles from four discus-sion groups on simulated auto racing, simulated aviation, real autos, and real aviation. BlogsGender is a binary data set of3232 blogs used for author gender classification [55]. Cade12 is a subset of the CADE Web Directory consisting of 40,983web pages classified across 12 categories [56]. ODP-S22 is a subset of ODP (Open Directory Project) [57] whose documentsare stored as RDF files. For our experimentation, we used the subset of 107,262 documents classified under the categoriesof the Top/Science subtree, which has 25 first-level categories. We first collapsed each of the 25 subtrees into the respectiveroot, thus obtaining a flat structure made of 25 categories. Then, we grouped together into one category “Misc” the 4 small-est categories, namely, Search Engines (7 documents), Charts-and-forums (16 documents), Directories (27 documents) andEvents (38 documents), thus getting a set of 22 categories. From each document (web page), we extracted the title and thedescription (thus, discarding the URL).8.3. Experimental setupWe preliminarily pre-processed all data sets downloaded in textual format, by performing tokenization (word unigrams)and stopword removal. We used the bag-of-words representation with binary word weighting. Each feature was representedas a numerical attribute.Experiments were performed in a binary classification setting. To this end, we binarized all data sets by performingmulti-class to two-class conversion. This way, the m-class learning problem is decomposed into m independent two-classsub-problems, one for each class, with the i-th classifier separating class i from all the remaining ones.Finally, for each category, feature scoring by CHI square [39] was performed (on the training set).Following are two major issues arose during the design of experiments.1. Dimensionality of the feature space.(a) Unlike the other systems, GAMoN automatically detects the appropriate dimensionality of the feature space. That is,no manual feature selection is preliminarily needed. As we will see later on this section, the feature spaces selectedby GAMoN usually consist of a few tens of features.(b) Previous works show that systems like Ripper, C4.5 and SVM require relatively large vocabularies (usually a fewthousands of features) to learn good prediction functions.(c) The efficiency of OlexGA, like that of most evolutionary methods, strongly depends on the feature space dimen-sionality, as many features imply long individuals and, thus, low efficiency.(d) BioHEL represents an exception in the evolutionary landscape, as it was designed to efficiently deal with highdimensional data sets. However, the memory space limitations of the KEEL platform severely limits the number ofattributes that can actually be used in case of large data sets.The above observations demonstrate the difficulty of applying a single feature selection policy to all systems. In fact,it would be unfair using for the non-evolutionary methods the feature dimensionalities detected by GAMoN (too smallfor their characteristics – see points (a) and (b)). On the other hand, running OlexGA and BioHEL over the same numberof features used for the non-evolutionary methods would practically be unfeasible – see points (c) and (d).2. Time efficiency. As we have seen, our empirical study involves very large data sets (e.g., ODP-S22 and Market), on whichmost of the experimented systems perform quite inefficiently. In particular, Ripper and C4.5 showed to be extremelyslow on such data sets, especially when we tried to use large vocabularies (for an instance, on vocabularies of 10,000features, we had to stop C4.5 as it was overly inefficient). This prevented us from performing optimization over morevocabularies.84V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95Table 2Feature space dimensionality range [kmin, kmax], size of the feature space on which the “optimal” classifier has been found kopt and F -measure for categoriesof R10.kminkmaxkoptPRavgacq105172132corn164321crud58106100earngrain376041427755int5910268mon8614899ship539761trad7814186wheat18412186.6690.2087.5095.1892.0956.0568.1280.4169.1588.66Given the above premises, the following experimental design choices were finally taken:1. GAMoN was run with 500 individuals, 200 generations, elitism rate 0.2, MUX probability 0.6 (see Section 7.5). The max-imum threshold bounds were P max = Nmax = 4.2. OlexGA and BioHEL were executed over the same vocabularies made of 100 features. The former was run with thedefault parameters shown at http://www.mat.unical.it/OlexGA, while the latter with those provided by KEEL.3. The remaining (non-evolutionary) systems were all executed over vocabularies made of 2000 terms, with the defaultsettings provided by Weka. The SMO normalization option was turned off to improve the training time.Due to efficiency reasons, we performed 5-fold cross validation (80% training, 20% test) only on small data sets (from Oh15up to R10), while holdout (70% training, 30% test) was applied on the remaining data sets.8.4. Predictive performance measure and statistical testsPerformance was measured, as is common in text classification, by the arithmetic mean of Precision and Recall – denotedby PRavg (an approximation of the Precision/Recall Break-Even Point). To obtain global estimates over more categories,the standard definitions of micro-averaged Precision and Recall were used, notably:μPr =μRe =(cid:14)(cid:14)(cid:14),(cid:14)c∈C |TPc|c∈C(|TPc| + |FPc|)c∈C |TPc|c∈C(|TPc| + |FNc|),where TPc is the set of documents correctly assigned by the classifier to category c, FPc is the set of documents incorrectlyassigned by the classifier to category c, and FNc is the set of documents incorrectly not assigned by the classifier to cate-gory c. We note that micro-averaging gives equal weight to every document (it is called a document-pivoted measure) andis largely dependent on the most common categories.Each run of the evolutionary algorithms was repeated 3 times, and the average PRavg was taken.In order to make comparisons statistically significant, we performed the Iman–Davenport test, with the Holm’s post-hoctest, recommended for comparison of more classifiers on multiple data sets in [58].9. Experimental results9.1. A glimpse to M-of-N{¬,∨}hypothesesThe experimental results show that GAMoN has a bias towards learning compact and readable hypotheses. The followingare examples of classifiers induced for categories “corn”, “wheat” and “grain” from R10:(cid:2)(cid:3)(cid:4)(cid:5)Hwheat =(cid:2)Hcorn =Hgrain ={wheat}, {deficit, investment, net, treasury, york},(cid:4)(cid:5){corn, maize}, {london, money, quarter},(cid:2){barley, cereals, corn, grain, maize, rice, sorghum, wheat},{acquisition, bank, earning, pay, profit, tax, york},(1, 1)(1, 1)(cid:3)(cid:3),,(cid:4)(cid:5)(1, 1), (2, 2).As we can see, the former two classifiers are atoms, while the latter is a 2-order classifier (for a description see Section 3).It must be emphasized the high semantic correlation between the positive features and the respective categories.9.2. Automatic selection of the feature space dimensionalityTable 2 shows, for the categories from R10, the values of kmin, kmax and kopt given by one execution of GAMoN, wherekmin and kmax define the dimensionality range of the feature space, and kopt (kmin < kopt < kmax) is the size of the featureV.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9585Fig. 8. Distribution of features by CHI square for two categories from R10 – “corn” (left side) and “acq” (right side). Only first 200 features are shown.Fig. 9. Decision boundary of the classifier (cid:5)Pos, Neg, {(1, 1), (2, 2), (3, 3)}(cid:6) for category “earn” from R10. Each label (π (x, y), ν(x, y)) represents the numberπ (x, y) of positive examples and the number ν(x, y) of negative ones with x positive features and y negative ones. Labels (0, 0) are omitted from thefigure.space on which the “optimal” classifier has been found. As it can be seen, the learning of all categories generally relies onsmall sets of candidate features. As an example, for “corn” we have kmin = 19 and kmax = 43 (and PRavg equal to 90.20),meaning that the positive terms for the “best” classifier are to be found among the first k higher scoring features, with19 (cid:2) k (cid:2) 43. This is clearly indicative of an aggressive feature selection. To see why, let us have a look at Fig. 8 – left side,where the distribution of features by CHI square is reported. As we can see, “corn” has a few features scoring very high,while the remaining ones rapidly approach near-zero values. The sharply declining shape of this graph is indicative of an“easy” category, i.e., a category for which a high performance can be achieved with only a few discriminative words.In contrast, “acq” is a more “difficult” category. As we can see from Fig. 8 – right side, it has lower initial CHI squarevalues, and the graph has a smooth (decreasing) trend. That is, no features with highly discriminative power there exist.As a consequence, the dimensionality range is shifted rightwards on the x-axis (kmin = 105 and kmax = 172), this beingindicative of a less aggressive reduction of the feature space.9.3. Decision boundariesFig. 9 shows the decision boundary DBHearn of the 3-order classifier Hearn = (cid:5)Pos, Neg, {(1, 1), (2, 2), (3, 3)}(cid:6) for category“earn” from R10. Here, Pos is made of 14 positive features and Neg consists of 16 negative features. As we can see, DBHearnis a three-step polyline. From the data reported in Fig. 9, it results that the subset of documents classified by Hearn consistsof the 2954 positive examples and the 113 negative ones lying in the classification region RHearn delimited by DBHearn .The generalization error and the PRavg value of Hearn areerr =b + ca + b + c + d= 0.031;PRavg = 2a2 + a + ac2(a + b)(a + c)= 0.9486V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95Table 3Micro-averaged PRavg values on each data set obtained by GAMoN and GAMoN* (a version of GAMoN with no GS reproduction operators). Legend –BG: BlogsGender, OhS: Ohscale, Ohsu: Ohsumed, Mkt: Market.GAMoNGAMoN*Oh1580.4677.59Oh584.8582.55Oh084.2782.90Oh1078.6275.54BG68.8268.33OhS75.0374.28R1086.5084.5620NG75.7275.21Ohsu67.2567.09cade48.4246.43SRAA85.2584.27ODP71.1470.20Mkt92.4290.59Table 4Micro-averaged PRavg results obtained by 5-fold cross-validation on Oh0, Oh5, Oh10, Oh15, BlogsGender, Ohscale and R10 (80/20 split) and by holdout onthe remaining data sets (70/30 split).DatasetOh15Oh5Oh0Oh10BlogsGenderOhscaleR1020NGOhsumedcadeSRAAODPMarketavg microPRavgavg rankRipper79.5583.7484.3778.8260.9672.9685.2172.6660.3544.3181.0466.7394.6374.263.96SMO79.9584.2984.8074.7060.9569.5288.9483.6466.9454.0690.0680.6595.5678.002.08C4.576.7582.3079.2474.7858.3370.7784.6774.8663.2548.1086.8574.7695.3774.623.62OlexGABioHELGAMoN74.3380.7681.2974.4666.7574.3684.0772.9765.5844.1079.6069.4688.9573.594.3166.0376.7273.2067.3962.8268.2683.5970.6563.7942.9681.3471.2174.7569.445.2380.4684.8584.2778.6268.8275.0386.5075.7267.2548.4285.2571.1492.4276.832.08where a, b, c and d are computed as follows (see Section 4.5):a =c =(cid:11)(x, y)∈RH(cid:11)hearn(x, y) /∈RHhearnπ (x, y) = 2954,b =π (x, y) = 205,d =(cid:11)(x, y)∈RH(cid:11)ν(x, y) = 113,hearnν(x, y) = 6988.(x, y) /∈RHhearn9.4. Effect of GS operatorsTo see the effect of the GS reproduction operators defined in this paper, we compared the accuracy results of GAMoNover the data sets previously seen with those obtained by running a version of GAMoN where the GS operators (GS Xoverand GS mutation) were disabled. The experimental results, reported in Table 3, show that the GS operators improve theaccuracy on every single data set, even though on some they induce only trivial improvements (e.g., Ohsu and BG), whileon other the gain is remarkable (e.g., Oh15, Oh10, R10, cade and Mkt). On average, the enhancement over all data sets is ofaround 1.5 points. As discussed earlier in this paper, the aim of GS operators is that of further refining fitter individuals byexploiting the structure of the hypothesis space. This explains why often significant improvements are obtained.9.5. Comparison with other systemsTable 4 shows, for each algorithm and data set, the micro-averaged PRavg. The average values over all data sets alongwith the average ranking of each algorithm, are also included (bottom of the table). The best results are stressed in bold-face. The ranking is obtained by assigning a position to each algorithm depending on its performance on each data set.The algorithm showing the best accuracy on a given data set is assigned rank 1.As we can see, SMO is the best performer (PRavg = 78.00), followed by GAMoN (PRavg = 76.83). The two algorithms,however, show the same average rank (2.08). We note that GAMoN outperforms all the other rule induction methods.In particular, it behaves uniformly better than OlexGA on all individual data sets, and compares favorably with the otherrule learners on most of the data sets.In order to establish whether the above differences in performance are statistically significant, the Iman–Davenport’s testis applied. This is a non-parametric statistical test recommended in [58] for comparing two or more classifiers on multipledata sets. In brief, with 13 data sets, 6 algorithms and confidence α = 0.05, the Iman–Davenport statistics is 9.54, greaterthan the critical value CV = 2.37. Thus, the null hypothesis (which states that all the algorithms are equivalent) is rejected.Hence, we apply the Holm’s post-hoc test [58], with GAMoN as control algorithm, for controlling the family-wise errorin multiple hypothesis testing. The results of this test are summarized in Table 5. Based on them, we can reject the nullV.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9587Table 5Holm’s test with GAMoN as control algorithm. The null hypothesis is rejected when p-value < α/i.i54321MethodBioHELOlexGARipperC4.5SMOz = (R0−Ri )SE−4.2980−3.0400−2.2014−2.09660.0000p-value0.00050.00240.02780.03661.0000α/i0.010.01250.01670.0250.05Table 6Avg size of the rule-based classifiers on R10.AlgorithmGAMoNRipperC4.5BioHELOlexGAAvg size of classifiers#Pos = 20, #Neg = 10, order = 1.8#Rules = 16#Rules = 78#Rules = 14, #literals/rule = 19#Pos = 16, #Neg = 15Table 7Learning times expressed in hours. Each run of GAMoN, OlexGA and BioHEL was repeated 3 times.Overall learning time (h)# runsAvg learning time per category (h)Ripper4453951.13C4.54883951.23SMO713950.18OlexGABioHELGamon4611850.0418511850.1615611850.12hypothesis of equivalence only for BioHEL and OlexGA (as p-value < α/i holds). That is, with confidence 95%, we can statethat GAMoN performs better than such algorithms, while it is statistically equivalent to SMO, Ripper and C4.5.9.6. Size of the classifiersApart from SMO, the other classifiers yield models as sets of rules. Although we do not have a unique formal definitionof size of a classifier, being either the number of rules, number of features, etc., in Table 6 we provide some statistical data(averaged over the five folds) giving an insight into the quantitative characteristics of the classifiers induced on R10. As wecan see, Ripper, C4.5 and BioHEL induce classifiers consisting on average of 16 rules, 78 rules and 14 rules, respectively,each BioHEL rule having 19 literals on average. In turn, GAMoN induces classifiers of 20 positive features and 10 negativeones on average, against the 16 positive features and 15 negative ones of OlexGA classifiers. Going beyond the results givenin the table, nearly 44% of the classifiers induced by GAMoN are atoms, 38% are of order 2, 18% of order 3 and 2% of order 4(note that with P = N = 4, the maximum order of a classifier is Min(P + 1, N) = 4 – see Proposition 4.10).9.7. Time efficiencyThe experiments previously described were performed on an Intel Xeon 2.33 GHz machine with 4 Gb RAM.The learning times needed to achieve the accuracy results previously seen are reported for each method in Table 7 –first row. As we can see, OlexGA (46 hours) is the best performers, followed by SMO (71), GAMoN (156), BioHEL (185),Ripper (445) and C4.5 (488) (recall that each run of GAMoN, OlexGA and BioHEL was repeated 3 times). Table 7 also reportsthe average learning times per category. Again, OlexGA is the fastest algorithm (0.04 h/category), followed by GAMoN (0.12),BioHEL (0.16) and SMO (0.18). Ripper and C4.5 are ten times slower than GAMoN (1.13 and 1.23, respectively).To see the effect of the training set size over learning times, in Fig. 10 we plotted the average learning times per categoryover each data set (data sets are ordered by increasing size). The graph provides an empirical picture of the progression oflearning times with the number of training documents. As we can see, GAMoN asymptotically behaves similarly to OlexGA,BioHEL and SMO, while its has a significantly smoother trend than both Ripper and C4.5. That is, GAMoN scales better thanthe two non-evolutionary rule induction methods.10. Discussion and related workThe experimental study described in the previous sections shows that GAMoN induces classifiers that are both accurateand compact. Interestingly, these properties have consistently been observed over all 13 data sets, on which GAMoN showeda uniform behavior. Given the very different application domains the corpora refer to, this is a clear proof of robustness.Further, GAMoN showed to perform efficiently on large data sets.M-of-Nrepresentation. As discussed in Section 4.6, the “family resemblance” metaphor provides us with a quali-tative understanding on the basic reason why the M-of-N paradigm is well suited for the purpose of text categorization.{¬,∨}88V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95Fig. 10. Comparison of the average learning time per category over each data set (data sets are ordered by increasing size).{¬,∨}extends M-of-N through negation and disjunction, two constructs that enables to express hypotheses capableM-of-Nof best fitting the true structure of the data (a discussion on the expressivity of M-of-Nhas been reported in Sec-tion 4.6). Unlike most of the existing classifiers which focus on features that positively discriminate a class, in our approachnegation is used as a “first class citizen” allowing us to explicitly model the interactions between positive and negativefeatures within a given example. In turn, disjunction enables to “modulate” such interactions, by capturing the positive cor-relation (that simple atoms would miss) existing between positive and negative features (see Section 4.5). Negation takesprecision under control, while disjunction improves recall.{¬,∨}{¬,∨}{¬,∨}One advantage of the proposed language over a DNF-type representation is conciseness. Indeed, although an M-of-Nhypothesis can be represented in terms of disjunctions of conjunctions (see Section 4.2), a DNF-type representationof an M-of-Nconcept would be prohibitively long (the number of disjuncts is exponential in the size of Pos and Neg).We believe that the proposed language is one main contribution of this paper. One interesting direction for future workcould be that of using a mathematical relationship (e.g., a linear function) between thresholds p and n, instead of thecurrent threshold multiple pairs.Feature space. M-of-Nhypotheses are built over a set of pre-selected candidate features. While there has been a longhistory of applying dimensionality reduction methods, one contribution of this paper is represented by an original definitionof the feature space, as consisting of both terms indicative of membership and terms indicative of non-membership fora category. Unlike in the traditional feature selection approach, where only positive terms are selected, using our definitionenables the learner to focus on negative information in the same way as it does with positive one (a similar approachdistinguishing between positive and negative features was proposed in [59]). A criterion for the automatic detection ofa suitable dimension has also been provided (see Definition 7.1). This criterion proved to be very effective in practice.Experimental results showed indeed that a few tens of well-selected features are sufficient to build accurate predictionfunctions, irrespective of the data set.{¬,∨}GAMoN biases. Apart from the language bias, we can characterize GAMoN in terms of both a search bias and an overfittingavoidance bias [18]. We have extensively discussed about the former, which refers to the way the hypothesis space issearched through the subsumption relationships by means of the task-dependent genetic operators. The proposed approach,actually not new in inductive learning (see, e.g., [30–35]), overcomes a major problem in the use of conventional GAswhich do not take into account the structure of the search space. The overfitting avoidance bias is a preference for simplerclassifiers. GAMoN includes such a bias in the induction mechanisms by using suitable feature probability distributions(see Section 7.4) which enable the reproduction operators to select few, high-quality features. In combination with theproposed feature selection technique, which provides GAMoN with an effective lexicon, capable of expressing the essentialpatterns, the overfitting avoidance bias guarantees the induction of classifiers that are parsimonious, made of a handful ofwell-selected features. This makes them effective on the unseen data, as few high-quality features drastically reduce the riskof overfitting the training data.V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9589GAMoN, C4.5 and Ripper. Unlike GAMoN, the two rule-based non-evolutionary classifiers used in this work, notably, C4.5and Ripper, achieve the full expressive power of DNF. Despite this, the conducted experimental study showed that they donot outperform GAMoN. This is a clear proof of the effectiveness of the proposed algorithm.In addition, GAMoN performs significantly more efficiently than both C4.5 and Ripper on large data sets, as the graphsof Fig. 10 show. This should not be surprising, as the time complexity of Ripper is O (m log2 m) and that of C4.5 O (m3),while the complexity of GAMoN is O (m), where m the number of examples in the training set (see Section 7.6). That is,GAMoN can scale up to large and realistic real-world domains better than the other two rule-based classifiers. We pointout that further improvements of the learning times may be obtained by, e.g., a more efficient implementation of the task-specific reproduction operators and, in a real application environment, by distributed approaches. Current research is likelyto further improve efficiency of GAMoN.{¬,∨}GAMoN, OlexGA and BioHEL. GAMoN is a substantial extension of OlexGA from two respects. The first is the language.atom where both thresholds are equal to 1, i.e., (cid:5)Pos, Neg, (1, 1)(cid:6)An OlexGA hypothesis is the special case of an M-of-Nis strictly more expressive than the language of OlexGA). The second is the genetic algorithm. Since the(thus, M-of-Nhypothesis space of OlexGA does not provide any structure, OlexGA relies on a simple, standard GA, where the populationis made of fixed-length individuals, and the reproduction operators are the standard uniform crossover and mutation. Thatis, OlexGA is a special case of GAMoN. As shown in Table 4, the proposed extension results in a statistically significantimprovement over OlexGA. Needless to say, the price for that is a slower learning procedure.{¬,∨}BioHEL (Bioinformatics-oriented Hierarchical Evolutionary Learning) is a state-of-the-art GA which showed to performvery effectively on non-textual data sets [60]. To the best of our knowledge, this is the first study where BioHEL has beentested on text classification problems. BioHEL inherits several features from GAssist. It relies on the Pittsburgh represen-tation approach and applies the iterative rule learning approach [28]. BioHEL was explicitly designed to handle large-scaledatasets. To this end, a rule, instead of coding all the domain attributes, keeps only a subset of them, thus avoiding hun-dreds of irrelevant computations. Using such an approach, BioHEL is able to handle problems with hundreds of attributes(in datasets with large sets of instances [13]) or even tens of thousands of attributes (but with few instances). In addition,in order to further reduce the computational cost, BioHEL uses a windowing scheme called ILAS (incremental learning withalternating strata). The experimental results of this paper confirm that BioHEL behaves quite efficiently, with a learningtime similar to that of GAMoN (see Table 7). On the contrary, in terms of predictive accuracy, it showed to be statisticallyinferior to GAMoN. However, we feel that better results could be obtained by a finer tuning of the system. For an instance,a recent publication [62] shows that BioHEL has a parameter which is highly problem sensitive, the coverage breakpoint.Also, an appropriate use of the ILAS windowing scheme, as well as the usage of the C++ implementation4 (in place of theKEEL implementation), could further improve efficiency.Other systems learning with negation. As already mentioned, using negative evidence is deemed important in the textclassification task. However, apart from OlexGA and GAMoN, none of the experimented systems focuses on the exploitationof negative information. In general, examples of IRL (Inductive Rule Learning) approaches that involve the direct generationof negation are very rare (see, e.g., [61,38]). Outside the realm of rule learners, Complement Naive Bayes (CNB) is amongthe few text classifiers that leverage negative features [21]. Its peculiarity is that of learning the weights for a class usingall training data not in that class. CNB works in a multi-class setting (i.e., it needs at least 3 classes). In [21], the authorsclaim that CNB approaches the state-of-the-art accuracy of SVMs. Unfortunately, we could not compare GAMoN with CNBin our empirical study, as the (binary) one-versus-all technique was used to deal with multi-label classification (basically,the problem was that Weka does not provide support for a multi-label data set representation that would be necessary inorder to provide CNB with the same input of the other systems).Other M-of-N approaches. Several research works have recently been done to develop methods for inducing M-of-N con-cepts but, to the best of our knowledge, none for text categorization. For an instance, in [1] a technique for extractingM-of-N hypotheses from neural networks is reported. However, most work in this field has been carried out for construc-tive induction. ID-2-of-3 [2] is an M-of-N induction algorithm which incorporates M-of-N tests in decision-tree learning.It is based on a (greedy) hill-climbing approach to get the best M-of-N hypotheses at each node of a decision tree. XofN [4]is another greedy constructive induction algorithm that learns X -of-N nominal attributes. Both ID-2-of-3 and XofN, whenbuilding a decision tree, construct a new attribute for each decision node using the local training set. More recently, a Ge-netic Algorithm for constructive induction has been proposed in [7]. It relies on a variable length individual representationencoding the set of N attribute-value pairs composing an X -of-N attribute. The fitness is defined as the information gainratio of the constructed attribute. The genetic operators are the standard uniform crossover along with a mutation which isa simple variant of the standard one. A conventional niching method to foster population diversity is also used.11. ConclusionsIn this paper we proposed a new language, called M-of-Nfor constructing M-of-Nhypotheses from training data.{¬,∨}{¬,∨}, for text classification, along with a GA-based approach4 Available at http://icos.cs.nott.ac.uk/software/biohel.html.90V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95{¬,∨}The M-of-Nrepresentation generalizes the classical notion of M-of-N concepts by allowing negation and disjunction.We conjectured that it is well-suited to express text classification conditions, as it complies with the so-called “familyresemblance” metaphor. We have shown that the space of M-of-Nhypotheses has a structure determined by twokinds of subsumption relationships – the feature and the threshold relationships, that form complete lattices. Based on that,suitable refinement operators for an effective exploration of the hypothesis space were designed.{¬,∨}To induce M-of-Nhypotheses, the task-specific genetic algorithm GAMoN was proposed. It is based on the Pittsburghapproach, where an individual encodes a candidate classifier, as well as on ad hoc GS reproduction operators which area stochastic implementation of the refinement operators. GAMoN dynamically adapts the probability of selecting the GSoperators. The population is partitioned into a number of competing subpopulations, each consisting of individuals belongingto the same hypothesis subspace. To this end, a statistical criterion for automatically detecting the dimensionality range ofthe feature space has been proposed.{¬,∨}This paper also presented empirical results obtained by extensive experiments on 13 real-world test collections in a widespectrum of sizes – from a few hundreds to a few hundreds thousands of documents. We found that GAMoN is competitivewith a large collection of state-of-the-art learning techniques belonging to different classes, and that it provides hypothesesthat are compact and easily interpretable. In particular, though there are small differences in predictive accuracy betweenGAMoN and SMO (the latter being a bit more performant), and between GAMoN and both Ripper and C4.5 (the latter twobeing a bit less performant), all such systems showed to be statistically equivalent. Whereas, GAMoN proved to be superiorto the other evolutionary algorithms. In particular, it showed statistically significant improvements over its predecessorOlexGA, thus confirming the effectiveness of the proposed extension. Finally, we observed that, as we scale up the size ofthe data set, GAMoN performs much more efficiently then both Ripper and C4.5.AcknowledgementThe authors wish to thank Giuseppe Manco for his helpful comments on earlier versions of this paper.Appendix A. Proofsc and H2c ). The proof proceeds by induction. (Basis) H1c be two classifiers in HT (Fc(k)). Next we show that H1Proof of Proposition 4.1. Let H1c and H2D(H2and (cid:5)Pos2, Neg2, {(p, n)}(cid:6). A document d is classified by H2is a set of features – see Section 4.1). It can be easily seen that, since both Pos2 ⊆ Pos1 and Neg1esis, |d ∩ Pos1| (cid:3) p and |d ∩ Neg1two generic classifiers (cid:5)Pos1, Neg1, T (cid:6) and (cid:5)Pos2, Neg2, T (cid:6) in HT (Fc(k)) such that H1in the following form: H1cH2,1= (cid:5)Pos2, Neg2, T1(cid:6) and H2,2ccany document classified by H2,1that H1c ) ⊇c are atoms in HT (Fc(k)) of the form, say, (cid:5)Pos1, Neg1, {(p, n)}(cid:6)| (cid:2) n (recall that a documentc iff |d ∩ Pos2| (cid:3) p and |d ∩ Neg2⊆ Neg2 hold by hypoth-c arec . Thus, they can be expressed= (cid:5)Pos1, Neg1, T2(cid:6),⊆ Neg2 hold,. It turns out= (cid:5)Pos2, Neg2, T2(cid:6). By inductive hypothesis, since both Pos2 ⊆ Pos1 and Neg1and any document classified by H2,2is classified by H1,2is classified by H1,1c ). (cid:2)| (cid:2) n is verified as well, that is, d is classified by H1c classifies all documents classified by H2= (cid:5)Pos1, Neg1, T1(cid:6), H1,2c . (Inductive step) H1c implies D(H1c , i.e., D(H1, where H1,1c ) ⊇ D(H2c and H2and H2c(cid:4)φ H2(cid:4)φ H2= H2,1= H1,1∨ H1,2∨ H2,2ccccccccccccc , H2c ) (cid:4)φ Hc ) (cid:4)φ H1c , H2(cid:7)c and, further, Hc , H2c and lubφ(H1c ) = (cid:5)Pos1 ∪ Pos2, Neg1c ) (cid:4)φ H2(cid:7)cProof of Proposition 4.2. Next we show that (HT (Fc(k)), (cid:4)φ) is a complete lattice. To this end, we first prove∩ Neg2, T (cid:6). From Definition 4.4 it immediately follows that bothstatement (a) – lubφ(H1= (cid:5)Pos, Neg, T (cid:6) suchc , H2lubφ(H1(cid:7)c , H2that lubφ(H1c we have that Pos ⊆ Pos1 ∪ Pos2c cannot hold, as Pos1 ⊆ Pos and Pos2 ⊆ Pos(see Definition 4.4). However, if so, the conditions Hcannot be both true (a contradiction). From which statement (a) follows. Now we prove statement (b) – glbφ(H1c ) =(cid:5)Pos1 ∩ Pos2, Neg1(cid:4)φ glbφ . Now let us assume, by ab-(cid:7)surd, the existence of H(cid:7)(cid:7)c and H2(cid:4)φ Hc . Fromcc(cid:7)(cid:4)φ HHc cannot hold,as Pos is not a subset of both Pos1 and Pos2 (a contradiction). From which statement (b) follows. (cid:2)∪ Neg2, T (cid:6). By Definition 4.4 we have that H1= (cid:5)Pos, Neg, T (cid:6) such that Hc ) it turns out that Pos1 ∩ Pos2 ⊆ Pos. But, if so, the conditions H1c hold. Now let us assume, by absurd, the existence of H(cid:7)c ) and, further, H1c(cid:4)φ H(cid:4)φ glbφ and H2c , H2c . From lubφ(H1(cid:7)(cid:4)φ H2c and Hc(cid:4)φ H(cid:7)c and H2(cid:7)(cid:4)φ H2c(cid:4)φ H1c and H(cid:7)c(cid:4)φ glbφ(H1(cid:4)φ glbφ(H1c ) (cid:4)φ H(cid:4)φ H1c , H2c , H2(cid:7)c(cid:7)ccccccc ) ⊇ D(H2c be two classifiers in HΦ (P , N). Next we show that H1c and H2Proof of Proposition 4.3. Let H1c ). The proof proceeds by induction. (Basis) H1D(H1(cid:5)Pos, Neg, {(p2, n2)}(cid:6). By Definition 4.2, a document d is classified by H2Now, H1cfied by H1an atom H1, jclassified by H2,icc ) ⊇ D(H2c such that H1, j(inductive hypothesis), it follows that H1(cid:4)τ H2c as well, i.e., D(H1appearing in H1c ). (Inductive step) H1impliesc are atoms of the form, say, (cid:5)Pos, Neg, {(p1, n1)}(cid:6) andc if (and only if) |d ∩ Pos| (cid:3) p2 and |d ∩ Neg| (cid:2) n2.c is classi-c there existsclassifies all documentsc . (cid:2)appearing in H2c only if p1 (cid:2) p2 and n1 (cid:3) n2 (by Definition 4.5), which implies that a document d classified by H2(cid:4)τ H2c only if, for each atom H2,i(immediate from Definition 4.5). Since H1, jc classifies all documents classified by H2c and H2(cid:4)τ H2,i(cid:4)τ H2ccccccccProof of Lemma 4.1. Next we prove that, given Hc = H1cc )) and (D(H1c ) or D(H2only if (D(H1c ) ⊇ D( ˆH1c ) ⊇ D( ˆH1∨ H2c ) ⊇ D( ˆH2c and ˆHc = ˆH1c ) or D(H2∨ ˆH2c ) ⊇ D( ˆH2c , the following holds: D(Hc) ⊇ D( ˆHc)c )). The proof proceeds by induction.cV.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9591= {(p1, n1)}, H2c= {(p2, n2)} and ˆHc = {( ˆp, ˆn)} are atoms. By Definition 4.2, D(Hc) = {d ∈ D s.t. |d ∩ Pos| (cid:3)(Basis) H1cp1 ∧ |d ∩ Neg| (cid:2) n1 ∨ |d ∩ Pos| (cid:3) p2 ∧ |d ∩ Neg| (cid:2) n2} and D( ˆHc) = {d ∈ D s.t. |d ∩ Pos| (cid:3) ˆp ∧ |d ∩ Neg| (cid:2) ˆn}. Thus, D(H) ⊇ D( ˆHc)ˆHc andonly if either (1) ˆp (cid:3) p1 and ˆn (cid:2) n1 or (2) ˆp (cid:3) p2 and ˆn (cid:2) n2. By Definition 4.5, condition (1) entails H1cc ) ⊇ D( ˆHc). (Inductive step) D(H) ⊇ D( ˆHc)condition (2) H2conly if D(H1c ) only if (by inductivehypothesis) D(H1ˆHc , so as D(H) ⊇ D( ˆHc) only if D(H1c ) ⊇ D( ˆH1c ) ∪ D(H2c ) ⊇ D( ˆHc) or D(H2c ) ⊇ D( ˆH1(cid:4)τc ) ∪ D(H2c ) only if D(H1c ) and D(H1c ) ⊇ D( ˆH2c ) ∪ D( ˆH2c ) ∪ D(H2c ). (cid:2)c ) and D(H1c ) ⊇ D( ˆH2c ) ⊇ D( ˆH1c ) ⊇ D( ˆH1c ) ⊇ D( ˆH2c ) or D(H2c ) or D(H2(cid:4)τc . The proof proceeds by induction. (Basis) Hc = {(p, n)} and H(cid:7)cc be two classifiers in HΦ (P , N). We next show that D(Hc) ⊇ D(H(cid:7)= {(p(cid:7), nProof of Proposition 4.4. Let Hc and H(cid:7)Hc (cid:4)τ H(cid:7){d ∈ D s.t. d ∩ Pos| (cid:3) p ∧ |d ∩ Neg| (cid:2) n} and D(H(cid:7)if p (cid:2) ponly if D(H1) ∪ D(H2) ⊇ D(H(cid:7)D(H(cid:7)if Hc (cid:4)τ H(cid:7)2) ∨ D(H2) ⊇ D(Hc . (cid:2), that is, only if Hc (cid:4)τ H(cid:7)1) ∪ D(Hand n (cid:3) n(cid:7)(cid:7)c) = {d ∈ D s.t. d ∩ Pos| (cid:3) p(cid:7) ∧ |d ∩ Neg| (cid:2) n2)) only if (by inductive hypothesis) H1 (cid:4)τ H(cid:7)c . (Inductive step) Let Hc = H1 ∨ H2 and H(cid:7)2) only if (by Lemma 4.1) (D(H1) ⊇ D(H(cid:7)1 or H2 (cid:4)τ H(cid:7)(cid:7)(cid:7)c) implies(cid:7))} are atoms. By Definition 4.2, D(Hc) =(cid:7)}. Clearly, D(Hc) ⊇ D(H(cid:7)c) only2. Now, D(Hc) ⊇ D(H(cid:7)c)1)) ∧ (D(H1) ⊇2 or H2 (cid:4)τ H(cid:7)2 only∨ H(cid:7)1) ∨ D(H2) ⊇ D(H(cid:7)1 and H1 (cid:4)τ H(cid:7)= H(cid:7)1cProof of Corollary 4.1. We next prove that, given classifiers H1c iff D(H1iff H1cD(H2c ) and D(H2c ) ⊇ D(H2c and H2(cid:4) H2c ). (cid:2)(cid:4) H1cc and H2c , H1c≡ H2c iff D(H1c ) = D(H2c ). Indeed, H1cc ) ⊇ D(H1c ) (by Proposition 4.3 and Proposition 4.4) iff D(H1≡ H2cc ) =Proof of Proposition 4.6. Next we show that, given Hc, ˆHc ∈ HΦ (P , N), the classifier and(Hc, ˆHc) is such that (1)and(Hc, ˆHc) ∈ HΦ (P , N), (2) D(and(Hc, ˆHc)) = D(Hc) ∩ D( ˆHc), and (3) Hc (cid:4)τ and(Hc, ˆHc) and ˆHc (cid:4)τ and(Hc, ˆHc).The proof proceeds by induction. (Basis) Hc = {(p, n)} and ˆHc = {( ˆp, ˆn)} are atoms. Statement (1). To show that and(Hc, ˆHc)is in HΦ (P , N) it suffices to observe that both p = Max{p, ˆp} (cid:2) P and n = Min{n, ˆn} (cid:2) N hold. Statement (2). A document dis classified by and(Hc, ˆHc) iff d contains x (cid:3) Max{p, ˆp} positive features and y < Min{n1, ˆn} negative features, iff x (cid:3) p,x (cid:3) ˆp, y < n and y < ˆn, iff d is classified by both Hc and ˆHc , i.e., D(Hc) = D(Hc) ∩ D( ˆHc). Statement (3). Immediate from∨ H2Statement 1 and Proposition 4.4. (Inductive step) Let Hc = H1c be two classifiers. Statement (1). FromcDefinition 4.7, and(Hc, ˆHc) = H1 ∨ H2 ∨ H3 ∨ H4, where H1 = and(H1c ), H3 = and(H2c ) andc ). By the inductive hypothesis, H1, H2, H3, H4 are in HΦ (P , N) and thus, by Definition 4.2, and(Hc, ˆHc)H4 = and(H2is in HΦ (P , N). Statement (2). By using the inductive step of Definition 4.7, along with the inductive hypothesis of State-ment (2), we get D(and(Hc, ˆHc)) = D(H1c ), from whichD(and(Hc, ˆHc)) = D(H1c ) immediately follows. Statement (3). By using the inductive step of Definition 4.7, and ap-(cid:4)τ H2plying the inductive hypothesis of Statement (3), we have that H1cand ˆH2(cid:4)τ H4c )). Thus, by Definition 4.5, it follows that both Hc (cid:4)τ and(Hc, ˆHc) and ˆHc (cid:4)τ and(Hc, ˆHc) hold(as H4 = and(H2(that is, and(Hc, ˆHc) is more specific than both Hc and ˆHc ). (cid:2)(cid:4)τ H1 (as H1 = and(H1c )), H2c , ˆH1c )), H1(cid:4)τ H4 and ˆH2(cid:4)τ H3 (as H3 = and(H2(cid:4)τ H2 (as H2 = and(H1(cid:4)τ H3 and ˆH1c and ˆHc = ˆH1c ), H2 = and(H1(cid:4)τ H1 and ˆH1c ) ∩ D( ˆH1c ) ∩ D( ˆH1c ) ∩ D( ˆH2c ) ∩ D( ˆH2c ) ∪ D(H2c ) ∪ D(H2c ) ∪ D(H1c ) ∩ D(H2c )), H2c , ˆH2c , ˆH1c , ˆH2c , ˆH2c , ˆH1c , ˆH2c , ˆH1∨ ˆH2ccccccccProof of Proposition 4.5. Let Hc = (cid:5)Pos, Neg, T (cid:6) be given. We next show that the following properties hold:1. Hc is redundant iff there exist (pi, ni), (p j, n j) ∈ T such that {(pi, ni)} (cid:4)τ {(p j, n j)}.2. Hc is minimal iff T = {(p1, n1), . . . , (pr, nr)} is such that pi < p j and ni < n j , or vice versa, for each i, j ∈ [1, r].3. If Hc = H1cc , then Hc ≡ H1c .c and H1(cid:4)τ H2∨ H2cH(cid:7)c∨ · · · ∨ H j−1∨ · · · ∨ Hr= H1c(1) Let Hc = H1cc , where H(cid:7)(cid:4) H jc(2) From point 1 above, Hcc . By Definition 4.5, {(pi, ni)} (cid:4)τ {(p j, n j)} iff Hi∨ · · · ∨ Hiciff for each pair (pi, ni), (p j, n j) ∈ T neither {(pi, ni)} (cid:4)τ {(p j, n j)} nor{(p j, n j)} (cid:4)τ {(pi, ni)} iff neither (pi (cid:2) p j and ni (cid:3) n j ) nor (p j (cid:2) pi and n j (cid:3) ni ) (by Definition 4.5) iff pi < p j and ni < n j ,or vice versa.(3) H1cc ) (by Proposition 4.3) only if D(Hc) = D(H1c , iff Hc is redundant (by Definition 4.6).∨ H j+1ccis minimalc , with i, j ∈ [1, r], iff Hc = H(cid:7)c ) only if Hc ≡ H1c only if D(H1c ) = D(H1c ) ⊇ D(H2c ) ∪ (H2∨ · · · ∨ Hr(cid:4)τ H2c and(cid:4) H j∨ H jccc(by Proposition 4.4). (cid:2)Proof of Proposition 4.7. Next we show that any equivalence class into which is partitioned the hypothesis subspaceHΦ (P , N) by the relation ≡ has a unique minimal classifier. To this end, we prove that, if Hc and ˆHc are two minimal clas-sifiers such that Hc ≡ ˆHc , then Hc = ˆHc . From which the statement immediately follows. The proof proceeds by induction.(Basis) Hc and ˆHc are atoms. Trivial. (Inductive step) Hc = H1c . Note that, from the minimality of Hcc follows. By Corollary 4.1, Hc ≡ ˆHc iff D(Hc) = D( ˆHc) iff D(Hc) ⊇ D( ˆHc)c , ˆH1and ˆHc , the minimality of H1c ) ⊇ D( ˆH2and D(Hc) ⊆ D( ˆHc). By Lemma 4.1, D(Hc) ⊇ D( ˆHc) only if D(H1c ) or D(H2c )or D(H2c ) orc ). Likewise, D(Hc) ⊆ D( ˆHc) only if D(H1c and ˆHc = ˆH1c ) and D(H2c ) and D(H1c ) ⊆ D( ˆH1c ) ⊆ D( ˆH1c ) ⊆ D( ˆH2c ) ⊇ D( ˆH2c ) or D(H1c ) ⊇ D( ˆH1c ) ⊇ D( ˆH1c and ˆH2c , H2∨ ˆH2∨ H2cc92V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95D(H2D(H2(2) H1cc ) ⊆ D( ˆH2c ) = D( ˆH2≡ ˆH2c ). It turns out that, because of the minimality of H1c ) = D( ˆH1c ) and D(H2c ) or (2) D(H1c . By the inductive hypothesis, Hic ) = D( ˆH2≡ ˆH1c and H2ccc , H2c , ˆH1c and ˆH2c , either (1) D(H1≡ ˆH1c ) = D( ˆH1c and H2c ) andc or≡ ˆH2cc ), only if (by Corollary 4.1) either (1) H1= ˆH j≡ ˆH jc , from which Hc = ˆHc . (cid:2)c only if HiccProof of Proposition 4.8. The statement we are going to prove is that the poset (MΦ (P , N), (cid:4)τ ), where MΦ (P , N) is theset of the minimal classifiers in HΦ (P , N), is a complete lattice. To this end, let us consider two classifiers H1c inMΦ (P , N). We first show that lubτ (H1c and H2∨ H2c ,i.e., Hc (cid:4)τ H1c ) hold. On the other hand,c ) ∪ D(H2∨ H2D(Min(H1c ),cfrom which the statement lubτ (H1Now we prove that glbτ (H1(cid:4)τ Hc and H2cc , H2c ). Let Hc ∈ MΦ (P , N) be a τ -generalization of both H1c ) and D(Hc) ⊇ D(H2i.e.,c ) ⊇ D(Hc) hold. On the other hand,c ))) ⊇ D(Hc). Therefore, by Proposi-c ), so that D(Hc) ⊇ D(Min(H1c , H2c ) follows.c , H2c , H2c )). Let Hc be a τ -specialization of both H1c )). Therefore, by Proposition 4.4, Hc (cid:4)τ Min(H1c . Thus, by Proposition 4.3, both D(Hc) ⊇ D(H1c ) by Proposition 4.6, so that D(Min(and(H1c ) ⊇ D(Hc) and D(H2c , H2(cid:4)τ Hc . Thus, by Proposition 4.3, both D(H 1c ) = Min(H1∨ H2c ) = Min(and(H1c and Hc (cid:4)τ H2c )) = D(H1c ) = Min(H1c and H2c ,c ) ∩ D(H2c and H2c , H2∨ H2∨ H2ccccc )) (cid:4)τ Hc , from which the statement glbτ (H1c , H2c ) = Min(and(H1c , H2c )) follows. (cid:2)H1cD(Min(and(H1tion 4.4, Min(and(H1c ))) = D(H1c , H2∨ H2Proof of Proposition 4.9. We first prove lubτ (H1c ) =Min(H1It is immediate to recognize that this classifier isc(cid:5)Pos, Neg, (cid:19)(T1, T2)(cid:6) (see Fig. 2), as function (cid:19)(T1, T2) simply minimizes T1 ∪ T2 by discarding all thresholds (p j, n j) suchthat there exists (pi, ni) such that {(pi, ni)} (cid:4)τ {(p j, n j)} holds (see Proposition 4.5 – Part 1).c ) = (cid:5)Pos, Neg, (cid:19)(T1, T2)(cid:6). By Proposition 4.8,c ) = Min((cid:5)Pos, Neg, T1 ∪ T2(cid:6)).c ), that is,lubτ (H1lubτ (H1c , H2c , H2c , H2Now we show that glbτ (H1c , H2c ) = (cid:5)Pos, Neg, (cid:18)(T1, T2)(cid:6). By Proposition 4.8, we have that glbτ = Min(and(H1c , H2c )). Nextwe show that Min(and(H1c )) = (cid:5)Pos, Neg, (cid:18)(T1, T2)(cid:6). To this end, we observe that lines 9–12 of Fig. 2 are the iterativeversion of the inductive definition of and(H1c are atoms, then the functioncomputes (cid:18)(T1, T2) = {(Max(p1, p2), Min(n1, n2))}, which coincides with the base step of Definition 4.7 (of course, the clas-sifier (cid:5)Pos, Neg, {(Max(p1, p2), Min(n1, n2))}(cid:6) is minimal, being an atom). Now, let us consider the general case. It is easyto see that the inductive step of Definition 4.7 generates, for each couple of pairs (p1, n1) ∈ T1 and (p2, n2) ∈ T2, a pair{(Max(p1, p2), Min(n1, n2))}. And this is exactly what function (cid:18)(T1, T2) does at lines 9–12. Thus, after the two nested “for”c , H2have been carried out (lines 10–12), the classifier and(H1c ) is generated. However, this classifier may not be minimal(see Example 4.8), so that function Minimize is invoked. So, we finally get (cid:5)Pos, Neg, (cid:18)(T1, T2)(cid:6) = Min(and(H1c ) (Definition 4.7). Indeed, if both H1c and H2c , H2c , H2c , H2c )). (cid:2)+k= {p1, . . . , pk} and TProof of Lemma 4.2. Given threshold bounds P and N, along with k (cid:2) Min(P + 1, N), let T={n1, . . . , nk} be sets of integers, where ∀i (cid:2) k, 0 (cid:2) pi (cid:2) P and 0 < ni (cid:2) N. Next we show that there exists a unique subset−++k having size k which is a minimal threshold set. Without loss of generality, we assume that ∀pi, p j ∈ T× TS ⊆ Tk andk−∀ni, n j ∈ Tk , such that i < j, both pi < p j and ni < n j hold.Uniqueness. We show that any another subset S(cid:7)Existence. The set S = {(p1, n1), . . . , (pi, ni), . . . , (pk, nk)} is a subset of T−k of size k where, for each pair of elements(pi, ni) and (p j, n j), with i < j, both pi < p j and ni < n j hold. Thus, by Proposition 4.5 – Part 2, S is a minimal threshold set.+−k , which is a minimal threshold set, has size lower thankis a minimal threshold set, by Proposi-, either ps < pi and nt < n j or pi < ps and n j < nt . Now, it is immediate to recognize+−k , the elements ps ∈ Tk such that s < i (i.e.,such that j < t (i.e., greater than nt ) are k − j. It turns out thatsuchk. Suppose that (pi, n j) ∈ Stion 4.5 – Part 2, for any (ps, nt) ∈ Sthat, by the above assumption on the ordering of the elements of Tsmaller than pi ) are i − 1, while the elements nt ∈ T(1) there are at most i − 1 elements (ps, nt) ∈ Sthat j < t. That is, the size of S(cid:7) (cid:12)= S of Tis such that i < j (the case j < i is likewise). Since Ssuch that s < i, and (2) there are at most k − j elements (ps, nt) ∈ Sis at most i + k − j < k. (cid:2)+k and T× T× T+k−k(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)−kIn fact, given T =Proof of Proposition 4.10. Part 1. Every classifier in MΦ (P , N) is of order r (cid:2) Min{P + 1, N}.{(p1, n1), . . . , (pr, nr)}, from Part 2 of Proposition 4.5 we have that pi (cid:12)= p j and ni (cid:12)= n j for all i, j ∈ [1, r], i (cid:12)= j. That is,in T there appear r different positive thresholds and r negative thresholds. Since 0 (cid:2) pi (cid:2) P and 0 < ni (cid:2) N, for eachi ∈ [1, r], it turns out that r (cid:2) P + 1 and r (cid:2) N, i.e., r (cid:2) Min{P + 1, N}.Part 2. Given P , N and s = Min{P + 1, N}, let us consider the sets T)is the set of possible values for the positive (resp. negative) thresholds appearing in the classifiers of MΦ (P , N). Now,−⊆ T∀r ∈ [1, s], there exist binom(P + 1, r) subsets T.−r orAlso, from Lemma 4.2 we know that, for each pair of sets T+. It turns out that there are binom(P + 1, r) × binom(N, r) minimal threshold sets oforder r constructible from Torder r. Therefore, since r (cid:2) Min{P + 1, N}, the total number of threshold sets in MΦ (P , N) is that given by Eq. (1). (cid:2)−r , there is a unique minimal threshold set S ⊂ T) and binom(N, r) subsets T+r+ = {0, 1, . . . , P } and T(made of r elements from T− = {1, . . . , N}. T−r× T(resp. T+r , Tand T⊆ T+r+−++−Correctness of the computation of ↑φ (Hc). – See Section 5.1. We restrict the proof to the correctness of the computationof ↑φ (Hc). The proof concerning ↓φ (Hc) follows a similar framework.Let us start by proving ↑φ (Hc) = (cid:5)Pos(cid:7) ⊇ Pos and Neg(cid:7)(cid:7), T (cid:6) such that ↑φ (Hc) (cid:4)τ H(cid:7), T (cid:6), where Pos(cid:7) = Neg. First of all we note(cid:7) = Neg, ↑φ (Hc) (cid:4)φ Hc holds – see Definition 4.4. Now assume by absurd the existence(cid:7)(cid:7) ⊆ Neg. However, since(cid:7) = Pos ∪ {t}, t ∈ Pos(cid:7)(cid:7) ⊇ Pos and Neg∗(k) and Neg(cid:4)τ Hc . Thus, both Pos(cid:7) ⊆ Neg(cid:7) ⊇ Pos(cid:7), Negthat, since both Pos(cid:7)(cid:7), Negof H(cid:7)(cid:7)= (cid:5)Posc(cid:7)(cid:7)cV.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9593and Pos differ for exactly one term, either Pos(cid:7)Posholds as well. That is, either H(cid:7)(cid:7)c= Hc or H(cid:7)(cid:7)(cid:7), Neg(cid:7)(cid:7)(cid:7) = Pos= ↑φ(Hc), a contradiction.(cid:7) = Pos, Negc(cid:7), T (cid:6), where Posor PosNow let us prove ↑φ (Hc) = (cid:5)Pos(cid:7) ⊆ Neg, ↑φ (Hc) (cid:4)φ Hc holds – see Definition 4.4. Now, by absurd, assume that there exists H(cid:7)(cid:7)(cid:7) = Neg \ {t} and t ∈ Neg. Since both Pos= (cid:5)Pos(cid:7)(cid:7) = Pos holds. Further, since Neg = Neg(cid:7), Neg(cid:7) = Neg(cid:7)(cid:7) = NegNegsuch that ↑φ (Hc) (cid:4)τ HNegH(cid:7)(cid:7)c(cid:7) ⊆ Neg= Hc or H(cid:7)(cid:7)c(cid:7)(cid:7)c(cid:7)(cid:7) ⊆ Neg and, further, Neg and Neg=↑φ (Hc), a contradiction.(cid:7)(cid:4)τ Hc . Since both Pos(cid:7) ⊇ Pos(cid:7)(cid:7)(cid:7) ⊇ Pos and Pos = Posdiffer for exactly one feature, either Neg, it follows that Pos(cid:7)(cid:7) = Neg or Neg(cid:7) = Pos and(cid:7)(cid:7), T (cid:6)(cid:7)(cid:7), Neg(cid:7)(cid:7) = Pos. Moreover, since(cid:7)(cid:7) = Neg. That is, eitherc(cid:7)(cid:18)Proof of Proposition 5.1. Next we show that, given classifiers H1H1(cid:4)xc(cid:4)x H1(cid:18)x(H1c ), where x ∈ {τ , φ}. By Definition 5.2, we have thatc , H2c , H2c , it turns out thatc , H2c ) follows.c . Dually, fromc ) (cid:4)x H1c , H2x(H1x(H1x(H1(cid:16)(cid:18)c and H2(cid:16)c , the following holds:c ) = lubx(H1x(H1c , H2c , H1,2c ) = glbx(H1) and H1cc , H1,2ccx(H1c , H2). Since lubx(H1c ) (cid:4)x H1c andc , H1,2)c), H1(cid:4)xc(cid:4)x glbx(H1c , H1,2c(cid:16)Proof of Proposition 6.1. We have to prove that the decision version of the GAMoN learning problem is NP-complete.The proof is by a reduction from the Knapsack problem. Given an atom Hc = (cid:5)Pos, Neg, {(p, n)}(cid:6), let S ⊆ T be the set oftraining documents classified by Hc under c, i.e., S = {d ∈ D s.t. |d ∩ Pos| (cid:3) p ∧ |d ∩ Neg| < n}. Precision is defined as theprobability that a document in S is also in the training set T c of c, i.e.,Pr(Hc, T ) =|S ∩ T c||S|and Recall is defined as the probability that a document in T c is also in S, i.e.,Re(Hc, T ) =|S ∩ T c||T c|.(A.1)(A.2)By replacing Eqs. (A.1) and (A.2) into Eq. (2), after some algebra, we get the following formulation of the objective functionF (Hc, T ) = 2 · ab + |T c|where a = |S ∩ T c| and b = |S \ T c|. Hence, to maximize F (Hc, T ) we want a to be as large as possible, while keeping bbound to some given value (note that |T c| is a constant). Thus, the problem of learning an atomic classifier, in its recognitionversion, can be formulated as follows:∗∗LEARN-ATOM-DECISION (LAD): Given the training set T , the feature space (cid:5)Posc (k)(cid:6) and two positive integers Uc (k), Neg∗∗and V , does there exist a hypothesis Hc = (cid:5)Pos, Neg, {(p, n)}(cid:6) over (cid:5)Posc (k)(cid:6) such that a (cid:3) U and b (cid:2) V ? That is,c (k), Negdoes there exist a hypothesis which is consistent with at least a positive examples and is not consistent with at most bnegative examples?Now KNAPSACK is the following NP-complete problem: Given 2n + 2 positive integers w 1, . . . , wn, v 1, . . . , vn, W and Z ,(cid:14)(cid:14)does there exist X ⊆ {1, . . . , n} such thati∈ X w i (cid:2) W andi∈ X v i (cid:3) Z ?We claim KNAPSACK polynomially reduces to LAD. To see this, suppose I = (w 1, . . . , wn, v 1, . . . , vn, W , Z ) is an instance∗∗of KNAPSACK. Make the following instance for LAD: (a) U = Z and V = W ; (b) (cid:5)Posc (k)(cid:6) = (cid:5){t1, . . . , tn}, ∅(cid:6), i.e.,c (k), Negthe feature space consists of n positive candidate features and no negative candidate feature; (c) the training set T is suchthat:(c.1) Θ(ti) ∩ Θ(t j) = ∅, for each ti, t j ∈ {t1, . . . , tn}, and(c.2) v i = |Θ(ti) ∩ T c| and w i = |Θ(ti) \ T c|, for each i ∈ [1, n](cid:14)where Θ(ti) denotes the set of examples (documents) in T where term ti occurs. From point (c.1) above it follows that eachdocument contains at most one positive candidate term. Further, from points (c.1) and (c.2) it turns out that, for a givenPos ⊆ {t1, . . . , tn}, the following holds: a =t∈Pos w i . Thus, LAD turns out to be the following problem:“does there exist Hc = (cid:5)Pos, ∅, {(1, ∗)}(cid:6) such thatt∈Pos w i (cid:2) C (the symbol “*” stands for “immaterial”,(cid:14)i∈ X w i (cid:2) C ?” Clearly, the an-as Neg = ∅)?” Or, equivalently: “does there exist X ⊆ {1, . . . , n} such thatswer to this LAD is “yes” iff I is an instance of KNAPSACK, then proving our claim. To conclude the proof it suffices to noticethat verifying a YES instance of LAD requires polynomial time. Hence, problem LAD, i.e., the problem of deciding whetherthere exists an atom satisfying the constraints a (cid:3) U and b (cid:2) V , is NP-complete. It is immediate to realize that the decisionversion of the learning problem (see Definition 6.1) is NP-complete as well.t∈Pos v i (cid:3) V andt∈Pos v i and b =i∈ X v i (cid:3) V and(cid:14)(cid:14)(cid:14)(cid:14)Appendix B. The non-deterministic function ↑TNext we give a description of the algorithm of Fig. 4. It creates a direct ancestor ↑T of T = {τ1, . . . , τk}, where τi =(pi, ni), 1 (cid:2) i (cid:2) k, by applying to T only local changes. We preliminarily recall that, by Proposition 4.5, the minimality of T94V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–95requires that pi < p j , ni < n j , or vice versa, for each i, j ∈ [1, k]. In the following discussion we assume pi < p j , ni < n j(read “τi smaller than τ j ”) if i < j.The algorithm starts by checking the condition T = {0, N}, that is, if T is the top element. Clearly, in such a caseno direct ancestor exists and the algorithm returns the empty set (line 12). Since the algorithm works on the “distance”between two elements of T , in order not to exclude the first element τ1 and the last one τk, two fictitious elements aredefined, namely, τ0 = (−1, 0) and τk+1 = (P + 1, N + 1) (line 2). Then, an element τi of T , along with one adjacent (left orright), are randomly selected at lines 14–17. Of course, if i = 1, i.e., τi = (pi, ni) is the smallest element of T , and pi = 0,then the adjacent of τi will be the right one, i.e., τi+1 (line 15). Symmetrically, if i = k, i.e., τk = (pk, nk) is the greatestelement of T , and ni = N (recall that N is the negative threshold bound), then the adjacent of τi will be the left one,i.e., τi−1 (line 16). Then, the function NewElement is invoked by passing τi and the selected adjacent adj (line 18). Thisfunction works as follows. First, it orders the element X = (px, nx) and its adjacent Y = (p y, n y) in such a way that X is thesmallest one (line 1). Then, the distances δ+between X and Y are computed (line 2). Now, there are two ways forconstructing an immediate ancestor of T : either (1) by adding a suitable element τ to T , or (2) by replacing an element τiof T by the most specific τ which generalizes τi . Which one of the two alternatives is applied depends on the distances δ+and δ−. In particular, if both distances are greater than one (line 3 – intuitively, this means that there is “enough room” inbetween X and Y to accommodate a new element in T ), then the most specific threshold pair (p, n) such that px < p < p yand nx < n < n y is computed, i.e., p = p y − 1 and n = nx + 1 (the most specific (p, n) is the one with the highest possible pvalue and the lowest possible n value). Then, ↑T is set to Minimize(T ∪ {(p, n)}) (line 18), where Minimize is the functionsketched in Fig. 2.and δ−As an example, let us consider the classifier Hc = (cid:5)Pos, Neg, T (cid:6), where T = {τ1, τ2}, τ1 = {(0, 1)} and τ2 = {(2, 3)},and assume that the threshold bounds are P = 2 and N = 3. Note that τ1 is smaller than τ2. Now, suppose that the algorithmat line 13 selects i = 2 (i.e., τ2 = (2, 3)). Since τ2 is the greatest element of T and n2 = N, the algorithm choses the leftadjacent, i.e., τ1 (line 15). Then, the function is invoked (line 18) and the distances δ+ = p2 − p1 = 2 and δ− = n2 − n1 = 2are computed (line 3). Since (δ+ > 1 and δ− > 1) holds (line 3), the function sets (p, n) = (p2 − 1, n1 + 1) = (1, 2) (line 4)and returns it to the main. The resulting threshold set is ↑ T = Minimize(T ∪ {(1, 2)}), that is, {(0, 1), (1, 2), (2, 3)}, whichis an immediate ancestor of T (see Fig. 1).If the condition on the distances at line 3 does not apply, then the most specific threshold pair (p, n) which generalizes. In particular, if δ+ > 1,either X or Y is computed. Again, this is done depending on the values of two distances δ+then the algorithm generates the most specific element (p, n) which generalizes Y , i.e., (p, n) = (p y − 1, n y). On the contrary,if δ− > 1, then the algorithm generates the most specific element (p, n) which generalizes X , i.e., (p, n) = (px, nx +1). Finally,if none of the above conditions hold (line 9), the algorithm generates the most specific element (p, n) which generalizesboth X and Y , i.e., (p, n) = (px, n y).As an example, assume that T = {τ1, τ2}, with τ1 = {(1, 1)} and τ2 = {(2, 2)}, and let i = 2 (i.e., τ2 = (2, 2)). Supposethat the chosen adjacent is the left one, i.e., τ1. Since δ+ = δ− = 1, none of the conditions at lines 3, 7 and 8 applies. Thus(p, n) = (p1, n2) = (1, 2) is computed at line 9 and returned to the main. This element is then added to T (line 18) and,after minimization, the algorithm returns ↑ T = {(1, 2)}, which is an immediate ancestor of T (see Fig. 1).and δ−References[1] R. Setiono, Extracting M-of-N rules from trained neural networks, IEEE Trans. Neural Netw. 11 (2001) 512–519.[2] P.M. Murphy, M.J. Pazzani, Id2-of-3: Constructive induction of M-of-N concepts for discriminators in decision trees, in: Proc. of the Eighth Int. Work-shop on Machine Learning, Evanston, IL, 1991, pp. 183–187.[3] G.G. Towell, J.W. Shavlik, Extracting refined rules from knowledge-based neural networks, Mach. Learn. 13 (1993) 71–101.[4] Z. Zheng, Constructing x-of-n attributes for decision tree learning, Mach. Learn. 40 (1) (2000) 35–75.[5] R. Setiono, S. Pan, M. Hsieh, A. Azcarraga, Automatic knowledge extraction from survey data: learning M-of-N constructs using a hybrid approach, J.Oper. Res. Soc. (2005) 3–14.[6] T. Joachims, Learning to Classify Text Using Support Vector Machines, Kluwer, 2002.[7] O. Larsen, A.A. Freitas, J.C. Nievola, Constructing X -of-N attributes with a genetic algorithm, in: Proc. of the Genetic and Evolutionary ComputationConference, Morgan Kaufmann, 2002, p. 1268.[8] V.L. Policicchio, A. Pietramala, P. Rullo, A GA-based learning algorithm for inducing M-of-N-like text classifiers, in: Proceedings of the 10th InternationalConference on Machine Learning and Applications and Workshops, ICMLA, vol. 1 2011, pp. 269–274.[9] F. Herrera, Genetic fuzzy systems: Status, critical considerations and future directions, International Journal of Computational Intelligence Research 1(2005) 59–67.[10] A. Pietramala, V. Policicchio, P. Rullo, I. Sidhu, A genetic algorithm for text classification rule induction, in: Proceedings of the European Conference onMachine Learning and Knowledge Discovery in Databases – Part II, ECML PKDD’08, Springer-Verlag, Berlin, Heidelberg, 2008, pp. 188–203.[11] I.H. Witten, E. Frank, Data Mining: Practical Machine Learning Tools and Techniques, 2nd edition, The Morgan Kaufmann Series in Data ManagementSystems, Morgan Kaufmann Publishers, San Francisco, CA, 2005.[12] J. Bacardit, E.K. Burke, N. Krasnogor, Improving the scalability of rule-based evolutionary learning, Memetic Comput. 1 (1) (2009) 55–67.[13] M. Franco, N. Krasnogor, J. Bacardit, Speeding up the evaluation of evolutionary learning systems using GPGPUs, in: Proceedings of the 12th AnnualConference on Genetic and Evolutionary Computation, GECCO’10, 2010, pp. 1039–1046.[14] W.W. Cohen, Y. Singer, Context-sensitive learning methods for text categorization, in: ACM Transactions on Information Systems, ACM Press, 1996,pp. 307–315.[15] J.R. Quinlan, Generating production rules from decision trees, in: Proceedings of the 10th International Joint Conference on Artificial Intelligence, vol. 1,Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1987, pp. 304–307.[16] J. Platt, Fast training of support vector machines using sequential minimal optimization, in: B. Scholkopf, C. Burges, A. Smola (Eds.), Advances in KernelMethods: Support Vector Learning, MIT Press, Cambridge, MA, 1998.V.L. Policicchio et al. / Artificial Intelligence 191–192 (2012) 61–9595[17] F. Sebastiani, Machine learning in automated text categorization, ACM Comput. Surv. 34 (2002) 1–47.[18] C. Schaffer, Overfitting avoidance as bias, Mach. Learn. 10 (1993) 153–178.[19] T. Joachims, Text categorization with support vector machines: learning with many relevant features, in: Proceedings of ECML-98, 10th EuropeanConference on Machine Learning, No. 1398, Springer-Verlag, Heidelberg, 1998.[20] A. McCallum, K. Nigam, A comparison of event models for naive Bayes text classification, in: AAAI-98 Workshop on Learning for Text Categorization,AAAI Press, 1998, pp. 41–48.[21] J.D. Rennie, L. Shih, J. Teevan, D.R. Karger, Tackling the poor assumptions of naive Bayes text classifiers, in: ICML, 2003, pp. 616–623.[22] J.R. Quinlan, Learning logical definitions from relations, Mach. Learn. 5 (1990) 239–266.[23] W. Li, J. Han, J. Pei, CMAR: Accurate and efficient classification based on multiple class-association rules, in: Proceedings of the IEEE InternationalConference on Data Mining, 2001, pp. 369–376.[24] X. Yin, J. Han, CPAR: Classification based on predictive association rules, in: Proceedings of the SIAM International Conference on Data Mining, 2003,pp. 331–335.[25] F. Coenen, P. Leng, The effect of threshold values on association rule based classification accuracy, Data Knowl. Eng. 60 (2007) 345–360.[26] A. Fernández, S. García, J. Luengo, E. Bernadó-Mansilla, F. Herrera, Genetics-based machine learning for rule induction: state of the art, taxonomy, andcomparative study, Trans. Evol. Comput. 14 (2010) 913–941.[27] S.W. Wilson, Classifier fitness based on accuracy, Evol. Comput. 3 (1995) 149–175.[28] G. Venturini, SIA: A supervised inductive algorithm with genetic search for learning attributes based concepts, Mach. Learn. ECML-93 (1993) 280–296.[29] J. Bacardit, D.E. Goldberg, M.V. Butz, Improving the performance of a Pittsburgh learning classifier system using a default rule, in: Proceedings of the2003–2005 International Conference on Learning Classifier Systems, IWLCS’03–05, Springer-Verlag, Berlin, Heidelberg, 2007, pp. 291–307.[30] J.J. Liu, J.T. Kwok, An extended genetic rule induction algorithm, in: Proceedings of the 2000 Congress on Evolutionary Computation (CEC00), 2000,pp. 458–463.[31] D.R. Carvalho, A.A. Freitas, A hybrid decision tree/genetic algorithm method for data mining, Inform. Sci. 163 (2004) 13–35.[32] A. Giordana, L. Saitta, F. Zini, Learning disjunctive concept definitions using a genetic algorithm, in: ECAI, 1994, pp. 483–486.[33] A. Giordana, C. Anglano, A. Giordana, G.L. Bello, L. Saitta, A network genetic algorithm for concept learning, in: Proceedings of the Sixth InternationalConference on Genetic Algorithms, Morgan Kaufmann, 1997, pp. 436–443.[34] F. Divina, M. Keijzer, E. Marchiori, A method for handling numerical attributes in GA-based inductive concept learners, in: GECCO, 2003, pp. 898–908.[35] J. Bacardit, N. Krasnogor, Performance and efficiency of memetic Pittsburgh learning classifier systems, Evol. Comput. 17 (3) (2009) 307–342.[36] E. Gabrilovich, S. Markovitch, Text categorization with many redundant features: Using aggressive feature selection to make SVMs competitive withC4.5, in: ICMLí04, 2004, pp. 321–328.[37] E. Baralis, P. Garza, Associative text categorization exploiting negated words, in: Proceedings of the 2006 ACM Symposium on Applied Computing,2006, pp. 530–535.[38] P. Rullo, L. Policicchio, C. Cumbo, S. Iiritano, Olex: effective rule learning for text categorization, IEEE Trans. Knowl. Data Eng. 21 (2009) 1118–1132.[39] G. Forman, I. Guyon, A. Elisseeff, An extensive empirical study of feature selection metrics for text classification, J. Mach. Learn. Res. 3 (2003) 1289–1305.[40] A. Tamaddoni-Nezhad, S. Muggleton, A genetic algorithms approach to ILP, in: Proceedings of the 12th International Conference on Inductive LogicProgramming, ILP’02, Springer-Verlag, Berlin, Heidelberg, 2003, pp. 285–300.[41] S.-H. Nienhuys-Cheng, R.d. Wolf, Foundations of Inductive Logic Programming, Springer-Verlag, New York, Secaucus, NJ, USA, 1997.[42] STOC’84: Proceedings of the Sixteenth Annual ACM Symposium on Theory of Computing, ACM, New York, NY, USA, 1984, 508840.[43] L. Pitt, L.G. Valiant, Computational limitations on learning from examples, J. ACM 35 (1988) 965–984.[44] Ahn, C. Wook, Advances in Evolutionary Algorithms: Theory, Design and Practice (Studies in Computational Intelligence), Springer-Verlag, New York,Secaucus, NJ, USA, 2006.[45] T. Baick, Optimal mutation rates in genetic search, in: Proc. Fifth International Conference on Genetic Algorithms, Morgan Kaufmann, San Mateo, CA,1993, pp. 2–9.[46] D.E. Goldberg, J. Richardson, Genetic algorithms with sharing for multimodalfunction optimization, in: ICGA, 1987, pp. 41–49.[47] J. Bacardit, Pittsburgh genetics-based machine learning in the data mining era: Representations, generalization, and run-time, Ph.D. thesis, Ramon LlullUniversity, Barcelona, Spain, 2004.[48] D.P. Greene, S.F. Smith, Competition-based induction of decision models from examples, Mach. Learn. 13 (1993) 229–257.[49] A.A. Freitas, Data Mining and Knowledge Discovery with Evolutionary Algorithms, Springer-Verlag, New York, Secaucus, NJ, USA, 2002.[50] J. Alcalá-Fdez, L. Sánchez, S. García, M.J. del Jesús, S. Ventura, J.M. Garell, J. Otera, C. Romero, J. Bacardit, V.M. Rivas, J.C. Fernández, F. Herrera, KEEL: asoftware tool to assess evolutionary algorithms for data mining problems, Soft Comput. 13 (3) (2009) 307–318.[51] F. Debole, F. Sebastiani, An analysis of the relative difficulty of Reuters-21578 subsets, in: Proceedings of the 4th International Conference on LanguageResources and Evaluation (LREC 2004), 2004, pp. 971–974.[52] W. Hersh, C. Buckley, T. Leone, D. Hickman, Ohsumed: an interactive retrieval evaluation and new large text collection for research, in: W.B. Croft, C.J.Van Rijsbergen (Eds.), Proceedings of SIGIR-94, 17th ACM International Conference on Research and Development in Information Retrieval, Springer-Verlag, Heidelberg/Dublin, 1994, pp. 192–201.[53] E. hong Han, G. Karypis, Centroid-based document classification: Analysis and experimental results, in: Principles of Data Mining and KnowledgeDiscovery, 2000, pp. 424–431.[54] http://www.cs.umass.edu/~mccallum/data.html.[55] http://www.cs.uic.edu/~liub/FBS/blog-gender-dataset.rar.[56] http://web.ist.utl.pt/~acardoso/datasets/.[57] http://www.dmoz.org/rdf.html (content.rdf.u8.gz).[58] J. Demšar, Statistical comparison of classifiers over multiple data sets, J. Mach. Learn. Res. 7 (1) (2006) 1–30.[59] Z. Zheng, R. Srihari, Optimally combining positive and negative features for text categorization, in: Workshop for Learning from Imbalanced Datasets II,Proceedings of the ICML, 2003.[60] J. Bacardit, M. Stout, J.D. Hirst, K. Sastry, X. Llorà, N. Krasnogor, Automated alphabet reduction method with evolutionary algorithms for proteinstructure prediction, in: GECCO’07: Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation, ACM Press, 2007, pp. 346–353.[61] S. Chua, F. Coenen, G. Malcolm, Classification inductive rule learning with negated features, in: Proceedings of the 6th International Conference onAdvanced Data Mining and Applications: Part I, Springer-Verlag, 2010, pp. 125–136.[62] M.A. Franco, N. Krasnogor, J. Bacardit, Analysing BioHEL using challenging boolean functions, Evol. Intell. 5 (2) (2012) 87–102.[63] D.D. Lewis, Y. Yang, T. Rose, F. Li, RCV1: A new benchmark collection for text categorization research, Journal of Machine Learning Research 5 (2004)361–397.