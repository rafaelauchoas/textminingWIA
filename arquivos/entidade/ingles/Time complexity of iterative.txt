Artificial Intelligence 129 (2001) 199–218Time complexity of iterative-deepening-A∗Richard E. Korf a,∗, Michael Reid b, Stefan Edelkamp ca Computer Science Department, University of California, Los Angeles, Los Angeles, CA 90095, USAb Department of Mathematics and Statistics, University of Massachusetts, Amherst, MA 01003-4515, USAc Institut für Informatik, Georges-Köhler-Allee, Gebäude 51, 79110 Freiburg, GermanyReceived 15 February 2000; received in revised form 3 February 2001Abstract∗∗(IDAWe analyze the time complexity of iterative-deepening-A). We first show how to calculatethe exact number of nodes at a given depth of a regular search tree, and the asymptotic brute-force∗with a consistent, admissible heuristicbranching factor. We then use this result to analyze IDAfunction. Previous analyses relied on an abstract analytic model, and characterized the heuristicfunction in terms of its accuracy, but do not apply to concrete problems. In contrast, our analysison actual problems such as the sliding-tileallows us to accurately predict the performance of IDApuzzles and Rubik’s Cube. The heuristic function is characterized by the distribution of heuristicvalues over the problem space. Contrary to conventional wisdom, our analysis shows that theasymptotic heuristic branching factor is the same as the brute-force branching factor. Thus, the effectof a heuristic function is to reduce the effective depth of search by a constant, relative to a brute-forcesearch, rather than reducing the effective branching factor.  2001 Elsevier Science B.V. All rightsreserved.∗∗Keywords: Problem solving; Heuristic search; Iterative-deepening-AHeuristic branching factor; Sliding-tile puzzles; Eight Puzzle; Fifteen Puzzle; Rubik’s Cube; Time complexity; Branching factor;1. Introduction and overviewOur goal is to predict the running time of iterative-deepening-A∗ (IDA∗) [5], a linear-space version of the A∗ algorithm [4]. Both these algorithms rely on a heuristic evaluationfunction h(n) that estimates the cost of reaching a goal from node n. If h(n) is admissi-ble, or never overestimates actual cost from node n to a goal, then both algorithms returnoptimal solutions.* Corresponding author.E-mail addresses: korf@cs.ucla.edu (R.E. Korf), reid@math.umass.edu (M. Reid),edelkamp@informatik.uni-freiburg.de (S. Edelkamp).0004-3702/01/$ – see front matter  2001 Elsevier Science B.V. All rights reserved.PII: S 0 0 0 4 - 3 7 0 2 ( 0 1 ) 0 0 0 9 4 - 7200R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218The running time of IDA∗ is usually proportional to the number of nodes expanded.This depends on the cost of an optimal solution, the number of nodes in the brute-forcesearch tree, and the heuristic function. In Section 2, we show how to compute the size of abrute-force search tree, and its asymptotic branching factor. In Section 3, we use this resultto predict the number of nodes expanded by IDA∗ using a consistent heuristic function.The key to this analysis is characterizing the heuristic function.Previous work on this problem characterized the heuristic by its accuracy as an estimateof actual solution cost. The accuracy of a heuristic is very difficult to obtain, and thecorresponding asymptotic results, based on an abstract model, don’t predict performanceon concrete problems. In contrast, we characterize a heuristic by its distribution of values,a characterization that is easy to determine. As a result, we can predict the performance ofIDA∗ on the sliding-tile puzzles and Rubik’s Cube to within 1% of experimental results. Incontrast to previous work, our analysis shows that the asymptotic heuristic branching factoris the same as the brute-force branching factor. This implies that the effect of a heuristicfunction is to reduce the effective depth of search by a constant, relative to a brute-forcesearch, rather than reducing the effective branching factor,Much of this work originally appeared in two AAAI-98 papers, one on the brute-forcebranching factor [2], and the other on the analysis of IDA∗ [7]. We begin with brute-forcesearch trees.2. Branching factor of regular search trees2.1. Graph versus tree-structured problem spacesMost problem spaces are graphs with cycles. Given a root node of any graph, however, itcan be expanded into a tree. For example, Fig. 1 shows a search graph, and the top part ofits tree expansion, rooted at node A. In a tree expansion of a graph, each distinct path to anode of the graph generates a different node of the tree. The tree expansion of a graph canbe much larger than the original graph, and in fact is often infinite even for a finite graph.In this paper, we focus on problem-space trees. The reason is that IDA∗ uses depth-firstsearch to save memory, and hence cannot detect most duplicate nodes. Thus, it potentiallyexplores every path to a given node, and searches the tree-expansion of the problem-spacegraph.We can characterize the size of a brute-force search tree by its asymptotic branchingfactor. The branching factor of a node is the number of children it has. In most trees,however, different nodes have different numbers of children. In that case, we define theasymptotic branching factor as the number of nodes at a given depth, divided by thenumber of nodes at the next shallower depth, in the limit as the depth goes to infinity.We present examples of problem-space trees, and compute their asymptotic branchingfactors. We formalize the problem as the solution of a set of simultaneous equations. Wepresent both analytic and numerical techniques for computing the exact number of nodesat a given depth, and determining the asymptotic branching factor. We give the branchingfactors of Rubik’s Cube and sliding-tile puzzles from the Five Puzzle to the Ninety-NinePuzzle.R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218201Fig. 1. Graph and part of its tree expansion.Fig. 2. Rubik’s Cube.2.2. Example: Rubik’s CubeConsider Rubik’s Cube, shown in Fig. 2. We define any 90, 180, or 270 degree twistof a face as one move. Since there are six faces, this gives an initial branching factor of6 · 3 = 18. We never twist the same face twice in a row, however, since the same result canbe obtained with a single twist of that face. This reduces the branching factor to 5 · 3 = 15after the first move.Note that twists of opposite faces are independent of each other and hence commute.For example, twisting the left face followed by the right face gives the same result astwisting the right face followed by the left face. Thus, if two opposite faces are twistedconsecutively, we require them to be twisted in a particular order, to eliminate the samestate resulting from twisting them in the opposite order. For each pair of opposite faces,we arbitrarily label one a “first” face, and the other a “second” face. Thus, if Left, Up andFront were the first faces, then Right, Down, and Back would be the second faces. Aftera first face is twisted, there are three possible twists of each of the remaining five faces,for a branching factor of 15. After a second face is twisted, however, we can only twist202R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218four remaining faces, excluding the face just twisted and its corresponding first face, for abranching factor of 12. Thus, the asymptotic branching factor is between 12 and 15.The exact asymptotic branching factor depends on the relative fraction of nodes wherethe last move was a twist of a first face (type-1 nodes), or a twist of a second face (type-2nodes). Define the equilibrium fraction of type-1 nodes as the number of type-1 nodes at agiven depth, divided by the total number of nodes at that depth, in the limit of large depth.The equilibrium fraction is not 1/2, because a twist of a first face can be followed by atwist of any second face, but a twist of a second face cannot be followed immediately bya twist of the corresponding first face. To determine the asymptotic branching factor, weneed the equilibrium fraction of type-1 nodes. The fraction of type-2 nodes is one minusthe fraction of type-1 nodes.Each type-1 node generates 2 · 3 = 6 type-1 nodes and 3 · 3 = 9 type-2 nodes as children,the difference being that you can’t twist the same first face again. Each type-2 nodegenerates 2 · 3 = 6 type-1 nodes and 2 · 3 = 6 type-2 nodes, since you can’t twist thecorresponding first face next, or the same second face again. Thus, the number of type-1nodes at a given depth is 6 times the number of type-1 nodes at the previous depth, plus 6times the number of type-2 nodes at the previous depth. The number of type-2 nodes at agiven depth is 9 times the number of type-1 nodes at the previous depth, plus 6 times thenumber of type-2 nodes at the previous depth.Let f1 be the fraction of type-1 nodes, and f2 = 1 − f1 the fraction of type-2 nodes ata given depth. If n is the total number of nodes at that depth, then there will be nf1 type-1nodes and nf2 type-2 nodes at that depth. In the limit of large depth, the fraction of type-1nodes will converge to the equilibrium fraction, and remain constant. Thus, at large depth,f1 = type-1 nodes at next leveltotal nodes at next level== 6f1 + 6f215f1 + 12f2= 6f1 + 6(1 − f1)15f1 + 12(1 − f1)6nf1 + 6nf26nf1 + 6nf2 + 9nf1 + 6nf2= 263f1 + 12=f1 + 4= f1.Cross multiplying gives us the quadratic equation f 21at f1 =f1) = 3+ 4f1 = 2, which has a positive root6 − 2 ≈ 0.44949. This gives us an asymptotic branching factor of 15f1 + 12(1 −6 + 6 ≈ 13.34847.√√2.3. A system of simultaneous equationsIn general, this analysis produces a system of simultaneous equations. For anotherexample, consider the Five Puzzle, the 2 × 3 version of the well-known sliding-tile puzzles(see Fig. 3A).In this problem, the branching factor of a node depends on the blank position. In Fig. 3B,the positions are labelled s and c, representing side and corner positions, respectively.We don’t generate the parent of a node as one of its children, to avoid duplicate nodesrepresenting the same state. This requires keeping track of both the current and previousblank positions. Let cs denote a node where the blank is currently in a side position, andthe last blank position was a corner position. Define ss, sc and cc nodes analogously. Sincecs and ss nodes have two children each, and sc and cc nodes have only one child each,we have to know the equilibrium fractions of these different types of nodes to determineR.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218203Fig. 3. The Five Puzzle.the asymptotic branching factor. Fig. 3C shows the different types of states, with arrowsindicating the type of children they generate. For example, the double arrow from ss to scindicates that each ss node generates two sc nodes at the next level.Let N(t, d) be the number of nodes of type t at depth d in the search tree. Then, we canwrite the following recurrence relations directly from the graph in Fig. 3C. For example,the last equation comes from the fact that there are two arrows from ss to sc, and one arrowfrom cs to sc.N(cc, d + 1) = N(sc, d),N(cs, d + 1) = N(cc, d),N(ss, d + 1) = N(cs, d),N(sc, d + 1) = 2N(ss, d) + N(cs, d).The initial conditions are that the first move either generates an ss node and two scnodes, or a cs node and a cc node, depending on whether the blank starts in a side or cornerposition, respectively.2.3.1. Numerical solutionA simple way to compute the branching factor is to numerically compute the valuesof successive terms of these recurrences, until the relative frequencies of different statetypes converge. Let fcc, fcs, fss and fsc be the number of nodes of each type at a givendepth, divided by the total number of nodes at that depth. After a hundred iterations,we get the equilibrium fractions fcc = 0.274854, fcs = 0.203113, fss = 0.150097, andfsc = 0.371936. Since cs and ss states generate two children each, and the others generateone child each, the asymptotic branching factor is fcc + 2fcs + 2fss + fsc = 1.35321.Alternatively, we can simply compute the ratio between the total nodes at two successivedepths to get the branching factor. The running time of this algorithm is the product of thenumber of different types of states, e.g., four in this case, and the search depth. In contrast,searching the actual tree to depth 100 would generate over 1013 states in this case.2.3.2. Analytical solutionTo compute the exact branching factor, we assume that the fractions eventually convergeto constant values. This generates a set of equations, one from each recurrence. Let brepresent the asymptotic branching factor. If we view fcc as the number of cc nodes atdepth d, for example, then the number of cc nodes at depth d + 1 will be bfcc. This allowsus to rewrite the above recurrences as the following set of equations. The last one constrainsthe fractions to sum to one.204R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218bfcc = fsc,bfcs = fcc,bfss = fcs,bfsc = 2fss + fcs,1 = fcc + fcs + fss + fsc.Repeated substitution to eliminate variables reduces this system of five equations in fiveunknowns to the single equation, b4 −b −2 = 0, with a solution of b ≈ 1.35321. In general,the degree of the polynomial will be the number of different types of states. The FifteenPuzzle, for example, has three types of positions, and six types of states.If we make the naive and incorrect assumption that each blank position is equally likelyin the Five Puzzle, we get an incorrect branching factor of (2 · 2 + 1 · 4)/6 = 1.33333.Another natural but erroneous approach is to include the parent of a node as one of itschildren, compute the resulting branching factor, and then subtract one from the result toeliminate the inverse of the last move. This gives an incorrect branching factor of 1.4142for the Five Puzzle. The error here is that eliminating the inverse of the last move changesthe equilibrium fractions of the different types of states.2.4. ResultsWe computed the asymptotic branching factors of square sliding-tile puzzles up to10 × 10. Table 1 gives the even- and odd-depth branching factors for each puzzle. Thelast column is their geometric mean, or the square root of their product, which is thebest estimate of the overall branching factor. Most of these values were computed bynumerical iteration of the recurrence relations. As n goes to infinity, all the values convergeto three, the branching factor of an infinite sliding-tile puzzle, since most positions havefour neighbors, one of which was the previous blank position.To see why the even and odd branching factors are different, color the positions of apuzzle in a checkerboard pattern, and note that the blank always moves between squares ofTable 1The asymptotic branching factor for the (n2 − 1)-Puzzlenn2 − 1Even depthOdd depth3456789108152435486380991.52.13042.302782.519642.599272.695902.739222.7902622.13042.434262.519642.646492.695902.760082.79026Mean√32.13042.367612.519642.622772.695902.749632.79026R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218205different colors. If the sets of different-colored squares are equivalent to each other, as inthe Five and Fifteen Puzzles, there is one branching factor. If the sets of different-coloredsquares are different however, as in the Eight Puzzle, there will be different even and oddbranching factors. In general, an n × m sliding-tile puzzle will have different branchingfactors if and only if both n and m are odd.2.5. Generality of this techniqueIn some problem spaces, every node has the same branching factor. In other spaces,every node may have a different branching factor, requiring exhaustive search to computethe average branching factor. The technique described above determines the size of a brute-force search tree in intermediate cases, where there are a small number of different typesof states, whose generation follows a regular pattern. Computing the size of the brute-forcesearch tree is the first step in determining the time complexity of IDA∗, our next topic.3. Time complexity of IDA∗IDA∗ [5] uses the cost function f (n) = g(n) + h(n), where g(n) is the sum of the edgecosts from the initial state to node n, and h(n) is an estimate of the cost of reaching a goalfrom node n. Each iteration is a depth-first search where a branch is pruned when it reachesa node whose total cost exceeds the cost threshold of that iteration. The cost threshold forthe first iteration is the heuristic value of the initial state, and increases in each iteration tothe lowest cost of all nodes pruned on the previous iteration. It continues until a goal nodeis found whose cost does not exceed the current cost threshold.3.1. Previous workMost previous analyses of heuristic search focused on A∗ [3,9,10], and used an abstractproblem-space tree where every node has b children, every edge has unit cost, and there is asingle goal node at depth d. The heuristic is characterized by its error in estimating actualsolution cost. This model predicts that a heuristic with constant absolute error results inlinear time complexity, while constant relative error results in exponential time complexity[3,10].There are several limitations of this model. The first is that it assumes there is onlyone path from the start to the goal state, whereas most problem spaces contain multiplepaths to each state. The second limitation is that in order to determine the accuracy ofthe heuristic on even a single state, we have to determine the optimal solution cost fromthat state, which is expensive to compute. Doing this for a significant number of states isimpractical for large problems. Finally, the results are only asymptotic, and don’t predictactual numbers of node generations. Because of these limitations, the previous work cannotbe used to accurately predict the performance of A∗ or IDA∗ on concrete problems withreal heuristics. That requires a different approach.206R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–2183.2. OverviewWe begin with the consistency property of heuristics, and the conditions for nodeexpansion by A∗ or IDA∗. Next, we characterize a heuristic by the distribution of heuristicvalues over the problem space. Our main result is a formula for the number of nodeexpansions as a function of the heuristic distribution, the cost threshold of an iteration, andthe number of nodes of each cost in a brute-force search. Finally, we compare our analyticpredictions with experimental data on Rubik’s Cube and the Eight and Fifteen Puzzles.One implication of our analysis is that the effect of a heuristic function is to decrease theeffective depth of search by a constant, rather than reducing the effective branching factor.3.3. Consistent heuristicsOne property of the heuristic required by our analysis is that it be consistent. A heuristicfunction h(n) is consistent if for any node n and any neighbor n(cid:9), h(n) (cid:1) k(n, n(cid:9)) + h(n(cid:9)),where k(n, n(cid:9)) is the cost of the edge from n to n(cid:9) [9]. An equivalent definition ofconsistency is that for any pair of nodes n and m, h(n) (cid:1) k(n, m) + h(m), where k(n, m) isthe cost of an optimal path from n to m. Consistency is similar to the triangle inequality ofmetrics, and implies admissibility, but not vice versa. However, most naturally occurringadmissible heuristic functions are consistent as well [9].3.4. Conditions for node expansionWe measure the time complexity of IDA∗ by the number of node expansions. If anode can be expanded and its children evaluated in constant time, the asymptotic timecomplexity of IDA∗ is simply the number of node expansions. Otherwise, it’s the product ofthe number of node expansions and the time to expand a node. Given a consistent heuristicfunction, both A∗ and IDA∗ must expand all nodes whose total cost, f (n) = g(n) + h(n),is less than c, the cost of an optimal solution [9]. Some nodes with the optimal solutioncost may be expanded as well, until a goal node is chosen for expansion, and the algorithmsterminate. In other words, f (n) < c is a sufficient condition for A∗ or IDA∗ to expand noden, and f (n) (cid:1) c is a necessary condition. For a worst-case analysis, we adopt the weakernecessary condition.An easy way to understand the node expansion condition is that any search algorithmthat guarantees optimal solutions must continue to expand every possible solution path,until its cost is guaranteed to exceed the cost of an optimal solution, lest it lead to a bettersolution.On the final iteration of IDA∗, the cost threshold will equal c, the cost of an optimalsolution. In the worst case, IDA∗ will expand all nodes n whose cost f (n) = g(n) + h(n) (cid:1)c. We will see below that this final iteration determines the overall asymptotic timecomplexity of IDA∗.3.5. Characterization of the heuristicPrevious analyses characterized the heuristic function by its accuracy as an estimator ofoptimal costs. As explained above, this is difficult to determine for a real heuristic, sinceR.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218207obtaining optimal solutions is extremely expensive. In contrast, we characterize a heuristicfunction by the distribution of heuristic values over the nodes in the problem space. Inother words, we need to know the number of states with heuristic value 0, how many stateshave heuristic value 1, the number with heuristic value 2, etc. Equivalently, we can specifythis distribution by a set of parameters D(h), which is the fraction of total states of theproblem whose heuristic value is less than or equal to h. We refer to this set of values asthe overall distribution of the heuristic. D(h) can also be defined as the probability that astate chosen randomly and uniformly from all states in the problem has heuristic value lessthan or equal to h. h can range from zero to infinity, but for all values of h greater than orequal to the maximum value of the heuristic, D(h) = 1.Table 2 shows the overall distribution for the Manhattan distance heuristic on the FivePuzzle. Manhattan distance is computed by counting the number of grid units that each tileis displaced from its goal position, and summing these values for all tiles. The first columnof Table 2 gives the heuristic value. The second column gives the number of states of theFive Puzzle with each heuristic value. The third column gives the total number of stateswith a given or smaller heuristic value, which is simply the cumulative sum of the valuesfrom the second column. The fourth column gives the overall heuristic distribution D(h).These values are computed by dividing the value in the third column by 360, the totalnumber of states in the problem space. The remaining columns will be explained below.The overall distribution is easily obtained for any heuristic. For heuristics implementedby table-lookup, or pattern databases [1,6,8], the distribution can be determined exactly byscanning the table. Alternatively, for a heuristic computed by a function, such as Manhattandistance on large sliding-tile puzzles, we can randomly sample the problem space toestimate the overall distribution to any desired degree of accuracy. For heuristics that areTable 2Heuristic distributions for Manhattan distance on the Five PuzzlehStatesSumD(h)CornerSideCsumSsumP (h)01234567891011121236305861586048248113612421001612192793273513593600.0027780.0083330.0166670.0333330.1166670.2777780.4472220.6083330.7750000.9083330.9750000.9972221.0000001115253838414431114001215202317161713411238337110915019422523624024001349295269851021151191200.0026950.0083330.0169150.0333330.1154240.2767010.4468080.6073400.7730120.9065940.9745030.9970571.000000208R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218the maximum of several different heuristics, we can approximate the distribution of thecombined heuristic from the distributions of the individual heuristics by assuming that theindividual heuristic values are independent.The distribution of a heuristic function is not a measure of its accuracy, and says littleabout the correlation of heuristic values with actual costs. The only connection betweenthe accuracy of a heuristic and its distribution is that given two admissible heuristics, theone with higher values will be more accurate than the one with lower values on average.3.5.1. The equilibrium distributionWhile the overall distribution is the easiest to understand, the complexity of IDA∗depends on a potentially different distribution. The equilibrium distribution P (h) is definedas the probability that a node chosen randomly and uniformly among all nodes at a givendepth of the brute-force search tree has heuristic value less than or equal to h, in the limitof large depth.If all states of the problem occur with equal frequency at large depths in the searchtree, then the equilibrium distribution is the same as the overall distribution. For example,this is the case with the Rubik’s Cube search tree described in Section 2.2. In general,however, the equilibrium distribution may not equal the overall distribution. In the FivePuzzle, for example, the overall distribution assumes that all states, and hence all blankpositions, are equally likely. As we saw in Section 2.3, however, at deep levels in the tree,the blank is in a side position in more than 1/3 of the nodes, and in a corner position inless than 2/3 of the nodes. In the limit of large depth, the equilibrium frequency of sidepositions is fs = fcs + fss = 0.203113 + 0.150097 = 0.35321. Similarly, the frequencyof corner positions is fc = fcc + fsc = 0.274854 + 0.371936 = 0.64679 = 1 − fs . Thus,to compute the equilibrium distribution, we have to take these equilibrium fractions intoaccount.The fifth and sixth columns of Table 2, labelled “Corner” and “Side”, give the number ofstates with the blank in a corner or side position, respectively, for each heuristic value. Theseventh and eighth columns, labelled “Csum” and “Ssum”, give the cumulative numbersof corner and side states with heuristic values less than or equal to each particular heuristicvalue. The last column gives the equilibrium distribution P (h). The probability P (h) thatthe heuristic value of a node is less than or equal to h is the probability that it is a cornernode, 0.64679, times the probability that its heuristic value is less than or equal to h,given that it is a corner node, plus the probability that it is a side node, 0.35321, timesthe probability that its heuristic value is less than or equal to h, given that it is a side node.For example, P (2) = 0.64679 · (3/240) + 0.35321 · (3/120) = 0.016915. This differs fromthe overall distribution D(2) = 0.016667.The equilibrium heuristic distribution is not a property of a problem, but of a problemspace. For example, including the parent of a node as one of its children can affect theequilibrium distribution, by changing the equilibrium fractions of different types of states.When the equilibrium distribution differs from the overall distribution, it can still becomputed from a pattern database, or by random sampling of the problem space, combinedwith the equilibrium fractions of different types of states, as illustrated above.R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218209Fig. 4. Sample tree for analysis of IDA∗.3.6. An example search treeTo provide some intuition behind our main result, Fig. 4 shows a schematic represen-tation of a search tree generated by an iteration of IDA∗ on an abstract problem instance,where all edges have unit cost. The vertical axis represents the depth of a node, which isalso its g value, and the horizontal axis represents the heuristic value of a node. Each boxrepresents a set of nodes at the same depth with the same heuristic value, labelled withthe number of such nodes. The arrows represent the relationship between parent and childnode sets. These particular numbers were generated by assuming that each node generatesone child each with heuristic value one less, equal to, and one greater than the heuristicvalue of the parent. For example, there are 6 nodes at depth 3 with heuristic value 1, 1whose parent has heuristic value 1, 2 whose parents have heuristic value 2, and 3 whoseparents have heuristic value 3. In this example, the maximum value of the heuristic is 4,and the heuristic value of the initial state is 3.One assumption of our analysis is that the heuristic is consistent. Because of this, andsince all edges have unit cost in this example, the heuristic value of a child must be atleast the heuristic value of its parent, minus one. We assume a cutoff threshold of eightmoves for this iteration of IDA∗. Solid boxes represent sets of “fertile” nodes that will be210R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218expanded, while dotted boxes represent sets of “sterile” nodes that will not be expanded,because their total cost, f (n) = g(n) + h(n) exceeds the cutoff threshold of 8. The thickdiagonal line separates the fertile node sets from the sterile node sets.3.6.1. Nodes expanded as a function of depthThe values at the far right of Fig. 4 show the number of nodes expanded at each depth,which is the number of fertile nodes at that depth. Ni is the number of nodes in the brute-force search tree at depth i, and P (h) is the equilibrium heuristic distribution. The numberof nodes generated is the branching factor times the number expanded.Consider the graph from top to bottom. There is a root node at depth 0, which generatesN1 children. These nodes collectively generate N2 child nodes at depth 2. Since the cutoffthreshold is 8 moves, in the worst-case, all nodes n whose total cost f (n) = g(n) + h(n) (cid:1)8 will be expanded. Since 4 is the maximum heuristic value, all nodes down to depth8 − 4 = 4 will be expanded, Thus, for d (cid:1) 4, the number of nodes expanded at depth dwill be Nd , the same as in a brute-force search. Since 4 is the maximum heuristic value,P (4) = 1, and hence N4P (4) = N4.The nodes expanded at depth 5 are the fertile nodes, or those for which f (n) =g(n) + h(n) = 5 + h(n) (cid:1) 8, or h(n) (cid:1) 3. At sufficiently large depths, the distributionof heuristic values converges to the equilibrium distribution. Assuming that the heuristicdistribution at depth 5 approximates the equilibrium distribution, the fraction of nodes atdepth 5 with h(n) (cid:1) 3 is approximately P (3). Since all nodes at depth 4 are expanded, thetotal number of nodes at depth 5 is N5, and the number of fertile nodes is N5P (3).There exist nodes at depth 6 with heuristic values from 0 to 4, but their distributiondoes not equal the equilibrium distribution. In particular, nodes with heuristic values 3and 4, shown in dotted boxes, are underrepresented relative to the equilibrium distribution,because these nodes are generated by parents with heuristic values from 2 to 4. At depth5, however, the nodes with heuristic value 4 are sterile, producing no offspring at depth 6,hence reducing the number of nodes at depth 6 with heuristic values 3 and 4.The number of nodes at depth 6 with h(n) (cid:1) 2 is completely unaffected by any pruninghowever, since their parents are nodes at depth 5 with h(n) (cid:1) 3, all of which are fertile. Inother words, the number of nodes at depth 6 with h(n) (cid:1) 2, which are the fertile nodes, isexactly the same as in the brute-force search tree, or N6P (2).Due to consistency of the heuristic function, all possible parents of fertile nodes arethemselves fertile. Thus, the number of nodes to the left of the diagonal line in Fig. 4 isexactly the same as in the brute-force search tree. In other words, heuristic pruning of thetree has no effect on the number of fertile nodes, although it does effect the sterile nodes. Ifthe heuristic was inconsistent, then the distribution of fertile nodes would change at everylevel where pruning occurred, making the analysis far more complex.When all edges have unit cost, the number of fertile nodes at depth i is Ni P (d − i),where Ni is the number of nodes in the brute-force search tree at depth i, d is the cutoffdepth, and P is the equilibrium heuristic distribution. The total number of nodes expandedby an iteration of IDA∗ to depth d isd(cid:1)i=0Ni P (d − i).R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–2182113.7. General resultHere we state and prove our main theoretical result. First, we assume a minimum edgecost, and divide all costs by this value, normalizing it to one. We express all costs asmultiples of the minimum edge cost. We allow operators with different costs, and replacethe depth of a node by g(n), the sum of the edge costs from the root to the node. Let Ni bethe number of nodes n in the brute-force search tree with g(n) = i.Next, we assume the heuristic returns an integer multiple of the minimum edge cost.Given an admissible non-integer valued heuristic, we round up to the next larger integer,preserving admissibility. We also assume that the heuristic is consistent, meaning that forany two nodes n and m, h(n) (cid:1) k(n, m) + h(m), where k(n, m) is the cost of an optimalpath from n to m.Given these assumptions, our task is to determine E(N, c, P ), the number of nodes nfor which f (n) = g(n) + h(n) (cid:1) c, given a problem-space tree with Ni nodes of cost i,with a heuristic characterized by the equilibrium distribution P (x). This is the number ofnodes expanded by an iteration of IDA∗ with cost threshold c, in the worst case.Theorem 1. In the limit of large c,E(N, c, P ) =c(cid:1)i=0Ni P (c − i).Proof. E(N, c, P ) is the number of nodes n for which f (n) = g(n) + h(n) (cid:1) c. Considerthe nodes n for which g(n) = i, which is the set of nodes of cost i in the brute-force searchtree. There are Ni such nodes. The nodes of cost i that will be expanded by IDA∗ in aniteration with cost threshold c are those for which f (n) = g(n) + h(n) = i + h(n) (cid:1) c,or h(n) (cid:1) c − i. By definition of P , in the limit of large i, the number of such nodes inthe brute-force search tree is NiP (c − i). It remains to show that all these nodes in thebrute-force search tree are also in the tree generated by IDA∗.Consider an ancestor node m of such a node n. Since m is an ancestor of n, thereis only one path between them in the tree, and g(n) = i = g(m) + K(m, n), whereK(m, n) is the cost of the path from node m to node n. Since f (m) = g(m) + h(m),and g(m) = i − K(m, n), f (m) = i − K(m, n) + h(m). Since the heuristic is consistent,h(m) (cid:1) k(m, n) + h(n), where k(m, n) is the cost of an optimal path from m to n inthe problem graph. Since K(m, n) (cid:2) k(m, n), h(m) (cid:1) K(m, n) + h(n). Thus, f (m) (cid:1)i − K(m, n) + K(m, n) + h(n), or f (m) (cid:1) i + h(n). Since h(n) (cid:1) c − i, f (m) (cid:1) i + c − i,or f (m) (cid:1) c. This implies that node m is fertile and will be expanded during the search.Since all ancestors of node n are fertile and will be expanded, node n must eventuallybe generated. Therefore, all nodes n in the brute-force search tree for which f (n) =g(n) + h(n) (cid:1) c are also in the tree generated by IDA∗. Since there can’t be any nodesin the IDA∗ tree that are not in the brute-force search tree , the number of such nodes atlevel i in the IDA∗ tree is Ni P (c − i). Therefore, the total number of nodes expanded byIDA∗ in an iteration with cost threshold c, which is the number in the last iteration, isE(N, c, P ) =c(cid:1)i=0Ni P (c − i).✷212R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–2183.8. The heuristic branching factorThe effect of earlier iterations on the time complexity of IDA∗ depends on the rate ofgrowth of node expansions in successive iterations. The heuristic branching factor is theratio of the number of nodes expanded in a search to cost threshold c, divided by thenodes expanded in a search to cost c − 1, or E(N, c, P )/E(N, c − 1, P ), where 1 is thenormalized minimum edge cost.Assume that the size of the brute-force search tree grows exponentially with cost, orNi = bi , where b is the brute-force branching factor. In that case, the heuristic branchingfactor E(N, c, P )/E(N, c − 1, P ) isci=0 biP (c − i)c−1i=0 biP (c − 1 − i)= b0P (c) + b1P (c − 1) + b2P (c − 2) + · · · + bcP (0)b0P (c − 1) + b1P (c − 2) + · · · + bc−1P (0)(cid:2)(cid:2).The first term of the numerator, b0P (c), is less than or equal to one, and can be droppedwithout significantly affecting the ratio. Factoring b out of the remaining numerator givesb(b0P (c − 1) + b1P (c − 2) + · · · + bc−1P (0))b0P (c − 1) + b1P (c − 2) + · · · + bc−1P (0)= b.Thus, if the brute-force tree grows exponentially with branching factor b, then therunning time of successive iterations of IDA∗ also grows by a factor of b. In other words,the heuristic branching factor is the same as the brute-force branching factor. In that case, itis easy to show that the overall time complexity of IDA∗ is b/(b − 1) times the complexityof the last iteration [5].Previous analyses, based on different assumptions, predicted that the effect of a heuristicfunction is to reduce search complexity from O(bc) to O(ac), where a < b, reducing theeffective branching factor [9]. Our analysis, however, shows that on an exponential tree,the effect of a heuristic is to reduce search complexity from O(bc) to O(bc−k), for someconstant k, which depends only on the heuristic function. If we define the effective depthof a search as the log base b of the number of nodes expanded, where b is the brute-forcebranching factor, then a heuristic reduces the effective depth from c to c − k for a constantk. In other words, a heuristic search to cost c generates the same number of nodes as abrute-force search to cost c − k.3.9. Experimental resultsWe tested our analysis experimentally by predicting the performance of IDA∗ on Rubik’sCube and sliding-tile puzzles, using well-known heuristics. Since all operators have unitcost in these problems, the g(n) cost of a node n is its depth. For Ni , we used the exactnumbers of nodes at depth i, which were computed from the recurrence relations describedin Section 2.3.3.9.1. Rubik’s CubeWe first predicted existing data on Rubik’s Cube [6]. The problem space, described inSection 2.2, allows 180-degree twists as single moves, disallows two consecutive twistsof the same face, and only allows consecutive twists of opposite faces in one order. ThisR.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218213Table 3Nodes generated by IDA∗on Rubik’s CubeDepthTheoreticalProblemsExperimentalError10111213141516171,51020,169269,2293,593,80047,971,732640,349,1938,547,681,506114,098,463,567100010001000100100100100251,5010.596%20,1510.089%270,3960.433%3,564,4950.815%47,916,6990.115%642,403,1550.321%8,599,849,2550.610%114,773,120,9960.591%search tree has a brute-force branching factor of about 13.34847. The median optimalsolution depth is 18 moves.The heuristic is the maximum of three different pattern databases [1,6]. It is admissibleand consistent, with a maximum value of 11 moves, and an average value of 8.898moves. The distribution of the individual heuristics was calculated exactly by scanningthe databases, and the three heuristics were assumed to be independent to calculate thedistribution of the combined heuristic. In this case, the equilibrium distribution is the sameas the overall distribution. We ignored goal states, completing each search iteration.In Table 3, the first column shows the cutoff depth, the next column gives the nodegenerations predicted by our theory, the next column indicates the number of probleminstances run, the next column displays the average number of nodes generated by IDA∗in a single iteration, and the last column shows the error between the theoretical predictionand experimental results.The theory predicts the data to within 1% accuracy in every case. Sources of errorinclude the limited number of problem instances, the assumption of independence ofthe heuristics, and the fact that the heuristic distribution at a finite depth does not equalthe equilibrium distribution. The ratio between the node generations in the last twolevels, which is the experimental heuristic branching factor, is 13.34595, compared to thetheoretical value of 13.34847. If we take the log, base 13.34847, of the predicted number ofnodes generated at depth 17 (114,098,463,567), we get about 9.825. Thus, this particularheuristic reduces the effective depth of search by 17 − 9.825 = 7.175 moves.3.9.2. Eight PuzzleWe also experimented with the Eight Puzzle, using the Manhattan distance heuristic. Ithas a maximum value of 22 moves, and a mean value of 14 moves. The optimal solutionlength averages 22 moves, with a maximum of 31 moves, assuming the blank is in a cornerin the goal state. Since the Eight Puzzle has only 181,440 solvable states, the heuristicdistributions were computed exactly. Three distributions were used, depending on whetherthe blank is in a center, corner, or side position. The number of nodes of each type at eachdepth of the brute-force tree was also computed exactly.214R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218Table 4Nodes expanded by IDA∗on the Eight PuzzleDepthTheoreticalProblemsExperimental2021222324252627282930313936571,1851,9773,5615,93610,68617,81532,07253,45096,207160,167181,440181,440181,440181,440181,440181,440181,440181,440181,440181,440181,440181,4403936571,1851,9773,5615,93610,68617,81532,07253,45096,207160,167Error0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%0.0%Table 4 shows a comparison of the number of node expansions predicted by ourtheoretical analysis, to the number of nodes expanded by a single iteration of IDA∗ tovarious depths, ignoring goal states. Each data point is the average of all 181,440 probleminstances. Since the average numbers of node expansions, the size of the brute-force tree,and the heuristic distributions are all exact, the model predicts the experimental dataexactly, to multiple decimal places, verifying that we have accounted for all the relevantfactors.The Eight Puzzle has even and odd-depth brute-force branching factors of 1.5 and 2.The corresponding heuristic branching factors are 1.667 and 1.8, but the product of the3, of the number oftwo branching factors is 3 in both cases. If we take the log, basenodes expanded at depth 31 (160,167), we get about 21.8. This implies that on the EightPuzzle, Manhattan distance reduces the effective depth of search by 31−21.8 = 9.2 moves.√3.9.3. Fifteen PuzzleWe ran a similar experiment on the Fifteen Puzzle, also using the Manhattan distanceheuristic. The average heuristic value is about 37 moves, and the maximum is 62 moves.The average optimal solution length is 52.5 moves. Since the Fifteen Puzzle has over 1013solvable states, we used a random sample of ten billion solvable states to approximatethe heuristic distributions. Three different distributions were used, one for the blank in amiddle, corner, or side position. The number of nodes of each type at each depth was alsocomputed exactly, for each different initial blank position.Table 5 is similar to Table 4. Each line is the average of 100,000 random solvableproblem instances. Despite over ten orders of magnitude variation in the nodes expandedin individual problem instances, the average values agree with the theoretical prediction towithin 1% in most cases. The ratio between the experimental number of node expansionsR.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218215Table 5Nodes expanded by IDA∗on the Fifteen PuzzleDepthTheoreticalProblemsExperimentalError404142434445464748495042,66490,894193,641412,535878,8641,872,3303,988,8058,497,73418,103,53638,567,69382,164,440100,000100,000100,000100,000100,000100,000100,000100,000100,000100,000100,00041,97391,495191,219415,490870,4401,886,3633,959,7298,562,82418,003,95938,864,26981,826,0081.65%0.66%1.27%0.72%0.96%0.75%0.73%0.77%0.55%0.77%0.41%at the last two depths is 2.105, compared to the brute-force branching factor of 2.130.If we take the log, base 2.13, of the predicted number of nodes expanded at depth 50(82,164,440), we get about 24.1. Thus, on the Fifteen Puzzle, Manhattan distance reducesthe effective depth of search by 50 − 24.1 = 25.9 moves.The results above are for single complete iterations to the given search depths, ignoringany solutions found. How well do these results predict the running time of IDA∗ to solve arandom problem instance? The average optimal solution length for random Fifteen Puzzleinstances is about 52.5 moves [8]. Multiplying the last value in Table 5 by b2 or 2.13042predicts 372,911,869 node expansions in a complete iteration to depth 52, or 794,451,446node generations. Multiplying by b2/(b2 − 1) = 1.2826 to account for all the previousiterations predicts about 1.019 billion node generations. Completing the final iteration tofind all optimal solutions to the same set of 1000 problem instances generates an averageof 1.178 billion nodes. Terminating IDA∗ when the first solution is found generates anaverage of 401 million nodes.3.9.4. Twenty-Four PuzzleWe can also predict the performance of IDA∗ on problems we can’t run experimentally,such as the Twenty-Four Puzzle with the Manhattan distance heuristic. The brute-forcebranching factor is 2.36761. Sampling ten billion random solvable states yields anapproximation of the overall heuristic distribution, which approximates the equilibriumdistribution. The average heuristic value is 76 moves. Experiments using more powerfuldisjoint pattern database heuristics [8] give an average optimal solution length of about100 moves. Our theory predicts that running all iterations up to depth 100 will generatean average of 1.217 × 1019 nodes. On a 440 MHz Sun Ultra 10 workstation, IDA∗ withManhattan distance generates about 7.5 million nodes per second. This predicts an averagetime to complete all iterations up to depth 100, on a random instance of the Twenty-Four216R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218Puzzle, ignoring any solutions found, of about 50,000 years! Manhattan distance reducesthe effective depth of search on the Twenty-Four Puzzle by about 49 moves.3.9.5. Observed heuristic branching factorIf we run IDA∗ on a single instance of a sliding-tile puzzle, we observe that the ratiobetween the numbers of nodes generated in successive iterations usually decreases witheach iteration, but exceeds the theoretical heuristic branching factor. On the sliding-tilepuzzles with Manhattan distance, the cost threshold increases by two in each successiveiteration, and hence the theoretical heuristic branching factor is the square of the brute-force branching factor. For example, in the Twenty-Four Puzzle, the observed heuristicbranching factor is often greater than 10, whereas b2 is only 5.6.The reason for this discrepancy is an initial transient in the observed heuristic branchingfactor. The formula in Theorem 1 is based on the equilibrium heuristic distribution. Startingfrom a single initial state, it takes many iterations of IDA∗ for the heuristic distribution toconverge to the equilibrium distribution. This effect is ameliorated in the results presentedabove because the experimental data is averaged over a large number of initial states. Ifwe run IDA∗ long enough on a single problem instance, the observed heuristic branchingfactor eventually converges to the square of the brute-force branching factor.Why is the observed heuristic branching factor greater than the theoretical branchingfactor? The heuristic distribution at the root of the search tree starts with the heuristicvalue of the initial state, and gradually spreads out to larger and smaller heuristicvalues with increasing depth. Thus, the frequency of small and large heuristic values isinitially zero, underestimating their frequency in the equilibrium heuristic distribution.Underestimating the large values has little effect, since the frequency of these values ismultiplied by the relatively small number of nodes at shallow depths. The frequencies ofsmall values, however, are multiplied by the large numbers of nodes at deep depths, andhence underestimating these values significantly decreases the number of node generations,relative to what happens at equilibrium. As the search depth increases in successiveiterations, the frequency of nodes with small heuristic values increases, which causes alarger observed heuristic branching factor than occurs at equilibrium.In Rubik’s Cube, however, the observed heuristic branching factor converges to thebrute-force value, without consistently overestimating it initially. This is due to the smallerrange of heuristic values, and the larger branching factor, which allows convergence to theequilibrium heuristic distribution more quickly.4. ConclusionsWe first show how to compute the exact number of nodes at different depths, andthe asymptotic branching factor, of brute-force search trees where different nodes havedifferent numbers of children. We begin by writing a set of recurrence relations for thegeneration of the different node types. By expanding these recurrence relations, we candetermine the exact number of nodes at a given depth, in time linear in the depth. Wecan also use the ratio of the numbers of nodes at successive depths to approximate theasymptotic branching factor with very high precision. Alternatively, we can rewrite theR.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218217recurrence relations as a set of simultaneous equations involving the relative frequenciesof the different types of nodes, and solve them analytically for small numbers of nodetypes. We give the asymptotic branching factors for Rubik’s Cube, the Five Puzzle, and thefirst nine square sliding-tile puzzles.We then use these results to predict the time complexity of IDA∗. We characterize aheuristic by the distribution of heuristic values, which can be obtained by random sampling,for example. We compare our predictions with experimental data on Rubik’s Cube, theEight Puzzle, and the Fifteen Puzzle, getting agreement within 1% for Rubik’s Cube andthe Fifteen Puzzle, and exact agreement for the Eight Puzzle. In contrast to previous results,our analysis and experiments indicate that on an exponential tree, the asymptotic heuristicbranching factor is the same as the brute-force branching factor. Thus, the effect of aheuristic is to reduce the effective depth of search by a constant, relative to a brute-forcesearch, rather than reducing the effective branching factor.5. Generality and further workTo what extent can these results can be applied to other problems? Our main result isTheorem 1. It says that the number of nodes n for which f (n) = g(n) + h(n) (cid:1) c is aconvolution of two distributions. The first is the number of nodes of a given cost in thebrute-force search space, and the second is the number of nodes with a given heuristicvalue. In order to apply this to a particular problem, however, we have to determine the sizeof the brute-force search space, and the heuristic distribution. Thus, we have decomposedthe problem of predicting the performance of a heuristic search algorithm into two simplerproblems.How could we use this analysis to predict the performance of A∗? The main differencebetween A∗ and IDA∗ is that A∗ detects duplicate nodes, and doesn’t reexpand them.Theorem 1 applies to A∗ as well, but Ni is the number of nodes in the problem-space graph,rather than its tree expansion. Unfortunately, the only technique known for computing thenumber of nodes at a given depth in a search graph is exhaustive search to that depth. As aresult, these values are unknown for even regular problem spaces such as the Fifteen Puzzleor Rubik’s Cube. The relevant heuristic distribution P (h) for analyzing A∗ is the overallheuristic distribution D(h), because each state occurs only once in the problem space.As another example, could we predict the performance of IDA∗ on the traveling sales-man problem? In a problem space that constructs a tour by adding one city at a time, eachnode represents a partial tour, and the number of nodes at depth d is the number of per-mutations of n − 1 elements taken d at a time. Computing the distribution for a heuristicsuch as the cost of a minimum spanning tree of the remaining cities is difficult, however. Itdepends on the depth of search, and the particular problem instance. If the edge costs andheuristic values are real numbers rather than integers, the discrete convolution of Theorem1 becomes a continuous convolution, and the summation becomes an integral. While wecan’t solve this problem currently, Theorem 1 tells us what distributions we need, and howto combine them.The running time of IDA∗ depends on the branching factor, the heuristic distribution,and the optimal solution cost. Predicting the optimal solution cost for a given problem218R.E. Korf et al. / Artificial Intelligence 129 (2001) 199–218instance, or even the average optimal solution cost, is an open problem, however. Sincethe number of nodes in a problem-space tree grows by a factor of b with each succeedingdepth, a lower bound on the maximum optimal solution depth is the log base b of thenumber of reachable states, rounded up to the next larger integer. This can be used as anestimate of the average solution depth. For example, this method predicts a depth of 22moves for the Eight Puzzle, which equals the average optimal solution length. For Rubik’sCube, this method predicts a value of 18 moves, which is the median optimal solutionlength. For the Fifteen Puzzle, however, we get an estimate of only 40 moves, while theaverage solution depth is 52.5 moves. The reason this method doesn’t accurately predictthe maximum solution depth is that it assumes that all states in the search tree are unique.For all these problems, however, there are multiple paths to the same state, giving rise toduplicate nodes in the tree representing the same state.AcknowledgementsWe would like to thank Eli Gafni, Elias Koutsoupias, and Mitchell Tsai for severalhelpful discussions. R. Korf was supported by NSF grant IRI-9619447. S. Edelkamp wassupported by DFG in a project entitled, “Heuristic Search and its Application to ProtocolValidation”.References[1] J. Culberson, J. Schaeffer, Pattern databases, Comput. Intelligence 14 (4) (1998) 318–334.[2] S. Edelkamp, R.E. Korf, The branching factor of regular search spaces, in: Proc. AAAI-98, Madison, WI,1998, pp. 299–304.[3] J. Gaschnig, Performance measurement and analysis of certain search algorithms, Ph.D. Thesis, Departmentof Computer Science, Carnegie-Mellon University, Pittsburgh, PA, 1979.[4] P.E. Hart, N.J. Nilsson, B. Raphael, A formal basis for the heuristic determination of minimum cost paths,IEEE Transactions on Systems Science and Cybernetics 4 (2) (1968) 100–107.[5] R.E. Korf, Depth-first iterative-deepening: An optimal admissible tree search, Artificial Intelligence 27 (1)(1985) 97–109.[6] R.E. Korf, Finding optimal solutions to Rubik’s Cube using pattern databases,in: Proc. AAAI-97,Providence, RI, 1997, pp. 700–705.[7] R.E. Korf, M. Reid, Complexity analysis of admissible heuristic search, in: Proc. AAAI-98, Madison, WI,1998, pp. 305–310.[8] R.E Korf, A. Felner, Disjoint pattern database heuristics, Artificial Intelligence (Special Issue: ChipsChallenging Champions: Advances in Computational Intelligence for Game-Playing) (2001), to appear.[9] J. Pearl, Heuristics, Addison-Wesley, Reading, MA, 1984.[10] I. Pohl, Practical and theoretical considerations in heuristic search algorithms, in: W. Elcock, D. Michie(Eds.), Machine Intelligence, Vol. 8, Ellis Horwood, Chichester, 1977, pp. 55–72.