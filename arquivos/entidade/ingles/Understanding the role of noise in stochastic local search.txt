Artificial Intelligence 172 (2008) 955–990www.elsevier.com/locate/artintUnderstanding the role of noise in stochastic local search:Analysis and experimentsOle J. MengshoelRIACS, NASA Ames Research Center, Mail Stop 269-3, Moffett Field, CA 94035, USAReceived 3 November 2006; received in revised form 17 September 2007; accepted 17 September 2007Available online 1 February 2008AbstractStochastic local search (SLS) algorithms have recently been proven to be among the best approaches to solving computationallyhard problems. SLS algorithms typically have a number of parameters, optimized empirically, that characterize and determinetheir performance. In this article, we focus on the noise parameter. The theoretical foundation of SLS, including an understandingof how to the optimal noise varies with problem difficulty, is lagging compared to the strong empirical results obtained usingthese algorithms. A purely empirical approach to understanding and optimizing SLS noise, as problem instances vary, can be verycomputationally intensive. To complement existing experimental results, we formulate and analyze several Markov chain models ofSLS in this article. In particular, we compute expected hitting times and show that they are rational functions for individual probleminstances as well as their mixtures. Expected hitting time curves are analytical counterparts to noise response curves reported inthe experimental literature. Hitting time analysis using polynomials and convex functions is also discussed. In addition, we presentexamples and experimental results illustrating the impact of varying noise probability on SLS run time. In experiments, wheremost probable explanations in Bayesian networks are computed, we use synthetic problem instances as well as problem instancesfrom applications. We believe that our results provide an improved theoretical understanding of the role of noise in stochastic localsearch, thereby providing a foundation for further progress in this area.© 2008 Elsevier B.V. All rights reserved.Keywords: Stochastic local search; Noise; Markov chain models; Expected hitting times; Rational functions; Noise response curves; Probabilisticreasoning; Bayesian networks; Most probable explanation; Systematic experiments; Polynomial approximation; Convexity1. IntroductionThe stochastic local search (SLS) approach has proven to be highly competitive for solving a range of hard compu-tational problems including satisfiability of propositional logic formulas [11,18,45,46] as well as computing the mostprobable explanation [22,27,30] and the maximum a posteriori hypothesis [36,37] in Bayesian networks. While thedetails of different SLS algorithms vary [18], by definition they all use stochasticity or noise. In this article we focuson noise during local search rather than, say, noisy initialization.Empirically, it turns out that noise has a dramatic impact on the run time of SLS algorithms [14,17,26,44,45]. Intu-itively, there is a fundamental trade-off between using high and low levels of noise in SLS. Let 0 (cid:2) p (cid:2) 1 represent theprobability of taking a noise step. The argument for using low noise p is that this enables an SLS algorithm to greedilyE-mail address: omengshoel@riacs.edu.0004-3702/$ – see front matter © 2008 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.09.010956O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990climb hills without taking unnecessary downhill noise steps. The argument for using high noise p is that this providesthe SLS algorithm with a powerful mechanism to escape local (but non-global) optima [17]. Empirically—and de-pending on the problem instance, the SLS algorithm, and its input parameters including noise level—an approximatelyoptimal noise level ˆp∗ can be found. The difficulty of approximating the optimal noise p∗ [26] has led to the devel-opment of adaptive noise, in which p is not static but varies as the SLS algorithm runs [7,14,26]. However, noiseadaptation is still an active area of research and we believe the present work provides several key insights that shouldbenefit further progress.Our main contributions in this article are as follows. Based on the seminal WALKSAT architecture [44,45], we in-troduce a simple but general SLS algorithm called SIMPLESLS. SIMPLESLS performs noisy steps with probability p,and greedy steps with probability 1 − p. We show that expected hitting times for SIMPLESLS are rational functionsP (p)/Q(p), where P (p) and Q(p) are polynomials. This explicitly provides a functional form corresponding tonoise response curves previously only established empirically in the SLS literature [7,8,14,16,26]. We also considerthe use of polynomials and convex functions. Convexity is important because local optimality implies global opti-mality, a dramatic simplification compared to unrestricted optimization. Rational functions as well as their specialcase polynomials can be used to analytically determine optimal noise levels; the latter are used in experiments in thisarticle.Using Markov chain analysis, and in particular by analyzing expected hitting times for trap Markov chains, weclearly show the impact of different settings of the noise parameter p when the difficulty of the problem instancevaries, a key concern in stochastic local search. Trap Markov chains are an idealized model for how SLS gets trappedby local (but non-global) optima in a search space, and how noise p impacts the capability of SLS to escape suchtraps. Further, we show that optimal noise probability p∗ varies dramatically depending on the problem instance.In particular, the optimal noise parameter varies from p∗ = 0 for easy problem instances to p∗ close to 1 for hardproblem instances.We back up our analysis with empirical results using Bayesian networks (BNs), both synthetic and from applica-tions. BNs play a central role in a wide range of uncertainty reasoning applications including diagnosis, probabilisticrisk analysis, language understanding, intelligent data analysis, error correction coding, and biological analysis. Manyinteresting computational BN problems, including MPE computation, are NP-complete or harder [37,40,47], henceheuristic methods such as SLS are of interest. In this work we study the problem of computing the most probable ex-planation (MPE) in Bayesian networks. We use an SLS algorithm known as stochastic greedy search (SGS) to searchfor MPEs in BNs. SGS can simulate SIMPLESLS and is a flexible, operator-based SLS approach in which differentinitialization and search operators can easily be investigated [27,30].Our approach to generating synthetic BNs includes satisfiability (SAT) as a special case (see [40,47] for the reduc-tion). Let V be the number of variables in propositional logic or the number of root nodes in BNs. Further, let C be thenumber of clauses in propositional logic or the number of non-root nodes in BNs. For V > 0, the ratio C/V is well-defined and has turned out to be useful in predicting inference difficulty for randomly generated problem instances[33,34]. An easy–hard–easy pattern in inference difficulty as a function of the C/V -ratio has been observed for SAT[34]. For BNs, an easy–hard–harder pattern has been established when inference difficulty is measured in terms ofupper bounds on minimal maximal clique size (or treewidth) [29,33]. Upper bounds on optimal clique tree size andoptimal maximal clique size can be computed using tree clustering [24]. In this article, we use the C/V -ratio directlyto characterize the difficulty of synthetic BNs for SLS.There is a clear relationship between our Markov chain approach and observed SLS run times. We illustrate that ouridealized trap Markov chain models are relevant to experiments with real problem instances. For a few small probleminstances we show complete search spaces and derive corresponding Markov chains. With these in hand, we compare(i) bounding hitting time results derived from idealized trap Markov chain models; (ii) analytical hitting time resultsderived from Markov chain models (which were created from real problem instances along with the behavior of SIM-PLESLS); (iii) real observed SGS run times for the same problem instances; and (iv) polynomial regression resultsfor the SGS run times A key point relating Markov chain models to classes of real problem instances is suggestedby the following: Increasing problem instance hardness as controlled by C/V -ratio corresponds, roughly speaking,to increasing the size of the trap in a trap Markov chain. Consequently, mixtures of problem instances that are easyon average (small C/V and small traps) should be solved by greedier (less noisy) SLS algorithms than mixtures ofproblem instances that are hard on average (large C/V and large traps). In experiments with synthetic problem in-stances, we indeed observe these patterns as C/V is varied. To complement our experiments with synthetic problems,O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990957we also investigate BNs from applications while also using more advanced initialization and noise algorithms. Here,we found that noise can sometimes have a rather minor impact on SLS performance while in other cases the impactcan be dramatic. Generally, the empirical results support our Markov chain-based analysis.We believe this work is significant for the following reasons. First, by using Markov chain hitting time analysisand introducing an explicit noise parameter p, this research provides an improved understanding of what role noiseplays in stochastic local search. Such theoretical understanding has traditionally been limited [14,16,44], even thoughthere exists research based on Markov chains which explores the role of traps and local maxima in SLS [15]. Second,while the experimental results of SLS are very impressive, their empirical basis means that these algorithms are quitecomputationally intense to optimize [26]. We believe that this work paves the way for improved approaches to optimizethe noise level in stochastic local search; optimization can take place in a more principled fashion once a betterunderstanding of the role of noise has been established. Third, we believe that Markov chain analysis and in particularexpected hitting times, and more generally stochastic process theory, has been under-utilized when researching SLSalgorithms. Hopefully, the impact of other parameters that describe problem instances and parameters that control theSLS search processes can be analyzed in a similar way by utilizing techniques from stochastic process theory.The rest of this article is organized as follows. Preliminary concepts are introduced in Section 2. In Section 3we present a simple but easy-to-analyze SLS algorithm called SIMPLESLS. Section 4 presents our three Markovchain models of SIMPLESLS, namely the exact, naive, and trap Markov chain models. Section 5 provides in-depthnumerical analysis and discussion of examples of trap Markov chains and their expected hitting times. In Section 6we present general expected hitting time and run time results. Section 7 provides empirical results using synthetic andapplication problem instances, specifically Bayesian networks, before we conclude in Section 8.2. PreliminariesWe assume that the reader is familiar with fundamental definitions and results from the areas of graph theory,probability theory, and statistics; and in particular Markov chains [23] and Bayesian networks [39]. Some of the mostpertinent concepts are briefly reviewed in this section.A direct and natural way to analyze an SLS algorithm’s operation on a problem instance is as a discrete timeMarkov chain with a discrete state space, defined as follows.Definition 1 (Markov chain). A (discrete time, discrete state space) Markov chain {Xn, n = 0, 1, 2, . . .} is defined bya 3-tuple M = (S, V, P) where S = {s1, . . . , sk} defines the set of k states while V = (π1, . . . , πk), a k-dimensionalvector, defines the initial probability distribution. The conditional state transition probabilities P can be characterizedby means of a k × k matrix.Only time-homogeneous Markov chains will be considered here. Many of the Markov chain models discussedbelow, including the trap Markov chains introduced in Section 4.3, are random walks with so-called boundary states{s1, sk} and internal states {s2, . . . , sk−1}. Further, the noise level p acts as a parameter in some of the Markov chainmodels we discuss in the following.In M, some states O ⊂ S are of particular interest since they represent optimal states, and we introduce thefollowing definition.Definition 2 (SLS model). Let M = (S, V, P) be a Markov chain. Further, assume an objective function f : S →Rand an optimal objective function value f ∗ ∈ R that defines optimal states O = {s | s ∈ S and f (s) = f ∗}. An SLSmodel is defined as a 2-tuple (M, O).The objective function f and the optimal states O are independent of the SLS algorithm and its parameters. Notethat neither M nor O are, in general, explicitly specified. Rather, they are induced by the objective function (orproblem instance), the SLS algorithm, and the SLS algorithm’s parameter settings. In fact, finding an s∗ ∈ O is thepurpose of computation, and it is given implicitly by means of the objective function f and the optimal objectivefunction value f ∗ ∈ R. Without loss of generality our main focus is on maximization problems in this article.Consider an SLS model (M, O). A hitting time analysis of the Markov chain M gives the expected number ofsteps needed to reach s∗ ∈ O. Expected hitting times, to be introduced in Definition 5, are based on first passage times958O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990and their expectations, which we introduce now. In the following definition, Xj is an arbitrary random variable amongthe random variables {Xn, n = 0, 1, 2, . . .} of a Markov chain.Definition 3 (First passage time). Consider an SLS model (M, O) and let si ∈ S where S is M’s states. The firstpassage time T into s∗ ∈ O, where |O| = 1, is given by T = min(j (cid:3) 0: Xj = s∗}. The expected value of T , giveninitial state X0 = si , is defined asmi := E(T | X0 = si).In this article, mi is sometimes a function of noise p, in which case we say mi(p). Note also that Definition 3 caneasily be generalized to cover passage time into multiple optimal states, |O| > 1, however to simplify the expositionwe generally focus on the one-state case here.We often consider problem instances represented as bitstrings of length n, b ∈ {0, 1}n, in which case s∗ = b∗ ∈{0, 1}n. We are interested in maximization: finding a b∗ ∈ O such that f (b∗) (cid:3) f (b) for all b ∈ {0, 1}n. The followingdefinition formally introduces the useful concept of unitation as summing over a bitstring.Definition 4 (Unitation). Let b = b1b2 . . . bn ∈ {0, 1}n be a bitstring of length n. The number of ones (or unitation) ofb = b1 . . . bn is defined as u(b) =(cid:2)ni=1 bi .Counting the number of ones is, after an easy transformation of the search space, equivalent to counting the numberof correct bits. The number of correct bits is the number of bits that are equal between b∗and the current stateof SLS search c. More formally, let b∗, b, c, d,∈ {0, 1}n. In order to normalize the search space, one can use thetransformations b := (b∗ xor c) and d := ¯b where xor denotes exclusive or and ¯b denotes the bit-wise inversion of b.Now, take u(d) in this transformed search space in order to obtain the number of correct bits; clearly d∗ = 1 . . . 1. Tosimplify notation, we often gloss over the transformations, and say that b∗ = 1 . . . 1 without loss of generality [6]. SeeFig. 7 for concrete examples.Using conditional expectations, one obtains from Definition 3 the following well-known result.Theorem 5 (Expected hitting time). Let M be a Markov chain with state space S = {s1, . . . , sk} and first passage timeT (into sk). The expected hitting time h is thenk(cid:3)k(cid:3)h :=E(T | X0 = i) Pr(X0 = i) =i=1miπi.i=1(1)Expected hitting time can be used to analyze the expected time to reach an optimal state, and is therefore closelyrelated to the observed run time for an SLS algorithm. In the context of SLS, the hitting time h is with respect tosome state in O and depends on the algorithm’s input parameters including the problem instance. Our main interestin this article is expected hitting time as a function of noise p. Typically, we therefore get h(p) instead of just h (as in(1)), and reserve the short form “expected hitting time” for h(p). By studying h(p) and varying p along the x-axis ingraphs, we obtain expected hitting time curves that are counterparts to experimental noise response curves. Clearly,one would like to find SLS parameters such that the minimal expected hitting time h∗ is obtained. Often, the searchfor minimal expected hitting time h∗ has an empirical component and the notation ˆh∗ may be used.In the experimental part of this work we will focus on an SLS approach to computing the most probable explanation[22,27,30] in Bayesian networks (BNs). This problem is interesting in its own right and also generalizes satisfiability[40,47]. BN nodes can be continuous, however we will here restrict ourselves to discrete BN nodes.Definition 6 (Bayesian network). A Bayesian network is a tuple β = (X, E, P ), where (X, E) is a DAG with anassociated set of conditional probability distributions P = {Pr(X1 | ΠX1), . . . Pr(Xn | ΠXn)}. Here, Pr(Xi | ΠXi ) isthe conditional probability distribution for Xi ∈ X. Let πXi represent the instantiation of the parents ΠXi of Xi . Theindependence assumptions encoded in (X, E) imply the joint probability distributionPr(x) = Pr(x1, . . . , xn) = Pr(X1 = x1, . . . , Xn = xn) =n(cid:4)i=1Pr(xi | πXi ).(2)O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990959A BN may be provided with observations or evidence by setting or clamping m nodes {O1, . . . , Om} ⊂ X toknown states o = {O1 = o1, . . . , Om = om} = {o1, . . . , om}. These nodes are called observation nodes and need to beconsidered in explanations, defined as follows.Definition 7 (Explanation). Consider a BN β = (X, E, P ) with X = {X1, . . . , Xn} and observations o = {o1, . . . , om}for m < n. An explanation x assigns states to all non-evidence nodes {Xm+1, . . . , Xn}: x = {xm+1, . . . , xn} ={Xm+1 = xm+1, . . . , Xn = xn}.Among all explanations in a BN, the u most probable ones are of particular interest.Definition 8 (Most probable explanation (MPE)). Let x range over all explanations in a BN β. Finding a most probableexplanation (MPE) in β is the problem of computing an explanation x∗ such that Pr(x∗) (cid:3) Pr(x). The u most probableexplanations is X∗ = {x∗} where Pr(x∗) = Pr(x∗i ) (cid:3) Pr(x) for 1 (cid:2) i (cid:2) u.1) = · · · = Pr(x∗u) and Pr(x∗1, . . . , x∗uAs is common, we compute just one MPE x∗ ∈ X∗ even when multiple MPEs exist in a BN. Computation of the Mmost probable explanations, for M (cid:3) 1 and where explanations do not have the same probability, is a generalizationthat has also been investigated [50]. Following Pearl, we sometimes denote computing an MPE as belief revision, whilecomputing the marginal distribution over a BN node is also denoted belief updating [39]. Many of the computationalBN problems of interest are hard. Exact MPE computation is NP-hard [47] and the problem of relative approximationof an MPE also been shown NP-hard [1]. Belief updating is computationally hard also [4,40].3. Stochastic local searchWe discuss a simplified variant of SLS, SIMPLESLS, based on the seminal WALKSAT architecture [16,44,45].SIMPLESLS is not intended to be competitive with state-of-the-art SLS algorithms, which often are tailored to thedomain and problem instances at hand [18]. Rather, SIMPLESLS is intended to enable analysis and capture what iscommon to SLS algorithms based on WALKSAT, in particular with respect to their noisy search components.SIMPLESLS maintains a current estimate of b∗The SIMPLESLS algorithm takes as input these parameters: n—bitstring length; p—noise probability; f —anobjective function used to evaluate f (b) where b = b1b2 . . . bn ∈ {0, 1}n is a bit string of length n; f ∗—optimumvalue of f ; MAX-FLIPS—the number of flips before restart; and finally MAX-TRIES—the number of restarts beforetermination. SIMPLESLS iteratively takes search steps, which are either greedy or noisy as further discussed below., namely ˆb, as well as the current state b. To initialize b, SIM-PLESLS puts bi := 0 with Pr(1/2) and bi := 1 with Pr(1/2) for all 1 (cid:2) i (cid:2) n. Such initialization uniformly at randomis common in SLS algorithms [18]. SIMPLESLS initializes ˆbin the same manner. Then, one-bit flips are repeatedlymade to b until success or restart, described below, takes place. A one-bit flip means that b’s ith bit, where 1 (cid:2) i (cid:2) n,:= ¯bi andis picked and then flipped. Suppose the current bitstring is b = b1b2 . . . bi . . . bn−1bn. Then a flip is to set b(cid:6)i∗) then ˆbis formed as b(cid:6) := b1b2 . . . ¯bi . . . bn−1bn and then setting b := b(cid:6)the new bitstring b(cid:6):= b.Further, if f (ˆb) = f ∗ then SIMPLESLS terminates successfully.. If f (b) (cid:3) f (ˆb∗∗∗∗The way in which the ith bit in b is picked depends on the search operator or algorithm used. A random variableO is now introduced, representing the random selection of which operator to apply next. To keep our analysis simplewe assume exactly two local search operators or operator types, oG (greedy) and oN (noisy) and hence Pr(O =oG) + Pr(O = oN ) = 1.1 The SIMPLESLS algorithm repeatedly instantiates the random variable O by randomlypicking one of the two operators oG and oN as follows:Greedy operator O = oG: With probability Pr(O = oG) = 1 − p, a greedy step is made from b to b(cid:6)objective function increase from f (b) to f (b(cid:6)). If there is tie between k candidate bitstrings {b(cid:6)algorithm picks one of them uniformly at random. If no f (b(cid:6)) (cid:3) f (b), it puts b(cid:6) := b., maximizing1, . . . , b(cid:6)}, thek1 In the SGS system, used for experimentation in Section 7, oN is implemented by the NU operator and oG is implemented by either the BMoperator or the GM operator.960O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990Noisy operator O = oN : With probability Pr(O = oN ) = p, the algorithm makes a noise step as follows. First, aninteger 1 (cid:2) i (cid:2) n is picked uniformly at random. This ith bit is then flipped, forming b(cid:6).∗The SIMPLESLS algorithm iteratively flips bits until f ∗ is reached or MAX-FLIPS flips have been performed.Once MAX-FLIPS flips have been done, a new try is started, and the process continues until MAX-TRIES has beenreached. The approach is closely related to WALKSAT [44,45], in particular its random noise variant.= b∗In our analysis here MAX-TRIES = MAX-FLIPS = ∞ is used; we do vary MAX-FLIPS in some of our ex-periments. We assume a Las Vegas algorithm that is guaranteed to eventually terminate with ˆb. This latterassumption simplifies our analysis and exposition since we need only be concerned with a randomly varying runtime T . One can of course also fix the run time and let the approximated output ˆbbe a random variable; the analysisis then different and we shall not follow this route in this article. Our approach is closely related to previous stochasticlocal search (SLS) algorithms for satisfiability [11,16,18,43,45,46] and Bayesian network problems [22,27,30,36,37].It is somewhat related to previous research on stochastic simulation and guided local search. Stochastic simulation,which can be used to compute MPEs [39], is essentially Gibbs sampling in Bayesian networks. Even though theGibbs sampler in many respects is general, it is quite different from most SLS approaches in that it typically operatesin cycles [25]. A cyclic Gibbs sampler iterates systematically over all non-evidence nodes in a BN. SLS algorithms,on the other hand, are generally more opportunistic and do not operate on such fixed schedules. Stochastic simula-tion has been shown to be outperformed by the SLS approach of combining greedy and noisy search [22], and wedo not investigate stochastic simulation in this article. There is also another class of local search algorithms, calledguided local search [20,35], which emphasizes changing the cost function rather than employing noise. Guided localsearch algorithms such as GLS [35] and GLS+ [20] are clearly very interesting, however our focus in this article is onstochastic local search algorithms and their analysis.∗4. Markov chain models of stochastic local searchImportant aspects of the behavior of many stochastic local search (SLS) algorithms can be represented by meansof discrete-time Markov chains. In Section 4.1 we discuss an exact Markov chain analysis of SLS along with its prosand cons. We then go on to provide approximate models and results. In Section 4.2 a simple 3-state Markov chainmodel is discussed, while in Section 4.3 a more general model, trap Markov chains, is developed.Readers who find the naive and trap Markov chain models too restrictive may want to skim Sections 4.2, 4.3, and 5,and instead consider our more general analysis in Sections 4.1 and 6 as well as experimental results in Section 7.4.1. Exact Markov chain analysisClearly, key aspects of the operation of SIMPLESLS and similar SLS algorithms on a specific problem instancecan be formalized as simulation of a Markov chain. The structure of the underlying exact Markov chain is that ofa hypercube, where each hypercube state b represents a bitstring. A state b ∈ {0, 1}n in such a Markov chain has nneighbors, namely those bitstrings one flip away. Stated formally, b(cid:6)can be obtained by flippingone of b’s bits.is a neighbor to b if b(cid:6)Definition 9 (Neighborhood). Let b be a bitstring of length n. b’s neighborhood n(b), strict neighborhood n(cid:6)(b), andnon-neighborhood ¯n(b) are defined as follows:(cid:7)(cid:5)n(b) =c ∈ {0, 1}n|bi − ci| (cid:2) 1n(cid:3)(cid:6)(cid:6)(cid:6)i=1(cid:6)(b) = n(b) − {b}n¯n(b) = {0, 1}n − n(b).The following Markov chain model is introduced in order to reflect the performance of SIMPLESLS as statedformally in Theorem 11.O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990961Definition 10 (Exact Markov chain model). The exact Markov chain model M of SIMPLESLS has states S ={s0, s1, . . .} = {b | b ∈ {0, 1}n} and an initial probability distribution V with Pr(X0 = si) = 1/2n for 1 (cid:2) i (cid:2) 2n. Thetransition probability matrix P is a stochastic matrix given byPr(Xj +1 = bj +1 | Xj = bj ) = 0 if bj +1 ∈ ¯n(bj )Pr(Xj +1 = bj +1 | Xj = bj ) (cid:3) 0 if bj +1 ∈ n(bj ).(3)(4)Theorem 11. SIMPLESLS simulates an exact Markov chain model up to MAX-FLIPS flips.Proof. Obviously, S and V are as stated and we now consider P. Clearly, SIMPLESLS can be regarded as a stochasticprocess {Xi, i = 0, 1, 2, . . .} over the discrete state space S = {b | b ∈ {0, 1}n}. Further, for fewer than MAX-FLIPSflips, the next state bj +1 is independent of the past given the current state bj , thereforePr(Xj +1 = bj +1 | X0 = b0, . . . , Xj = bj ) = Pr(Xj +1 = bj +1 | Xj = bj ),which defines a Markov chain. For the transition probabilities P there are by construction two SIMPLESLS operatorsto consider, namely oG and oN . Since the number of flips is less than MAX-FLIPS we obtainPr(Xj +1 = bj +1 | Xj = bj ) =Pr(Xj +1 = bj +1 | Xj = bj , Oj = oG)Pr(Xj +1 = bj +1 | Xj = bj , Oj = oN )In both cases, conditions (3) and (4) are obeyed and the theorem follows. (cid:2)if oG pickedif oN picked.(cid:8)Here is an example; see Definition 4 for a formal introduction of unitation u(b).Example 12. An example hypercube representing an exact Markov chain model of a 5-bit SLS search space is shownin Fig. 1.We now consider a state that is a local minimum. In such a case the neighboring states n(cid:6)(b) must have the sameor higher objective function value, and since without loss of generality we have assumed that SIMPLESLS performsmaximization, the following result is obtained.Lemma 13. If the current state bj in SIMPLESLS is a local minimum, the next state bj +1 will always be a state inthe strict neighborhood: bj +1 ∈ n(cid:6)(bj ).Fig. 1. The hypercube over all 5-bit bitstrings b ∈ {0, 1}5. Neighboring bitstrings, bitstrings in which one bit differs, have an edge between them.The unitation u(b) for all bitstrings at the same level is shown to the left.962O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990Proof. Suppose an arbitrary candidate for next state bj +1 is denoted b(cid:6): b(cid:6) ∈ n(bj ). SIMPLESLS applies one of twooperators, O = oG (greedy) or O = oN (noisy). If O = oN , a bit is always flipped such that bj +1 (cid:8)= bj . If O = oG,bj +1 := bj only if no f (b(cid:6)) (cid:3) f (bj ), where b(cid:6) ∈ n(cid:6)(b), exists. However, this is clearly not the case here since bj byassumption is a local minimum and thus f (b(cid:6)) (cid:3) f (bj ) for all b(cid:6) ∈ n(cid:6)(bj ). (cid:2)Since SIMPLESLS searches in a state space defined by an n-dimensional hypercube, one might be tempted toalso perform all analysis in this space. However, such exactness comes at a steep price. Since the size of P is|{0, 1}n| × |{0, 1}n| = 2n+1 and the size of V is |{0, 1}n| = 2n, the specification of M becomes impossibly largeeven for moderately sized problem instances. For small n, exact Markov chain analysis is feasible. For large n, exactMarkov chain analysis may be infeasible.In the following we provide Markov chain analysis results, similar to above, however they are approximate sincewe operate on smaller state spaces compared to the exact space traversed by SLS algorithms such as SIMPLESLS.4.2. Naive Markov chain analysisHere, we are interested in an abstract Markov chain model of SLS algorithms, and introduce the following simplerandom walk model, which constitutes a stepping stone for the trap Markov chains in Section 4.3.Definition 14 (Naive Markov chain model). The naive Markov chain model (M, O) has Markov chain M with statesS = {s0, s1, s2} = {0, 1, 2}, initial probability distribution V = (π0, π1, π2), transition probability matrix,(5)(cid:9)P =11 − b − c0and O ={s0} = {0}.(cid:10)0ba0c1 − aThe naive Markov chain model, along with an example, is illustrated in Fig. 2. The values of a, b, and c as well asthe values of the initial probabilities π0, π1, and π2 all depend on the problem instance and SLS algorithm at hand.In (5), s0 = 0 represents the optimum O; s1 = 1 represents the search space close to s0 = 0, and s2 = 2 representsthe search space distant from s0 = 0. Distance is measured using Hamming distance d(b1, b2) between two bitstrings,b1, b2 ∈ {0, 1}n. There is also a threshold 0 (cid:2) z < n. Formally, s0 represents {b∗}, s1 represents {b | b ∈ {0, 1}n, 0 <d(b, b∗) (cid:2) z}, and s2 represents {b | b ∈ {0, 1}n, d(b, b∗) > z}. The state s2 = 2 can be used to represent search spacetraps [11,15], a topic we will discuss in more detail in Section 4.3. The states s0 = 0 and s1 = 1 represent the part ofthe search space where a strong SLS initialization algorithm will reliably start search, leading to a low number of flipsbefore reaching O, while s2 = 2 gives a high number of flips. Suppose thatPr(Xi+1 = 1 | Xi = 1) = b < Pr(Xi+1 = 2 | Xi = 2) = 1 − a.Here, the probability b of staying at s1, in other words close to s0 while not transitioning to the optimum state s0 orfurther away to s2, is smaller than the probability 1 − a of staying at s2, distant from optimum. In other words, onceFig. 2. A naive Markov chain model (top) with parameters a, b, and c as well as an example of parameter settings (bottom). This is a top–downmodel, in which each state represents multiple states in the underlying exact Markov chain model of the SLS search process.O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990963SIMPLESLS is close to optimum it may be quite likely to find optimum, while on the other hand if SIMPLESLS is farfrom optimum it may be likely to stay far away. These concepts are reflected in the numerical example in Fig. 2.Expected times for first entering the optimum state s0 = 0 is given by our following result.Lemma 15. In the naive Markov chain model, suppose that a(b + c − 1) (cid:8)= 0 and b (cid:8)= 1. Then, the first passage timesm0, m1, and m2 arem0 = 0a + cm1 = −a(b + c − 1)m2 = − a − b + 1a(b + c − 1).Proof. The fact that m0 = 0 is obvious. A hitting time analysis of the Markov chain (5) gives these two simultaneousequations:m1 = 1 + b × m1 + c × m2m2 = 1 + a × m1 + (1 − a) × m2,which under the assumption a(b + c − 1) (cid:8)= 0 has the above solutions. (cid:2)Here is an illustration of the potentially dramatic difference between SLS starting in state s1 = 1 (giving firstpassage time m1) versus in state s2 = 2 (giving first passage time m2).Example 16. Let, in (5), a = 1/1000, b = 7/10, and c = 1/100. Using Lemma 15 we obtaina + cm1 = −a(b + c − 1)m2 = − a − b + 1a(b + c − 1)= 37.93= 1038.Suppose now that we are able to increase the probability of “good” jumps from state s2 = 2 (distant from optimum)to state s1 = 1 (close to optimum) to a = 1/100, while only increasing the probability of “bad” jumps from state s1 = 1to state s2 = 2 to c = 1/50. These changes to a and c may for example be implemented by increasing the SIMPLESLSnoise p, giving the following result.Example 17. Let, in (5), a = 1/100, b = 7/10, and c = 1/50. Using Lemma 15 again givesa + cm1 = −a(b + c − 1)m2 = − a − b + 1a(b + c − 1)= 10.71= 110.7.In this case, we see improvements for both m1 and m2 compared to Example 16; relatively speaking the almost10-fold reduction in passage time m2 is most significant.Benefits of the naive Markov chain model include its simplicity and ease of specification. This again leads toimproved understanding of SLS through the calculation of passage times, extending previous research [15]. However,this naive model is perhaps too simple to capture several important facets of SLS algorithms, in particular there is nodirect representation of noise probability p. We now turn to more realistic Markov chain models, trap Markov chainmodels, in which p is explicitly represented and there is an attempt to strike a balance between the state space sizesof exact and naive Markov chain models.964O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–9904.3. Trap Markov chain analysisIn the following, we continue to use Markov chains but introduce a few extensions, namely a noise parameter p, avariable sub-string length (cid:5) (cid:2) n, and an approach to vary problem difficulty at a higher abstraction level than for thenaive Markov chain model. To make the size of M tractable even for large problem instances, we focus on objectivefunctions f in which it is reasonable to count the number of correct bits in the current bitstring b compared to theoptimum b∗.We introduce an approach and a class of Markov chains inspired by deceptive trap functions [6]. Trap functionsare related to search space traps, which are portions of the search space that are attractive to SLS but do not containoptima [11,15]. In deceptive trap functions, only the number of 1’s (or the unitation) and not their positions determineobjective function values. In deceptive trap functions over bitstrings of length (cid:5) = n, there is a local deceptive optimumat 0 representing b = 0 . . . 0, a global optimum at (cid:5) representing b∗ = 1 . . . 1, and a slope-change location at z.Definition 18 (Trap function). A trap function g(x; (cid:5), z) (abbreviated g(x)) of length (cid:5) with slope-change location z ∈{0, . . . , (cid:5) − 1} is a function with domain {0, . . . , (cid:5)} ⊂ N where g(x) > g(x + 1) for 0 (cid:2) x (cid:2) z − 1 and g(x) < g(x + 1)for z (cid:2) x (cid:2) (cid:5) − 1. There is a unique global optimum g∗ = g((cid:5)) and g(z + 1) > g(z − 1) for z > 0.Intuitively, a trap function is such that for x < z, an SLS algorithm such as SIMPLESLS will greedily (using oG)move towards the local optimum x = 0, representing trapping in the part of search space dominated by local maxima.For x (cid:3) z, SIMPLESLS will greedily (again using oG) move towards the global optimum at x = (cid:5), representing searchin the part of the search space dominated by the global optimum b∗. The size of the domain as well as the placementof the slope-change parameter z determine the difficulty of the trap function for SLS. Here is an example trap functionrelated to SAT. Note that this example is a simplification compared to actual SAT problem instances, which we returnto in Section 7.Example 19 (Easy SAT instance). Consider a conjunctive normal form (CNF) formula with V = 5 variables andC = 20 clauses. Further, assume that the formula has exactly one satisfying assignment and that the number of satisfiedclauses g varies from 15 to 20 as follows: g(0) = 16, g(1) = 15, g(2) = 17, g(3) = 18, g(4) = 19, and g(5) = 20.This is a trap function with slope-change location z = 1, since g(0) > g(1), g(5) > g(4) > g(3) > g(2) > g(1),g(2) > g(0), and g∗ = g(5).Example 19 is illustrated in Fig. 3 along with other examples of varying difficulty. A key point here is that movingthe slope-change location z from the left to the right corresponds to increasing problem instance hardness. Intuitively,easy problem instances (with slope-change location close to b = 0 . . . 0) should be solved by a greedier algorithm thana hard problem instance (with slope-change location close to b = 11 . . . 1). Two complete search spaces are shown inFig. 4.We assume that SLS searches over bitstrings rather than integers (as used in Definition 18), hence we introduce thefollowing definition.Definition 20 (Binary trap function). Let b ∈ {0, 1}(cid:5) be a bitstring and let g(x; (cid:5), z) be a trap function as introducedin Definition 18. Then f (b; (cid:5), z) := g(u(b); (cid:5), z), abbreviated f (b), defines a binary trap function with parameters (cid:5)and z.The concept of trap functions defined over bitstrings might seem somewhat abstract, but it can be used to representcritical aspects of how SLS algorithms get trapped when applied to NP-hard problems such as SAT and MPE. ForSAT, a bitstring b ∈ {0, 1}(cid:5) represents a truth assignment to all (cid:5) = n variables in a CNF formula, for MPE it representsan explanation over (cid:5) = n binary nodes.Definition 21 (Trap). Let c ∈ {0, 1}(cid:5) be a bitstring and let b∗ ∈ {0, 1}(cid:5) be an optimal bitstring. A greedy neighborg ∈ n(c) is a neighbor that is reachable by a greedy SIMPLESLS step. Then c is a trap state (bitstring) if all greedysteps from c increase the distance to all optima: d(g, b∗) > d(c, b∗) for all g and b∗. The search space’s trap is definedas T = {c is trap state | c ∈ {0, 1}(cid:5)}.O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990965Fig. 3. Trap functions g(x; (cid:5), z) = g(x; 5, z) based on the satisfiability problem (3SAT) with V = 5 variables and C = 20 clauses, assuming onesatisfying assignment. We consider five hypothetical problem instances, ranging from very easy (with slope-change location z = 0) to hard (wherez = 4). For an assignment to all V = 5 variables, the number of correct assignments ranges from x = 0 to x = 5 as shown on the x-axis. Thenumber of satisfied clauses is shown on the y-axis. The complete search spaces for Very Easy and Hard are shown in Fig. 4.Our concept of trap is related to search space traps [11,15] as well as search space reachability analysis, wherestates from which a solution is not reachable are determined [51].It is easy to show that the trap size of a binary trap function is as follows.Lemma 22. Let f (b; (cid:5), z) be a binary trap function. The trap size |T | of f is given by(cid:8)|T | =0(cid:2)z−1i=0(cid:12)(cid:11)(cid:5)iif z = oif z (cid:3) 1.(6)The following result holds in general, where Tmax is the largest trap possible over {0, 1}n and Tmin is the smallesttrap possible.Lemma 23. The maximal trap size |Tmax| over {0, 1}n is given by |Tmax| = 2n − n − 1; the minimal trap size is|Tmin| = 0.Proof. In the worst case there is only one optimum b∗ ∈ O. Clearly, n(b∗) can not be part of the trap, n(b∗) (cid:8)⊂ Tmax,and since |n(b∗)| = n + 1 we obtain the desired result. |Tmin| = 0 is obvious. (cid:2)Given the above result, f (b; (cid:5), 0) and f (b; (cid:5), (cid:5) − 1) play distinguished roles as bounding binary trap functions:f (b; (cid:5), 0) is the easiest trap function over {0, 1}(cid:5) and in fact achieves |Tmin| = 0, while f (b; (cid:5), (cid:5) − 1) is the hardesttrap function over {0, 1}(cid:5), achieving |Tmax| = 2(cid:5) − (cid:5) − 1 as follows from (6).A trap Markov chain is induced by a problem instance (such as the ones in Fig. 3) and an SLS algorithm, specif-ically SIMPLESLS, along with its input parameters including noise parameter p. Given a trap functions g(x; (cid:5), z)we construct a trap Markov chain (S, V, P) such that |S| = (cid:5) + 1. When constructing P, we take into account theunderlying binary trap function f (b; (cid:5), z), and also map the slope-change location z into a slope-change state. Thefollowing definition of a trap Markov chain (TMC) describes the performance of SIMPLESLS on a trap function asstated formally in Theorem 25.966O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990Fig. 4. The complete search spaces for two trap functions f (b; 5, 0) and f (b; 5, 4) representing hypothetical SAT problem instances, namely theeasiest and hardest ones according to the trap function definition. The number of satisfied clauses is shown for each point in the search spaces,which are laid out as shown in Fig. 1.Definition 24 (Trap Markov chain). An (cid:5)-bit trap Markov chain with change state z and noise parameter p, abbreviatedas TMC(p; (cid:5), z), is defined as follows. It has (cid:5) + 1 states S = {0, . . . , (cid:5)} and the initial distribution V is defined, forx ∈ S, asPr(X1 = x) =(cid:5)x(cid:12)(cid:11)2(cid:5) .(7)The state transition probabilities of P are for Xi = (cid:5) defined asPr(Xi+1 = (cid:5) | Xi = (cid:5)) = 1 − pPr(Xi+1 = (cid:5) − 1 | Xi = (cid:5)) = p,and for Xi = 0, when z > 0, defined asPr(Xi+1 = 0 | Xi = 0) = 1 − pPr(Xi+1 = 1 | Xi = 0) = p,while Pr(Xi+1 = 1 | Xi = 0) = 1 when z = 0. For internal states Xi (cid:8)= 0 and Xi (cid:8)= (cid:5) we havePr(Xi+1 = x + 1 | Xi = x) =(cid:8)Pr(Xi+1 = x − 1 | Xi = x) =(cid:8)(cid:5)−x(cid:5) p1 − x1 − (cid:5)−xx(cid:5) pfor 1 (cid:2) x (cid:2) z − 1(cid:5) p for z (cid:2) x (cid:2) (cid:5) − 1(cid:5) p for 1 (cid:2) x (cid:2) z − 1for z (cid:2) x (cid:2) (cid:5) − 1with Pr(Xi+1 = y | Xi = x) = 0 for all x and y not listed above.We emphasize that trap Markov chains (TMCs) are idealized models. Quantitatively, the purpose of TMCs is toprovide an interesting range of expected hitting times h(p) including bounding cases (for examples see the lowerbound Very Easy h5,0(p) and the upper bound Hard h5,4(p) in Section 5.1). Qualitatively, the purpose of TMCs is todisplay different shapes of expected hitting time curves, ranging from ones that are monotonically increasing with p(and where p = 0 is the optimal noise level) to ones with a decreasing–increasing pattern and optimal noise levels thatincrease with the value of the slope-change state z. One should not expect to find real-world models of non-trivial sizethat exactly match TMCs, and our intention is not to fit real SLS behavior to TMCs. For results showing that theseidealized models aid in the understanding of experiments with real problem instances, we refer to Section 7.1.Examples of trap Markov chains are presented in Section 5. The reason for using the terminology “trap Markovchain” in Definition 24 should become clear with the following result, where we formally state how trap functions areprocessed by SIMPLESLS.O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990967Theorem 25 (Trap Markov chain). Let f (b; (cid:5), z) be an (cid:5)-bit binary trap function with slope-change location z. If fis given as input to SIMPLESLS along with noise probability p, then a trap Markov chain TMC(p; (cid:5), z) is simulatedup to MAX-FLIPS flips.Proof. Results for the boundary states (cid:5) and 0 follow easily with the exception of z = 0. For z = 0, Lemma 13 applieswith bj = 0 . . . 0 and hence Pr(Xi+1 = 1 | Xi = 0) = 1. We now turn to the internal states 0 < x < (cid:5). First, considerPr(Xi+1 = x + 1 | Xi = x). Conditioning on SIMPLESLS operators oN and oG and using the law of total probabilitygivesPr(Xi+1 = x + 1 | Xi = x) = Pr(Xi+1 = x + 1 | Xi = x, Oi = oN )Pr(Oi = oN )+ Pr(Xi+1 = x + 1 | Xi = x, Oi = oG)Pr(Oi = oG).There are two cases to consider, namely Case (i) 0 < x < z and Case (ii) z (cid:2) x < (cid:5). Case (i): Suppose that 0 < x < z. Inthis case g(x + 1) < g(x − 1), which means that SIMPLESLS only moves from x to x + 1 in case of a noise operationoN , in other words Pr(Xi+1 = x + 1 | Xi = x, Oi = oG) = 0 while Pr(Xi+1 = x + 1 | Xi = x, Oi = oN ) > 0. Thus,we obtainPr(Xi+1 = x + 1 | Xi = x) = Pr(Xi+1 = x + 1 | Xi = x, Oi = oN )Pr(Oi = oN ).Since Pr(Oi = oN ) = p and Pr(Xi+1 = x + 1 | Xi = x, Oi = oN ) = ((cid:5) − x)/(cid:5), we getPr(Xi+1 = x + 1 | Xi = x) = (cid:5) − x(cid:5)p,which by law of total probability and the fact that we have a random walk without internal self-loops givesPr(Xi+1 = x − 1 | Xi = x) = 1 − (cid:5) − x(cid:5)p.Case (ii): Next, suppose that z (cid:2) x < (cid:5). In this case g(x + 1) > g(x − 1). The derivation is similar to above, resultinginPr(Xi+1 = x − 1 | Xi = x) = xp(cid:5)Pr(Xi+1 = x + 1 | Xi = x) = 1 − x(cid:5)thus concluding the proof. (cid:2)p,Trap Markov chains are related to the naive Markov chain model in Section 4.2 as well as the so-called simpleand branched Markov chain models introduced by Hoos [15]. The simple and branched Markov chain models capturesimilar phenomena, but our trap Markov chains have a few novel and important features. First, a trap Markov chainTMC(p; (cid:5), z) is parametrized with a noise parameter p, which is essential when analyzing the impact of noise onSLS. Second, while it is based on empirical considerations, the trap Markov chain is an analytically derived model(see above as well as earlier work [6]). Markov chains have also been used in the analysis of genetic and evolutionaryalgorithms [3,9,12,48]. Generally, this analysis emphasizes population-level effects and is quite different from ouranalysis in this article. In particular, we do not know of any analysis of genetic or evolutionary algorithms that includesnoise level p as an explicit parameter. Our approach allows, for example, for the derivation of closed form solutionsfor expected hitting times which is defined as follows.Definition 26 (TMC hitting time). The notation hk,z(p) is used for the expected hitting time for a trap Markov chainTMC(p; k, z).Expected hitting times are, in a sense, more important than their underlying Markov chains, and we often do notexplicitly show the Markov chains. There are several reasons for our emphasis on expected hitting times: First, theirempirical counterpart, noise response curves, are a common way of reporting results in the SLS literature [7,8,14,16,26]. Second, optimal noise levels p∗ can be derived analytically from expected hitting time formulas as we will seein Section 5. Third, displaying the Markov chain transition matrices is in practice impossible for the many non-trivialsearch spaces used in this article, while expected hitting time expressions are often compact. We do, however, showand discuss the Markov chains for a few smaller problem instances in Section 7.1.968O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990Fig. 5. Trap Markov chain models TMC(p; 5, z) for bitstrings of length (cid:5) = 5, and where the slope-change state z is colored and varies from z = 0(at the bottom) to z = 4 (at the top). Each of these trap Markov chain models represents the combined effect of a problem instance (see Fig. 3)and SIMPLESLS. These are top–down models where each state represents multiple states in the underlying exact Markov chain model of the SLSprocess.5. Trap Markov chain examplesIn this section we illustrate the trap Markov chain model, presented in Section 4.3, by means of examples.5.1. Analysis of trap Markov chainsPerhaps the easiest way to illustrate the utility of trap Markov chain models is to discuss concrete problem in-stances.Example 27 (Trap Markov chains). See Fig. 5 for graph representations of the transition probabilities for theTMC(p; 5, 0), TMC(p; 5, 1), TMC(p; 5, 2), TMC(p; 5, 3), and TMC(p; 5, 4) Markov chains.In preparation for a formal result, we provide some intuition. Let us consider TMC(p; 5, 0), the very easy case, andsuppose first that p = 0. It is then easily seen from Fig. 5 that regardless of the initial state 0 (cid:2) X0 (cid:2) 5, SIMPLESLSsearch proceeds directly towards state 5 without making any transitions from Xi = k to Xi+1 = k − 1 for k > 1, i > 0.Once p > 0, there is a chance that SIMPLESLS makes such transitions, and the search becomes longer.The following result, illustrated in Fig. 6, gives expected hitting times for SIMPLESLS as formalized in Exam-ple 27. This shows how the noise probability p impacts the expected hitting time hk,z(p) for problem instances ofvarying difficulty. Difficulty increases with z, reflecting Lemma 22.Lemma 28. Assuming SIMPLESLS initialization uniformly at random, the expected hitting times for TMC(p; 5, z),where 0 (cid:2) z < 5, are for SIMPLESLS as follows:h5,0(p) = 24p4 − 1870p3 + 2875p2 − 625p − 75008(p − 5)(2p − 5)(20p + 3p2 − 25)h5,1(p) = 48 875p − 39 175p2 + 17 865p3 + 194p4 + 62532p(4p − 5)(3p − 5)(p − 5)(2p − 5)O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990969Fig. 6. The expected hitting time h(p) as a function of noise probability p for the five 5-bit trap Markov chain models. The curves are, starting at thebottom, for: h5,0(p) (dashed brown), h5,1(p) (solid blue), h5,2(p) (dashed green), h5,3(p) (solid red), h5,4(p) (dashed purple). (For interpretationof the references to color in this figure legend, the reader is referred to the web version of this article.)h5,2(p) = 750p − 18 625p2 + 9670p3 − 4112p4 − 1875h5,3(p) = 33p4 + 1465p3 + 1550p2 − 750p + 125064p2(4p − 5)(3p − 5)(2p − 5)48p3(4p − 5)(3p − 5)h5,4(p) =−912p4 − 4530p3 − 3875p2 + 3250p − 8125384p4(4p − 5)Proof. The Markov chain’s initial probability distribution isV = (π0, π1, π2, π3, π4, π5),which according to (7) is(cid:13) (cid:11)(cid:12)(cid:11)(cid:12)55022525(cid:11)5125V =(cid:12),,(cid:12)(cid:11)5325,(cid:12)(cid:11)5425,(cid:12)(cid:11)5525(cid:14).,(8)We focus on h5,3(p). In order to obtain h5,3(p) from the TMC(p; 5, 3) model we form the following simultaneoussystem of equations of expected first passage times mi for 0 (cid:2) i (cid:2) 5:pp(cid:13)(cid:13)m2 = 1 +m1 = 1 +m0 = 1 + (1 − p)m0 + pm1(cid:14)m0 + 41 − 455(cid:14)m1 + 31 − 355(cid:13)1 − 35(cid:13)1 − 45m3 = 1 + 35m4 = 1 + 45m5 = 0,pm2 +pm3 +pppm2pm3(cid:14)m4(cid:14)m5which when solved gives first passage times {m0, . . . , m5} as followsm0 = 775p2 − 375p − 205p3 + 204p4 + 625m1 = 475p2 − 375p + 215p3 + 60p4 + 625300p3 − 420p4 + 144p5300p3 − 420p4 + 144p5970O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990300p3 − 420p4 + 144p5m2 = 925p2 − 750p + 140p3 + 24p4 + 625m3 = 285p2 − 50p − 60p3 + 125100p2 − 140p3 + 48p4m4 = 15p + 22p2 + 2525p − 35p2 + 12p3m5 = 0.Introducing (8), we now compute E(T | X0 = i) Pr(X0 = i) = miπi , for 0 (cid:2) i (cid:2) 5, as followsm0π0 = 775p2 − 375p − 205p3 + 204p4 + 625300p3 − 420p4 + 144p5m1π1 = 475p2 − 375p + 215p3 + 60p4 + 625300p3 − 420p4 + 144p5m2π2 = 925p2 − 750p + 140p3 + 24p4 + 625m3π3 = 285p2 − 50p − 60p3 + 125100p2 − 140p3 + 48p4300p3 − 420p4 + 144p5(cid:12)(cid:11)5325(cid:12)(cid:11)5025(cid:12)(cid:11)5125(cid:12)(cid:11)5225m4π4 = 15p + 22p2 + 2525p − 35p2 + 12p3.(cid:12)(cid:11)5425(cid:2)5i=0 miπi as desired. The remaining h5,j (p) for j ∈ {0, . . . , 2} and j = 4 areAdding up, we obtain h5,3(p) =developed in a similar manner and to save space we do not detail the proof here. (cid:2)Illustrating Lemma 28, the graphs in Fig. 6 show the impact of varying noise p for the example TMCs. The differ-ence in the shapes of the curves for the easiest case h5,0(p), compared to the hardest case h5,4(p), is quite dramatic.One extreme, h5,0(p), is monotonically increasing. The other extreme, h5,4(p), is first monotonically decreasing, thenmonotonically increasing. Clearly, this has an impact on the optimal noise level, which we discuss next.5.2. Optimal noise level for trap Markov chainsFig. 6 clearly shows the difference in optimal noise levels p∗. For h5,0(p), it is intuitively clear that one should uselow noise p, more specifically p∗= 0, since this enables SIMPLESLS to greedily hill-climb to b = 11111 without5,0taking unnecessary downhill noise steps. For h5,4(p), on the other hand, one should intuitively use high noise pin order to let SIMPLESLS more easily escape the trap b = 00000 containing the local (but non-global) optimum.Given that expected hitting times h(p) are rational functions, for example as derived in Lemma 28, optimal noiseprobabilities can be derived analytically. The following example of deriving optimal noise p∗ illustrates the use of thequotient rule.Example 29. Consider h5,3(p) from Theorem 28; using (16) gives(cid:6)h5,3(p) = − 198p6 + 17 580p5 + 1850p4 − 72 250p3 + 96 250p2 − 106 250p + 46 8753456p8 − 20 160p7 + 43 800p6 − 42 000p5 + 15 000p4,and by solving for p in h(cid:6)p∗5,3= 0.6687.5,3(p) = 0 and checking boundaries p = 0 and p = 1 we obtain optimal noise probability5.3. Discussion of trap Markov chainsSeveral points may be made with respect to Lemma 28 and Fig. 6. First, we note that these are all convex rationalfunctions of the form h(p) = P (p)/Q(p), where P (p) and Q(p) are polynomials. Further, if we for a short momentdisallow the use of restarts, these examples illustrate how p = 0 can lead to unbounded hitting times h(p); see Fig. 6.O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990971The reason for this is that p = 0 can, for certain (unfortunate) initializations, give unbounded search in the trap part ofTMCs. Fig. 6 clearly shows the unboundedness of h5,z(p) for z (cid:3) 1 and p = 0. Similar trapping effects with respectto local minima can take place in real problem instances, illustrating the need for p > 0 or MAX-FLIPS < ∞ in SLS.2Second, while the trap function setting is restricted, similar patterns appear in experiments in the literature [14,26]as well as in Section 7. For instance, six different SLS algorithms each had, when tested on 400 hard random 3SATproblem instances using variable noise, a relatively clear noise level where it performed optimally [26]. (Note thatthe dependent variable was fraction of problem instances solved rather than mean run time, thus the curves wereconcave, with a maximum value as optimum, rather than convex as ours [26].) Along similar lines, three empiricalnoise response curves for the Novelty+ SLS algorithm all have convex shapes similar to h5,3(p) [14]. In all thesecases, performance improves with increasing noise until it hits an optimum, and performance then deteriorates withincreasing noise. The pattern is in other words similar to the curves for h5,2(p), h5,3(p), and h5,4(p). The benefitof very small noise levels, illustrated by h5,0(p) and h5,1(p) has, to our knowledge, received less attention in theliterature. However, there are SLS results in the areas of planning and scheduling where small noise levels haveempirically been shown to be optimal [7,8].Third, we notice that the curves for h5,z(p), for 0 (cid:2) z (cid:2) 4, get closer as p increases, and in particular thatlimp→1h5,0(p) = · · · = limp→1h5,4(p) = 88724.There is a much greater performance difference between the problem instances for small p compared to for large p.Specifically, fix p1 and (cid:6)p, say p1 = 0.7 and (cid:6)p = 0.1. Now, form p= p1 + (cid:6)p = 0.8.−Clearly, the difference in performance between the problem instances h5,0(p) through h5,4(p) is much greater for p11 . This suggest that in general, setting the noise level too low, to p∗ − (cid:6)p, may be more detrimental thanthan for psetting it too high, to p∗ + (cid:6)p, when operating under conditions of uncertainty about p∗ and the problem instancedistribution. Similar recommendations have in fact already been made based on experimental observations [17].= p1 − (cid:6)p = 0.6 and p+1−1+We believe these results shed additional light on the significant impact of varying noise as observed in experiments[17,26,45]. When one does not know much about the problem instance distribution ahead of time, these results mayalso argue in favor of the use of adaptive noise [14,26].6. Hitting time analysisExperimentally, it has been observed that curves for mean SLS run times [14,16] and the fraction of solved instanceswhen using SLS [26], plotted as functions of noise p, have shapes suggesting underlying convex functions as wellas rational functions for hitting times. We note that convexity of hitting times is also observed in Section 5.1 and inexperiments in Section 7.We now provide several general SLS hitting time results, focusing on rational functions, convexity, and polyno-mials. The justifications are a bit different for these cases. The condition of rationality is supported by our analysisin Section 6.1 below. For convexity, the justification is empirical results in the literature as well as in this article. Forpolynomials, our justification is partly Weierstrass’ theorem, partly our empirical results (see Section 7). The analysisis always with respect to a specific SLS model (M, O), where M = (S, V, P) has k := |S| states. Further, T is (asbefore) a random variable representing hitting time.The results in this section apply to hitting times in general, and do not depend on the approximate Markov chainmodels developed in Section 4 and Section 5. In other words, readers who found the approximate Markov chainmodels too restrictive may still want to consider the more general analysis provided in this section.6.1. Single problem instancesWe consider single problem instances and assume that the noise probability p = Pr(O = oN ) is the independentparameter. Let P (p) and Q(p) be polynomials. Based on our results in Section 5.1, one might hypothesize that the2 An alternative way of escaping from traps is to use restarts, and an SLS practitioner will most likely use both non-zero noise and non-infiniteMAX-FLIPS. While the topic of restart is crucially important in SLS, our main focus in this article is not on the effect of restart, but on the effectof noise on SLS, and a detailed discussion of the joint effect of restarts and noise is beyond the scope of this article.972O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990expected hitting time for a problem instance has the form of a rational function h(p) = P (p)/Q(p), and this is indeedsupported by the following analysis. From linear algebra we know that an n × n system of equations has no solution,exactly one solution, or infinitely many solutions. While in theory there might exist conditions under which a systemof hitting time equations for SLS have no or infinitely many solutions, the one solution case is clearly of greatestpractical interest. We will in this article assume the existence of exactly one solution that can be found by Gaussianelimination, as discussed below, in a finite number of steps. In the proof of the following theorem, the key idea isto perform Gaussian elimination in a symbolic fashion such that the noise parameter p is preserved throughout thederivation.Theorem 30. Consider an SLS model (M, O), where the Markov Chain M = (S, V, P) is defined over a bitstring oflength n and with noise parameter p. Let κ = 2n, assume optimum states {sλ, . . . , sκ } = O where λ (cid:2) κ, and form asystem of equations for M’s expected first passage times mi for 1 (cid:2) i (cid:2) κ. There exists an equivalent upper triangularsystem U m = b, where m = (m1, . . . , mλ−1)T , in which all coefficients in U and b are rational functions of p.Proof. We form, based on M, this system of equations for expected first passage times into O:m1 = 1 + f1,1(p)m1 + f1,2(p)m2 + · · · + f1,κ−1(p)mκ−1 + f1,κ (p)mκm2 = 1 + f2,1(p)m1 + f2,2(p)m2 + · · · + f2,κ−1(p)mκ−1 + f2,κ (p)mκ· · ·mλ−1 = 1 + fλ−1,1(p)m1 + fλ−1,2(p)m2 + · · · + fλ−1,κ−1(p)mκ−1 + fλ−1,κ (p)mκmλ = 0· · ·(cid:2)κi=1 fj,i(p) = 1 for 1 (cid:2) j (cid:2) κ.mκ = 0,where fj,i(p) = (αj,i + βj,ip)/γj,i for constants αj,i, βj,i ∈ N, γj,i ∈ N+, andClearly, this system can be written as(cid:11)(cid:12)1 − f1,1(p)m1 − f1,2(p)m2 − · · · − f1,κ−1(p)mκ−1 − f1,κ (p)mκ = 1−f2,1(p)m1 +· · ·m2 − · · · − f2,κ−1(p)mκ−1 − f2,κ (p)mκ = 1(cid:12)1 − f2,2(p)(cid:11)(cid:12)(cid:11)mλ−1 − fλ−1,κ (p)mκ = 11 − fλ−1,κ−1(p)−fλ−1,1(p)m1 − fλ−1,1(p)m1 − · · · +mλ = 0· · ·mκ = 0.We proceed by means of induction on the number of elementary operations, iteratively creating a system of equationsS(t+1)from system S(t) for t (cid:3) 1. Here, S(1) is the following system created from the above by dropping the triv-ially rational mλ = · · · = mκ = 0, substituting mλ = · · · = mκ = 0 into the other equations, and performing a slightrenaming:1,2(p)m1 + · · · + a(1)2,2(p)m2 − · · · + a(1)1,λ−1(p)mλ−1 = b(1)2,λ−1(p)mλ−1 = b(1)1 (p)2 (p)a(1)1,1(p)m1 + a(1)a(1)2,1(p)m1 + a(1)· · ·λ−1,1(p)m1 + a(1)a(1)For the base case t = 1, a(1)tth step (where t (cid:3) 2):λ−1,2(p)m2 − · · · + a(1)i,j (p) and b(1)iλ−1,λ−1(p)mλ−1 = b(1)κ−1(p).(p) are clearly rational. Following Gaussian elimination, we have for theO.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990i,ki,k (p) = a(t−1)r (t)i,j (p) = a(t−1)a(t)i (p) = b(t−1)b(t)i,j(p)/a(t−1)k,k(p) − r (t)i,k(p) − r (t)i,ki(p)× a(t−1)k,j× b(t−1)kfor k + 1 (cid:2) i (cid:2) λ − 1 and t (cid:2) k (cid:2) λ − 1(p)for k + 1 (cid:2) i, j (cid:2) λ − 1(p)for k + 1 (cid:2) i, j (cid:2) λ − 1 and t (cid:2) k (cid:2) λ − 1973(9)(10)(11)(p), a(t−1)Under the inductive hypothesis that a(t−1)(p) are all rational,i,jit follows that the left hand sides in (9), (10), and (11) are all rational as well, since rationality is closed underaddition and multiplication as applied there. (Rational functions are in abstract algebra known to be fields, which areclosed under multiplication and addition.) Consequently, after a finite number of Gaussian elimination steps, an uppertriangular system U m = b is obtained where all coefficients in U and b are rational functions in p. (cid:2)(p) and b(t−1)(p), a(t−1)k,j(p), b(t−1)i(p), a(t−1)k,ki,kkThe above theorem creates upper triangular systems; we now turn to the impact of Gaussian back elimination onsuch systems.Theorem 31. Suppose that we have a system of equations, in upper triangular form U m = b, for expected first passagetimes mi(p), where 1 (cid:2) i (cid:2) λ − 1 < κ. Further suppose that all entries in U and b are non-zero rational functionsof p. Then there exists a rational function Pi(p)/Qi(p) such that mi(p) = Pi(p)/Qi(p).Proof. We perform back substitution on U m = b. For arbitrary mi(p) we consequently have:bi(p) −mi(p) =(cid:2)λ−1k=i+1 ai,k(p)mk(p)ai,i(p)for i = λ − 1, λ − 2, . . . , 1.From Theorem 30 we know that bi(p) in b and ai,k(p), and ai,i(p) in U are rational, hence any mi(p) above is alsorational due to closure under addition and multiplication. (cid:2)The above result shows how the noise probability p impacts the expected first passages times mi , where1 (cid:2) mi (cid:2) 2n. In particular, we have rational function mi(p) = Pi(p)/Qi(p) where we put Pi(p) = 0 for λ (cid:2) i (cid:2) κ.We next show that the expected hitting time E(T ) is a rational function h(p), if we assume that first passage timesare rational functions.Theorem 32. Let the first passage time mi(p) = E(T | X0 = i) be a rational function of p for any 1 (cid:2) i (cid:2) k, andsuppose that Pr(X0 = i) is constant. Then the expected hitting time E(T ) is a rational function of p, h(p).Proof. For random variables T and C, the law of conditional expectation says that E(T ) is given by(cid:3)E(T ) =E(T | C = i)Pr(C = i).(12)i∈(cid:11)(C)Here, we have (cid:11)(C) = {1, . . . , k} and E(T | C = i) = E(T | X0 = i) = mi(p). Since rational functions are closedunder multiplication with constant Pr(X0 = i), and under addition, we obtain the desired result. (cid:2)The following result follows easily from our results so far in this section and gives expected hitting times forSIMPLESLS in general.Corollary 33 (Rationality of SIMPLESLS hitting time). Consider an SLS model (M, O), where M = (S, V, P) is anexact Markov Chain defined over a bitstring of length n and with noise parameter p. The expected hitting time for Mis a rational function of p, h(p) = P (p)/Q(p), where P (p) and Q(p) are polynomials.Proof. From Theorem 31 it follows that first passage times are mi(p) = Pi(p)/Qi(p) for 1 (cid:2) i (cid:2) 2n, where Pi(p)and Qi(p) are polynomials. Applying Theorem 32, it follows that the expected hitting time h(p) for M is a rationalfunction h(p). (cid:2)974O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990The above result generalizes Theorem 28, and shows that rational functions are of great interest in the analysis ofSLS. In particular, they are analytical counterparts to noise response curves from the experimental literature on SLS[7,8,14,16,26].For a couple of reasons, we often employ polynomial regression rather than rational function regression. The firstreason is that over an interval [a, b], in our case [a, b] = [0, 1] since 0 (cid:2) p (cid:2) 1, Weierstrass’ celebrated theorem tellsus that polynomials P (x) that give arbitrary good approximations exist.Theorem 34 (Weierstrass). Suppose that f (x) is continuous on [a, b] and let ε > 0. Then there exists a polynomialP (x) such that(cid:15)(cid:15)(cid:15) < ε,(cid:15)f (x) − P (x)where (cid:9) · (cid:9) denotes the uniform norm over the interval [a, b].The second reason for our use of polynomial regression is that it is better understood and more wide-spread thanrational function regression. For these reasons we use polynomial regression rather than rational function regressionin experiments in Section 7.We now provide a sufficient condition for the expected hitting time E(T ) to be a convex function h(p).Theorem 35. Let the first passage time mi = E(T | X0 = i) be a convex function of p for any 1 (cid:2) i (cid:2) k, and supposethat Pr(X0 = i) is constant. Then the expected hitting time E(T ) is a convex function of p, h(p).Proof. Similar to the proof of Theorem 32, we use conditional expectation (12) and observe that convexity is pre-served under nonnegative multiplication and addition, and we obtain the desired result. (cid:2)Experimental results giving noise response curves for individual problem instances are reported in Section 7.1 (forsynthetic instances) and in Section 7.2 (for BNs from applications).6.2. Mixtures of problem instancesWe now investigate multiple problem instances, or classes of instances, using finite mixture distributions. Again, weconsider noise probability p = Pr(O = oN ) to be the independent parameter. We assume that our problem instancescome from a probability distribution as follows.Definition 36 (SLS mixture model). Suppose there are ξ SLS models {c1, . . . , cξ }, with ci = (Mi, Oi) for 1 (cid:2) i (cid:2) ξ .ξi=1 Pr(C = ci) = 1, then {(c1, Pr(C = c1)),If each SLS model is observed with probability Pr(C = ci), where. . . , (cξ , Pr(C = cξ ))} defines an SLS mixture model.(cid:2)Definition 37 (First passage time, mixture). Consider an SLS mixture model {(c1, Pr(C = c1)), . . . , (cξ , Pr(C = cξ ))}.Let T be a conditional first passage time random variable Pr(T = t | C = ci) for 1 (cid:2) i (cid:2) ξ . The first passage time ofthe mixture is defined asPr(T = t) =ξ(cid:3)i=1Pr(T = t | C = ci)Pr(C = ci).This is a finite mixture distribution with ξ mixture components. There are a number of reasons why such mixturesare interesting for stochastic local search. Algorithms for NP-hard problem are typically developed with a class ormixture of problem instances in mind. Problem instances may be dynamically generated, and thus the generationprocess induces an SLS mixture. During early system design the system being modeled is not completely known,and a mixture of BNs may represent all possibilities [28]. Finally, a given C/V -ratio also defines a problem instancemixture [33,34] and in Section 7.2 we empirically investigate such mixtures.In a mixture, there is a distinct Markov chain for each problem instance. Informally, we first pick the ith Markovchain with probability Pr(C = ci), and then use that Markov chain to give Pr(T = t | C = ci) along with the chain’spassage time (Definition 3) and instance hitting time (Theorem 5).O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990975Definition 38 (SLS mixture hitting time). Consider an SLS mixture model with ξ components and expected hittingtimes hj (p) for 1 (cid:2) j (cid:2) ξ . The SLS mixture hitting time H (p) is defined asH (p) =ξ(cid:3)j =1hj (p)Pr(C = cj ).Having formally introduced H (p), we next show what this function actually is. To keep the notation simple, weassume that the state space size is the same for each Markov chain Mi in the SLS mixture model {c1, . . . , cξ } ={(M1, O1), . . . , (Mξ , Oξ )}.Theorem 39. Let, for an SLS mixture model with ξ mixture components, first passage time be a random variable T .The expected hitting time E(T ) is the mixture hitting time H (p); E(T ) = H (p).Proof. The expected value for T for the mixture, given initial state X0 = si and component C = cj in the SLS mixturemodel, is defined similar to Definition 3 as E(T | X0 = si, C = cj ). Using the law of conditional expectation (12), weobtainE(T | C = cj ) =κ(cid:3)i=1E(T | X0 = si, C = cj )Pr(X0 = si | C = cj ).Using on (13) the law of conditional expectation (12) again, we obtainE(T ) ==ξ(cid:3)j =1ξ(cid:3)j =1E(T | C = cj )Pr(C = cj )hj (p)Pr(C = cj ),(13)(14)where we used E(T | C = cj ) = hj (p) from Corollary 33. By Definition 38, (14) is H (p) and we have the desiredresult. (cid:2)We now turn to rational functions; our interest in them is motivated by our results in Section 6.1.Theorem 40. Consider an SLS mixture model {(c1, Pr(C = c1)), . . . , (cξ , Pr(C = cξ ))}. Suppose that the individualcomponent hitting times hi(p), for 1 (cid:2) i (cid:2) ξ , are rational functions. Then the SLS mixture hitting time H (p) is alsoa rational function.Proof. Since hi(p) by assumption is a rational function, it can be written as hi(p) = Pi(p)/Qi(p), where Pi(p) andQi(p) are polynomials. Due to closure under multiplication, Pr(C = ci)hi(p) = Pr(C = ci)Pi(p)/Qi(p) is also arational function. There is also closure under addition, and thusH (p) =ξ(cid:3)i=1Pr(C = ci)hi(p),which is the definition of H (p), is also a rational function. (cid:2)From the results above, the expected hitting time for SIMPLESLS for a mixture of problem instances can bederived.Corollary 41 (Rationality of SIMPLESLS mixture). Consider an SLS mixture model {(c1, Pr(C = c1)), . . . ,(cξ , Pr(C = cξ ))}. Suppose that each ci = (Mi, Oi), for 1 (cid:2) ci (cid:2) ξ , is an exact SIMPLESLS model with noiseparameter p. The expected hitting time for this mixture is a rational function of p, H (p) = P (p)/Q(p), where P (p)and Q(p) are polynomials.976O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990Proof. From Corollary 33 it follows that a SIMPLESLS hitting time hi(p), where 1 (cid:2) i (cid:2) ξ , is a rational function.Applying Theorem 40 we conclude that the mixture H (p) is also a rational function. (cid:2)The trap Markov chain results presented in Sections 4.3 and 5 were originally intended to be models of individualproblem instances. However, since the hitting time for a problem instance is a rational function, it follows—as statedabove—that the hitting time for a mixture of problem instances (from some class, for example as defined by a partic-ular C/V -ratio) is also a rational function. Hence, one can also use our trap Markov chain results as a model for thehitting time of a mixture of problem instances. This is a mathematical consequence of the functional form of hittingtimes rather than an artifact of our analysis.We now turn our attention to convex functions.Theorem 42. Suppose that problem instance hitting times hi(p), for 1 (cid:2) ci (cid:2) ξ , are convex functions. Then themixture hitting time H (p) is also a convex function.Proof. Since hi(p) is convex, Pr(C = ci)hi(p) is convex due to the fact that convexity is preserved under nonnegativemultiplication. Further,H (p) =ξ(cid:3)i=1Pr(C = ci)hi(p)is convex since convexity is preserved under addition. (cid:2)Convexity is important because local optimality means global optimality in convex functions, thus simplifyingoptimization algorithms. Polynomials are also helpful in noise optimization, and Section 7.2 contains experimentswith mixtures of problem instances along with polynomial regression results.6.3. Optimal noise levelsWhat is the optimal value p∗ of an SLS noise parameter p? This is the question that will be discussed now, in lightof our analysis earlier in this section.Definition 43 (Hitting time minimization). Let the SLS noise be p and let h(p) be an expected hitting time. Theoptimal noise probability, for minimizing h(p), is defined asp∗h= arg min0(cid:2)p(cid:2)1h(p),with minimal expected hitting time h∗ = h(p∗h).(15)We note that h(p) in (15) can be the expected hitting time for one problem instance, h(p) = hi(p), or a problemh is therefore with respect to one problem instance or ainstance distribution, h(p) = H (p). Optimal noise level p∗mixture of problem instances.If we make certain assumptions about the form of h(p), further progress can be made. We now consider differen-tiable h(p), which it certainly is if it is a rational function or a polynomial. We may then take the derivative h(cid:6)(p)and in order to find the optimal SLS noise level p∗h solve the equation h(cid:6)(p) = 0. Forming h(cid:6)(p) for polynomial h(p)is trivial, however it is a mathematical fact that in the general case, polynomials have complex roots. In other words,solving h(cid:6)(p) = 0 or r (cid:6)(p) = 0 (where r(p) is expected run time, see Section 6.4) may give complex solutions only.Certain strict subsets of polynomials, for instance polynomials with real coefficients of odd degree, have at least onereal root. Given this setting, there are two options for h(p) or r(p) of odd degree (so h(cid:6)(p) = 0 and r (cid:6)(p) have evendegree, thus they may not have at least one root): (i) Disregard all h(p) or r(p) of even degree a priori, since at leastone real root for h(cid:6)(p) = 0 or r (cid:6)(p) = 0 is not guaranteed. (ii) Not disregard all h(p) or r(p) of even degree a priori,since even though there is no guarantee for one or more real roots, in many cases (for example, in all cases excepttwo in Table 3 containing r (cid:6)(p)) real roots may be found and provide useful insight. In a noise optimization algorithmO.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990977based on computing h(cid:6)(p) = 0 one would perhaps prefer to follow approach (i). In this article, however, the purposeis analysis and insight rather than optimization algorithm development and thus we prefer (ii).When h(p) is a rational function, h(p) = P (p)/Q(p), the well-known quotient rule has this form:(cid:6)h(p) = Q(p)P (cid:6)(p) − P (p)Q(cid:6)(p)Q(p)2.(16)In addition to a better understanding of the noise phenomenon, the results above may also serve—in futureh. For example, rational function or polynomial re-h is estimated (in part) empirically, the notation ˆp∗h andresearch—as a basis for improved algorithms for computing p∗gression may play a role in such improved algorithms. When p∗terminology such as optimized noise level or estimated optimal noise level is used.6.4. Initialization and restartUsing more advanced initialization algorithms than initialization uniformly at random has proven to be a powerfulway to improve SLS performance [22,30,31,36,37]. SIMPLESLS can clearly support different ways of initializing b,and SGS in fact takes an initialization algorithm portfolio I as input. In the Markov chain M, the initial distribu-tion V then needs to reflect the particular initialization algorithm portfolio. In general, each initialization algorithm oroperator has a distinct initialization distribution V, and Theorem 28 can be adapted correspondingly to reflect this.Another technique, namely restarts or using MAX-FLIPS < ∞, has been shown to be beneficial in SLS [38] aswell as in systematic search [10]. In many cases, restarts play a central role in SLS and using a close to optimalMAX-FLIPS value is essential for strong performance [38,43]; for TMCs this was observed in Section 5.3. In recentresearch, an approach to dynamically optimizing the SLS restart parameter MAX-FLIPS has been developed [41,42],based on learned Bayesian networks that predict inference run times [19].In general, it is consequently important to distinguish between expected run time and expected hitting time. Wenow formally introduce expected run time.Definition 44 (Expected run time). Let X, a random variable, be the number of flips performed by SIMPLESLS whenthe noise probability is p. The expected run time (or number of flips) is defined as r(p) = E(X).Note that the concepts of run time and expected run time do not (necessarily) use Markov chains. Our hitting timeresults, on the other hand, rely on the use of Markov chains. Unfortunately, SLS restarts, which may take place whenMAX-FLIPS (cid:8)= ∞ in SIMPLESLS, may violate the Markov property.3Expected run times are well-defined even when restarts occur, and we are interested in their minimization.Definition 45 (Run time minimization). Let r(p) be the expected run time (or number of flips) for SIMPLESLS. Theoptimal noise probability, which minimizes r(p), is defined as∗pr= arg min0(cid:2)p(cid:2)1r(p),with minimal expected run time r ∗ = r(p∗r ).= p∗If MAX-FLIPS = ∞ then obviously p∗rh; however in general it is clear that p∗h. In the theoretical partof this article we are discussing the optimal noise probability with respect to expected hitting time, p∗h, while in theexperimental part of this article we are in addition discussing optimal noise probability with respect to expected runtime, p∗h is referred to, we will simply say “optimal noiseprobability” and p∗. In Section 7, we empirically investigate estimates of expected hitting times ˆh(p) as well asexpected run times ˆr(p) using very similar techniques. It turns out that the noise response curves for expected runtimes are similar to those for expected hitting times.r . As it should be clear from the context whether p∗r or p∗(cid:8)= p∗r3 In making this claim, we assume that Markov chain states do not reflect the number of flips made. One could perhaps expand the exact Markovchain model to also reflect number of flips made by SIMPLESLS, however this is beyond the scope of this work.978O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–9907. ExperimentsOur analysis made certain assumptions and simplifications that raise questions such as the following: Is there arelationship between the trap Markov chain model and real problem instances? What is the SLS search behavior onsets of problem instances, corresponding to SLS mixtures? What is the impact of varying noise, on SLS run times, inmore realistic, large-scale problem instances from applications?To answer these questions, we here report on real search behavior from experiments with Bayesian networks us-ing SGS, an SLS system. There is strong evidence that a problem instance that is hard for one SLS algorithm isalso hard for other SLS algorithms [17], hence we investigate one SLS system in depth. Stochastic greedy search(SGS), which can simulate SIMPLESLS, computes MPEs and supports multiple search operators and initializationoperators [27,30]. SGS has these input parameters: β—a BN; f ∗—MPE probability Pr(x∗i ); I—a portfolio of ini-tialization algorithms; S—a portfolio of search algorithms; MAX-FLIPS—the number of flips before a restart; andMAX-TRIES—the number of tries before termination. For S, our focus is on greedy operators, named BM and GM,as well as noisy operators, named NU, BS, and GS. NU implements uniform noise oN as described in Section 3.BS and GS are noisy but biased towards improving the current explanation’s probability. BM and GM, which corre-spond to oG, are pure hill-climbers and maximize gain without any noise. Without going into too much detail, whichis covered elsewhere [27,30], suffice it to say that BM and BS operate on gain in probability based on (2), while GMand GS are probabilistic generalizations of GSAT gain [45,46] and have turned out to be powerful in BNs with manydeterministic nodes [27,30].In Section 7.1, based on 100 small synthetic BNs, we investigate the connection between trap functions, trapMarkov chains, and real SGS search behavior. In Section 7.2 we investigate real SGS search behavior using 800 syn-thetic BNs, emphasizing BNs of varying C/V -ratio and approximation using polynomial regression. In Section 7.3,we experiment with SGS using application BNs, and also investigate noise strategies that go beyond uniform randomnoise as well as more advanced initialization algorithms than initialization uniformly at random.7.1. Experiments with small synthetic Bayesian networksWhat is the relationship between the TMC models from Sections 4 and 5 and real SLS search behavior? In order toclearly link the TMC analysis and experimental parts of this article, we here investigate 100 randomly generated 3SATproblem instances, represented as BNs, where the search space is the same size as the example trap Markov chainsused in our analysis. In the following, we first discuss how a sample of synthetic problem instances was generated,and then discuss noise response experiments for all satisfiable problem instances in the sample. A detailed analysisthen follows, where we show the complete search spaces for two interesting problem instances, analytically derivetheir Markov chains, and compare analytical results with empirical SGS search behavior.7.1.1. Methodology for generating small synthetic Bayesian networksLet, in a Bayesian network (BN), V be the number of root nodes and C the number of non-root nodes. It hasbeen demonstrated that the ratio C/V is a key parameter for SAT and BN inference hardness for randomly generatedproblem instances [27,33,34]. For BNs, the C/V -ratio can be used to predict upper and lower bounds on the optimalmaximal clique size (or treewidth) of the induced clique tree for bipartite BNs randomly generated using the BPARTalgorithm [27,29,33]. The BPART algorithm has these input parameters: Q—CPT type of root nodes; F —CPT typeof the non-root nodes; V —the number of root nodes; C—the number of leaf nodes; S—the number of states per node;P —the number of parents per non-root node; and R—regularity of the BN’s underlying graph.The input parameters of BPART were set as follows to generate 3SAT BNs for experimentation: The CPT typeof the root nodes was Q = uniform; the CPT type of the non-root nodes was F = or; the number of root nodeswas V = 5; the number of leaf nodes was C = 20; the number of states per node was S = 2; the number of parentsper leaf node was P = 3; and irregular BNs were created by setting R = false. This gives C/V = 4.0, which liessomewhat below the phase transitions of SAT [34], meaning that most generated problems would be satisfiable, butsome might not be. Using these parameter settings, 100 problem instances were generated. The existence of satisfyingassignments was checked by processing the BNs, with clamped leaf nodes, using the HUGIN tree clustering system,which implements the tree clustering algorithm [5,24].O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990979The SGS system [27,30,31] with no restarts, uniform initialization in I, and search portfolio S(p) := {(p,UN),(1 − p, GM)} was employed, with varying noise probability. Following standard methodology, experiments withSGS were only conducted using instances with one or more satisfying assignments.7.1.2. Noise experiments using small synthetic Bayesian networksExperimentation progressed in two phases: First, noise responses for all satisfiable problem instances were gen-erated empirically by varying the noise level from p = 0.1 to p = 0.9 in increments of (cid:6)p = 0.05. The empiricallyobserved optimal values for the noise parameter ranged from ˆp∗ = 0.1 to ˆp∗ = 0.7. Second, more detailed studieswere performed, focusing on: (i) expected hitting time results derived analytically for a few real problem instances;(ii) real SGS search behavior for the same problem instances; and (iii) polynomial regression results based on SGS’ssearch behavior under varying p.The complete search spaces for the easiest and hardest problem instances in the sample, denoted β81 and β8 respec-tively, are illustrated in Fig. 7. Let b+ ∈ {0, 1}5 range over all almost-satisfying assignments, defined as assignmentswith C − 1 = 19 satisfied clauses. Such almost-satisfying assignments are of interest because they may form localoptima that trap the SIMPLESLS search process. From Fig. 7 we see that both β81 and β8 have eight almost-satisfyingassignments. For the easy problem instance β81, Hamming distance to optimum b∗is d(b+, b∗) = 1 or d(b+, b∗) = 2in all but two cases. Clearly, the three search space states with d(b+, b∗) = 1 do not form local optima in β81. Forthe hard problem instance β8, on the other hard, d(b+, b∗) = 3 or d(b+, b∗) = 4 in all but one case and all of thealmost-satisfying assignments, or “19” states, form a local optimum.More specifically, the eight “19” states in β81 are all connected, and three of them have the optimal “20” state asa neighbor. Hence, these “19” states form a plateau from which “20” is reached with relative ease. In β8, the eight“19” states are all connected also; however for this harder problem instance these states form a local optimum sincenone of them has “20” as a neighbor. Based on this inspection of search spaces, we expect β81 to be easier than β8for SIMPLESLS because β81 does not trap the search process to the same degree. More generally, this illustrates thatinstances can be easier because of the number or relative location of almost-optimal states or more generally localoptima in the search space, which in idealized form is illustrated in trap functions.To provide a more detailed quantitative analysis, we created two distinct Markov chains based on the two prob-lem instances, also taking into account the behavior of SGS. These two Markov chain models, which we denoteMC(p; β81) and MC(p; β8), are derived by inspecting the complete search spaces of β8 and β81 respectively. Theseapproximate Markov chains are shown in Fig. 8. In the following table we compare Pr(Xi+1 = 3 | Xi = 2) andPr(Xi+1 = 4 | Xi = 3), which lead search towards optimum, for TMC(p; 5, 0), TMC(p; 5, 4), MC(p; β81), andMC(p; β8):TMC(p; 5, 0)—Very easyMC(p; β81)—Easiest in sampleMC(p; β8)—Hardest in sampleTMC(p; 5, 4)—HardPr(Xi+1 = 3 | Xi = 2)1.0 − 0.4p0.85 − 0.25p0.23 + 0.37p0.6pPr(Xi+1 = 4 | Xi = 3)1.0 − 0.6p0.78 − 0.38p0.1 + 0.3p0.4pJust by considering the signs of the coefficients in front of p in this table, we clearly see the similarity betweenTMC(p; 5, 0) and MC(p; β81) on the one hand and TMC(p; 5, 4) and MC(p; β8) on the other. For TMC(p; 5, 0) andMC(p; β81), increasing the noise parameter p decreases the probabilities Pr(Xi+1 = 3 | Xi = 2) and Pr(Xi+1 = 4 |Xi = 3) that SIMPLESLS moves towards the optimal state s∗ = 5. This reflects that pure or almost pure hill-climbing,with no or minimal noise p, is optimal in the underlying search spaces. For TMC(p; 5, 4) and MC(p; β8), on the otherhand, increasing the noise parameter p increases the probabilities Pr(Xi+1 = 3 | Xi = 2) and Pr(Xi+1 = 4 | Xi = 3)that SIMPLESLS moves towards s∗ = 5. This shows how, in order to counter-act search being trapped in parts of thesearch space that do not contain optima, the noise parameter p can be increased. There is a similarity between thesearch traps observed in the idealized TMC models and the search traps as reflected in the Markov chains of the realproblem instances β8 and β81.With the Markov chains MC(p; β81) and MC(p; β8) in hand, we can analytically derive expected hitting timecurves, and compare them to empirical noise response curves. In Fig. 9, we show: (i) expected hitting times derivedfrom extreme trap Markov chains (TMCs); (ii) expected hitting time curves and run times, in the form of rational980O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990Fig. 7. The complete search spaces for two randomly generated BNs that represent instances of the satisfiability problem (3SAT) with V = 5variables and C = 20 clauses. We show, for instances with one satisfying assignment, the easiest and the hardest problem instances from a randomsample of 100 BNs. The number of satisfied clauses is shown for each state in the search space.Fig. 8. Markov chains created from considering the search spaces of two problem instances β81 (top) and β8 (bottom) along with the behavior ofSIMPLESLS. Fig. 7 shows the underlying search spaces.functions, derived from the Markov chains in Fig. 8; and (iii) data points reporting real SGS behavior on the sameproblem instances. There is a very good correspondence between analytical hitting time results in (ii) and observedSGS run times in (iii). For both (ii) and (iii), (i) provides lower bounding hitting time h5,0(p) and upper boundinghitting time h5,4(p). To our knowledge, similar displays that compare analytical and experiments results have notbeen reported in the literature earlier.We learned above that closed-form expressions of expected hitting times can be found for β81 and β8. Next, wedetermine experimentally polynomial regression approximations ˆP (p) for SGS run times. Our polynomial approx-imations are shown in Table 1, and in Fig. 10 we present empirical results along with regression curves. Regressionresults are significant according to the R2 values, and Fig. 10 shows very good correspondence between empirical andanalytical results. The polynomial regression lines are very close except for towards the less interesting endpoints oftheir domain [0.1, 0.9], as is typically the case.7.2. Experiments with large synthetic Bayesian networksIn this section we systematically and simultaneously vary the hardness of problem instances as well as the SLSnoise probability. We report on observed SGS search behavior, and how it can be fitted using polynomials. Here,real search results for BNs corresponding to SAT instances are reported for V = 30, with C/V = 2.0 to C/V = 3.4,where for each C/V -value 100 problem instances were generated and searched over. Hence, the real search behaviorof SGS for 800 problem instances is summarized and analyzed in this section. To our knowledge, similar noiseresponse experiments have not been reported in the literature earlier.O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990981Fig. 9. The expected hitting time h as a function of noise probability p for the two extreme problem instances β8 and β81 (black line for β81 andred line for β8), picked from a sample of 100, along with actual SGS behavior for the same two instances (black squares for β81 and red circlesfor β8). Expected hitting times h5,0(p) (dashed brown) and h5,4(p) (dashed purple) for the two 5-bit trap Markov chain models TMC(p, 5, 0) andTMC(p, 5, 4) are also shown. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of thisarticle.)Table 1Regression polynomials of order k = 4, k = 5, and k = 6 created from empirical SGS search data for the two extreme problem instances β81 andβ8 (Easiest and Hardest in sample respectively)InstanceOrderβ81β81β81β8β8β8456456Polynomial approximation ˆh(p)112.72p4 − 160.74p3 + 89.826p2 − 13.69p + 5.0822−36.836p5 + 209.41p4 − 255.9p3 + 133.1p2 − 22.627p + 5.7406−2455.5p6 + 7697.9p5 − 9428.3p4 + 5770.7p3 − 1840.9p2 + 294.85p − 13.635495.22p4 − 1151.8p3 + 1021.2p2 − 418.25p + 94.197−682.5p5 + 2201.5p4 − 2724.4p3 + 1673.8p2 − 536.31p + 101.33−1161.6p6 + 2802.4p5 − 1878.8p4 − 372.05p3 + 982.67p2 − 440.52p + 96.57R20.99730.99730.99830.99590.99700.9971Fig. 10. Comparison of (i) analytical hitting times derived from Markov chains MC(p; β) (black dashed line); (ii) empirical data points generatedfrom real SGS search behavior (black circles); and (iii) polynomial regression curves estimated from the empirical data points. The polynomialregressions lines are of order k, with k = 4 (green line), k = 5 (red line), and k = 6 (blue line). Left: Results for problem instance β81; Right:Results for problem instance β8. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of thisarticle.)7.2.1. Methodology for generating large synthetic Bayesian networksFor the experiments reported here, where SAT-like BNs were generated, we again used the BPART approachdiscussed in Section 7.1. Here, BPART’s input parameters were set as follows, generating larger problem instancescompared to those in Section 7.1: The CPT type of the root nodes was Q = uniform; the CPT type of the non-rootnodes was F = or; the number of root nodes was V = 30; the number of states per node was S = 2; the number of982O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990Fig. 11. Experimental results for synthetic BNs with C/V -ratios ranging from C/V = 2.0 to C/V = 3.4. Sample means for the run times (numberof flips) is shown as a function of SLS noise probability p. Note that all these piecewise linear functions are convex.parents per leaf node was P = 3; and irregular BNs were created by setting R = false. We varied the number of leafnodes while keeping V = 30 constant, giving C/V -ratios varying from C/V = 2.0 to C/V = 3.4. This is in the satis-fiable region of SAT [34] where solutions exist with very high probability. The existence of solutions was also checkedby processing the BNs using the HUGIN tree clustering system, which implements the tree clustering algorithm [5,24].7.2.2. Noise experiments using large synthetic Bayesian networksThe purpose of the second set of BN experiments was to investigate in more detail the combined effect of varyingnoise p and varying the hardness of larger, more realistically sized problem instances. For the purpose of theseexperiments we measured instance hardness by means of the C/V -ratio. The stochastic local search algorithm SGS[27,30] was employed, using a restart parameter value MAX-FLIPS optimized for the C/V -ratio and p at hand. Thesearch portfolio SN (p) := {(p, UN), (1 − p, GM)} was used, with noise probability varying from p = 0.1 to p = 0.7in increments of (cid:6)p = 0.1. Initialization uniformly at random was used in I.Fig. 11 summarizes the experimental results in the form of noise response curves. Each BN was searched 100 timesby SGS, and since for each C/V -ratio 100 BNs were generated, each data point in Fig. 11 represents 10,000 suc-cessful searches by SGS. There are eight different C/V -ratios, ranging from C/V = 2.0 to C/V = 3.4. The noiseprobability p was varied as reflected on the x-axis of Fig. 11. The y-axis measures the mean number of flips until anoptimum b∗was found. For the relatively easy C/V = 2.0 BNs, the sample mean is monotonically increasing withthe noise p, and the experimentally determined global minimum run time ˆr ∗ was found at the minimal noise levelinvestigated, so ˆp∗ = 0.1. For the hardest problem instances, with C/V = 3.4, the sample mean is first monotonicallydecreasing, then increasing, as a function of p. Here, the experimentally determined optimal noise level is ˆp∗ = 0.4.For C/V = 2.6, the sample average is close to constant from p = 0.1. to p = 0.4; it then increases monotonically.Results for the other intermediate C/V -ratios, 2.0 < C/V < 2.6 and 2.6 < C/V < 3.4, are similar to the C/V = 2.0case or the C/V = 3.4 case respectively.As argued in Section 6.2, similar analysis approaches can be used for mixtures, one of which has been sampledhere for each C/V -ratio, as for individual problem instances. The noise response curves in Fig. 11 are similar tothose reported for TMCs in Fig. 6 and for smaller problem instances in Fig. 10, illustrating how our results carryover from individual problem instances to mixtures of problem instances as predicted by our analysis in Section 6.Polynomial approximation results are shown in Table 2 and in Fig. 12. For each C/V -ratio we give three alternatives,namely polynomials of orders 4, 5, and 6. Polynomials of smaller order did not give good results. From C/V = 2.0to C/V = 2.8, all polynomials give rather similar results. From C/V = 3.0 to C/V = 3.4, the polynomials of orderk = 4 provide visibly poorer results as can be seen in Fig. 12 and from R2 in Table 2.Qualitatively, these results are consistent with the analysis in Sections 4, 5, and 6 as well as earlier experiments[16,17,26,45]. A key point here is that increasing average problem instance hardness, as controlled by the C/V -ratio,O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990983Table 2Polynomial approximations for SGS sample average run times (flips), as a function of noise level p, for varying C/V -ratios. The polynomialswere determined from experimental data using non-linear regression. For each C/V -ratio three alternatives are given, namely polynomials of order4, 5, and 6C/VOrder2.02.02.02.22.22.22.42.42.42.62.62.62.82.82.83.03.03.03.23.23.23.43.43.4456456456456456456456456Polynomial approximation, ˆr(p)y = 512.9p4 − 530.2p3 + 224.2p2 − 30.88p + 15.43y = 1500p5 − 2486p4 + 1694p3 − 525.6p2 + 80.83p + 9.771y = 2798p6 − 5216p5 + 3861p4 − 1298p3 + 205.9p2 − 5.501p + 13.51y = 1255p4 − 1568p3 + 740.9p2 − 133.1p + 27.17y = 1112p5 − 968.7p4 + 81.31p3 + 184.9p2 − 50.21p + 22.97y = 1285p6 − 1973p5 + 1947p4 − 1293p3 + 520.9p2 − 89.87p + 24.69y = 1554p4 − 1766p3 + 758.2p2 − 127.2p + 33.84y = 2249p5 − 2944p4 + 1570p3 − 366.4p2 + 40.40p + 25.36y = 833.3p6 + 249.1p5 − 1054p4 + 679.5p3 − 148.6p2 + 14.69p + 26.48y = 1998p4 − 2186p3 + 919.9p2 − 168.8p + 51.50y = 4797p5 − 7596p4 + 4930p3 − 1479p2 + 188.6p + 33.41y = 1246p6 + 1807p5 − 4770p4 + 3597p3 − 1153p2 + 150.1p + 35.08y = 3732p4 − 4380p3 + 1941p2 − 392.6p + 85.94y = 8302p5 − 12872p4 + 7935p3 − 2210p2 + 225.9p + 54.63y = −27081p6 + 73297p5 − 74297p4 + 36887p3 − 9289p2 + 1061p + 18.42y = 9418p4 − 12094p3 + 5708p2 − 1179p + 175.8y = 23729p5 − 38041p4 + 23104p3 − 6157p2 + 588.9p + 86.35y = −22734p6 + 78291p5 − 89606p4 + 47409p3 − 12100p2 + 1290p + 55.95y = 22176p4 − 29756p3 + 14929p2 − 3402p + 438.8y = 46501p5 − 70825p4 + 39220p3 − 8321p2 + 61.77p + 263.4y = 270595p6 − 602928p5 + 542934p4 − 250071p3 + 62412p2 − 8286p + 625.3y = 39431p4 − 51539p3 + 24947p2 − 5500p + 698.2y = 82600p5 − 125769p4 + 70985p3 − 16353p2 + 653.6p + 386.7y = 165545p6 − 314708p5 + 249717p4 − 105998p3 + 26920p2 − 4454p + 608.1R20.99981.01.00.99991.01.00.99991.01.00.99961.01.00.99941.01.00.99841.01.00.99810.99961.00.99851.01.0corresponds to moving the slope-change state z in a trap Markov chain towards the optimum state (see Fig. 5). Inother words, problem instances that are easy on average (corresponding to TMCs with slope-change states z close to00 . . . 0) should be solved by a greedier SLS algorithm than problem instances that are hard on average (correspondingto TMCs with slope-change state z close to 11 . . . 1). We also note that the piecewise linear curves are convex for allC/V -ratios. What is novel here, compared to earlier experiments that we know of, is the different pattern for the easyC/V = 2.0 case versus the hard C/V = 3.4 case, our extensive regression analysis using polynomials, and the cleardisplay of convexity across a wide range of C/V .7.2.3. Optimal noise level experiments using large synthetic Bayesian networksFrom earlier analysis and experiments, we know that the noise probability has a significant impact on the numberof search steps needed to reach an optimum. How does the optimal noise probability p∗ change as the C/V -ratiochanges? This is the research question investigated in this section.We wanted to minimize the number of flips as a function of the C/V -ratio. We first found approximate optimaˆr(p) = ˆr (cid:6)(p) andbased on the regression polynomials reported in Table 2. This was done by taking derivatives ddpthen solving the equation ˆr (cid:6)(p) = 0 with respect to p in order to compute optimized noise ˆp∗. The results are shownin Table 3. For each value of C/V , one empirical optimum and three regression polynomials, of different orders, arepresented.In a few cases, ˆr (cid:6)(p) = 0 had multiple candidate solutions but it was obvious which one was most reasonable tochoose as reflected in the table. Further, in two cases there were only complex solutions and in one case (C/V = 2.4and polynomial of order 6) the solution was less than zero so it was set to zero. As discussed in Section 6.3, even-degree polynomials are not guaranteed to have real roots. Given this fact, we have still opted to include in Table 3 thecases where r (cid:6)(p) has degree 4. Even though there is no guarantee for a real root, in most cases (specifically, in allcases except two in Table 3 containing r (cid:6)(p)) a real root was found and thus additional insight is provided.984O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990Fig. 12. Polynomial approximations for SGS average run times (flips), as a function of noise level p, for BNs with varying C/V -ratios. For eachC/V -ratio, polynomial regression lines of varying order k are presented, with k = 4 (line indicated by green crosses), k = 5 (line indicated by redsquares), and k = 6 (line indicated by blue diamonds). The means of the measured data, connected with straight lines (black), are also included.(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)Clearly, there is a good correspondence between the polynomials and the empirical results, especially for thepolynomials of order k = 6 for the higher C/V -values. Considering Fig. 12, and excluding C/V = 2.0 and C/V =2.2, one can see that k = 5 and k = 6 give quite similar results for ˆp∗.7.3. Experiments with application Bayesian networksIt is of great interest to consider also more advanced initialization and noise strategies on problem instances fromapplications. Here, we report empirical SGS results for application BNs, many of which are taken from Friedman’sBayesian Network Repository at http://www.cs.huji.ac.il/labs/compbio/Repository/. The application BNs investigatedO.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990985Table 3Noise optimization based on derivatives of the polynomial approximations for SGS sample average run times (number of flips), as a function ofnoise level p = x, for varying C/V -ratios. For each C/V -ratio three alternatives are given, namely a polynomial of order 4, 5, and 6. In addition,the optimal noise levels from the experiments are shownC/VOrder2.02.02.02.02.22.22.22.22.42.42.42.42.62.62.62.62.82.82.82.83.03.03.03.03.23.23.23.23.43.43.43.4456N/A456N/A456N/A456N/A456N/A456N/A456N/A456N/ADerivative of polynomial approximation, ˆr(cid:6)(p)2051.6p3 − 1591p2 + 448.4p − 30.887498p4 − 9945p3 + 5083p2 − 1051.2p + 80.837712p5 − 9864p4 + 7786p3 − 3878p2 + 1042p − 89.87Empirical5020p3 − 4704p2 + 1482p − 133.15560p4 − 3875p3 + 243. 9p2 + 369.8p − 50. 217712p5 − 9864p4 + 7786p3 − 3878p2 + 1042p − 89.87Empirical6215p3 − 5297p2 + 1516p − 127.211246p4 − 11 778p3 + 4711p2 − 732.8p + 40.405000p5 + 1245p4 − 4217p3 + 2038p2 − 297.1p + 14.69Empirical7994p3 − 6559p2 + 1840p − 168. 823986p4 − 30383p3 + 14789p2 − 2957p + 188.67476p5 + 9034p4 − 19079p3 + 10792p2 − 2306p + 150.1Empirical14929p3 − 13140p2 + 3882p − 392.641 512p4 − 51 488p3 + 23 805p2 − 4420p + 225.9−162 486p5 + 366 485p4 − 297 188p3 + 110 661p2 − 18 578p + 1061Empirical37671p3 − 36 282p2 + 11416p − 1179118645p4 − 152 164p3 + 69 312p2 − 12314p + 588.9−136 404p5 + 391 455p4 − 358 424p3 + 142 227p2 − 24 200p + 1290Empirical88 704p3 − 89 268p2 + 29 858p − 3402232 505p4 − 283 300p3 + 117 660p2 − 16643p + 61.771623 570p5 − 3014 640p4 + 2171 736p3 − 750 213p2 + 124 824p − 8286Empirical157 724p3 − 154 617p2 + 49 894p − 5500413 000p4 − 503 076p3 + 212 955p2 − 32 706p + 653.6993 270p5 − 1573 540p4 + 998 868p3 − 317 994p2 + 3840p − 4454EmpiricalOptimized noise ˆp∗−29.95 × 10Complex1.55 × 100.1−20.1490.1480.1470.1 and 0.20.144 24Complex00.10.1960.2790.2800.30.3660.3270.3050.30.4120.3150.3070.30.4370.3550.3840.40.4480.3890.3940.4are Mildew, Munin1, Pir3, and Water. The Mildew BN is for determining the amount of fungicides to use to counter-act mildew attacks on wheat. The Munin1 network is a medical BN from the field of electromyography [2]. The Pir3BN is for information filtering for the purpose of battlefield situation awareness [21,32]. The Water BN models thebiological processes of water purification.The purpose of these experiments was to investigate the effect of varying p, and also investigate SLS strategiesthat go beyond uniform random noise and initialization uniformly at random. More specifically, we compared thefollowing two search algorithm portfolios SG(p) and SU (p).Definition 46 (Guided noise). The guided noise portfolio SG(p) is a function of noise probability p and is defined as(cid:13)(cid:14)(cid:16)(cid:8)(cid:13)(cid:14)(cid:14)(cid:13)(cid:13)(cid:14)SG(p) :=p2p2, BS,, GS,, BM,, GM.1 − p21 − p2Definition 47 (Uniform noise). The uniform noise portfolio SU (p) is a function of noise probability p and is definedas(cid:8)(cid:13)(cid:14)(cid:13)(cid:14)(cid:13)(cid:14)(cid:16)SU (p) :=p, NU,, BM,, GM.1 − p21 − p2986O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990Fig. 13. Empirical results for the Water BN under four different conditions (1), (2), (3), and (4). In all cases, the noise probability p, varying fromp = 0.1 to p = 0.8, is shown on the x-axis. The mean run time (measured in flips) is displayed using a logarithmic scale on the y-axis. Each pointrepresents the sample mean of 1000 runs. For each condition, three different values of MAX-FLIPS were investigated as shown.In experiments, we used either SG(p) or SU (p) as the search algorithm portfolio of SGS. We note that all experi-ments using SG(p) do not use uniform random noise, but more advanced approaches to noisy search. In addition, wealso compared IU (initialization uniformly at random) versus IG (guided initialization), and also varied MAX-FLIPS.7.3.1. Varying noise, initialization, and restart point: One application BNThe purpose of our first set of experiments was to establish the effect, if any, of using different variants of SGS forone particular BN, Water. Specifically, we varied both I and S. We studied the impact of varying the noise p, fromp = 0.1 to p = 0.8, under conditions of different noise and initialization portfolios as well as different values of theMAX-FLIPS restart parameter. The two orthogonal dimensions investigated were:• Initialization portfolio I: Uniform initialization IU versus guided initialization IG. Here, uniform means initial-ization uniformly at random while guided means the use of forward simulation [13].• Search portfolio S: Uniform noise SU (p) versus guided noise SG(p).The four conditions investigated were: (1)—IU and SU (p); (2)—IG and SU (p); (3)—IU and SG(p); and (4)—IGand SG(p). Fig. 13 presents the results for these four different conditions in the form of sample means and piecewiselinear approximations for ˆr(p). In all cases, ˆr(p) is convex or close to convex. There are quite different run timeresponses to changes in noise, depending on the initialization operator used in I, the noise operator used in S, and thevalue of the MAX-FLIPS parameter. Clearly, condition (4) has the shortest run time and also the overall average runtime is shortest; the impact of varying p on ˆr(p) is small. Condition (1) has the longest run times; here the impact ofvarying p is large. Overall, perhaps the greatest impact on ˆr(p) is due to whether uniform noise SU (p) (top row, (1)and (2)) or guided noise SG(p) is used (bottom row, (3) and (4)). For uniform noise, the impact of varying noise p isdramatic and ˆp∗ (cid:2) 0.4 for a given MAX-FLIPS level in all cases. For guided noise, on the other hand, the effect ofvarying p on ˆr(p) is rather minimal. Further, and perhaps surprisingly, ˆp∗ is in (3) and (4) quite large in all cases butone, namely MAX-FLIPS = 50 in (3). Overall, the non-trivial interactions between BN, SLS algorithm parameters,O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990987and noise p are illustrated here. However, the piecewise linear approximation are convex or close to convex in allcases, thus supporting our analytical results.7.3.2. Varying noise and restart point: All application BNsThe purpose of the second set of application BN experiments was to establish the effect, if any, of varying thenoise and the value of MAX-FLIPS for different applications BNs. Two variants of SGS, namely SGS with uniformnoise SU (p) and SGS with guided noise SG(p), were used. Each SGS variant was tested with four different BNs—Mildew, Munin1, Pir3, and Water—giving a total of eight conditions as shown in Fig. 14. Further, a BN-specificoptimal initialization algorithm—either forward simulation [13] or a randomized variant of the Viterbi algorithm [31,49]—was used in I for each application BN. Forward simulation was used for Munin1 and Water. Mildew and Pir3were respectively initialized using the forward and backward variants of the randomized Viterbi algorithm.For each condition, noise was varied from p = 0.1 to p = 0.8, and different values for the MAX-FLIPS restartparameter were also used. Results, in the form of sample means and piecewise linear approximations for ˆr(p), arereported in Fig. 14. Each data point represents the mean for 1000 runs. In general, guided noise SG(p) performedbetter than uniform noise SU (p), however the effect on estimated run time ˆr(p) of increasing noise varied dramati-cally between problem instances. For Pir3, initialization was strong and increasing noise only hurt performance. ForMunin1, there was a marked difference between uniform noise and guided noise. Uniform noise hurt performance,while guided noise had, except for MAX-FLIPS = 10, minor impact on run time. Water and Mildew had somewhatsimilar performance. Low levels of uniform noise were helpful for some values of MAX-FLIPS, while for other valuesof MAX-FLIPS increasing noise from p = 0.1 did not help. For Mildew, rather high levels of guided noise were op-timal for all MAX-FLIPS levels investigated. This was also the case for Water, except for MAX-FLIPS = 10. Exceptfor some of the curves for Pir3, these piecewise linear curves are clearly convex or close to convex, thus supportingour analytical results.8. Conclusion and future workThe use of randomization, in the form of noisy initialization and noisy local search steps, has empirically beenshown to have a dramatic and positive impact on the performance of local search [7,8,14,17,18,26,27,30,45,46].Consequently, stochastic local search (SLS) algorithms are currently strong performers in several areas of automatedreasoning, including the problems of satisfiability (SAT) in propositional logic [11,18,45,46] and computing MPE andMAP in Bayesian networks [22,27,30,36,37]. Previous research on stochastic local search has been predominantlyexperimental, and there is a need for theoretical foundations [16]. We have in this article, based on discrete Markovchain models derived from a simple but general SLS algorithm called SIMPLESLS, developed a theoretical foundationfor the role of noise in stochastic local search. Analytically, we used hitting time analysis in Markov chain modelsto obtain our results. Curves for expected hitting times are the analytical counterpart to empirical noise responsecurves often reported in the literature [7,8,14,16,26]. Our analysis shows that expected hitting times, for individualinstances as well as for mixtures of instances, are rational functions with noise p as the independent variable. We alsoemphasize the use of polynomials and convex functions. Analytically, we have found that the impact of noise is quitecontext-dependent, and the shape of the expected hitting time curve depends strongly on the problem instance at hand.In order to improve our understanding of the interaction between noise and the presence of local maxima, a simpleclass of trap functions based on deceptive functions [6] with these characteristics was introduced: (i) one globalmaximum; (ii) one local (deceptive) maximum at maximal distance from the global maximum; and (iii) a slope-change location controlling the size of the region from which greedy search only is sufficient to reach the globalmaximum. Obviously, there are other crucial issues involved in SLS applied to NP-hard problems. Our model, fromwhich we derive trap Markov chains and then expected hitting times, highlights the problem of how local maxima trapthe search process, and how the careful application of noise helps in escaping such traps. Trap functions are closelyrelated to search space traps—portions of the search space that are attractive to SLS but do not contain solutions[11,15].Our results also include experiments. In this area, we used the stochastic local search algorithm SGS (stochas-tic greedy search) for computing MPEs in Bayesian networks [27,30]. Using SGS, we experimented with syntheticBayesian network of varying difficulty, measured in terms of C/V -ratio [33]. In these experiments, we illustratedthat the Markov chain models are relevant to real problem instances. For instance, we found good correspondence988O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990Fig. 14. Empirical results for the BNs Pir3, Munin1, Water, and Mildew under different experimental conditions. In all cases, the noise probabilityp, varying from p = 0.1 to p = 0.8, is shown on the x-axis. The mean run time (measured in flips) is displayed using a logarithmic scale on they-axis. Each point represents the sample mean of 1000 runs. Two different noise mechanisms (Uniform and Guided) and five different values ofMAX-FLIPS (from MAX-FLIPS = 10 to MAX-FLIPS = 1000) were used. In most cases, the value of the noise probability has a significant impacton SLS run time.between expected hitting time results derived analytically from real problem instances, SGS’s behavior on the sameproblem instances, and polynomial regression results. These noise response curves were, as expected, similar to butnot as extreme as the bounding hitting times for the two extreme trap Markov chains defined over the same searchspace. We also sampled mixtures of problem instances, as defined by C/V , and found that SGS generated noise re-sponse curves consistent with our hitting time analysis. Here, we also performed extensive polynomial approximationO.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990989experiments. Finally, experiments with SGS using application Bayesian networks were also performed. Here, otherinput parameters of SGS—including restarts, initialization algorithms, and noise algorithms—were varied in additionto the level of noise. Results consistent with our analysis were in general found.We conclude by outlining a few areas for future work. First, a natural extension of this research would be to analyzeand optimize the joint effect of multiple SLS parameters, taking into account varying distributions of problem instanceinputs (including Bayesian networks), perhaps using convex optimization and response surface techniques. Second,given the benefit of guided noise observed in experiments, it would be fruitful to study such approaches analytically.Third, this work provides a framework for improved analysis of other algorithms that can be formalized by meansof Markov chains, for instance genetic and evolutionary algorithms. Fourth, the optimal noise level might vary fordifferent sub-problems of one problem instance as well as between different problem instances and problem instancedistributions. Following this line of reasoning, it would pay off to adapt the noise level for one problem instance,within a try or between different tries. While results on adaptive noise already exist [14,26], we hope that the analysisprovided here will inspire further research in this area.AcknowledgementsThis material is based upon work supported by NASA under award NCC2-1426. The anonymous reviewers areacknowledged for their comments, which helped improve the article.References[1] A.M. Abdelbar, S.M. Hedetnieme, Approximating MAPs for belief networks is NP-hard and other theorems, Artificial Intelligence 102 (1998)21–38.[2] S. Andreassen, M. Woldbye, B. Falck, S.K. Andersen, MUNIN—A causal probabilistic network for interpretation of electromyographicfindings. In: Proceedings of the Tenth International Joint Conference on Artificial Intelligence, Milan, Italy, August 1987, pp. 366–372.[3] E. Cantu-Paz, Markov chain models of parallel genetic algorithms, IEEE Transactions on Evolutionary Computation 4 (3) (2000) 216–226.[4] F.G. Cooper, The computational complexity of probabilistic inference using Bayesian belief networks, Artificial Intelligence 42 (1990) 393–405.[5] A.P. Dawid, Applications of a general propagation algorithm for probabilistic expert systems, Statistics and Computing 2 (1992) 25–36.[6] K. Deb, D.E. Goldberg, Analyzing deception in trap functions, in: D. Whitley (Ed.), Foundations of Genetic Algorithms II, Morgan Kaufmann,San Mateo, CA, 1993, pp. 93–108.[7] A. Fukunaga, G. Rabideau, S. Chien, Robust local search for spacecraft operations using adaptive noise, in: Proceedings of the 4th InternationalWorkshop on Planning and Scheduling for Space (IWPSS-04), Darmstadt, Germany, 2004.[8] A. Gerevini, A. Saetti, I. Serina, An empirical analysis of some heuristic features for local search in LPG, in: Proceedings of the FourteenthInternational Conference on Automated Planning and Scheduling (ICAPS 2004), Whistler, British Columbia, Canada, 2004, pp. 171–180.[9] D.E. Goldberg, P. Segrest, Finite Markov chain analysis of genetic algorithms, in: J.J. Grefenstette (Ed.), Genetic Algorithms and TheirApplications: Proceedings of the Second International Conference on Genetic Algorithms, Erlbaum, Hillsdale, NJ, 1987, pp. 1–8.[10] C.P. Gomes, B. Selman, H. Kautz, Boosting combinatorial search through randomization, in: Proceedings of the Fifteenth National Conferenceon Artificial Intelligence (AAAI-98), Madison, WI, 1998, pp. 431–437.[11] P.W. Gu, J. Purdom, J. Franco, B.W. Wah, Algorithms for the satisfiability SAT problem: A survey, in: Satisfiability Problem: Theory andApplications, in: DIMACS Series in Discrete Mathematics and Theoretical Computer Science, American Mathematical Society, 1997, pp. 19–152.[12] G. Harik, E. Cantu-Paz, D.E. Goldberg, B.L. Miller, The gambler’s ruin problem, genetic algorithms, and the sizing of populations, in:Proceedings of the IEEE Conference on Evolutionary Computation, Indianapolis, IN, 1997, pp. 7–12.[13] M. Henrion, Propagating uncertainty in Bayesian networks by probabilistic logic sampling, in: Uncertainty in Artificial Intelligence 2, Elsevier,Amsterdam, 1988, pp. 149–163.[14] H.H. Hoos, An adaptive noise mechanism for WalkSAT, in: Proceedings of the Eighteenth National Conference on Artificial Intelligence(AAAI-02), Edmonton, Alberta, Canada, 2002, pp. 655–660.[15] H.H. Hoos, A mixture-model for the behaviour of SLS algorithms for SAT, in: Proceedings of the Eighteenth National Conference on ArtificialIntelligence (AAAI-02), Edmonton, Alberta, Canada, 2002, pp. 661–667.[16] H.H. Hoos, T. Stützle, Towards a characterisation of the behaviour of stochastic local search algorithms for SAT, Artificial Intelligence 112 (1–2) (1999) 213–232.[17] H.H. Hoos, T. Stützle, Local search algorithms for SAT: An empirical evaluation, Journal of Automated Reasoning 24 (4) (2000) 421–481.[18] H.H. Hoos, T. Stützle, Stochastic Local Search: Foundations and Applications, Morgan Kaufmann, San Francisco, CA, 2005.[19] E. Horvitz, Y. Ruan, C. Gomes, H. Kautz, B. Selman, D. Chickering, A Bayesian approach to tackling hard computational problems, in:Proceedings of the 17th Annual Conference on Uncertainty in Artificial Intelligence (UAI-01), Seattle, WA, 2001, pp. 235–244.[20] F. Hutter, H.H. Hoos, T. Stützle, Efficient stochastic local search for MPE solving, in: Proceedings of the Nineteenth International JointConference on Artificial Intelligence (IJCAI-05), Edinburgh, Scotland, 2005, pp. 169–174.990O.J. Mengshoel / Artificial Intelligence 172 (2008) 955–990[21] P. Jones, C. Hayes, D. Wilkins, R. Bargar, J. Sniezek, P. Asaro, O.J. Mengshoel, D. Kessler, M. Lucenti, I. Choi, N. Tu, J. Schlabach,CoRAVEN: Modeling and design of a multimedia intelligent infrastructure for collaborative intelligence analysis, in: Proceedings of theInternational Conference on Systems, Man, and Cybernetics, San Diego, CA, October 1998, pp. 914–919.[22] K. Kask, R. Dechter, Stochastic local search for Bayesian networks, in: Proceedings Seventh International Workshop on Artificial Intelligenceand Statistics, Fort Lauderdale, FL, January 1999, Morgan Kaufmann, 1999.[23] V.G. Kulkarni, Modeling, Analysis, Design, and Control of Stochastic Systems, Springer, New York, 2005.[24] S. Lauritzen, D.J. Spiegelhalter, Local computations with probabilities on graphical structures and their application to expert systems (withdiscussion), Journal of the Royal Statistical Society series B 50 (2) (1988) 157–224.[25] D.J.C. MacKay, Information Theory, Inference and Learning Algorithms, Cambridge University Press, Cambridge, UK, 2002.[26] D. McAllester, B. Selman, H. Kautz, Evidence for invariants in local search, in: Proceedings of the 14th National Conference on ArtificialIntelligence (AAAI-97), Providence, RI, 1997, pp. 321–326.[27] O.J. Mengshoel, Efficient Bayesian network inference: Genetic algorithms, stochastic local search, and abstraction, PhD thesis, Departmentof Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, April 1999.[28] O.J. Mengshoel, Designing resource-bounded reasoners using Bayesian networks: System health monitoring and diagnosis, in: Proceedingsof the 18th International Workshop on Principles of Diagnosis (DX-07), Nashville, TN, 2007, pp. 330–337.[29] O.J. Mengshoel, Macroscopic models of clique tree growth for Bayesian networks, in: Proceedings of the Twenty-Second National Conferenceon Artificial Intelligence (AAAI-07), Vancouver, British Columbia, 2007, pp. 1256–1262.[30] O.J. Mengshoel, D. Roth, D.C. Wilkins, Stochastic greedy search: Computing the most probable explanation in Bayesian networks, TechnicalReport UIUCDCS-R-2000-2150, Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, February 2000.[31] O.J. Mengshoel, D. Roth, D.C. Wilkins, Initialization and restart in stochastic local search: Computing a most probable explanation in Bayesiannetworks, IEEE Transactions on Knowledge and Data Engineering (2007), submitted for publication.[32] O.J. Mengshoel, D.C. Wilkins, Raven: Bayesian networks for human-computer intelligent interaction, in: M.S. Vassiliou, T.S. Huang (Eds.),Computer Science Handbook for Displays, Rockwell Scientific Company, 2001, pp. 209–219.[33] O.J. Mengshoel, D.C. Wilkins, D. Roth, Controlled generation of hard and easy Bayesian networks: Impact on maximal clique tree in treeclustering, Artificial Intelligence 170 (16–17) (2006) 1137–1174.[34] D. Mitchell, B. Selman, H.J. Levesque, Hard and easy distributions of SAT problems, in: Proceedings of the Tenth National Conference onArtificial Intelligence (AAAI-92), San Jose, CA, 1992, pp. 459–465.[35] J. Park, Using weighted MAX-SAT engines to solve MPE, in: Proceedings of the 18th National Conference on Artificial Intelligence (AAAI-04), Edmonton, Alberta, Canada, 2004, pp. 682–687.[36] J.D. Park, A. Darwiche, Approximating MAP using local search, in: Proceedings of the Seventeenth Conference on Uncertainty in ArtificialIntelligence (UAI-01), Seattle, WA, 2001, pp. 403–410.[37] J.D. Park, A. Darwiche, Complexity results and approximation strategies for MAP explanations, Journal of Artificial Intelligence Research(JAIR) 21 (2004) 101–133.[38] A.J. Parkes, J.P. Walser, Tuning local search for satisfiability testing, in: Proceedings of the Thirteenth National Conference on ArtificialIntelligence (AAAI-96), Portland, OR, 1996, pp. 356–362.[39] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, San Mateo, CA, 1988.[40] D. Roth, On the hardness of approximate reasoning, Artificial Intelligence 82 (1996) 273–302.[41] Y. Ruan, E. Horvitz, H. Kautz, Restart policies with dependence among runs: A dynamic programming approach, in: Proceedings of theEighth International Conference on Principles and Practice of Constraint Programming, Ithaca, NY, 2002, pp. 573–586.[42] Y. Ruan, E. Horvitz, H. Kautz, Hardness-aware restart policies, in: IJCAI-03 Workshop on Stochastic Search Algorithms, Acapulco, Mexico,2003.[43] D. Schuurmans, F. Southey, Local search characteristics of incomplete SAT procedures, Artificial Intelligence 132 (2) (2001) 121–150.[44] B. Selman, H. Kautz, Domain-independent extensions to GSAT: Solving large structured satisfiability problems, in: Proceedings of the Inter-national Joint Conference on Artificial Intelligence (IJCAI-93), Chambery, France, 1993, pp. 290–295.[45] B. Selman, H.A. Kautz, B. Cohen, Noise strategies for improving local search, in: Proceedings of the Twelfth National Conference on ArtificialIntelligence (AAAI-94), Seattle, WA, 1994, pp. 337–343.[46] B. Selman, H. Levesque, D. Mitchell, A new method for solving hard satisfiability problems, in: Proceedings of the Tenth National Conferenceon Artificial Intelligence (AAAI-92), San Jose, CA, 1992, pp. 440–446.[47] E. Shimony, Finding MAPs for belief networks is NP-hard, Artificial Intelligence 68 (1994) 399–410.[48] J. Suzuki, A Markov chain analysis on a genetic algorithm, in: S. Forrest (Ed.), Proceedings of the Fifth International Conference on GeneticAlgorithms, San Mateo, CA, 1993, pp. 146–153.[49] A.J. Viterbi, Error bounds for convolutional codes and an asymptotically optimal decoding algorithm, IEEE Transactions on InformationTheory 13 (1967) 260–269.[50] C. Yanover, Y. Weiss, Finding the m most probable configurations in arbitrary graphical models, in: S. Thrun, L. Saul, B. Schölkopf (Eds.),Advances in Neural Information Processing Systems 16, MIT Press, Cambridge, MA, 2004.[51] M. Yokoo, Why adding more constraints makes a problem easier for hill-climbing algorithms: Analyzing landscapes of CSPs, in: Proceedingsof the Third International Conference on Principles and Practice of Constraint Programming, in: LNCS, vol. 1330, Springer Verlag, 1997,pp. 357–370.