ELSEVIER Artificial Intelligence 75 ( 1995) 297-345 Artificial Intelligence Multiagent negotiation under time constraints * Sarit Kraus a,l, Jonathan Wilkenfeld b12, Gilad Zlotkin ‘y3 a Department of Mathematics and Computer Science, Bar Ilan University, Ramat Gan, 52900 Israel b Department of Government and Politics, CJniversify of Maryland, College Park, MD 20742, USA ’ Center of Coordination Science, Sloan School of Management, Massachusetts Institute of Technology, I Amherst St., E40-I 79, Cambridge, MA 02139, USA Received November 1992; revised January 1994 Abstract Research in distributed artificial intelligence (DAI) is concerned with how automated agents can be designed to interact effectively. Negotiation is proposed as a means for agents to communicate and compromise to reach mutually beneficial agreements. The paper examines the problems of resource allocation and task distribution among autonomous agents which can benefit from sharing a common resource or distributing a set of common tasks. We propose a strategic model of negotiation that takes the passage of time during the negotiation process itself into account. A distributed negotiation mechanism is introduced that is simple, efficient, stable, and flexible in various situations. The model considers situations characterized by complete as well as incomplete information, and ones in which some agents lose over time while others gain over time. Using this negotiation mechanism autonomous agents have simple and stable negotiation strategies that result in efficient agreements without delays even when there are dynamic changes in the environment. 1. Introduction Research in distributed artificial intelligence (DAI) agents can be designed to interact effectively. One important capability is concerned with how automated that could aid *This material 9123460. Some material for her comments. is based upon work supported by the National Science Foundation under Grant No. IRI- in this paper appeared in preliminary form in [26,27]. We thank Karen Lochbaum ’ E-mail: sarit@bimacs.cs.biu.ac.il. Also affiliated with the Institute for Advanced Computer Studies, University of Maryland, College Park. ‘E-mail: s E-mail: gilad@mit.edu. jwilkenf@bss2.umd.edu. This research was done while the author was at the Computer Science Department in the Hebrew University and was supported by Leibniz Center for Research in Computer Science. 0004-3702/95/$09.50 SSDlOOO4-3702(94)00021-R @ 1995 Elsevier Science B.V. All rights reserved 298 S. Kraus et al./Artificial Intelligence 75 (1995) 297-345 cooperation inter-agent their respective desires and compromise One of the presumed difficulties is negotiation; agents could be built that are able to communicate to reach mutually beneficial agreements. is that negotiation the overhead of coordination benefit may increase planning be either about job sharing or resource allocation. agents from spending timetables and negotiation for satisfying their goals. too much in using negotiation is a costly and time-consuming as a way of reaching mutual it process and, consequently, (see [ 1 ] ) . In the presence of time constraints, time should be taken into consideration. The negotiation may the to their In both cases we want to prevent and therefore not keeping time on negotiation (MA) Research in DA1 is divided and Multi-Agent Systems 1.5 below). Research problem can be divided among a number of modules or “nodes”. The modules system are centrally reliability. They include solution into two basic classes: Distributed Problem Solving (DPS) [ 1 ] (see discussion of previous work in DA1 in Section in solving a particular in a DPS and/or to find a stability, modularity, of cooperation mechanisms designed in DPS considers how the work involved to improve performance, to a given problem. the development designed Research in MA is concerned with coordinating (possibly heterogeneous) tion of autonomous In MA, shared goals or success criteria. There agents. intelligent (possibly pre-existing) intelligent behavior among a collec- agents. and no globally the among for real competition is a possibility there is no global control, no globally consistent knowledge, These classes are actually falls “closer” the two extreme poles in the DA1 research spectrum. Our self- to the MA pole since rational and autonomous research motivated, that the agents may share a common goal, although even in such situations, are self-motivated has its own utility utility. agents. However, we also deal with the possibility the agents that each agent expected to their interests. We assume and that rational behavior and act only according involves maximizing it deals with interactions function, among allocation of resource We examine the problems task distribution agents. resource In some domains, (e.g., among au- resources must share a agents, due roads, bridges, clean air). and to limited tonomous In other domains, when resources common resource since are unlimited, a set of com- resources may be expensive are symmetrical. In mon tasks. Both problems resource, with each the resource problem where agent seeking a larger share of the resource. agents have a common goal, several tasks need to be performed to fulfill the goal. Each agent would like the common goal to be achieved with the least amount of effort on its part. This cooperative case also has a competitive element. Each agent wants to perform a smaller part of the job (task). agents may still mutually benefit from sharing a common (e.g., printers, satellites), or from distributing sharing and task distribution) for a valuable In the task distribution there is competition sharing problem (resource In this paper we suggest a strategic model of negotiation the negotiation process over time during preferences the agreements agreements can be avoided. We will examine time will change they are willing itself their strategies into consideration. Changes that takes the passage of in the agents’ and, as a result, in reaching the following possible situations where the in the negotiation to reach. This model will show that delays strategic model (1, S. Kraus et al./Art$cial Intelligence 75 (1995) 297-345 299 is applicable: is reached time need information (Section 3). that lose over leave the negotiations to satisfy a common information the negotiations lose over time need to share a common about until an agreement to cooperate goal. about the other agent. The agents can resource. Each agent the other agent. They have no alternative, Two agents that knows all relevant but to continue Two agents Each agent knows all relevant unilaterally (Section 4). Two agents need to share a resource. One of the agents already has access to the It is gaining over time. resource and is using to use the resource and loses over time. Both agents The other agent is waiting have full information leave the negotiations Similar other (Section 6). Several agents need to cooperate over time, have full information negotiations to satisfy a common goal. All of them are losing leave the about each other and can unilaterally and can unilaterally to case (3)) but the agents do not have complete the negotiation process. (Section 7). (Section 5). about each information it during (2) (3) (4) (5) 1.1. The resource allocation problem A set of agents shares a joint resource. The joint agent at a time. Agreement resource. An agreement agents. 4 is sought so that all the agents will be able resource can only be used by one to use the the the usage of the resource among is a schedule that divides Examples of joint road fresh water, clean air, etc. (Other work in the DAI community dealing with the lines, printers, disks, bridges, resources are: communication junctions, resource allocation problem negotiation protocol arising [ 331 address mechanism [ 231 propose [ 41 proposes includes, for example, that is useful for cooperatively in distributed networks of semi-autonomous [ 6,301 which present a multistage resolving resource allocation conflicts problem solving nodes. Lesser et al. and develop a for resource allocation based on the criticality of tasks; Kornfeld and Hewitt and real-time performance, in resource allocation tradeoff resource allocation using specialist resource allocation via resource pricing.) “sponsor” agents; and Chandrasekan A communications cost of its launching get access competing project. satellite and maintenance. is a good example of a shared resource, due to the high In many cases can is by sharing one with other companies. Even in such a joint to participate satellite it mutually the only way a company beneficial to a communications companies may find Sharing a common resource requires a coordination mechanism that will manage usage of the resource. Discussion may even conclude) about the coordination mechanism will begin before discussion of other technical aspects of the joint project. the (and A coordination mechanism it can be an on-line negotiation mechanism can be a static division of frequencies or time slots. On resolves that dynamically the other hand, 40ur model is also applicable in the case where agents. This case does not differ significantly the resource the from the case where only the resource usage time can be divided. itself can actually be divided between 300 S. Kraus et al. /Artificial Intelligence 75 (1995) 297-345 local conflicts over the usage of the common of the coordination mechanism tion mechanisms schedules. that generate agreements spectrum. On this spectrum on long resource. These are the two extreme poles there are also coordina- term (an hour, a day, . . .) global This paper addresses the kinds of attributes a coordination mechanism a negotiation mechanism is eficiency. The coordination mechanism resource. Efficiency and presents tribute usage of the common instantaneously As in the case of the communications (i.e., a local conflict should be resolved without delay). resources common satellite, that satisfies should have at- joint implies other attributes such as simplicity and those attributes. An important should in an efficient result companies with possibly different the mechanism efficiency, different ensure those attributes formally defines on-line coordination mechanisms an efficient that elapses between agent actually gains access the agent. For example, joint usage of the resource. the time that the resource (Section 1.3) and presents symmetric, goals. Therefore, and even conflicting are shared by to should also be stable and symmetric. This paper stable and simple local conflicts without delay and result in is some cost associated with the time is needed by an agent and the time the state of There that resolve to the resource. This cost depends on the internal its task load, its disk space, etc. 1.2. The task distribution problem A set of autonomous its costs, i.e., prefers to do as little as possible. We note that even though agents has a common goal it wants to satisfy as soon as possible. the In order to satisfy any goal, costly actions must be taken and an agent cannot satisfy reaching an agreement with the other agents. Each of the agents wants to goal without the minimize agents have the same goal (under our simplified assumptions), there is actually a conflict of interests. The agents try to reach an agreement over the division of labor. We assume for reaching that each step of the negotiation in the agreements includes, area of Distributed Problem Solving systems for example, Davis and Smith’s [ 541, Cammarata et al.‘s work on strategies of cooperation work on the Contract Net in the context of collision that are needed to solve shared for groups avoidance interpretation system that is able to function effectively even though processing nodes have inconsistent [ 321, and Carver et al.‘s work on agents with sophisticated and incomplete models [2], Lesser and Erman’s model of a distributed takes time, and the agents have preferences that support complex and dynamic (research on the task distribution interactions between tasks effectively in air traffic time periods in different information problem the agents [ l-5,49,60,64]. [ 31). A group of An example of task distribution is the “delivery domain” requirement of tasks. One company, delivery companies can reduce their overall and individual delivery costs by coordinating their deliveries. Each delivery the exchanging actually delivery from A to B and a delivery to B with no extra cost. Therefore, another A-to-B delivery. The mechanisms presented companies be mutually beneficial. is to make a from A its C-to-D delivery with in this paper allow multiple delivery that will for example, from C to D can execute other deliveries it may agree to exchange to reach an efficient agreement on task distribution without delay task. Delivery coordination is a single that needs S. Kraus et al./Artijcial Intelligence 75 (1995) 297-345 301 1.3. Criteria for evaluation of negotiation protocols In a multi-agent competition that allows agents tocol) Those mechanisms situation there is a need to define a mechanism (a pro- their conflicts and to reach a cooperative agreement. to resolve are usually called negotiation protocols. Given a multi-agent domain, we are interested in investigating protocols that are available strategy) that is suitable follow agent should should be no reason What are the conditions distributed multi-agent agents (for that specific domain)? domain), to the agents, and also the agent’s behavior for a given protocol. We will present in a given protocol, and show that in the design of agents, to adopt any other strategy. that a Negotiation Protocol should satisfy (for any specific such that it should be accepted by all the designers of both the negotiation (negotiation the optimal strategy an there l Distributed. The decision making process should be distributed. There should be no central unit or agent that is managing the process. l Instantaneously. Conflict should be resolved without delay. l Eficiency. The outcome of the negotiations (i.e., the agreements) should be effi- should be avoided when possible and the mechanism cient: - Conflict agents it, i.e., there is is Pareto-optimal no other deal that is better for some of the agents and not worse for the others. the to reach Pareto-optimal agreements with high probability. An agreement if there is no other agreement that dominates should allow - In the resource allocation problem, the resource is not in use only when there is no agent in the group that currently needs the resource (there are no deadlocks). l Simplicity. The negotiation process itself should be simple and efficient. It should and computation be short and consume only a reasonable amount of communication resources. In l Symmetry. The coordination mechanism their attributes. role the situations the encounter of non-relevant and functions like an agent’s color, name or manufacture attributes, symmetry that given a specific situation, implies another which outcome of the negotiation. is identical with respect that we consider, are the relevant should not treat agents differently because the agents’ utility attributes. All other is, of an agent with the to the above attributes, will not change are not relevant. That the replacement in (Nash or even subgame-perfect) l Stability. There should be a distinguishable librium point to the negotiation protocol situation, we would mend building late the efficiency condition, agreement. Being a “simple equi- as a game). 5 Given a specific that we could recom- to build into their agents. No designer will benefit by point should not vio- in a Pareto-optimal to build that use any other strategy. The equilibrium i.e., the negotiation strategy” means should result that it is feasible like to be able to find simple strategies to all agent designers (considered it into an agents 5 For additional discussion of the concepts of Nash and subgame-perfect equilibrium, and Pareto-optimality, see Sections I .5 and 2. I below. 302 S. Kraus et ul./Art$cial Intelligence 75 (1995) 297-345 Table 1 The rows indicate Degree of control on the “social Layer”; the columns indicate degree of control on other agents in the domain Structured Unstructured DPS MA 0 Moses, Shoham & Tennenholtz [ 38,53 1, Davis & Smith [54], Malone 1371, Lesser [5,31], Durfee [I I, 121, Zlotkin & Rosenschein [ 45.6 1,641, Wellman [ 601, Ephrati & Rosenschein [ 131, Kraus, Wilkenfeld & Zlotkin Sycara [57,58], Kraus & Lehmann 124,251, Grosz ] 19,341, Gasser 1171 agent. A “simple strategy” also presumes that an agent will be able to in a reasonable the strategy automated compute Sutisfiabifity or accessibility. In agent that needs the resource starvation). performed. In the task distribution amount of time. the resource allocation to eventually have access to the resource case, we would case we would like an is no like the task to eventually be (there 1.4. Related work in DA1 The study of multi-agent interaction has been receiving increasing attention within ar- (AI). This is a direct outgrowth of the serious consideration tificial intelligence being given to agents operating highly restricted domains were considered such as Shakey environments. in challenging, currently real-world environments. For many years, for AI research purposes, and agents restricted in simplified, sufficient [ 141 could be designed and built for operation The research on agent architectures and on planning the existence of a static domain, i.e., our agent. Once researchers began, typically made several standard and the for a variety of the lack of deadlines, to move into realistic domains, including assumptions, existence of a single agent, reasons, The research precisely on the transformation agent, temporal, dynamically in planning capable ones. and agent architectures these assumptions had to be quickly discarded. of the last decade has been focused into multi- atemporal, theories static of single-agent, Researchers on agent interaction differ over the basic assumption of the degree of that the designer has over individual control (i.e., interaction mechanisms). (see Table 1) . On the first dimension we have the degree of control over the social layer of the agents. It ranges from a highly structured to a totally unstructured agents and over their social environment classification Therefore, we can make a two-dimensional interaction mechanism interaction. the degree of control On the second dimension we have agents. It ranges from the case where a single designer individual even explicitly design) as Distributed Problem Solving and each is able to design only its agent and has no control over the internal design of other agents that a designer has over is able to control (or [those systems are known to the case where there are multiple designers (those systems are known as Multi-Agent each individual in the domain in the domain (DPS)), (MA)). system agent S. Kraus et al./Art~~cial Intelligence 75 (1995) 297-345 303 The second dimension is also tightly coupled to the issue of agents’ to be centrally designed agents are assumed general goal. In such cases each agent When agents are designed by different designers, individual motivation to achieve tries they are also assumed to maximize incentives. When to have a common some system global utility. to have they are usually assumed their own goal and to maximize their own utility. to coordinate laws” are able and Moses on social laws [ 38,531 which shows that “pre-compiled” In the upper left corner of our two-dimensional matrix (i.e., designer can fully control agent and also the interaction environment) we find the work of Shoham, highly to restrict on-line they were designed to from the “social laws”. The same approach in the case were the social laws are “stable”, to follow the law. In our research we assume the issue of stability plays an each individual Tennenholtz “social structured conflict. Agents are assumed benefit and not because in an MA system could have been applied interest individual i.e., it is in each agent’s that the agents are individually motivated and therefore important Even role in the design of the interaction mechanism. it may be useful agent activity and laws” since to incorporate pure competition in DPS systems they individually the “social to follow introduced among the a form of simple the availability of tasks among cooperative agents, with one agent announcing them to other bidding agents. Malone it with a more sophisticated agents. Davis and Smith’s work on the Contract Net [54] negotiation and awarding by overlaying under certain conditions. oriented are motivated self-motivated. to help each other. Such an assumption the main underlying refinements assumption this technique considerably [37], proving optimality In the general contract net approach and also in the economic- economic model refined is that agents are “benevolent” and is not feasible when agents are to construct global solutions. Multi-agent planning in the communication Another more experimentally domain (DVMT) has been [ 7,11,12]. architecture the on-going Accurate, Cooperative, is presented in the “Functionally tial Global Planning” monitoring tial solutions network domain was treated as a distributed mented by using a multi-stage each agent sufficient correct. These researchers approach directly and general mechanisms negotiation to enable information based approach for inter-agent research of Lesser, Durfee and (FA/C)” paradigm. For example and evaluated implemented The agents iteratively exchange collaboration their colleagues in DPS using the “Par- in the vehicle tentative par- constraint on satisfaction [5]. The multi-stage it to make local decisions and was imple- provides that are globally the issue of global efficiency and performance more tools the use of formal negotiation in real-world working systems, while we are analyzing in more idealized domains. communication We are unfamiliar with work that belongs are centrally designed, but use unstructured due to the fact that since structured communication cient cooperation, structured communication to the upper right corner where the agents protocols. This is, of course, protocols usually provide more effi- and if the designer has control over all the agents, it can incorporate a in the agents to make the DPS system more efficient. (i.e., no the case in agents. a model of are interacting with each other and with autonomous Sycara the designers have control only over their agents control over other agents or the interaction mechanism). domains where humans For example, in the case of labor negotiation, In the lower right corner is usually presented protocol [57,58] This 304 S. Kraus et al. /Art$cial Intelligence 75 (199.5) 297-345 negotiation that combines case-based reasoning and optimization of multi-attribute util- ities. In her work agents try to influence the goals and intentions of their opponents. In 124,251 Kraus and Lehmann developed an automated Diplomacy player that nego- tiates and plays well in actual games against human players. Researchers in discourse [ 19,341 ) develop formal models to support communication in understanding (e.g., human/machine interaction. There, models of individual and shared plans are used for understanding non-structured communication. Gasser [ 171 focuses on the social aspects of agent knowledge and action in multi- agent systems (“communities of programs”), As in real-world societies, social mech- anisms can dynamically emerge. Communities of programs can generate, modify, and codify their own local languages of interaction. Gasser’s approach may be most effective when agents are interacting in unstructured domains, or in domains where their structure is continuously changing. In our research, we choose to pre-design the social layer of multi-agent systems by creating a structured interaction mechanism, i.e., a model of alternating offers. In most of the unstructured negotiation scenarios there is no guarantee that agreement will be reached, and the negotiation may take a long time. In the current work the negotiation always ends at the latest in the second stage of the negotiation and if there is complete information, agreement is guaranteed. The work we describe in the present paper, resides in the lower left corner. It assumes that there is full control over the agent interaction mechanism by bounding the agents to highly structured public behavior (like negotiation protocols, voting procedures, bidding mechanisms, etc.). However, as we mentioned in the introduction, our work is concerned with problems in developing agents in multi-agent systems. That is, there is no control over the other agent’s private behavior. This gap is bridged by carefully adjusting the interaction mechanism such that it will be stable. Using a stable mechanism, it is to the benefit of each individual agent (that wishes to maximize its own private utility) to adopt a given private behavior. When those private behaviors (strategies) are in equilibrium, then the designers of the interaction protocols can assume that the individual agents will be designed to have those private behaviors even though the protocols’ designers have no explicit control. Ephrati and Rosenschein [ 131 used the Clarke Tax voting procedure as a consensus mechanism. The mechanism assumes an explicit utility transferability (i.e., a kind of monetary system). In the problem of task distribution and resource allocation there is no explicit way to transfer utility. There is an implicit way to transfer utility, e.g., by executing one of your tasks I may transfer some utility I could have been getting to you. However, this implicit utility transfer is not sufficient for the implementation of the Clark Tax procedure. The Clark Tax mechanism assumes that agents are able to transfer utility out of the system (the taxes that are being paid by the agents). The utility that is transferred out of the system is actually wasted and reduces the efficiency of the consensus that is reached. This is the price that needs to be paid to ensure stability. In the paper we introduce a negotiation mechanism that provides both efficiency and stability. Zlotkin and Rosenschein [45,61,64] analyze the relationship between the attributes of the domain in which the agents are operating and the availability of interaction S. Kraus et al./Artijicial Intelligence 75 (1995) 297-345 305 interaction domains as task-oriented domains, state-oriented domains. functions, In all of the above domains It may be appropriate when negotiation to plan execution to satisfy the efficiency, simplicity, mechanisms have classified worth-oriented agent’s utility relative plays an important be neglected. The approach presented domain and provides coordination mechanisms the MA/structured delay. Within temporal aspect of negotiation In this paper we consider time. However, time plays no explicit symmetry, and stability conditions. They domains and role in the time can be neglected time systems, negotiation of the system and cannot in this paper focuses precisely on this kind of that ensure efficient agreements with no to treat the it is the first attempt in highly dynamic role in the evaluation of the performance group of researchers explicitly. the problem where agreements in situations negotiation mechanisms, that includes some of the agents while excluding others, are discussed involve the agents. in which agents are free to form in all Multi-agent’s any coalition [22,52,62]. Wellman anism design reduced used to ensure efficiency mapped resource whole community To summarize, that [60] uses a market-oriented (“market-oriented to a simple consumer-producer programming”). When approach the agent for inter-agent coordination mech- can be interaction can be relation a market pricing mechanism interaction can be in the case of a common the in the present approach), paradigm. For example, that are considered and stability. However, not all inter-agent to the consumer-producer (i.e., one of the encounters of agents are consumers. our work is characterized by providing a formal strategic model of the negotiation into and task distribution, without side that we made is on the negotiation protocol, which is in Section 3. However, we as the negotiation in detail process itself about the offers the agents make during takes the passage of time during It can be used for both resource allocation negotiation account. payments. The only assumptions a protocol of alternating don’t make any assumptions is the case in some other work (e.g., to any previous offers that have been made. Nevertheless, delay. offers which we describe [63]). the agents are not bounded ends with no the negotiation In particular, 1.5. Related work in economics and game theory is the formal [ 39,401, who attempted There are two main approaches ation process. The first is informal for a negotiator and to assist a negotiator The other approach Nash and to prove different different circumstances. various situations and precise results concerning However, group. it requires making This formal game theorems about to construct restrictive to the development of theorems relating theories which attempt to the negoti- to identify possible strategies in achieving optimal results (see [ 10,16,21] ) . theory of bargaining originating with the work of John formal models of negotiation environments the best strategies a negotiator can follow under theory approach provides clear analyses of the strategy a negotiator should choose. to the first that are unacceptable assumptions Following Genesereth, Ginsberg, Rosenschein the use of game-theoretic techniques for artificial and Doyle, intelligence [ 8,9,18,44], purposes. We propose we propose to 306 S. Kraus et al./Art@cial Intelligence 75 (1995) 297-345 develop a strategic model of negotiation that can serve as the basis for building efficient automated negotiators. The formal game theory approach is also divided into two central sub-approaches concerning the bargaining problem (see [ 201) . The first is the strategic approach. The agents’ negotiating maneuvers are moves in a noncooperative game and the rationality assumption is expressed by investigation of the Nash equilibrium. 6 The second approach is the axiomatic method. It makes assumptions about the solution of a negotiation situation without specifying the bargaining process itself (the literature on the axiomatic approach to bargaining is surveyed by Roth [46], and can also be found in [ 351 with a general introduction to game theory). Since we intend to use our theoretical work as a basis for the development of auto- mated negotiators, we have adopted the strategic approach. Rubinstein [47] and Stahl [ 561 developed models of alternating offers, which take time into consideration. Shaked and Sutton [51] extended these works by developing models in which an agent can opt out of the game. Those works are closely related to our desired models (see [ 411 for a detailed review of the bargaining game of alternating offers). Nevertheless, several important modifications are needed. These mainly concern the way time influences the preferences of the agents, the possibility that both agents can opt out, and the preferences of the agents over opting out. Only the results in Section 3.2 are based on Rubinstein’s previous work; all the other results are ours. 2. lko fully informed agents In the next three sections we consider the case where two fully informed agents negotiate to reach agreement on resource allocation or on task distribution. These situations are characterized by the following assumptions. (1) Bilateral Negotiation. Even if there are several agents in the environment, the initial assumption is that in a given period of time no more than two agents need the same resource (we will relax this assumption in Section 7). When there is an overlap between the time segments in which two agents need the resource, these agents will be involved in a negotiation process. Full Information. Each agent knows all relevant information including the other agent’s utilities for the different outcomes over time (we will relax this assump- tion in Section 6). Rationality. The agents are rational; they try to maximize their utilities and behave according to their preferences. Commitments are Kept. If an agreement is reached both sides will honor it. No Long-Tenn Commitments. Each negotiation stands alone. An agent cannot commit itself to any future activity other than the agreed-upon schedule. Resource Division Possibilities. We assume that any division of the resource is possible (we will relax this assumption from Section 4 onwards). (2) (3) (4) (5) (6) 6 A pair of strategies (u, T) is a Nash equilibrium if, given 7, no strategy of agent 1 results in an outcome that agent 1 prefers to the outcome generated by (v, 7) and similarly for agent 2 given (T. S. Kraus et al. /Artificial Intelligence 75 (1995) 297-345 307 (7) (8) No Other Options. The agents have no alternative, but to continue until an agreement this assumption onwards). Common Belief. Assumptions ( 1)-( 7) are common belief. (we will relax is reached the negotiations from Section 4 2.1. Strategies and equilibrium Our strategic model of negotiation How will a rational agent choose is the Nash equilibrium is designed useful notion it is known strategy other than this one. that an agent is a model of Alternating Offers. 7 in such an environment? A strategy its negotiation [35,40]. and if If there is a unique equilibrium, to use this strategy, no agent will prefer to use a is not an effective way of analyzing there may be some points the out- in the However, the use of Nash equilibrium comes of the models of Alternating Offers since negotiation where one or more agents prefer strategies. Nash equilibrium the negotiation, restrictions the proof). but may be unstable on the outcome and also yields to diverge from strategies may be in equilibrium in intermediate only stages. Nash equilibrium too many equilibrium points their Nash equilibrium in the beginning of puts few (see ( [47] for [47,50]) which requires Therefore, we will use the stronger notion of (subgame-)perJect equilibrium (PE) at any that the agents’ strategies that an agent i.e., in each stage of the negotiation, its own the other agent has no strategy better argument, of agents at each stage of the game to decide what a good choice and to use this strategy, no agent will prefer to use (see stage of the negotiation, follows the PE strategy, PE strategy. Subgame-perfect using is and then rolling backward if it is known a strategy other than this one in each stage of the negotiations. induce an equilibrium assuming than to follow induction [ 591. So, if there is a (unique) that an agent is designed perfect equilibrium, the rationality is essentially a backward equilibrium We will consider different variations of this model. In the first case, we assume the agents are bound reaching an agreement. Otherwise, second case, the agents are able to opt out at any stage of the negotiation. i.e., the negotiation the agents will continue to an agreement, to negotiate that process can end only by forever. In the 3. The bounding negotiations mechanism When agents are bounded to an agreement, force for reaching an agreement driving attitudes into consideration by each agent. Even though we will show that agreement will be reached without any delay. time. We assume that negotiation toward negotiation in a reasonable the negotiation negotiation may continue forever. The amount of time is the agents’ time is expensive and taken can continue indefinitely We utilize modified definitions from [41]. We assume that there is a set of agents A = {I, 2). We present a formal definition of an agreement. ’ See [41] for a detailed review of the bargaining game of Alternating Offers. 308 S. Kraus et al./Artijicial Intelligence 75 (1995) 297-345 Definition 1 (Agreement). An agreement agent i’s portion of the resource. The set of possible agreements is an ordered pair (~1. ST), in which Si is is S={(.S~,S~)EIW~: sl+.s2=1andsiLO, fori=1,2} Each agent has a preference over the set of possible agreements S. Our sole assumption is that an agent prefers an agreement an agreement Negotiation that gives it less. is a process that may that gives it a larger portion of the resource over include several iterations and may even continue times (j says Yes), In each period (j) either accepts that are fixed in advanced. from S, and the other agent that agents can take actions only at certain forever. We assume (0, 1,2...} an agreement it (No). If the offer is accepted tation of the agreement a rejection, no rules which bind not bound framework is no limit on the number of periods. An agent’s negotiation function in the set 7 = t E 7 one agent, say i, proposes the offer (Yes) or rejects ends with implemen- to the agreement). After and so on. There are the agents are to any previous offers that have been made. The mechanism only provides a condition, but there for the negotiation process and specifies is any from the history of the negotiations then has to make a counteroffer to any specific strategy. then the negotiation is used according to its next move. (i.e., agent the termination In particular, the rejecting the resource the agents in general strategy Definition 2 (Negotiation strategies). A strategy is a sequence of functions The domain of the ith element of a strategy possible histories up to period move). is a sequence of offers of length i (all is the set {Yes, No} U S (its current i) and its range f = { f’}z. That is, if f is a strategy for the first agent to make an offer (agent 1) then f” E S and for t even f’ : S’ + S, and for t odd f’ : St+’ -+ {Yes, No} (S’ is the set of all in S and Yes and No are defined above). We denote sequences of length by F the set of all strategies of the agent which starts the bargaining. Similarly, let G to the in the first move, has to respond be the set of all strategies of the agent which, other agent’s offer; such that is, G is the set of all sequences of functions g = {g’}z that for t even g’ : St+’ --f {Yes, No} and for t odd g’ : S’ -+ S. t of elements Let C( f, g) be a sequence of offers in which agent 1 starts the bargaining f E F, and agent 2 adopts g E G. Let Length( f,g) the length may be infinite). Let Last( f,g) such an element). We present a formal definition be the length of a(f,g> be the last element of a(f,g) for the outcome of the negotiation. and adopts (where (if there is Definition 3 (Outcome of the negotiation). The outcome defined by function of the negotiation is outcome(f’ g, = Disagreement, (Last ( f, g) , Length( f, g) - 1) , if Length( f, g) = 00, otherwise. Thus, in period the outcome t and the symbol Disagreement (s, t) where s E S is interpreted as the reaching of agreement s indicates a perpetual disagreement. S. Kraus et al./Artijicial Intelligence 75 (1995) 297-345 309 We note here that by defining an outcome we have made a restrictive that about agents care only about the nature of the agreement, and the time at which the outcome is reached, and not about to the agreement, the sequence of offers and counteroffers (see i.e., there is no “decision-regret” assumption that leads [ 421) . to be either a pair (s, t) or Disagreement, the agents’ preferences. We assume 3.1. The agents’ utility functions We also assume that agent i E A has a continuous outcomes: Ui: {S x I} U {Disagreement} for an agent from one outcome the agent prefers in Section 2). the first outcome over the second With the exception of Section 6 which considers utility -+ IX. Throughout function over all possible the paper, when the utility is greater than from another outcome, we will assume that (Due to assumption (3), Rationality, is the relation tion, none of the results reported functions specific agreement whether and not where functions. One of the issues related of these utility in Section 9. the agents have preferences functions, or determining among situations with incomplete the utility values of the outcomes. That informa- in this paper depend on the exact values of the utility the key role in reaching a is, than the utility s’ for a given agent, in systems on the possible outcomes and not numerical utility is the computing to the application of these results this will be discussed briefly the preferences; s is greater the utility of an agreement the exact utility values of s and s’. The results can be obtained over the possible outcome. The factor that plays 3.1. I. Attributes of the utility functions We now present a number of assumptions agents. This basic set of assumptions will be added sections as we introduce time, and multiple out, refers. assumption additional agents. Subscripts conditions denote concerning the utility to and modified such as incomplete information, the section to which functions of the in subsequent opting the specific The first assumption states that agents prefer any agreement in any given time period over the continuation of the negotiation process indefinitely. A03 (Disagreement U’( (s, t) ) > U’( Disagreement). is the worst outcome). For every s E S, i E A and t E 7, The next two conditions i.e., agreements U’ on S x I, that among agreements resource. (Al3 and A23) concern function time periods. Condition A13 requires in the same period, agent i prefers larger portions of the the behavior of the utility in different reached reached (The resource is valuable). For all Al3 U’( (r, t) > > fJ’( (s, t) > . 8 For agreements each agent prefers to get a larger portion of the resource. t E 7, that are reached within r, s E S and i E A: ri > si + the same time period, ’ For all s E S and i E A, Si is agent i’s portion of the resource. 310 S. Kraus et al. /Art@cial Intelligence 75 (I 995) 297-345 The next assumption states that time is valuable to both sides. A23 (Time is valuable). For any tl, t2 E I, s E S and i E A, if tl < t2, U'( (s, tl)) 2 U'((s,tz)). The next assumption It requires ments. only on st , s2 and the differences between that the difference in utility between tl and t2. greatly simplifies the behavior of the utility for agree- function (~1, tl) and (~2, t2) depends A33 (Stationarity).Forallr,sES, 6)) iffu'((r,t:!)) L U'((s,t:!+S)) ti,t2,6EIandiEd, U’((r,tl)) >Ui((s,tl+ By assumption A23 the agents prefer to receive any given share of the resource sooner that the loss associated rather than later. The following assumption with any given amount is an increasing imposes the condition function of that amount. A43 (Increasing loss). For every i E A, t E 7 and s E S, there exists s’ E S such that for every s,r,s’,r’ E S such that U’((s,t>> = U’((s,t)) = U’((s’,O)). Furthermore, d((s’,O)) and U”((r,t)) =U’((r’,O)) ifs; > ri then si-si > ri-ri. 3.1.2. Examples of utility functions We will examine two examples of utility functions which conform to assumptions A03-A43. 3.1.2.1. Time constant discount rates In the first case, we consider a utility function with a time constant discount rate. That rate 0 < Si < 1. If the agents reach an agreement is, every agent i has a fixed discount in time period t in which agent i’s portion of the resource is si, then its utility will be Siaf. Definition 4 (Utility an outcome of the negotiation, then the Utility,( Sisi, where 0 < Si < 1, and Utilityi{Disagreement} function with time constant discount rate). Let (s, t) E S x 7 be to be (s, t)) where i E A is defined = -CXJ.~ 3.1.2.2. Constant cost of delay The second case is of a utility agent bears a fixed cost for each period. That is, each agent and if the agents reach an agreement resource is si, then its utility will be si - qt. Formally: in time period function with a constant cost due to delay. Here, every i has a constant ci > 0, t in which agent i’s portion of the Definition 5 ( Utility function with a constant cost of delay). Let (s, t) E S x 7 be an then the Utility: { (s, t)} where i E A is defined to be si - Cit outcome of the negotiation; where ci > 0 and Utilityl{Disagreement} = -cc. ’ Here, Sfsi denotes Si to the rth power times Si. S. Kraus et al. /Artificial Intelligence 75 (1995) 297-345 311 A utility function with a time constant discount rate satisfies all the above conditions, while a utility function with a constant cost due to delay satisfies all but A4s. 3.2. Equilibrium yields agreement with no delay In [ 471 it was proved that for continuous utility functions that satisfy axioms AOs-A43 there exists a unique PE, which results in the successful termination of the negotiation after the first period. lo This unique solution is characterized by a pair of agreements x* and y* that satisfy these conditions: (1) agent 1 is indifferent between “y* today” and “x* tomorrow”, and (2) agent 2 is indifferent between “x* today” and “y* tomorrow”. When a unique pair of x* and y* satisfies this statement, there exists a unique PE [47]. The structure of the unique perfect equilibrium is as follows: agent 1 [2] always suggests x* [y*] and agent 2 [l] accepts any offer which is at least as good for it as .X* ry*1. We will now demonstrate the usage of these results for the two types of utility functions: (1) a constant discount rate; and (2) a constant cost of delay. 3.2.1. Constant discount rate In the case of a utility function with a constant discount rate, as in Definition 4, agent 1 has a discount rate of 0 < 61 < 1, and agent 2 has a discount rate of 0 < 8~ < 1, where Utility, ( (s, t) ) = 8: ( si) . According to the PE strategies, in every period of time, when it is agent l’s turn to make an offer, it will offer agent 2 ( &$-, w ) . When agent 1 receives an offer from agent 2, agent 1 will accept only the offers where its share of the resource is at least s1(1-82). On the other hand, when it ‘G”:gent 2’s turn to make an offer, it will offer , &$-). Agent 2 will accept an offer only if its share in it is at least w. ( w The agreement that will be reached in the first period is ( &$-, w ). Formally: Lemma 6 (Rubinstein[47] ). Suppose agent 1 starts the negotiations. Let (f?g^) is a subgame-perfect equilibrium of the strategic model of Alternating Offers where the agents’ utility function is defined in Definition 4 iff f3(s”, . . . . s’-‘) = x* for all (so ,..., s’-l) E S’, if t is even, and &SO, . ..) s’) = Yes, No ’ { if si 2 y;, ifs: <yl*, if t is odd. The strategy g^ of agent 2 has the same structure; the roles of x* and y* are reversed, the words “odd” and “even” are interchanged, and each subscript 1 is lo Rubinstein’s their utility functions. We use utility function results are actually more general, and consider the case of agents’ preferences to be consistent with our approach in Section 6. in addition to 312 S. Kraus et al./Artijicial Intelligence 75 (1995) 297-345 replaced by 2. The outcome is that agent 1 proposes a? in the$rst period (period 0), and agent 2 immediately accepts this offer. Proof. The proof and additional discussion can be found in [41]. 0 Even indefinitely, though to continue in the unique subgame-perfect the structure of the strategic model of Alternating Offers allows nego- it terminates that will be reached depends mainly on the patience losses over time are less than its opponent’s will get function with less patient means having a larger value of aj. That is, if 61 is l’s share tiation immediately. The exact agreement of the agents. An agent whose a larger share of the resource. For example, discount smaller, agent is larger. is smaller, while if 82 is smaller, agent if the agents have a utility l’s share of the resource rates then being equilibrium In addition, the negotiation the agent which starts has an advantage over the other if both agents have the same rate of delay 6, then the first one will l/( 1 + 8) of the resource and the other will receive 6/( 1 + 6)). A simple way of to [ 4 1 ] > : at the beginning across periods) agent (for example, receive to avoid this asymmetry each period each agent be the one to make the first offer. is chosen with probability i (independently is the following in the model (see 3.2.2. Constant cost of delay In the second case, suppose the agents’ utility function includes a constant cost of delay as defined where i E A. Even though PE if ct # ~2. in Definition 5. That is, Utilityi{(s, this utility function does not satisfy t)} = si - tit where ci > 0 and there is a unique (A43), ( l,O)), Suppose agent 1 is more patient than agent 2. That is, it loses less over time (i.e., y* = ( 1 - cl, cl ) ) and “( 1,0) tomorrow” than agent 2 (i.e., ci < ~2). In such a situation agent 1 is indifferent between “( 1 - cl, cl ) (i.e., x* = ( 1,O) ) and agent 2 is today” indifferent between “( 1,O) today” and “( 1 - cl, cl ) tomorrow”. Therefore, if it is agent l’s turn to make an offer it will always offer (l,O). Agent 1 will accept any agreement is greater or equal to 1 - cl. Agent 2 will accept any in which its share of the resource agreement that will (including be reached agent 1 will use the resource alone. The prediction here is quite surprising. than agent 2, it can gain all the resource. Agent 2 prefers Since agent 1 is more patient it over waiting an additional than period. agent 1, x* = (~2, 1 - ~2) and y* = (0,l). l’s turn to make - ~2)) and will accept any offer. Agent 2 will accept an offer it will offer agent 2 (c2,l 1 - c2 and will offer (0,l). Here also the results are quite extreme. Agent l’s share will only equal agent 2’s delay. Here again, the agent whose turn it is to make the first offer is in a better position These results demonstrate than the one which goes second. i’ that introducing and will offer ( 1 - cl, cl ). In the agreement If cl > ~2, that is, agent 2 is more patient the time factor into the negotiation process That is, when it is agent can lead to an efficient negotiation. ‘I Where cl = CT there are multiple subgame-perfect equilibria. S. Kraus et al./Artijicial Intelligence 75 (1995) 297-345 313 4. Unbounded negotiation mechanism when both agents lose over time Up to this point we have assumed since disagreement the agents have the ability is the worst outcome that the agents have no choice but to continue to both sides. Let us consider opt out of the negotiation. to unilaterally the agents they can satisfy try to satisfy a common goal but the agents have the (usually with lower priority); to leave a threat the outcome in some cases. the negotiation the case in which This can happen when some other goals negotiation may influence to carry blocks, If two agents need In the previous section, we have also assumed them, in any way that they have agreed upon. Unfortunately, that the agents can divide the work/re- this cannot to deliver packages or to build in a finite number the case of a finite discrete case. source between usually be done. tools, this work can be divided only in a discrete manner and usually of possible agreements. From now on we will consider That is, the simplifying Rationality, Commitments ( Common Belief) assumptions longer valid. We assume divided by two agents. ( 1 )-( 5) (Bilateral Negotiation, Full Information, (8) that are described at the beginning of Section 2 are still valid, while are no that must be that there are A4 units of the work (or resource) are Kept, No Long-Term Commitments) (Resource Division Possibilities) (No Other Options) and assumption assumptions and (7) (6) that are responsible for the delivery of electronic of two different companies. The delivery Example 7. There are two agents newsletters (either by fax machines or electronic mail). The expenses of the agents depend only on the number to both companies’ of phone calls. Therefore, newsletters, price of only one phone call. The agents negotiate over the distribution subscriptions. own newsletters by itself. if there the two newsletters may be delivered for the of the common and deliver all of its Each of the agents can opt out of the negotiations is someone who subscribes to it by one of the agents is done by phone We slightly modify the definition of an agreement 1). The set of possible agreements, S, includes all the pairs (sr , ~2) E N2 where st + s2 = M. We also modify i receives an offer from its the negotiation partner the offer (Yes) to accepting or rejecting it can opt out of the negotiation 2) such that if agent in addition (Definition (Definition strategies it (No). (Opt), a( f, g), Length( f, g), Outcome( f, g) and the outcome of the negotiations as in Section 2, but Outcome( f,g), which is the last element of a(f,g), s E S or Opt. Thus the outcome of the negotiation agents’ utility time, and over opting out at various points in this case are over agreements t. We note that the length of the time periods (Opt, t) is interpreted in time. That is, at period functions are defined can be either as one of the agents opting out is fixed. The in reached at various points U’: {{S U {Opt}} x 7) U {Disagreement} + R. 314 S. Kraus et al./Artijicial Intelligence 75 (1995) 297-345 4.1. Attributes of the utility functions The following set presented parties can choose set of assumptions, in Section 3, are necessary to opt out. some of which are modifications to model the negotiation situation of the original in which the Condition A03 (Disagreement is the worst outcome) of Section 3 is still valid. That is, disagreement is even worse than opting out. Formally we state it as follows: (Disagreement A04 U’( (s, t)) > U’(Disagreement) is the worst outcome). For every s E S, i E A and t E I, and U”( (Opt, t)) > U’(Disagreement). In this section we deal with that each agent prefers the case of task distribution. to do as little as possible. Therefore, assume (which was appropriate modified condition by A14. That is Al4 requires same period, agent i prefers smaller numbers of units si. to the resource allocation case) In such situations, we condition A13 the in the reached is modified. We denote that among agreements A14 (Actions are costly). For all t U’( (s, t) ) . For agreements prefers to perform a smaller portion of the labor. that are reached within E I, Y, s E S and i E A: Yi > si + U'( (r, t) ) < time period, each agent the same A23 (Time We will consider is valuable) is still valid. We denote it by A24. the case of constant delay, in which any agent has a number ci > 0 i E { 1,2} that satisfies the following condition.12 (Agreement’s A34 that: U’((s,tl)) cost over time). Each agent i E { 1,2} has a number ci > 0 such 1 P((S,ta)) iff (si+citl) 5 (Si+Citz). We note that assumption A34 does not hold for Opt. We also assume that both agents prefer to opt out sooner rather than later. Formally: A44 (Opting out costs over time). For tl, t2 E 7 and i E { 1,2}, if tl < t2 then U’((Opt,t,)) > W(Opt,tz)). We do not make any assumption concerning versus an agreement. This enables us to consider different Formally, as in [51]. there is no fixed s E S such that for every the preferences of an agent for opting out types of cases of opting out. t E 7, U’( (s, t) ) = U’( (Opt, t) ) The main factor out of the negotiation still preferable that plays a role in reaching is the worst agreement for agent an agreement when agents can opt is this agreement by i in a given period t. We will denote t which to i than opting out in time period I2 In the rest of the paper we assume is any real number. that CL is an integer. However, similar results can be obtained when Ci S. Kraus er al./Artijicial Intelligence 75 (1995) 297-345 315 $’ E S. If agent i will not agree to such an agreement, but to opt out. its opponent has no other choice Definition 8 (Agreements and agent i E A let that are preferred over opting out). For every period t E 7 Possiblef “zf {s’ 1 s’ E s, U”((s’,t)) > U’((Opt,t))) be the set of all the possible opting out in period to be the only one that satisfies t. If Possiblef agreements that are preferred by agent i in period t to is not empty, we define the agreement 3’9’ E Possible: U’W,t)) =sEmir&uiw)). Otherwise we define j:‘,‘= (-l,M+ 1) and j;2*t= (M+ 1,-l). If Possible,’ is not empty then there will be only one minimal Sl’*‘. This is because of assumption A 14 above. if there is at least one agreement An agreement may only be reached that both agents prefer over opting out. So, in order to reach an agreement, an agent i should prefer over opting out the worst agreement is, for agent j in a given time t that is better to i’s utility agent j than opting out (3’) for itself (i.e., if U’( (?i’, t)) 2 U’( (?*j, t))). Note that by that is better then Uj( (Zip’, t)) 2 Uj( ($j,‘, t)). Condition A34 if U’( (@, that will ensure We will now introduce is at least equal to i’s utility from the worst agreement t)) 2 U’( (3’*‘, t)) two additional j other than j’s opting out. That from the worst agreement than opting out (@) that an agreement for its opponent assumptions will be reached. A54 (Agreements U’((Opt,t)) versus opting out). For every then U’((s,t- 1)) > U’((Opt,t- i E { 1,2}, if U’( (s, t) ) > t E 7 1)). Assumption A54 then indicates that if an agreement some time period, That is, the set of acceptable An additional assumption agreements is necessary it will also be preferred in the previous in the first period. That is, there is an agreement for an agent is not increasing over time. to ensure that an agreement is possible at least that both agents prefer over opting out. is preferred over opting out in time periods over opting out. A64 (Possible agreement). For all i, j E A, U’( ( ?i”, 0) ) 2 U”( (?,O, 0) ) ii,’ is the worst agreement We will assume that there is some time period T in which there is no agreement for agent i in period 0 which is still better than opting out. that time period may be viewed as a for both agents over opting out. This is acceptable deadline. A74 (Time period when agreement for all i,j E d, U’((s^ Jr T) ) < U’( ( 3i,T, T) ). We denote periods by ?‘. is not possible). There exists a time period T where time the earliest of these , 316 S. Kruus et al./Artifcial Intelligence 7.5 (1995) 297-345 4.2. Agreement is guaranteed with no delay Even though of the negotiation there are always agreements the agents have the option to opt out of the negotiation in any step, if the strategies, agreement will be reached without any delay. in the that are better to both agents than opting out. If the agents in the first time period by the that is offered to make an offer, which will be preferred by its opponents over all possible to an in this case is the agents use perfect equilibrium No agent will use the option of opting out since beginning use PE strategies, first agent future outcomes. As in the previous case (Section 3) when agents were bounded agreement, cost of the negotiation time. The agents’ attitudes will only affect the details of the actual agreement of the agents toward opting out versus agreements that is reached, but won’t drive any force for the agent to reach an agreement the main driving is an agreement to opt out. there As the first step to proving that under the above assumptions, then an agreement will be reached in p - 1. The main reason to avoid opting out and will agree to the worst agreement better than opting out. the existence of such an agreement, we will now prove if the negotiation has not ended in periods prior to p, i.e., in the period prior to this period will try for themselves which is still prior to this period, is that both agents in the period immediately is Lemma 9 (Agreement will be reached prior All the perfect equilibrium (PE) strategies of a model satisfying no longer possible). A04-A64 satisfy the following: If it is agent 2’s turn in time period F - 1 then using its PE strategy it will suggest SI1vT-’ and if it is agent 1 ‘s turn it will suggest S^2*T-1. In both cases the other party will accept the offer. to the time period when agreement since disagreement process will end with one of the agents opting out. Actually, Proof. First note that by A64 ? # 0 and therefore ? - 1 E 7. Now, suppose that it is agent 2’s turn to make an offer at time period ? - 1. It is clear that agreement won’t (A04), be reached after this period. Therefore, the negotiation the agents prefer opting out sooner rather than later (A44), agent 2 will opt out in the next time period. But, by A64 and A74, in time period ‘? - 1 there are still some agreements that both agents prefer over opting out (at least one). Agent 2 can choose agreement accept this offer. The best agreement when it is agent l’s turn to make an offer in time period p-- 1, is similar the best its point of view and agent 1 does not have any other choice but to from agent 2’s point of view is s”l,T-‘. The proof, 0 is the worst outcome to this one. since from In the rest of the section, we assume l’s and agent 2’s positions Since agent when agent 2 is the first to make an offer. that agent 1 is the first agent to make an offer. are similar, all the results can also be proved We will now define the agreement this definition to make an offer. This agreement will be acceptable behind offer considers It may offer an agreement is the following: the possible agreement that will be better in each step the agent whose turn it is to make an that can be reached in the following time periods. than what that other to the other agent that will be offered by an agent when it is its turn to the other agent. The intuition S. Kraus et al./Artifcial Intelligence 75 (1995) 297-345 317 agent can get in the next periods. However, these possible agent will offer the second agent second agent’s will be signed is even or odd. the offer will be the worst agreement among future agreements. That is, since the agents are losing over time, the first the that lemma. The definition depends on whether ? losses over time. The starting point is clear from the previous is ir’ - 1 where the agreement in the next period minus the possible agreement Definition 10 (Acceptable agreements). l p is even. Suppose it is agent 2’s turn to make an offer in time period ir’ - 1, (i.e, ~-1isodd).Letusdefinexr-‘=~‘~T-1.Foranyt~7,t=~-kk,1<k<~, if t is even we define Xf = (gf.T-1 -~k~~+(;k-l)q&~-~ +;kcz-(;k- 1)ct). If t is odd we define X’ = ($>‘-I - ;(k- 1)c2+ ;(k- I)c,,$~‘-~ + ;(k- l)c2-- ;(k- 1)~~). l ?’ is odd. In this case it-is agent l’s turn to make an offer in time period ir’ - 1. Letusdefine~~-*=3~,~-~.ForanytEI,t=~-k,l<kI~,iftisevenwe define -2.C 1 x’ = (St - ;(k - l)c2 + ;(k - l)c&*- + +(k- 1)~ - ;(k- 1)ct). If t is odd we define x’ = ($P’ + +kq - (;k - l)~~,sI;‘~-’ + (;k - 1)c2 - ;kq). If i‘ is even, no agreement is some advantage advantage can be reached, that is, it is agent 2’s turn the period where to agent 2. If f is odd, there the .?*T~-’ are quite close, in both cases is small. Also, the agent that is more patient gets a better offer. there is a small advantage to make an offer before to agent 1. However, since S1l*T-l and 1 We will show by induction on k that if the agents follow their perfect equilibrium strategies, will accept the agent whose turn it is to make an offer will offer xt and the other agent this offer. the proof idea behind both agents prefer xf in time period is the following: Both agents prefer X’ over opting The main t + 1. out. Furthermore, turn it is_ to make an offer in And time period in Lemma 9), it is clear that in p - 2, xtw2 is the best option for the agent whose turn it is to make an offer in time period p - 2. Similarly .t? is the best such agreement t. In particular, since in f - 1 the agreement will be xT-’ t over x’+’ at time period for the agent whose (as we proved time periods. in previous AS4 (Losses due to opting out versus losses resulting from agreement). (1) For any t < ih, U’((@,t)) (2) For any t < f, Zi2’ - itg’-’ 1. i(c2 - cl) and #St - 3i*r-1 5 i(ct > Ui((Zi,r-l,t - 1)). - ~2). We note that the first part indicates that if there is at least one agreement to both sides (i.e., U’( ($Tj t) ) > U’( ( Slisr, t) ) ), then the agents also prefer acceptable the other 318 S. Kruus et al. /Artijicial Intelligence 75 (1995) 297-345 that is still better to it than opting out in the next period, agent’s worst agreement opting out in the current period. If c’ = c2 then the second part (2) true. is even. The proofs where f is odd are similar. We first prove that x1 is preferred by both agents over opting out. is always the proofs for the case where f lemmas we will describe In the following than Lemma 11 ( xr is acceptable). t~l, l<k<?, t=p-k, If the model satisfies assumptions A04-A84 then for any U’((x’,t)) >U’((Opt,t)). Proof. The proof is based on backward induction on t. Base case (t = ?” - 2): In this case, t is even and ,rfP2 = ( $qf-’ We first show that the hypothesis - 1)) I)) > U’((Opt,f is correct it is also + ~2). for agent 1. By A54, since U1 (( 3” > the case that U’((Z’x’-I,?-2)) - ~2, $,‘- f- l,?- U’ ((Opt,? - 2)). But, since actions are costly (A14), it is clear that U’( (Zt,‘-’ - c2,s2 Al&' +c2),?-2) > U’((Opt,P-2)). We now show it for agent 2. By A84, U2((i1*f-‘,f- U2 ( (Opt, ? - 2) ). By A34, it is clear U2((31,%1 , T - 1)) and we can conclude that lJ2 ( (,I;,‘-’ 1)) > U2((~2,f-2,f - c2,9 n’,P-’ + cz),P - 2)) > - 2) 2 that U2( ($*‘-’ _ c2,32 nl,f-l +Q),T-2) > U2((Opt,C 2)). Induction case (t < f - 2): Suppose the hypothesis is true for any t’, t < t’ < f - 2 and let t = ? - k. (1) If t is even x’ = ($t,‘-’ - ikc2 + (ik - l)c’,5iXf-’ + ikc2 - (ik - l)c’), which is actually (xi+’ - ~2, xi+’ + ~2). For agent 1, by the induction hypothesis, U’ ( (x’+’ , t + 1) ) > U’ ( (Opt, t+ I> > and by A54 17’ ( (x”l , t) ) > U’ ( (Opt, t) ) . By A33 it is clear that U’ ( (xi+’ - c2,x;+' +C2),t) > U'((Opt,t)). For agent 2, by A84 U2( ($l,t-l ,p- 1)) > U2((327p-2,?-2)) and by A34 _ c2 and s2 ^“T-1+~kc2-(~k-l)c, < Z;3f-2-c2+;kc2-(;k- . It is enough *‘ST-’ < 3;J-2 s2 1) c’ is 32,~-2 _ S12,~-k <c2--kc2+(;k-l)c, 2;(f(q -cz)) to show that Pi,‘-2 - c2 -t 4 kc2 - ( i k - 1) c’ < @rPk. That and by A84 we can conclude that U2((x’,t)) > U2((Opt,t)). But,c2-;kc2+(&k-1)~’ =(k- (2) If t is odd, X’ = ( $j9i-1 - ;(k - 1)c2 + ;(k - l)c,, @-’ + ;(k - 1)c2 - i( k - 1)~’ ) which is actually (x’+’ + c’, x'+' - CI ). The proof for agent 2 is similar to the proof for agent 1 when t is even. For agent 1, we need to show that Slf9r-’ - *.1,-i-l Sl ^ - 3:vT-k < $( k - 1) (c:! - c’ ). This is clear by A84. 0 i(k - 1)~ + i(k - 1)~’ < c?~,~-~, i.e., We will now prove that xf in period This is due to the construction do less in XI than in x’+‘. So, it is clear that agent 1 prefers period). For agent 2, it needs utility is the same from both options. of the x’s. For example, t is preferred by both agents over x’+’ in t + 1. if t is even then agent 1 will it (agent 1 also gains a the to do exactly what it loses over time, and therefore S. Kraus et al./Artificial Intelligence 75 (1995) 297-34.5 319 Lemma 12 (x’ is preferred over x’+‘). thenforanytEI, t=F--k, l<k~~,UU’((x’,t))>Ui((xtf’,t+l)). If the model satisfies assumptions A04-A84 Proof. If t is even then xi = xi” agent 2, x$ = xi+’ + c:! and the claim - c2 and by A24 the claim is clear for agent 1. For is clear by A34. Similarly, when t is odd. 0 We will now state our final results for this section. Theorem 13 (Agreement will be reached sumptions A04-A84 and the agents follow in the first period). Z’ the model satisfies as- their pegect equilibrium strategies, then l If p is even, agent 1 will ofSeer agent 2 in the first period ( ZiTT-’ - ipc2 + (if - I)c&’ + $fc, - (if - 1)cl) and agent 2 will accept the ofser. l If ? is odd, agent 1 will offer agent 2 in the first period ( ?f*T-’ - k (f - 1 )c2 + g- l)c&-1 +i(f--l)c2-i(f-1)~1)andagent2willaccepttheofler. Proof. Clear from the above lemmas. 0 ( NI and N2) are delivered by separate delivery services to the example of the newsletter deliverers. Two electronic (D1 and 02). The Example 14. We return newsletters publisher of N1 pays DI $200 for the delivery of one edition of NI to all its subscribers, of N2 pays D2 $225 per delivery. Each delivery and the publisher to any subscriber loses $1 (i.e., a phone call to the subscriber’s to both N1 and N2, for each time period. There are M subscribers with subscriptions if one or the other can deliver and there are substantial that there is an agreement between D1 and D2 for joint both newsletters. deliveries then the publisher of N1 will pay DI $170, and the publisher of N2 will pay D2 $200 (the lower prices reflect the fact that there are their joint delivery may competing detract from the sales impact of each newsletter). They must still pay $1 per phone call to the server, and will lose $2 over time. In the event to the M joint subscribers, server) costs D1 or D2 $1, and each in the two newsletters, and consequently to a delivery advertisers savings service A dollar Formally, is the smallest unit of currency in this example. U’((Opt,t)) U2( (Opt, =200-M-t, t)) = 225 - A4 - t, U’( (s, t)) = 170 - Sl - 2t, U2((s,t>> =200-s2-2t. Suppose M = 100. Then Sl’*f = (69 - t, 31 + t), j;2,t = (26 + t, 74 - Since p is even, by Theorem 13 the agreement (46,54). that will be reached t) and ?’ = 22. l3 is in the first period I3 We note that Ui(?-‘, t) > b”( (Opt, t)) (see Definition 8). 320 S. Kraus et al./Art$cial Intelligence 75 (1995) 297-345 5. One agent gains over time while the other loses time. This process. That We now consider and one of them already has access the case where one of the agents is the usual situation when losing over resource, the negotiation A which the resource. The agents start a negotiation process on the re-division them. A continues between ever). As in the previous reached. That can opt out of the negotiation. is gaining over time and one is the agents are sharing a common it during there are two agents: to use of the resource (if can be that both agents to use the resource until section, we assume is, s = { (SA, SW) E N*: s.4 + SW = M}. l4 We assume is, in each negotiation to) the resource and W which is waiting that only discrete agreements to the resource and is using is currently using the negotiation process ends interaction, (attached 5.1. Attributes of the utility functions We will now modify the definitions of the previous sections We will need to modify some of our assumptions agents. concerning to fit this new situation. the utility functions of the First we assume that the least preferred outcome for W is disagreement (Disagree- ment) while for A it is the most preferred outcome. A05 (Disagreement). For each x E {{~U{Opt}} and Uw (Disagreement) sible outcomes while agent W prefers any possible outcome over disagreement. < Uw (x) . Agent A prefers disagreement x I}: UA(x) < UA(Disagreement) over all other pos- Assumption AOs is not a formal conclusion from assumptions A25 and A35 below, but it is well motivated by them. The reason is that we need to define the limit situation. We could define it differently as well without changing anything. For example we could assume is the worst outcome that disagreement to both agents. Since we consider here the resource allocation problem, assumption Als it by Alj. is also valid in this case. We denote that the resource is valuable that asserts A23 is no longer valid, since time is valuable only for W and not for A. Therefore we modify condition A23. A25 (Cost over time). For any uW((s,tl)) > uW((s,t2)) tl, t2 E I, s E S and I UA((s,t2)). and UA((s,tl)) i E A, if tl < t2, then Similarly, we modify assumption A3. A35 (Agreement’s cost over time). Each agent i E {W, A} has a number ci such that: 2 (Si+cit2), where Vtl,t2 cw < 0 and CA > 0. l5 S,S E S, U’((s,tl)) 2 Ui((S,t2)) iff (si+citl) E 7, the rest of the paper, A’s portion I4 Throughout I5 We note the change in Section 4 we deal with costly action. in the direction of the inequality in an agreement will be written first. from A34, since here the resource is valuable, while S. Kraus et al./Art@cial Intelligence 75 (1995) 297-345 321 We assume ( cw < 0)) i.e., agent W prefers later, while agent A prefers that agent A gains over time (CA > 0) and that agent W loses over time to obtain any given number of units sooner rather than to obtain any given number of units later rather than sooner. Notice that assumptions Al5 and A25 are simple conclusions like to be able to distinguish between from assumption A35. the two different properties of the functions. One is the desirability of the resource and the second is monotonic We still would utility cost over time. A45 (Cost of opting out over time). For any t E 7, Uw ( (Opt, t) ) > Uw( (Opt, t + I ) ) and UA((Opt,t>> < UA((Opt,t+ 1)). W prefers opting out sooner than later and A always prefers opting out later rather than sooner. This is because A gains over time while W loses over time. For this reason A would never opt out. In the worst case A would prefer for agent W to opt out in the next period. rather 5.2. Agreement is guaranteed at the latest in the second period Even though agent A prefers to continue the negotiation indefinitely, will be reached W can threaten negotiation process time t over W’s opting out in the next period (after a finite number of periods). The reason to opt out at any given toward an agreement. time. This If there is some agreement threat t + 1, then it may agree to s. is the driving an agreement for this is that agent force of the s that A prefers at So the main factor that plays a role in reaching an agreement is still preferable is the worst agreement to W than opting out in time this agreement by @’ E S. If agent A will not for agent W in a given period period agree to such an agreement, t. As in Section 4 we will denote t which its opponent has no other choice but to opt out. Agent A’s loss from opting out is greater than that of W. This is because A’s session in the middle. Thus, we must modify assumption (of using is interrupted A5 to meet the current circumstances. the resource) A55 (Range for agreement). For every UW( (Opt, t)) > P( 1)) and UA((?w’+‘,t+ 1)) > UA((iKr,t)). (Ff+l t E 7, Uw ( ( jK:‘, t) ) > Uw( ( 3K:r+1, t + 1) >, ,t+l)),andifs^T’>OthenUA((ZW,‘,t)) >UA((Opt,t+ If there are some agreements that agent W prefers over opting out, then agent A also prefers at least one of those agreements over W’s opting out even in the next period. We assume and A84 are not needed that assumption A64 is still valid and denote in the current situation. it by A65. Assumptions A74 We consider than it can gain per period while using loses more while waiting this second agent, sharing a resource with others is not efficient. Therefore, two cases. In the first case an agent loses less per period while waiting an agent In the second situation, than it can gain while using the resource. For to for the resource the resource. it prefers 322 s. Kraus et al./Art@cial Intelligence 75 (1995) 297-345 have its own private resource any choice, but to share a resource if possible. However, in some cases the agents don’t have (like a road junction or another expensive resource). We first consider if it is big enough, that will be better for both sides, the case where W loses less over time than A can gain over time. in In such a case for any offer, total gain. the future Although in reaching an agreement, we will prove that in fact the delay will be at most one period since W may opt out. However, since better agreements in the future, the agreement it might appear that such an assumption will cause long delays for both parties can be found i.e, both agents have positive is not Pareto-optimal to find an offer that is reached it is possible over time. this proof is as follows. ?zt and S$’ - 1). So, in time period t, it can always opt out and gain utility similar If it is not agent W’s turn to make an The intuition behind to that of j.wt offer in some time period t - 1, W will never make a (actually, between better offer to A than $7’ + jcw[, which is its benefit from opting out in the next period with the addition of W’s loss over time (note that cw < 0). But A will refuse such an offer, since A prefers waiting a period and offering W twr E S. This offer will prevent W from opting out, and if W accepts is better to A than 37’ + \cw/ since the offer, A’s share will be $7’ + CA which /cw/ < CA. of an agreement So, an agreement won’t be achieved when it is W’s turn to make an offer and there is in the next period. On the other hand if A offers W still the possibility less preferred by W than j.w,t, W will opt out since it will never receive in any something given time period in the future t’ anything more than Z’@, and W prefers opting out over it. So, in order to prevent W from opting out A should offer it ?w;r which is acceptable to W. So, if W is the first agent assumption to make an offer since A is using agreement will be reached will be reached the in the second period with iw;‘. If A is the first one, agreement the resource and does not have a motive in the first period with iw,‘. to start the negotiations), is a reasonable (this The second case considers in the future than agent A’s gains. In this model, other agreement other hand in a period earlier our assumptions, period. if an agreement than the situation where agent W’s losses over time are greater there is no that both agents will prefer over this agreement. On the t is small enough, one can find an agreement for any agreement in period s in period t E 7, t which both agents prefer over s in period the agents to reach an agreement this property will cause t. According to in the first In each period in this case, if an agreement exists which agent W prefers over opting out there exists such an agreement which agent A cannot reject. The idea is the following. Agent W will accept or make an offer only receives an offer such that there is no better agreement better for W than opting out, and if A prefers period, out as soon as possible, A prefers accept the offer. if it is better for it than opting out. If A for it in the future, and it is also this offer over W opting out in the next is rejected, W should opt it cannot expect to do any better than opting out. But if it should the proposed agreement over W’s opting out in the next time period, this offer. Otherwise, if this agreement it must accept since Such an agreement, i.e., the type which will be reached there at most is still a possibility an agreement (from W’s point of view) 2%‘. The reason for reaching in some period t E ‘T where in the next time period, will be for that is that if there is still a S. Kraus et al./Art$cial Intelligence 75 (1995) 297-345 323 in reaching to delay t + 1, A wants for an agreement an agreement. By possibility offering 9w,t A prevents W from opting out, and gains another period of time. On the less to it than slwt+‘, since A can always other hand, A won’t accept anything worth the next period, gain a period, and reach such an agreement. Therefore, wait until in a time period T + 1, A won’t accept anything worth less than ?Tr+’ + CA + 1. But given to W in the future. On the this agreement other hand, since W loses over time more than A can gain, this agreement is also better to W over anything The next theorem it can get in the future. is a formal statement of the above. is better to A than anything that is acceptable Theorem 15 (Agreement will be reached a PE of a model satisfying A&-A&. in the first or second period). Let (f?g^) be Suppose agent W is the first to make an offer. l W loses less than A can gain. If Icw( < CA, then Outcome(~,~) = (@‘,q’>, l W loses more than A can gain. Zf 1~~1 > CA, then Outcome( f? g) = ((q’ 1). $ 1 -t C,,$ - 1 -cA),o) If A is the first agent to make an offer then Outcome( x g) = ((q”, co), 0). is better techniques is still possible to the proof of Theorem 13. The first claim Proof. The proof uses similar above. For the second part, we will show that in any given is clear from the intuition time period T, where agreement in the to A than ?Fr+’ + CA + 1 which is also better to W than opting future which out in T + 1. It is clear for time period T + 1. Suppose there is an s in time T + t such that sA + tcA > slFT+’ + CA + 1. That is, SW + CA + 1 < agreement izr+’ + tc,J, i.e., sw < izr+’ + CA (t - 1) - 1. However, /CW[ > CA and therefore, S’V < SW +r+’ + Icwl(t - 1) - 1 and SW - tlcwj < $?+’ - lcwl - 1 and we may conclude that Uw( (s, t)) < Uw( (ZT’+’ + l,@r+’ - in T + 1, there is no agreement t > 1 and suppose 1) 5 UW((Opt,T+ l),T+ 1)). 0 labs launched separate mobile Example 16. The US and Germany have embarked on a joint scientific mission involving Each country has contracts with a number of companies These experiments were preprogrammed to launch the mission. to begin each experiment must be sent from Earth. to Mars the planet. for the conduct of experiments. prior to launch. Arrangements were made prior and excess weight on from a single shuttle in orbit around for the sharing of some equipment to avoid duplication Instructions The US antenna was damaged during landing, and it is expected for repairs that communications for one day its lab on Mars will be down the US and of the planned between minutes) less reliable backup experiments, would conduct time, and that line will be in use for the entire duration of the particular experiment. (1440 five-day duration of the mission. The US can use a weaker and this line from other costly space is very high to the US. The US the one-day period so that it can research program. Only one research group can use the line at a like to share use of the German line, but this involves diverting and thus the expense of using line during its planned this line A negotiation ensues between line, during which time the Germans have sole access to the line, and the US cannot conduct any of its experiments (except by use of the very expensive backup). By prearrangement, the two labs over division of use of the German 324 S. Kraus et al./Art$ciai Intelligence 75 (1995) 297-345 for their experiments, the Germans are using some of the US equipment $5000 per minute. While some US equipment, equipment. The US is losing $3000 per minute during rely on their backup communications to share the communications each group. and are gaining the Germans cannot conduct any of their experiments without some of its experiments without German they must the US and Germany for in a $1000 gain per period line. An agreement between the US could conduct line will result the period in which (minute) the line is not reached, the US can threaten and no German equipment, If an agreement on sharing by using all of its equipment and by using line. The overall US gain will be $550,000, but it will of the arrangement. experiments backup communications $1000 per any minute of the negotiation. able to continue restricted opt out, they will need to pay the US $100,000 point. Note that the Germans play the role of A (attached the role of W (waiting and the US plays currency to opt out In this case, the US will be able to conduct a small portion of its the lose If the US opts out, the Germans will not be and their gain will be the US opted out. If the Germans for use of the US equipment up to that line) is the smallest unit of they had gained at the point for the line). A dollar to the communication the US equipment) their experiments in this example. to whatever (without Formally, ug((&t>> V( (Opt,, V((Opts,t)) = lOOOs, + 5000t, t) ) = 5000t, =5000t- 1OOOOO; UU((&O) rrU( (Opt,, V((Opt,,t>> = lOOOs, - 3000t, t)) = 550000 - = -1OOOt; lOOOt, M = 1440. The Germans prefer any agreement over opting out. SZ,’ = 551 + 2t. An agreement will be reached It should be noted that there are agreements in the second period (period 1) with (887,553). in the future that both agents prefer over the Germans (879,561) (887,553) the agreement in the second period. This is because time period time period. The problem reaching gain more over time than the US loses over time. For example, in the fourth second when the fourth time period arrives, the German will offer them (879,561) the Germans need to offer only (885,555) and they don’t have any motivation in the is that there is no way that the US can be sure that . In that time the US from opting out, (period 3) is better for both agents in order to prevent to offer more. the agreement (887,553) than S. Kraus et al./Artijkial Intelligence 75 (1995) 297-345 325 6. Agents with incomplete information to one agent are not known Up to this point we have assumed that the agents have full information The incompleteness the environment. factors. For example, other. But in most cases, the agents do not have complete and about different an agent may not be able to explore about the resources the others; or one agent is not familiar with its opponent’s utility function. information an agent may hide the environment that are available situations when the agents have incomplete the environment; information about each about each other of information may be the result of the other agents; information and may be missing its actions from obtained information in one encounter We will consider functions. The situation of incomplete about each becomes even more the same two agents. The in a subsequent one. So, we will that there is a set of agents whose members negotiate with each other, from time that in a given period of time of a Bilateral Negotiation two in a negotiation process. of Section 2 (No Long-Term itself to any future activity the other’s utility interesting when we can expect recurring encounters between agents can use information assume to time, on sharing a resource. However, we still assume no more than two agents need the same resource of Section 2) agents need the same resource, Also Commitments) is still valid. That is, an agent cannot commit other than the agreed-upon scope of this paper. schedule. The relaxation of this assumption these agents will be involved is an overlap between (5) of the beginning the time segments the simplifying is not within (assumption assumption in which . When there to We note that a given agent may play different roles in different negotiation it uses the resource while another agent is also trying interactions. to use it (i.e., it plays an agent may need the resource while another agent Sometimes, the role of A). In other situations, is currently using We assume it (i.e., it plays the role of IV). in the environment of resource usage among that there is a finite set of types of agents tasks that the agents are executing, or different configurations. servers (Type = function which depends on its resource the agents can be due to For example, line than an that has smaller disk space will use the resource more frequently that has larger disk space. The first agent is a heavier user of the resource, while { 1,. . . , It}), and each has a different utility usage. The different distributions different if all the agents are communications then an agent agent the second When and when also assume type. I6 We denote by c#$, where i E {A, W}, j E Type, i’s probability to the resource) we denote some probability belief concerning it by Wj, it by Aj. We its opponent’s of its opponent that share a common communication is the lighter user of the resource. for the resource) we denote the role of A (attached that each agent maintains the role of W (waiting j E Type plays it plays being of type j. We assume changes over time. We denote by A the set of all possible configurations A = {w,, w2,. . . , Wk,Al,. that Vi E {A, W}, J$, $$. = 1. This probability . . ,fik}. of agents, belief i.e., I6 In recent work into the strategic model of negotiation. information. [ 281 we have developed a logic of probabalistic It was shown to be useful belief and time, which was integrated incomplete in cases of more complicated 326 S. Kraus et al./Art$ciul Intelligence 75 (1995) 297-345 6.1. Sequential equilibrium In Sections 3, 4 and 5 we have analyzed That is, we required the situation using the notion of “( subgame- for any that each agent’s action be optimal )perfect equilibrium”. “subgame”, not just at the start of the negotiation. When there is incomplete there is no proper subgame. In the incomplete about sequential equilibrium instead [ 291. A sequential equilibrium to a profile of strategies (as in PE) , a system of beliefs. information situation we will be talking in addition information includes, ,..., Ak,Wl,.. That is, a sequential equilibrium is a sequence of 2k strategies (one for each possible . , Wk) and a system of belief with the following properties: step r the strategy (at step t) and its opponents possible its opponent’s is consistent with the history of the negotiation. That is, the agents’ belief may that each agent in a type. agentAi,Az each agent has a belief about its opponent’s for agent strategies type) change over time, but only consistent with the history. We assume negotiation i is optimal given in the SE. At each negotiation interaction has an initial probability belief about its opponent’s step t each agent’s belief (about type. At each negotiation its current belief We will define these notions formally. Definition through 17 (History). For any step t E 7 of the negotiation let h(t) be the history time step t. h(t) is a sequence of t proposals and responses. A strategy for each agent as defined above specifies an action for every possible history it has to move. A sequence of 2k strategies, one for each possible agent to a probability the point of view of the agents, , wk, leads, from ,,... after which A ,,..., &,W distribution opponent by the strategy equilibrium that with probability is specified expected utility In order tory, we must specify equilibrium sequential the beliefs of the agents. over outcomes. For example, is of type 1 then A expects that with probability that is specified to WI. If A believes to A and the strategy that W’s type is k with probability if agent A believes with probability #;’ that its c#$ the outcome is determined in the sequential c#;, then it assumes that for type k and its own strategy. The agents use that is specified c#$ the outcome will be the result of W’s usage of the strategy in the sequential equilibrium to compare among to state the requirement these outcomes. its beliefs about requires us to specify that an agent’s strategy be optimal type. Therefore the other agent’s for every his- the notion of the profile of strategies and two elements: Definition probability 18 (System of beliefs). A system of beliefs distribution of i’s opponents is a function pi(h) which is a as a function of the history. That is, pi(h) = to type according . , c#&} describes agent i E {A, W} belief about its opponent’s {#&.. a given history of offers and counteroffers h. For example, suppose there are three types of agents in the environment, that before the negotiation starts A believes that with probability 1 its opponent and suppose is of type 1, with probability is of type 3. That is, PA (0) = ( i, $, a). Now suppose A receives an offer s from its opponent W. i it is of type 2 and with probability $ its opponent S. Kraus et al./Artijcial Intelligence 75 (1995) 297-345 321 A may now change of type 3, but rather there is probability its beliefs. For example, it may conclude that its opponent can’t be 3 that it is of type 1 and 5 that it is of type 2. That is, PA(S) = (5, i,O). We impose three conditions on the sequence of strategies and the agent’s system of beliefs: l Sequential Rationality. The optimality of agent depends on the strategies of WI, . . . , Wk and on its beliefs pi(h). That is, agent its expected utility, with regard to the strategies of its opponents tries to maximize to the given and its beliefs about the probabilities history. It does not take into consideration of its opponent’s type according in the future. interactions possible i’s strategy after any history h i l Consistency. Agent i’s belief pi(h) should be consistent with its initial belief pi( 0) and with the possible strategies of its opponent. An agent must, whenever possible, use Bayes’ rule to update If, after any history, its beliefs. then the strategies of the agent’s opponent, call for it to reject an offer and make the same counteroffer, is indeed made, offer. If only one of the strategies of the opponent, that the offer made by the agent may be rejected and the counteroffer and the counteroffer opponent’s the same as before the agent’s beliefs s is indeed made, then it believes with probability type is indeed for example, remains j. the it made type j, specifies s be made, 1 that its regardless of its type, and this counteroffer there are three types of agents that the strategies of Wi, W2 and W3 specify We return environment. suppose them will make an offer s, then A’s beliefs can’t be changed offer S. However, the offer s’ then if A receives an offer s’ it believes That is, PA(S) = (O,O, 1). in the to the above example, where Suppose A’s original belief was PA (8) = (3, $, i) as above. And all of the if it indeed receives the offer s, but Ws specifies is of type 3. if the strategies Wt and W2 specify that in the beginning that its opponent l Never dissuaded once convinced. Once an agent opponent with probability type, i.e., the probability condition the conclusion subsequently perfect information 1, or convinced of this type is 0, it is never dissuaded is convinced of the type of its that its opponent can’t be of a specific from its view. The that in the above example, once agent A reaches its belief, even if agent W is Ws it cannot in a from Ws’s strategy. From negotiation with agent W3. I7 revise this point on A is engaged for example, that its opponent deviates implies, Definition 19. Sequential beliefs pi(h) and Never Dissuaded Once Convinced. equilibrium , i E Type that satisfy the conditions of Sequential Rationality, Consistency is a sequence of 2k strategies and a system of 6.2. Attributes of the utility functions We will now change our assumptions to fit the case of incomplete information. Assumptions AOs-A65 are still valid. We denote them by A@-A66. We only add some I7 See [ 361 for a discussion of this assumption. We leave the relaxation of this assumption for future work. 328 S. Kraus et al./Art@ial Intelligence 75 (1995) 297-345 additional requirements to A45. As in A45 we assume that each agent has utility with a constant cost of time ci. We will concentrate on the cases where agent Ai, i E Type, gains over time (CA, > 0) and agent Wi loses over time ( CW, < 0). Agent W prefers any given portion of the resource later sooner information. That is, agent rather than sooner. The exact values, CA, and CW, are private Ai knows it knows that it is one of k values. than later, while agent A prefers any given portion of the resource its private gain CA,, but may not know CW,, although rather We return gaining over time, while over time. The set of agents of this section). to the situation of two agents, one A is attached the other W is waiting to access is A = {WI, W2,. . . , Wk, Al,. to the resource, and is the resource, and is losing . . , Ak} (see the beginning We will consider the situation where it is common belief that 1~~~1 < (cw~_, 1 < ’ .’ < ICAal < for the resource. Agent Ak also gains (CA, I. That is, agent Wk loses less than A, while using lose less while waiting than they can gain while using that for any time period to opt out (compared with reaching an agreement) that satisfy conditions AOh-A66, t, s^F%’ < sA -wk-I ,f there exists a sequence of strategies equilibrium. 2o . . . < less than agent WI loses while the resource. the resource. ‘* We I9 That is, Wk is than WI. We show that that < . . . < iy”. lcW,I < waiting Both agents also assume more willing in situations are in sequential 6.3. Negotiation ends in the second period If the above assumptions hold and the agents use sequential then the negotiation will end in the second period. The agents will reach an agreement this period with high probability. The exact probability depend on agent A’s initial belief. As A’s belief about its opponent’s adequate, that its opponent will opt out decreases. strategies, in and the details of the agreement type becomes more the probability equilibrium The probability that an agreement will be reached depends also on A’s type. As the from agreement and its utility from opting out decreases, difference between A’s utility the probability that W will opt out decreases. We will show that all agents that play the role of W (regardless of their type) will and behave as the strongest agent Wk in the first period. try to deceive Agent A will in the second period, based on its initial belief and its type. In most of the cases this offer will be accepted. In the rest of the paper, we will describe their opponents, ignore their offer, and will make its counteroffer these results. to have its own private resource there are situations where an agent loses more while waiting lx As we explained above, than it can gain while using the resource. For this agent sharing a resource with others is not efficient. Therefore, in some cases the agents have no choice but it prefers if possible. However, resource). Our approach to share a resource in some of these situations but common belief about the agents’ beliefs is needed. t9 As we defined above, for i, j E Type, cw, denotes agent Wi’s loss over time and CA, denotes agent Aj’s gain over time. Bwt,’ is the worst agreement is still better than opting out. The subscript for agent Wi which A, i.e., iy3’ (like a road junction or another expensive indicates A’s portion of the resource is also applicable in the agreement for the resource s^wc,‘. + Icw,_, 1 < ” < 3y + IcwlI. *” It is enough to assume that sA -wkJ + lCWkl < q-r3 S. Kraus et al./Artijicial Intelligence 75 (1995) 297-345 329 We first define another notion that captures the belief of an agent about how strong its opponent is. Definition 20 (The strongest an opponent can be believed to be). Let pnl (h) be the system of beliefs of agent m after history h. Let n’ be the maximal n E Type such that q@ # 0. n’ is the strongest agent that m believes its opponent may be. In the next lemma we will show the exact agreements each of the agents makes or accepts in a given time period t. Lemma 21( 1) indicates that Wi won’t offer A anything better than its possible utility in the next period, with the addition of Wt’s loss over time. It can time period and opt out. That is, Wi won’t offer A anything better from opting out always wait another than its offer in the situation of full information. the strongest In Lemma 21(2) we will show that A will behave than it will that W can’t than Wi, it won’t offer it more than it will offer to Wj when there is full and W is indeed Wj. toward W no better is, if A believes type A believes W may be. That toward be stronger information Lemma 21(3) indicates than sewed’), it will really opt out. We will show that if Wi rejects receive any future offers better than this one. But it prefers opting out over reaching agreement. On the other hand, can get from opting out, it should accept than that (Lemma 21(4)). that if Wi gets an offer worth less to it than opting out (less this offer it won’t this at least as good as it it. Wi won’t be offered any agreement better if Wi does receive an agreement that are rejected by the that are accepted and agreements Lemma 21 (Agreements agents). Suppose agent W is of type i E Type and A is of type j E Type, and the agents’ utility fUnCtiOnS nA E Type be the strongest tT’+’ type A believes its opponent can be (as defined in Definition 20). If both W and A use their sequential equilibrium strategies then the following holds: A16-A66, < C& Sati& Let 2:’ - (1) The best offer to A that may be made by Wi: Wi will not ofleer A in step t more than s”T”+’ + (cw,l. (2) The best agreement to agent W that may be made or accepted by Aj: Aj will not accept anything less than sA than ?wC1~ 9’. ..W,ga .r+ 1 + cAj in step t and won’t offer anything more (3) When Wi will opt out: Ifin step t the offer that Aj makes to Wt is less than Sl$“, then Wt opts out in step t. (4) The offers that will be accepted by Wi: Zf in step t, Aj makes an ofSeer s such that SW 2 Sl$“, Wi accepts the offer. Proof. it can always wait until ( 1) It is clear that any type of W won’t offer A more than $y,‘+’ + Icw;/ since (2) When the next time period, opt out, and achieve a better outcome. it is A’s turn to make an offer in a given time period t, and it believes that type W may be, then it will never offer anything better for W than nA is the strongest QW,z~.r. This offer will prevent any type of W that A believes W can be, from opting out. 330 S. Kraus et al./Arr#cial Intelligence 7.5 (1995) 297-345 This is the main goal of A; if W rejects time period of using the resource. its offer but doesn’t opt out, A earns another less than ?TAVf+’ + ICA~ 1 amount of the resource in time period this offer; time period, offer W $w~~J+’ it gains over time CA,, its utility will be at least as large as the utility of it can always wait another If A is offered anything reject t it should and since “W,ZA Jfl SA + lcAjl at time t. (3) This will be proved by induction on the number of types ( IType]). We note that UwJ ( (Opt, t) ) > Uwi ( ( Zwl*‘+‘, t + 1) > . This is because we deal with a discrete case and by assumption A&. Base case (only two types) k = 2: If there are only two types, it is clear by Lemma time period that in any future t’ A won’t offer W more than 9Wz,“. However, W2 21(2) t’. The prefers opting out now over the possibility of getting s^w2~” in future time periods only way for A to prevent W2 from opting out now is by offering at least s^w2g’, which is the worst agreement to WZ that is still better than opting out. That is, if W2 is offered anything less than Zw2,’ at time t it will opt out. Suppose A offers since $7,’ < Q’. to A that its opponent better to W than s”wI*T’, but WI prefers opting out now over S^W1*r’ less than 3wl*‘. It is clear that in such a situation W2 will opt out such an offer, it will be clear is WI. So, by (2) A won’t offer to W in the future (t’) anything So, if W doesn’t opt out after receiving in time t’. k = k’ + 1: Suppose the assumption added. If A offers something than i opt out, by the induction hypothesis. Therefore, that its opponent Zwl,t’ (by (2)). (4) Similar is at most to (3). 0 is correct for IType = k’, and a new type i is less than s^w;v’, all the other types of W that are stronger if Wi won’t opt out, A will know than in the future i, and won’t offer anything better (t’) that play it is clear type when Based on this lemma we prove that all agents of their type, will behave as the strongest From Lemma 21(l), Zy*t+l + 1~~~1. If so, if there is an agent that offers more, A can conclude weaker than i. But in such a case, A is better off waiting and offering W an agreement on a scale that it might make if its opponent was weak. So, it isn’t worth it for an agent to reveal that it is weak. It can only lose from that. Therefore, all the in the first period types of agent W behave as the strongest one. the role of W, regardless it is their turn to make an offer. to A than that its type is that agent Wi won’t offer anything more When it is A’s turn to make an offer, it should try to maximize to calculate it needs will be the highest. This agreement of type i or weaker that fits Wi ( s^w-‘), and its opponent is stronger than i it will accept the offer. for which agreement, is, by taking according into account to its beliefs, that if it offers its expected utility. So, its expected utility an its opponent than i, it will opt out. If it is Lemma 22 ( W will pretend utility). agents’ utility functions sequential equilibrium to its expected Suppose, agent W is of type i E Type and A is of type j E Type, and the satisfy AOe-A66, $7’ - Slytf’ < c&. If both W and A use their to be strong and A will behave according strategies then the following properties hold: S. Kraus et al./Art@cial Intelligence 75 (1995) 297-345 331 (i) An agent of type i E Type will accept any offer greater or equal to $*‘. (ii) All agents of type W, regardless of their types, will offer in any time period t SIT”+’ + Icwk 1 amount of the resource. (iii) Suppose A has a probability belief of (c&‘, . ...&). where#+...+&‘= I.Let Expect(s^Wit’) =(~1+...+~i)UA((S”W,,r,t))+(~i+l+...+~k)UA((Upt,t)). Let 1 E Type such that Expect( s^yg’) is maximal over any i E Type. A will offer s^w;*t in time period t when it is its turn to make an offer. Proof. (i) It is clear by Lemma 21(4). (ii) We prove by induction on the number of types. Base case (k = IType/ = 2): By Lemma 21( 1) it is clear to A than ~2” + Jcgl which will be rejected by A according that W2 will not of- fer anything more to Lemma 21(2). So if WI will offer something better than SF*’ + Icwz(, A can conclude it will reject the offer, and in that its type is 1, i.e., nA = 1. But, by (2) of the lemma, the next period will offer WI nothing more to pretend than 3”“. Therefore, WI should prefer to be W2. Inductive case (k = [Type1 = k’ + 1) : Suppose types. From k’ number of types. And suppose another previous the role of W, will pretend know who it is, reject future. So, if i plays the role of W, it should pretend to be strong, the induction hypothesis therefore its offer, and won’t offer it anything better type i’ is added which to be strong. the induction hypothesis is correct for the than it is clear that all of them, if they play if WiT will behave differently, A will than Slwif,‘+’ in the is weaker (iii) By Lemma 21(3), that are stronger expected utility it is clear that if A will offer sw’*’ in period 1, all the agents its than i will opt out, while the others will accept it. So, A calculates from all its options, and chooses the best option for itself. 0 We will now state our final results for this section. in the second period, or W will opt Theorem 23 (Either an agreement will be reached out). Suppose agent W is of type i E Type and A is of type j E Type, and the agents’ utilityfunctions satisfy A&j-&j, $7’ -.?Tt+’ < c,&. Let 2 E Type such that Expect( s^w;,’ ) is maximal over any i E Type (where Expect is defined as in Lemma 22( iii) ). If both W and A use their sequential equilibrium strategies then in the first period (period 0) all types of agents playing the role of W will offer A ?Aw’ + lc~l amount of the resource. A will reject the offer. In period 1, A will offer s^w;*l. If W is at least of type 1 it will accept the offer. Otherwise, it will opt out. Proof. Clear by Lemma 22. 0 We would like to indicate each negotiation session. that Aj may revise its beliefs about its opponent’s If in the second period, W accepts an offer equal type after to s1:“, A that W is at most of type i. If A offers $’ concludes A concludes agent A can update is at most one agent this additional its beliefs about other agents. For example, in the system about W, that there that is of type 1, and it finds out that its opponent and W opts out of the negotiation information if A knows than i. Using that it is of type greater 332 S. Kraus et al./ArtiJicial Intelligence 75 (1995) 297-345 from the previous of other agents. The updated belief will be used in future interaction is of type 1, then it can adjust its beliefs about the types interactions. This is the only case, among the ones that we have studied, agents may opt out. However, as more interactions another is collected, and less opting out will occur in the future. occur, more information in which one of the about one to Mars. Suppose on Mars does not know to the example of the mission the exact details of the contracts the German’s hold is h, then their utility that each of Example 24. We return the other the labs (agents) for the contracts: high (h) and low (1). has with companies. There are two possibilities are similar If the type of contracts to those of Example 16. They gain $5000 per minute during and gain $1000 per minute when they share the line with the US. If the US also holds contracts 16. The of type h, then the negotiation period and gains $1000 per minute US loses $3000 per minute during when sharing If the US opts out the overall US gain will be $550,000, but they will also lose $1000 per minute during functions the negotiation the line with the Germans. to those of Example are also similar the negotiation. their utility functions if the German contracts are of type 1, then they only gain $4000 per minute However, while using if their contracts the line by themselves. The US losses while negotiating are type I are only $2000 per minute. But if the US opts out, their overall gain is only $450,000. They still negotiate in the next 24 hours (i.e., M = 1440) from the time the negotiation for the usage of the German ends. line Formally, let s E S, t E 7, = 1000s,f50OOt, t) ) = 5000t, t) > = 5000t - 1000, @“((s,t)) lJRk ( (Opt,, U”” ( (Opt,, UUk ( (s, t) ) = lOOos, - 3000t, UUh ((Opt,, IP((Optg,t)) Pf = (889 - 2t, 551 + 2t), t)) = 550000 - = -lOOOt, lOOOt, = 1000s,+40OOt, t)) = 4ooot, P((s,t)) Ug’( (Opt,, CP (( Opt,, t)) = 40t - 1000, v ( (s, t) ) = lOOOs, - 2000t, t)) = 450000 - 1OOOt, U”‘( (Opt,, UU’ ( (Opt,, t) ) = - 10OOt, s ““J = (989 - t, 45 1 + t) . (playing that with probability that Germany is of type 1. We denote the role of A) is of type h and the US (playing them by gh and ~1. We will consider two is of type h and its opponent Let us assume the role of W) cases. Suppose gh believes with probability 0.5 its opponent to be of type h and will offer gh to Theorem 23, in the first period, UI will pretend ( V + Ic,,, 1, sp’ time the offer. In the second period, gh compares between offering W (887,553), which will be accepted by both (988,452) which will be accepted by type 1, but after such an offer, types, and offering if W is of type h, it will opt out. Since is higher, is of type 1, i.e., 4: = 0.5 and 4: = 0.5. According this offer which is accepted by ~1. Ic,,~() = (890,550). its expected utility gh will reject from offering (887,553) it makes 0.5 - However, suppose g/, believes only with probability 0.1 its opponent is of type 1. The behavior of ~1 in the first period 0.9 its opponent to the previous case. It pretends from (988,452), with probability similar expected utility offer to W, which is accepted by ul. is higher to be h. However, than (887,553) is of type h, and is in the second period, gh’s it makes this and therefore S. Kraus et d/Artificial intelligence 75 (1995) 297-345 333 7. Negotiation mechanism for multiple fully informed agents Up to this point we have assumed that only two agents participate in each interaction in Section 2). We will now relax this assumption (assumption by extending the agents have full information. ( 1 ), Bilateral Negotiation, the framework of Section 4. We note that we assume in this section that We assume that a set of agents wants to satisfy a goal. All agents can take part the goal, but they all need to agree on the schedule. There are no side- in satisfying the agents. An additional option payments, which we do not deal with in this paper involves one of the agents opting out, and the remaining an agreement. several agents we mean more than two. In the rest of the section when we mention i.e., no private deals can be reached among reaching agents (more than two), is done by phone to the example of the newsletter deliverers. Here Example 25. We return several electronic newsletters service agents. The delivery mail). The expenses of the agents depend only on the number of phone calls. There are to all the newsletters. All the delivery agents negotiate several subscribers over the distribution subscriptions. Each of the agents can opt out of the negotiations and deliver all of its own newsletters by itself. The agents are paid according there are that are delivered by separate delivery (either by fax machines or electronic to the time of the delivery (the faster the better). of the common that subscribe As in previous sections, we denote the set of agents by A. We denote the number of agents (i.e., (Al) by n and we attach to each agent an integer between 1 and IZ. Definition 26 (Agreement). An agreement st + . . . + s, = M. si is agent i’s portion of the work. 21 is a tuple (st , . . . , s,), where Si E N and to fit the multi-agent the negotiation procedure the agents can take actions only at certain We will now modify sections, interaction. As in previous times in 7. In each period t E 7 one of the agents, say i, proposes an agreement to all the other agents. Each of the agents either accepts it (chooses No), or opts out of the negotiation If the offer is accepted by all the agents, then is implemented. Also, opting out by one of the the negotiation agents ends the negotiations. After a rejection by at least one agent, another agent must make a counter offer, and so on. That is, this mechanism provides each of the agents that will be reached. However, we will show that with a veto power on the agreements agreement this extreme assumption, even under (chooses Opt). ends and the agreement the offer (chooses Yes) or rejects in the first time period. is guaranteed We require that the agents always make offers in the same order, i.e., if IAl = n the first agent makes offers in time periods 0, rr, 2n,. _ ., the second agent makes offers in periods 1, n + 1,. . ., etc. We do not make any assumption the to make an negotiations, according offer. So, without the first offer and who is the second agent loss of generality we assume that the agents are numbered about who begins i.e., who makes *’ A similar definition can be given concerning a division of resources. 334 S. Kraus et al./Art$cial Intelligence 75 (1995) 297-345 to the order second by 2, etc. in which they make the offers, i.e., the first agent is denoted by 1, the Definition 27 (Negotiation strategies). A strategy is a sequence of functions. The do- i and its main of the ith element of a strategy range is the set {Yes, No, Opt} U S. We first define a strategy f for an agent i which is the first agent to make an offer. is a sequence of agreements of length Let f = {f’}g, where fo E S, for t = kn, k E 7 f’ : S’ + S, and for other t E I, f’ : S’ x S --+ {Yes, No, Opt.} We denote by F the set of all strategies of the agent who starts the bargaining. the strategies for the other agents are defined. Similarly, 7.1. Attributes of the utility functions We will now modify the assumptions situation. We note that we are dealing with the task distribution of the previous sections case. to fit the multi-agent A17 (Actions are costly). For all t E 7, Y, s E S and i, j E d, i # j: ri > si + U’( (r, t)) < U’( (s, t)). For agreements the same time period each agent prefers to perform a smaller portion of the labor. that are reached within However, the utilities of the agents from agreements in which their parts are equivalent may be different. That is, ri = si f, U’( (ri, t) ) = U’( (si, t) ) . Other parameters, the quality of the performance of the other agents, may also play a role. such as We first consider the case that all agents lose over time. Assumption A2 of Sections 3 and 4 is still valid. That is: A27 (Time is valuable). For any tl,t2 E 7, s E S and i E A, if tl < t2, U’((s,tl)) Ui((s,t2)) and Ui((Opt,tl)) 2 Ui((Opt,t2)). 2 Assumption A3 of Section 3 and Section 4 is not valid. We also need to modify the definition of ?,r in order to be able to compare between agreements and opting out. Definition 28 (Agreements that are preferred over opting out). For every i E A we define ,?i’ = {s / s E S s.t. U”( (s, t)) > U’( (Opt, t))}. We denote by 5’ the t E ‘I- and set of agreements that are preferred by all agents over opting out, i.e., s’ = ni,, ji’ Suppose all agents are losing over time. We assume rather sooner than that all agents prefer to reach later, i.e., assumption A2 is still valid. We also to opt out sooner rather than later, that is assumption A4 that if an agent prefers an agreement over opting out in some t’ prior to t over opting out in time period the same agreement that all agents prefer a given agreement assume is valid. We also assume period in t’. Formally: t, then it prefers S. Kraus et al./Ar@cial Intelligence 75 (1995) 297-345 335 (Opting out costs over time). For tl, t2 E 7 and If U’((s,t)) > U’((Opt,t’)). > U’((Opt,t)), If U’((s,t)) > U’((Opt,r+ i E A, if tl < then for any t2 then t’ E 7 then I)), A47 U’((opt,tl)) such that t’ < t, U’((s,t’)> t’E7suchthat forany > Ui((Opt,t2)). t’<t, U’((s,t’)) >U’((Opt,t’+l)). It is clear that under We will concentrate on the cases where there is an agreement this assumption, for any t E 7, niEA sit+’ C niEA &. is acceptable to all agents and that there is a time period where in the first time period is no there that all agents prefer over opting out. This is stated in the next assumption. which agreement A57 (Possible agreements). n,, denote the minimal time period among these time periods by p. jio # 0. There exists T E 7 such that ST = 8. We 7.2. Agreement is guaranteed with no delay two agents in the environment We are able to show that the results of Section 4 are also valid when there are more and when all agents have veto power. As in the strategies, agreement will be reached in this agreement toward opting out versus that will be reached, but than bilateral case, if the agents use perfect equilibrium force of the agent without any delay. The main driving case is the cost of the negotiation time. The agents’ attitudes agreements will only affect the details of the actual agreement won’t drive any of the agents to opt out. reaching We will first show that in such a case if the game has not ended in prior periods, then in the period prior to that in which there is no agreement an agreement will be reached acceptable to all agents, i.e., in period ? - 1. (N-agent version) ) . Suppose the model satisfies A&-A27 and A+A57. Lemma 29 (Agreement will be reached prior to the time period when agreement longer possible If the agents are using their perfect equilibrium strategies, and the negotiation process is not over until time p - 1 and it is agent i’s turn To make an ofeel; then it will offer s’ = maxLit ST-’ and all the other agents will accept the offer. 22 is no In period p and later, there is no agreement to all the agents. Proof. is either opting out or Therefore, disagreement. and the agents prefer opting out sooner rather than later, at least one of the agents will opt out at time period the only possible outcome is the worst outcome Since disagreement that time period that is acceptable (A&) after ‘* For simplification, we will assume that only one such maximal agreement exists. This will be the case either if the only factor determining or if the quality of the performance this uniqueness assumption, is difficult for the other agents utilities agreements with equal probability, the utility for an agent is its own portion of the task (see Example 34). of the other agents yields a different utility for each agreement. Without if there are several agreements which have the same maximal utility for agent i, it to predict which offer agent i will make. These agreements may have different that in such situations, agent i chooses any one of the maximal and that the other agents behave according to the other agents. We can assume to their expected utilities. 336 S. Kraus et af./Art@cial Intelligence 75 (I995) 297-345 If. But all the agents prefer an agreement Since it is i’s turn, it can choose all the agents will accept (cid:144)i it. the best agreement from ST-’ over opting out in the next period. from its point of view, offer it, and Now, we will show possible agreements, offer, should choose make this offer. time period that in each acceptable the best of these agreements is always a set of to all the agents. The agent whose turn it is to make an and less than ? there to its utility according function agreements by induction We first define the sets of acceptable this set the best agreement of ?. In the period before p, this set contains only s^. In the prior period ? - 2 it includes all the agreements that are preferred From In computing chosen. is used as the basis, similar that are better to the agents than this value and also better than opting out. Similarly prior periods. in this time period by all agents over opting out and over 3 in p - 1. it is to make an offer is set in the prior period f - 3 this value in ? - 3 are those for i.e. the acceptable agreements the acceptable agreements for the agent whose to s^ before, turn Definition 30 (Acceptable agreements). Let x’-’ Lemma 29). For each t E 7, t < rf‘ - 1, let X’ include all the agreements followingcondition:sEX’iffsES’andforanyjEd,Uj((s,t)) If it is i’s turn to make an offer in time period = s^ (where t, we define x’ = maxD X’. s^ is as defined in that satisfy the >Uj((x’+‘,t+l)). This definition will prove to X’, since belongs opting out, at a given periods. this in the next lemma. The intuition behind is sound, since X’ is not empty for any time period before f - 1. We is that CC’+’ always is preferred over time it is also preferred over opting out in previous lose over time, and if an agreement the agents time period, the proof Lemma 31 (Acceptable A27 and A47-A57 agreements do exists). then for t E 7, t < f - 1, X’ # 8. If the model satisjies conditions A07- Proof. We will show by backward induction on t that for all t < p - 1, X’ # 8. Basecuse(t=?-2):ByA27ViEd, A47 it is clear that U’((s^,f-2)) U’((?,?-2))>U’((?,?--1)) andby > U’((Opt,f-2)), and therefore $ E XT-*. Inductive cuse (t < p - 2): By the induction hypothesis, X’+’ # 8. Therefore, x”’ to the base case, it is easy to see that is well defined. But, by A27 and A47, similarly Xffl E X’. cl We will show now that in any time period all the agents will accept ~8, and the agent this is that x’ that can whose turn it is to make an offer will also offer x’. The intuition behind is preferred by the agents over opting out, and it is better than any agreement be reached in the future. S. Kraus et al./Art@cial Intelligence 75 (1995) 297-345 337 Lemma 32 (x1 is offered and accepted). If the model satisfies conditions AOl-A27 and A+A57 t < f - 1, the agents will accept any offer s E X’ and the agent whose turn it is to make an offer will offer xt. then in any time period Proof. The proof is by backward induction of t. Base case (t = f - 2): By Lemma-29, in the future better agreement tion 30), any agreement and than opting out at p - 2. Therefore, On the other hand, any agreement by at least one of the agents, since or even opt out. But since i, similar over this possibility, turn to make an offer, it has the opportunity view. it should offer an agreement than x r-l. However, by the definition it is clear that the agents won’t reach any of X’ (Defini- in XPP2 is better for the agents than ,r-’ in time period ? - 1, they should accept these offers. that does not belong to Xfw2 won’t be accepted it will prefer to wait another period and receive 3 to the other agents, prefers the agreements of XTP2 from this set. However, since it is its the best one from its point of to choose Inductive case ( t < f - 2) : By the induction hypothesis, in this time period, the agreements of X’ at time period over opting out at t; the proof proceeds as in the base case. isn’t reached the outcome of the negotiation process will be (xl+’ , t + 1). But, t are preferred by all agents over ( .rt+‘, t + 1) and if an agreement 0 We summarize our results by the following theorem. Theorem 33. If the model satisfies conditions A01-A27 and A+A57, use their perfect equilibrium strategies, x0, and all other agents will accept then in the first the offer. ana’ the agents time period agent 1 will offer Proof. Clear, by Lemma 32. 0 discussed subscriber to a given arrangements the publisher of one edition. As was for DI and D2 are as previously (NJ, N2 and Ns) are delivered by separate delivery services of Nt pays D1 $200 per delivery of one edition, of N2 pays 02 $225 per delivery of one edition. The publisher to the example of the newsletter deliverers. Three electronic (Di, D2 and in ex- and of for D1 and D2, server) loses $1 for each time period. There are M subscribers Example 34. We return newsletters 03). The payment ample 14, i.e., the publisher Ns pays 03 $250 per delivery each delivery also costs Ds $1, and each with subscriptions there are substantial to the same subscribers. all newsletters joint deliveries to the M joint pay D3 only $215 per delivery of an edition, such an event pay D2 $200. They must still pay $1 per phone call $2 for any negotiation of phone calls service If an agreement is reached, 14 if one of the agents can deliver for of Ns will example, in of N2 will lose the number in its pay- the publisher the previous the publisher then and as in of Ni will pay D1 $170, and to the subscribers made by a delivery agent plays a role (i.e., Ni, N2 and Ns), and as in example among Dl, D2 and 03 to the server, and will time period. Notice (i.e., a phone call this example, only to this subscriber’s to all newsletters to a delivery the publisher subscribers the case savings that in 338 S. Kraus et al./Art@cial Intelligence 75 (1995) 297-345 Table 2 Summary of results. both agents over opting out. ci of column 7 is the constant cost of delay. ZiJ is the worst agreement i in a given period t which defined there is no agreement preferred by for agent to i than opting out in time period t. .KO of row 7 in column 8 is t of column 6 is the earliest in Definition 15. Expect is still preferable in Lemma 6 time period, is defined in which Type & Section Opting Number out of agents Full Who loses info ? - - C, ?Q Results c, > Q cl < c2 ((l>O),O) ((c2.1 - C2)TO) Yes Both Yes Both Yes Both even - ((p _ pc2+ (if- I)q, Yes Both odd - - - I (w) Yes Yes 1 (w) all Yes 1 (w) = No lcwl 5 CA lcwl 2 CA ;wi <CA *‘T--l s2 ((.$.‘_’ + $2 - (if - l)C’),O) - f(C 1)ca + +(f - l)C’, + ;(f 3Y-l (SK’, 1) - l)c2 - @- l)C’),O) - 1 -cA),o) + 1 + CA, 3:’ ” 1) where Expect( c?~;,~) ((Z’ (nO;‘O) (S ?3’, is maximal OR W opts out in period 1 the distribution of the rest of the subscribers between the other two Resource 3 No Resource 3 No Task 4 Yes Task 4 Yes Resource 5 Yes Resource 5 Yes Task 7 Yes Resource 6 Yes 2 2 2 2 2 2 N>2 2 ments and not agents. Formally, U’((Opt,t)) =200-M-t, =225-M-t, U2((Opt,t)) U3((0pt,t))=250-M-t, U’((s,t>)=170-s, U*((S,t))=200-s*-2t, U3((S,t))=215-ss-2t. -2t, t Z*,’ = 74 - t, and S^3,t - 64 - t. Note that for all ’ 2 - 3 Suppose M = 100. Then i E { 1,2,3}, Zi*’ is not unique ? = 36 and it is Ds’s turn ii,’ = 69 - Al.35 = 34) and D2 is willing period, D1 is willing (i.e., sr So, x35 = (34,39,27). It is easy an offer (t is divided by 3), XI= xt = (35,36,29) xr = (33,40,27). its opponents will accept its offer. in this case. to make an offer in the time period prior to deliver up to 34 newsletters, to deliver up to 39 newsletters to ?. In this if an agreement will be reached (i.e., s”i’35 = 39). that whenever to make when it is D2’s turn to make an offer, to time period 35), and it is DI’S turn and when it is Ds’s turn to make an offer (prior Therefore, in the first time period (0), DI will offer (31,38,31), to compute, (31,38,31), 8. Evaluation of the results We will analyze of the results appears in Table 7.2. the results of the paper using the criteria of Section 1.3. A summary l Distributed. In all the situations we analyzed there is no central unit that is involved in the inter-agent encounters. The agents negotiate to reach an agreement. l Instantaneously. Conflict are resolved without delay. In most of the situations we includes dealt with, an agreement will be reached in the first time period. This S. Kraus et al/Artificial Intelligence 75 (1995) 297-345 339 time, and the situations there are multiple the situations of Sections 3 and 4 where two agents negotiate and both are losing over that of Section 7 where negotiate and all of them are losing over time. However, also when one of the is gaining over time (Section 5), but the loser W loses more than the gainer agents in the first period. In the second (A) gains over time, agreement will be reached case of Section 5, when W loses less than A gains is the more common situation), in the second period. Similarly, even if there (Section 6) the negotiation will end at the second period. is incomplete the agents reach an agreement agreement will be reached information (which agents l EfJiciency. In almost all the cases that we analyzed, is avoided. Furthermore, in the cases of task distribution (Sections 4 and conflict and 6) and in the cases where process of two agents the first period, that are waiting there won’t be a deadlock. the resource is not in use during the negotiation (Sections 3)) agreement will be reached for the resource, since agreement will be reached in the first period. So, in the case in information (Section 6). Also The only situation where the negotiation process may end up with an agent opting in this situation, out, is in the case of incomplete in most of the cases the negotiation process will also end with an agreement, but it will end with W opting out. This depends on the accuracy of in some cases from opting out versus reaching an A’s beliefs about agreement. However, after each negotiation learn more about its belief. The beliefs of the agents about the types its opponent’s of other agents become more and more accurate with each interaction. Therefore the probability is decreasing over time and the efJiciency of the system that the resource will be damaged interaction, A may type and update and A’s utility is increasing. its opponent is, there the agreement is big enough, is no other agreement there are some agreements is not always Pareto-optimal. in the case of resource allocation In most of the cases we dealt with, that loses over time loses less than the agent that is reached is Pareto that is reached that all the agents prefer over the optimal. That (Sections 5 and one that they reached. However, that gains over 6) where the agent If A’s share in the time, the agreement agreement that both agents prefer over the one that they reached. However, since A gains over time, it prefers to delay reaching an agreement as long as possible. Therefore, even if it promises W to give it a better offer in the future, when the time comes, A to keep its promise. A would rather wait another accepts a lower offer and an agreement is reached though an agreement over time. In that time period, agents. for time period. Knowing that, W in the second period. So, even is that is reached without a big delay, even though one of the agents gains that is preferred by both the advantage of the solution there is no other agreement is not Pareto-optimal, it is not rational the agreement in the future l Simplicity. The strategies are simple and depend only on the current period. the next section we will explain how these strategies can be implemented automated negotiator. In in an l Symmetry. The negotiation mechanism is sensitive not to their other’s private negotiation identity strategy. and types. The agents’ to agent’s roles (A or W), but the type and identity only influence 340 S. Kraus et al. /Artijicial Intelligence 75 (1995) 297-345 l Stability. In all the situations that we considered we found subgame-perfect equi- librium or sequential equilibriums. l Satisjiability or accessibility. In all the cases of task distribution the task will be performed. When we consider there are some situations where an agent won’t get the resource first one is in the case of incomplete We assume getting access if the agent’s utility agreement due to the structure of the situation, and giving up the usage of the resource second agent’s preference. that we considered the resource allocation problem, immediately. The information, when one of the agents opts out. is repaired, both agents have some probability of in Section 3. Here, include a constant cost of delay, and ct < cz in the is the first agent will get all the resource. This is the to the resource. The second case was described that after the resource that will be reached, functions 9. Complexity and implementation We are in the process of implementing an automated negotiator that will participate in situations characterized by time constraints. negotiation results of this paper. We will report on the implementation but we discuss here some important related questions. It will be based on the theoretical in a different paper, issues We have suggested that autonomous agents will use equilibrium Such strategies are stable since no designer will benefit by building strategies for the an that the other agents are using that uses any other strategy when it is known negotiation. agent equilibrium strategies. approach to finding There are two approaches equilibria. One is the straight game set up a maximization is straightforward theory approach: search for Nash or perfect or sequential strategies. The other is the economist’s [ 431. The standard approach: functions of the agents are maximization some well known chosen correctly, techniques to situations the agents must such as ours, solve their optimization problem and vice versa. (e.g., technique jointly: A’s strategy affects W’s maximization can be solved using [ 551). However, when applied is less appropriate because problem and solve using calculus and if the utility of linear programming the maximization the maximization problems problem The drawback of the game theory approach is that finding equilibrium strategies is not mechanical: is in equilibrium it tests it. There is no general way to make the initial guess. an agent must somehow make a guess that some strategy combination before In situations of multistage negotiations the set of actions by trying state of the game. Working with this guess an agent can either construct a sequential equilibrium or show that none exists with this guess and go on and try another guess. If there is a point where it is clear that the negotiation will end it is often best to work through problems strategies can be found in each that are used with positive probability (or games in general) like this backward. to “guess” In our negotiation protocol agent can make (i.e., the number of possible agreements) of actions with which the other agents can respond. If we assume that there is some there are A4 A-1 /( IAl - 1) ! possible offers the offering and 31a] possible combinations S. Kraus et al./Art$cial Intelligence 75 (1995) 297-345 341 f after which no agreement can be reached the overall number of pure strategies if there is no such ? then the number of pure strategies is 0( (Mldl-t/( (e.g., W will prefer opting JAI - 1) !>r). (in theory) can be time period out to agreement), However, infinite. If we consider cases of incomplete the problem becomes even harder. In addition of the agents’ beliefs in each state of the negotiation. This can be done by stating a set of inequalities which are the constraints on the beliefs there should be a construction to guesses of the actions, in each state. information In general, it is too time consuming we suggest that equilibrium In this paper we presented appropriate by several environmental characterized and the agents’ utility functions. negotiation) to compute in real time. Therefore, the strategies strategies be identified before the negotiation process starts. for varied situations. The situations are (e.g., number of agents, purpose of the strategies factors If the automated agents act in a static environment, by the designer of the agent and inserted environment we are in the process of developing that consists of the strategies that are appropriate the automated negotiator may participate in. When it will choose the appropriate variables the frame-strategy the appropriate into its database. strategy can be If the agents a library for the different the automated negotiator for that is appropriate (e.g., ?wlV’, CA), and negotiate in a dynamic precomputed are active of frame-strategies situations acts in one of these situations its current situation, initialize according to this strategy. In general, the automated negotiator will be composed of three modules: the Meta- (see Fig. 1). Strategies Library, A meta-strategy the Identifier and the Controller is a frame strategy that includes strategies are determined by parameters of the situation. For example, situations that we have investigated in each of the negotiation equilibrium case in which depend on the constant delays and better for it than opting out in period 1 (slip’). We use these parameters meta-strategies. the agents can opt out and they have constant delays the worst agreement for an agent which that until now, the perfect in the the strategies is still the to construct (ci), several parameters. We note The Identifier will depend on the environment, special parameters of the environment in the library, convert meta-strategy it. parameters, and operate and its purpose the [ 481) . The Controller will find the correct its into a strategy by initiating is to identify the meta-strategy (see in any of the negotiation the Identifier will recognize So, in general, when the agent participates to the agent), and the Controller will construct have considered, will be given specific case and use it in the negotiations. perfect equilibrium the other agents cannot do better than to use their similar strategies. Since both the Meta- Strategies Library and the Controller the automated negotiator are domain can be used in a variety of different applications, that we (or they the exact strategy for the the agents with unique Since we provide it to the other agents in the environment, independent, simply by changing the parameters of the situation if we announce the Identifier. strategies, situations 342 S. Kruus et al./Arii&ial Intelligence 75 (1995) 297-345 Meta-Strategies Library Specification Specification 1: Cl <Q... ;, >c2_.k 2: lq? Fig. I. General structure of the automated negotiator. The Identifier is responsible for determining the specific arguments of the situation. The Controller should choose a strategy from the Meta-Strategies’ Library, accord- ing to the parameters. Then the Controller will operate according to the strategy and will send messages to other agents and receive messages from other agents. Messages 10. Conclusion in both resource allocation This paper has been concerned with how automated agents can be designed effectively model of negotiation avoiding costly and time consuming coordination. That is, we have provided a model in which agents can avoid spending much time negotiating for satisfying to interact environments. A strategic as a way of reaching mutual benefit while the overhead of too an agreement and therefore are better able to stick to a timetable interactions which might and task distribution has been proposed their goals. increase In the process of developing and specifying as well as incomplete single as well as multi-agent information, have examined complete payoffs of the participants. While some combinations delays, early periods of the negotiation. the model nevertheless situations environments, and the differing the strategic model of negotiation, we characterized by impact of time on the of these factors can result in minor in for reaching agreement reveals an important capacity Throughout the paper, we have referred artificial in distributed model to problems problem has been examined share a resource has been examined via a scenario and both parties benefit the evaluation of a negotiation through in order to achieve intelligence the development of a scenario their separate goals. The task distribution to two examples of application of the strategic (DAI) . The resource allocation in which agents must problem in which savings can result from the sharing of tasks, for protocol which we proposed at the outset of the paper: In both cases, we have met the criteria from cooperation. S. Kraus et al./Art$cial Intelligence 75 (1995) 297-345 343 symmetrical deadlocks able equilibrium completed). distribution in outcome), (no central unit or agent), efficiency simplicity (process simple and efficient), (access point), and satisfiability or accessibility (conflict avoided and no stability to the resource or task (distinguish- We have ended with a brief discussion of the general structure of an automated to be based on the theoretical results of this paper. The functioning negotiator, automated negotiator will depend upon whether it will be operating environment. environment assumptions trained. The implementation in which experimental work on the strategic model under varying can be undertaken, of a prototype automated negotiator will provide an initial can be in which human negotiators in a static or dynamic as well as one of the We believe that our model can be useful in other situations beside situations where there are several resources situations where the agents have incomplete the ones we an- in the informa- in the paper. For example, alyzed environment, tion. We leave this for future work. or task distribution References [ I 1 A.H. Bond and L. Gasser, An analysis of problems and research in DAI, in: A.H. Bond and L. Gasser, eds., Readings in Distributed Artificial Intelligence (Morgan Kaufmann, San Mateo, CA, 1988) 3-35. [ 2 1 S. Cammarata, D. McArthur and R. Steeb, Strategies of cooperation in distributed problem solving, in: Proceedings IJCAI-83, Karlsruhe, Germany ( 1983) 767-770. [ 3 1 N. Carver, 2. Cvetanovic and V.R. Lesser, Sophisticated cooperation in: Proceedings AAAI-91, Anaheim, CA (1991) 191-198. systems, in FA/C distributed problem solving (41 B. Chandrasekan, Natural and social system metaphors the issue, IEEE Trans. Syst.. Man Cybernet., 11 (1) (1981) l-5. for distributed problem solving: introduction to 15 I SE. Conry, K. Kuwabara, V.R. Lesser and R.A. Meyer, Multistage negotiation IEEE Trans. Syst., Man Cybernet. 21 (6) Intelligence. (1991) 1462-1477; Special for distributed satisfaction, Issue on Distributed Artificial 16 1 SE. Conry, R.A. Meyer and V.R. Lesser, Multistage negotiation in: A.H. Bond and L. Gasser, eds., Readings in Distributed Artt@ial Intelligence (Morgan Kaufmann, San Mateo, CA, 1988) 367-384. in distributed planning, 17 I K. Decker and V.R. Lesser, A one-shot dynamic coordination in: Proceedings AAAI-93, Washington, DC (1993) 210-216. representation, 18 1 J. Doyle, Reasoning, and rational algorithm for distributed sensor networks, self-government, in: Proceedings 4th International Symposium on Methodologies for Intelligent Systems ( 1989) 367-380. and its role in reasoning, I9 I J. Doyle, Rationality in: Proceedings AAAI-90, Boston, MA ( 1990) Invited Talk. [ 10 I D. Druckman, Negotiations (Sage Publications, [ 11 I E.H. Durfee, Coordination of Distributed Problem Solvers (Kluwer Academic Publishers, Boston, MA, 1977). 1988). [ 12 I E.H. Durfee and V. Lesser, Global plans IJCAI-87, Milan Italy (1987) 875-883. to coordinate distributed problem solvers, in: Proceedings I 13 I E. Ephrati and J. Rosenschein, The Clarke tax as a consensus mechanism among automated agents, in: Proceedings AAAI-91, Anaheim, CA (1991) 173-178. I 14 I R.E. Fikes, P. Hart and N.J. Nilsson, Learning and executing generalized robot plans, Art8 Intell. 3 (4) (1972) 251-288. [ 15 I K. Fischer and N. Kuhn, A DAI approach RR-93-25, Deutsches Forschungszentrum Wr Kunstliche to modeling the trasportation Intelligenz GmbH domain, Technical Report (1993). 344 S. Kraus et al./Artijicial intelligence 75 (1995) 297-345 [ 161 R. Fisher and W. Ury, Getting to Yes: Negotiating Agreement without Giving in (Houghton Mifflin, Boston, MA, 1981). I171 L. Gasser, Social knowledge and social action, in: Proceedings IJCAI-93, Chambery, France (1993) 75 l-757. [ 181 M. Genesereth, M. Ginsberg and J. Rosenschein, Cooperation without communication, in: Proceedings AAAI-86, Philadelphia, PA (1986) 51-57. I191 B. Grosz and C. Sidner, Plans for discourse, in: P Cohen, J. Morgan, and M. Pollack, eds., Intentions in Communication (Bradford Books/MIT Press, Cambridge, MA, 1990). I201 J.C. Harsanyi, Rational Behavior and Bargaining Equilibrium in Games and Social Situations (Cambridge University Press, Cambridge, UK, 1977). [21] C. Karrass, The Negotiating Game: How to Get What You Want (Thomas Crowell, New York, NY, 1970). [22 ] S.P. Ketchpel, Coalition formation among autonomous agents, in: Proceedings MAAMAW-93, Neuchltel, Switzerland ( 1993). 1231 W. Komfeld and C. Hewitt, The scientific community metaphor, IEEE Trans. Sysf., Man Cybernet. 11 (1) (1981) 24-33. [24] S. Kraus, E. Ephrati and D. Lehmann, Negotiation in a non-cooperative environment, J. Experimental Theoret. AI 3 (4) (1991) 255-282. [25] S. Kraus and D. Lehmann, Designing and building a negotiating automated agent, Comput. Intell. 11 (1) (1995) 132-171. [ 261 S. Kraus and J. Wilkenfeld, The function of time in cooperative negotiations, in: Proceedings AAAI-91, Anaheim, CA (1991) 179-184. [27] S. Kraus and J. Wilkenfeld, Negotiations over time in a multi agent environment: Preliminary report, in: Proceedings IJCAI-91, Sidney, Australia ( 199 1) 56-61. [28] S. Kraus and J. Wilkenfeld, The updating of beliefs in negotiations under time constraints with uncertainty, in: Proceedings IJCAI-93 Workshop on Arttj?cial Economics, Cambery, France ( 1993) 57-68; also presented at BISFA193. (291 D. Kreps and R. Wilson, Sequential equilibria, Econometrica 50 (1982) 863-894. 1301 K. Kuwabara and V. Lesser, Extended protocol for multistage negotiation, in: Proceedings Ninth Workshop on Distributed Artificial Intelligence, Washington (1989) 129-161. [ 3 1 ] V. Lesser, A retrospective view of fa/c distributed problem solving, IEEE Trans. Syst., Man Cybernet. 21 (6) (1991) 1347-1362; Special Issue on Distributed Artificial Intelligence. [32] V.R. Lesser and L.D. Erman, Distributed interpretation: a model and experiment, IEEE Trans. Comput. 29 (12) (1980) 1144-1163. [ 331 V.R. Lesser, J. Pavlin and E.H. Durfee, Approximate processing in real time problem solving, Al Mag. 9 (1) (1988) 49-61. 1341 K. Lochbaum, B. Grosz and C. Sidner, Models of plans to support communication: An initial report, in: Proceedings AAAI-90, Boston, MA (1990) 485-490. [35] R.D. Lute and H. Raiffa, Games and Decisions (Wiley, New York, NY, 1957). [ 361 V. Madrigal, T. Tan and R. Werlang, Support restrictions and sequential equililibria, J. Economic Theory 43 (1987) 329-334. [371 T.W. Malone, R.E. Fikes, K.R. Grant and M.T. Howard, Enterprise: a marketlike task schedule for distributed computing environments, in: B.A. Huberman, ed., The Ecology of Compuration (North- Holland, Amsterdam, 1988) 177-205. 1381 Y. Moses and M. Tennenholtz, Off-line reasoning for on-line efficiency, in: Proceedings IJCAI-93, Chambery, France (1993) 490-495. [ 391 J.F. Nash, The bargaining problem, Econometrica 18 ( 1950) 155-162. [40] J.F. Nash, Two-person cooperative games, Econometrica 21 (1953) 128-140. [41] M.J. Osborne and A. Rubinstein, Bargaining and Markets (Academic Press, San Diego, CA, 1990). [ 421 H. Raiffa, The Art and Science of Negotiation (Harvard University Press, Cambridge, MA, 1982). [43 1 E. Raizsmusen, Games and Information (Basil Blackwell, Cambridge, MA, 1989). [ 441 J. Rosenschein, Rational interaction: cooperation among intelligent agents, Ph.D. Thesis, Stanford University ( 1986). S. Kraus et al/Artificial Intelligence 75 (1995) 297-345 345 [45] J.S. Rosenschein and G. Zlotkin, Rules of Encounter: Designing Conventionsfor Automated Negotiation among Computers (MIT Press, Boston, MA, 1994). [461 A.E. Roth, Axiomatic Models of Bargaining, Lecture Notes in Economics and Mathematical Systems 170 ( Springer-Verlag, Berlin, 1979). [47] A. Rubinstein, Perfect equilibrium 1481 K. Ruoff, D. de Hilster, A. Horry, L. Johnston, C. Kowalski, A. Meyers and G. Vamos, Cooperation in: Proceedings among heterogeneous in a military AAAI-91 workshop on Cooperation Among Heterogeneous Intelligent Agents, Anaheim, CA ( 199 1). in a bargaining model, Econometrica 50 (1) intelligence processing (1982) 97-109. subsystems intelligent system, 1491 T. Sandholm, An implementation of the contract net protocol based on marginal cost calculations, in: Proceedings AAAI-93, Washington, DC (1993) 256-262. 1501 R. Selten, Re-examination of the perfectness concept for equilibrium points in extensive games, Int. J. Game Theory 4 (1975) 25-55. [ 5 I 1 A. Shaked and J. Sutton, unemployment as a perfect equilibrium in a bargaining model, Econometrica 52 (6) Involuntary (1984) 1351-1364. [ 521 0. Shechory and S. Kraus, Coalition formation among autonomous agents: Strategies and complexity, in: Ffih European Workshop on Modelling Autonomous Agents in a Multi-Agent World (1993) [ 53 1 Y. Shoham and M. Tennenholtz, On the synthesis of useful social laws for artificial agent societies, in: Proceedings AAAI-92, San Jose, CA ( 1992) 276-281. as a metaphor 1541 R.G. Smith and R. Davis, Negotiation (1983) 63-109. for distributed problem solving. Artif Intell. 20 [55] W. Spivey and R. Thrall, Linear Optimization (Holt, Rinehart and Winston, New York, NY, 1970). [ 561 I. Stahl, An n-person bargaining in: R. Henn and 0. Moeschlin, in an extensive form, game Mathematical Economics and Game Theory, Lecture Notes 141 ( Springer-Verlag. Berlin, 1977). in Economics and Mathematical [ 57 ] K.P. Sycara, Resolving adversarial conflicts: an approach and Computer Science, Georgia to integrating case-based and analytic methods, Institute of Technology, Atlanta, eds., Systems Ph.D. Thesis, School of Information GA (1987). I58 1 K.P. Sycara, Persuasive argumentation [591 T. Tan and S. Werlang, A guide in negotiation, Theory Dec. 28 ( 1990) 203-242. to knowledge and games, in: Proceedings Second Conference on Theoretical Aspects of Reasoning about Knowledge, Pacific Grove, CA (1988) 163-177. [ 601 M. Wellman, A general-equilibrium San Jose, CA ( 1992) 282-289. approach to distributed transportation planning, in: Proc. of AAAI-92, 1611 G. Zlotkin and J. Rosenschein, Cooperation among autonomous domains, IEEE Trans. Syst., Man Cybernet. 21 (6) (1991) 1317-1324; Special resolution via negotiation and conflict agents in noncooperative Issue on Distributed Artificial Intelligence. 1621 G. Zlotkin and J. Rosenschein, One, MAAMAW-93, Neuchatel, Switzerland two, many: coalitions ( 1993). in multi-agent systems, in: Proceedings 1631 G. Zlotkin and J.S. Rosenschein, Negotiation and task sharing among autonomous in: Proceedings IJCAZ-89, Detroit, MI (1989) 912-917. domains, agents in cooperative 1641 G. Zlotkin and J.S. Rosenschein, A domain theory for task oriented negotiation, in: Proceedings IJCAI- 93, Chambery, France (1993) 416-422. 