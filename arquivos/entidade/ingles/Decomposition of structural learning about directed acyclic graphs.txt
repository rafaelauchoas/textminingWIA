Artificial Intelligence 170 (2006) 422–439www.elsevier.com/locate/artintDecomposition of structural learning about directed acyclic graphsXianchao Xie a, Zhi Geng a,∗, Qiang Zhao a,ba School of Mathematical Sciences, LMAM, Peking University, Beijing 100871, Chinab Institute of Population Research, Peking University, Beijing 100871, ChinaReceived 8 February 2005; received in revised form 21 November 2005; accepted 16 December 2005Available online 3 February 2006AbstractIn this paper, we propose that structural learning of a directed acyclic graph can be decomposed into problems related to itsdecomposed subgraphs. The decomposition of structural learning requires conditional independencies, but it does not requirethat separators are complete undirected subgraphs. Domain or prior knowledge of conditional independencies can be utilized tofacilitate the decomposition of structural learning. By decomposition, search for d-separators in a large network is localized to smallsubnetworks. Thus both the efficiency of structural learning and the power of conditional independence tests can be improved. 2005 Elsevier B.V. All rights reserved.Keywords: Bayesian network; Conditional independence; Decomposition; Directed acyclic graph; Junction tree; Structural learning; Undirectedgraph1. IntroductionDirected acyclic graphs (DAGs) are widely used to represent independencies, conditional independencies andcausal relationships among variables [5,6,8,15,18,19,24]. Structure recovery of DAGs has been discussed by manyauthors [5,12,19,24,27]. Search for d-separators of vertex pairs is a key issue for orientation of directed edges and forrecovering DAG structures and causal relations among variables. To recover structure of DAGs, Verma and Pearl [27]presented the inductive causation (IC) algorithm which searches for a d-separator S from all possible variable subsetssuch that two variables u and v are independent conditional on S. A systematic way of searching for d-separatorsin increasing order of cardinality was proposed in [23,24]. The PC algorithm limits possible d-separators to verticesthat are adjacent to u and v [19,24]. A decomposition approach of searching for d-separators was presented in [11].To decompose a graph into two subgraphs, the approach in [11] needs a moral graph and it requires two conditions:(i) variable sets in two subgraphs are independent conditional on their separator and (ii) the separator must be acomplete subgraph in the moral graph. The two conditions are often used to define decomposition of an undirectedgraph, see Definitions 2.1 and 2.2 in [15].In this paper, we present a decomposition approach for recovering structures of DAGs. The ultimate use of the con-structed DAGs is to interpret association and causal relationships among variables. Decomposition in our approach* Corresponding author.E-mail addresses: xie1981@water.pku.edu.cn (X.C. Xie), zgeng@math.pku.edu.cn (Z. Geng), zhq@math.pku.edu.cn (Q. Zhao).0004-3702/$ – see front matter  2005 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2005.12.004X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439423only needs an undirected independence graph which may not be a moral graph and may have extra edges added tothe moral graph, and further it only requires condition (i) of conditional independencies but it does not require condi-tion (ii) of complete separators. Thus the decomposition is weaker than the weak decomposition defined in [15] andalso than that proposed in [11]. Deleting condition (ii) from decomposition conditions is important since it is difficultwith domain or prior knowledge to judge whether a separator is complete or not. In many practical applications, con-dition (i) of conditional independencies can be judged with domain or prior knowledge or with incompletely observeddata patterns, such as Markov chain, chain graphical models, dynamic or temporal models, file-matching for largedatabases and split questionnaire survey sampling [6,16,20].Section 2 gives notation and definitions. In Section 3, we show a condition for decomposing structural learningof DAGs. Construction of d-separation trees to be used for decomposition is discussed in Section 4. We propose themain algorithm and then give an example in Section 5 to illustrate our approach for recovering the global structureof a DAG. Section 6 discusses the complexity and advantages of the proposed algorithms. Conclusions are given inSection 7. The proofs of our main results and algorithms are given in Appendix A.2. Notation and definitions2.1. Directed acyclic graphs and undirected graphsLet (cid:2)GV = (V , (cid:2)EV ) denote a DAG where V = {X1, . . . , Xn} is the vertex set and (cid:2)EV the set of directed edges.A directed edge from a vertex u to a vertex v is denoted by (cid:3)u, v(cid:4). We assume that there is no directed loop in (cid:2)GV . Wesay that u is a parent of v and v is a child of u if there is a directed edge (cid:3)u, v(cid:4), and denote the set of all parents of avertex v by pa(v). We say that two vertices u and v are adjacent in (cid:2)GV if there is an edge connecting them. A path lbetween two distinct vertices u and v is a sequence of distinct vertices in which the first vertex is u, the last one is vand two consecutive vertices are connected by an edge, that is, l = (c0 = u, c1, . . . , cm−1, cm = v) where (cid:3)ci−1, ci(cid:4) or(cid:3)ci, ci−1(cid:4) is contained in (cid:2)EV for i = 1, . . . , m (m (cid:1) 1), and ci (cid:5)= cj for all i (cid:5)= j . We say that u is an ancestor of v and vis a descendant of u if there is a path between u and v in (cid:2)GV and all edges on this path point at the direction toward v.The set of ancestors of v is denoted as an(v), and define An(v) = an(v) ∪ {v}. A path l is said to be d-separated bya set of vertices Z if and only if(1) l contains a ‘chain’: u → v → w or a ‘fork’ u ← v → w such that the middle vertex v is in Z, or(2) l contains a ‘collider’ u → v ← w such that the middle vertex v is not in Z and no descendant of v is in Z.Two distinct sets X and Y of vertices are d-separated by a set Z if Z d-separates every path from any vertex in X toany vertex in Y ; We call Z a d-separator of X and Y . In a DAG (cid:2)GV , a collider u → v ← w is called a v-structure if uand w are non-adjacent in (cid:2)GV .Let ¯GV = (V , ¯EV ) denote an undirected graph where ¯EV is a set of undirected edges. An undirected edge betweentwo vertices u and v is denoted by (u, v). For a subset A of V , let ¯GA = (A, ¯EA) be the subgraph induced by Aand ¯EA = {e ∈ ¯EV | e ∈ A × A} = ¯EV ∩ (A × A). An undirected graph is called complete if any pair of vertices isconnected by an edge. For an undirected graph, we say that vertices u and v are separated by a set of vertices Z ifeach path between u and v passes through Z. We say that two distinct vertex sets X and Y are separated by Z if andonly if Z separates every pair of vertices u and v for any u ∈ X and v ∈ Y . We say that an undirected graph ¯GV isan undirected independence graph for a DAG (cid:2)GV if the fact that a set Z separates X and Y in ¯GV implies that Zd-separates X and Y in (cid:2)GV . We say that ¯GV can be decomposed into subgraphs ¯GA and ¯GB if(1) A ∪ B = V , and(2) C = A ∩ B separates A \ B and B \ A in ¯GV .The above decomposition does not require that the separator C is complete, which is required for weak decompositiondefined in [15] and for decomposition of search for v-structures proposed in [11]. In the next section, we show that aproblem of structural learning of a DAG can also be decomposed into problems for its decomposed subgraphs even ifthe separator is not complete.424X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439(a)(b)Fig. 1. A directed graph, a moral graph and a triangulated graph. (a) The DAG (cid:2)GV . (b) The moral graph (cid:2)GmV . (c) A triangulated graph (cid:2)GtV .(c)Define a moral graph (cid:2)GmA triangulated graph is an undirected graph whose every cycle of length (cid:1) 4 possesses a chord [15]. For anundirected graph ¯GV which is not triangulated, we can add extra edges to it such that it becomes to be a triangulatedgraph, denoted by ¯GtV .V for a DAG (cid:2)GV to be an undirected graph ¯GV = (V , ¯EV ) whose vertex set is V andwhose edge set is constructed by marrying parents and dropping directions, that is, ¯EV = {(u, v): (cid:3)u, v(cid:4) or (cid:3)v, u(cid:4) ∈(cid:2)EV } ∪ {(u, v): (u, w, v) forms a v-structure} [15]. An undirected edge added for marrying parents is called a moraledge. The moral graph (cid:2)GmV is an undirected independence graph for (cid:2)GV [15].Example 1. Consider a DAG (cid:2)GV in Fig. 1(a). 2 → 4 ← 3 and 4 → 7 ← 6 are two v-structures. A path l = (2, 1, 5)is d-separated by vertex 1, and another path l(cid:11) = (2, 4, 7, 6, 5) is d-separated by an empty set since 4 → 7 ← 6 is acollider. Vertices 2 and 5 are d-separated by vertex 1. an(4) = {1, 2, 3} and An(4) = {1, 2, 3, 4}. The moral graph (cid:2)GmVis shown in Fig. 1(b), whose edges (2, 3) and (4, 6) are moral edges. Set {2, 3, 5} separates {1} and {4, 6, 7}, and thus(cid:2)GmV can be decomposed into two undirected subgraphs over {1, 2, 3, 5} and {2, . . . , 7}. An undirected independencegraph for (cid:2)GV may have extra undirected edges added to the moral graph, say edges (1, 4) and (1, 6) added to (cid:2)GmV , seedashed edges in Fig. 1(c). The graph in Fig. 1(c) is a triangulated graph of (cid:2)GmV .Given a DAG (cid:2)GV , a joint distribution or density of variables X1, . . . , Xn isP (x1, . . . , xn) =n(cid:1)i=1P (xi | pai),where P (xi | pai) is the conditional probability or density of Xi given pa(Xi) = pai . The DAG (cid:2)GV and the distributionP are said to be compatible [19] and P obeys the global directed Markov property of (cid:2)GV [15]. Let X Y denotethe independence of X and Y , and X Y | Z the conditional independence of X and Y given Z. If sets X and Y ared-separated by Z, then X is independent of Y conditional on Z in every distribution that is compatible with (cid:2)GV [19].In this paper, we assume that all the distributions are compatible with (cid:2)G. We also assume that all independencies of aprobability distribution of variables in V can be checked by d-separations of (cid:2)GV , called the faithfulness assumption[24]. The faithfulness assumption means that all independencies and conditional independencies among variables canbe represented by (cid:2)GV .The global skeleton is an undirected graph obtained by dropping direction of a DAG. Thus the absence of anedge (u, v) implies that there is a variable subset S of V such that u and v are independent conditional on S, thatv|S for some S ⊆ (V \ {u, v}). Two DAGs over the same variable set are called Markov equivalent if theyis, uinduce the same conditional independence restrictions. Two DAGs are Markov equivalent if and only if they have thesame global skeleton and the same set of v-structures [27]. An equivalence class of DAGs consists of all DAGs whichare Markov equivalent, and it is represented as a partially directed graph where the directed edges represent arrowsX.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439425Fig. 2. A d-separation tree.that are common to every DAG in it, while the undirected edges represent that any proper orientation of them leadsto a Markov equivalent DAG. Therefore the goal of structural learning is to construct a partially directed graph torepresent the equivalence class. A local skeleton for a subset A of variables is an undirected subgraph for A in whichthe absence of an edge (u, v) implies that there is a subset S of A such that uv|S.2.2. d-separation treesTo depict separations and conditional independencies among multiple variable sets, we introduce a notion ofd-separation trees in this subsection. Let C be a collection of variable sets, that is, C = {C1, . . . , CH }, such that(cid:2)Hh=1 Ch = V and Ci (cid:5)⊆ Cj for i (cid:5)= j . Let T be a tree whose every node is a variable set in C and is displayed as atriangle. The term ‘node’ is used for a tree to distinguish the term ‘vertex’ for a graph. An edge eh = (Ci, Cj ) connectsnodes Ci and Cj in T , and it is attached with a separator S displayed as a rectangle, which is the intersection of adja-cent nodes, that is, S = Ci ∩ Cj ; We say that the separator S connects the nodes Ci and Cj . Removing a separator Sfrom the tree T splits T into two subtrees T1 and T2 with node sets C1 and C2 respectively. Let Vi =C be theunion of the nodes in the subtree Ti for i = 1 and 2.C∈Ci(cid:2)Definition 1. A tree T is said to be a d-separation tree for a DAG (cid:2)G if any separator S in T d-separates, in (cid:2)G, thevertex sets V1 \ S and V2 \ S of two subtrees T1 and T2 obtained by removing S.The notion of a d-separation tree is very similar to that of a junction tree. A d-separation tree is defined with d-separation and it does not need that every node is a clique, while a junction tree is a d-separation tree (see Theorem 2).Example 1. (Continued) Let C = {{1, 2, 3, 4}, {1, 4, 6}, {1, 5, 6}, {4, 6, 7}} be a collection of variable sets. A d-sepa-ration tree with C as the node set is depicted in Fig. 2. Removing the separator S = {1, 4}, we obtain two subtrees T1and T2 with the node sets C1 = {{1, 2, 3, 4}} and C2 = {{1, 4, 6}, {1, 5, 6}, {4, 6, 7}} respectively, and the separator Sd-separates V1 \ S = {2, 3} and V2 \ S = {5, 6, 7} in (cid:2)GV .2.3. HypergraphA collection of variable sets C = {C1, . . . , CH } is said to be a hypergraph on V where each hyperedge Ch isHh=1 Ch = V , see Chapter 17 in [4]. A hypergraph is a reduced hypergrapha nonempty subset of variables, andif Ci (cid:5)⊆ Cj for i (cid:5)= j [3]. In this paper, only reduced hypergraphs are used, and thus simply called hypergraphs. InSection 4.2, a hyperedge can be used to represent the domain knowledge of association among variables or to representmultiple databases with overlapping.(cid:2)Example 1. (Continued) Let C = {{1, 2, 3, 4}, {1, 3, 5, 6}, {4, 6, 7}} be a hypergraph, as shown in Fig. 3.3. Decomposition of structural learningIn this section, we show that if vertices u and v are d-separated by a d-separator S in a DAG (cid:2)GV , then u andv are not contained by any node of the d-separation tree for (cid:2)GV or there exists a node C that contains u, v and S(cid:11)v|S(cid:11), and vice versa. Applying this result to structural learning, we can split a problem of searching forsuch that u426X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439Fig. 3. A hypergraph.d-separators and building the skeleton of a DAG into small problems for every node of T . This result may also beuseful for designing observational studies for recovering local causal relationships.Theorem 1. Let T be a d-separation tree for a DAG (cid:2)G. Vertices u and v are d-separated by S (⊆ V ) in (cid:2)GV if andonly if (i) u and v are not contained together in any node C of T or (ii) there exists a node C that contains both uand v such that a subset S(cid:11) of C d-separates u and v.According to Theorem 1, a problem of searching for a d-separator S of u and v in all possible subsets of V islocalized to all possible subsets of nodes in a d-separation tree that contain u and v. For a given d-separation tree Twith the node set C = {C1, . . . , CH }, we can recover the skeleton and all v-structures for a DAG as follows. Firstwe construct a local skeleton for every node Ch of T , which is constructed by starting with a complete undirectedsubgraph and removing an undirected edge (u, v) if there is a subset S of Ch such that u and v are independentconditional on S. Next to construct the global skeleton, we combine all these local skeletons together and removeedges that are present in some local skeletons but absent in other local skeletons. Then we determine every v-structureif two non-adjacent vertices u and v have a common neighbor in the global skeleton but the neighbor is not containedin the d-separator of u and v. Finally we can orient more undirected edges if each opposite of them creates either adirected cycle or a new v-structure [17]. This process is formally described in the following algorithm.Algorithm 1 (Construct the equivalence class of DAGs from a d-separation tree).1. Input: a d-separation tree T with a node set C = {C1, . . . , CH }.2. Construct a local skeleton ¯Gh for each h separately:• Initialize ¯Gh as a complete undirected graph;• If there exists a subset Suv of Ch \ {u, v} such that uthe d-separator list S;3. Construct the global skeleton ¯GV :v|Suv, then delete edge (u, v) from ¯Gh and save Suv to• Initialize the edge set ¯EV of ¯GV as the union of all edge sets of ¯Gh, h = 1, . . . , H ;• For a pair of vertices u and v contained in several local skeletons, delete edge (u, v) from ¯EV if it is absent insome skeleton.4. For each d-separator Suv in the list S, determine a v-structure u → w ← v if u–w–v appears in the global skeletonand w is not in Suv.5. Orient other edges if each opposite of them creates either a directed cycle or a new v-structure.6. Output: the equivalence class of DAGs.According to Theorem 1, we can prove that the global skeleton and all v-structures obtained by applying the abovedecomposition algorithm are correct, that is, they are the same as those obtained from the joint distribution of V , seeAppendix A for the detail proof. Note that separators in a d-separation tree may not be complete in the moral graph.Thus the decomposition is weaker than the decomposition usually defined for parameter estimation [5,10,15].Example 1. (Continued) Consider the d-separation tree in Fig. 2 as the input of Algorithm 1. At step 2, we separatelybuild the local skeleton for each node of T , as shown in Fig. 4(a). At step 3, the global skeleton is obtained bycombining the local skeletons in Fig. 4(a). Edge (1, 6) in the subgraph for node {1, 4, 6} is a spurious edge and itis removed from the global graph since the edge (1, 6) does not appear in the subgraph for node {1, 5, 6}, that is,X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439427(a)(b)Fig. 4. Skeleton and v-structures for (cid:2)GV . (a) Local skeletons for every node of T . (b) The global skeleton and all v-structures.there is a d-separator S16 = {5} which d-separates 1 and 6 in the subgraph. Similarly edge (4, 6) is removed sinceit is absent in the third local skeleton. At step 4, all v-structures can be determined, see Fig. 4(b). For example, thev-structure 2 → 4 ← 3 can be determined since for S23 = {1} ∈ S, 2–4–3 appears in the global skeleton and vertex 4is not contained in S23. Similarly, the v-structure 4 → 7 ← 6 is found since for S46 = {1} ∈ S, 4–7–6 appears in theglobal skeleton and vertex 7 is not contained in S46. But for S16 = {5} ∈ S, 1–5–6 appears, but vertex 5 is containedin S16, and thus it is not a v-structure. For this DAG, no other edges can be oriented at step 5. The partially directedgraph in Fig. 4(b) is the equivalence class of the DAG in Fig. 1(a).By Theorem 1, it is ensured that the global skeleton and all v-structures in (cid:2)GV can be recovered by combiningsubgraphs over all nodes of T . In the next section, we discuss how to construct a d-separation tree.4. Constructing a d-separation treeIn this section, we discuss how to construct a d-separation tree from observed data or from domain or prior knowl-edge of conditional independencies or from a collection of databases. We first propose an approach in which anundirected independence graph and then a junction tree are built from observed data, and we show that a junction treeis a d-separation tree. Next we propose an approach for constructing a d-separation tree from domain knowledge orfrom a collection of databases with different observed variable sets.4.1. Constructing a d-separation tree from observed dataIn several algorithms of structural learning, the first step is to construct an undirected independence graph in whichv|V \ {u, v}. To construct such an undirected graph, we can start with athe absence of an edge (u, v) implies ucomplete undirected graph, and then for each pair of variables u and v, an undirected edge (u, v) is removed if u andv are independent conditional on the set of all other variables. For linear Gaussian models, the undirected graph canbe efficiently constructed by removing an edge (u, v) if and only if the corresponding entry in the inverse covariancematrix is zero. For discrete data, a test of conditional independence given a large number of discrete variables maybe extremely low power. To cope with such difficulty, for a vertex u, we first use information criterion to find avariable subset which contains the Markov blanket of u [18], and then we test independence of u and another variableconditionally on the variable subset [14,26]. This test is efficient if the Markov blanket of u is not large. For discretedata, we can also use the algorithm proposed in [9] which reduces the requirement for testing high order conditionalindependencies.A d-separation tree can be built by constructing a junction tree from an undirected independence graph [5] or byusing the algorithm presented in Section 4.2.Theorem 2. A junction tree constructed from an undirected independence graph for (cid:2)GV is a d-separation tree for (cid:2)GV .428X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439A d-separation tree T only requires that all d-separation properties of T also hold for (cid:2)GV , but the reverse isnot required. Thus we only need to construct an undirected independence graph that may have fewer conditionalindependencies than the moral graph, and this means that the undirected independence graph may have extra edgesadded to the moral graph. Thus the null hypothesis of the absence of an undirected edge may be tested statistically ata larger significance level provided that no nodes of a d-separation tree contain too many variables.Example 1. (Continued) To construct a d-separation tree for (cid:2)GV in Fig. 1(a), at first an undirected independence graphv|V \ {u, v}. An undirected graphis constructed by starting with a complete graph and removing an edge (u, v) if uobtained in this way is the moral graph in Fig. 1(b). In fact, we only need to construct an undirected independencegraph which may have extra edges added to the moral graph. Next triangulate the undirected graph and finally obtainthe d-separation tree, as shown in Fig. 1(c) and Fig. 2 respectively.4.2. Constructing a d-separation tree from domain knowledge or from observed data patternsIn this subsection, we propose an approach for constructing a d-separation tree from domain knowledge or fromobserved data patterns without conditional independence tests. The domain knowledge may be experts’ prior knowl-edge of dependencies among variables, such as Markov chains, chain graphical models and dynamic or temporalmodels. In many practical applications, such as file-matching and split questionnaire survey sampling [16,20], thereare many large databases that have different data patterns, that is, databases have different attributes but may overlayeach other. Based on the domain knowledge of dependencies, data patterns of databases can be designed to recover theentire structure for the full variable set V correctly. The problem of reconstructing a DAG from multiple overlappingdatabases has been considered, and two rules for determining the absence of edges were proposed in [7]. As the authornoticed, however, the two rules do not exhaust all possible rules and the existence of others remains an open problem.We theoretically prove that our approach is perfect, that is, the entire DAG reconstructed by using multiple databasesis the same as that reconstructed by using joint data over the full variable set V , if multiple databases are designedbased on the domain knowledge of dependencies.We first consider a simple case with two variable sets A and B. Let C = A ∩ B. Suppose that we have a domainknowledge that any variable in A \ C associates with any variable in B \ C only through variables in C, which implies(B \ C)|C. We can depict this with a hypergraph and thus obtain a d-separation tree, as shown in Fig. 5(a)(A \ C)and (b) respectively. The domain knowledge can also be seen as the Markov property that the future state B \ C isindependent of the previous state A \ C conditional on the current state C.For a general case, a domain knowledge of variable dependencies can be represented as a collection of variablesets C = {C1, . . . , CH }, in which variables contained in the same set may associate each other directly but variablescontained in different sets associate each other through other variables. This means that two variables that are notcontained in the same set are independent conditionally on all other variables. We depict such a domain knowledgewith a hypergraph. Then equivalently the domain knowledge is legitimate if every edge of the moral graph ¯GmV of theunderlying DAG is contained in some hyperedge in C. A slightly stronger condition for judging the legitimacy of adomain knowledge is that for each variable u, there is a hyperedge Ch in C which contains both u and its parent set.On the other hand, in an application study, observed data may have a collection of different observed patterns,C = {C1, . . . , CH }, where Ch is the set of observed variables for the hth group of individuals. For example, observeddata patterns C = {{a, b, c}, {b, c, d}} mean that there are two groups of individuals: (1) variables a, b and c areobserved but variable d is missing for the first group, and (2) variables b, c and d are observed but variable a ismissing for the second group. Data having such observed patterns are not uncommon, such as in the file-matchingand split questionnaire survey [16,20]. Given observed data patterns C = {C1, . . . , CH }, there is no information on theassociation among variables that are never observed together, and thus parameters that relate to the association are(a)(b)Fig. 5. A domain knowledge about associations among variables. (a) A hypergraph. (b) A d-separation tree.X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439429inestimable without other assumptions. The condition to make our algorithms correct for structural learning from acollection C is that C must contain sufficient data such that parameters of the underlying DAG are estimable. For theDAG, its parameters are estimable if, for each variable u, there is an observed data pattern Ch in C which containsboth u and its parent set. Thus a collection C of observed patterns has sufficient data for correct structural learningif there is a pattern Ch in C for each u such that Ch contains both u and its parent set in the underlying DAG. Everypattern Ch can be seen as a hyperedge or a maximum complete undirected graph to depict possible association amongvariables in Ch. When all variables are categorical, a log-linear model with a generating class C = {C1, . . . , CH } is thehighest order interaction model without latent variables. We assume that all independencies inferred from observeddata patterns are true for the underlying DAG.Example 1. (Continued) Suppose that we have a hypergraph C = {{1, 2, 3, 4}, {1, 3, 5, 6}, {4, 6, 7}}, as depicted inFig. 3. It can be considered as a domain or prior knowledge of associations among variables. The hypergraph in Fig. 3can also be considered as observed data patterns in three databases, that have different attributes and overlap each other.In the case that all variables are categorical and there is no latent variable, the log-linear model with the generatingclass {[1234], [1356], [467]} includes all interactions among variables that can be estimated from the observed data.Since for each variable u, there is a hyperedge in C which contains both u and its parent set, the hypergraph C is alegitimate domain knowledge and a legitimate collection of databases for the underlying DAG in Fig. 1(a). However, ifthe hypergraph C is changed to C(cid:11) = {{1, 2, 4}, {1, 3, 5, 6}, {4, 6, 7}}, that is, the hyperedge {1, 2, 3, 4} in C is replacedby {1, 2, 4}, then there is not any hyperedge in C(cid:11) which contains both variable 4 and its parent set {2, 3}, and thus thehypergraph C(cid:11) is neither a legitimate collection of databases nor a legitimate domain knowledge for the DAG.Now we discuss how to construct a d-separation tree from a hypergraph that represents domain knowledge ofdependencies or observed data patterns. First we explain in the following example why the global skeleton of a DAG(cid:2)GV cannot be constructed by combining all subgraphs obtained separately from every data pattern. Then we proposean algorithm for constructing a correct global skeleton.Example 2. Consider a DAG (cid:2)GV and observed data patterns as depicted in Fig. 6(a). From three data patterns, we getseparately three undirected subgraphs 1–2, 1–3 and 2–3. Combining them together, we obtain a combined undirectedgraph in Fig. 6(b), which is not a correct skeleton for (cid:2)GV since edge (2, 3) is a spurious edge.Below we propose an algorithm for constructing a d-separation tree T from domain knowledge or from observeddata patterns such that a correct skeleton can be constructed by combining subgraphs for nodes of T . Since a hyper-edge Ch represents all possible associations among variables in Ch, we first use a complete subgraph over Ch as theundirected independence graph over variables in Ch, and then piece all subgraphs together into an entire undirectedgraph. To reduce the sizes of tree nodes, we construct a junction tree in terms of triangulating the undirected graph.In this way, the following algorithm constructs a d-separation tree T from a hypergraph C = {C1, . . . , CH } of domainknowledge or observed data patterns.Algorithm 2 (Construct a d-separation tree from a hypergraph).1. Input: a hypergraph C = {C1, . . . , CH } whose each hyperedge Ch is a variable set.2. For each hyperedge Ch, construct a complete undirected graph ¯Gh with the edge setCh × Ch.¯Eh = {(u, v), ∀u, v ∈ Ch} =Fig. 6. An incorrect skeleton combined from subgraphs. (a) A DAG and observed data patterns. (b) The combined undirected graph.(a)(b)430X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439(a)(b)Fig. 7. Construct a d-separation tree from a hypergraph. (a) The undirected graph obtained by combining complete subgraphs. (b) The d-separationtree.3. Construct the entire undirected graph ¯GV = (V , ¯E) whose edge set ¯E = ¯E1 ∪ · · · ∪ ¯EH .4. Construct a junction tree T by triangulating ¯GV .5. Output: T , which is a d-separation tree for the hypergraph C.The correctness of Algorithm 2 is proven in Appendix A. Note that we do not need any conditional independencetest in Algorithm 2 to construct a d-separation tree. In this algorithm, we can use a heuristic triangulation algorithmwith less computational complexity to construct a junction tree [2,13,21]. Below we give two examples to illustrateAlgorithm 2.Example 1. (Continued) Consider the domain knowledge of associations among all variables given in Fig. 3. At step 2,we construct a complete subgraph for each hyperedge, and then at step 3 we combine them together, as depicted inFig. 7(a). At step 4, we construct the d-separation tree in Fig. 7(b). Note that no edges are added for triangulationsince the undirected graph in Fig. 7(a) is triangulated.Example 3. Suppose that V = {1, . . . , 6} and that the domain knowledge is C = {C1, C2, C3, C4} = {{1, 2, 3}, {1, 2, 4},{1, 3, 5}, {4, 5, 6}} as depicted in Fig. 8(a). At step 2, we construct four complete subgraphs, and then at step 3 wecombine them together, as shown by the undirected graph of solid lines in Fig. 8(b). At step 4, we triangulate it withan edge (3, 4) of a dash line, and then we obtain the d-separation tree in Fig. 8(c).The nodes of a d-separation tree T constructed from domain knowledge or from observed data patterns may bestill quite large. In this case, we use conditional independence tests to reduce the node sizes. We first can construct anundirected independence subgraph for each node, then combine these subgraphs into a global undirected independencegraph and finally construct a refined d-separation tree, see Section 5. For every node Ch of T for h = 1, . . . , H ,an undirected independence subgraph ¯Gh = (Ch, Eh) can be constructed by starting with a complete subgraph andremoving an undirected edge (u, v) if u and v are independent conditional on Ch \ {u, v}.Theorem 3. Suppose that ¯Gh = (Ch, ¯Eh) is an independence subgraph for the node Ch of the d-separation treefor h = 1, . . . , H . The undirected graph ¯GV with the edge set EV =Hh=1 Eh is an undirected independence graphfor (cid:2)GV .(cid:2)X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439431(a)(b)Fig. 8. Construct the d-separation tree. (a) Domain knowledge of associations. (b) The undirected graph and triangulation. (c) The d-separationtree.(c)Fig. 9. An undirected independence graph obtained by combining subgraphs. (a) Undirected independence subgraphs for each node. (b) Combiningsubgraphs in (a).(a)(b)Note that this combination for obtaining a global undirected independence graph is different from that for obtaininga global skeleton proposed in Section 3. The former pools all edges in subgraphs into the global graph, while the latterdeletes an edge (u, v) from the global graph if it is present in a subgraph but absent in another subgraph.Example 1. (Continued) We can suppose that a d-separation tree for (cid:2)GV has the node set C = {{1, 2, 3, 5},{2, 3, 4, 5, 6, 7}} since their intersection S = {2, 3, 5} d-separates {1} and {4, 6, 7}. Construct undirected independencesubgraphs for every node separately, as shown in Fig. 9(a); and then construct the global undirected independencegraph with all edges of subgraphs, as shown in Fig. 9(b). The undirected independence graph is not the moral graphsince it has two extra edges (3, 5) and (2, 5) added to the moral graph.5. Illustration of structural learning via decompositionIn this section, we first formally describe the complete algorithm for structural learning of DAGs via decomposition.Then we illustrate the algorithm using the ALARM network in Fig. 10 that is often used to evaluate structural learningalgorithms [1,12,24].Main algorithm (The decomposition approach for structural learning of DAGs with domain knowledge).1. Input: a hypergraph C = {C1, . . . , CH } as the domain knowledge, or databases with observed data patterns C.2. Call Algorithm 2 to construct a d-separation tree T from the hypergraph C.3. If the sizes of nodes in T are too large, then refine T with observed data:• Construct an undirected independence subgraph over each node of T ;• Let the edge set ¯EV of the entire undirected independence graph ¯GV be the union of all edge sets of subgraphs;• Construct a junction tree T (cid:11) by triangulating ¯GV , which is a d-separation tree.432X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439Fig. 10. The ALARM network and domain knowledge.Fig. 11. The d-separation tree T from domain knowledge.4. Call Algorithm 1 to construct the equivalence class of DAGs from T (cid:11) (or T if the sizes of nodes are not verylarge).5. Output: the equivalence class of DAGs.The correctness of the main algorithm is proven in Appendix A. To reduces the requirement for testing high orderconditional independencies, we first construct a d-separation tree at step 2 based on the prior knowledge, and then ifnecessary, we further reduce the size of nodes in the d-separation tree at step 3. Note that the undirected independencegraph obtained at step 3 may have extra edges, and thus the tests of conditional independencies can be performed witha large significance level. The conditional independence tests performed in Algorithm 1 are marginalized to nodes ofthe d-separation tree.The ALARM network in Fig. 10 describes causal relations among 37 variables in a medical diagnostic systemfor patient monitoring. Using the network, some researchers generate continuous data from normal distributions andothers generate discrete data from multinomial distributions [12,24]. Our approach is applicable for both continuousand discrete data. Since the correctness of the algorithms are proved in Appendix A and the complexity analysisand the accuracy of independence tests are discussed in the next section, the main algorithm is illustrated by usingconditional independencies from the underlying DAG rather than conditional independence tests from simulated data.Suppose that we have the domain knowledge of associations among all variables as depicted by the hypergraph inFig. 10. The hypergraph can also be seen as three databases with overlap. Since for each variable u in the ALARM,there is a hyperedge which contains both u and its parent set, the domain knowledge or the collection of three data-base is legitimate for structural learning of the ALARM network. However, if the hyperedge {8, 28, 29, 30, 31, 32}is changed to another hyperedge {8, 29, 30, 31, 32}, then there is no hyperedge which contains both variable 29 andits parent set {8, 28}, and thus parameters of the ALARM network are inestimable and the changed hypergraph as adomain knowledge or as a collection of databases is not legitimate.At step 2, the d-separation tree T is constructed from the hypergraph, as shown in Fig. 11. Note that at step 2we need not do any conditional independence test, and we construct the d-separation tree only based on the domainknowledge.Since the three nodes of T obtained at step 2 are still quite large, we first construct undirected independencesubgraphs for three nodes separately at step 3, as shown in Fig. 12; Next combining three subgraphs together andX.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439433(a)(b)Fig. 12. Undirected independence graphs for nodes of T . (a) A subgraph for the left node of T . (b) A subgraph for the middle node of T .(c) A subgraph for the right node of T .(c)Fig. 13. The global triangulated graph.triangulating it, we obtain the triangulated independence graph in Fig. 13. From the triangulated graph, we constructa d-separation tree T (cid:11) in Fig. 14 (separators are omitted), in which the largest node has only 5 variables.At step 4, we construct a sub-skeleton for every node in T (cid:11), as shown in Fig. 15. Combining all sub-skeletonstogether, determining v-structures and orienting edges as much as possible, we obtain the equivalence class in Fig. 16,in which all directed edges are oriented correctly, except that four undirected edges (5, 27), (10, 33), (21, 34) and(35, 37) cannot be oriented because any of their orientation leads to a Markov equivalent DAG. Note that some edgesin Fig. 16 are oriented at step 5 of Algorithm 1. For example, the direction of the undirected edge (1, 2) is determinedas (cid:3)2, 1(cid:4) by (cid:3)4, 2(cid:4) so as not to create a new v-structure, and the direction of the undirected edge (16, 23) is determinedas (cid:3)16, 23(cid:4) by (cid:3)16, 20(cid:4) and (cid:3)20, 23(cid:4) so as not to create a cycle.The global skeleton and all v-structures depicted in Fig. 16 are the same as those obtained from the joint distributionof all variables in V . Applying decomposition, we split the graph with 37 vertices into 28 subgraphs, of which thelargest one contains only 5 vertices. In such a way, a problem of high-dimensional structural learning is reduced intoseveral small problems.434X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439Fig. 14. The d-separation tree T (cid:11).Fig. 15. Skeletons for all nodes of T (cid:11).6. Complexity analysis and advantagesThere are several obvious advantages of the decomposition approach for structural learning proposed in this paper.First a d-separation tree can be constructed based on the prior or domain knowledge rather than conditional inde-pendence tests. By using the d-separation tree, independence tests are performed only conditionally on smaller setscontained in a node of the d-separation tree rather than on the full set of all other variables. Thus our algorithm hashigher power for statistical tests.X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439435Fig. 16. The partially directed acyclic graph.Second, the theoretical results proposed in this paper can be applied to scheme design of multiple databases.Without loss of information on structural learning of DAGs, a joint data set can be replaced by a group of incompletedata sets based on the domain or prior knowledge of conditional independencies among variables.Finally, the computational complexity can be reduced. This complexity analysis focuses only on the number ofconditional independence tests for constructing the equivalence class. Decomposition of graphs is a computationallysimple task compared to the conditional independence tests. The triangulation of an undirected graph is used in ouralgorithms to construct a d-separation from an undirected independence graph. Although the problem for optimallytriangulating an undirected graph is NP-hard, sub-optimal triangulation methods [2,13,21] may be used providedthat the obtained tree does not contain too large nodes to test conditional independencies. Two most well-knownalgorithms are the lexicographic search [22] and the maximum cardinality search [25], and their complexities areO(ne) and O(n + e) respectively. Thus in our algorithms, the conditional independence tests dominate the algorithmiccomplexity.Let V denote the set of all variables, and n the number of variables in V . In the IC algorithm, to delete an edgebetween a pair of variables u and v, we may need to test independencies of u and v conditionally on all possiblesubsets S of V \ {u, v}. Thus the complexity for deleting an edge in the global skeleton is O(2n), and then thecomplexity for constructing the global skeleton is O(n22n).In our decomposition algorithm, suppose that the set V of all variables is decomposed into H nodes {C1, . . . , CH }in the d-separation tree T , where H (cid:2) n. Let m denote the number of variables in the largest node, that is, m =maxh |Ch| where |Ch| denotes the number of variables in Ch. The complexity for constructing a local skeleton isO(m22m), and thus that of all local skeletons is O(H m22m). Since m usually is much less than n, our algorithm is lesscomputationally complex than the IC algorithm.In the PC algorithm, to delete an edge between a pair of variables u and v, we may need to check all possible subsetsof variables adjacent to u and v. The decomposition of a large graph into small subgraphs can also be used to improvethe PC algorithm. The sets of variables adjacent to u and v can be split into small subsets by the decomposition. Thusto delete an edge between u and v, we only need to check each subset of variables adjacent to u or v that is containedin a subgraph with u or v. Let Adj(u) denote the set of variables adjacent to u in the global skeleton, and Adjh(u) theset of variables adjacent to u in the hth local skeleton. Since Adjh(u) ⊆ Adj(u), the complexity for checking all subsetsof Adjh(u) is less than that for checking all subsets of Adj(u). Thus the decomposition can reduce the complexity ofthe PC algorithm.7. ConclusionsWe proposed a decomposition approach for structural learning of DAGs. In this approach, a problem of learning alarge DAG is split into problems of learning small subgraphs. Domain or prior knowledge of conditional independen-cies can be utilized to facilitate the decomposition of structural learning. We theoretically proved the correctness ofthe proposed algorithms. Both the complexity of the algorithms and the power of conditional independence tests canbe improved by decomposing a large graph into small subgraphs. The theoretical results can also be used for scheme436X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439design of multiple databases. Although we assumed in this paper that there is no latent variables, some of our resultsmay be applied to the case with latent variables. For example, suppose that we have some domain knowledge. Thenwe can apply our approach for local structural learning, which is discussed in an uncompleted paper.AcknowledgementsWe would like to thank the referees for their valuable comments and suggestions that improved the presentation ofthis paper. This research was supported by NSFC, MSAR and NBRP 2003CB715900.Appendix AA.1. Proofs of theoremsFirst we give a definition and several lemmas to be used in proofs of theorems.Definition A.1. Let T be a d-separation tree for a DAG (cid:2)GV with the node set C = {C1, . . . , CH }. For any two vertices uand v in (cid:2)GV , the distance between u and v in the tree T is defined byd(u, v) = minCi (cid:14)u,Cj (cid:14)vd(Ci, Cj ),where d(Ci, Cj ) is the distance between nodes Ci and Cj in T . We call Ci and Cj minimizers for u and v whichminimize the distance d(Ci, Cj ).Lemma A.1. Let l be a path from u to v, and W be the set of all vertices on l (W may or may not contain u and v).Suppose that a path l is d-separated by S. If W is contained in S, then the path l is d-separated by W and by any setcontaining W .Proof. Since the d-separation of the path l depends on which vertices between u and v are contained in the d-separatorand W contains all vertices on l, l is also d-separated by S ∩ W = W if l is d-separated by S. Since all colliders on lhave already been made active conditionally on W , adding other vertices into the conditional set does not make anynew collider active on l. This implies that l remains to be d-separated by any set containing W . (cid:1)Lemma A.2. Let T be a d-separation tree for a DAG (cid:2)GV , and K be a separator of T which separates T into twosubtrees T1 and T2 with variable sets V1 and V2 respectively. Suppose that l is a path from u to v in (cid:2)GV whereu ∈ V1 \ K and v ∈ V2 \ K. Let W denote the set of all vertices on l (W may or may not contain u and v). Then thepath l is d-separated by W ∩ K and by any set containing W ∩ K.Proof. Since u ∈ V1 \ K and v ∈ V2 \ K, there is a series from s (may be u) to y (may be v) in l =(u, . . . , s, t, . . . , x, y, . . . , v) such that s ∈ V1 \ K and y ∈ V2 \ K and all vertices from t to x are contained in K.Let l(cid:11) be the sub-path of l from s to y and W (cid:11) the vertex set from t to x and thus W (cid:11) ⊂ K. Since s ∈ V1 \ K andy ∈ V2 \ K, we have from definition of d-separation tree that K d-separates s and y in (cid:2)GV , that is, K separates l(cid:11).By Lemma 1, we get that l(cid:11) is d-separated by W (cid:11) (⊂ K) and by any set containing W (cid:11). Since W (cid:11) ⊂ (W ∩ K), l(cid:11) isd-separated by W ∩ K and by any set containing W ∩ K. Thus l (⊇ l(cid:11)) is also d-separated by them. (cid:1)Lemma A.3. Let u and v be two non-adjacent vertices in (cid:2)GV , and let l be a path from u to v. If l is not contained inAn(u) ∪ An(v), then l is d-separated by any subset S of an(u) ∪ an(v).Proof. Since l (cid:5)⊆ An(u) ∪ An(v), there is a series from s (may be u) to y (may be v) in l = (u, . . . , s, t, . . . , x, y, . . . , v)such that s and y are contained in An(u) ∪ An(v) and all vertices from t to x are out of An(u) ∪ An(v). Then the edgess–t and x–y must be oriented as s → t and x ← y, otherwise t or x belongs to an(u) ∪ an(v). Thus there exists at leastone collider between s and y on l. The middle vertex w of the collider closest to s between s and y is not contained inan(u) ∪ an(v), and any descendant of w is not in an(u) ∪ an(v), otherwise there is a directed cycle. So l is d-separatedby the collider, and it cannot be made active conditionally on any vertex in S where S ⊆ an(u) ∪ an(v). (cid:1)X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439437Lemma A.4. Let T be a d-separation tree for a DAG (cid:2)GV and C a node of T . Let u and v be two vertices in C whichare non-adjacent in (cid:2)GV . If u and v are not contained in the same separator connecting C, then there exists a subsetof C which d-separates u and v in (cid:2)GV .Proof. Define S = (an(u)∪an(v))∩C. We show below that u and v are d-separated by S, that is, every path between uand v in (cid:2)GV is d-separated by S.If l is not contained in An(u) ∪ An(v), then we obtain from Lemma A.3 that l is d-separated by S.When l is contained in An(u) ∪ An(v), let x and y be two vertices that are closest to u on l, that is, l =(u, x, y, . . . , v). The path l can have the following possible cases:(1) u ← x and x ∈ C,(2) u → x → y and x ∈ C,(3) u → x ← y and both x and y ∈ C,(4) u → x ← y, x ∈ C but y /∈ C, and(5) x /∈ C.For cases (1) and (2), since x is contained in C and u–x–y is not a collider, the path l is d-separated by x which iscontained in S. Thus l is d-separated by S.For case (3), we have from x ∈ An(u) ∪ An(v) that y must not be v, otherwise there exists a directed cycle in (cid:2)GV .Since y is not the middle vertex of a collider on l and y ∈ C, the path l is d-separated by y which is contained in S.Thus l is d-separated by S.For case (4), let C(cid:11) ((cid:5)= C) be the node of T which contains y. Since y ∈ C(cid:11) but v ∈ C, there is a sub-path l(cid:11)from y to v that passes through a separator K connecting C toward C(cid:11), which d-separates C \ K from C(cid:11) \ K. FromK ⊆ C and l(cid:11) ⊆ An(u) ∪ An(v), we have that [K ∩ l(cid:11)] ⊆ [C ∩ (an(u) ∪ an(v))] = S. Since y /∈ C and vertices xand y are adjacent in (cid:2)GV , we have that x ∈ K. Notice that u → x ← y is a collider, thus we know u ∈ K from thedefinition of the d-separation tree. By the assumption that u and v are not in the same separator connecting C, we havev ∈ C \ K. Since y ∈ C(cid:11) \ C ⊂ C(cid:11) \ K and v ∈ C \ K, by Lemma A.2, we obtain that the sub-path l(cid:11) is d-separated byK ∩ (l(cid:11) \ {y, v}) and by S. Thus l is d-separated by S.For case (5), let C(cid:11) ((cid:5)= C) be the node of T which contains x. Similar to case (4), we can show the result. (cid:1)Lemma A.5. Let T be a d-separation tree for a DAG (cid:2)GV . For any vertex u there exists at least one node of T whichcontains u and pa(u).Proof. If pa(u) is empty, it is trivial. Otherwise let C denote the node of T which contains u and the most parentsof u.Since no set can separate u from a parent, there must be a node of T that contains u and the parent. If u has onlyone parent, then we obtained the lemma. If u has two or more parents, we suppose, by reduction to absurdity, thatu has two parents v and w which are not contained in a single node but are contained in two different nodes of T ,say {u, v} ⊆ C and {u, w} ⊆ C(cid:11) respectively, since all vertices in V appear in T . On the path from C to C(cid:11) in T , allseparators must contain u, otherwise they cannot separate C from C(cid:11). However any separator containing u cannotseparate v and w. Thus we got a contradiction. (cid:1)Lemma A.6. Let T be a d-separation tree for a DAG (cid:2)GV and C a node of T . Let u and v be two vertices in C whichare non-adjacent in (cid:2)GV . If u and v are contained in the same separator S connecting C in T , then there exists anode C(cid:11) of T containing u, v and a set S(cid:11) such that S(cid:11) d-separates u and v in (cid:2)GV .Proof. Without loss of generality, we can suppose that v is not a descendant of u in (cid:2)GV . By Lemma A.5, there is anode C1 of T that contains u and pa(u). Let C2 contains both u and v, and the distance d(C1, C2) is minimum.If C1 and C2 are the same node of T , then S(cid:11) defined as the parents of u separates u from v.If C1 and C2 are different nodes, then d(C1, C2) (cid:1) 1, and there is at least one parent p of u that is not containedin C2. Thus there is a separator K connecting C2 toward C1 in T such that K d-separates p from all vertices inC2 \ K. Since u and p are adjacent in (cid:2)GV and the distance d(C1, C2) (cid:1) 1 is minimum, we have u ∈ K but v /∈ K (if438X.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439v ∈ K, then d(C1, C2) is not minimum). For every parent p(cid:11) of u that is contained in C1 but not in C2, we can showin the same way as p that K also separates p(cid:11) from all vertices in C2 \ K.Define S(cid:11) = (An(u) ∪ An(v)) ∩ C2. Similar to proof of Lemma A.4, we show below that every path from u to vin (cid:2)GV is d-separated by S(cid:11), that is, u and v are d-separated by S(cid:11).If a path l between u and v is not contained in An(u) ∪ An(v), then we obtain from Lemma A.3 that l is d-separatedby S(cid:11).When l is contained in An(u) ∪ An(v), let x be adjacent to u on l, that is, l = (u, x, . . . , v). Because l is containedin An(u) ∪ An(v) and v is not a descendant of u, the edge between u and x must be oriented as u ← x, that is, x is aparent of u.If x is contained in C2, then l is d-separated by x which is contained in S(cid:11).If the parent x of u is not contained in C2, as shown above, we have that x and v are d-separated by K. ByLemma A.2, we can obtain that the sub-path l(cid:11) from x to v can be d-separated by W ∩ K where W denotes the set ofall vertices between x and v (not containing x and v) on l(cid:11). Since S(cid:11) ⊇ (W ∩ K), we get from Lemma A.2 that l(cid:11) isalso d-separated S(cid:11). Hence the path l is d-separated by S(cid:11). (cid:1)Lemma A.7. Let T be a d-separation tree for a DAG (cid:2)GV . If two vertices u and v have distance d(u, v) > 0 in Twhere Ci and Cj are minimizers of u and v in T , then any separator on the path between Ci and Cj in T d-separatesu and v in (cid:2)GV .Proof. For any separator S on the path between Ci and Cj in T , we have by definition of d(u, v) that neither u norv can be contained in S, otherwise Ci and Cj are not minimizers of u and v. Since Ci and Cj belong to differentsubtrees obtained by removing the edge with the separator S attached, we know that u and v are d-separated by S. (cid:1)Proof of Theorem 1. Suppose that u and v are two non-adjacent vertices in (cid:2)GV . If d(u, v) > 0 in T , then we getfrom Lemma A.7 that u and v are non-adjacent in (cid:2)GV . If d(u, v) = 0, then we have from Lemmas A.4 and A.6 thatthere exists a node C of T that contains u, v and a subset S such that S d-separates u and v in (cid:2)GV . (cid:1)Proof of Theorem 2. From [5, p. 53] , we have that any separator S in the junction tree T separates V1 \ S and V2 \ Sin the triangulated graph ¯GtV , where Vi denotes the variable set of the subtree Ti induced by removing the edge witha separator S attached, for i = 1 and 2. Since the edge set of ¯GtV contains that of ¯GV , V1 \ S and V2 \ S are alsoseparated in ¯GV . Further, since ¯GV is an undirected independence graph for (cid:2)GV , we obtain immediately that T is ad-separation tree for (cid:2)GV . (cid:1)Proof of Theorem 3. Since a moral graph for a DAG (cid:2)GV is an undirected independence graph, by definition ofan undirected independence graph, we only need to show that ¯G defined in Theorem 3 contains all edges of (cid:2)GmV . Itis obvious that E contains all edges obtained by dropping directions of directed edges in (cid:2)GV since any set cannotd-separate two vertices that are adjacent in (cid:2)GV .Now we show that E also contains any moral edge that connects vertices u and v having a common child w, thatis, (u, v) ∈ E. We know from Lemma A.5 that there is at least one node Ch in a d-separation tree T that contains u,v and w. The undirected subgraph ¯Gh for Ch must contain edges u–w and v–w, that is, ¯Gh has an undirected pathu–w–v. By definition of an undirected independence graph, w must be contained in a separator to separate u from vin this undirected subgraph, but u and v cannot be d-separated conditional on any set containing w since there is acollider u → w ← v. Thus ¯Gh contains the moral edge (u, v). (cid:1)A.2. Proofs of algorithms’ correctnessProof of Algorithm 1’s correctness. By the sufficiency of Theorem 1(1), the initializations at steps 2 and 3 forcreating edges guarantee that no edge is created between any two variables which are not in the same node of thed-separation tree. By the sufficiency of Theorem 1(2), deleting edges at steps 2 and 3 guarantees that any other edgebetween two d-separated variables can be deleted in some local skeleton. Thus the global skeleton obtained at step 3 iscorrect. According to the necessity of Theorem 1, we have that each moral edge (u, v) in the undirected independenceX.C. Xie et al. / Artificial Intelligence 170 (2006) 422–439439graph must be deleted at some subgraph over a node of the d-separation tree since a separator of a d-separationtree cannot separate u and v of a v-structure u → w ← v. Thus we can determine all v-structures at step 4, whichcompletes our proof. (cid:1)Proof of Algorithm 2’s correctness. Each complete undirected subgraph ¯Gh describes a saturated model over Chand the entire graph ¯GV obtained at step 3 represents all dependencies from the prior knowledge. The triangulation atstep 4 does not bring any new conditional independence. Thus the junction tree is a d-separation tree. (cid:1)Proof of Main Algorithm’s correctness. The correctness of Algorithms 1 and 2 has been proved above. Thus weonly need to prove that step 3 is correct for obtaining a d-separation tree. According to Theorem 3, we have that theentire undirected independence graph is constructed correctly at the step 3. Then the d-separation tree can be obtainedcorrectly by using an algorithm for constructing a junction tree. (cid:1)References[1] I. Beinlich, H. Suermondt, R. Chavez, G. Cooper, The ALARM monitoring system: A case study with two probabilistic inference techniquesfor belief networks, in: Proceedings of the 2nd European Conference on Artificial Intelligence in Medicine, Springer-Verlag, Berlin, 1989,pp. 247–256.[2] A. Becker, D. Geiger, A sufficiently fast algorithm for finding close to optimal clique trees, Artificial Intelligence 125 (2001) 3–17.[3] C. Beeri, R. Fagin, D. Maier, M. Yannakakis, On the desirability of acyclic database schemes, J. ACM 30 (1983) 479–513.[4] C. Berge, Graphs and Hypergraphs, second ed., North-Holland, Amsterdam, 1976.[5] R.G. Cowell, A.P. David, S.L. Lauritzen, D.J. Spiegelhalter, Probabilistic Networks and Expert Systems, Springer Publications, New York,1999.[6] D.R. Cox, N. Wermuth, Multivariate Dependencies: Models, Analysis, and Interpretation, Chapman and Hall, London, 1993.[7] D. Danks, Learning the causal structures of overlapping variable sets, in: Proceedings of the 5th International Conference on DiscoveryScience, Springer-Verlag, Berlin, 2002, pp. 178–191.[8] D. Edwards, Introduction to Graphical Modelling, Springer-Verlag, New York, 1995.[9] R. Fung, S. Crawford, CONSTRUCTOR—a system for the induction of probabilistic models, in: Proceedings of the Eighth National Confer-ence on AI, American Association for Artificial Intelligence, Boston, 1990, pp. 762–779.[10] Z. Geng, Decomposability and collapsibility for log-linear models, Appl. Stat. 38 (1) (1989) 189–197.[11] Z. Geng, C. Wang, Q. Zhao, Decomposition of search for v-structures in DAGs, J. Multivariate Anal. (2005), submitted for publication.[12] D. Heckerman, A tutorial on learning with Bayesian networks, in: M.I. Jordan (Ed.), Learning in Graphical Models, Kluwer Academic,Dordrecht, 1998, pp. 301–354.[13] F.V. Jensen, F. Jensen, Optimal junction trees, in: Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence, MorganKaufmann, San Fransisco, CA, 1994, pp. 360–366.[14] D. Koller, M. Sahami, Toward optimal feature selection, in: Proceedings of the 13th International Conference on Machine Learning, Bari,Italy (1996) pp. 284–292.[15] S.L. Lauritzen, Graphical Models, Clarendon Press, Oxford, 1996.[16] R.J.A. Little, D.B. Rubin, Statistical Analysis with Missing Data, second ed., Wiley, New York, 2002.[17] C. Meek, Causal inference and causal explanation with background knowledge, in: Proceedings of the 11th Conference on Uncertainty inArtificial Intelligence, Morgan Kaufmann, San Francisco, CA, 1995, pp. 403–410.[18] J. Pearl, Probabilistic Reasoning in Intelligent Systems, Morgan Kaufmann, San Mateo, CA, 1988.[19] J. Pearl, Causality, Cambridge University Press, Cambridge, 2000.[20] S. Rassler, Statistical Matching, Lecture Notes in Statistics, vol. 168, Springer, New York, 2002.[21] N. Roberston, P.D. Seymour, Graph minors XIII. The disjoint paths problems, J. Combin. Theory B 53 (1995) 65–100.[22] D. Rose, R. Tarjan, G. Lueker, Algorithmic aspects of vertex elimination on graphs, SIAM J. Comput. 5 (1976) 266–283.[23] P. Spirtes, C. Glymour, An algorithm for fast recovery of sparse causal graphs, Social Sci. Comput. Rev. 9 (1991) 62–72.[24] P. Spirtes, C. Glymour, R. Scheines, Causation, Prediction and Search, second ed., MIT Press, Cambridge, MA, 2000.[25] R.E. Tarjan, M. Yannakakis, Simple linear-time algorithm to test chordality of graphs, test acyclicity of hypergraphs, and selective reduceacyclic hypergraphs, SIAM J. Comput. 13 (1984) 566–579.[26] I. Tsamardinos, C. Aliferis, A. Statnikov, Time and sample efficient discovery of Markov blankets and direct causal relations, in: Proceedingsof the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, DC, 2003, pp. 673–678.[27] T. Verma, J. Pearl, Equivalence and synthesis of causal models, in: Proceedings of the 6th Conference on Uncertainty in Artificial Intelligence,Elsevier, Amsterdam, 1990, pp. 255–268.