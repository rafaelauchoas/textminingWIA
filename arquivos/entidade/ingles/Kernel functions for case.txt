Artificial Intelligence 174 (2010) 1369–1406Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintKernel functions for case-based planningIvan SerinaFree University of Bozen-Bolzano, Viale Ratisbona, 16, I-39042 Bressanone, Italya r t i c l ei n f oa b s t r a c tArticle history:Received 5 September 2008Received in revised form 14 July 2010Accepted 16 July 2010Available online 30 July 2010Keywords:Case-based planningDomain-independent planningCase-based reasoningHeuristic search for planningKernel functionsCase-based planning can take advantage of former problem-solving experiences by storingin a plan library previously generated plans that can be reused to solve similar planningproblems in the future. Although comparative worst-case complexity analyses of plangeneration and reuse techniques reveal that it is not possible to achieve provable efficiencygain of reuse over generation, we show that the case-based planning approach can be aneffective alternative to plan generation when similar reuse candidates can be chosen.In this paper we describe an innovative case-based planning system, called OAKplan,which can efficiently retrieve planning cases from plan libraries containing more than tenthousand cases, choose heuristically a suitable candidate and adapt it to provide a goodquality solution plan which is similar to the one retrieved from the case library.Given a planning problem we encode it as a compact graph structure, that we callPlanning Encoding Graph, which gives us a detailed description of the topology of theplanning problem. By using this graph representation, we examine an approximate retrievalprocedure based on kernel functions that effectively match planning instances, achievingextremely good performance in standard benchmark domains.The experimental results point out the effect of the case base size and the importanceof accurate matching functions for global system performance. Overall, we show thatOAKplan is competitive with state-of-the-art plan generation systems in terms of numberof problems solved, CPU time, plan difference values and plan quality when cases similarto the current planning problem are available in the plan library.© 2010 Elsevier B.V. All rights reserved.1. IntroductionPlanning is a process which usually involves the use of a lot of resources. The efficiency of planning systems can beimproved by avoiding repeating the planning effort whenever it is not strictly necessary. For example this can be donewhen the specification of the goals undergoes a variation during plan execution or execution time failures turn up: it isthen advisable to change the existing plan rather than replanning from scratch. One might even think of basing the wholeplanning process on the modification of plans, a procedure also known as planning from second principles [47]. In fact thismethod does not generate a plan from scratch, but aims at exploiting the knowledge contained in plans that were generatedbefore. The current problem instance Π is thus employed to search for a plan in a library that, maybe after a number ofchanges, might turn out useful to solve Π .In Case-Based Planning (CBP), previously generated plans are stored as cases in memory and can be reused to solvesimilar planning problems in the future. CBP can save considerable time over planning from scratch, thus offering a po-tential (heuristic) mechanism for handling intractable problems. Similarly to other Case-Based Reasoning (CBR) systems,CBP is based on two assumptions on the nature of the world [38]. The first assumption is that the world is regular: sim-E-mail address: ivan.serina@unibz.it.0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.07.0071370I. Serina / Artificial Intelligence 174 (2010) 1369–1406ilar problems have similar solutions; as a consequence, solutions for similar problems are a useful starting point for newproblem-solving. The second assumption is that the types of problems an agent encounters tend to recur; hence futureproblems are likely to be similar to current problems.Different case-based planners differ on how they store cases, how they adapt a solution to a new problem, whether theyuse one or more cases for building a new solution or not, etc. [58]. From a theoretical point of view, in the worst case,adapting an existing plan to solve a new problem is not more efficient than a complete regeneration of the plan [47]. More-over finding a good reuse candidate in a plan library may be already very expensive, because it leads to more computationalcosts than those that can be saved by reusing the candidate. In fact, the retrieval of a good plan from a library of plansrepresents a serious bottleneck for plan reuse in domain independent case-based planning systems. This happens becausethe problem of defining the best matching among the objects of two planning problems is NP-hard.In this paper we present some data structures and new matching functions that efficiently address the problem ofmatching planning instances, which is NP-hard in the general case. These functions lead to a new case-based plannercalled OAKplan (acronym of Object Assignment Kernel case-based planner), which is competitive with state of the art plangeneration systems when sufficiently similar reuse candidates can be chosen.Following the formalisation proposed by Liberatore [39], a planning case is a pair (cid:2)Π0, π0(cid:3), where Π0 is a planningproblem and π0 is a plan for it, while a plan library is a set of cases {(cid:2)Πi, πi(cid:3) | 1 (cid:2) i (cid:2) m}. Our approach is based on acompact graph representation which uses the initial and goal facts in order to define a detailed description of the topologyof the planning problem examined. On the basis of this graph representation we use ideas from different research areas. Inparticular a lot of work has been done in molecular biology to analyse efficiently chemical databases which typically containthousands of molecules encoded as graphs. Similarly to the rascal system [51], we use graph degree sequences [55] in orderto filter out unpromising planning cases and reduce the set Cds = {(cid:2)Πi, πi(cid:3)} of cases that have to be examined accuratelyup to a suitable number.1Following Nebel and Koehler’s formalisation of matching functions [47], we examine the problem of defining a matchbetween the objects of the current planning problem and those of the selected planning cases. Since an exact matchingevaluation is infeasible from a computational point of view even for a limited number of candidate cases [47], we developan approximate evaluation based on kernel functions [56] to define a match among the objects of the planning problemsconsidered. Our kernel functions are inspired by Fröhlich et al.’s work [19–21] on kernel functions for molecular structures,where a kernel function can be thought of as a special similarity measure that can be defined among arbitrarily structuredobjects, like vectors, strings, trees or graphs [35,65]. The computational attractiveness of kernel methods comes from thefact that they can be applied in high-dimensional feature spaces without suffering the high cost of explicitly computing themapped data [56].In contrast to other CBP approaches that define exact matching functions among the objects of Π0 and those of theplan library whose computation requires exponential time [29,34,47], our kernel functions can compute in polynomial timean approximate matching function for each element of the set Cds; this matching function can choose a subset of thecandidate plans efficiently for the successive plan evaluation phase. These plans are evaluated accurately through a simulatedexecution that determines the capacity of a plan πi to solve the current planning problem. This phase is performed byexecuting πi and evaluating the presence of inconsistencies corresponding to the unsupported preconditions of the actionsof πi ; in the same way the presence of unsupported goals is identified. The best plan is then adapted, if necessary, in orderto be applicable to the current initial state and solve the current goals. This phase is based on the lpg-adapt system [16]which has shown excellent performance in many domains. When the adaptation phase is concluded, a new planning casecorresponding to the current planning problem and its solution plan can be inserted into the library or can be discarded.OAKplan can efficiently retrieve planning cases from plan libraries with more than ten thousand elements, heuristicallychoose a suitable candidate, possibly the best one, and adapt it to provide a good quality solution plan similar to the oneretrieved from the case base. We hope that this work will be able to renew interest in the case-based planning approach.Current research in planning has been devoted primarily to generative planning since no effective retrieval functions wereavailable in the past. To the best of our knowledge this is the first case-based planner that performs an efficient domain in-dependent objects matching evaluation. We examine it in comparison with state of the art plan generation systems showingthat the case-based planning approach can be an effective alternative to plan generation when “sufficiently similar” reusecandidates can be chosen. This is a major improvement on previous approaches on CBP which can only handle small planlibraries (see Section 5) and can hardly be compared with plan generation systems.The paper is organised as follows. Section 2 introduces the essential notions required by the paper. In particular weexpose the notion of union of graphs which is fundamental for the definition of the graphs used by our matching functionsand we introduce the basic concepts of kernel functions. Section 3 presents the main phases of our case-based plannerexamining the different steps required by the Retrieval and Evaluation phases in detail. In Section 4 a detailed analysis ofthe importance of an accurate matching function and of the case base size for the global system performance is provided.1 The rascal system uses degree sequences and a rigorous maximum common edge subgraph (MCES) detection algorithm based on a maximum cliqueformulation to compute the exact degree and composition of similarity in chemical databases. Our techniques use degree sequences and an approximatemaximum common subgraph (MCS) detection algorithm based on kernel functions to analyse the cases of huge plan libraries and choose a good candidatefor adaptation.I. Serina / Artificial Intelligence 174 (2010) 1369–14061371Then we examine the results produced by OAKplan in comparison with four state of the art plan generation systems. Finally,Section 6 gives the conclusions and indicates future work.2. PreliminariesIn the following we present some notation that will be used in the paper with an analysis of the computational com-plexity of the problems considered.2.1. Planning formalismSimilarly to Bylander’s work [8], we define an instance of propositional planning as:Definition 1. A propositional STRIPS planning problem is a tuple Π = (cid:2)Pr, I, G, O p(cid:3) where:• Pr is a finite set of ground atomic propositional formulae;• I ⊆ Pr is the initial state;• G ⊆ Pr is the goal specification;• O p is a finite set of operators, where each operator o ∈ O p has the form o p ⇒ o+, o− such that– o p ⊆ Pr are the propositional preconditions,– o+ ⊆ Pr are the positive postconditions (add list),– o− ⊆ Pr are the negative postconditions (delete list)and o+ ∩ o− = ∅.We assume that the set of propositions Pr has a particular structure. Let O be a set of typed constants ci , with theunderstanding that distinct constants denote distinct objects (corresponding to individual entities following the ConceptualGraphs notation [11]). Let P be a set of predicate symbols, then Pr(O, P) is the set of all ground atomic formulae over thissignature. Note that we use a many-sorted logic formalisation since it can significantly increase the efficiency of a deductiveinference system by eliminating useless branches of the search space of a domain [12,13,67]. A fact is an assertion thatsome individual entities exist and that these entities are related by some relationships.(cid:9)1, . . . , a(cid:9)m), a precondition f of a plan action a(cid:9)i is supported if (i) f(cid:9)k with j (cid:2) k < i or (ii) f is true in the initial state and (cid:2)aA plan π is a partially ordered sequence of actions π = (a1, . . . , am, C), where ai is an action (completely instantiatedoperator) of π and C defines the ordering relations between the actions of π . A linearisation of a partially ordered planis a total order over the actions of the plan that is consistent with the existing partial order. In a totally ordered plan(cid:9)π = (aj and not deleted(cid:9)(cid:9)k with k < i s.t. f ∈ del(ak). In a partiallyby an intervening action aordered plan, a precondition of an action is possibly supported if there exists a linearisation in which it is supported, whilean action precondition is necessarily supported if it is supported in all linearisations. An action precondition is necessarilyunsupported if it is not possibly supported. A valid plan is a plan in which all action preconditions are necessarily supported.The complexity of STRIPS planning problems has been studied extensively in the literature. Bylander [8] has definedPLANSAT as the decision problem of determining whether an instance Π of propositional STRIPS planning has a solution ornot. PLANMIN is defined as the problem of determining if there exists a solution of length k or less, i.e., it is the decisionproblem corresponding to the search problem of generating plans with minimal length. Based on this framework, he hasanalysed the computational complexity of a general propositional planning problem and a number of generalisations andrestricted problems. In its most general form, both PLANSAT and PLANMIN are PSPACE-complete. Severe restrictions on theform of the operators are necessary to guarantee polynomial time or even NP-completeness [8].is added by an earlier action aIt is important to remark that our approach is more related to the generative case-based planning approach than to thetransformational approach, in that the plan library is only assumed to be used when it is useful in the process of searchingfor a new plan, and is not necessarily used to provide a starting point of the search process. If a planning case (cid:2)Π0, π0(cid:3) isalso known, the current planning problem Π cannot become more difficult, as we can simply disregard the case and find theplan using Π only. Essential to this trivial result is that, similarly to most modern plan adaptation and case-based planningapproaches [2,28,61,62], we do not enforce plan adaptation to be conservative, in the sense that we do not require to reuseas much of the starting plan π0 to solve the new plan. The computational complexity of plan Reuse and Modification forSTRIPS planning problems has been analysed in a number of papers [8,9,36,39,47]. While a problem cannot be made moredifficult by the presence of a hint, which corresponds in our context to the solution of a planning case, it may becomeeasier. Unfortunately the following theorem shows that this is not the case for plan adaptation.Theorem 1. (See [39].) Deciding whether there exists a plan for a STRIPS instance Π , given a case (cid:2)Π0, π0(cid:3), is PSPACE-complete, evenif Π0 and Π only differ on one condition of the initial state.Moreover empirical analyses show that plan modification for similar planning instances is somewhat more efficient thanplan generation in the average case [7,16,27,28,47,59,62].13722.2. GraphsI. Serina / Artificial Intelligence 174 (2010) 1369–1406Graphs provide a rich means for modelling structured objects and they are widely used in real-life applications torepresent molecules, images, or networks. On a very basic level, a graph can be defined by a set of entities and a set ofconnections between these entities. More formally:Definition 2. A labeled graph G is a 3-tuple G = (V , E, λ), in which• V is the set of vertices,• E ⊆ V × V is the set of directed edges or arcs,• λ : V ∪ E → ℘s(Lλ) is a function assigning labels to vertices and edges;where Lλ is a finite set of symbolic labels and ℘s(Lλ) represents the set of all the multisets on Lλ. Note that our labelfunction considers multisets of symbolic labels, with the corresponding operations of union, intersection and join [5], sincein our context they are more suitable than standard sets of symbolic labels in order to compare vertices or edges accuratelyas described later. The above definition corresponds to the case of directed graphs; undirected graphs are obtained if werequire for each edge [v 1, v 2] ∈ E the existence of an edge [v 2, v 1] ∈ E with the same label. |G| = |V | + |E| denotes the sizeof the graph G, while an empty graph such that |G| = 0 will be denoted by ∅. An arc e = [v, u] ∈ E is considered to bedirected from v to u; v is called the source node and u is called the target node of the arc; u is said to be a direct successorof v, v is said to be a direct predecessor of u, while v is said to be adjacent to the vertex u and vice versa.Here we present the notion of graph union which is essential for the definition of the graphs used by our matchingfunctions:Definition 3. The union of two graphs G 1 = (V 1, E1, λ1) and G 2 = (V 2, E2, λ2), denoted by G 1 ∪ G 2, is the graph G =(V , E, λ) defined by• V = V 1 ∪ V 2,• E = E1 ∪ E2,⎧⎨• λ(x) =⎩λ1(x)λ2(x)λ1(x) (cid:13) λ2(x) otherwiseif x ∈ (V 1\V 2) ∨ x ∈ (E1\E2),if x ∈ (V 2\V 1) ∨ x ∈ (E2\E1),where (cid:13) indicates the join, sometimes called sum, of two multisets [5], while λ(·) associates a multiset of symbolic labelsto a vertex or to an edge.In many applications it is necessary to compare objects represented as graphs and determine the similarity amongthese objects. This is often accomplished by using graph matching, or isomorphism techniques. Graph isomorphism can beformulated as the problem of identifying a one-to-one correspondence between the vertices of two graphs such that anedge only exists between two vertices in one graph if an edge exists between the two corresponding vertices in the othergraph. Graph matching can be formulated as the problem involving the maximum common subgraph (MCS) between thecollection of graphs being considered. This is often referred to as the maximum common substructure problem and denotesthe largest substructure common to the collection of graphs under consideration. More precisely:Definition 4. Two labeled graphs G = (V , E, λ) and Gf : V → Vsuch that(cid:9)(cid:9) = (V(cid:9), E(cid:9), λ(cid:9)) are isomorphic if there exists a bijective function• ∀v ∈ V , λ(v) = λ(cid:9)( f (v)),• ∀[v 1, v 2] ∈ E, λ([v 1, v 2]) = λ(cid:9)([ f (v 1), f (v 2)]),• [u, v] ∈ E if and only if [ f (u), f (v)] ∈ E.(cid:9)We shall say that fis an isomorphism function.Definition 5. An Induced Subgraph of a graph G = (V , E, λ) is a graph S = (V(cid:9), E(cid:9), λ(cid:9)) such that(cid:9) ⊆ V and ∀v ∈ V(cid:9)(cid:9) ⊆ E and ∀e ∈ E(cid:9)• V• E• ∀v, u ∈ V, [v, u] ∈ E(cid:9)(cid:9), λ(cid:9)(v) ⊆ λ(v),, λ(cid:9)(e) ⊆ λ(e),if and only if [v, u] ∈ E.I. Serina / Artificial Intelligence 174 (2010) 1369–14061373A graph G is a Common Induced Subgraph (CIS) of graphs G 1 and G 2 if G is isomorphic to induced subgraphs of G 1 and G 2.A common induced subgraph G = (V , E, λ) of G 1 and G 2 is called Maximum Common Induced Subgraph (MCIS) if there exists|λ(v)| greater than G. Similarly, a common induced subgraphno other common induced subgraph of G 1 and G 2 withv∈VG = (V , E, λ) of G 1 and G 2 is called Maximum Common Edge Subgraph (MCES), if there exists no other common induced|λ(e)| greater than G. Note that, since we are considering multiset labeled graphs,subgraph of G 1 and G 2 withwe require a stronger condition than standard MCIS and MCES for labeled graph, in fact we want to maximise the totalcardinality of the multiset labels of vertices/edges involved instead of the simple number of vertices/edges.e∈E(cid:5)(cid:5)As it is well known, subgraph isomorphism and MCS between two or among more graphs are NP-complete problems [22],while it is still an open question if also graph isomorphism is an NP-complete problem. As a consequence, worst-case timerequirements of matching algorithms increase exponentially with the size of the input graphs, restricting the applicabilityof many graph based techniques to very small graphs.2.3. Kernel functions for labeled graphsIn recent years, a large number of graph matching methods based on different matching paradigms have been proposed,ranging from the spectral decomposition of graph matrices to the training of artificial neural networks and from continuousoptimisation algorithms to optimal tree search procedures.The basic limitation of graph matching is due to the lack of any mathematical structure in the space of graphs. Kernelmachines, a new class of algorithms for pattern analysis and classification, offer an elegant solution to this problem [56]. Thebasic idea of kernel machines is to address a pattern recognition problem in a related vector space instead of the originalpattern space. That is, rather than defining mathematical operations in the space of graphs, all graphs are mapped into avector space where these operations are readily available. Obviously, the difficulty is to find a mapping that preserves thestructural similarity of graphs, at least to a certain extent. In other words, if two graphs are structurally similar, the twovectors representing these graphs should be similar as well, since the objective is to obtain a vector space embedding thatpreserves the characteristics of the original space of graphs.A key result from the theory underlying kernel machines states that an explicit mapping from the pattern space into avector space is not required. Instead, from the definition of a kernel function it follows that there exists such a vector spaceembedding and that the kernel function can be used to extract the information from vectors that is relevant for recognition.As a matter of fact, the family of kernel machines consists of all the algorithms that can be formulated in terms of such akernel function, including standard methods for pattern analysis and classification such as principal component analysis andnearest-neighbour classification. Hence, from the definition of a graph similarity measure, we obtain an implicit embeddingof the entire space of graphs into a vector space.A kernel function can be thought of as a special similarity measure with well defined mathematical properties [56].Considering the graph formalism, it is possible to define a kernel function which measures the degree of similarity betweentwo graphs. Each structure could be represented by means of its similarity to all the other structures in the graph space.Moreover a kernel function implicitly defines a dot product in some space [56]; i.e., by defining a kernel function betweentwo graphs we implicitly define a vector representation of them without the need to explicitly know about it.(cid:9) ∈ X it holds that k(x, xFrom a technical point of view a kernel function is a special similarity measure k : X × X → R between patterns lyingin some arbitrary domain X , which represents a dot product, denoted by (cid:2)·,·(cid:3), in some Hilbert space H [56]; thus, for two(cid:9))(cid:3), where φ : X → H is an arbitrary mapping of patterns fromarbitrary patterns x, xthe domain X into the feature space H. In principle the patterns in domain X do not necessarily have to be vectors; theycould be strings, graphs, trees, text documents or other objects. The vector representation of these objects is then given bythe map φ. Instead of performing the expensive transformation step explicitly, the kernel can be calculated directly, thusperforming the feature transformation only implicitly: this is known as kernel trick. This means that any set, whether alinear space or not, that admits a positive definite kernel can be embedded into a linear space.(cid:9)) = (cid:2)φ(x), φ(xMore specifically, kernel methods manage non-linear complex tasks making use of linear methods in a new space. Forinstance, take into consideration a classification problem with a training set S = {(u1, y1), . . . , (un, yn)}, (ui, yi) ∈ X × Y , fori = 1, . . . , n, where X is an inner-product space (i.e. Rd) and Y = {−1, +1}. In this case, the learning phase corresponds tofrom the training set S by associating a class y ∈ Y to a pattern u ∈ X so that the generalisationbuilding a function f ∈ Yerror of fis as low as possible.A functional form for f consists in the hyperplane f (u) = sign((cid:2)w, u(cid:3) + b), where sign(·) refers to the function returningthe sign of its argument. The decision function f produces a prediction that depends on which side of the hyperplane(cid:2)w, u(cid:3) + b = 0 the input pattern u lies. The individuation of the best hyperplane corresponds to a convex quadratic optimi-sation problem in which the solution vector w is a linear combination of the training vectors:Xw =n(cid:6)i=1αi yi ui,for some αi ∈ R+, i = 1, . . . , n.In this way the linear classifier f may be rewritten as(cid:8)f (u) = signαi yi(cid:2)ui, u(cid:3) + b.(cid:7)n(cid:6)i=11374I. Serina / Artificial Intelligence 174 (2010) 1369–1406Fig. 1. The kernel approach for classification. Left: non-linearly separable input provided by dots and crosses. Middle: perfect or approximate linear-separability can be achieved in feature space via the mapping φ. Right: linear decision surface in feature space defines a complex decision surface ininput space.As regards complex classification problems, the set of all possible linear decision surfaces might not be rich enough in or-der to provide a good classification, independently from the values of the parameters w ∈ X and b ∈ R (see Fig. 1). The aimof the kernel trick is that of overcoming this limitation by adopting a linear approach to transformed data φ(u1), . . . , φ(un)rather than raw data. Here φ indicates an embedding function from the input space X to a feature space H, provided witha dot product. This transformation enables us to give an alternative kernel representation of the data which is equivalentto a mapping into a high-dimensional space where the two classes of data are more readily separable. The mapping isachieved through a replacement of the inner product:(cid:2)ui, u(cid:3) →(cid:9)(cid:10)φ(ui), φ(u)and the separating function can be rewritten as:(cid:8)(cid:7)n(cid:6)αi yii=1(cid:10)(cid:9)φ(ui), φ(u)f (u) = sign+ b.(1)The main idea behind the kernel approach consists in replacing the dot product in the feature space using a kernelk(u, v) = (cid:2)φ(v), φ(u)(cid:3); the functional form of the mapping φ(·) does not actually need to be known since it is implicitlydefined by the choice of the kernel. A positive definite kernel [23] is:Definition 6. Let X be a set. A symmetric function k : X × X → R is a positive definite kernel function on X iff ∀n ∈ N,∀x1, . . . , xn ∈ X , and ∀c1, . . . , cn ∈ R(cid:6)cic jk(xi, x j) (cid:3) 0i, j∈{1,...,n}where N is the set of positive integers. For a given set S u = {u1, . . . , un}, the matrix K = (k(ui, u j))i, j is known as Grammatrix of k with respect to S u . Positive definite kernels are also called Mercer kernels.Theorem 2. (Mercer’s property). (See [44].) For any positive definite kernel function k ∈ RX ×Xthe feature space H equipped with the inner product (cid:2)·,·(cid:3)H, such that:, there exists a mapping φ ∈ HXinto∀u, v ∈ X ,k(u, v) =(cid:10)(cid:9)φ(u), φ(v)H.The kernel approach replaces all inner products in Eq. (1) and all related expressions to compute the real coefficients αiand b, by means of a Mercer kernel k. For any input pattern u, the relating decision function fis given by:(2)f (u) = signαi yik(ui, u) + b.(cid:8)(cid:7)n(cid:6)i=1This approach transforms the input patterns u1, . . . , un into the corresponding vectors φ(u1), . . . , φ(un) ∈ H through(cf. Mercer’s property, Theorem 2), and uses hyperplanes in the feature space H for the purpose ofi=1 ui v i of Rd is actually a Mercer kernel, while other commonly usedthe mapping φ ∈ HXclassification (see Fig. 1). The dot product (cid:2)u, v(cid:3) =(cid:5)dI. Serina / Artificial Intelligence 174 (2010) 1369–14061375Mercer kernels, like polynomial and Gaussian kernels, generally correspond to non-linear mappings φ into high-dimensionalfeature spaces H. On the other hand the Gram matrix implicitly defines the geometry of the embedding space and permitsthe use of linear techniques in the feature space so as to derive complex decision surfaces in the input space X .While it is not always easy to prove positive definiteness for a given kernel, positive definite kernels are characterisedby interesting closure properties. More precisely, they are closed under sum, direct sum, multiplication by a scalar, tensorproduct, zero extension, pointwise limits, and exponentiation [56].A remarkable contribution to graph kernels is the work on convolution kernels, that provides a general framework todeal with complex objects consisting of simpler parts [31]. Convolution kernels derive the similarity of complex objectsfrom the similarity of their parts. Given two kernels k1 and k2 over the same set of objects, new kernels may be built byusing operations such as convex linear combinations and convolutions. The convolution of k1 and k2 is a new kernel k withthe form(cid:6)k1 (cid:8) k2(u, v) ={u1,u2}=u; {v1,v2}=vk1(u1, v 1)k2(u2, v 2)where u = {u1, u2} refers to a partition of u into two substructures u1 and u2 [31,56]. The kind of substructures dependson the domain of course and could be, for instance, subgraphs or subsets or substrings in the case of kernels definedover graphs, sets or strings, respectively. Different kernels can be obtained by considering different classes of subgraphs(e.g. directed/undirected, labeled/unlabeled, paths/trees/cycles, deterministic/random walks) and various ways of listing andcounting them [35,48,49]. The consideration of space and time complexity so as to compute convolution/spectral kernels isimportant, owing to the combinatorial explosion linked to variable-size substructures.In the following section we present our Optimal Assignment Kernel as a symmetric and positive definite similarity measurefor directed graph structures and it will be used in order to define the correspondence between the vertices of two directedgraphs. For an introduction to kernel functions related concepts and notation, the reader is referred to Scholkopf and Smola’sbook [56].3. Case-based planningA case-based planning system solves planning problems by making use of stored plans that were used to solve analogousproblems. CBP is a type of case-based reasoning, which involves the use of stored experiences (cases); moreover there isstrong evidence that people frequently employ this kind of analogical reasoning [24,54,66]. When a CBP system solves anew planning problem, the new plan is added to its case base for potential reuse in the future. Thus we can say that thesystem learns from experience.In general the following steps are executed when a new planning problem must be solved by a CBP system:1. Plan Retrieval to retrieve cases from memory that are analogous to the current (target) problem (see Section 3.1 for adescription of our approach).2. Plan Evaluation to evaluate the new plans by execution, simulated execution, or analysis and choose one of them (seeSection 3.2).3. Plan Adaptation to repair any faults found in the new plan (see Section 3.3).4. Plan Revision to test the solution new plan π for success and repair it if a failure occurs during execution (see Sec-tion 3.4).5. Plan Storage to eventually store π as a new case in the case base (see Section 3.4).In order to realise the benefits of remembering and reusing past plans, a CBP system needs efficient methods for re-trieving analogous cases and for adapting retrieved plans together with a case base of sufficient size and coverage to yielduseful analogues. The ability of the system to search in the library for a plan suitable to adaptation2 depends both on theefficiency/accuracy of the implemented retrieval algorithm and on the data structures used to represent the elements of thecase base.Similarly to the Aamodt and Plaza’s classic model of the problem solving cycle in CBR [1], Fig. 2 shows the main steps ofour case-based planning cycle and the interactions of the different steps with the case base. In the following we illustratethe main steps of our case-based planning approach, examining the different implementation choices adopted.3.1. Plan retrievalAlthough the plan adaptation phase is the central component of a CBP system, the retrieval phase critically affects thesystem performance too. As a matter of fact the retrieval time is a component of the total adaptation time and the qualityof the retrieved plan is fundamental for the performance of the successive adaptation phase. With OAKplan a number2 A plan suitable to adaptation has an adaptation cost that is lower with respect to the other candidates of the case base and with respect to plangeneration.1376I. Serina / Artificial Intelligence 174 (2010) 1369–1406Fig. 2. The case-based planning cycle.of functions for the management of the plan library and matching functions for the selection of the candidate plan foradaptation have been implemented.The retrieval phase has to consider all the elements of the plan library in order to choose a good one that will allowthe system to solve the new problem easily. Hence it is necessary to design a similarity metric and reduce the number ofcases that must be evaluated accurately so as to improve the efficiency of the retrieval phase. Anyway the efficiency of aplan adaptation system is undoubtedly linked to the distance between the problem to solve and the plan to adapt. In orderto find a plan which is useful for adaptation we have to reach the following objectives:• The retrieval phase must identify the candidates for adaptation. The retrieval time should be as small as possible asit will be added to the adaptation time and so particular attention has been given to the creation of efficient datastructures for this phase.• The selected cases should actually contain the plans that are easier to adapt; since we assume that the world is regular,i.e. that similar problems have similar solutions, we look for the cases that are the most similar to the problem to solve(with respect to all the other candidates of the case base). In this sense, it is important to define a metric able to givean accurate measure of the similarity between the planning problem to solve and the cases of the plan library.To the end of applying the reuse technique, it is necessary to provide a plan library from which “sufficiently similar”reuse candidates can be chosen. In this case, “sufficiently similar” means that reuse candidates have a large number ofinitial and goal facts in common with the new instance. However, one may also want to consider the reuse candidates thatare similar to the new instance after the objects of the selected candidates have been systematically renamed. As a matterof fact, every plan reuse system should contain a matching component that tries to find a mapping between the objectsof the reuse candidate and the objects of the new instance such that the number of common goal facts is maximised andthe additional planning effort to achieve the initial state of the plan library is minimised. Following Nebel and Koehler’sformalisation [47], we will have a closer look at this matching problem.3.1.1. Object matchingAs previously said we use a many-sorted logic in order to reduce the search space for the matching process; moreover weassume that the operators are ordinary STRIPS operators using variables. If there are two instances(cid:9) =Π(cid:11)(cid:9)Pr(cid:9)(cid:9), PO(cid:12), I(cid:9), G(cid:9), O(cid:9)p(cid:10),(cid:9)Π =Pr(O, P), I, G, O p(cid:10)such that (without loss of generality)(cid:9) ⊆ O,O(cid:9) = P,PO(cid:9)p⊆ O pthen a mapping, or matching function, from Π (cid:9)to Π is a functionμ : O(cid:9) → O.The mapping is extended to ground atomic formulae and sets of such formulae in the canonical way, i.e.,I. Serina / Artificial Intelligence 174 (2010) 1369–14061377(cid:12)(cid:11)p(c1 : t1, . . . , cn : tn)μ(cid:14)(cid:12)(cid:11)(cid:13)(cid:11)= p(cid:13)=μ(c1) : t1, . . . , μ(cn) : tn(cid:11)(cid:11), . . . , μμ(cid:12)p1(..)(cid:12),(cid:12)(cid:14).μp1(..), . . . , pm(..)pm(..)If there exists a bijective matching function μ from Π (cid:9)(cid:9)) = I , then it is obvious thata solution plan π (cid:9)and Π are identical within a renaming of constantsymbols, i.e., μ(π (cid:9)) solves Π . Even if μ does not match all goal and initial-state facts, μ(π (cid:9)) can still be used as a startingpoint for the adaptation process that can solve Π .can be directly reused for solving Π since Π (cid:9)to Π such that μ(G(cid:9)) = G and μ(Ifor Π (cid:9)In order to measure the similarity between two objects, it is intuitive and usual to compare the features which arecommon to both objects [40]. The Jaccard similarity coefficient used in information retrieval is particularly interesting. Herewe examine an extended version that considers two pairs of disjoint sets:complete_similμ(cid:11)(cid:9)Π, Π(cid:12)=|μ(G(cid:9)) ∩ G| + |μ(I(cid:9)) ∩ I||μ(G(cid:9)) ∪ G| + |μ(I(cid:9)) ∪ I|.(3)In the following we present a variant of the previous function so as to overcome the problems related to the presenceof irrelevant facts in the initial state description of the current planning problem Π and additional goals that are presentin Π (cid:9). In fact while the irrelevant facts can be filtered out from the initial state description of the case-based planningproblem Π (cid:9), this is not possible for the initial state description of the currentplanning problem Π . Similarly, we do not want to consider possible “irrelevant” additional goals of G; this could happenwhen Π (cid:9)solves a more difficult planning problem with respect to Π . We define the following similarity function so as toaddress these issues:(cid:12)using the corresponding solution plan π (cid:9)(cid:9).(4)(cid:11)Π(cid:9), Π=similμ|μ(G(cid:9)) ∩ G| + |μ(I(cid:9)) ∩ I||G| + |μ(I(cid:9))|Using similμ we obtain a value equal to 1 when there exists a mapping μ s.t. ∀ f ∈ Is.t. g = μ(g, μ( f ) ∈ I (to guarantee the(cid:9)) (to guarantee the achievement of the goals of the current planning) and ∀g ∈ G, ∃g(cid:9) ∈ G(cid:9)(cid:9)applicability of π (cid:9)problem).Finally we define the following optimisation problem, which we call obj_match:Instance: Two planning instances, Π (cid:9)Question: Does a mapping μ from Π (cid:9)and Π , and a real number k ∈ [0, 1].to Π such that similμ(Π (cid:9), Π) = k exist and there is no mapping μ(cid:9)with similμ(cid:9) (Π (cid:9), Π) > k?from Π (cid:9)to ΠIt should be noted that this matching problem has to be solved for each potentially relevant candidate in the plan libraryto select the corresponding best reuse candidate. Of course, one may use structuring and indexing techniques to avoidconsidering all plans in the library. Nevertheless, it seems unavoidable solving this problem a considerable number of timesbefore an appropriate reuse candidate is identified. For this reason, the efficiency of the matching component is crucial forthe overall system performance. Unfortunately, similarly to Nebel and Koehler’s analysis [47], it is quite easy to show thatthis matching problem is an NP-hard problem.Theorem 3. obj_match is NP-hard.The proof of this theorem and of the following ones can be found in Appendix B. This NP-hardness result implies thatmatching may be indeed a bottleneck for plan reuse systems. As a matter of fact, it seems to be the case that planninginstances with complex goal or initial-state descriptions may not benefit from plan-reuse techniques because matching andretrieval are too expensive. In fact existing similarity metrics address the problem heuristically, considering approximationsof it [46,63]. However, this theorem is interesting because it captures the limit case for such approximations.We define a particular labeled graph data structure called Planning Encoding Graph which encodes the initial and goal factsof a single planning problem Π to perform an efficient matching between the objects of a planning case and the objects ofthe current planning problem. The vertices of this graph belong to a set VΠ whose elements are the representation of theobjects O of the current planning problem Π and of the predicate symbols P of Π :(cid:15)(cid:15)VΠ = O ∪I p ∪Gqp∈Pq∈Pi.e. for each predicate we define two additional nodes, one associated to the corresponding initial fact predicate called I pand the other associated to the corresponding goal fact predicate called Gq. The labels of this graph are derived from thepredicates of our facts and the sorts of our many-sorted logic. The representation of an entity (an object using planningterminology) of the application domain is traditionally called a concept in the conceptual graph community [11]. Followingthis notation a Planning Encoding Graph is composed of three kinds of nodes: concept nodes representing entities (objects)that occur in the application domain, initial fact relation nodes representing relationships that hold between the objects ofthe initial facts and goal fact relation nodes representing relationships that hold between the objects of the goal facts.1378I. Serina / Artificial Intelligence 174 (2010) 1369–1406Fig. 3. Initial Fact Encoding Graph E I (p) of the propositional initial fact p = p(c1 : t1, . . . , cn : tn).Fig. 4. Initial Fact Encoding Graph E I (on A B) of the propositional initial fact (on A B).The Planning Encoding Graph of a planning problem Π(I, G) is built using the corresponding initial and goal facts. Inparticular for each propositional initial fact p = p(c1 : t1, . . . , cn : tn) ∈ I we define a data structure called Initial Fact EncodingGraph which corresponds to a graph that represents p. More precisely:Definition 7. Given a propositional typed initial fact p = p(c1 : t1, . . . , cn : tn) ∈ I of Π , the Initial Fact Encoding GraphE I (p) = (V p, Ep, λp) of fact p is a directed labeled graph where• V p = {I p, c1, . . . , cn} ⊆ VΠ ;• Ep = {[I p, c1], [c1, c2], [c1, c3], . . . , [c1, cn], [c2, c3], [c2, c4], . . . , [cn−1, cn]} = [I p, c1] ∪• λp(I p) = {I p}, λp(ci) = {ti} with i = 1, . . . , n;• λp([I p, c1]) = {I 0,1}; ∀[ci, c j] ∈ Ep, λp([ci, c j]) = {I};i, jpp(cid:16)i=1,...,n; j=i+1,...,n[ci, c j];i.e. the first node of the graph E I (p), see Fig. 3, is the initial fact relation node I p labeled with the multiset λp(I p) ={(I p, 1)} = {I p},3 it is connected to a direct edge to the second node of the graph, the concept node c1, which is labeled bysort t1 (i.e. λp(c1) = {(t1, 1)} = {t1}); the node c1 is connected with the third node of the graph c2 which is labeled by sortt2 (i.e. λp(c2) = {(t2, 1)} = {t2}) and with all the remaining concept nodes, the third node of the graph c2 is connected withc3, c4, . . . , cn and so on. The first edge of the graph [I p, c1] is labeled by the multiset {I 0,1}, similarly a generici, jedge [ci, c j] ∈ Ep is labeled by the multiset {Ipp , 1} = {I 0,1}.pFor example, in Fig. 4 we can see the Initial Fact Encoding Graph of the fact “p = (on A B)” of the BlocksWorld domain.The first node is named as “Ion” and its label is the multiset λp(Ion) = {(Ion, 1)} = {Ion}, the second node represents theobject “A” with label λp( A) = {(Obj, 1)} = {Obj} and finally the third node represents the object “B” and its label is λp(B) ={Obj}; the label of the [Ion, A] arc is the multiset {(I 0,1on , 1)} ={I 1,2}.onSimilarly to Definition 7 we define the Goal Fact Encoding Graph E G (q) of the fact q = q(c} and the label of the [ A, B] arc is the multiset {(I 1,2on , 1)} = {I 0,1: t: ton(cid:9)1(cid:9)1, . . . , c(cid:9)m(cid:9)m) ∈ G using{Gq} for the labeling procedure.Given a planning problem Π with initial and goal states I and G, the Planning Encoding Graph of Π , that we indicateas EΠ , is a directed labeled graph derived by the encoding graphs of the initial and goal facts:(cid:15)(cid:15)EΠ (I,G) =E I (p) ∪E G (q)(5)p∈Iq∈Gi.e. the Planning Encoding Graph of Π(I, G) is a graph obtained by merging the Initial and Goal Fact Encoding Graphs. Forsimplicity in the following we visualise it as a three-level graph. The first level is derived from the predicate symbols of the3 In the following we indicate the multiset {(x, 1)} as {x} for sake of simplicity.I. Serina / Artificial Intelligence 174 (2010) 1369–14061379Fig. 5. Planning Encoding Graph for the Sussman anomaly planning problem in the BlocksWorld domain.initial facts, the second level encodes the objects of the initial and goal states and the third level shows the goal fact nodesderived from the predicate symbols of the goal facts.4Fig. 5 illustrates the Planning Encoding Graph for the Sussman anomaly planning problem in the BlocksWorld domain.The nodes of the first and third levels are the initial and goal fact relation nodes: the vertices Ion, Iclear and Ion-table arederived by the predicates of the initial facts, while G on by the predicates of the goal facts. The nodes of second level areconcept nodes which represent the objects of the current planning problem A, B and C , where the label “Obj” correspondsto their type. The initial fact “(on C A)” determines two arcs, one connecting Ion to the vertex C and the second connectingC to A; the labels of these arcs are derived from the predicate symbol “on” determining the multisets {I 0,1} and {I 1,2}ononrespectively. In the same way the other arcs are defined. Moreover since there is no overlapping among the edges of theInitial and Goal Fact Encoding Graphs, the multiplicity of the edge label multisets is equal to 1; on the contrary the labelmultisets of the vertices associated to the objects are:λ( A) =(cid:13)(cid:14)(Obj, 3),λ(B) =(cid:13)(cid:14)(Obj, 4)and λ(C) =(cid:13)(cid:14)(Obj, 3).This graph representation can give us a detailed description of the “topology” of a planning problem without requiringany a priori assumptions on the relevance of certain problem descriptors for the whole graph. Furthermore it allows us touse Graph Theory based techniques in order to define effective matching functions. In fact a matching function from Π (cid:9)to Π can be derived by solving the Maximum Common Subgraph problem on the corresponding Planning Encoding Graphs.A number of exact and approximate algorithms have been proposed in the literature so as to solve this graph problemefficiently. With respect to normal conceptual graphs [11] used for Graph-based Knowledge Representation, we use a richerlabel representation based on multisets. A single relation node is used to represent each predicate of the initial and goalfacts which reduces the total number of nodes in the graphs considerably. This is extremely important from a computationalpoint of view since, as we will see in the following sections, the matching process must be repeated several times and itdirectly influences the total retrieval time.In the following we examine a procedure based on graph degree sequences that is useful to derive an upper bound onthe size of the MCES of two graphs in an efficient way. Then we present an algorithm based on Kernel Functions that allowsto compute an approximate matching of two graphs in polynomial time.3.1.2. Screening procedureAs explained previously, the retrieval phase could be very expensive from a computational point of view; so we havedeveloped a screening procedure that can be used in conjunction with an object matching algorithm.Similarly to the rascal system [51], we use degree sequences [55] to calculate an upper bound on the size of a MaximumCommon Edge Subgraph (MCES) between a pair of graphs. Note that degree sequences of a graph have already been usedby other authors to establish upper bounds on graph invariants [30,41] and for indexing graph databases [50].First, the set of vertices in each graph is partitioned into l partitions by label type, and then sorted in a non-increasingtotal order by degree.5 Let Li2 denote the sorted degree sequences of a partition i in the Planning Encoding GraphsG 1 and G 2, respectively. An upper bound on the number of vertices Vertices(G 1, G 2) and edges Edges(G 1, G 2) of the MCESgraph can be computed as follows:1 and Li4 Following the conceptual graph notation, the first and third level nodes correspond to initial and goal fact relation nodes, while the nodes of the secondlevel correspond to concept nodes representing the objects of the initial and goal states.5 The degree or valence of a vertex v of a graph G is the number of edges which touch v.1380I. Serina / Artificial Intelligence 174 (2010) 1369–1406Fig. 6. Planning Encoding Graphs, degree sequences and the corresponding similarity value for two planning problems in the BlocksWorld domain.Vertices(G 1, G 2) =(cid:11)(cid:17)(cid:17)Li1min(cid:17)(cid:17),(cid:17)(cid:17)Li2(cid:17)(cid:12)(cid:17),l(cid:6)i=1(cid:18)l(cid:6)min(|Li1|)(cid:6)|,|Li2min(|E(vEdges(G 1, G 2) =i=1j=1(cid:19)i, j1 )|, |E(v2i, j2 )|)where vi, j1indicates the j-th element (vertex) of the Lii, j1 ) indicates the set of arcs con-1 sorted degree sequence and E(v. An upper bound on the similarity between G 1 and G 2 can be expressed using Johnson’s similaritynected to the vertex vcoefficient [33]:i, j1similds(G 1, G 2) = (Vertices(G 1, G 2) + Edges(G 1, G 2))2(|V (G 1)| + |E(G 1)|) · (|V (G 2)| + |E(G 2)|)= (Vertices(G 1, G 2) + Edges(G 1, G 2))2|G 1| · |G 2|.(6)Since Vertices(G 1, G 2) and Edges(G 1, G 2) determine an upper bound on the number of nodes and arcs of the MCES of G 1 andG 2, the similds(G 1, G 2) similarity measure ranges from 0 to 1 and it is easy to show that it obeys the following inequality,similds(G 1, G 2) (cid:3) (|V (G 12)| + |E(G 12)|)2|G 1| · |G 2|=|G 12|2|G 1| · |G 2|where G 12 is the MCES between graphs G 1 and G 2. Clearly, similds(G 1, G 2) can be used as an upper bound for the size ofthe MCES between G 1 and G 2 and this procedure provides a rapid screening mechanism which takes advantage of localconnectivity and vertex labels to help eliminate unnecessary and costly MCES comparisons. For screening purposes, it is onlynecessary to specify a minimum acceptable value for the MCES based graph similarity measure. If the value determined bysimilds(G 1, G 2) is less than the minimum acceptable similarity, then the object matching comparison can be avoided. Thisprocedure can be performed by using the quick sort algorithm in O (n · log n) time, where n = maxi(|Li1|, |Li2= A” and “v 1,3= C ” with degree 5, 4 and 4 respectively. Similarly the second degree sequence L21 degree sequence is of type “Obj” and it has three elements: the vertices “v 1,1Fig. 6 shows the degree sequences of two Planning Encoding Graphs and the corresponding similarity value similds. Tocompute the degree sequences, the vertices of the graphs are first separated into partitions according to their label type.= B”,Considering the G 1 graph, the L11“v 1,21 of G 1 is of type1“G on” which has only one element: the vertex “v 2,1= D” of graph G 2belongs to the sorted degree sequence L12 of type “Obj”, it is in third position in the degree sequence and its degree is equalto 4; while the entry “v 3,12 which is of type “Iclear”, it is in firstposition in the degree sequence L32 and it has degree 3. Vertices(G 1, G 2) is calculated by summing the size of the set Liwith the fewest non-null elements in graphs G 1 and G 2. Since L11 has 3 elements and L12 has 4 elements, the first term ofVertices(G 1, G 2) is equal to min(|L11|) = 3. Considering the other partitions L2, . . . , L6 we obtain Vertices(G 1, G 2) = 8.= Iclear” of graph G 2 belongs to the degree sequence L3= G on” with degree 2. Moreover, the entry “v 1,3|, |L12|).2211I. Serina / Artificial Intelligence 174 (2010) 1369–14061381Fig. 7. Possible assignments of vertices from G to those of graph Gis to find the optimal assignment of all vertices from G to those of Gbipartite graph, where each edge can be used at most once.(cid:9). The kernel function k measures the similarity of a pair of vertices (v, u). The goal, which maximises the overall similarity score, i.e., the sum of edge weights in the(cid:9)Besides Edges(G 1, G 2) is determined by summing the min(|E(vi, j2 )|) values of each partition i, adding the re-sulting values together and then integer dividing the result by two. For example, the first term of Edges(G 1, G 2) is obtainedby dividing by two the following value:(cid:12)(cid:17)(cid:17),i, j1 )|, |E(v+ min(cid:11)(cid:17)(cid:17)E(cid:11)(cid:17)(cid:17)E(cid:11)(cid:17)(cid:17)E(cid:12)(cid:17)(cid:17),(cid:12)(cid:17)(cid:17),(cid:12)(cid:17)(cid:12)(cid:17)(cid:12)(cid:17)(cid:12)(cid:17)(cid:17)(cid:17)E(cid:17)(cid:17)E(cid:17)(cid:17)Emin(cid:12)(cid:17)(cid:17)(cid:11)(cid:12)(cid:11)(cid:11)(cid:11)(cid:11)(cid:11)v 1,31v 1,32v 1,11v 1,22= min(5, 6) + min(4, 5) + min(4, 5) = 5 + 4 + 4.+ minv 1,21v 1,12If we consider the elements of the other partitions we obtain Edges(G 1, G 2) = 10; since |V (G 1)| = 8, |E(G 1)| = 10,|V (G 2)| = 9 and |V (G 2)| = 15, the degree similarity value of G 1 and G 2 (similds(G 1, G 2)) is equal to 0.75.3.1.3. Kernel functions for object matchingAs previously exposed obj_match is an NP-hard problem and its exact resolution is infeasible from a computationalpoint of view also for a limited number of candidates in the case base. In the following we present an approximate evalu-ation based on kernel functions. Our kernel functions are inspired by Fröhlich et al.’s work [20,21,19] on kernel functionsfor molecular structures. Their goal is to define a kernel function which measures the degree of similarity between twochemical structures which are encoded as undirected labeled graphs. Our goal is to define a matching function among theobjects of two planning problems encoded as directed graphs.The intuition of these kernel functions is that the similarity between two graphs depends mainly on the matching ofthe pairs of vertices and the corresponding neighbourhoods; i.e., two graphs are more similar if the structural elementsfrom both graphs fit together better and if these structural elements are connected in a more similar way in both graphs.Thereby, the graph properties of every single vertex and edge in both structures have to be considered. On a vertex levelthis leads to the idea of looking for those vertices in the two graphs that have the best match with regard to structuralproperties. In this way it is possible to consider not only direct neighbours, but also neighbours that are farther away, upto some maximal topological distance. We now want to associate each vertex in one graph to exactly one vertex in anothergraph such that the overall similarity is maximised. This problem can be modeled as a maximum weight bipartite matchingproblem where our kernel function determines the similarity between pairs of vertices. From this algorithm we know foreach vertex in one graph to which vertex in the other graph it is assigned to. This guarantees us an easy way of interpretingand understanding our kernel function since a matching between an object of the planning problem Π and an object of Π (cid:9)directly derives from a matching between a vertex of the Planning Encoding Graph GΠ and a vertex of GΠ (cid:9) .Let us assume we have two graphs G and G, which have vertices v 1, . . . , vn and u1, . . . , um respectively. Let us furtherassume we have a kernel function k, which compares a pair of vertices v i, u j from both graphs, including informationon their neighbourhoods. We now want to assign each vertex of the smaller of both graphs to exactly one vertex of thebigger one such that the overall similarity score, i.e., the sum of kernel values between individual vertices, is maximised.Mathematically this can be formulated as follows: let ζ denote a permutation of an n-subset of natural numbers 1, . . . , m,or a permutation of an m-subset of natural numbers 1, . . . , n, respectively. Then we are looking for the quantity(cid:9)(cid:20)(cid:5)(cid:11)KG, G(cid:12)(cid:9)=maxζmaxζmh=1 k(vζ (h), uh)(cid:5)nh=1 k(vh, uζ (h)) otherwise.if n (cid:3) m,(7)K is a valid kernel function, as shown by [19], and hence a similarity measure for graphs. Implicitly it computes a dotproduct between two vector representations of graphs in some Hilbert space. Fig. 7 illustrates this idea: between any pairof vertices from the upper and the lower structure there is some similarity, which can be thought as the edge weights ofa bipartite graph. We now have to find a combination of edges such that the sum of edge weights is maximised. Therebyeach edge can be used at most once. That means in the end exactly min(n, m) out of n · m edges are used up.We now have to define the kernel function k. For this purpose let us suppose we have two kernel functions kv and kewhich compare the vertex and edge labels λ(·), respectively. In the following e j(v) denotes the j-th edge of the vertex v,i/oj (v) denoteswhile n j(v) denotes the node adjacent to the vertex v associated to the j-th edge e j(v). In the same way e1382I. Serina / Artificial Intelligence 174 (2010) 1369–1406the j-th incoming/outgoing edge, while ni/oj (v) denotes the direct predecessor/successor of the vertex v associated to thei/oj (v). N (v j) denotes the set of vertices adjacent to the vertex v j , while E(v j) denotes thej-th incoming/outgoing edge eset of incoming and outgoing edges of vertex v j . Similarly N i/o(v j) denotes the set of direct predecessor/successor verticesof the vertex v j .Given a pairs of vertices v and u, we use the kernel function kv (v, u) = γ0(v, u) · |λ(v)∩λ(u)||λ(v)∪λ(u)| , where γ0(v, u) is equal to1.1 if u and v correspond to the same object (it is verified considering the names of the objects represented by verticesu and v), otherwise it is equal to 1.0. The γ0 coefficient has been introduced in our kernel functions in order to allow agreater stability in the activity assignment which is useful especially when human agents are handled by the planner. Forexample, in a logistic domain, we would like the drivers to be assigned the same set of activities as much as possible. Whilefor pairs of edges we use ke(ek(v), e j(u)) = |λ(ek(v))∩λ(e j (u))||λ(ek(v))∪λ(e j (u))| if ek(v) and e j(u) are both incoming or outgoing edges of thevertices v and u, otherwise ke(ek(v), e j(u)) is equal to 0. Formally, this corresponds to the multiplication by a so-calledδ-kernel.We define the base kernel between two vertices v and u, including their direct neighbourhoods askbase(v, u) = kv (v, u) +(cid:6)1|N i(v)| · |N i(u)|(cid:11)h(v), nono(cid:6)h,h(cid:9)kvkv+1|N o(v)| · |N o(u)|h,h(cid:9)(cid:11)h(v), nini(cid:12)h(cid:9) (u)(cid:11)h(v), eiei(cid:12)h(cid:9) (u)· ke(cid:12)h(cid:9) (u)(cid:11)h(v), eoeo(cid:12)h(cid:9) (u).· ke(8)This means that the similarity between two vertices consists of two parts: first the similarity between the labels of thevertices and second the similarity of the neighbourhood structure. It follows that the similarity of each pair of neighboursi/oi/oh(cid:9) (u) is weighed by the similarity of the edges leading to them. The normalisation factors before the sums areh (v), nnintroduced to ensure that vertices with a higher number of arcs do not automatically achieve a higher similarity. Hence wedivide the sums by the number of the addends in them. It is also interesting to point out that the previous definition is justa classical convolution kernel as introduced by Haussler [31].In the following we define a more accurate kernel R1, which compares all the direct neighbours of the vertices (v, u)as the optimal assignment kernel between all the neighbours of v and u and the edges leading to them so that we can toimprove the similarity values that can be obtained simply using kbase; more precisely:⎧⎨(cid:5)|E(v)|(cid:5)|E(u)|1⎩|E(v)| maxζR1(v, u) =i=1 kv (nζ (i)(v), ni(u)) · ke(eζ (i)(v), ei(u))i=1 kv (ni(v), nζ (i)(u)) · ke(ei(v), eζ (i)(u)) otherwise.Similarly to the graph kernel K of Eq. (7), the intuition behind this kernel function is that the similarity between twonodes depends not only on the nodes structure but also on the matching of the corresponding neighbourhoods; i.e., twonodes are more similar if their neighbourhood elements are connected in a more similar way in both nodes.if |E(v)| (cid:3) |E(u)|,|E(u)| maxζ(9)1Theorem 4. R1 is a kernel function.Of course it would be beneficial not to consider the match of direct neighbours only, but also that of indirect neighboursand vertices having a larger topological distance. For this purpose we can evaluate R 1 not at (v, u) only, but also at all pairsof neighbours, indirect neighbours and so on, up to some topological distance L. The weighed average of all these valuescorresponds to the weighed mean match of all indirect neighbours and vertices of a larger topological distance. Addingthem to kv (v, u) leads to the following definition of the neighbourhood kernel kN :kN (v, u) = kv (v, u) + γ (1)R1(v, u) +L(cid:6)l=2γ (l)Rl(v, u)(10)where γ (l) denotes a decay parameter which reduces the influence of neighbours that are at topological distance l6; simi-larly, Rl denotes the average of all R1 evaluated for neighbours at distance l and it is computed from Rl−1 via the recursiverelation:Rl(v, u) =1|N i(v)| · |N i(u)|+1|N o(v)| · |N o(u)|(cid:6)h,h(cid:9)(cid:11)h(v), nini(cid:12)h(cid:9) (u)(cid:11)h(v), eiei(cid:12)h(cid:9) (u)· keRl−1(cid:11)h(v), nono(cid:12)h(cid:9) (u)(cid:11)h(v), eoeo(cid:12)h(cid:9) (u)· keRl−1(cid:6)h,h(cid:9)(11)6 The γ (·) function used in our experimental evaluation is defined in Section 4.1.I. Serina / Artificial Intelligence 174 (2010) 1369–14061383i.e., we can compute kN (v, u) by iteratively revisiting all direct neighbours of v and u. The first addend in Eq. (10) takesinto account the nodes (v, u), while the second addend takes into account the direct neighbours of (v, u) computing theR1(v, u) kernel function, then the next addend (i.e. γ (2) · R2(v, u)) computes the average of the match of all neighbourswhich have topological distance 2 by evaluating R1 for all direct neighbours of (v, u). The fourth addend (i.e. γ (3) · R3(v, u))does the same for all neighbours with topological distance 3. Finally, the last addend considers all neighbours which havetopological distance L by evaluating R1 for all neighbours at topological distance L − 1. Furthermore we can easily showthat:Theorem 5. Let γ (l) = pl with p ∈ (0, 0.5). If there exists a C ∈ R+e1, e2 then Eq. (10) converges for L → ∞.such that kv (v 1, v 2) (cid:2) C for all v 1, v 2 and ke(e1, e2) (cid:2) 1 for allTo briefly summarise, our approach works as follows: we first compute the similarity of all vertex and edge features usingthe kernels kv and ke . Having these results we can compute the match of direct neighbours R 1 for each pair of verticesfrom both graphs by means of Eq. (9). From R1 we can compute R2, . . . , R L by iteratively revisiting all direct neighbours ofeach pair of vertices and computing the recursive update formula (11). Having kv and R1, . . . , R L directly gives us kN , thefinal similarity score for each pair of vertices, which includes structural information as well as neighbourhood properties.With kN we can finally compute the optimal assignment kernel between two graphs G and Gusing Eq. (7). Moreover (7)can be calculated efficiently by using the Hungarian method [37] in O (n3), where n is the maximum number of vertices ofboth graphs.The kernel functions kbase and kN can be used in Eq. (7) to define the optimal assignment kernels Kbase and KNrespectively. Our optimal assignment kernel functions also define a permutation ζ that allows to easily determine thematching function μ associating each object in the smaller planning problem to exactly one object in the other planningproblem.(cid:9)The worst case scenario is determined when we consider two complete graphs with the same number of nodes n. Inthis case, Eq. (8) has computational complexity O (n2) and since it has to be examined n2 time in Eq. (7) we obtain acomputational complexity of n2 · O (n2) + O (n3) = O (n4) for the Kbase kernel function. Moreover, function R1 has a compu-tational complexity of O (n3) since we use the Hungarian method for its computation; similarly to kbase the Rl function hascomputational complexity O (n2) and, limiting the kN evaluation to a topological distance L which is a polynomial of n, weobtain a computational complexity for kN equal to 1 + O (n3) + O (n) · O (n2) = O (n3). The kN kernel function has to beexamined n2 times in Eq. (7), determining a computational complexity for KN equal to n2 · O (n3) + O (n3) = O (n5).As it will be described in the next section, in OAKplan both Kbase and KN have been used; Kbase, which has a lowercomputational complexity, has been used in order to prune unpromising case base candidates. It allows to define a firstmatching function μbase and the corresponding similarity function similμbase , as described in the following section. On theother hand KN has been used to define a final matching function μ and the corresponding similarity function similμ.3.2. Plan evaluation phaseThe purpose of plan evaluation is that of defining the capacity of a plan π to resolve a particular planning problem. Itis performed by simulating the execution of π and identifying the unsupported preconditions of its actions; in the sameway the presence of unsupported goals is identified. The plan evaluation function could be easily defined as the numberof inconsistencies in the current planning problem. Unfortunately this kind of evaluation considers a uniform cost in orderto resolve the different inconsistencies and this assumption is generally too restrictive. Then our system considers a moreaccurate inconsistency evaluation criterion so as to improve the plan evaluation metric. The inconsistencies related to un-supported facts are evaluated by computing a relaxed plan starting from the corresponding state and using the RelaxedPlanalgorithm in lpg [25]. The number of actions in the relaxed planning graph determines the difficulty to make the selectedinconsistencies supported; the number of actions in the final relaxed plan determines the accuracy of the input plan π tosolve the corresponding planning problem.(cid:9)Fig. 8 describes the main steps of the RelaxedPlan function.7 It constructs a relaxed plan through a backward processwhere Bestaction(g) is the action a; (ii) all precon-ditions of arequires a minimumnumber of actions, evaluated as the maximum of the heuristically estimated minimum number of actions required to sup-subverts a minimum number of supported precondition nodes in A (i.e.,port each precondition p of athe size of the set Threats(aare reachable from the current state INIT; (iii) the reachability of the preconditions of achosen to achieve a (sub)goal g, and such that: (i) g is an effect of a(cid:9)) is minimal).from INIT; (iv) a(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)Fig. 9 describes the main steps of the EvaluatePlan function. For all actions of π (if any), it checks if at least oneprecondition is not supported. In this case it uses the RelaxedPlan algorithm (step 4) so as to identify the additionalactions required to satisfy the unsupported preconditions. If Rplan contains a number of actions greater than Climit we can7 RelaxedPlan is described in detail in [25]. It also computes an estimation of the earliest time when all facts in G can be achieved, which is notdescribed in this paper for sake of simplicity.1384I. Serina / Artificial Intelligence 174 (2010) 1369–1406Algorithm RelaxedPlanInput: a set of goal facts (G), the set of facts that are true in the current state (INIT),a possibly empty relaxed plan ( A)(cid:16)G = G − INIT; ACTS = AF =a∈ACTS Add(a)while G − F (cid:17)= ∅Output: a relaxed plan ACTS estimating a minimal set of actions required to achieve G1.2.3.4.5.6.7.8.9.g = “a fact in G − F ”bestact = Bestaction(g)Rplan = RelaxedPlan(Pre(bestact), INIT, ACTS)ACTS = Aset(Rplan) ∪ {bestact}F =return ACTSa∈ACTS Add(a)(cid:16)Fig. 8. Algorithm for computing a relaxed plan estimating a minimal set of actions required to achieve a set of facts G from the state INIT. Bestaction(g) isthe action that is heuristically chosen to support g as described in [25].CState = I ; Rplan = ∅forall a ∈ πi doAlgorithm EvaluatePlanInput: a planning problem Π = (I, G), an input plan π and an adaptation cost limit ClimitOutput: a relaxed plan to adapt π in order to resolve Π1.2.3.4.5.6.7.8.Rplan = RelaxedPlan(Pre(a), CState, Rplan)if ∃ f ∈ Pre(a) s.t. f /∈ CState thenCState = (CState/Del(a)) ∪ Add(a)if ∃g ∈ G s.t. g /∈ CState thenif |Rplan| > Climit thenreturn RplanRplan = RelaxedPlan(G, CState, Rplan)9.return RplanFig. 9. Algorithm to evaluate the ability of π to solve the planning problem Π .Algorithm RetrievePlanInput: a planning problem Π , a case base C = (cid:2)Πi , πi (cid:3)Output: candidate plan for the adaptation phaseπR = Evaluate_plan(Π, empty_plan, ∞)1.1.= I ∩Define the set of initial relevant facts of Π using πR : IπR1.2.Compute the Planning Encoding Graphs EΠ and EΠR of Π(I, G) and ΠR (IπR , G) respectively, and the degree sequences Lforall Πi ∈ C dopre(a)a∈πR(cid:16)jΠR1.3.1.4.1.5.1.6.1.7.2.1.2.2.2.3.2.4.3.1.3.2.3.3.3.4.3.5.4.1.4.2.4.3.4.4.4.5.4.6.4.7.5.1.simili = similds(EΠi , EΠR )push((Πi , simili ), queue)best_ds_simil = max(best_ds_simil, simili )forall (Πi , simili ) ∈ queue s.t. best_ds_simil − simili (cid:2) limit do*Load the Planning Encoding Graph EΠi and compute the matching function μbase using Kbase(EΠi , EΠ )push((Πi , μbase), queue1)best_μbase_simil = max(best_μbase_simil, similμbase (Πi , Π))forall (Πi , μbase) ∈ queue1 s.t. best_μbase_simil − similμbase (Πi , Π) (cid:2) limit doCompute the matching function μN using KN (EΠi , EΠ )if similμN (Πi , Π) (cid:3) similμbase (Πi , Π) then μi = μNelse μi = μbasepush((Πi , μi ), queue2)best_simil = max(best_simil, similμi (Πi , Π))best_cost = |πR |; best_plan = empty_planforall (Πi , μi ) ∈ queue2 s.t. best_simil − similμi (Πi , Π) (cid:2) limit doRetrieve πi from Ccosti = |EvaluatePlan(Π, μi (πi ), best_cost · similμi (Πi , Π))|if best_cost · similμi (Πi , Π) > costi thenbest_cost = costi /similμi (Πi , Π)best_plan = μi (πi )return best_plan* We limited this evaluation to the best 700 cases of queue.Fig. 10. Algorithm to find a suitable plan for the adaptation phase from a set of candidate cases or the empty plan (in case the “generative” approach isconsidered more suitable).stop the evaluation, otherwise we update the current state CState (step 7). Finally we examine the goal facts G (step 8) toidentify the additional actions required to satisfy them, if necessary.Fig. 10 describes the main steps of the retrieval phase. We initially compute a relaxed plan πR for Π (step 1.1) usingthe EvaluatePlan function on the empty plan which is needed so as to define the generation cost of the current planningI. Serina / Artificial Intelligence 174 (2010) 1369–14061385problem Π (step 4.1) and an estimate of the initial state relevant facts (step 1.2). In fact we use the relaxed plan πRso as to filter out the irrelevant facts from the initial state description.8 This could be easily done by considering all thepreconditions of the actions of πR :(cid:15)IπR= I ∩pre(a).a∈πRThen in step 1.3 the Planning Encoding Graph of the current planning problem Π and the degree sequences that will beused in the screening procedure are precomputed. Note that the degree sequences are computed considering the PlanningEncoding Graph EΠR of the planning problem ΠR (IπR , G) which uses IπR instead of I as initial state. This could be extremelyuseful in practical applications when automated tools are used to define the initial state description without distinguishingamong relevant and irrelevant initial facts.Steps 1.4–1.7 examine all the planning cases of the case base so as to reduce the set of candidate plans to a suitablenumber. It is important to point out that in this phase it is not necessary to retrieve the complete Planning EncodingGraphs of the case base candidates GΠ (cid:9) but only their sorted degree sequences LiΠ (cid:9) which are precomputed and stored inthe case base. On the contrary the Planning Encoding Graph and the degree sequences of the input planning problem areonly computed in the initial preprocessing phase (step 1.3).All the cases with a similarity value sufficiently close9 to the best degree sequences similarity value (best_ds_simil)are examined further on (steps 2.1–2.4) using the Kbase kernel function. Then all the cases selected at steps 2.x with asimilarity value sufficiently close to the best similμbase similarity value (best_μbase_simil) (step 3.1) are accurately evaluatedusing the KN kernel function, while the corresponding μN function is defined at step 3.2. In steps 3.3–3.5 we select thebest matching function found for Πi and the best similarity value found until now.We use the relaxed plan πR in order to define an estimate of the generation cost of the current planning problem Π(step 4.1). The best_cost value allows to select a good candidate plan for adaptation (which could also be the empty plan).This value is also useful during the computation of the adaptation cost through EvaluatePlan, in fact if such a limit isexceeded then it is wasteful to use CPU time and memory to carry out the estimate and the current evaluation could beterminated. The computation of the adaptation cost of the empty plan allows to choose between an adaptive approach anda generative approach, if no plan gives an adaptation cost smaller than the empty plan.For all the cases previously selected with a similarity value sufficiently close to best_simil (step 4.2) the adaptation cost isdetermined (step 4.4). If a case of the case base determines an adaptation cost which is lower than best_cost · similμi (Πi, Π)then it is selected as the current best case and also the best_cost and the best_plan are updated (steps 4.5–4.7). Note thatwe store the encoded plan μi(πi) in best_plan since this is the plan that can be used by the adaptation phase for solvingthe current planning problem Π . Moreover we use the similμi (Πi, Π) value in steps 4.4–4.6 as an indicator of the effectiveability of the selected plan to solve the current planning problem maintaining the original plan structure and at the sametime obtaining low distance values.3.3. Plan adaptationAs previously exposed, the plan adaptation system is a fundamental component of a case-based planner. It consistsin reusing and modifying previously generated plans to solve a new problem and overcome the limitation of planningfrom scratch. As a matter of fact, in planning from scratch if a planner receives exactly the same planning problem it willrepeat the very same planning operations. In our context the input plan is provided by the plan retrieval phase previouslydescribed; but the applicability of a plan adaption system is more general. For example the need for adapting a precomputedplan can arise in a dynamic environment when the execution of a planned action fails, when the new information changingthe description of the world prevents the applicability of some planned actions, or when the goal state is modified byadding new goals or removing existing ones [16,25].From a theoretical point of view, in the worst case, plan adaptation is not more efficient than a complete regenerationof the plan [47] when a conservative adaptation strategy is adopted. However adapting an existing plan can be in practicemore efficient than generating a new one from scratch, and, in addition, this worst case scenario does not always hold, asexposed in [2] for the Derivation Analogy adaptation approach. Plan adaptation can also be more convenient when the newplan has to be as “similar” as possible to the original one.Our work uses the lpg-adapt system given its good performance in many planning domains but other plan adaptationsystems could be used as well. lpg-adapt is a local-search-based planner that modifies plan candidates incrementally in asearch for a flawless candidate. It is important to point out that this paper relates to the description of a new efficient case-based planner and in particular to the definition of effective plan matching functions, no significant changes were made tothe plan adaptation component (for a detailed description of it see [16]).8 In the relaxed planning graph analysis the negative effects of the domain operators are not considered and a solution plan πR of a relaxed planningproblem can be computed in polynomial time [32].9 In our experiments we used limit = 0.1.1386I. Serina / Artificial Intelligence 174 (2010) 1369–1406Fig. 11. Retrieval and Adaptation phases.Quite interesting the lpg-adapt planner has an anytime behaviour, in fact it can produce a succession of valid plans,where each plan is an improvement of the previous ones in terms of its “adaptation quality”. This is a process that incre-mentally improves the total quality of the plans, which can be stopped at any time to give the best plan computed so far(πbest). Each time we start a new search, the input plan π0 provided by the retrieval phase is used to initialise the datastructures. Furthermore, during search some random inconsistencies are forced in the plan currently undergoing adaptationwhen a valid plan that does not improve πbest is reached. This is a mechanism for leaving local optima. In our adaptationcontext the total quality of a plan is derived considering a weighed evaluation of the metric quality and the “distance” of πfrom π0:Q π0 (π , πbest) = αQuality(π )Quality(πbest)+ (1 − α)|π ∩ π0||πbest ∩ π0|(12)the rationale of this choice is that of trying to balance the quality of the plan produced and the distance from the inputplan π0.10 The Q π0 (·,·) value is used to choose between the new valid plan π and the best plan πbest found until now.Fig. 11 visualises the main steps of the retrieval and adaptation phases. The screening procedure computes the simildsdistance function so as to filter out the dissimilar cases and reduce the number of planning cases that have to be examinedaccurately up to a suitable number. Then we compute the Kbase and KN kernel functions and the corresponding μbaseand μN matching functions to define the corresponding similμ similarity values and identify the candidate plans for theadaptation phase. These plans are evaluated using the EvaluatePlan procedure and if the best case has an adaptationcost inferior to the generation cost then it is used by lpg-adapt in order to find a suitable solution, otherwise a completegeneration phase is performed providing the empty plan to lpg-adapt.3.4. Plan revision and case base updateAny kind of planning system that works in dynamic environments has to take into account failures that may arise duringplan generation and execution. In this respect case-based planning is not an exception; this capability is called plan revision.When a failure is discovered, the system may react by looking for a repair or aborting the plan. In the former case thelpg-adapt system is invoked on the remaining part of the plan.10 In our tests we used α = 0.5.I. Serina / Artificial Intelligence 174 (2010) 1369–14061387Algorithm Insert_Case(Case, π , Π(I, G))Input: a case base Case, a solution plan π for planning problem Π with initial state I and goal state G.Output: insert the planning case in Case if not present.1.2.3.4.5.6.7.8.9.Define the set of initial state relevant facts Iπ of Π using the input plan πCompute the Planning Encoding Graph Eπ of Ππ (Iπ , G)for each case (Πi , πi ) ∈ CaseCompute the matching function μi using KN (EΠi , Eπ )if complete_similμi (Πi , Ππ ) = 1 ∧ |πi | (cid:2) |π | thenreturn FALSE;enforInsert the planning problem Ππ (Iπ , G), its solution plan π , the Planning Encoding Graph Eπand the data structures for the screening procedure in Casereturn TRUE;Fig. 12. High-level description of Insert_Case.After finding the plan from the library and after repairing it with the lpg-adapt techniques the solution plan can beinserted into the library or be discarded. Fig. 12 describes the main steps of the function used to evaluate the insertion ofa planning problem Π solved in the case base. First of all we compute the set of initial state relevant facts Iπ using theinput plan π ; this set corresponds to a subset of the facts of I relevant for the execution of π . It can be easily computed,as described in Section 3.2, using the preconditions of the actions in π :Iπ = I ∩(cid:15)pre(a).a∈πNote that Iπ identifies all the facts required for the execution of the plan π and that this definition is consistent withthe procedure used in the RetrievePlan algorithm for the relaxed plan πR . Then we compute the Planning Encoding GraphEπ of the new planning problem Ππ (Iπ , G) having Iπ as initial state instead of I . At steps 3–6 the algorithm examines allthe cases of the case base and if it finds a case that solves Π with a plan of a better quality with respect to π then it stopsand exits. In order to do so we use the similarity function complete_similμi , described in Section 3.1.1, which compares allthe initial and goal facts of two planning problems. Otherwise if there is no case that can solve Ππ with a plan of a betterquality with respect to π then we insert the solved problem in the case base. As we can observe at step 8, a planning caseis made up not only by Ππ and π , but also other additional data structures are precomputed and added to the case baseso that their recomputation during the Retrieval Phase can be avoided.4. Experimental resultsIn this section, we present an experimental study aimed at testing the effectiveness of OAKplan in a number of standardbenchmark domains. In the first subsection, we describe the experimental settings and then, in the second subsection, wepresent the system overall results. In particular we examine the behaviour of OAKplan when different matching functionsare used and we experimentally analyse the impact of the size of the case base (number of planning cases) in the overallperformance of the system. In the third subsection, we experimentally investigate the similarity values of our matchingfunctions when the case base objects are progressively renamed. Finally, we compare our planner with four state-of-the-artplanners.4.1. Experimental settingsHere we present and discuss the general results for the experimental comparison, moreover we examine the importanceof the matching functions and the size of the case base in the overall performance of the system.OAKplan is written in C++ and uses the SQLite3 library11 for storing and retrieving the data structures of the case baseand the VFLIB library [14] so as to create and elaborate our graph data structures.12 The OAKplan code and the benchmarkplanning problems are available from the OAKplan website http://pro.unibz.it/staff/iserina/OAKplan/.We have conducted all the experimental tests using an AMD Sempron(tm) Processor 3400+ (with an effective 2000 MHzrating) with 1 Gbyte of RAM. Unless otherwise specified, the CPU-time limit for each run is 10 minutes for OAKplan and30 minutes for all the other planners, after which termination is forced.13 In the following tests the maximum topologicaldistance L considered for the computation of the kN kernel function in Eq. (10) is set to half of the number of nodes of11 SQLite is a software library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine; further information canbe found at http://www.sqlite.org/.12 Although the VFLIB library can solve subgraph-isomorphism problems, its use turns out to be computationally too expensive and the kernel functionsdescribed in Section 3.1.3 are used instead.13 We use 10 minutes for OAKplan since many experimental tests have been conducted on it and a CPU limit of 30 minutes has turned out to becomputationally too expensive. However, as we will see in Table 1, the CPU limit of 10 minutes is enough for OAKplan to solve more than 95% of theproblems attempted and additional CPU-time could only modify the number of solved problems, the plan quality and the difference values obtained slightly.1388I. Serina / Artificial Intelligence 174 (2010) 1369–1406the smaller of the two graphs examined (L = (cid:19) min(|V 1|,|V 2|)problems the γ (l) coefficient of Eq. (10) is set equal to γ (l) = (1 − 12L )l.(cid:20)); since this value is sufficiently small to avoid convergenceSince our planner and lpg use a randomised search algorithm, the corresponding results are median values over five runsfor each problem considered. Moreover, since OAKplan and lpg are incremental planners we evaluate their performancewith respect to three different main criteria: CPU-time required to compute a valid plan, the plan stability [16] of thegenerated plans with respect to the corresponding solutions of the target plans and the quality of the best plan generatedwithin the given CPU-time limit.In these tests the solution plans of the planning cases are obtained by using the domain dependent planner tlplan [3]unless otherwise specified. tlplan is a planning system that utilises domain specific search control information to guidesimple forward chaining search, and it is the winner of the Hand Coded track of the 3rd IPCs. Its use allows us to use a highquality input plan with comparatively low investment of initial computation time. Using a plan from a different planneralso ensures that we are not artificially enhancing stability by relying on the way in which the planner explores its searchspace.Our tests are conducted on a series of variants of problems from different domains:• BlocksWorld and Logistics Additionals (2nd International Planning Competition),• DriverLog and ZenoTravel Strips (3rd IPC),• Rovers-IPC5 and TPP Propositional (5th IPC).These tests are generally performed by taking six problems from the benchmark test suite in each case and then me-thodically generating a series of variants for those problems for a total of 216 planning problems for each domain. In eachcase, we take three medium size problems and three largest size problem instances as base problems, so as to make thequestion of case-based planning versus replanning an interesting one.14 The variant problems are generated by modifyingthe initial and goal facts of the original problem. These modifications are performed randomly, although the number ofmodifications is increased systematically: we consider from zero to five modifications of the goal set and from zero to fivemodifications of the initial state.15Although we use only six base problems in each domain, we generate a large number of variants and we considerproblems from several domains, so these results can be considered representative of the behaviour of the system for othersimilarly sized base problems. To confirm that the results are not an artifact of the particular problem instances chosen,we adopt a different problem generation strategy for creating problem instances in the Logistics domain. Thus we selectproblems randomly from the benchmark suites considering the “Additionals” planning problems created in the 2nd IPC forthe Domain Dependent planners, distributed across the smallest and the largest problem instances, and generate variantproblems for each case. We use the same scheme as above to determine the combination of modification values for theinitial state and goals, but select the base problem to apply the modifications randomly. The list of base problems selectedfor each domain and the random modifications applied are described in Appendix C (see Supplementary material for acomplete description of the random modifications applied).For each of the benchmark domains we build a case base library used by OAKplan. All the problems generated in thedifferent IPCs belong to these libraries. Using the problem generators provided by the IPC organisers, a number of planningproblems, with the same features as the IPC planning problems considered, are generated and added to the libraries, for atotal of 10000 planning problems for each of the benchmark domains considered except TPP where we only use the originalIPC planning problems since it is not possible to use tlplan to solve the planning problems of this domain; then we useSGPlan-ipc5 to determine the solutions of the TPP planning cases.In the following we report the summary results obtained by OAKplan considering (1) case base libraries whose casesuse the same objects as the planning problems in the test set, so as to verify the system behaviour when the matchingfunction of OAKplan could be simply obtained by using the identity function (we add the suffix “Ons”, which stands forOriginal object names, to the corresponding results as in OAKplan-Ons); (2) case base libraries where the object names ofthe planning cases are randomly modified with respect to the objects of the planning problems in the test set, to verifythe system behaviour when completely new problems are provided to OAKplan (we add the suffix “Nns”, which stands forNew object names, to the corresponding results as in OAKplan-Nns); (3) case base libraries which contain only the baseproblems used to generate the variants of the benchmark set (we add the suffix “small” to the corresponding results as inOAKplan-small).16 See Supplementary material for a detailed comparison of the results produced in the different domains.14 For small problems, the difference among these strategies is not particularly interesting except for the situation in which the stability of the planproduced is fundamental.15 In the following experimental results when a planning problem is solved by OAKplan it is not inserted in the case base but simply discarded.16 The number of cases of the plan libraries in the “small” tests is always lower than 15 except in the Logistics domain where we consider 170 baseplanning problems.I. Serina / Artificial Intelligence 174 (2010) 1369–14061389Table 1Results of OAKplan-Nns in the different domains: number of solutions found, average CPU-time of the first solutions (in milliseconds) and correspondingaverage matching time, average best plan quality, average best plan stability and average best plan differences.DomainBlocksWorldLogisticsDriverLogZenoTravelRoversTPPTOTALSolutions187 (86%)213 (98%)197 (91%)211 (97%)214 (99%)210 (97%)1232 (95%)Speed214244.888928.4112556.686722.162421.326837.896162.0Matching timeQualityStabilityDifferences121535.669606.331107.434123.053719.5859.250777.4346.0390.2230.2194.6374.4308.8307.80.920.880.910.840.980.960.9249.676.623.647.211.420.938.24.2. Overall resultsIn this section we report the overall results of OAKplan considering the number of solutions found, the CPU-time, theplan quality and the plan stability [16] of the solutions produced by the adaptation process with respect to the plan obtainedby the RetrievePlan function (best_plan). While the first three terms are standard evaluation parameters commonly adoptedin planning, the plan stability deserves some additional considerations. The importance of plan stability has been examinedby Fox et al. [16] in the context of plan adaptation, where the authors use the term plan stability to refer to a measureof the difference a process induces between an original (source) plan and a new (target) plan. In [16] the plan stabilityis measured considering the distance, expressed in terms of number of different actions, between the source plan π andthe target plan π0. In this paper we also consider an additional plan stability function ((cid:12)) derived by the formalisationpresented by Srivastava et al. [59]:(cid:12)(π , π0) = 1 −(cid:21)|π − π0||π | + |π0|+|π0 − π ||π | + |π0|(cid:22).The second term represents the contribution of the actions in π to the plan stability, while the third term indicates thecontribution of π0 to (cid:12). This function assumes a 0 value when the two plans are completely different and a value equalto 1 when π and π0 have exactly the same actions. From a practical point of view we think that plan stability can bequite important in different real world applications. For example more stable plans offer a greater opportunity for gracefulelision of activities and less stress on execution components. Preserving plan stability also reduces the cognitive load onhuman observers of a planned activity, by ensuring coherence and consistency of behaviours, even in the face of dynamicenvironments [16]. Finally, in a case-based approach, a high plan stability is a clear indicator that the system correctlyselects plans that can be easily adapted.In Table 1 we present the results of OAKplan-Nns in the different benchmark domains. Here we consider the averageCPU-time and the Matching Time for the first solutions generated (in milliseconds). In the fifth column we present theaverage plan quality of the best solution generated in the different variants and finally the average plan stability and plandistance (in terms of number of different actions) of the best solution produced with respect to the plan obtained by theRetrievePlan function (best_plan). OAKplan-Nns solves 95% of the problems attempted and the average difference withrespect to the target plans is 38.2, i.e. considering all the 1232 planning problems solved by OAKplan there are on average38 actions introduced or removed with respect to the target plans which corresponds to a stability of 92%. It requires 96seconds to solve the different benchmark planning problems of which 51 seconds are required by the matching process.It is important to point out that more than 10000 cases belong to each plan library, which have to be considered by thematching process. We think that such a high number of cases is hardly required by real applications: in fact case basemaintenance policies [57] could be used in real word applications in order to reduce the number of cases that have to behandled by a case-based planner significantly.In the following part of this section we examine the relevance of the kernel functions used by OAKplan considering theKbase kernel function described in Section 3.1.3 and a new kernel function, that we call Knode, which simply uses the kvkernel function in Eq. (7) and only compares the labels of the pairs of nodes considered. Finally we examine the influence ofthe case base size on the system performance and how OAKplan performs when a planning problem with the same objectsnames as the case base is considered.4.2.1. Matching functionsTo verify the relevance of the matching functions used by our system, we compare OAKplan with simpler matchingfunctions; in particular we examine the system behaviour considering the Kbase and the Knode kernel functions.In Table 2 we compare OAKplan-Nns with one of its reduced versions called OAKplan-Nns-Kbase that avoids the compu-tation of steps 3.x in the RetrievePlan procedure, i.e. the best matching function is obtained only by considering the Kbase1390I. Serina / Artificial Intelligence 174 (2010) 1369–1406Table 2Results of OAKplan-Nns-Kbase vs. OAKplan-Nns.DomainBlocksWorldLogisticsDriverLogZenoTravelRoversTPPTOTALSolutions68.0 (−64%)123 (−42%)157 (−20%)142 (−33%)216 (0.93%)198 (−5.7%)904 (−27%)SpeedMatching244907 (173%)101027 (362%)113622 (110%)142705 (252%)47931 (−25%)73923 (198%)122391 (198%)87582 (390%)18982 (3.3%)51823 (176%)38183 (−30%)928 (9.4%)Quality330 (56%)293 (14%)209 (19%)191 (31% )374 (0.08%)332 (9.4%)101961 (122%)41886 (69%)293 (13%)Stability0.71 (−19%)0.59 (−31%)0.60 (−33%)0.25 (−69%)0.97 (−0.7%)0.20 (−79%)0.55 (−39%)Table 3Results of OAKplan-Nns-Knode vs. OAKplan-Nns.DomainBlocksWorldLogisticsDriverLogZenoTravelRoversTPPTOTALSolutions65.0 (−65%)103 (−52%)108 (−45%)108 (−49%)214 (0.0%)4.00 (−98%)602 (−51%)SpeedMatchingQuality275902 (244%)129886 (620%)54862 (353%)98908 (245%)36560 (−42%)502450 (10901%)147690 (317%)125599 (725%)46725 (553%)78695 (290%)18985 (−65%)740 (24%)317 (57%)241 (7.8%)94.6 (4.1%)96.6 (−2.5%)346 (−7.5%)520 (106%)95935 (133%)66690 (120%)236 (3.3%)Stability0.41 (−54%)0.31 (−63%)0.07 (−91%)0.17 (−78%)0.15 (−85%)0.20 (−79%)0.19 (−78%)Differences280 (407%)328 (415%)98.9 (401%)161 (290%)29.6 (136%)76.5 (267%)132 (338%)Differences293 (462%)282 (320%)137 (676%)151 (267%)345 (2940%)504 (2701%)258 (708%)kernel function. In this table and in the following ones, we report the percent error in brackets17 with respect to OAKplan-Nns where, except for the column of the solutions found, we consider only the problems solved by both planners. By usingthis less accurate matching function the number of problems solved is 904 (down 27%), the average CPU-time required is101 seconds (up 122% considering only the problems solved by both systems) and, most important of all, a plan differenceof 132 actions (up 338%) and a plan stability of 0.55 (down 39%). Note that the CPU-time required by the matching processincreases significantly (plus 69%) since a less accurate matching function determines a greater number of problems thathave to be examined by steps 4.2–4.7 of the RetrievePlan procedure.In Table 3 we compare OAKplan-Nns with a relaxed version of it called OAKplan-Nns-Knode which avoids the com-putation of steps 3.x of the RetrievePlan procedure and uses the Knode kernel function instead of the Kbase function atstep 2.2. In this test we want to examine the system behaviour when a very simple matching function is used. The numberof problems solved is 602 (down 51%), the CPU-time required to solve the planning problems is 95.9 seconds (up 133%considering the problems solved by both systems), the plan difference is of 258 actions (up 708%) and plan stability is only0.19 (down 78%). This clearly indicates the extraordinary importance of an accurate matching function for the global systemperformance, not only in order to obtain low distance values but also to solve a reasonable number of planning problems.In Tables 4 and 5 we examine the situation where the best planning case is selected by the standard RetrievePlanprocedure but the best_plan at step 4.7 is not identified by using the best matching function found until now but byapplying the corresponding μbase(πi) or μnode(πi)18 matching functions. The relating results are indicated respectively withOAKplan-Nns-adapt-Kbase and OAKplan-Nns-adapt-Knode. In this way we provide the same planning case used by OAKplan-Nns to the adaptation process while the encoded solution plan (step 4.7, Fig. 10) is obtained by using the Kbase and Knodekernel functions. The number of problems solved increases considerably since the correct planning case is provided to lpg-adapt, but the average CPU-time and the average plan differences remain significantly greater than the corresponding onesof OAKplan-Nns. In particular OAKplan-Nns-adapt-Kbase determines an average plan distance of 180 actions (up 434%),while OAKplan-Nns-adapt-Knode determines an average plan distance of 338 actions (up 781%) and a plan stability of 0.17.The plan distance values are even greater than the ones obtained in the previous tests essentially because now the systemis able to solve much more difficult planning problems.We carried out a statistical analysis based on the Wilcoxon signed rank test [68] to understand the significance of theperformance gaps in the planners compared during the experiments. The organisers of IPC-3 have also utilised this statisticaltest to study the performance of the planners in the competition [42]. The data necessary to effect the Wilcoxon test areobtained in the following way. The difference between the CPU-times of the two planners compared is computed and thesamples of the test for the CPU-time analysis are defined. The absolute values of these differences are then ranked byincreasing numbers, starting from the lowest value. (The lowest value is ranked 1, the next lowest value is ranked 2, andso on.) After that the ranks of the positive differences and the ranks of the negative differences are summed respectively.Should it happen that the performance of the planners compared are not very different, then the number of the positive17 Given two values a and b the percent error of a with respect to b is equal tothe absolute value in the previous formula and a negative percent error indicates that a is less than b.18 The μbase matching function is computed using the Kbase kernel function, similarly the μnode matching function is computed using the Knode kernelfunction.· 100%. Since our values are always positive we have not considered|a−b||b|I. Serina / Artificial Intelligence 174 (2010) 1369–14061391Table 4Results of OAKplan-Nns-adapt-Kbase vs. OAKplan-Nns.DomainBlocksWorldLogisticsDriverLogZenoTravelRoversTPPTOTALSolutions65.0 (−65%)178 (−16%)157 (−20%)160 (−24%)214 (0.0%)203 (−3.3%)977 (−21%)SpeedMatching172152 (117%)130661 (132%)112493 (112%)156323 (198%)63247 (1.3%)79418 (225%)109291 (112%)35607 (0.20%)45745 (0.43%)18328 (−0.1%)21714 (−0.2%)53737 (0.03%)858 (0.35%)29153 (0.11%)Quality338 (67%)411 (21%)209 (19%)214 (31%)375 (0.08%)340 (11%)319 (16%)Stability0.44 (−49%)0.30 (−66%)0.74 (−18%)0.44 (−47%)0.96 (−2.1%)0.88 (−8.5%)0.67 (−27%)Table 5Results of OAKplan-Nns-adapt-Knode vs. OAKplan-Nns.DomainBlocksWorldLogisticsDriverLogZenoTravelRoversTPPTOTALSolutions69.0 (−63%)181 (−15%)108 (−45%)126 (−40%)214 (0.0%)4.00 (−98%)702 (−43%)SpeedMatchingQuality181379 (111%)133987 (133%)15154 (25%)109391 (169%)71984 (15%)437260 (9463%)98775 (92%)36344 (0.08%)45543 (−1.0%)7146 (−0.2%)21098 (−1.0%)53745 (0.04%)585 (−1.7%)36588 (−0.4%)348 (68%)410 (19%)95.0 (4.7%)139 (8.9%)351 (−6.2%)518 (109%)289 (11%)Stability0.42 (−52%)0.18 (−80%)0.06 (−93%)0.09 (−88%)0.19 (−81%)0.19 (−81%)0.17 (−81%)Differences294 (447%)488 (574%)99.0 (406%)194 (347%)26.8 (136%)86.7 (325%)180 (434%)Differences314 (455%)523 (617%)144 (716%)223 (419%)351 (2986%)507 (3460%)338 (781%)Fig. 13. Partial order of the performance of OAKplan-Nns, OAKplan-Nns-Kbase, OAKplan-Nns-Knode, OAKplan-Nns-adapt-Kbase and OAKplan-Nns-adapt-Knode according to the Wilcoxon signed rank test for our benchmark problems.differences is more or less equal to the number of the negative differences. Moreover the sum of the ranks in the set ofthe positive differences is approximately equal to the sum of the ranks in the other set. From an intuitive point of view, thetest takes into consideration a weighted sum of the number of times one planner performs better than the other. The testmakes use of the performance gap to give a rank to each performance difference, thus we say that the sum is weighted.Fig. 13 gives a graphical summary of the Wilcoxon results for the relative performance of OAKplan-Nns with OAKplan-Nns-Kbase, OAKplan-Nns-Knode, OAKplan-Nns-adapt-Kbase and OAKplan-Nns-adapt-Knode in terms of CPU-time, plan qualityand difference values for our benchmark problems (see Supplementary material for detailed results, e.g. Figs. A1–A3). A solidarrow from a planner A to a planner B (or to a cluster of planners B) indicates that the performance of A is statisticallydifferent from the performance of B (every planner in B), and that A performs better than B (every planner in B) with aconfidence level of 99.9%. A dashed arrow from A to B indicates that A is better than B with a confidence level of 99%.Here we can observe as expected that OAKplan-Nns is statistically better than the other OAKplan variants both in terms ofCPU-time and plan distance values. Quite interesting we can note that OAKplan-Nns-Knode is statistically the most efficientplanner in terms of quality of the plans generated followed by OAKplan-Nns. This can be explained by the fact that duringthe incremental adaptation process it has not been able to reduce significantly the plan distance values but only the qualityof the plans produced.1392I. Serina / Artificial Intelligence 174 (2010) 1369–1406Table 6Summary Table OAKplan-Ons vs. OAKplan-Nns.DomainSolutionsBlocksWorldLogisticsDriverLogZenoTravelRoversTPP213 (14%)211 (−0.9%)199 (1.0%)211 (0.0%)214 (0.0%)211 (0.47%)Speed66831 (−73%)33729 (−62%)112552 (−4.2%)39625 (−54%)61688 (−1.2%)24537 (−13%)Matching10648 (−92%)17804 (−74%)29052 (−9.1%)7989 (−77%)52975 (−1.4%)748 (−13%)TOTAL1259 (2.2%)55988 (−45%)19846 (−61%)Table 7Summary Table OAKplan-small-Nns vs. OAKplan-Nns.DomainSolutionsBlocksWorldLogisticsDriverLogZenoTravelRoversTPPTOTAL200 (6.9%)214 (0.46%)205 (4.1%)216 (2.4%)216 (0.93%)210 (0.0%)1261 (2.3%)Speed118823 (−56%)30131 (−67%)97810 (−28%)56226 (−39%)27221 (−56%)26604 (−0.9%)58584 (−47%)Matching1596 (−99%)11436 (−84%)4724 (−86%)3146 (−92%)22527 (−58%)858 (−0.1%)7502 (−85%)Quality335 (−7.3%)371 (−4.3%)233 (−0.1%)178 (−8.8%)374 (−0.0%)308 (−0.5%)301 (−3.2%)Quality359 (−0.3%)392 (0.03%)242 (0.05%)196 (−0.4%)374 (0.0%)309 (0.0%)312 (−0.1%)Stability0.98 (6.8%)0.94 (6.8%)0.91 (0.27%)0.91 (8.8%)0.98 (−0.1%)0.97 (0.31%)0.95 (3.6%)Stability0.91 (0.12%)0.88 (−0.1%)0.92 (1.7%)0.84 (0.55%)0.98 (0.0%)0.96 (0.0%)0.92 (0.37%)Differences13.9 (−76%)31.7 (−59%)22.8 (−4.3%)19.8 (−58%)11.2 (−1.3%)17.3 (−18%)19.4 (−50%)Differences54.5 (−2.2%)76.9 (0.37%)24.3 (−4.2%)46.3 (−3.3%)11.3 (0.0%)20.9 (−0.1%)39.0 (−1.4%)4.2.2. Object names renaming analysisIn Table 6 we compare OAKplan-Ons and OAKplan-Nns. The CPU-time required by OAKplan-Ons is lower than the CPU-time of OAKplan-Nns since the Kbase kernel function in OAKplan-Nns produces lower similarity values than in OAKplan-Ons, as we can see more precisely in the following subsection. These lower values determine a greater number of casesthat must be evaluated using KN while the number of solutions produced and the plan qualities are very close. On thecontrary the difference values decrease considerably with respect to the values of Table 1 (38.2 vs. 19.4): it is important topoint out that with OAKplan-Ons it has been possible to use the solution plans stored in the case base directly to computethe distance values since these test problems and the planning cases have the same domain objects. In particular whilein DriverLog, Rovers and TPP the plans produced by OAKplan-Nns and OAKplan-Ons are very similar, in BlocksWorld andLogistics the plans produced by OAKplan-Nns are clearly worse than the corresponding ones produced by OAKplan-Onswith respect to the difference values. In the BlocksWorld domain the main difficulties are related to the very simple typedencoding which sometimes does not allow our kernel functions to easily identify the best object matching function. In thisdomain, the initial and goal state descriptions are very homogeneous since all objects are of the same type “Obj” and thisleads to many different matching possibilities. As regards the Logistic domain, the main drawbacks are related to the factthat sometimes some trucks are assigned to different cities with respect to the original ones, unfortunately in this domainthe trucks can be used only if they are positioned in specific cities and incorrect truck assignments could determine a highnumber of not applicable actions.Considering real word applications we think that the effective performance of OAKplan should be placed between theresults obtained by OAKplan-Nns and OAKplan-Ons. Although it is not realistic that all the current planning problem objectshave to belong to the case base, it is quite common that the domain topology does not change significantly during the systemevolution. For example, considering a department robotics domain where one or more robots have to move some packagesfrom different locations, it is reasonable that the department locations and the corresponding connections do not changesignificantly as time goes by (see Supplementary material for detailed results, e.g. Figs. A16–A21).4.2.3. Case base size analysisIn Table 7 we compare OAKplan-small-Nns to OAKplan-Nns. We can observe that the “small version” is clearly fasterthan the complete version since the number of cases is considerably lower; OAKplan-small-Nns is able to solve 29 moreproblems than OAKplan-Nns (up 2.3%) with an average CPU-time of 58.5 seconds (down 47% with respect to OAKplan-Nns)and if we consider the matching CPU-time, it requires 7.5 seconds (down 85% with respect to OAKplan-Nns). It follows thatthe number of problems solved, the plan quality and the difference values are very close (see Supplementary material fordetailed results and a graphical summary of the Wilcoxon test, e.g. Figs. A4–A6, A7 and A22–A38).Hence we can observe that OAKplan is significantly faster and solves more problems when it runs on small rather thanlarge case bases, with only minimal impact on solution quality, stability and differences. This clearly indicate the importancein CBP of developing highly scalable retrieval mechanisms to analyse efficiently the case base, in fact all CBP systems haveat least a retrieval component, and the success of a given system depends critically on the efficient retrieval of the rightcase at the right time. In this paper we consider a relatively simple screening procedure that filters out efficiently irrelevantcases; moreover our procedure could be combined with other retrieval techniques based on a model of case competence[57,61] so as to improve the global system efficiency, which is left as future work.I. Serina / Artificial Intelligence 174 (2010) 1369–14061393Fig. 14. Cumulative CPU-times (seconds) required by the different phases of OAKplan-Nns vs. the number of planning cases of the case base.Here we examine the CPU-time (in seconds) required by the different phases of OAKplan-Nns vs. the number of elementsin the corresponding case base considering some specific benchmark planning problems. In particular in Fig. 14 we show thecumulative CPU-time required by the different phases of OAKplan-Nns; the CPU-times can be simply derived by consideringthe distance of the corresponding line from the previous one. So we can obtain the CPU-times required:1. by the preprocessing phase so as to instantiate the data structures used by OAKplan, compute the mutex relations,connect to the case base and load the objects and predicated indexes;2. by the screening procedure to retrieve the degree sequences from the case base and compute the similds values(steps 1.4–1.7 of the RetrievePlan procedure);3. by the Planning Encoding Graph retrieval procedure and the computation of the Kbase kernel function on the casesselected in the previous phase (steps 2.1–2.4 of the RetrievePlan procedure);4. by the computation of the KN kernel function and the corresponding matching function on the cases selected in theprevious phase (steps 3.1–3.5 of the RetrievePlan procedure);5. by the evaluation of the selected plans so as to define the corresponding adaptation cost (steps 4.2–4.7 of the Retrieve-Plan procedure);6. by the lpg-adapt system to find a first solution; the adaptation time can be obtained by the difference between the Totaltime required to find a first solution and the total Evaluation time.Here we can observe that the screening procedure is extremely fast and the CPU-time required by the preprocessingand evaluation phases is always limited. Quite interesting in the BlocksWorld variant the CPU-time required by the KNcomputation is particularly relevant since the Kbase kernel function is not precise enough to filter out a significant numberof cases. In fact, in this domain, a correct matching of the objects of two different planning problems is particularly difficultsince all the objects are of the same type called “Obj” as exposed previously. We can also observe in the BlocksWorld variantthat the CPU-time required by the matching phase stabilises when the case base size is nearly of 5500 cases. This is causedby the maximum number of cases that can be examined at step 2.1 of RetrievePlan. Besides in the Rovers domain theCPU-time required to find a first solution plan is always very limited and in this case the CPU-times for the computationof the Kbase and KN kernel functions are comparable. In the DriverLog and in the ZenoTravel domains the matching and1394I. Serina / Artificial Intelligence 174 (2010) 1369–1406Fig. 15. Similarity values for the Neighbourhood kernel function, the Base kernel function, the Knode kernel function and the direct matching for the hardestproblems in the ZenoTravel and Rovers-StripsIPC5 domains.evaluation times are clearly dominated by the CPU-time required to find a first solution. Finally note that the first solutionproduced by lpg-adapt simply represents the first step of a potentially much longer incremental process.At last, we can observe a drop in the CPU-time required for the computation of the kernel functions in the Roverdomain in Fig. 14 when the case base size is close to 4000 cases. This drop is related to an insertion in the case baseof a planning case with a high similds screening value when the case base size varies from 3900 to 4000 instances. Thisscreening value determines a new best_ds_simil value and consequently the number of cases that satisfy the constraint“best_ds_simil − simili (cid:2) limit” at step 2.1 of Algorithm RetrievePlan at page 1384 decreases from 600 to 250. So the numberof Planning Encoding Graphs that have to be loaded and the number of kernel functions that have to be computed is clearlyless than the previous iteration and thus the corresponding CPU-time required for their computation (see Supplementarymaterial for detailed results, e.g. Fig. A11).4.3. Matching functions similarity resultsHere we examine the similarity values obtained with the Neighbourhood kernel function KN , the Base kernel functionKbase, the Knode kernel function and the direct matching function produced by OAKplan for the hardest problems of ourbenchmarks in relation to a progressive renaming of the domain objects involved. In this way we analyse the effectivenessof our matching processes in comparison to a “direct matching” process simply based on the object names of the planningproblems.In Fig. 15 we can see how the initial similarity values of the matching functions are equal to 1 for the plots on theleft which correspond to the original problems in the case base and close to 1 for the plots on the right where we change5 initial facts and 5 goal facts with respect to the corresponding element of the case base. As expected the similarityvalue of the direct matching progressively reduces itself to zero, whereas the similarity value of the KN kernel functionalways remains greater than 0.9. If we consider the Kbase and Knode kernel functions we can see that the correspondingsimilarity values progressively decrease with the increase of the number of renamed objects. This is acceptable and notcrucial for OAKplan since the Kbase function is essentially used in order to reduce the number of cases that must beI. Serina / Artificial Intelligence 174 (2010) 1369–14061395Fig. 16. Plan similarity values of KN vs. Kbase.evaluated accurately with KN . Considering the plots on the left, which are associated to the original problem in the casebase, we can observe that the similμN values are always equal to 1, showing that KN is able to match all the planningproblem objects correctly (see Supplementary material for additional results, e.g. Figs. A12–A13).In order to examine KN and Kbase more accurately, in Fig. 16 we compare their similarity values for all our 1296benchmark problems using the corresponding case base with all the objects renamed so as to verify the system behaviourwhen completely new problems are provided to OAKplan. Each point corresponds to the similarity values produced by KNand Kbase. If a point is above the solid diagonal, then KN performs better than Kbase and vice versa. Here we can seethat KN always performs better than Kbase and in only 38 variants similμN is less than 0.9. It is also important to pointout that in all our experiments we obtain a similarity value equal to one for all the test variants in which there is nomodifications with respect to the initial and goal facts (which correspond to the I0–G0 instances). In fact these planningproblems are already “present” in the case base and OAKplan identifies a mapping that correctly assigns all the objectsof the selected planning case to the objects of the current planning problem (see Supplementary material for additionalresults, e.g. Figs. A14–A15).4.4. OAKplan vs. state of the art plannersIn this section we analyse the OAKplan behaviour with respect to four state-of-the-art planners, showing its effectivenessin different benchmark domains; in particular, we consider metric-ff (winner of the 2nd IPC), lpg (winner of the 3rd IPC),downward (1st Prize, Suboptimal Propositional Track 4th IPC) and SGPlan-ipc5 (winner of the 5th IPC).In Fig. 17 we graphically report the number of solutions found, the average CPU-time of the solutions of downward,metric-ff and SGPlan-ipc5 and the CPU-time of the first solutions19 of lpg and OAKplan. Then we consider the average plandifference values expressed as the number of different actions with respect to the solution produced by the correspondingplanner on the problems used to generate the variants and the average best plan quality of the solutions generated byconsidering all benchmark domains.Here we can see that OAKplan can solve the greatest number of variants, followed by SGPlan-ipc5 and lpg. Regardingthe CPU-time, we remark that downward, lpg and OAKplan present similar computation time, while the CPU-time is moresignificant in metric-ff and SGPlan-ipc5. However these average values are computed only by considering the problemssolved by every single planner. In this case the SGPlan-ipc5 planner solves 211 variants in the TPP domain requiring942 seconds for them, whereas these variants only marginally influence the results of lpg. Regarding the difference valueswe can see that OAKplan clearly produces better results than the other planners. With respect to the plan quality we can19 Considering the median ones over five runs.1396I. Serina / Artificial Intelligence 174 (2010) 1369–1406Fig. 17. Summary results.note that metric-ff gives better results whereas OAKplan produces the worst results. We would like to point out thatin OAKplan the optimisation process tries to balance between good quality and low distance values since we are muchmore interested in generating a plan with a limited number of differences with respect to the target plan than producingsolutions of good quality. Moreover OAKplan is able to solve much more difficult planning problems than the other plannersand these solutions weigh significantly on the average plan quality produced.In Table 8 we report the summary results of OAKplan-Nns compared to the other planners. In the second columns wereport the number of the solutions found by the other planners, in the third columns we report the average speed of theproblems solved (in milliseconds), then the average plan qualities produced and finally the average plan stability and theaverage plan differences with respect to the solutions of the set of target plans produced by every single planner. In thebrackets we report the percent errors with respect to OAKplan-Nns: we consider only the problems solved by both plannersfor this comparison, except for the column of the solutions found.Downward cannot solve any problem in the Rovers domain. Globally it can solve 679 problems in comparison with the1232 solved by OAKplan-Nns. downward is 141% slower than OAKplan-Nns while their plan quality is comparable. Thedistance values of the plans generated by downward with respect to the solutions produced by the same planner on theproblems used to generate the variants is 411% greater than OAKplan-Nns. This high value is not particularly surprisingsince downward and the other planners do not know the target plans used for this comparison. Moreover the searchprocesses and the solution plans produced by a planner could be significantly different also for two planning instances thatonly differ in a single initial fact. These distance values are interesting since they are a clear indicator of the good behaviourof OAKplan and show that the generative approach is not feasible when we want to preserve the stability of the plansproduced.LPG can solve 840 of the 1296 variants, requiring 32% CPU-time more than OAKplan-Nns and the average distance ofthe solutions on target planning problems is 354 actions (which corresponds to +734% with respect to OAKplan-Nns).It is interesting to remark that the CPU-time needed by lpg to solve the Rovers variants (19.4 seconds) is significantlylower than in OAKplan (62.4 seconds) due to the additional CPU-time required by the matching process of OAKplan-Nns(53.7 seconds). The distance of the plans generated by lpg in this domain is 4201% greater than OAKplan-Nns.I. Serina / Artificial Intelligence 174 (2010) 1369–14061397Table 8Summary Tables of the different planners examined and a comparison with respect to the corresponding results produced by OAKplan-Nns.DomainSolutionsSpeedQualityStabilityDifferencesResults for DOVNWARD and percent errors of DOWNWARD vs. OAKplan-NnsBlocksWorldLogisticsDriverLogZenoTravelTPPTOTAL64.0 (−66%)198 (−7.0%)76.0 (−61%)130 (−38%)211 (+0.47%)679 (−45%)474335 (+437%)93899 (+19%)41738 (+381%)54752 (−27%)148591 (+444%)133420 (+141%)Results for LPG and percent errors of LPG vs. OAKplan-NnsTOTALBlocksWorldLogisticsDriverLogZenoTravelRoversTPP73.0 (−61%)211 (−0.9%)122 (−38%)216 (+2.4%)216 (+0.93%)2.00 (−99%)840 (−32%)94078 (+4.3%)139416 (+58%)108708 (+412%)174570 (+95%)19440 (−69%)496110 (+41415%)110054 (+52%)Results for Metric-FF and percent errors of Metric vs. OAKplan-Nns307669 (+429%)35598 (+345%)219264 (+200%)745702 (+1089%)899049 (+7054%)433941 (+737%)171 (−20%)65.0 (−67%)164 (−22%)198 (−7.5%)77.0 (−63%)675 (−45%)LogisticsDriverLogZenoTravelRoversTPPTOTALResults for SGPLAN-IPC5 and percent errors of SGPLAN-IPC5 vs. OAKplan-NnsLogisticsDriverLogZenoTravelRoversTPPTOTAL216 (+1.4%)106 (−46%)180 (−15%)216 (+0.93%)211 (+0.47%)929 (−25%)462093 (+404%)321346 (+2538%)171353 (+126%)163457 (+162%)942278 (+3414%)429328 (+644%)572 (+155%)353 (−4.8%)78.8 (+14%)128 (−23%)293 (−5.1%)281 (+5.9%)238 (+12%)451 (+16%)127 (+6.8%)202 (+2.3%)335 (−11%)393 (+69%)291 (+3.8%)304 (−8.0%)72.2 (+16%)123 (−29%)299 (−19%)246 (−3.9%)229 (−15%)414 (+4.6%)119 (+31%)142 (−23%)343 (−8.4%)314 (+1.6%)288 (−2.4%)0.60 (−32%)0.66 (−24%)0.45 (−46%)0.58 (−29%)0.45 (−53%)0.55 (−38%)0.71 (−19%)0.60 (−31%)0.32 (−63%)0.25 (−70%)0.18 (−82%)0.40 (−60%)0.37 (−58%)0.69 (−20%)0.19 (−77%)0.57 (−30%)0.16 (−84%)0.51 (−47%)0.44 (−51%)0.68 (−22%)0.18 (−79%)0.53 (−36%)0.15 (−84%)0.41 (−57%)0.41 (−55%)375 (+634%)242 (+217%)91.6 (+447%)85.7 (+72%)315 (+1403%)230 (+411%)149 (+160%)396 (+413%)199 (+956%)332 (+591%)489 (+4201%)468 (+46650%)354 (+734%)196 (+163%)124 (+655%)97.0 (+98%)391 (+3366%)240 (+1202%)227 (+500%)268 (+246%)190 (+971%)137 (+175%)467 (+4011%)354 (+1593%)300 (+710%)Metric-FF cannot solve any variant in the BlocksWorld domain. Globally it can solve 675 problems and is 757% slowerthan OAKplan-Nns while its plan quality is 15% better. Finally the distance of the plans generated by metric-ff with respectto the solutions produced by the same planner on the target problems is 500% greater than OAKplan-Nns.SGPlan-ipc5 planner can solve 929 problems and is 644% slower than OAKplan-Nns, the plan qualities are very similarand considering the distance of the plans generated by SGPlan-ipc5 are on average 710% greater than with OAKplan-Nns.Fig. 18 gives a graphical summary of the Wilcoxon results for the relative performance of OAKplan-Nns with downward,lpg, metric-ff and SGPlan-ipc5 in terms of CPU-time, plan quality and difference values for our benchmark problems (seeSupplementary material for detailed results, e.g. Figs. A8–A10). Here we can observe that OAKplan-Nns is statistically moreefficient values than all the other planners in terms of CPU-time and plan distance. On the contrary OAKplan-Nns and lpgproduce statistically worse plans from the quality point of view than the other planners, while metric-ff produces thehighest quality plans.Globally we can note that OAKplan-Nns is able to solve many more problems than the other planners and the firstsolution is usually generated in less time. In addition the distance values are significantly lower with respect to the targetplans although the quality of the plans produced is slightly worse than that of the plans produced by the other planner;this is also related to the optimisation performed by OAKplan where we try to minimise not only the plan quality but alsothe distance with respect to the solution plan of the planning case selected (see Supplementary material for detailed results,e.g. Figs. A39–A50).Finally in Fig. 19 we can observe the cumulative distribution of the total number of variants solved by the differentplanners vs. time. OAKplan-Nns is able to solve 1263 variants considering a maximum CPU-time of 1800 seconds even ifmost solutions are found in the first 800 seconds. A similar behaviour can be observed considering the lpg and downwardplanners although they are able to solve a lower number of variants. On the contrary metric-ff and SGPlan-ipc5 show aconstant increment of the number of variants solved, which are in any case less than the variants solved by OAKplan. TheCPU-time limit of 1800 seconds is used in the International Planning Competitions for the competitors evaluation and wethink that it is adequate for the evaluation of the planners used in practical applications (see Supplementary material foradditional results, e.g. Figs. A51–A63).1398I. Serina / Artificial Intelligence 174 (2010) 1369–1406Fig. 18. Partial order of the performance of OAKplan-Nns, downward, lpg, metric-ff and SGPlan-ipc5 according to the Wilcoxon signed rank test for ourbenchmark problems.Fig. 19. Cumulative distribution of the total number of variants solved by the different planners vs. time.5. Related work on case-based planningIn the following section we examine the most relevant case-based planners considering their retrieval, adaptation andstorage capabilities. Moreover, we present an empirical comparison of the performance of OAKplan vs. the far-off systemand some comments on the advantages of OAKplan with respect to other case-based planners.Some CBP systems designed in the past do not consider any generative planning in their structure, and find a solutiononly by the cases stored in the case base. These CBP systems are called reuse-only systems. As reuse-only systems cannotfind any planning solution from scratch, they cannot find a solution unless they find a proper case in the case base thatI. Serina / Artificial Intelligence 174 (2010) 1369–14061399can be adapted through single rules. An alternative approach to reuse-only systems is the reuse-optional approach, whichuses a generative planning system that is responsible to adapt the retrieved cases. This feature allows a CBP system tosolve problems that cannot be solved only by using stored cases and simple rules in the adaptation phase. Empirically,a great number of reuse-optional CBP systems has shown that the use of a case base can permit them to perform better inprocessing time and in a number of planning solutions than the generative planning that they incorporate.Obviously the retrieval phase critically affects the systems performance; it must search in a space of cases in order tochoose a good one that will allow the system to solve a new problem easily. In order to improve efficiency in the retrievalphase, it is necessary either to reduce the search space or to design an accurate similarity metric. Reducing the searchspace, only a suitable subset of cases will be available for the search process and an accurate similarity metric will choosethe most similar case to decrease the adaptation phase effort. In the literature there are different domain dependent and afew domain independent plan adaptation and case-based planning systems, which mostly use a search engine based on aspace of states [28,29,61,62]. An alternative approach to planning with states is that of plan-space planning or hierarchicalsystems [4] that search in a space of plans and have no goals, but only tasks to be achieved. Since tasks are semanticallydifferent from goals, the similarity metric designed for these CBP systems is also different from the similarity rules designedfor state-space based CBP systems. For a detailed analysis of case-based and plan adaptation techniques see the papers ofSpalazzi [58] and Munoz-Avila and Cox [45].Three interesting works developed at the same time adopt similar assumptions: the priar system [34], the spa system[29] and the Prodigy/Analogy system [63,64]. priar uses a variant of Nonlin [60], a hierarchical planner, whereas spa usesa constraint posting technique similar to Chapman’s Tweak [10] as modified by McAllester and Rosenblitt [43]. priar’splan representation and thus its algorithms are more complicated than those of spa. There are three different types ofvalidations (filter condition, precondition, and phantom goal) as well as different reduction levels for the plan that representsa hierarchical decomposition of its structure, along with five different strategies for repairing validation failures. In contrastto this representation the plan representation of spa consists of causal links and step order constraints. The main ideabehind the spa system that separates it from the systems mentioned above is that the process of plan adaptation is a fairlysimple extension of the process of plan generation. In the spa view, plan generation is just a special case of plan adaptation(one in which there is no retrieved structure to exploit). With respect to our approach that defines a matching functionμ from Π to Π (cid:9)that maximises the similarity function similμ, it should be noted that in priar and spa the conditionsfor the initial state match are slightly more complicated. In priar the number of inconsistencies in the validation structure ofthe plan library is minimised; in spa the number of violations of preconditions in the plan library is maximised. Moreoverthe problem-independent matching strategy implemented in spa runs in exponential time in the number of objects since itsimply evaluates all possible mappings. On the contrary we compute an approximate matching function in polynomial timeand use an accurate plan evaluation function on a subset of the plans in the library.The Prodigy/Analogy system also uses a search oriented approach to planning. A library plan (case) in a transformationalor case-based planning framework stores a solution to a prior problem along with a summary of the new problems forwhich it would be a suitable solution, but it contains little information on the process that generates the solution. On theother hand derivational analogy stores substantial descriptions of the adaptation process decisions in the solution, whereasVeloso’s system records more information at each choice point than spa does, like a list of failed alternatives. An interestingsimilarity rule in the plan-space approach is presented in the caplan/cbc system [46] which extends the similarity ruleintroduced by the Prodigy/Analogy system [63,64] by using feature weights in order to reduce the errors in the retrievalphase. There are two important differences between our approach and the similarity rules of caplan/cbc, one of which isthat the former is designed for state-space planning and the latter for plan-space planning. Another difference is that ourretrieval function does not need to learn any knowledge to present an accurate estimate: our retrieval method only needsthe knowledge that can be extracted from the problem description and the actions of the planning cases.mlr [47] is another case-based system and it is based on a proof system. While retrieving a plan from the library thathas to be adapted to the current world state, it makes an effort to employ the retrieval plan as if it were a proof toset the goal conditions from the start. Should this happen, there is no need for any iteration to use the plan, otherwise,the outcome is a failed proof that can provide refitting information. On the basis of the failed proof, a plan skeleton isbuilt through a modification strategy and it makes use of the failed proof to obtain the parts of the plan that are usefuland removes the useless parts. After the computation of this skeleton, gaps are filled through a refinement strategy whichmakes use of the proof system. Although our object matching function is inspired to the Nebel and Koehler’s formalisation,our approach significantly differs from theirs since they do not present an effective domain independent matching function.In fact, their experiments exhibit an exponential run time behaviour for the matching algorithm they use, instead we showthat the retrieval and matching processes can be performed efficiently also for huge plan libraries. The matching functionformalisation proposed by Nebel and Koehler also tries to maximise first the cardinality of the common goal facts set andsecond the cardinality of the common initial facts set. On the contrary we try to identify the matching function μ thatmaximise the similμ similarity value which considers both the initial and goal relevant facts and an accurate evaluationfunction based on a simulated execution of the candidate plans is used to select the best plan that has to be adapted.Nebel and Koehler [47] present an interesting comparison of the mlr, spa and priar performance in the BlocksWorlddomain considering planning instances with up to 8 blocks. They show that also for these small sized instances and usinga single reuse candidate the matching costs are already greater than adaptation costs. When the modification tasks becomemore difficult, since the reuse candidate and the new planning instance are structurally less similar, the savings of plan1400I. Serina / Artificial Intelligence 174 (2010) 1369–1406modification become less predictable and the matching and adaptation effort is higher than the generation from scratch. Onthe contrary OAKplan shows good performance with respect to plan generation and our tests in the BlocksWorld domainconsider instances with up to 140 blocks and a plan library with ten thousands cases.A very interesting case-based planner is the far-off20 (Fast and Accurate Retrieval on Fast Forward) system [61]. It uses agenerative planning system based on the FF planner [32] to adapt similar cases and a similarity metric, called ADG (ActionDistance-Guided), which, like EvaluatePlan, determines the adaptation effort by estimating the number of actions that isnecessary to transform a case into a solution of the problem. The ADG similarity metric calculates two estimate values ofthe distance between states. The first value, called initial similarity value, estimates the distance between the current initialstate I and the initial state of the case Iπ building a relaxed plan having I as initial state and Iπ as goal state. Similarlythe second value, called goal similarity value, estimates the distance between the final state of the case and the goals ofthe current planning problem. Our EvaluatePlan procedure evaluates instead every single inconsistency that a case basesolution plan determines in the current world state I .The far-off system uses a new competence-based method, called Footprint-based Retrieval [57], to reduce the space ofcases that will be evaluated by ADG. The Footprint-based Retrieval is a competence-based method for determining groupsof footprint cases that represent a smaller case base with the same competence of the original one. Each footprint case hasa set of similar cases called Related Set [57]. The union of footprint cases and Related Set is the original case base. On thecontrary OAKplan uses a much more simple procedure based on the similds function to filter out irrelevant cases. The use ofFootprint-based Retrieval techniques and case base maintenance policies in OAKplan is left for future work. It is importantto point out that the retrieval phase of far-off does not use any kind of abstraction to match cases and problems.The far-off system retrieves the most similar case, or the ordered k most similar cases, and shifts to the adaptationphase. Its adaptation process does not modify the retrieved case, but only completes it; it will only find a plan that beginsfrom the current initial state and then goes to the initial state of the case, and another plan that begins from the stateobtained by applying all the actions of the case and goes to a state that satisfies the current goals G. Obviously, thecompleting of cases leads the far-off system to find longer solution plans than generative planners, but it avoids wastingtime in manipulating case actions in order to find shorter solutions length. To complete cases, the far-off system usesa FF-based generative planning system, where the solution is obtained by merging both plans that are found by the FF-based generative planning and the solution plan of the planning case selected. On the contrary OAKplan uses the lpg-adaptadaptation system, which uses a local search approach and works on the whole input plan so as to adapt and find a solutionto the current planning problem.In Fig. 20 we can observe the behaviour of OAKplan vs. far-off considering different variants of the greater case basesprovided with the far-off system in the Logistics domain21; similar results have been obtained in the BlocksWorld, Driver-Log and ZenoTravel domains. Globally, we can observe that far-off is always faster than OAKplan both considering theretrieval and the total adaptation time although also the OAKplan CPU-time is always lower than 0.6 seconds. ConsideringOAKplan, most of the CPU-time is devoted to the computation of the matching functions which are not computed by far-off since it simply considers the identity matching function that directly assigns the objects of the case base to those of thecurrent planning problem with the same name. In fact, it does not consider objects which are not already present in thecase base and, to overcome this limitation, the variants used in this test are directly obtained by the problems stored in thecase bases.Regarding the plan qualities22 and the plan distances, it is important to point out that for each variant solved by OAK-plan we consider only the first solution produced since far-off does not perform a plan optimisation process. HoweverOAKplan is able to obtain better plans both considering the plan quality and the plan distance values. Globally, OAKplan isable to find plans with 20% better quality and 24% better plan distances. Moreover further improvements on plan qualitiesand distance values of OAKplan could be obtained by performing the optimisation process of lpg-adapt.Finally, note that in this experiment we have used the case bases provided by far-off which contain 700 elements eachand the corresponding cases are generated by creating randomly planning problems all with the same configuration: sameobjects, trucks and airplanes simply disposed in different ways. This kind of experiment is highly unfavourable to OAKplansince our first screening procedure cannot filter out a significant number of cases as they all have the same structure.On the contrary, in the experiments described in the previous sections the case bases used by OAKplan in the standardconfiguration (not the “small” versions) are not constrained to a particular planning problem but they have been generatedby considering all the different planning problems configurations used in the International Planning Competitions. This isa much more realistic situation, where the cases are added to the case base when the planning problems provided by theusers are resolved as time goes by.20 far-off is available at http://www.fei.edu.br/~flaviot/faroff.21 We have used the case bases for the logistics-16-0, logistics-17-0 and logistics-18-0 Logistics IPC2 problems. For each problem considered the far-offsystem must have a case base with the same structure to perform tests. More than 700 cases belong to each case base and for each case base we haveselected two planning cases and randomly generated 36 variants.22 In STRIPS domains the plan quality is obtained by considering the number of actions in the solution plan.I. Serina / Artificial Intelligence 174 (2010) 1369–14061401Fig. 20. CPU-time, plan qualities and number of different actions for the Logistics variants. Here we examine OAKplan vs. far-off.6. Summary and future workCBP systems can take advantage of plan reuse where possible. The success of these systems depends on the ability toretrieve old cases that are similar to the target problem and to adapt these cases appropriately. In this paper our aim is toprovide a new and effective case-based planner which is able to retrieve planning cases from huge plan libraries efficiently,choose a good candidate and adapt it in order to provide a solution plan which has good plan quality and is similar tothe plan retrieved from the case base. We have described a novel case-based planning system, called OAKplan, which usesideas from different research areas showing excellent performance in many standard planning benchmark domains. In thispaper we have analysed the main components of our CBP system, which presents significant improvements as to the stateof the art especially in the filtering and retrieval phases.Experimental results show the crucial importance of an accurate matching function for the global system performance,not only in order to obtain low distance values but also to solve a reasonable number of planning problems. To the bestof our knowledge this is the first case-based planner that performs an efficient domain independent objects matchingevaluation on plan libraries with thousands of cases.We have examined OAKplan in comparison with four state of the art plan generation systems showing its extremelygood performance in terms of the number of problems solved, CPU time, plan difference values and plan quality. Results arevery encouraging and show that the case-based planning approach can be an effective alternative to plan generation when“sufficiently similar” reuse candidates can be chosen. This happens to different practical applications especially when the“world is regular” and the types of problems the agents encounter tend to recur. Moreover this kind of approach could beextremely appealing in situations in which the “stability” of the plan produced is fundamental. This is the case, for example,in mission critical applications where end users do not accept newly generated plans and prefer to use known plans thathave already been successful in analogous situations and can be easily validated.We believe that even more significant results will come from combining our approach with ideas and methods thathave been developed in planning, case-based reasoning, graph theory and supervised learning research areas. Specifically,directions we are considering include:1402I. Serina / Artificial Intelligence 174 (2010) 1369–1406• Case base maintenance: the efficiency of the retrieval phase can be improved by using case base maintenance policiesand a more thorough evaluation of the competence of the library as proposed by [57,61].• Graph representation: our current graph representation is based only on the initial and goal states of the planning prob-lem examined; a more accurate representation could try to consider the actions in the solution plans and the domainoperators available, or give more importance to the most relevant initial state facts. It could also be very interestingto extend our graph representation to afford temporal and numeric planning problems effectively. In fact, althoughOAKplan can afford temporal and metric domains, the numeric description of the planning problems examined is notactually used in the definition of the Planning Encoding Graph and in the corresponding kernel functions, determiningpotential low performance.• Matching functions: new and more effective matching functions could be obtained by considering additional informationthat can be derived from domain analysis such as invariants [17,26] and symmetries [18]. These functions may alsobe defined by examining particular structures of the Planning Encoding Graphs like cliques, line-graphs, or using newapproaches derived by graph matching and graph edit distance techniques [6,23,48,49,52].• Learning: these techniques found to be useful in different planning methods; our kernel functions can be pluggedinto any kernel-based machine learning algorithm, like, e.g., SVMs [15], SVR and Kernel PLS [53] to better classify theplanning cases or improve the matching functions themselves.• Adaptation: our retrieval/evaluation/update techniques are independent by the adaptation mechanism adopted and otheradaptation methods like adjust-plan [27,28] and POPR [62] could be effectively used as well.AcknowledgementsThis research was supported by the research project “Study and Design of a Prototype of an Intelligent Planning Sys-tem for the Building of Learning Paths” of the Free University of Bozen. We thank Piergiorgio Bertoli, Alfonso E. Gerevini,Alessandro Saetti and especially the anonymous referees for their helpful comments. The authors would like to thank FlavioTonidandel and Márcio Rillo for putting their benchmark set and the far-off planning system at our disposal.Appendix B. ProofsTheorem 3. obj_match is NP-hard.Proof. Similarly to the Nebel and Koehler’s analysis [47], NP-hardness is proved by a polynomial transformation from thesubgraph isomorphism problem for directed graphs, which is NP-Complete [22, p. 202], to obj_match. The subgraphisomorphism problem is defined as follows:Instance: Two directed Graphs G 1 = (V 1, E1) and G 2 = (V 2, E2).Question: Does G 2 contain a subgraph isomorphic to G 1, i.e., do there exist a subsets V ⊆ V 2 and a subset E ⊆ E2 suchthat |V | = |V 1|, |E| = |E1|, and there exists a one-to-one function μ : V 1 → V satisfying (u, v) ∈ E1 if and only if(μ(u), μ(v)) ∈ E?Given an instance of the subgraph isomorphism problem, we construct an instance of obj_match as follows; letΠ1 =(cid:9)Pr(O1, P1), I1, G1, O p1(cid:10),(cid:9)Π2 =Pr(O2, P2), I2, G2, O p2(cid:10)be two planning instances such thatO1 = O2 = V 1 ∪ V 2,G1 =p(u, v)(cid:13)(cid:17)(cid:17) (u, v) ∈ E2Now G 2 contains a subgraph isomorphic to G 1 iff there exists a mapping μ(·) such that |μ(G1) ∩ G2| = |E1| and(cid:17)(cid:17) (u, v) ∈ E1= O p2.p(u, v)G2 =O p1(cid:14),,P1 = P2 = {p},(cid:13)(cid:14)I1 = I2 = ∅,similμ(Π1, Π2) =|μ(G1) ∩ G2| + |μ(I1) ∩ I2||G2| + |μ(I1)|=|E1| + |∅||E2|.Note that G 2 is isomorphic to G 1 iff there exists a mapping μ(·) such thatsimilμ(Π1, Π2) = 1. (cid:2)|μ(G1) ∩ G2| = |E1| = |E2| andTheorem 4. R1 is a kernel function.Proof. We have to show that given a set of patterns x1, . . . , xn the kernel matrix R = (R1(xi, x j))i, jpositive semidefinite [56].is symmetric andClearly, R1 is symmetric, because of the definition. Let ζ denote a permutation of an n-subset of natural numbers1, . . . , m, or a permutation of an m-subset of natural numbers 1, . . . , n, respectively; then for any ζ it isI. Serina / Artificial Intelligence 174 (2010) 1369–1406kv(cid:23)· ke(cid:12)(cid:12)(cid:11)(cid:11)eζ (1)(x), e1(x)nζ (1)(x), n1(x)(cid:12)(cid:11)(cid:2) 1n1(x), n1(x)kv2(cid:12)(cid:11)+ kvnn(x), nn(x)(cid:6)(cid:12)(cid:11)ni(x), ni(x)(cid:12)(cid:11)· kee1(x), e1(x)(cid:12)(cid:11)en(x), en(x)(cid:12)(cid:11)ei(x), ei(x)· ke· kekv=(cid:12)(cid:11)+ · · · + kvnζ (n)(x), nn(x)(cid:12)(cid:11)+ kvnζ (1)(x), nζ (1)(x)(cid:12)(cid:11)nζ (n)(x), nζ (n)(x)+ kv(cid:12)(cid:11)eζ (n)(x), en(x)· ke(cid:12)(cid:11)eζ (1)(x), eζ (1)(x)(cid:12)(cid:24)· ke(cid:11)eζ (n)(x), eζ (n)(x)· ke+ · · ·1403(B.2)(B.3)(B.4)ibecause for any i2 · kv(cid:11)(cid:12)nζ (i)(x), ni(x)(cid:11)(cid:12)ni(x), ni(x)(cid:11)(cid:12)eζ (i)(x), ei(x)· ke(cid:11)(cid:12)· keei(x), ei(x)+ kv(cid:2) kv(cid:11)(cid:12)nζ (i)(x), nζ (i)(x)(cid:11)(cid:12)eζ (i)(x), eζ (i)(x)· kesince kv and ke are positive semidefinite kernel functions and the product of two kernel functions is a kernel function. Now,if we take the maximum over all ζ then R1(x, x) = (B.2) = (B.3) = (B.4).Similarly R1(y, y) =j kv (n j(y), n j(y)) · ke(e j(y), e j(y)). Without loss of generality we can assume that |y| (cid:3) |x|. Further(cid:5)it holds for all α, β ∈ R and i, j(cid:11)(cid:12)ei(x), e j(y)(cid:11)(cid:12)ni(x), n j(y)2αβkv· ke(cid:11)(cid:12)ni(x), ni(x)(cid:2) α2kv(cid:11)(cid:12)ei(x), ei(x)+ β 2kv(cid:11)(cid:12)n j(y), n j(y)(cid:11)(cid:12)e j(y), e j(y)· ke· kebecause kv and ke are positive semidefinite kernel functions. It isα2 R1(x, x) − 2αβ R1(x, y) + β 2 R1(y, y)(cid:6)= α2(cid:11)(cid:12)ni(x), ni(x)(cid:11)(cid:12)ei(x), ei(x)· ke(cid:11)(cid:12)n j(y), n j(y)(cid:11)(cid:12)e j(y), e j(y).· kekvi+ β 2kv(cid:6)j− 2αβ maxπ(cid:11)(cid:12)ni(x), nπ (i)(y)kv(cid:11)(cid:12)ei(x), eπ (i)(y)· ke(cid:6)iBy definition of R1 the second sum of previous equation has min(|x|, |y|) = |x| addends. Using (B.5) we have|x|(cid:6)(B.6) (cid:3)(cid:11)(cid:12)ni(x), ni(x)(cid:11)(cid:12)ei(x), ei(x)· keα2kv− 2αβkv(cid:11)(cid:12)ni(x), nζ (i)(y)(cid:11)(cid:12)ei(x), eζ (i)(y)· kei=1+ β 2kv(cid:11)(cid:12)nζ (i)(y), nζ (i)(y)(cid:11)(cid:12)eζ (i)(y), eζ (i)(y)· ke(cid:3) 0.This proves the positive semidefiniteness of each 2 × 2 kernel matrix. From this we can generalise the result to n × nmatrices by induction using the assumption that kv and ke are non-negative. Suppose we already know that each n × nkernel matrix R = (R1(xi, x j))i, j for a set of objects x1, . . . , xn is positive semidefinite. Now assume we extend the matrixto size n + 1 × n + 1 by adding an object xn+1. It isn+1(cid:6)i, j=1viv jRi, j =n(cid:6)i, j=1viv jRi, j + 2n(cid:6)j=1vn+1v jRn+1, j + v2n+1Rn+1,n+1.(B.8)By induction assumption we know the first part of (B.8) to be non-negative. Furthermore, by definition kv and ke aren+1Rn+1,n+1 (cid:3) 0.non-negative and thus also R1 is non-negative. Hence, we have v2(cid:5)Therefore, in order to make (B.8) < 0 we have to suppose 2 ·nj=1 vn+1v jRn+1, j < 0 and similarly to (B.5) we have that2 · vn+1v jRn+1, j (cid:2) v2n+1Rn+1,n+1 + v2j R j, j.This leads ton(cid:6)2 ·j=1vn+1v jRn+1, j (cid:2)n+1Rn+1,n+1 + v2v2j R j, j(cid:12)< 0n(cid:6)(cid:11)j=1which is a contradiction to the non-negativity of R1. Hence, it is (B.8) (cid:3) 0, which proofs the theorem. (cid:2)Theorem 5. Let γ (l) = pl with p ∈ (0, 0.5). If there exists a C ∈ R+e1, e2 then Eq. (10) converges for L → ∞., such that kv (v 1, v 2) (cid:2) C for all v 1, v 2 and ke(e1, e2) (cid:2) 1 for all(B.5)(B.6)(B.7)1404Proof. It isI. Serina / Artificial Intelligence 174 (2010) 1369–1406R1(u, v) (cid:2)1max(|E(u)|, |E(v)|)(cid:2) min(|E(u)|, |E(v)|)max(|E(u)|, |E(v)|)min(|E(u)|,|E(v)|)(cid:6)i=1C (cid:2) Cmaxv1,v2kv (v 1, v 2) · maxe1,e2ke(e1, e2)and thusR2(u, v) (cid:2)1|N i(v)| · |N i(u)|(cid:6)h,h(cid:9)+1|N o(v)| · |N o(u)|(cid:11)h(v), nini(cid:12)h(cid:9) (u)(cid:11)eh(v), ei(cid:12)h(cid:9) (u)ke(cid:9) (u)· maxeih(v),ei(cid:12)h(cid:9) (u)h· maxh(v),eoeoh(cid:9) (u)(cid:11)h(v), nono(cid:11)h(v), eoeo(cid:12)h(cid:9) (u)keR1maxh(v),nini(cid:6)h(cid:9) (u)maxh(v),nonoh(cid:9) (u)R1h,h(cid:9)|N i (v)|(cid:6)|N i (u)|(cid:6)(cid:2)1|N i(v)| · |N i(u)|C +1|N o(v)| · |N o(u)||N o(v)|(cid:6)|N o(u)|(cid:6)i=1j=1C = 2 · C.i=1j=1Similarly we can show that Rl(u, v) (cid:2) 2l−1C for l = 3, . . . , L. Therefore we haveEq. (10) (cid:2) C + pC + p22C + · · · + p L2L−1C (cid:2) C +L(cid:6)(2p)lCl=1which converges for L → ∞ if p ∈ (0, 0.5). (cid:2)Appendix C. VariantsAs previously described (see p. 1388), our tests have been conducted on a series of variants of problems from differentstandard benchmark domains of the 2nd, 3rd and 5th International Planning Competitions.23 The variant problems havebeen generated by taking six problems from each benchmark test suite (except for the Logistics domain) and then randomlymodifying the initial state and goal states for a total of 216 planning problems for each domain. The problems consideredare:• probblocks-40-0, probblocks-60-0, probblocks-80-0, probblocks-100-0, probblocks-120-1 and probblocks-140-1 forBlocksWorld Additionals;• randomly selected from logistics-16-0 to logistics-100-1 for Logistics Additionals Track2;• pfile14, pfile17, pfile20, pfile-HC03, pfile-HC06, pfile-HC09 for DriverLog;• pfile14, pfile17, pfile20, pfile-HC14, pfile-HC17, pfile-HC20 for ZenoTravel;• pfile35, pfile36, pfile37, pfile38, pfile39, pfile40 for Rovers-IPC5;• pfile25, pfile26, pfile27, pfile28, pfile29, pfile30 for TPP.In the following we present a brief description of the operators used in order to modify the initial and the goal states ofa base problem. In order to modify the initial state, we randomly choose a completely instantiated “noisy” operator amongthose with all the preconditions satisfied; the effects of the “noisy” operator determine a new initial state.With respect to the goals, we propagate the effects of the actions of the solution plan of the base problem in order todefine a complete goal state; then we randomly choose a completely instantiated “noisy” operator among those with all thepreconditions satisfied and at least one goal of the base problem that belongs to them. The effects of the “noisy” operatorchange the goal state; in particular the negative effects delete the corresponding goals, while the positive effects are addedto the goal set.BlocksWorld: The NOISE-falldown noisy operator is used to randomly split a pile of blocks into piles; on the contrarythe NOISE-pile-up operator is used to randomly pile up a pile of blocks on the top of another one.DriverLog: The NOISE-move-package and NOISE-move-driver noisy operators are used to randomly change thelocation of a package and of a driver respectively.Logistics: The NOISE-move-package and the NOISE-fly-airplane noisy operators are used to change the locationof a package and of an airplane respectively.23 The IPCs test problems that we have used are available at the following websites: for IPC2, http://www.cs.toronto.edu/aips2000/; for IPC3,http://planning.cis.strath.ac.uk/competition/; for IPC5, http://ipc5.ing.unibs.it.I. Serina / Artificial Intelligence 174 (2010) 1369–14061405Rovers: The NOISE-communicate_soil_data, the NOISE-communicate_rock_data and the NOISE-communi-cate_image_data noisy operators are used to change the status (either “communicated” or “not communicated”)of soil data, rock data and image data respectively from a waypoint x to a waypoint y.The NOISE_drive and the NOISE_on-sale noisy operators are used to randomly change the location of atruck and to randomly change the sale conditions of some goods respectively.TPP:ZenoTravel: For this domain, we defined five noisy operators:• the NOISE-fuel operator can be used to randomly change the fuel level of an aircraft;• the NOISE-fly and the NOISE-zoom operators can be used to randomly modify the location of an aircraftusing different amount of fuel;• the NOISE-move-package operator can be used to randomly change the location of a package;• the NOISE-debark operator can be used to randomly modify the objects inside an aircraft.Supplementary dataSupplementary data associated with this article can be found, in the online version, at doi:10.1016/j.artint.2010.07.007.References[1] A. Aamodt, E. Plaza, Case-based reasoning: foundational issues, methodological variations, and system approaches, AI Commun. 7 (1) (March 1994)39–59.[2] T. Au, H. Muñoz-Avila, D.S. Nau, On the complexity of plan adaptation by derivational analogy in a universal classical planning framework, in: Proceed-ings of the 6th European Conference on Advances in Case-Based Reasoning, Springer-Verlag, London, UK, 2002, pp. 13–27.[3] F. Bacchus, F. Kabanza, Using temporal logic to express search control knowledge for planning, Artificial Intelligence 116 (1–2) (2000) 123–191.[4] R. Bergmann, W. Wilke, Building and refining abstract planning cases by change of representation language, Journal of Artificial Intelligence Research 3(1995) 53–118.[5] W.D. Blizard, Multiset theory, Notre Dame Journal of Formal Logic 30 (1) (1989) 36–66.[6] H. Bunke, Recent developments in graph matching, in: 15th International Conference on Pattern Recognition, vol. 2, 2000, pp. 117–124.[7] T. Bylander, An average case analysis of planning, in: Proceedings of the Eleventh National Conference of the American Association for ArtificialIntelligence (AAAI-93), AAAI Press/MIT Press, Washington, DC, 1993, pp. 480–485.[8] T. Bylander, The computational complexity of propositional STRIPS planning, Artificial Intelligence 69 (1994) 165–204.[9] T. Bylander, A probabilistic analysis of propositional STRIPS planning, Artificial Intelligence 81 (1–2) (1996) 241–271.[10] D. Chapman, Planning for conjunctive goals, Artificial Intelligence 32 (3) (1987) 333–377.[11] M. Chein, M. Mugnier, Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs, Springer, 2008.[12] Y.P. Chien, A. Hudli, M. Palakal, Using many-sorted logic in the object-oriented data model for fast robot task planning, Journal of Intelligent andRobotic Systems 23 (1) (1998) 1–25.[13] A.G. Cohn, Many sorted logic = unsorted logic + control?, in: Proceedings of Expert Systems ’86, the 6th Annual Technical Conference on Researchand Development in Expert Systems III, Cambridge University Press, New York, NY, USA, 1987, pp. 184–194.[14] L.P. Cordella, P. Foggia, C. Sansone, M. Vento, A (sub)graph isomorphism algorithm for matching large graphs, Pattern Analysis and Machine IntelligenceIEEE Transactions on 26 (10) (2004) 1367–1372.[15] N. Cristianini, J. Shawe-Taylor, An Introduction to Support Vector Machines, Cambridge University Press, Cambridge, 2000.[16] M. Fox, A. Gerevini, D. Long, I. Serina, Plan stability: Replanning versus plan repair, in: Proceedings of International Conference on AI Planning andScheduling (ICAPS), AAAI Press, 2006.[17] M. Fox, D. Long, The automatic inference of state invariants in TIM, Journal of Artificial Intelligence Research (JAIR) 9 (1998) 367–421.[18] M. Fox, D. Long, The detection and exploitation of symmetry in planning problems, in: Proceedings of the 16th International Joint Conference onArtificial Intelligence (IJCAI-99), 1999, pp. 956–961.[19] H. Fröhlich, J.K. Wegner, F. Sieker, A. Zell, Optimal assignment kernels for attributed molecular graphs, in: L. De Raedt, S. Wrobel (Eds.), ICML, in: ACMInternational Conference Proceeding Series, vol. 119, ACM, 2005, pp. 225–232.[20] H. Fröhlich, J.K. Wegner, F. Sieker, A. Zell, Kernel functions for attributed molecular graphs – a new similarity based approach to ADME prediction inclassification and regression, QSAR Comb. Sci. 25 (2006) 317–326.[21] H. Fröhlich, J.K. Wegner, A. Zell, Assignment kernels for chemical compounds, in: International Joint Conference on Neural Networks 2005 (IJCNN’05),2005, pp. 913–918.[22] M.R. Garey, D.S. Johnson, Computers and Intractability: A Guide to the Theory of NP-Completeness, Series of Books in the Mathematical Sciences, W.H.Freeman, 1979.[23] T. Gärtner, A survey of kernels for structured data, SIGKDD Explor. Newsl. 5 (1) (2003) 49–58.[24] D. Gentner, The mechanisms of analogical learning, in: B.G. Buchanan, D.C. Wilkins (Eds.), Readings in Knowledge Acquisition and Learning: Automatingthe Construction and Improvement of Expert Systems, Kaufmann, San Mateo, CA, 1993, pp. 673–694.[25] A. Gerevini, A. Saetti, I. Serina, Planning through stochastic local search and temporal action graphs, Journal of Artificial Intelligence Research (JAIR) 20(2003) 239–290.[26] A. Gerevini, L. Schubert, On point-based temporal disjointness, Artificial Intelligence 70 (1994) 347–361.[27] A. Gerevini, I. Serina, Plan adaptation through planning graph analysis, in: AI*IA 99, in: Lecture Notes in Artificial Intelligence, Springer-Verlag, 1999,pp. 356–367.[28] A. Gerevini, I. Serina, Fast plan adaptation through planning graphs: Local and systematic search techniques, in: Proceedings of the 5th InternationalConference on Artificial Intelligence Planning and Scheduling (AIPS-00), AAAI Press/MIT Press, 2000, pp. 112–121.[29] S. Hanks, D.S. Weld, A domain-independent algorithm for plan adaptation, Journal of Artificial Intelligence Research (JAIR) 2 (1995) 319–360.[30] P. Hansen, Upper bounds for the stability number of a graph, Rev. Roumaine Math. Pures Appl. 24 (1979) 1195–1199.[31] D. Haussler, Convolution kernels on discrete structures, Technical Report UCS-CRL-99-10, UC Santa Cruz, 1999.[32] J. Hoffmann, B. Nebel, The FF planning system: Fast plan generation through heuristic search, Journal of Artificial Intelligence Research (JAIR) 14 (2001)253–302.[33] M. Johnson, Relating Metrics, Lines and Variables Defined on Graphs to Problems in Medicinal Chemistry, John Wiley & Sons, Inc., New York, NY, USA,1985.[34] S. Kambhampati, J.A. Hendler, A validation-structure-based theory of plan modification and reuse, Artificial Intelligence 55 (1992) 193–258.1406I. Serina / Artificial Intelligence 174 (2010) 1369–1406[35] H. Kashima, K. Tsuda, A. Inokuchi, Marginalized kernels between labeled graphs, in: T. Fawcett, N. Mishra (Eds.), ICML, AAAI Press, 2003, pp. 321–328.[36] V. Kuchibatla, H. Muñoz-Avila, An analysis on transformational analogy: General framework and complexity, in: ECCBR, in: Lecture Notes in ComputerScience, vol. 4106, Springer, 2006, pp. 458–473.[37] H.W. Kuhn, The Hungarian method for the assignment problem, Naval Research Logistic Quarterly 2 (1955) 83–97.[38] D.B. Leake (Ed.), Case-Based Reasoning, MIT Press, Cambridge, MA, 1996.[39] P. Liberatore, On the complexity of case-based planning, Journal of Experimental & Theoretical Artificial Intelligence 17 (3) (2005) 283–295.[40] D. Lin, An information-theoretic definition of similarity, in: J.W. Shavlik (Ed.), ICML, Morgan Kaufmann, 1998, pp. 296–304.[41] R.Y. Liu, An upper bound on the chromatic number of a graph, J. Xinjiang Univ. Natur. Sci. 6 (1989) 24–27.[42] D. Long, M. Fox, The 3rd international planning competition: Results and analysis, Journal of Artificial Intelligence Research (JAIR) 10 (2003) 1–59.[43] D. McAllester, D. Rosenblitt, Systematic nonlinear planning, in: Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), July1991, pp. 634–639.[44] J. Mercer, Functions of positive and negative type and their connection with the theory of integral equations, Philos. Trans. Roy. Soc. London 209 (1909)415–446.[45] H. Muñoz-Avila, M. Cox, Case-based plan adaptation: An analysis and review, IEEE Intelligent Systems 23 (4) (2008) 75–81.[46] H. Muñoz-Avila, J. Hüllen, Feature weighting by explaining case-based planning episodes, in: EWCBR ’96: Proceedings of the Third European Workshopon Advances in Case-Based Reasoning, Springer-Verlag, London, UK, 1996, pp. 280–294.[47] B. Nebel, J. Koehler, Plan reuse versus plan generation: A complexity-theoretic perspective, Artificial Intelligence (Special Issue on Planning and Schedul-ing) 76 (1995) 427–454.[48] M. Neuhaus, H. Bunke, A convolution edit kernel for error-tolerant graph matching, in: 18th International Conference on Pattern Recognition (ICPR’06),vol. 4, IEEE Computer Society, Washington, DC, 2006, pp. 220–223.[49] M. Neuhaus, H. Bunke, Bridging the Gap Between Graph Edit Distance and Kernel Machines, World Scientific, 2007.[50] A.N. Papadopoulos, Y. Manolopoulos, Structure-based similarity search with graph histograms, in: Proceedings of the 10th International Workshop onDatabase & Expert Systems Applications, IEEE Computer Society Press, 1999, pp. 174–178.[51] J.W. Raymond, E.J. Gardiner, P. Willett, Rascal: Calculation of graph similarity using maximum common edge subgraphs, The Computer Journal 45 (6)(June 2002) 631–644.[52] K. Riesen, H. Bunke, Approximate graph edit distance computation by means of bipartite graph matching, Image Vision Comput. 27 (7) (2009) 950–959.[53] R. Rosipal, L.J. Trejo, Kernel partial least squares regression in reproducing kernel Hilbert space, J. Mach. Learn. Res. 2 (2002) 97–123.[54] B.H. Ross, Some psychological results on case-based reasoning, in: Proc. of a Workshop on Case-Based Reasoning, Pensacola Beach, FL, 1989, pp. 144–147.[55] F. Ruskey, R. Cohen, P. Eades, A. Scott, Alley cats in search of good homes, in: Twenty-Fifth Southeastern Conference on Combinatorics, Graph Theoryand Computing 102 (1994) 97–110.[56] B. Scholkopf, A.J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond, MIT Press, Cambridge, MA, USA,2001.[57] B. Smyth, E. McKenna, Footprint-based retrieval, in: K.D. Althoff, R. Bergmann, K. Branting (Eds.), ICCBR, in: Lecture Notes in Computer Science,vol. 1650, Springer, 1999, pp. 343–357.[58] L. Spalazzi, A survey on case-based planning, Artificial Intelligence Review 16 (1) (2001) 3–36.[59] B. Srivastava, T.A. Nguyen, A. Gerevini, S. Kambhampati, M.B. Do, I. Serina, Domain independent approaches for finding diverse plans, in: M.M. Veloso(Ed.), IJCAI, 2007, pp. 2016–2022.[60] A. Tate, Generating project networks, in: Proceedings of the Fifth International Joint Conference on Artificial Intelligence (IJCAI-77), MIT, Cambridge,MA, 1977, pp. 888–889.[61] F. Tonidandel, M. Rillo, The FAR-OFF system: A heuristic search case-based planning, in: M. Ghallab, J. Hertzberg, P. Traverso (Eds.), AIPS, AAAI, 2002,pp. 302–311.[62] R. van der Krogt, M. Weerdt, Plan repair as an extension of planning, in: S. Biundo, K.L. Myers, K. Rajan (Eds.), ICAPS, AAAI, 2005, pp. 161–170.[63] M. Veloso, Learning by analogical reasoning in general problem solving, Technical report, CMU-CS-92-174, Department of Computer Science, CarnegieMellon University, 1992.[64] M. Veloso, Planning and Learning by Analogical Reasoning, in: Lecture Notes in Artificial Intelligence, vol. 886, Springer-Verlag Inc., New York, USA,1994.[65] S.V.N. Vishwanathan, A.J. Smola, Fast kernels for string and tree matching, in: S. Becker, S. Thrun, K. Obermayer (Eds.), NIPS, MIT Press, 2002, pp. 569–576.[66] S. Vosniadou, A. Ortony (Eds.), Similarity and Analogical Reasoning, Cambridge University Press, New York, NY, USA, 1989.[67] C. Walther, A mechanical solution of Schubert’s steamroller by many-sorted resolution, Artificial Intelligence 26 (2) (1985) 217–224.[68] F. Wilcoxon, R.A. Wilcox, Some Rapid Approximate Statistical Procedures, American Cyanamid Co., Pearl River, NY, USA, 1964.