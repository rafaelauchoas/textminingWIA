Artificial Intelligence 172 (2008) 823–851www.elsevier.com/locate/artintNegotiating with bounded rational agents in environments withincomplete information using an automated agent ✩Raz Lin a,∗, Sarit Kraus a,c, Jonathan Wilkenfeld b,c,d, James Barry da Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel 52900b Department of Government and Politics, University of Maryland, College Park, Maryland, USAc Institute of Advanced Computer Studies, University of Maryland, USAd Center for International Development and Conflict Management, University of Maryland, College Park, Maryland, USAReceived 6 March 2007; received in revised form 24 September 2007; accepted 24 September 2007Available online 17 October 2007AbstractMany tasks in day-to-day life involve interactions among several people. Many of these interactions involve negotiating over adesired outcome. Negotiation in and of itself is not an easy task, and it becomes more complex under conditions of incompleteinformation. For example, the parties do not know in advance the exact tradeoff of their counterparts between different outcomes.Furthermore information regarding the preferences of counterparts might only be elicited during the negotiation process itself.In this paper we propose a model for an automated negotiation agent capable of negotiating with bounded rational agents underconditions of incomplete information. We test this agent against people in two distinct domains, in order to verify that its modelis generic, and thus can be adapted to any domain as long as the negotiators’ preferences can be expressed in additive utilities.Our results indicate that the automated agent reaches more agreements and plays more effectively than its human counterparts.Moreover, in most of the cases, the automated agent achieves significantly better agreements, in terms of individual utility, than thehuman counterparts playing the same role.© 2007 Elsevier B.V. All rights reserved.Keywords: Bilateral negotiation; Bounded rationality; Incomplete information; Automated agent1. IntroductionVarious tasks in day-to day life require negotiation capabilities. These can be as simple and ordinary as hagglingover a price in the market, through deciding what show to watch on TV. On the other hand, it can also involve issuesover which millions of lives are at stake, such as interstate disputes [25] and nuclear disarmament [9]. No matter whatthe domain, the negotiation process itself is not an easy task. The parties will have conflicting interests with referenceto some aspects of the negotiation. On the other hand, both sides might also have the incentive to cooperate with each✩ This research was supported in part by NSF under grant #IIS-0208608. A preliminary version of this paper was published at ECAI 2006, Italy.This paper was selected for the Fast Track submission.* Corresponding author.E-mail addresses: linraz@cs.biu.ac.il (R. Lin), sarit@cs.biu.ac.il (S. Kraus), jwilkenf@gvpt.umd.edu (J. Wilkenfeld), jbarry6899@aol.com(J. Barry).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.09.007824R. Lin et al. / Artificial Intelligence 172 (2008) 823–851other, as reaching an agreement could be more beneficial for them than walking away without any agreement ([20],Chapter 7).Negotiation is a profession, yet on many occasions, ordinary people need to become involved in negotiation tasks.Thus, success in modeling such an agent has great advantages and implications: from using it for training people innegotiations, to assisting in e-commerce environments, as well as for the development of tools for modeling negotia-tion behavior in general. We propose a model of such an automated agent. Thus, we make a significant contributionin this respect.With regard to the negotiation model, we consider a setting of a finite horizon bilateral negotiation with incompleteinformation between an automated agent and a human counterpart. The negotiation is said to have a finite horizonif the length of every possible history of the negotiation is finite ([20], p. 90). Incomplete information is expressedas uncertainty regarding the utility preferences of the opponent, and we assume that there is a finite set of differentagent types. The negotiation itself consists of a finite set of multi-attribute issues and time-constraints. The negotiationconsists of multi-attribute issues if the parties have to negotiate an agreement which involves several attributes for eachissue. This can help in making complex decisions while taking into account multiple factors [10]. Costs are assignedto each agent, such that during the negotiation process, the agents might gain or lose utility over time. If no agreementis reached by the given deadline a status quo outcome is enforced.Our automated agent is capable of negotiating efficiently in such environments, as our experimental results show.Nonetheless, in order to allow our agent to negotiate efficiently in these settings, we had to decide how to allow itto deal with the uncertainty both regarding the environment and the opponent. To achieve this, we incorporated twomechanisms in the automated agent. The first deals with incomplete information regarding the opponent by using apractical learning algorithm based on Bayes’ theorem which updates the agent’s beliefs regarding its opponent. Thesecond mechanism deals with the bounded rationality of the opponent. While our model applies utility functions, itis a based on a non-classical decision making method. Instead of focusing on maximizing the expected utility, weare motivated by qualitative decision making approaches [4,24] and we use the maximin function and the qualitativevaluation of offers in our model. Using these methods our automated agent generates offers and decides whether toaccept or reject proposals it has received.We conducted three sets of experiments in which we matched our automated agent against (a) human negotiators,(b) an automated agent following an equilibrium strategy, and (c) against itself—that is, the same models of an agentplaying both sides. The experiments were run on two distinct domains. In the first domain, England and Zimbabwenegotiate in order to reach an agreement evolving from the World Health Organization’s Framework Convention onTobacco Control, the world’s first public health treaty. In the second domain a negotiation takes place after a successfuljob interview between an employer and a job candidate.By analyzing the results of the experiments we conducted, we show that our automated agent is capable of negoti-ating efficiently and reaching multi-attribute agreements in such environments.When playing one of the sides in the negotiation (England in the first domain and the job candidate in the seconddomain) our agent achieved significantly higher utility values than the human players, and agreements were reachedfaster than when an agent was not present in the negotiation. On the other hand, when the agent played the otherside, though it reached higher utility values than the human player, these results were not significantly higher than thehumans’ results.This paper contributes to research on automated negotiation in several ways. First, it tackles the problem ofmulti-attribute negotiation with incomplete information. Given the importance of negotiating efficiently in such anenvironment we provide a generic mechanism that achieves just that. Second, we present an automated negotiationagent which is domain independent and allows to experiment with almost any real-life domain. Together, the auto-mated negotiation environment will enable exploration of future research directions and thereafter it can be used tobetter understand behavioral and cognitive aspects of negotiations undertaken by human negotiators.The remainder of the paper is organized as follows. Section 2 provides an overview of bilateral negotiation withincomplete information. Section 3 surveys related work in the field of negotiation with incomplete information andbounded rational agents. Section 4 presents the design of the automated agent, including its beliefs updating anddecision making mechanisms. Section 5 describes the experimental setting and methodology and reviews the results.Finally, Section 6 provides a summary and lists recommendations for future work in this area.R. Lin et al. / Artificial Intelligence 172 (2008) 823–8518252. Problem descriptionWe consider a bilateral negotiation in which two agents negotiate to reach an agreement on conflicting issues.The negotiation can end either when (a) the negotiators reach a full agreement, (b) one of the agents opts out, thusforcing the termination of the negotiation with an opt-out outcome denoted OPT, or (c) a predefined deadline isreached, denoted dl, whereby, if a partial agreement is reached it is implemented or, if no agreement is reached,a status quo outcome, denoted SQ, is implemented. Since no agreement is worse than any agreement, and a statusquo is implemented if the deadline is reached, we assume that default values are assigned to each attribute. Thus, ifboth sides agree only on a subset of the issues and the deadline is reached, the unresolved issues are assigned withtheir default value and thus a partial agreement can be implemented. Let I denote the set of issues in the negotiation,Oi the finite set of values for each i ∈ I and O a finite set of values for all issues (O1 × O2 × · · · × O|I |). We allowpartial agreements, ⊥ ∈ Oi for each i ∈ I . An offer is then denoted as a vector (cid:5)o ∈ O. It is assumed that the agentscan take actions during the negotiation process until it terminates. Let Time denote the set of time periods in thenegotiation, that is Time = {0, 1, . . . , dl}. Time also impacts the agents’ utilities. Each agent is assigned a time costwhich influences its utility as time passes.In each period t ∈ Time of the negotiation, if the negotiation has not terminated earlier, each agent can propose apossible agreement, and the opponent can either accept the offer, reject it or opt out. Each agent can either proposean agreement which consists of all the issues in the negotiation, or a partial agreement. In contrast to the model ofalternating offers ([20], pp. 118–121), each agent can perform up to M > 0 interactions with the opponent agent ineach time period. Thus, an agent must take into account that its opponent may opt out in any time period.Since we deal with incomplete information, we assume that there is a finite set of agent types. These types areassociated with different additive utility functions (e.g., one type might have a long term orientation regarding thefinal agreement, while the other type might have a more constrained orientation). Formally, we denote the possibletypes of the agents Types = {1, . . . , k}. Given l ∈ Types, 1 (cid:2) l (cid:2) k, we refer to the utility of an agent of type l asul, and ul : {(O ∪ {SQ} ∪ {OPT}) × Time} → R. Each agent is given its exact utility function. Also, each agent oftype l ∈ Types is given a certain reservation price, denoted rl, which is held private. The reservation price states theminimum value of the utility of an offer under which the agent is unwilling to accept the offer. The agent, and thesubjects in the experiments described later in the paper, are also aware of the set of possible types of the opponent.However, the exact utility function of the rival is private information. Our agent has some probabilistic belief aboutthe type of the other agent. This belief may be updated over time during the negotiation process (for example, usingBayes’ rule).3. Related workThe problem of modeling an automated agent for bilateral negotiation is not new for researchers in the fields ofMulti-Agent Systems and Game Theory. However, most research makes assumptions that do not necessarily applyin genuine negotiations with humans, such as assuming complete information [5,19] or the rationality of the humannegotiator [5–7,13]. In this sense, they assume that both parties are rational in their behavior (e.g., describing thedecisions made by the agents as rational and that they are utility maximizing agents that cannot deviate from theirprescribed behavior). For example, Faratin et al. [5] assume that the agents are motivated by maximizing the jointutility of the outcome, that is, the agents are utility maximizers that seek Pareto-optimal agreements. In a similarmanner, [6,7,13] assert that the agent’s strategy must follow the equilibrium’s strategy such that it should be the bestresponse to the opponent’s strategy. In other words, no agent will have an incentive to deviate from the strategy.None of the above researchers has looked into the negotiation process involving both incomplete information andnegotiations against humans. While their approaches might be appropriate in their context, they cannot be applied toour settings.Dealing only with the bounded rationality of the opponent, several researchers suggested new notions of equilibria(e.g., the trembling hand equilibrium described in Rasmusen [21] (p. 139)) or other probability models. For example,Capra et al. [3] use what is called a “standard logit model”. In this model probabilities are assigned to the decisions.Those probabilities are proportional to exponential functions of expected payoffs. They use this model in order toenable the players to update their beliefs about other players. This model is equivalent to assuming expected payoffsare subjected to deviations with an extreme value distribution. That is, the logit model assumes that the decisions are826R. Lin et al. / Artificial Intelligence 172 (2008) 823–851not perfect and may have some noise. It also tries to deal with such situations. These noisy decisions can be interpretedeither as unobserved random changes in preferences or as errors in responding to expected payoffs. Similar to Capraet al., our agent also assigns probability to the believed type of the opponent. However, we try to avoid the need ofadding a special mechanism that assumes that the actions of the opponent are characterized by noise.In addition, in a recent paper, Kraus et al. [11] describe an automated agent that negotiates efficiently with humans.Although they also deal with negotiation against humans, in their settings there is complete information. That is, theagents know exactly what the world state parameters are and how changing them affects the other agent. First, theyidentified the perfect equilibrium strategies in their model. Then, however, they observed, that the human players donot necessarily follow these equilibrium strategies. Thus, they added heuristics and argumentation tailored to theirspecific settings to enable effective negotiation by their perfect equilibrium agent. We, on the other hand, proposea general model of an agent, who can negotiate efficiently in multi-attribute negotiations against bounded rationalagents with incomplete information.Other researchers suggested shifting from quantitative decision theory to qualitative decision theory [24]. In usingsuch a model we do not necessarily assume that the opponent will follow the equilibrium strategy or try to be autility maximizer. Also, this model is better suited for cases in which the utility or preferences are unknown but canbe expressed in ordinal scales or as preference relations [4]. This approach seems appropriate in our settings, andusing the maximin criteria, which is generally used in this context, enables our agent to follow a pessimistic approachregarding the probability that an offer will be accepted.Another way to deal with bounded rationality was suggested by Luce [15], who introduced the choice axiom. Thechoice axiom, in relation to negotiations, states that the probability of selecting one offer over another from a pool ofoffers, is not affected by the presence or absence of other items in that pool. The axiom introduces the notion of Lucenumbers. A Luce number is a non-negative number that is associated with each offer. The Luce number of an offer(cid:5)o ∈ O is calculated using the following formula:lu((cid:5)o) =u((cid:5)o)(cid:2)(cid:5)x∈O u((cid:5)x)From the mathematical definition the following property follows:(1)Property 3.1 (Luce Number Relation). For every offer (cid:5)x and (cid:5)y, if u((cid:5)x) (cid:3) u((cid:5)y) then lu((cid:5)x) (cid:3) lu((cid:5)y) when lu((cid:5)x) andlu((cid:5)y) denote the Luce number associated with offer (cid:5)x and (cid:5)y respectively.Mostly in economics, this model is used to assign probabilistic beliefs regarding the tendency of the consumer’s tochoose one offer over another (e.g., see Luce’s survey of the choice axiom [16]). As such, we believe that this modelcan be used as an estimation of the acceptance rate of the opponent’s offer.Several methods are proposed when dealing with incomplete information regarding the preferences of an opponent.For example, Bayes’ theorem is the core component of the Bayesian Nash equilibrium ([20], pp. 24–29), and it is usedto deduce the current state given a certain signal. One motivation for using this notion of equilibrium is that it allowsone to compensate for incomplete information and enables good adaptation in negotiations with time-constraints. Infinite horizon negotiations there are no past interactions to learn from and not enough time periods to build a completemodel. Thus this model provides a good probabilistic tool to model the opponent, as opposed to using feed-forwardneural networks [19] or genetic algorithms [13], both of which require considerable time to facilitate adequate learningand are more sensitive to the domain in which they are run.Zeng and Sycara [26] also use the Bayesian analysis as a learning mechanism in negotiations. And like them, wealso use Bayes’ theorem to update the believed type of the opponent. Thus, we allow the negotiator, at each timeperiod, to act as if the opponent is of a certain type.Since we aim to design an agent that can negotiate efficiently against bounded rational agents in conditions ofincomplete information, we should design it not only with a good mechanism to deal with the bounded rationality ofthe opponent, but also with a mechanism which can help overcome the incomplete information settings. Following thesurveyed related work, we believe that implementing a non-classical valuation mechanism, using the Luce numbers,for generating and responding to offers, while also incorporating a Bayesian belief update component to deal with theincomplete information, can lead to an agent’s efficient design. We elaborate on this design in the next section.R. Lin et al. / Artificial Intelligence 172 (2008) 823–8518274. Agent designDue to the unique settings in which we operate—incomplete information and bounded rationality of the opponent—the agent is built with two mechanisms: (a) a learning mechanism, and (b) a decision making mechanism.For the learning mechanism we use the formal model of the Bayesian updating rule and for the decision makingmechanism we incorporate a non-classical model of offers’ valuation, rather than the traditional quantitative decisionmaking model. We describe both mechanisms in the following subsections.4.1. The decision making valuation componentAs Brafman and Tennenholtz [1] state, there are extensions to qualitative decision theory yet to be pursued. Wecontend that one such extension should be in negotiation under uncertainty against a bounded rational agent. Onereason is due to the fact that, as in real life, we do not impose restrictions on the bounded rational agent and we donot assert that it will follow an equilibrium strategy. We propose a unique decision making model. While our model isquantitative in spirit, we shift from the model of expected utility maximization and try to evaluate the offers in a morequalitative way, using the maximin method and the ranking of offers, described below.The decision making valuation component takes into account the agent’s utility function, as well as the believedtype of the opponent (note that the believed type of the opponent is also influenced by the offers proposed by theopponent, as described in Section 4.2). This data is used both for deciding whether to accept or reject an offer andfor generating an offer. In our settings, although several offers can be proposed in each time period, we restrict theagent to making a single offer in each period. This is done due to the fact that our mechanism for generating offersonly produces one distinct offer at a given time period. The opponent, on the other hand, is free to propose severaloffers, and the agent can respond to all the offers, which indeed happened in the experiments. While we providesome theoretical foundations for our approach, we demonstrate its effectiveness using experiments with people in anenvironment of incomplete information, as described in Section 5.4.1.1. Generating offersThe motivation behind the mechanism for generating offers is that the automated agent would like to propose anoffer which yields it the highest utility value. However, due to conflicting interests, there is a high probability that thisoffer will be rejected by the opponent. To overcome this, the agent evaluates all possible offers based on their utilityand the probability that the rival will accept them.An initial version of this offer generating mechanism was presented in [14]. Based on the preliminary experimentalresults, we have decided to improve this mechanism. Thus, instead of directly using the utility value of an offer, ourmechanism uses the ranking value of an offer, which is associated with each offer and a given utility function u,denoted rank(·). The rank number of an offer (cid:5)o ∈ O is calculated using the following formula:rank((cid:5)o) = order((cid:5)o, O)|O|(2)where order(·,·) is the ordering function which orders the offer (cid:5)o in an ordinal scale between 1 and |O| accordingto its utility value compared to all other offers in O. In order to facilitate the computations we also divide the offer’sordering number by |O| to obtain a range between [0, 1]. Note that a certain offer might be ranked differently whenusing the utility value of the agent for that offer and the believed utility value of the opponent. In addition, the Lucenumbers are used to estimate the probability of the agent accepting an offer, following Luce’s choice axiom models,which assign a probability to each offer [15]. Based on Property 3.1, we get that the higher the utility of the offerthe higher its Luce number as well. As the choice axioms is usually used to assign probabilistic beliefs regarding thetendency of an agent to propose an offer, we also believe that using the Luce numbers can provide a good estimationof an acceptance of offers.Since the opponent also tries to estimate whether an offer will be accepted by the agent, we take the Luce numbersof both into account. Then, the agent tries to estimate, from the opponent’s point of view, whether the opponent willaccept the offer. This is done by estimating the sum of the Luce numbers of the agent and the presumed type of theopponent. This sum is used as an estimation of the acceptance of the offer, and is multiplied by the ranking value ofthat offer of the opponent. The sum of both agents is used in the calculation as a method for estimating the social828R. Lin et al. / Artificial Intelligence 172 (2008) 823–851welfare of both sides. Finally, the agent compares these values with its own estimation of the offer, which is based onits ranking of the offer and the probability that it will accept the offer.Similar to qualitative decision theory, which uses the maximin value [4,24], the agent selects the minimum valuebetween these two values (the agent’s own estimation of the offer and the agent’s estimation from the opponent’spoint of view), under the pessimistic assumption that the probability that an offer is accepted is based on the agentthat favors the offer the least. After calculating the minimum value of all the offers, the agent selects the offer with themaximum value among all the minima, in order to also try and maximize its own utility. Thus, our qualitative offergeneration mechanism selects, intuitively, the best offer among the offers that the agent believes might be accepted.In the rest of this section we describe this process formally.First we will demonstrate this using the following example.Example 1. Bob (b) and Alice (a) are negotiating about what to do over the weekend. They have to decide aboutthe activity and on which night they will do it. Given that I is the set of issues, let Activity ∈ I and Night ∈ I be thepossible issues. There are two possible values for the activity—seeing a movie (M) or going to a basketball game (B)and two possible values for the chosen night—either they will go on a Friday night (F ) or on Saturday night (S). Bobprefers going to a basketball game than to a movie. He also prefers it to be on a Saturday night, as the match shouldbe more interesting than on Friday night. However, if a movie is chosen, he prefers to go on Friday to allow him tocatch the game on ‘Pay Per View’ on Saturday. On the other hand, Alice prefers going out to a movie rather than to abasketball game. The best night for a movie would be Saturday as on Saturday is the opening night for the premieres,while if going to a basketball game, she prefers to stay home on Saturday and not engage in any activity. Table 1reflects the utility values for Bob and Alice derived from their preferences, described above, for all the possible offers,along with the Luce number values and the ranking of the offers. In addition, Table 2 also compares the ranking of theoffers between Bob and Alice. Let the reservation price for both sides be equal to 5. For simplicity and demonstrationpurposes, we assume that there is only one type of utility for each side and no time effect for both sides.Assume that our agent plays the role of Bob. It will now choose the maximum value among all the minima betweenthe values in lines 7 and 9 in Table 1. Thus, our agent will offer Alice to go to a basketball game on Friday.We continue now to describe the process formally. Formally, we assume that, at each time period t, the automatedagent has a belief about the type of its opponent. This believed type, denoted by BT(t), is the one that the agentpresumes to be the most probable for the opponent. The agent uses the utility function associated with that type inTable 1Example of calculating QO12345678910ub((cid:5)oi )ua((cid:5)oi )lub((cid:5)oi )lua ((cid:5)oi )rankb((cid:5)oi )ranka ((cid:5)oi )lub((cid:5)oi ) · rankb((cid:5)oi )lua ((cid:5)oi ) · ranka((cid:5)oi )[lub((cid:5)oi ) + lua ((cid:5)oi )] · ranka ((cid:5)oi )[lua((cid:5)oi ) + lub((cid:5)oi )] · rankb((cid:5)oi )(cid:5)o1 = {M, S}4104/28 = 0.1410/29 = 0.341/4 = 0.254/4 = 1.000.040.340.490.12(cid:5)o2 = {M, F }696/28 = 0.219/29 = 0.312/4 = 0.503/4 = 0.750.110.230.390.26(cid:5)o3 = {B, S}10410/28 = 0.364/29 = 0.144/4 = 1.001/4 = 0.250.360.030.120.49(cid:5)o4 = {B, F }868/28 = 0.296/29 = 0.213/4 = 0.752/4 = 0.500.210.100.250.37Table 2Ranking of offersRanking4/4 = 1.003/4 = 0.752/4 = 0.501/4 = 0.25Bob(cid:5)o3 = {B, S}(cid:5)o4 = {B, F }(cid:5)o2 = {M, F }(cid:5)o1 = {M, S}Alice(cid:5)o1 = {M, S}(cid:5)o2 = {M, F }(cid:5)o4 = {B, F }(cid:5)o3 = {B, S}R. Lin et al. / Artificial Intelligence 172 (2008) 823–851829all of the calculations in that time period. In Section 4.2 we describe in detail how the belief is dynamically updated.We denote by uBT(t)the utility function associated with the believed type of the opponent at time t. From this utilityoppfunction, the agent derives the Luce numbers. Since the Luce number is calculated based on a given utility function,we denote the Luce number of an offer, (cid:5)o ∈ O, derived from the opponent’s believed utility at time t, uBT(t)opp , byluopp((cid:5)o|uBT(t)opp ), and the Luce number for an offer derived by the agent’s own utility simply as lu((cid:5)o). We denote ourfunction by QO(t) (standing for Qualitative Offer), where t ∈ Time is the time of the offer.Thus, if the current agent is j , the strategy selects an offer o in time t such that:QO(t) = arg maxmin{α, β}o∈O(cid:3)luopp((cid:5)o|uBT(t)where α = rank((cid:5)o) · lu((cid:5)o) and β =(cid:4)opp ) + lu((cid:5)o)In the example specified in Example 1 and Table 1, assuming our agent plays the role of Bob, lines 7 and 9 representα and β, respectively. Then, to calculate the actual offer to propose to Alice, the agent calculates the minimum valuesbetween these two values, and finally chooses the maximum between all the minima calculated. That is, the agent willchoose (cid:5)o3 since it has the maximum value of the minimum values associated with the four offers (0.04, 0.11, 0.12,0.21).· rankBT(t)opp ((cid:5)o)(3)Seemingly, our QO function is a non-classical method for generating offers. However, not only were we able toshow its efficacy through empirical experiments, in which people used it in negotiations, as we describe in Section 5,we also showed (Section 4.1.3) that it conforms to some properties from classical negotiation theory, which aremainly used by mediators. In addition, our QO function does not build on a fixed opponent’s type. That is, elicitingthe believed type of the opponent is external to the computation.The function can work as well with other software agents, and was tested in another two sets of experiments—the first matched our agent against an automated agent that follows a Bayesian Equilibrium strategy, and the secondmatched our agent against itself. Detailed results are described in Section 5.3.The next subsection deals with the question of when the agent should accept or reject an incoming offer.4.1.2. Accepting offersThe agent needs to decide what to do when it receives an offer from its opponent at the current time t. In thefollowing analysis, the terms a and b are used synonymously as the types of agents a and b, respectively. In theanalysis we will refer to the automated agent as agent type a and its opponent as agent type b. Similarly, the terms (cid:5)oaand (cid:5)ob respectively denote an offer received from agent a and agent b. If ua((cid:5)ob) (cid:3) ua(QO(t + 1)) then the automatedagent accepts the offer. Otherwise, it should not immediately rule out accepting the offer it has just received. Instead, itshould take into consideration the probability that its counter-offer will be accepted or rejected by the opponent. Thisis done by comparing the (believed) utility of the opponent from the original offer as compared with the opponent’sutility from the agent’s offer. If the difference is lower than a given threshold1 T , that is |ub(QO(t + 1)) − ub((cid:5)ob)| (cid:2) T ,then there is a high probability that the opponent will be indifferent to its original offer and the agent’s counter-offer.Thus, the automated agent will reject the offer and propose a counter-offer (taking a risk that the offer will be rejected),since the counter-offer has a higher utility value for the agent. If the difference is greater than the threshold, i.e., thereis a higher probability that the opponent will not accept the agent’s counter-offer, the automated agent will accept theopponent’s offer with a given probability, which is attached to each outcome, as described below. While in our originalversion of the agent [14] we did not impose any restrictions on accepting offers, in the new version, acceptance by theagent depends on whether the value of the offer is greater than the agent’s reservation price (r). If this is the case, thenthe probability is calculated based on the ranking number of the offer. The intuition behind this is to enable the agentto also accept offers based on their relative values compared to the reservation price, on an ordinal scale of [0..1].That is, the agent will accept an offer if ua((cid:5)ob) (cid:3) ra and with a probability of rank((cid:5)ob) it will reject it and make acounter-offer.Let’s return to Example 1 presented above. Assume Alice suggests to Bob to go to a movie on Friday. The utilityvalue of that offer to Bob equals 6. Bob now checks what would be his utility value from an offer he would make toAlice in the next time period. We showed earlier that Bob will suggest an offer to go to a basketball game on Friday.1 In the experiments, T was set to 0.05.830R. Lin et al. / Artificial Intelligence 172 (2008) 823–851The utility value of that offer to Bob is 8. Thus, since the utility value of the offer Bob would make is higher than theone received, Bob does not automatically accept the received offer. Bob needs to take into consideration whether hisoffer will be accepted or rejected by the opponent. So bob checks the difference between Alice’s utility value of theoffer he made and her utility value of the offer she made. The difference in this case equals 3 (as Alice’s utility valuefor Bob’s offer equals 6, while the utility value of the offer she made equals 9). If we assume that the threshold forthe difference is 0.05 (like in our experiments) than this condition does not hold as well, and Bob needs to continueto check whether he should reject or accept the offer. Since the utility value of the offer received from Alice is higherthan the reservation price (5), Bob now decides whether to accept or reject the offer based on the probability derivedfrom the ranking value of the offer. Since the ranking value for Bob from the offer of seeing a movie on Friday is 0.5,Bob will decide according to this probability whether to accept or reject the offer.In the next subsection we demonstrate that our proposed solution also conforms to some properties of the Nashbargaining solution. This gives us the theoretical basis required for use of our technique in bilateral negotiation, andfor the assumption that offers proposed by the agent will also be considered to be accepted by the opponent.4.1.3. QO: An alternative to the Nash bargaining solutionFrom Luce and Raiffa ([17], Chapter 6), we employ the definitions of a bargaining problem and the Nash bar-gaining solution. We denote by B = (cid:8)(ua(·), ub(·)), (cid:5)d(cid:9) the bargaining problem with two utilities, ua and ub, and adisagreement point, denoted (cid:5)d, which is the worst outcome for both agents (in our model, d is equivalent to optingout). The Nash bargaining solution (which is not the offer itself, but rather the payoff of the offer) is defined by severalcharacteristics and is usually designed for a mediator in an environment with complete information. A bargaining (ora negotiation) solution f should satisfy symmetry, efficiency, invariance and independence of irrelevant alternatives([20], Chapter 15), as defined in the definitions below.Definition 4.1 (Symmetric). A bargaining problem is symmetric if a bijection φ : O → O, also called a symmetryfunction, exists such that:(1) φ−1 = φ,(2) φ( (cid:5)d) = (cid:5)d,(3) ∀(cid:5)x, (cid:5)y ∈ O, ua((cid:5)x) > ub((cid:5)y) ⇔ ub(φ((cid:5)x)) > ub(φ((cid:5)y)).For example, suppose that two friends want to split $100 among themselves. Each friend needs to decide howto split the money, but they both receive nothing if they disagree. This problem is a symmetric one (consider thesymmetric function given by φ(x, y) = (y, x)).Definition 4.2 (Efficiency). An outcome (cid:5)x ∈ O is efficient if there is no outcome (cid:5)y ∈ O with uj ((cid:5)y) > uj ((cid:5)x) for bothj = {a, b}.Let’s look again at the two friends trying to split the $100 between themselves. A solution which will leave someof the money undivided between the two is inefficient, in the sense that both friends are better off if the remainingmoney is split between them.Definition 4.3 (Equivalence). Two bargaining problems B = (cid:8)(ua(·), ub(·)), (cid:5)d(cid:9) and B(cid:12) = (cid:8)(u(cid:12)alent if there are γj > 0 and γj ∈ R+, δj ∈ R, for j = {a, b} such that u(cid:12)j= γj uj + δj .a(·), u(cid:12)b(·)), (cid:5)d(cid:9) are equiv-To understand the notion of the equivalence problem, assume that the first involves temperature that is measuredin Fahrenheit. An equivalent problem would be the same problem with the transformation from Fahrenheit to Celsius(γj = 5/9, δj = −160/9).Definition 4.4 (Subset Problems). A bargaining problem B(cid:12) = (cid:8)(u(cid:12)subset of another bargaining problem B = (cid:8)(ua(·), ub(·)), (cid:5)d(cid:9), uj : O → R2, denoted by B(cid:12) ⊆ B, if O(cid:12) ⊆ O.j : O(cid:12) → R2 for j = {a, b} is aa(·), u(cid:12)b(·)), (cid:5)d(cid:9), u(cid:12)R. Lin et al. / Artificial Intelligence 172 (2008) 823–851831For example, the problem of splitting the $100 between two friends such that both friends receive an equal split isa subset of the problem in which both friends can receive any split of the money.Given the above definitions we state that a bargaining (or a negotiation) solution should have the following proper-ties:Property 4.1 (Symmetry). A negotiation solution f satisfies symmetry if for all symmetric problems B with symmetryfunction φ, f (B) = φ(f (B)).The symmetry property above states that if both players have the same bargaining power (since it deals with sym-metric negotiation problems), then neither player will have any reason to accept an agreement which yields a lowerpayoff for it than for its opponent. For example, for the solution to be symmetric, it should not depend on the agentwhich started the negotiation process. In our example of splitting the $100 between the two friends, both friendshave the same utility function. For the solution to be symmetric, both must receive an equal payoff, that is, an equaldistribution of the money.Property 4.2 (Efficient). A negotiation solution f satisfies efficiency if f (B) is efficient for all B.Efficiency states that two rational agents will not reach an agreement if its utility is lower for both of them thananother possible agreement. This solution is said to be Pareto-optimal. In this case, each friend will not agree on anyagreement other than splitting the money equally between themselves in a way that each receive exactly half of themoney, as every other split will generate a lower payoff for either of them.Property 4.3 (Invariance). A negotiation solution f satisfies invariance if for all equivalent problems B =(cid:8)(ua(·), ub(·)), (cid:5)d(cid:9) and B(cid:12) = (cid:8)(u(cid:12)b(·)), (cid:5)d(cid:9), f (B) = f (B(cid:12)).a(·), u(cid:12)Invariance states that for all equivalent problems B and B(cid:12), that is B(cid:12) = (γa ·ua(·)+δa, γb ·ub(·)+δb), γa, γb ∈ R+,δa, δb ∈ R, the solution is also the same, f (B) = f (B(cid:12)). That is, two positive affine transformations can be applied onthe utility functions of both agents and the solution will remain the same. For example, the solution for the problem ofsplitting the $100 between the two friends is equivalent to the solution in the case of splitting £500 between themselvesinstead of dollars. Thus, this solution satisfies the invariance property.Property 4.4 (Independence of irrelevant alternatives). A negotiation solution f satisfies independence of irrelevantalternatives if f (B) = f (B(cid:12)) whenever B(cid:12) ⊆ B and f (B) ⊆ B(cid:12).Finally, independence of irrelevant alternatives asserts that the solution f (B) = f (B(cid:12)) whenever B(cid:12) ⊆ B andf (B) ⊆ B(cid:12). That is, if new agreements are added to the problem in such a manner that the status quo remains un-changed, either the original solution is unchanged or it becomes one of the new agreements. For example, as we statedabove, the problem of splitting the $100 between two friends in such that both friends receive an equal split is a subsetof the problem in which both friends can receive any split of the money. If we assume that also all the money has tobe distributed between the friends, then the solution of an equal split between the friends satisfies the independenceof irrelevant alternatives, as the added alternatives of unequal split, are not part of the solution.It was shown by Nash [18] that the only solution that satisfies all of these properties is the product maximizing theagents’ utilities, described in Eq. (4).(cid:5)ua((cid:5)x) − ua( (cid:5)d)arg max(cid:5)x∈O(cid:6)(cid:5)(cid:6)ub((cid:5)x) − ub( (cid:5)d).(4)However, as we stated, the Nash solution is usually designed for a mediator. Since we propose a model for anautomated agent which negotiates with bounded rational agents following the QO function (Eq. (3)), our solutioncannot satisfy all of these properties. To this end, we modified the independence of irrelevant alternatives property toallow for a set of possible solutions instead of one unique solution:Property 4.5 (Independence of irrelevant alternative solutions). A negotiation solution f satisfies independence ofirrelevant alternative solutions if the set of all possible solutions of f (B) is equal to the set of all possible solutionsof f (B(cid:12)) whenever B(cid:12) ⊆ B and f (B) ⊆ B(cid:12).832R. Lin et al. / Artificial Intelligence 172 (2008) 823–851In this case, assume that in the problem of splitting the $100 between two friends any split of the money is legiti-mate. Also, the problem in which both friends receive an equal split is a subset of the problem in which both friendscan receive any split of the money. In this case, the negotiation solution f is a set which consists of every equal split.This solution satisfies the independence of irrelevant alternative solutions, as the added alternatives of unequal split,are not part of the solution.Proving that the agent’s strategy for proposing offers conforms to these properties (Properties 4.1, 4.2, 4.3 and 4.5)is important since although the agent should maximize its own utility, it should also find agreements that would beacceptable to its opponent. The following claims and their proofs lay the theoretical foundation for this.Theorem 4.1. The QO function satisfies the properties of symmetry, efficiency and independence of irrelevant alter-native solutions.The proof of the theorem can be found in Appendix A, Claims A.2, A.3 and A.5. In addition, we show that undersome conditions QO also satisfies the property of invariance (see Claim A.4 in Appendix A).We recall that the Nash bargaining solution should have four properties: symmetry (Property 4.1), efficient (Prop-erty 4.2), invariance (Property 4.3) and independent of irrelevant alternatives (Property 4.4). By proving the abovetheorem, we show that our QO function satisfies only three of the four properties of the Nash bargaining solution.Thus the question is, why not use the Nash solution stated in Eq. (4) instead of our proposed solution? The reasoningfor not using the above Nash solution is presented in the next claim.Claim 4.1. Let the agreement given by the Nash solution be (cid:5)x. If an agreement (cid:5)y exists where lua((cid:5)y) · ranka((cid:5)y) >lua((cid:5)x) · ranka((cid:5)x) and lua((cid:5)y) · ranka((cid:5)y) < [lua((cid:5)y) + lub((cid:5)y)] · rankb((cid:5)y) then QO’s solution will be (cid:5)y rather than (cid:5)x.The proof of the claim is given in Appendix A, Claim A.6. However, to clarify this claim we return to Example 1.Table 1 shows the utility value for Bob and Alice for 4 different offers. Assuming our agent plays the role of Bob, weshow that it will suggest to Alice to go to a basketball game on Friday. However, if the agent follows the Nash solution,it will suggest to see a movie on Friday, which is the product maximization of the agents’ utilities (6 × 9 = 54), whilethe QO solution has a product value of 8 × 6 = 48. Though, the Nash solution generates a utility value of 6 for Bob,the QO solution generates a value of 8.We continue to investigate the effects of time on the offers our agent generates. The following definition definesthe concept of time constant discount rate ([20], Chapter 7):Definition 4.5 (Time constant discount rate). In the case of a time constant discount rate, every agent j has a fixeddiscount rate 0 < δj < 1, that is: uj ((cid:5)o, t) = δtj uj ((cid:5)o, 0).We show that if both agents have the same time constant discount rate δ, then QO will generate the same solutionat each time unit. The proof can be found in Appendix A, Claim A.7.In the next section we present the component responsible for the belief update regarding the opponent.4.2. The Bayesian updating rule componentThe Bayesian updating rule is based on Bayes’ theorem and it provides a practical learning algorithm. Bayes’theorem is generally used for calculating conditional probabilities and basically states how to update or revise beliefsin light of new evidence a posteriori ([12], Chapter 2). The calculation is given in the following formula:P(A|B) = P(B|A) · P(A)P(B)(5)where P(A) and P(B) are the prior probabilities of A and B, respectively, P(A|B) is the conditional probability of A,given B, and P(B|A) is the conditional probability of B, given A.We assert that there is a set of different agent types. The bounded rational agent should be matched to one suchtype. In each time period, the agent consults the component in order to update its belief regarding the opponent’s type.R. Lin et al. / Artificial Intelligence 172 (2008) 823–851833Table 3Example: Calculating Alice’s believed type1234ua ((cid:5)oi ), type1ua ((cid:5)oi ), type2lua ((cid:5)oi ), type1lua ((cid:5)oi ), type2(cid:5)o1 = {M, S}101010/29 = 0.3410/31 = 0.32(cid:5)o2 = {M, F }979/29 = 0.317/31 = 0.23(cid:5)o3 = {B, S}454/29 = 0.145/31 = 0.16(cid:5)o4 = {B, F }696/29 = 0.219/31 = 0.29t=0) = 1Recall that there are k possible types of agents. At time t = 0 the prior probability of each type is equal, thatk , ∀i ∈ Types. Then, for each time period t we calculate the a posteriori probability for each of theis, P(typeipossible types, taking into account the history of the negotiation. This is done incrementally after each offer is receivedor accepted. That is, the believed type is updated every time an offer is received or accepted, thus eventually it is basedon the overall total history thus far. Then, this value is assigned to P(typet ). Using the calculated probabilities, theagent selects the type whose probability is the highest and proposes an offer as if it were the opponent’s type. Formally,at each time period t ∈ Time and for each type ∈ Types and (cid:5)ot ∈ O (the offer at time period t) we compute:(6)P(typei|(cid:5)ot ) = P((cid:5)ot |typei)P(typeit )P((cid:5)ot )(cid:2)ki=1 P((cid:5)ot |typei) · P(typeiwhere P((cid:5)ot ) =P((cid:5)ot |typei) is computed using the Luce numbers.t ). Since the Luce numbers actually assign probabilities to each offer,Now we can deduce the believed type of the opponent for each time period t, BT(t), using the following equation:BT(t) = arg maxi∈TypesP(typei|(cid:5)ot ),∀t ∈ Time(7)We will extend Example 1 to demonstrate this. Let’s assume that there are two types of possible utilities for Alice(k = 2). In the first, given in Table 1, Alice prefers movies over basketball. In the second type, however, let’s assumethat Alice prefers going to a movie on Friday, and if this is not possible going to a basketball game on Friday. AssumeTable 3 reflects the two different types of possible utilities for Alice derived from these preferences.Initially, a probability of 1/2 is assigned to both types. Let’s assume that Alice suggests at time t = 1 to go to abasketball on Friday night. Based on this suggestion, our agent needs to update the type it believes Alice to be. Basedon Eq. (6), the way P((cid:5)ot ) is calculated and the Luce numbers, we need to update the probability of each type. Forsimplicity, we omit the time from the calculations given below:P(type1|(cid:5)o4) = P((cid:5)o4|type1)P(type1)P(type2|(cid:5)o4) = P((cid:5)o4|type2)P(type2)P((cid:5)o)P((cid:5)o)==0.21 · 0.50.21 · 0.5 + 0.29 · 0.50.29 · 0.50.21 · 0.5 + 0.29 · 0.5= 0.42= 0.58(8)(9)Now, based on Eq. (7) we deduce that the believed type for Alice is type2.Using this updating mechanism enables our updating component to conform to the following conditions, whichare generally imposed on an agent’s system of beliefs, and which are part of the conditions for a sequential Bayesianequilibrium ([20], Chapter 12):(1) Consistency. Agent i’s belief should be consistent with its initial belief and with the possible actions of its oppo-nents. Whenever possible, an agent should update its beliefs. If, after any history, all the actions of agent j ’s inthe given sequence of actions, regardless of its type, indicate that it has to take the same action, and this actionis indeed taken by agent j , then agent i’s beliefs remain as they were before the action was taken. On the otherhand, if an action is taken by j and this action can only be attributed to a single type of agent j , type l, then ibelieves with a probability of 1 that j ’s type is indeed of type l. The agent uses the same reasoning about itsopponent j ’s beliefs based on the given sequence of actions and updates its model of j ’s beliefs in a similarmanner.834R. Lin et al. / Artificial Intelligence 172 (2008) 823–851(2) Never dissuaded once convinced. Once an agent is convinced of its opponent’s type with a probability of 1, orconvinced that its opponent cannot be of a specific type, that is, the probability of this type is 0, it is neverdissuaded from its viewpoint.The results of the experiments indeed show that in the majority of the experiments the agent believed that itsopponent is of the correct type with the highest probability amongst all possible opponent’s types.5. ExperimentsWe developed a simulation environment which is adaptable such that any scenario and utility function, expressed asmulti-issue attributes, can be used, with no additional changes in the configuration of the interface of the simulationsor the automated agent. The agent can play either role in the negotiation, while the human counterpart accesses thenegotiation interface via a web address. The negotiation itself is conducted using a semi-formal language. Each agentconstructs an offer by choosing the different values constituting the offers. Then, the offer is constructed and sent inplain English to its counterpart.We conducted experiments on two distinct domains to test the efficiency of the proposed agent.2 In the experi-ments, human subjects were matched both against the automated agent and against other human counterparts. Theseexperiments show that the agent is capable of negotiating in various domains. That is, only the utility functions play arole, and not the scenario nor the domain. In addition, the experiments show the benefits achieved for both sides in theagreements (in terms of utility and time) when using an automated agent as compared to human only negotiations. Inthe following subsections we describe the two domains and the experimental methodology and we review the results.5.1. Experimental domainThe experimental domains match the problem definitions described in Section 2. In the first domain one agent gainsas time advances, while the other loses; the status quo value for one of the agents is much higher than for the opponent,and there is an option to reach partial agreements. In the second domain, both agents lose as time advances, and thestatus quo value for both players is quite similar. In both domains we modeled three possible agent types, and thus aset of six different utility functions was created for each domain. These sets describe the different types or approachestowards the negotiation process and the other party. For example, the different approaches can describe the importanceeach agent attaches to the effects of the agreement over time. One agent might have a long term orientation regardingthe final agreement. This type of agent would favor agreements which are concerned more with future outcomes of thenegotiations, than those focusing only on solving the present problem. On the other hand, another agent might have ashort term orientation which focuses on solving only the burning issues under negotiation without dealing with futureaspects that might arise from the negotiation or its solutions. Finally, there can also be agents with a compromiseorientation. These agents try to find the middle grounds between the possible agreements.Each negotiator was assigned a utility function at the beginning of the negotiation but had incomplete informationregarding the opponent’s utility. That is, the different possible types of the opponent were public knowledge, but theexact type of the opponent was unknown. The negotiation lasts at most 14 time periods, each with a duration of twominutes. If an agreement is not reached by the deadline then the negotiation terminates with a status quo outcome.Each party can also opt out of the negotiation if it decides that the negotiation is not proceeding in a favorable way.One of the domains was based on an international crisis, and the subjects had to play a role that was outside oftheir normal experience. On the other hand, the second domain was more related to the subjects’ experience, so theycould identify with it better. We describe the two domains in the following subsections. Detailed score functions forboth domains can be found in Appendix B. A snapshot of one of the negotiation experiments in the second domain isgiven in Appendix C.5.1.1. The World Health Organization’s Framework Convention on Tobacco Control domainIn this scenario England and Zimbabwe negotiate in order to reach an agreement evolving from the World HealthOrganization’s Framework Convention on Tobacco Control, the world’s first public health treaty. The principal goal2 Preliminary results on the first domain were presented in [14].R. Lin et al. / Artificial Intelligence 172 (2008) 823–851835of the convention is “to protect present and future generations from the devastating health, social, environmental andeconomic consequences of tobacco consumption and exposure to tobacco smoke”.The leaders of both countries are about to meet at a long scheduled summit. They must reach an agreement on thefollowing issues:(1) The total amount to be deposited into the Global Tobacco Fund to aid countries seeking to rid themselves ofeconomic dependence on tobacco production. This issue has an impact on the budget of England and on theeffectiveness of short-range and long-range economic benefits for Zimbabwe. The possible values are (a) $10billion, (b) $50 billion, (c) $100 billion, or (d) no agreement. Thus, a total of 4 possible values are allowed forthis issue.(2) Impact on other aid programs. This issue affects the net cost to England and the overall benefit for Zimbabwe.If other aid programs are reduced, the economic difficulties for Zimbabwe will increase. The possible values are(a) no reduction, (b) reduction equal to half of the Global Tobacco Fund, (c) reduction equal to the size of theGlobal Tobacco Fund, or (d) no agreement. Thus, a total of 4 possible values are allowed for this issue.(3) Trade issues. Both countries can use trade policy to extract concessions or provide incentives to the other party.They can use restrictive trade barriers such as tariffs (taxes on imports from the other country) or they can liber-alize their trade policy by increasing imports from the other party. There are both benefits and costs involved inthese policies: tariffs may increase revenue in the short run but lead to higher prices for consumers and possibleretaliation by affected countries over the long run. Increasing imports can cause problems for domestic industries.But it can also lead to lower consumer costs and improved welfare. Thus, the possible values are divided betweenZimbabwe’s (a) reducing tariffs or (b) increasing tariffs on imports, and England’s (a) reducing or (b) increasingimports. Both can also choose not to agree on this. Thus, a total of 9 possible values are allowed for this issue.(4) Creation of a forum to explore comparable arrangements for other long-term health issues. This issue relates tothe precedent that may be set by the Global Tobacco Fund. If the fund is established, Zimbabwe will be highlymotivated to apply the same approach to other global health agreements. This would be very costly to England.The possible values are (a) creation of a fund, (b) creation of a committee to discuss the creation of a fund,(c) creation of a committee to develop an agenda for future discussions, or (d) no agreement. Thus, a total of 4possible values are allowed for this issue.Consequently, a total of 576 possible agreements exist (4 × 4 × 3 × 3 × 4 = 576). While for the first two issuesthere are contradicting preferences for England and Zimbabwe, for the last two issues there are options which mightbe jointly preferred by both sides.Each turn in the scenario is equivalent to a week of the summit, while the summit is limited to 14 weeks. If noagreement is reached within the specified time limit, the Framework Convention will be seen as an empty document,devoid of any political significance. This will be a blow to England, which has invested political capital to reach anagreement, in the hope of gaining support for other, perhaps more important, international agreements in the future. Itwill also, however, save England money in the near term. For Zimbabwe, failure to reach an agreement will create amajor financial hardship and deprive it of a precedent that can be used for future negotiations. Consequently, Englandis better able to accept a failure than is Zimbabwe. This outcome is modeled for both agents as the status quo outcome.Opting out of the negotiation is also an option. Opting out by England means trade sanctions imposed by Englandon Zimbabwe (including a ban on the import of tobacco from Zimbabwe), while if Zimbabwe opts out then it willboycott all British imports. However, if England opts out it also saves the funds that would have been spent on theTobacco Fund, and if Zimbabwe opts out it loses the opportunity for financial gain and for assistance in reducingthe health problems that arise from tobacco use. Consequently, England will likely be more willing to opt out if thenegotiations are not going its way, and Zimbabwe will be more willing to continue negotiations until agreement isreached.Time also has an impact on the negotiations. Creation of the fund is more urgent for Zimbabwe than for England.Consequently, Zimbabwe has an incentive to reach an agreement earlier rather than later; thus as time advancesZimbabwe’s utility reduces. On the other hand, England gains as time advances, as it postpones the time at which itmust transfer money to the fund.Taking into account the different types of agents, we can say, for example, that an agent representing Zimbabwewith a short term orientation, will focus on a short term redistribution of resources, insist on the largest possible836R. Lin et al. / Artificial Intelligence 172 (2008) 823–851current assistance and help with long-term health problems, as well as trade concessions. On the other hand, an agentrepresenting England with the same short term orientation, for example, will aim to minimize current cost, limitimpact on trade, and maintain its economic and political position in the near term.5.1.2. The Job Candidate domainIn this scenario, a negotiation takes place after a successful job interview between an employer and a job candidate.In the negotiation both the employer and the job candidate wish to formalize the hiring terms and conditions of theapplicant. In contrast to the England–Zimbabwe scenario, some issues must be agreed upon to achieve even a partialagreement. Below are the issues under negotiation:(1) Salary. This issue dictates the total net salary the applicant will receive per month. The possible values are(a) $7000, (b) $12,000, or (c) $20,000. Thus, a total of 3 possible values are allowed for this issue.(2) Job description. This issue describes the job description and responsibilities given to the job applicant. The jobdescription has an effect on the advancement of the candidate in his/her work place and his/her prestige. Thepossible values are (a) QA, (b) programmer, (c) team manager, or (d) project manager. Thus, a total of 4 possiblevalues are allowed for this issue.(3) Social benefits. The social benefits are an addition to the salary and thus impose an extra expense on the employer,yet they can be viewed as an incentive for the applicant. The social benefits are divided into two categories:company car and the percentage of the salary allocated, by the employer, to the candidate’s pension funds. Thepossible values for a company car are (a) providing a leased company car, (b) no leased car, or (c) no agreement.The possible value for the percentage of the salary deposited in pension funds are (a) 0%, (b) 10%, (c) 20%, or(d) no agreement. Thus, a total of 12 possible values (3 × 4 = 12) are allowed for this issue.(4) Promotion possibilities. This issue describes the commitment by the employer regarding the fast track for pro-motion for the job candidate. The possible values are (a) fast promotion track (2 years), (b) slow promotion track(4 years), or (c) no agreement. Thus, a total of 3 possible values are allowed for this issue.(5) Working hours. This issue describes the number of working hours required by the employee per day (not includingover-time). This is an integral part of the contract. The possible values are (a) 8 hours, (b) 9 hours, or (c) 10 hours.Thus, a total of 3 possible values are allowed for this issue.In this scenario, a total of 1296 possible agreements exist (3 × 4 × 12 × 3 × 3 = 1296).Each turn in the scenario equates to two minutes of the negotiation, and the negotiation is limited to 28 minutes. Ifthe sides do not reach an agreement by the end of the allocated time, the job interview ends with the candidate beinghired with a standard contract, which cannot be renegotiated during the first year. This outcome is modeled for bothagents as the status quo outcome.Each side can also opt-out of the negotiation if it feels that the prospects of reaching an agreement with the opponentare slim and it is impossible to negotiate anymore. Opting out by the employer entails the postponement of the projectfor which the candidate was interviewing, with the possible prospect of its cancellation and a considerable amount ofexpenses.Opting-out by the job candidate will make it very difficult for him to find another job, as the employer will spreadhis/her negative impression of the candidate to other CEOs of large companies.Time also has an impact on the negotiation. As time advances the candidate’s utility decreases, as the employer’sgood impression has of the job candidate decreases. The employer’s utility also decreases as the candidate becomesless motivated to work for the company.5.2. Experimental methodologyWe evaluated the performance of the agent against human subjects, all of whom were computer science undergrad-uates at Bar-Ilan University in Israel. The experiment involved 88 simulations with human subjects, divided into 44pairs, such that 44 simulations were run for each domain. Each simulation was divided into two parts: (i) negotiatingagainst another human subject, and (ii) negotiating against the automated agent. While the subjects knew that they willnegotiate against both an automated agent and against another human, they did not know in advance against whomthey played. Also, in order not to bias the results as a consequence of the subjects’ familiarity with the domain andR. Lin et al. / Artificial Intelligence 172 (2008) 823–851837the simulation, for exactly half of the subjects the first part of the simulation consisted of negotiating with a humanopponent, while the other half negotiated first with the automated agent. The outcome of each negotiation is eitherreaching a full agreement, opting out, or reaching the deadline without an agreement. Prior to the experiments, thesubjects were given oral instructions regarding the experiment and the domain. The subjects were instructed to playbased on their score functions and to achieve the best possible agreement for them.5.3. Experimental resultsThe main goal of the experiments was to verify that the automated agent is capable of achieving better agreementsthan a human playing the same role, and to facilitate an earlier end to the negotiation as compared to negotiationswithout the agent. A secondary goal was to check whether indeed the agent facilitated an increase in the social welfareof the outcome, that is, improved the utility scores for both parties, as compared to negotiations without an automatedagent. When analyzing the results we use three types of statistical tests:• t-test: A statistical hypothesis test in which the test statistic has a t-distribution if the null hypothesis is true.This test requires a normal distribution of the measurements ([2], Chapter 3). Thus, it is used in our analysis forcomparing utility values, which have continuous values.• Wilcoxon signed-rank test: A non-parametric alternative to the paired t-test for the case of two related samples orrepeated measurements on a single sample. This test does not require any assumptions regarding the distributionof the measurements ([22], Chapter 5). This test is used in our analysis for comparing discrete samples.• Fisher’s Exact test: Fisher’s exact test is a statistical significance test used in the analysis of categorical data wheresample sizes are small. This test is used to examine the significance of the association between two variables ina 2 × 2 contingency table ([8], Chapter 14.4). We use this test in order to show the correlation between the typeof agreements reached (full agreement or partial) and the type of negotiators who reach them (two humans or ahuman and an automated agent).As we mentioned earlier, we experimented in two distinct domains. Tables 4 and 5 summarize the average utilityvalues of all the negotiations, the average ranking of the agreements reached, and the average of the sums of utilityvalues and ranking of the agreements in all the experiments in the England–Zimbabwe domain and the Job Candidatedomain, respectively. HZim, HEng, HCan and HEmp denote the utility value gained by people playing the role of Zim-Table 4Utility values, ranking values, sums of utility values and sums of ranking valuesof final negotiations in the England–Zimbabwe domainParameterQEng vs. HZimrank(QEng) vs. rank(HZim)HEng vs. HZimrank(HEng) vs. rank(HZim)QZim vs. HEngrank(QZim) vs. rank(HEng)HZim vs. HEngrank(HZim) vs. rank(HEng)HZim vs. QEngrank(HZim) vs. rank(QEng)HEng vs. QZimrank(HEng) vs. rank(QZim)Sum-HEng vs. QZimSum-rank(HEng) vs. rank(QZim)Sum-HZim vs. QEngSum-rank(HZim) vs. rank(QEng)Sum-HEng vs. HZimSum-rank(HEng) vs. rank(HZim)Avg565.10.82331.80.5818.450.64−92.60.60−322.550.41311.500.573301.21242.51.25239.21.17Stdev283.300.17210.60.19223.10.13247.900.15265.940.16204.790.18222.80.07409.40.03298.80.07838R. Lin et al. / Artificial Intelligence 172 (2008) 823–851Table 5Utility values, ranking values, sums of utility values and sums of ranking valuesof final negotiations in the Job Candidate domainParameterQCan vs. HEmprank(QCan) vs. rank(HEmp)HCan vs. HEmprank(HCan) vs. rank(HEmp)QEmp vs. HCanrank(QEmp) vs. rank(HCan)HEmp vs. HCanrank(HEmp) vs. rank(HCan)HCan vs. QEmprank(HCan) vs. rank(QEmp)HEmp vs. QCanrank(HEmp) vs. rank(QCan)Sum-HEmp vs. QCanSum-rank(HEmp) vs. rank(QCan)Sum-HCan vs. QEmpSum-rank(HCan) vs. rank(QEmp)Sum-HEmp vs. HCanSum-rank(HEmp) vs. rank(HCan)Avg4090.75309.70.56437.30.77410.60.75342.450.58448.820.74852.81.49779.71.35720.31.30Stdev93.950.19140.20.29121.70.19114.00.20114.400.2482.410.21132.20.23199.00.24212.50.27babwe or England and the role of the job candidate or the employer, respectively, and QZim, QEng, QCan and QEmpdenote the utility value gained by the QO agent playing either role in either domain.The utility values range from −575 to 895 for the England role and from −680 to 830 for the Zimbabwe role, andin the Job Candidate domain from 170 to 620 for the employer role and from 60 to 635 for the job candidate role. Thestatus quo value in the beginning of the negotiation was 150 for England and −610 for Zimbabwe, and in the seconddomain it was 240 for the employer and −160 for the job candidate. England had a fixed gain of 12 points per timeperiod, while Zimbabwe had a fixed loss of −16 points. In the Job Candidate domain both players had a fixed loss pertime period—the employer of −6 points and the job candidate of −8 points per period.In both domains similar results were achieved. Thus, in this section we elaborate mainly on the results of the firstdomain. Later, we discuss the results in both domains.5.3.1. Results of negotiations against peopleFirst, we examine the final utility values of all the negotiations for each player, and the sums of the final utilityvalues. When the automated agent played the role of England the average utility value achieved by the automatedagent was 565.1, while the average for the human playing the role of England was 331.8. The results show that ouragent achieves significantly higher utility values as opposed to a human agent playing the same role (using the 2-sample t-test: t (22) = 3.10, p < 0.004). (This was also the case when the automated agent played the role of the jobcandidate in the second domain (using the 2-sample t-test: t (22) = 2.76, p < 0.008).) On the other hand, when theagent played the role of Zimbabwe, there was no significant difference between the utility values of the agent and thehuman player, though the average utility value for the automated agent was higher (18.45) than that of the humans(−92.6). One explanation for the higher values achieved by the QO agent is that the QO agent is more eager toaccept agreements than humans, when playing the Zimbabwe side, which has a negative time cost (when QO playedZimbabwe the average end turn of the negotiation was 5, while when the humans played Zimbabwe the average endturn was 7). Thus, accepting agreements sooner rather than later allows the agent to gain higher utility values than thehuman playing the same side. On the other hand, when the agent played the role of England, the average end turn forthe negotiation was 7 and the same average was achieved when the humans played the role of England.The above results are also supported by the results received from ranking the agreements. When the automatedagent played the role of Zimbabwe, the average ranking it achieved was similar to the ranking the human playersattained playing the same role (0.64 and 0.60). On the other hand, when the automated agent played the role ofR. Lin et al. / Artificial Intelligence 172 (2008) 823–851839England it achieved significantly higher ranking values than the human playing the same role, with an average of 0.82as compared to only 0.58 (using the 2-sample Wilcoxon test, p < 0.002).Comparing the sum of utility values of both negotiators, based on the role the agent played, we show that this sumis higher when the agent is involved in the negotiations. When the automated agent played the role of Zimbabwe,the sum of utility values was 330 as opposed to only 239.2 when two humans were involved. When the automatedagent played the role of England, the sum of utility values was 242.5, which is only marginally higher than thescore of 239.2 reached by the human subjects. (In the second domain the sum was also higher. Furthermore, whenthe automated agent played the role of the job candidate the sum was even significantly higher, using the 2-samplet-test: t (22) = 2.48, p < 0.002, when compared to negotiations in which no automated agent was involved.) Whencomparing the sum of the rankings, we note that when the automated agent was involved the sum of rankings washigher than when only humans were involved (an average of 1.21 and 1.25 when the automated agent played the roleof Zimbabwe and England, respectively, and an average of 1.17 when the human players played against each other).However, this is only significant when the automated agent played the role of England (using the 2-sample Wilcoxontest, p < 0.001).Another important aspect of the negotiation is the outcome—whether a full agreement was reached or whether thenegotiation ended with no agreement (either status quo or opting out) or with a partial agreement. While only 64%of the negotiations involving only people ended with a full agreement, more than 72% of the negotiations involvingthe automated agent ended with a full agreement (and in the Job Candidate domain, respectively 72% and 86%).Using the Fisher’s Exact test we determined that a correlation exists between the kind of opponent agent (be it anautomated agent or a human) and the form of the final agreement (full, partial or none). The results show that there isa significantly higher probability of reaching a full agreement when playing against an automated agent (p < 0.006for both domains).In the next section we discuss the results of the experiments in both domains, when negotiating against people.5.3.2. Discussion: Results against peopleThe results of the experiments, described above, show that the automated agent achieved higher utility values thanthe human counterpart. This can be explained by the nature of our agent both in reference to accepting offers andgenerating offers. Using the decision making mechanism we allow the agent to propose agreements that are goodfor it, but also reasonable for its opponent. In addition, the automated agent makes straightforward calculations. Itevaluates the offer based on its attributes, and not based on its content. In addition, it also places more weight on thefact that it loses or gains as time advances. This is not the case, however, when analyzing the logs of the people. Itseems that people put more weight on the content of the offer than on its value. This was more evident in the JobCandidate domain with which the human subjects could more easily identify.Yet, this does not explain why, in both domains, these results are significant only for one of the sides. In theEngland–Zimbabwe domain, the results are significant when the agent played the role of England, while in the JobCandidate domain these results are significant when it played the role of the job candidate. It is interesting to notethat our results, which show that the automated agents play significantly better when playing one of the sides, are notunique. Kraus et al. [11] also experimented with an automated agent playing against humans. While they experimentwith a single-issue negotiation in one domain only (i.e. a fishing dispute domain—which is different from ours) theyalso showed that their agent design, which has a different design and logic than the one implemented by our agent,played significantly better only when playing one of the sides.In the original version of the agent [14] we believed this to be attributed to the fact that the agent is more eagerto accept agreements than people. To this end, we updated the agent and made it less eager (and more conservative)when it comes to accepting agreements. While in the original version of the agent we did not impose any restrictionson accepting agreements (agreements were accepted purely on probability based on the ranking of the offer), inthe current version, acceptance by the agent depends on whether the value of the offer is greater than the agent’sreservation price (r). While still many negotiations ended by the agent accepting the offer (and not by the agentproposing the winning offer), it allowed us to improve the scores of the agent. However, the fact still remained thatthese results are only significant for one of the roles in each domain.Another possible explanation for this phenomenon can be found by examining the logs of the negotiations and thevalues of the agreements. In both domains we can see that the British side and the job candidate sides are the moredominant sides and have more leverage than the other side. For example, for England the fact that it gains as time840R. Lin et al. / Artificial Intelligence 172 (2008) 823–851advances could place more pressure on the other side to accept agreements. For the job candidate side, a psychologicalinterpretation could serve as an explanation. It seems that the job candidate’s side has less to lose in the negotiation.While both the employer and the job candidate lose as time passes, the status quo agreement ensures the hiring of thecandidate.5.3.3. Results of an automated agent playing against another automated agentIn this set of experiments, we matched our automated agent against another automated agent. We conducted twosets of experiments. In both experiments the agents negotiated in both domains—the England–Zimbabwe domain andthe Job candidate domain. In the first we matched our automated agent against itself. In the second set of experimentswe matched it against another automated agent which followed a Bayesian equilibrium approach. In a Bayesian game,agents face uncertainties about the characteristics (types) of other agents. This imperfect information can be modeledby letting Nature select the agents’ types. Agents have initial beliefs about the type of each agent and can updatetheir beliefs according to Bayes’ rule. Since the agents only know their own types, they must seek to maximize theirexpected payoff, given their beliefs about the other players. Note that our domains and experimental settings can beviewed as Bayesian games. The Bayesian Nash equilibrium is then defined as a strategy profile and beliefs specifiedfor each agent about the types of the others agents that maximizes the expected payoff for each agent given theirbeliefs about the other agents’ types and given the strategies played by the other agents ([20], pp. 24–29). Recall thatthere are k possible types of agents. Both our automated agent and the agent which followed a Bayesian equilibriumapproach assume that the initial prior probability of each type is equal, that is, P(typeit=0) = 1k , ∀i ∈ Types.In the first set of experiments, when the automated agent was matched against itself, most of the agreements werereached by the earlier turns (by the third round in the England–Zimbabwe domain and by the second round in theJob Candidate domain). The average utility values for the QO agent playing England was 325.13 and for the QOagent playing Zimbabwe 79.93 and 499.58 and 423.06 when playing the role of the employer and job candidate,respectively. When looking at the utility values gained by the automated agent itself in both domains, the automatedagent’s results are higher than the results obtained by humans when playing the same role against either a humanor an automated agent (the results are also significant when the automated agent played the role of Zimbabwe withp < 0.004 and both the employer and job candidate with p < 0.002 for both roles).In addition, the average sum of utility values, 405.07 in the England–Zimbabwe domain and 922.65 in the JobCandidate domain, is also higher (significantly higher in the Job Candidate domain with p < 0.002 when it played therole of the employer and p < 0.02 when it played the role of the job candidate) than the sum obtained when either theautomated agent played against people (330 when it played the role of Zimbabwe and 242.5 when it played the roleof England; 779.7 when it played the role of the employer and 852.8 when it played the role of job candidate) and itwas significantly higher than negotiations in which only people were involved (an average of 239.2 and p < 0.017 inthe England–Zimbabwe domain and an average of 720.3 and p < 0.001 in the Job Candidate domain).We can see that in both domains when the automated agent is matched against itself, it reaches better agreementsfor both sides. This can be attributed to the decision making component of the agent, which, as we described above,allows the agent both to generate agreements that are good for it, but also reasonable for the other side, and also toaccept such agreements when they are proposed by their rival.In the next set of experiments, we matched our automated agent against an automated agent that followed theBayesian equilibrium strategy.In the first domain, when the equilibrium agent played the role of Zimbabwe and the QO agent played the roleof England, most of the negotiations ended by the early time periods, while in the second domain, for both rolesthe negotiation ended early (third time period in the England–Zimbabwe domain and second time period in the JobCandidate domain). The average final utility values of the negotiations were 398.38 for the QO agent playing the roleof England and 61.38 for the equilibrium agent. In the Job candidate domains the values were 488.28 for the QO agentplaying the role of the job candidate and 426.89 for the equilibrium agent and 459.8 for the QO agent playing therole of the employer and 488.4 for the equilibrium agent. In all these cases, the final utility values for the QO agentwere higher than the average utility values achieved by the humans playing either against our automated agent oragainst themselves. On the other hand, when the equilibrium agent played the role of England, all of the negotiationslasted until the last time period and eventually ended with a status quo agreement, giving England a very high scoreof 981 and Zimbabwe a very low score of −548. The differences in the results between the two domains is that in theEngland–Zimbabwe domain, an agent playing Zimbabwe loses as time advances, so the equilibrium agent playing theR. Lin et al. / Artificial Intelligence 172 (2008) 823–851841role of Zimbabwe is highly motivated to propose an attractive offer to the opponent to facilitate the termination of thenegotiation sooner rather than later. In the job-candidate domain, however, both sides lose as time advances, and thuswhen the equilibrium agent played either side the negotiation ended quickly and did not drag on until the last turn.Though we did not run simulations of the equilibrium agent against human agents, the utility values of the opponentfrom the offers suggested by the equilibrium agent are much lower than the final utility values of the human negotia-tions. By also analyzing the simulation process of the human negotiations, we can deduce that without incorporatingany heuristics into the equilibrium agent, the human players would not have accepted the offers proposed by it. Thus,when the equilibrium agent would play the role of England the negotiation might be dragged out until the last turnwith the implementation of a status quo agreement. However, perhaps the human playing the role of Zimbabwe wouldhave given up and preferred opting-out, resulting in an outcome which would have been worse (utility-wise) for theequilibrium agent than the status quo outcome.6. ConclusionsThis paper presents an automated agent design for bilateral negotiation with bounded rational agents where thereis incomplete information regarding the opponent’s utility preferences. The results show that the agent is indeedcapable of negotiating successfully with human counterparts and reaching efficient agreements. In addition, the resultsdemonstrate that the agent plays at least as well as, and in the case of one of the two roles, achieved significantly higherutility values, than the human player. By running the experiments on two distinct domains we have shown that it isquite straightforward to adapt the simulation environment and the agent to any given scenario.We have developed an automated negotiation environment. However, we do not intend to replace humans in ne-gotiation, but rather to use the model as an efficient decision support tool or as a training tool for negotiations withpeople. Thus, this model can be used to support training in real life negotiations, such as: e-commerce, and it can alsobe used as the main tool in conventional lectures or online courses, aimed at turning the trainee into a better negotiator.We have shown the importance of designing an automated negotiator that can negotiate efficiently with humansand we have shown that indeed it is possible to design such a negotiator. We believe that the results of our researchcan be particularly useful for constructing agents in open environments where uncertainty prevails. By pursuing non-classical methods of decision making it could be possible to achieve greater flexibility and effective outcomes. As wehave shown, this can also be done without constraining the model to the domain. Thus these agents could be extremelyuseful in e-commerce environments and e-negotiations.Most negotiation tools today are domain-dependent and focus on a single negotiation issue (e.g., see [23]). Thesetools do not provide an efficient training and learning experience for the trainee. Instead of providing the trainee witha wide range of test cases, they constrain him/her to a predefined scenario, which is only a fragment of the variety ofscenarios he/she might encounter in the real world. We have demonstrated that our automated negotiation environmentis adaptable such that any scenario and utility function, expressed as a single issue or multi-issue attributes, can beused, with no additional changes in the configuration of the interface of the simulations or the automated agents. Theautomated agents can play either role in the negotiation. In addition, our environment embodies an automated agentthat plays against the trainee. This allows the trainee to use it anytime in order to test his/her capabilities and noteimprovements.Although much time was spent on designing the mechanism for generating an offer, the results show that mostof the agreements reached were offered by the human counterpart. This, indeed, allowed for more agreements to bereached when the automated agent was involved (as compared to negotiations in which only humans were involved).Nonetheless, a careful investigation should be made to examine how the offer generation mechanism can be improved.Another direction for future would be to improve this mechanism in order to allow the agent to make more than oneoffer per turn and to add more ‘personality’ to the agent, by allowing it to interact more with the opponent and adaptits approach based on this interaction and the pressure of time.Appendix A. Theoretical proofsTo prove Theorem 4.1 we will prove the following claims. Combining those proofs depicts the correctness ofour theorem. In the following analysis, the terms a and b are used synonymously as the types of agents a and b,respectively. In the analysis we will refer to the automated agent as the agent of type a and its opponent as the agent842R. Lin et al. / Artificial Intelligence 172 (2008) 823–851of type b. We also assume that there is a unique negotiation solution. Recall also that in our model, the disagreementpoint d is equivalent to opting out.Claim A.1. QO always generates an agreement which is not worse than the disagreement point,3 (cid:5)d, for both agents.Proof. Since (cid:5)d is the worst outcome for both agents, then:∀(cid:5)x ∈ O rankj ((cid:5)x) (cid:3) rankj ( (cid:5)d)Assume, by contradiction, that QO generated (cid:5)d as the offer. Following the QO function (Eq. (3)), we obtain:(cid:4)(cid:3)lua( (cid:5)d) + lub( (cid:5)d)· rankb( (cid:5)d)(cid:5)d = arg min{αd , βd } where αd = ranka( (cid:5)d) · lua( (cid:5)d) and βd =Also assume that there is another agreement (cid:5)x ∈ O, (cid:5)x (cid:14)= (cid:5)d such that:αx = ranka((cid:5)x) · lua((cid:5)x)(cid:4)(cid:3)lua((cid:5)x) + lub((cid:5)x)βx =· rankb((cid:5)x)(A.1)(A.2)(A.3)Thus, we can distinguish between two options. In the first, αd < βd . In this case, following our assumption, αd mustnow be selected as the maximum of all the minima. Now, if the agreement (cid:5)x satisfies αx < βx , then αx will be chosenas the minima and must be compared to αd when choosing the maximum. Since (cid:5)d is the worst outcome, and followingEq. (A.1), αx > αd , which contradicts our assumption that (cid:5)d was chosen by QO. Thus, we find that αx > βx . In thiscase, βx is compared to αd . Following Eq. (A.1) and Property 3.1 we obtain for every (cid:5)x (cid:14)= (cid:5)d also luj ((cid:5)x) (cid:3) luj ( (cid:5)d).Thus, βx > αd , which means that (cid:5)d cannot have been chosen by QO.In the second option we have αd > βd . Following similar considerations as the ones stated in the first case we alsoobtain that βd cannot be chosen as the maximum, and thus (cid:5)d cannot have been chosen by QO. (cid:2)Claim A.2. QO satisfies symmetry (Property 4.1).Proof. Let B = (cid:8)(ua(·), ub(·)), (cid:5)d(cid:9) be a symmetric negotiation problem with a symmetric function φ. Let (cid:5)x∗be the negotiation solution generated by QO. We need to show that φ((cid:5)x∗) is also a negotiation solution, i.e.,(ua(φ((cid:5)x∗)), ub(φ((cid:5)x∗))) = (ua((cid:5)x∗), ub((cid:5)x∗)). From the definition of the bargaining problem and Claim A.1 we knowthatua((cid:5)xub((cid:5)x∗∗) (cid:3) ua( (cid:5)d)) (cid:3) ub( (cid:5)d)Since φ is a symmetric function we find that (Definition 4.1)(cid:5)uaubφ((cid:5)x(cid:5)φ((cid:5)x∗∗(cid:6))(cid:6))(cid:6)(cid:5)φ( (cid:5)d)(cid:6)φ( (cid:5)d)(cid:5)(cid:3) ua(cid:3) ub= ua( (cid:5)d)= ub( (cid:5)d)Assume, by contradiction, that φ((cid:5)x∗) is not a negotiation solution. Then, there is (cid:5)y ∈ O such thatua((cid:5)y) (cid:3) uaub((cid:5)y) (cid:3) ub∗(cid:5)φ((cid:5)x(cid:5)∗φ((cid:5)x(cid:6))(cid:6))(A.4)(A.5)(A.6)(A.7)(A.8)(A.9)(cid:5)But since φ is a symmetric function we attain(cid:5)φ ◦ φ((cid:5)x(cid:5)φ ◦ φ((cid:5)x(cid:6)φ((cid:5)y)(cid:6)(cid:5)φ((cid:5)y)= ua((cid:5)x= ub((cid:5)x(cid:3) ua(cid:3) ubuaub(cid:6))(cid:6))))∗∗∗∗(A.10)From Eq. (A.10) we reveal that there is another solution, φ((cid:5)y), in the original negotiation problem which is not worsethan (cid:5)x∗, which is the negotiation solution QO generated. However, this contradicts the fact that (cid:5)x∗ is the uniquenegotiation solution in the original problem. (cid:2)3 If the disagreement point is unique, then QO always generates an agreement which has a higher utility value than the disagreement point.R. Lin et al. / Artificial Intelligence 172 (2008) 823–851843Claim A.3. QO satisfies efficiency (Property 4.2).Proof. We will prove this by contradiction. Let (cid:5)x∗ be the solution. That is:(cid:5)x∗ = arg min{αx, βx} where αx = ranka((cid:5)x(A.11)Assume that (cid:5)x∗ is not efficient, that is, there is (cid:5)y ∈ O such that ua((cid:5)y) > ua((cid:5)x∗) and ub((cid:5)y) > ub((cid:5)x∗). For (cid:5)y to be) and βx =) + lub((cid:5)x· rankb((cid:5)x) · lua((cid:5)x)∗∗∗∗∗(cid:3)lua((cid:5)x(cid:4))chosen by QO:(cid:5)y = arg min{αy, βy} where αy = ranka((cid:5)y) · lua((cid:5)y) and βy =(A.12)QO generates a set of minima from which it selects the maximum as (cid:5)x∗. We will distinguish between four possibleselections:· rankb((cid:5)y)(cid:4)(cid:3)lua((cid:5)y) + lub((cid:5)y)(1) αx < βx .Here there are two possible options:(a) αy < βy .Since QO selected agreement (cid:5)x∗ we know that αx > αy . However, this contradicts the assumption thatua((cid:5)y) > ua((cid:5)x∗) (since ranka((cid:5)y) > ranka((cid:5)x∗) and lua((cid:5)y) > lua((cid:5)x∗)).(b) αy > βy .That is, βy is chosen as the minimum. Since QO selects αx as the maximum element in the set we ob-tain βx > αx > βy . However, since ua((cid:5)y) > ua((cid:5)x∗) and ub((cid:5)y) > ub((cid:5)x∗) we reveal that lua((cid:5)y) > lua((cid:5)x∗),ranka((cid:5)y) > ranka((cid:5)x∗), lub((cid:5)y) > lub((cid:5)x∗), rankb((cid:5)y) > rankb((cid:5)x∗). However, this requires that βy > βx . Thus,the contradiction assumption is wrong.(2) αx > βx .As in the previous case, there are two possible options:(a) αy < βy .Since QO selected βx then βx > αy . By the assumption, ua((cid:5)y) > ua((cid:5)x∗). Thus, we obtain βx > αy > αxwhich is in contradiction with our base case (Case A.3). Thus, the contradiction assumption is wrong.(b) αy > βy .Since QO selects βx as the maximum element in the set of all minima we find that βx > βy . However,since ua((cid:5)y) > ua((cid:5)x∗) and ub((cid:5)y) > ub((cid:5)x∗) we reveal that lua((cid:5)y) > lua((cid:5)x∗), ranka((cid:5)y) > ranka((cid:5)x∗), lub((cid:5)y) >lub((cid:5)x∗), rankb((cid:5)y) > rankb((cid:5)x∗). However, this requires that βy > βx . Thus, the contradiction assumption iswrong. (cid:2)Claim A.4. QO satisfies invariance (Property 4.3) under the following conditions:(1) ui((cid:5)o) < uj ((cid:5)o) ⇔ u(cid:12)(2) ∀(cid:5)o, (cid:5)p ∈ O ui((cid:5)o) > uj ( (cid:5)p) ⇒ u(cid:12)i((cid:5)o) < u(cid:12)j ((cid:5)o);4i((cid:5)o) > u(cid:12)j ( (cid:5)p).Proof. Let B = (cid:8)(ua(·), ub(·)), (cid:5)d(cid:9) and B(cid:12) = (cid:8)(u(cid:12)b(·)), (cid:5)d(cid:9) be equivalent problems. Let (cid:5)x∗ be the negotiationsolution generated by QO. We need to show that f (B) = f (B(cid:12)), that is, the same negotiation solution is obtained byf (B(cid:12)). From Eq. (3), we observe that we perform a maximum over the set of minima which is generated from uaand ub. Condition 1 above guarantees us that if ua((cid:5)x) (cid:2) ub((cid:5)x) then also u(cid:12)b((cid:5)x). Thus, the set of minimaa, u(cid:12)remains identical using ua, ub and u(cid:12)b.a((cid:5)x) (cid:2) u(cid:12)a(·), u(cid:12)Note that since we perform a linear transformation where αi > 0 and αj ∈ R+, βi ∈ R then(cid:12)(cid:12)i((cid:5)y).i((cid:5)x) > uui((cid:5)x) > ui((cid:5)y) ⇒ u(A.13)Condition 2 and Eq. (A.13) above guarantee that the preference relation among this set will also remain the same.Thus, we will attain the same solution (cid:5)x∗. The value of this solution for B(cid:12) can be obtained using the same lineartransformation, that is, QOB(cid:12) (t) = αax∗ + βi . (cid:2)4 This condition is applicable in domains such as ours, in which there are contradictory preferences.844R. Lin et al. / Artificial Intelligence 172 (2008) 823–851Claim A.5. QO satisfies independence of irrelevant alternative solutions (Property 4.5).a(·), u(cid:12)Proof. Let B = (cid:8)(ua(·), ub(·)), (cid:5)d(cid:9) and B(cid:12) = (cid:8)(u(cid:12)b(·)), (cid:5)d(cid:9) be negotiation problems, where B(cid:12) ⊆ B. Let X be theset of all possible negotiation solutions generated by QOB (|X| = 1 if there is only one maximum, and |X| > 1 if thereare several maxima) such that X ⊆ B(cid:12). We need to show that X is also the set of all possible negotiation solutionsgenerated by QOB(cid:12) . Let (cid:5)x∗ ∈ X be an arbitrary solution. That is, (cid:5)x∗ is chosen as the maximum element of all thevalues in the minima set. Since the agreement associated with (cid:5)x∗ is also in B(cid:12) and since B(cid:12) contains only a subset ofthe agreements in B, (cid:5)x∗ must also be the maximum element in the minima set for B(cid:12). That is, (cid:5)x∗ is also chosen as themaximum. (cid:2)Claim A.6. Let the agreement given by the Nash solution be (cid:5)x. If an agreement (cid:5)y exists where lua((cid:5)y) · ranka((cid:5)y) >lua((cid:5)x) · ranka((cid:5)x) and lua((cid:5)y) · ranka((cid:5)y) < [lua((cid:5)y) + lub((cid:5)y)] · rankb((cid:5)y) then QO’s solution will be (cid:5)y rather than (cid:5)x.Proof. From the assumption we know the lua((cid:5)y) · ranka((cid:5)y) is chosen as the minimum. We distinguish between thetwo possible cases:(1) lua((cid:5)x) · ranka((cid:5)x) < [lua((cid:5)x) + lub((cid:5)x)] · rankb((cid:5)x).(2) lua((cid:5)x) · ranka((cid:5)x) > [lua((cid:5)x) + lub((cid:5)x)] · rankb((cid:5)x).In this case, lua((cid:5)x) · ranka((cid:5)x) is also chosen as the minimum and since ua((cid:5)y) > ua((cid:5)x), QO will prefer (cid:5)y over (cid:5)x.In this case, [lua((cid:5)x) + lub((cid:5)x)] · rankb((cid:5)x) is chosen as the minimum but since ua((cid:5)y) > ua((cid:5)x), which requireslua((cid:5)y) · ranka((cid:5)y) > lua((cid:5)x) · ranka((cid:5)x), QO will prefer (cid:5)y over (cid:5)x. (cid:2)Claim A.7 (Identical discount rate). If both agents have the same time constant discount rate, QO will generate thesame solution at each time unit.0 be the solution generated by QO at time t = 0. We need to show that ∀t > 0, (cid:5)x∗Proof. Let (cid:5)x∗0 . Since both agentsmultiply their utilities by the same factor, 0 < δj < 1, the preference relations among their own utility and betweentheir utilities remain the same. Thus, the probability of accepting an offer and the ranking of the offers also remain thesame. As a result, the agreement generated by QO at time t will be the same as the one generated at the previous timeunit t − 1, and thus will be the same as the one generated at time t = 0. (cid:2)= (cid:5)x∗tAppendix B. Score functionsTables B.1 and B.2 present the score functions for both negotiators, in both domains. While the human subjectis given his own score function at the beginning of the negotiation, he is also given three additional score functionswhich model the different possible types of his opponent.Appendix C. Negotiation exampleThe following is a snapshot of a negotiation log between the automated agent and a human counterpart in the Job-Candidate domain. The automated agent played the role of the job candidate (denoted QOCan) and the human playedthe role of the employer (denoted HEmp). The following is the internal log of our program. The people themselvescommunicated and received messages via GUI windows. Examples of the main screen, an offer generation screen anda receiving offer screen are given in Figs. C.1, C.2 and C.3, respectively.The log below depicts a successful negotiation which enabled the sides to reach an agreement after 5 turns. Alter-nating offers between the automated agent and the human can be observed until the offer is accepted by the automatedagent in the third turn (offer #6). Yet, the negotiation is not over since this is a partial agreement. This agreement isenforced and the agents cannot withdraw from it, unless both sides agree to a new agreement (or one side decides toopt out). We can see that the sides still need to resolve the issue of the leased car. However, since the negotiation is notover, both sides are free to propose additional offers, not necessarily ones which refer to the unresolved issue. Indeedthis is what the automated agent does. Eventually, the human player proposes to resolve the issue of the leased car(offer #12). The automated agent agrees and thus the negotiation terminates with a full agreement.R. Lin et al. / Artificial Intelligence 172 (2008) 823–851845Table B.1The England–Zimbabwe domain (i) short-term, (ii) long-term and (iii) compromise orientation score functionsOutcomesSize of fund$100 Billion$50 Billion$10 BillionNo agreementImpact on other aidNo reductionReduction is equal to half of the fund sizeReduction is equal to the fund sizeNo agreementTrade policyZimbabwe will reduce tariffs on importsZimbabwe will increase tariffs on importsEngland will increase importsEngland will reduce importsNo agreementForum on other health issuesCreation of fundCreation of committee to discuss creation of fundCreation of committee to develop agendaNo agreementTime effectStatus quoOpting outZimbabweOutcome Weight/ImportanceEnglandOutcome Weight/Importance(i)50%92−5−830%80−3−510%−637−8010%93−5−6−16−610−530(ii)10%52−3−610%60−3−430%−368−9050%85−6−8−16−500−520(iii)20%642−220%30−2−430%−4410−8030%753−3−16−210−240(i)50%−5210730%−4410−710%3−3−44010%−826112150−105(ii)10%136−110%123030%4−6−86050%74−2−412−210−240(iii)30%246−230%035−210%5−6−54030%471−212−180−75To: HEmpFrom: QOCanMessage Type: Offer (id = 1)Turn: 1Message: <Salary: 20,000 NIS, Job Description: Programmer, Leased Car: With leased car, Pension Fund: 20%,Promotion Possibilities: Fast promotion track, Working Hours: 8 hours>To: QOCanFrom: HEmpMessage Type: ResponseTurn: 1Message: Offer (id = 1) refusedTo: QOCanFrom: HEmpMessage Type: Offer (id = 2)Turn: 1Message: <Salary: 12,000 NIS, Job Description: Programmer, Leased Car: Without leased car, Pension Fund: 10%,Promotion Possibilities: Fast promotion track, Working Hours: 10 hours>To: HEmpFrom: QOCanMessage Type: Response846R. Lin et al. / Artificial Intelligence 172 (2008) 823–851Table B.2The Job Candidate domain (i) short-term, (ii) long-term and (iii) compromise orientation score functionsOutcomesSalary7000 NIS12,000 NIS20,000 NISJob descriptionQAProgrammerTeam ManagerProject ManagerLeased carWithout leased carWith leased carNo agreementPension fund0% pension fund10% pension fund20% pension fundNo agreementPromotion possibilitiesSlow promotion trackFast promotion trackNo agreementWorking hours10 hours9 hours8 hoursTime effectStatus quoOpting outJob CandidateOutcome Weight/ImportanceEmployerOutcome Weight/Importance(i)20%36815%245620%−55010%−23505%45030%357−8160150(ii)30%26925%−23685%−5505%−246025%15010%345−813575(iii)15%35620%246810%−22010%−235035%−25010%456−87080(i)20%86320%464210%3−2010%343010%33030%863−6240210(ii)15%76330%263110%42010%643020%85015%864−6306150(iii)10%76420%364310%54010%643020%64030%963−6306215Turn: 1Message: Offer (id = 2) refusedTo: QOCanFrom: HEmpMessage Type: Offer (id = 3)Turn: 2Message: <Salary: 12,000 NIS, Job Description: Programmer, Leased Car: Not discussed, Pension Fund: 10%, Pro-motion Possibilities: Fast promotion track, Working Hours: 9 hours>To: HEmpFrom: QOCanMessage Type: Offer (id = 4)Turn: 2Message: <Salary: 20,000 NIS, Job Description: Programmer, Leased Car: With leased car, Pension Fund: 20%,Promotion Possibilities: Fast promotion track, Working Hours: 9 hours>To: HEmpFrom: QOCanR. Lin et al. / Artificial Intelligence 172 (2008) 823–851847Fig. C.1. Main negotiation screen.Message Type: ResponseTurn: 2Message: Offer (id = 3) refusedTo: QOCanFrom: HEmpMessage Type: ResponseTurn: 2Message: Offer (id = 4) refusedTo: HEmpFrom: QOCanMessage Type: Offer (id = 5)Turn: 3Message: <Salary: 12,000 NIS, Job Description: Team Manager, Leased Car: With leased car, Pension Fund: 20%,Promotion Possibilities: Fast promotion track, Working Hours: 9 hours>To: QOCanFrom: HEmpMessage Type: ResponseTurn: 3Message: Offer (id = 5) refusedTo: QOCanFrom: HEmpMessage Type: Offer (id = 6)848R. Lin et al. / Artificial Intelligence 172 (2008) 823–851Fig. C.2. Generating offers screen.Turn: 3Message: <Salary: 12,000 NIS, Job Description: Programmer, Leased Car: Not discussed, Pension Fund: 20%, Pro-motion Possibilities: Slow promotion track, Working Hours: 9 hours>To: HEmpFrom: QOCanMessage Type: ResponseTurn: 3Message: Offer (id = 6) agreedTo: QOCanFrom: HEmpMessage Type: Offer (id = 7)Turn: 4Message: <Salary: Not discussed, Job Description: Not discussed, Leased Car: Without leased car, Pension Fund:Not discussed, Promotion Possibilities: Not discussed, Working Hours: Not discussed >To: HEmpFrom: QOCanMessage Type: Offer (id = 8)R. Lin et al. / Artificial Intelligence 172 (2008) 823–851849Fig. C.3. Receiving offers screen.Turn: 4Message: <Salary: 12,000 NIS, Job Description: Programmer, Leased Car: With leased car, Pension Fund: 20%,Promotion Possibilities: Fast promotion track, Working Hours: 9 hours>To: QOCanFrom: HEmpMessage Type: ResponseTurn: 4Message: Offer (id = 8) refusedTo: HEmpFrom: QOCanMessage Type: ResponseTurn: 4Message: Offer (id = 7) refusedTo: QOCanFrom: HEmpMessage Type: Offer (id = 9)Turn: 4Message: <Salary: Not discussed, Job Description: Not discussed, Leased Car: Without leased car, Pension Fund:Not discussed, Promotion Possibilities: Fast promotion track, Working Hours: Not discussed>To: HEmpFrom: QOCanMessage Type: ResponseTurn: 5Message: Offer (id = 9) refused850R. Lin et al. / Artificial Intelligence 172 (2008) 823–851To: QOCanFrom: HEmpMessage Type: Offer (id = 10)Turn: 5Message: <Salary: Not discussed, Job Description: Not discussed, Leased Car: Without leased car, Pension Fund:Not discussed, Promotion Possibilities: Fast promotion track, Working Hours: Not discussed>To: HEmpFrom: QOCanMessage Type: ResponseTurn: 5Message: Offer (id = 10) refusedTo: HEmpFrom: QOCanMessage Type: Offer (id = 11)Turn: 5Message: <Salary: 12,000 NIS, Job Description: Team Manager, Leased Car: With leased car, Pension Fund: 20%,Promotion Possibilities: Slow promotion track, Working Hours: 9 hours>To: QOCanFrom: HEmpMessage Type: ResponseTurn: 5Message: Offer (id = 11) refusedTo: QOCanFrom: HEmpMessage Type: Offer (id = 12)Turn: 5Message: <Salary: Not discussed, Job Description: Not discussed, Leased Car: With leased car, Pension Fund: Notdiscussed, Promotion Possibilities: Not discussed, Working Hours: Not discussed>To: HEmpFrom: QOCanMessage Type: ResponseTurn: 5Message: Offer (id = 12) agreedEnd Negotiation: a full agreement was reached:<Salary: 12,000 NIS, Job Description: Programmer, Leased Car: With leased car, Pension Fund: 20%, PromotionPossibilities: Slow promotion track Working Hours: 9 hours>QOCan Score: 468.0HEmp Score: 436.0References[1] R.I. Brafman, M. Tennenholtz, An axiomatic treatment of three qualitative decision criteria, Journal of the ACM 47 (3) (2000) 452–482.[2] S.R. Brown, L.E. Melamed, Experimental Design and Analysis, Sage Publications, Inc., CA, 1990.[3] C.M. Capra, J.K. Goeree, R. Gomez, C.A. Holt, Anomalous behavior in a traveler’s dilemma? American Economic Review 89 (3) (1999)678–690 (June 1999).R. Lin et al. / Artificial Intelligence 172 (2008) 823–851851[4] D. Dubois, H. Prade, R. Sabbadin, Decision-theoretic foundations of qualitative possibility theory, European Journal of Operational Re-search 128 (2001) 459–478.[5] P. Faratin, C. Sierra, N.R. Jennings, Using similarity criteria to make issue trade-offs in automated negotiations, AIJ 142 (2) (2002) 205–237.[6] S. Fatima, M. Wooldridge, An agenda based framework for multi-issue negotiation, AIJ 152 (2004) 1–45.[7] S. Fatima, M. Wooldridge, N.R. Jennings, Bargaining with incomplete information, AMAI 44 (3) (2005) 207–232.[8] J.D. Gibbons, S. Chakraborty, Nonparametric Statistical Inference, Marcel Dekker, New York, 1992.[9] P.T. Hoppman, The Negotiation Process and the Resolution of International Conflicts, University of South Carolina Press, Columbia, SC,1996.[10] R. Keeney, H. Raiffa, Decisions with Multiple Objective: Preferences and Value Tradeoffs, John Wiley & Sons, New York, 1976.[11] S. Kraus, P. Hoz-Weiss, J. Wilkenfeld, D.R. Andersen, A. Pate, Resolving crises through automated bilateral negotiations, Artificial Intelli-gence 172 (1) (2008) 1–18.[12] T. Leonard, J.S.J. Hsu, Bayesian Methods—An Analysis for Statisticians and Interdisciplinary Researchers, Cambridge University Press,Cambridge, 1999.[13] R.J. Lin, S.T. Chou, Bilateral multi-issue negotiations in a dynamic environment, in: Proc. of the AAMAS Workshop on Agent MediatedElectronic Commerce (AMEC V), Melbourne, Australia, 2003.[14] R. Lin, S. Kraus, J. Wilkenfeld, J. Barry, An automated agent for bilateral negotiation with bounded rational agents with incomplete informa-tion, in: Proc. of ECAI-06, 2006, pp. 270–274.[15] R.D. Luce, Individual Choice Behavior: A Theoretical Analysis, John Wiley & Sons, New York, 1959.[16] R.D. Luce, The choice axiom after twenty years, Journal of Mathematical Psychology 15 (1977) 215–233.[17] R.D. Luce, H. Raiffa, Games and Decisions—Introduction and Critical Survey, John Wiley & Sons, New York, 1957.[18] J.F. Nash, The bargaining problem, Econ. 18 (1950) 155–162.[19] M. Oprea, An adaptive negotiation model for agent-based electronic commerce, Studies in Informatics and Control 11 (3) (2002) 271–279.[20] M.J. Osborne, A. Rubinstein, A Course In Game Theory, MIT Press, Cambridge, MA, 1994.[21] E. Rasmusen, Games and Information: An Introduction to Game Theory, third ed., Blackwell Publishers, February 2001.[22] S. Siegel, Non-Parametric Statistics for the Behavioral Sciences, McGraw–Hill, New York, 1956.[23] Stitt Feld Handy Group Online Negotiation Course, http://www.negotiate.tv/, 2007.[24] M. Tennenholtz, On stable social laws and qualitative equilibrium for risk-averse agents, in: KR, 1996, pp. 553–561.[25] J. Wilkenfeld, K. Young, D. Quinn, V. Asal, Mediating International Crises, Routledge, London, 2005.[26] D. Zeng, K. Sycara, Bayesian learning in negotiation, in: S. Sen (Ed.), Working Notes for the AAAI Symposium on Adaptation, Co-evolutionand Learning in Multiagent Systems, Stanford University, CA, 1996, pp. 99–104.