bioRxiv preprint doi: https://doi.org/10.1101/2020.09.21.306597; this version posted April 3, 2021. The copyright holder for this preprint (whichwas not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 20211SingleChannelNet: A Model for Automatic SleepStage Classification with Raw Single-ChannelEEGDongdong Zhou, Guoqiang Hu, Jiacheng Zhang, Jian Wang, Rui Yan, Fan Li, Qi Xu, Lauri Kettunen,Zheng Chang, Senior Member, IEEE and Fengyu Cong, Senior Member, IEEEAbstract— In diagnosing sleep disorders, sleep stageclassification is a very essential yet time-consuming pro-cess. Most of the existing state-of-the-art approaches relyon hand-crafted features and multi-modality polysomnog-raphy (PSG) data, where prior knowledge is compulsoryand high computation cost can be expected. Besides, fewstudies are able to obtain high accuracy sleep stagingusing raw single-channel electroencephalogram (EEG). Toovercome these shortcomings, this paper proposes anend-to-end framework with a deep neural network, namelySingleChannelNet, for automatic sleep stage classificationbased on raw single-channel EEG. The proposed modelutilizes a 90s epoch as the textual input and employs twomulti-convolution blocks and several max-average poolinglayers to learn different scales of feature representations.To demonstrate the efficiency of the proposed model,we evaluate our model using different raw single-channelEEGs (C4/A1 and Fpz-Cz) on two different datasets (CC-SHS and Sleep-EDF datasets). Experimental results showthat the proposed architecture can achieve better over-all accuracy and Cohen’s kappa (CCSHS: 90.2%-86.5%,Sleep-EDF: 86.1%-80.5%) compared with state-of-the-artThis work was supported by National Natural Science Founda-tion of China (Grant No.91748105), National Foundation in China(No. JCKY2019110B009 & 2020-JCJQ-JJ-252) and the Fundamen-tal Research Funds for the Central Universities [DUT20LAB303 &DUT20LAB308]in Dalian University of Technology in China andthe Scholarships from China Scholarship Council (No.201806060164;No.201806060038; No.201606060227; No.202006060226).D. Zhou and R. Yan are with School of Biomedical Engineering,Faculty of Electronic and Electrical Engineering, Dalian University ofTechnology, 116024, Dalian, China & Faculty of Information Technol-ogy, University of Jyv ¨askyl ¨a, 40014, Jyv ¨askyl ¨a, Finland (e-mail: dong-dong.w.zhou@student.jyu.fi; ruiyanmodel@foxmail.com).G. Hu, J. Wang and F. Li are with School of Biomedi-cal Engineering, Faculty of Electronic and Electrical Engineering,Dalian University of Technology, 116024, Dalian, China (e-mail:guoqiang.hu@mail.dlut.edu.cn; wangjian009@mail.dlut.edu.cn;lifan-dlpu@foxmail.com).J. Zhang is with School of Information and Communication Engineer-ing, Faculty of Electronic and Electrical Engineering, Dalian University ofTechnology, 116024, Dalian, China (e-mail: 56best@mail.dlut.edu.cn).Q. Xu is with College of Computer Science and Technology, ZhejiangUniversity, 310027, Hangzhou, China (e-mail: xuqi123@zju.edu.cn).L. Kettunen and Z. Chang are with Faculty of Information Tech-nology, University of Jyv ¨askyl ¨a, 40014, Jyv ¨askyl ¨a, Finland (e-mail:lauri.y.o.kettunen@jyu.fi; zheng.chang@jyu.fi).F. Cong is with School of Biomedical Engineering & School of ArtificalIntelligence, Faculty of Electronic and Electrical Engineering & KeyLaboratory ofIntegrated Circuit and Biomedical Electronic System,Liaoning Province, Dalian University of Technology, 116024, Dalian,China & Faculty of Information Technology, University of Jyv ¨askyl ¨a,40014, Jyv ¨askyl ¨a, Finland (e-mail: cong@dlut.edu.cn).approaches. Additionally, the proposed model can learnfeatures automatically for sleep stage classification usingdifferent single-channel EEGs with distinct sampling ratesfrom different datasets without using any hand-engineeredfeatures.Index Terms— Sleep stage classification, Convolutionalneural network, Raw single-channel EEGI. INTRODUCTIONH UMANS spend about one-third time of life on sleeping,and high-quality sleep plays a vitally important rolein the restoration of body and mind [1]. Whereas roughly33% of the population in the world suffers from insomniadisorder [2]. Correctly identifying sleep stage using whole-night PSG data is essentialto diagnose and treat sleep-related disorders [3]–[6]. The PSG recordings comprise ofthe EEG, electrocardiogram (ECG), electrooculogram (EOG),electromyogram (EMG) and other respiration signals [7].According to the guidelines of the Rechtschaffen and Kales(R&K) [8] or American Academy of Sleep Medicine (AASM)[9], the PSG data should be first segmented into 30s epochstypically, then these sequential epochs are defined as differentstages. Some sleep-related disorders have particular sleepstructure, it is therefore beneficial to diagnose them with ac-curate sleep stage classification. Traditionally, the sleep stageclassification task is conducted by experts manually followingthe R&K and AASM rule which is often time-consuming,labor-intensive and prone to subjective mistakes [6]. Hence,there is an urgent need for automatic sleep stage classificationapproach to assist the clinician’s work and achieve reliableresults.Some methods based on machine learning have been pro-posed to identify the sleep stage. These approaches gener-ally extract either time-domain features [3], [10], [11] orfrequency-domain features [12]–[16] from the PSG signals andthese pre-extracted features are then fed into the conventionalclassifier, such as support vector machine (SVM) [4], [14],[17], [18], k-nearest neighbors (KNN) [16], [19], [20], randomforest [21]–[24] and so on. The performance tremendouslyrelies on the categories and the number of features, which areextracted based on the characteristics of experimental datasets.Therefore, these approaches may not be robust enough tobioRxiv preprint doi: https://doi.org/10.1101/2020.09.21.306597; this version posted April 3, 2021. The copyright holder for this preprint (which2was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2021be generalized to different datasets because of the distinctproperties between datasets.In recent years, the deep networks show great capacity forautomatic features learning from data, and it can avoid thereliance on hand-engineered features. Meanwhile, a series ofdeep learning methods are applied to sleep stage classification.Here, we categorize these approaches into multi-channel [6],[25]–[29] versus single-channel schemes [30]–[35] based onthe number of input channels. Following the multi-channelscheme, Phan et al. [6] first transformed the raw signals intothe time-frequency image through the short-time Fourier trans-form as the input of the proposed convolutional neural network(CNN). The overall accuracy achieved was equal to 82.3%,in which there is room for improvement. Besides, the time-frequency image relies much on many preprocessing steps,it would be time-consuming and in need of prior knowledgeof signal processing. Aiming at this, Chambon et al. [27]proposed a novel network architecture of low computationalcost adopting multivariate and multimodal time series fromEEG, EMG and EOG, but the classification performance isnot good enough with the accuracy of 80% compared to state-of-the-art methodologies. One important reason is that theconvolutional layers with fixed filter size were stacked sequen-tially, which can not learn multiscale features simultaneously.A promising approach was proposed by Zhang et al. [29], whoemployed the CNN and recurrent neural network (RNN) tocapture temporal and spatial information simultaneously fromthe PSG data. The architecture attained an accuracy of 87%.Although the combination of CNNs and RNNs can enhancethe model performance to some extent, the high computationalcost of RNNs should be taken into consideration. To thebest of our knowledge, the training speed of CNNs wouldbe dozens of times faster than that of RNNs under the sameGPU acceleration when implementing long time-series input.To sum up, despite the fact that multi-channel PSG data canprovide additional referenced information compared to single-channel EEG, there is also some irrelevant information beingintroduced. Furthermore, multi-channel recordings can limitthe practical application on account of more complex operationand equipment costs.Compared to the multi-channel scheme, the single-channelscheme can reduce the related cost and be much easier fordata acquisition. Under the single-channel scheme, Suprataket al. [30] introduced a deep learning model called Deep-SleepNet. DeepSleepNet utilizes the capacity of deep learningto extract time-invariant features automatically, the proposedmodel can be adapted to different datasets. However,theaccuracy obtained from DeepSleepNet was 82%, which cannot outperform the state-of-the-art approaches. A promisingCNN model was proposed by Sors et al. [31], who usedraw single-channel EEG to classify the sleep stage withoutany preprocessing. The architecture attained an accuracy of87%, whereas the model complexity is a bit high with 12convolutional layers. Furthermore, the filer size was chosenamong 7, 5, 3, the performance of larger size filters should becompared considering the long length of input (1.5 × 104).To tackle these problems,this paper proposes the Sin-gleChannelNet (SCNet), a model for automatic sleep stageclassification based on raw single-channel EEG, which canlearn different scales features simultaneously. We aim toautomate the sleep stage classification completely by utilizingthe capabilities of the proposed model. The main contributionsof this work are as follows:i) We propose a new deep learning model with low modelcomplexity for sleep stage classification using 90s rawsingle-channel EEG.ii) We implement two multi-convolution (MC) blocks withdifferent filter sizes in our model. In addition, the max-average (M-Apooling) layer is applied to take place of theconventional max-pooling layer. Two strategies are usedfor capturing more feature representations from differentscales to enhance the capacity of the feature extraction.iii) The results demonstrate that our model can obtainpromising performance on different raw single-channelEEGs (C4/A1, Fpz-Cz) from CCSHS and Sleep-EDFdatasets, without modifying the architecture and hyper-parameters of model and training algorithm. Moreover, allfeatures are learned by the proposed model automatically.The rest of this paper is organized as follows. We representthe experimental datasets in Sec. II. Sec. III describes thestructure of the SCNet model and the training algorithm. InSec. IV, the experimental results are represented. The finaldiscussion and conclusion are included in Sec. V.II. EXPERIMENTAL DATASETSTwo public datasets are employed to evaluate the per-formance of the proposed framework in this work, namelyCleveland Children’s Sleep and Health Study (CCSHS) [36],[37] and Sleep-EDF Database Expanded (Sleep-EDF, 2018version) [38]. It should be noted that all hypnograms ofexperimental datasets are manually scored according to theR&K manual rather than the AASM rule.A. Cleveland Children’s Sleep and Health Study(CCSHS)The CCSHS dataset comprises of overnight PSG recordingsfrom 515 subjects aged 8-11 years, which is one of the largestpopulation-based pediatric cohorts studied with objective sleepstudies. Each 30s epoch is manually divided by experts intoseveral stages: Wake (W), Rapid Eye Movement (REM),Non-REM1 (N1), Non-REM2 (N2), and Non-REM3 (N3). Inthis work, single-channel EEG C4/A1 sampled at 128 Hz isselected.B. Sleep-EDF Database Expanded (Sleep-EDF)The Sleep-EDF dataset consists of two subsets: sleep-cassette (SC) contains 78 healthy Caucasians aged from 25 to101 years and sleep-telemetry (ST) comprises 22 Caucasiansreceiving temazepam treatment. Each participant was recordedtwo subsequent night PSG data except the subject 13, subject36 and subject 52, from the SC subset who had only a one-night record. Each epoch of recordings is manually labelledby clinicians according to the R&K rule into W, N1, N2, N3,N4, REM, MOVEMENT and UNKNOWN stages respectively.bioRxiv preprint doi: https://doi.org/10.1101/2020.09.21.306597; this version posted April 3, 2021. The copyright holder for this preprint (whichAUTHOR et al.:was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 3Fig. 1.Xm and Xm+1, 2 ≤ m ≤ n − 1.Illustration of 90s epochs and labels used in this paper, n donates the number of 30s epochs for a subject, Zm is comprised of Xm−1,TABLE INUMBER OF 90S EPOCHS FOR EACH SLEEP STAGE FROMEXPERIMENTAL DATASETSDatasetWN1N2N3REMTotalCCSHS2110301922124968117242110188690372Sleep-EDF6951821522691321303925835199046Sleep-EDF-v1101972804177995703771744220In addition, MOVEMENT and UNKNOWN are excluded, asthey do not belong to the six stages. The PSG data includetwo-channel EEGs (Fpz-Cz and Pz-Oz), single-channel EOG,single-channel EMG and the event marker (sampled at 1 Hz).The sampling rate fs of EEG, EOG, and EMG is 100 Hz.Single-channel EEG Fpz-Cz is adopted in our experiment.For the Sleep-EDF dataset, stages N3 and N4 are mergedinto stage N3 which is consistent with the AASM manual.Additionally, resampling operation is not applied to C4/A1 andFpz-Cz EEGs. We also found that most previous studies usethe Sleep-EDF dataset of the first 20 subjects (Sleep-EDF-v1).For a fairer comparison, we also experiment with the Sleep-EDF-v1 dataset.C. Contextual inputIn previous works, most schemes use a single 30s epoch asthe classifier input [7], [31], [35] and then produce a singleoutput label. Although being straightforward, this classifica-tion method ignores the existing correlation and dependencybetween surrounding epochs. It is considered that the sleepstage classification depends not only on the local epoch,but also on the prior and following temporal features [6],[9]. For this reason, an extension of single 30s epoch inputis conducted by combining it with its neighboring epochsinput. Furthermore, we employ 90sto make a contextualepoch (Zm) as contextual input of the proposed model, and itcontains three sequential epochs: prior 30s epoch (Xm−1), 30sepoch (Xm) and subsequent 30s epoch (Xm+1). The groundtruth label of Zm is ym which also denotes Xm’s label. As inZm = (Xm−1, Xm, Xm+1) (cid:55)→ ym.(1)experiments. The distribution of the number of five stages isimbalanced. For all datasets, W and N2 stages account formore than 60% of all 90s epochs. By contrast, the proportionof stages N1 and N3 is the smallest.III. PROPOSED SCNETFig. 2 shows the overall architecture of the SCNet. Theconvolution block performs three operations sequentially: one-dimensional convolutional layer (Conv1D), batch normaliza-tion and M-Apooling1D. Similarly, each MC Block is followedby batch normalization, M-Apooling1D and Dropout layerin sequence. In our model, we employ the concatenation ofmax-pooling and average-pooling to take place of the max-pooling for capturing more representable features. Similar tothe inception module [39], the MC block contains differentsizes of convolutional filters to capture the correspondinginformation. Besides, we use the GAP layer to replace thetraditional fully connected layer, and it is proved to be morerobust spatialtranslations of the input without parameteroptimization [40].A. Model SpecificationIn Table II, we relate detailed parameters of the proposedmodel. The size of the model’s input is (90 × fs, 1), wherefs is the sampling rate. To be specific, the fs of EEG C4 andFpz-Cz is 128 Hz and 100 Hz, respectively. Here, the SCNetdoes not restrict the length of input which can be applied todifferent datasets.The first convolutional layer with 128 filters of size 128and a stride of 2 is applied to obtain the feature map fromraw single-channel EEG. The activation function of this layeris rectified linear unit (ReLU) which is defined as the positivepart of its argument:f (x) = max(0, x)(2)where x is the input of a neuron. To normalize the prior layeroutput, we apply the batch normalization technique. Besides,the M-Apooling layer can get the combination of maximumand average values from each of a cluster of neurons at theprevious layer.Details are illustrated in Fig. 1. As shown in Table I, wesummarize the number of 90s epochs for each sleep stagefrom CCSHS, Sleep-EDF and Sleep-EDF-v1 datasets in ourWe implement two MC blocks in our model, and the filtersizes are selected among 1, 3, 16 and 64 to obtain multiscalerepresentative features. More specifically, the small filter isXmXm-1Xm+1. . .X4. . .Xn-3ymym-1ym+1. . .y4. . .yn-3X3X1X2Xn-2Xn-1Xny3y1y2yn-2yn-1ynZmZm-1Zm+1. . .Z4. . .Zn-3Z3Z2Zn-2Zn-1ymym-1ym+1. . .y4. . .yn-3y3y2yn-2yn-190s epoch labels30s epoch labels30s epochs  90s epochsbioRxiv preprint doi: https://doi.org/10.1101/2020.09.21.306597; this version posted April 3, 2021. The copyright holder for this preprint (which4was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2021Fig. 2. An overall architecture of the proposed SCNet.prone to learn temporal information, while the large filter isbetter to capture frequency information. Considering the longlength of input (128×90, 100×90), we optimize the filter sizesfrom the small sizes (3, 5 and 7), medium sizes (16 and 32) andbig sizes (64, 128 and 256). The filter size of 1 is to improvethe nonlinearity of the network and reduce the dimensionof previous layer output. The filter sizes are chosen with 1,3, 16 and 64 based on the optimized results. Furthermore,after concatenating the output of all convolutional layers, thedimension of the MC block1 output is ((cid:100)45 × fs/2(cid:101), 272).The following M-Apooling layer can get ((cid:100)45 × fs/4(cid:101), 544)dimension feature map. Each MC block is followed by a batchnormalization layer, a M-Apooling layer with size of 3 and adropout layer with the probability of 0.1. To find appropriatestrides, we test 4 strides: 1, 2, 3 and 5. The stride of two MCblocks is set to 1, while the stride of the M-Apooling layer andthe first convolutional layer is 2. The GAP layer is applied toflat the previous output before the final decision layer. Througha drop layer with drop rate of 0.5, the dense layer usingsoftmax as the activation function makes the final decision.Softmax function can calculate the probabilities of five stages,the stage with maximum probability is as the consequence ofthe predicted sleep stage.B. RegularizationWe adopt two regularization approaches to help prevent theoverfitting problem. The first technique is L2 regularizationthat adds squared magnitude of coefficient as penalty term tothe loss function. It is important to choose a proper regular-ization rate (lambda), if lambda is very large, it would addTABLE IIPARAMETERS OF THE PROPOSED MODELLayerLayer TypeFilters Size Stride Activation Output dimensionMC Block1InputConv1D123 M-Apooling1D45 M-Apooling1DDropout(0.1)67MC Block28 M-Apooling1DDropout(0.1)9GAP10Dropout(0.5)11Dense12-128-----------1283-3--3-----2212-12-----Relu-Relu--Relu----Softmax(90 × fs, 1)(45 × fs, 128)((cid:100)45 × fs/2(cid:101), 256)((cid:100)45 × fs/2(cid:101), 272)((cid:100)45 × fs/4(cid:101), 544)((cid:100)45 × fs/4(cid:101), 544)((cid:100)45 × fs/4(cid:101), 272)((cid:100)45 × fs/8(cid:101), 544)((cid:100)45 × fs/8(cid:101), 544)5445445too much weight causing an underfitting issue. By contrast, avery small lambda would make the model more complex, thenthe model would learn too much about the particularities ofthe training data, L2 regularization therefore has little effect onavoiding overfitting. Hence, we test four lambda values: 10−1,10−2, 10−3 and 10−4, the results show that 10−3 achievesthe best performance. The L2 regularization is applied to allconvolutional layers, including the MC block.Another regularization method is dropout, which randomlydrops units from the model during training with a specificprobability from 0 to 1. It is noteworthy that the dropoutlayers is not used for testing. Dropout layers with probabilityof 0.1 and 0.5 are employed for the MC block and GAP layer,respectively.M-Apooling1DXm-1XmXm+1. . .. . .Conv1D(128,128@2)BatchNormalizationM-Apooling1D(3, 2)MC Block1BatchNormalizationM-Apooling1D(3, 2)Dropout(0.1)MC Block2BatchNormalizationM-Apooling1D(3, 2)Dropout(0.1)GlogalAvaragePooling1DDropout(0.5)Dense(5, softmax)Max-pooling1D(3, 2)Avarage-pooling1D(3, 2)InputOutputM-Apooling1DInputConv1D(32, 1@1)Conv1D(48, 1@1)Conv1D(64, 1@1)Conv1D(16, 1@1)M-Apooling1D(3, 1)Conv1D(32, 1@1)Conv1D(64, 3@1)Conv1D(96, 16@1)Conv1D(48, 64@1)OutputMC Block90s EEG epochSleep StageConvolutionBlockbioRxiv preprint doi: https://doi.org/10.1101/2020.09.21.306597; this version posted April 3, 2021. The copyright holder for this preprint (whichAUTHOR et al.:was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 5(a)(b)Fig. 3. The normalized confusion matrices of each fold cross-validation. (a) CCSHS dataset and (b) Sleep-EDF dataset.C. Training SetupWe select Adam as the network optimizer whose parameters((learning rate) lr, beta1 and beta2) are set to 10−3, 0.9 and0.999 respectively. Moreover, ReduceLROnPlateau of Call-back in Keras is implemented to reduce the lr. Specifically,when the model monitors the validation accuracy showing noimprovement within 3 epochs, the lr would drop to half of it.The minimum lr is set to 10−7. To find out appropriate batchsize of mini-batch, size of 32, 64, 128, and 256 are evaluated,we select 64 as the size of mini-batch finally. The categoricalcross entropy is chosen as the loss function of the model whichis always used for classifying multi-class tasks. The modelconverges to the optimal solution within 40 iterations, hencethe number of iteration is set to 40.There are two types of methods to split the training and testsets [30], [41]. One is the subject-wise scheme which splitsthe training and test datasets based on the subjects. Anotherone is the epoch-wise method in which the split is conductedby epochs rather than subjects. In the epoch-wise scheme, Weuse 20% of whole data set as the test set and the remaining 90sepochs as the training set. As for the subject-wise approach,80% subjects are selected as the training set, the other 20%subjects are used as the test set. Furthermore, we use the 5-foldcross-validation (80% training set for training, 20% training setfor validation) scheme to train and evaluate our model for bothdatasets. In addition, only 90s epochs from the CCSHS datasetare used to determine the hyper-parameters of the proposedmodel. Once achieving optimal hyper-parameters, they wouldbe used in all experiments. To be specific, when the model isapplied to another dataset, there would be no need to modifythe architecture and hyper-parameters of the model except forthe input length which should adapt to the fs of EEG fromdifferent datasets.Graphic card Nvidia Tesla P100 with 16 Gbytes memoryis used for model training. The implementation is written inKeras [42] with the Tensorflow backend [43].TABLE IIIMEAN CONFUSION MATRIX OF 5-FOLD CROSS-VALIDATION ON RAWSINGLE-CHANNEL EEG C4/A1 FROM THE CCSHS DATASETPredictedPer-class MetricsOverall MetricsW N1 N2N3 REM P R(%) RE(%) F 1(%) ACC(%) K(%)W 40450 440 934124N1 1039 1253 740139276594.752.8N2N3766 382 45679 1739 135988.86003126 18791891.0REM 384 299 971618358 87.995.533.091.585.591.795.140.690.188.189.890.286.5TABLE IVMEAN CONFUSION MATRIX OF 5-FOLD CROSS-VALIDATION ON RAWSINGLE-CHANNEL EEG FPZ-CZ FROM THE SLEEP-EDF DATASETPredictedPer-class MetricsOverall MetricsW N1 N2 N3 REM P R(%) RE(%) F 1(%) ACC(%) K(%)W 14650 498142882N1N2N3712 2020 121411330109705 12255 385 38345532 2127494.158.484.984.0REM 992332991457385.195.347.188.679.687.994.752.186.781.786.586.180.5IV. EXPERIMENTAL RESULTSA. Performance MetricsWe evaluate the model performance (epoch-wise) usingaccuracy (ACC), precision (P R), recall (RE), F1 score (F 1),and Cohen’s kappa coefficient (K). ACC is the proportion ofbioRxiv preprint doi: https://doi.org/10.1101/2020.09.21.306597; this version posted April 3, 2021. The copyright holder for this preprint (which6was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2021(a)(b)Fig. 4. The comparison between hypnogram labeled by the clinician and the model’s prediction. The solid black line is the ground truth, the dottedred line donates the hypnogram labeled by the prediction of the proposed model. (a) CCSHS dataset and (b) Sleep-EDF dataset.correct predictions made by the model to the total predications.P R calculates the ratio of correctly predicted positives to allpositives. RE means the fraction between true positives andall predications in the actual class. F 1 represents the weightedaverage of P R and RE. K measures the agreement betweentrue labels and predicted labels. A large value of K canindicate good performance of the model. They are calculatedas follows:ACC =.P R =.T P + T NT P + F N + T N + F PT PT P + F PT PT P + F NRE · P RRE + P Ri=1((cid:80)n(cid:80)n(cid:80)n..RE =F 1 = 2 ·K =(cid:80)ni=1 xiiN1 −−i=1((cid:80)n(cid:80)nj=1 xijN 2(cid:80)nj=1 xijN 2j=1 xji)j=1 xji)(3)(4)(5)(6)(7).where T P , T N , F N and F P donate the true positives, truenegatives, false negatives and false positives, respectively. Nis the number of 90s epochs of the test set, n represents thenumber of classes. In this work, n equals 5, xii (1 ≤ i ≤ 5)represents the diagonal value of the confusion matrix.To show the performance of each fold cross-validationfrom the CCSHS and Sleep-EDF datasets, we present thenormalized confusion matrices (CM) in Fig. 3. Firstly, weuse single-channel EEG C4/A1 (90s epochs) from the CCSHSdataset to tune the hyper-parameters. Once getting the bestperformance, the hyper-parameters and model architecture arefixed for all experiments. Table III provides the mean CM of5-fold cross-validation from the CCSHS dataset, we can seethat the overall accuracy and K are respectively 90.2% and86.5%. The proposed model shows the best ability to detect theW stage with the P R of 94.7%. By contrast, the performanceof stage N1 classification is the worst which is consistent withthe results of existing works. To be specific, there are 33.0% ofN1 90s epochs being recognized correctly. In addition, 27.4%of N1 samples are misclassified as W, 19.5% as N2 and 20.1%as REM. Stages N2, N3 and REM have similar classificationresults in terms of the P R corresponding to 88.8%, 91.0%and 87.9% respectively.To demonstrate the generalization capability of the proposedarchitecture, we also conduct the 5-fold cross-validation usingthe same model determined by the CCSHS dataset (i.e.,without any hyper-parameters modification except for the inputlength) on the Sleep-EDF dataset. As can be seen from Table I,the distribution of the numbers of five stages is a bit different.Stage W has the biggest proportion and the number of N3is the smallestin Sleep-EDF dataset, whereas the largestpercentage is stage N2 in the CCSHS dataset. Besides, theEEG channel used in two datasets is also distinct, C4/A1 forthe CCSHS dataset and Fpz-Cz for the Sleep-EDF dataset. It isworthy to note that despite the EEG channel and the size of theinput length (90×fs, 1) are quite different, the proposed modelcan obtain promising performance on two different datasets bycomparing Table III and Table IV.We further reveal the hypnogram comparison labeled byexperts and the model’s prediction for one subject of CCSHSand Sleep-EDF datasets in Fig. 4.B. Performance ComparisonWe make a comparison between the proposed model (epoch-wise and subject-wise) with some existing works using thesame datasets in terms of the ACC and K in Table V.bioRxiv preprint doi: https://doi.org/10.1101/2020.09.21.306597; this version posted April 3, 2021. The copyright holder for this preprint (whichAUTHOR et al.:was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 7TABLE VPERFORMANCE COMPARISON BETWEEN THE PROPOSED METHOD AND PREVIOUS METHODS ON THE CCSHS, SLEEP-EDF AND SLEEP-EDF-V1DATASETSPreprocessingMethodDatabaseInput channelStudyNakamura et al. [44]C4/A1 + C3/A2 YesHMMCCSHSLi et al. [45]YesC4/A1Random ForestCCSHSNoC4/A1Deep CNNProposed (subject-wise) CCSHSNoDeep CNNC4/A1Proposed (epoch-wise) CCSHSMousavi et al. [46]NoCNN + LSTM Fpz-CzSleep-EDFSupratak et al. [47]NoCNN + LSTM Fpz-CzSleep-EDFNoFpz-CzDeep CNNProposed (subject-wise) Sleep-EDFFpz-CzSleep-EDFProposed (epoch-wise)NoDeep CNNMikkelsen et al. [28]Fpz-Cz + EOG NoSleep-EDF-v1 Deep CNNSupratak et al. [30]NoSleep-EDF-v1 CNN + LSTM Fpz-CzWei et al. [33]YesFpz-CzSleep-EDF-v1 Deep CNNPhan et al. [6]Fpz-Cz + EOG YesSleep-EDF-v1 Deep CNNPhan et al. [34]YesSleep-EDF-v1 1-max CNNFpz-CzPhan et al. [35]YesSleep-EDF-v1 Attentional RNN Fpz-CzNoFpz-CzProposed (subject-wise) Sleep-EDF-v1 Deep CNNNoFpz-CzSleep-EDF-v1 Deep CNNProposed (epoch-wise)Input typeSubjects ACC(%) K(%)SpectrogramFeaturesTime seriesTime seriesTime seriesTime seriesTime seriesTime seriesTime seriesTime seriesTime seriesTime-frequency imageTime-frequency imageFeaturesTime seriesTime series515116515515787878782020202020202020-86.088.290.280.083.183.986.184.082.084.382.379.879.186.291.07380.583.886.5737777.880.5-767875727081.187.8Table V reveals that the proposed framework can achievehigher ACC and K using raw single-channel C4/A1 EEGcompared to approaches using multi-channel PSG data [44] orthe single-channel EEG [45] on the CCSHS dataset. For theSleep-EDF and Sleep-EDF-v1 databases, the proposed modelalso achieves comparable performance compared to state-of-the-art methods. Some studies [34], [35] extract featuresmanually or multi-channel signals are used as input [6], [28]or some methods adopt single-channel EEG [33], [34], [46],the proposed[47]. Considering results of the comparison,framework can achieve promising performance on CCSHS,Sleep-EDF and Sleep-EDF-v1 datasets.V. DISCUSSION AND CONCLUSIONIn this paper, we propose an end-to-end framework withCNNs, namely SCNet, which combines the feature learningisability and classification capacity. The proposed modelapplied to classify sleep stages automatically from raw single-channel EEG without using any hand-engineered features andany other preprocessing (e.g., signal filtering and resampleimplementation). There are two main advantages that we trainand evaluate the model with raw single-channel EEG. Com-paring with those methods with hand-crafted features [4], [12],[48], where extracting hand-engineered features is conductedwith priori knowledge and not in a data-driven way, and it istime-consuming for the researchers. Moreover, the selection oftypes and number of features would result in different modelperformance, there is no gold standard about the extraction ofhand-crafted features. The second advantage is that it is mucheasier and more comfortable to record single-channel EEGdata compared to the multi-channel scheme [6], [28] eitherat the hospital or home. Moreover, multi-channel PSG dataused as input can increase the computational cost. Consideringpractical applications, the use of raw single-channel EEG cansimplify the measurement scheme and reduce the related cost.Comparing with the conventional deep neural network basedon CNNs, where the convolutional layers with the fixed filtersize are assembled in sequence. In such a case, it is not capableof capturing features representation from different scales. Toaddress this issue, our model employs two MC blocks, whichlayers withare the concatenation of several convolutionalfour distinct filer sizes,to extract different scale features.Instead of using the traditional max-pooling layer, we adoptthe M-Apooing layer to add average feature representationwith maximum features simultaneously, which further improvethe proposed model’s ability of feature learning. In addition,the SCNet model is quite simple and compact with a total5 × 105 parameters compared to the methods in [46] whichhas 2.1 × 107 parameters and [30] in which the numberof parameters of the representation learning and sequencelearning parts has up to 6 × 105 and 2 × 107residualrespectively. Moreover, the proposed SCNet model can achievethe comparable performance with less computing resourcesoccupied. Concerning online and realtime applications (e.g.,sleep monitoring), our model with raw single-channel EEG ismore reasonable to reduce the time latency and obtain reliableresults.To demonstrate the generalization of the proposed archi-tecture, different single-channel EEGs from two datasets areadopted. The length of input is not restricted to a fixed number,our model can be adapted to different length of input relatingto the fs of EEG efficiently. Experimental results show thatthe proposed model can obtain promising performance on twodatasets (CCSHS: ACC-90.2%, K-86.5%; Sleep-EDF: ACC-86.1%, K-80.5%), which indicate the desirable generalizationof the SCNet model.It is challenging to train on dataset A and test on B, notonly for the proposed SCNet but also for typical CNNs.CNNs are running in a data-driven way which means thelearn some crucial features from the trainingmodel mustsamples. Otherwise, it cannot perform well on an unfamiliarbioRxiv preprint doi: https://doi.org/10.1101/2020.09.21.306597; this version posted April 3, 2021. The copyright holder for this preprint (which8was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2021dataset. This is also the biggest difference (generalizationability) between machine and human, human beings are goodat deducing and inducing. To further show the generalizationability of the proposed model, we perform two additionalexperiments. Firstly, we train our model with the CCSHSdatabase, the obtained model then is tested on the Sleep-EDFdataset without any training, the accuracy is 65.9%. In reverse,The proposed model is trained on the Sleep-EDF dataset andtested with the CCSHS database, the accuracy achieved is70.2%. In our future work, we will try to construct a morebrain-inspired model with some cognitive neural dynamic fromneuroscience [49] to increase the generalization ability for thesleep stage classification task. Also, it is valuable to adoptclinic datasets that have rarely been explored in previousstudies.ACKNOWLEDGMENTThis study is to memorize Prof. Tapani Ristaniemi fromUniversity of Jyv¨askyl¨a for his great help to the authors andProf. Tapani Ristaniemi has supervised this study very much.REFERENCES[1] R. Ferri et al., “A quantitative statistical analysis of the submentalismuscle emg amplitude during sleep in normal controls and patients withrem sleep behavior disorder,” J. Sleep Res., vol. 17, no. 1, pp. 89–100,2008.[2] C. Kuo and S. Liang, “Automatic stage scoring of single-channel sleepeeg based on multiscale permutation entropy,” in Proc. IEEE Biomed.Circuits Syst. Conf (BioCAS)., pp. 448–451, 2011.[3] S. J. Redmond and C. Heneghan, “Cardiorespiratory-based sleep stagingin subjects with obstructive sleep apnea,” IEEE Trans. Biomed. Eng.,vol. 53, no. 3, pp. 485–496, 2006.[4] G. Zhu, Y. Li, and P. Wen, “Analysis and classification of sleep stagesbased on difference visibility graphs from a single-channel eeg signal,”IEEE J. Biomed. Health. Inf., vol. 18, no. 6, pp. 1813–1821, 2014.[5] H. G. Jo et al., “Genetic fuzzy classifier for sleep stage identification,”Comput. Biol. Med., vol. 40, no. 7, pp. 629–634, 2010.[6] H. Phan et al., “Joint classification and prediction cnn framework forautomatic sleep stage classification,” IEEE Trans. Biomed. Eng., vol. 66,no. 5, pp. 1285–1296, 2018.[7] H. Dong et al., “Mixed neural network approach for temporal sleepstage classification,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 26,no. 2, pp. 324–333, 2017.[8] A. Rechtschaffen, “A manual of standardized terminology and scoringsystem for sleep stages of human subjects,” Electroencephalogr. Clin.Neurophysiol., vol. 26, no. 6, p. 644, 1969.[9] C. Iber et al., The AASM manual for the scoring of sleep and asso-ciated events: rules, terminology and technical specifications, vol. 1.Westchester, IL, USA: Amer. Acad. Sleep Med., 2007.[10] B. Koley and D. Dey, “An ensemble system for automatic sleep stageclassification using single channel eeg signal,” Comput. Biol. Med.,vol. 42, no. 12, pp. 1186–1195, 2012.[11] K. ˇSuˇsm´akov´a and A. Krakovsk´a, “Discrimination ability of individualmeasures used in sleep stages classification,” Artif. Intell. Med., vol. 44,no. 3, pp. 261–277, 2008.[12] F. Karimzadeh et al., “A distributed classification procedure for auto-matic sleep stage scoring based on instantaneous electroencephalogramphase and envelope features,” IEEE Trans. Neural Syst. Rehabil. Eng.,vol. 26, no. 2, pp. 362–370, 2017.[13] O. Tsinalis, P. M. Matthews, and Y. Guo, “Automatic sleep stage scoringusing time-frequency analysis and stacked sparse autoencoders,” Ann.Biomed. Eng., vol. 44, no. 5, pp. 1587–1597, 2016.[14] M. Sharma et al., “An accurate sleep stages classification system usinga new class of optimally time-frequency localized three-band waveletfilter bank,” Comput. Biol. Med., vol. 98, pp. 58–75, 2018.[15] S. Kouchaki et al., “Tensor based singular spectrum analysis for auto-matic scoring of sleep eeg,” IEEE Trans. Neural Syst. Rehabil. Eng.,vol. 23, no. 1, pp. 1–9, 2014.[16] H. Phan et al., “Metric learning for automatic sleep stage classification,”in Proc. IEEE Eng. Med. Biol. Soc (EMBC)., pp. 5025–5028, 2013.[17] E. Alickovic and A. Subasi, “Ensemble svm method for automaticsleep stage classification,” IEEE Trans. Instrum. Meas., vol. 67, no. 6,pp. 1258–1265, 2018.[18] T. Lajnef et al., “Learning machines and sleeping brains: automaticsleep stage classification using decision-tree multi-class support vectormachines,” J. Neurosci. Methods., vol. 250, pp. 94–105, 2015.[19] R. Boostani, F. Karimzadeh, and M. Nami, “A comparative review onsleep stage classification methods in patients and healthy individuals,”Comput. Methods Programs Biomed., vol. 140, pp. 77–91, 2017.[20] S. G¨unes¸, K. Polat, and S¸ . Yosunkaya, “Efficient sleep stage recognitionsystem based on eeg signal using k-means clustering based featureweighting,” Expert Syst. Appl., vol. 37, no. 12, pp. 7922–7928, 2010.[21] L. Fraiwan et al., “Automated sleep stage identification system basedon time–frequency analysis of a single eeg channel and random forestclassifier,” Comput. Methods Programs Biomed., vol. 108, no. 1, pp. 10–19, 2012.[22] M. Xiao et al., “Sleep stages classification based on heart rate variabilityand random forest,” Biomed. Signal Process. Control., vol. 8, no. 6,pp. 624–633, 2013.[23] T. L. da Silveira, A. J. Kozakevicius, and C. R. Rodrigues, “Single-channel eeg sleep stage classification based on a streamlined set ofstatistical features in wavelet domain,” Med. Biol. Eng. Comput., vol. 55,no. 2, pp. 343–352, 2017.[24] P. Memar and F. Faradji, “A novel multi-class eeg-based sleep stageclassification system,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 26,no. 1, pp. 84–95, 2017.[25] H. Phan et al., “SeqSleepNet: end-to-end hierarchical recurrent neuralnetwork for sequence-to-sequence automatic sleep staging,” IEEE Trans.Neural Syst. Rehabil. Eng., vol. 27, no. 3, pp. 400–410, 2019.[26] X. Zhang et al., “Sleep stage classification based on multi-level featurelearning and recurrent neural networks via wearable device,” Comput.Biol. Med., vol. 103, pp. 71–81, 2018.[27] S. Chambon et al., “A deep learning architecture for temporal sleepstage classification using multivariate and multimodal time series,” IEEETrans. Neural Syst. Rehabil. Eng., vol. 26, no. 4, pp. 758–769, 2018.[28] K. Mikkelsen and M. De Vos, “Personalizing deep learning models forautomatic sleep staging,” arXiv Prepr. arXiv:1801.02645., 2018.[29] L. Zhang et al., “Automated sleep stage scoring of the sleep heart healthstudy using deep neural networks,” Sleep., vol. 42, no. 11, p. zsz159,2019.[30] A. Supratak et al., “DeepSleepNet: A model for automatic sleep stagescoring based on raw single-channel eeg,” IEEE Trans. Neural Syst.Rehabil. Eng., vol. 25, no. 11, pp. 1998–2008, 2017.[31] A. Sors et al., “A convolutional neural network for sleep stage scoringfrom raw single-channel eeg,” Biomed. Signal Process. Control., vol. 42,pp. 107–114, 2018.[32] H. Seo et al., “Intra-and inter-epoch temporal context network (iitnet)using sub-epoch features for automatic sleep scoring on raw single-channel eeg,” Biomed. Signal Process. Control., vol. 61, p. 102037,2020.[33] Q. Wei et al., “A residual based attention model for eeg based sleepstaging,” IEEE J. Biomed. Health Inform., 2020.[34] H. Phan et al., “Dnn filter bank improves 1-max pooling cnn for single-channel eeg automatic sleep stage classification,” in Proc. IEEE Eng.Med. Biol. Soc (EMBC)., pp. 453–456, 2018.[35] H. Phan et al., “Automatic sleep stage classification using single-channeleeg: Learning sequential features with attention-based recurrent neuralnetworks,” in Proc. IEEE Eng. Med. Biol. Soc (EMBC)., pp. 1452–1455,2018.[36] G. Zhang et al., “The national sleep research resource: towards a sleepdata commons,” J. Am. Med. Inform. Assoc., vol. 25, no. 10, pp. 1351–1358, 2018.[37] C. L. Rosen et al., “Prevalence and risk factors for sleep-disorderedbreathing in 8-to 11-year-old children: association with race and prema-turity,” J. Pediatr., vol. 142, no. 4, pp. 383–389, 2003.[38] B. Kemp et al., “Analysis of a sleep-dependent neuronal feedback loop:the slow-wave microcontinuity of the eeg,” IEEE Trans. Biomed. Eng.,vol. 47, no. 9, pp. 1185–1194, 2000.[39] C. Szegedy et al., “Going deeper with convolutions,” in Proc. IEEEComput. Soc. Conf. Comput. Vis. Pattern Recognit (CVPR)., pp. 1–9,IEEE, 2015.[40] M. Lin, Q. Chen, and S. Yan, “Network in network,” arXiv Prepr.arXiv:1312.4400., 2013.bioRxiv preprint doi: https://doi.org/10.1101/2020.09.21.306597; this version posted April 3, 2021. The copyright holder for this preprint (whichAUTHOR et al.:was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. 9[41] T. Zhu, W. Luo, and F. Yu, “Convolution-and attention-based neuralnetwork for automated sleep stage classification,” Int. J. Environ. Res.Public Health., vol. 17, no. 11, p. 4152, 2020.[42] F. Chollet et al., “Keras: Deep learning library for theano and tensor-flow,” URL: https://keras. io/k., vol. 7, no. 8, p. T1, 2015.[43] M. Abadi et al., “Tensorflow: Large-scale machine learning on hetero-geneous distributed systems,” arXiv Prepr. arXiv:1603.04467., 2016.[44] T. Nakamura, H. J. Davies, and D. P. Mandic, “Scalable automatic sleepstaging in the era of big data,” in Proc. IEEE Eng. Med. Biol. Soc(EMBC)., pp. 2265–2268, IEEE.[45] X. Li et al., “Hyclasss: a hybrid classifier for automatic sleep stagescoring,” IEEE J. Biomed. Health Inform., vol. 22, no. 2, pp. 375–385,2017.[46] S. Mousavi, F. Afghah, and U. R. Acharya, “Sleepeegnet: Automatedsleep stage scoring with sequence to sequence deep learning approach,”PLOS ONE., vol. 14, no. 5, pp. 1–15, 2019.[47] A. Supratak and Y. Guo, “TinySleepNet: An efficient deep learningmodel for sleep stage scoring based on raw single-channel eeg,” in Proc.IEEE Eng. Med. Biol. Soc (EMBC)., pp. 641–644, 2020.[48] Y. L. Hsu et al., “Automatic sleep stage recurrent neural classifier usingenergy features of eeg signals,” Neurocomputing., vol. 104, pp. 105–114,2013.[49] Q. Xu et al., “Deep CovDenseSNN: A hierarchical event-driven dynamicframework with spiking neurons in noisy environment,” Neural Netw.,vol. 121, pp. 512–519, 2020.