Machine learning fairness notions: Bridging the gapwith real-world applicationsKarima Makhlouf, Sami Zhioua, Catuscia PalamidessiTo cite this version:Karima Makhlouf, Sami Zhioua, Catuscia Palamidessi. Machine learning fairness notions: Bridg-Information Processing and Management, 2021, 58 (5),ing the gap with real-world applications.￿10.1016/j.ipm.2021.102642￿. ￿hal-03624025￿HAL Id: hal-03624025https://hal.science/hal-03624025Submitted on 13 Jun 2023HAL is a multi-disciplinary open accessarchive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come fromteaching and research institutions in France orabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL, estdestinée au dépôt et à la diffusion de documentsscientifiques de niveau recherche, publiés ou non,émanant des établissements d’enseignement et derecherche français ou étrangers, des laboratoirespublics ou privés.Distributed under a Creative Commons Attribution - NonCommercial| 4.0 InternationalLicenseVersion of Record: https://www.sciencedirect.com/science/article/pii/S0306457321001321Manuscript_fc01a0c56074f6184ab81f45e475e037Machine Learning Fairness Notions:Bridging the Gap with Real-world ApplicationsKarima Makhloufa, Sami Zhiouab, Catuscia Palamidessic,∗aUniversité du Québec à Montréal, Québec, CanadabHigher Colleges of Technology, Dubai, UAEcInria, École Polytechnique, IPP, Paris, FranceAbstractFairness emerged as an important requirement to guarantee that Machine Learning (ML) predictivesystems do not discriminate against specific individuals or entire sub-populations, in particular,minorities. Given the inherent subjectivity of viewing the concept of fairness, several notions offairness have been introduced in the literature. This paper is a survey that illustrates the subtletiesbetween fairness notions through a large number of examples and scenarios. In addition, unlikeother surveys in the literature, it addresses the question of “which notion of fairness is most suitedto a given real-world scenario and why?”. Our attempt to answer this question consists in (1)identifying the set of fairness-related characteristics of the real-world scenario at hand, (2) analyzingthe behavior of each fairness notion, and then (3) fitting these two elements to recommend the mostsuitable fairness notion in every specific setup. The results are summarized in a decision diagramthat can be used by practitioners and policy makers to navigate the relatively large catalogue of MLfairness notions.Keywords: Fairness, Machine learning, Discrimination, Survey , Systemization of Knowledge (SoK)1. IntroductionDecisions in several domains are increasingly taken by “machines”. These machines try to takethe best decisions based on relevant historical data and using Machine Learning (ML) algorithms.∗Corresponding author.Email addresses: karima.makhlouf@courrier.uqam.ca (Karima Makhlouf), szhioua@hct.ac.ae (Sami Zhioua),catuscia@lix.polytechnique.fr (Catuscia Palamidessi)Preprint submitted to Journal of Information Processing and ManagementApril 9, 2021© 2021 published by Elsevier. This manuscript is made available under the CC BY NC user licensehttps://creativecommons.org/licenses/by-nc/4.0/Overall, ML-based decision-making (MLDM)1 is beneficial as it allows to take into considerationorders of magnitude more factors than humans do and hence outputting decisions that are more5informed and less subjective. However, in their quest to maximize efficiency, ML algorithms cansystemize discrimination against a specific group of population, typically, minorities. As an example,consider the automated candidates selection system of St. George Hospital Medical School [1, 2].The aim of the system was to help screening for the most promising candidates for medical studies.The automated system was built using records of manual screenings from previous years. Duringthose manual screening years, applications with grammatical mistakes and misspellings were rejectedby human evaluators as they indicate a poor level of English. As non-native English speakers aremore likely to send applications with grammatical and misspelling mistakes than native Englishspeakers do, the automated screening system built on that historical data ended up correlating race,birthplace, and address with a lower likelihood of acceptance. Later, while the overall English levelof non-native speakers improved, the race and ethnicity bias persisted in the system to the extentthat an excellent candidate may be rejected simply for her birthplace or address.Given that MLDM can have a significant impact in the lives and safety of human beings, it isno surprise that social and political organization are becoming very concerned with the possibleconsequences of biased MLDM, and the related issue of lack of explanation and interpretability ofML-based decisions. The European Union has been quite active in this respect. Already in theGeneral Data Protection Regulation (GDPR) there were directives concerning Automated DecisionMaking: for instance, Article 22 states that “The data subject shall have the right not to be subjectto a decision based solely on automated processing.” Other initiatives include the European Union’sEthics Guidelines for Trustworthy AI (April 2019), and OECD’s Council Recommendation onArtificial Intelligence (May 2019).In the scientific community, the issue of fairness in machine learning has become one of the mostpopular topics in recent years. The number of publications and conferences in this field has literallyexploded, and a huge number of different notions of fairness have been proposed, leading sometimesto possible confusion. This paper, like other surveys in the literature (cf. Section 1), attempts toclassify and systematize these notions. The characteristic of our work, however, consists in our point10152025301We focus on automated decision-making system supported by ML algorithms. In the rest of the paper we refer tosuch systems as MLDM.2of view, which is that the very reason for having different fairness notions is how suitable each oneof them is for specific real-world scenarios. We feel that none of the existing surveys has addressedthis aspect specifically. Discussion about the suitability (and sometimes the applicability) of thefairness notions is very limited and scattered through several papers [3, 4, 5, 6, 7, 8]. In this survey35paper we show that each MLDM system can be different based on a set of criteria such as: whetherthe ground-truth exists, difference in base-rates between sub-groups, the cost of misclassification,the existence of a government regulation that needs to be enforced, etc. We then revisit exhaustivelythe list of fairness notions and discuss the suitability and applicability of each one of them based onthe list of criteria.40Another set of results from the literature which is particularly related to the applicability problemwe are addressing in this paper is the tensions that exist between some definitions of fairness. Severalpapers in the literature provide formal proofs of the impossibility to satisfy several fairness definitionssimultaneously [3, 6, 8, 9, 10]. These results are revisited and summarized as they are related to theapplicability of fairness notions.45The results of this survey are finally summarized in a decision diagram that hopefully can helpresearchers, practitioners, and policy makers to identify the subtleties of the MLDM system at handand to choose the most appropriate fairness notion to use, or at least rule out notions that can leadto wrong fairness/discrimination result.50The paper is organized as follows. Section 3 lists notable real-world MLDMs where fairness iscritical. Section 4 identifies a set of fairness-related characteristics of MLDMs that will be used in thesubsequent sections to recommend and/or discourage the use of fairness notions. Fairness notions arelisted and described in the longest section of the survey, Section 5. Section 6 discusses relaxations ofthe strict definitions of fairness notions. Section 7 describes classification and tensions that existbetween some fairness notions. The decision diagram is provided and discussed in Section 8.552. Related Work and ScopeWith the increasing fairness concerns in the field of automated decision making and machinelearning, several survey papers have been published in the literature in the few previous years. Thissection revisits these survey papers and highlights how this proposed survey deviates from them.60In 2015, Zliobaite compiled a survey about fairness notions that have been introduced previ-ously [11]. He classified fairness notions into four categories, namely, statistical tests, absolute3measures, conditional measures, and structural measures. Statistical tests indicate only the presenceor absence of discrimination. Absolute and conditional measures quantify the extent of discriminationwith the difference that conditional measures consider legitimate explanations for the discrimination.6570These three categories correspond to the group fairness notions in this survey. Structural measurescorrespond to individual fairness notions2. Most of the fairness notions listed by Zliobaite arevariants of the group fairness notions in this survey. For instance, difference of means test (Section4.1.2 in [11]) is a variant of balance for positive class (Section 5.7 in this paper). Although, hededicated one category for individual notions (structural measures), Zliobaite did not mentionimportant notions, in particular fairness through awareness. Regarding the applicability of notions,the only criterion considered was the type of variables (e.g. binary, categorical, numerical, etc.).The survey of Berk et al. [12] listed only group fairness notions that are defined using theconfusion matrix. Similar to this survey, they used simple examples based on the confusion matrix tohighlight relationships between the fairness notions. The applicability aspect has not been addressedas the paper focused only on criminal risk assessment use case.75The survey of Verma and Rubin [13] described a list of fairness notions similar to the list in thissurvey. To illustrate how each notion can be computed in real scenarios, they used a loan grantingreal use case (German credit dataset [14]). Rather than using a benchmark dataset, this survey usesa smaller and fictitious use case (job hiring) which allows to illustrate better the subtle differencesbetween the fairness notions. For instance, counterfactual fairness is more intuitively described usinga small job hiring example than the loan granting benchmark dataset. Verma and Rubin did notaddress the applicability aspect in their survey.Gajane and Pechenizkiy [4] focused on formalizing only notable fairness notions (e.g. statisticalparity, equality of opportunity, individual fairness, etc.) and discussed their implications ondistributive justice from the social sciences literature. In addition, they described two additionalfairness notions that are studied extensively in the social sciences literature, namely, equality ofresources and equality of capability. These notions, however, do not come with a mathematicalformalization. This survey is more exhaustive as it analyzes a much larger number of fairness notions.However, being focused on the implication on distributive justice, Gajane and Pechenizkiy’s survey8085addresses the suitability of the discussed fairness notions in real world domains.902Zliobaite does not use group vs individual notions, but indirect and direct discrimination.4Mehrabi et al. [15] considered a more general scope for their survey: in addition to briefly listing10 definitions of fairness notions (Section 4.2), they surveyed different sources of bias and differenttypes of discrimination, they listed methods to implement fairness categorized into pre-processing,in-processing, and post-processing, and they discussed potential directions for contributions in thefield. This survey is more focused on fairness notions which are described in more depth.95A more recent survey by Mitchell et al. [3] presents an exhaustive list of fairness notions inboth categories (group and individual) and summarizes most of the incompatibility results in theliterature. Although Mitchell et al. discuss a “catalogue” of choices and assumptions in the contextof fairness, the aim of these choices and assumptions is different from the criteria defined in thissurvey (Section 4). The assumptions and choices discussed in Section 2 in [3] address the question of100how social goals are abstracted and formulated into a prediction (ML) problem. In particular, howthe choice of the prediction goal, the choice of the population, and the choice of the decision spacecan have an impact on the degree of fairness of the prediction. Whereas the choices and criteriadiscussed in this survey (Section 4) are used to help identify the most suitable fairness notion toapply in a given scenario.105Other surveys include the one by Friedler et al. [16] which considered only group fairness notionsand focused on surveying algorithms to implement fairness.Overall most of existing review papers do not address all flavors of fairness notions in the samesurvey. In particular, most of them focus on statistical and group fairness notions. Causality basedfairness notions, however, are not covered in several surveys while it is the most reliable category of110notions in the disparate treatment legal framework. However, the main contribution of this surveyis the focus on the applicability of fairness notions and the identification of fairness-related criteriato help select the most suitable notion to use given a scenario at hand. Brief discussions aboutthe suitability of specific fairness notions can be found in few papers. For instance, Zafar et al. [5]mentioned some application scenarios for statistical parity and equalized odds. Kleinberg et al.[6]115discussed the applicability of calibration and balance notions. Through a discussion about the costof unfair decision on society, Corbett-Davies et al.[7] analyzed the impact of using statistical parity,predictive equality, and conditional statistical parity on public safety (criminal risk assessment).Gajane and Pechenizkiy [4] discuss the suitability of notable fairness notions (statistical parity,individual fairness, etc.) from the distributive justice point of view. Unlike the scattered discussions120about the applicability of fairness notions found in the literature, this survey provides a complete5reference to systemize the selection procedure of fairness notions. A short version of this paper waspresented in BIAS 2020 workshop at ECMLPKDD 2020 [17].Fairness in machine learning can be categorized according to two dimensions, namely, the task andthe type of learning. For the first dimension, there are two tasks in fairness-aware machine learning:125discrimination discovery (or assessment) and discrimination removal (or prevention). Discriminationdiscovery task focuses on assessing and measuring bias in datasets or in predictions made by theMLDM. Discrimination removal focuses on preventing discrimination by manipulating datasets(pre-processing), adjusting the MLDM (in-processing) or modifying predictions (post-processing).For the second dimension, fairness can be investigated for different learning types including fairness130in classification, fairness in regression [18, 19], fairness in ranking [20], fairness in reinforcementlearning [21], etc. This survey focuses on the task of discrimination discovery (assessing fairness) in“pure prediction” [22] classification problems with a single decision making task (not sequential) andwhere decisions do not impact outcomes [23].1353. Real-world scenarios with critical fairness requirementsAs the paper is focusing on the applicability of fairness notions, we provide here a list of notablereal-world MLDMs where fairness is critical. In each of these scenarios, failure to address the fairnessrequirement will lead to unacceptable biased decisions against individuals and/or sub-populations.These scenarios will be used to provide concrete examples of situations where certain fairness notions140are more suitable than others.Job hiring : MLDMs in hiring are increasingly used by employers to automatically screencandidates for job openings3. Commercial candidate screening MLDMs include XING4, Evolv [26],Entelo, Xor, EngageTalent, GoHire and SyRI 5. Typically, the input data used by the MLDMinclude: affiliation, education level, job experience, IQ score, age, gender, marital status, address, etc.1453In 2014, the automated job screening systems market was estimated at $500 million annual business and wasgrowing at a rate of 10 to 15% per year [24]4A job platform similar to LinkedIn. It was found that this platform ranked less qualified male candidates higherthan more qualified female candidates [25].5System Riscico Indicatie, or SyRI for short, is a risk profiling system being deployed in the Netherlands by theDepartment of Social Affairs and Employment with the intention of identifying individuals who are at a high risk ofcommitting fraud in relation to employment and other matters like social security and taxes. Its use raised a lot ofcontroversy, and its case was brought to the Court of the Hague, that concluded on the 5th of February 2020 that theGovernment’s use of SyRI violates the European Convention on Human Rights. To a very large extent, the Court’sjudgment was based on the lack of transparency in the algorithm at the heart of the system.6The MLDM outputs a decision and/or a score indicating how suitable/promising the applicationis for the job opening. A biased MLDM leads to rejecting a candidate because of a trait that shecannot control (gender, race, sexual orientation, etc.). Such unfairness causes a prejudice on thecandidate but also can be damaging for the employer as excellent candidates might be missed.150Granting loans: Since decades, statistical and MLDM systems are used to assess loan appli-cations and determine which of them are approved and with which repayment plan and annualpercentage rate (APR). The assessment proceeds by predicting the risk that the applicant willdefault on her repayment plan. Loan Granting MLDMs currently in use include: FICO, Equifax,Lenddo, Experian, TransUnion, etc. The common input data used for loan granting include: credithistory, purpose of the loan, loan amount requested, employment status, income, marital status,155gender, age, address, housing status and credit score. An unfair loan granting MLDM will eitherdeny a deserving applicant a requested loan, or give her an exorbitant APR, which on the long runwill create a vicious cycle as the candidate will be very likely to default on her payments.College admission: Given the large number of admission applications, several colleges arenow resorting to MLDMs to reduce processing time and cut costs6. Existing college admission160MLDMs include GRADE [27], IBM Watson7, Kira Talent8 . Typically, the candidates’ features usedinclude: the institutions previously attended, SAT scores, extra-curricular activities, GPAs, testscores, interview score, etc. The predicted outcome can be a simple decision (admit/reject) or a scoreindicating the candidate’s potential performance in the requested field of study [10]. Unfair collegeadmission MLDMs may discriminate against a certain ethnic group (e.g. African-American [28])165which could lead, in the long term, to economic inequalities and corrupting the role of highereducation in society as a whole. For instance, in 2020 Ofqual, the UK Office of Qualificationsand Examinations Regulation, used a MLDM to assess students for university admission decisions.Nearly 40% of students ended up receiving exam scores downgraded from their teachers’ predictions,threatening to cost them their university spots. Analysis of the algorithm revealed that it had170disproportionately hurt students from working-class and disadvantaged communities and inflatedthe scores of students from private schools [29].6While the final acceptance decision is taken by humans, MLDMs are typically used as a first filter to “clean-up”the list from clear rejection cases.7A platform that uses natural language processing and personality traits in order to help students find the suitableand right college for them.8A Canadian startup that sells a cloud-based admissions assessment platform to over 300 schools.7Criminal risk assessment: There is an increasing adoption of MLDMs that predict riskscores based on historical data with the objective to guide human judges in their decisions. Themost common use case is to predict whether a defendant will re-offend (or recidivate). Examples of175risk assessment MLDMs include COMPAS [30], PSA [31], SAVRY [32], predPol [33]. Predictingrisk and recidivism requires input information such as: number of arrests, type of crime, address,employment status, marital status, income, age, housing status, etc. Unfair risk assessment MLDMs,as revealed by the highly publicized 2016 proPublica article [34], may result in biased treatment ofindividuals based solely on their race. In extreme cases, it may lead to wrongful imprisonments for180innocent people, contributing to the cycle of violation and crime.Teachers evaluation and promotion: MLDMs are increasingly used by decision makers todecide which teachers to retain after a probationary period [35] and which tenured teachers topromote. An example of such MLDM is IMPACT [36]. Teacher evaluation MLDMs take as inputteacher related features (age, education level, experience, surveys, classroom observations), students185related features (test scores, sociodemographics, surveys), and principals related features (surveysabout the school and teachers), to predict whether teachers are retained. A biased teacher evaluationMLDM may lead to a systematic unfair low evaluation for teachers in poor neighborhoods, which,very often, happen to be teachers belonging to minority groups [37]. On the long term, this may leadto a significant drop in students’ performance and the compromise of overall school reputation [2].190Child maltreatment prediction: The objective of the MLDM in child maltreatment predictionis to estimate the likelihood of substantiated maltreatment (neglect, physical abuse, sexual abuse,or emotional maltreatment) among children. The system generates risk scores, which would thentrigger a targeted early intervention in order to prevent children maltreatment. PRM (predictiverisk model) [38] has been developed to estimate the likelihood of substantiated maltreatment among195children enrolled in New Zealand’s public benefit system.In Finland, the government uses aML-based system called “Kela” to administer benefits and to identify risk factors indicating that achild might need welfare services.In the US, the Allegheny County uses AFST (Allegheny FamilyScreening Tool) [39] to improve decision-making in child welfare system. The features consideredin this type of MLDM include both contemporaneous and historical information for children and200caregivers. An unfair MLDM may use a proxy variable to predict decisions based on the communityrather than which child gets harmed. For example, a major cause of unfairness in AFST is the rateof referral calls; the community calls the child abuse hotline to report non-white families at a much8higher rate than it does to report white families [39]. On the long term, this creates a vicious cycleas families which have been reported will be the subject of more scrutiny and more requirements to205satisfy, and eventually, will be more likely to fail short of these requirements and hence confirm theprediction of the system.Health care: Since decades, ML algorithms are able to process anonymized electronic healthrecords and flag potential emergencies, to which clinicians are invited to respond promptly. Examplesof features that might be used in disease (chronic conditions) prediction include vital signs, blood210test, socio-demographics, education, health insurance, home ownership, age, race, address. Theoutcome of the MLDM is typically an estimated likelihood of getting a disease. A biased diseaseprediction MLDM can misclassify individuals in certain sub-populations in a disproportionatelyhigher rate than the dominant population. For instance, diabetic patients have known differences inassociated complications across ethnicities [40]. Obemeyer et al. [41] give another example of an215MLDM that predicts the health care spending for individuals in the coming years (useful informationfor insurance companies). They observe that the MLDM is biased against African-Americansbecause it uses the cost of health services in the previous year to predict the spending in the comingyears. As African-Americans were spending less on health services than whites in the previousyear, they were predicted to be spending less in the coming years. Hence, for the same prediction220score, African-Americans were found to be sicker (more health issues) than whites. Consequently,white patients were benefiting more from additional help programs than African-Americans. Moregenerally, because different sub-populations might have different characteristics, a single model topredict complications is unlikely to be best-suited for specific groups in the population even if theyare equally represented in the training data [42]. Failure to predict disease likelihood in a timely225manner may, in extreme cases, have an impact on people’s lives.Online recommendation: Recommender systems are among the most widespread MLDMsin the market, with many services to assist users in finding products or information that are ofpotential interest [43]. Such systems find applications in various online platforms such as Amazon,Youtube, Netflix, LinkedIn, etc. An unfair recommender MLDM can amplify gender bias in the230data. For example, a recommender MLDM called STEM, which aims to deliver advertisementspromoting jobs in Science, Technology, Engineering, and Math fields, is deemed unfair as it hasbeen shown that less women compared to men saw the advertisements due to gender imbalance[44]. Datta et al. [45] found that changing the gender bit in Google Ad Setting [46] resulted in a9significant difference in the type of job ads received: men received much more ads about high paying235jobs and career coaching services towards high paying jobs compared to women.Facial analysis: Automated facial analysis systems are used to identify perpetrators fromsecurity video footage, to detect melanoma (skin cancer) from face images [47], to detect emo-tions [48, 49, 50], and to even determine individual’s characteristics such as IQ, propensity towardsterrorist crime, etc. based on their face images [51]. The possible applications of Facial Analysis240are innumerable. For instance, in France, FRT (Facial Recognition Tool) has been used on anexperimental basis at various schools, with the aim of making access more fluid and secure forpupils. Furthermore, the government announced in 2020 that it would start to use an FRT systemcalled “Alicem” in order to create a digital identification system by which its citizens could accessgovernmental online services. Both of these, however, have sparked a lot of controversy leading to an245announcement that the French government would be reviewing the use of FRT. Indeed, these devicesare particularly intrusive and present major risks of invasion of the privacy and individual freedoms.Worse yet, a flawed MLDM may lead to biased outcomes such as wrongfully accusing individualsfrom specific ethnic groups (e.g. Asians, dark skin populations) for crimes (based on security videofootage) at a much higher rate than the rest of the population. For instance, African-Americans250have been reported to be more likely to be stopped and investigated by law enforcement due toa flawed face recognition system [52]. An investigation of three commercial face-based genderclassification systems found that the error rate for dark-skinned females can be as high as 34.7%while for light-skinned males the maximum error rate is 0.8% [53].255Others: Other MLDMs with fairness concerns include: insurance policy prediction [54], incomeprediction [15], [55, 56, 57, 58], and university ranking [59, 2].For a survey of the various kinds of MLDMs used in European countries, and a description ofthe debates and legal actions they have triggered, we recommend the excellent report by RobinAllen QC and Dee Masters [60] for the European Network of Equality Bodies.2604. Fairness notion selection criteriaIn order to systemize the procedure for selecting the most suitable fairness notion for a specificMLDM system, we identify a set of criteria that can be used as as roadmap. For each criterion, wecheck whether it holds in the problem at hand or not. Telling whether a criterion is satisfied or notdoes not typically require an expertise in the problem domain.10265This section presents a list of 13 selection criteria. These criteria are derived mainly from threesources. First, the types of bias. For instance, the unreliable outcome criterion is a manifestation of ahistorical bias. Second, the mathematical formulation of the fairness notions themselves. For instance,the emphasis on precision vs recall criterion reflects a fundamental difference in the mathematicalformulations of two families of notions, namely, predictive parity and equal opportunity. Third, theexisting anti-discrimination legislation. The last two criteria are inspired by the current legislation.270We note here that in some cases, these criteria can, not only indicate if a fairness notion issuitable, but whether it is “acceptable” to use in the first place.Ground truth availability : A ground truth value is the true and correct observed outcomecorresponding to given sample in the data. It should be distinguished from an inferred subjectiveoutcome in historical data which is decided by a human. An example of a scenario where ground275truth is available is when predicting whether an individual has a disease. The ground truth value isobserved by submitting the individual to a blood test9 for example. An example of a scenario whereground truth is not available is predicting whether a job applicant is hired. The outcome in thetraining data is inferred by a human decision maker which is often a subjective decision, no matterhow hard she is trying to be objective. It is important to mention here that the availability of the280ground truth depends on how the outcome is defined. Consider, for example, college admissionscenario. If the outcome in the training data is defined as whether the applicant is admitted orrejected, ground truth is not available. If, however, the outcome is defined as whether the applicantwill ultimately graduate from college with a high GPA, ground truth is available as it can be observedafter a couple of years.285Base rate is the same across groups: The base rate is the proportion of positive outcomein a population (Table 1). A positive outcome is the goal of the prediction (e.g. a candidate tocollege is admitted, a child is maltreated, an individual is granted a loan, etc.). Note that thepositive outcome can be desirable (e.g. hiring, admission) or undesirable (e.g. firing, high criminalrisk). The base rate can be the same or differs across sub-populations. For example, the base rates290for diabetes disease occurrence for men and women is typically the same. But, for another diseasesuch as prostate cancer, the base rates are different between men and women10.9Assuming the blood test is flawless.10While male prostate cancer is the second most common cancer in men, female prostate cancer is rare [61].11(Un)reliable outcome: In scenarios where ground truth is not available, the outcome (label)in the data is typically inferred by humans. The outcome in the training data in that case canor cannot be reliable as it can encode human bias. The reliability of the outcome depends on the295data collection procedure and how rigorous the data has been checked. Scenarios such as job hiringand college admission may be more prone to the unreliable outcome problem than recommendersystem for example. A “one-size-fit-all” MLDM model in disease prediction that does not take intoconsideration the ethnic group of the individual may result in unreliable outcome as well.300Presence of explanatory variables: An explanatory variable11 is correlated with the sensitiveattribute (e.g. race) in a legitimate way. Any discrimination that can be explained using thatvariable is considered legitimate and is acceptable. For instance, if all the discrepancy between maleand female job hiring rates is explained by their education levels, the discrimination can be deemedlegitimate and acceptable.305Emphasis on precision vs recall : Precision (the complement of target population error [62])is defined as the fraction of positive instances among the predicted positive instances. In otherwords, if the system predicts an instance as positive, how precise that prediction is. Recall (thecomplement of model error [62]) is defined as the fraction of the total number of positive instancesthat are correctly predicted positive. In other words, how many of the positive instances the systemis able to identify. There is always a trade-off between precision and recall (increasing one will310lead, very often, to decreasing the other). Depending on the scenario at hand, the fairness of theMLDM may be more sensitive to one on the expense of the other. For example, granting loans tothe maximum number of deserving applicants contributes more to fairness than making sure thatan applicant who has been granted a loan really deserves it12. When firing employees, however, theopposite is true: fairness is more sensitive to wrongly firing an employee, rather than, firing the315maximum number of under-performing employees.Emphasis on false positive vs false negative: Fairness can be more sensitive to falsepositive misclassification (type I error) rather than false negative misclassification (type II error),or the opposite. For example, in criminal risk assessment scenario, it is commonly accepted that11Referred also as a resolving variable.12It is important to mention here that from the loan granting organization’s point of view, the opposite is true.That is, it is more important to make sure that an applicant who has been granted a loan really deserves it and willnot default in payments because the interest payments resulting from a loan are relatively small compared to the loanamount that could be lost. Our aim here is fairness, while the loan granting organization’s goal is benefit.12incarcerating an innocent person (false positive) is more serious than letting a guilty person escape320(false negative).Cost of misclassification: Depending on the scenario at hand, the cost of misclassificationcan be significant (e.g. incarcerating an individual, firing an employee, rejecting a college application,etc.) or mild and without consequential impact (e.g. useless product recommendation, misleadingincome prediction, offensive online translation, abusive results in online autocomplete, etc.)325Prediction threshold is fixed or floating : Decisions in MLDM are typically made based onpredicted real-valued score. In the case of binary outcome, the score is turned into a binary valuesuch as {0, 1} by thresholding13. In some scenarios, it is desirable to interpret the real-value scoreas probability of being accepted (predicted positive). The threshold used as a cutoff point wherepositive decisions are demarcated from negative decisions can be fixed or floating. A fixed threshold330is set carefully and tends to be valid for different datasets and use cases. For instance, in recidivismrisk assessment, high risk threshold is typically fixed. A floating threshold can be selected andfine-tuned arbitrarily by practitioners to accommodate a changing context. Acceptance score in loangranting scenarios is an example of a floating threshold as it can move up or down depending on theeconomic context. When the threshold is floating in a given application, assessing fairness should be335done using a suitable fairness notion (e.g. calibration) otherwise, the result of the assessment maybe misleading for specific threshold values.Likelihood of intersectionality : Intersectionality theory [63] focuses on a specific type of biasdue to the combination of sensitive factors. An individual might not be discriminated based on raceonly or based on gender only, but she might be discriminated because of a combination of both.340Black women are particularly prone to this type of discrimination.Likelihood of masking : Masking is a form of intentional discrimination that allows decisionmakers with prejudicial views to mask their intentions [64]. Masking is typically achieved byexploiting how fairness notions are defined. For example, if the fairness notion requires equal numberof candidates to be accepted from two ethnic groups, the MLDM can be designed to carefully select345candidates from the first group (satisfying strict requirements) while selecting randomly from thesecond group just to “make the numbers”.Sources of Bias: Bias in the MLDM outcome can arise from several possible sources at any13The threshold is defined by the decision makers depending on the context of interest.13stage in the data generation and machine learning pipeline. Framing sources of bias necessitates deepunderstanding of the application at hand and, typically, can only be identified after a "post-mortem"350analysis of the predicted outcome. However, in some real-world scenarios, one or more sources ofbias may be more likely than others. In such cases, the suspected source of bias can be used asa criterion to select the most appropriate notion for fairness assessment. Sources of bias can begrouped broadly into six categories: historical, representation, measurement, aggregation, evaluation,and deployment [42]. Historical bias arises when the data reliably collected from the world leads to355outcomes which are unwanted and socially unfavorable. For example, while data reliable collectedindicates that only 5% of Fortune 500 CEOs are women [65], the resulting outcome of a predictionsystem based on this data is typically not wanted14. Representation bias arises when some non-protected populations are under-represented in the training data. Measurement bias arises when thefeatures or label values are not measured accurately. For example, Street Bump is an application360used in Boston city to detect when residents drive over potholes thanks to the accelerometers builtinto smartphones [66]. Collecting data using this application introduces a measurement bias due tothe disparity in the distribution of smartphones according to the different districts in the city, whichare often correlated with race or level of income. Aggregation bias arises when sub-populations areaggregated together while a single model is unlikely to fit all sub-populations. For instance, the365genetic risk scores derived largely on European populations have been shown to generally performvery poorly in the prediction of osteoporotic fracture and bone mineral density on non-Europeanpopulations, in particular, on Chinese population [67]. Evaluation bias arises when the trainingdata differs significantly from the testing data. For instance, several MLDMs are trained usingbenchmark datasets which may be very different from the target dataset. Deployment bias arises370when there is a disparity between the initial purpose of an MLDM and the way it is actually used.For instance, a child maltreatment MLDM might be designed to predict the risk of child abuse aftertwo years from the reception of a referral call, while in practice it may be used to help social agentstake decisions about an intervention. This can lead to a bias since the decision has an impact onthe outcome [23].375Legal Framework : Anti-discrimination regulations in several countries, in particular US,distinguish between two legal frameworks, namely disparate treatment and disparate impact [64]. In14For this reason, Google has changed their image search result for CEO to return a higher proportion of women.14the disparate treatment framework, a decision is considered unfair if it uses (directly or indirectly)the individual’s sensitive attribute information. In the disparate impact framework, a decisionis unfair if it results in an outcome that is disproportionately disadvantageous (or beneficial) to380individuals according to their sensitive attribute information. Zafar et al. [5] formalized anotherfairness criterion, namely, disparate mistreatment according to which, a decision is unfair if it resultsin different misclassification rates for groups of people with different sensitive attribute information.Note that this criterion is currently not supported by a legal framework. Machine learning fairnessnotions can be classified according to the type of fairness it is evaluating. For instance, if a plaintiff385is accusing an employer for intentional discrimination, she should consider the disparate treatmentlegal framework, and hence a fairness notion which falls in that framework.The existence of regulations and standards:In some domains, laws and regulationsmight be imposed to avoid discrimination and bias. For instance, guidelines from the U.S. Equal390Employment Opportunity Commission state that a difference of the probability of acceptancebetween two sub-populations exceeding 20% is illegal [8]. Another example might be an internalorganizational policy imposing diversity among its employees.5. Fairness notionsLet V , A, and X be three random variables representing, respectively, the total set of attributes,395400the sensitive attributes, and the remaining attributes describing an individual such that V = (X, A)and P (V = vi) represents the probability of drawing an individual with a vector of values vi fromthe population. For simplicity, we focus on the case where A is a binary random variable whereA = 0 designates the protected group, while A = 1 designates the non-protected group. Let Yrepresent the actual outcome and ˆY represent the outcome returned by the prediction algorithm(MLDM). Without loss of generality, assume that Y and ˆY are binary random variables where Y = 1designates a positive instance, while Y = 0 a negative one. A perfect MLDM will match perfectlythe actual outcome ( ˆY = Y ). Typically, the predicted outcome ˆY is derived from a score representedby a random variable S where P (S = s) is the probability that the score value is equal to s.All fairness notions presented in this section address the following question:“is the out-come/prediction of the MLDM fair towards individuals?”. So fairness notion is defined as a405mathematical condition that must involve either ˆY or S along with the other random variables. As15such, we are not concerned by the inner-workings of the MLDM and their fairness implications.What matters is only the score/prediction value and how fair/biased it is.Most of the proposed fairness notions are properties of the joint distribution of the above random410variables (X, A, Y , ˆY , and S). They can also be interpreted using the confusion matrix and therelated metrics (Table 1).Table 1: Metrics based on confusion matrix.Actual PositiveActual NegativeY = 1Y = 0PredictedPositiveTP(True Positive)FP(False Positive)PPV = T PT P +F PFDR = F PT P +F PˆY = 1Type I errorPositive Predictive ValuePrecisionPV+Target Population ErrorFalse Discovery RateTarget Population ErrorPredictedNegativeFN(False Negative)TN(True Negative)FOR = F NF N +T NNPV = T NF N +T NˆY = 0Type II errorFalse Omission RateSuccess Predictive ErrorNegative Predictive ValuePV-wwwwTPR = T PT P +F NFPR = F PF P +T NOA =T P +T NT P +F P +T N +F N BR =T P +F NT P +F P +T N +F NTrue Positive RateSensitivityRecallFalse Positive RateModel ErrorOverall AccuracyBase RatePrevalence (p)FNR = F NT P +F NTNR = T NF P +T NFalse Negative Rate True Negative RateModel ErrorSpecificityWhile presenting and discussing fairness notions, whenever needed, we use the simple jobhiring scenario of Table 2. Each sample in the dataset has the following attributes: education level(numerical), job experience (numerical), age (numerical), marital status (categorical), gender (binary)and a label (binary). The sensitive attribute is the applicant gender, that is, we are focusing on415whether male and female applicants are treated equally. Table 2(b) presents the predicted decision(first column) and the predicted score value (second column) for each sample. The threshold value16is set to 0.5.Table 2: A simple job hiring example. Y represents the data label indicating whether the applicant is hired (1) orrejected (0). ˆY is the prediction which is based on the score S. A threshold of 0.5 is used.(a) DatasetEducationLevelJob Expe-rienceAge88121191187881210228353032282392632352934484326413028MaritalStatussinglemarriedmarriedsinglemarriedsinglemarriedsinglemarriedsinglesinglemarriedY011011011011(b) PredictionˆYS1010010011100.50.10.50.20.30.80.10.10.50.50.80.3GenderFemale 1Female 2Female 3Female 4Female 5Male 1Male 2Male 3Male 4Male 5Male 6Male 7A simple and straightforward approach to address fairness problem is to ignore completely any420sensitive attribute while training the MLDM system. This is called fairness through unawareness 15.We don’t treat this approach as fairness notion since, given MLDM prediction, it does not allowto tell if the MLDM is fair or not. Besides, it suffers from the basic problem of proxies. Manyattributes (e.g. home address, neighborhood, attended college) might be highly correlated to thesensitive attributes (e.g. race) and act as proxies of these attributes. Consequently, in almost allsituations, removing the sensitive attribute during the training process does not address the problem425of fairness.5.1. Statistical parityStatistical parity [70] (a.k.a demographic parity [71], independence [72], equal acceptance rate[73], benchmarking [74], group fairness [70]) is one of the most commonly accepted notions of fairness.430It requires the prediction to be statistically independent of the sensitive attribute ( ˆY ⊥ A). Thus, a15Known also as: blindness, unawareness [3], anti-classification [68], and treatment parity [69].17classifier ˆY satisfies statistical parity if:P ( ˆY | A = 0) = P ( ˆY | A = 1)(1)In other words, the predicted acceptance rates for both protected and unprotected groups shouldbe equal. Using the confusion matrix (Table 1), statistical parity implies that (T P + F P )/(T P +F P + F N + T N ) should be equal for both groups. In the MLDM of Table 2, it means that oneshould not hire proportionally more applicants from one group than the other. The calculated435predicted acceptance rate of hiring male and female applicants is 0.57 (4 out of 7) and 0.4 (2 out of5), respectively. Thus, the MLDM of Table 2 does not satisfy statistical parity.Statistical parity is appealing in scenarios where there is a preferred decision over the other, andprovided there are no other considerations relevant for the decision, in which case, the followingfairness notion namely, conditional statistical parity, is more suitable. For example, being accepted440to a job, not being arrested, being admitted to a college, etc.16. What really matters is a balance inthe prediction rate among all groups.Statistical parity is suitable when the label Y is not trustworthy due to some flawed or biasedmeasurement17. An example of this type of problem was observed in the recidivism risk predictiontool COMPAS [34]. Because minority groups are more controlled, and more officers are dispatched445in their regions, the number of arrests (used to assess the level of crime [42]) of those minoritygroups is significantly higher than that of the rest of the population. Hence, for fairness purposes,in the absence of information to precisely quantify the differences in recidivism by race, the mostsuitable approach is to treat all sub-populations equally with respect to recidivism [76].450Statistical parity is also well adapted to contexts in which some regulations or standards areimposed. For example, a law might impose to equally hire or admit applicants from differentsub-populations.The main problem of statistical parity is that it doesn’t consider a potential correlation betweenthe label Y and the sensitive attribute A. In other words, if the underlying base rates of the protectedand unprotected groups are different, statistical parity will be misleading. In particular, modifying45516This might not be the case in other scenarios such as disease prediction, child maltreatment, where imposing aparity of positive predictions is meaningless.17This is also known as differential measurement error [75].18an MLDM with a perfect prediction (ˆy = y) so to satisfy statistical parity while the base rates aredifferent will lead to loss of utility [77]. As an example, Figure 1 illustrates a scenario for hiringcomputer engineers where equal proportions of male/female applicants have been predicted hired(60%) thus, satisfying statistical parity. However, when considering the label and more precisely460the base rates that differ in both groups (0.3 for men versus 0.4 for women), the classifier becomesdiscriminative against female applicants (50% of qualified female applicants are not predicted hired).More generally, when the ground truth is available and is used in the training of the MLDM,statistical parity is not recommended because, very often, it conflicts with the ground truth [5].Another issue with this notion is its “laziness”; if we hire carefully selected applicants from malegroup and random applicants from female group, we can still achieve statistical parity, yet leading465to negative results for the female group as its performance will tend to be worse than that of malegroup. This practice is an example of self-fulfilling prophecy [70] where a decision maker may simplyselect random members of a protected group rather than qualified ones, and hence, intentionallybuilding a bad track record for that group. Barocas and Selbst refer to this problem as masking [64].Masking is possible to game several fairness notions, but it is particularly easy to carry out in the470case of statistical parity.Figure 1: Fi and Mi (i ∈ [1 − 10]) designate female and male applicants, respectively. The grey shaded circles indicateapplicants who belong to the positive class while white circles indicate applicants belonging to the negative class. Thedotted vertical line is the prediction boundary. Thus, applicants at the right of this line are predicted hired whileapplicants at the left are predicted not hired.5.2. Conditional statistical parityConditional statistical parity [7], called also conditional discrimination-aware classification in [78]is a variant of statistical parity obtained by controlling on a set of legitimate attributes18. The475legitimate attributes (we refer to them as E) among X are correlated with the sensitive attributeA and give some factual information about the label at the same time leading to a legitimate18Called explanatory attributes in [78].19discrimination. In other words, this notion removes the illegal discrimination, allowing the disparityin decisions to be present as long as they are explainable [7].In the hiring example, possibleexplanatory factors that might affect the hiring decision for an applicant could be the education leveland/or the job experience. If the data is composed of many highly educated and experienced male480applicants and only few highly educated and experienced women, one might justify the disparitybetween predicted acceptance rates between both groups and consequently, does not necessarilyreflect gender discrimination. Conditional statistical parity holds if:P ( ˆY = 1 | E = e, A = 0) = P ( ˆY = 1 | E = e, A = 1) ∀e(2)Table 3: Application of conditional statistical parity by controlling on education level and job experience.(a) DatasetEducationLevelJob Expe-rienceAge88128812228228392632264130GenderFemale 1Female 2Female 3Male 4Male 5Male 6MaritalStatussinglemarriedmarriedmarriedsinglesingleY011101(b) PredictionˆYS1011110.50.10.50.50.50.8Table 3 shows two possible combinations values for E. The first combination (education level=8485and job experience=2) includes samples Female 1, Female 2, Male 4, and Male 5 for which theprediction is clearly discriminative against women as the predicted acceptance rates for men andwomen are 1 and 0.5, respectively. The second combination (education level=12 and job experience=8)includes Female 3 and Male 6 in which the prediction is fair (predicted acceptance rate is 1 for bothapplicants). Overall, the prediction is not fair as it does not hold for one combination of values of E.490In practice, conditional statistical parity is suitable when there is one or several attributes thatjustify a possible disparate treatment between different groups in the population. Hence, choosingthe legitimate attribute(s) is a very sensitive issue as it has a direct impact on the fairness of thedecision-making process. More seriously, conditional statistical parity gives a decision maker a toolto game the system and realize a self-fullfilling prophecy. Therefore, it is recommended to resort to20domain experts or law officers to decide what is unfair and what is tolerable to use as legitimate495discrimination attribute [78].5.3. Equalized oddsUnlike the two previous notions, equalized odds [79] (separation in [72], conditional procedureaccuracy equality in [12], disparate mistreatment in [5], error rate balance in [9]) considers boththe predicted and the actual outcomes. Thus, the prediction is conditionally independent from the500protected attribute, given the actual outcome ( ˆY ⊥ A | Y ). In other words, equalized odds requiresboth sub-populations to have the same TPR and FPR (Table 1). In our example, this means thatthe probability of an applicant who is actually hired to be predicted hired and the probability of anapplicant who is actually not hired to be incorrectly predicted hired should be both same for menand women:505P ( ˆY = 1 | Y = y, A = 0) = P ( ˆY = 1 | Y = y, A = 1) ∀y ∈ {0, 1}(3)In the example of Table 2, the TPR for male and female groups is 0.6 and 0.33, respectivelywhile the FPR is exactly the same (0.5) for both groups. Consequently, the equalized odds does nothold.By contrast to statistical parity, equalized odds is well-suited for scenarios where the groundtruth exists such as: disease prediction or stop-and-frisk [80]. It is also suitable when the emphasis is510on recall (the fraction of the total number of positive instances that are correctly predicted positive)rather than precision (making sure that a predicted positive instance is actually a positive instance).A potential problem of equalized odds is that it may not help closing the gap between theprotected and unprotected groups. For example, consider a group of 20 male applicants of which 16515are qualified and another equal size group of 20 females of which only 2 are qualified. If the employerdecides to hire 9 applicants and while satisfying equalized odds, 8 offers will be granted to the malegroup and only 1 offer will be granted to the female group. While this decision scheme looks fairon the short term, on the long term, however, it will contribute to confirm this “unfair” status-quoand perpetuate this vicious cycle19. Whether to consider this long term impact as a problem of19If the job is a well-paid, male group tends to have a better living condition and affords better education for theirkids, and thus enable them to be qualified for such well-paid jobs when they grow up. The gap between the groupswill tend to increase over time.21equalized odds is a controversial issue as it overlaps with the different but related question of “how520to address unfairness?”. Note that other fairness notions, such as statistical parity, help closing thegap between the protected and unprotected groups on the long term.Because equalized odds requirement is rarely satisfied in practice, two variants can be obtainedby relaxing Eq. 3. The first one is called equal opportunity [79] (false negative error rate balancein [9]) and is obtained by requiring only TPR equality among groups:525P ( ˆY = 1 | Y = 1, A = 0) = P ( ˆY = 1 | Y = 1, A = 1)(4)In the job hiring example, this is to say that we should hire equal proportion of individuals fromthe qualified fraction of each group.As T P R = T P/(T P + F N ) (Table 1) does not take into consideration F P , equal opportunityis completely insensitive to the number of false positives. This is an important criterion whenconsidering this fairness notion in practice. More precisely, in scenarios where a disproportionate530number of false positives among groups has fairness implications, equal opportunity should not beconsidered. The scenario in Table 4 shows an extreme case of a job hiring dataset where the malegroup has a large number of false positives (Male 7 − 100) while equal opportunity is satisfied.Table 4: An extreme job hiring scenario satisfying equal opportunity. All Male 7 − 100 samples are false positives(label Y is 0 and prediction ˆY is 1).(a) DatasetEducationLevelJob Expe-rienceAge8812881210. . .82282285. . .1039263226413032. . .27GenderFemale 1Female 2Female 3Male 4Male 5Male 6Male 7. . .Male 100MaritalStatussinglemarriedmarriedmarriedsinglesinglemarried. . .singleY101101000(b) PredictionˆYS1001001110.50.10.30.50.20.40.8. . .0.7To decide about the suitability of equal opportunity in the job hiring example, the question thatshould be answered by stakeholders and decision makers is “if all other things are equal, is it fair to53522hire disproportionately more unqualified male candidates?”. For the employer, it is undesirable tohave several false positives (regardless of their gender) as the company will end up with unqualifiedemployees. For a stakeholder whose goal is to guarantee fairness between males and females, it isnot very critical to have more false positives in one group, provided that these two groups have thesame proportion of false negatives (a qualified candidate which is not hired).540In the scenario of predicting which employees to fire, however, a false positive (firing a well-performing employee) is critical for fairness. Hence, equal opportunity should not be used as ameasure of fairness.The second relaxed variant of equalized odds is called predictive equality [7] (false positiveerror rate balance in [9]) which requires only the FPR to be equal in both groups.545In other words, predictive equality checks whether the accuracy of decisions is equal acrossprotected and unprotected groups:P ( ˆY = 1 | Y = 0, A = 0) = P ( ˆY = 1 | Y = 0, A = 1)(5)In the job hiring example, predictive equality holds when the probability of an applicant with anactual weak profile for the job to be incorrectly predicted hired is the same for both men and women.550Since F P R = F P/(F P +T N ) (Table 1) is independent from F N , predictive equality is completelyinsensitive to false negatives. One can come up with an extreme example similar to Table 4 with adisproportionate number of false negatives but predictive equality will still be satisfied (keeping allother rates equal). Hence, in scenarios where fairness between groups is sensitive to false negatives,predictive equality should not be used. Such scenarios include hiring and admission where a falsenegative means a qualified candidates are rejected disproportionately among groups. Predictive555equality is acceptable in criminal risk assessment scenarios as false negatives (releasing a guiltyperson) are less critical than false positives (incarcerating an innocent person).Predictive equality is particularly suitable to measure the fairness of face recognition systems incrime investigation where security camera footage are analyzed. Fairness between ethnic groupswith distinctive face features is very sensitive to the FPR. A false positive means an innocent person560is being flagged as participating in a crime. If this false identification happens at a much higher ratefor a specific sub-population (e.g. dark skinned group) compared to the rest of the population, it isclearly unfair for individuals belonging to that sub-population.23Looking to the problem from another perspective, choosing between equal opportunity andpredictive equality depends on how the outcome/label is defined. In scenarios where the positive565outcome is desirable (e.g. hiring, admission), typically fairness is more sensitive to false negativesrather than false positives, and hence equal opportunity is more suitable. In scenarios where thepositive outcome is undesirable for the subjects (e.g. firing, risk assessment), typically fairness ismore sensitive to false positives rather than false negatives, and hence predictive equality is moresuitable.570The following proposition states formally the relationship between equalized odds, equal oppor-tunity, and predictive equality.Proposition 5.1. Satisfying equal opportunity and predictive equality is equivalent to satisfyingequalized odds:Eq. 3 ⇔ Eq. 4 ∧ Eq. 55.4. Conditional use accuracy equalityConditional use accuracy equality [12] (called sufficiency in [72]) is achieved when all population575groups have equal P P V = T PT P +F Pand N P V = T NF N +T N. In other words, the probability of subjectswith positive predictive value to truly belong to the positive class and the probability of subjectswith negative predictive value to truly belong to the negative class should be the same:P (Y = y | ˆY = y, A = 0) = P (Y = y | ˆY = y, A = 1) ∀y ∈ {0, 1}(6)Intuitively, this definition implies equivalent accuracy for male and female applicants from bothpositive and negative predicted classes [13]. By contrast to equalized odds (Section 5.3), one isconditioning on the algorithm’s predicted outcome not the actual outcome. In other words, this580notion emphasis the precision of the MLDM system rather than its sensitivity (a trade-off discussedearlier in Section 4).The calculated PPVs for male and female applicants in our hiring example (Table 2) are 0.75and 0.5, respectively. NPVs for male and female applicants are both equal to 0.33. Overall thedataset in Table 2 does not satisfy conditional use accuracy equality.585Predictive parity [9] (called outcome test in [74]) is a relaxation of conditional use accuracyequality requiring only equal PPV among groups:24P (Y = 1 | ˆY = 1, A = 0) = P (Y = 1 | ˆY = 1, A = 1)(7)In our example, this is to say that the prediction used to determine the candidate’s eligibility fora particular job should reflect the candidate’s actual capability of doing this job which is harmoniouswith the employer’s benefit.590Like predictive equality (Eq. 5), predictive parity is insensitive to false negatives. Hence in anyscenario where fairness is sensitive to false negatives, predictive parity should not be consideredsufficient.Choosing between predictive parity and equal opportunity depends on whether the scenario athand is more sensitive to precision or recall. For precision-sensitive scenarios, typically predictive595parity is more suitable while for recall-sensitive scenarios, equal opportunity is more suitable.Precision-sensitive scenarios include disease prediction, child maltreatment risk assessment, andfiring from jobs. Recall-sensitive scenarios include loan granting, recommendation systems, andhiring. Very often, precision-sensitive scenarios coincide with situations where the positive prediction600( ˆY = 1) entails a higher cost [5]. For example, a predicted child maltreatment case will result inplacing the child in a foster house which will generally entail a higher cost compared to a negativeprediction (low risk of child maltreatment) in which case the child stays with the family and typicallyno action is taken.Conditional use accuracy equality (Eq. 6) is “symmetric” to equalized odds (Eq. 3) with the only605difference of switching Y and ˆY . The same holds for equal opportunity (Eq. 4) and predictive parity(Eq. 7). However, there is no “symmetric” notion to predictive equality (Eq. 5). For completeness,we define such notion and give it the name negative predictive parity.Definition 5.1. Negative predictive parity holds iff all sub-groups have the same N P V = T NF N +T N :P (Y = 1 | ˆY = 0, A = 0) = P (Y = 1 | ˆY = 0, A = 1)(8)The following proposition states formally the relationship between conditional use accuracyequality, predictive parity, and negative predictive parity.610Proposition 5.2. Satisfying predictive parity and negative predictive parity is equivalent to satisfying25conditional use accuracy equality:Eq. 6 ⇔ Eq. 7 ∧ Eq. 85.5. Overall accuracy equalityOverall accuracy equality [12] is achieved when overall accuracy for both groups is the same.Thus, true negatives and true positives are equally considered and desired. Using the confusionmatrix (Table 1), this implies that (T P + T N )/(T P + F N + F P + T N ) is equal for both groups. Inour example, it is to say that the probability of well-qualified applicants to be correctly accepted for615the job and non-qualified applicants to be correctly rejected is the same for both male and femaleapplicants:P ( ˆY = Y |A = 0) = P ( ˆY = Y |A = 1)(9)Table 5: A job hiring scenario satisfying overall accuracy but not conditional use accuracy equality.OA = 0.625PPV = 1NPV = 0.25Group 1 (Female)Group 2 (Male)GenderF1F2F3F4F5F6F7F8Y11101111ˆY10001101GenderM1M2M3M4M5M6M7M8Y10000001ˆY11100011OA = 0.625PPV = 0.4NPV = 1Overall accuracy equality is closely related to equalized odds (Eq. 3) and to conditional useaccuracy equality (Eq. 6). The main difference is that overall accuracy equality aggregates togetherpositive class and negative class misclassifications (FP and FN). Aggregating together FP and FN620(and hence TP and TN) without any distinction is very often misleading for fairness purposes.Proposition 5.3. An MLDM that satisfies equalized odds or conditional use accuracy equalityalways satisfies overall accuracy.Eq. 3 ∨ Eq. 6 ⇒ Eq. 926The reverse, however, is not true. That is, an MLDM that satisfies overall accuracy does notnecessarily satisfy equalized odds or conditional use accuracy equality. To prove it, consider theexample in Table 5 satisfying overall accuracy equality but not conditional use accuracy equality.For the female group, there are only FN misclassifications (no FP) and more TPs than TNs, while625in the male group, there are only FP misclassifications (no FN) and more TNs than TPs. But sincethe proportion of correct classifications is the same in both groups (5 out of 8), overall accuracyequality holds. In real-world applications, it is very uncommon that TP (or FN) and TN (or FP)are desired at the same time and without distinction. For example, overall accuracy equality is notsuitable to measure fairness in child maltreatment prediction because a False Positive (misclassifying630a child case which is not at risk20) is less damaging than a False Negative (misclassifying a child casewhich is at risk21). A hypothetical health care scenario where overall accuracy equality is suitable iswhen both types of misclassifications have the same cost/benefit. For example, an eventual healthcondition that yields very similar complications (1) when the treatment is administered wronglyand (2) when the treatment is not administered while it is needed.6355.6. Treatment equalityTreatment equality [12] is achieved when the ratio of FPs and FNs is the same for both protectedand unprotected groups:F NF P (A=0) =F NF P (A=1)(10)Treatment equality is insensitive to the numbers of TPs and TNs which are important to identifybias between sub-populations in most real-world scenarios. Berk et al. [12] note that treatment640equality can serve as an indicator to achieve other kinds of fairness. Table 6 shows a dataset whichfails to satisfy all previous notions, yet, treatment equality is satisfied. Treatment equality can beused in real-world scenarios where only the type of rate of misclassification matters for fairness.Treatment equality can be suitable to use in case the cost (or benefit) of a FP is a fixed ratio (orreciprocal) of the cost (or benefit) of a FN. For example, one can think of a loan granting scenario645where the cost of a FP (misclassifying a non-defaulter) is exactly a fraction (e.g. 1/3) of the cost of20Results in a useless intervention, because the child is not at risk anyway.21Results in a failure to anticipate a child maltreatment.27Table 6: A job hiring scenario satisfying treatment equality but not satisfying all of the previous notions.Group 1 (Female)Group 2 (Male)GenderF1F2F3F4F5F6F7F8Y10000011ˆY10111100GenderM1M2M3M4M5M6M7M8Y11110001ˆY11110110TPR = 0.33FPR = 0.8PPV = 0.2NPV = 0.33OA = 0.25FN/FP = 0.5TPR = 0.8FPR = 0.66PPV = 0.66NPV = 0.5OA = 0.625FN/FP = 0.5a FN (misclassifying a defaulter).Total fairness [12] is another notion which holds when all aforementioned fairness notions aresatisfied simultaneously, that is, statistical parity, equalized odds, conditional use accuracy equality(hence, overall accuracy equality), and treatment equality. Total fairness is a very strong notion650which is very difficult to hold in practice. Table 7 shows a scenario where total fairness holds. Moregenerally, total fairness is satisfied in the very uncommon situation where the proportions of TPs,TNs, FPs, and FNs are the same in all groups.Table 7: A job hiring scenario satisfying total fairness.Group 1 (Female)Group 2 (Male)GenderF1F2F3F4F5Y10001ˆY10110TPR = 0.5FPR = 0.66PPV = 0.33NPV = 0.5OA = 0.4FN/FP = 0.5GenderM1M2M3M4M5M6M7M8M9M10Y1100000011ˆY1100111100TPR = 0.5FPR = 0.66PPV = 0.33NPV = 0.5OA = 0.4FN/FP = 0.5Total fairness can be considered in scenarios where any deviation in misclassification or acceptance28rates between sub-populations is very costly22.6555.7. BalanceThe predicted outcome ( ˆY ) is typically derived from a score (S) which is returned by the MLalgorithm. All aforementioned fairness notions do not use the score to assess fairness. Typically, thescore value is normalized to be in the interval [0, 1] which makes it possible to interpret the score660as the probability to predict the sample as positive. Balance for positive class [6] focuses onthe applicants who constitute positive instances and is satisfied if the average score S received bythose applicants is the same for both groups. In other words, a violation of this balance means thatapplicants belonging to the positive class in one group might receive steadily lower predicted scorethan applicants belonging to the positive class in the other group:E[S | Y = 1, A = 0)] = E[S | Y = 1, A = 1](11)665Table 8 shows a job hiring scenario where the average score for female candidates that should behired (Y = 1) is 7.1 while it is 4.7 for male candidates. The scenario is not balanced for positive class.Note that, despite the significant difference between these two average values, for a score thresholdvalue of 5, the scenario of Table 8 satisfies both statistical parity (Eq. 1) and equal opportunity(Eq. 4).Table 8: A job hiring scenario satisfying statistical parity and equal opportunity (for a score threshold value of 5) butneither balance for positive class nor balance for negative class.(a) Group 1 (Female)(b) Group 2 (Male)GenderF1F2F3F4F5F6Y110100S9884.54.53.5GenderM1M2M3M4M5M6Y110010S6.265.5122670Balance of negative class [6] is an analogous fairness notion where the focus is on the negative22The cost can be financial, ethical, reputation, etc.29class:E[S | Y = 0, A = 0] = E[S | Y = 0, A = 1](12)The scenario in Table 8 is not balanced for the negative class either since the average scores forthe negative class (Y = 0) for the female and male groups are 5.3 and 2.8, respectively.Both variants of balance can be required simultaneously (Eq. 11 and 12) which leads to a strongernotion of balance. Since no previous work reported such fairness notion, for completeness, we define675it and call it overall balance.Definition 5.2. Overall balance is satisfied iff:E[S | Y = y, A = 0] = E[S | Y = y, A = 1] ∀y ∈ {0, 1}(13)Balance fairness notions are relevant in the criminal risk assessment scenario because a divergencein the score values of individuals from different races may indicate a difference in the type ofcrime that can be committed (high risk score typically means a serious crime). Balance fairness680notions are also suitable in the teacher firing scenario since any discrepancy between the averageevaluation scores of fired teachers in different groups is a clear indicator of bias. On the other hand,balance fairness notions can be misleading in presence of clusters of samples sharing very similarattribute values and having score values in the vicinity of the positive/negative outcome threshold.In such case, the average score of the positive/negative class can change significantly due to a slight685increase/decrease of the threshold value.5.8. CalibrationCalibration [9] (a.k.a. test-fairness [9], matching conditional frequencies [79]) relies on the scorevariable as follows. To satisfy calibration, for each predicted probability score S = s, individuals inall groups should have the same probability to actually belong to the positive class:690P (Y = 1 | S = s, A = 0) = P (Y = 1 | S = s, A = 1) ∀s ∈ [0, 1](14)Eq. 14 is very unlikely to be satisfied in practice as the probability of two individuals havingexactly the same real number score is very small. Moreover, technically, the probability that S30exactly equal to s is typically 0. Therefore, in practice, the space of score values [0, 1] is binned intointervals called bins such that any two values falling in the same bin are considered equal [6, 13, 81].695In our job hiring example, this implies that for any score value s ∈ [0, 1], the probability of trulybeing hired should be the same for both male and female applicants.Table 9: A job hiring scenario satisfying predictive parity (for any threshold smaller than 0.7 or larger than 0.8) butnot calibration.(a) Group 1 (Female)(b) Group 2 (Male)GenderF1F2F3F4F5F6F7F8Y11010010S0.850.80.80.70.70.40.40.4GenderM1M2M3M4M5M6M7M8Y11100100S0.850.80.80.70.70.40.40.4Eq. 14 is very similar to Eq. 7 corresponding to predictive parity. Table 9 illustrates a job hiringscenario that may or may not satisfy predictive parity depending on the score threshold to hire acandidate; for a threshold value of 0.6, PPV rate for both male and female groups is the same, 0.6,700while for a threshold value of 0.75, PPV for female group is 0.66 but for male it is 1.0. However, thecalibration score (P (Y = 1 | S = s, A = a) a ∈ {0, 1}, s ∈ [0, 1]) for every value of s is as follows:s0.40.70.80.85Female0.330.5Male0.3300.51.01.01.0Calibration is satisfied for score values 0.4 and 0.85, but not satisfied for score values 0.7 and 0.8.Overall, the scenario of Table 9 does not satisfy calibration.705Interestingly, calibration is not always stronger than predictive parity [82]. Table 10 shows ajob hiring scenario satisfying calibration, but not predictive parity. Calibration is suitable to use inscenarios where the threshold is not fixed and is very likely to be tuned to accommodate a changingcontext. A first example is the acceptance score in loan granting applications which may changeabruptly due to economic instability. A second example is the child maltreatment risk assessment31Table 10: A job hiring scenario satisfying calibration but not predictive parity (for any threshold).(a) Group 1 (Female)(b) Group 2 (Male)GenderF1F2F3F4F5F6F7F8Y11110000S0.80.80.70.70.70.70.30.3GenderYSM1M2M3M4M5M61110000.80.80.70.70.30.3where the threshold for intervention (withdrawing a child from his family) depends on the available710seats in foster houses.Well-calibration [6] is a stronger variant of calibration. It requires that (1) calibration issatisfied, (2) the score is interpreted as the probability to truly belong to the positive class, and (3)for each score S = s, the probability to truly belong to the positive class is equal to that particularscore:715P (Y = 1 | S = s, A = 0) = P (Y = 1 | S = s, A = 1) = s ∀ s ∈ [0, 1](15)Intuitively, for a set of applicants who have a certain probability s of being hired, approximatelys percent of these applicants should truly be hired. Table 11 (a) is a job hiring scenario which iscalibrated (the proportion of applicants which should be hired for every score value is the samefor male and female groups) but not well-calibrated (the score value does not coincide with theproportion of applicants that should be hired). Table 11 (b) is both calibrated and well-calibrated.720Garg et al. [82] show that the difference between calibration and well-calibration is a simple differencein mapping. That is, “the scores of a calibrated predictor can, using a suitable transformation, beconverted to scores satisfying well-calibration”.5.9. Group vs individual fairness notions725All the fairness notions discussed above are considered as group fairness where their commonobjective is to ensure that groups who differ by their sensitive attributes are treated equally. These32Table 11: Calibration vs well-calibration.(a) Calibrated but not well-calibrated(b) Calibrated and well-calibratedsFemaleMale0.40.330.330.70.50.50.80.60.60.850.60.6sFemaleMale0.40.40.40.70.70.70.80.80.80.850.850.85notions, mainly based on statistical measures, generally ignore all attributes of the individualsexcept the sensitive attribute A. Such treatment might hide unfairness. Dwork et al. [70] statedthat group fairness, despite its suitability for policies among demographic sub-populations, does notguarantee that individuals are treated fairly. This is illustrated in the simple example in Table 12.730The example satisfies most of group fairness notions, including total fairness (Section 5.6). However,based on the applicants profiles, it is clear that the predictor is unfair towards applicant Female4. The fairness notions which follow attempt to address such issues by not marginalizing overnon-sensitive attributes X of an individual, therefore they are called individual fairness notions 23.Table 12: A simple job hiring example satisfying most of group fairness notions, but unfair towards Female 4 applicant.GenderFemale 1Female 2Female 3Female 4Female 5Male 1Male 2Male 3Male 4Male 5EducationLevelJob Expe-rienceAge8861297811782218103081239263235293428432641MaritalStatussinglemarriedmarriedsinglemarriedsinglemarriedsinglemarriedsingleY ˆY00011011001100110101TPR = 0.5FPR = 0.66PPV = 0.33OA = 0.4TPR = 0.5FPR = 0.66PPV = 0.33OA = 0.423The term individual fairness is used in some papers to refer to fairness through awareness (Section 5.11). In thispaper, the term individual fairness refers to fairness notions which cannot be considered as group fairness notions.337355.10. Causal discriminationCausal Discrimination [83] implies that a classifier should produce exactly the same predictionfor individuals who differ only from gender while possessing identical attributes X. In our hiringexample, this is to say that male and female applicants with the same attributes X should have thesame predictions:X (A=0) = X (A=1) ∧ A(A=0) (cid:54)= A(A=1) ⇒ ˆy(A=0) = ˆy(A=1)(16)In our example, this implies that male and female applicants who otherwise have the same attributes X740will either both be assigned a positive prediction or both assigned a negative prediction. Consideringthe example of Table 2, two applicants of different genders (Female 2 and Male 4) have identicalvalues of X yet, getting different predictions (negative for female applicant while positive for maleapplicant). The predictor is then unfair towards Female 2 applicant.745At a first glance, causal discrimination can be seen as an extreme case of conditional statisticalparity (Section 5.2) when conditioning on all non-sensitive attributes (E = X). However, conditionalstatistical parity is a group fairness notion which is satisfied if the proportion of individuals havingthe same non-sensitive attribute values and predicted accepted in both groups (e.g. male andfemale) is the same. This is why Eq. 2 is expressed in terms of conditional probabilities. Causaldiscrimination, however, consider every individual separately regardless of its contribution to750sub-population proportions. To illustrate this subtlety, consider the following scenario:Female 1Female 2Male 1Male 28888222226262626singlesinglesinglesingleˆY = 0ˆY = 1ˆY = 1ˆY = 0Conditional statistical parity with E = X (conditioning on all non-sensitive attributes) is satisfiedas the proportion of males and females having the exact same attribute values and predicted accepted755is the same (0.5). However, at the individual level, causal discrimination is not satisfied as there aretwo violations: Female 1 vs Male 1 and Female 2 vs Male 2. The two violations compensated eachothers and as a result conditional statistical parity is satisfied.34Causal discrimination is suitable to use in decision making scenarios where it is very common tofind individuals sharing exactly the same attribute values. For example, admission decision makingbased mainly on test scores and categorical attributes. To apply this fairness notion on a loan760granting scenario where there are only few individuals with exactly the same attribute values, Vermaand Rubin [13] generated, for every applicant in the dataset, an identical individual of the oppositegender. The result of applying causal discrimination is the percentage of violations in the entirepopulation (i.e. how many individuals are unfairly treated?).7655.11. Fairness through awarenessFairness through awareness [70] (a.k.a individual fairness [4, 71]) is a generalization of causaldiscrimination which implies that similar individuals should have similar predictions. Let i and j betwo individuals represented by their attributes values vectors vi and vj. Let d(vi, vj) represent thesimilarity distance between individuals i and j. Let M (vi) represent the probability distributionover the outcomes of the prediction. For example, if the outcome is binary (0 or 1), M (vi) mightbe [0.2, 0.8] which means that for individual i, P ( ˆY = 0) = 0.2 and P ( ˆY = 1) = 0.8. Let D be adistance metric between probability distributions. Fairness through awareness is achieved iff, for770any pair of individuals i and j:D(M (vi), M (vj)) ≤ d(vi, vj)(17)For our hiring example, this implies that the distance between the distribution of outcomesof two applicants should be at most the distance between those applicants24. A possible relevant775features to use for measuring the similarity between two applicants might be the education level andthe job experience. The distance metric d between two applicants could be defined as the average ofthe normalized difference (the difference divided by the maximum difference in a dataset) of their780education level and their job experience. More formally, let Eviindividuals i and j, respectively, and let NE be the normalized difference between education levels,where mE is the maximum difference in education level in the dataset.that is, NE =be the job experience of individuals i and j, while NJ is the normalizedSimilarly, let Jvi|Evi −Evj |mEand Jvjbe the education levels ofand Evj24Reducing all difference between two applicants/instances to a single distance value is often not easy to do inpractice.35difference of the job experience, that is, NJ =experience in the dataset. The distance metric is defined as:|Jvi −Jvj |mJwhere mJ is the maximum difference in jobd(vi, vj) =NE + NJ2,785The distance between the probability distributions over the outcomes could be the Hellingerdistance [84]. Let {y1, y2, . . . , yK} be the set of possible outcomes and let P and Q two (discrete)probability distributions. The Hellinger distance between P and Q is defined as:H(P, Q) =(cid:118)(cid:117)(cid:117)(cid:116)K(cid:88)k=11√2(cid:16)(cid:112)P (yk) − (cid:112)Q(yk)(cid:17)2Table 13 shows a sample from the job hiring dataset on which fairness through awareness is applied.The result of applying fairness through awareness is shown in Table 14. Each cell at the left ofthe shaded diagonal represents a distance between two individuals and each cell at the right of the790shaded diagonal represents the distance between probability outcomes of two individuals.For instance:While:d(F 1, F 2) = 0.25D(M (F 1), M (F 2)) ==1√21√2(cid:114)(cid:16)√√(cid:17)20.3+(cid:16)√0.6 −√(cid:17)20.70.4 −√0.0081 + 0.0036= 0.07The cell values in bold represent the cases where fairness through awareness is not satisfied:D (cid:2) d. For example, 0.07 (< 0.0) implies that F1 is discriminated compared to M3. Similarly, M2is discriminated compared to F3, F2, and M3.795Fairness through awareness is more fine-grained than any group fairness notion presented earlierin Sections 5.1– 5.8. For instance, in the example of Table 13, statistical parity is satisfied: 0.33for both men and women. Likewise, equalized odds ( 5.3) is satisfied as the TPR and the FPR areequal for male and female applicants (0.5 and 0, respectively). Nevertheless, Table 14 shows that36Table 13: Job hiring sample used to apply fairness through awareness.(a) DatasetEducationLevelJob Expe-rienceAge121213131212211112392632264130MaritalStatussinglemarriedmarriedmarriedsinglesingleGenderFemale 1Female 2Female 3Male 1Male 2Male 3(b) PredictionˆYSLabel1011010010010.40.30.90.20.20.7Table 14: Application of fairness through awareness. Each cell at the left of the shaded table’s diagonal represents adistance between a pair of applicants. Those at the right represent the distance between probability distributions.Values in bold imply cases where D > d, meaning fairness through awareness is not satisfied.F10.250.750.750.250.0F1F2F3M1M2M3F20.070.50.50.00.25F30.260.180.00.50.75d(vi, vj)M10.160.080.10.50.75M20.160.080.540.00.25M30.070.290.180.080.37))jv(M,)iv(MD(when comparing each pair of individuals (regardless of their gender) cases of discrimination have800been discovered.It is important to mention that, in practice, fairness through awareness introduces some challenges.For instance, it assumes that the similarity metric is known for each pair of individuals [85]. Thatis, a challenging aspect of this approach is the difficulty to determine what is an appropriate metricfunction to measure the similarity between two individuals. Typically, this requires careful human805intervention from professionals with domain expertise [71]. For instance, suppose a company isintending to hire only two employees while three applicants i1, i2 and i3 are eligible for the offeredjob. Assume i1 has a bachelor’s degree and 1 year related work experience, i2 has a master’s degreeand 1 year related work experience and i3 has a master’s degree but no related work experience(Figure 2). Is i1 closer to i2 than i3? If so, by how much? This is difficult to answer, especially ifthe company overlooked such specific cases and did not carefully define and set a suitable and fair81037similarity metric in order to rank applicants for job selection. Thus, fairness through awareness cannot be considered suitable for domains where trustworthy and fair distance metric is not available.Figure 2:An example showing the difficulty of selecting a distance metric in fairness through awareness5.12. Causality-based fairness notions815Causality-based fairness notions differ from all aforementioned statistical fairness approaches inthat they are not totally based on data but consider additional knowledge about the structure ofthe world, in the form of a causal model. This additional knowledge helps us understand how datais generated in the first place and how changes in variables propagate in a system. Most of thesefairness notions are defined in terms of non-observable quantities such as interventions (to simulaterandom experiments) and counterfactuals (which consider other hypothetical worlds, in addition to820the actual world).A variable X is a cause of a variable Y if Y in any way relies on X for its value [86]. Causalrelationships are expressed using structural equations [87] and represented by causal graphs wherenodes represent variables (attributes) and edges represent causal relationships between variables.Figure 3 shows a possible causal graph for our hiring example where directed edges indicate causal825relationships.Statistical parity (Section 5.1) is known also as total variation (TV) as it can be expressed bysubtracting the two terms in Eq. 1 as follows:T Va1,a0(ˆy) = P ( ˆY = ˆy | A = a1) − P ( ˆY = ˆy | A = a0)(18)A T V equal zero indicates fairness according to statistical parity. As T V is purely a statistical38Figure 3: A possible causal graph for the hiring example.830notion, it is unable to reflect the causal relationship between A and Y , that is, it is insensitive tothe mechanism by which data is generated.Total effect (T E) [88] is the causal version of T V and is defined in terms of experimentalprobabilities as follows :T Ea1,a0 (ˆy) = P (ˆyA←a1) − P (ˆyA←a0 )(19)P (ˆyA←a) = P ( ˆY = ˆy | do(A = a)) is called an experimental probability and is expressed usingintervention. An intervention, noted do(V = v), is a manipulation of the model that consists in835fixing the value of a variable (or a set of variables) to a specific value. Graphically, it consists indiscarding all edges incident to the vertex corresponding to variable V . Intuitively, using the jobhiring example, while P ( ˆY = 1 | A = 0) reflects the probability of hiring among female applicants,P ( ˆYA←0 = 1 = P ( ˆY = 1) | do(A = 0)) reflects the probability of hiring if all the candidatesin the population had been female. The obtained distribution P ( ˆYA←a) can be considered as acounterfactual distribution since the intervention forces A to take a value different from the one itwould take in the actual world. Such counterfactual variable is noted also ˆYA=a or ˆYa for short.T E measures the effect of the change of A from a1 to a0 on ˆY = ˆy along all the causal pathsfrom A to ˆY . Intuitively, while T V reflects the difference in proportions of ˆY = ˆy in the currentcohort, T E reflects the difference in proportions of ˆY = ˆy in the entire population. A more involvedcausal-based fairness notion considers the effect of a change in the sensitive attribute value (e.g.840845gender) on the outcome (e.g. probability of hiring) given that we already observed the outcome forthat individual. This typically involves an impossible situation which requires to go back in the pastand change the sensitive attribute value. Mathematically, this can be formalized using counterfactualquantities. The simplest fairness notion using counterfactuals is the effect of treatment on the85039treated (ETT) [88].The effect of treatment on the treated (ETT) is defined as:ET Ta1,a0(ˆy) = P (ˆyA←a1 | a0) − P (ˆy | a0)(20)P (ˆyA←a1 | a0) reads the probability of ˆY = ˆy had A been a1, given A had been observed to be a0.For instance, in the job hiring example, P ( ˆYA←1 | A = 0) reads the probability of hiring an applicanthad she been a male, given that the candidate is observed to be female. Such probability involves two855worlds: an actual world where A = a0 (the candidate is female) and a counterfactual world wherefor the same individual A = a1 (the same candidate is male). Notice that P (ˆyA←a0 | a0) = P (ˆy | a0),a property called consistency [88].Counterfactual fairness [71] is a fine-grained variant of ETT conditioned on all attributes. That860is, a prediction ˆY is counterfactually fair if under any assignment of values X = x,P ( ˆYA←a1 = ˆy | X = x, A = a0) = P ( ˆYA←a0 = ˆy | X = x, A = a0)(21)where X is the set of all attributes excluding A. Since conditioning is done on all remaining variablesX, counterfactual fairness is an individual notion. According to Eq. 21, counterfactual fairness issatisfied if the probability distribution of the outcome ˆY is the same in the actual and counterfactualworlds, for every possible individual. In the job hiring example, an MLDM is counterfactually fair if:P ( ˆYA←1 | X = x, A = 0) = P ( ˆYA←0 | X = x, A = 0)(22)865The main problem with the applicability of TE, ETT, and counterfactual fairness is the com-putation of the non-observable terms in Eqs 19, 20, and 21. These terms are either interventional)) or counterfactual (e.g. P ( ˆYA←a1 = ˆy | X = x, A = a0). In scenarios where these(e.g. P (ˆyA←a1quantities can be expressed in terms of observable probabilities (e.g. joint probabilities, conditionalprobabilities, etc.), it is said that they are identifiable. Otherwise, they are unidentifiable. Typically,the identifiability of interventional and counterfactual quantities depends on the structure of the870causal graph [89, 88]. Alternatively, if all parameters of the causal model are known (including thelatent variables distributions P (U = u)), any counterfactual is identifiable and can be computedusing the three steps abduction, action, and prediction (Theorem 7.1.7 in [88]). The details of40the computation of a counterfactual probability using a simple deterministic example are providedin Appendix A.875A simple but important implication of Eq. 21 is that, given a causal graph, a predictor ˆY iscounterfactually fair if it is a function of non-descendants of the sensitive variable A. In otherwords, if ˆY is a function of variables that depend on A (there is a directed path between any one ofthose variables and A), it is not counterfactually fair. Consequently, one can tell if a predictor iscounterfactually fair by simply checking the causal graph25.880No unresolved discrimination [90] is another causal-based fairness notion which is satisfied whenno directed paths from the sensitive attribute A to the predictor ˆY are allowed, except via a resolvingvariable. A resolving variable is any variable in a causal graph that is influenced by the sensitiveattribute in a manner that is accepted as nondiscriminatory (this is similar to explanatory attributesin conditional statistical parity (Section 5.2)). In the job hiring example, if we assume that the effect885of A on the education level is nondiscriminatory, it implies that the differences in education levelfor different values of A are not considered as discrimination. Thus, a disparity in the predictionsbetween men and women might been explained and justified by their corresponding education levels.Hence, the education level acts as a resolving variable. Figure 4 shows two similar causal graphsfor our hiring example, yet differ in some of the causal relations between variables. By considering890the education as a resolving variable, the graph at the left exhibits unresolved discrimination alongthe dashed paths: A → Experience → ˆY and A → ˆY . By contrast, the graph at the right does notexhibit any unresolved discrimination as the effect of A on ˆY is justified by the resolved variableEducation: A → Education → ˆY .Figure 4: Two possible graphs for the hiring example. If Education is a resolving variable, the predictor ˆY exhibitsunresolved discrimination in the left graph (along the dashed paths), but not in the right one.25Kusner et al. [71] identify some exceptions, but guaranteeing that they will not happen in general.41895No unresolved discrimination is equivalent to other fairness notions in some interesting specialcases [90]. For instance, if no resolving variables exist, no unresolved discrimination is analogous tostatistical parity (Section 5.1) in a causal context. A and ˆY are statistically independent and nodirected paths from A to ˆY are allowed. Likewise, no unresolved discrimination might be equivalentto equalized odds (Section 5.3) in a causal context if the set of resolving variables is the singleton900set of actual outcomes: {Y }. Compared to counterfactual fairness, no unresolved discrimination isa weaker notion. That is, a counterfactually unfair scenario may be identified as fair based on nounresolved discrimination. This can happen in case one or several variables in the causal graph areidentified as resolving.A causal graph exhibits potential proxy discrimination [90] if there exists a path from the905protected attribute A to the predicted outcome ˆY that is blocked by a proxy variable Px. A proxyis a descendant of A that is chosen to be labelled as a proxy because it is significantly correlatedwith A. Given a causal graph, a predictor ˆY exhibits no proxy discrimination if following equalityholds for all potential proxies Px.P ( ˆYPx←p) = P ( ˆYPx←p(cid:48)) ∀ p, p(cid:48)(23)In other words, Eq. 23 implies that changing the value of Px should not have any impact on theIn the job hiring example, the job experience can be considered as a proxy of anprediction.910individual’s gender. Figure 5 shows two similar causal graphs. The one at the left presents apotential proxy discrimination via the path: A → Experience → ˆY . However, the graph at theright is free of proxy discrimination as the edge between A and its proxy Px (here Experience) hasbeen removed along with all incoming arrows of Px (the edge between Education and Experience).Figure 5: Two possible graphs to describe proxy discrimination. If we consider Experience as a proxy of the sensitiveattribute A, the graph at the left exhibits a potential proxy discrimination (along the dashed edge between A andExperience), but not in the right one.42915Other causal based fairness notions include direct/indirect effect [91], FACE/FACT [92], counter-factual effects [93], counterfactual error rates [94], and path-specific counterfactual fairness [95, 96].As a general rule, causality-based fairness notions can be used as long as the causal relationshipsbetween the attributes are identified and represented using a reliable and plausible causal graph. Theconstruction of the causal graph requires typically domain-specific expertise and can be validatedby existing datasets. In practice, however, causality-based fairness notions are recommended in920at least two notable scenarios. The first scenario is when the legal framework of the case at handis disparate treatment. In such framework, to win a discrimination case, the plaintiff must show925that the defendant has used (directly or indirectly (via proxy)) the sensitive attribute A to take thediscriminatory decision ˆY . In other words, she must prove that the variable A is a cause of ˆY whilethe causal effect of A on ˆY is central to all causal-based fairness notions mentioned above. Thesecond scenario is when there is confounding between A and ˆY . That is, there is a covariate whichis a common cause of A and ˆY . Such scenario can lead to statistical anomalies such as Simpson’sparadox [97, 88] where the statistical conclusions drawn from the sub-populations differ from thatfrom the whole population. The Berkeley admission case [98] is a known real-world example ofsuch statistical anomaly. In such scenarios, any statistical fairness notion which relies solely on930correlation between variables, will fail to detect bias. Hence, causality-based fairness notions arenecessary to appropriately address the problem of fairness.6. RelaxationAlmost all fairness notions presented so far involve a strict equality between quantities, inparticular probabilities. In real scenarios, however, it is more suitable to opt for an approximate or935relaxed form of fairness constraint. The need for relaxation might be due to the impossibility toapply fairness strictly on the application at hand, or merely, it is not a requirement to impose anexact constraint [99].Fairness notion definitions can be relaxed by considering a threshold on the ratio or differencebetween quantities. For instance, the requirement for statistical parity (Section 5.1) can be relaxed940in one of the two following ways:• By allowing the ratio between the predicted acceptance rates of protected and unprotectedgroups to reach the threshold of (cid:15) (a.k.a p% rule defined as satisfying this inequality when43(cid:15) = p/100 [100]):P ( ˆY | A = 0)P ( ˆY | A = 1)≥ 1 − (cid:15) ∀ (cid:15) ∈ [0, 1](24)945For (cid:15) = 0.2, this condition relates to the 80% rule in disparate impact law [101, 64].• By allowing the difference between the predicted acceptance rates of different groups to reacha threshold of (cid:15) [70]:| P ( ˆY | A = 0) − P ( ˆY | A = 1) | ≤ (cid:15) ∀ (cid:15) ∈ [0, 1](25)A notable difference between the two types of relaxation is that the second one (Eq. 25) isinsensitive to which group/individual is the victim of discrimination as the formula is using absolutevalue.950Fairness through awareness can be relaxed using three threshold values, α1, α2, and γ asfollows [102]:(cid:104)P [|M (vi) − M (vj)| > d(vi, vj) + γ ] > α2(cid:105)P≤ α1.(26)The relaxation is allowing M (vi) − M (vj) to exceed d(vi, vj) by a margin of γ, but the fraction ofindividuals differing from them by γ should not exceed α2. If the fraction exceeds α2, the individualis said to be α2-discriminated against.955To allow for more flexibility in the application of fairness notions, other relaxations can beconsidered. For instance, Eq. 2 of conditional statistical parity (Section 5.2) can be modified byrelaxing the strict equality E = e as follows:P ( ˆY = 1 | e − (cid:15) ≤ E ≤ e + (cid:15), A = 0) = P ( ˆY = 1 | e − (cid:15) ≤ E ≤ e + (cid:15), A = 1)(27)7. Classification and tensions960Group fairness notions fall into three classes defined in terms of the properties of joint distributions,namely, independence, separation, and sufficiency [8]. These properties are used in the literatureto prove the existing of tensions between fairness notions, that is, it is impossible to satisfy all44fairness notions simultaneously except in extreme, degenerate, and dump scenarios. Besides, theapplicability of most of fairness notions can be ameliorated by relaxing their strict definitions.9657.1. ClassificationGroup fairness (a.k.a statistical fairness) notions can be characterized by the properties of thejoint distribution of the sensitive attribute A, the label Y , and the classifier ˆY (or score S). Thismeans that we can write them as some statement involving properties of these three random variablesresulting in the three following fairness criteria [72, 8]:970Independence. Independence means that the sensitive feature A is statistically independent of theclassifier ˆY (or the score S).ˆY ⊥ A (or S ⊥ A)(28)In the case of binary classification, independence is equivalent to statistical parity as defined inSection 5.1, Eq. 1. Conditioning on explanatory variables (E) yields a variant of independence asfollows.Conditional independence.ˆY ⊥ A | E (or S ⊥ A | E)(29)This class includes conditional statistical parity defined in Section 5.2, Eq. 2.975Separation. Separation denotes a class of fairness notions satisfying, at different degrees, conditionalindependence between the prediction ˆY and the sensitive attribute A given the actual outcome Y .ˆY ⊥ A | Y(or S ⊥ A | Y )(30)In the case where ˆY is a binary classifier, the formulation of separation is equivalent to thatof the equalized odds (Eq. 3). Equal opportunity (Eq. 4), predictive equality (Eq. 5), balance forpositive class (Eq. 11), and balance for negative class (Eq. 12) are all relaxations of separation.980Some incompatibility results do hold for separation, but do not hold for the relaxations. More onthis in the next section (Section 7.2).45Sufficiency. Sufficiency is a class of fairness notions satisfying, at different degrees, conditionalindependence between the target variable Y and the sensitive attribute A given the prediction ˆY .Y ⊥ A | ˆY(or Y ⊥ A | S)(31)In the case of binary classification, strict sufficieny corresponds to conditional use accuracy equality985(Eq. 6). Using the score S, calibration (Eq. 14), and well-calibration (Eq. 15) can be consideredas sufficiency [9]. Relaxation of sufficiency yields to predictive parity (Eq. 7) which also does notsatisfy exactly the same incompatibility result as sufficiency (Section 7.2).Table 15 lists all fairness notions along with their classification.46Table 15: Classification of fairness notions. (∗ notion newly defined in this paper)Fairness NotionRef.FormulationStatistical parity[70]P ( ˆY | A = 0) = P ( ˆY | A = 1)ClassificationTypeIndependence(equivalent or relaxed(cid:70))Conditional statistical parity[7]P ( ˆY = 1 | E = e, A = 0) = P ( ˆY = 1 | E = e, A = 1)(cid:70)Equalized oddsEqual opportunityPredictive equalityBalance for positive class[79][7][6]P ( ˆY = 1 | Y = y, A = 0) = P ( ˆY = 1 | Y = y, A = 1) ∀y ∈ {0, 1}P ( ˆY = 1 | Y = 1, A = 0) = P ( ˆY = 1 | Y = 1, A = 1)(cid:70)Separation(equivalent or relaxed(cid:70))P ( ˆY = 1 | Y = 0, A = 0) = P ( ˆY = 1 | Y = 0, A = 1)(cid:70)E[S | Y = 1, A = 0)] = E[S | Y = 1, A = 1](cid:70)Balance for negative classE[S | Y = 0, A = 0] = E[S | Y = 0, A = 1](cid:70)Overall balance*E[S | Y = y, A = 0] = E[S | Y = y, A = 1] ∀y ∈ {0, 1}47Conditional use acc. equality[12]P (Y = y | ˆY = y, A = 0) = P (Y = y | ˆY = y, A = 1) ∀y ∈ {0, 1}Predictive parityNegative predictive parityCalibrationWell-calibration[9]*[9][6]P (Y = 1 | ˆY = 1, A = 0) = P (Y = 1 | ˆY = 1, A = 1)(cid:70)P (Y = 1 | ˆY = 0, A = 0) = P (Y = 1 | ˆY = 0, A = 1)(cid:70)P (Y = 1 | S = s, A = 0) = P (Y = 1 | S = s, A = 1) ∀s ∈ [0, 1]P (Y = 1 | S = s, A = 0) = P (Y = 1 | S = s, A = 1) = s ∀ s ∈ [0, 1]Overall accuracy equalityP ( ˆY = Y |A = 0) = P ( ˆY = Y |A = 1)F NF P (A=0) = F NF P (A=1)−T Ea1,a0 (ˆy) = P (ˆyA←a1 ) − P (ˆyA←a0 )ET Ta1,a0 (ˆy) = P (ˆyA←a1 | a0) − P (ˆy | a0)Treatment equalityTotal fairnessTotal effectEffect of treatment on treatedNo unresolved discriminationNo proxy discriminationCounterfactual fairnessCausal discrimination[12][88][90][71][83]Fairness through awareness[70]D(M (vi), M (vj )) ≤ d(vi, vj )−CausalityP ( ˆY | do(Px = p)) = P ( ˆY | do(Px = p(cid:48))) ∀Px and ∀ p, p(cid:48)P ( ˆYA←a(U ) = y | X = x, A = a) = P ( ˆYA←a(cid:48)(U ) = y | X = x, A = a)X (A=0) = X (A=1) ∧ A(A=0) (cid:54)= A(A=1) ⇒ ˆy(A=0) = ˆy(A=1)Similarity Metric.Sufficiency(equivalent or relaxed(cid:70))Other metricsfrom confusion matrixIndependence, Separationand SufficiencyGroupIndividual9907.2. TensionsIt has been proved that there are incompatibilities between fairness notions. That is, it is notalways possible for an MLDM to satisfy specific fairness notions simultaneously [72, 8, 9, 5, 3]. Inpresence of such incompatibilities, the MLDM should make a trade-off to satisfy some notions onthe expense of others or partially satisfy all of them. Incompatibility26 results are well summarizedby Mitchell et al. [3] as follows:995Statistical parity (independence) versus conditional use accuracy equality (sufficiency). Independenceand sufficiency are incompatible, except when both groups (protected and non-protected) have equalbase rates or ˆY and Y are independent. Note, however, that ˆY and Y should not be independentsince otherwise the predictor is completely useless. More formally,1000ˆY ⊥ A(independence)ANDY ⊥ A | ˆY(strict sufficiency)⇒Y ⊥ AOR(equal base rates)ˆY ⊥ Y(useless predictor)It is important to mention here that this result does not hold for the relaxation of sufficiency, inparticular, predictive parity. Hence, it is possible for the output of an MLDM to satisfy statisticalparity and predictive parity between two groups having different base rates. Such example needs to1005satisfy the following constraints, assuming two groups a and b:T Pa+F PaT Pa+F Pa+F Na+T Na=T Pb+F PbT Pb+F Pb+F Nb+T Nb(independence)T PaT Pa+F Pa= T PbT Pb+F Pb(predictive parity)T Pa+F NaT Pa+F Pa+F Na+T Na(cid:54)=T Pb+F NbT Pb+F Pb+F Nb+T Nb(different base rates)An example scenario satisfying the above constrains is the following:P P Va = 0.4T Pa = 9F Pa = 6T Pb = 12F Pb = 8P P Vb = 0.4baseratea = 0.43F Na = 4 T Na = 11F Nb = 2T Nb = 18baserateb = 0.3526The term impossibility is commonly used as well.481010Statistical parity (independence) versus equalized odds (separation). Similar to the previous result,independence and separation are mutually exclusive unless base rates are equal or the predictor ˆY isindependent from the actual label Y [8]. As mentioned earlier, dependence between ˆY and Y is aweak assumption as any useful predictor should satisfy it. More formally,ˆY ⊥ A(independence)ANDˆY ⊥ A | Y(strict separation)⇒Y ⊥ AOR(equal base rates)ˆY ⊥ Y(useless predictor)1015Considering a relaxation of equalized odds, that is, equal opportunity or predictive equality,breaks the incompatibility between independence and separation. An MLDM whose output satisfiesindependence and equal opportunity, but with different base rates between groups should satisfy thefollowing constraints:T Pa+F PaT Pa+F Pa+F Na+T Na=T Pb+F PbT Pb+F Pb+F Nb+T Nb(independence)T PaT Pa+F Na= T PbT Pb+F Nb(equal opportunity)T Pa+F NaT Pa+F Pa+F Na+T Na(cid:54)=T Pb+F NbT Pb+F Pb+F Nb+T Nb(different base rates)1020An example scenario satisfying the above constrains is the following:T P Ra = 0.6T Pa = 9 F Pa = 3T Pb = 12 F Pb = 6T P Rb = 0.6baseratea = 0.55F Na = 2 T Na = 6F Nb = 8T Nb = 4baserateb = 0.71Equalized odds (separation) vs conditional use accuracy equality (sufficiency). Separation andsufficiency are mutually exclusive, except in the case where groups have equal base rates. Moreformally:1025ˆY ⊥ A | Y(strict separation)ANDY ⊥ A | ˆY(strict sufficiency)⇒Y ⊥ A(equal base rates)Both separation and sufficiency have relaxations. Considering only one relaxation will only dropthe incompatibility for extreme and degenerate cases. For example, predictive parity (relaxed versionof sufficiency) is still incompatible with separation (equalized odds), except in the following threeextreme cases [9]:491030• both groups have equal base rates.• both groups have F P R = 0 and P P V = 1.• both groups have F P R = 0 and F N R = 1.The incompatibility disappears completely when considering relaxed versions of both separationand sufficiency. For example, the following scenario satisfies equal opportunity (relaxed version of1035separation) and predictive parity (relaxed version of sufficiency) while base rates are different inboth groups:T P Ra = 0.4P P Va = 0.75baseratea = 0.6T Pa = 9 F Pa = 6T Pb = 12 F Pb = 8F Na = 3 T Na = 2F Nb = 4T Nb = 8T P Rb = 0.4P P Vb = 0.75baserateb = 0.57.3. Group vs individual fairnessCompared to individual fairness notions, the main concern for group fairness notions is that1040they are only suited to a limited number of coarse-grained, predetermined protected groups basedon some sensitive attribute (e.g. gender, race, etc.). Hence group fairness notions are not suitablein presence of intersectionality [63] where individuals are often disadvantaged by multiple sourcesof discrimination: their race, class, gender, religion, and other inner traits. Typically, statisticalfairness can only be applied across a small number of coarsely defined groups, and hence failing1045to identify discrimination on structured subgroups (e.g. single women) known also as “fairnessgerrymandering” [103]. A simple alternative might be to apply statistical fairness across everypossible combination of protected attributes. There are at least two problems to this approach.First, this can lead to an impossible statistical problem with the large number of sub-groups whichmay lead in turn to overfitting. Second, groups which are not (yet) defined in anti-discrimination1050law may exist and may need protection [104]. Another issue with group fairness notions is theirsusceptibility to masking. Most of group fairness notions can be gamed by adding arbitrarily selectedsamples to satisfy the fairness notion formula, that is, to just “make up the numbers”.Compared to group fairness notions, individual fairness notions have the drawback that they canresult in “unjust disparities in outcomes between groups” [105]. For illustration, consider the example1055in Table 16 where fairness through awareness is satisfied (Eq. 17) whereas statistical parity Eq. (1) is50not. Fairness through awareness is satisfied since for every pair of candidates, the distance betweenthe probability distributions on the outcomes (M ()) is smaller than the distance between the pair ofcandidates. On the other hand, if the hiring threshold is 0.6, only one female candidate (F 2) willbe hired as she has a probability of acceptance P ( ˆY = 1) = 0.8 > 0.6 whereas all male candidateswill be hired. Another important issue for similarity-based individual fairness (e.g. fairness through1060awareness) is the difficulty to obtain a similarity value between every pair of individuals. Forexample, even with the assumption that the similarity can be quantified between all individuals inthe training data, it might be challenging to generalize to new individuals [105].Table 16: A job hiring scenario satisfying fairness through awareness (Eq. 17) but not statistical parity (Eq. 1) for athreshold of 0.6. The second row (M ()) indicates the probability distribution on the outcomes. For example, for thefirst female applicant F 1, P ( ˆY = 1) = 0.58 and P ( ˆY = 0) = 0.42. Each cell at the left of the shaded table’s diagonalrepresents a distance between a pair of applicants. Those at the right represent the distance between probabilitydistributions on the outcomes.F1[0.58, 0.42]F2[0.8, 0.2]0.170.210.060.10.20.050.220.150.010.17M()F1F2F3M1M2M3F3[0.55, 0.45]0.0210.190.10.30.08d(vi, vj)M1[0.65, 0.35]0.0510.110.070.150.05M2[0.81, 0.19]0.180.0080.200.120.17M3[0.61, 0.39]0.020.150.040.0290.15))jv(M,)iv(MD(Several researchers assume that both group and individual fairness are prominent, yet, conflicting1065and suggest approaches to minimize the trade-offs between these notions [105]. For instance, [10]define two different worldviews, WYSIWYG and WAE. The WYSIWYG (What you see is what youget) worldview assumes that the unobserved (construct) space and observed space are essentiallythe same while the WAE (we’re all equal) worldview implies that there are no innate differencesbetween groups of individuals based on certain potentially discriminatory characteristics. These two1070worldviews highlight the tension between group and individual fairness. For instance, in the jobhiring example, the WYSIWYG might be the assumption that attributes like education level andjob experience (which belong to the observed space) correlate well with the applicant’s seriousnessor hardworking (properties of the construct space). This is to say that there is some way to combinethese two spaces to correctly compare true applicant aptitude for the job. On the other hand, the1075WAE claims that all groups will have almost the same distribution in the construct space of inherent51abilities (here, seriousness and hardworking), chosen as important inputs to the decision makingprocess. The idea is that any difference in the groups’ performance (e.g., academic achievementor education level) is due to factors outside their individual control (e.g., the quality of theirneighborhood school) and should not be taken into account in the decision making process. Thus,1080the choice between fairness notions must be based on an explicit choice in worldviews.8. Diagram and discussion5253Figure 6: Fairness notions applicability decision diagram.With the large number of fairness notions and the subtle resemblance between MLDM scenarios,deciding about which fairness notion to use is not a trivial task. More importantly, selecting andusing a fairness notion in a scenario inappropriately may detect unfairness in an otherwise fair1085scenario, or the opposite, i.e., fail to identify unfairness in an unfair scenario.One of the objectives of this survey is to systemize the selection procedure of fairness notions.This is achieved by identifying a set of fairness-related characteristics (Section 4) of the scenario athand and then use them to recommend the most suitable fairness notion for that specific scenario.The proposed systemized selection procedure is illustrated in the decision diagram of Figure 6. The1090diagram is called “decision diagram” and not “decision tree” for the following reason. In typicaldecision trees, every leaf corresponds to a single decision, which is a fairness notion that should beused. However, the diagram in Figure 6 is designed such that every node indicates which notionsare recommended, which notions to be avoided, and which notions must not be used. In addition, ifa notion is not mentioned along the path, it means, it can be safely used.1095The diagram is composed of four types of nodes:• Decision node (diamond): based on fairness-related characteristics (Section 4).• Recommended node (rectangle): a leaf node indicating that the fairness notion is suitableto be used given all fairness-related characteristics in the path to that node.• Warning node (triangle): indicates that the fairness notion(s) is/are not recommended1100in all the branch in the right of the node. This node can appear in the middle of the edgebetween two decision nodes.• Must-not node (circle): the fairness notion must not be used.To illustrate how the diagram should be interpreted, consider the recommended node predictiveparity (node 38). According to the diagram, predictive parity is recommended in the scenario where1105the legal framework is disparate impact (decision node 1), intersectionality and/or masking areunlikely (decision node 2), there is no evidence that representation bias is likely (decision node 2),standards do not exist (decision node 4), ground-truth is available or outcome Y is reliable (decisionnode 11), historical and measurement bias are unlikely (decision node 11), fairness is more sensitiveto precision rather than recall (decision node 22), the prediction threshold is typically fixed (decision1110node 24) and the emphasis is on false positives rather than false negatives (decision node 28). In54that particular scenario, equal opportunity must not be used (must-not node 45) because fairnessin this scenario is particularly sensitive to false positives, while equal opportunity is completelyinsensitive to false positives. Similarly, negative predictive parity must not be used (must-not node46) as fairness is sensitive to precision rather than recall. The warning node 17 along the same path1115indicates that statistical parity is not suitable in this scenario. Finally, any fairness notion for whichthere is no a warning node or a must-not node along the path of the scenario can be used in thisscenario. For instance, all individual fairness notions can be used.As concrete example of situations where predictive parity (node 38) is recommended, consider thefollowing. In situations when the outcome is influenced by the decision, some statistical quantities1120(e.g. FN, TN, etc.) are unlikely to be observed, and hence, any fairness notion that is defined interms of those quantities is not suitable to use. For example, in real-world cases of loan-granting,a loan application which is predicted to be defaulting, will not be approved. Consequently, bothnegative statistics (true negative (TN) and false negative (FN)) will not be typically observed.Hence, fairness notions such as equalized odds and equality of opportunity cannot be used as they1125are defined in terms of TN and FN. In such cases, predictive parity (node 38) is recommended.Node 1: Assessing fairness is very often performed in the context of a legal case where a plaintiffis filing a claim against a party that is using an MLDM. According to real-world legislation, inparticular, the American anti-discrimination law, this can fall into one the two legal frameworks,namely, disparate impact and disparate treatment. If the plaintiff is filing the claim under the1130disparate impact framework, she can prove the liability of the defendant by using an observationalgroup or individual fairness notion as the goal is to show that the practices and policies used bythe defendant are facially neutral but have a disproportionately adverse impact on the protectedclass [64]. If, however, the plaintiff is filing a claim under the disparate treatment framework,observational fairness notions are often not enough to prove the liability of the defendant as the goal1135is to show that the defendant has used the sensitive attribute to take the discriminatory decision.The recommended fairness notions in that case are causality-based (recommended node 3) since allof them are expressed in terms of the causal effect of the sensitive attribute on the prediction.Node 2: As explained above, any unintentional type of bias can also be "orchestrated" inten-tionally by decision makers with prejudicial views. For instance, decision makers can purposefully1140bias the data collection step to ensure that the MLDM remains less favorable to protected classes.To reliably assess the bias in presence of such masking attempts, all group fairness notions should55be avoided as they are defined in terms of statistics about the different sub-populations and hencecan more easily be gamed by prejudicial decision makers. Intersectionality is similar to masking asboth lead to a discrimination which is difficult to detect using statistical measures and consequently1145requires more fine-grained measures. Therefore individual fairness notions are recommended inpresence of both criteria (nodes 9 and 18).Nodes 2, 3, and 11:In case one or more sources of bias are suspected ahead of time(before assessing fairness), the information can help warn against the use of some fairness notions.If representation bias is likely, the performance (accuracy) of the MLDM on under-represented1150categories will often be worse. Such disparity in performance between groups may lead to unreliablefairness assessment in case a group fairness notion is used, in particular disparate mistreatmentnotions (grayed section of the diagram). In such case, individual fairness notions can assess fairnessmore reliably provided that measurement bias is not likely (node 2). A suspicion of historical ormeasurement bias means that the features (X) and/or the label (Y) are not reliable. All group1155fairness notions using the label Y (disparate mistreatment) as well as individual notions are notrecommended in that case. Statistical parity is recommended in such situation. Finally, in presence ofeither aggregation, evaluation, or deployment bias, causality-based fairness notions are recommended.The reason is that the interventional and counterfactual quantities used in the definitions of thesenotions go beyond mere correlations and hence allow to assess fairness more reliably in presence of1160such bias. For instance, Coston et al. [23] propose counterfactual formulations of fairness metrics toproperly account for the effect of intervention (decision) on the outcome. Such effect is a type ofdeployment bias.Node 3: As discussed in Section 5.12, there are several notions that use causal reasoning toassess fairness. Counterfactual fairness is suitable in case a fine-grained assessment is required as the1165equality of Eq. 21 conditions on all features (X). Counterfactual fairness, however, requires strongassumptions to be applicable in real scenarios (the availability of the full causal model includingthe latent variables distributions). Total effect (TE), effect of treatment on treated (ETT), andno proxy discrimination (nodes 13, 14 and 10), on the other hand, require a weaker assumptionto be applicable, namely, the identifiability of the causal quantities used in their definitions. No1170proxy discrimination is recommended in presence of potential proxies, however, the identification ofproxy variables requires a domain expertise of the application at hand. Finally, in case there arevariables in the causal graph which are correlated with the sensitive attribute but in a manner that56is accepted as nondiscriminatory, no unresolved discrimination is recommended while the remainingcausal based fairness notions should be avoided. No unresolved discrimination is easier to apply in1175practice as it only needs the availability of the causal graph.Node 4: To reduce inequality and historical discrimination against sub-populations, in particular,minorities, some states and organizations resort to equality standards and regulations such as thelaws enforced by the US Equal Employment Opportunity Commission [106]. In presence of suchstandards, to be deemed fair, an MLDM should satisfy such standards. Consequently, all what1180matters for fairness assessment is the proportion of positive prediction across all groups whichcorresponds to statistical parity.Node 17: If no standards/regulations exist (node 4) and either the ground truth exists or theoutcome label Y is available (node 11), statistical parity is not recommended (node 17) as it canlead to misleading results such as detecting unfairness in an otherwise fair scenario or failing to1185identify fairness in an unfair scenario. For instance, in stop-and-frisk real world scenario appliedin New York city starting 1990 [80]27, the ground truth is available as by frisking an individual, apolice officer can know with certainty the presence or no of illegal substance. In such case, one orseveral disparate mistreatment notions (nodes 30-41) are more suitable to assess fairness.Nodes 22-47: The bulk of Figure 6 is dedicated for disparate mistreatment fairness notions1190and the criteria leading to each one of them. These notions define fairness in terms of the disparityof misclassification rates among the different groups in the population. Based on their definitions,selecting the most suitable notion to use depends on four citeria, namely, whether the emphasisis on precision or recall (node 22), whether the threshold is fixed or floating (nodes 23 and 24),whether the emphasis is on false negatives or false positives (nodes 26 and 28), and finally whether1195the emphasis is on the positive or negative class (node 27). As some notions focus only on either FPor FN (nodes 31, 32, 38, and 39), any notion that is insensitive to either FP or FN must not beused (nodes 42 - 47).The diagram may be misleading if it is interpreted very categorically. This occurs when a userof the diagram navigates it and ends up using the recommended fairness notion without considering1200other important elements specific to the scenario at hand. The diagram can be misleading also whenit is not clear which branch to take in a decision node. For example, the question in decision node27Assuming the absence of measurement bias.5722 (emphasis on precision or recall?) is difficult to answer categorically in several scenarios. Thedecision nodes 4, 21, 12, and even 2, are typically easier to navigate, but can be challenging to settlein a number of scenarios. Moreover, in presence of measurement bias, the values of some features1205and even the outcome label may not be reliable which can make the diagram navigation morechallenging. A potential solution would be to label one of the branches as default (to be followedwhen the answer is not clear), but this can, often result in a suboptimal decision. In summary, thediagram should be considered as guide and should never be used to supersede important elementsspecific to the scenario at hand.58Table 17: Correspondence between Fairness notions and the selection criteria: C1: disparate impact , C2: disparate treatment , C3: intersectionality/masking,C4: historical bias, C5: representational bias, C6: measurement bias, C7: aggregation/evaluation/deployment bias, C8: standards, C9: ground truth available,C10: y not reliable, C11: explanatory variables, C12: precision, C13: recall, C14: FP, C15: FN, C16: causal graph available, C17: threshold floating.Notation: (cid:51): recommended, (cid:66): warning, (cid:55): must not, −: insensitive.Legal FrameSuspected source of biasEmphasis onEmphasis onCriterionFairness notionStatistical parityC1C2C3C4C5C6C7C8C9C10 C11 C12 C13C14 C15C16 C17(cid:51) (cid:66)(cid:66) (cid:51) (cid:66) (cid:66) (cid:66)(cid:51) (cid:66) (cid:51) (cid:66) − −− −− (cid:66)Conditional statistical parity(cid:51) (cid:66)(cid:66) (cid:51) (cid:66) (cid:66) (cid:66)− (cid:66) (cid:51)(cid:51)− −− −− (cid:66)Equalized odds(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:66) (cid:51)Equal opportunity(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:66) (cid:51)Predictive equality(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:66) (cid:51)59Balance for positive class(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:66) (cid:51)Balance for negative class(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:66) (cid:51)Overall balance(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:66) (cid:51)Conditional use acc. equality(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:51) (cid:66)Predictive parity(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:51) (cid:66)Negative predictive parity(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:51) (cid:66)(cid:51)(cid:55)(cid:51)(cid:55)(cid:51)(cid:51)(cid:51)(cid:51)(cid:55)(cid:51)(cid:51)(cid:51)(cid:51)(cid:55)(cid:51)(cid:51)(cid:55)(cid:51)− (cid:66)− (cid:66)− (cid:66)− (cid:51)− (cid:51)− (cid:51)− (cid:66)− (cid:66)− (cid:66)CalibrationWell-calibration(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:51) (cid:66)− −− (cid:51)(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:51) (cid:66)− −− (cid:51)Overall accuracy equality(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) (cid:51)(cid:51)Treatment equality(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) − −Total fairness(cid:51) (cid:66)(cid:66) (cid:66) (cid:66) (cid:66) (cid:66)− (cid:51) (cid:66) (cid:66) − −(cid:51)(cid:51)(cid:51)(cid:51)(cid:51)(cid:51)− (cid:66)− −− (cid:66)Causal discrimination(cid:51) (cid:66)(cid:51) (cid:66) (cid:51) (cid:66) −− (cid:51) (cid:66) (cid:66) − −− −− −Fairness through awareness(cid:51) (cid:66)(cid:51) (cid:66) (cid:51) (cid:66) (cid:66)− (cid:51) (cid:66) − − −− −− −Total effect− (cid:51)(cid:66) − − − (cid:51)− − − − − −− −(cid:51) −Effect of treatment on treated− (cid:51)(cid:66) − − − (cid:51)− − − − − −− −(cid:51) −Counterfactual fairness− (cid:51)(cid:51)− (cid:51) − (cid:51)− − − − − −− −(cid:51) −No unresolved discrimination− (cid:51)(cid:66) − − − (cid:51)− − − (cid:51)− −− −(cid:51) −No proxy discrimination− (cid:51)(cid:66) − − − (cid:51)− − − − − −− −(cid:51) −1210Finally, Table 17 states explicitly the relationship between every selection criterion and everyfairness notion. The table uses four symbols, namely, recommended ((cid:51)), warning ((cid:66)), must-not((cid:55)), and insensitive (−). Insensitive means that the choice of the fairness notion is independent ofthe selection criterion.9. Conclusion1215With the increasingly large number of fairness notions considered in the relatively new field offairness in ML, selecting a suitable notion for a given MLDM (machine learning decision making)becomes a non-trivial task. There are two contributing factors. First, the boundaries between thedefined notions are increasingly fuzzy. Second, applying inappropriately a fairness notion may reportdiscrimination in an otherwise fair scenario, or vice versa, fail to identify discrimination in an unfair1220scenario. This survey tries to address this problem by identifying fairness-related characteristicsof the scenario at hand and then use them to recommend and/or discourage the use of specificfairness notions. The main contribution of this survey is to systemize the selection process based ona decision diagram. Navigating the diagram will result in recommending and/or discouraging theuse of fairness notions.1225One of the main objectives of this survey is to bridge the gap between the real-world use casescenarios of automated (and generally unintentional) discrimination and the mostly technical tacklingof the problem in the literature. Hence, the survey can be of particular interest to civil right activists,civil right associations, anti-discrimination law enforcement agencies, and practitioners in fieldswhere automated decision making systems are increasingly used.1230More generally, in real-scenarios, there are still two important obstacles to address the unfairnessproblem in automated decision systems. First, the victims of such systems are, very often, membersof minority groups with limited influence in the public sphere. Second, automated decision systemsare geared towards efficiency (typically money) and to optimize profit, they are designed to sacrificethe outliers as tolerable collateral damage. After all, the system is benefiting most of the population1235(employers finding ideal candidates, banks giving loans to minimum risk borrowers, a society withrecidivists locked in prisons, etc.).60AcknowledgementThe work of Catuscia Palamidessi was supported by the European Research Council (ERC)under the European Union’s Horizon 2020 research and innovation programme. Grant agreement1240835294.[1] S. Lowry, G. Macpherson, A blot on the profession, British medical journal (Clinical researched.) 296 (6623).[2] C. O’Neill, Weapons of math destruction: How Big Data Increases Inequality and ThreatensDemocracy, Crown Publishers, 2016.1245[3] S. Mitchell, E. Potash, S. Barocas, A. D’Amour, K. Lum, Prediction-based decisions andfairness: A catalogue of choices, assumptions, and definitions, arXiv preprint arXiv:1811.07867.[4] P. Gajane, M. Pechenizkiy, On formalizing fairness in prediction with machine learning, arXivpreprint arXiv:1710.03184.[5] M. B. Zafar, I. Valera, M. Gomez Rodriguez, K. P. Gummadi, Fairness beyond disparate1250treatment & disparate impact: Learning classification without disparate mistreatment, in:Proceedings of the 26th international conference on world wide web, 2017, pp. 1171–1180.[6] J. Kleinberg, S. Mullainathan, M. Raghavan, Inherent Trade-Offs in the Fair Determination ofRisk Scores, in: C. H. Papadimitriou (Ed.), 8th Innovations in Theoretical Computer ScienceConference (ITCS 2017), Vol. 67 of Leibniz International Proceedings in Informatics (LIPIcs),1255Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, Dagstuhl, Germany, 2017, pp. 43:1–43:23.doi:10.4230/LIPIcs.ITCS.2017.43.URL http://drops.dagstuhl.de/opus/volltexte/2017/8156[7] S. Corbett-Davies, E. Pierson, A. Feller, S. Goel, A. Huq, Algorithmic decision making andthe cost of fairness, in: Proceedings of the 23rd ACM SIGKDD International Conference on1260Knowledge Discovery and Data Mining, 2017, pp. 797–806.[8] S. Barocas, M. Hardt, A. Narayanan, Fairness and Machine Learning, fairmlbook.org, 2019,http://www.fairmlbook.org.61[9] A. Chouldechova, Fair prediction with disparate impact: A study of bias in recidivismprediction instruments, Big data 5 (2) (2017) 153–163.1265[10] S. A. Friedler, C. Scheidegger, S. Venkatasubramanian, On the (im) possibility of fairness,arXiv preprint arXiv:1609.07236.[11] I. Zliobaite, A survey on measuring indirect discrimination in machine learning, arXiv preprintarXiv:1511.00148.[12] R. Berk, H. Heidari, S. Jabbari, M. Kearns, A. Roth, Fairness in criminal justice risk1270assessments: The state of the art, Sociological Methods & Research.[13] S. Verma, J. Rubin, Fairness definitions explained, in: 2018 IEEE/ACM International Work-shop on Software Fairness (FairWare), IEEE, 2018, pp. 1–7.[14] A. Asuncion, D. Newman, Uci machine learning repository (2007).[15] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, A. Galstyan, A survey on bias and fairness1275in machine learning, arXiv preprint arXiv:1908.09635.[16] S. A. Friedler, C. Scheidegger, S. Venkatasubramanian, S. Choudhary, E. P. Hamilton, D. Roth,A comparative study of fairness-enhancing interventions in machine learning, in: Proceedingsof the Conference on Fairness, Accountability, and Transparency, 2019, pp. 329–338.[17] K. Makhlouf, S. Zhioua, C. Palamidessi, On the applicability of machine learning fairness1280notions, in: Bias and Fairness in AI Workshop at ECMLPKDD 2020, 2020.[18] T. Kamishima, S. Akaho, J. Sakuma, Fairness-aware learning through regularization approach,in: 2011 IEEE 11th International Conference on Data Mining Workshops, IEEE, 2011, pp.643–650.[19] A. Agarwal, M. Dudik, Z. S. Wu, Fair regression: Quantitative definitions and reduction-based1285algorithms, in: International Conference on Machine Learning, PMLR, 2019, pp. 120–129.[20] L. E. Celis, D. Straszak, N. K. Vishnoi, Ranking with fairness constraints, in: 45th InternationalColloquium on Automata, Languages, and Programming (ICALP 2018), Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.62[21] S. Jabbari, M. Joseph, M. Kearns, J. Morgenstern, A. Roth, Fairness in reinforcement learning,1290in: International Conference on Machine Learning, PMLR, 2017, pp. 1617–1626.[22] J. Kleinberg, J. Ludwig, S. Mullainathan, Z. Obermeyer, Prediction policy problems, AmericanEconomic Review 105 (5) (2015) 491–95.[23] A. Coston, A. Mishler, E. H. Kennedy, A. Chouldechova, Counterfactual risk assessments,evaluation, and fairness, in: Proceedings of the 2020 Conference on Fairness, Accountability,1295and Transparency, 2020, pp. 582–593.[24] L. Weber, E. Dwoskin, Are workplace personality tests fair?, The Wall Street Journalhttps://www.wsj.com/articles/are-workplace-personality-tests-fair-1412044257.[25] P. Lahoti, K. P. Gummadi, G. Weikum, ifair: Learning individually fair data representations foralgorithmic decision making, in: 2019 IEEE 35th International Conference on Data Engineering1300(ICDE), IEEE, 2019, pp. 1334–1345.[26] J. Leber, The machine-readable workforce, MIT Technology Reviewhttps://www.technologyreview.com/2013/05/27/178320/the-machine-readable-workforce.[27] A. Waters, R. Miikkulainen, Grade: Machine learning support for graduate admissions, AIMagazine 35 (1) (2014) 64–64.1305[28] M. V. Santelices, M. Wilson, Unfair treatment? the case of freedle, the sat, and the standard-ization approach to differential item functioning, Harvard Educational Review 80 (1) (2010)106–134.[29] K. Hao. The uk exam debacle reminds us that algorithms can’t fix broken systems [online](August 2020).1310[30] COMPAS, Compas, https://www.equivant.com/northpointe-risk-need-assessments/(2020).[31] A. Majdara, M. R. Nematollahi, Development and application of a risk assessment tool,Reliability Engineering & System Safety 93 (8) (2008) 1130–1137.[32] J. R. Meyers, F. Schmidt, Predictive validity of the structured assessment for violence risk in1315youth (savry) with juvenile offenders, Criminal Justice and Behavior 35 (3) (2008) 344–355.63[33] predPol, predpol, https://www.predpol.com (2020).[34] J. Angwin, J. Larson, S. Mattu, L. Kirchner, Machine bias. propublica, See https://www.propublica. org/article/machine-bias-risk-assessments-in-criminal-sentencing.[35] A. Chalfin, O. Danieli, A. Hillis, Z. Jelveh, M. Luca, J. Ludwig, S. Mullainathan, Productivity1320and selection of human capital with machine learning, American Economic Review 106 (5)(2016) 124–27.[36] M.Rhee,Impact:Thedcpsevaluationandfeedbacksys-temforschool-basedpersonnel,https://dcps.dc.gov/page/impact-dcps-evaluation-and-feedback-system-school-based-personnel (2019).1325[37] K. Quick,Theunfaireffectsofimpactonteachers withthetough-estjobs,TheCenturyFoundationhttps://tcf.org/content/commentary/the-unfair-effects-of-impact-on-teachers-with-the-toughest-jobs/?agreed=1.[38] R. Vaithianathan, T. Maloney, E. Putnam-Hornstein, N. Jiang, Children in the public benefitsystem at risk of maltreatment: Identification via predictive modeling, American journal of1330preventive medicine 45 (3) (2013) 354–359.[39] V. Eubanks, Automating inequality: How high-tech tools profile, police, and punish the poor,St. Martin’s Press, 2018.[40] E. K. Spanakis, S. H. Golden, Race/ethnic difference in diabetes and diabetic complications,Current diabetes reports 13 (6) (2013) 814–823.1335[41] Z. Obermeyer, B. Powers, C. Vogeli, S. Mullainathan, Dissecting racial bias in an algorithmused to manage the health of populations, Science 366 (6464) (2019) 447–453.[42] H. Suresh, J. V. Guttag, A framework for understanding unintended consequences of machinelearning, arXiv preprint arXiv:1901.10002.[43] D. Jannach, M. Zanker, A. Felfernig, G. Friedrich, Recommender systems: an introduction,1340Cambridge University Press, 2010.64[44] A. Lambrecht, C. E. Tucker, Algorithmic bias? an empirical study into apparent gender-based discrimination in the display of stem career ads, An Empirical Study into ApparentGender-Based Discrimination in the Display of STEM Career Ads (March 9, 2018).[45] A. Datta, M. C. Tschantz, A. Datta, Automated experiments on ad privacy settings: A tale of1345opacity, choice, and discrimination, Proceedings on privacy enhancing technologies 2015 (1)(2015) 92–112.[46] Google, Google ad setting, http://www.goolge.com/settings/ads.[47] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, S. Thrun, Dermatologist-level classification of skin cancer with deep neural networks, Nature 542 (7639) (2017) 115–118.1350[48] A. Dehghan, E. G. Ortiz, G. Shu, S. Z. Masood, Dager: Deep age, gender and emotionrecognition using convolutional neural network, arXiv preprint arXiv:1702.04280.[49] C. Fabian Benitez-Quiroz, R. Srinivasan, A. M. Martinez, Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild, in:Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp.13555562–5570.[50] R. Srinivasan, J. D. Golomb, A. M. Martinez, A neural basis of facial action recognition inhumans, Journal of Neuroscience 36 (16) (2016) 4434–4442.[51] X. Wu, X. Zhang, Automated inference on criminality using face images, arXiv preprintarXiv:1611.04135 (2016) 4038–4052.1360[52] C. Garvie, The perpetual line-up: Unregulated police face recognition in America, GeorgetownLaw, Center on Privacy & Technology, 2016.[53] J. Buolamwini, T. Gebru, Gender shades: Intersectional accuracy disparities in commercialgender classification, in: Conference on fairness, accountability and transparency, 2018, pp.77–91.1365[54] Y. R. Shrestha, Y. Yang, Fairness in algorithmic decision-making: Applications in multi-winnervoting, machine learning, and recommender systems, Algorithms 12 (9) (2019) 199.65[55] Y. Zhao, M. K. Hryniewicki, F. Cheng, B. Fu, X. Zhu, Employee turnover prediction withmachine learning: A reliable approach, in: Proceedings of SAI intelligent systems conference,Springer, 2018, pp. 737–758.1370[56] A. M. Esmaieeli Sikaroudi, R. Ghousi, A. Sikaroudi, A data mining approach to employeeturnover prediction (case study: Arak automotive parts manufacturing), Journal of Industrialand Systems Engineering 8 (4) (2015) 106–121.[57] R. S. Sexton, S. McMurtrey, J. O. Michalopoulos, A. M. Smith, Employee turnover: a neuralnetwork solution, Computers & Operations Research 32 (10) (2005) 2635–2651.1375[58] D. Alao, A. Adeyemo, Analyzing employee attrition using decision tree algorithms, Computing,Information Systems, Development Informatics and Allied Research Journal 4.[59] P. T. M. Marope, P. J. Wells, E. Hazelkorn, Rankings and accountability in higher education:Uses and misuses, Unesco, 2013.[60] R. A. QC, D. Masters, Regulating For an Equal AI: A New Role for Equality Bodies,1380EQUINET: the European Network of Equality Bodies, 2020, iSBN: 978-92-95112-35-3.https://equineteurope.org/wp-content/uploads/2020/06/ai_report_digital.URLpdf[61] M. K. Dodson, W. A. Cliby, G. L. Keeney, M. F. Peterson, K. C. Podritz, Skene’s glandadenocarcinoma with increased serum level of prostate-specific antigen, Gynecologic oncology138555 (2) (1994) 304–307.[62] W. Dieterich, C. Mendoza, T. Brennan, Compas risk scales: Demonstrating accuracy equityand predictive parity, Northpointe Inc.[63] K. Crenshaw, Mapping the margins: Intersectionality, identity politics, and violence againstwomen of color, Stan. L. Rev. 43 (1990) 1241.1390[64] S. Barocas, A. D. Selbst, Big data’s disparate impact, Calif. L. Rev. 104 (2016) 671.[65] V. Zarya, The share of female ceos in the fortune 500 dropped by 25% in 2018, https://fortune.com/2018/05/21/women-fortune-500-2018/ (2018).66[66] K. Crawford, Think again: big data. why the rise of machines isn’t all it’s cracked up to be,Foreign Policy 10.1395[67] Y.-M. Li, C. Peng, J.-G. Zhang, W. Zhu, C. Xu, Y. Lin, X.-Y. Fu, Q. Tian, L. Zhang, Y. Xiang,et al., Genetic risk factors identified in populations of european descent do not improve theprediction of osteoporotic fracture and bone mineral density in chinese populations, Scientificreports 9 (1) (2019) 1–9.[68] S. Corbett-Davies, S. Goel, The measure and mismeasure of fairness: A critical review of fair1400machine learning, arXiv preprint arXiv:1808.00023.[69] Z. Lipton, J. McAuley, A. Chouldechova, Does mitigating ml’s impact disparity requiretreatment disparity?, in: Advances in Neural Information Processing Systems, 2018, pp.8125–8135.[70] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, R. Zemel, Fairness through awareness, in:1405Proceedings of the 3rd innovations in theoretical computer science conference, 2012, pp.214–226.[71] M. J. Kusner, J. Loftus, C. Russell, R. Silva, Counterfactual fairness, in: Advances in NeuralInformation Processing Systems, 2017, pp. 4066–4076.[72] S. Barocas, M. Hardt, A. Narayanan, Fairness in machine learning, NIPS Tutorial.1410[73] I. Zliobaite, On the relation between accuracy and fairness in binary classification, arXivpreprint arXiv:1505.05723.[74] C. Simoiu, S. Corbett-Davies, S. Goel, et al., The problem of infra-marginality in outcometests for discrimination, The Annals of Applied Statistics 11 (3) (2017) 1193–1216.[75] T. J. VanderWeele, M. A. Hernán, Results on differential and dependent measurement error1415of the exposure and the outcome using signed directed acyclic graphs, American journal ofepidemiology 175 (12) (2012) 1303–1310.[76] J. E. Johndrow, K. Lum, et al., An algorithm for removing sensitive information: applicationto race-independent recidivism prediction, The Annals of Applied Statistics 13 (1) (2019)189–220.671420[77] M. Hardt, E. Price, N. Srebro, Equality of opportunity in supervised learning, in: Advances inneural information processing systems, 2016, pp. 3315–3323.[78] F. Kamiran, I. Zliobaite, T. Calders, Quantifying explainable discrimination and removingillegal discrimination in automated decision making, Knowledge and information systems(Print) 35 (3) (2013) 613–644.1425[79] M. Hardt, E. Price, N. Srebro, Equality of opportunity in supervised learning, Advances inneural information processing systems 29 (2016) 3315–3323.[80] J. Bellin, The inverse relationship between the constitutionality and effectiveness of new yorkcity stop and frisk, BUL Rev. 94 (2014) 1495.[81] J. Kleinberg, H. Lakkaraju, J. Leskovec, J. Ludwig, S. Mullainathan, Human decisions and1430machine predictions, The quarterly journal of economics 133 (1) (2018) 237–293.[82] P. Garg, J. Villasenor, V. Foggo, Fairness metrics: A comparative analysis, arXiv preprintarXiv:2001.07864.[83] S. Galhotra, Y. Brun, A. Meliou, Fairness testing: testing software for discrimination, in:Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, 2017, pp.1435498–510.[84] M. S. Nikulin, Hellinger distance, Encyclopedia of mathematics 78.[85] M. Kim, O. Reingold, G. Rothblum, Fairness through computationally-bounded awareness, in:Advances in Neural Information Processing Systems, 2018, pp. 4842–4852.[86] J. Pearl, M. Glymour, N. P. Jewell, Causal inference in statistics: A primer, John Wiley &1440Sons, 2016.[87] K. A. Bollen, Structural equations with latent variables wiley, New York.[88] J. Pearl, Causality, Cambridge university press, 2009.[89] I. Shpitser, J. Pearl, Complete identification methods for the causal hierarchy, Journal ofMachine Learning Research 9 (Sep) (2008) 1941–1979.681445[90] N. Kilbertus, M. R. Carulla, G. Parascandolo, M. Hardt, D. Janzing, B. Schölkopf, Avoidingdiscrimination through causal reasoning, in: Advances in Neural Information ProcessingSystems, 2017, pp. 656–666.[91] J. Pearl, Direct and indirect effects, in: Proceedings of the Seventeenth conference on Uncer-tainty in artificial intelligence, 2001, pp. 411–420.1450[92] A. Khademi, S. Lee, D. Foley, V. Honavar, Fairness in algorithmic decision making: Anexcursion through the lens of causality, in: The World Wide Web Conference, 2019, pp.2907–2914.[93] J. Zhang, E. Bareinboim, Fairness in decision-making—the causal explanation formula, in:Proceedings of the AAAI Conference on Artificial Intelligence, 2018.1455[94] J. Zhang, E. Bareinboim, Equality of opportunity in classification: A causal approach, in:Advances in Neural Information Processing Systems, 2018, pp. 3671–3681.[95] S. Chiappa, Path-specific counterfactual fairness, in: Proceedings of the AAAI Conference onArtificial Intelligence, Vol. 33, 2019, pp. 7801–7808.[96] Y. Wu, L. Zhang, X. Wu, H. Tong, Pc-fairness: A unified framework for measuring causality-1460based fairness, in: Advances in Neural Information Processing Systems, 2019, pp. 3404–3414.[97] E. H. Simpson, The interpretation of interaction in contingency tables, Journal of the RoyalStatistical Society: Series B (Methodological) 13 (2) (1951) 238–241.[98] P. J. Bickel, E. A. Hammel, J. W. O’Connell, Sex bias in graduate admissions: Data fromberkeley, Science 187 (4175) (1975) 398–404.1465[99] J. S. Kim, J. Chen, A. Talwalkar, Model-agnostic characterization of fairness trade-offs, arXivpreprint arXiv:2004.03424.[100] M. B. Zafar, I. Valera, M. G. Rodriguez, K. P. Gummadi, Fairness constraints: Mechanismsfor fair classification, arXiv preprint arXiv:1507.05259.[101] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, S. Venkatasubramanian, Certifying1470and removing disparate impact, in: proceedings of the 21th ACM SIGKDD internationalconference on knowledge discovery and data mining, 2015, pp. 259–268.69[102] G. Yona, G. Rothblum, Probably approximately metric-fair learning, in: International Confer-ence on Machine Learning, PMLR, 2018, pp. 5680–5688.[103] M. Kearns, S. Neel, A. Roth, Z. S. Wu, Preventing fairness gerrymandering: Auditing and1475learning for subgroup fairness, in: International Conference on Machine Learning, PMLR,2018, pp. 2564–2572.[104] S. Wachter, B. Mittelstadt, A right to reasonable inferences: re-thinking data protection lawin the age of big data and ai, Colum. Bus. L. Rev. (2019) 494.[105] R. Binns, On the apparent conflict between individual and group fairness, in: Proceedings of1480the 2020 Conference on Fairness, Accountability, and Transparency, 2020, pp. 514–524.[106] Us equal employment opportunity commission, https://www.eeoc.gov/.[107] P. Judea, Causality: models, reasoning, and inference, Cambridge University Press. ISBN 0521 (77362) (2000) 8.Appendix A. Counterfactual probability computation using the three-step procedureFigure A.7: A simple deterministic causal graph for the hiring example.1485The probability of the counterfactual realization P ( ˆYA←a1 | X = x, A = a0) is computed usingthe following three-steps process [107]:1. Abduction: update the probability P (U = u) given the evidence to obtain: P (U = u | X =x, A = a0).2. Action: set the sensitive attribute value A to a1 and update all structural functions of the1490causal graph accordingly.703. Prediction: compute the outcome ( ˆY ) value using the updated probability P (U | X = x, A =a0) and structural functions.To illustrate how counterfactual quantities are computed, consider the simplified deterministic1495version of the hiring example in Figure A.7. For simplicity, the hiring score variable S depends onthe observable variable JE representing job experience and the exogenous variable Uh representinghow hard working the candidate is. The variable JE in turn depends on the observable sensitivevariable A representing the gender (male or female) and the exogenous variable Us representing theseriousness of the candidate. The causal graph in Figure A.7 is represented by the two followingequations:JE = a.A + c.UsS = b.JE + d.Uh(A.1)(A.2)For simplicity of the illustration, assume that both U (Us and Uh) variables are independentand all the parameters of the model (Eq. A.1 and A.2) are known. Assume that the values of thecoefficients are given as follows:a = 0.1,b = 0.7,c = 0.9,d = 0.31500Given this causal model, consider a candidate John who is male (AJohn = 1), with the normalized28job education level JEJohn = 0.6 and a predicted score ˆSJohn = 0.55. Assessing the fairness of thehiring score prediction with respect to gender is achieved through answering the following question:what would John’s hiring score have been had he was of opposite gender (female)? This correspondsto the hiring score of John in the counterfactual world where John is a female ( ˆSJohnA←0this quantity, the three-steps process above is used, namely, abduction, action, and prediction.). To compute1505The abduction step consists in using the evidence (AJohn = 1, JEJohn = 0.6, ˆSJohn = 0.55) toidentify the specific characteristics of John, namely, his level of seriousness and hard working (Us28To keep the computation simple, all variable values are normalized between 0 and 1.71and Uh)29 as follows:U JohnsU Johnh====JEJohn − a.AJohnc59ˆSJohn − b.JEJohnd1330(A.3)The second step consists in setting the sensitive attribute AJohn to the opposite gender (0) and1510updating all equations of the model. This consists in replacing the variable A in Eq. A.1 by 0.The third step consists in the prediction, that is computing ˆSA←0 in the counterfactual world., that is, the job experience of John in a world where JohnThis requires the computation of JEJohnA←0is a female.JEJohnA←0 = a.0 + c.U Johns= 0.5ˆSJohnA←0 = b.JEJohnA←0 + d.U Johnh= 0.48(A.4)(A.5)1515Hence, the hiring score of John had he was female is ˆSJohnA←0 = 0.48 which is considered a violationof counterfactual fairness as the predicted hiring score of John in the original world is ˆSJohn = 0.55.Consider now a female candidate Marie (AM arie = 0), with the a job education level JEM arie =0.61 and a predicted score ˆSM arie = 0.65. The question to investigate is now: what would Marie’shiring score have been had she was male? This boils down to computing ˆSM arieand comparing itA←1with ˆSM arie = 0.65. Applying the three-steps process:29Since this example is deterministic, every individual is characterized by a unique assignment for exogenousvariables Us and Uh. In typical (non-deterministic) scenarios, every individual is assigned a probability distributionover the exogenous variables.72(A.6)(A.7)(A.8)1520Abduction:U M ariesU M arieh====JEM arie − a.AM ariec6190ˆSM arie − b.JEM aried22330Action: replacing the variable A in Eq. A.1 by 1.Prediction:JEM arieA←1= a.1 + c.U M aries= 0.71ˆSM arieA←1= b.JEM arieA←1 + d.U M arieh= 0.72A←1 = 0.72 > ˆSM arie = 0.65 is another violation for counterfactual fairness.ˆSM arie73