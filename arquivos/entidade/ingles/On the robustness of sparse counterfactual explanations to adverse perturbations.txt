Artificial Intelligence 316 (2023) 103840Contents lists available at ScienceDirectArtificial Intelligencejournal homepage: www.elsevier.com/locate/artintOn the robustness of sparse counterfactual explanations to adverse perturbationsMarco Virgolin a,∗a Evolutionary Intelligence Group, Centrum Wiskunde & Informatica, Science Park 123, 1098 XG Amsterdam, the Netherlandsb Department of Mathematics and Geosciences, University of Trieste, via Weiss 2, 34128 Trieste, Italy, Saverio Fracaros ba r t i c l e i n f oa b s t r a c tArticle history:Received 8 April 2022Received in revised form 25 November 2022Accepted 12 December 2022Available online 16 December 2022Keywords:Counterfactual explanationExplainable machine learningExplainable artificial intelligenceRobustnessUncertaintyCounterfactual explanations (CEs) are a powerful means for understanding how decisions made by algorithms can be changed. Researchers have proposed a number of desiderata that CEs should meet to be practically useful, such as requiring minimal effort to enact, or complying with causal models. In this paper, we consider the interplay between the desiderata of robustness (i.e., that enacting CEs remains feasible and cost-effective even if adverse events take place) and sparsity (i.e., that CEs require only a subset of the features to be changed). In particular, we study the effect of addressing robustness separately for the features that are recommended to be changed and those that are not. We provide def-initions of robustness for sparse CEs that are workable in that they can be incorporated as penalty terms in the loss functions that are used for discovering CEs. To carry out our experiments, we create and release code where five data sets (commonly used in the field of fair and explainable machine learning) have been enriched with feature-specific anno-tations that can be used to sample meaningful perturbations. Our experiments show that CEs are often not robust and, if adverse perturbations take place (even if not worst-case), the intervention they prescribe may require a much larger cost than anticipated, or even become impossible. However, accounting for robustness in the search process, which can be done rather easily, allows discovering robust CEs systematically. Robust CEs make ad-ditional intervention to contrast perturbations much less costly than non-robust CEs. We also find that robustness is easier to achieve for the features to change, posing an impor-tant point of consideration for the choice of what counterfactual explanation is best for the user. Our code is available at: https://github .com /marcovirgolin /robust -counterfactuals.© 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).1. IntroductionModern Artificial Intelligence (AI) systems often rely on machine learning models such as ensembles of decision trees and deep neural networks [1–3], which contain from thousands to billions of parameters. These large models are appealing because, under proper training and regularization regimes, they are often unmatched by smaller models [4,5]. However, as large models perform myriads of computations, it can be very difficult to interpret and predict their behavior. Because of this, large models are often called black-box models, and ensuring that their use in high-stakes applications (e.g., of medicine and finance) is fair and responsible can be challenging [6,7].* Corresponding author.E-mail address: marco.virgolin@cwi.nl (M. Virgolin).https://doi.org/10.1016/j.artint.2022.1038400004-3702/© 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840The field of eXplainable AI (XAI) studies methods to dissect and analyze black-box models [8,9] (as well as methods to generate interpretable models when possible [10]). Famous methods of XAI include feature relevance attribution [11,12], explanation by analogy with prototypes [13,14], and, of focus in this work, counterfactual explanations. Counterfactual explanations enable to reason by contrast rather than by analogy, as they show in what ways the input given to a black-box model needs to be changed for the model to make a different decision [15,16]. A classic example of counterfactual explanation is: “Your loan request has been rejected. If your salary was 60 000$ instead of 50 000$ and your debt was 2500$ instead of 5000$, your request would have been approved.” A user who obtains an unfavorable decision can attempt to overturn it by intervening according to the counterfactual explanation.Normally, the search of counterfactual explanations is formulated as an optimization problem (see Sec. 2.1 for a formal description). Given the feature values that describe the user as starting point, we seek the minimal changes to those feature values that result in a point for which the black-box model makes a different (and oftentimes, a specific favorable) deci-sion. We wish the changes to be minimal for two reasons: one, to learn about the behavior of the black-box model for a neighborhood of data points, e.g., to assess its fairness (although this is not guaranteed in general, see e.g., [17]); two, in the hope that putting the counterfactual explanation into practice by means of real-life intervention will require minimal effort too. For counterfactual explanations to be most useful, more desiderata than requiring minimal feature changes may need to be taken into account (see Sec. 9) [18].In this paper, we consider a desideratum that can be very important for the usability of counterfactual explanations: ro-bustness to adverse perturbations. By adverse perturbations we mean changes in feature values that happen due to unforeseen circumstances beyond the user’s control, making reaching the desired outcome no longer possible, or requiring the user to put more effort than originally anticipated. These unforeseen circumstances can have various origins, e.g., time delays, mea-surement corrections, biological processes, and so on. For example, if a counterfactual explanation for improving a patient’s heart condition prescribes lowering the patient’s blood pressure, the chosen treatment may need to be employed for longer, or even turn out to be futile, if the patient has a genetic predisposition to resist that treatment (for more examples, see Sec. 5.1 and choices made in the coding of our experiments, in robust_cfe/dataproc.py).We show that, if adverse perturbations might happen, one can and should seek counterfactual explanations that are ro-bust to such perturbations. A particular novelty of our work is that we distinguish between whether perturbations impact the features that counterfactual explanations prescribe to change or keep as they are (note that some features may be irrele-vant and can be changed differently than how prescribed by a counterfactual explanation, we address this in Sec. 2.3). This is because counterfactual explanations are normally required to be sparse in terms of the intervention they prescribe (i.e., only a subset of the features should be changed), for better usability (see Sec. 2.1). As it will be shown, making this discrim-ination allows to improve the effectiveness and efficiency with which robustness can be accounted for. Consequently, one might need to consider carefully which counterfactual explanation to pursue, based on whether they are robust to features to change or keep as they are.In summary, this paper makes the following contributions:1. We propose two workable definitions of robustness of counterfactual explanations that concern, respectively, the fea-tures prescribed to be changed and those to be kept as they are;2. We release code to support further investigations, where five existing data sets are annotated with perturbations and plausibility constraints that are tailored to the features and type of user seeking recourse;3. We provide experimental evidence that accounting for robustness is important to prevent adverse perturbations from making it very hard or impossible to achieve recourse through counterfactual explanations, when adverse perturbations are sampled from a distribution (i.e., they are not necessarily worst-case ones);4. We show that robustness for the features to change is far more reliable and computationally efficient to account for than robustness for the features to keep as they are;5. Additionally, we propose a simple but effective genetic algorithm that outperforms several existing gradient-free search algorithms for the discovery of counterfactual explanations. The algorithm supports plausibility constraints and imple-ments the proposed definitions of robustness.2. PreliminariesIn the following, we introduce preliminary concepts for reasoning about robustness of counterfactual explanations in a sparse sense. In particular, we (i) describe the problem statement of searching for counterfactual explanations, (ii) present the notions of perturbation and robustness in general terms, and (iii) introduce the definitions of C and K, which are sets that partition the features of a counterfactual explanation. The following Secs. 3 and 4 will then present the main contribution of this paper: notions of robustness that are tailored to sparse counterfactual explanations, i.e., specific to Cand K.2.1. Problem statementLet us assume we are given a point x = (x1, . . . , xd), where d is the number of features. Each feature takes values either in (a subset of) R, in which case we call it a numerical feature, or in (a subset of) N, in which case we call it a categorical2M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840feature. For categorical features, we use natural numbers as a convenient way to identify their categories, but disregard ordering. For example, for the categorical feature gender, 0 might mean male, 1 might mean female, and 2 might mean non-binary. Thus, x ∈ Rd1 × Nd2 , where d1 + d2 = d.A counterfactual example1 for a point x is a point z ∈ Rd1 × Nd2 such that, given a classification (black-box) machine learning model f : Rd1 × Nd2 → {c1, c2, . . . } (ci is a decision or class), f (z) (cid:5)= f (x). We wish z to be close to x under a meaningful distance function δ that is problem-specific and meets several desiderata (see Sec. 9). For example, commonly-used distances that are capable of handling both numerical and categorical features are variants of Gower’s distance [19](see Eq. (9) and, e.g., [20] for a variant thereof). Often, when dealing with more than two classes, we also impose f (z) = t, i.e., the target class we desire z to be. Other times, we wish to find a set of counterfactual examples {z1, . . . , zk}, possibly of different classes, to obtain multiple means of recourse or simply gain information on the decision boundary of f nearby x(e.g., to explain f ’s local behavior) [15,21,22].For the sake of readability, we provide formal definitions only for the case when all features are numerical (i.e., x ∈Rd, d1 = d, d2 = 0). For completeness, we include explanations of how to deal with categorical features in the running text. Furthermore, we assume feature independence. While this assumption is rarely entirely met in real-world practice, it is commonly done in literature due to the lack of causal models (e.g., only four works consider causality in Sec. 9), and allows us to greatly simplify the introduction of the concepts hereby presented. We discuss the limitations that arise from this assumption in Sec. 8.A counterfactual explanation is represented by a description of how x needs to be changed to obtain z. In other words, a counterfactual explanation is a prescription on what interventions should be made to reach the respective counterfactual example. For example, under the assumption of independence and all-numerical features, the difference z − x is typically considered the counterfactual explanation for how to reach z from x. What particular form counterfactual explanations take is not crucial to our discourse, and we will use z − x for simplicity.We proceed by considering the following traditional setting where, for simplicity of exposition and without loss of generality, we will assume that features are pre-processed so that a difference in one unit in terms of feature i is equivalent to a difference in one unit in feature j (i.e., the user’s effort is commensurate across different features). Alternatively, one can account for this in the computation of the distance (see, e.g., Eq. (9)). We seek the (explanation relative to an) optimalz(cid:3) with:z(cid:3)∈argminzδ(z, x)with δ(z, x) := ||z − x||1 + λ||z − x||0and subject to f (z) = t and z − x ∈ P.(1)In other words, δ is a linear combination, weighed by λ, of the sum of absolute distances between the feature values of xand z, and the count of feature values that are different between x and z. Note that z(cid:3) needs not be unique, i.e., multiple optima may exist. Moreover, the difference z − x must abide to some plausibility constraints specified in a collection P . We model plausibility constraints as a set of specifications, each relative to a feature i, concerning whether zi − xi is allowed to be > 0, < 0, and (cid:5)= 0, i.e., a feature can increase, decrease, or change at all (for categorical features, we only consider the latter). For example for a private individual who wishes to be granted a loan, one of such constraints may specify that they cannot reasonably intervene to change the value of a currency (such a feature is called mutable but not actionable), i.e., counterfactual explanations must have zi − xi = 0, for i representing currency value. Similarly, the individual’s age may increase but not decrease, i.e., zi − xi > 0, for i representing age.We particularly consider the L1-norm (i.e., the term || · ||1 of δ in Eq. (1)) because it is reasonable to think that, for independent features, the total cost of intervention (i.e., the effort the user must put) is the sum of the costs of intervention for each feature separately, and that these costs grow linearly. Some works (e.g., [20,23]) choose the L2-norm (|| · ||2, also known as Euclidean norm) instead of the L1-norm; the definitions of robustness given in this paper can be easily adapted for the L2-norm. Regarding the L0-norm (i.e., the term || · ||0 of δ in Eq. (1)), this term explicitly promotes a form of sparsity, as it seeks to minimize how many features have a different value between z and x. This is desirable because, oftentimes, the user can only reasonably focus on, and intervene upon, a limited number of features (even if this amounts to a larger total cost in terms of L1 compared to intervention on all the features) [24].2.2. Perturbations & robustnessUnforeseen circumstances (e.g., inflation) might lead to more or different intervention to be needed, compared to what was originally prescribed by a counterfactual explanation (e.g., increase savings by 1000 $ to be granted credit access). Thus, instead of reaching z as intended by the counterfactual explanation, a different point zis obtained. Note that while the effects of unforeseen circumstances can impact feature values, the circumstances themselves need not be encoded as feature values. In fact, we will only focus on the extent by which feature values may be perturbed by such circumstances.(cid:6)(cid:6)1 Many authors use xthe manuscript, for readability.to represent a counterfactual example for x, instead of z. We chose z not to overload the notation with superscripts later on in 3M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. 1. Example of considering robustness to perturbations when seeking counterfactual examples. The red and green areas respectively represent high riskand low risk classifications of a cardiac condition according to a model f . The patient, represented by x, is at high risk. Three possible counterfactual examples are shown for different interventions (white arrows): zb for treating blood pressure, zv for treating vitamin deficiency, and zb,v for treating both. We assume to know the maximal extent of perturbation (under reasonable risk) for blood pressure and vitamin deficiency due to natural physiological events. This allows us to define the blue areas surrounding each counterfactual example. Perturbations w to one of the counterfactual examples can lead to any other point in the blue area. zv is the best of the three in terms of proximity to x but its blue area partly overlaps with the red area. This means that there exist w such that zv + w leads to a point in the red area, invalidating the counterfactual explanation. In such cases, it is important to estimate if additional intervention is possible so that zv can still be reached, and at what cost. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)(cid:6)p(cid:2)(cid:3), p{+}1{−}1{−}d, . . . , pLet us define the vector w = z − zas a perturbation for the counterfactual example z. We assume that perturbations that impact feature i are sampled from some distribution P (w i) and we are interested in controlling for, or being robust to, large magnitude perturbations that have reasonable risk. For example for normally-distributed perturbations, we might want to consider the values that can be sampled at the 95th or 99th percentile. We will therefore assume that we can define a vector p =≥ 0 represent, respectively, the smallest negative and largest positive perturbations that can reasonably happen to the ith feature. For example, if the ith feature represents tells by how much the blood pressure might lower at most (e.g., as a conse-the blood pressure of a patient, then pquence of dehydration) and ptells by how much the blood pressure might raise at most (e.g., as a consequence of anti-inflammatory drug intake). Clinicians may be able to define this information from their experience or retrieve it from {−}need not be the same, i.e., |p|. Note that for an ithmedical literature. In general, the magnitudes of pi{−}feature that is categorical, decreases or increases pas explained for numerical features are no longer meaningful. iFor categorical features, we will assume that p contains elements that represent what categorical perturbations are possible for that feature, i.e., pi will be a set of indices that represent categories.{+}, pi{+}, pi≤ 0 and pwhere p| (cid:5)= |p{+}d{−}i{+}i{−}i{+}i{−}i{+}1, pUnder the problem setting we considered in Sec. 2.1, perturbations that may impact a counterfactual explanation define that can be reached from z due to perturbations. An example is illustrated (cid:6)a box (hyper-rectangle) of all possible points zin Fig. 1. We define the concept of p-neighborhood of z as follows:Definition 1. (p-neighborhood and p-neighbors of a counterfactual example) Given a model f , a point x, a respective counter-factual example z, and a vector of possible perturbations p, the p-neighborhood of z is the set:(cid:4)N :=(cid:6) | zz(cid:6)i∈ [zi + p{−}i, zi + p{+}i(cid:5)].A point z(cid:6) ∈ N such that z(cid:6) (cid:5)= z is called a p-neighbor of z.(2)Not all perturbations are problematic. Our goal is to study robustness to adverse perturbations, i.e., those for which (cid:6)) (cid:5)= t. In other words, we wish to seek counterfactual examples z that have no (or the fewest possible) f (z + w) = f (zp-neighbors for which perturbations can cause the classification performed by f to be different from t. When that happens, we say that the counterfactual explanation has been invalidated by the perturbation. However, it may be the case that invalidation is not permanent: there may still exist intervention (i.e., a new counterfactual explanation) that adheres to the constraints in P and allows us to overcome invalidation. Therefore, in this work, we will seek to discover counterfactual explanations that are robust in the sense that (i) if invalidated, additional intervention remains possible, (ii) the cost of additional intervention is small.Unforeseenly, if fis assumed to be a general model (e.g., not necessarily a linear one), then the following argument holds.4M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Proposition 1. For a general f , information on the classification of a p-neighbor (e.g., that f (zprovides no information about the classification of another p-neighbor (e.g., that f (z(cid:6)) (cid:5)= f (z(cid:6)(cid:6)) for z(cid:6)) = f (z) for z(cid:6)(cid:6)(cid:6)on the boundary of N) in the interior of N).Proof. We cannot preclude that the model frem [25], f may represent any function. Thus, f may represent a Swiss cheese-like function, where for example f (zwith zis, for example, a neural network. Under the universal approximation theo-(cid:6)(cid:6))(cid:6) + e and e = (ε1, . . . , εd) different from the zero-vector, however small |εi|, ∀i. (cid:2)(cid:6)) (cid:5)= f (z(cid:6)(cid:6) := z(cid:6)This proposition means that if no information on, e.g., regularity or smoothness of fis available, then we must check (cid:6)) (cid:5)=each and every p-neighbor zt. Checking all neighbors is typically not feasible, e.g., as soon as some of the features are real valued. Thus, the best one can do is to take an approximate approach. For example, a Monte-Carlo sampling approach can be used where a batch of random points within N is considered, hoping that the batch is representative of all points in N. As we will show in the next sections, a better strategy can be designed if sparsity is considered.of z to assess whether some of them may invalidate the explanation, i.e., ∃zsuch that f (z(cid:6)We conclude this section by noting that perturbations, as described so far, are absolute, i.e., independent of the starting point x, the counterfactual in consideration z, or the intervention entailed by the counterfactual explanation z − x. Pertur-bations to feature i might however depend on xi and zi , i.e., be sampled from a distribution P (w i|xi, zi). For example, due to market fluctuations, a return on investment may be smaller than anticipated by 5% of the expected value. Such type of relative perturbations entail different p-neighborhoods for different x and z. For simplicity and without loss of generality, we will proceed by assuming that perturbations can only be absolute. We explain how we also included relative perturbations in the annotations used for our experiments in Sec. 5.2.3. Sparsity, features in C and KWe use the form of sparsity mentioned in Sec. 2.1 to partition the features into two sets. As mentioned before, sparsity is an important desideratum because it may not be reasonable to expect that the user can realistically intervene on, and keep track of, all the features to achieve recourse. Given a specific counterfactual explanation z for the point x, we call the set containing the (indices of the) features whose values should change C = {i ∈ {1, . . . , d} | zi (cid:5)= xi}, and its complement, i.e., the set of the (indices of the) features whose values should be kept as they are, K = {i ∈ {1, . . . , d} | zi = xi}. Typically, because a sufficiently large λ is used, or because of the plausibility constraints specified in P , K (cid:5)= ∅.Note that the proposed partitioning between C and K implicitly assumes that all features are relevant to the counter-factual explanation. If certain features are irrelevant, perturbations to those features will have no effect on f ’s decision, and thus those features need not be accounted for when assessing robustness. This means that accounting for irrelevant features makes assessing robustness more computationally expensive than needed. However, as f is considered a black-box, we cautiously assume that all features are relevant for assessing robustness.We will proceed by accounting for perturbations and respective robustness separately for features in C and K. Accounting for robustness separately is important because, as we will show, assessing robustness for features in C can be done far more efficiently and be more effective than for features in K. Knowing this, if multiple counterfactual explanations can be found, the user may want to choose the counterfactual explanation that fits him/her best based on the robustness it exhibits in terms of C and K. In the next section, we present our first notion of robustness, which concerns C.3. Robustness for CWe begin by focusing on the features that the counterfactual explanation instructs to change, i.e., the features (whose indices are) in C. Recall that we assume that a vector of maximal perturbation magnitudes p can be defined. This leads us to the following definition.Definition 2. (C-perturbation) Given a point x, a respective counterfactual example z, and the vector of maximal magnitude perturbations p, a C-perturbation for the counterfactual explanation z − x is a vector1, . . . , wcd), where{+}≤ piwc = (wc(cid:6){−}piwci≤ wci= 0if i ∈ C,otherwise, i.e., i /∈ C(3)(4)and such that ∃i : wci > 0, i.e., wc is not the zero-vector.In other words, a C-perturbation is a perturbation that acts only on features in C, and at least on one of such features. Next, we use the concept of C-perturbations to introduce the one of C-setbacks.5M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. 2. Example of C-setbacks. The red and green areas represent high risk and low risk classifications of a cardiac condition according to a model f . The patient, represented by x, is at high risk. Two treatments are possible but cannot be administered jointly due to drug incompatibility, hence sparsity of intervention is needed (only one of the two treatments can be pursued). The closest (and thus optimal) counterfactual example is zv and concerns treating vitamin deficiency (white arrow pointing down). Another counterfactual example is zb and concerns treating blood pressure (white arrow pointing left). Maximal C-setbacks are shown for both counterfactual examples (blue dashed segments). The setbacks can make both counterfactual examples invalid. However, the counterfactual examples can still be reached with additional intervention (treatment administration). Indeed, if one accounts for the possibility that maximal C-setbacks may take place, then the total cost (original intervention + additional intervention to remedy the perturbation, i.e., 3 + 2.5) to reach zv becomes larger than the total cost to reach zb (i.e., 4 + 1).Fig. 3. One-dimensional example of how a C-setback can be advantageous if the magnitude of perturbation exceeds the magnitude of intervention.Definition 3. (C-setback) A C-setback for the counterfactual explanation z − x is a C-perturbation such that⎧⎪⎨⎪⎩≤ wci≤ p{−}ip0 ≤ wci= 0wci≤ 0 if zi − xi > 0{+}if zi − xi < 0iotherwise, i.e., i /∈ C.(5)We denote C-setbacks with wc,s.ii+i= pIn words, a C-setback is a C-perturbation where each and every element of the perturbation wc,sis of opposite sign to the counterfactual explanation zi − xi . We can interpret the meaning of C-setbacks wc,s as vectors that push the user away from z and back towards x along the direction of intervention. Furthermore, we call a maximal C-setback, denoted by wc,sif zi − xi > 0and wc,smax, C-setback whose elements that correspond to features in C have maximal magnitude, i.e., wc,sif zi − xi < 0. An example is given in Fig. 2.C-setbacks are arguably more interesting than C-perturbations because C-setbacks are the subset of these perturbations that plays against the user. In fact, certain C-perturbations might be advantageous, enabling to reach z with less intervention i and that of zi − xi matches). To account for robustness, we are than originally provisioned (i.e., when the sign of wcinterested in understanding whether perturbations can prevent us to reach z, hence we will proceed by focusing exclusively on C-setbacks.It is important to note that even C-setbacks can be advantageous if one allows their perturbations to be of larger magnitude than intervention, i.e., if |wc,s| >|zi − xi|, then a C-setback can lead to a point that “precedes” x in terms of the direction of intervention. For that point, the intervention may be less costly than the one that was originally planned or entirely not needed because the point is of the target class (see, e.g., Fig. 3). Advantageous situations are not interesting for robustness and counterfactual explanations that can be overturned by perturbations may well not be interesting to pursue. We therefore consider any C-setback to have elements capped by |wc,s| > |zi − xi| is allowed ([26] discuss this aspect in detail). In a nutshell, if |wc,s| ≤ |zi − xi|.Perhaps the most interesting scenario for considering C-setbacks is when dealing with z(cid:3), since a counterfactual example = p−iiiiithat is optimal (i.e., one minimizes Eq. (1)) is an ideal outcome. The following simple result holds for z(cid:3):Proposition 2. For any C-setback wc,s of z(cid:3) (such that |wc,si| ≤ |z(cid:3)i− xi| for all i), f (z(cid:3) + wc,s) (cid:5)= t.6M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Proof. We use reduction ad absurdum. Let us assume the opposite of what was said in Proposition 2, i.e., there exists wc,s(cid:6), x) = δ(z(cid:3) + wc,s, x) < δ(z(cid:3), x). such that f (z(cid:3) + wc,s) = t. Let z(cid:6)In other words, zis of the target class and is closer to x than z(cid:3) is. This contradicts the fact that z(cid:3) is optimal. (cid:2)(cid:6)) = t. By construction of wc,s, δ(z(cid:6) := z(cid:3) + wc,s, and so f (zNow, because of Proposition 2, we are guaranteed that if a C-setback wc,s happens to z(cid:3), the resulting point will no longer be classified as t. Intuitively, this is a natural consequence of the fact that optimal counterfactual examples lay on the border of the decision boundary as otherwise they would not be optimal. Also, since z(cid:3) is optimal, the respective L0 component for the distance between x and z(cid:3) is minimal, i.e., all features in C and thus in wc,s are relevant for the classification. Given the premises just made, it becomes important to understand whether invalidation to z(cid:3) can be averted with additional intervention and, if so, whether the cost of such intervention can be minimized.It is important to note that invalidation of a counterfactual explanation due to a C-setback can always be averted, i.e., additional intervention to reach the intended zi for all i ∈ C is always possible. To see this, consider the fact that the intervention entailed by the counterfactual explanation z − x must adhere to the plausibility constraints specified in P (else, z − x would not be a possible counterfactual explanation). Since C-setbacks are aligned with the direction of the original intervention, the point z + wc,s − x, which is in between x and z, must meet P . It therefore suffices to apply additional intervention along the originally-intended direction to recover the desired counterfactual example. Under the L1-norm (as per the choice of δ in Eq. (1)), the cost associated with the additional intervention needed to overcome a C-setback wc,s is simply ||wc,s||1.Since invalidation due to C-setbacks can be dealt with additional intervention, and since one can reasonably assume that the user keeps track of how the value of xi changes for i ∈ C over the course of intervention (otherwise, (s)he would not know when to stop the intervention), it follows that there is no necessity for counterfactual examples to be far from the decision boundary in terms of their features in C. (Note that this is in contrast with prior work on aspects of robustness for counterfactuals, where the possibility of additional intervention is not considered and counterfactual examples are required to be far from the decision boundary in terms of all of their features; see Sec. 9.) Thus, rather than seeking counterfactual examples that are not invalidated by C-setbacks, we seek counterfactual examples for which the additional intervention that is needed to contrast C-setbacks is minimal. To this end, we can use Proposition 2 in order to seek counterfactual examples that are optimal (i.e., require minimum intervention cost) when the additional cost to contrast maximal C-setbacks wc,smax is factored in. In the following definition, to highlight that C-setbacks depend on the specific z and x (as they determine C) and avoid confusion, we use the function notation W c,smax(z, x) in place of wc,smax.Definition 4. (Optimal counterfactual example under C-setbacks) Given a model f , a point x, and a vector p, we call a point z(cid:3),c such that(cid:11)z − W c,smax(z, x), x(cid:12),(6)z(cid:3),c − W c,smax(z(cid:3),c, x) ∈ argmin(cid:11)max(z,x)an optimal counterfactual example under C-setbacks.z−W c,s(cid:12)δThis definition gives us a way to seek a (multiple may exist) counterfactual explanation that entails minimal intervention cost when accounting for maximal C-setbacks. Indeed, it suffices to equip a given search algorithm with Eq. (6), i.e., perform the following steps: (1) for any z to be evaluated, compute the respective wc,smax, (2) instead of computing δ(z, x), compute δ((z − wc,smax), x), and (3) at the end of the search, return the point that minimizes such distance, i.e., z(cid:3),c .Performing the computations just mentioned takes linear time in the number of features (O (d)) because we only need to build wc,smax (step 1 above) and subtract it from z prior to computing δ (step 2 above) for any given z ( f should still be evaluated on z). This is relatively fast (as demonstrated in B.3.2), especially compared to the situation described in Sec. 2.2, where one would need to use f to predict the class of a number of neighbors of z. Note also that in Eq. (6) setbacks are subtracted from counterfactual examples when computing δ, to account for the fact that the cost should increase (recall the construction of C-setbacks in Definition 3).4. Robustness for KWe now consider K, i.e., the set concerning the features that should be kept to their current value. Mirroring the notion if i ∈ K and = 0 if i /∈ K. Similarly, we can cast the concept of neighborhood from Definition 1 to consider only K-perturbations, of C-perturbation (Definition 2), we can define a K-perturbation to be a vector wk such that pwkileading to:≤ wki{−}i{+}i≤ pDefinition 5. (K-neighborhood and K-neighbors of a counterfactual example) Given a model f , a point x, a respective counter-factual example z, and a vector of possible perturbations p, the K-neighborhood of z under p is the set:(cid:14)(cid:13).(7)K :={−}i∈ [zi + p= zi otherwise(cid:6)(cid:6) | zzi(cid:6)zi(cid:6) ∈ K such that zA point z, zi + p{+}i], if i ∈ K(cid:6) (cid:5)= z is called a K-neighbor of z.7M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. 4. Example of K-perturbations. The counterfactual example zv is vulnerable to K-perturbations because these can lead to the red area; the same is not true for zb . If it is not plausible to reduce blood pressure, then K-perturbations to zv can lead to permanent invalidity. Else, they can be resolved with additional intervention, in terms of blood pressure.For a categorical feature i ∈ K, the neighborhood can be built by swapping zi with one of the possibilities listed in pi ∈ p, where pi will be a set containing categories perturbations can lead to.Next, we use K to define the concept of vulnerability to K-perturbations:Definition 6. (Vulnerability to K-perturbations) Given a model f , a point x, and a vector p, a counterfactual example z is vulnerable to K-perturbations if ∃z(cid:6) ∈ N(z, p) such that f (z(cid:6)) (cid:5)= f (z).Informally, this definition says that z is vulnerable to K-perturbations if the decision boundary surrounding z is not suf-ficiently loose with respect to the features in K. Fig. 4 shows an example. The reason why vulnerability to K-perturbations is particularly important is that, differently from the case of C-perturbations, a K-perturbation can invalidate the counter-factual explanation permanently. In fact, a K-perturbation changes z along a different direction than the one of intervention. Thus, a K-perturbation can lead to a point zfrom which there exists no plausible intervention to reach the originally-intended z from.(cid:6)For example, consider the feature i to represent inflation as a mutable but not actionable feature, i.e., a feature that can be changed (e.g., by global market trends) but not by the user. P will state that no (user) intervention can exist to change i, i.e., P imposes zi − xi = 0. However, an unforeseen circumstance such as the financial crisis of 2008 may lead to a large +inflation increase (pi > 0). Consequently, it may become impossible for the user to obtain the desired loan, e.g., because the bank does not hand out certain loans when the inflation is too high.Now, recall that the reason why Definition 4 can be used for the case of C-perturbations is that Proposition 2 holds, i.e., there cannot exist points of class t between x and an optimal counterfactual example z(cid:3). The same does not hold for K-perturbations, i.e., since the features in K are orthogonal to the direction of intervention, it can happen that the maximal (cid:6)) = t, while a non-maximal perturbation to the same feature perturbation to a feature i ∈ K leads to a point z(cid:6)(cid:6)) (cid:5)= t. Thus, checking for maximal perturbations is no longer sufficient: we must check can lead to a point zinstead for all points in the K-neighborhood K .for which f (zfor which f (z(cid:6)(cid:6)(cid:6)As mentioned in Sec. 2.2, checking each and every point in a neighborhood may not be feasible. Thus, we propose to approximate the assessment of how K-robust (i.e., non-vulnerable to perturbations in K) counterfactual explanations can be, with Monte-Carlo sampling. Let 1 f (z) : K → {0, 1} be the indicator function that returns 1 for K-neighbors that share the same class of z (i.e., f (z)), and 0 for those that do not. Taken a random sample of m K-neighbors, we define the following score:K-robustness score(z, m) = 1mm(cid:15)i=1(cid:6)i).1 f (z)(z(8)We remark that even if K-robustness score(z, m) = 1, we are not guaranteed that z is K-robust, because the score is an approximation. Still, this score can be used to determine which counterfactual examples are preferable to pursue in that they are associated with a smaller risk that adverse perturbations will invalidate them (permanently or not).5. Experimental setupIn this section, we firstly describe the preparation of the data sets used in our experiments. Secondly, we describe the search algorithms considered for finding near-optimal counterfactual explanations. Lastly, we describe the loss function considered, as well as how to incorporate the proposed notions of robustness into it.8M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Table 1Considered data sets, where n and d (resp., d2) indicate the number of observations and features (only categorical) after pre-processing. The column t is the target class for the (simulated) user. Plausib. constr. reports the number of plausibility constraints that allow features to only increase (≥), remain equal (=), and decrease (≤). The column Perturb. reports the number of perturbations concerning numerical (N) and categorical (C) features. Finally, Acc.rf and Acc.nn report the average (across five folds) test accuracy of the random forest and neural network models.Data set (abbrev.)Credit risk (Cre)Income (Inc)House price (Hou)Productivity (Pro)Recidivism risk (Rec)n1000188350611962000d2012131210d2ClassesUser67156High, lowHigh, lowHigh, lowHigh, med., lowHigh, lowIndividualIndividualMunicipalityCompanyInmatetLowHighLowHighLowPlausib. constr.≥:3, =:8, ≤:0≥:2, =:3, ≤:0≥:0, =:3, ≤:1≥:0, =:0, ≤:0≥:2, =:2, ≤:0Perturb.Acc.rfAcc.nnN:6, C:0N:4, C:4N:11, C:0N:5, C:2N:3, C:20.760.830.930.790.800.750.820.930.700.78Table 2Examples of perturbations that we manually annotated on the considered data sets. We take relative perturbations (those with %) with respect to the value of the feature in the intended counterfactual example z in consideration by the search algorithm.D.setFeatureCreIncHouProRecSavingsMarital statusCrime rateOvertimeAgeDecrease Increase or Categories10%10%{single, married, widowed, . . . }1%305%32NoteMight happen to save less or more relative to what intended.Unforeseen change due to, e.g., proposal, divorce, death.Relative, might increase more than decrease.Up to 3 more or less days of overtime might be needed.Judicial system delays for up to 2 years.5.1. Data setsTable 1 summarizes the data sets we consider. For each data set, we make an assumption on the type of user who seeks recourse, e.g., the user could be a private individual seeking to increase their income, or a company seeking to improve the productivity of its employees. Based on this, we manually define the target class t, the set of plausibility constraints P on what interventions are reasonably plausible, and the collection p of maximal magnitudes from which perturbations can be sampled (we will consider uniform and normal distributions). We named the data sets in Table 1 to represent their purpose. Originally, Credit risk (abbreviated to Cre) is known as South German Credit Data [27], which is a recent update that corrects inconsistencies in the popular Statlog German Credit Data [28]. Income (Inc) is often called Adult or Census income [29,30]. Housing price (Hou) is also known as Boston housing [31] and is often used for research on fairness and interpretability because one of its features raises ethical concerns [32]. Productivity (Pro) concerns the productivity levels of employees producing garments [33]. Lastly, Recidivism (Rec) is a data set collected by an investigation of ProPublica about possible racial bias in the commercial software COMPAS, which intends to estimate the risk that an inmate will re-offend [34]. Examples of recent works on fair and explainable machine learning that adopted (some of) these data sets are [20,35–40].We pre-process the data sets similarly to how done often in the literature. This includes, e.g., removal of redundant features and of observations with missing values, and limiting the number of observations considered for Rec. Regarding our annotations for the perturbations, numerical features can have perturbations that increase or decrease the feature value, in absolute or relative terms; we compute relative perturbations with respect to z. For example, for the numerical feature capital-gain of Inc, we assume that perturbations can happen that lead up to a relative 5% increase or 10% decrease of that feature, based on the value to achieve for that feature. For categorical features, we define only absolute perturbations, i.e., possible changes of category are not conditioned to the current category. The choices we made to build p are subjective, we elaborate on this in Sec. 8. We sample the amount of perturbation using a uniform or normal distribution, as indicated in Sec. 7. Table 2 shows some examples of maximal perturbations we annotated. As mentioned before, we also define plausibility constraints P for each data set. Each constraint is specific to a feature. For an ith numerical feature, possible constraints are zi − xi ≥ 0, zi − xi ≤ 0, zi − xi = 0, and none. For an ith categorical feature, possible constraints are zi = xiand none. Full details about our pre-processing and definition of p and P are documented in the form of comments in our code, in robust_cfe/dataproc.py.5.2. Black-box modelsWe consider random forest and neural networks (with standard multi-layer perceptron architecture) as black-box ma-chine learning models f . We use Scikit-learn’s implementations [41]. We assume that we can only access the predictions of f , and no other information such as model parameters or gradients. Our experiments are repeated across a stratified five-fold cross-validation, and each model is obtained by grid-search hyper-parameter tuning. Once trained, the models obtain test accuracy varying from 70% to more than 90% on average across the different data sets, i.e., meaningful decision bound-aries are learned. See Appendix A for details on hyper-parameter tuning, and the accuracy of the models on the different data sets. For the discovery of counterfactual examples, we consider observations x such that f (x) (cid:5)= t, from the test sets of the cross-validation.9M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Table 3Settings of the considered counterfactual search algorithms. For NeMe, we only set the max-imum number of iterations to 100 to achieve commensurate runtimes to those of CoGS (other settings are default). For DiCE, we consider two configurations (“a” and “b”). The loss used (except for DiCE a) is Eq. (9).CoGSSettingPopulation sizeNum. generationsTournament sizesmutValue1000100225%SettingMethodTotal CEsMax. iterationsLoss weightsDiCE (a, b)ValueGenetica : 20, b : 100a : 500, b : 100a : Default,b : 0.5 prox., 0.5 spars., 0 div.GrSpSettingNum. in layerFirst radiusDecrease radiusSparseValue20000.110TrueLORESettingPopulation sizeNum. generationsDiscrete use probabilitiesContinuous function estim.Value100010FalseFalse5.3. Counterfactual search algorithmsTo provide experimental results concerning robustness (Sec. 7), we firstly seek a counterfactual search algorithm that performs best overall among several candidates. To that end, we consider and benchmark the following algorithms from the literature, that can operate upon black-box f : Diverse Counterfactual Explanations (DiCE) [22], Growing Spheres (GrSp) [23], LOcal Rule-based Explanations (LORE) [20,42], and the Nelder-Mead method (NeMe) [43,44]. Furthermore, we devise our own algorithm, a genetic algorithm that we name Counterfactual Genetic Search (CoGS).2The settings used for the algorithms are reported in Table 3. We describe the algorithms below. Note that all of the algorithms are heuristics with no guarantee of discovering optimal (i.e., minimal distance) counterfactual examples, given the nature of the search problem (general, black-box f ).5.3.1. DiCEDiCE is actually a library that includes three algorithms: random sampling, KD-tree search (i.e., a fast-retrieval data structure built upon the points in the training set), and a genetic algorithm. Of the three, we consider the latter because it performed substantially better in preliminary experiments (and simply refer to it by DiCE). DiCE is configured to return a collection of counterfactual examples rather than a single one. However, three of the other algorithms we consider return a single counterfactual example. Thus, to compare the algorithms on an equal footing, we set DiCE to return a single counterfactual example too. We achieve this by ranking each counterfactual example in the collection according to the loss function in consideration (explained below, see Sec. 5.4), and picking the best-ranking point. We will further consider two different configurations of DiCE:• Configuration “a” uses the default settings except for allowing for a longer number of iteration, to match the same computational budget given to the other algorithms.• Configuration “b” uses custom settings that are aligned to be similar to those used for CoGS, since both DiCE and CoGS are genetic algorithms.5.3.2. GrSpGrSp is a greedy algorithm that iteratively samples neighbors of the starting point x within spheres (i.e., in an L2 sense) that have increasing radius, until counterfactual examples are found. GrSp includes feature selection to promote sparsity. Unforeseenly, GrSp can only handle numerical features. To be able to use GrSp in our comparison, we let GrSp operate on categorical features as if they were numerical ones (categories are encoded as integers). At the end of the optimization, we transform numerical values back to categories by rounding. Note that this is sub-optimal because an artificial ordering is introduced between categories.5.3.3. LORELORE works by generating a neighborhood around x with random search or with a genetic algorithm, finding multiple counterfactual explanations at different distance. We consider the variant that adopts the genetic algorithm, because it performed substantially better in preliminary experiments. After the neighborhood is determined, LORE fits a decision tree 2 https://github .com /marcovirgolin /cogs.10M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. 5. Runtimes (means and 95% confidence intervals) of the counterfactual search algorithms for the considered data sets and black-boxes (random forest and neural network). The right plots are zoomed-out versions of the left ones.upon it. Since each path from the root of the decision tree to a leaf represents a classification rule (e.g., “AGE >= 3.4 & SALARY_CATEGORY = HIGH → t”), LORE essentially returns multiple counterfactual explanations expressed as rules. To be able to compare with the other algorithms (which return a single counterfactual example), we build one counterfactual example z by taking the shortest rule returned by LORE, and applying the rule to the starting point x (e.g., using the rule above, we set the x’s age and salary to 3.4 and high, respectively).We found (confirmed by a discussion with the authors) that applying LORE’s rules may result in points that are not actually classified as t. When that happens, we perform up to 15 attempts at generating a counterfactual example from the (shortest returned) rule, by focusing on numerical features that are prescribed to be >, ≥ (or <, ≤) than a certain value. In particular, in applying such part of the rule to x, we add (or subtract) to the prescribed value a term (cid:6), which is initially −3 and is doubled at every attempt. Moreover, since we found LORE to be computationally expensive to run (see set to 10Fig. 5), we used a fraction of the computation budget allowed for the other algorithms (see Table 3).5.3.4. NeMeNeMe is a classic simplex-based algorithm for gradient-free optimization. Like GrSp, also NeMe cannot naturally handle categorical features. Thus, we use the same approximation used for GrSp, i.e., encode categories with integers, let NeMe treat categories as numerical values, and map such values back to integers (and thus categories) by rounding at the end. We use SciPy’s implementation with default parameters [45].5.3.5. CoGSWe design CoGS as a relatively standard genetic algorithm, adapted for the search of points neighboring x (especially in terms of the L0-norm). CoGS operates as follows. First, an initial population of candidate solutions is generated by sampling feature values uniformly within an interval for numerical features, and from the possible categories for categorical features. These intervals can be specified or taken automatically from the training set. With probability of 2/d (d being the total number of features), the feature value of a candidate solution is copied from x rather than sampled. Every iteration of the algorithm (in the jargon of evolutionary computation, generation), offspring solutions are produced from the current population by crossover and mutation. Following this, survival of the fittest is applied to form the population for the next generation.Our version of crossover produces two offspring solutions by simply swapping the feature values of two random par-ents, uniformly at random. Our version of mutation produces one offspring solution from one parent solution by randomly altering its feature values. A feature value is altered with probability of 1/d (else, it is left untouched). If the feature to alter is categorical, then the category is swapped with another category, uniformly at random. If the feature to alter is nu-merical, firstly a random number r is sampled uniformly at random between −smut/2 and +smut/2, where smut ∈ (0, 1] is a hyper-parameter that represents the maximal extent of allowed mutations; secondly, the original feature value is changed by adding r × (maxi − mini), where maxi and mini are, respectively, the maximum and minimum values that are possible for that feature.After crossover and mutation, the quality (fitness) of offspring solutions is evaluated using the loss function (Eq. (9)) as fitness function (minimization is sought). Finally, we use tournament selection [66]) to form the population for the next generation.11M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840We set CoGS to allow for plausibility constraints (P ) to be specified. If plausibility constraints are used, then mutation is restricted to plausible changes (e.g., the feature that represents age can only increase). If mutation makes a numerical feature obtain a value bigger than maxi (resp., smaller than mini ), then the value of that feature is set to maxi (resp., mini ).CoGS is written in Python, and relies heavily on NumPy [67] for speeding up key computations. For example, the popu-lation is encoded as a NumPy matrix, and crossover and mutation are implemented with matrix operations.5.4. LossWe use the following loss to drive the search of counterfactual examples (where f (z) and t are treated as integers):12γ (z, x) + 12γ (z, x) = 1d||z − x||0d⎛d1(cid:15)⎝+ || f (z) − t||0, where⎞|zi − xi|maxi − minid2(cid:15)+j||z j − x j||0⎠ .i(9)(10)The function γ in the equation above is Gower’s distance [19,46], where features indexed by i are numerical and those indexed by j are categorical (with values treated as integers); the maximal and minimal values of a numerical feature, maxiand mini , can be taken from the (training) data set or, as done in our case, are provided as extra annotations of the data sets. The term ||z − x||0/d promotes sparsity of intervention and, like Gower’s distance, ranges from zero to one. The third and last term requires the execution of the machine learning model f , and simply returns zero when f (z) = t and one when f (z) (cid:5)= t.5.4.1. Incorporating robustness in the lossTo seek robust counterfactual examples, we make use of the notions described in Sec. 3 and Sec. 4. When optimizing for robustness to perturbations concerning C, we use Definition 4, i.e., maximal C-setbacks are computed on the fly for the candidate z and their contribution is used to update the contribution of γ to the loss function. When optimizing for robust-2 (1 − K-robustness score)ness to perturbations concerning K, we compute the K-robustness score with Eq. (8) and add 1to the loss. In the results presented below, we use m = 64 to compute the K-robustness score; an analysis on the impact of m is provided in B.3.6. Preliminary results: choosing a suitable counterfactual search algorithmThis section reports on the benchmarking of the considered search algorithms, to identify an overall best. We repeat the execution of each algorithm five times and consider the best-found counterfactual example out of the five repetitions. We search for a counterfactual example for each x in the test sets from the five cross-validation, for x such that f (x) (cid:5)= t. Since LORE takes much longer to execute than the other algorithms (see Fig. 5), we perform three repetitions instead of five, and consider only the first five x in each test set of the five folds. Since only DiCE and CoGS support plausibility constraints, we do not use plausibility constraints in this comparison (a comparison between DiCE and COGS under plausibility constraints is provided in Appendix B.1).6.1. RuntimesFig. 5 shows the runtime of the algorithms across the different data sets, irrespective of whether they succeed or fail to find a counterfactual example, i.e., a point for which f predicts t. The experiments were run on a cluster where the computing nodes can have slightly different CPUs, thus we invite to consider the order of magnitude of the runtimes rather than the exact numbers. The figure shows that, using random forest, CoGS and DiCE (configuration a) are the fastest algorithms (or, at least, have fastest implementations), but GrSp and NeMe are competitive. LORE is much slower to execute than the other algorithms. When using a neural network, inference times are generally faster, and CoGS, DiCE-a, GrSp and NeMe are competitive.6.2. Success in discovering counterfactual examplesTable 4 shows the frequency with which the counterfactual search algorithms succeed in finding a counterfactual ex-ample, i.e., a point for which f predicts t. CoGS and the two variants of DiCE succeed systematically, whereas the other algorithms do not. GrSp performs third-best overall. In particular, GrSp always finds counterfactual examples on Hou, which is a data set with a single categorical feature. Since GrSp is intended to operate solely with numerical features, this resultnicely supports the hypothesis that GrSp works well when (almost all) features are numerical. Although LORE supports both numerical and categorical features, it does not perform better than GrSp on most data sets; at least for the limited number of runs conducted with LORE due to excessive runtime, as explained before. Lastly, NeMe often performs substantially worse than all other algorithms.12M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Table 4Mean ± standard deviation across five cross-validation folds of the frequency with which the counterfactual search algorithms succeed in finding a counterfactual example. Plausibility con-straints are not considered here because not all algorithms support them.Alg.t CoGSserofDiCE-aCre1.001.00Inc1.001.00modnar=fkrowtenlaruen=fDiCE-bGrSpLORENeMeCoGSDiCE-aDiCE-bGrSpLORENeMe1.000.46 ± 0.110.56 ± 0.200.08 ± 0.031.000.89 ± 0.060.20 ± 0.130.05 ± 0.021.001.001.001.001.000.87 ± 0.070.52 ± 0.200.14 ± 0.071.000.25 ± 0.040.28 ± 0.160.11 ± 0.03Hou1.001.001.001.000.68 ± 0.200.04 ± 0.051.001.001.001.000.68 ± 0.100.09 ± 0.01Pro1.001.00Rec1.001.001.000.86 ± 0.040.24 ± 0.200.03 ± 0.011.000.30 ± 0.150.60 ± 0.380.14 ± 0.021.001.001.001.001.000.51 ± 0.120.76 ± 0.230.11 ± 0.041.000.49 ± 0.100.84 ± 0.230.51 ± 0.03Fig. 6. Boxplots of relative change in loss with respect to CoGS for GrSp, LORE, and NeMe, on the different data sets and black-boxes, for success cases.6.3. Quality of discovered counterfactual examplesAs last part in our benchmarking effort, we consider what algorithm manages to produce near-optimal counterfactual examples (i.e., those with smallest loss). In particular, we report the relative change in loss for the best-found counterfactual example with respect to the loss obtained by CoGS, only for success cases. Since we consider only successes, the last term of the loss (Eq. (9)) is always null, i.e., || f (z) − t||0 = 0. The relative change in loss with respect to CoGS for another algorithm Alg is:LAlg(z) − LCoGS(z)LCoGS(z).Fig. 6 shows the relative change in loss of DiCE, GrSp, LORE, and NeMe with respect to CoGS. DiCE, GrSp and LORE typically (but not always) find points that have larger loss than those found by CoGS. NeMe performs very similarly to CoGS, however NeMe seldom succeeds (cf.. Table 4). This suggests that NeMe can explore a small neighborhood of x particularly well, but fails if counterfactual examples are relatively distant from x.13M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Table 5Mean ± standard deviation of the frequency with which the best-found (among five search repetitions) counterfactual example when not accounting for robustness is accidentally robust w.r.t. C or K. For numerical features, we consider them to match in value if they are within a tolerance level (Tol.) of 1%, 5% or 10% of the range for that feature.RobustnessOnly COnly KBoth C, KOnly COnly KBoth C, Ktserofmodnar=fkrowtenlaruen=fTol.1%5%10%1%5%10%1%5%10%1%5%10%1%5%10%1%5%10%CreIncHouProRec0.40 ± 0.060.42 ± 0.070.43 ± 0.070.37 ± 0.010.44 ± 0.030.46 ± 0.040.23 ± 0.040.27 ± 0.030.30 ± 0.050.25 ± 0.120.27 ± 0.120.29 ± 0.110.13 ± 0.070.26 ± 0.080.39 ± 0.020.02 ± 0.020.06 ± 0.030.12 ± 0.080.020.04 ± 0.020.05 ± 0.020.06 ± 0.020.40 ± 0.080.58 ± 0.070.000.000.000.01 ± 0.010.02 ± 0.010.02 ± 0.010.35 ± 0.020.52 ± 0.030.70 ± 0.040.000.000.000.76 ± 0.100.84 ± 0.090.85 ± 0.090.33 ± 0.240.63 ± 0.170.67 ± 0.160.21 ± 0.210.54 ± 0.210.60 ± 0.190.96 ± 0.020.97 ± 0.020.97 ± 0.020.07 ± 0.050.80 ± 0.120.93 ± 0.040.07 ± 0.050.69 ± 0.090.93 ± 0.040.53 ± 0.050.57 ± 0.060.58 ± 0.060.26 ± 0.050.37 ± 0.060.46 ± 0.070.19 ± 0.060.26 ± 0.050.34 ± 0.060.87 ± 0.050.89 ± 0.050.89 ± 0.050.08 ± 0.060.42 ± 0.190.58 ± 0.140.06 ± 0.060.38 ± 0.180.52 ± 0.140.27 ± 0.060.37 ± 0.070.40 ± 0.090.04 ± 0.040.08 ± 0.030.12 ± 0.020.03 ± 0.030.06 ± 0.040.08 ± 0.040.50 ± 0.080.56 ± 0.050.57 ± 0.040.010.01 ± 0.010.02 ± 0.020.00 ± 0.010.01 ± 0.010.01 ± 0.026.4. Conclusion of benchmarkingThe results show that, overall, CoGS performs best. DiCE (in particular, DiCE-a) is the closest competitor in terms of speed and success rates, but the algorithm finds counterfactual examples that are substantially more distant from x (i.e., have larger loss) than those found by CoGS. GrSp has good runtime and generally finds closer counterfactual examples (i.e., lower loss) than DiCE, but it remains inferior to CoGS, both in terms of distance (loss) and success rate. LORE has worse success rate than GrSp, and NeME worse of all. Therefore, we use CoGS for the following experiments on robustness.We remark that DiCE, like CoGS, supports the specification of plausibility constraints. We show that CoGS performs better than DiCE also under plausibility constraints in Appendix B.1.7. Experimental results: robustnessWe proceed with presenting the experimental results regarding robustness to perturbations in C, K, and jointly. We focus on results that allow us to answer what we believe to be important research questions: (RQ1) Do we need to account for robustness to discover robust counterfactual examples? (RQ2) Does a lack of robustness compromise the feasibility of correcting perturbations with additional intervention? (RQ3) Are robust counterfactual explanations advantageous in terms of additional in-tervention cost? These questions are addressed, in order, in the next subsections. Because of space limitations, a number of additional results is reported in Appendix B, including runtime taken to account for robustness w.r.t. C and K, and the effect of varying m when computing the K-robustness score. We now account for plausibility constraints P in all of the following experiments. We remark that in all our experiments, CoGS always succeeded in discovering a counterfactual example for which f predicts t, except for having a mean success rate of 99% (st.dev. of 1%) on the Rec data set when f is implemented as a neural network.7.1. (RQ1) Do we need to account for robustness to discover robust counterfactual examples?Table 5 shows the frequency with which robust counterfactual examples are discovered accidentally. To realize this, we compare the best-found counterfactual example that is discovered by CoGS when robustness is not accounted for, and the one that is found when C- or K-robustness is accounted for (as indicated in Sec. 5.4.1). We take the frequency by which the two match as indication of whether robust counterfactual examples can be discovered by accident. Since numerical feature values may differ only slightly between two best-found counterfactual examples, we consider the values to match if they are sufficiently close to each other, according to a tolerance level of 1%, 5%, or 10% of the range of that feature. As reasonable to expect, the results show that the larger the tolerance level, the more a z(cid:3) discovered when not accounting for robustness matches the respective one that is discovered when accounting for robustness. In general, the result depends on the data set in consideration, and also (albeit arguably less so) on whether random forest or a neural network is used as black-box model f .For brevity, we now focus on the tolerance level of 5% and random forest. On Inc, best-found counterfactual examples rarely match with those discovered when accounting for C-robustness (4% on average for the tolerance of 5%), while the 14M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. 7. Mean frequency with which a plausible additional intervention exists, to contrast the perturbations and reach the intended counterfactual example (uniformly-distributed categorical changes and normally- or uniformly-distributed numerical changes, f = random forest). Darker colors represent worse cases.vice versa happens on Hou (84% on average for the same tolerance). For K-robustness like for C-robustness, the result depends on the data set. Importantly, the data sets where the frequencies are high for C-robustness and K-robustness are not necessarily the same. On Inc, best-found counterfactual examples are rarely optimal under C-setbacks, but can often match with counterfactual examples discovered when penalizing low K-robustness scores (40% on average for the tolerance of 5%). This should not be surprising because C- and K-robustness are orthogonal to each other under the assumption of feature independence. The last row shows how often best-found counterfactual examples happen to be both robust to perturbations to C and K. The frequencies are clearly always lower than for the previous triplets of rows. Hou is the only data set for which the frequency of discovering a counterfactual example that happens to be both robust w.r.t. C and K by chance is relatively large (e.g., above 50% for the tolerance of 5%).When using the neural network instead of random forest, the trends mentioned before remain the same, but the specific magnitudes can differ. For example, the accidental discovery of robust counterfactual examples w.r.t. C and/or K is lower on Cre with the neural network compared to random forest, but the opposite holds for Hou (with some exceptions, e.g., the tolerance level of 1% when both C- and K-robustness are sought).Overall, this result indicates that, except for lucky cases (e.g., Hou with f being the neural network), it is unlikely to discover robust counterfactual examples by chance. Hence, if one wishes to achieve robustness, the search must be explicitly instructed to that end. In the next sections, we investigate whether achieving robustness can actually be important.7.2. (RQ2) Does a lack of robustness compromise the feasibility of correcting perturbations with additional intervention?At this point, current works on the robustness of counterfactual explanations typically consider the extent by which ro-bustness helps preventing the invalidation of counterfactual explanations (see Sec. 9). In other words, they consider whether the point zthat is given by perturbing the best-found counterfactual example is still classified as t. For completeness, we report on this in B.2. Current works do not, however, consider whether an additional intervention that allows to correct the perturbation and obtain t might exist.(cid:6)Figs. 7 and 8 show the frequency with which achieving the intended counterfactual explanation remains possible after random perturbations take place. The frequency is computed by applying, for each counterfactual explanation outcome of the search, 100 perturbations that are sampled uniformly at random from the categorical possibilities for categorical features, and normally (with st.dev. of 0.1) or uniformly within the numerical intervals for numerical features, as defined in p. We note that similar results are obtained between choosing random forest or a neural network as f .As expected, it is always possible to contrast C-setbacks, because these happen along the direction of intervention. In-stead, perturbations concerning K can lead to a zsuch that no further plausible intervention exists to reach the originally intended counterfactual example. We do not report a result for perturbations concerning both C and K at the same time (cid:6)15M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. 8. Mean frequency with which a plausible additional intervention exists, to contrast the perturbations and reach the intended counterfactual example (uniformly-distributed categorical changes and normally- or uniformly-distributed numerical changes, f = neural network). Darker colors represent worse cases.because, by construction, it is the same as the result for perturbations concerning only K. Like for the results of Sec. 7.1, the extent by which perturbations to K reduce the possibility for additional intervention depends on the data set. On Pro, all perturbations can be contrasted by an additional intervention because there are no plausibility constraints (see Table 1). Conversely, on Rec, perturbations to K can often make it impossible to reach the originally-intended counterfactual example, unless K-robustness is accounted for. In fact, accounting for K-robustness generally improves the chances that additional intervention is possible, at times substantially (e.g., on Inc, Hou, and Rec). Cre represents the only exception to this, as accounting for K-robustness performs similar (or sometimes worse) than accounting for none. This suggests that the de-cision boundary learned by f on this data set may not be very smooth, making the use of the K-robustness score a too coarse approximation to be helpful. Generally, accounting for perturbations to C alone does not help achieving substantial robustness to perturbations to K, except for on Hou. This suggests that, on Hou, flearns decision boundaries that incor-porate interesting interactions between certain features. Importantly, accounting for C-robustness together with accounting for K-robustness does not substantially compromise the gains obtained by accounting for K-robustness alone, even though perturbations to C always admit additional intervention. Overall, these results show that accounting for robustness can be crucial to ensure that, if perturbations happen, additional intervention to obtain t remains possible.7.3. (RQ3) Are robust counterfactual explanations advantageous in terms of additional intervention cost?We present the following results in terms of a relative cost, namely, the ratio between the cost of intervention to reach the intended z when random perturbations take place (i.e., initial the cost of reaching z from x plus the cost of reaching z from the perturbed z), and the ideal cost, i.e., the cost incurred in complete absence of perturbations (i.e., the cost of reaching z from x). We compute this relative cost when the notions of robustness are or are not accounted for. The ideal cost is computed when not accounting robustness. The cost is modeled by 1(i.e., the first part of Eq. (9)). (cid:6)) = t, we assume no additional intervention to be needed, and thus the additional cost is zero and the Moreover, if f (zrelative cost is 1.2 γ (z, x) + 1||z−x||0d2(cid:6)Figs. 9 and 10 (for random forest and neural network, respectively) show that when no robustness is accounted for (the left-most triplets of boxes in each plot), the relative cost can become dramatically large. In other words, additional interven-tion to correct the perturbations can be extremely costly. Whether the relative cost increases mostly due to perturbations to C (blue boxes) or to K (orange boxes) depends on the data set. For example, perturbations to K have the largest effect on Rec, while those to C have the largest effect on Inc (by far), across types of distribution and types of f . For both the random forest and the neural network, the relative cost ranges from around 5× or 10× the ideal cost, up to over 100×(Inc, perturbations to C) when not accounting for robustness.16M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. 9. Cost in terms of different configurations of accounting for robustness and under different perturbations, relative to the ideal cost (with random forest). Due to perturbations, t perturbations, the relative cost for when no notion of robustness is accounted for (label None) is typically much larger than the one for when the right notion of robustness is accounted for (matching color between box and label). The vertical axis for Inc is in logarithmic scale.Fig. 10. Cost in terms of different configurations of accounting for robustness and under different perturbations, relative to the ideal cost (with neural network). Due to perturbations, the relative cost for when no notion of robustness is accounted for (label None) is typically much larger than the one for when the right notion of robustness is accounted for (matching color between box and label). The vertical axis for Inc is in logarithmic scale.When one accounts for the notion of robustness that is meant to deal with the respective type of perturbation, the relative cost often decreases substantially. Accounting for C-robustness (second blue box from the left in each plot) counters perturbations to C very well on all the data sets. On Inc in particular, the relative cost improves by two orders of magnitude. As found in Sec. 7.2, accounting for perturbations to K with the K-robustness score can remain insufficient, as it can be observed on Cre and Inc for both types of f . Again, this is likely a limitation of using a simple heuristic such as the K-robustness score to deal with K-robustness. Accounting for robustness w.r.t. C (resp., K) does not, in general, lead to smaller relative cost under perturbations to K (resp., C). We confirm this general trend with statistical testing in Appendix C. Lastly, 17M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. 11. Cost of accounting for robustness relative to not accounting for robustness (i.e., ideal cost) when no perturbations take place. Note that the (rare) relative costs smaller than 1 are due to a lack of optimality of the search algorithm.Fig. 12. Reduction in sparsity (relative to the number of features) caused by accounting for robustness. Note the different vertical axes.accounting for both C- and K-robustness (right-most triplets of boxes in each plot) offers protection (lower relative cost) from situations in which both types of perturbations take place. In general across data sets and types of f , the distribution of relative costs for when perturbations to both C and K take place and both C- and K-robustness are accounted for (right-most green box in each plot) is better than the distribution for when the same perturbations take place but no notion of robustness is accounted for (left-most green box in each plot).Since the ideal cost is computed when no notion of robustness is accounted for, part of the relative costs for when robustness is accounted for comes from the fact that robust counterfactual examples are generally farther away from x than non-robust ones. Fig. 11 shows the cost increase that comes solely from accounting for robustness on the considered data sets and types of f , without any perturbation taking place. We remark that values smaller than 1 happen only because the discovered counterfactual examples can be suboptimal. Importantly, we find that the cost when accounting for robustness is between 1× and 7× the ideal cost, i.e., when not accounting for robustness. In general, this is significantly smaller than the increase incurred when perturbations take place and robustness is not accounted for, as reported before (generally between 5× and 10× the ideal cost, with up to 100×).Lastly, Fig. 12 shows what part of the cost increase comes from counterfactuals becoming less sparse. For certain data sets (e.g., Cre for both random forest and neural network), robust counterfactual explanations are as sparse as non-robust ones. In general, however, robust counterfactual explanations tend to be less sparse, depending on the choice of f and the 18M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840type of robustness that is accounted for. The reduction in sparsity can be moderate or substantial. For example, less than 10% more of the features need to change to account for K perturbations on Hou with random forest (i.e., approximately one feature). Instead, up to 30% more of the features need to change to account for K perturbations on Rec with the neural network (i.e., approximately three features). The fact that sparsity decreases when seeking robust counterfactuals is a natural consequence of adopting a linearization of the objectives (see Sec. 5.4). Thus, the reduction in sparsity can be tackled by tuning the weight attributed to the L0-norm in Eq. (9).These results confirm that even though robust counterfactual explanations are, in principle, more costly to pursue than non-robust ones, if random perturbations take place, robust counterfactual explanations require much less additional inter-vention than non-robust ones.8. DiscussionOur experimental results provide a positive answer to all three research questions. In general, counterfactual explanations are not robust, be it in terms of the features whose value is prescribed to be changed (C-robustness), or those whose value is prescribed to be kept as is (K-robustness). Moreover, non-robust counterfactual explanations are more susceptible to make it impossible for the user to remedy perturbations by additional intervention, and the cost of additional intervention is larger for non-robust counterfactual explanations than for robust ones. Ultimately, it is clear that accounting for robustness is important.Our experimental results suggest that accounting for robustness for features in C tempers perturbations to C, and simi-larly, accounting for robustness for features in K tempers perturbations to K. Moreover, even though f can learn non-linear feature interactions, accounting for C (or K) has limited effect on contrasting perturbations to K (resp., C). Only in some cases (e.g., on Hou), robustness w.r.t. C has substantial repercussions on the effect of perturbations to K or vice versa.In addition to this, even if a counterfactual search algorithm does not guarantee that the discovered counterfactual example will be optimal, we experimentally see that incorporating our Definition 4 into the loss (Sec. 5.4.1) produces a strong resilience to additional cost (Sec. 7.3) for perturbations to the features in C. Besides being effective, implementation of Definition 4 is also efficient (see B.3.2).What our results also show is that seeking robustness with respect to features in K is problematic. This is because of Proposition 1 and the fact that features in K are not aligned with the direction of intervention. Thus, we proposed to control for K-robustness using an approximation, i.e., the K-robustness score. We found that seeking counterfactual examples that maximize the K-robustness score are often but not always sufficient to obtain a good resilience to perturbations to the features in K. Moreover, the K-robustness score requires to sample (and evaluate with f ) multiple points, which is far more expensive than computing Definition 4. Therefore, future work should consider whether a better method can be used than the K-robustness score. For example, if information on fis available, that information may be used to provide guarantees on the neighborhood of z (see, e.g., Theorem 2 in [40] for linear f ).The assumption that features are independent is simplistic but often made in literature, because only a small number of works assume a causal model is available (e.g., [47,48]). Under the assumption of feature independence, as done here, one models the neighborhood of a counterfactual example with a box (under L1) or a hyper-sphere (under L2). However, if certain features have a causal dependency on other features, this neighborhood morphs into other, possibly very complex shapes (e.g., when this dependency is not linear). Importantly, if feature i depends on j, then one cannot change j without having that i implicitly changes too. Similarly, a perturbation happening to j would implicitly alter i. As our framework currently assumes independence, it is important to study to what extent separation between C and K remains possible and meaningful. For many real-world problems, it is reasonable to expect that there exist groups of features that are truly independent from other groups of features. Thus, the study of robustness for C and K could be carried out at a higher level, i.e., of feature groups in future work.There is a number of further aspects worth mentioning when one wishes to implement a research work like on coun-terfactual explanations into practice, including this work. For example, we use the L1-norm within Gower’s distance to measure intervention cost. In fact, literature works typically choose one distance measure (e.g., ours, or Gower’s with L2-norm instead, or other variants, see Sec. 9). Of course, a realistic implementation of intervention cost may need to be more refined, e.g., by mixing different types of norms. Similarly, one might wish to use different distributions to sample meaning-ful perturbations (as opposed to only uniform or only normal as done in our synthetic experiments), and different functions to define the maximal extent of perturbation, which may e.g., account for the distribution of feature values. For example, should be smaller than for less dense areas. Other desiderata may need to be for denser areas of feature i, pincluded when seeking counterfactuals in practice (see, e.g., [49,50]), including accounting for multiple types of robustness of the same time, such as those related to uncertainties of f [51,52]+i and pLastly, we made subjective choices to define perturbations (p) and plausibility constraints (P ) in the data sets. We made these choices as best as we could, based on reading the meta-information in web sources and the papers that describe the data sets. We have no doubt that domain experts would make much better choices than ours. Nevertheless, we argue that this is not an important limitation because, as long as the community agrees that our choices are reasonable, they suffice to provide a sensible test bed for benchmarking robustness. Hopefully, other researchers will find our annotations to be useful for future experiments on the robustness of counterfactual explanations. Similarly, we hope that other researchers will find CoGS to be an interesting algorithm to benchmark against.−i19M. Virgolin and S. Fracaros9. Related workArtificial Intelligence 316 (2023) 103840A number of works in literature propose several new desiderata that are largely orthogonal to our notions of robust-ness but can be important to enhance the practical usability of counterfactual explanations. For example, Dandl et al. [49]consider, besides proximity of z to x according to different distances, whether other training points xare sufficiently close to z for it to reasonably belong to the training data distribution. A similar desideratum is considered in [21] and [53]; the latter work employs neural autoencoders to that end. [54] remarks the importance of sparsity for explanations, with the concepts of pertinent negatives (the minimal features that should be different to (more) confidently predict the given class) and pertinent positives (the minimal features that help correctly identifying the class). Laugel et al. [36,50] require that z can always be reached from a training point xwithout having to cross the decision boundary of f , for z not to be the result of an artifact in the decision boundary of f . In [47] and [22], counterfactual explanations are studied through the lens of causality. For recent surveys on counterfactual explanations, the reader is referred to [55,16,56].(cid:6)(cid:6)We now focus on works that deal with some notion of robustness and/or perturbations explicitly. Artelt et al. [57] present theoretical results on the effect of perturbations (e.g., under linear f ), evaluate the effect of different type of perturbations (Gaussian, uniform, masking) with three classifiers, and find that counterfactual explanations that obey plausibility con-straints are more robust than counterfactual explanations that do not. Differently from us, Artelt et al. do not consider sparsity and do not optimize for robustness. The work by Karimi et al. [48] extends [47] to consider possible uncertainties in causal modeling. In [17], it is shown that a malicious actor can, in principle, jointly optimize small perturbations and the model f such that, when applying the perturbations to points of a specific group (e.g., white males), the respective counterfactual explanations are much less costly than normal (in fact, counterfactual explanations are conceptually similar to adversarial examples, see, e.g., [58–60]). Some works consider forms of robustness of counterfactual explanations with respect to changes of f (e.g., whether z is still classified as t if fis used instead of f ) [51,61] or updates to f (e.g., after data distribution shift of temporal or geospatial nature) [62,52]. In [63], robustness of counterfactual explanations is stud-ied in the context of differentially-private support vector machines. Dominguez et al. [40] consider whether counterfactual explanations remain valid in presence of uncertainty on x, and also account for causality. We also note that Dominiguez et al. consider a neighborhood of uncertainty around x which is akin to Definition 1; in fact, such sort of neighborhoods is common tools in post-hoc explanation methods, e.g., the Anchor explainer by [64] seeks representative points for a class by assessing that the prediction of f for the points in their neighborhood is the same. Zhang et al. [65] propose a counter-factual search method based on linear programming that works for neural networks with ReLU activations; this work can be seen through the lens of robustness in that the method produces regions of points that share the desired class. Finally, contemporary to our work, Fokkema et al. [26] provide important theoretical results that counterfactual explanations (and other XAI methods such as feature attribution ones) can be dramatically different when small perturbations are applied to the starting point x (or, in general, point to explain).(cid:6)To the best of our knowledge, there exists no other work prior to ours that attempts to exploit sparsity when assessing robustness, although sparsity is an important property for counterfactual explanations. Moreover, existing works typically consider whether robustness helps preventing counterfactual explanations from becoming invalid, while we further consider that additional intervention may be possible, and assess the associated cost.10. ConclusionCounterfactual explanations can help us understand how black-box AI systems reach certain decisions, as well as what intervention is possible to alter such decisions. For counterfactual explanations to be most useful in practice, we studied how they can be made robust to adverse perturbations that may naturally happen due to unforeseen circumstances, to ensure that the intervention they prescribe remains valid, and potential additional intervention cost that may be needed remains limited. We presented novel notions of robustness, which concern adverse perturbations to the features that a counterfactual explanation prescribes to change (C-robustness) and to keep as they are (K-robustness), respectively. We have annotated five existing data sets with reasonable perturbations and plausibility constraints and developed a competitive counterfactual search algorithm to search for (robust) counterfactual explanations. Our experimental results show that, most often than not, counterfactual explanations do not happen to be robust by accident. Consequently, if adverse perturbations take place, counterfactual explanations may require a much larger cost to be realized than anticipated, or even make it impossible for the user to achieve recourse. Our definitions of robustness can be incorporated in the search process, and robust counter-factual explanations can be discovered. We have shown that C-robustness can be accounted for efficiently and effectively, while the same is not always true for K-robustness. This aspect should be taken into account when choosing what coun-terfactual explanation is best for the user. Overall, robust counterfactual explanations are resilient against invalidation and require much smaller additional intervention to contrast perturbations.Declaration of competing interestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.20M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Table A.1Hyper-parameter settings considered for tuning random forest.NameNo. treesMin. samples splitMax. featuresOptions{50, 500}{2, 8}√{d, d }Table A.2Hyper-parameter settings considered for tuning the neural network.NameLearning rateMax. iterationsSolverOptions{0.0001, 0.01}{200, 1000}{Adam, SGD}Table A.3Test accuracy of hyper-parameter-tuned random forests acting as for the considered data sets across five-fold black-box models fcross-validation.Fold01234Avg.Cre0.710.780.780.740.760.76Inc0.860.820.790.820.830.83Hou0.930.900.910.910.970.93Pro0.790.770.780.820.780.79Rec0.800.820.780.770.800.80Table A.4Test accuracy of hyper-parameter-tuned neural networks acting as for the considered data sets across five-fold black-box models fcross-validation.Fold01234Avg.Cre0.740.780.730.780.740.75Inc0.830.820.800.820.820.82Hou0.940.930.910.910.960.93Pro0.620.700.690.770.720.70Rec0.780.790.750.780.790.78Data availabilityThe data is available at the github repository linked in the abstract.AcknowledgementsWe thank dr. Stef C. Maree for insightful early discussions. This work made use of the Dutch national e-infrastructure with the support of the SURF Cooperative using grant no. EINF-2512. Funding: This publication is part of the project Ro-bust Counterfactual Explanations (with project number EINF-2512) of the research program Computing Time on National Computer Facilities which is (partly) financed by the Dutch Research Council (NWO).Appendix A. Hyper-parameter optimization of random forest and neural networkTo obtain a black-box model f for a given cross-validation fold, we train a random forest model or a neural network (a multi layer perceptron for classification) optimized with grid-search hyper-parameter tuning (with five-fold cross-validation on the training set). The hyper-parameter settings we considered are listed in Tables A.1 and A.2, all other being Scikit-learn’s default (v. 1.0.1). For random forest, we one-hot encode categorical features when training and querying the random forest model (see the code robust_cfe/blackbox_with_preproc.py). For the neural network, we additionally scale numerical features to have mean of zero and standard deviation of one.The performance of tuned random forest on all folds is shown in Table A.3, the respective one for the neural network is shown in Table A.4.21M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Table B.1Mean ± standard deviation across five cross-validation folds of the frequency with which CoGS and the two variants of DiCE succeed in finding a counterfactual example under plau-sibility constraints..f.r=f.nn=fAlg.CoGSDiCE-aDiCE-bCoGSDiCE-aDiCE-bCre1.001.001.001.001.001.00Inc1.001.001.001.001.00 ± 0.041.00Hou1.001.001.001.001.001.00ProRec1.001.000.20 ± 4.001.001.001.001.000.99 ± 0.061.00 ± 0.051.00 ± 0.041.000.80 ± 4.00Fig. B.1. Boxplots of relative change in loss with respect to CoGS for the two variants of DiCE when using plausibility constraints, on the different data sets and black-boxes, for success cases.Appendix B. Additional resultsWe provide additional results. These are (i) a further comparison between DiCE and CoGS when plausibility constraints are enforced, (ii) the (possibly non-permanent) invalidity caused by perturbations, as typically done in the literature of robustness, (iii) and effect of increasing m for the computation of the K-robustness score.B.1. DiCE vs. CoGS with plausibility constraintsTable B.1 shows the average and the standard deviation of the success rate (how many times a counterfactual of the desired class is found) for the two variants of DiCE and of CoGS when plausibility constraints are active. The algorithms are comparable in that they succeed in most cases. However, DiCE-b can perform substantially worse in two cases, i.e., data set Pro when using the random forest, and data set Rec when using the neural network.Lastly, Fig. B.1 shows the relative change in loss obtained by the algorithms. As it can be seen, DiCE comes close to the performance of CoGS only on Rec. Thus, overall, CoGS remains superior to DiCE also when plausibility constraints are enforced.We attribute this to the fact that, differently from CoGS, DiCE is inherently designed to discover a diverse set of counter-factuals instead of a single and closest-possible counterfactual.B.2. Invalidity of counterfactual explanationsWe now show whether the fact that best-found counterfactual explanations are typically not robust is associated with is the point to which z(cid:3) is a greater chance that perturbations can make them invalid, i.e., such that f (zshifted by the perturbation. Here, we do not consider whether additional intervention may or may not be possible. Fig. B.2shows the average frequency with which perturbations cause invalidity. The frequencies are computed by applying, to each (cid:6)) (cid:5)= t where z(cid:6)22M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. B.2. Mean frequency of invalidity of counterfactual explanations under different types of perturbations and when accounting for different types of robustness. Darker colors represent worse scenarios, i.e., larger average invalidity.discovered counterfactual example, 100 perturbations that are sampled uniformly at random for categorical features (from the categorical possibilities) and uniformly or normally (with st.dev. of 0.1) for numerical features (within the numerical intervals). The figure shows that when no notion of robustness is accounted for, perturbations generally have a larger chance of causing invalidity of the counterfactual explanation.Regarding perturbations to (features in) C (i.e., C-setbacks), recall that accounting for the respective notion of robustness is intended to provide counterfactual explanations with minimal additional intervention cost, the maximal C-setback were to happen. Ideally, the returned counterfactual example should still be optimal, i.e., as near to x as possible, which means that the example is on the border of the decision boundary of f . Thus, under optimality guarantees, f (z(cid:3) + wc,s) (cid:5)= t(Proposition 2); This means that any C-setback should result in invalidity (i.e., all entries for perturbations to C should report 1). This does not always happen in Fig. B.2 because CoGS does not guarantee to discover optimal counterfactual examples and, thus, in many cases the returned example is not on the boundary, and the C-setback is too small to cross the boundary. The frequency of this phenomenon depends on the data set. Also, while accounting for robustness w.r.t. Cshould not, in theory, decrease invalidity rate but only make additional intervention less costly, as confirmed in Sec. 7.3), 23M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. B.3. Approximated ground-truth K-robustness scores (using 1000 samples) for increasing m, to determine what value of m is needed for good robust-ness to perturbations to K. Shaded areas represent standard deviations.we find that accounting for robustness w.r.t. C lowers invalidity rate on Hou (e.g., most evident for both types of f with normally-distributed perturbations).When K-robustness is accounted for, the best-found counterfactual explanation is supposed to be in a region such that the decision boundary is relatively loose with respect to the features in K. Consequently, accounting for K-robustness should, in fact, counter invalidity, as we do not wish risking that it becomes impossible to carry out additional intervention due to the plausibility constraints. The figure shows that, in general, there can be a substantial gain in lowering invalidity by accounting for K-robustness. At times, accounting for K-robustness allows to reach almost zero invalidity, see the cell that corresponds to robustness for K and perturbations to K, on Inc, Hou, and Pro, for both types of f and sampling distributions. However, it is not always the case that K-robustness helps, due to the heuristic nature of the K-robustness score: see, e.g., Cre.Lastly, we observe that the frequency of invalidity can raise when both notions of robustness are accounted for at the same time (e.g., on Inc for uniformly-distributed perturbations when using the neural network). Note that this is not necessarily a problem because invalidity from perturbations to C is expected to be high, as the goal of robustness w.r.t. C is to be able to minimize additional intervention cost.B.3. Setting m for K-robustnessWe report results on setting the hyper-parameter m for computing K-robustness scores (see Eq. (8)). In particular, we run CoGS accounting for K-robustness in the loss function, for m ∈ {0, 4, 16, 64}. Note that using m = 0 corresponds to notaccounting for K-robustness.B.3.1. Achieved K-robustnessWe consider how increasing m improves K-robustness, using an approximated ground-truth. We approximate the ground-truth of the true K-robustness by calculating the K-robustness score over 1000 samples over the counterfactual example discovered using a specific m.Fig. B.3 shows the results obtained for this experiment. We also consider the case in which C-robustness is accounted for. If K-robustness is not accounted for (m = 0), then the (approximated ground-truth) K-robustness of the discovered counterfactual examples can be quite low, see, e.g., Rec for random forest (score approximately of 0.4) and neural network (score below 0.4). As soon as a few samples are considered (m = 4), the K-robustness increases substantially (see, e.g., Cre). Further increasing m has diminishing returns (note that m is increased exponentially). Accounting for C-robustness is largely orthogonal, meaning, it has no effect in terms of K-robustness.B.3.2. Additional required runtimeFig. B.4 shows the additional runtime incurred between runs of CoGS that account for some notion of robustness and runs that do not account for it, in particular for increasing m in the calculation of the K-robustness score. The figure shows that accounting for C-robustness comes at no significant extra cost in runtime. This follows from the fact that we can use Definition 4 and thus only need to compute the maximal C-setback. Conversely, accounting for K-robustness can come at a relatively large additional cost in runtime, which appears to be linear in m (note that m grows exponentially in the plots). Fortunately, the experimental results of B.3 suggest that small values of m are often sufficient to obtain good K-robustness scores.24M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Fig. B.4. Additional runtime of CoGS for different configurations of accounting for robustness with respect to the runtime when not accounting for robust-ness.Fig. B.5. Cost of accounting for robustness relative to not accounting for robustness (i.e., ideal cost) when no perturbations take place for different values of m. Note that the (rare) relative costs smaller than 1 are due to a lack of optimality of the search algorithm.B.3.3. Additional cost from accounting for robustnessFig. B.5 expands on the results reported in Fig. 11 by including different values of m. We do not find major differences based on the setting of m for computing the K-robustness score, except for the tails of the respective distributions on Hou, and slightly less so on Pro (for both types of f ). Accounting for C- and K-robustness at the same time leads to larger costs than accounting for only one of the two, as it is reasonable to expect. On average, the cost that comes from accounting for robustness alone is limited (up to 6.5× the ideal cost, see Inc), especially in light of the results found for 25M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Table C.1Result of pairwise comparison on the effect of accounting for different types of robustness for data set Cre under different perturbations (both random forest and neural network, both uniform and normal sampling distributions). The displayed p-values are obtained with the Mann-Whitney U test under Holm-Bonferroni correction and post Kruskal-Wallis test rejecting the null hypothesis with p-value (cid:13) 0.01.RobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KNone1.0000.0000.0000.000None1.0000.5200.0000.000None1.0000.0000.0000.000None1.0000.0000.0000.000Cre, perturbations to COnly COnly KBoth C, K0.0001.0000.0000.1250.0000.0001.0000.0000.0000.1250.0001.000Cre, perturbations to KOnly COnly KBoth C, K0.5201.0000.0000.0000.0000.0001.0000.912Cre, perturbations to C and KOnly COnly KNone1.0000.0000.0000.0000.0001.0000.0000.0000.0000.0001.0000.0030.0000.0000.9121.000Both C, K0.0000.0000.0031.000Inc, perturbations to COnly COnly KBoth C, K0.0001.0000.0000.0000.0000.0001.0000.0000.0000.0000.0001.000Inc, perturbations to KOnly COnly KBoth C, K0.0001.0000.0000.0000.0000.0001.0000.000Inc, perturbations to C and KOnly COnly KNone1.0000.0000.0000.0000.0001.0000.0000.0000.0000.0001.0000.0000.0000.0000.0001.000Both C, K0.0000.0000.0001.000Table C.2Result of pairwise comparison on the effect of accounting for different types of robustness for data set Inc under different perturbations (both random forest and neural network, both uniform and normal sampling distributions). The displayed p-values are obtained with the Mann-Whitney U test under Holm-Bonferroni correction and post Kruskal-Wallis test rejecting the null hypothesis with p-value (cid:13) 0.01.when perturbations take place, described in Sec. 7.3 (additional intervention due to perturbations can lead to 100× times larger costs for non-robust counterfactual explanations, see Inc on Fig. 9).Appendix C. Statistical significanceWe report the statistical significance for the results displayed in Sec. 7.3. For each data set and type of perturbation, we perform the Kruskall-Wallis tests (since we cannot assume normality) to determine whether significant differences are present between the relative cost induced by applying the different notion of robustness. In all cases, the outcome of the test is that significant differences are present (p-value (cid:13) 0.01). Next, we perform post-hoc pairwise comparisons with the Mann-Whitney-U test to assess whether one notion of robustness protects from the considered perturbation significantly differently than another. The result of the pairwise comparison analysis is shown in Tables C.1 to C.5.26M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Table C.3Result of pairwise comparison on the effect of accounting for different types of ro-bustness for data set Hou under different perturbations (both random forest and neural network, both uniform and normal sampling distributions). The displayed p-values are obtained with the Mann-Whitney U test under Holm-Bonferroni correction and post Kruskal-Wallis test rejecting the null hypothesis with p-value (cid:13) 0.01.RobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KNone1.0000.0000.0000.000None1.0000.0000.0000.000None1.0000.0000.0000.000None1.0000.0000.0000.000Hou, perturbations to COnly COnly KBoth C, K0.0001.0000.2610.0060.0000.2611.0000.0830.0000.0060.0831.000Hou, perturbations to KOnly COnly KBoth C, K0.0001.0000.0000.0000.0000.0001.0000.070Hou, perturbations to C and KOnly COnly KNone1.0000.0000.0000.0000.0001.0000.0000.0000.0000.0001.0000.0080.0000.0000.0701.000Both C, K0.0000.0000.0081.000Pro, perturbations to COnly COnly KBoth C, K0.0001.0000.0000.0080.0000.0001.0000.0000.0000.0080.0001.000Pro, perturbations to KOnly COnly KBoth C, K0.0001.0000.0000.0000.0000.0001.0000.760Pro, perturbations to C and KOnly COnly KNone1.0000.0000.0000.0000.0001.0000.0000.0000.0000.0001.0000.0140.0000.0000.7601.000Both C, K0.0000.0000.0141.000Table C.4Result of pairwise comparison on the effect of accounting for different types of robustness for data set Pro under different perturbations (both random forest and neural network, both uniform and normal sampling distributions). The displayed p-values are obtained with the Mann-Whitney U test under Holm-Bonferroni correction and post Kruskal-Wallis test rejecting the null hypothesis with p-value (cid:13) 0.01.On Cre under K-perturbations (middle part of Table C.1), accounting for robustness w.r.t. C is not significantly different than not accounting for any notion of robustness (p-value = 0.52 > 0.01); similarly, accounting for robustness w.r.t. Kinduces the same relative cost as accounting for robustness w.r.t. both C and K. On Hou under C-perturbations (top part of Table C.3), accounting for C is not significantly different than accounting for K (p-value = 0.52 > 0.01), while accounting for K is not significantly different than accounting for both C and K (p-value = 0.083 > 0.01). When perturbations happen to both C and K on Pro and Rec (bottom part of respective tables), accounting for K is not significantly different than accounting for both C and K (p-value = 0.014 > 0.01 and p-value = 0.271 > 0.01, respectively). In general, the results match what can be seen in Figs. 9 and 10. Also, we note that in the majority of the cases, accounting for one notion of robustness is significantly different than accounting for another (or for none).27M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840Table C.5Result of pairwise comparison on the effect of accounting for different types of robustness for data set Rec under different perturbations (both random forest and neural network, both uniform and normal sampling distributions). The displayed p-values are obtained with the Mann-Whitney U test under Holm-Bonferroni correction and post Kruskal-Wallis test rejecting the null hypothesis with p-value (cid:13) 0.01.None1.0000.0000.0000.000None1.0000.0000.0000.000Rec, perturbations to COnly COnly KBoth C, K0.0001.0000.0000.0000.0000.0001.0000.1150.0000.0000.1151.000Rec, perturbations to KOnly COnly KBoth C, K0.0001.0000.0000.0000.0000.0001.0000.651Rec, perturbations to C and KOnly COnly KNone1.0000.0000.0000.0000.0001.0000.0000.0000.0000.0001.0000.2710.0000.0000.6511.000Both C, K0.0000.0000.2711.000RobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KRobustnessNoneOnly COnly KBoth C, KReferencesSyst. 30 (2017) 3146–3154.116 (32) (2019) 15849–15854.Exp. 2021 (12) (2021) 124003.[1] J.H. Friedman, Greedy function approximation: a gradient boosting machine, Ann. Stat. 29 (2001) 1189–1232.[2] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T.-Y. Liu, LightGBM: a highly efficient gradient boosting decision tree, Adv. Neural Inf. Process. [3] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436–444.[4] M. Belkin, D. Hsu, S. Ma, S. Mandal, Reconciling modern machine-learning practice and the classical bias–variance trade-off, Proc. Natl. Acad. Sci. [5] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, I. Sutskever, Deep double descent: where bigger models and more data hurt, J. Stat. Mech. Theory [6] B. Goodman, S. Flaxman, European Union regulations on algorithmic decision-making and a “right to explanation”, AI Mag. 38 (3) (2017) 50–57.[7] A. Jobin, M. Ienca, E. Vayena, The global landscape of AI ethics guidelines, Nat. Mach. Intell. 1 (9) (2019) 389–399.[8] A. Adadi, M. Berrada, Peeking inside the black-box: a survey on eXplainable Artificial Intelligence (XAI), IEEE Access 6 (2018) 52138–52160.[9] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, D. Pedreschi, A survey of methods for explaining black box models, ACM Comput. Surv. 51 (5) (2018) 1–42.(2019) 206–215.pp. 4768–4777.Systems, vol. 29.[10] C. Rudin, Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead, Nat. Mach. Intell. 1 (5) [11] M.T. Ribeiro, S. Singh, C. Guestrin, “Why should I trust you?” explaining the predictions of any classifier, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1135–1144.[12] S.M. Lundberg, S.-I. Lee, A unified approach to interpreting model predictions, in: Advances in Neural Information Processing Systems, 2017, [13] B. Kim, R. Khanna, O.O. Koyejo, Examples are not enough, learn to criticize! Criticism for interpretability, in: Advances in Neural Information Processing [14] C. Chen, O. Li, D. Tao, A. Barnett, C. Rudin, J.K. Su, This looks like that: deep learning for interpretable image recognition, in: H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information Processing Systems, vol. 32, 2019.[15] S. Wachter, B. Mittelstadt, C. Russell, Counterfactual explanations without opening the black box: automated decisions and the GDPR, Harv. J. Law Technol. 31 (2017) 841.intelligence, IEEE Access 9 (2021) 11974–12001.[16] I. Stepin, J.M. Alonso, A. Catala, M. Pereira-Fariña, A survey of contrastive and counterfactual explanation generation methods for explainable artificial [17] D. Slack, S. Hilgard, H. Lakkaraju, S. Singh, Counterfactual explanations can be manipulated, arXiv preprint, arXiv:2106 .02666.[18] S. Barocas, A.D. Selbst, M. Raghavan, The hidden assumptions behind counterfactual explanations and principal reasons, in: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 2020, pp. 80–89.[19] J.C. Gower, A general coefficient of similarity and some of its properties, Biometrics (1971) 857–871.[20] R. Guidotti, A. Monreale, S. Ruggieri, D. Pedreschi, F. Turini, F. Giannotti, Local rule-based explanations of black box decision systems, arXiv preprint, [21] S. Sharma, J. Henderson, J. Ghosh, CERTIFAI: a common framework to provide explanations and analyse the fairness and robustness of black-box models, in: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 2020, pp. 166–172.[22] R.K. Mothilal, A. Sharma, C. Tan, Explaining machine learning classifiers through diverse counterfactual explanations, in: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 2020, pp. 607–617.arXiv:1805 .10820.28M. Virgolin and S. FracarosArtificial Intelligence 316 (2023) 103840[23] T. Laugel, M.-J. Lesot, C. Marsala, X. Renard, M. Detyniecki, Comparison-based inverse classification for interpretability in machine learning, in: Interna-tional Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems, Springer, 2018, pp. 100–111.[24] M.T. Keane, B. Smyth, Good counterfactuals and where to find them: a case-based technique for generating counterfactuals for explainable AI (XAI), in: International Conference on Case-Based Reasoning, Springer, 2020, pp. 163–178.[25] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks are universal approximators, Neural Netw. 2 (5) (1989) 359–366.[26] H. Fokkema, R. de Heide, T. van Erven, Attribution-based explanations that provide recourse cannot be robust, arXiv preprint, arXiv:2205 .15834.[27] U. Grömping, South German credit data: correcting a widely used data set, report 4/2019, Reports in Mathematics, Physics and Chemistry, Department II, Beuth University of Applied Sciences Berlin, 2019, https://archive .ics .uci .edu /ml /datasets /South +German +Credit +%28UPDATE %29.[28] H. Hofmann, Statlog German credit data, https://archive .ics .uci .edu /ml /datasets /Statlog +(German +Credit +Data), 1994.[29] R. Kohavi, B. Becker, Census income, https://archive .ics .uci .edu /ml /datasets /adult, 1996.[30] R. Kohavi, Scaling up the accuracy of naive-Bayes classifiers: a decision-tree hybrid, in: Proceedings of the Second International Conference on Knowl-edge Discovery and Data Mining, vol. 96, 1996, pp. 202–207.[31] D. Harrison Jr., D.L. Rubinfeld, Hedonic housing prices and the demand for clean air, J. Environ. Econ. Manag. 5 (1) (1978) 81–102.[32] M. Carlisle, Racist data destruction?, https://medium .com /@docintangible /racist -data -destruction -113e3eff54a8, 2019.[33] A.A. Imran, M.S. Rahim, T. Ahmed, Mining the productivity data of the garment industry, Int. J. Bus. Intell. Data Min. 19 (3) (2021) 319–342.[34] J. Larson, S. Mattu, L. Kirchner, J. Angwin, How we analyzed the COMPAS recidivism algorithm, https://www.propublica .org /article /how-we -analyzed -the -compas -recidivism -algorithm, 2016.Machine Learning, PMLR, 2018, pp. 2564–2572.preprint, arXiv:1907.09294.Processing Systems, 2021.[35] M. Kearns, S. Neel, A. Roth, Z.S. Wu, Preventing fairness gerrymandering: auditing and learning for subgroup fairness, in: International Conference on [36] T. Laugel, M.-J. Lesot, C. Marsala, X. Renard, M. Detyniecki, The dangers of post-hoc interpretability: unjustified counterfactual explanations, arXiv [37] F. Ding, M. Hardt, J. Miller, L. Schmidt, Retiring adult: new datasets for fair machine learning, in: Thirty-Fifth Conference on Neural Information [38] W. La Cava, J.H. Moore, Genetic programming approaches to learning fair classifiers, in: Proceedings of the 2020 Genetic and Evolutionary Computation Conference, GECCO ’20, Association for Computing Machinery, New York, NY, USA, 2020, pp. 967–975.[39] M. Virgolin, A. De Lorenzo, F. Randone, E. Medvet, M. Wahde, Model learning with personalized interpretability estimation (ml-pie), in: Proceedings of the Genetic and Evolutionary Computation Conference Companion, GECCO ’21, Association for Computing Machinery, New York, NY, USA, 2021, pp. 1355–1364.[40] R. Dominguez-Olmedo, A.H. Karimi, B. Schölkopf, On the adversarial robustness of causal algorithmic recourse, in: Proceedings of the 39th International Conference on Machine Learning, vol. 162, PMLR, 2022, pp. 5324–5342.[41] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: machine learning in Python, J. Mach. Learn. Res. 12 (2011) 2825–2830.[42] R. Guidotti, A. Monreale, F. Giannotti, D. Pedreschi, S. Ruggieri, F. Turini, Factual and counterfactual explanations for black box decision making, IEEE Intell. Syst. 34 (6) (2019) 14–23.[43] J.A. Nelder, R. Mead, A simplex method for function minimization, Comput. J. 7 (4) (1965) 308–313.[44] F. Gao, L. Han, Implementing the Nelder-Mead simplex algorithm with adaptive parameters, Comput. Optim. Appl. 51 (1) (2012) 259–277.[45] P. Virtanen, R. Gommers, T.E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S.J. van der Walt, M. Brett, J. Wilson, K.J. Millman, N. Mayorov, A.R.J. Nelson, E. Jones, R. Kern, E. Larson, C.J. Carey, ˙I. Polat, Y. Feng, E.W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E.A. Quintero, C.R. Harris, A.M. Archibald, A.H. Ribeiro, F. Pedregosa, P. van Mulbregt, SciPy 1.0 Contributors, SciPy 1.0: fundamental algorithms for scientific computing in Python, Nat. Methods 17 (2020) 261–272.[46] M. D’Orazio, Distances with mixed type variables some modified Gower’s coefficients, arXiv preprint, arXiv:2101.02481.[47] A.-H. Karimi, J. Von Kügelgen, B. Schölkopf, I. Valera, Algorithmic recourse under imperfect causal knowledge: a probabilistic approach, arXiv preprint, arXiv:2006 .06831.[48] A.-H. Karimi, B. Schölkopf, I. Valera, Algorithmic recourse: from counterfactual explanations to interventions, in: Proceedings of the 2021 ACM Confer-ence on Fairness, Accountability, and Transparency, FAccT ’21, Association for Computing Machinery, New York, NY, USA, 2021, pp. 353–362.[49] S. Dandl, C. Molnar, M. Binder, B. Bischl, Multi-objective counterfactual explanations, in: International Conference on Parallel Problem Solving from Nature, Springer, 2020, pp. 448–469.[50] T. Laugel, M.-J. Lesot, C. Marsala, X. Renard, M. Detyniecki, Unjustified classification regions and counterfactual explanations in machine learning, in: Joint European Conference on Machine Learning and Knowledge Discovery in Databases, Springer, 2019, pp. 37–54.[51] M. Pawelczyk, K. Broelemann, G. Kasneci, On counterfactual explanations under predictive multiplicity, in: Conference on Uncertainty in Artificial Intelligence, PMLR, 2020, pp. 809–818.[52] K. Rawal, E. Kamar, H. Lakkaraju, Algorithmic recourse in the wild: understanding the impact of data and model shifts, arXiv preprint, arXiv:2012 .11788.[53] A. Van Looveren, J. Klaise, Interpretable counterfactual explanations guided by prototypes, arXiv preprint, arXiv:1907.02584.[54] A. Dhurandhar, P.-Y. Chen, R. Luss, C.-C. Tu, P. Ting, K. Shanmugam, P. Das, Explanations based on the missing: towards contrastive explanations with pertinent negatives, in: Advances in Neural Information Processing Systems, vol. 31.[55] S. Verma, J. Dickerson, K. Hines, Counterfactual explanations for machine learning: a review, arXiv preprint, arXiv:2010 .10596.[56] A.-H. Karimi, G. Barthe, B. Schölkopf, I. Valera, A survey of algorithmic recourse: contrastive explanations and consequential recommendations, ACM [57] A. Artelt, V. Vaquet, R. Velioglu, F. Hinder, J. Brinkrolf, M. Schilling, B. Hammer, Evaluating robustness of counterfactual explanations, in: IEEE Sympo-[58] M. Pawelczyk, C. Agarwal, S. Joshi, S. Upadhyay, H. Lakkaraju, Exploring counterfactual explanations through the lens of adversarial examples: a theo-Computing Surveys (CSUR).sium Series on Computational Intelligence, IEEE, 2021, pp. 01–09.retical and empirical analysis, arXiv preprint, arXiv:2106 .09992.[59] V. Ballet, X. Renard, J. Aigrain, T. Laugel, P. Frossard, M. Detyniecki, Imperceptible adversarial attacks on tabular data, arXiv preprint, arXiv:1911.03274.[60] T. Freiesleben, The intriguing relation between counterfactual explanations and adversarial examples, Minds Mach. (2021) 1–33.[61] A. Ferrario, M. Loi, The robustness of counterfactual explanations over time, IEEE Access.[62] A. Ferrario, M. Loi, A series of unfortunate counterfactual events: the role of time in counterfactual explanations, arXiv preprint, arXiv:2010 .04687.[63] R. Mochaourab, S. Sinha, S. Greenstein, P. Papapetrou, Robust counterfactual explanations for privacy-preserving SVM, in: International Conference on Machine Learning (ICML 2021), Workshop on Socially Responsible Machine Learning, 2021.[64] M.T. Ribeiro, S. Singh, C. Guestrin Anchors, High-precision model-agnostic explanations, in: Proceedings of the AAAI Conference on Artificial Intelligence, [65] X. Zhang, A. Solar-Lezama, R. Singh, Interpreting neural network judgments via minimal, stable, and symbolic corrections, in: Advances in Neural vol. 32, 2018.Information Processing Systems, vol. 31.29M. Virgolin and S. FracarosAppendix referencesArtificial Intelligence 316 (2023) 103840[66] B.L. Miller, D.E. Goldberg, Genetic algorithms, tournament selection, and the effects of noise, Complex Syst. 9 (3) (1995) 193–212.[67] C.R. Harris, K.J. Millman, S.J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N.J. Smith, R. Kern, M. Picus, S. Hoyer, M.H. van Kerkwijk, M. Brett, A. Haldane, J.F. del Río, M. Wiebe, P. Peterson, P. Gérard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, T.E. Oliphant, Array programming with NumPy, Nature 585 (7825) (2020) 357–362.30