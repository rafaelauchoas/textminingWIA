Artificial Intelligence 195 (2013) 440–469Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintImproving resource allocation strategies against human adversariesin security games: An extended studyRong Yang a,∗, Christopher Kiekintveld b, Fernando Ordóñez a,c, Milind Tambe a, Richard John aa University of Southern California, Los Angeles, CA, USAb University of Texas El Paso, El Paso, TX, USAc University of Chile, Santiago, Chilea r t i c l ei n f oa b s t r a c tArticle history:Received 16 December 2011Received in revised form 13 November 2012Accepted 15 November 2012Available online 20 November 2012Keywords:Bounded rationalityStackelberg gamesDecision-makingStackelberg games have garnered significant attention in recent years given their deploy-ment for real world security. Most of these systems, such as ARMOR, IRIS and GUARDShave adopted the standard game-theoretical assumption that adversaries are perfectly ra-tional, which is standard in the game theory literature. This assumption may not hold inreal-world security problems due to the bounded rationality of human adversaries, whichcould potentially reduce the effectiveness of these systems.In this paper, we focus on relaxing the unrealistic assumption of perfectly rational adver-sary in Stackelberg security games. In particular, we present new mathematical models ofhuman adversaries’ behavior, based on using two fundamental theory/method in humandecision making: Prospect Theory (PT) and stochastic discrete choice model. We also pro-vide methods for tuning the parameters of these new models. Additionally, we proposea modification of the standard quantal response based model inspired by rank-dependentexpected utility theory. We then develop efficient algorithms to compute the best responseof the security forces when playing against the different models of adversaries. In orderto evaluate the effectiveness of the new models, we conduct comprehensive experimentswith human subjects using a web-based game, comparing them with models previouslyproposed in the literature to address the perfect rationality assumption on part of the ad-versary.Our experimental results show that the subjects’ responses follow the assumptions ofour new models more closely than the previous perfect rationality assumption. We alsoshow that the defender strategy produced by our new stochastic discrete choice modeloutperform the previous leading contender for relaxing the assumption of perfect rational-ity. Furthermore, in a separate set of experiments, we show the benefits of our modifiedstochastic model (QRRU) over the standard model (QR).1© 2012 Elsevier B.V. All rights reserved.* Corresponding author.E-mail address: yangrong@usc.edu (R. Yang).1 This paper significantly extends our previous conference paper (Yang et al., 2011) [1] by providing (i) new methods for setting parameters of theProspect Theory model; (ii) an additional variant of Quantal Response model and a new algorithm to compute defender strategies against the new model;(iii) a more comprehensive set of experiments which includes multiple new algorithms and updated settings for the algorithms; (iv) new analysis of therobustness of different defender strategies and the predictive accuracy of different models; (v) additional discussion of related work.0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.11.004R. Yang et al. / Artificial Intelligence 195 (2013) 440–4694411. IntroductionStackelberg game models have recently become important tools for analyzing real-world security resource allocationproblems, such as critical infrastructure protection [2] and robot patrolling strategies [3,4]. These models provide a sophisti-cated approach for generating unpredictable, randomized strategies that mitigate the ability of attackers to find weaknessesusing surveillance. The ARMOR [5], IRIS [6], GUARDS [7] and PROTECT [8] systems are notable examples where this approachhas been used to develop decision-support systems for real-world security problems. One of the key sets of assumptionsthat these systems make are about how attackers will choose attack strategies based on their preferences and observationsof the security policy. Typically, such systems have applied the standard game-theoretic assumption that attackers are per-fectly rational and will strictly maximize their expected utility. This is a reasonable starting point for the first generationof deployed systems. Unfortunately, this standard game-theoretic assumption leaves open the possibility that the defender’sstrategy is not robust against attackers using different decision procedures, and it fails to exploit known weaknesses in thedecision-making of human attackers.It is widely accepted that standard game-theoretic assumptions of perfect rationality are not ideal for predicting thebehavior of humans in multi-agent decision problems [9,10]. A large variety of alternative models have been studied inbehavioral game theory and cognitive psychology that capture some of the deviations of human decisions from perfectrationality. In the multi-agent systems community there is a growing interest in adopting these models to improve decisionsin agents that interact with humans or to provide better advice to human decision-makers in multi-agent decision-supportsystems [11,12]. Our work in this paper focuses on integrating these more realistic models of human behavior into thecomputational analysis of Stackelberg game models in security settings, which are often referred to as Stackelberg securitygames [13–15]. We also provide a case study in this general paradigm of introducing more realistic models of humanbehavior into game theoretic analysis. While there are quite a few studies looking at the problem of predicting humanbehavior, there are very few examples where this is actually included in a real decision-making system. Our work here isone of the first examples showing that this is possible, and actually improves performance in an important class of games.In order to move beyond perfect rationality assumptions to integrate more realistic models of human decision-making inreal-world security systems, we address several key challenges. First, the literature has introduced a multitude of potentialmodels on human decision making [16,9,17,10], but each of these models has its own set of assumptions and there is littleconsensus on which model is best for different types of domains. Therefore, there is an important empirical question ofwhich model best represents the salient features of human behavior in the important class of applied security games. Sec-ond, integrating any of the proposed models into a decision-support system (even for the purpose of empirically evaluatingthe model) requires developing new algorithms for computing solutions to Stackelberg security games, since most existingalgorithms are based on mathematically optimal attackers [18,19]. One notable exception is Cobra developed by Pita etal. [20]. Cobra is one example of modeling bounded rationality of human adversaries by taking into account(i) the anchoring bias of humans while interpreting the probabilities of several events [21,22];(ii) the limited computational ability of humans which may lead to deviation from their best response.To the best of our knowledge, Cobra is the best performing strategy for Stackelberg security games in experiments withhuman subjects. Thus, the open question is whether there are other approaches that allow for fast solutions and outperformCobra in addressing human behavior in security games.In this paper, we significantly expand the previous work on modeling human behavior in Stackelberg security games byimplementing and evaluating strategies based on two very important methods in literature of modeling human decision-making. The first relates to Prospect Theory (PT), which provides a descriptive framework for decision-making under un-certainty that accounts for both risk preferences (e.g. loss aversion) and variations in how humans interpret probabilitiesthrough a weighting function [16]. The other method adapts the ideas in the literature on discrete choice problems [23–26]to a game-theoretic framework with the basic premise that humans will choose better actions more frequently, but withsome noise in the decision-making process that leads to stochastic choice probabilities following a logit distribution. We firstpropose two mathematical models of the adversary’s decision-making based on Prospect Theory: one of them assumes theadversary maximizes ‘prospect’ in their decision making process and the other assumes the adversary makes bounded errorin computing such ‘prospect’ so he may deviate to a sub-optimal solution within a bound. We then propose two mathemat-ical models of how an adversary makes decisions based on using a logit discrete choice models. One model (QR) couplesthe quantal response of the adversary with the expected utility for attacking each target; the other model (QRRU) modifiesthe expected utility by adding extra weight to the target covered with minimum resources, inspired by rank-dependentexpected utility theory [27].Based on the above models of adversary decision making, computing the defender’s corresponding best response isalso challenging since it involves solving non-convex and non-linear optimization problems. We develop new techniques toaddress these problems. In particular, we develop a Mixed Integer Linear Program to compute the defender optimal strategyagainst the PT based models by representing the non-linear functions from Prospect Theory with piecewise approximations.Furthermore, we present a local search method with random restarts to compute the defender optimal strategy against thestochastic models of the adversary.442R. Yang et al. / Artificial Intelligence 195 (2013) 440–469Table 1Notations used in this paper.TxiqiRdiP diRaiP aiMSet of targets; ti in T denotes the ith targetProbability that target ti is covered by a resourceProbability that target ti is attacked by the adversaryDefender reward when covering ti if it’s attackedDefender penalty when not covering ti if it’s attackAttacker reward for attacking ti if it’s not coveredAttacker penalty on attacking ti if it’s coveredTotal number of resourcesIn order to compare the performance of different adversary models, we conduct an extensive empirical evaluation usingthe crowd-source platform Amazon Mechanical Turk2 (AMT). First, we design an online game called “The Guard and theTreasure” to simulate a security scenario similar to the ARMOR program for the Los Angeles International (LAX) airport [5].We then develop classification techniques to select payoff structures for experiments such that the models are well separatedfrom each other and the payoff structures are representative of the game space. We compare our new methods against arobust baseline algorithm MAXIMIN, a perfect rationality baseline (Dobss) and the previous leading contender (Cobra) in theexperiments. Our experimental results show that: (i) our new models more accurately represent the adversaries’ behaviorin security games than previous methods; (ii) strategies based on our new models lead to statistically (and practically)significant higher defender expected utility than the previous leading contender (Cobra). Moreover, we identify situationswhere the QRRU model of adversary leads to significantly better strategies than the QR model.The rest of the paper is organized as follows. Section 2 provides necessary background information of Stackelberg securitygames and defines the notation that will be used in the paper. Section 3 presents the new models of adversary decision-making based on Prospect Theory and Quantal Response Equilibrium. Following that, Section 4 describes the algorithms wedeveloped to compute optimal defender strategy against these new adversary models. In Section 5, we explain the methodswe used to decide the parameters of different models. Section 6 presents our experimental setup and results. We thendiscuss additional related work in Section 7 and summarize the paper in Section 8.2. Stackelberg security gamesIn this section, we first define Stackelberg security games as well as the notation used in this paper. We then introducean online game designed as a testbed to collect data and evaluate performance of the different algorithms introduced inthis paper for solving Stackelberg security games.2.1. Definition and notationWe consider a Stackelberg Security Game (SSG) [1,28] with a single leader and one follower, where the defender playsthe role of the leader and the adversary plays the role of the follower. The defender has to protect a set of targets frombeing attacked by the adversary. The defender has a limited number of resources, e.g., she may need to protect 8 targetswith 3 guards. Each player has a set of pure strategies. In SSGs, a pure strategy of an adversary is defined as attacking asingle target; and a pure strategy of a defender is defined as an assignment of all the security resources to the set of targets(e.g. assigning the three resources to targets 1, 3 and 6). An assignment of a security resource to a target is also referred toas covering a target. A mixed-strategy is defined as a probability distribution over the set of all possible pure strategies.We use the following notation to describe a SSG, also listed in Table 1: the defender has a total of M resources to protecta set of targets T = {ti}. The outcomes of the SSG depend only on whether or not the attack is successful. Given a target ti ,the defender receives reward Rdif the adversary attacks a target that is covered by the defender; otherwise, the defenderireceives penalty P dA key property of SSG is that while the games may be non-zero-sum, Rdresources to cover a target helps the defender and hurts the attacker.in the former case; and reward Raii > P ai > P din the latter case.i , ∀i [28]. In other words, addingi . Correspondingly, the attacker receives penalty P ai and RaiWe represent the defender’s mixed-strategy by x which describes the probability that each target will be protected bya resource and denote these individual probabilities by xi . So we have x = (cid:4)xi(cid:5) as the marginal distribution on each target.In the example where the defender has to protect 8 targets with 3 resources (guards), the defender’s mixed-strategy can bewritten as x = (cid:4)x1, . . . , x8(cid:5). We focus on generating marginal distributions rather than distributions over the original defenderpure strategies (e.g., the originalpure strategies) for improved algorithmic efficiency [19,29]. In this paper, we considerthe case without any constraints on assigning the resources, which models important domains such as ARMOR deployedat LAX [5]. Korzhyk et al. show in [29] that the marginal probability distribution of covering each target is equivalent toa mixed-strategy over the original combinational defender pure strategies in such domains. Moreover, given the marginalcoverage on each target, we could use a technique called ‘comb sampling’ [30] to implement the corresponding mixed-strategy over the set of the actual assignments of the resources.(cid:2)83(cid:3)2 https://www.mturk.com.R. Yang et al. / Artificial Intelligence 195 (2013) 440–469443Fig. 1. Game interface for our simulated online SSG.In SSGs, the defender (leader) first commits to a mixed-strategy, assuming the attacker (follower) decides on a purestrategy after observing the defender’s strategy. This models the situation where an attacker conducts surveillance to learnthe defender’s mixed-strategy and then launches an attack on a single target. We denote the attacker’s choice using a vectorof variables q = (cid:4)qi(cid:5) for ti ∈ T , where qi ∈ [0, 1] represents the probability that target ti will be attacked. Furthermore, wecould compute the expected utility for the adversary assuming the target ti is attacked by the adversary asi (xi) = xi P aU ai+ (1 − xi)Raiand the expected utility for the defender in this case isi (xi) = xi RdU di+ (1 − xi)P di2.2. A simulated online SSG(1)(2)We develop a game, called “The Guards and The Treasure”, to simulate the security model at the LAX airport, whichhas eight terminals that can be targeted in an attack [5]. Fig.1 shows the interface of the game. Players are introduced tothe game through a series of explanatory screens describing how the game is played. In each game instance a subject isasked to choose one of the eight gates to open (attack). They are told that guards are protecting three of the eight gates,but not which ones. The defender’s mixed strategy, represented as the marginal probability of covering each target, (cid:4)xi(cid:5),is given to the subjects. At the same time, the subjects are also told the reward on successfully attacking each target aswell as the penalty of getting caught at each target. The three gates protected by the guards are drawn randomly from theprobability shown on the game interface. If subjects select a gate protected by the guards, they receive a penalty; otherwise,they receive a reward. Subjects are rewarded based on the reward/penalty shown for each gate. For example, in the gameshown in Fig. 1, the probability that gate 1 (target 1) will be protected by a guard is 0.59. Assuming the subjects choosegate 1, he/she gets reward of 8 if gate 1 is not protected by the guard; or get a penalty of −3 if gate 1 is protected by aguard.3. New models for predicting attacker behaviorsExisting models of adversary behavior in SSGs have poor performance in predicting the behavior of human adversaries[20]. In order to design better defender strategy, better models of adversary decision-making need to be developed. In thissection, we present three models of adversary’s behavior in SSGs, based on using Prospect Theory and Quantal ResponseEquilibrium. All of the models have key parameters. We describe in the next section our methodology for setting theseparameters in each case.444R. Yang et al. / Artificial Intelligence 195 (2013) 440–4693.1. Prospect TheoryFig. 2. Prospect Theory empirical function forms.Prospect Theory provides a descriptive model of how humans make decision among alternatives with risk, which isa process of maximizing the ‘prospect’, which will be defined soon, rather than the expected utility. More formally, theprospect of a certain alternative is defined as(cid:4)lπ (xl)V (Cl)(3)In Eq. (3), xl denotes the probability of receiving Cl as the outcome. The weighting function π (·) describes how proba-bility xl is perceived by individuals. An empirical function form of π (·) (Eq. (4)) is shown in Fig. 2(a) [31].π (x) =xγ(xγ + (1 − x)γ )1γ(4)The key concepts of a weighting function are that individuals overestimate low probability and underestimate high proba-bility [16,31]. Also, π () is not consistent with the definition of probability, i.e. π (x) + π (1 − x) (cid:2) 1 in general.The value function V (Cl) in Eq. (3) reflects the value of the outcome Cl. PT predicts that individuals are risk averseregarding gain but risk seeking regarding loss, implying an S-shaped value function [16,31]. A key component of ProspectTheory is the reference point. Outcomes lower than the reference point are considered as loss and higher as gain.(cid:5)V (C) =C α,C (cid:3) 0−θ(−C)β , C < 0(5)Eq. (5) is a general form for the value function where C is the relative outcome to the reference. In Eq. (5), we assumethe reference point to be at 0. α and β determine the extent of non-linearity in the curves. If the parameters α = 1.0 andβ = 1.0, the function would be linear; typical values for both α and β are 0.88 [31]. θ captures the idea that the loss curveis usually steeper than the gains curve, a typical value of θ is 2.25 [31], which reflects a finding that losses are a little morethan twice as painful as gains are pleasurable. The function is also displayed in Fig. 2(b) [31]. Given these parameters, wewill henceforth denote this value function with V α,β,θ .In a SSG, the prospect of attacking target ti for the adversary is computed as(cid:3)(cid:3)(cid:2)(cid:2)prospect(ti) = π (xi)V α,β,θP ai+ π (1 − xi)V α,β,θRaiAccording to Prospect Theory, subjects will choose the target with the highest prospect. Thus,(cid:5)qi =if prospect(ti) (cid:3) prospect(ti(cid:7) ), ∀ti(cid:7) ∈ T1,0, otherwise(6)(7)3.2. Quantal ResponseQuantal Response Equilibrium (QRE) is an important solution concept in behavioral game theory [17]. It is based on along history of work in single-agent problems and brings that work into a game-theoretic setting [32,33]. It assumes thatinstead of strictly maximizing utility, individuals respond stochastically in games: the chance of selecting a non-optimalR. Yang et al. / Artificial Intelligence 195 (2013) 440–469445strategy increases as the cost of such an error decreases. Given the strategy profile of all the other players, the response ofa player is modeled as a quantal response (QR model): he/she selects action i with a probability given byqi(x) =(cid:6)eλU ai (x)tk∈T eλU ak (x)(8)i (x) is the expected utility for the attacker for selecting pure strategy i. Here, λ ∈ [0, ∞] is the parameter thatwhere, U acaptures the rational level of player p: one extreme case is λ = 0, when player p plays uniformly random; the otherextreme case is λ → ∞, when the quantal response is identical to the best response. Combining Eqs. (8) and (1),qi(x) =(cid:6)−P aeλRai etk∈T eλRak e−λ(Rai−λ(Raki )xi−P ak )xk(9)In applying the QR model to the security game domain, we only consider noise in the response of the adversary. Thedefender uses a computer decision support system to choose her strategy hence is able to compute optimal strategy. On theother hand, since the attacker observes the defender’s strategy first to decides his response, it can only hurt the defenderto add noise in her response. Recent work [33] shows Quantal Level-k [32] to be best suited for predicting human behaviorin simultaneous move games. The key idea of level-k is that humans can perform only a bounded number of iterations ofstrategic reasoning: a level-0 player plays randomly, a level-k (k > 1) player best response to the level-(k − 1) player. Weapplied QR instead of Quantal Level-k to model the attacker’s response because in Stackelberg security games the attackerobserves the defender’s strategy, so level-k reasoning is not applicable.3.3. Quantal Response with Rank-related Expected UtilityWe modify the Quantal Response Model by taking into consideration the fact that individuals are attracted to extremeevents, such as the less uncertain and highest payoff. This idea is inspired by the rank-dependent Expected Utility Model[27], in which the utilities of choosing different alternatives are based on the their ranks. We adapt this idea to securitygames, but we only consider such effect on the target covered with minimum resources. That is the adversary wouldprefer the target covered with minimum resources since he is most likely to be successful attacking that target. This couldsignificantly reduce the defender’s reward in the case when this target with fewest resources also gives a large penalty tothe defender.We modify the QR model by adding extra weight to the target covered with minimum resources. We refer this modifiedmodel as Quantal Response with Rank-related expected Utility (QRRU) model, where the probability that the attacker attackstarget ti is computed asqi(x) =(cid:6)i (xi )eλs S i (x)eλu U atk∈T eλu U ak (xk)eλs Sk(x)where S i(x) ∈ {0, 1} indicating whether ti is covered with least resource.(cid:5)S i(x) =(cid:7)i, ∀ti(cid:7) ∈ Tif xi (cid:2) x1,0, otherwise(10)(11)The denominator in Eq. (10) is only for normalizing the probability distribution so all the qi sum up to 1. In the numerator,we have two terms deciding the probability that target ti will be chosen by the adversary. The first term eλu U ai (xi ) relates toi (xi) is computed as in Eq. (1). The parameter λu (cid:3) 0 representsthe expected utility for the adversary to choose target ti . U athe level of error in adversary’s computation of the expected utility, which is equivalent to λ in Eq. (8). The second termeλs S i (x) relates to the adversary’s preference for the least covered target. Note that if ti is not covered with the minimumresource, this term equals to 1 so there is no extra weight added to the non-minimum covered targets; if ti is coveredwith minimum resource, this term will be (cid:3) 1, adding extra weight to the probability that adversary will choose ti . Theparameter λs (cid:3) 0 represents the level of the adversary’s preference to the minimum covered target. λs = 0 indicates nopreference to the minimum covered target. As λs increase, this preference becomes stronger.4. Computing optimal defender strategyGiven the new models of adversary behavior in SSG, new algorithms need to be developed to compute the optimal de-fender strategy since the existing algorithms are based on the assumption of a perfectly rational adversary. We now describeefficient computation of the optimal defender mixed strategy assuming a human adversary whose response follows one ofthe three models we proposed: Prospect Theory (PT-Adversary), Quantal Response (QR-Adversary) or Quantal Response withRank-related Utility (QRRU-Adversary).446R. Yang et al. / Artificial Intelligence 195 (2013) 440–4694.1. Computing against a PT-adversaryAssuming that the adversary’s response follows Prospect Theory (PT-adversary), we developed two methods to computethe optimal defender strategy.4.1.1. BrptBest Response to Prospect Theory (Brpt) is a mixed integer programming formulation for computing the optimal leaderstrategy against players whose responses follow a PT model. We first present an abstract version of our formulation ofBrpt in Eqs. (12)–(16), and then present a more detailed operational version in Eqs. (17)–(29) that uses piecewise linearapproximation to provide the Brpt MILP (Mixed Integer Linear Program).maxx,q,a,d,zds.t.n(cid:4)i=1n(cid:4)xi (cid:2) Mqi = 1,(cid:2)i=10 (cid:2) a −K (1 − qi) +qi ∈ {0, 1}(cid:2)(cid:3)(cid:2)(cid:3)(cid:3)π (xi)V(cid:2)+ π (1 − xi)VP ai+ (1 − xi)P diRai(cid:3) d, ∀i(cid:3)xi Rdi(cid:2) K (1 − qi), ∀i(12)(13)(14)(15)(16)The objective is to maximize d, the defender’s expected utility. Eq. (13) enforces that the constraint on the total amountof resources is met. In Eq. (14), the integer variables qi represent the attacker’s pure strategy. In Brpt, qi is constrained to bebinary variable, since, as justified and explained in [18], we assume the adversary has a pure strategy best response: qi = 1 ifti is attacked and 0 otherwise. Eq. (15) is the key to decide the attacker’s strategy, given a defender’s mixed strategy x = (cid:4)xi(cid:5).The variable a represents the attacker’s ‘benefit’ of choosing a pure strategy (cid:4)qi(cid:5). Since we are modeling attacker’s decisionis the attacker’s ‘prospect’,making using Prospect Theory, the benefit perceived by the adversary for attacking target tiwhich is calculated as (π (xi)V (P ai )) following Eq. (3). The attacker tries to maximize a by choosing thetarget with the highest ‘prospect’, as enforced by Eq. (15). In particular, the inequality on the left side of Eq. (15) enforcesthat a is greater or equal to the ‘prospect’ of attacking any target. On the right hand of Eq. (15), we have a constantparameter K with a very large positive value. For targets with qi = 0, the upper bound of the difference between a andthe ‘prospect’ is K , therefore, the bounds is not operational. For target with qi = 1 (i.e. the target chosen by the attacker),the value of a is forced to be equal to the actual ‘prospect’ of attacking that target. In Eq. (16), the constant parameter Kenforces that d is only constrained by the target that is attacked by the adversary (i.e. qi = 1).i ) + π (1 − xi)V (Rai ) and (Rai )(cid:7) = V (P aWe now present the Brpt MILP based on our piecewise linear approximation of the weighting function as discussedearlier. We use the empirical functions introduced in Section 3.1 for the weighting function π (·) and value function V (·).Let (P ai , which are both given asinput to the optimization formula in Eqs. (13)–(16). The key challenge to solve that optimization problem is that the π (·)function is non-linear and non-convex. If we apply the function directly, we have to solve a non-linear and non-convexmixed-integer optimization problem, which is difficult. Therefore, we approximately solve the problem by representingthe non-linear π (·) function as a piecewise linear function. This transforms the problem into a MILP, which is shown inEqs. (17)–(29).i ) denote the adversary’s value of penalty P ai and reward Rai )(cid:7) = V (Ramaxx,q,a,d,zdn(cid:4)5(cid:4)s.t.xik (cid:2) Mk=1i=15(cid:4)(xik + ¯xik) = 1, ∀ik=10 (cid:2) xik, ¯xik (cid:2) ck − ck−1, ∀i, k = 1 . . . 5zik · (ck − ck−1) (cid:2) xik, ∀i, k = 1 . . . 4¯zik · (ck − ck−1) (cid:2) ¯xik, ∀i, k = 1 . . . 4xi(k+1) (cid:2) zik, ∀i, k = 1 . . . 4¯xi(k+1) (cid:2) ¯zik, ∀i, k = 1 . . . 4zik, ¯zik ∈ {0, 1}, ∀i, k = 1 . . . 4(17)(18)(19)(20)(21)(22)(23)(24)(25)R. Yang et al. / Artificial Intelligence 195 (2013) 440–469447Fig. 3. Piecewise approximation of the weighting function.(cid:7)xi=5(cid:4)k=1bkxik,(cid:7)¯xi=5(cid:4)k=1¯xik, ∀ibkn(cid:4)qi = 1,i=10 (cid:2) a −(cid:2)(cid:2)(cid:7)xiqi ∈ {0, 1}(cid:3)(cid:7) + ¯xP ai5(cid:4)(cid:2)(cid:2)(cid:7)iRaiM(1 − qi) +xik Rdi+ ¯xik P di(cid:3)(cid:3) d, ∀i(cid:3)(cid:7)(cid:3)(cid:2) M(1 − qi), ∀i(26)(27)(28)(29)k=1Let ˜π (·) denote the use of a piecewise linear approximation of the weighting function π (·), as shown in Fig. 3. We em-pirically set 5 segments3 for ˜π (·). This function is defined by {ck|c0 = 0, c5 = 1, ck < ck+1, k = 0, . . . , 5} that representthe endpoints of the linear segments and {bk | k = 1, . . . , 5} that represent the slope of each linear segment. In order torepresent the piecewise linear approximation, i.e. ˜π (xi) (and simultaneously ˜π (1 − xi)), we partition xi (and 1 − xi ) into five(cid:7)segments, denoted by variables xik (and ¯xik). Therefore, xi which equals ˜π (xi) can be calculated as the sum of the linearfunction in each segment(cid:7)xi= ˜π (xi) =5(cid:4)k=1bk · xikwhich is shown in Eq. (26). At the same time, we can enforce the correctness of partitioning xi (and 1 − xi ) by ensuring thatsegment xik (and ¯xik) is positive only if the previous segment is used completely. This is enforced in Eqs. (19)–(25) by usingthe auxiliary integer variable zik (and ¯zik). zik = 0 indicates that the kth segment of xi (i.e. xik) has not been completely(cid:7)= ˜π (xi) as the value of theused, therefore, the following segments can only be set to 0, and vice versa. Eq. (26) defines xi(cid:7)piecewise linear approximation of xi , and ¯xi= ˜π (1 − xi) as the value of the piecewise linear approximation of 1 − xi .4.1.2. RptRobust-PT (Rpt) modifies the base Brpt method to account for the possible uncertainty in adversary’s choice caused (forexample) by imprecise computations [34]. Similar to Cobra, Rpt assumes that the adversary may choose any strategy within(cid:8) of the best choice, defined here by the prospect of each action. It optimizes the worst-case outcome for the defenderamong the set of strategies that have the prospect for the attacker within (cid:8) of the optimal prospect.maxx,h,q,a,d,zds.t. Constraints (18)–(28)(30)3 This piecewise linear representation of π (·) achieves a small approximation error: supz∈[0,1] (cid:10)π (z) − ˜π (z)(cid:10) (cid:2) 0.03.448R. Yang et al. / Artificial Intelligence 195 (2013) 440–469n(cid:4)hi (cid:3) 1i=1hi ∈ {0, 1},(cid:8)(1 − hi) (cid:2) a −5(cid:4)(cid:2)M(1 − hi) +qi (cid:2) hi, ∀i(cid:3)(cid:7) + ¯x(cid:2)P ai(cid:7)xi(cid:2)(cid:7)i(cid:2)xik Rdi+ ¯xik P diRai(cid:3)(cid:3)(cid:7)(cid:3)(cid:2) M(1 − hi) + (cid:8), ∀i(cid:3) d, ∀i(31)(32)(33)(34)k=1We modify the Brpt optimization problem as follows: the first 11 constraints are equivalent to those in Brpt (Eq. (18)–(28)); in Eq. (31), the binary variable hi indicates the (cid:8)-optimal strategy for the adversary; the (cid:8)-optimal assumption isembedded in Eq. (33), which forces hi = 1 for any target ti that leads to a prospect within (cid:8) of the optimal prospect, i.e.a; Eq. (34) enforces d to be the minimum expected utility for defender on the targets that lead to (cid:8)-optimal prospect forthe attacker. Rpt attempts to maximize the minimum for the defender over the (cid:8)-optimal targets for the attacker, thusproviding robustness against attacker (human) deviations within that (cid:8)-optimal set of targets.4.2. Computing an optimal strategy against a Quantal Response adversaryAssuming the adversary follows a quantal response (QR-adversary), we now present the algorithm to compute the de-fender’s optimal strategy against a QR-adversary. Given the quantal response of the adversary, which is described in Eq. (9),the best response of defender is to maximize her expected utility:U d(x) =maxxn(cid:4)i=1qi(x)U di (x)Combined with Eqs. (9) and (2), the problem of finding the optimal mixed strategy for the defender can be formulated as(cid:6)maxx−P a−λ(Raiti ∈T eλRai e(cid:6)tk∈T eλRak e− P di )xi ((Rdi−P a−λ(Rak )xkki )xi + P di )n(cid:4)xi (cid:2) Ms.t.i=10 (cid:2) xi (cid:2) 1, ∀i, jAlgorithm 1 Brqr.← −∞;1: opt g2: for it ← 1, . . . , IterN do3:4:5:6:7:8: end for9: return opt g , xoptopt gend if∗← optl , xopt ← xx(0) ← randomly generate a feasible starting point∗) ← Find-Local-Minimum(x(0))(optl, xif opt g > optl then(35)(36)(37)Unfortunately, since the objective function in Eq. (35) is non-linear and non-convex, finding the global optimum is ex-tremely difficult. Therefore, we focus on methods to find local optima. To compute an approximately optimal strategy againsta QR-adversary efficiently, we develop the Best Response to Quantal Response (Brqr) heuristic described in Algorithm 1.We first take the negative of Eq. (35), converting the maximization problem to a minimization problem. In each iteration,we find the local minimum using the fmincon() function in Matlab with the Interior Point Algorithm with a given startingpoint. If there are multiple local minima, by randomly setting the starting point in each iteration, the algorithm will reachdifferent local minima with a non-zero probability. By increasing the iteration number, IterN, the probability of reaching theglobal minimum increases. We empirically set IterN to 300 in our experiments.4.3. Computing against a QRRU-adversaryWe now present the algorithm to compute defender optimal strategy assuming the adversary’s behavior follows theQRRU model. The adversary’s response given this model is computed as in Eq. (10). The optimal defender strategy against aQRRU-adversary is computed by solving the following optimization problem:R. Yang et al. / Artificial Intelligence 195 (2013) 440–469(cid:6)maxx,s,xmin−P a−λu(Rati ∈T eλu Rai ei(cid:6)tk∈T eλu Rak es.t. Constraint (36), ( 37)− P di )xi eλs si ((Rdi−λu(Rak )xk eλs skk−P ai )xi + P di )xi − (1 − si)K (cid:2) xmin (cid:2) xi, ∀ti ∈ T(cid:4)si = 1ti ∈Tsi ∈ {0, 1}, ∀ti ∈ T449(38)(39)(40)(41)where the integer variables si are introduced to represent the function S i(x) as shown in Eq. (11). In constraint (39), K isa constant with a very large value. Constraints (39) and (40) enforces xmin to be the minimum value among all the xi .Simultaneously, si is set to 1 if target ti has the minimum coverage probability assigned; and is set to 0 otherwise. Theabove optimization problem is a non-linear and non-convex mixed integer programming problem, which is difficult to solvedirectly. Therefore, we developed Best Response to a QRRU-Adversary (Brqrru), an algorithm that iteratively computesthe defender’s optimal strategy. The iterative approach breaks down the mixed-integer non-linear programming probleminto sub-problems without integer variables. For each sub-problem, one of the target is assumed to be the least coveredtarget. Then, under this constraint, the maximum defender expected utility and the associated defender mixed strategy arecomputed by solving a non-linear programming problem (similar to Brqr). Finally, the sub-problem generating the highestmaximum defender expected utility is found as the ‘actual’ optimal solution, and the associated defender mixed-strategy isthe optimal defender strategy assuming a QRRU-adversary.∗) ← Find-Optimal-Defender-Strategy(si(cid:7) = 1)Algorithm 2 Brqrru.← −∞;1: opt g2: for ti(cid:7) ∈ T do(optl, x3:if opt g > optl then4:5:6:7: end for8: return optg , xoptopt gend if∗← optl , xopt ← xAlgorithm 2 shows the pseudo code of the algorithm. Algorithm 2 describes Brqrru. In each iteration, one target ti(cid:7) isconditioned to be covered with minimum resource, therefore si∗ = 1. This reduces the optimization problem to the following(cid:6)maxx−P a−λu(Rati ∈T eλu Rai ei(cid:6)tk∈T eλu Rak e− P di )xi eλs si ((Rdi−λu(Rak )xk eλs skk−P ai )xi + P di )s.t. Constraint (36), ( 37)xi∗ (cid:2) xi, ∀ti ∈ T(42)(43)where there are no integer variables involved since si , ∀ti ∈ T are all pre-defined parameters of the optimizationproblem. Therefore, we could solve it using the same method of local search with random restart as that in Brqr.Find − Optimal − Defender − Strategy(si(cid:7) = 1) on Line (3) in Algorithm 2 calls Algorithm 1 to solve the optimizationproblem in Eqs. (42)–(43).5. Parameter estimationIn this section, we describe our methodology for setting the values of the parameters for the different models of humanbehavior introduced in the previous section. We set the parameters for our later experiments using data collected in apreliminary set of experiments with human subjects playing the online game we introduced in Section 2.2. We posted thegame on Amazon Mechanical Turk as a Human Intelligent Task (HIT) and asked subjects to play the game. Subjects playedthe role of the adversary and were able to observe the defender’s mixed strategy (i.e., randomized allocation of securityresources). In order to avoid non-compliant participants, we only allowed workers whose HIT approval rates were greaterthan 95% and who had more than 100 approved HITs to participate in the experiment.Let G denote a game instance, which is a combination of a payoff structure {(Rai ), ti ∈ T }, and a defender’si , P a∈ T . We include seven payoff structuresstrategy x. Given a game instance G, we denote the choice of the jth subject as τ Gjin the experiments: four of which are selected based on using a classification method we explain in detail in Section 5.1;the other three are taken directly from Pita et al. [20]. For each payoff structure we tested five different defender strategies.This results in 7 ∗ 5 = 35 different game instances. Each of the subjects played all 35 games. In total, 80 subjects participatedin the preliminary experiment.i , P di , Rd450R. Yang et al. / Artificial Intelligence 195 (2013) 440–469Table 2A-priori defined features.Feature 1mean(| RaiP aiFeature 5mean(| RaiP di|)|)Feature 2std(| RaiP ai|)Feature 6std(| RaiP di|)Feature 3mean(| RdiP diFeature 7mean(| RdiP ai|)|)Feature 4std(| RdiP di|)Feature 8std(| RdiP ai|)5.1. Selecting payoff structuresFig. 4. Payoff structure clusters (color in the web version).Even for a restricted class of games such as security games, there are an infinite number of possible game instancesdepending on the specific values of the payoffs for each of the targets. Since we cannot conduct experiments on everypossible game instance we need a method to select a set of payoffs structures to use in our experiments. Our main criteriafor selecting payoffs structures are (1) to select a diverse set of payoff structures that cover different regions in the spaceof possible security games and (2) to select payoff structures that will differentiate between the different behavioral models(in other words, the models should make different predictions in different test conditions). In the first round our goal wasto select game instance that would distinguish between the three key families of prediction methods (Brpt, Rpt, Brqr). Inthe second round of selection we need to further differentiate within the families. Since there is not yet a well-understoodmethod to select such game instances in the literature, we introduce a procedure for making such selections below.i and P d+[1, 10]; P aWe first sample randomly 1000 different payoff structures, each with 8 targets. Rai are integers drawn from−[−10, −1]. This scale is similar to the payoff structures used in [20].ZWe then use k-means clustering to group the 1000 payoff structures into four clusters based on eight features, which aredefined in Table 2. Intuitively, features 1 and 2 describe how good the game is for the adversary, features 3 and 4 describehow good the game is for the defender, and features 5∼8 reflect the level of conflict between the two players in the sensethat they measure the ratio of one player’s gain over the other player’s loss.i are integers drawn from Zi and RdIn Fig. 4, all 1000 payoff structures are projected onto the first two Principal Component Analysis (PCA) dimensions forvisualization. The three payoff structures (5–7) that were first used in Pita et al. [20] are marked in Fig. 4. All three of thesepayoff structures belong to cluster 3, indicating that the game instances used in the previous experiments we all similar interms of the features we used for classification.4To select specific payoff structures from these clusters we first generated five defender strategies based on the followingfamilies of algorithms: Dobss, Cobra, Brpt, Rpt and Brqr. Here we select only one algorithm from each family (e.g., onlyone version of Brqr). At this point we did not have preliminary data to set the parameters of the algorithms, since we aredeciding which payoff structures to test on. Instead, we set the parameters as follows: Dobss has no parameters; for Cobra4 In [20], there were four payoff structures used, but we only use three of those here. The fourth payoff structure is a zero-sum game, and the deployedStackelberg security games have not been zero sum [5,6]. Furthermore, in zero-sum games, defender’s strategies computed from Dobss, Cobra and Maximincollapse into one — they turn out to be identical.R. Yang et al. / Artificial Intelligence 195 (2013) 440–469451we use parameters drawn from [20]; Brpt and Rpt use the empirical parameter settings for Prospect Theory [31]; Brqr usesa value of λ = 0.76 which we set using the data reported in [20] (using the method to be described in Section 5.3).We use the following the criteria to select payoff structures that differentiate among the different families of algorithms:• We define the distance between two mixed strategies, xk and xl, using the Kullback–Leibler divergence: D(xk, xl) =i /xl• For each payoff structure, D(xk, xl) is measured for every pair of strategies. With five strategies, we have 10 suchDKL(xk|xl) + DKL(xl|xk), where DKL(xk|xl) =(cid:6)ni=1 xki log(xki).measurements.• We remove payoff structures that have a mean or minimum of these 10 quantities below a given threshold. This resultsin a subset of about 250 payoff structures in total for all four clusters. We then select one payoff structure closest tothe cluster center from each of these subsets.The four payoff structures (1–4) we selected from different clusters and are marked in Fig. 4.5.2. Parameter estimation for Prospect TheoryAn empirical setting of parameter values is suggested in the literature [31] based on various experiments conducted withhuman subjects. We also include this setting of parameter values in our experiments to evaluate the benchmark performanceof the prospect theory. At the same time, we provide a method to estimate the parameter values for the PT model usinga set of empirical response data collected for the SSG domain. In this section, we describe our method of estimating theparameter values based on using grid search.The empirical functions we used in the PT model for the adversary have four parameters that must be specified:α, β, θ, γ , as shown in Eqs. (4) and (5). Varying the values for these four parameters will change the responses predicted bythe PT-model. We denote the weighting and value function as πγ (·) and V α,β,θ (·), for a given a set of parameter values. Wethen define the fit of a parameter setting to a given data set of subjects’ choices as the percentage of subjects who choosethe target predicted by the model. The fit can be computed asFit(α, β, θ, γ | G) = 1N(cid:4)j=1..Nqτ Gj(α, β, θ, γ | G) =(cid:4)ti ∈TNiNqi(α, β, θ, γ | G)where qi(·) ∈ {0, 1} indicates whether the PT model predicts target ti to be chosen by the subjects and is computed usingEq. (7), Ni is the number of subjects who choose target ti , and N =ti ∈T Ni is the total number of subjects.We estimate the parameter setting with the best fit for PT model by maximizing the fit function over all 35 game(cid:6)instancesmaxα,β,θ,γ(cid:4)GFit(α, β, θ, γ | G)s.t. 0 < α, β < 1, θ (cid:3) 1, 0 < γ < 1(44)(45)The constraints in (45) restrict the feasible range of all the four parameters, as defined in the prospect theory model. Theobjective function in Eq. (44) cannot be expressed as a closed-form expression of α, β, θ and γ . Without a closed form it isdifficult to apply gradient descent or any other analytical search algorithm to find the optimal solution. Therefore, we usegrid search [35,36] to solve the problem as follows:(1) We first uniformly sample a set of values for each parameter across the feasible ranges, with the following grid intervals:(cid:10)α = 0.05, (cid:10)β = 0.05, (cid:10)γ = 0.05, and (cid:10)θ = 0.1. This gives a set of different values for each of the four parameters. For= αl + k1 · (cid:10)α}, where αl is the lowersimplicity, we represents the four sets of sampled values as the following: {αk1= γl + k4 · (cid:10)γ }. The feasible region of θ= θl + k3 · (cid:10)θ }; and {γk4bound of the region; similarly {βk2does not have upper bound, so we set it to 5 which is twice as the suggested empirical value [31].= βl + k2 · (cid:10)β }; {θk3(2) In total, we have 20 · 20 · 20 · 40 = 320k different combinations of the four parameter values. We then evaluate theobjective function on each of the combinations (αk1 , βk2 , θk3 , γk4 ) and take the parameter combination with the bestaggregate fit as the solution:(cid:2)α∗, β∗, θ∗, γ ∗(cid:3)= arg maxk1,k2,k3,k4(cid:4)GFit(αk1 , βk2 , θk3 , γk4| G)The parameter settings estimated using the method described above are:(cid:2)α∗, β∗, θ∗, γ ∗(cid:3)= (1.0, 0.6, 2.2, 0.6)452R. Yang et al. / Artificial Intelligence 195 (2013) 440–4695.3. Parameter estimation for the QR modelWe now explain how we estimate the parameter for the Quantal Response Model (QR Model). The parameter λ in theQR model represents the level of noise in the adversary’s response function. We employ Maximum Likelihood Estimation(MLE) to fit λ using data we collected. Given a game instance G and N samples of the subjects’ choices {τ j(G), j = 1 . . . N},the likelihood of λ is(cid:7)L(λ | G) =(λ | G)qτ Gjj=1..N∈ T denotes the target attacked by the jth player and qτ Gwhere, τ Gjplayer j attacks target t3 in game G, we would have qτ Gjj(λ | G) can be computed by Eq. (9). For example, if(λ | G) = q3(λ | G). Furthermore, the log-likelihood of λ islog L(λ | G) =N(cid:4)j=1log qτ j (G)(λ | G) =(cid:4)ti ∈TNi log qi(λ)Combining with Eq. (8),log L(λ | G) = λ(cid:4)ti ∈TNi U ai (xi) − N log(cid:9)eλU ai (x)(cid:8) (cid:4)ti ∈TWe learn the optimal parameter setting for λ by maximizing the total log-likelihood over all 35 game instances:log L(λ | G)(cid:4)maxλGs.t. λ (cid:3) 0(46)(47)The objective function in Eq. (46) is concave, since for each G, a log L(λ | x) is a concave function. This can be demonstratedby showing that the second order derivative of log L(λ | G) is non-positive ∀G:(cid:6)i< jd2 log Ldλ2=−(U ai (xi) − U a(cid:6)(j (x j))2ei (xi))2i eλU aλ(U ai (xi )+U aj (x j))(cid:2) 0Therefore, log L(λ | x) only has one local maximum. We use gradient descent solve the above optimization problem. TheMLE of λ is∗ = 0.55λ5.4. Parameter estimation for the QRRU modelFor the QRRU Model, we need to estimate two parameters: λu and λs as defined in Eq. (10). We again apply MaximumLikelihood Estimation, similar to the method for the QR model. Given a game instance G, and the responses of N subjects{τ j(G), j = 1 . . . N}, the log-likelihood of a parameter setting (λu, λs) islog L(λu, λs | G) =N(cid:4)j=1log qτ j (G)(λu, λs | G) =(cid:4)ti ∈TNi log qi(λu, λs)Combining with Eq. (10),log L(λu, λs | G) = λu(cid:4)ti ∈TNi U ai (xi) + λs(cid:4)ti ∈TNi S i(x) − N log(cid:9)eλu U ai (xi )+λs S i (x)(cid:8) (cid:4)ti ∈TWe learn the optimal parameter settings for the QRRU Model by maximizing the total log-likelihood over all 35 gameinstances:(cid:4)maxλu,λsGlog L(λu, λs | G)s.t. λu (cid:3) 0, λs (cid:3) 0(48)(49)The objective function in Eq. (48) is a concave function, since ∀G the Hessian matrix of log L(λu, λs | G) is negative semi-definite. We include the details of proof in the appendix and only show here that ∀(cid:4)λu, λs(cid:5)R. Yang et al. / Artificial Intelligence 195 (2013) 440–469453(cid:4)λu, λs(cid:5) · H(λu, λs | G) · (cid:4)λu, λs(cid:5)T (cid:2) 0where H(λu, λs | G) is the Hessian matrix of log L(λu, λs | G) computed as the followingH(λu, λs | G) = −N⎛(cid:6)Ai+ A j(cid:6)Ai+ A j⎞(cid:6)⎜⎝−U aj )(S i−S j)e(cid:6)∈T e Ai )2tiAii< j (S i−S j)2e∈T e Ai )2i (xi) + λs S i(x). Therefore, we can use gradient descent to solve the optimization problem in Eqs. (48) andi< j (U ai(cid:6)(ti−U ai< j (U ai(cid:6)(ti−U aj )2e∈T e Ai )2j )(S i−S j)e∈T e Ai )2i< j (U ai((cid:6)ti⎟⎠+ A j+ A j(cid:6)Ai(where, Ai = λu U a(49). The MLE parameters based on our data set are:(cid:3)(cid:2)λ∗u, λ∗s= (0.6, 0.77)6. Experimental results and discussionWe evaluated the performances of defender strategies as well as the accuracy of different adversary models with humansubjects using the online game “The Guard and The Treasure” introduced in Section 2.2. We conducted two set of evalua-tions: the first set includes the same 7 payoff structures used in the experiments in the previous section; the second setfocuses on comparison between the QR model and the QRRU model.6.1. Experimental settingsThe design of the simulated game was already provided in Section 2.2. We now present a detailed description of theexperimental settings. In total, we included 70 game instances (comprising 7 payoff structures and 10 strategies for eachpayoff structure) in the first set and 12 game instances (comprising 4 new payoff structures and 3 strategies for each payoffstructure) in the second set. To avoid confusion between these two sets of payoff structures, we will number the first sevenpayoff structures as 1.1–1.7, and the next four as 2.1–2.4.Each game instance is played by at least 80 different participants (the actual number of subjects for each game in-stance ranges between 80 to 91). Each subject is asked to play 40 out of the 70 games. For the purpose of a within-subjectcomparison, we want a subject to play the 10 different strategies for the same payoff structure. Therefore, the 40 gamesis composed of 4 payoff structures and 10 defender strategies for each. Furthermore, in order to mitigate the orderingeffect on subject responses, we randomize the order of the game instances played by each subject. We generated 40different orderings of the games using Latin square design. The order played by each subject was drawn uniformly ran-domly from the 40 possible orderings. To further mitigate ordering effect, no feedback on success or failure is given tothe subjects until the end of the experiment. As motivation to the subjects, they earn or lose money based on whether ornot they succeed in attacking a gate; if the subject opens a gate not protected by the guards, they win; otherwise, theylose.The participants were recruited on Amazon Mechanical Turk. Note that these participants differ from those who playedthe game to provide data for estimating the parameter, as discussed in the previous section. In order to avoid non-compliantparticipants, we only allowed workers whose HIT approval rates were greater than 95% and who had more than 100 ap-proved HITs to participate in the experiment. They were first given a detailed instruction of the game explaining to themhow the game is played. Then two practical rounds of games were provided to help them get familiar with the game. Afterall the learning and practising, they were given enough time to finish all the games.Each participant first received 50 cents for participating in the game. Then they gain bonus based on the outcomes of thegames they played, with each point worth 1 cent. On average, the subjects who participated in the first set of experiment(i.e. payoff 1.1–1.7) received $1.45 as bonus based on their total scores across 40 game instances they played; the subjectswho participated in the second set of experiment (i.e. payoff 2.1–2.4) received $0.44 as bonus based on their total scoresacross 12 game instances they played. Participants were given 5 hours in total to finish the experiment which was shownto be sufficiently long given that the average time they spent was 28 minutes for the first set of 40 games and 8 minutesfor the second set of 12 games.In the following part of this section, we first describe the parameter settings for the different leader strategies. Wethen provide our experimental results, and follow that up with analysis. We compare both the quality of different defenderstrategies against the human participants and the accuracy of different adversary models in the sense that how well thehuman participants follow the assumption of these models.6.2. Algorithm parametersFor the seven payoff structures (1.1–1.7) introduced in Section 5, we tested ten different mixed strategies generatedfrom seven different algorithms: Maximin, Dobss [18], Cobra [20], Brpt, Rpt, Brqr, Brqrru. We include Maximin as abenchmark algorithm. Maximin assumes that adversary always selects the target that is worst to the defender. Table 3 liststhe parameter settings of these ten strategies for each of the seven payoff structures.454R. Yang et al. / Artificial Intelligence 195 (2013) 440–469Table 3Parameter settings for different algorithms.PayoffCobra-αCobra-(cid:8)Brpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru1.11.21.30.152.00.152.90.152.5(α, β, θ, γ ) = (0.88, 0.88, 2.25, 0.64)(α, β, θ, γ ) = (0.88, 0.88, 2.25, 0.64), (cid:8) = 2.5(α, β, θ, γ ) = (1, 0.6, 2.2, 0.6)(α, β, θ, γ ) = (1, 0.6, 2.2, 0.6), (cid:8) = 2.5λ = 0.76λ = 0.55(λu , λs) = (0.6, 0.77)1.40.152.751.50.372.51.602.51.70.252.5• Dobss and Maximin have no parameters.• For Cobra, we set the parameters following the methodology presented in [20] as closely as possible for payoff struc-tures 1.1∼1.4. In particular, the values we set for α meet the entropy heuristic discussed in that work. For payoffstructures 1.5∼1.7 that are identical to payoff structures first used by Pita et al., we use the same parameter settings asin their work.• For both Brpt-E and Rpt-E, the parameters for Prospect Theory are empirical values suggested by literatures [31]. ForRpt-E, we empirically set (cid:8) to 25% of the maximum potential reward for the adversary, which is 10 in our experimentalsettings.• We tried another set of parameters for Prospect Theory, which are learned from our first set of experiment as describedin Section 5.2. We denote these two algorithms as Brpt-L and Rpt-L.• For Brqr, we tried two different values for the parameter λ, λ = 0.76 is the values learned from the data reportedby Pita et al. [20]; λ = 0.55 is the value learned from data collected in our first set of experiments with participantsfrom Amazon Mechanical Turk. We will refer to the strategies resulting from these two parameter settings of the Brqralgorithm as Brqr-76 and Brqr-55 respectively.• For Brqrru, the parameters are learned from the data collected our first set of experiments.6.3. Quality comparisonWe evaluated the performance of different defender strategies using the defender’s expected utility and the statisticalsignificance of our results using the bootstrap-t method [37].6.3.1. Average performanceWe first evaluated the average defender expected utility, U davg(x), of different defender strategies based on the subjects’choices:avg(x) = 1U dNN(cid:4)j=1U dτ j(x) = 1N(cid:4)ti ∈TNi U di (xi)where τ j is the target selected by the jth subject, Ni is the number of subjects that chose target ti and N is the total numberof subjects. Fig. 5 displays U davg(x) for the different strategies in each payoff structure. We also displayed the normalizeddefender average expected utility of different strategies within each payoff structure in Fig. 6. After normalization, U davg(x)for each defender strategy varies between 0 and 1, with the highest U davg(x) in each payoff structure scaled to 1 and thelowest U davg(x) scaled to 0.Overall, Brqr-76, Brqr-55 and Brqrru performed better than other algorithms. We compare the performance of threealgorithms with each of the other seven algorithms and report the level of statistical significance in Tables 4, 5 and 6. Wesummarize the results below:• Maximin is outperformed by all three algorithms with statistical significance in all seven payoff structures. Dobss is alsooutperformed by all three algorithms with statistical significance except for payoff structure 1.6.• In five of the seven payoff structures, Cobra is outperformed by all three algorithms with statistical significance. Inpayoff structure 1.3, the performance of Cobra is very close to the three algorithms, but there is no statistical signifi-cance either way. In payoff structure 1.5, Cobra is outperformed by all three algorithms but no statistical significance isachieved.• The three algorithms outperform Brpt-E with statistical significance in all seven payoff structures. Furthermore, Brpt-Lis outperformed by the three algorithms in all seven payoff structures with statistical significance in six cases exceptfor in payoff structure 1.6.• In four of the seven payoff structures, Rpt-E is outperformed by the three algorithms with statistical significance. Inpayoff 1.3, Rpt-E is outperformed by all three algorithms but the result is not statistical significant. In payoff structureR. Yang et al. / Artificial Intelligence 195 (2013) 440–469455Fig. 5. Defender average expected utility achieved by different strategies.1.4, Rpt-E achieves very similar performance to Brqr-55 and is outperformed by Brqr-76 and Brqrru. In payoff 1.5,Rpt-E achieves very similar performance as Brqr-76 and is outperformed by Brqr-55 and Brqrru. Furthermore, Rpt-Lis outperformed by all three algorithms with statistical significance in almost all seven payoff structures, except for inpayoff structure 1.2 where the result of comparing Brqr-76 and Brqrru with Rpt-L doesn’t have statistical significance.Overall, any of the three quantal response (Brqr-76, Brqr-55 and Brqrru) strategies would be preferred over the otherstrategies. However, the performance of the three strategies are close to each other in this set of experiments. In order tofurther differentiate the three strategies as well as prove the effectiveness of QRRU model, we conducted a separate set ofexperiments. We first select four new payoff structures from the 1000 random samples using the following rules:(cid:6)ni=1 xki log(xki /xli).• We first measure the distance between the Brqrru strategy and each of the other two Brqr strategies using Kullback–Leibler (KL) divergence: D(xk, xl) = DKL(xk|xl) + DKL(xl|xk), where DKL(xk|xl) =• For each payoff structure, we measure this KL distance for the pair (Brqrru, Brqr-76) and the pair (Brqrru, Brqr-55).So we have two such measurements for each payoff structure.• We sort the payoff structures in a descending order of the mean of these two distance.• In the top 10 payoff structures, we select two payoff structures where the targets assigned with minimum coverageprobability by Brqr-76 or Brqr-55 have large penalty for the defender; and two payoff structures where the penaltyfor the defender on such target is small.The details of these four payoff structures and the defender strategies are included in the appendix. We conducted a newset of experiments with human subjects using these four payoff structures and the three QR model based strategies for eachpayoff structure. In total, we have 4 ∗ 3 = 12 game instances included in these experiments. Each subject is asked to playagainst all these 12 game instances. 80 subjects are involved in these experiments.456R. Yang et al. / Artificial Intelligence 195 (2013) 440–469Fig. 6. Defender average expected utility (normalized between 0 and 1) achieved by different strategies.Table 4Level of statistical significance of comparing Brqr-76 to other algorithms: ***(p (cid:2) 0.01), **(p (cid:2) 0.05), *(p (cid:2) 0.1).v.s.payoff 1.1payoff 1.2payoff 1.3payoff 1.4payoff 1.5payoff 1.6payoff 1.7DobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-L***************0.20******************************0.96*0.26*****************************0.210.250.99********************0.13******0.15**************Table 5Level of statistical significance of comparing Brqr-55 to other algorithms: ***(p (cid:2) 0.01), **(p (cid:2) 0.05), *(p (cid:2) 0.1).v.s.payoff 1.1payoff 1.2payoff 1.3payoff 1.4payoff 1.5payoff 1.6payoff 1.7DobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-L***************0.16****************************0.86**0.37******************************0.160.950.12*********************0.11*******************R. Yang et al. / Artificial Intelligence 195 (2013) 440–469457Table 6Level of statistical significance of comparing Brqrru to other algorithms: ***(p (cid:2) 0.01), **(p (cid:2) 0.05), *(p (cid:2) 0.1).v.s.payoff 1.1payoff 1.2payoff 1.3payoff 1.4payoff 1.5payoff 1.6payoff 1.7DobssMaximinCobraBrpt-ERpt-EBrpt-L***************0.15****************************0.99**0.40******************************0.270.180.33*********************0.11***Rpt-L***0.27************Fig. 7. Defender average expected utility achieved by QR model based strategies.Table 7Statistical significance (**: p (cid:2) 0.05; ***: p (cid:2) 0.01).payoff 2.1payoff 2.2payoff 2.3payoff 2.4Brqrru v.s. Brqr-76Brqrru v.s. Brqr-55Brqrru v.s. Brqr-76Brqrru v.s. Brqr-55Brqr-76 v.s. BrqrruBrqr-55 v.s. BrqrruBrqrru v.s. Brqr-76Brqr-55 v.s. Brqrru*********0.870.400.970.35Fig. 7 displays the defender average expected utility achieved by the three strategies. We report the statistical signifi-cance results in Table 7. In payoff structures 2.1 and 2.2, Brqrru outperforms both Brqr-76 and Brqr-55 with statisticalsignificance. In payoff structures 2.3 and 2.4, the three strategies have very close performance. No statistical significance isfound in the results, as reported in Table 7.As noted earlier, a very important feature of payoff structures 2.1 and 2.2, compared to payoff structures 2.3 and 2.4,is that the target covered with minimum resource by Brqr-76 and Brqr-55 (target 3 in payoff structure 2.1 and target 3in payoff structure 2.2) has a large penalty ((cid:2) −6) for the defender. In the experiments with payoff structures 2.1 and 2.2,more than 10% of subjects selected these targets (target 3 in payoff structure 2.1 and 2.2) while playing against Brqr-76or Brqr-55, while no subjects chose this target while playing against Brqrru — Brqrru covers these targets with moreresources. This is the main reason why Brqrru significantly outperforms Brqr in payoff 2.1 and payoff 2.2. In payoff 2.3and 2.4, similar observation is obtained in subjects’ choice: the targets covered with minimum resources by Brqr-76 andBrqr-55 are selected more frequently compared to the case when Brqrru is played. However, these targets (i.e. target 1 inpayoff 2.3 and target 2 in payoff 2.4) have very small penalty for the defender (−1). Therefore we do not see significantdifferences in performance among the different Brqr strategies.Based on the result in both sets of experiments, we conclude that the stochastic model based strategies are superior totheir competitors, and Brqrru is the preferred strategy within the stochastic model based strategies. In particular, Brqrruachieves significantly better performance than Brqr when the target covered with minimum resource by Brqr has po-tentially a large penalty for the defender; and has a performance similar to the other stochastic model based strategiesotherwise.458R. Yang et al. / Artificial Intelligence 195 (2013) 440–469Fig. 8. Distribution of defender’s expected utility against each individual subject.6.3.2. Performance distributionWe now analyze the distribution of the performance of each defender strategy while playing against different adversaries(subjects). Given a game instance G, the defender expected utility achieved by playing strategy x against a subject j isdenoted as U d(x) for different defender strategies against individualτ Gj(x). Figs.8 and 9 display the distribution of U dτ Gjsubjects in each payoff structure. The y-axis shows the range of the defender’s expected utility against all different subjects.Each box with the extended dash line in the figure shows the distribution of this defender expected utility for each of theten defender strategies: the dashed line specifies the range of U d(x) with the bottom band showing the minimum valueτ Gjand the top band showing the maximum value; the box specified the 25th to 75th percentiles of U dτ Gj(x) with the bottomshowing the 25th percentile value and the top showing the 75th value; the band inside the box specifies the median (50thpercentile) of U d(x). We compare the distributions of different defender strategies from two perspectives:τ GjRange: As presented in Fig. 8 and Fig. 9, in general, the defender expected utility has the smallest range when Maximinstrategy is played (except that in payoff structure 1.7, the range of the defender expected utility when RPT-L is played isslightly smaller than that when Maximin is played). Cobra, Rpt, Brqr and Brqrru lead to larger range of defender expectedutility than Maximin. Defender expected utility has the largest range when Dobss or Brpt is played.Worst case: The lower band of the dashed line indicates the worst-case defender expected utility when different strategiesare played. Maximin has the highest worst-case defender expected utility in general (except that in payoff 1.5, the worst-case defender expected utility by playing Brqr-76 is better than that by playing Maximin). Dobss and Brpt lead to lowestworst-case defender expected utility. The worst-case defender expected utility from playing Cobra, Rpt, Brqr and Brqrruare in between the two extreme cases. Furthermore, Brqr and Brqrru lead to higher worst-case defender expected utilitythan Cobra and Rpt.In general, by playing Maximin, the defender expected utility against each individual adversary achieves the smallestvariance, hence it is most robust to the uncertainty in adversary’s choice. However, it does so by assuming that the adver-sary could select any target hence making the expected utility on each target equal. Maximin does not exploit the differentpreferences adversary may have among different targets. Brpt and Dobss assume the subjects select the target that maxi-R. Yang et al. / Artificial Intelligence 195 (2013) 440–469459Fig. 9. Distribution of defender’s expected utility against each individual subject.mizes their expected utility and do not consider the possibility of deviations from the optimal choice by the adversary. Thisleads to arbitrarily lower defender expected utility when the adversary deviates from the predicted choice.Cobra, Rpt, Brqr and Brqrru all try to be robust against such deviations. Brqr and Brqrru consider some (possiblyvery small) probability of adversary attacking any target using a soft-max function. In contrast, Cobra and Rpt separate thetargets into two groups, the (cid:8)-optimal set and the non-(cid:8)-optimal set, using a hard threshold. They then try to maximizethe worst case for the defender assuming the response will be in the (cid:8)-optimal set, but assign less resources to the non-(cid:8)-optimal targets. When the non-(cid:8)-optimal targets have high defender penalties, Cobra and Rpt become vulnerable toadversary’s deviation. For example, target 6 in payoff structure 1.2 has a small reward (= 1) and a large penalty (= −10) forthe attacker. Both Cobra and Rpt consider this target to be in the non-(cid:8)-optimal set and assign very small probability tocover this target ((cid:2) 0.05). However, approximately 10% of the subjects have chosen this target. Since this target has a highdefender penalty (−6), Cobra and Rpt lose reward on this target. Similar examples include target 5 in payoff structure 1.4and target 8 in payoff structure 1.1.6.4. Model prediction accuracyIn this section, we evaluate how well each model predicts the actual responses of human participants using three differ-ent metrics [38]: mean square deviation (MSD), a proportion of inaccuracy (POI), and Euclidean distance (ED).We first extend the definition of MSD from that in [38] which is designed for a 2-action game, in order to suit ourdomain where the player has 8 actions to take. Given the choices of the N subjects, the MSD of a model is computed asMSD =(cid:17)1/2(cid:16)1NN(cid:4)(pτ (n) − 1)2n=1(50)where, τ (n) represents the index of the target chosen by subject n, pi is the predicted probability by a model that target iwill be chosen.460R. Yang et al. / Artificial Intelligence 195 (2013) 440–469Table 8Ability of behavioral models to predict attacker decision.ModelDobssPT-EPT-LQR-76QR-55QRRUCobraRPT-ERPT-LOut of sampleIn sampleMSD0.810.840.840.790.810.800.910.930.93POI0.670.710.710.670.670.650.83 (0.35)0.87 (0.52)0.86 (0.49)ED0.760.810.810.230.220.210.940.990.98MSD0.850.870.860.830.840.830.910.940.93POI0.730.750.740.730.730.700.83 (0.42)0.88 (0.56)0.86 (0.54)ED0.800.840.830.220.210.180.930.990.96The POI score is meant to put models with deterministic prediction on the same footing as those with stochastic predic-tion. It treats the target with the highest predicted probability as the predicted target, and computes the proportion of thesubjects who didn’t choose the predicted target. The POI score is computed asPOI = 1NN(cid:4)(1 − ˜pτ (n))n=1(51)where, τ (n) is the index of the target chosen by subject n. ˜pτ (n) = 1 if τ (n) is the predicted target; and ˜pτ (n) = 0 otherwise.Note that for models with deterministic prediction, the POI score is exactly equal to the square of MSD value.The Euclidean distance measures the difference between the actual distribution of the subjects’ choices and the predic-tion of the model. It is computed as(cid:18)(cid:4)(cid:2)(cid:3)ED =pi − pacti2i∈Twhere pisubjects who have chosen target i.(52)is the actually percentage ofiis the probability predicted by the model that target i will be chosen, and pactTable 8 presents the ability of different models to predict the attacker decision measured with the three different cri-teria.5 The measurements for both the out-of-sample data (70 rounds of games) and in-sample data (35 rounds of games)are displayed in the table. Better predictive power is indicated by lower MSD value and POI score and lower ED value. Thetop four models all have deterministic prediction and the three quantal response related models have stochastic prediction.The last three models (Cobra, RPT-E and RPT-L) don’t have a strict definition of the prediction of the attacker’s behavior.They are modifications of the base models for robustness. For example, Cobra modifies Dobss by assuming that attackerwill deviate from choosing the target with the highest expected utility to any other targets whose expected utilities arewithin (cid:8) of the highest value. However, within this subset of possibly chosen targets, the model doesn’t explicitly predictthe behavior of the attacker but rather plays a maximin strategy (i.e. maximizing the lowest expected utility). RPT-E andRPT-L modify PT-E and PT-L in similar ways. Given the above property of these three models, we compute the POI score intwo different ways by using two different definitions of the model prediction.• The first definition predicts a single target with the lowest expected utility for the defender within the subset of possibledeviations. Therefore the POI score counts the proportion of subjects who have chosen any other targets.• The second definition predicts all the targets within the subset of the possible deviations. Therefore, the POI score onlycounts for the targets outside this subset.The POI score computed with the first definition should be equal to or higher than the value computed with the seconddefinition. Note that the second definition doesn’t satisfy the property of prediction since the sum of the predictions on alltargets might be larger than 1. We use this definition to mainly show the importance of accounting for deviation of attackers’decision. The POI values computed with the second definition are shown in parentheses in Table 8. The observations fromthe table are summarized blow,1. For the out-of-sample data, less than 30% of the subjects have selected the target predicted by PT-E or PT-L; in theother words, more than 70% of the subjects have deviated from the prediction. For Dobss, on average 67% of the subjectsdeviated from the predicted response. Similar patterns can be observed for the in-sample data.2. Both Rpt and Cobra take into consideration the deviation of the subjects’ responses from their optimal action. Thepercentage of subjects deviate from the model prediction decreased significantly: for the out-of-sample data, the POI score5 Maximin doesn’t have a prediction of adversary behavior, so we exclude it from the analysis.R. Yang et al. / Artificial Intelligence 195 (2013) 440–469461of Cobra is 0.35 compared to 0.67 of Dobss; the POI score of RPT-E decreased by 0.19 compared to PT-E; the POI score ofRPT-L decreased by 0.22 compared to PT-L. Similar patterns are observed for the in-sample data.3. The POI score of QR-76 and QR-55 is the same as Dobss. This is expected since the target predicted by the QR modelto be chosen with the highest probability is the target with the highest expected utility for the attacker, which is the alsoprediction of Dobss. In other words, QR-76 and QR-55 have the same predicted target as Dobss. At the same time, QRRUhas the lowest POI score among all the models in both the out-of-sample data and in-sample data. The MSD scores of thethree QR-related models are better (lower) than other models (except that in the out-of-sample data QR-55 has the samescore as Dobss).4. The advantage of the three QR related models is most significant under the ED score, which represents the error of themodel in predicting the distribution of subjects’ choices. As shown in Table 8, the three QR-related models have significantlylower ED scores than the other models. This is essentially the reason why the three models achieved significantly betterdefender expected utility than the other models.7. Related workIn the first few sections of the paper, we discussed recent developments of game-theoretical approaches to solve Stack-elberg security games. We discuss additional related work in this section.Motivated by real-world security problems, there have been many algorithms developed to compute optimal defenderstrategies in Stackelberg games [19,18,30]. The first such algorithm to be used in a real application is Dobss (DecomposedOptimal Bayesian Stackelberg Solver) [18], which is central to the ARMOR system [5] at LAX airport and the GUARDS system[7] built for the Transportation Security Administration. Other works related to Stackelberg security games include those ofAgmon et al. [3,39] and those of Gatti et al. [40,4] on multi-robot patrolling. However, an important limitation of all of thiswork is the assumption of a perfectly rational adversary, which may not hold in many real world domains.Recent work [20] developed a new algorithm Cobra, which provided a solution for designing better defender strategiesagainst human adversaries by modeling an adversary’s behavior taking into consideration (i) human deviation from theutility maximizing strategy and (ii) human anchoring bias when given limited observation of defender mixed strategy.Cobra significantly outperforms Dobss in the experimental results against human subjects. We have provided an extensivecomparison of our new approaches with COBRA in this paper.Another line of related work in Stackelberg security games have been trying to design more robust strategies to dealwith different kinds of uncertainties [41–43]. Yin et al. [42] proposed a unified efficient algorithm that addresses bothexecution uncertainties of the defender and observation uncertainties of adversaries in SSGs. Kiekintveld et al. [43] addresspayoff uncertainty by introducing a general model of infinite Bayesian Stackelberg security games which allows payoffs tobe represented using continuous payoff distributions. Although the simulation based experiment showed promising resultof these studies, the performances of these models against real human subjects are left unaddressed. Our work differs fromthese efforts in that they do not handle bounded rationality of human adversary while we propose different models toexplicitly predict human decision making accounting for their bounded rationality.Many models have been proposed to capture human bounded rationality in their decision making in psychology andcognitive science [44,34,45]. A key challenge of applying these models to game-theoretical framework to help design betterstrategy is the transition from a (sometimes descriptive) model to a computational model. On the other hand, there hasbeen a growing interest in the game theory literature to develop more realistic computational models of human decisionmaking in games [9,11,32]. Most of these models find empirical support from the data of human playing games. However,few research efforts have attempted to bridge the gap between computational game theory and behavioral game theory,and fewer still in the important area of SSGs, which is the topic of this article. In this paper, we explore such method bydeveloping computation models of human adversary decision making based on two descriptive theories: Prospect Theoryand rank-dependent Expected Utility. The outperforming result of these new models provide a clear direction for futurework in further improve the models of human bounded rationality in security games.Outside the area of Stackelberg security games, there have been several recent investigations of human subjects inter-acting with agents. For example, Melo et al. [46] investigate the impact of expression of an automated agent’s anger orhappiness in how a human participant may play the game. In repeated prisoner’s dilemma games, agents’ expressions areshown to significantly affect human subjects’ cooperation or defection. Similarly, Azaria et al. [47] focus on road selectiongames, and advice an automated system may provide to human subjects; Peled et al. [48] focus on bilateral bargaininggames, designing agents that negotiate proficiently with people. Aside from the obvious difference that our focus is on SSGs,another key is our focus on efficiently computing optimal mixed strategies for the defender.8. Summary and future workThere is a significant interest in game-theoretic techniques to solve security problems. Several real-world applicationbased on using these techniques have been deployed across the nation, including ARMOR [5], IRIS [6], GUARDS [7] and PRO-TECT [8]. These systems have adopted the traditional game-theoretical assumption of perfectly rational adversaries. Whilethis was appropriate in the first generation of Stackelberg security game applications, it is now critical to develop new462R. Yang et al. / Artificial Intelligence 195 (2013) 440–469methods to compute defender strategies addressing human bounded rationality — particularly as the range of SSG applica-tions where these deployed systems will face human adversaries continue to grow. New methods need to be developed tocompute defender strategy against bounded rationality of real human adversaries.In this paper, we address this problem by applying two fundamental theories in human decision-making, Prospect Theory(PT) and Quantal Response Equilibrium (QRE), to model the adversary’s behavior in security games. The contributions of thisarticle include:(i) proposing two mathematical models of adversary’s decision-making based on using PT;(ii) providing a method to adapt the parameters of PT for the two PT-based methods;(iii) proposing one mathematical model of adversary’s decision-making based on quantal response (QR) and a second onethat uses modified QR (based on rank dependent utility);(iv) developing efficient algorithms to compute optimal strategy for the defender under each of these four adversary mod-els;(v) most extensive-to-date experiment to verify the effectiveness of the proposed approaches.We compare our new approaches to three benchmark algorithms, Dobss, Maximin and Cobra in seven different payoffstructures, where Cobra is the leading contender for addressing human bounded rationality presented in previous work.Experiment results show that our new methods based on using the QR model achieved statistically significant better per-formance than all three benchmark algorithms as well as the PT-based new methods. Furthermore, we identify that in caseswhere the targets covered with minimum resource have large penalty to the defender, the modified QR model achieved sig-nificantly better performance than the basic QR model. By providing new models that better predicts the behavior of humanadversary and new algorithms that computes strategies outperforming our leading competitor, this paper has advanced thestate-of-the-art.While the research reported in this article takes an important step forward in addressing bounded rationality of humanadversaries in the context of security games, there are still many open topics for future research. One key area is to translatethe results obtained here in controlled experiments on AMT into specific, real-world security applications. Most of theissues related to making this transition are not unique to our work, but apply more generally to studies in agent/humaninteractions. For example, the specific conditions tested in the lab and the way in which decisions are presented is notlikely to be exactly reflected in real interactions, and neither is the population of adversaries identical to the population ofadversaries in a real-world security setting. However, our methods are based on fundamental features of human decision-making that are robustly supported in a large number of behavioral studies and these methods would thus translate intoreal-world applications. In addition, the parameters offer some ability to tune the models over time to specific settings orpopulations of interest, and our methodology provides techniques for tuning these parameters. The parameter settings in ourwork can serve as initial settings in a real deployment to be adapted over time. Alternatively, the parameters can initiallybe set conservatively (e.g., somewhat close to settings that result in a standard equilibrium), and adapted over time fromthis starting point. Another interesting possibility that could be explored in future work is to develop ways to incorporatedifferent sources of information (such as prior knowledge of the biases of specific adversaries) into the models in a generalway.Furthermore, the current game model is an abstraction of real-world security scenario, in particular, the one at the LosAngeles international airport. The model can be further refined to reflect more details of the scenario. For example, thecurrent game assumes covering each target with a single unit of resources and a binary effect of protecting the targetswith the resources (i.e. protected/not protected). An interesting direction for future work is to explore the effect of havingmultiple units of resources to protect a target. At the same time, another interesting direction for future work is to ex-tend the current game model to deal with domains with continual and online interaction between the defender and theattacker.AcknowledgementsThis research was supported by Army Research Office under the grant # W911NF-10-1-0185. We also thank MohitGoenka and James Pita for their help on developing the web-based game. F. Ordonez would also like to acknowledge thesupport of Conicyt, through Grant No. ACT87.Appendix A. Payoff structure informationThe four payoff structures selected from the four clustering groups are displayed in Table A.9. The three payoffs thatare identical to that first used by Pita et al. are shown in Table A.10. The four payoff structures selected for the secondevaluation set of experiment for comparing the three QR model based strategies are listed in Table A.11.Table A.9Payoff structures.Targetdefender rewarddefender penaltysubject rewardsubject penaltydefender rewarddefender penaltysubject rewardsubject penaltydefender rewarddefender penaltysubject rewardsubject penaltydefender rewarddefender penaltysubject rewardsubject penaltyTable A.10Payoff structures.Targetdefender rewarddefender penaltysubject rewardsubject penaltydefender rewarddefender penaltysubject rewardsubject penaltydefender rewarddefender penaltysubject rewardsubject penaltyTable A.11Payoff structures.Targetdefender rewarddefender penaltysubject rewardsubject penaltydefender rewarddefender penaltysubject rewardsubject penaltydefender rewarddefender penaltysubject rewardsubject penaltyR. Yang et al. / Artificial Intelligence 195 (2013) 440–46946312−810−73−109−105−28−65−103−411−51−24−88−34−88−3110−47−42−67−61−12−1026−108−48−28−13−56−99−47−824−89−43−105−23−55−321−66−37−46−61−71−134(a) Payoff structure 1.17−33−67−17−8(b) Payoff structure 1.29−52−109−19−8(c) Payoff structure 1.38−41−33−63−7(d) Payoff structure 1.410−93−52−39−834(a) Payoff structure 1.52−15−33−66−3(b) Payoff structure 1.61−13−35−810−2(c) Payoff structure 1.71−12−35−1010−334(c) Payoff structure 2.11−91−48−106−8(b) Payoff structure 2.21−51−610−110−2(c) Payoff structure 2.310−81−47−64−1058−106−47−710−43−31−710−102−954−57−31−11−31−51−358−77−101−76−24−71−668−57−27−61−104−107−24−109−461−11−22−33−32−33−366−47−510−43−99−15−276−28−94−210−53−73−58−27−175−710−45−119−25−99−372−56−43−42−1089−52−31−11−36−25−28−58−682−73−32−54−32−64−384−86−52−78−36−86−1(continued on next page)9−77−8464R. Yang et al. / Artificial Intelligence 195 (2013) 440–469Table A.11 (continued)Targetdefender rewarddefender penaltysubject rewardsubject penalty17−27−923−11−134(d) Payoff structure 2.46−56−101−41−7510−53−361−81−178−87−589−102−1Appendix B. Defender mixed-strategyThe defender’s mixed-strategy from each algorithm in each payoff structures are displayed in Tables B.12–B.19.Table B.12Defender’s mixed-strategy for payoff structure 1.1.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru10.490.740.570.390.440.530.600.570.580.5520.530.590.620.510.580.540.630.580.590.56Table B.13Defender’s mixed-strategy for payoff structure 1.2.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru10.420.750.480.280.310.430.510.540.560.4620.780.180.530.930.610.780.490.520.500.40Table B.14Defender’s mixed-strategy for payoff structure 1.3.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru10.530.120.480.420.410.580.360.360.340.3320.370.480.420.240.290.390.470.430.430.38Table B.15Defender’s mixed-strategy for payoff structure 1.4.30.150.240.180.170.240.100.200.180.180.2530.080.340.090.070.090.040.090.210.230.3030.120.240.160.290.410.090.270.200.220.3340.360.060.220.260.160.360.170.210.190.2040.470.080.430.340.380.480.390.360.340.2540.250.540.290.190.250.210.320.360.370.3350.440.520.510.430.510.420.520.510.520.4850.640.490.740.590.640.650.710.640.630.5450.060.310.070.090.140.050.110.130.120.3360.590.340.440.700.410.600.410.470.470.45600.4500.050.070.010.030.160.190.3060.720.630.810.780.780.760.750.720.730.6770.370.180.340.260.280.390.290.300.280.2870.600.300.700.520.560.610.690.580.550.4570.310.580.360.280.360.280.400.430.440.35TargetDobssMaximin10.220.5920.370.2130.190.4140.440.3650.050.4460.580.6370.690.08080.070.320.110.280.380.060.180.180.200.25800.400.020.230.340.010.09000.3080.640.100.420.720.360.650.320.370.360.2880.470.29R. Yang et al. / Artificial Intelligence 195 (2013) 440–469465Table B.15 (continued)TargetCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru10.240.280.370.160.250.350.370.3420.420.270.310.370.430.330.320.34Table B.16Defender’s mixed-strategy for payoff structure 1.5.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru100.5600.160.280.020.130.120.130.1520.590.530.640.490.530.610.620.610.620.60Table B.17Defender’s mixed-strategy for payoff structure 1.6.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru10.560.530.580.490.560.580.580.580.590.5820.450.640.550.460.560.430.560.590.600.59Table B.18Defender’s mixed-strategy for payoff structure 1.7.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru10.590.490.570.540.540.610.520.590.590.59Table B.19Defender’s mixed-strategy.TargetBrqrruBrqr-55Brqr-7610.300.310.3120.440.360.480.410.440.420.410.440.440.4420.450.500.5230.210.220.290.140.210.300.320.3430.4500.230.410.120.430.130.160.120.1030.19000.210.010.150000.0530.10000.1900.08000030.350.170.1340.500.330.370.480.530.440.410.3940.510.490.630.460.520.500.600.550.560.5240.680.490.530.670.540.730.540.600.610.6040.650.520.590.600.570.700.550.630.640.644(a) Payoff structure 2.10.380.420.4150.040.080.100.040.070.200.230.3450.560.370.520.510.480.570.490.520.520.4950000.050.0100000500.4800.090.180.010.210.080.080.0850.350.360.3660.660.540.600.600.660.620.620.5860000.160.180.020.13000.1560.190.270.310.210.310.150.280.190.160.1660.250.170.330.270.300.200.250.220.200.2060.350.340.3670.390.900.530.730.350.360.330.2970.620.450.550.520.530.650.530.570.580.5670.650.590.620.640.630.690.630.660.670.6670.620.490.560.570.540.660.520.600.610.6180.530.380.430.480.500.420.400.3880.270.600.400.280.360.210.370.460.480.4180.300.480.410.290.380.260.400.380.370.3680.360.480.470.350.430.320.530.450.450.45780.390.430.45(continued on next page)0.420.470.47466R. Yang et al. / Artificial Intelligence 195 (2013) 440–469Table B.19 (continued)TargetBrqrruBrqr-55Brqr-76BrqrruBrqr-55Brqr-76BrqrruBrqr-55Brqr-7610.380.510.510.320.100.150.260.290.2920.350.390.400.320.380.380.310030.350.070.050.320.360.320.370.400.404(b) Payoff structure 2.20.300.360.41(c) Payoff structure 2.30.320.360.35(d) Payoff structure 2.40.310.190.2050.430.630.630.320.300.270.360.420.4160.350.220.200.270.320.330.310.470.5070.350.160.140.660.710.710.520.550.5480.510.660.650.440.470.470.560.690.65Appendix C. Distribution of subjects’ choicesThe distributions of subjects’ choices while playing against each payoff/strategy combination are displayed in Tables C.20–C.27.Table C.20Distribution of subjects’ choices (%) in payoff structure 1.1.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru117.445.818.1423.2611.638.144.653.495.818.1425.813.492.338.141.164.655.819.301.162.3336.982.331.1622.092.3324.422.338.149.301.16Table C.21Distribution of subjects’ choices (%) in payoff structure 1.2.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru113.332.228.8923.3320.0013.338.891.110.002.22211.1130.0022.2210.0025.5613.3340.0021.1135.5632.2234.441.1111.118.897.7814.448.894.440.001.11Table C.22Distribution of subjects’ choices (%) in payoff structure 1.3.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-L123.0841.7623.0831.8724.1819.7846.15210.991.108.7917.5820.8810.991.10318.680.0010.994.400.0023.0813.1941.1650.0026.744.6543.025.8145.3530.2336.0536.0543.3331.1114.4414.4410.004.444.4417.7814.4435.5642.200.003.306.593.305.493.3054.650.000.003.491.168.145.811.162.332.33515.5612.2210.0013.3312.2214.4413.3315.566.678.89512.090.008.7910.998.7910.9912.09623.2627.9131.4016.2838.3722.0918.6032.5629.0733.7267.780.0014.448.898.895.567.782.225.561.11619.7820.8817.5819.7816.4815.3812.0974.6510.4710.4719.772.335.812.339.309.308.14717.7822.2212.2220.0015.5616.6711.1116.6717.7817.7875.490.003.306.592.207.694.40836.050.0019.772.330.0020.9315.125.816.988.14826.671.116.671.110.0017.785.5621.1120.001.1187.6936.2624.182.2024.186.597.69R. Yang et al. / Artificial Intelligence 195 (2013) 440–469467Table C.22 (continued)TargetBrqr-76Brqr-55Brqrru131.8738.4632.9721.102.200.00Table C.23Distribution of subjects’ choices (%) in payoff structure 1.4.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru15.430.004.353.260.0010.873.260.000.001.0926.527.615.4314.1310.8714.132.1711.968.702.17Table C.24Distribution of subjects’ choices (%) in payoff structure 1.5.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru114.771.147.9512.500.0011.366.823.410.000.0025.681.141.1419.326.826.824.552.271.141.14Table C.25Distribution of subjects’ choices (%) in payoff structure 1.6.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru18.333.330.0026.678.335.000.003.333.333.33211.670.001.671.671.6713.331.670.000.000.0034.403.300.0038.700.005.434.352.1710.878.700.001.090.00313.6447.7340.911.1454.5513.6454.5538.6453.4151.14313.3341.6746.6710.0040.0015.0013.3345.0045.0030.00Table C.26Distribution of subjects’ choices (%) in payoff structure 1.7.TargetDobssMaximinCobraBrpt-ERpt-EBrpt-LRpt-LBrqr-76Brqr-55Brqrru16.107.322.446.106.1012.2017.077.326.104.88210.989.768.549.7612.2010.981.2213.4113.4112.20319.5132.9345.1212.2039.0214.6321.9541.4637.8029.2742.200.002.20410.877.6111.9617.3922.8313.049.789.7813.0410.8741.142.270.004.553.415.682.270.000.001.14428.3350.0043.3325.0035.0026.6763.3343.3341.6745.00423.1719.5124.3932.9326.8318.2923.1717.0728.0523.17514.2910.990.00521.740.0015.2219.5714.1315.2217.3913.0416.301.0957.9510.237.951.143.413.411.144.555.689.09518.331.673.3320.003.3313.3315.003.331.6715.00519.511.223.6618.290.0014.630.001.221.224.88610.9918.6815.38620.658.7010.8718.489.7810.877.6111.968.7010.87617.051.1411.366.821.1412.507.9519.3212.503.4168.333.333.3310.001.673.333.333.335.001.67612.209.766.109.761.2213.4121.957.326.1018.2970.000.000.00722.8370.6541.3013.0438.0422.8348.9153.2648.9167.39728.4136.3630.6838.6430.6825.0022.7331.8227.2734.0976.670.000.005.006.676.673.331.671.673.3371.2219.518.544.888.544.8814.6312.207.323.66835.1626.3749.4583.265.435.439.782.172.172.170.003.266.52811.360.000.0015.910.0021.590.000.000.000.0085.000.001.671.673.3316.670.000.001.671.6787.320.001.226.106.1010.980.000.000.003.66468R. Yang et al. / Artificial Intelligence 195 (2013) 440–469Table C.27Distribution of subjects’ choices (%).Target172.9454.1258.822.352.352.350.0014.1214.1249.4135.2941.18BrqrruBrqr-55Brqr-76BrqrruBrqr-55Brqr-76BrqrruBrqr-55Brqr-76BrqrruBrqr-55Brqr-76References210.5916.479.413.535.885.884.712.352.359.4135.2929.4130.0010.5911.760.0010.5916.471.182.350.008.242.354.714(a) Payoff structure 2.12.361.183.53(b) Payoff structure 2.284.7168.2457.65(c) Payoff structure 2.32.352.351.18(d) Payoff structure 2.42.351.180.0053.537.065.881.181.183.530.000.001.184.715.887.066789.417.068.240.002.357.0667.0643.5348.245.883.531.181.182.352.353.531.182.3515.2922.3516.4715.2916.4715.290.001.180.004.718.244.719.4112.9416.474.710.001.18[1] R. Yang, C. Kiekintveld, F. Ordonez, M. Tambe, R. John, Improving resource allocation strategy against human adversaries in security games, in: IJCAI,2011, pp. 458–464.[2] N. Gatti, Game theoretical insights in strategic patrolling: Model and algorithm in normal-form, in: ECAI-08, 2008, pp. 403–407.[3] N. Agmon, S. Kraus, G.A. Kaminka, Multi-robot perimeter patrol in adversarial settings, in: ICAT, 2008, pp. 2339–2345.[4] N. Basiloco, N. Gatti, F. Amigoni, Leader-follower strategies for robotic patrolling in environments with arbitrary topologies, in: AAMAS, 2009, pp. 57–64.[5] J. Pita, M. Jain, F. Ordonez, C. Portway, M. Tambe, C. Western, P. Paruchuri, S. Kraus, Deployed armor protection: The application of a game theoreticmodel for security at the Los Angeles international airport, in: AAMAS, 2008, pp. 125–132.[6] J. Tsai, S. Rathi, C. Kiekintveld, F. Ordonez, M. Tambe, IRIS – a tool for strategic security allocation in transportation networks, in: AAMAS, 2009,pp. 37–44.[7] J. Pita, M. Tambe, C. Kiekintveld, S. Cullen, E. Steigerwald, GUARDS – game theoretic security allocation on a national scale, in: AAMAS, 2011, pp. 37–44.[8] B. An, J. Pita, E. Shieh, M. Tambe, C. Kiekintveld, J. Marecki, GUARDS and PROTECT: Next generation applications of security games, ACM SIGecomExchanges 10 (2011) 31–34.[9] C.F. Camerer, T. Ho, J. Chongn, A cognitive hierarchy model of games, QJE 119 (2004) 861–898.[10] M. Costa-Gomes, V.P. Crawford, B. Broseta, Cognition and behavior in normal-form games: An experimental study, Econometrica 69 (2001) 1193–1235.[11] S. Ficici, A. Pfeffer, Simultaneously modeling humans’ preferences and their beliefs about others’ preferences, in: AAMAS, 2008, pp. 323–330.[12] Y. Gal, A. Pfeffer, Modeling reciprocal behavior in human bilateral negotiation, in: AAAI, 2007[13] M. Tambe, Security and Game Theory: Algorithms, Deployed Systems, Lessons Learned, Cambridge University Press, New York, NY, 2011.[14] D. Korzhyk, V. Conitzer, R. Parr, Security games with multiple attacker resources, in: IJCAI, 2011, pp. 273–279.[15] J. Letchford, Y. Vorobeychik, Computing randomized security strategies in networked domains, in: AARM Workshop in AAAI, 2011.[16] D. Kahneman, A. Tvesky, Prospect theory: An analysis of decision under risk, Econometrica 47 (1979) 263–292.[17] R.D. McKelvey, T.R. Palfrey, Quantal response equilibria for normal form games, Games and Economic Behavior 2 (1995) 6–38.[18] P. Paruchuri, J.P. Pearce, J. Marecki, M. Tambe, F. Ordonez, S. Kraus, Playing games for security: An efficient exact algorithm for solving bayesianStackelberg games, in: AAMAS, 2008, pp. 895–902.[19] C. Kiekintveld, M. Jain, J. Tsai, J. Pita, F. Ordonez, M. Tambe, Computing optimal randomized resource allocations for massive security games, in: AAMAS,2009, pp. 689–696.[20] J. Pita, M. Jain, F. Ordonez, M. Tambe, S. Kraus, Solving Stackelberg games in the real-world: Addressing bounded rationality and limited observationsin human preference models, Artificial Intelligence Journal 174 (2010) 1142–1171.[21] C. Fox, R. Clemen, Subjective probability assessment in decision analysis: Partition dependence and bias toward the ignorance prior, ManagementScience 51 (2005) 1417–1432.[22] K.E. See, C.R. Fox, Y.S. Rottenstreich, Between ignorance and truth: Partition dependence and learning in judgment under uncertainty, Journal ofExperimental Psychology: Learning, Memory, and Cognition 32 (2006) 1385–1402.[23] D.L. McFadden, Econometric analysis of qualitative response models, in: Z. Griliches, M.D. Intriligator (Eds.), Handbook of Econometrics, vol. 2, Elsevier,1984, pp. 1395–1457.[24] K. Train, Discrete Choice Methods with Simulation, Cambridge University Press, Cambridge, UK, 2003.[25] D.L. McFadden, Quantal choice analysis: A survey, Annals of Economic and Social Measurement 5 (1976) 369–390.[26] D.L. McFadden, A method of simulated moments for estimation of discrete choice models without numerical integration, Econometrica 57 (1989)995–1026.[27] E. Diecidue, P.P. Wakker, On the intuition of rank-dependent utility, The Journal of Risk and Uncertainty 23 (2001) 281–289.[28] D. Korzhyk, Z. Yin, C. Kiekintveld, V. Conitzer, M. Tambe, Stackelberg vs. Nash in security games: An extended investigation of interchangeability,equivalence, and uniqueness, in: JAIR, 2011, pp. 297–327.[29] D. Korzhyk, V. Conitzer, R. Parr, Complexity of computing optimal Stackelberg strategies in security resource allocation games, in: AAAI, 2010.[30] J. Tsai, Z. Yin, J. Kwak, D. Kempe, C. Kiekintveld, M. Tambe, Urban security: Game-theoretic resource allocation in networked physical domains, in:AAAI, 2010.[31] D. Kahneman, A. Tvesky, Advances in prospect theory: Cumulative representation of uncertainty, Journal of Risk and Uncertainty 5 (1992) 297–322.[32] D.O. Stahl, P.W. Wilson, Experimental evidence on players’ models of other players, JEBO 25 (1994) 309–327.[33] J.R. Wright, K. Leyton-Brown, Beyond equilibrium: Predicting human behavior in normal-form games, in: AAAI, 2010.[34] H. Simon, Rational choice and the structure of the environment, Psychological Review 63 (1956) 129–138.[35] M.K. Sen, P.L. Stoffa, Global Optimization Methods in Geophysical Inversion, Elsevier, New York, 1995.[36] J.C. Becsey, L. Berke, J.R. Callan, Nonlinear least squares methods: A direct grid search approach, Journal of Chemical Education 45 (1968) 728.R. Yang et al. / Artificial Intelligence 195 (2013) 440–469469[37] R.R. Wilcox, Applying Contemporary Statistical Techniques, Academic Press, 2003.[38] N. Feltovich, Reinforment-based vs. belief-based learning models in experimental asymmetric-information games, Econometrica 68 (2000) 605–641.[39] N. Agmon, S. Kraus, G.A. Kaminka, V. Sadow, Adversarial uncertainty in multi-robot patrol, in: IJCAI, 2009, pp. 1811–1817.[40] N. Gatti, Game theoretical insights in strategic patrolling model and algorithm in normal-form, in: ECAI, 2008, pp. 403–407.[41] M. Aghassi, D. Bertsimas, Robust game theory, Mathematical Programming 107 (2006) 231–273.[42] Z. Yin, M. Jain, M. Tambe, F. Ordonez, Risk-averse strategies for security games with execution and observational uncertainty, in: AAAI, 2011.[43] C. Kiekintveld, J. Marecki, M. Tambe, Approximation methods for infinite bayesian Stackelberg games: Modeling distributional payoff uncertainty, in:AAMAS, 2011, pp. 1005–1012.[44] R. Hastie, R.M. Dawes, Rational Choice in an Uncertain World: the Psychology of Judgement and Decision Making, Sage Publications, Thousand Oaks,2001.[45] C. Starmer, Developments in non-expected utility theory: The hunt for a descriptive theory of choice under risk, Journal of Economic Literature XXXVIII(2000) 332–382.[46] C. de Melo, P. Carnevale, J. Gratch, The effect of expression of anger and happiness in computer agents on negotiations with humans, in: AAMAS, 2011,pp. 937–944.[47] A. Azaria, Z. Rabinovich, S. Kraus, C.V. Goldman, Strategic information disclosure to people with multiple alternatives, in: AAAI, 2011, pp. 594–600.[48] N. Peled, Y. Gal, S. Kraus, A study of computational and human strategies in revelation games, in: AAMAS, 2011, pp. 345–352.