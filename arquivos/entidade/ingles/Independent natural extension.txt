Artificial Intelligence 175 (2011) 1911–1950Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintIndependent natural extensionGert de Cooman a, Enrique Miranda b, Marco Zaffalon c,∗a Ghent University, SYSTeMS Research Group, Technologiepark–Zwijnaarde 914, 9052 Zwijnaarde, Belgiumb University of Oviedo, Department of Statistics and Operations Research, C-Calvo Sotelo, s/n, 33007 Oviedo, Spainc IDSIA, Galleria 2, CH-6928 Manno (Lugano), Switzerlanda r t i c l ei n f oa b s t r a c tThere is no unique extension of the standard notion of probabilistic independence to thecase where probabilities are indeterminate or imprecisely specified. Epistemic independenceis an extension that formalises the intuitive idea of mutual irrelevance between differentsources of information. This gives epistemic independence very wide scope as well asappeal: this interpretation of independence is often taken as natural also in precise-probabilistic contexts. Nevertheless, epistemic independence has received little attentionso far. This paper develops the foundations of this notion for variables assuming valuesin finite spaces. We define (epistemically) independent products of marginals (or possiblyconditionals) and show that there always is a unique least-committal such independentproduct, which we call the independent natural extension. We supply an explicit formulafor it, and study some ofits properties, such as associativity, marginalisation andexternal additivity, which are basic tools to work with the independent natural extension.Additionally, we consider a number of ways in which the standard factorisation formulafor independence can be generalised to an imprecise-probabilistic context. We show,under some mild conditions, that when the focus is on least-committal models, using theindependent natural extension is equivalent to imposing a so-called strong factorisationproperty. This is an important outcome for applications as it gives a simple tool to makesure that inferences are consistent with epistemic independence judgements. We discussthe potential of our results for applications in Artificial Intelligence by recalling recent workby some of us, where the independent natural extension was applied to graphical models.It has allowed, for the first time, the development of an exact linear-time algorithm for theimprecise probability updating of credal trees.© 2011 Elsevier B.V. All rights reserved.Article history:Received 1 October 2010Received in revised form 8 June 2011Accepted 11 June 2011Available online 15 June 2011Keywords:Epistemic irrelevanceEpistemic independenceIndependent natural extensionStrong productFactorisationCoherent lower previsions1. Introduction1.1. Background and motivationThis is a paper on the notion of independence in probability theory. Anyone interested in or familiar with uncertainreasoning or statistics knows how fundamental this notion is. But what is independence?Most of us have been taught that two variables X1 and X2 are independent when their joint probability distributionP {1,2} factorises as the product of its marginals P 1 and P 2. This is the formalist route that defines independence through amathematical property of the joint, and that has its roots in the Kolmogorovian, measure- and integral-theoretic formalisa-tion of probability theory.* Corresponding author.E-mail addresses: gert.decooman@ugent.be (G. de Cooman), mirandaenrique@uniovi.es (E. Miranda), zaffalon@idsia.ch (M. Zaffalon).0004-3702/$ – see front matter © 2011 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2011.06.0011912G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950In Artificial Intelligence (AI)—thanks to Judea Pearl in particular [27]—, but also in the tradition of subjective probability—due to a large extent to Bruno de Finetti [17]—, independence has much more often an epistemic flavour: it is a subject whoregards two variables as independent, because she judges that learning about the value of any one of them will not affecther beliefs about the other. This means that the subject assesses that her conditional beliefs equal her marginal ones:P 1(·| X2) = P 1 and P 2(·| X1) = P 2, in more mathematical parlance.That the epistemic approach has become so popular, should not be all that surprising. The formalist approach comeswith the idea that independence is something given, which might hold or not: it is just a property of the joint. On theepistemic view, however, independence is something we are (to some extent) in control of. And this control is essential inorder to aggregate simple, independent components into complex multivariate models.It might be argued that the difference between the two approaches is mostly philosophical: in fact, the two routes areknown to be formally equivalent.1 But it turns out that we lose this formal equivalence as soon as we consider probabilitiesthat may be imprecisely specified, meaning that the available information is conveniently expressed through sets of proba-bilities (sets of mass functions). In this case the two routes diverge also mathematically, as we shall see further on. This isexemplified by the existence of the different notions of strong and epistemic independence, respectively.2 Of these two, strongindependence has been most thoroughly investigated in the literature. Studies of epistemic independence are confined toa relatively small number of papers [6,10,26,29] inspired by Peter Walley’s [30, Section 9.3] seminal ideas. We mention inparticular Paolo Vicig’s interesting study [29], for the case of coherent lower probabilities (which may be defined on infinitespaces), of some of the notions considered in this paper as well.This situation is somewhat unfortunate as the scope of strong independence is relatively narrow: in fact, its justificationseems to rely on a sensitivity analysis interpretation of imprecise probabilities. On this interpretation, one assumes that thereexists some (kind of ‘ideal’ or ‘true’) precise probability P T{1,2} for the variables X1 and X2 that satisfies stochastic indepen-dence, and that, due to the lack of time or other resources, can only be partially specified or assessed. Then one considersall the precise-probabilistic models P {1,2} that are consistent with the partial assessments and that satisfy stochastic inde-pendence. Taken together, they constitute the set of probabilities for the problem under consideration. This set models asubject’s (partial) ignorance about the true model P T{1,2}.It is questionable that this sensitivity analysis interpretation is broadly applicable, for the simple reason that it hingeson the assumption of the existence of the underlying ‘true’ probability P T{1,2}. Consider the situation where we wish tomodel an expert’s beliefs: the expert usually does not know much about ideal probabilities, and what she tells us is simplythat information about one variable does not influence her beliefs about the other. Moreover, we could well argue thatexpert knowledge is inherently imprecise to some extent, no matter the resources that we employ to capture it.3 Therefore,why not take the expert at her word and model only the information she provides us about the mutual irrelevance ofthe two variables under consideration? After all, forcing a sensitivity analysis interpretation here would amount to addingunwarranted assumptions, which may lead us to draw stronger conclusions than those the expert herself might be preparedto get to.In order to model such mutual irrelevance, we need a different understanding of imprecise probability models thatdoes not (necessarily) rely on precise probability as a more primitive notion: Walley’s behavioural theory of impreciseprobability [30], which models beliefs by looking at a subject’s buying and selling prices for gambles. The perceived mutualirrelevance of two sources of information can be formalised easily in this framework: we state that the subject is not goingto change her prices for gambles that depend on one variable, when the other variable is observed. This turns out to bestill equivalent to modelling the problem through a set of precise probabilities P {1,2} but, in contradistinction with thecase of sensitivity analysis, not all those probabilities satisfy stochastic independence in general. The reason for this is thatepistemic independence is a property of the set of probabilities that cannot be explained through the properties of theprecise probabilities that make up the set. This point is not without importance, as it shows that buying and selling pricesfor gambles are actually a more primitive and fundamental notion in a theory of personal probability.This illustrates that a behavioural theory of probability and the notion of epistemic independence fit nicely together.It also indicates that epistemic independence has a very wide scope, as it needs to meet fewer requirements than strongindependence in order to be employed. That being so, why has strong independence been studied and applied much moreextensively than its epistemic counterpart, even in work based on Walley’s approach? This is probably due to a numberof concurring factors: (i) a tendency in the literature to extend stochastic independence, perhaps somewhat uncritically, ina straightforward way to imprecise probabilities; (ii) the fact that epistemic independence does not appear to be as well-behaved as strong independence, for instance with respect to the graphoid axioms [6]4; and, perhaps more importantly,(iii) the lack of formal tools for handling epistemic independence assessments. To give a telling illustration of this lastpoint: epistemically independent products have so far been given a formal definition [30, Section 9.3] only for the case of1 There may be subtleties, however, related to events of probability zero. See Refs. [6, Notes 5 and 6 in Section 3], [30, Sections 6.5 and 6.10] and [1] formore information.2 Other possible ways to define independence under imprecise probability are given in Ref. [2].3 See [30, Chapter 5] for a detailed exposition of this view.4 But also see Ref. [26] for a discussion with less negative conclusions.G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501913two variables X1 and X2. Finally, note that preliminary work on the subject of this paper has appeared in the conferencesIPMU 2010 [15] and SMPS 2010 [14].1.2. Aims and contributionsIn the present paper, then, we intend to address and remedy this lack of formal tools by providing a firm foundation for,and a thorough mathematical discussion of, epistemic independence in the case of a finite number of variables Xn takingvalues in finite sets Xn, n ∈ N. Perhaps surprisingly, this will also shed positive new light on the second of the above-mentioned factors: it will allow us to show that, despite the apparently negative results in Ref. [6], epistemic independencecan actually be used effectively in at least some types of graphical models. We will come back to this issue later in thisIntroduction.We ground our analysis in the conceptual and formal framework of coherent lower previsions, which are lower expectationfunctionals equivalent to closed convex sets of probability mass functions. In the case of precise probabilities, we refer toan expectation functional as a linear prevision. Section 2 gives a brief introduction to coherent lower previsions and reportsbasic results that will be used in the rest of the paper. It should make the paper as self-contained as is reasonably achievablewithin the scope of a research paper.The real work starts in Section 3, where we introduce and discuss several generalisations to coherent lower previsionsof the standard notion of factorisation: productivity, which was used by some of us in Ref. [12] to derive a very general lawof large numbers, factorisation and strong factorisation, which we needed in our research on credal networks (an imprecise-probabilistic graphical model) [11], the Kuznetsov and strong Kuznetsov properties, originating in the work of the Russianmathematician Vladimir Kuznetsov [19], and also studied by Fabio Cozman [3]. It is useful to keep in mind that the ‘strong’versions of these properties involve factorisations over any subsets of variables, while the ‘plain’ ones are the special caseobtained when some of the subsets are singletons. For linear previsions—the precise-probability models—all these propertiescoincide with the classical notion of stochastic independence. For the more general lower previsions, we investigate howthese notions are related, and we show that the strong product—the product that arises through strong independence—isstrongly factorising.In Section 4, we go over to the epistemic side. We introduce two notions: many-to-many independence, where a subjectjudges that learning about the value that any subset of the variables { Xn: n ∈ N} assumes will not affect her beliefs aboutthe values of any other disjoint subset; and the weaker notion of many-to-one independence, where she judges that learningabout the value that any subset of the variables assumes will not affect her beliefs about the value of any remaining singlevariable. This leads to the definition of two corresponding types of independent products. We prove some useful associativityand marginalisation properties for these, which form a basis for building them recursively, and prove a very useful theoremthat immediately allows all these notions, as well as the results in the rest of the paper, to be extended to the caseof conditional independence. Moreover, we show that the strong product is one particular many-to-many (and thereforemany-to-one) independent product, and that it is the only such independent product when the given marginals are linearprevisions.There is no such uniqueness in the more general case of marginal lower previsions: the strong product is only one ofthe generally infinitely many possible independent products. In Section 5, we focus on the pointwise smallest of all these:the least-committal many-to-many, and the least-committal many-to-one, independent products of given marginals. It isan important, and quite involved, result of our analysis that these two smallest independent products turn out to alwaysexist, and to coincide. We call this common smallest independent product the independent natural extension of the givenmarginals. It generalises, to any finite number of variables, a definition given by Walley for two variables [30, Section 9.3].5We then go on to derive an explicit and constructive formula for the independent natural extension, and we prove that ittoo satisfies useful associativity and marginalisation properties, and that it is externally additive. We work out interestingparticular cases in some detail.The relation with the more formal factorisation properties considered in Section 3 comes to the fore in our importantnext result: that the independent natural extension is strongly factorising. We go somewhat further in Section 7, wherewe show that, quite naturally, any factorising lower prevision must be a many-to-one independent product. Under somemild conditions, we also show that any strongly factorising lower prevision must be a many-to-many independent product.And since we already know that the smallest many-to-one independent product is the independent natural extension, wededuce that when looking for least-committal models, it is immaterial whether we focus on factorisation or on being anindependent product. This outcome might be very important in applications, as it allows one to work with the independentnatural extension simply by imposing a (strong) factorisation property while searching for least-committal models. On themore theoretical side, it constitutes a solid bridge between the formalist and epistemic approaches to independence.We believe these results give epistemic independence an opportunity to become a competitive alternative to moreconsolidated notions of independence. But how can epistemic independence be used in specific examples, and are thereadvantages to doing so? We present an interesting case study in Section 8, where we survey and discuss some of our recentwork on credal trees [11]. These constitute a special case of credal networks [4], which in turn extend Bayesian nets to5 In the simple case of two variables, there is no need to distinguish between many-to-one and many-to-many independence.1914G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950deal with imprecise probabilities. Traditionally, the extension is achieved by replacing the precise-probabilistic parametersof a Bayesian net with imprecise ones, and by re-interpreting the Markov condition through strong rather than stochasticindependence. But we have already argued that this might not be the best choice in all cases. For this reason, the workin Ref. [11] imposes an epistemic Markov condition on directed trees. We discuss this condition and provide some exam-ples, showing that it makes certain variables in the tree become epistemically many-to-many independent. Moreover, weshow how the independent natural extension, and its properties proved here, are crucial stepping stones that allow us toconstruct the least-committal joint model over the tree that arises out of the parameters through the epistemic Markovcondition. This particular type of joint allows for the development of an exact linear-time message-passing algorithm thatperforms imprecise-probabilistic updating of the epistemic tree. That this is at all possible, is rather surprising because ofthe above-mentioned perceived incompatibilities between epistemic independence and the graphoid axioms. It shows thatepistemic independence has a significant role to play in probabilistic-graphical models.We summarise our views on the results of this paper in Section 9. Appendices A and B respectively collect the proofs ofall results, and the counterexamples needed to explore the relations between the many notions we introduce and study.2. Coherent lower previsionsLet us give a brief overview of the concepts and results from the theory of coherent lower previsions that we use in thispaper. We refer to Ref. [30] for an in-depth study, and to Ref. [21] for a survey.2.1. Lower and upper previsionsConsider a variable X taking values in some possibility space X, which we assume in this paper to be finite. The theoryof coherent lower previsions aims to model uncertainty about the value of X by means of lower and upper previsions ofgambles. A gamble is a real-valued function on X, and we denote by L (X) the set of all gambles on X. This set isa linear space under pointwise addition of gambles, and pointwise multiplication of gambles with real numbers. For anysubset A of L (X), we denote by posi(A ) the set of all positive linear combinations of gambles in A :(cid:2)n(cid:3)(cid:4)posi(A ) :=λk fk: fk ∈ A , λk > 0, n > 0.k=1We call A a convex cone if it is closed under positive linear combinations, meaning that posi(A ) = A .For any two gambles f and g on a set X, we write ‘ f (cid:2) g’ if (∀x ∈ X ) f (x) (cid:2) g(x), and ‘ f > g’ if f (cid:2) g and f (cid:5)= g.A gamble f > 0 is called positive. A gamble g (cid:3) 0 is called non-positive. L (X)(cid:5)=0 denotes the set of all non-zero gambles,L (X)>0 the convex cone of all positive gambles, and L (X)(cid:2)0 the convex cone of all non-positive gambles on X.A lower prevision is a real-valued functional P defined on L (X). The lower prevision P is said to be coherent when itsatisfies the following three conditions:C1. P ( f ) (cid:2) min f for all f ∈ L (X);C2. P (λ f ) = λP ( f ) for all f ∈ L (X) and real λ (cid:2) 0;C3. P ( f + g) (cid:2) P ( f ) + P (g) for all f , g ∈ L (X).The conjugate of a lower prevision P is called an upper prevision. It is denoted by P , and defined by P ( f ) := −P (− f ) forany gamble f on X.One interesting particular case of lower previsions are the vacuous ones. Given a non-empty subset A of X, the vacuouslower prevision P A relative to A is given by P A( f ) = minx∈ A f (x). It serves as an appropriate model for those situationswhere the only information we have about X is that it takes a value in the set A.2.2. Linear previsions and envelope theoremsA coherent lower prevision P on L (X) satisfying P ( f + g) = P ( f ) + P (g) for allf , g ∈ L (X) is called a linearprevision, and is usually denoted by P . It corresponds to an expectation operator associated with the additive probabilitythat is its restriction to events. We denote the set of all linear previsions on L (X) by P(X). For any linear prevision Pon L (X), the corresponding mass function p is defined by p(x) := P (I{x}), x ∈ X, where I{x} denotes the indicator of thesingleton {x}. Then of course P ( f ) =(cid:5)x∈X f (x)p(x).Linear previsions can also be used to characterise the notion of coherence for lower previsions: a lower prevision P iscoherent if and only if it is the lower envelope of the closed convex set of dominating linear previsions(cid:6)M (P ) :=P ∈ P(X):(cid:7)(cid:8)∀ f ∈ L (X)(cid:9)P ( f ) (cid:2) P ( f ),so we have(cid:6)P ( f ) = min(cid:9)P ( f ): P ∈ M (P ).G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501915This is also equivalent to requiring that P should be the lower envelope of the set of extreme points of M (P ), which wedenote by ext(M (P )):(cid:6)P ( f ) = min(cid:7)P ( f ): P ∈ ext(cid:8)(cid:9)M (P ),where P is an extreme point of M (P ) when it cannot be written as a non-trivial convex combination of two differentelements of M (P ).2.3. Conditional lower previsionsNext, consider a number of variables Xn, n ∈ N, taking values in the respective finite sets Xn. Here N is some finiteindex set.(cid:10)(cid:10)r∈RFor every subset R of N, we denote by X R the tuple of variables (with one component for each r ∈ R) that takes valuesin the Cartesian product XR :=×r∈RXr such thatxr := xR (r) ∈ Xr for all r ∈ R. The elements of XR are generically denoted by xR or zR , with corresponding componentsxr := xR (r) or zr := zR (r), r ∈ R. We will always assume that the variables Xr are logically independent, which means that foreach non-empty subset R of N, X R may assume all values in XR .Xr . This Cartesian product is the set of all maps xR from R toWe must pay particular attention to the case R = ∅. By definition, X∅ is the set of all maps from ∅ tor∈∅ Xr = ∅. Itcontains only one element x∅: the empty map. This means that there is no uncertainty about the value of the variable X∅:it can assume only one value (the empty map). Moreover I{x∅} = 1.We also denote by L (XR ) the set of gambles defined on XR . We will frequently use the simplifying device of iden-tifying a gamble f R on XR with its cylindrical extension to XN , which is the gamble ˜f R defined by ˜f R (xN ) := f R (xR ) forall xN ∈ XN , where xR is the restriction (i.e., the projection) of xN to XR . To give an example, if K ⊆ L (XN ), this trickallows us to consider K ∩ L (XR ) as the set of those gambles in K that depend only (at most) on the variable X R . Asanother example, this device allows us to identify the gambles I{xR } and I{xR }×XN\R , and therefore also the events {xR } and{xR } × XN\R . More generally, for any event A ⊆ XR , we can identify the gambles I A and I A×XN\R , and therefore also theevents A and A × XN\R . In the same spirit, a lower prevision on all gambles in L (XR ) can be identified with a lowerprevision defined on the set of corresponding gambles on XN , a subset of L (XN ). If in particular R is the empty set, thenL (X∅) corresponds to the set of real numbers, which we can also identify with the set of constant gambles on XN .If P N is a coherent lower prevision on L (XN ), then for any non-empty subset R of N we can consider its XR -marginalP R as the coherent lower prevision on L (XR ) defined by P R ( f ) := P N ( f ) for all gambles f on XR : the restriction of P Nto gambles that depend only (at most) on X R .Given two disjoint subsets O and I of N, we define a conditional lower prevision P O ∪I (·| X I ) as a special two-placefunction. For any xI ∈ XI , P O ∪I (·|xI ) is a real functional on the set L (XO ∪I ) of all gambles on XO ∪I . For any gamble fon XO ∪I , P O ∪I ( f |xI ) is the lower prevision of f , conditional on X I = xI . Moreover, the object P O ∪I ( f | X I ) is considered as thegamble on XI that assumes the value P O ∪I ( f |xI ) in xI .We are allowing for I and O to be empty, mainly for the sake of generality and elegance in mathematical formulationand proofs. If I = ∅, then X I = X∅ assumes its only possible value (the empty map x∅) with certainty, so conditioning onX∅ = x∅ amounts to not conditioning at all, and P O ∪I ( f | X I ) is then essentially the same thing as an unconditional lowerprevision P O . We will come back to the other case O = ∅ shortly.We now turn to the most important rationality criteria for such conditional lower previsions. The conditional lowerprevision P O ∪I (·| X I ) is called separately coherent when it satisfies the following three conditions for all xI ∈ XI , non-negative λ and gambles f , g ∈ L (XO ∪I ):SC1. P O ∪I ( f |xI ) (cid:2) minxO ∈XO f (xO , xI );SC2. P O ∪I (λ f |xI ) = λP O ∪I ( f |xI );SC3. P O ∪I ( f + g|xI ) (cid:2) P O ∪I ( f |xI ) + P O ∪I (g|xI ).When SC3 is satisfied with equality for allusually denoted by P O ∪I (·| X I ). It is an expectation operator with respect to a conditional probability (or mass function).f , g ∈ L (XO ∪I ), then P O ∪I (·| X I ) is called a conditional linear prevision, andAn important consequence of separate coherence is that(cid:7)(cid:8)(cid:11)(cid:11)xIg(·, xI )P O ∪I (g|xI ) = P O ∪Iand P O ∪I ( f g| X I ) = f P O ∪I (g| X I )(1)for all xI ∈ XI , all non-negative gambles f on XI and all gambles g on XO ∪I [30, Theorems 6.2.4 and 6.2.6(l)]. The firstequality tells us that P O ∪I (·|xI ) is completely determined by its behaviour on L (XO ), and we will therefore often identifyP O ∪I (·|xI ) with a lower prevision on L (XO ). To prove (1), observe that if h1(·, xI ) = h2(·, xI ) then it follows from SC1 thatP O ∪I (h1 − h2|xI ) = P O ∪I (h2 − h1|xI ) = 0, and therefore from SC3 that P O ∪I (h1|xI ) = P O ∪I (h2|xI ). The first equality thenfollows by letting h1 := g and h2 := g(·, xI ).It is clear from SC1–SC3 that P O ∪I (·| X I ) is separately coherent if and only if for all xI ∈ XI , P O ∪I (·|xI ) is a coherentlower prevision on L (XO ) and moreover condition (1) holds [this second condition turns out to be equivalent to requiringthat P O ∪I ({xI }|xI ) = 1 for every xI ∈ XI ].1916G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950In the degenerate case that O = ∅, separate coherence guarantees that P O ∪I ( f |xI ) = f (xI ) and therefore P O ∪I ( f | X I ) = ff ∈ L (XI ). When I = ∅, separate coherence of P O ∪I (·| X I ) = P O reduces to coherence of the unconditional lowerfor allprevision P O .2.4. The behavioural interpretationThe coherence concepts introduced above may be better understood in terms of the behavioural interpretation of (condi-tional) lower previsions.6 If we see a gamble f as an uncertain reward, then the lower prevision P ( f ) can be interpreted asa subject’s supremum acceptable price for buying the gamble f , in the sense that it is the supremum real μ such that sheconsiders the transaction f − μ, which is equivalent to buying f for a price μ, to be desirable. It follows that she considersit desirable to buy f for any price P ( f ) − ε, ε > 0. Similarly, we can regard her upper prevision P ( f ) for f her infimumacceptable selling price for f , in the sense that it is the infimum real μ such that she considers the transaction μ − f ,which is equivalent to selling f for a price μ, to be desirable. In particular, she considers it desirable to sell f for any priceP ( f ) + ε, ε > 0. And a linear prevision P ( f ) corresponds to the case where her supremum acceptable buying price for thegamble f coincides with her infimum acceptable selling price, meaning that she expresses a preference between buyingand selling the gamble f for a price μ for almost all prices μ.If we follow this interpretation, we can similarly interpret a subject’s conditional lower prevision for a gamble f condi-if she were to find out (at some later point)tional on a value xI as her current supremum acceptable buying price for fthat X I = xI .A coherent unconditional lower prevision P is one for which we cannot raise the supremum acceptable buying priceP ( f ) for any gamble f by taking into account the implications of other desirable transactions. A separately coherent condi-tional lower prevision P O ∪I (·| X I ) is one where a similar requirement holds for every conditioning event X I = xI , and wheremoreover our subject is currently disposed to betting at all odds on the event X I = xI if she were to observe it at somelater point.2.5. Coherence and weak coherenceWe now turn from separate to joint coherence. For any gamble f on XO ∪I and any xI ∈ XI , we defineG O ∪I ( f |xI ) := I{xI }(cid:12)(cid:13)f − P O ∪I ( f | X I )(cid:12)= I{xI }f (·, xI ) − P O ∪I(cid:7)f (·, xI )(cid:8)(cid:13)(cid:11)(cid:11)xIandG O ∪I ( f | X I ) := f − P O ∪I ( f | X I ) =(cid:3)xI ∈XIG O ∪I ( f |xI ) =(cid:3)(cid:12)I{xI }xI ∈XIf (·, xI ) − P O ∪I(cid:7)(cid:11)(cid:11)xIf (·, xI )(cid:8)(cid:13).Taking into account the behavioural interpretation of conditional lower previsions summarised in Section 2.4, we may regardG O ∪I ( f |xI ) as an almost-desirable gamble, in the sense that for every ε > 0, the gamble G O ∪I ( f |xI ) + εI{xI } corresponds tobuying f for a price P O ∪I ( f |xI ) + ε, contingent on the event {xI }. And since taking finite sums of almost-desirable gamblesproduces almost-desirable gambles, so should G O ∪I ( f | X I ) be.Observe that G O ∪I ( f | X I ) is always equal to 0 when O = ∅. We also define, for any gamble f on XO ∪I , the XI -supportsuppI ( f ) of f as the set of elements of XI where the partial gamble f (·, xI ) is non-zero:(cid:6)(cid:9)(cid:9)(cid:6)suppI ( f ) :=xI ∈ XI : I{xI } f (cid:5)= 0=xI ∈ XI : f (·, xI ) (cid:5)= 0.This support suppI ( f ) is a subset of XI , but as we already mentioned before, it will be convenient to identify it with thesubset suppI ( f ) × XN\I of XN .Consider disjoint subsets O j and I j of N. A collection of (separately coherent) conditional linear previsions P O j∪I j (·| X I j )j ∈j ∈ {1, . . . , m}, is called (strongly) coherent if for allf j ∈ L (XO j ∪I j ),defined on the sets of gambles L (XO j ∪I j ),{1, . . . , m}, there is some zN ∈(cid:15)mj=1 suppI j(cid:10)(cid:14)( f j) such that:m(cid:3)G O j ∪I j ( f j| X I j )(zN ) (cid:2) 0.(2)j=1j ∈The (separately coherent) conditional{1, . . . , m}, are called coherent if and only if they are lower envelopes of a collection {P λ(·| X I j ): λ ∈ Λ} of coherent con-ditional linear previsions. This is equivalent to requiring that for all f j ∈ L (XO j ∪I j ) where j ∈ {1, . . . , m}, all k ∈ {1, . . . , m},} ∪all xIklower previsions P O j∪I j (·| X I j ) defined on the sets of gambles L (XO j ∪I j ),∈ XIk and all g ∈ L (XO k∪Ik ), there is some zN ∈ {xIkmj=1 suppI j ( f j) such that:O j∪I j(cid:10)6 See Refs. [30,32] for more details.G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950(cid:14)m(cid:3)(cid:15)G O j ∪I j ( f j| X I j ) − G O k∪Ik (g|xIk )(zN ) (cid:2) 0.j=11917(3)We say that the conditional lower previsions P O j ∪I j (·| X I j ) are weakly coherent if for all{1, . . . , m}, all k ∈ {1, . . . , m}, all xIk∈ XIk and all g ∈ L (XO k∪Ik ), there is some zN ∈ XN such that:f j ∈ L (XO j ∪I j ) where j ∈(cid:15)G O j ∪I j ( f j| X I j ) − G O k∪Ik (g|xIk )(zN ) (cid:2) 0.(cid:14)m(cid:3)j=1This condition requires that our subject should not be able to raise her supremum acceptable buying price P O k∪Ik (g|xIk ) for} by taking into account the implications of other conditional assessments. However, undera gamble g contingent on {xIkthe behavioural interpretation, a collection of weakly coherent conditional lower previsions can still present some forms ofinconsistency with one another. See Refs. [30, Chapter 7], [22] and [31] for discussion. These inconsistencies are eliminatedby the stronger notion of coherence given by Eq. (3), where we focus only on the elements in the supports of the gambles.If the conditional lower previsions P O j ∪I j (·| X I j ) are coherent, then they are clearly also weakly coherent. The followingcharacterisation of weak coherence will be useful. The equivalence between the first two statements was proved in Ref. [24,Theorem 1], while the equivalence between the second and third statements is a consequence of Ref. [30, Section 6.5.4].Theorem 1. The conditional lower previsions P O j∪I j (·| X I j ), j = 1, . . . , m, are weakly coherent if and only if there is some coherentlower prevision P N on L (XN ) satisfying any (and hence all) of the following equivalent conditions:(a) P N and P O j∪I j (·| X I j ), j = 1, . . . , m, are weakly coherent;(b) For all j = 1, . . . , m, P N and P O j∪I j (·| X I j ) are pairwise coherent;∈ XI j and all gambles f on XO j ∪I j :(c) For all j = 1, . . . , m, all xI j(cid:7)(cid:8)= 0.G O j ∪I j ( f |xI j )P N(GBR)The last condition in this theorem is called the Generalised Bayes Rule, and reduces to Bayes’s rule in the case of linearconditional and unconditional previsions. A consequence of (GBR) in our context is that(cid:7)(cid:8)P O j ∪I j ( f | X I j )P N(cid:2) P N ( f ) (cid:2) P N(cid:7)(cid:8)P O j ∪I j ( f | X I j )for every gamble f on XO j ∪I j .We will also use the following result in our argumentation.(4)Theorem 2. (See Reduction Theorem [30, Theorem 7.1.5].) Let P O j∪I j (·| X I j ) be separately coherent conditional lower previsions de-fined on the sets of gambles L (XO j ∪I j ) with I j (cid:5)= ∅ for all j = 1, . . . , m, and let P N be a coherent lower prevision on L (XN ). ThenP N and P O j ∪I j (·| X I j ), j = 1, . . . , m, are coherent if and only if the following two conditions hold:(a) P N and P O j∪I j (·| X I j ), j = 1, . . . , m, are weakly coherent;(b) P O j ∪I j (·| X I j ), j = 1, . . . , m, are coherent.2.6. Natural and regular extensionLet P O j∪I j (·| X I j ) be coherent conditional lower previsions defined on the sets of gambles L (XO j ∪I j ),j = 1, . . . , m. Ifwe now consider any disjoint subsets O and I of N, the natural extension P O ∪I (·| X I ) of these conditional lower previsionsis defined on L (XO ∪I ) by(cid:15)(cid:4)E O ∪I ( f |xI ) := supμ:G O j ∪I j (g j| X I j ) − I{xI }( f − μ)< 0 on {xI } ∪suppI j(g j) for some g j ∈ L (XO j ∪I j )(cid:2)(cid:14)m(cid:3)j=1m(cid:16)j=1f ∈ L (XO ∪I ) and all xI ∈ XI . E O ∪I ( f |xI ) represents the supremum acceptable buying price for a gamble f con-for alltingent on {xI } that can be derived from the assessments in P O j ∪I j (·| X I j ), j = 1, . . . , m, using arguments of coherence. SeeRefs. [30, Chapter 8], [22] and [31] for additional information.In particular, the unconditional natural extension of P O j ∪I j (·| X I j ), j = 1, . . . , m, is given by(cid:2)(cid:14)(cid:15)(cid:4)E N ( f ) := supminf −G O j ∪I j (g j| X I j ): g j ∈ L (XO j ∪I j ),(5)m(cid:3)j=1for all gambles f on XN , and it is the pointwise smallest coherent lower prevision that is coherent with the P O j∪I j (·| X I j ),j = 1, . . . , m.1918G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950Another particular case of interest is when we want to derive conditional lower previsions from unconditional ones.Consider a subset I of N, and a coherent lower prevision P N on XN . The natural extension E N (·| X I ) of P N to a lowerprevision on L (XN ) conditional on X I is given by(cid:17)E N ( f |xI ) =max{μ ∈ R: P N (I{xI }[ f − μ]) (cid:2) 0}minzN\I ∈XN\I f (zN\I , xI )if P N ({xI }) > 0,if P N ({xI }) = 0.It defines a separately coherent conditional lower prevision that is also coherent with P N , and it is indeed the smallest suchconditional lower prevision.On the other hand, the regular extension R(·| X I ) of P N to a lower prevision on L (XN ) conditional on X I is given by(cid:17)R N ( f |xI ) :=max{μ ∈ R: P N (I{xI }[ f − μ]) (cid:2) 0}minzN\I ∈XN\I f (zN\I , xI )if P N ({xI }) > 0,if P N ({xI }) = 0.The natural and regular extensions coincide unless P N ({xI }) > P N ({xI }) = 0, in which case we may have R N ( f |xI ) >E N ( f |xI ). In fact, when P N ({xI }) > 0 there is a unique value of P N ( f |xI ) satisfying (GBR) with respect to P N , but thisis no longer true if P N ({xI }) = 0. See Refs. [22,25] for additional information.The regular extension defines a separately coherent conditional lower prevision that is also coherent with P N , and wewill have occasion to use it as a tool for deriving conditional lower previsions from unconditional ones. The following result,which follows from Theorem 6 in Ref. [22], makes regular extension especially useful in this respect:Theorem 3. Let P N be a coherent lower prevision on L (XN ). Consider disjoint O j and I j for j = 1, . . . , m. Assume that P N ({xI j0 for all xI jprevisions P O 1∪I1 (·| X I1 ), . . . , P O m∪Im (·| X Im ).}) >∈ XI j , and define P O j ∪I j (·| X I j ) using regular extension for j = 1, . . . , m. Then P N is coherent with the conditional lower3. The formal approach to independence3.1. Basic definitionsConsider a number of (logically independent) variables Xn, n ∈ N, assuming values in the respective finite sets Xn. HereN is some finite index set. We assume that for each of these variables Xn, we have an uncertainty model for the values thatit assumes in Xn, in the form of a coherent lower prevision P n on the set L (Xn) of all gambles (real-valued maps) onXn.We begin our discussion of independence by following the formalist route: we introduce a number of interesting gener-alisations of the notion of an independent product of linear previsions.The first is a stronger, symmetrised version of the notion of ‘forward factorisation’ that was introduced elsewhere [12].Definition 1 (Productivity). Consider a coherent lower prevision P N on L (XN ). We call this lower prevision productive iffor all disjoint subsets7 I and O of N, all g ∈ L (XO ) and all non-negative f ∈ L (XI ), P N ( f [g − P N (g)]) (cid:2) 0.The intuition behind this definition is that a coherent lower prevision P N is productive if multiplying an almost-desirablegamble on X O (the gamble g − P N (g), which has lower prevision zero) with any non-negative gamble f that depends on adifferent variable X I , preserves its almost-desirability, in the sense that the lower prevision of the product is non-negative.In other words, if we construct a gamble on XO ∪I by piecing together almost-desirable gambles from L (XO ), we obtaina gamble that is still almost-desirable.A lower envelope P N of productive coherent lower previsions P λ, λ ∈ Λ, is again productive: for all disjoint subsets Iand O of N, all g ∈ L (XO ) and all non-negative f ∈ L (XI ), we have that P λ( f [g − P N (g)]) (cid:2) P λ( f [g − P λ(g)]) (cid:2) 0, andtherefore indeed P N ( f [g − P N (g)]) (cid:2) 0.In the paper [12] on laws of large numbers for coherent lower previsions, which generalises and subsumes most knownversions in the literature, we have proved that the condition for forward irrelevance8 (which is implied by the presentproductivity condition) is sufficient for a weak law of large numbers to hold. So we are led to the following immediateconclusion.Theorem 4 (Weak law of large numbers). (See [12, Theorem 2].) Let the coherent lower prevision P N on L (XN ) be productive. Letε > 0 and consider arbitrary gambles hn on Xn, n ∈ N. Let B be a common bound for the ranges of these gambles and let min hn (cid:3)mn (cid:3) P N (hn) (cid:3) P N (hn) (cid:3) Mn (cid:3) max hn for all n ∈ N. Then7 The coherence of P N guarantees that for empty I or O the corresponding condition is trivially satisfied.8 This condition is a particular instance of the epistemic irrelevance we discuss in Section 4: if we consider n variables X1, . . . , Xn , then for all k = 2, . . . , nthe variables X1, . . . , Xk−1 are epistemically irrelevant to Xk . See Refs. [12,13] for more information.G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501919(cid:18)(cid:17)P NxN ∈ XN :(cid:3)n∈Nmn|N|− ε (cid:3)(cid:3)n∈Nhn(xn)|N|(cid:3)(cid:3)n∈NMn|N|(cid:19)(cid:20)(cid:18)+ ε(cid:2) 1 − 2 exp−(cid:20).|N|ε24B2Next comes a version of a condition that has proved quite useful in the context of research on credal networks [11],which we shall discuss in Section 8.Definition 2 (Factorisation). Consider a coherent lower prevision P N on L (XN ). We call this lower prevision(i) factorising9 if for all o ∈ N and all I ⊆ N \ {o},10 allfo ∈ L (Xo) and all non-negative f i ∈ L (Xi), i ∈ I , P N ( f I fo) =(ii) strongly factorising if P N ( f g) = P N ( f P N (g)) for all g ∈ L (XO ) and non-negative f ∈ L (XI ), where I and O areP N ( f I P N ( fo)), where f I :=(cid:21)i∈I f i ;any11 disjoint subsets of N.It will at this point be useful to introduce the following notation. Consider a real interval a := [a, a] and a real number b,thena (cid:10) b :=(cid:17)ab if b (cid:2) 0,ab if b (cid:3) 0.Also, we denote by P ( f ) the interval [P ( f ), P ( f )].It follows from the coherence of P N that given a factorising (respectively strongly factorising) coherent lower previsionwe also get P N ( f I P N ( fo)) = P N ( f I ) (cid:10) P N ( fo) (respectively P N ( f P N (g)) = P N ( f ) (cid:10) P N (g)) in Definition 2. We then havethe following characterisations of factorising lower previsions, based on their coherence and conjugacy properties.Proposition 5. A coherent lower prevision P N on L (XN ) is factorising if and only if for all o ∈ N, all fo ∈ L (Xo) and all non-negative f i ∈ L (Xi), i ∈ N \ {o}, any (and hence all) of the following equivalent conditions holds:(cid:21)(cid:21)(i) P N ((ii) P N (n∈N fn) = P N (P N ( fo)(cid:21)(cid:17)(cid:21)n∈N fn) =P N ( fo)P N ( fo)(cid:21)i∈N\{o} f i) = P N ((cid:21)i∈N\{o} f i) (cid:10) P N ( fo);i∈N\{o} P N ( f i)i∈N\{o} P N ( f i)if P N ( fo) (cid:2) 0,if P N ( fo) (cid:3) 0.The difference between factorisation and strong factorisation lies in the types of products considered: in the first case,we only consider gambles that are products of non-negative gambles each depending on a single variable, while in thesecond case the non-negative gamble considered need not be such a product.Finally, there is the property that the late Russian mathematician Vladimir Kuznetsov first drew attention to [19]. Inorder to define it, we use (cid:4) to denote the (commutative and associative) interval product operator defined by:[a, b] (cid:4) [c, d] :==(cid:6)(cid:9)xy: x ∈ [a, b] and y ∈ [c, d](cid:12)min{ac, ad, bc, bd}, max{ac, ad, bc, bd}(cid:13)for all a (cid:3) b and c (cid:3) d in R.Definition 3 (Kuznetsov product). Consider a coherent lower prevision P N on L (XN ). We call this lower prevision(i) a Kuznetsov product, or simply, Kuznetsov, if P N ((ii) a strong Kuznetsov product, or simply, strongly Kuznetsov, if P N ( f g) = P N ( f ) (cid:4) P N (g) for all g ∈ L (XO ) and all(cid:21)n∈N fn) =(cid:4)n∈N P N ( fn) for all fn ∈ L (Xn), n ∈ N;L (XI ), where I and O are any disjoint subsets of N.11f ∈These two properties are based on the sensitivity analysis interpretation of coherent lower previsions: if we consider aproduct of gambles and for each of gamble we have an interval of possible values for its expectation, the Kuznetsov propertyimplies that the interval of possible values for the expectation of the product coincides with the product of the intervals ofthe expectations of the different gambles. As before, the difference between being a Kuznetsov product and being a strongKuznetsov product resides in whether the gambles we are multiplying depend on one variable only or on several variablesat once.We will show later that all the properties introduced above generalise the factorisation property of precise probabilitiesto the imprecise case. Here are the general relationships between them:9 The present notion of factorisation when restricted to lower probabilities and events, is called strict factorisation in Ref. [29].10 The coherence of P N guarantees that for empty I the corresponding condition is trivially satisfied.11 The coherence of P N guarantees that for empty I or O the corresponding condition is trivially satisfied.1920G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950Proposition 6. Consider a coherent lower prevision P N on L (XN ). ThenP N is strongly Kuznetsov ⇒ P N is strongly factorising ⇒ P N is productive⇓⇓P N is Kuznetsov⇒P N is factorising.The intuition behind these implications is the following (see Appendix A for a detailed proof). On the one hand, beingstrongly Kuznetsov clearly implies being Kuznetsov, and strong factorisation implies factorisation, because we allow for moreinvolved products of gambles in the definition of the former. On the other hand, the factorisation conditions focus on thelower prevision only, whereas the Kuznetsov ones involve both lower and the upper previsions, and therefore give rise tostronger conditions. And finally, it follows from its definition that a strongly factorising coherent lower prevision satisfiesthe condition of productivity with equality instead of with inequality.In Appendix B we present examples showing that the converses of the implications in this proposition do not hold ingeneral. Specifically, in Example 3 we give a coherent lower prevision that satisfies productivity but none of the other prop-erties; in Example 5 we give a coherent lower prevision that is strongly factorising (and as a consequence also factorisingand productive) but not Kuznetsov (and therefore not strongly Kuznetsov); and in Example 6 we have a factorising coherentlower prevision that satisfies none of the other properties. The only related open problem at this point is whether beingKuznetsov is generally equivalent to being strongly Kuznetsov.We will show that the independent natural extension from Section 5 is strongly factorising but not Kuznetsov in general.In the rest of this section, we look at a number of special cases that will prove instrumental in what follows. In particular,the strong product we will study in Section 3.3 satisfies all the properties we have introduced here.3.2. The product of linear previsionsIf we have linear previsions Pn on L (Xn) with corresponding mass functions pn, then their product S N :=×n∈N Pn isthe linear prevision on L (XN ) defined asS N ( f ) =(cid:3)xN ∈XNf (xN )(cid:22)n∈Npn(xn)for all f ∈ L (XN ).(6)For any non-empty subset R of N, we also denote by S R :=×r∈R P r the product of the linear previsions P r , r ∈ R.Useful, and immediate, are the following marginalisation and associativity properties of the product of linear previsions.They imply that for linear previsions all the properties introduced in Section 3.1 coincide.Proposition 7. Consider arbitrary linear previsions P n on L (Xn), n ∈ N.Pn) × (×n∈N2(i) For any non-empty subset R of N, S R is the XR -marginal of S N : S N (g) = S R (g) for all gambles g on XR ;Pn = (×n∈N1(ii) For any partition N1 and N2 of N,×n∈N1∪N2Pn), or in other words, S N = S N1Moreover, for any linear prevision P N on L (XN ), the following statements are equivalent:(a) P N =×n∈N Pn is the product of its marginals P n;n∈N fn) =(b) P N ((c) P N is strongly Kuznetsov;(d) P N is Kuznetsov;(e) P N is strongly factorising;(f) P N is factorising;(g) P N is productive.n∈N Pn( fn) for all fn ∈ L (Xn), n ∈ N;(cid:21)(cid:21)× S N2 .3.3. The strong product of coherent lower previsionsIn a similar vein, if we have coherent lower previsions P n on L (Xn), then their so-called strong product [30, Sec-tion 9.3.5]12 S N :=×n∈N P n is defined as the coherent lower prevision on L (XN ) that is the lower envelope of the set ofindependent products {×n∈N Pn: (∀n ∈ N) Pn ∈ M (P n)}, or equivalently, of the set {×n∈N Pn: (∀n ∈ N) Pn ∈ ext(M (P n))}.So for every f ∈ L (XN ):12 Walley [30, Section 9.3.5] calls this lower prevision the type-1 product. The term ‘strong product’ seems to go back to Cozman [3].G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950(cid:24)P n( f ): (∀n ∈ N) P n ∈ M (P n)P n( f ): (∀n ∈ N) P n ∈ ext(cid:7)(cid:8)(cid:24).M (P n)1921(7)(8)(cid:23)×(cid:23)×n∈NS N ( f ) = inf= infn∈NFor any non-empty subset R of N, we also denote by S R :=×r∈R P r the strong product of the coherent lower previsionsP r , r ∈ R. Like the product of linear previsions, the strong product of lower previsions satisfies the following marginalisationand associativity properties, and also all the properties defined in Section 3.1:13Proposition 8. Consider arbitrary coherent lower previsions P n on L (Xn), n ∈ N.(i) For any non-empty subset R of N, S R is the XR -marginal of S N : S N (g) = S R (g) for all gambles g on XR ;(ii) ext(M (S N )) = {×n∈N Pn: (∀n ∈ N) Pn ∈ ext(M (P n))};P n = (×n∈N1(iii) For any partition N1 and N2 of N,×n∈N1∪N2P n), or in other words, S N = S N1P n) × (×n∈N2× S N2 ;(iv) The strong product S N is strongly Kuznetsov, and therefore also Kuznetsov, strongly factorising, factorising and productive.The nice characterisation of the set ext(M (S N )) in the second statement guarantees that the infima in Eqs. (7) and (8)are actually minima. This set of extreme points may be infinite even when dealing with finite Xn, n ∈ N, and thereforehence the second statement is not immediate. In addition, this result guarantees, amongst other things, that the strongproduct satisfies the weak law of large numbers of Theorem 4.This marks a preliminary end to our formalist discussion of independence for coherent lower previsions. In the nextsection, we turn to the treatment of independence following an epistemic and coherentist approach, where independence isconsidered to be an assessment a subject makes. The more formalist thread will be taken up again in Section 6.4. Epistemic irrelevance and independenceConsider two disjoint subsets I and O of N. We say that a subject judges that X I is epistemically irrelevant to X O whenshe assumes that learning which value X I assumes in XI will not affect her beliefs about X O . Taking into account thebehavioural interpretation of coherent lower previsions summarised in Section 2.4, this means that the subject’s supremumacceptable buying price for a gamble f contingent on the event that X I = xI coincides with her supremum acceptablebuying price for f , irrespective of the value xI ∈ XI that is observed.Now assume that our subject has a coherent lower prevision P N on L (XN ). If she assesses that X I is epistemicallyirrelevant to X O , this implies that she can infer from her joint model P N the following conditional model P O ∪I (·| X I ) onthe set L (XO ∪I ):P O ∪I (h|xI ) := P N(cid:8)(cid:7)h(·, xI )for all gambles h on XO ∪I and all xI ∈ XI .So an assessment of epistemic irrelevance is useful because it allows us to derive conditional lower previsions from uncon-ditional ones. It follows from the comments in Section 2 that such an assessment is trivial when either O or I is the emptyset. Similarly, if the gamble h depends only on X O , then the conditional lower prevision for h conditional on some valuexI ∈ XI coincides with the unconditional lower prevision of h.It should be clear from the previous discussion that epistemic irrelevance is an asymmetric notion: we only require thatX I is epistemically irrelevant to X O ; and this does not necessarily imply that X O should be epistemically irrelevant to X I .Refs. [6,13] include thorough discussions of this point, along with examples illustrating how such asymmetries may appearin concrete cases. Because of this, it becomes necessary to pay special attention to the mutual irrelevance of two or morevariables. We then talk about epistemic independence, a suitably symmetrised version of epistemic irrelevance.4.1. Epistemic many-to-many independenceWe say that a subject judges the variables Xn, n ∈ N, to be epistemically many-to-many independent when she assumesthat learning the value of any number of these variables will not affect her beliefs about the others. In other words, if shejudges for any disjoint subsets I and O of N that X I is epistemically irrelevant to X O .Again, if our subject has a coherent lower prevision P N on L (XN ), and she assesses that the variables Xn, n ∈ N, areepistemically many-to-many independent, then she can infer from her joint model P N a family of conditional models(cid:6)I (P N ) :=P O ∪I (·| X I ): I and O disjoint subsets of N(cid:9),where P O ∪I (·| X I ) is the coherent lower prevision on L (XO ∪I ) given by:13 For the case of two variables (N = {1, 2}), Cozman [3] was the first to prove that the strong product is Kuznetsov.1922G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950P O ∪I (h|xI ) := P N(cid:8)(cid:7)h(·, xI )for all gambles h on XO ∪I and all xI ∈ XI .The crucial idea that the arguments in this paper hinge on, is that a coherent lower prevision P N expresses independencewhen it does not lead to incoherence when combined with an assessment of epistemic independence: the lower previsionP N and the conditional supremum acceptable buying prices derived from it using epistemic independence should not violatethe consistency conditions introduced in Section 2.Definition 4 (Many-to-many independence). A coherent lower prevision P N on L (XN ) is called many-to-many independentif it is coherent with the family of conditional lower previsions I (P N ). For a collection of coherent lower previsions P non L (Xn), n ∈ N, any many-to-many independent coherent lower prevision P N on L (XN ) that coincides with the P n ontheir domains L (Xn), n ∈ N, is called a many-to-many independent product of these marginals.As we show in the following example, this requirement of coherence of the unconditional and resulting conditionalmodels is by no means trivial: not all coherent lower previsions P N express—are compatible with an assessment of—epistemic independence.Example 1 (Independence is not trivial). Consider X1 = X2 = {0, 1}, and let P {1,2} be the linear prevision determined by2 . On the other hand, with x2 = 1,2 . Consider the gamble f := I{1} on X1, so P {1,2}( f ) = 1P {1,2}({(0, 0)}) = P {1,2}({(1, 1)}) = 1we get(cid:12)(cid:7)I{x2}P {1,2}(cid:13)(cid:8)f − P {1,2}( f )= 12− 12· 12= 14(cid:5)= 0.This shows that P {1,2} is not coherent with the conditional prevision P {1,2}(·| X2), and as a consequence it is not many-to-many independent. (cid:2)That a coherent lower prevision is not a many-to-many independent product need not mean that it cannot be coherentlyupdated: in the case of Example 1, we can always do so by applying Bayes’s rule. What it does mean is that it cannot becoherently updated in such a way that at the same time the epistemic independence conditions are satisfied.We shall see examples further on that seem to suggest that being a many-to-many independent product might be tooweak a requirement in certain situations: Example 3 in Appendix B establishes the existence of a non-vacuous many-to-many independent product with vacuous marginals, and intuition might suggest that an independent product of vacuousmarginals should be vacuous. The reason why such examples exist, is that coherence with the conditional lower previsionsinduced by the marginals can sometimes be very easy to satisfy, as Proposition 26 further on will show. Because of this, itmight be thought useful to require, in addition, some of the factorisation conditions from Section 3. We come back to thisissue in the Conclusions.4.2. Epistemic many-to-one independenceThere is a weaker notion of independence that we will consider here, which is mainly useful as a kind of catalyst,facilitating our search for the many-to-many independent products we are really after. We will coin the term ‘epistemicmany-to-one independence’ to identify it. We say that a subject judges the variables Xn, n ∈ N, to be epistemically many-to-one independent when she assumes that learning the value of any number of these variables will not affect her beliefs aboutany single other. In other words, if she judges for any o ∈ N and any subset I of N \ {o} that X I is epistemically irrelevant toXo.Once again, if our subject has a coherent lower prevision P N on L (XN ), and she assesses that the variables Xn, n ∈ N,are epistemically many-to-one independent, then she can infer from her joint model P N a family of conditional modelsN (P n, n ∈ N) :=(cid:6)(cid:9)P {o}∪I (·| X I ): o ∈ N and I ⊆ N \ {o},where P {o}∪I (·| X I ) is the coherent lower prevision on L (X{o}∪I ) given by:(cid:7)(cid:8)h(·, xI )(cid:7)(cid:8)h(·, xI )= P oP {o}∪I (h|xI ) := P Nfor all h ∈ L (X{o}∪I ) and xI ∈ XI ,(9)where of course P o is the Xo-marginal lower prevision of P N . In the set N (P n, n ∈ N) we are also allowing for empty I , inwhich case the conditional lower prevision P {o}∪I (·| X I ) reduces to the marginal P o. So we see that the family of conditionallower previsions N (P n, n ∈ N) only depends on the joint model P N through its Xn-marginals P n, n ∈ N (which, of course,explains our notation for it). This allows us, in particular, to define the family N (P n, n ∈ N) also starting from coherentlower previsions P n on L (Xn), rather than from a joint P N . The distinction between the two cases will be clear from thecontext. We use this observation in the definition of the many-to-one independent product of given marginals:Definition 5. A coherent lower prevision P N on L (XN ) with marginals P n, n ∈ N, is called many-to-one independent if it iscoherent with the family N (P n, n ∈ N). For a collection of coherent lower previsions P n on L (Xn), n ∈ N, any coherentG. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501923lower prevision P N on L (XN ) that is coherent with the family N (P n, n ∈ N) is called a many-to-one independent productof these lower previsions P n.If a coherent lower prevision P N is many-to-many independent, then it is also many-to-one independent: if P N iscoherent with the family I (P N ), it is certainly also coherent with the subfamily N (P n, n ∈ N) of I (P N ). Trivially, bothconditions are equivalent when N = {1, 2}, and as a consequence we can also use Example 1 to conclude that also many-to-one independence is not trivial. However, many-to-many and many-to-one independence are generally not equivalent whenthe set N has more than two elements: an explicit example is provided in Example 6 in Appendix B.Any many-to-one independent product of the coherent lower previsions P n, n ∈ N, must have these lower previsions asits marginals. This follows by applying the coherence condition in particular to the pairs P N and P {n}∪∅(·| X∅), as will bemade explicit in Corollary 15.4.3. Useful basic propertiesA basic coherence result [30, Theorem 7.1.6] states that taking lower envelopes of a family of coherent conditional lowerN , λ ∈ Λ, of many-previsions produces coherent conditional lower previsions. As a consequence, if we consider a family P λto-one independent products, then for each λ the lower prevision P λN is coherent with the family of conditional lowerprevisions N (P λN is coherent with the lower envelopesof N (P λn, n ∈ N), which are precisely the family N (P n, n ∈ N) of conditional lower previsions that can be derived from P Nusing epistemic irrelevance: the marginal lower previsions of P N are the lower envelopes of the marginal lower previsionsof P λN . Hence, P N is also a many-to-one independent product. A similar argument shows that many-to-many independenceis preserved by taking lower envelopes.n, n ∈ N). By taking lower envelopes, we deduce that P N := infλ∈Λ P λWe also have the following marginalisation and associativity properties.Proposition 9. Consider arbitrary coherent lower previsions P n, n ∈ N. Let P N be any many-to-one independent product and Q N anymany-to-many independent product of the marginals P n, n ∈ N. Let R and S be any subsets of N.(i) The XR -marginal P R of P N is a many-to-one independent product of its marginals P r , r ∈ R;(ii) The XR -marginal Q R of Q N is a many-to-many independent product of its marginals P r , r ∈ R;(iii) If R and S constitute a partition of N, then Q N is a many-to-many independent product of its XR -marginal Q R and its XS -marginal Q S .The associativity property in (iii) follows immediately from the definition of a many-to-many independent product, andmeans that the joint model still satisfies many-to-many independence with respect to the marginals Q R and Q S ; wehave established a similar property for strong independence in Proposition 8. To see that an analogous property does notgenerally hold for many-to-one products, consider the coherent lower prevision Q N in Example 6 in Appendix B.To conclude, we consider the case where all the lower previsions we want to combine into an independent joint modelare actually linear previsions. The following result shows that our definitions of many-to-one and many-to-many indepen-dent products extend the existing ones for linear previsions. It will also provide the basis for Proposition 12, which willallow us to kick-start the discussion in Section 5.Proposition 10. Any linear previsions P n on L (Xn), n ∈ N, have a unique many-to-many independent product and a unique many-to-one independent product, and both are equal to the (strong) product S N :=×n∈N Pn.4.4. Conditional independenceBesides the variables Xn, n ∈ N, considered so far, we now consider another variable Y assuming values in a finite set Y .We assume the variables X N and Y to be logically independent: the variable ( X N , Y ) can assume all values in the Cartesianproduct XN × Y .We also consider separately coherent conditional lower previsions P O ∪I (·| X I , Y ) on L (XO ∪I × Y ) where I and O aredisjoint subsets of N. It is important to realise that in all these conditional lower previsions, the variable Y consistently appears as aconditioning variable.We can use this set-up to generalise the notions of epistemic irrelevance and independence to those of conditionalepistemic irrelevance and independence. As an example, consider two disjoint subsets I and O of N. We say that a subjectjudges that X I is epistemically irrelevant to X O , conditional on Y , when she assumes that when knowing the value of Y , learningin addition which value X I assumes in XI will not affect her beliefs about X O .Assume that our subject has a separately coherent conditional lower prevision P N (·|Y ) on L (XN × Y ). If she as-sesses that X I is epistemically irrelevant to X O conditional on Y , this implies that she can infer from her model P N (·|Y ) aconditional model P O ∪I (·| X I , Y ) on the set L (XO ∪I × Y ) given byP O ∪I (h|xI , y) := P N(cid:7)h(·, xI , y)(cid:11)(cid:11) y(cid:8)for all h ∈ L (XO ∪I × Y ) and all (xI , y) ∈ XI × Y .1924G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950We can now extend the entire discussion in this and the next section to the conditional case. We will, however, refrainfrom doing so explicitly, or in any detail. Rather, we present a result that simply reduces problems of conditional epistemicirrelevance and independence, as formulated above, to a collection of problems of epistemic irrelevance and independence.This will allow us to immediately and automatically extend all the results in this and the next section from the case ofindependence to that of independence conditional on the additional variable Y .With any conditional lower prevision P O ∪I (·| X I , Y ) on L (XO ∪I × Y ), we associate a collection of (separately coherent)conditional lower previsions QQyO ∪I ( f |xI ) := P O ∪I ( f |xI , y) = P O ∪I(cid:11)(cid:11)xI , yf (·, xI )yO ∪I (·| X I ) on L (XO ∪I ), one for each y in Y , defined by(cid:8)(cid:7)for all f ∈ L (XO ∪I ) and xI ∈ XI .(10)We can reduce the problem of checking the coherence of a collection of conditional lower previsions P O k∪Ik (·| X Ik , Y ),k = 1, . . . , m, to a number |Y | of coherence problems that are simpler, in the sense that the conditioning variable Ydisappears from them; we have, for each y ∈ Y , to check the coherence of the collection Q(·| X Ik ), k = 1, . . . , m.yO k∪IkTheorem 11 (Elimination of common conditioning variables). Consider m arbitrary but different pairs of disjoint subsets O k and Ik of× Y ), k = 1, . . . , m, andN, k = 1, . . . , m. Consider separately coherent conditional lower previsions P O k∪Ik (·| X Ik , Y ) on L (XO k∪Ikfor each y ∈ Y , the corresponding separately coherent Q(·| X Ik ) on L (XO k∪Ik ), k = 1, . . . , m. Then the following statementsare equivalent:yO k∪Ik(i) The collection P O k∪Ik (·| X Ik , Y ), k = 1, . . . , m, is coherent;(ii) For each y in Y , the collection Q(·| X Ik ), k = 1, . . . , m, is coherent.yO k∪Ik5. Independent natural extensionA number of examples in Appendix B show that, when we leave linear for lower previsions, many-to-one and many-to-many independent products are generally speaking not unique.What we want to do in this section, then, is to show that any collection of coherent marginals always has a pointwisesmallest many-to-one, and a pointwise smallest many-to-many, independent product, and that these products coincide.We begin by observing that there always is at least one many-to-many (and therefore also many-to-one) independentproduct:Proposition 12. Consider coherent lower previsions P n on L (Xn), n ∈ N. Then their strong product×n∈N P n is a many-to-manyand many-to-one independent product of the marginals P n.5.1. Definition of the many-to-one independent natural extensionAlthough the notion of epistemic many-to-many independence seems to be the more intuitively appealing and useful ofthe two, it turns out to be easier to approach the study with many-to-one independent products. So, for the time being, weconcentrate on the latter notion of independence, which is related to the collection of conditional lower previsions:N (P n, n ∈ N) =(cid:6)P {o}∪I (·| X I ): o ∈ N and I ⊆ N \ {o}(cid:9),where the conditional lower previsions are given by Eq. (9). Proposition 12 is instrumental in establishing the followingcrucial observation.Proposition 13. Consider arbitrary coherent lower previsions P n on L (Xn), n ∈ N. Then the collection N (P n, n ∈ N) of conditionallower previsions P {o}∪I (·| X I ) is coherent.As an immediate consequence, we see that checking whether a joint lower prevision is a many-to-one independentproduct is a fairly straightforward matter.Corollary 14. Consider a coherent lower prevision P N on L (XN ), and coherent lower previsions P n on L (Xn), n ∈ N. Then P N is amany-to-one independent product of the P n, n ∈ N, if and only if any (and hence all) of the following equivalent conditions is satisfied:(i) P N is weakly coherent with the collection N (P n, n ∈ N) of conditional lower previsions P {o}∪I (·| X I );(ii) P N (I{xI }[g − P o(g)]) = 0 for all o ∈ N, all I ⊆ N \ {o}, all gambles g on Xo and all xI ∈ XI .If we apply condition (ii) in this corollary to the special case where I is the empty set, we deduce immediately that any(many-to-one) independent product of a number of lower previsions must have these lower previsions as its marginals:G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501925Corollary 15. Consider arbitrary coherent lower previsions P n on L (Xn), n ∈ N. If the coherent lower prevision P N on L (XN ) is amany-to-one independent product of these lower previsions P n, then for all n ∈ N, P n is the Xn-marginal of P N : P N (g) = P n(g) forall gambles g on Xn.Because all the sets Xn are finite, we can invoke Walley’s Finite Extension Theorem [30, Theorem 8.1.9] to derive fromProposition 13 that there always is a pointwise smallest joint coherent lower prevision E N that is coherent with the coherentfamily N (P n, n ∈ N). This leads to the following definition.Definition 6 (Many-to-one independent natural extension). Consider arbitrary coherent lower previsions P n on L (Xn), n ∈ N.We call the pointwise smallest coherent lower prevision that is coherent with the family of conditional lower previsionsN (P n, n ∈ N) the many-to-one independent natural extension of the marginals P n, and we denote it byn∈N P n, or alterna-tively by E N when it is clear from the context what the marginals are.(cid:25)(cid:25)Alternatively, and equivalently, the many-to-one independent natural extension of the marginals P n is the pointwisesmallest many-to-one independent coherent lower prevision on L (XN ) whose marginals coincide with the given P n. Wegather from Corollary 14 thatn∈N P n is also the smallest coherent lower prevision that is weakly coherent with theconditional lower previsions in N (P n, n ∈ N).Since the strong product ×n∈N P n is a many-to-one independent product of the marginals P n, n ∈ N, by Proposition 12,n∈N P n. These products do notit has to dominate the many-to-one independent natural extensioncoincide in general: Walley [30, Section 9.3.4] discusses an example where the many-to-one independent natural extensionis not a lower envelope of independent linear products, and as a consequence cannot coincide with the strong product.n∈N P n: ×n∈N P n (cid:2)(cid:25)(cid:25)On the other hand, the strong product is not generally the greatest many-to-one independent product of given marginals,as we show in Example 3 in Appendix B.5.2. Immediate properties of the many-to-one independent natural extensionIt will pay to study this many-to-one independent natural extension in greater detail. We begin by deriving a workableexpression for it. By definition, E N is the smallest joint coherent lower prevision on L (XN ) that is coherent with the familyof conditional lower previsions N (P n, n ∈ N), so we can infer from Walley’s Finite Extension Theorem [30, Theorem 8.1.9]that E N is the natural extension of the coherent collection N (P n, n ∈ N) to an unconditional (joint) lower prevision. UsingEq. (5), we find that it is given by:(cid:17)(cid:26)E N ( f ) = supminf −(cid:3)o∈N, I⊆N\{o}(cid:27)G{o}∪I (g I,o| X I )(cid:19): g I,o ∈ L (X{o}∪I ), o ∈ N, I ⊆ N \ {o},for all gambles f on XN , where we letG{o}∪I (g I,o| X I ) :=(cid:3)(cid:12)(cid:13)g I,o − P {o}∪I (gi,o| X I )I{xI }(cid:3)=(cid:12)I{xI }xI ∈XIg I,o(·, xI ) − P o(cid:7)(cid:8)(cid:13)g I,o(·, xI ).Therefore we find that:xI ∈XIE N ( f ) =supg I,o∈L (X{o}∪I )o∈N, I⊆N\{o}minzN ∈XN(cid:26)f (zN ) −(cid:3)(cid:12)o∈N,I⊆N\{o}g I,o(zo, zI ) − P o(cid:7)(cid:8)(cid:13)g I,o(·, zI )(cid:27).(11)We first show that we can simplify this expression, by restricting the I in the supremum to their largest possible values.Theorem 16. Consider coherent lower previsions P n on L (Xn), n ∈ N. Then for all gambles f on XN :(cid:12)hn(zN ) − P n(cid:7)hn(·, zN\{n})E N ( f ) =f (zN ) −(cid:3)(cid:8)(cid:13)(cid:26)(cid:27).suphn∈L (XN )n∈NminzN ∈XNn∈N(12)The coherent lower prevision in Eq. (12) is actually the natural extension of the following coherent family of conditionallower previsions:Next(P n, n ∈ N) :=(cid:6)P {n}∪N\{n}(·| X N\{n}): n ∈ N(cid:9)⊆ N (P n, n ∈ N).Relying on this expression for the independent natural extension, it is fairly straightforward to show that it has the followingmonotonicity property.Proposition 17. Let P n and Q n be coherent lower previsions on L (Xn) such that P n (cid:3) Q n, n ∈ N. Then(cid:25)n∈N P n (cid:3)(cid:25)n∈N Q n.1926G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19505.3. Marginalising and conditioning the many-to-one independent natural extensionLet us consider the coherent lower previsions P n on L (Xn), n ∈ N. For any non-empty subset R of N, we denote ther∈R P r . This E R turns out to be theindependent natural extension of the marginal lower previsions P r , r ∈ R, by E R =XR -marginal of E N =(cid:25)(cid:25)n∈N P n.Theorem 18. Consider a non-empty subset R of N. Then E N ( f ) = E R ( f ) for all gambles f on XR .The argumentation leading to this marginalisation property also allows us to prove the following result.Proposition 19. E N is productive. Moreover, E N (I{xI }[g − E O (g)]) = 0 for all disjoint subsets I and O of N, all xI ∈ XI and allg ∈ L (XO ).This implies that the independent natural extension E N satisfies the law of large numbers of Theorem 4. So does, therefore,any (many-to-one or many-to-many) independent product of these marginals, as it must dominate E N , even though, as we showin Appendix B, not all such independent products are productive!Let us now define, for any disjoint subsets I and O of N, the conditional lower previsions E O ∪I (·| X I ) on the setL (XO ∪I ) as follows:E O ∪I (h|xI ) := E N(cid:7)(cid:8)h(·, xI )(cid:7)(cid:8)h(·, xI )= E Ofor all h ∈ L (XO ∪I ) and xI ∈ XI ,(13)where the last equality follows from Theorem 18. This allows us to infer from the many-to-one independent natural exten-sion E N a family of conditional models(cid:6)I (E N ) :=E O ∪I (·| X I ): I and O disjoint subsets of N(cid:9),Interestingly, E N is coherent with the family I (E N ), and therefore:Theorem 20. E N is a many-to-many independent product of the coherent lower previsions P n, n ∈ N.Since any many-to-many independent product is in particular also a many-to-one independent product, we are led tothe following immediate conclusion:Theorem 21 (Independent natural extension). The many-to-one independent natural extension E N =n∈N P n of the coherent lowerprevisions P n, n ∈ N, is also their pointwise smallest many-to-many independent product. We can therefore simply call it the indepen-dent natural extension of the marginals P n.(cid:25)The independent natural extension is not only a many-to-one and many-to-many independent product of its marginals:it is also a factorising lower prevision.Theorem 22. Consider coherent lower previsions P n on L (Xn), n ∈ N. Then their independent natural extensiontorising.(cid:25)n∈N P n is fac-To show that it is also strongly factorising, it seems easiest to first show that it has an associativity property that issimilar to the one discussed in Propositions 8 and 9.5.4. Associativity of the independent natural extensionAssume that N is the union of two disjoint sets N1 and N2. Then it is natural to ask whether taking the independentnatural extension is an associative operation, i.e., whether(cid:28)(cid:18) (cid:28)(cid:20)P n⊗P n =(cid:18) (cid:28)(cid:20)P n?n∈N1∪N2n∈N1n∈N2:=(cid:25)Let us look at this formulation from a slightly different angle. We can consider the tuple X N1 as a variable assuming valuesP n as the corresponding ‘marginal’ lower prevision on L (XN1 ). Similarly, we canin the set XN1 , and E N1n∈N1consider the tuple X N2 as a variable assuming values in XN2 , and E N2P n as the corresponding ‘marginal’ lowern∈N2prevision on L (XN2 ). We now consider the joint variable X{N1,N2} assuming values in X{N1,N2}, and the independentnatural extension E{N1,N2} := E N1P n) of these two ‘marginals’. Since the variable X N1∪N2 isn∈N2(essentially) the same as the variable X{N1,N2}, the natural question to ask is, whether E N1∪N2= E{N1,N2}?P n) ⊗ (⊗ E N2n∈N1= ((cid:25)(cid:25)(cid:25):=G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501927Theorem 23. Consider arbitrary coherent lower previsions P n on L (Xn), n ∈ N. Consider a partition N1 and N2 of N, then(cid:25)(cid:25)(cid:25)n∈N1∪N2P n = (P n) ⊗ (n∈N1n∈N2P n).One important and fairly immediate consequence of this associativity is that it allows us to derive from the factorisingcharacter of the independent natural extension that it is also strongly factorising:Theorem 24. Consider coherent lower previsions P n on L (Xn), n ∈ N. Then their independent natural extensionfactorising.(cid:25)n∈N P n is strongly5.5. Interesting special casesWhen some of the marginals are linear or vacuous, the expression for the independent natural extension in Eq. (12)simplifies to a great extent. Because of the associativity result in Theorem 23, it suffices to consider the case of two variablesX1 and X2, so we let N = {1, 2}.When one of the marginals is linear, all independent products coincide:Proposition 25. Let P 1 be any linear prevision on L (X1), and let P 2 be any coherent lower prevision on L (X2). Let P {1,2} be anyindependent product of P 1 and P 2. Then for all gambles f on X1 × X2:P {1,2}( f ) = (P 1 × P 2)( f ) = (P 1 ⊗ P 2)( f ) = P 2(cid:7)(cid:8),P 1( f )where P 1( f ) is the gamble on X2 defined by P 1( f )(x2) := P 1( f (·, x2)) for all x2 ∈ X2.On the other hand, when one of the marginals is vacuous, then the strong product and the independent natural extensionare also guaranteed to coincide:Proposition 26. Let P A1coherent lower prevision on L (X2). For all gambles f on X1 × X2:1 be the vacuous lower prevision on L (X1) relative to the non-empty set A1 ⊆ X1, and let P 2 be any(i) (P A11(ii) If P is a factorising coherent lower prevision with these marginals, then⊗ P 2)( f ) = minx1∈ A1 P 2( f (x1, ·)).× P 2)( f ) = (P A11(cid:7)P ( f ) =P A11× P 2(cid:7)(cid:8)( f ) =(cid:8)P A11⊗ P 2( f ) = minx1∈ A1(cid:7)(cid:8)f (x1, ·).P 2(iii) If P 2 = P A22 is the vacuous lower prevision on L (X2) relative to the non-empty set A2 ⊆ X2, then any coherent lower previsionwith these marginals P is an independent product of P A11 and P A22 .In contrast with what we might come to expect from Proposition 25, when one of the marginals is vacuous, we cannotguarantee that all independent products coincide: see Example 3 in Appendix B. This implies that the second statement ofProposition 26 cannot be extended from factorising coherent lower previsions to independent products.146. External additivityWe can now bring what we have learnt about the independent natural extension to bear on our discussion of themore formalist approaches to independence. In the following section, we investigate the connections between epistemicindependence and factorisation. Here, we discuss weakened versions for lower previsions of the additivity property that alllinear previsions have. Vicig [29] discusses a related but weaker notion of external n-monotonicity for the case of coherentlower probabilities.Definition 7 (External additivity). Consider a coherent lower prevision P N on L (XN ). We call this lower prevision:(i) externally additive if for all non-empty R ⊆ N and all gambles fr on Xr , r ∈ R, P N ((ii) strongly externally additive if P N ( f + g) = P N ( f ) + P N (g) for allf ∈ L (XI ) and g ∈ L (XO ), where I and O are any(cid:5)(cid:5)r∈R fr) =r∈R P N ( fr);disjoint subsets15 of N.14 It also explains why the lower prevision in Example 3 of Appendix B cannot be factorising.15 The coherence of P N guarantees that for empty I or O the corresponding condition is trivially satisfied.1928G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950Clearly, strong external additivity implies external additivity. The latter is called summation independence by Cozman [3],who also gives, for the case N = {1, 2}, a proof for the external additivity of the strong product [3, Theorem 1]. We generalisehis result by proving that both the strong product and the epistemic natural extension are generally (strongly) externallyadditive.Proposition 27. Consider arbitrary coherent lower previsions P n, n ∈ N. Then both their strong product S N and their independentnatural extension E N are strongly externally additive, and therefore also externally additive.It follows from the definition that any convex combination of (strongly) externally additive coherent lower previsions isagain (strongly) externally additive. In fact, looking at the proof of this result in Appendix A, we see that any many-to-oneindependent product of the given marginals that is dominated by the strong product is also externally additive. To see that this doesnot extend to all many-to-one independent products, consider Example 3 in Appendix B.On the other hand, Example 4 in the same appendix shows that the properties of external additivity and strong externaladditivity are not equivalent. It also shows that not all many-to-one independent products between the independent naturalextension and the strong product are strongly externally additive.7. Factorisation and independenceSince we know from Proposition 8(iv) that the strong product is factorising, we wonder if we can use factorising lowerprevisions as many-to-one independent products. That this is indeed the case is proved in the following theorem:Theorem 28. Consider an arbitrary coherent lower prevision P N on L (XN ). If it is factorising, then it is a many-to-one independentproduct of its marginals P n, n ∈ N.It is therefore natural to ask whether, by extension, all strongly factorising lower previsions are many-to-many indepen-dent. While we have not been able to answer this question in its full generality, we are able to show that this is indeed thecase under fairly weak additional positivity conditions.Recall that for a factorising (or indeed any) coherent lower prevision P N on L (XN ) to be an independent product,it must be strongly coherent with the family I (P N ) of conditional lower previsions. It turns out that at least the weakcoherence is never an issue.Theorem 29. Consider a coherent lower prevision P N on L (XN ). If it is strongly factorising, then it is weakly coherent with thefamily I (P N ).Next, we turn to deriving a sufficient condition for a strongly factorising lower prevision P N on L (XN ) to be alsomany-to-many independent, so strongly coherent with the family I (P N ). For any non-empty subset I of N, we see at oncethatP N(cid:30)=Ai(cid:29)×i∈I(cid:22)i∈IP N ( Ai) and P N(cid:29)×i∈I(cid:30)=Ai(cid:22)i∈IP N ( Ai),where Ai ⊆ Xi for all i ∈ I . Now suppose we want to condition P N on an observation X I = xI , where I is some subset of N.To this end, we calculate the regular extension, as discussed in Section 2.6:R(h|xI ) := max(cid:6)μ ∈ R: P N(cid:7)(cid:8)I{xI }[h − μ](cid:9),(cid:2) 0where h is any gamble on XO and O is any subset of N \ I . If P N is strongly factorising, we see that(cid:7)(cid:8)I{xI }[h − μ]P N= P N(cid:17)=(cid:7)(cid:8)I{xI } P N (h − μ)P N ({xI })(P N (h) − μ)P N ({xI })(P N (h) − μ)= P N(cid:7)(cid:7)P N (h) − μI{xI }if P N (h) (cid:2) μ,if P N (h) (cid:3) μ,(cid:8)(cid:8)so we conclude thatR(h|xI ) = P N (h) as soon asP N(cid:8)(cid:7){xI }> 0.This means that, heuristically speaking and under some positivity assumptions, the conditional lower previsions that arefound by conditioning a strongly factorising joint lower prevision using regular extension, reflect the irrelevance conditionsthat are involved in the definition of many-to-many independence. Taking into account that, from Theorem 3, the conditionallower previsions derived by regular extension are strongly coherent, we deduce the following:G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501929Theorem 30. Let P N be a strongly factorising coherent lower prevision. If P N ({xI }) > 0 for every {xI } ∈ XI , then P N is many-to-manyindependent.It is an open problem at this point whether this positivity condition is really necessary.Since the independent natural extension is the pointwise smallest many-to-one independent product of given marginals,and since we have shown in Proposition 8(iv) that the strong product is in particular Kuznetsov, we deduce that thesmallest many-to-one independent product that is still Kuznetsov lies between the independent natural extension and thestrong product. For the case N = {1, 2}, this was also established by Cozman [3].Concerning the other conditions introduced in Section 3, we point out the following:Proposition 31. Consider arbitrary coherent lower previsions P n on L (Xn), n ∈ N, and let Q 1 and Q 2 be coherent lower previsionson L (XN ) with these marginals P n. Let Q 3 be any coherent lower prevision on L (XN ) such that Q 1 (cid:3) Q 3 (cid:3) Q 2. Then thefollowing statements hold:(i) If Q 1 and Q 2 are many-to-one independent products, then so is Q 3;(ii) If Q 1 and Q 2 are factorising, then so is Q 3;(iii) If Q 1 and Q 2 are Kuznetsov, then so is Q 3;(iv) If Q 2 is externally additive, then so is Q 3.We deduce that a convex combination of many-to-one independent products of the same given marginals is again amany-to-one independent product of these marginals. A similar result holds for factorising or Kuznetsov lower previsions.It is also interesting to investigate the connections between independent products and the notion of productivity.Proposition 32. Consider a coherent lower prevision P N on L (XN ). Then the following statements are equivalent:(i) P N is weakly coherent with I (P N );(ii) P N is productive.In addition, any many-to-many independent lower prevision is productive, and any productive coherent lower prevision is many-to-one independent. Moreover, if P N ({xI }) > 0 for every xI ∈ XI and every subset I of N, then P N is many-to-many independent if andonly if it is productive.We conclude that productivity, which is sufficient for the law of large numbers in Theorem 4 to hold, is intermediatebetween being a many-to-one and being a many-to-many independent product, and is equivalent to many-to-many inde-pendence when all the conditioning events have positive lower probability. Example 6 in Appendix B shows that not allmany-to-one independent lower previsions are productive, and that many-to-one and many-to-many independence are notequivalent even if the conditioning events all have positive lower probability.8. An application: probabilistic inference in imprecise Markov treesIndependence is at the very heart of much research done in Artificial Intelligence. In this section we show how theindependent natural extension affects a very traditional domain of AI: probabilistic graphical models. We do so by reviewingand discussing a model and algorithm recently introduced elsewhere by some of us [11]. The coherence results that are atthe core of that algorithm rely quite heavily on the properties of the independent natural extension proved in this paper,16such as its being the smallest many-to-many independent product, its marginalisation and associativity properties, and itsbeing strongly factorising.8.1. Background notions and notationAs is widely known, a graphical model consists of a graph enriched with specific probabilistic information. One suchmodel is a Bayesian network [27]. Here a directed graph represents variables through nodes, and independence statementsbetween them by arcs. Conditional probabilities are associated with the nodes of the graph.Let us make this more precise by introducing some notation. We tailor the notation to the case of tree topologies, whichare the focus of our attention: every node of the graph has exactly one parent, with the exception of one node, called root,which has no parents.We call T the set of the nodes s of the tree, and we denote the root node by (cid:5). For any node s, we denote its mothernode by m(s). The root (cid:5) has no mother node, and we use the convention m((cid:5)) = ∅. For each node s, we denote the set16 The paper was written jointly with the present one, but published earlier due to circumstance.1930G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950of its children by C (s). If C (s) = ∅, then we call s a leaf, or terminal node. Moreover, D(s) denotes the set of descendantsof s (its successors in the graph). We also use the notation ↓s := D(s) ∪ {s} for the subtree with root s. Similarly, welet ↓S :={↓s: s ∈ S} for any subset S ⊆ T . For any node s, its set of non-parent non-descendants is then given bys := T \ ({m(s)} ∪ ↓s).(cid:10)With each node s of the tree, there is associated a variable Xs. We adopt the usual notation and assumptions for thevariables in the tree. In particular, variable Xs takes values in the corresponding non-empty finite set Xs, and we assumeall variables to be logically independent.At this point we can define the probabilistic information in a Bayesian net more precisely: a generic node s is equippedwith a mass function for Xs conditional on Xm(s) = zm(s), for all zm(s) ∈ Xm(s). We can rephrase this in the language ofprevisions by saying that each node s has a (separately coherent) conditional linear prevision Q s(·| Xm(s)) on L (Xs): foreach possible value zm(s) of the variable Xm(s) associated with its mother node m(s), we have a linear prevision Q s(·|zm(s))for the value of Xs, conditional on Xm(s) = zm(s).17 We call Q s(·| Xm(s)) a local uncertainty model.The tree together with the local uncertainty models provides a compact way to define a joint probability mass functionover X T . This follows from the Markov condition, which is also what provides the graphical model with a probabilistic seman-tics: conditional on its mother variable Xm(s), variable Xs is assumed to be independent of its non-parent non-descendantvariables Xs. Again, in terms of lower previsions, this means that the graphical model is an equivalent representation of aglobal uncertainty model P T , that is, a linear prevision defined on L (XT ).The global uncertainty model is then used for inference, which most often amounts to computing posterior beliefs (i.e.,probabilities or expectations) for a query variable Xs conditional on X E = xE , where E is a non-empty subset of T whosevariables are in the known state xE . This task is called updating. Updating is performed by applying Bayes’s rule to theglobal uncertainty model while exploiting the structure of the graph in order to perform the computation as efficiently aspossible. In fact, the computation of updating on a tree-shaped Bayesian network is an easy task, which is solved exactly intime linear in the size of the tree.8.2. Epistemic treesBayesian networks have been extended to deal with imprecisely specified probabilities. The resulting models are calledcredal networks [4]. The extension is achieved primarily by replacing the local uncertainty models of Bayesian networks withimprecise ones: in the most common case, this means that each mass function required to specify a Bayesian net is replacedby a closed convex set of mass functions. In the language of previsions, in a credal network each node s is equipped with a(·| Xm(s)) on L (Xs): for each possible value zm(s) of the variable Xm(s)(separately coherent) conditional lower prevision Q(·|zm(s)) for the value of Xs, conditional onassociated with its mother node m(s), we have a coherent lower prevision QXm(s) = zm(s).ssAs in Bayesian networks, these local uncertainty models need to be combined into a global uncertainty model that islater used for (imprecise-probabilistic) inference. The Markov condition still plays the leading role in this process, but has tobe modified to take into account the specific notion of independence (or irrelevance) that the graph is assumed to represent.The traditional approach in the literature focuses on strong independence: in this case, conditional on its mother variableXm(s), a variable Xsis assumed to be strongly independent of its non-parent non-descendant variables Xs. The globaluncertainty model P T obtained through this so-called strong Markov condition, is called the strong extension. The strongextension comes with a sensitivity analysis interpretation: in fact, each of its extreme points can be regarded as arisingfrom a Bayesian network with the same graph as the credal net, and whose local uncertainty models dominate the localuncertainty models of the credal network. In other words, a credal network under strong independence can be regarded asthe set of all the Bayesian networks that are compatible with the probabilities that have been imprecisely specified in thedesign of the credal net.But the sensitivity analysis interpretation of an imprecise probability model is not always applicable, as we have dis-cussed in the Introduction with reference to modelling expert knowledge. In this case an assumption of epistemic irrelevancemay allow one to represent more faithfully an expert’s beliefs.18 This point is especially important because modelling expertknowledge is among the main motivations for using credal networks (as well as Bayesian nets).This lead has been followed in Ref. [11] by looking at the following type of Markov condition based on epistemicirrelevance:CI. Consider any node s in the tree T , any subset S of its set of children C (s), and the set S :=c∈S c of their commonnon-parent non-descendants. Then conditional on the variable Xs, the non-parent non-descendant variables X S are assumedto be epistemically irrelevant to the variables X↓S associated with the children in S and their descendants.(cid:31)This condition turns T into a credal tree under epistemic irrelevance, which we call imprecise Markov tree.17 In the root, this corresponds to having an unconditional local uncertainty model Q (cid:2) for X(cid:2): a linear prevision on L (X(cid:2)).18 Obviously, there may be cases where strong independence is justified in order to model an expert’s knowledge. Moreover, strong independence couldprovide a good approximation to more accurate models, even when it is not entirely appropriate.G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501931Before we proceed, let us briefly address a technical question: the form of CI might look unusual when compared tothe common statement of the Markov condition. In fact, CI seems to impose a wider set of irrelevances, focusing as it doeson sets of children of s, and on the related subtrees, rather than on a single child. But this difference is only apparent,because the strong Markov condition, as well as the precise-probabilistic Markov condition, can be reformulated equivalentlyin a completely similar way: the apparent additional irrelevances (or independencies) are actually implied by those in thestandard Markov condition [use d-separation to see that s blocks all the relevant paths]. Whether or not this is the casewhen we use epistemic irrelevance is not obvious to us at present; this is due to epistemic irrelevance being a relativelyweak notion for imprecise probability models. For this reason, CI is formulated by imposing all the additional epistemicirrelevances explicitly.We now shed more light on two immediate consequences of CI.First, consider some non-terminal node s different from (cid:5), and its mother variable Xm(s). We infer from CI that thismother variable Xm(s) is epistemically irrelevant to the variable X↓C (s) conditional on Xs:It is worth stressing that such is not necessarily the case when we reason in the opposite way: CI does not imply that X↓C (s)is epistemically irrelevant to Xm(s) in case we observe Xs. This kind of symmetrised irrelevance (that is, independence)can be imposed too, but its treatment is much more involved and problematical from the algorithmic side, because itcomplicates the construction of the global model tremendously. In addition, we would argue that the irrelevances imposedby CI are more natural than their symmetrised counterparts for directed graphical models. We will come back to this pointin Section 8.4.Next, consider some node s. Then CI tells us that for any two children c1, c2 ∈ C (s) of s, the variable X↓c1 is epistemicallyirrelevant to the variable X↓c2 , conditional on Xs.It even tells us that for any two disjoint non-empty sets S1 ⊆ C (s) and S2 ⊆ C (s) of children of s, the variable X↓S1 isepistemically irrelevant to X↓S2 , conditional on Xs. In contradistinction with the previous example, here we see that thesymmetrised irrelevance conditions (here between children) originate spontaneously from CI: we conclude that, conditionalon a node, its children c (or rather, the variables associated with their subtrees ↓c) are epistemically many-to-many indepen-dent.This is the specific point where our present work on the independent natural extension meets credal trees. If we wantto obtain the most conservative global model that arises through coherence from the local models and the statements ofconditional irrelevance implied by CI, then we need to compute the independent natural extension in order to summarisethe information carried by the variables associated with the subtrees ↓c, with c ∈ C (s).8.3. Constructing the most conservative global modelLet us illustrate how to construct the most conservative global model for the variables in the tree that extends the localmodels and expresses all conditional irrelevancies encoded by the imprecise Markov tree through CI. Consider the followingfragment of the tree.Here we denote by P ↓ck (·| Xs) the lower prevision on L (X↓ck ) that is the most conservative global model for X↓ck con-structed from CI and the local models in the subtree with root ck. This is a conditional global model as it depends on the valueof Xs. For the time being, we assume that such conditional global models related to the children of s exist and have al-ready been computed. Since we know from the foregoing discussion that the X↓c1 , . . . , X↓cn are many-to-many independent1932G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950c∈C (s) P ↓c(·| Xs), which is a conditional lowerconditional on Xs, we can compute their independent natural extensionprevision P ↓C (s)(·| Xs) on L (X↓C (s)). We can reorganise the graph accordingly by clustering all the children into a singlenode.(cid:25)At this point, the local model Qglobal conditional model about X↓s. This is achieved by taking their marginal extension19s(·| Xm(s)) must be combined with P ↓C (s)(·| Xs) into a least-committal (pointwise smallest)(cid:7)QsP ↓C (s)(·| X{m(s),s})| Xm(s)(cid:8)= Q(cid:7)sP ↓C (s)(·| Xs)| Xm(s)(cid:8);see Refs. [23] and [30, Section 6.7.2] for more details. Graphically:It is clear that this process can be iterated by starting from the leaves of the tree and letting P ↓t(·| Xm(t)) := Q(·| Xm(t)) forall leaves t, and working our way recursively up to the root. When we eventually get to the root, this process yields a globalmodel P ↓(cid:3)(·| Xm((cid:3))) =: P T .tA central theorem in Ref. [11] then guarantees that, under mild positivity conditions on the local upper previsions, P T isthe most conservative (pointwise smallest) joint lower prevision that coherently extends the local models and that expressesall the conditional irrelevance statements implied by CI. This remarkable result relies quite heavily on the independentnatural extension and its properties, as introduced and studied in this paper. The particular properties that are crucial inestablishing it are: that it is the smallest many-to-many independent product (Theorem 21), that it is strongly factorising(Theorem 24), that it satisfies some marginalisation and associativity properties (Theorems 18 and 23), and that all of thiscan be extended to the conditional setup (Theorem 11).These results have important implications for imprecise-probabilistic graphical models under epistemic irrelevance. Theyshow that epistemic irrelevance can be used in the context of at least some graphical models in much the same way asstochastic independence or strong independence. In fact, not only does the work in Ref. [11] show by construction thatthe ‘epistemic extension’ P T generally exists, but it goes further by using it to derive an efficient algorithm for updating inimprecise Markov trees—another name for credal trees. Similarly to the traditional algorithms for Bayesian nets, it works ina distributed fashion by passing (imprecise-)probabilistic messages along the nodes of the tree until it converges and thusyields the (exact) updated lower previsions of interest. Moreover, it works in time linear in the size of the tree, as is thecase with the more traditional algorithms for precise probabilities.8.4. Some remarksThe results obtained with imprecise Markov trees are particularly interesting because it had been uncertain until quite re-cently whether or not epistemic irrelevance could be used to design efficient algorithms in graphical models. This is relatedto epistemic irrelevance being not as well-behaved as other independence notions with respect to the graphoid axioms; seein particular Ref. [6], but also Ref. [26] for a more positive view. Part of the interest in these results is due to complexityreasons. Computation in credal nets based on strong independence is substantially harder than the case of Bayesian nets: itis an NP-hard task even on polytrees20 [9]. What complexity updating credal trees under strong independence has, is stillan open problem, but preliminary analyses made by Cassio P. de Campos, based on Ref. [8, Theorem 7], indicate that thistask should be NP-hard too. If this were to be confirmed, we should have a clear example where epistemic irrelevance leadsto simpler models of computation than strong independence.On the other hand, we should take into account that these positive results have been obtained in the case of treetopologies. The expressivity of trees should not be overlooked because, for example, updating problems in Bayesian networkscan be solved through the well-known join tree structure (this is an undirected tree, but it can be as well representedin a directed way with minor changes).21 And yet, it does not seem likely that something similar can be done under19 Marginal extension is, in the special case of precise probability models, also known as the law of total probability, or the law or iterated expectations.20 These are directed graphs that become (undirected) trees after dropping the orientations of the arcs.21 This was observed by Pearl already at the time of the original proposal by Lauritzen and Spiegelhalter, see Ref. [20, p. 211].G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501933epistemic irrelevance. The crucial point here is that when we convert a polytree into a (directed) join tree, we shall beobliged to invert the direction of some arcs. This is easy to see by considering a V-shaped graph made of two chainsthat converge into a node. The problem is that the epistemic irrelevances coded by the original V-shaped polytree willnot imply in general those related to the inverted arcs in the directed tree, simply because epistemic irrelevance is anasymmetric notion. Therefore, imprecise Markov trees do not seem suited to be exploited as tools for solving updatingproblems in more general credal networks under epistemic irrelevance. This situation could perhaps change if conditionCI were reformulated to code symmetric irrelevances (that is, epistemic independencies): in this case inverting the arcsshould cause no problems. However, as we already suggested above, the development of efficient algorithms for trees undersuch a modified (strengthened) Markov condition appears to be quite problematic, because the condition complicates thecomputation of the joint model. All these considerations lead us to conjecture that the extension of the existing results fromtrees to polytrees might require a substantial increase in computational complexity.Still, there is another direction, relying on tree topologies, that appears to be particularly promising for addressing in-teresting problems under epistemic irrelevance. It is based on the observation that discrete-time sequential processes arevery often represented by hidden Markov models [28], which are special trees in the language of graphical models. HiddenMarkov models have a number of applications, often related to some kind of recognition: speech, gesture, or word recog-nition. Technically, this is achieved by computing sequences of hidden variables (states) from the observation of sequencesof other, manifest, variables (outputs). In fact, in order to turn credal trees into workable imprecise hidden Markov models,the additional complication that needs to be faced is that of querying the tree for the joint value of the hidden variables,rather than for a single one. Recent work [7] has shown that, fortunately, such a task can be solved again exactly and witha complexity that is essentially the same as that required for precise-probabilistic hidden Markov models. Once again, thekey to this result is the use of the independent natural extension and its properties, as developed here.9. ConclusionsWe have worked out the foundations of epistemic independence, a generalisation of stochastic independence to impreciseprobabilities. This has led to a definition of independent products, and in particular to the especially interesting and usefulnotion of independent natural extension. Like the strong product, or any other independent product, for that matter, it capturesthe idea of mutual irrelevance between sources of information. But it is the most conservative independent product to doso, which indicates that it is the only one that is based solely on this mutual irrelevance (and coherence, of course).22 Wesee that, because it encompasses all these types of independent product, the notion of epistemic independence has verywide scope.We have carried out our study by focusing on variables assuming values in finite spaces, and have followed two dif-ferent routes. On the one hand, we have considered generalisations to imprecise probabilities of the factorisation formula:productivity, (strong) factorisation, being (strongly) Kuznetsov. On the other, we have defined many-to-one and many-to-many independent products of marginals, or, possibly, conditionals, based on a behavioural notion of symmetrised epistemicirrelevance, or in other words, epistemic independence. We have shown that the two routes are tightly interwoven, as fac-torisation implies many-to-one independence and strong factorisation implies many-to-many independence (under weakpositivity assumptions). The most important notion of this paper, the independent natural extension, has been shown tobe the smallest many-to-many (and many-to-one) independent product. It also satisfies useful basic properties related tomarginalisation, associativity, and external additivity.What the independent natural extension embodies, in other words, is a way to coherently extend marginal impreciseprobability models into a joint model using symmetrised epistemic irrelevance judgements alone. As far as practical appli-cations are concerned, this task is simplified by our next important result: under mild conditions, using the independentnatural extension is equivalent to imposing strong factorisation when looking for least-committal models. In fact, this is thecrucial result that has allowed the independent natural extension to be used successfully in graphical models in Ref. [11],as we have discussed above.All these results appear here, at this level of generality, for the first time. It is natural to wonder what they mighteventually lead us to.As far as applications are concerned, we see much scope for epistemic independence. As already indicated, the domain ofgraphical models is particularly worth of consideration in this respect. Future research could, as mentioned in Section 8.4,try to extend the work in Ref. [11] to more general graphs, such as polytrees. The recent development of an efficientalgorithm for the exact computation of state sequences in imprecise hidden Markov models [7] (which are special impreciseMarkov trees) should favour the emergence of new interesting applications exploiting epistemic independence. Anotherpotential domain of application could be probabilistic logic, and in particular the extensions that have been proposed toembed independence [5]. This would lead to approaches to probabilistic logic allowing both for imprecise probabilities andepistemic independence judgements.As regards more technical questions arising from this paper, we summarise the main problems that remain open at thisstage.22 The ‘extra information’ entering the strong product seems to be the underlying assumption of an ideal precise model. In order to make the strongproduct the smallest coherent independent product, the notion of coherence would have to be strengthened.1934G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950Primo, in Proposition 6, we have established relationships—in fact, implications—between the different factorisation con-ditions introduced in Section 3. In Appendix B, we give a number of counterexamples that show that none of the converseimplications hold in general, except for one of them: we still do not know whether the Kuznetsov and strong Kuznetsovconditions are generally equivalent.Secundo, in Section 4 we have introduced many-to-one and many-to-many independent products, and we have shown inAppendix B that the second of these notions is more restrictive. But let us look at the connections between these epistemicnotions and the more formal conditions introduced in Section 3. We have proved that a factorising coherent lower prevision(and as a consequence also one that is strongly factorising, Kuznetsov or strongly Kuznetsov) is many-to-one independent,although in Appendix B we can see that the converse is not true in general. We also show in Appendix B that not everymany-to-many independent product is factorising (and therefore it need not be Kuznetsov, strongly factorising or stronglyKuznetsov). But we do not know whether a strongly factorising coherent lower prevision is necessarily many-to-manyindependent.Tertio, we have established in Section 5 that the independent natural extension is the smallest many-to-one independentproduct of given marginals, and that it also is the smallest product that is factorising, strongly factorising, or many-to-many independent. We show in Appendix B that it is neither the smallest Kuznetsov nor the smallest strongly Kuznetsovproduct. Another example of this is given in Ref. [3], where it is shown that the least-committal Kuznetsov product of givenmarginals may be different from the strong product.Quarto, and related to this, we show in Example 3 that there are many-to-many independent products that dominate thestrong product, but our next conjecture, that the strong product could be the greatest factorising product of given marginals,still remains to be proved or disproved. If the conjecture were to hold, then the strong product would also be the greateststrongly factorising, Kuznetsov and strongly Kuznetsov product. Related to this, it might be argued that the coherent lowerprevision in Example 3 represents a somewhat pathological situation, and that it points to the fact that considering onlymany-to-many independence as our main requirement could be too weak. One possibility would be to restrict ourselvesto many-to-many independent products which satisfy one of the factorisation conditions we have introduced in this paper;this seems to be in accordance with the results in Section 5.5, which show that the coherent lower prevision in Example 3is not factorising, and that for those marginals the only factorising product is the intuitive one. Hence, we may focus forinstance on factorising coherent lower previsions, or on many-to-many independent products which are at the same timestrongly factorising.Finally, we do not know whether all factorising coherent lower previsions are externally additive, nor whether all stronglyfactorising coherent lower previsions are. These open problems could be related to our conjecture about the strong productbeing the greatest factorising lower prevision with given marginals.AcknowledgementsThis work has been supported by SBO project 060043 of the IWT-Vlaanderen, projects TIN2008-06796-C04-01, MTM2010-17844, by Swiss NSF grants nos. 200020_134759/1, 200020-121785/1, and by the Hasler foundation grant no. 10030. Wewould also like to thank the reviewers for their helpful comments.Appendix A. Proofs of resultsProof of Proposition 5. We first show that P N is factorising if and only if (i) holds. Since the direct implication is trivial, itfo ∈ L (Xo) and non-negative f i ∈ L (Xi) for i ∈ I , where I is any subsetsuffices to prove the converse. Consider o ∈ N,of N that does not include o. Let f j be the gamble with the constant value 1, for every j /∈ I ∪ {o}. We deduce from (i) that(cid:20)(cid:18)(cid:18)(cid:18)(cid:20)(cid:20)(cid:20)(cid:18)(cid:22)(cid:22)f i= P Nfof i= P NP N ( fo)f i= P NP N ( fo)i(cid:5)=oi(cid:5)=o(cid:22)P Nfoi∈Iso P N is factorising.(cid:22)f i,i∈I(cid:21)Next, we prove that (i) and (ii) are equivalent. Let, for notational simplicity, f R :=(i) ⇒ (ii). If P N ( fo) (cid:2) 0, then the coherence of P N tells us that P N (P N ( fo) f N\{o}) = P N ( fo)P N ( f N\{o}). Similarly, ifr∈R fr for any subset R of N.P N ( fo) (cid:3) 0, then(cid:7)P NP N ( fo) f N\{o}(cid:8)(cid:7)= −P N−P N ( fo) f N\{o}(cid:8)= P N ( fo)P N ( f N\{o}).(cid:21)It now suffices to establish the equalities P N ( f N\{o}) =by applying induction on the number of the elements in the product.i∈N\{o} P N ( f i) and P N ( f N\{o}) =(cid:21)(ii) ⇒ (i). By letting fo := 1, we infer from (ii) that P N ( f N\{o}) =(cid:21)P N ( f N\{o}) =P N ( fo) (cid:2) 0:i∈N\{o} P N ( f i). Going back to generali∈N\{o} P N ( f i), and by letting fo := −1, thatfo, we now derive from (ii) and the coherence of P N that, when(cid:21)i∈N\{o} P N ( f i). These follow easilyP N ( fo)P N ( f i) = P N ( fo)P N ( f N\{o}) = P N(cid:7)(cid:8)P N ( fo) f N\{o},(cid:22)i∈N\{o}G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501935and that when P N ( fo) (cid:3) 0:(cid:22)P N ( fo)i∈N\{o}P N ( f i) = P N ( fo)P N ( f N\{o}) = −P N(cid:7)−P N ( fo) f N\{o}(cid:8)(cid:7)= P N(cid:8)P N ( fo) f N\{o}.(cid:2)Proof of Proposition 6. First, assume that P N is strongly Kuznetsov. We show that P N is strongly factorising. Considerdisjoint subsets I and O of N, a gamble g on XO and a non-negative gamble f on XI . Since P N ( f ) (cid:2) P N ( f ) (cid:2) 0 andP N (g) (cid:2) P N (g), we infer that=P N ( f g) = P N ( f ) (cid:4) P N (g)(cid:12)(cid:6)min⎧[P N ( f )P N (g), P N ( f )P N (g)]⎨[P N ( f )P N (g), P N ( f )P N (g)][P N ( f )P N (g), P N ( f )P N (g)](cid:9), maxP N ( f )P N (g), P N ( f )P N (g)if P N (g) (cid:2) 0,if P N (g) (cid:3) 0 (cid:3) P N (g),if P N (g) (cid:3) 0,⎩=(cid:6)P N ( f )P N (g), P N ( f )P N (g)(cid:9)(cid:13)and by considering the lower interval bounds, we see that P N is indeed strongly factorising.Next, assume that P N is strongly factorising. We show that P N is productive. Consider disjoint subsets I and O of N,a gamble g on XO and a non-negative gamble f on XI . Then it follows from the fact that P N is strongly factorising andcoherent that(cid:12)(cid:8)(cid:8)(cid:13)(cid:8)(cid:13)(cid:8)(cid:7)(cid:7)(cid:7)(cid:7)(cid:12)P Nfg − P N (g)= P Nf P Ng − P N (g)= P NfP N (g) − P N (g)= 0,so P N is indeed productive.The proofs of the remaining implications are either similar, or trivial. (cid:2)Proof of Proposition 7. The first part of the proposition is immediate. For the second, use the first to see that (c) and (d) areequivalent, and that (e) and (f) are equivalent. For the other equivalences, use the self-conjugacy and coherence of P N . (cid:2)Proof of Proposition 8. The first statement is an immediate consequence of Eq. (8) and Proposition 7(i).We turn to the proof of the second statement. We first show that the set ext(M (S N )) is included in {×n∈N Pn: (∀n ∈N) Pn ∈ ext(M (P n))}. The sets M (P n), n ∈ N, are closed [in the topology of pointwise convergence, or equivalently, inthe Euclidean topology, because we are working in finite-dimensional linear spaces]. So the Cartesian product×n∈NM (P n)is closed in the product topology. Since it is clear from Eq. (6) that taking a product of linear previsions is a continuousoperation with respect to these topologies, it follows that the set of linear previsions M := {×n∈N Pn: (∀n ∈ N) Pn ∈M (P n)} is closed. It follows from Walley’s weak* compactness theorem [30, Theorem 3.6.1] that since S N is the lowerenvelope of M , the convex compact set M (S N ) is equal to the closed convex hull of its subset M . It then follows fromthe extended form of the Krein–Milman Theorem in Ref. [18, p. 74] that ext(M (S N )) ⊆ M (because M is closed). Supposeex absurdo that some extreme point S N =×n∈N Q n of M (S N ) does not belong to {×n∈N Pn: (∀n ∈ N) Pn ∈ ext(M (P n))}.in M (P r)Then there must be some r ∈ N such that Q r is not an extreme point of M (P r), so there are different Q 1and α ∈ (0, 1) such that P r = α Q 1∈ M andr∈ M . Since M ⊆ M (S N ), this contradicts that S N is an extreme point of M (S N ). We deduce thatQ 2Nindeed ext(M (S N )) ⊆ {×n∈N Pn: (∀n ∈ N) Pn ∈ ext(M (P n))}.(∀n ∈ N) Pn ∈ ext(M (P n))} ⊆ ext(M (S N )). Consider arbitrary Pn ∈ext(M (P n)) for all n ∈ N. Then S N :=×n∈N Pn ∈ M (S N ) by Eq. (8). It follows from Minkowski’s Theorem (or Krein–Milman Theorem in finite dimensions) that S N is a convex combination of elements of ext(M (S N )): there are m (cid:2) 1,N . If m = 1non-negative real α1, . . . , αm such thatthen clearly S N ∈ ext(M (S N )), so we may assume without loss of generality that m > 1.:= (×n(cid:5)=r Pn) × Q 2On to the converse inequality {×n∈N Pn:N in ext(M (S N )) such that S N =r and Q 2:= (×n(cid:5)=r Pn) × Q 1rrr . But then S N = α Q 1k=1 αk = 1 and Q 1N , where Q 1+ (1 − α)Q 2+ (1 − α)Q 2N , . . . , Q mmk=1 αk Q k(cid:5)(cid:5)mNNr(cid:5)Consider any n ∈ N, then it follows by marginalisation that P n =mk=1 αk Q kn , where Q kk = 1, . . . , m. [That the Xn-marginal of S N is Pn follows from Proposition 7(i).] Since Q kNS N (g) = P n(g) for any g ∈ L (Xn), where the equality follows from (i). This implies that Q kn= Pn.since we assumed that Pn ∈ ext(M (P n)) and m > 1, we must have that Q 1n= · · · = Q mn∈ M (S N ) we find that Q kn is the Xn-marginal of Q kN ,N (g) (cid:2)∈ M (P n), k = 1, . . . , m. ButQ kNSince this holds for all n ∈ N, and since we already know from the argumentation above [ext(M (S N )) ⊆ M ] that=×n∈N Q kThe third statement is an immediate consequence of (ii) and Proposition 7(ii).Let us finally prove the fourth statement. Consider arbitrary disjoint proper subsets I and O of N, gambles f ∈ L (XI )= S N and therefore S N ∈ ext(M (S N )).n for k = 1, . . . , m, we see that Q 1= · · · = Q mNNand g ∈ L (XO ). It follows from the third statement, Eqs. (7) and (8), Proposition 7 and conjugacy that(cid:9)(cid:6)P I ( f )P O (g): P I ∈ M (S I ) and P O ∈ M (S O )S N ( f g) = min,(cid:9)(cid:6)P I ( f )P O (g): P I ∈ M (S I ) and P O ∈ M (S O )S N ( f g) = max.1936G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950This clearly implies thatS N ( f g) = minS N ( f g) = max(cid:9)(cid:6)ab: a ∈ S I ( f ) and b ∈ S O (g),(cid:9)(cid:6)ab: a ∈ S I ( f ) and b ∈ S O (g),or in other words S N ( f g) = S I ( f ) (cid:4) S O (g). Now use the first statement and conjugacy to find that S I ( f ) = S N ( f ) andS O (g) = S N (g).The rest of the proof now follows from Proposition 6. (cid:2)Proof of Proposition 9. The proof of (i) is an easy consequence of the coherence condition: if P N is coherent with the familyof conditional lower previsions N (P n, n ∈ N), then its restriction to L (XR ) is coherent with the subfamily N (P r, r ∈ R)containing restrictions of certain elements of N (P n, n ∈ N).The argumentation for (ii) is similar.For (iii), we have to prove that Q N , Q N (·| X R ) and Q N (·| X S ) are coherent, where for instanceQ N ( f |xR ) := Q N(cid:7)(cid:8)f (·, xR )for all xR ∈ XR and f ∈ L (XN ).Since both Q N (·| X R ) and Q N (·| X S ) belong to I (Q N ), this follows from the coherence of Q N with I (Q N ). (cid:2)Proof of Proposition 10. We first show that S N is a many-to-many independent product of its marginals. It will thenautomatically follow that S N is a many-to-one independent product of its marginals as well. This will establish that arbitrarylinear previsions Pn on L (Xn), n ∈ N, always have many-to-many and many-to-one independent products.To show that S N is a many-to-many independent product of its marginals, we use Theorem 2. This means that we needto prove that (a) the family of conditional lower previsions I (S N ) is coherent; and (b) that the joint model S N is weaklycoherent with the family I (S N ).We begin with (a). Because we are dealing with linear previsions, coherence is equivalent to the condition in Eq. (2).Assume ex absurdo that there are f O ,I ∈ L (XO ∪I ) for all disjoint subsets I and O of N, and δ > 0 such that(cid:3)G O ∪I ( f O ,I | X I ) (cid:3) −δI A(14)O ,I(cid:10)and A :=O ,I suppI ( f O ,I ) (cid:5)= ∅.There are two possibilities. The first is that P n({xn}) > 0 for all xn ∈ Xn and all n ∈ N. Since S N is linear and stronglyfactorising, it follows that(cid:8)G O ∪I ( f O ,I | X I )S N(cid:7)(cid:3)=(cid:12)(cid:7)I{xI }S Nf O ,I (·, xI ) − S N(cid:7)(cid:8)(cid:13)(cid:8)f O ,I (·, xI )xI ∈XI(cid:3)=xI ∈XI(cid:8)(cid:7){xI }S N· 0 = 0.Then, if we apply S N to both sides of the inequality in Eq. (14), we get that S N ( A) = 0, which contradicts the assumptionthat Pn({xn}) > 0 for all xn ∈ Xn and all n ∈ N.The second possibility is that there are n ∈ N and xn ∈ Xn such that Pn({xn}) = 0. Consider any ε ∈ (0, 1). For all n ∈ N,let An := {xn ∈ Xn: Pn({xn}) = 0}, and let P εn be the linear prevision on L (Xn) defined by(cid:17)(cid:8)(cid:7)P εn{xn}:=εif xn ∈ An,| An|(1 − ε)P n({xn}) otherwisewhen An is non-empty, and P εnevery I , O and xI ∈ XI , it holds that, with obvious notations:(cid:8)f I,O (·, xI )(cid:11)(cid:11)G O ∪I ( f O ,I |xI ) − Gε(cid:11)(cid:11) = I{xI }O ∪I ( f O ,I |xI ):= Pn when An is empty. Let Sε(cid:11)(cid:11)S N(cid:7)N be the product of the linear previsions P εn , n ∈ N. Then for(cid:7)− SεNf O ,I (·, xI )(cid:8)(cid:11)(cid:11) (cid:3) I{xI }ε|XN | max | f O ,I |,whence(cid:11)(cid:11)G O ∪I ( f O ,I | X I ) − Gε(cid:11)(cid:11) (cid:3)O ∪I ( f O ,I | X I )(cid:3)(cid:11)(cid:11)G O ∪I ( f O ,I |xI ) − Gε(cid:11)(cid:11) (cid:3) I Aε|XN | max | f O ,I |,O ∪I ( f O ,I |xI )xI ∈suppI ( f O ,I )recalling that A =(cid:10)(cid:11)(cid:3)(cid:11)(cid:11)(cid:11)O ,IO ,I suppI ( f O ,I ). By summing over all disjoint subsets O , I of N, we obtain that(cid:3)G O ∪I ( f O ,I | X I ) − Gε(cid:11)(cid:11)(cid:11)(cid:11) (cid:3) I Aε|XN |O ∪I ( f O ,I | X I )max | f O ,I | =: I Aε K .O ,IIf we let 0 < ε < min{δ/2K , 1}, then we infer thatG. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501937(cid:3)O ,IO ∪I ( f O ,I | X I ) (cid:3) − δGε2I A.As before, applying SεN to both sides of this inequality leads to SεN ( A) = 0, a contradiction.Next, we turn to (b). By Theorem 1, we must establish the coherence of S N with each conditional linear previsionS O ∪I (·| X I ) (taken separately) for each pair of disjoint subsets I and O of N, or equivalently, that S N (I{xI }[ f − S O ∪I ( f |xI )]) =0 for all f ∈ L (XO ) and all xI ∈ XI . We see that indeed:(cid:13)(cid:8)(cid:13)(cid:8)(cid:7)(cid:7)(cid:7)(cid:8)(cid:7)(cid:12)(cid:12)S NI{xI }f − S O ( f |xI )= S NI{xI }f − S N ( f )= S N{xI }S N(cid:8)f − S N ( f )= 0,where the first equality follows from the definition of the conditional linear prevision S O ∪I (·| X I ), and the second onebecause S N is linear and strongly factorising.To complete the proof, we show that S N is the only joint coherent lower prevision that is a many-to-one independentproduct of the marginals Pn, n ∈ N. Since any many-to-many independent product of the marginals P n, n ∈ N, is in particularalso a many-to-one independent product of these marginals, it will then also follow that S N is the only joint coherent lowerprevision that is a many-to-many independent product of these marginals.Consider any linear prevision P N that is a many-to-one independent product of the marginals P n, n ∈ N. Fix o ∈ Nand I ⊆ N \ {o}, g ∈ L (Xo) and non-negative f i ∈ L (Xi), i ∈ I . Let f I :=i∈I f i . By assumption P N and P {o}∪I (·| X I ) arecoherent, and therefore we infer from (GBR) that P N (I{xI }[g − P N (g)]) = 0 for every xI ∈ XI . Hence also P N ( f I [g − P N (g)]) =(cid:5)f I (xI )P N (I{xI }[g − P N (g)]) = 0, where the first equality is due to the linearity of P N . It follows that P N is factorising,(cid:21)xi ∈XIand applying Proposition 7, we deduce that it coincides with the product S N .Finally, assume that the lower prevision P N is a many-to-one independent product of the marginals P n, n ∈ N. So P N iscoherent with the family of conditional linear previsions N (P n, n ∈ N), so it must be a lower envelope of linear previsionsP N coherent with N (Pn, n ∈ N) by the lower envelope theorem [30, Theorem 8.1.10], given that the spaces Xn, n ∈ N, arefinite. Since we have seen above that there is a unique linear prevision S N coherent with N (Pn, n ∈ N), we deduce thatP N = S N . (cid:2)Proof of Theorem 11. (i) ⇒ (ii). Fix y in Y . We show that the conditional lower previsions Qare coherent. Consider arbitrary gambles fk on XO k∪Ik , k = 1, . . . , m, and arbitrary j ∈ {1, . . . , m}, xI jL (XO j ∪I j ). Let gk := I{ y} fk ∈ L (XO k∪Ikwith obvious notations, for all zN in XN :(·| X Ik ), k = 1, . . . , m,(cid:15) ∈∈ XI j and f× Y ). Then it follows from Eq. (10) that,(cid:15) ∈ L (XO j ∪I j× Y ) and g(cid:15) := I{ y} fyO k∪IkGyO k∪Ik( fk| X Ik )(zN ) = fk(z O k , zIk ) − Q(cid:7)yO k∪Ik(cid:8)fk(·, zIk )|zIk(cid:7)= gk(z O k , zIk , y) − P O k∪Ikgk(·, zIk , y)|zIk , y(cid:8)= G O k∪Ik (gk| X Ik , Y )(zN , y),and similarly,GyO j ∪I j(cid:7)(cid:8)(cid:15)|xI jf(cid:15)f(zN ) = I{xI j= I{xI j= G O j ∪I j(cid:12)}(zI j )(cid:12)}(zI j )I{ y}( y)(cid:8)(cid:15)|xI j , yg(cid:7)(zN , y),(cid:7)(cid:15)(cid:8)(cid:13)(z O j , xI j ) − Q(cid:15)yO j ∪I j(z O j , xI j , y) − P O j ∪I j(·, xI j )|xI j(cid:7)(cid:15)ggf(·, xI j , y)(cid:8)(cid:13)(cid:11)(cid:11)xI j , yand therefore(cid:14)m(cid:3)k=1GyO k∪Ik( fk| X Ik ) − GyO j ∪I j(cid:7)(cid:8)(cid:15)|xI jf(cid:15)(zN ) =(cid:14)m(cid:3)k=1G O k∪Ik (gk| X Ik , Y ) − G O j ∪I j(cid:7)(cid:15)|xI j , yg(cid:15)(cid:8)(zN , y).Moreover, it follows, again with obvious notations, that(cid:6)suppIk(gk) :=(xIk , u) ∈ XIk× Y : gk(·, xIk , u) (cid:5)= 0(cid:9)= suppIk( fk) × { y}and therefore(cid:6)(cid:9)(xI j , y)∪m(cid:16)k=1#suppIk(gk) ={xI j} ∪$suppIk( fk)× { y}.m(cid:16)k=1Using this equality and (i), we find that there is some zN ∈ {xI jG O j ∪I j (gQ} ∪( fk| X Ik ) − G(·| X Ik ), k = 1, . . . , m, is coherent. Since y is arbitrary, (ii) holds.(cid:15)|xI j , y)](zN , y) (cid:2) 0, and therefore [yO k∪Ik(ii) ⇒ (i). Consider arbitrary gk ∈ L (XO k∪IkyO k∪Ikmk=1 G(cid:5)(cid:10)yO j ∪I jmk=1 suppIk( f( fk) such that [k=1 G O k∪Ik (gk| X Ik , Y ) −(cid:15)|xI j )](zN ) (cid:2) 0. This implies that the collectionm(cid:5)and g(cid:15) ∈ L (XO j ∪I j× Y ). We have to prove that there is some (zN , u) in {(xI j , y)} ∪mk=1 suppIk(gk) such that× Y ), k = 1, . . . , m, as well as arbitrary j ∈ {1, . . . , m}, (xI j , y) ∈ XI j× Y(cid:10)1938G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950G O k∪Ik (gk| X Ik , Y ) − G O j ∪I j(cid:15)(cid:8)(cid:7)(cid:15)|xI j , yg(zN , u) (cid:2) 0.(cid:14)m(cid:3)k=1Let fk := gk(·, y) ∈ L (XO k∪Ik ) and f(cid:15) := g(cid:15)(·, y) ∈ L (XO j ∪I j ). Then it holds for all zN ∈ XN thatGyO k∪Ik( fk| X Ik )(zN ) = fk(z O k , zIk ) − QyO k∪Ik(cid:7)(cid:8)fk(·, zIk )|zIk(cid:7)= gk(z O k , zIk , y) − P O k∪Ikgk(·, zIk , y)|zIk , y(cid:8)= G O k∪Ik (gk| X Ik , Y )(zN , y),and similarly,GyO j ∪I j(cid:7)(cid:8)(cid:15)|xI jf(cid:15)(zN ) = I{xI j= I{xI j= G O j ∪I j(cid:12)}(zI j )f(cid:12)}(zI j )I{ y}( y)(cid:8)(cid:15)|xI j , yg(cid:7)(zN , y),(cid:7)(cid:15)(cid:8)(cid:13)(z O j , xI j ) − Q(cid:15)yO j ∪I j(z O j , xI j , y) − P O j ∪I j(·, xI j )|xI j(cid:7)(cid:15)ggf(·, xI j , y)(cid:8)(cid:13)(cid:11)(cid:11)xI j , yand therefore(cid:14)m(cid:3)k=1GyO k∪Ik( fk| X Ik ) − GyO j ∪I j(cid:7)(cid:8)(cid:15)|xI jf(cid:15)(zN ) =(cid:14)m(cid:3)k=1G O k∪Ik (gk| X Ik , Y ) − G O j ∪I j(cid:7)(cid:15)|xI j , yg(cid:15)(cid:8)(zN , y).Since the collection Qthat [yO k∪Ikk=1 G O k∪Ik (gk| X Ik , Y ) − G O j ∪I j (g(cid:5)m(·| X Ik ), k = 1, . . . , m, is coherent, we infer that there is some zN ∈ {xI j(cid:15)|xI j , y)](zN , y) (cid:2) 0. It therefore suffices to prove that} ∪(cid:10)mk=1 suppIk( fk) such(zN , y) ∈(cid:6)(cid:9)(xI j , y)∪m(cid:16)suppIk(gk).This certainly holds if zI j0 (cid:5)= fk(·, zIk ) = gk(·, zIk , y), so indeed (zN , y) ∈ suppIk(gk). (cid:2)k=1= xI j . If not, then there must be some k ∈ {1, . . . , m} such that zN ∈ suppIk( fk), meaning thatProof of Proposition 12. It follows from its definition that the strong product is a lower envelope of product linearprevisions. By Proposition 10, it is therefore a lower envelope of many-to-many independent (respectively many-to-oneindependent) products. Since both of these two properties are preserved by taking lower envelopes, ×n∈N P n is also amany-to-many and many-to-one independent product. (cid:2)Proof of Proposition 13. We infer from Proposition 12 that the strong product ×n∈N P n is coherent with the collectionN (P n, n ∈ N). This implies in particular that N (P n, n ∈ N) is itself coherent. (cid:2)Proof of Corollary 14. By Proposition 13, the collection N (P n, n ∈ N) is coherent. Theorem 2 then tells us that P N iscoherent with N (P n, n ∈ N) if and only if it is weakly coherent with N (P n, n ∈ N). Taking into account Theorem 1, thisholds if and only if for every o ∈ N and I ⊆ N \ {o},(cid:7)(cid:12)(cid:13)(cid:8)P NI{xI }Now use Eq. (9) to find thatf − P {o}∪I ( f |xI )= 0 for all f ∈ L (X{o}∪I ) and all xI ∈ XI .(cid:12)(cid:13)f − P {o}∪I ( f |xI )I{xI }= I{xI }(cid:12)f (·, xI ) − P o(cid:7)f (·, xI )(cid:8)(cid:13).(cid:2)Proof of Theorem 16. Denote the right-hand side in Eq. (12) by Q N ( f ). It follows easily from Eq. (11) that E N ( f ) (cid:2) Q N ( f ),so we concentrate on the converse inequality. Consider any real α < E N ( f ), then there are gambles g I,o in L (X{o}∪I ) forall o ∈ N and I ⊆ N \ {o} such that(cid:26)minzN ∈XNf (zN ) −(cid:3)(cid:12)g I,o(zo, zI ) − P o(cid:7)g I,o(·, zI )(cid:27)(cid:8)(cid:13)o∈N,I⊆N\{o}For every n in N, define the gamble hn on XN by hn :=and(cid:7)(cid:8)hn(·, zN\{n})P n= P n(cid:18) (cid:3)(cid:20)g I,n(·, zI )(cid:2)(cid:3)(cid:7)(cid:8)g I,n(·, zI ),P n(cid:2) α.(15)(cid:5)I⊆N\{n} g I,n. Then for all zN ∈ XN , hn(zN ) =(cid:5)I⊆N\{n} g I,n(zn, zI )where the inequality follows from the coherence of P n. We then infer from Eq. (15) and the definition of Q N ( f ) thatI⊆N\{n}I⊆N\{n}G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501939(cid:26)Q N ( f ) (cid:2) minzN ∈XNf (zN ) −(cid:12)hn(zN ) − P n(cid:3)n∈N(cid:7)hn(·, zN\{n})(cid:27)(cid:8)(cid:13)(cid:2) α.Since this inequality holds for all real α < E N ( f ), we see that indeed Q N ( f ) (cid:2) E N ( f ). (cid:2)Next, we turn to the proof of Theorem 18. In order to do this, it will be very helpful to work with sets of the so-calledstrictly desirable gambles [30]. For every n ∈ N, consider the following subset of L (Xn):(cid:6)An:=f ∈ L (Xn): f > 0 or P n( f ) > 0(cid:9);we use these sets to define the following subsets of L (XR ), where R is any non-empty subset of N:(cid:6)A Rr:=f ∈ L (XR )(cid:5)=0: (∀xR\{r} ∈ XR\{r}) f (·, xR\{r}) ∈ Ar∪ {0}(cid:9),r ∈ R.We also define, for any subset S of N:(cid:18)(cid:20)(cid:16)ES := posiL (XS )>0 ∪A Ss.(16)s∈SFor S = ∅, this leads to E∅ = L (X∅)>0, which we have identified with the set of positive real numbers.We begin by proving a crucial property of all these sets An and EN in Lemma 35. In order to prove this result, anda few more involved ones further on, we need the following lemmas, one of which is a convenient version of the separatinghyperplane theorem:n, A NLemma 33. Consider a finite subset A of L (X). Then 0 /∈ posi(L (X)>0 ∪ A ) if and only if there is some linear prevision P onL (X) with mass function p such that P ( f ) =x∈X p(x) f (x) > 0 for all f ∈ A and p(x) > 0 for all x ∈ X.(cid:5)Proof. It clearly suffices to prove necessity. Since 0 /∈ posi(L (X)>0 ∪ A ), we infer applying a version of the separatinghyperplane theorem in finite-dimensional spaces23 that there is a linear functional Λ on L (X) such that(∀x ∈ X)Λ(I{x}) > 0 and (∀ f ∈ A )Λ( f ) > 0.(cid:5)Then Λ(X) =P ( f ) > 0 for all f ∈ A . Moreover, for any x ∈ X, p(x) = P (I{x}) = Λ(I{x})/Λ(X) > 0. (cid:2)x∈X) Λ(I{x}) > 0, and if we let P := Λ/Λ(X) then P is clearly a linear prevision on L (X) for whichLemma 34. Consider a convex cone A of gambles on X such that max f > 0 for all f ∈ A . Consider any non-zero gamble g on X.If g /∈ A then 0 /∈ posi(A ∪ {−g}).Proof. Consider a non-zero gamble g /∈ A , and assume ex absurdo that 0 ∈ posi(A ∪ {−g}). Then it follows from thek=1 λk fk + μ(−g). Henceassumptions that there are m > 0, λk > 0,g ∈ posi(A ) = A , a contradiction. (cid:2)fk ∈ A , k = 1, . . . , m, and μ > 0 such that 0 =(cid:5)mLemma 35. Let S be any subset of N, and let R be any non-empty subset of N. Consider any n ∈ N and r ∈ R.n is a convex cone such that L (Xn)>0 ⊆ An and L (Xn)(cid:2)0 ∩ A(i) A(ii) A Rr and L (XR )(cid:2)0 ∩ A Rr(iii) ES is a convex cone such that L (XS )>0 ⊆ ES and L (XS )(cid:2)0 ∩ ES = ∅.is a convex cone such that L (XR )>0 ⊆ A Rnr= ∅:= ∅;Proof. (i) Immediate: use the coherence of the lower prevision P n.(ii) Consider any f ∈ L (XR )>0. Then for all zR\{r} ∈ XR\{r},moreover f (cid:5)= 0, it follows that f ∈ A RTo prove that A Rrm > 0, λk > 0 and fk ∈ A Rrfrom (i) that this can only happen if alla contradiction. Hence A Nnr . This shows that L (XR )>0 ⊆ A Rr .is a convex cone, it clearly suffices to show that 0 /∈ posi(A Rk=1 λk fk. Fix any zR\{r} ∈ XR\{r}. Then 0 =such that 0 =(cid:5)mis indeed a convex cone.Finally, consider f ∈ L (XR )(cid:2)0 and assume that f ∈ A Rtherefore also − f ∈ A RHence indeed L (XR )(cid:2)0 ∩ A Rr= ∅.r . Therefore 0 = f + (− f ) ∈ A Rrsince A Rrfk(·, zR\{r}) = 0. But since this holds for all zR\{r} ∈ XR\{r}, we infer that all(cid:5)r ). Assume ex absurdo that there arek=1 λk fk(·, zN\{n}), and it followsfk = 0,mr . It already follows from the reasoning above that f < 0, andis a convex cone [closed under addition], a contradiction.f (·, zR\{r}) (cid:2) 0 and therefore f (·, zR\{r}) ∈ An∪ {0}. Since23 We use the version in Appendix E.1 of Ref. [30], for the special choice V := A ∪ {I{x} : x ∈ X} and W := {0}.1940G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950(cid:5)(iii) It clearly suffices to prove that L (XS )(cid:2)0 ∩ ES = ∅. This is trivially so if S = ∅, so let us assume that S is non-s and g > 0 such thatempty. Let f ∈ L (XS )(cid:2)0 and assume ex absurdo that f ∈ ES . Then there are λs (cid:2) 0, μ (cid:2) 0,f = μg +s∈S λs f s and max{μ, maxs∈S λs} > 0.Fix any s ∈ S. Let A := { f s(·, zS\{s}): zS\{s} ∈ XS\{s}, f s(·, zS\{s}) (cid:5)= 0}, then it follows from the assumptions that A iss, and therefore it follows from (i) and Lemma 33 that there is a linear prevision P s onf s ∈ A Sa finite non-empty subset of AL (Xs) with mass function ps such that(∀xs ∈ Xs)ps(xs) > 0,(cid:7)(∀zS\{s} ∈ XS\{s})(cid:17)(cid:8)f s(·, zS\{s})We conclude that if we define the gamble gs on XS\{s} by gs(zS\{s}) := P s( f s(·, zS\{s})) for all zS\{s} in XS\{s}, then gs > 0.s∈S ps(zs) > 0 forall zS ∈ XS . The corresponding linear prevision P S is of course the product linear prevision×s∈S P s of the marginal linearSince we can do this for all s ∈ S, we can define a mass function p S on XS by letting p S (zS ) :=f s(·, zS\{s}) (cid:5)= 0 ⇒ P s> 0(cid:8).(cid:21)(cid:7)previsions P s. But then it follows from the reasoning and assumptions above thatP S ( f ) = μP S (g) +λs P S ( f s) = μP S (g) +λs P S (gs) > 0,(cid:3)(cid:3)s∈Sbecause P S (g) > 0 and all P S (gs) > 0, whereas f (cid:3) 0 leads us to conclude that P S ( f ) (cid:3) 0, a contradiction. (cid:2)s∈SIt turns out that there is a very close relationship between the set of gambles EN and the many-to-one independentnatural extension E N :Lemma 36. For all f ∈ L (XN ), E N ( f ) = sup{α: f − α ∈ EN }.(cid:5)Proof. Let, for the sake of notational simplicity, Q N ( f ) := sup{α:Consider any real α such that f − α ∈ EN . Then there are non-negative μ and λn, g > 0 and fn ∈ A Nf − α ∈ EN }, then we have to prove that E N ( f ) = Q N ( f ).n such that f − α =∪ {0} for all zN\{n} ∈ XN\{n}, soμg +we conclude by considering gn := λn fn ∈ L (XN ) that λn fn(·, zN\{n}) (cid:2) gn(·, zN\{n})− P n(gn(·, zN\{n})) for all zN\{n} ∈ XN\{n},and thereforen∈N λn fn with max{μ, maxn∈N λn} > 0. We infer from fn ∈ A Nn that fn(·, zN\{n}) ∈ Anλn fn(zN ) (cid:2) gn(zN ) − P n(cid:7)(cid:8)gn(·, zN\{n})for all zN ∈ XN and all n ∈ N.So we find thatf (zN ) − α (cid:2)(cid:3)(cid:12)n∈N(cid:7)gn(zN ) − P n(cid:8)(cid:13)gn(·, zN\{n})for all zN ∈ XN ,and therefore α (cid:3) E N ( f ). Hence Q N ( f ) (cid:3) E N ( f ).Conversely, let α < E N ( f ), then there are ε > 0 and gn ∈ L (XN ), n ∈ N, such that(cid:7)(cid:3)(cid:8)(cid:13)(cid:12)α + ε|N| (cid:3) f (zN ) −gn(zN ) − P ngn(·, zN\{n})for all zN ∈ XN ,n∈N(cid:5)or in other words f − α (cid:2)that hn(·, zN\{n}) ∈ Atherefore also that E N ( f ) (cid:3) Q N ( f ). (cid:2)n for all zN\{n} ∈ XN\{n}, whence hn ∈ A Nn∈N hn, where we let hn(zN ) := gn(zN ) − P n(gn(·, zN\{n})) + ε for all zN ∈ XN . This impliesn , and therefore f − α ∈ EN . We infer that α (cid:3) Q N ( f ) andWe now show that for any subset R of N, ER is the set of those gambles in EN that depend at most on the variablesX R , and not on X N\R .Lemma 37. For every subset R of N, ER = EN ∩ L (XR ).⊆ A NrProof. The result is trivial for R = ∅ and R = N. Let us therefore assume that both R and N \ R are proper subsets of N.Recall from Section 2 that we are interpreting gambles on XR as special gambles on XN . Keeping this in mind, it is obviousthat A Rrfor all r ∈ R, and therefore ER ⊆ EN . So we already find that ER ⊆ EN ∩ L (XR ).We prove the converse inequality. Let f ∈ EN ∩L (XR ) and assume ex absurdo that f /∈ ER . It follows from Lemma 35(iii)f = g +s∈S f s.f = f (·, xN\R ) = g(·, xN\R ) +thatClearly S \ R (cid:5)= ∅, because S \ R = ∅ would imply that, with xN\R any element of XN\R ,(cid:5)s , s ∈ S and g ∈ L (XN ) with g (cid:2) 0 such thatf (cid:5)= 0. Since f ∈ EN , there are S ⊆ N,s∈S∩R f s(·, xN\R ) ∈ ER , since we infer from Lemma 38 below that f s(·, xN\R ) ∈ A RIt follows from Lemmas 35(iii) and 34 and f /∈ ER that 0 /∈ posi(ER ∪ {− f }). Let A := { f s(·, zN\R ): s ∈ S ∩ R, zN\R ∈XN\R , f s(·, zN\R ) (cid:5)= 0}. Then A is clearly a finite subset of ER [to see this, use a similar argument as above, involving∪ {0} for all s ∈ S ∩ R.f s ∈ A N(cid:5)sG. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501941Lemma 38], so we deduce from Lemmas 35(iii) and 33 that there is some linear prevision P R on L (XR ) with massfunction p R such that⎧⎨⎩(∀xR ∈ XR )p R (xR ) > 0,(∀s ∈ S ∩ R)(∀zN\R ∈ XN\R )P RP R ( f ) < 0.(cid:7)(cid:8)f s(·, zN\R )(cid:2) 0,We then infer from f = f (·, zN\R ) = g(·, zN\R ) +(cid:7)(cid:7)(cid:3)0 > P R ( f ) − P R(cid:8)g(·, zN\R )−(cid:5)(cid:5)s∈S∩R f s(·, zN\R ) +(cid:8)f s(·, zN\R )(cid:3)=P R(cid:7)(cid:8)f s(·, zN\R )=s∈S\R f s(·, zN\R ) that for all zN\R in XN\R :(cid:3)(cid:3)p R (xR ) f s(xR , zN\R ).P Rs∈S∩Rs∈S\Rs∈S\RxR ∈XRThe gambles f s(xR , ·) on XN\R [where xR ∈ XR and s ∈ S \ R] can clearly not all be zero, and the non-zero onesbelong to EN\R by Lemma 38. Since EN\R is closed under positive linear combinations by Lemma 35, the gambleh :=p R (xR ) f s(xR , ·) is an element of EN\R that is everywhere strictly negative. But on the other handwe should have that max h > 0 by Lemma 35(iii), a contradiction. We may therefore conclude that indeed f ∈ ER . (cid:2)xR ∈XRs∈N\R(cid:5)(cid:5)Lemma 38. Consider an arbitrary non-empty subset R of N and let r ∈ R. Then f (·, xN\R ) ∈ A RrxN\R ∈ XN\R .∪ {0} for allf ∈ A Nr and allProof. Fix f ∈ A Nthat for all xR\{r} ∈ XR\{r}:r and xN\R ∈ XN\R and consider the gamble g := f (·, xN\R ) on XR . Then it follows from the assumptionsg(·, xR\{r}) = f (·, xR\{r}, xN\R ) = f (·, xN\{r}) ∈ Ar∪ {0},whence indeed g ∈ A Rr∪ {0}. (cid:2)Proof of Theorem 18. Use Lemma 36 and the fact that f − α ∈ EN ⇔ f − α ∈ ER for all f ∈ L (XR ), by Lemma 37. (cid:2)Next, consider an arbitrary subset R of N and consider, for each xR ∈ XR , the set of gambles:(cid:6)EN |xR :=f ∈ L (XN\R ): I{xR } f ∈ EN(cid:9).Note that if R is the empty set then we obtain trivially that EN |x∅ = EN .Then the set EN satisfies the following interesting epistemic irrelevance condition:Lemma 39. For every subset R of N and every xN\R ∈ XN\R , EN |xN\R = ER .Proof. The proof is similar to that of Lemma 37. Again, it is trivial for R = ∅ or R = N, so we turn to the case where both Rand N \ R are proper subsets of N. We first show that EN |xN\R ⊇ ER . Consider any gamble f ∈ ER , so there are non-negativer∈R λr fr , with max{μ, maxr∈R λr} > 0. Fix r ∈ Rλr and μ,(cid:15)and let fr(cid:5)= 0, and for all zN\{r} ∈ XN\{r}, it follows from the definition of A Rrfr ∈ A Rr:= I{xN\R } fr ∈ L (XN ). Then ffor all r ∈ R and g ∈ L (XR )>0 such that f = μg +that(cid:5)(cid:15)r(cid:15)r (·, zN\{r}) = I{xN\R }(zN\R ) fr(·, zR\{r}) ∈ Arf∪ {0},and therefore the definition of A N(cid:5)rI{xN\R } f = μgr∈R λr f(cid:15) +(cid:15)r , and therefore I{xN\R } f ∈ EN .tells us that f(cid:15)r∈ A Nr. Similarly, if we let g(cid:15) := I{xN\R } g ∈ L (XN ), then g(cid:15) > 0. Hence(cid:5)We now turn to the converse inequality EN |xN\R ⊆ ER . Consider any gamble f ∈ L (XR ) such that I{xN\R } f belongs tos , s ∈ S and g ∈ L (XN ) with g (cid:2) 0(cid:5)s∈S∩R f s(·, xN\R ) ∈EN and assume ex absurdo that f /∈ ER . Since I{xN\R } f ∈ EN , there are S ⊆ N,such that I{xN\R } f = g +ER , since Lemma 38 shows that f s(·, xN\R ) ∈ A Rss∈S f s. Clearly S \ R (cid:5)= ∅, because S \ R = ∅ would imply that f = g(·, xN\R ) +It follows from Lemmas 35(iii), 34 and f /∈ ER that 0 /∈ posi(ER ∪ {− f }). Let A := { f s(·, xN\R ): s ∈ S ∩ R, f s(·, xN\R ) (cid:5)= 0}.Then A is clearly a finite subset of ER [to see this, use a similar argument as above, involving Lemma 38], so we deducefrom Lemma 33 that there is some linear prevision P R on L (XR ) with mass function p R such thatfor all s ∈ S ∩ R.f s ∈ A N⎧⎨⎩(∀xR ∈ XR )p R (xR ) > 0,(∀s ∈ S ∩ R)P RP R ( f ) < 0.(cid:8)f s(·, xN\R )(cid:7)(cid:2) 0,(cid:5)We then infer from f = g(·, xN\R ) +(cid:8)g(·, xN\R )0 > P R ( f ) − P R(cid:7)−s∈S∩R f s(·, xN\R ) +(cid:3)(cid:7)s∈S\R f s(·, xN\R ) that:(cid:7)(cid:3)(cid:8)f s(·, xN\R )=(cid:5)P Rs∈S∩RP Rs∈S\R(cid:8)f s(·, xN\R )(cid:3)(cid:3)=s∈S\RxR ∈XRp R (xR ) f s(xR , xN\R ).1942G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950Similarly, since 0 = g(·, zN\R ) +(cid:7)(cid:8)g(·, zN\R )−0 (cid:2) −P R(cid:5)(cid:3)s∈S∩R f s(·, zN\R ) +(cid:8)f s(·, zN\R )P R(cid:7)(cid:5)=s∈S\R f s(·, zN\R ) for all zN\R ∈ XN\R \ {xN\R }, we infer that:(cid:3)(cid:3)(cid:3)(cid:7)(cid:8)f s(·, zN\R )=P Rp R (xR ) f s(xR , zN\R ).s∈S∩Rs∈S\Rs∈S\RxR ∈XRHence(cid:3)(cid:3)h :=s∈S\RxR ∈XRp R (xR ) f s(xR , ·) < 0.The gambles f s(xR , ·) on XN\R [where xR ∈ XR and s ∈ S \ R] can clearly not all be zero, and the non-zero ones belong toEN\R by Lemma 38. Since EN\R is closed under positive linear combinations by Lemma 35, the gamble h < 0 is an elementof EN\R . But on the other hand we should have that max h > 0 by Lemma 35(iii), a contradiction. We may therefore concludethat indeed f ∈ ER . (cid:2)From this lemma, we deduce the following:Lemma 40. Consider any subset R of N. Then f g ∈ EN for all f ∈ L (XN\R )>0 and all g ∈ ER .Proof. Consider any g ∈ ER and any f ∈ L (XN\R )>0. Then it follows from Lemma 39 that I{xN\R } g ∈ EN for all xN\R ∈XN\R . But EN is closed under positive linear combinations by Lemma 35, and there is at least one xN\R ∈ XN\R for whichf (xN\R )I{xN\R } g is essentially a positive linear combination of gambles inf (xN\R ) > 0, so we deduce that f g =EN , and therefore also belongs to EN . (cid:2)xN\R ∈XN\R(cid:5)Proof of Proposition 19. We begin by showing that E N is productive. First, consider arbitrary disjoint proper subsets I andO of N, xI ∈ XI , g ∈ L (XO ) and non-negative f ∈ L (XI ), and let us prove that E N ( f [g − E N (g)]) (cid:2) 0.Fix α > 0 and β > 0, then f + β > 0 and g − E O (g) + β ∈ EO , by Lemma 36 [where we replace the set N with O ].Hence ( f + α)[g − E O (g) + β] ∈ EO ∪I ⊆ EN , by Lemmas 40 [where we replace N with O ∪ I and R with O ] and 37. ThenLemma 36 tells us that E N (( f + α)[g − E O (g) + β]) (cid:2) 0. If we now invoke the coherence of the lower prevision E N , we seethat0 (cid:3) E N(cid:7)(cid:12)( f + α)g − E O (g) + β(cid:13)(cid:8)(cid:7)(cid:12)f(cid:3) E N(cid:13)(cid:8)g − E O (g)+ β E N ( f ) + α(cid:12)E N (g) − E O (g) + β(cid:13).Since this holds for all α > 0 and all β > 0, we infer that E N ( f [g − E N (g)]) = E N ( f [g − E O (g)]) (cid:2) 0, where the equalityfollows from Theorem 18. Hence E N is indeed productive.Next let us consider the special cases where I or O are empty. If I = ∅, then we obtain E N ( f [g − E N (g)]) = f (E N (g) −E N (g)) = 0, since f is then a non-negative real number. If on the other hand O = ∅, then E N ( f [g − E N (g)]) = E N ( f · 0) = 0,since in this case g is a real number.This implies in particular that E N (I{xI }[g − E O (g)]) (cid:2) 0 for arbitrary disjoint subsets I and O of N. Assume ex absurdo thatE N (I{xI }[g − E O (g)]) > 0. By Lemma 36, there is some α > 0 such that I{xI }[g − E O (g)] − α ∈ EN . Since I{xI }[g − E O (g) − α] (cid:2)I{xI }[g − E O (g)] − α, this implies that I{xI }[g − E N (g) − α] ∈ EN . By Lemmas 39 and 37, this implies that g − E O (g) − α ∈ EO .But then Lemma 36 implies that −α = E O (g − E O (g) − α) (cid:2) 0, a contradiction. Hence indeed E N (I{xI }[g − E O (g)]) = 0. (cid:2)Proof of Theorem 20. By Theorem 2, it suffices to prove that (a) E N is weakly coherent with the family I (E N ); and that(b) the family I (E N ) is coherent.We begin by showing that (a) E N is weakly coherent with the family I (E N ). Consider any disjoint subsets I and O of N.Taking into account Theorem 1, it suffices to show that E N (I{xI }[ f − E O ∪I ( f |xI )]) = 0 for all xI ∈ XI and all f ∈ L (XO ∪I ).If we look at Eq. (13), we see that this amounts to proving that E N (I{xI }[g − E O (g)]) = 0 for all xI ∈ XI and all g ∈ L (XO ).Now use Proposition 19.To finish the proof, we show that (b) the family I (E N ) is coherent. Assume ex absurdo that there are f O ,I ∈ L (XO ∪I )∗∗for all disjoint subsets I and O of N, disjoint subsets Iand Oof N, g ∈ L (XO ∗∪I ∗ ), xI ∗ ∈ XI ∗ and δ > 0 such that(cid:3)G O ∪I ( f O ,I | X I ) − G O ∗∪I ∗ (g|xI ∗ ) (cid:3) −δI A,(cid:10)O ,Iwhere A := {xI ∗ } ∪∗ } −ε := δ/K > 0 such that −δI A (cid:3) −εI{xI(cid:8)g(·, xI ∗ )g(·, xI ∗ ) − E O ∗− ε∗ }(cid:7)(cid:12)I{xI(cid:13)(cid:2)O ,I suppI ( f O ,I ). But K I A (cid:2) I{xI(cid:5)IsuppI ( f O ,I ) for some natural number K > 0, so there is someO ,IO ,I εIsuppI ( f O ,I ) and therefore also [see Theorem 18]:(cid:3)(cid:12)(cid:13)G O ∪I ( f O ,I | X I ) + εIsuppI ( f O ,I ).(17)(cid:5)∗ } +For arbitrary disjoint I and O , it follows from the definition of suppI ( f O ,I ) and Theorem 18 thatO ,IG. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501943G O ∪I ( f O ,I | X I ) + εIsuppI ( f O ,I ) =(cid:3)(cid:12)I{xI }f O ,I (·, xI ) − E O(cid:7)(cid:8)f O ,I (·, xI )(cid:13),+ εxI ∈suppI ( f O ,I )so we infer from Lemmas 36 and 40 that the gamble G O ∪I ( f O ,I | X I ) + εIsuppI ( f O ,I ) belongs to the convex cone EN . Sodoes, therefore, the right-hand side in Eq. (17). As a consequence, I{xI∗ }[g(·, xI ∗ ) − E O ∗ (g(·, xI ∗ )) − ε] ∈ EN , so we inferfrom Lemmas 39 and 37 that g(·, xI ∗ ) − E O ∗ (g(·, xI ∗ )) − ε ∈ EO ∗ . But then Lemma 36 and the coherence of E O ∗ lead to−ε = E O ∗ (g(·, xI ∗ ) − E O ∗ (g(·, xI ∗ )) − ε) (cid:2) 0, a contradiction. (cid:2)n∈N P n by E N , and the strong product ×n∈N P n by S N .Proof of Theorem 22. Denote the independent natural extensionWe use the characterisation of factorisation in Proposition 5, and we take into account that for all n ∈ N and fn ∈ L (Xn),E N ( fn) = S N ( fn) = P n( fn).(cid:25)We begin by proving an auxiliary result, namely that(cid:18)(cid:22)(cid:20)=f iE Ni∈I(cid:22)i∈IP i( f i)for all non-negative f i ∈ L (Xi), i ∈ I,(H 1I )for all subsets I of N. We give a proof by induction. It is clear from the coherence of E N that the statement (H 1I = ∅. Next, assume that (H 1induction hypothesis]. Then the statement is proved if we can show that (H 1element of the non-empty set N \ M.I ) holds forI ) holds for I = M, where M is some subset of N that does not coincide with N [this is the(cid:15) := M ∪ {n}, where n is anyI ) holds for I = M(cid:21)So consider any non-negative f i ∈ L (Xi), i ∈ Mof the induction hypothesis, E N ( f M ) =product is factorising [see Propositions 8 and 5], we also have that S N ( f M ) =i∈M(cid:15) f i = f M fn. Becausef M :=i∈M P i( f i), so we have to show that E N ( f M fn) = E N ( f M )P n( fn). Because the strongi∈M P i( f i), and therefore E N ( f M ) = S N ( f M ).Since E N is dominated by the (many-to-one independent) strong product S N [see Definition 6 and Proposition 12], and. Let, for ease of notation,i∈M f i , so(cid:21)(cid:21)(cid:21)(cid:15)because the strong product is factorising [see Propositions 8 and 5], we see thatE N ( f M fn) (cid:3) S N ( f M fn) = S N ( f M )P n( fn) = E N ( f M )P n( fn).We now prove the converse inequality. Recall from its definition that E N is coherent with the conditional lower previsionP {n}∪M (·| X M ) on L (X{n}∪M ) defined byP {n}∪M (h|xM ) := P n(cid:7)(cid:8)h(·, xM )for all h ∈ L (X{n}∪M ) and all xM ∈ XM ,so it follows that:E N ( f M fn) (cid:2) E N(cid:7)(cid:8)P {n}∪M ( f M fn| X M )(cid:7)(cid:8)f M P n( fn)= E N= E N ( f M )P n( fn).(18)Here, the inequality follows from the coherence of E N with P n(·| X M ) and Eq. (4), the first equality from the fact thatP n( f M fn| X M ) = f M P n( fn| X M ) by Eqs. (1) and (18), and the last equality from the coherence of E N and the fact thatP n( fn) (cid:2) 0. This completes the proof of the auxiliary result.The proof that E N is factorising goes along similar lines. Fix any o in N and any I ⊆ N \ {o}, any fo ∈ L (Xo), and non-i∈I f i . Then we already know from the argumentation abovef I :=negative f i ∈ L (Xi), i ∈ I . Let, for ease of notation,that E N ( f I ) = S N ( f I ) =i∈I P i( f i). We have to show that E N ( fo f I ) = E N ( f I P o( fo)).(cid:21)(cid:21)As before, since E N is dominated by the (many-to-one independent) strong product S N , and because the strong productis factorising, we see thatE N ( fo f I ) (cid:3) S N ( fo f I ) = S N ( f I ) (cid:10) P o( fo) = E N ( f I ) (cid:10) P o( fo) = E N(cid:7)(cid:8)f I P o( fo).We now prove the converse inequality. Recall from its definition that E N is coherent with the conditional lower previsionP {o}∪I (·| X I ) on L (Xo) defined byP o(h|xI ) := P o(cid:7)(cid:8)h(·, xI )for all h ∈ L (X{o}∪I ) and all xI ∈ XI ,so it follows that:E N ( fo f I ) (cid:2) E N(cid:7)(cid:8)P {o}∪I ( fo f I | X I )(cid:7)(cid:8)f I P {o}∪I ( fo| X I )(cid:7)(cid:8)f I P o( fo).= E N= E N(19)Here, similarly as before, the inequality follows from the coherence of E N with P o(·| X I ) and Eq. (4), the first equality fromthe fact that P {o}∪I ( fo f I | X I ) = f I P {o}∪I ( fo| X I ) by Eq. (1), and the second equality from Eq. (19). (cid:2)Proof of Theorem 23. We construct the set E{N1,N2} after the fashion of Eq. (16). We let(cid:6)AN1:=andf ∈ L (XN1 ): f > 0 or E N1 ( f ) > 0(cid:9)1944G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950∈ XN2 ) f (·, zN2 ) ∈ A(cid:9)∪ {0}N1(cid:6)A {N1,N2}N1:=and similarly for AN2E{N1,N2} := posif ∈ L (X{N1,N2})(cid:5)=0: (∀zN2and A {N1,N2}. ThenN2(cid:7)L (X{N1,N2})>0 ∪ A {N1,N2}N1⊆ EN1 and A∪ A {N1,N2}N2(cid:8).(20)N1⊆ EN2 . Now let f 1 ∈ A {N1,N2}N2} f 1(·, zN2 ) ∈ EN1∪N2I{zN2⊆ EN1∪N2 , so we infer from Eq. (20) that E{N1,N2} ⊆ EN1∪N2 . Hence E {N1,N2} (cid:3) E N1∪N2 , by Lemma 36.We infer from Lemma 36 that A∪ {0}, and therefore24 I{zN2f 1(·, zN2 ) ∈ EN1cone by Lemma 35(iii), it follows that f 1 =A {N1,N2}N2For the converse inequality, it suffices to prove that E{N1,N2} is an independent many-to-one product of the marginals P n,n ∈ N1 ∪ N2. We use Corollary 14(ii). Consider any o ∈ N1 ∪ N2, any I ⊆ (N1 ∪ N2) \ {o}, any g ∈ L (Xo), and any xI ∈ XI .We have to prove that E{N1,N2}(I{xI }[g − P o(g)]) = 0. Let I1 := I ∩ N1 and I2 := I ∩ N2. We may assume without loss ofgenerality that o ∈ N2. Since the independent natural extension is factorising [by Theorem 22], we find that indeed∈ XN2 . Then∪ {0} by Lemma 40. Since f 1 (cid:5)= 0 and EN1∪N2 is a convex⊆ EN1∪N2 and similarly,} f 1(·, zN2 ) ∈ EN1∪N2 . Hence A {N1,N2}and consider any zN2∈XN2(cid:5)zN2N1N1(cid:12)(cid:7)I{xI }E{N1,N2}(cid:13)(cid:8)g − P o(g)(cid:7)(cid:12)= E{N1,N2}= E N1 (I{xI1= E N1 (I{xI1I{xI1}I{xI2}(cid:7)}) (cid:10) E N2}) (cid:10) 0 = 0,(cid:13)(cid:8)g − P o(g)(cid:12)I{xI2}g − P o(g)(cid:13)(cid:8)where the third equality follows from Corollary 14(ii). (cid:2)Proof of Theorem 24. Consider arbitrary disjoint subsets I and O of N, an arbitrary gamble g on XO and an arbitrarynon-negative gamble f on XI . We have to show that E N ( f g) = E N ( f E N (g)). Consider any partition N1 and N2 of N suchthat I ⊆ N1 and O ⊆ N2. Since the independent natural extension E{N1,N2} = E N1⊗ E N2 of E N1 and E N2 is factorising byTheorem 22, we see that E{N1,N2}( f g) = E {N1,N2}( f E{N1,N2}(g)). Now use Theorem 23 to find that this implies that indeedE N ( f g) = E N ( f E N (g)). (cid:2)P {1,2}( f |x2) := P 1Proof of Proposition 25. Let P {1,2} be any (many-to-one) independent product of P 1 and P 2, and consider the conditionallinear prevision P {1,2}(·| X2) on L (X{1,2}) defined by(cid:7)(cid:8)f (·, x2)Then P {1,2} is in particular coherent with P {1,2}(·| X2). Fix fin L (X{1,2}). It follows from Eq. (4) and the self-conjugacyof P {1,2}(·| X2) that P {1,2}( f ) = P {1,2}(P {1,2}( f | X2)). Moreover, it follows from Corollary 15 that P 2 is the X2-marginal ofP {1,2}, so P {1,2}( f ) = P 2(P {1,2}( f | X2)) = P 2(P 1( f )). This holds in particular for P N = (P 1 × P 2) and P N = (P 1 ⊗ P 2). (cid:2)for all f ∈ L (X{1,2}) and x2 ∈ X2.Proof of Proposition 26. Let P {1,2}(·| X1) be the conditional lower prevision on L (X{1,2}) defined byP {1,2}( f |x1) := P 2(cid:7)(cid:8)f (x1, ·)for all f ∈ L (X{1,2}) and x1 ∈ X1.Then the independent natural extension P A11(P A11⊗ P 2)(P {1,2}(·| X1)) = P A1(cid:8)(cid:7)(cid:8)(cid:7)P A11× P 2( f ) (cid:2)P A11⊗ P 2( f ) (cid:2) minx1∈ A1P 2(cid:7)(cid:8)f (x1, ·)⊗ P 2 is coherent with P {1,2}(·| X1), so we infer from Eq. (4) that P A11⊗ P 2 (cid:2)1 (P {1,2}(·| X1)), where the equality follows from Corollary 15. So we find thatfor every gamble f on X1 × X2.∗To prove that the equalities hold, consider any gamble f on X{1,2}. Since A1 is finite there is some x1∈ A1 such∗1, ·)) = minx1∈ A1 P 2( f (x1, ·)). Moreover, it follows from the coherence of the lower prevision P 2 that there isthat P 2( f (x∗∗1, ·)). Let P 1 := P1, ·)) = P 2( f (xsome linear prevision P 2 ∈ ext(M (P 2)) such that P 2( f (xdenote the (degenerate) linear∗1. Observe that P 1 ∈ ext(M (P A1prevision on L (X1), all of whose probability mass lies in x1 )). Then the definition of thestrong product implies that∗{x11}(cid:7)(cid:8)f (x1, ·)= P 2(cid:8)(cid:8)(cid:7)(cid:7)f∗1, ·x= P 2(cid:8)(cid:8)(cid:7)(cid:7)f∗1, ·x= (P 1 × P 2)( f ) (cid:2)(cid:7)P A11× P 2(cid:8)( f ).minx1∈ A1P 2We turn to the second statement. Let P be any factorising product of P A11 and P 2. Consider any gamble f ∈ L (X{1,2}),and let us show that the equalities hold. It suffices to give a proof for non-negative f , since for arbitrary gambles we24 We can identify a gamble on X{N1,N2} in a trivial way with a unique corresponding gamble on XN1∪N2 .G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501945only need to add a non-negative constant [coherence guarantees that we can take additive real numbers out of the lowerprevision operator]. Let a1 and b1 be elements of A1 such that P 2( f (a1, ·)) = minx1∈ A1 P 2( f (x1, ·)) and P 2( f (b1, ·)) =maxx1∈ A1 P 2( f (x1, ·)). Then(cid:8)(cid:8)f − P 2P(cid:7)(cid:7)(cid:18) (cid:3)f (a1, ·)(cid:12)(cid:7)f (x1, ·) − P 2f (a1, ·)I{x1}(cid:20)(cid:8)(cid:13)(cid:12)f (x1, ·) − P 2(cid:7)(cid:8)(cid:13)(cid:8)f (a1, ·)I{x1}x1∈X1(cid:7)(cid:3)P(cid:7)x1∈X1(cid:3)P= P(cid:2)==(cid:12)I{x1}f (x1, ·) − P 2(cid:7)(cid:8)(cid:13)(cid:8)f (a1, ·)(cid:3)+(cid:12)(cid:7)PI{x1}f (x1, ·) − P 2(cid:7)(cid:8)(cid:13)(cid:8)f (a1, ·)x1∈ A1(cid:3)x1∈ A1(cid:12)P 1(I{x1})(cid:7)(cid:8)f (x1, ·)P 2(cid:7)− P 2x1∈ Ac1(cid:8)(cid:13)f (a1, ·)+ 0 (cid:2) 0,and P A22 with respect2 . Because it is coherent, Pwhere the first inequality follows from the coherence of P and the last equality holds because P is assumed to be factorisingand because P 1({x1}) = P 1({x1}) = 0 for every x1 /∈ A1. Hence P ( f ) (cid:2) minx1∈ A1 P 2( f (x1, ·)). Using the conjugacy betweenP and P , we get P ( f ) (cid:3) maxx1∈ A1 P 2( f (x1, ·)).On the other hand,(cid:7)(cid:8)f (b1, ·)P 2= P 1({b1})P 2(cid:7)(cid:8)f (b1, ·)= P(cid:7)(cid:8)I{b1} f (b1, ·)(cid:3) P ( f ) (cid:3) maxx1∈ A1(cid:7)(cid:8)f (x1, ·),P 2where the first equality holds because P 1({b1}) = 1, and the second because P is assumed to be factorising. The firstis non-negative. We conclude that for any gamble f on X{1,2}, P ( f ) = maxx1∈ A1 P 2( f (x1, ·)),inequality holds because fand therefore also P ( f ) = minx1∈ A1 P 2( f (x1, ·)), by conjugacy.We conclude with a proof for the third statement. Assume that P 2 is the vacuous lower prevision P A2to A2, and let P be any coherent lower prevision on L (X{1,2}) with marginals P A11satisfies(cid:7)(cid:8)(cid:7)(cid:8)(cid:7)(cid:8)P ( A1 × A2) = 1 − P(cid:2) 1 − P A1Ac11and therefore it dominates the vacuous lower prevision P A1× A2{1,2}1 and P A2extension P A12 , by the second statement.1× X2 ∪ X1 × Ac22 of P A1⊗ P A2Ac2− P A22Ac1relative to A1 × A2, which also is the independent natural= 1,There are now two possibilities. Either A1 has more than one element. To prove that P is an independent product of P A11and P A22 , we use Corollary 14(ii). Consider any z1 ∈ X1 and any gamble f on X2. If z1 /∈ A1, then we find by inspectionthat P A1× A2{1,2}0 = P A1× A2f − P 2( f ){1,2}(cid:7)(cid:8)I{x1}[max f − min f ](I{z1}[ f − P 2( f )]) = 0, and therefore P (I{z1}[ f − P 2( f )]) = 0. If z1 ∈ A1, then(I{z1}[ f − P 2( f )]) = P A1× A2{1,2}(cid:12)(cid:13)(cid:8)f − P 2( f )= [max f − min f ]P A11I{x1}I{z1}= 0.{x1}(cid:3) P(cid:3) P(cid:13)(cid:8)(cid:8)(cid:7)(cid:7)(cid:7)(cid:12)Here, the first and the last equalities hold because A1 has more than one element.Or A1 has only one element x1, and then Lemma 41 implies that P ( f ) = P A2there is only one coherent lower prevision that has these marginals. But in this case the marginal P A11and Proposition 25 tells us that the marginals P A12 have only one independent product, and it is equal to P . (cid:2)1 and P A22 ( f (x1, ·)) for all gambles f on X1 × X2, sois a linear prevision,{x1}1be the vacuous lower prevision on L (X1) relative to the singleton {x1} ⊆ X1, and let P 2 be any coherent lowerLemma 41. Let Pprevision on L (X2). Let P be any coherent lower prevision with these marginals. Then P is unique and given by P ( f ) = P 2( f (x1, ·))for all gambles f on X1 × X2.Proof. First of all, consider any gamble g on X1 × X2 and any z1 ∈ X1 \ {x1}. We show that P (I{z1} g) = P (I{z1} g) = 0.Indeed, by coherence of P :{x1}1(cid:7)0 = P(I{z1}) min g(z1, ·) = P(cid:8)I{z1} g(z1, ·)(cid:3) P= P (I{z1} g)(cid:7)(cid:8)I{z1} g(z1, ·)(cid:3) P (I{z1} g) = P(cid:3) P(I{z1}) max g(z1, ·) = P{x1}1{x1}1(I{z1}) min g(z1, ·){x1}1(I{z1}) max g(z1, ·) = 0.1946G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950For any gamble f on X1 × X2 we then infer from the coherence of P thatP (I{x1} f ) = P (I{x1} f ) +(cid:3)P (I{z1} f ) (cid:3) P(cid:18) (cid:3)(cid:20)I{z1} f= P ( f ) (cid:3) P (I{x1} f ) +z1∈X1\{x1}z1∈X1= P (I{x1} f ).But this tells us that indeedP ( f ) = P (I{x1} f ) = P(cid:7)(cid:8)I{x1} f (x1, ·)(cid:7)(cid:8)f (x1, ·)= P(cid:7)(cid:8)f (x1, ·).= P 2(cid:2)(cid:3)z1∈X1\{x1}P (I{z1} f )Proof of Proposition 27. We begin with the strong product. Because of its marginalisation and associativity properties[Proposition 8], it clearly suffices to consider the case N = {1, 2}, and to show that S{1,2} is externally additive. So considerarbitrary f 1 in L (X1) and f 2 in L (X2), then indeed:(cid:7)(cid:8)(cid:9)(cid:6)(cid:6)S N ( f 1 + f 2) = inf= inf= inf= P 1( f 1) + P 2( f 2).(P 1 × P 2)( f 1 + f 2): P 1 ∈ extP 1( f 1) + P 2( f 2): P 1 ∈ ext(cid:8)(cid:9)P 1( f 1): P 1 ∈ extM (P 1)(cid:6)(cid:7)(cid:7)(cid:8)M (P 1)(cid:8)M (P 1)(cid:6)+ inf(cid:7)M (P 2)(cid:8)(cid:9)and P 2 ∈ ext(cid:7)M (P 2)and P 2 ∈ ext(cid:7)P 2( f 2): P 2 ∈ extM (P 2)(cid:8)(cid:9)We finish by considering the independent natural extension. Here too, because of its marginalisation and associativityproperties [Theorems 18 and 23], it clearly suffices to consider the case N = {1, 2}, and to show that E{1,2} is externallyadditive. So consider arbitrary f 1 in L (X1) and f 2 in L (X2). Since we know that E{1,2} is dominated by S{1,2}, we seethat E{1,2}( f 1 + f 2) (cid:3) S{1,2}( f 1 + f 2) = P 1( f 1) + P 2( f 2). To prove the converse inequality, it suffices to use the coherence ofE{1,2} to deduce thatE{1,2}( f 1 + f 2) (cid:2) E{1,2}( f 1) + E{1,2}( f 2) = P 1( f 1) + P 2( f 2),where the equality follows from Corollary 15. (cid:2)Proof of Theorem 28. We use Corollary 14(ii). Consider arbitrary o ∈ N, I ⊆ N \ {o}, xI ∈ XI and g ∈ L (Xo). Then, sinceP N is factorising and has Xo-marginal P o:(cid:7)(cid:8)(cid:8)(cid:13)(cid:8)(cid:7)(cid:12)P NI{xI }g − P o(g)= P Ng − P o(g)= P N (0) = 0.(cid:2)(cid:7)I{xI } P NProof of Theorem 29. By Theorem 1, P N is weakly coherent with the family I (P N ) if and only if it is pairwise coherentwith each of its members. Let us therefore establish the coherence of P N with each conditional lower prevision P O (·| X I )(taken separately, for each pair of disjoint subsets I and O of N). Again using Theorem 1, we see we have to show that(cid:12)(cid:7)I{xI }P N(cid:13)(cid:8)f − P O ( f |xI )= 0 for all f ∈ L (XO ) and all xI ∈ XI .We see that indeed:(cid:7)(cid:12)(cid:13)(cid:8)(cid:7)(cid:12)(cid:13)(cid:8)(cid:7)(cid:7)(cid:8)(cid:8)P NI{xI }f − P O ( f |xI )= P Nwhere the first equality follows from the definition of the conditional lower prevision P O (·| X I ), and the second one fromthe strongly factorising character of P N . (cid:2)= P N (0) = 0,f − P N ( f )f − P N ( f )I{xI } P N= P NI{xI }Proof of Proposition 31. To prove (i), we use Corollary 14(ii). Observe that Q 1, Q 2 and Q 3 all have the same marginals P n.Consider arbitrary o ∈ N, I ⊆ N \ {o}, xI ∈ XI and g ∈ L (Xo). Then it follows from the inequalities Q 1 (cid:3) Q 3 (cid:3) Q 2 that0 = Q 1(cid:12)(cid:7)I{xI }(cid:13)(cid:8)g − P o(g)(cid:3) Q 3(cid:12)(cid:7)I{xI }(cid:13)(cid:8)g − P o(g)(cid:3) Q 2(cid:12)(cid:7)I{xI }(cid:13)(cid:8)g − P o(g)= 0,where the equalities hold because Q 1 and Q 2 are many-to-one independent products. We deduce that Q 3 is a many-to-oneindependent product too.To prove (ii), consider arbitrary o ∈ N, I ⊆ N \ {o}, fo ∈ L (Xo) and non-negative f i ∈ L (Xi), i ∈ N \ {o}. Let, for ease ofi∈I P i( f i) = Q 2( f I )i∈I f i . Since Q 1 and Q 2 are factorising, we deduce from Proposition 5 that Q 1( f I ) =i∈I P i( f i) = Q 2( f I ), whence Q 3( f I ) =i∈I P i( f i). Moreover,i∈I P i( f i) and Q 3( f I ) =notation f I :=and Q 1( f I ) =(cid:21)(cid:21)(cid:21)(cid:21)(cid:21)Q 1( f I ) (cid:10) P o( fo) = Q 1( fo f I ) (cid:3) Q 3( fo f I ) (cid:3) Q 2( fo f I ) = Q 2( f I ) (cid:10) P o( fo).Hence Q 3( fo f I ) = Q 3( f I ) (cid:10) P o( fo) = Q 3( f I P o( fo)). Applying Proposition 5 again, we deduce that Q 3 is also factorising.G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501947The proof of (iii) is similar to that of (ii). Finally, to prove (iv) use that the inequality Q 3(r∈R P r( fr) followsfrom the super-additivity of the coherent lower prevision Q 3, and that the converse inequality follows from Q 3 (cid:3) Q 2. (cid:2)(cid:5)(cid:5)r∈R fr) (cid:2)Proof of Proposition 32. (i) ⇒ (ii). Assume that P N is weakly coherent with the family I (P N ). Consider any disjoint propersubsets I and O of N, g ∈ L (XO ) and non-negative f ∈ L (XI ). By Theorem 1, it follows from the assumption that P N iscoherent with P O ∪I (·| X I ), and this implies the equality P N (I{xI }[g − P N (g)]) = 0 for every xI ∈ XI . The coherence [super-additivity] of P N and the non-negativity of f then imply that:(cid:3)(cid:13)(cid:8)(cid:13)(cid:8)(cid:7)(cid:7)(cid:12)(cid:12)P Nfg − P N (g)(cid:2)I{xI }g − P N (g)= 0.f (xI )P NxI ∈XIHence P N is productive.(ii) ⇒ (i). Assume P N is productive. Consider any disjoint proper subsets I and O of N, g ∈ L (XO ) and non-negativef ∈ L (XI ). The assumption implies in particular that P N (I{xI }[g − P N (g)]) (cid:2) 0 for all xI ∈ XI and all g ∈ L (XO ). If therewere some xI ∈ XI such that P N (I{xI }[g − P N (g)]) > 0, then the coherence of P N would imply that0 = P N(cid:7)(cid:8)g − P N (g)= P N(cid:18) (cid:3)(cid:12)I{xI }(cid:20)(cid:13)g − P N (g)(cid:3)(cid:2)(cid:12)(cid:7)I{xI }P N(cid:13)(cid:8)g − P N (g)> 0,xI ∈XIxI ∈XIa contradiction. So we infer from Theorem 1 that P N is weakly coherent with the family I (P N ).To complete the proof, if P N is many-to-many independent then it is in strongly, and therefore also weakly, coherentwith the family I (P N ), and therefore productive as well. On the other hand, the first part of this proposition, togetherwith Corollary 14, shows that if P N is productive it is in particular many-to-one independent. Finally, the last statement isa consequence of Ref. [22, Theorem 11], which shows that all the conditioning events have positive lower probability thenweak and strong coherence are equivalent. (cid:2)Appendix B. CounterexamplesIn this appendix, we have gathered a few examples with additional information on the notions introduced in this paper.Example 2 (Factorisation properties are not preserved by taking lower envelopes). Let N := {1, 2} and X1 := X2 := {0, 1}. Con-2 , Q 1({1}) := 1 and Q 1({0}) := 0. Similarly,sider the linear marginals P 1 and Q 1 for X1 defined by P 1({0}) := P 1({1}) := 1consider the linear marginals P 2 and Q 2 for X2 defined by P 2({0}) := P 2({1}) := 12 , Q 2({1}) := 1 and Q 2({0}) := 0.Let P {1,2} be the product of P 1 and P 2, and let Q {1,2} be the product of Q 1 and Q 2. It follows from Proposition 7 thatP {1,2} and Q {1,2} are factorising and therefore strongly factorising [for N = {1, 2}, these notions are equivalent].Now let P {1,2} be the lower envelope of P {1,2} and Q {1,2}. Consider the gamble f := I{0} on X1 and the gamble g :=I{0} − I{1} on X2. Then P 1( f ) = 12 , Q 1( f ) = 0, P 2(g) = 0 and Q 2(g) = −1, and therefore(cid:6)P {1,2}( f g) = min(cid:9)P {1,2}( f g), Q {1,2}( f g)= min· 0, 0 · −1= min{0, 0} = 0,(cid:19)(cid:17)12while P {1,2}(g) = min{P 2(g), Q 2(g)} = min{0, −1} = −1, and(cid:7)(cid:8)f P {1,2}(g)(cid:6)= P {1,2}(− f ) = min(cid:9)P 1(− f ), Q 1(− f )P {1,2}= min(cid:19)(cid:17)− 12, 0= − 12.Hence P {1,2} is not (strongly) factorising.Example 3 (The strong product is not the greatest independent product; many-to-many independent (cid:2) factorising; many-to-manyindependent (cid:2) externally additive; productive (cid:2) strongly factorising). Consider the possibility spaces X1 = X2 = {0, 1}, andlet P 1 and P 2 be the vacuous lower previsions on L (X1) and L (X2), respectively. From the second statement in Propo-sition 26, the strong product S{1,2} := P 1 × P 2 is the vacuous lower prevision on L (X{1,2}), and it coincides with theindependent natural extension E {1,2} := P 1 ⊗ P 2.Let Q {1,2} be the vacuous lower prevision relative to {(0, 0), (1, 1)}. This lower prevision strictly dominates the strongproduct S{1,2}: we have for instance that(cid:7)(cid:6)(cid:9)(cid:8)Q {1,2}(0, 0), (1, 1)= 1 > 0 = S{1,2}(cid:7)(cid:6)(cid:9)(cid:8)(0, 0), (1, 1).Yet Q {1,2} is a many-to-one independent product of the marginals P 1 and P 2. [Since N = {1, 2} it is then also many-to-manyindependent.] To prove this, use the third statement in Proposition 26. Applying Proposition 32, it is productive too.The lower prevision Q {1,2} is not factorising. To see this, consider the non-negative gambles f := I{1} + I{0,1} on X1 andg := I{0} + I{0,1} on X2. Then Q {1,2}( f g) = min{2 · 1, 1 · 2} = 2, whereas Q {1,2}( f )Q {1,2}(g) = 1 · 1 = 1. As a consequence, it is1948G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950not strongly factorising either. On the other hand, if we consider the gambles f 1 := I{0} on X1 and f 2 := I{1} on X2 we seethat f 1 + f 2 (cid:2) I{(0,0),(1,1)} and therefore Q {1,2}( f 1 + f 2) = 1 > 0 = P 1( f 1) + P 2( f 2). This shows that not every many-to-manyindependent product is externally additive.Example 4 (Externally additive (cid:2) strongly externally additive; factorising (cid:2) strongly factorising). Let N := {1, 2, 3} and considerthe binary variables X1, X2 and X3 assuming values in X1 = X2 = X3 = {0, 1}. Consider the corresponding marginal lowerprevisions P 1, P 2 and P 3 given byP j( f j) := 12f j(0) + 25f j(1) + 110(cid:6)min(cid:9)f j(0), f j(1)for f j ∈ L (X j) and j = 1, 2, 3.Walley [30, Example 9.3.4] has shown that the independent natural extension E{1,2} of P 1 and P 2 is the lower envelope ofthe set of linear previsions P k with mass functions pk, k = 1, . . . , 6 given byk123456pk(1, 1)pk(1, 0)pk(0, 1)pk(0, 0)141515425211291415310625311291431015625311291431031092531113On the other hand, the strong product S{1,2} of P 1 and P 2 is the lower envelope of the set of linear previsionsLet P 7 and P 8 be the linear previsions on X3 whose respective mass functions p7 and p8 are determined by p7(1) = 252 , so P 3 is the lower envelope of P 7 and P 8. Let Q {1,2,3} be the lower envelope of the following set of linear{P 1, P 2, P 3, P 4}.and p8(1) = 1previsions on L (X{1,2,3}):{P 1 × P 7, P 2 × P 7, P 3 × P 7, P 4 × P 7, P 1 × P 8, P 2 × P 8, P 3 × P 8, P 4 × P 8, P 5 × P 8}.Then Q {1,2,3} = min{S{1,2,3}, P 5 × P 8}, where S{1,2,3} is the strong product of P 1, P 2 and P 3: because of the associativityof the strong product, established in Proposition 8, the strong product S{1,2,3} is the lower envelope of the set of linearprevisions:{P 1 × P 7, P 2 × P 7, P 3 × P 7, P 4 × P 7, P 1 × P 8, P 2 × P 8, P 3 × P 8, P 4 × P 8}.To see that Q {1,2,3} is not strongly externally additive, let f be the indicator of the set {(0, 0), (1, 1)} × X3 and let g bethe indicator of the set X{1,2} × {1}. ThenQ {1,2,3}( f ) + Q {1,2,3}(g) = P 5(cid:7)(cid:6)(cid:9)(cid:8)(0, 0), (1, 1)(cid:8)(cid:7){1}+ P 7= 511+ 25= 4755,whereasQ {1,2,3}( f + g) = min= min= min(cid:6)(cid:6)(cid:9)S{1,2,3}( f + g), P 5 × P 8( f + g)S{1,2,3}( f + g), P 5(cid:19)(cid:17)51(0, 0), (1, 1)47(cid:7)(cid:6)+ 25,211+ 12= 910>.55(cid:9)(cid:8)(cid:8)(cid:9)(cid:7){1}+ P 8Let us show that Q {1,2,3} is not productive, from which it follows, taking into account Proposition 6, that it is not stronglyfactorising either. Let f be minus the indicator of the set {1} × X{2,3} and g minus the indicator of the set X1 × {1} × X3.Then it follows that(cid:6)Q {1,2,3}(g) = min(cid:6)= min(cid:7)(cid:9)S{1,2,3}(g), P 5 × P 8(g)−S{1,2}(cid:17)− 12X1 × {1}= − 12, − 511(cid:8), −P 5(cid:19),= min(cid:7)X1 × {1}(cid:8)(cid:9)and as a consequenceG. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–19501949(cid:7)(cid:12)Q {1,2,3}fg − Q {1,2,3}(g)(cid:13)(cid:8)(cid:12)(cid:7)f(cid:9)(cid:8)(cid:3) P 5 × P 8(cid:7)(cid:6)= P 5(1, 1)= 211− 12511(cid:13)(cid:8)(cid:8){1} × X2g − Q {1,2,3}(g)(cid:7)− 12= − 122< 0.P 5We now show that, nevertheless, Q {1,2,3} is externally additive. For this, use that Q {1,2,3} is dominated by S{1,2,3} andthat both these coherent lower previsions have P 1, P 2 and P 3 as their marginals, and apply Proposition 31(iv).Now, the associativity of the independent natural extension [Theorem 23] and its being dominated by the strong productimply thatE{1,2,3} (cid:3) E{1,2} ⊗ P 3 (cid:3) E{1,2} × P 3 = min{P i × P j: i = 1, . . . , 6, j = 7, 8} (cid:3) Q {1,2,3}.Applying Proposition 31(i), we deduce that Q {1,2,3} is a many-to-one independent product, and from Proposition 31(ii) weinfer that it is factorising.Example 5 (Factorising (cid:2) Kuznetsov; strongly factorising (cid:2) strongly Kuznetsov). Consider two binary variables X1, X2 assumingvalues in the set X1 = X2 = {0, 1}. Let P 1, P 2 be the marginal lower previsions from Example 4. Consider the gambles f :=I{0} − I{1} on X1 and g := I{0} − I{1} on X2. Then P 1( f ) = P 2(g) = 0 and P 1( f ) = P 2(g) = 15 . As a consequence, P 1( f ) (cid:4)], whereas, considering the linear previsions P 1, . . . , P 6 in that example, we see that their independent naturalP 2(g) = [0, 1extension provides the following value for the lower bound:25(cid:17)E{1,2}( f g) = min0, 0, 0,125, − 111,19(cid:19)= − 111.This shows that the independent natural extension E{1,2}, which is factorising by Theorem 22, is not Kuznetsov. Moreover,in this example where N = {1, 2}, factorisation is equivalent to strong factorisation, and being Kuznetsov is equivalent tobeing strongly Kuznetsov.Example 6 (Many-to-one independent (cid:2) many-to-many independent; factorising (cid:2) strongly factorising). Let N := {1, 2, 3} andconsider the binary variables X1, X2 and X3 assuming values in X1 = X2 = X3 = {0, 1}, and consider the correspondingmarginal lower previsions P 1, P 2 and P 3 given byf j(0) + 25P j( f j) := 12(cid:9)f j(0), f j(1)for all f j ∈ L (X j)f j(1) + 110(cid:6)minfor j = 1, 2, 3. Let E{1,2,3} denote their independent natural extension and S{1,2,3} their strong product.Define the coherent lower prevision Q {1,2,3} on L (X{1,2,3}) as the convex mixture Q {1,2,3} := 12 (E{1,2,3} + S{1,2,3}). Itfollows from Proposition 31 that Q {1,2,3} is factorising, and a many-to-one independent product. We are going to prove thatQ {1,2,3} is not a many-to-many independent product. It will then follow from Theorem 29 that it is not strongly factorisingeither.Consider the conditional lower prevision Q {1,2,3}(·| X3) derived from the joint lower prevision Q {1,2,3} using the epis-temic irrelevance of X3 to X{1,2}:(cid:7)Q {1,2,3}( f |x3) := Q {1,2,3}(cid:8)f (·, x3)for all x3 ∈ X3 and all f ∈ L (X{1,2,3}).In order to show that it Q {1,2,3} is not a many-to-many independent product, it suffices to show that it is not weaklycoherent with this conditional lower prevision Q {1,2,3}(·| X3). Consider the event A := {(0, 0), (1, 1)} that X1 = X2, and thecorresponding indicator g := I A on X{1,2}. It follows from Ref. [30, Example 9.3.4] that E{1,2,3}( A) = 511 and S{1,2,3}( A) = 12 ,so(cid:7)(cid:8)E{1,2,3}( A) + S{1,2,3}( A)Q {1,2,3}( A) = 12+ 511Let x3 = 0. Since both E{1,2,3} and S{1,2,3} are strongly factorising [by Theorem 24 and Proposition 8(iv), respectively], wesee that= 2144= 1221.(cid:18)(cid:20)g − Q {1,2,3}(g)(cid:13)(cid:8)(cid:8)(cid:7){0}= P 3(cid:7)(cid:8)g − Q {1,2,3}(g)E{1,2,3}= 35511− 2144= − 3220,(cid:18)(cid:20)(cid:7)I{x3}E{1,2,3}whereas(cid:7)I{x3}S{1,2,3}(cid:12)(cid:12)g − Q {1,2,3}(g)(cid:13)(cid:8)(cid:8)(cid:7){0}= P 3(cid:7)(cid:8)g − Q {1,2,3}(g)S{1,2,3}(cid:20)(cid:18)= 1212− 2144= 188.1950G. de Cooman et al. / Artificial Intelligence 175 (2011) 1911–1950As a consequence, we deduce thatQ {1,2,3}(cid:12)(cid:7)I{x3}g − Q {1,2,3}(g)(cid:13)(cid:8)(cid:18)= 12− 3220+ 188(cid:20)= − 1880< 0,so Q {1,2,3} is not coherent with Q {1,2,3}(·| X3). This also shows that Q {1,2,3} is not productive, applying Proposition 32.Finally, note that, from Example 5 we can deduce that the independent natural extension E {1,2,3} is not Kuznetsov, andsince Q {1,2,3} has the same marginals we deduce that it is not Kuznetsov either: it suffices to take the same gambles f andg from that example.This example shows that if we consider a many-to-one independent product Q N of some given marginals P n, n ∈ N, anda partition of N given by sets R and S, then Q N need not be coherent with the conditional lower previsions Q R∪S (·| X S ) andQ R∪S (·| X R ). In this sense the associativity properties satisfied by the strong product and the independent natural extensiondo not extend towards their convex combinations.References[1] Giulanella Coletti, Romano Scozzafava, Probabilistic Logic in a Coherent Setting, Kluwer, 2002.[2] Inés Couso, Moral Serafín, Peter Walley, A survey of concepts of independence for imprecise probabilities, Risk Decision and Policy 5 (2000) 165–181.[3] Fabio G. Cozman, Constructing sets of probability measures through Kuznetsov’s independence condition, in: G. de Cooman, T.L. Fine, T. Seidenfeld(Eds.), ISIPTA ’01 – Proceedings of the Second International Symposium on Imprecise Probabilities and Their Applications, Shaker Publishing, Maastricht,2000, pp. 104–111.[4] Fabio G. Cozman, Credal networks, Artificial Intelligence 120 (2000) 199–233.[5] Fabio G. Cozman, Cassio P. de Campos, José C. Ferreira da Rocha, Probabilistic logic with independence, International Journal of Approximate Reason-ing 49 (1) (2008) 3–17.[6] Fabio G. Cozman, Peter Walley, Graphoid properties of epistemic irrelevance and independence, Annals of Mathematics and Artificial Intelligence 45 (1–2) (2005) 173–195.[7] Jasper De Bock, Gert de Cooman, State sequence prediction in imprecise hidden Markov models, in: ISIPTA ’11 – Proceedings of the Seventh Interna-tional Symposium on Imprecise Probability: Theories and Applications, 2011, accepted for publication.[8] Cassio P. de Campos, New complexity results for MAP in Bayesian networks, in: Proceedings of the International Joint Conference on Artificial Intelli-gence, 2011, accepted for publication.[9] Cassio P. de Campos, Fabio G. Cozman, The inferential complexity of Bayesian and credal networks, in: Proceedings of the International Joint Conferenceon Artificial Intelligence, Edinburgh 2005, pp. 1313–1318.[10] Cassio P. de Campos, Fabio G. Cozman, Computing lower and upper expectations under epistemic independence, International Journal of ApproximateReasoning 44 (3) (2007) 244–260.[11] Gert de Cooman, Filip Hermans, Alessandro Antonucci, Marco Zaffalon, Epistemic irrelevance in credal nets: the case of imprecise Markov trees,International Journal of Approximate Reasoning 51 (9) (2010) 1029–1052.[12] Gert de Cooman, Enrique Miranda, Weak and strong laws of large numbers for coherent lower previsions, Journal of Statistical Planning and Infer-ence 138 (8) (2008) 2409–2432.[13] Gert de Cooman, Enrique Miranda, Forward irrelevance, Journal of Statistical Planning and Inference 139 (2) (2009) 256–276.[14] Gert de Cooman, Enrique Miranda, Marco Zaffalon, Factorisation properties of the strong product, in: Combining Soft Computing and Statistical Meth-ods in Data Analysis, Proceedings of the 5th International Conference on Soft Methods in Probability and Statistics, SMPS 2010, Oviedo and Mieres(Asturias), Spain, September 28–October 1, 2010, in: Advances in Soft Computing, vol. 77, Springer, 2010, pp. 139–147.[15] Gert de Cooman, Enrique Miranda, Marco Zaffalon, Independent natural extension, in: Computational Intelligence for Knowledge-Based Systems Design,13th International Conference on Information Processing and Management of Uncertainty, IPMU 2010, Dortmund, Germany, June 28–July 2, 2010.Proceedings, in: Lecture Notes in Computer Science, vol. 6178, Springer, 2010, pp. 737–746.[16] Bruno de Finetti, Teoria delle Probabilità, Einaudi, Turin, 1970.[17] Bruno de Finetti, Theory of Probability: A Critical Introductory Treatment, John Wiley & Sons, Chichester, 1974–1975 (English translation of [16], twovolumes).[18] Richard Holmes, Geometric Functional Analysis and Its Applications, Springer-Verlag, New York, 1975.[19] Vladimir Kuznetsov, Interval Statistical Methods, Radio i Svyaz Publ., 1991 (in Russian). Downloadable at http://www.sipta.org/index.php?id=res#kuz.[20] Steffen L. Lauritzen, David J. Spiegelhalter, Local computations with probabilities on graphical structures and their application to expert systems (withdiscussion), Journal of the Royal Statistical Society, Series B 50 (1988) 157–224.[21] Enrique Miranda, A survey of the theory of coherent lower previsions, International Journal of Approximate Reasoning 48 (2) (2008) 628–658.[22] Enrique Miranda, Updating coherent lower previsions on finite spaces, Fuzzy Sets and Systems 160 (9) (2009) 1286–1307.[23] Enrique Miranda, Gert de Cooman, Marginal extension in the theory of coherent lower previsions, International Journal of Approximate Reason-ing 46 (1) (2007) 188–225.[24] Enrique Miranda, Marco Zaffalon, Coherence graphs, Artificial Intelligence 173 (2009) 104–144.[25] Enrique Miranda, Marco Zaffalon, Natural extension as a limit of regular extensions, Journal of Statistical Planning and Inference 140 (7) (2010) 1805–1833.[26] Moral Serafín, Epistemic irrelevance on sets of desirable gambles, Annals of Mathematics and Artificial Intelligence 45 (2005) 197–214.[27] Judea Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, San Mateo, CA, 1988.[28] Lawrence R. Rabiner, A tutorial on HMM and selected applications in speech recognition, Proceedings of the IEEE 77 (2) (February 1989) 257–286.[29] Paolo Vicig, Epistemic independence for imprecise probabilities, International Journal of Approximate Reasoning 24 (3) (2000) 235–250.[30] Peter Walley, Statistical Reasoning with Imprecise Probabilities, Chapman and Hall, London, 1991.[31] Peter Walley, Renato Pelessoni, Paolo Vicig, Direct algorithms for checking consistency and making inferences from conditional probability assessments,Journal of Statistical Planning and Inference 126 (2004) 119–151.[32] Peter M. Williams, Notes on conditional previsions, Technical report, School of Mathematical and Physical Science, University of Sussex, UK, 1975.Revised journal version: [33].[33] Peter M. Williams, Notes on conditional previsions, International Journal of Approximate Reasoning 44 (2007) 366–383. Revised journal version of [32].