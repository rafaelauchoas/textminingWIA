3202beF32]PS.ssee[6v80231.1112:viXraEvaluation of Interpretability for Deep Learning algorithmsin EEG Emotion Recognition: A case study in AutismJuan Manuel Mayor-Torres, Sara Medina-DeVilliers, Tessa Clarkson, Matthew D. Lerner, and GiuseppeRiccardi Member and Fellow, IEEE1Abstract—Current models on Explainable Artificial Intelligence (XAI) have shown an evident and quantified lack of reliability formeasuring feature-relevance when statistically entangled features are proposed for training deep classifiers. There has been anincrease in the application of Deep Learning in clinical trials to predict early diagnosis of neuro-developmental disorders, such asAutism Spectrum Disorder (ASD). However, the inclusion of more reliable saliency-maps to obtain more trustworthy and interpretablemetrics using neural activity features is still insufficiently mature for practical applications in diagnostics or clinical trials. Moreover, inASD research the inclusion of deep classifiers that use neural measures to predict viewed facial emotions is relatively unexplored.Therefore, in this study we propose the evaluation of a Convolutional Neural Network (CNN) for electroencephalography (EEG)-basedfacial emotion recognition decoding complemented with a novel RemOve-And-Retrain (ROAR) methodology to recover highly relevantfeatures used in the classifier. Specifically, we compare well-known relevance maps such as Layer-Wise Relevance Propagation (LRP),PatternNet, Pattern-Attribution, and Smooth-Grad Squared. This study is the first to consolidate a more transparent feature-relevancecalculation for a successful EEG-based facial emotion recognition using a within-subject-trained CNN in typically-developed and ASDindividuals.Index Terms—Convolutional Neural Networks (CNN), Explainable AI (XAI), re-training, RemOve-And-Retrain (ROAR),Electroencephalography (EEG), Autism Spectrum Disorder, Autism, XAI methods, Emotion Recognition(cid:70)1 INTRODUCTIOND EEP Learning (DL) has led to large improvements inmany domains: e.g. image recognition [1], automatedtranslation [2], and object detection [3]. In recent years, re-searchers have started to investigate DL for clinical applica-tion. For example: the use of DL for the diagnosis of complexneurodevelopmental disorders, such as, Parkinson [4], Rett[5], or Alzheimer [6]. Other uses include facial emotionrecognition (FER) in typically-developed (TD) individuals[7], [8], [9], [10] and individuals with ASD [11]. Despite this,most research on processing electroencephalography (EEG)data still relies on more traditional machine approaches suchas Support Vector Machine (SVM) and Linear DiscriminantAnalysis (LDA). This includes the work on ASD diagnoses[12], [13] and FER [14], [15], [16], [17]. These methods werein multiple applications such as motorquite successfulimagery decoding [18] and artifact removal [19]. However,their accuracy is limited for more complex applications suchas FER decoding.Traditional classifiers necessitate explicit feature extrac-tion prior to classification in order to achieve high accuracy[20], [21], [22]. In the aforementioned case of motor imagerydecoding, features were generated using Common-Spatial••Juan Manuel Mayor-Torres** and Giuseppe Riccardi are with theDepartmentInformation Engineering and Computer Science,University of Trento, Via Sommarive, Povo, Trento, 1328, Italy. E-mail:juan.mayortorres@unitn.it, giuseppe.riccardi@unitn.itofSara Medina-Devilliers and Matthew D. Lerner are with the Departmentof Psychology, StonyBrook University NY, USA, E-mail sara.medina-devilliers@stonybrook.edu, matthew.lerner@stonybrook.edu• Tessa Clarkson is with the Department of Psychology, Temple University,Philadelphia, PA email: tessa.clarkson@temple.eduPatterns (CSP) which is directly linked to neurophysio-logical processes [23]. Unfortunately, in the case of FERdecoding relevant features are not known or directly linkedto neurophysiological data [24], [25], [26]. This limitationmotivates use of DL, such as CNNs, to process minimallypreprocessed EEG data and recover features [27], [28].To our knowledge, our previous study is the first suc-cessful application of DL classifiers on neural activity to de-code facial emotions in ASD populations [11]. In this study,we constructed a 2D image from the EEG as an input bystacking the individual EEG channels vertically - columnsrepresent time and rows represent different EEG channelsso that a single EEG image can be processed directly by aCNN [11], [28]. One could think of these EEG images asscribble drawings made by a child who is learning to draw.Each scribble drawing represents a different encodedfacial emotion in the neural activity or a single-trial. Inother words, the CNN would need to find the best classseparability using the features that are specific to eachscribble drawing or facial emotion. What makes this taskparticularly difficult is that the representations are highlynon-deterministic and noisy, just as the scribble drawingsfrom a child. Consequently, understanding which featureswere used to classify facial emotions for typically developed(TD) controls or ASD participants performing FER [11], [29]is difficult. This contributes to the CNN being an inaccessi-ble black-box system, which can not be explained directlyfrom the neural features or the DL models we commonlyuse [30].To alleviate this problem, Explainable AI (XAI) methods(i.e, saliency-maps) have been introduced recently as a wayto understand DL classifiers. Some well-know XAI methods,such as Grad-CAM [31], Grad-CAM++ [32], Integrated Gra-   dients (IG) [33], and Smooth-Grad [34], were developed forimage classification and semantic segmentation [3], [25]. Itis not clear how applicable these methods are to decodingemotions from EEG images [35], [36]. A key differencebetween image classification and EEG image decoding is,as mentioned before, the presence of noise. For these rea-sons it is not clear which XAI methods are best suited forrecovering features of the decoded EEG image.In this study, we utilize a CNN to classify emotions FERusing neural activity in the form of an EEG image. Ourprevious work demonstrated that a CNN was able to decodeneural activity during an FER task in TD and ASD indi-viduals, suggesting intact FER encoding. The discrepancybetween encoded FER and behavioral performance on theFER task in ASD suggests that impairments arise as a resultof problems deploying properly encoding facial emotioninformation into behavioral responses [11], [29]. To betterunderstand when and how FER is encoded from the neuralactivity in TD and ASD, we can use XAI methods to recoverrelevant features.To recover what EEG features are necessary to obtainan accurate facial emotion classification, we must first un-derstand which XAI methods are reliable. With this goal,we analyze the following methods: Layer-Wise RelevancePropagation (LRP) [37], [38], PatternNet, Pattern-Attribution[39], and Smooth-Grad Squared [40] using an approachcalled RemOve-And Retrain (ROAR) [41]. ROAR works bysystematically removing features, indicated to be informa-tive according to the XAI methods, one a time from the CNNand obtaining classifier accuracies without that features. Ifafter their feature-removal the classifier cannot obtain a highcategorization accuracy, then the feature identified by XAImethods is indeed informative, reliable and necessary fordecoding. This approach allows us to recover which XAImethods are definitely reliable for classifying correct facialemotion using features from neural activity.This study is the first to evaluate reliable XAI methodsusing ROAR, including EEG data from TD and ASD groups,and comparing the final CNN-FER metrics with the metricsassociated with the FER (behavioral performance) task.This paper will be structured as follows: (1) In thefirst part of the methods section, we will describe thedemographics of the TD and ASD samples, the EEG pre-processing methods used for artifact-removal and signalprocessing, and the CNN architecture and training. (2) Inthe second part of the methods section, we will describeROAR and discuss the XAI methods we use in the ROARevaluation. (3) In the results section, we will report the com-parisons between the different XAI methods, and (4) finally,we will discuss our conclusions in the context of ASD, andMachine Learning (ML) research. This study will provideimportant contributions for evaluating XAI methods usingROAR, and its application for decoding FER in individualswith and without ASD [42], [43].2 MATERIALS AND METHODSIn the first subsection, we describe the participant samplesand the corresponding demographics. In the second sub-section, we describe that the FER task is completed whileundergoing EEG. In the third subsection, we describe theEEG data pre-processing, artifact removal, and whitening2using Zero-Component-Analysis (ZCA) procedures imple-mented prior to training the CNN. In the fourth subsection,we outline the ROAR methodology and the XAI methodsfor feature evaluation via feature-relevance calculation.2.1 ParticipantsEighty-eight participants (Age: 15.34±1.58 years), were in-cluded in the following analyses and taken from a largerstudy on emotional and social processing [11], [44]. Ofthese, 48 participants (29 male; Age: 15.39±1.55 years) wereTD, and 40 participants (32 male; Age: 14.77±2.16 years)had a diagnosis of ASD. All participants with an ASDdiagnosis were confirmed using the Autism DiagnosticObservation Schedule, Version 2.0 (ADOS-2; ASD severityComparison Score ADOS-CS: TD 3.33±2.71, ASD 8.15±2.05)[45] and were considered high-functioning on the Kauf-man Brief Intelligence Test-2 (KBIT-2). ASD participantshad significantly elevated ADOS-CS compared to the TDgroup (p=0.038), but there was no significant difference inintellectual functioning (p=0.227).2.2 Face Emotion Recognition (FER)Participants completed a web-based FER task while under-going EEG [29], [46]. During this task; participants viewedemotional facial expressions from 48 children and adults’face photographs. These facial images were taken fromthe DANVA-2 image-set [44]. We presented the emotionalfaces randomly with a cross-fixation of 200 ms, and a 2-second length face presentation followed by an emotionlabeling menu. The emotion labeling menu presented theface-photograph again with four basic emotion labels on thebottom (i.e., happy, sad, anger, and fear).From the DANVA-2 image-set, 24 faces showed adultsportraying a particular emotion, and the other 24 showedchildren portraying the same emotion, resulting in sixstimuli for each emotion and for each modality (i.e, chil-dren/adult). Each EEG trial is associated with a single facialemotion stimulus presented per subject. For this study, weconsider the performances obtained from this task as ahuman behavioral performance quota - see section 2.6 fora more detailed description of these metrics. Each partic-ipant’s behavioral performance was calculated after all 48faces were presented. The behavioral performance metricsinclude error-rates, accuracies, and reaction-times per par-ticipant.EEG trials were sorted by facial emotion, combined andstored as a single EEGlab [47] structure for each emotion,resulting in 4 EEGlab structures (happy, sad, anger, and fear)per participant, comprising 12 EEG trials of both adults andchildren stimuli.2.3 EEG recordingsEEG neural activity was recorded using a Brain Products 32-channels Brain-Vision ActiCHAmp device with an originalsample rate at 1KHz. Each 2D EEG image was then com-posed of 752 points on the time-domain and 30 channels inthe spatial domain. EEG data were digitized at 16-bit reso-lution. The raw EEG signal was filtered with a notch filterat 60Hz with a half-power cut-off of 12db/Oct. Each activeelectrode was measured online with respect to a commonmode-sense active electrode producing a monopolar (non-differential) channel. The EEG data collection procedures ofthis study adhered to best practices for EEG data collectionin ASD [48].EEG trials were segmented between -200-1500ms anddownsampled to 500Hz to avoid classifier overfitting [18],[26]. Each EEG trial was re-referenced to the T9-T10 bilateralreference [48]. After the re-reference process, we used theremaining 30 channels (i.e., FT9, F7, FC5, FP1, FZ, FP2, F4,F8, FC6, FT10, F4, F3, FC1, C3, FC1, FC2, C4, T7, CP5, CP1,Cz, CP2, P4, P8, CP6, T8, P7, P3, Pz, O1, O2, and Oz) tocreate an EEG image composed of the 30 channels × 752time-points for each trial.2.4 EEG pre-processingEach EEG trial was automatically cleaned using the follow-ing processes in sequence: 1) the Koethe’s cleanraw ArtifactSubspace Reconstruction (ASR) in Prep pipeline comple-mented by the Makoto’s pipeline [49] for bad channelsremoval, and 2) the ADJUST EEGlab plugin for blinking andmovement artifact removal using the 2 electro-occulogram(EOG) channels [50].These processes were applied with the purpose of ex-cluding corrupted or distorted channels with artifacts, sig-nal dropout, and electrode malfunction before using AD-JUST. The ADJUST plugin uses spatial and temporal fea-tures such as temporal kurtosis, spatial average-difference,maximum epoch variance, and generic discontinuities forEEG spatial features to detect horizontal or vertical eyeblinking artifacts from independent-components (ICs) [51].The resulting EEG artifact-free trials were baseline corrected-200 ms, prior to stimuli onset (0ms) using a linear de-trending described in [11], then re-segmented for featureextraction to 0-1500ms.2.5 ZCA whitening transformationThe Zero Components Analysis (ZCA) is a whitening trans-formation used to normalize the images amplitude usinga Zero Phase Mahalanobis Distance criterion [52], withoutchanging the correlation between the feature domains in theresulting covariance matrix.The artifact-free input EEG image is denoted as x in thefollowing analyses. The covariance matrix associated to theinput x, denoted as Sx, is calculated following Sx = V DV Twhere V is the eigen-vectors matrix of x and D is thediagonal matrix to construct the eigenvalue decompositionof x. Thus, the new whitened-image Xzca is calculatedfollowing Equation 1 controlling the level of output contrastusing (cid:15)zca. For our specific facial emotion decoding pipelinewe use a low contrast of (cid:15)zca = 0.01.V V T xD + (cid:15)zcaIXzca =(1)√The resulting Xzca has the same size of input x. Thenew image Xzca represents a non-rotated (i.e., zero-phasefeature-space) whitened image to efficiently feed (CNN)-based pipelines [53], [54]. We used this new ZCA imageXzca for training the CNN classifier on a single-trial orimage level. Following this approach we could obtain abetter separability than only using x [11].32.6 CNN architecture and trainingThe complete pipeline for the proposed EEG-based facialemotion decoding on individuals with and without ASD weused here is described in [11], and it is described graphicallyin the supplementary material - Figure S.1. In this subsectionwe will describe the CNN architecture including: the clas-sifier convolutional-pooling (conv-pool) blocks, parameters,dimensions, and training methods including: initialization,learning rates, and stopping criteria.Our motivation for using the (CNN)-based architecturewas based on previous studies which used three normalizedconv-pool blocks connected to a fully-connected (FC) layerand a final decision layer [11], [28]. These type-of networksare suitable CNN architectures for the specific amount oftrials we include in this study and to avoid overfittingeffects. As in Schirrmeister et al. [28], we constructed ourCNN with three conv-pool blocks - going from high-to-low kernel dimensionality, and from low-to-high filters perlayer. Specifically, we set the kernel dimensions per layerbased on the size of Xzca, in a rectangular 30 channels ×752 time-points image, to make the conv-pool blocks morerectangular than the typical CNN architectures used forimage categorization [1], [25].The first conv-pool block was composed of a convolu-tional layer with a kernel-size of 100×10 and 32 filters, and asubsequent max-pooling layer with a size of 5 × 2 connectedto an amplitude normalization layer. The second conv-poolblock was composed of a convolutional layer with a kernel-size of 20 × 5, and a max-pooling layer with a size of2 × 2 units connected to a second amplitude normalizationlayer. A third conv-pool block was composed of a conv-layer with a size of 10 × 2, and a max-pooling layer with asize of 2 × 2 and 128 filters. No batch-level normalizationwas used. Each conv-pool block had a stride factor of 2and non-zero-padding. Thus, the output size was half thesize of the x and y dimensions - without adding any extrazeros in the image edges. The third max-pooling layer onthe last conv-pool block was connected to a dense fully-connected (FC) layer with 1024 sigmoid units, and this layerwas connected to a softmax layer for computing the fourclasses probabilities associated with a particular emotion(i.e., happy, sad, anger, and fear). This last conv-pool blockdid not have a normalization layer connected before thesoftmax layer.The training method was based on the Adam optimiza-tion [55]. We set an initial learning-rate equaling 0.00001with a linear weight decay of 0.000001 per iteration. Toassure a faster convergence in the training process. we usedthe Glorot’s initializer for the kernel weights and for all theconvolutional and max-pool layers [56]. For the biases ini-tialization, we use an uniform random distribution across allthe layers with µ = 0 and σ = 0.1. No random or heuristicsearch was used to set the initial learning rate or the decayrates. We used a dropout layer with p = 0.25 applied tothe FC layer. All the conv-pool activation-functions wereRectified Linear Unit (ReLU) and trained with 4 size mini-batches - changing the training indexes randomly on eachiteration. A maximum of 500 iterations was set as part ofthe training process. We used the early-stopping criteriondescribed in [28] and 72.5% of trials across all participantsTABLE 1: Mean and std for Face Emotion Recogniton (FER) or human performance, and CNN (machine) performances metrics for the eighty-eight participants onthis study . The results are computed averaging the Accuracy (Acc), precision (Pre), Recall (Re), and F1 score (F1) from all the confusion matrices constructed perparticipant. For ASD group comparing the metrics across FER and CNN modalities we found always significant differences p < 0.05*.Metrics/GroupsTDASD*FERAcc0.815±0.0830.776±0.093Pre0.808±0.0790.774±0.089Re0.802±0.0770.768±0.088F10.807±0.0790.771±0.088CNNAcc0.860±0.2130.934±0.134Pre0.864±0.2010.935±0.132Re0.860±0.2040.933±0.134F10.862±0.2020.934±0.1324fell into this early stop criterion requiring less than 200iterations - 73.31% TD and 74.65% ASD.The (CNN)-based pipeline performance was evaluatedusing a Leave-One-Trial-Out (LOTO) cross-validation foreach participant. This means that the performance was mea-sured iterating intra-subjectly across all the 48 EEG trials,per participant, using 47 for train and 1 for testing. TheAccuracy (Acc), precision (Pre), Recall (Re), and F1 scoresreported on Table 1 are calculated using a wrapped (macro)confusion matrix calculated for each participant obtainedin the LOTO cross-validation. We used and reported thesame metrics in our previous study [11]. The evaluation ofthe metrics for the human behavioral performance on theFER task consists of the same approach explained above -assuming each participant has an equal level of entropy asobserved in the 47 trials used for training the CNN. Theseperformance metrics using the LOTO cross validation pro-vide a personalized neural representation for facial emotiondecoding in individuals with and without ASD [11].2.7 XAI methodsfor estimating feature-We used four XAI methodsimportance levels on the trained CNN. We used the Smooth-Grad [34] method as a baseline and three other XAI saliency-maps: Smooth-Grad Squared [40] - a simple numerical vari-ation of Smooth-Grad, PatternNet, Pattern-Attribution [57],and the Layer-Wise Relevance Propagation (LRP) [37]. TheseXAI methods are included in the iNNvestigate softwarepackage [58], a Python module we used for evaluating eachXAI method proposed in this study.Forthe subsequent analyses we define a featurerelevance-map based on the LRP model [36], [38], and forall the XAI methods analyzed here as R1q. These relevancevalues are normalized in amplitude, limiting the propagatedrelevance between [-1, 1]. Thus, the R1q ≥0 values are con-sidered relevant - or a positive contribution to successfulfacial emotion decoding. All the features associated withpositive values represent a hit or an accurate CNN facialemotion decoding. On the other hand, the R1q <0 valuesare considered irrelevant or a negative contribution for asuccessful facial emotion decoding.To obtain a final average relevance measure for eachfacial emotion, we average the resulting relevance mapfor each iteration - across the iterations of the LOTO -persubject- cross-validation [11]. Figure 1 shows the averagerelevance maps for TD and ASD, and for all the XAI meth-ods. In the following subsections, we describe the detailedmodels of each XAI method included in this study forstatistical evaluation and ROAR.2.7.1 Smooth-Grad and Smooth-Grad Squaredthe gradient of logit (unit) i w.r.t. to the input, denotedas g(x) = ∂f (x)i∂x . Typically this results in noise saliency-maps which can be mitigated by averaging the gradients ofmultiple noisy versions of the input, thereby improving thesignal to noise ratio.ˆg(x) =1NN(cid:88)i=1g(x + N (cid:0)0, σ2I(cid:1))(2)The hyper parameters for this method are the number ofsamples N we use, the mean – typically zero – and variance2 of the Gaussian noise. From Equation 2, it should beclear that this method can in principle be applied to anyXAI method and not just the gradients. Recent studies [35],[41] used a variation called Smooth-Grad squared. Herethe gradients are squared before averaging as shown inEquation 3.ˆg(x) =1NN(cid:88)i=1g(x + N (cid:0)0, σ2I(cid:1))2(3)Previously, this approach performed better than the originalSmooth-Grad implementation for vision tasks [40], [41].The relevance maps for TD and ASD groups calculatedusing the Smooth-Grad and the Smooth-Grad Squared areshown from subfigure 1a to 1b - in Figure 1. We considerSmoothGrad as a baseline because of its simplicity and itsbroad usage on other XAI studies [35].2.7.2 Layer-Wise Relevance PropagationLayer-Wise Relevance Propagation (LRP) [37], [59] is afamily of XAI methods based on the Lebesgue energy-conservation law [60]. The quantity observed at the logit i isseen as the relevance for a certain class and in each layer ofthe network we assume that the same amount of relevanceis present for this class. Equation 4 makes this explicit fora multiple-layer network. The relevance observed at thelogit is equal to f (x)i. This is then distributed towards theinput in a layerwise manner such that relevance is preservedthrough the layers. The relevance at neuron d in layer l isdenoted as Rld in LRP.f (x) =(cid:88)R1q =(cid:88)Ri,jl+1d =qd∈(l+1)(cid:88)d∈(l)Ri,jld = . . . =(cid:88)dRi,jLd(4)LRP methods have variations of this process that are opti-mized for specific tasks. In this study, we used LRP presetB (LRP-B) as implemented in the iNNvestigate toolbox.This LRP configuration makes use of the epsilon LRP rulefor dense layers and the alpha-beta rule for convolutionallayers.Smooth-Grad is a XAI method proposed by Smilkov etal. [34]. In the basic form they start out by computingThe epsilon rule is based on the z-rule where the rele-vance is proportional to the weight multiplied neuron con-5(a) Smooth-Grad Squared TD(b) Smooth-Grad Squared ASD(c) PatternNet TD(d) PatternNet ASD(e) Pattern-Attribution TD(f) Pattern-Attribution ASD(g) LRP B TD(h) LRP B ASDFig. 1: Average relevance-maps for Smooth-Grad Squared, PatternNet, Pattern-Attribution, and LRP-B flat preset. The relevance-maps for TD are shown on the left,and for ASD on the right. These relevance maps are normalized between [-1, 1] and coloured using the jet colormap being the more relevant values on dark-red, andthe more unrelevant on darker-blue.tribution denoted as zij = xlare then normalized as seen below.ijiω(l,l+1). These contributionslayers the α-β rule is used.Rli =(cid:88)jziji zi,j(cid:80)Rl+1j =(cid:88)jiji ω(l,l+1)xl+1iω(l,l+1)i xlij+ bj(cid:80)This normalization can become problematic if the denu-merator/denominator is (close to) zero. In this case, a verysmall value epsilon is added (or subtracted to keep the signconstant) to ensure numerical stability. For the convolutionalRl+1j(5)Rli =(cid:34)α(cid:88)jz+ijj z+ij(cid:80)+ βz−ijj z−ij(cid:80)(cid:35)(6)Using this approach with the alpha-beta rule includes pa-rameters α and β that controls how much weight is givento the positive relevance components (z+ij), and negative rel-evance components (z−ij ). While the iNNvestigate package[58] includes many variants of the LRP rules as presets, weuse preset B as described above with α = 2 and β = 1.Preset A with α = 1 and β = 0. was also evaluated, but wasless significant that preset B on our data and therefore is notincluded [11].2.7.3 PatternNet and Pattern-AttributionPatternNet and Pattern-Attribution XAI salience methodshave previously been described by P.J Kindermans et al.[57]. These methods can be seen as an extension of theLRP or Deep-Taylor Decomposition framework. PatternNetand Pattern-Attribution consider which parts of the input aneuron is invariant to and which parts it is trying to detect.By doing this, the explanation of a single artificial neuronwithout the non-linearity is consistent with the explana-tion of multivariate linear models in neuroimaging [61].PatterNet and Pattern-Attribution do still rely on layerwisepropagation from LRP to combine individual neuron-wiseexplanations to whole network explanations.In Pattern-Attribution and PatternNet XAI methods theinput x to a neuron is assumed to be composed of an infor-mative signal component and a non-informative distractorcomponent from the output of that neuron y.x = asy + ad(cid:15)(7)Here (cid:15) can be seen as a noise source that minimizes thedistractor so it is essentially ignored. PatternNet and Patter-Attribution operate under the assumption that the linearmodel explanation should not change if is zero. Therefore,the estimation of as is necessary to propagate according tothe signal component. PatternNet and Pattern-Attributionpropose Equation 7 to find a set of new filter weights ˆωthat are separated as much as possible from the networkweights ω- in order to maximize the signal componentacross the layers of the trained CNN following yl = ˆωT xlylis the output of the layer l and xl the corresponding inputassociated with the filter weights ˆω.For PatternNet and Pattern-Attribution the calculationof the distractor d assumes the following equivalences:yl = ˆωT xl and ˆωT d = 0. We obtain this latter equivalencederiving Equation 7 in terms of ˆω and optimizing. Thus, tocompute a new signal estimator that isolates the networknoise from we can define Sa(x) = ad ˆωT x. This new estima-tor increases the correlation between x and s in a new termdenoted as ρ. This correlation can be calculated with theestimation of the input distribution of x - denoted as u oneach subsequent layer, and considering ˆωT d = 0 during therelevance propagation across all those layers after training.ρ is then defined in Equation 8 - assuming all the noise willbe reflected in the estimated variance σu,d = σy.ρ(S(x)) = 1 − max(cid:18) uT cov(d, y)√σu,dσy(cid:19)(8)In order to assure that ˆωT d = 0 will occur during the rele-vance propagation, the covariance between the distractor dand the layer output y must be zero or as close as possibleto zero cov(d, y). In this manner, the new signal estimatorSa(x), or for simplicity reasons a, can be obtained assumingthat cov(x, y) = cov(Sa(x), y). Thus, assuming that thelearning rates are independent of the interaction betweenx and y on each layer, we can obtain cov(x, y) = acov(y, y)6and the definitive signal estimator a is defined in Equation9.a =cov(x, y)σy=[E+[xy] − E+(x)E+(y)][ωT E+[xy] − ωT E+(x)E+(y)](9)E is the expected value for any single or joint variable xydescribed in Equation 9. The propagation of the estimatora is the main difference between PatternNet and Pattern-Attribution. Both methods use the same model describedon Equation 9 to calculate the estimator per layer. However,PatternNet uses a similar propagation as the LRP Deep-Taylor model [37], [38] based on the ”message passing”modality - without propagating the estimator using infor-mation from ˆω. Pattern-Attribution method uses the numer-ical incidence of the filter weights ˆωT which are estimatedthrough ω. This method propagates a part of ˆω using aproduct between the filter weights and the signal estimatorˆωT a, instead of only a [57]. This last consideration of filterweights supports a more noisy relevance map calculatedfrom Pattern-Attribution as we can see in Figures 1c and 1d.The top part of each subfigure in Figure 1 are the Averagesaliency-maps with the channel indexes on the y-axis, andthe time-points in ms on the x-axis. The bottom part of eachsubfigure shows five topo-plots representing the averagerelevance on five different time ranges, such as 0-500, 250-750, 500-1000, 750-1250, and 1000-1500 ms.2.8 Remove-And-Retrain (ROAR) - Certainty AnalysisFor evaluating the certainty and reliability of the XAI meth-ods, we used the RemOve-And-RetrAin (ROAR) method-ology [41]. ROAR uses the average relevance map R1s foreach participant group (i.e., TD and ASD) for each facialemotion as a feature importance indicator. ROAR weightsthe feature-importance directly from the input feature-spaceusing the relevance values calculated from each XAI methodand sorts the values from high-to-low relevance. The ROARpipeline is reported in Figure S.2 in supplementary material.In ROAR, the average XAI relevance map, per group, R1sdetermines whether features are included in each new train-ing process. Features are suppressed in order of high-to-lowrelevance. Feature removal is done based on an element-wise product between the average posthoc relevance mapobtained from an XAI method and the input EEG image. Theproduct dictates what features will be removed or acceptedfor the CNN re-training based on a relevance threshold.Previous studies have used ROAR to assess the levelof certainty of multiple XAI methods in a quantitativeway [64,65]. These studies compare the level of accuracydetriment associated with the relevant features removalusing random relevance patterns as a baseline. We usedthis same approach to evaluate and compare XAI methodsusing ROAR and random relevance baselines within eachgroup. To do this, we set various relevance thresholds as apixel/feature-removal rate r (r=0.1, 0.2, 0.5, 0.7, and 0.9) togenerate a binarized-mask, Rb, from the average relevance-map R1s using Equation 10.Rb =(cid:40)1 Rs ≤ r0 Rs > r(10)The relevance R1s is averaged across facial emotionsR1s = (Rhappy +Rsad +Rangry +Rf ear)/4 for normalization.The resulting normalized binary-mask dictates which pix-els/features are admitted into the re-training. These binarymasks have the same channels and time-points indexes andsize of Xzca. Different binary masks used in this analysisare reported and illustrated in the supplementary materialin Figures S.3 and S.4 including different values of r.In the following subsection, we will introduce the ran-dom baselines we used to compare the final ROAR metrics,and check if the re-training performances are more inter-pretable using the XAI methods or the random patterns inthe feature removal.2.8.1 ROAR baselinesWe include two basic distribution-based binary masks asrandom baselines for the ROAR evaluation. The first, mostuninformed, baseline we used is a common random base-line based on an uniform distribution with a pixel/featureremoval set to r=0.5.The second baseline we used evaluated time-domainfeature relevance per channel. In this baseline, two typesof random slices are generated. The first slice is a randompattern based on a 47 × 1 slice covering 47 time-pointsand a single channel. This pattern allows all features inROAR, while occluding time-domain slices with a size of47 time-points × 1 channel. This retains a cohesive set ofslices of approximately 20ms around important positive ornegative EEG event-related potentials (ERPs) evoked duringFER, such as N1, P3, and Late-Positive Potential (LPP) [62],[63]. The second slicing baseline is identical to the first, butdoes not sort the 47×1 slices randomly. Instead, it sortsrelevance values based on a XAI method to dictate whichfeature will be removed. This pattern will be referred to asa method-related slice baseline in the following analyses.Binary-masks are reported in the supplementary material inFigure S.5. As a gold-standard, we expect that the featureremoval associated with the more reliable XAI saliency-maps reduces more accuracy than the random baselines.We also expect that the accuracy detriment will be moreplausible for higher r values in comparison with lower rvalues, thus assuring that those removed features are trulyrelevant.3 RESULTSThis results section is divided in three subsections 1) humanbehavioral FER and CNN performance results for boththe TD and ASD groups 2) ROAR results comparing XAIsalience methods relative to random baselines and eachother within groups for interpretability of FER encoding,and 3) group differences in XAI relevance maps for eachfacial emotion or class.3.1 Performances - FER and CNNTable 1 includes the Accuracy (Acc), precision (Pre), Recall(Re), and F1 scores for human behavioral FER and CNN per-formances. The metrics are formally described in [64]. Usingone-way ANOVA, we found significant differences in Acc(F(1,87)=10.43, p=0.00144), pre (F(1,87)=6.31, p=0.0301), Re(F(1,87)=9.35, p=0.00561), and F1 (F(1,87)=8.66, p=0.0232),7between FER and CNN metrics. For all metrics, CNN wasmore accurate than FER.3.2 XAI methods - Statistical comparisonsThe five time ranges mentioned above are used for adjustingthe p-values of the statistical comparison reported belowusing a Bonferroni-Holm correction [65].3.2.1 XAI saliency-maps comparisonVisual inspection of XAI saliency-maps in Figure 1, showssimilarities in the relevance distribution within the five time-ranges mentioned above. To test these statistical similarities,we used the Kolmogorov-Smirnov test (KS-test) [66]. Com-paring the relevances obtained in the Smooth-Grad methodand Smooth-Grad Squared method we found similaritiesacross the five time ranges h=1, p≤ 0.001**.Similarities were also observed in late-time ranges, suchas 750-1250 and 1000-1500 ms, between PatternNet andLRP-B h=1, p≤0.028*, and between PatternNet and Pattern-Attribution, h =1, p≤0.043*. We attributed this to a morenoisy relevance pattern obtained from Pattern-Attributionafter the marginal values of the weights ˆωT can also mod-ulate the calculation of the estimator a (see section 2.7.3).There were no other statistical similarities between otherXAI methods, h=0, p’s>0.05. All these findings are in linewith our expectations after training the CNN with this typeof EEG image.3.2.2 Differences between TD and ASD relevance mapsThere were no significant differences in the Smooth-Grad,Smooth-Grad average, Smooth-Grad Squared, Pat-ternNet, or Pattern-Attribution XAI salience maps for anyspecific facial emotion, or the average across facial emotionsbetween TD and ASD groups. For Smooth-Grad we ob-tained F’s(1,87)≤1.334, p’s>0.05, across all emotions or anytime-range (see section XAI methods). Similar differenceswere obtained for Smooth-Grad Squared F(1,87)≤0.129 withp>0.05, PatternNet F(1,87)≤0.883, p’s>0.05, and Pattern-Attribution F(1,87)≤0.222, p’s>0.05.The main significant differences found between TD andASD XAI salience maps are observed in the LRP-B method,specifically for negative emotions, such as Anger and Fear.This is consistent with the observable behavioral deficitsin ASD performing FER. We found significant differencesfor Average in 0-500ms, (F(1,87)=7.889, p=0.0344), TD>ASD,and in 1000-1500ms, (F(1,87)=11.56, p=0.0033), TD<ASD.For Sad in 750-1250ms, (F(1,87)=8.491, p=0.0141), TD>ASD,and in 1000-1500ms, (F(1,87)=13.54, p=0.0005), TD>ASD.For anger in 0-500ms, F(1,87)=10.85, p=0.0095, TD>ASD,and in 1000-1500ms, (F(1,87)=9.667, p=0.0102), TD<ASD.For Fear in 0-500ms, (F(1,87)=23.47, p=7.6E-6), TD>ASD,in 500-1000ms, (F(1,87)=7.193, p=0.0263), TD<ASD, and in[750-1250]ms, (F(1,87)=9.313, p=0.0121), TD<ASD. All indi-vidual F and p values for each facial emotion, group, andXAI method are reported in the supplementary material inTable S.1.3.2.3 Differences between binary masksTo evaluate the differences between binary masks obtainedfrom all the trained TD and ASD CNNs for each facialemotion we used an One-way ANOVA and Bonferroni-Holm correction over the same five time-ranges. Figure 2a8(a) 500-1000ms binary mask topo-maps(b) 750-1250ms binary mask topo-mapsFig. 2: Topoplots examples for the binary masks on 500-1000ms and 750-1250ms time ranges. These topo-maps cover all the saliency methods analyzed in this studysuch as Smooth-Grad, Smooth-Grad Squared, PatternNet, Pattern-Attribution, and LRP-B flat preset in rows, and the r values in columns. The plots are illustratedusing a redblue colormap with limits between [-0.2, 0.2]. F and p-values are reported only for the binary-mask comparisons that are significantly different aftercorrection.shows the topo-plots related to the binary masks for the timerange between 500-1000ms. In this time range we only foundsignificant differences for r=0.5. Specifically, we found dif-ferences in Smooth-Grad (F(1,87)=3.55, p=0.050), Smooth-Grad Squared (F(1,87)=3.44, p=0.049), Pattern-Attribution(F(1,87)= 4.57, p=0.031), and LRP-B (F(1,87)=3.67, p=0.044).In PatternNet we did not find any significant difference onthis early time range. This must be related to the behavioralassociated with the CNN, less noisy, and coarser relevancepatterns associated with PatternNet.Figure 2b shows the topo-plots related to the binarymasks for the interval between 750-1250ms. There wefound differences for r=0.2 and PatternNet (F(1,87)=6.22,p=0.021). We also found differences for r=0.5 in Smooth-Grad Squared (F(1,87)=3.51, p=0.048), Pattern-Attribution(F(1,87)=3.78, p=0.041), and the LRP-B preset (F(1,87)=3.81,p=0.043). These results suggest that in terms of the r values,the binary mask patterns are consistent. This also shows thatbinary masks are consistent across r in the late time-ranges- excepting r=0.5. To assure a more consistent comparisonbetween the binary masks of TD and ASD, we evaluatethem using the K-S test. For all the significant differencesfound in this analysis we can surmise that the binary masksall come from the same distribution - h=1, p<0.05. Therewere no significant differences between 0-500ms for any XAImethod.3.3 Accuracy detriment differences across r valuesFor comparing the accuracy detriment across the r valuesof 0, 0.1, 0.2, 0.35, 0.5, 0.7, 0.9, and 1, we used an one-way ANOVA with a subsequent Bonferroni-Holm correc-tion across the r values. The comparison was evaluatedand adjusted by grouping the accuracies obtained for eachTD and ASD participant described in Table 1. Figures 3aand 3b shows the accuracy detriment for all the XAI meth-ods included in this study for TD and ASD respectively.Similarly, to illustrate the performance detriment obtainedfrom the method-related 47×1 baseline we show the ac-curacy detriment in Figure 3c and 3d for TD and ASD.Analyzing the differences between TD and ASD accuraciesacross the XAI methods we observed significant differ-ences between ROAR-related accuracies and the randombaselines. For Smooth-Grad we found significant differ-ences in comparison with the random baseline for r=0.7(F(1,95)=3.31, p=0.00155) and r=0.9 (F(1,95)=2.65, p=0.0224)in TD. For ASD we found differences in r=0.7 (F(1,79)=4.01,p=0.000331), and r=0.9 (F(179)=2.23, p=0.0338).and in r=0.9Again for Smooth-Grad we found differencesinin r=0.7comparison with the method-related slices(F(1,95)=7.33,(F(1,95)=10.58,p=1.45e-6),p=0.00023) for TD. For ASD we found differences in r=0.7(F(1,79)=11.11, p=2.28E-7), and in r=0.9 (F(1,79)=7.45,p=0.000148). For this particular method, all the method-related slice baseline accuracies were lower in comparisonwith the SmoothGrad-related performance detriment.Other differences in the accuracy detriment are moreobservable in low values of r. For instance, for Smooth-Grad Squared and r=0.2 (F(1,95)=10.99, p=0.0000274) forTD, and r=0.2 (F(1,79)=12.45, p=0.0000345) for ASD. Theaccuracy detriments associated with the Smooth-GradSquared method-related slice baseline are lower than theXAI method itself. However, we did not find significantfor TD (F(1,95)≤2.58, p>0.1893) or ASDdifferences(F(1,79)≤2.88, p>0.1910) for r values different than 0.2.9(a) TD - Saliency maps(b) ASD - Saliency maps(c) TD - slices(d) ASD - slicesFig. 3: Average accuracies and detriments comparison between all the saliency maps evaluated in this study. On Figures 3a and 3b, and for the method-based sliceson Figure Figures 3c and 3d. TD plots are on the left and ASD on the right columns. Bars showed for each value of r represent the standard-deviation of the set ofaccuracies wrapped for on each r value and on each group - TD or ASD.For PatternNet we only found significant differencesin comparison with the method-related slices baseline.For r=0.7 the XAI method always shows a lower accu-racy for TD (F(1,95)=13.38, p=0.0001178), and for ASD(F(1,79)=20.45, p=2.77E-7). In r=0.9 we found differencesfor TD (F(1,95)=3.42, p=0.0267), and for ASD (F(1,79)=2.99,p=0.0321). For other values of r we did not find differencesafter correction. For Pattern-Attribution we found differ-ences in r=0.2 for TD (F(1,95)=8.35, p=0.00327), and forASD (F(1,79)=7.91, p=0.00899). We found other differenceswhere the method-related slice baseline is showing a loweraccuracy than the XAI method itself, specifically in r=0.5for TD (F(1,95)=10.12, p=0.000224) and ASD (F(1,79)=9.88,p=0.003367). For other r values we did not find any signifi-cant differences.Comparing the ROAR accuracy detriments for the LRP-B preset method, we found significant differences in r=0.2for TD (F(1,95)=3.56, p=0.00214), and for ASD (F(1,79)=3.66,p=0.00203). An important observation is that the accuracydetriment associated with the LRP-B method is only show-ing significant differences for ASD in r=0.7 (F(1,79)=3.91,p=0.000156). For other r values we did not find any signifi-cant difference - (F(1,95)≤1.99, p>0.2489).4 DISCUSSIONDL pipelines are effective [11], [27], [28] at decoding facialemotions, but it is unclear what neural information is moreimportant in this process. To address this, we comparedXAI methods to determine their trustworthiness in thisapplication [36]. We compared XAI saliency-maps (Figure 1)generated from LRP-B, PatternNet, and Pattern-Attributionmethods and identified features that are consistent withknown patterns observed in EEG during facial emotiondecoding for individuals with and without ASD [62], [67].Even though the previously-mentioned XAI methods iden-tify features that are consistent with prior knowledge onEEG-based facial emotion decoding [11], we observed somequantitative differences between the associated relevancepatterns. Because of these differences, it is not clear whichmethod best represents the neural-network facial emotion-decoding process. This motivates the usage of ROAR andrandom and method-slices baseline for quantifying the levelof relevance of each of these XAI methods.ROAR yields areliable evaluation of what features of the EEG input areused to train the CNN for FER decoding. We observedthat LRP, PatternNet and Pattern-Attribution identify thelate time-ranges, after 500ms relative to the stimulus onset,as essential for correct facial emotion decoding, which isconsistent with previous literature [29], [42], [68].These analyses demonstrate that XAI methods can beapplied to recover features necessary for neural encoding.However, this effect is only observed when less than 50% ofthe relevant features are removed, as we observed in Figures8.a and 8.c. While the best performing method differs basedon the exact setting, we recommend using the SmoothGrad-Squared approach, since it is the easiest to implement, beforeusing more complicated methods.The results discussed above have shown that the pat-terns found by the XAI methods are meaningful. For Pat-ternNet, Pattern-Attribution, Smooth-Grad Squared, andLRP-B when r=0.5 we see significant differences in time-ranges later than 500 ms. These differences are consistentwith the late activation neural component in ASD whenperforming FER [46]. The three time-ranges that were es-sential for facial emotion decoding in ASD were centeredat 250, 600, and 1100 ms. These ranges were not essentialfor facial emotion decoding in TD. Instead, in TD subjectsonly two time ranges were essential for facial encoding- centered at 250, and 1200 ms. This suggests that EEGfacial emotion decoding is done differently between groups,which is consistent with other EEG studies [9], [29]. Thesuccess of the CNN as a fully-personalized classifier -basedon a LOTO cross-validation per subject- is an importantnovelty including XAI methods evaluation. This evaluationshows that it is possible to decode emotions reliably fromthe EEG of ASD populations when they perform FER. Thisconfirms that there is intact emotion information encodingin ASD from the EEG single trial/image level [69], un-derstanding fully-personalized neural representations usingXAI methods might be useful to develop more efficient data-driven ASD interventions [70].5 CONCLUSIONThe certainty analysis employed in this study suggeststhat the correctly decoded emotions/trials on our proposedEEG-based CNN pipeline are associated with relevance pat-terns that show high-relevance values on late time-ranges.In time-ranges between 500-1500ms after the stimulus onsetwe find significant differences associated with the relevancepatterns and the ROAR binary masks obtained from someXAI methods, such as, LRP-B, PatternNet and Pattern-Attribution. This effect is also observed when evaluatingROAR on those methods, thus suggesting that the EEG rel-evant features assessed with ROAR are particularly usefulfor obtaining a successful emotion recognition.The differences in accuracy detriment observed afterevaluating ROAR are important for supporting the differ-ences observed between TD and ASD relevance maps. Themore reliable XAI methods obtained after using ROAR andremoving more than the 50% of the relevant features areprecisely the LRP-B, PatternNet and Pattern-Attribution.These methods are the ones that show significant differencesin the late-timing between TD and ASD. Specifically, thosedifferences between TD and ASD observed in the morereliable XAI methods are consistent with the altered neuralconnectivity patterns observed when individuals with ASDprocess emotion from faces.This study consolidates important findings in ASD andcomputational-neuroscience research. The ROAR evaluationcan identify the more reliable and intuitively importantfeatures that can successfully decode an emotion from EEGactivity. These distinguishable patterns are setting a moreconsistent and remarkable set of information that is pre-cisely relevant for the emotion decoding. This representsa different and intact emotion information encoding inindividuals with ASD that can be efficiently extracted bythe CNN. These results can re-define the current state-of-the-art of facial emotion-decoding pipelines and XAI.This supports, CNN as a perceptual-based classifier, whichovercomes the behavioral/neural emotion comprehensiondeficits observed in individuals with ASD.This study is the first, quantitatively speaking, to employROAR for evaluating robust XAI methods on EEG-basedfacial emotion recognition. This study is also the first to useEEG features for evaluating the reliability and correctness ofcurrent state-of-the-art XAI methods, including EEG trialsfrom ASD and non-ASD individuals.10ACKNOWLEDGMENTWe would like to express our thankful feelings and deep appreciation to Pieter-Jan Kindermans, PhD and Maximilian Alber, PhD, affiliated with Google Brain,for their extensive and receptive collaboration in the construction and evaluationof all the Explainable Artificial Intelligence (XAI) methods included in thisstudy, and for their effort in the instruction and evaluation of the iNNvesti-gate package including EEG data https://github.com/albermax/innvestigate.MDL was supported by the NationalInstitute of Mental Health (GrantNo.R01MH110585), grants from the Alan Alda Fund for Communication, theAmerican Psychological Association, and Association for Psychological Science,as well as Fellowships from the American Psychological Foundation, JeffersonScholars Foundation, and International Max Planck Research. We would liketo thank Stony Brook Research Computing and Cyber-infrastructure, and theInstitute for Advanced Computational Science at Stony Brook University foraccess to the SeaWulf computing system, which was made possible by a $1.4M National Science Foundation grant(#1531492). Code for the CNN can befound in https://github.com/meiyor/Deep-Learning-Emotion-Decoding-using-EEG-data-from-Autism-individuals. Data for replication samples can be madeavailable upon request to the corresponding authors. Tessa Clarkson was sup-ported by the National Institute of Mental Health of the National Institutesof Health under Award Number [F31MH122091, 2020]. The content is solelythe responsibility of the authors and does not necessarily represent the officialviews of the National Institutes of Health. Tessa Clarkson was also supportedby the Temple University Dissertation Completion Grant, the Temple UniversityPublic Policy Lab Graduate Fellowship, American Psychological Association(APA) Dissertation Research Award, and the Dr. Phillip J Bersh Memorial StudentAward.REFERENCES[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classifi-cation with deep convolutional neural networks,” in Advances inneural information processing systems, 2012, pp. 1097–1105.I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequencelearning with neural networks,” in Advances in neural informationprocessing systems, 2014, pp. 3104–3112.[2][4][3] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” inProceedings of the IEEE international conference on computer vision,2017, pp. 2961–2969.J. Prince, F. Andreotti, and M. De Vos, “Evaluation of source-wisemissing data techniques for the prediction of parkinson’s diseaseusing smartphones,” in ICASSP 2019-2019 IEEE International Con-ference on Acoustics, Speech and Signal Processing (ICASSP).IEEE,2019, pp. 3927–3930.[5] H. M. O’Leary, J. M. Mayor, W. E. Kaufmann, and M. Sahin, “Mul-timodal hand stereotypies detection in rett syndrome treatmentusing deep belief neural networks,” 2017 39th Annual InternationalConference of the IEEE Engineering in Medicine and Biology Society(EMBC), 2017.[6] F. Andreotti, H. Phan, N. Cooray, C. Lo, M. T. Hu, and M. De Vos,“Multichannel sleep stage classification and transfer learning us-ing convolutional neural networks,” in 2018 40th Annual Interna-tional Conference of the IEEE Engineering in Medicine and BiologySociety (EMBC).IEEE, 2018, pp. 171–174.[7] W. Liu, W.-L. Zheng, and B.-L. Lu, “Emotion recognition usingmultimodal deep learning,” in International conference on neuralinformation processing. Springer, 2016, pp. 521–529.[8] H. Li, Y.-M. Jin, W.-L. Zheng, and B.-L. Lu, “Cross-subject emo-tion recognition using deep adaptation networks,” in InternationalConference on Neural Information Processing.Springer, 2018, pp.403–413.[9] K. Weitz, T. Hassan, U. Schmid, and J. Garbas, “Towards explain-ing deep learning networks to distinguish facial expressions ofpain and emotions,” in Forum Bildverarbeitung 2018. KIT ScientificPublishing, 2018, p. 197.[10] B. Ghoshal, A. Tucker, B. Sanghera, and W. L. Wong, “Estimatinguncertainty in deep learning for reporting confidence to clinicianswhen segmenting nuclei image data,” in 2019 IEEE 32nd Inter-national Symposium on Computer-Based Medical Systems (CBMS).IEEE, 2019, pp. 318–324.[11] J. M. M. Torres, T. Clarkson, K. M. Hauschild, C. C. Luhmann,M. D. Lerner, and G. Riccardi, “Facial emotions are accuratelyencoded in the neural signal of those with autism spectrum dis-order: A deep learning approach,” Biological Psychiatry: CognitiveNeuroscience and Neuroimaging, pp. S2451–9022, 2021.[12] W. Bosl, A. Tierney, H. Tager-Flusberg, and C. Nelson, “Eegcomplexity as a biomarker for autism spectrum disorder risk,”BMC medicine, vol. 9, no. 1, p. 18, 2011.[13] J. Castelhano, P. Tavares, S. Mouga, G. Oliveira, and M. Castelo-Branco, “Stimulus dependent neural oscillatory patterns showreliable statistical identification of autism spectrum disorder in aface perceptual decision task,” Clinical Neurophysiology, vol. 129,no. 5, pp. 981–989, 2018.[14] R. Jenke, A. Peer, and M. Buss, “Feature extraction and selectionfor emotion recognition from eeg,” IEEE Transactions on AffectiveComputing, vol. 5, no. 3, pp. 327–339, 2014.[15] S. Koelstra, C. Muhl, M. Soleymani,J.-S. Lee, A. Yazdani,T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras, “Deap: A database foremotion analysis; using physiological signals,” IEEE transactionson affective computing, vol. 3, no. 1, pp. 18–31, 2011.[16] J. Fan, E. Bekele, Z. Warren, and N. Sarkar, “Eeg analysis of facialaffect recognition process of individuals with asd performanceprediction leveraging social context,” in 2017 Seventh InternationalConference on Affective Computing and Intelligent Interaction Work-shops and Demos (ACIIW).IEEE, 2017, pp. 38–43.[17] J. Fan, J. W. Wade, A. P. Key, Z. E. Warren, and N. Sarkar,“Eeg-based affect and workload recognition in a virtual drivingenvironment for asd intervention,” IEEE Transactions on BiomedicalEngineering, vol. 65, no. 1, pp. 43–51, 2017.[18] B. Blankertz, S. Lemm, M. Treder, S. Haufe, and K.-R. M ¨uller,“Single-trial analysis and classification of erp components—a tu-torial,” NeuroImage, vol. 56, no. 2, pp. 814–825, 2011.[19] I. Winkler, S. Haufe, and M. Tangermann, “Automatic classifi-cation of artifactual ica-components for artifact removal in eegsignals,” Behavioral and Brain Functions, vol. 7, no. 1, p. 30, 2011.[20] H. M. O’Leary, J. M. Mayor, C.-S. Poon, W. E. Kaufmann, andM. Sahin, “Classification of respiratory disturbances in rett syn-drome patients using restricted boltzmann machine,” in 2017 39thAnnual International Conference of the IEEE Engineering in Medicineand Biology Society (EMBC).IEEE, 2017, pp. 442–445.[21] J. M. M. Torres, “Eeg signals classification using linear and non-linear discriminant methods,” El Hombre y la M´aquina, no. 41, pp.71–80, 2013.[22] J. M. Mayor Torres, E. Libsack, T. Clarkson, C. Keifer, G. Ric-cardi, and M. Lerner, “Eeg-based single trial classification emo-tion recognition: A comparative analysis in individuals with andwithout autism spectrum disorder,” International Society for AutismResearch, INSAR 2018, vol. 85, no. 10, pp. S149–S150, 2018.[23] H. Yang, S. Sakhavi, K. K. Ang, and C. Guan, “On the use ofconvolutional neural networks and augmented csp features formulti-class motor imagery of eeg signals classification,” in 201537th Annual International Conference of the IEEE Engineering inMedicine and Biology Society (EMBC).IEEE, 2015, pp. 2620–2623.[24] K. K. Ang, Z. Y. Chin, H. Zhang, and C. Guan, “Filter bankcommon spatial pattern (fbcsp) in brain-computer interface,” in2008 IEEE International Joint Conference on Neural Networks (IEEEWorld Congress on Computational Intelligence).IEEE, 2008, pp.2390–2397.[25] Y.-Y. Lee and S. Hsieh, “Classifying different emotional states bymeans of eeg-based functional connectivity patterns,” PloS one,vol. 9, no. 4, p. e95415, 2014.[26] B. Blankertz, K.-R. Muller, G. Curio, T. M. Vaughan, G. Schalk, J. R.Wolpaw, A. Schlogl, C. Neuper, G. Pfurtscheller, T. Hinterbergeret al., “The bci competition 2003: progress and perspectives indetection and discrimination of eeg single trials,” IEEE transactionson biomedical engineering, vol. 51, no. 6, pp. 1044–1051, 2004.[27] J. M. M. Torres, T. Clarkson, E. A. Stepanov, C. C. Luhmann, M. D.Lerner, and G. Riccardi, “Enhanced error decoding from error-related potentials using convolutional neural networks,” 40th In-ternational Engineering in Medicine and Biology Conference, EMBC2018, Jul. 2018.[28] R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstet-ter, K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, andT. Ball, “Deep learning with convolutional neural networks for eegdecoding and visualization,” Human brain mapping, vol. 38, no. 11,pp. 5391–5420, 2017.[29] M. H. Black, N. T. Chen, K. K. Iyer, O. V. Lipp, S. B ¨olte, M. Falkmer,T. Tan, and S. Girdler, “Mechanisms of facial emotion recognitionin autism spectrum disorders: insights from eye tracking and elec-troencephalography,” Neuroscience & Biobehavioral Reviews, vol. 80,pp. 488–515, 2017.11[31] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, andD. Batra, “Grad-cam: Visual explanations from deep networks viagradient-based localization,” in Proceedings of the IEEE InternationalConference on Computer Vision, 2017, pp. 618–626.[32] A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubra-manian, “Grad-cam++: Generalized gradient-based visual expla-nations for deep convolutional networks,” in 2018 IEEE WinterConference on Applications of Computer Vision (WACV).IEEE, 2018,pp. 839–847.[33] B. Kim, J. Seo, S. Jeon, J. Koo, J. Choe, and T. Jeon, “Why aresaliency maps noisy? cause of and solution to noisy saliencymaps,” arXiv preprint arXiv:1902.04893, 2019.[34] D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, and M. Wattenberg,“Smoothgrad: removing noise by adding noise,” arXiv preprintarXiv:1706.03825, 2017.[35] P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. T. Sch ¨utt,S. D¨ahne, D. Erhan, and B. Kim, “The (un) reliability of saliencymethods,” arXiv preprint arXiv:1711.00867, 2017.[36] W. Samek, G. Montavon, S. Lapuschkin, C. J. Anders, and K.-R.M ¨uller, “Toward interpretable machine learning: Transparent deepneural networks and beyond,” arXiv preprint arXiv:2003.07631,2020.[37] A. Binder, S. Bach, G. Montavon, K.-R. M ¨uller, and W. Samek,“Layer-wise relevance propagation for deep neural network ar-chitectures,” in Information Science and Applications (ICISA) 2016.Springer, 2016, pp. 913–922.[38] G. Montavon, W. Samek, and K.-R. M ¨uller, “Methods for inter-preting and understanding deep neural networks,” Digital SignalProcessing, vol. 73, pp. 1–15, 2018.[39] P.-J. Kindermans, K. T. Sch ¨utt, M. Alber, K.-R. M ¨uller, D. Er-han, B. Kim, and S. D¨ahne, “Learning how to explain neu-ral networks: Patternnet and patternattribution,” arXiv preprintarXiv:1705.05598, 2017.[40] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, andB. Kim, “Sanity checks for saliency maps,” in Advances in NeuralInformation Processing Systems, 2018, pp. 9505–9515.[41] S. Hooker, D. Erhan, P.-J. Kindermans, and B. Kim, “Evaluatingfeature importance estimates,” arXiv preprint arXiv:1806.10758,2018.[42] G. Dawson, S. J. Webb, and J. McPartland, “Understanding thenature of face processing impairment in autism: insights frombehavioral and electrophysiological studies,” Developmental neu-ropsychology, vol. 27, no. 3, pp. 403–424, 2005.[43] G. Dawson and R. Bernier, “Development of social brain circuitryin autism,” Human behavior, learning, and the developing brain: Atyp-ical development, pp. 28–56, 2007.[44] S. Nowicki, “Manual for the receptive tests of the diagnosticanalysis of nonverbal accuracy 2,” Atlanta, GA: Department ofPsychology, Emory University, 2000.[45] C. Lord, E. H. Cook, B. L. Leventhal, and D. G. Amaral, “Autismspectrum disorders,” Neuron, vol. 28, no. 2, pp. 355–363, 2000.[46] G. Dawson, L. Carver, A. N. Meltzoff, H. Panagiotides, J. Mc-Partland, and S. J. Webb, “Neural correlates of face and objectrecognition in young children with autism spectrum disorder, de-velopmental delay, and typical development,” Child development,vol. 73, no. 3, pp. 700–717, 2002.[47] A. Delorme and S. Makeig, “Eeglab: an open source toolboxfor analysis of single-trial eeg dynamics including independentcomponent analysis,” Journal of neuroscience methods, vol. 134, no. 1,pp. 9–21, 2004.[48] S. J. Webb, R. Bernier, H. A. Henderson, M. H. Johnson, E. J.Jones, M. D. Lerner, J. C. McPartland, C. A. Nelson, D. C. Rojas,J. Townsend et al., “Guidelines and best practices for electrophysi-ological data collection, analysis and reporting in autism,” Journalof autism and developmental disorders, vol. 45, no. 2, pp. 425–443,2015.[49] N. Bigdely-Shamlo, G. Ibagon, C. Kothe, and T. Mullen, “Findingthe optimal cross-subject eeg data alignment method for analysisand bci,” in 2018 IEEE International Conference on Systems, Man, andCybernetics (SMC).IEEE, 2018, pp. 1110–1115.[50] A. Mognon, J. Jovicich, L. Bruzzone, and M. Buiatti, “Adjust: Anautomatic eeg artifact detector based on the joint use of spatialand temporal features,” Psychophysiology, vol. 48, no. 2, pp. 229–240, 2011.[30] B. Kovalerchuk and N. Neuhaus, “Toward efficient automationof interpretable machine learning,” in 2018 IEEE InternationalConference on Big Data (Big Data).IEEE, 2018, pp. 4940–4947.[51] A. Hyv¨arinen and E. Oja, “Independent component analysis:algorithms and applications,” Neural networks, vol. 13, no. 4-5, pp.411–430, 2000.[52] A. Coates and A. Y. Ng, “Learning feature representations withSpringer, 2012,k-means,” in Neural networks: Tricks of the trade.pp. 561–580.[53] C. S. Lee, S. H. Lam, S. T. Tsang, C. M. Yuen, and C. K. Ng,“The effectiveness of technology-based intervention in improvingemotion recognition through facial expression in people withautism spectrum disorder: a systematic review,” Review Journal ofAutism and Developmental Disorders, vol. 5, no. 2, pp. 91–104, 2018.[54] H. Huang, D. Li, Z. Zhang, X. Chen, and K. Huang, “Adversariallyoccluded samples for person re-identification,” in Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition, 2018,pp. 5098–5107.[55] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-tion,” arXiv preprint arXiv:1412.6980, 2014.[56] X. Glorot, A. Bordes, and Y. Bengio, “Deep sparse rectifier neuralnetworks,” in Proceedings of the fourteenth international conference onartificial intelligence and statistics, 2011, pp. 315–323.[57] P.-J. Kindermans, K. T. Sch ¨utt, M. Alber, K.-R. M ¨uller, andS. D¨ahne, “Patternnet and patternlrp–improving the interpretabil-ity of neural networks,” stat, vol. 1050, p. 16, 2017.[58] M. Alber, S. Lapuschkin, P. Seegerer, M. H¨agele, K. T. Sch ¨utt,G. Montavon, W. Samek, K.-R. M ¨uller, S. D¨ahne, and P.-J. Kinder-mans, “innvestigate neural networks!” Journal of Machine LearningResearch, vol. 20, no. 93, pp. 1–8, 2019.[59] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M ¨uller, andW. Samek, “On pixel-wise explanations for non-linear classifierdecisions by layer-wise relevance propagation,” PloS one, vol. 10,no. 7, p. e0130140, 2015.[60] A. Szepessy, “An existence result for scalar conservation lawsusing measure valued solutions.” Communications in Partial Dif-ferential Equations, vol. 14, no. 10, pp. 1329–1350, 1989.[61] I. Winkler, S. Debener, K.-R. M ¨uller, and M. Tangermann, “On theinfluence of high-pass filtering on ica-based artifact reduction ineeg-erp,” in 2015 37th Annual International Conference of the IEEEEngineering in Medicine and Biology Society (EMBC).IEEE, 2015,pp. 4101–4105.[62] S. J. Webb, E. Neuhaus, and S. Faja, “Face perception and learningin autism spectrum disorders,” The Quarterly Journal of Experimen-tal Psychology, vol. 70, no. 5, pp. 970–986, 2017.[63] M. X. Cohen, “Where does eeg come from and what does itmean?” Trends in neurosciences, vol. 40, no. 4, pp. 208–218, 2017.[64] D. M. Powers, “Evaluation: from precision, recall and f-measureto roc, informedness, markedness and correlation,” 2011.[65] H. Abdi, “Holm’s sequential bonferroni procedure,” Encyclopediaof research design, vol. 1, no. 8, pp. 1–8, 2010.[66] B. Banerjee and B. Pradhan, “Kolmogorov–smirnov test for life testdata with hybrid censoring,” Communications in Statistics-Theoryand Methods, vol. 47, no. 11, pp. 2590–2604, 2018.[67] S. J. Webb, E. J. Jones, K. Merkle, K. Venema, J. Greenson,M. Murias, and G. Dawson, “Developmental change in the erpresponses to familiar faces in toddlers with autism spectrumdisorders versus typical development,” Child development, vol. 82,no. 6, pp. 1868–1886, 2011.[68] E. V. Friedrich, A. Sivanathan, T. Lim, N. Suttie, S. Louchart,S. Pillen, and J. A. Pineda, “An effective neurofeedback inter-vention to improve social interactions in children with autismspectrum disorder,” Journal of autism and developmental disorders,vol. 45, no. 12, pp. 4084–4100, 2015.[69] J. M. Mayor-Torres, M. Ravanelli, S. E. Medina-DeVilliers, M. D.Lerner, and G. Riccardi, “Interpretable sincnet-based deep learn-ing for emotion recognition from eeg brain activity,” arXiv preprintarXiv:2107.10790, 2021.[70] J.-M. Fellous, G. Sapiro, A. Rossi, H. Mayberg, and M. Ferrante,“Explainable artificial intelligence for neuroscience: Behavioralneurostimulation,” Frontiers in neuroscience, vol. 13, p. 1346, 2019.Juan Manuel Mayor-Torres received the PhD incomputer science in 2020 from the Department of In-formation Engineering and Compute (DISI), Universityof Trento, Italy.He received the BSc and MSc in Elec-trical Engineering from Pontificia Universidad Javeri-ana, Colombia in 2010 and 2014 respectively. He hasworked as a Research Associate in the Department ofPsychology of Cardiff University in 2020.He currentlyworks as Postdoctoral Fellow in the School of PublicHealth, Physiotherapy & Population Science of Univer-sity College Dublin. He has been a reviewer in journalssuch as Expert Systems in Elsevier, MDPI Electronics and Applied Sciences, andin the International Conference of the IEEE Engineering in Medicine and BiologySociety (EMBC) and ACM Interspeech. His research interests include MachineLearning, Emotion Recognition, Explainable AI, and Biomedical Signal Processing12Sara Medina-Devilliers received her PhD in Clini-cal Psychology at the University of Virginia in 2021 andher BA in Psychology and Neuroscience at DartmouthCollege in 2012. Her research interests include investi-gating neural and physiological mechanisms underlyingsocial relationships and social communication and theirassociations with health and well-being. She is currentlya Postdoctoral Fellow at Boston Children’s HospitalTessa Clarkson is a sixth-year doctoral candidateof the Clinical Psychology Ph.D. program at TempleUniversity. She received her MA in Psychology at StonyBrook University in 2018 and her BS in Human Physi-ology at Boston University in 2011. She has worked asa reviewer in journals such as: the Journal of Autismand Developmental Disorders, Psychophysiology, Jour-nal of Child Psychology and Psychiatry, Brain and Be-havior, Plos One, Autism, and Social Cognitive andAffective Neuroscience. Her research interests includestudying the underlying neural mechanisms of sociallearning, behavior, and cognition. She is also interested in how these mechanismsrelate to the development and maintenance of psychopathology.Matthew D. Lerner is an Associate Professor ofPsychology Psychiatry, & Pediatrics in the Departmentof Psychology at Stony Brook University, where he di-rects the Social Competence and Treatment Lab. He isa Founder and Research Director of the Stony BrookAutism Initiative, and Co-Director of the Stony BrookLEND Center. He received his PhD in Clinical Psychol-ogy from the University of Virginia in 2013. Dr. Lerner’sresearch focuses on understanding emergence and”real-world” implications of social problems in childrenand adolescents (especially those with Autism Spec-trum Disorders [ASD]), as well as development, evaluation, and disseminationof novel, evidence-based approaches for ameliorating those problems. He haspublished more than 100 peer-reviewed articles and book chapters, he servesas Associate Editor of the Journal of Autism and Developmental Disorders, and onthe Editorial Boards of 7 other academic outlets. He has presented at more than150 national and international conferences on topics related to social developmentand developmental disorders. Dr. Lerner has received grants from organizationsincluding the National Institutes of Health (NIH), the Health Resources & ServicesAdministration (HRSA), the Brain & Behavior Research Foundation, the SimonsFoundation, the American Psychological Association, and the American Academyof Arts & Sciences. He has received several acknowledgments and awards, includ-ing the Biobehavioral Research Award for Innovative New Scientists (BRAINS)from the National Institute of Mental Health (NIMH), the Early Career ResearchContributions Award from the Society for Research in Child Development (SRCD),the David Shakow Early Career Award for Distinguished Scientific Contributionsto Clinical Psychology from the Society of Clinical Psychology (APA Division 12),the Sara S. Sparrow Early Career Research Award (APA Division 33), the SusanNolen-Hoeksema Early Career Research Award from the Society for a Science ofClinical Psychology, the Richard “Dick” Abidin Early Career Award from the Societyof Clinical Child and Adolescent Psychology (APA Division 53), Young InvestigatorAwards from the Brain & Behavior Research Foundation (NARSAD) and theInternational Society for Autism Research, the Transformative Contributions Awardfrom the Autism & Developmental Disabilities SIG of the Association for Behavioraland Cognitive Therapies, and the Rising Star designation from the Association forPsychological Science.Giuseppe Riccardi (M96-SM04-F10) is founder anddirector of the Signals and Interactive Systems Labat University of Trento, Italy. He received his Laurea(MSEE) degree in Electrical Engineering and Master inInformation Technology, from the University of Paduaand Polytechnic of Milan (Italy), respectively. In 1995he received his PhD in Electrical Engineering from theUniversity of Padua, Italy. From 1993 to 2005, he wasat AT&T Bell Laboratories (USA) and then AT&T Labs-Research (USA) where he worked in the Speech andLanguage Processing division. In 2005 joined the De-partment of Information Engineering and Computer Science (University of Trento).He has pioneered language modeling, understanding and human-machine dia-logue system research. At AT&T labs following the success in the 1994 DARPAspoken language understanding shared-task, he pioneered the well-known ”HowMay I Help You?” research program which led to the first deployment of naturallanguage dialogue systems. His team at University of Trento contributed to theIBM WATSON computer that won the Jeopardy! challenge in 2011. Prof. Ric-cardi has co-authored more than 230 papers and holds more than 90 patents.His research interests are language modeling, understanding, spoken/multimodaldialogue modeling and personal agents and affective computing. Prof. Riccardihas been on the scientific and organizing committee of IEEE, ISCA, ACMand ACLconferences. He has been a founder and Editorial Board member of the ACMTransactions of Speech and Language Processing ( now IEEE/ACM Transactions).He has been elected member of the IEEE SPS Speech Technical Committee(2005-2008). He is a member of ACL, ISCA, ACM and Fellow of IEEE and ISCA.