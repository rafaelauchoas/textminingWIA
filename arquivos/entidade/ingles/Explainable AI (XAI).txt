1202voN11]GL.sc[1v02460.1112:viXraEXPLAINABLE AI (XAI): A SYSTEMATIC META-SURVEY OFCURRENT CHALLENGES AND FUTURE OPPORTUNITIESA PREPRINTWaddah SaeedCenter for Artificial Intelligence ResearchUniversity of AgderGrimstad, Norwaywaddah.waheeb@uia.noChristian OmlinCenter for Artificial Intelligence ResearchUniversity of AgderGrimstad, Norwaychristian.omlin@uia.noNovember 15, 2021ABSTRACTThe past decade has seen significant progress in artificial intelligence (AI), which has resulted inalgorithms being adopted for resolving a variety of problems. However, this success has been metby increasing model complexity and employing black-box AI models that lack transparency. Inresponse to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thusadvance the adoption of AI in critical domains. Although there are several reviews of XAI topics inthe literature that identified challenges and potential research directions in XAI, these challenges andresearch directions are scattered. This study, hence, presents a systematic meta-survey for challengesand future research directions in XAI organized in two themes: (1) general challenges and researchdirections in XAI and (2) challenges and research directions in XAI based on machine learning lifecycle’s phases: design, development, and deployment. We believe that our meta-survey contributes toXAI literature by providing a guide for future exploration in the XAI area.Keywords Explainable AI (XAI) · interpretable · black-box · machine learning · deep learning · challenges · researchdirections · meta-survey1IntroductionArtificial intelligence (AI) has undergone significant and continuous progress in the past decade, resulting in theincreased adoption of its algorithms (e.g., machine learning (ML) algorithms) for solving many problems, even thosethat were difficult to resolve in the past. However, these outstanding achievements are accompanied by increasingmodel complexity and utilizing black-box AI models that lack transparency. Therefore, it becomes necessary to comeup with solutions that can contribute to addressing such a challenge, which could help expand utilizing AI systems incritical and sensitive domains (e.g., healthcare and security domains) where other criteria must be met besides the highaccuracy.Explainable artificial intelligence (XAI) has been proposed as a solution that can help to move towards more transparentAI and thus avoid limiting the adoption of AI in critical domains [1, 2]. Generally speaking, according to [3], XAIfocuses on developing explainable techniques that empower end-users in comperhending, trusting, and efficientlymanaging the new age of AI systems. Historically, the need for explanations dates back to the early works in explainingexpert systems and Bayesian networks [4]. Deep learning (DL), however, has made XAI a thriving research area.Every year, a large number of studies dealing with XAI are published. At the same time, various review studies arepublished covering a range of general or specific aspects of XAI. With many of these review studies, several challengesand research directions are discussed. While this has led to identifying challenges and potential research directions,however, they are scattered.   Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTFigure 1: The five main perspectives for the need for XAI.To the best of our knowledge, this is the first meta-survey that explicitly organizes and reports on the challenges andpotential research directions in XAI. This meta-survey aims to provide a reference point for researchers interested inworking on challenges and potential research directions in XAI.The organization of the paper is as follows. In Section 2, we discuss the need for XAI from various perspectives.Following that, Section 3 tries to contribute to a better distinction between explainability and interpretability. Theprotocol used in planning and executing this systematic meta-survey is presented in Section 4. Afterward, Section 5discusses the challenges and research directions in XAI. Lastly, final remarks are highlighted in Section 6.2 Why Explainable AI is Needed?Nowadays, we are surrounded by black-box AI systems utilized to make decisions for us, as in autonomous vehicles,social networks, and medical systems. Most of these decisions are taken without knowing the reasons behind thesedecisions.According to [1], not all black-box AI systems need to explain why they take each decision because this could resultin many consequences such as reducing systems efficiency and increasing development cost. Generally, explainabil-ity/interpretability is not needed in two situations [5]: (1) results that are unacceptable are not accompanied by severeconsequences, (2) the problem has been studied in-depth and well-tested in practice, so the decision made by theblack-box system is trustworthy, e.g., advertisement system and postal code sorting. Therefore, we should think aboutwhy and when explanations/interpretations can be helpful [1].Based on the retrieved surveys in this work, the need for XAI can be discussed from various perspectives as shown inFig. 1. The perspective groups below are to some extent based on the work in [6]:2Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINT• Regulatory perspective: Black-box AI systems are being utilized in many areas of our daily lives, whichcould resulting in unacceptable decisions, especially those that may lead to legal effects. Thus, it poses anew challenge for the legislation. The European Union’s General Data Protection Regulation (GDPR)1 is anexample of why XAI is needed from a regulatory perspective. These regulations create what is called "right toexplanation," by which a user is entitled to request an explanation about the decision made by the algorithmthat considerably influences them [7]. For example, if an AI system rejects one’s application for a loan, theapplicant is entitled to request justifications behind that decision to guarantee it is in agreement with otherlaws and regulations [8]. However, the implementation of such regulations is not straightforward, challenging,and without an enabling technology that can provide explanations, the "right to explanation" is nothing morethan a "dead letter" [8, 9, 10].• Scientific perspective: When building black-box AI models, we aim to develop an approximate function toaddress the given problem. Therefore, after creating the black-box AI model, the created model representsthe basis of knowledge, rather than the data [11]. Based on that, XAI can be helpful to reveal the scientificknowledge extracted by the black-box AI models, which could lead to discovering novel concepts in variousbranches of science.• Industrial perspective: Regulations and user distrust in black-box AI systems represent challenges to theindustry in applying complex and accurate black-box AI systems [12]. Less accurate models that are moreinterpretable may be preferred in the industry because of regulation reasons [12]. A major advantage of XAI isthat it can help in mitigating the common trade-off between model interpretability and performance [2], thusmeeting these common challenges. However, it can increase development and deployment costs.• Model’s developmental perspective: Several reasons could contribute to inappropriate results for black-box AIsystems, such as limited training data, biased training data, outliers, adversarial data, and model’s overfitting.Therefore, what black-box AI systems have learned and why they make decisions need to be understood,primarily when they affect humans’ lives. For that, the aim will be to use XAI to understand, debug, andimprove the black-box AI system to enhance its robustness, increase safety and user trust, minimize or preventfaulty behavior, bias, unfairness, and discrimination [13]. Furthermore, when comparing models with similarperformance, XAI can help in the selection by revealing the features that the models use to produce theirdecisions [14, 15]. In addition, XAI can serve as a proxy function for the ultimate goal because the algorithmmay be optimized for an incomplete objective [5]. For instance, optimizing an AI system for cholesterolcontrol with ignoring the likelihood of adherence [5].• End-user and social perspectives: In the literature of deep learning [10, 16, 17], it has been shown that alteringan image such that humans cannot observe the change can lead the model in producing a wrong class label.On the contrary, completely unrecognizable images of humans can be recognizable with high confidence usingDL models. Such findings could raise doubts about trusting such black-box AI models [10]. The possibility toproduce unfair decisions is another concern about black-box AI systems. This could happen in case black-boxAI systems are developed using data that may exhibit human biases and prejudices [10]. Therefore, producingexplanations and enhancing the interpretability of the black-box AI systems will help in increasing trustbecause it will be possible to understand the rationale behind the model’s decisions, and we can know if thesystem serves what it is designed for instead of what it was trained for [10, 18]. Furthermore, the demand forthe fairness of black-box AI systems’ decisions, which cannot be ensured by error measures, often leads to theneed for interpretable models [19].The above list is far from complete, and there may be an overlap between these perspectives. However, these highlightthe most critical reasons why XAI is needed.3 From Explainability to InterpretabilityIn the literature, there seems to be no agreement upon what “explainability” or “interpretability” mean. While bothterms are often used interchangeably in the literature, some examples from the selected papers distinguish them[20, 2, 21, 22, 23, 24]. To show that there are no agreement upon definitions, three different definitions from [20, 2, 21]are provided. In [20], the authors stated that “... we consider interpretability a property related to an explanation andexplainability a broader concept referring to all actions to explain.”. In another work [2], interpretability is definedas “the ability to explain or to provide the meaning in understandable terms to a human.”, while “explainability isassociated with the notion of explanation as an interface between humans and a decision-maker that is, at the sametime, both an accurate proxy of the decision-maker and comprehensible to humans...”. Another distinction is drawn in1https://www.privacy-regulation.eu/en/r71.htm3Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINT[21], in which authors stated that “... In the case of interpretation, abstract concepts are translated into insights usefulfor domain knowledge (for example, identifying correlations between layers in a neural network for language analysisand linguistic knowledge). An explanation provides information that gives insights to users as to how a model came toa decision or interpretation.”. It can be noticed from these distinctions that the authors have different definitions forthese two terms. In addition, there is still considerable ambiguity in some of the given distinctions.To contribute to a better distinction between explainability and interpretability, this paper attempts to present a distinctionbetween these terms as follows:Explainability provides insights to a targeted audience to fulfill a need, whereas interpretabilityis the degree to which the provided insights can make sense for the targeted audience’s domainknowledge.There are three components in the definition of explainability, as shown in the above distinction: insights, targetedaudience, and need. Insights are the output from explainability techniques used (e.g., text explanation, featurerelevance, local explanation). These insights are provided to a targeted audience such as domain experts (e.g., medicaldoctors), end-users (e.g., users affected by the model decision), modeling experts (e.g., data scientists). The need forthe provided insights may be to handle any issues discussed in Section 2 such as justifying decisions, discovering newknowledge, improving the black-box AI model, and ensuring fair decisions. That means explainability aims to help thetargeted audience to fulfill a need based on the provided insights from the explainability techniques used.As for interpretability, are the provided explanations consistent with the targeted audience’s knowledge? Do theexplanations make sense to the targeted audience? Is the targeted audience able to reason/inference to supportdecision-making? Are the provided explanations reasonable for the model’s decision?Although the distinction is not ideal, we believe that it represents an initial step toward understanding the differencebetween explainability and interpretability. This paper will use this proposed distinction when discussing the challengesand research directions in XAI.4 Systematic review planning and executionThis work is mainly based on a systematic literature review (SLR) introduced by Kitchenham and Charters [25]. Westarted our SLR by specifying the research question: What are the challenges and research directions in XAI reportedin the existing survey studies? The answer to this question will help researchers and practitioners to know the variousdimensions that one can consider when working in the XAI research area.Having the research question established, the search terms based on the research question are:• XAI keywords: explainable, XAI, interpretable.• Review keywords: survey, review, overview, literature, bibliometric, challenge, prospect, agenda, trend, insight,opportunity, lesson, research directionRelevant and important electronic databases were selected and used for searching the primary studies based on thesearch terms. These databases are:1. Scopus2. Web of Science3. Science Direct4. Institute of Electrical and Electronics Engineers Xplore Digital Library (IEEEXplore)5. Springer Link6. Association for Computing Machinery Digital Library (ACM)7. Google Scholar8. arXivThe last search using these databases was conducted on 16 Feb. 2021. After obtaining search results, all studies wereanalyzed individually to assess their relevance in the context of this SLR. Inclusion and exclusion criteria were used toselect or discard the retrieved studies. The inclusion criteria are the following:• The study presents a survey of explainable AI.4Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTTable 1: Distribution of selected papers per year.Year Number of papers20172018201920202021Total41210221058Table 2: Distribution of selected papers per publication type.Publication typeNumber of papersJournal articlesConference papersarXivBook chapterBookTotal2812125158Table 3: Distribution of selected papers per publisher.Publisher Number of papersSpringerarXivIEEEElsevierACMFrontiersMDPIWileySageOthersTotal12129762111758• The study presents challenges and/or research directions for XAI.On the other hand, the exclusion criteria are the following:• The study is not written in English.• The study presents a survey of XAI without discussing any challenges or research directions.The retrieved studies were first analyzed by their titles and abstracts to decide if the paper matched the first inclusioncriterion. If matched, the paper was analyzed in detail in the second step. In the second step, the exclusion criteria andthe second inclusion criterion were checked.We reviewed the list of references of the selected studies to include other papers that may not be retrieved from theselected electronic databases, which resulted in retrieving eight non-survey papers that reported challenges and/orresearch directions in XAI [26, 5, 27, 28, 19, 11, 29, 30].Overall, the total number of selected papers is 58. The majority of the selected papers were published in 2020, as shownin Table 1. In 2021, we found 10 papers. However, since the last search was in Feb 2021, we expect more publicationsto appear until the end of the year. As shown in Table 2, the primary outlet for the selected papers are journal articlesfollowed by conference papers and arXiv papers. The distribution of the selected papers per publisher is shown in Table3.5Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTFigure 2: The proposed organization to discuss the challenges and research directions in XAI. For simplicity, the arrowsthat show the flow in the life cycle are removed.5 DiscussionThis section discusses the challenges and research directions in XAI. In order to place them in a meaningful context, thediscussion is organized into two main themes, as shown in Fig. 2. The first theme focuses on the general challengesand research directions in XAI. The second theme is about the challenges and research directions of XAI based onthe ML life cycle’s phases. For simplicity, we divided the life cycle into three main phases: design, development, anddeployment phases. The following subsections are shed light on these challenges and research directions.5.1 General Challenges and Research Directions in XAIIn this section, we reported the general challenges and research directions in XAI.5.1.1 Towards more formalismIt is one of the most raised challenges in the literature of XAI [1, 31, 10, 32, 9, 41, 33, 26, 43, 42, 2, 34, 8, 44, 35,36, 37, 18, 45, 38, 39, 40, 46]. It was suggested that more formalism should be considered in terms of systematicdefinitions, abstraction, and formalizing and quantifying [1].Starting from the need for systematic definitions, until now, there is no agreement on what an explanation is [10].Furthermore, it has been found that similar or identical concepts are called by different names and different conceptsare called by the same names [1, 31]. In addition, without a satisfying definition of interpretability, how it is possibleto determine if a new approach better explains ML models [32]? Therefore, to facilitate easier sharing of results andinformation, definitions must be agreed upon [1, 31].With regards to the abstraction, many works have been proposed in an isolated way; thus, there is a need to beconsolidated to build generic explainable frameworks that would guide the development of end-to-end explainableapproaches [1]. Additionally, taking advantage of the abstraction explanations in identifying properties and generatinghypotheses about data-generating processes (e.g., causal relationships) could be essential for future artificial generalintelligence (AGI) systems [31].Regarding the formalization and quantification of explanations, it was highlighted in [1] that some current worksfocus on a detailed problem formulation which becomes irrelevant as the method of interpretation or the explanation6Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTTable 4: A summary of the selected papers, categorized by phases as well as challenges and research directions.PhasesChallenges and research directionsPapersGeneralTowards more formalismMultidisciplinary research collaborationsExplanations and the nature of user experience and expertiseXAI for trustworthiness AIInterpretability vs. performance trade-offCausal explanationsContrastive and counterfactual explanationsXAI for non-image, non-text, and heterogeneous dataExplainability methods compositionChallenges in the existing XAI models/methodsNatural language generationAnalyzing models, not dataCommunicating uncertaintiesTime constraintsReproducibilityThe economics of explanationsDesignCommunicating data qualityData sharingDevelopmentKnowledge infusionDeveloping approaches supporting explaining the training pro-cessDeveloping model debugging techniquesUsing interpretability/explainability for models/architecturescomparisonDeveloping visual analytics approaches for advanced DL archi-tecturesSparsity of analysisModel innovationRules extractionBayesian approach to interpretabilityExplaining competenciesDeploymentImproving explanations with ontologiesXAI and privacyXAI and securityXAI and safetyHuman-machine teamingExplainable agencyMachine-to-machine explanationXAI and reinforcement learningExplainable AI planning (XAIP)Explainable recommendationXAI as a service∗ A non-peer-reviewed paper from arXiv.7[1, 31, 10, 32, 9, 33, 26, 2, 34, 8, 35,36, 37, 18, 38, 39, 40], [41, 42, 43, 44,45, 46] ∗[47, 35, 18, 48, 49, 50], [45, 51, 42] *[28, 24, 52, 26, 39, 2], [43, 41, 42] ∗[36, 30, 20, 22, 45, 53, 24, 2],[41]*[2, 28, 26, 54, 35][32, 26, 53, 36, 55][56], [43] ∗[34, 48], [57, 42]*[26, 1, 53, 30], [42] ∗[26, 32, 8][42]*[11][24, 32][41, 5]*[9][1][20, 18][58][59, 60, 38], [46, 23]∗[59, 24, 28][60], [41]∗[24][24, 59][61]*[62, 53][63, 50, 49][22][28][18][35, 58][2, 64, 62, 30][65, 66, 2][40, 30, 28, 66, 1, 50, 59, 24], [45,46]*[67][29, 68, 1][19, 10, 69], [61]*[1],[27]*[70, 66][11]Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTdiffers. Therefore, regardless of components that may differ, the expansibility problem must be generalized andformulated rigorously, and this will improve the state-of-the-art for identifying, classifying, and evaluating sub-issues ofexplainability [1].Establishing formalized rigorous evaluation metrics need to be considered as well [1]. However, due to the absence of anagreement on the definitions of interpretability/explainability, no established approach exists to evaluating XAI results[9]. The lack of ground truth in most cases is the biggest challenge for rigorous evaluations [41, 32]. So far, differentevaluation metrics have been proposed, such as reliability, trustworthiness, usefulness, soundness, completeness,compactness, comprehensibility, human-friendly or human-centered, correctness or fidelity, complexity, generalizability[41]. However, it seems that there are two main evaluation metrics groups: objective and human-centered evaluations[32]. The former is quantifiable mathematical metrics, and the latter relies on user studies [32]. Further progressare needed towards evaluating XAI techniques’ performance and establishing objective metrics for evaluating XAIapproaches in different contexts, models, and applications [2]. Achieving that may help in developing a model-agnosticframework that can suggest the most appropriate explanation taking into account problem domain, use case, and user’stype [33].5.1.2 Multidisciplinary research collaborationsOne area of research that can offer new insights for explainable methods is working closely with researchers from otherdisciplines such as psychology, behavioral and social sciences, human-computer interaction, physics, and neuroscience.Multidisciplinary research is therefore imperative to promote human-centric AI and expand utilizing XAI in criticalapplications [45].Several studies, for instance [35, 18, 42, 48, 49, 47, 50], have been suggested some potential multidisciplinary researchworks. In [35], it has been highlighted that approaching psychology discipline can help to get insights on both thestructure and the attributes of explanations and the way they can influence humans. They also have suggested thatdefining the context of explanations is an important research direction. Here, it is essential to consider the domain ofapplication, the users, type of explanations (e.g., textual, visual, combinations of solutions), and how to provide theexplanations to the users. This research direction can form a connection with behavioral and social sciences. The paperin [47] also has shown that XAI can benefit from the work in philosophy, cognitive psychology/science, and socialpsychology. The paper summarizes some findings and suggests ways to incorporate these findings into work on XAI.Approaching HCI studies are essential to XAI. However, few user experiments have been conducted in the area ofexplainability [18]. Therefore, more should be conducted to study the topic adequately [18]. Humans must be includedin the process of creating and utilizing XAI models, as well as enhancing their interpretability/explainability [35]. In[42], it has been highlighted that interactive tools may help users understand, test, and engage with AI algorithms,thereby developing new approaches that can improve algorithms’ explainability. Furthermore, interactive techniquescan help users to interpret predictions and hypothesis-test users’ intuitions rather than relying solely upon algorithmsto explain things for them. In [50], it has been suggested drawing from the HCI research on interaction design andsoftware learnability to improve the usability of intelligible or explainable interfaces. Additionally, HCI researchers cantake advantage of the theoretical work on the cognitive psychology of explanations to make understandable explanations.They can also empirically evaluate the effectiveness of new explanation interfaces.The advances in neuroscience should be of great benefit to the development and interpretation of DL techniques(e.g., cost function, optimization algorithm, and bio-plausible architectural design) owing to the close relationshipbetween biological and neural networks [49]. It is imperative to learn from biological neural networks so that better andexplainable neural network architectures can be designed [49]. Finally, connecting with physics and other disciplinesthat have a history of explainable visual methods might provide new insights for explainable methods [48].5.1.3 Explanations and the nature of user experience and expertiseBased on the nature of the application, users who use ML models can vary (e.g., data scientists, domain experts,decision-makers, and non-experts). The nature of user experience and expertise matters in terms of what kind ofcognitive chunks they possess and the complexity they expect in their explanations [5]. In general, users have varyingbackgrounds, knowledge, and communication styles [5]. However, it seems that the current focus of explanationmethods is tailored to users who can interpret the explanations based on their knowledge in the ML process [30, 41].The works in [41, 43, 28, 24, 52, 26, 39] have highlighted what is needed to be considered with regards to explanationsand the nature of user experience and expertise. In [41], user-friendly explanations have been suggested so users caninterpret the explanations with less technical knowledge. Therefore, figuring out what to explain should follow theidentification of the end-user. In [43], it has been highlighted that previous works in explainable AI systems (e.g., expertsystems) generally neglected to take into account the knowledge, goals, skills, and abilities of users. Additionally, the8Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTgoals of users, systems, and explanations were not clearly defined. Therefore, clearly stating goals and purposes areneeded to foster explanations testing within the appropriate context. In [52], the authors have discussed that identifyingthe users’ goals and keeping up with their dynamic nature means collecting more data from them. It is also essentialto develop changes detection approaches of goals and needs for the purpose of adapting these changes to end-users.For a deeper understanding of these dynamics, user studies (e.g., diary studies, interviews, and observation) can helpdevelop guidelines for developing long-term explainable systems and determining which user data to gather to improvepersonalization.In [28], it has been suggested that abstraction can be used to simplify the explanations. Understanding how abstractionsare discovered and shared in learning and explanation is an essential part of the current XAI research. The work in [26]has mentioned that the inclusion of end-users in the design of black-box AI models is essential, especially for specificdomains, e.g., the medical domain. That would help to understand better how the end-users will use the outputs andinterpret explanations. It is a way to educate them about the predictions and explanations produced by the system. In[24], the authors have discussed that utilizing users’ previous knowledge is a significant challenge for visualizationtools today. Customizing visualization tools for different user types can be useful at several stages of the ML modelpipeline. However, to use prior users’ knowledge in predictive models, it is important to establish processes to digitallycapture and quantify their knowledge.In [42], it has been mentioned that DL models often use concepts that are unintelligible to predict outcomes. Therefore,using systems that use such models requires human-centric explanations that can accurately explain a decision and makesense to the users (e.g., medical domain expert). An approach to come with human-centric explanations is examiningthe role of human-understandable concepts acquired by DL models. It is also essential to analyze the features usedby the DL models in predicting correct decisions, but based on incorrect reasoning. Having an understanding of themodel’s concepts would help reduce reliability concerns and develop trust when deploying the system, especially incritical applications. The authors also highlighted the importance of addressing the domain-specific needs of specificapplications and their users when developing XAI methods. Finally, the work in [2] has discussed that XAI can facilitatethe process of explaining to non-experts how a model reached a given decision, which can substantially increaseinformation exchange among heterogeneous people regarding the knowledge learned by models, especially whenworking in projects with multi-disciplinary team.To sum up, it is crucial to tailor explanations based on user experience and expertise. Explanations should be provideddifferently to different users in different contexts [71]. In addition, it is also essential to clearly define the goals ofusers, systems, and explanations. Stakeholder engagement and system design are both required to understand whichexplanation type is needed [71].5.1.4 XAI for trustworthiness AIIncreasing the use of AI in everyday life applications will increase the need for AI trustworthiness, especially insituations where undesirable decisions may have severe consequences [41]. The High-Level Expert Group in EuropeanCommission put seven essentials for achieving trustworthy AI 1: (1)human agency and oversight; (2) robustness andsafety; (3) privacy and data governance; (4) transparency; (5) diversity, non-discrimination and fairness; (6) societaland environmental well-being; and (7) accountability. The discussion about privacy, security, and safety are given inXAI and Privacy Section, XAI and Security Section, and XAI and Safety Section, respectively. The discussion in thissection is for what is reported in the selected papers regarding fairness and accountability.With regards to fairness, ML algorithms must not be biased or discriminatory in the decisions they provide. However,with the increased usage of ML techniques, new ethical, policy, and legal challenges have also emerged, for example, therisk of unintentionally encoding bias into ML decisions [22]. Meanwhile, the opaque nature of data mining processesand the complexity of ML make it more challenging to justify consequential decisions [22]. The work in [45] arguesthat data, algorithmic, and social biases need to be remedied in order to promote fairness. Further, it is imperative tobe able to analyze AI systems to have trust in the model and its predictions, especially for some critical applications.Researchers started trying to form a definition of fairness and the meaning of fairness in an algorithm as discussed in[22]. According to [22], it would also be necessary to devise new techniques for discrimination-aware data mining. It isalso worth noting that when converting fairness into a computational problem, we need to keep the fairness measuresfair [22]. The work in [30] states that it is possible to visualize learned features using XAI methods and assess biasusing methods other than explanation methods. On the other hand, regulations and laws are necessary for the suspicionabout unfair outcomes [30].Having accountability means having someone responsible for the results of AI decisions if harm occurs. In [71], it hasbeen mentioned that investigating and appealing decisions with major consequences for people is an important aspect1https://ec.europa.eu/commission/presscorner/detail/en/IP_19_18939Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTof systems of accountability, and some current regulations also aim to achieve this. XAI can be an important factor insystems of accountability by providing users with the means to appeal a decision or modify their behavior in the futureto achieve a better result. However, more work should be done to establish an environment that promotes individualautonomy and establish a system of accountability. It has also been discussed in [22] that developing procedures fortesting AI algorithms for policy compliance is necessary so that we can establish whether or not a given algorithmadheres to a specific policy without revealing its proprietary information. It is also desirable for a model to specifyits purposes and provide external verification of whether these goals are met and, if not, describe the causes of thepredicted outcomes.The use of XAI can enhance understanding, increase trust, and uncover potential risks [45]. Therefore, when designingXAI techniques, it is imperative to maintain fairness, accountability, and transparency [45]. On the other hand, it isnecessary to highlight that not only black-box AI models are vulnerable to adversarial attacks, but also XAI approaches[72]. There is also a risk that to promote trust in black-box AI models predictions; explainers may be more persuasivebut misleading than informative, so users may become deceived, thinking the system to be trustworthy [36, 71]. It ispossible to increase trust through explanations, but explanations do not always produce systems that produce trustworthyoutputs or ensure that system implementers make trustworthy claims about its abilities [71].The work in [20] discusses measures to create trustworthy AI. It has been highlighted that before employing AIsystems in practice, it is essential to have quantitative proxy metrics to assess explanation quality objectively, compareexplanation methods, and complement them with human evaluation methods (e.g., data quality reporting, extensivetesting, and regulation).Finally, it is good to note that a further explore the idea of Responsible AI with a discussion about principles of AI,fairness, privacy, and data fusion can be found in [2].5.1.5 Interpretability vs. performance trade-offThe belief that complicated models provide more accurate outcomes is not necessarily correct [73]. However, thiscan be incorrect in cases when the given data is structured and with meaningful features [73]. In a situation wherethe function being approximated is complex, that the given data is widely distributed among suitable values for eachvariable and the given data is sufficient to generate a complex model, the statement “models that are more complex aremore accurate” can be true [2]. In such a situation, the trade-off between interpretability and performance becomesapparent [2].When the performance is coupled with model complexity, model interpretability is in question [2]. Explainabilitytechniques, however, could help in minimizing the trade-off [2]. However, according to [26], what determines thistrade-off? and who determines it? The authors have highlighted the importance of discussing with end-users thistrade-off so that they can be aware of the potential risks of misclassification or opacity. Another point that should beconsidered is the approximation dilemma: models need to be explained in enough detail and in a way that matches theaudience for whom they are intended while keeping in mind that explanations reflect the model and do not oversimplifyits essential features [2]. Even though studying the trade-off is essential, it is impossible to proceed without standardizedmetrics for assessing the quality of explanations [54].Another possible solution for the trade-off is suggested in [35] which is developing fully transparent models throughoutthe entire process of creation, exploitation, and exploration and can provide local and global explanations. In turn, thisleads to using methods that embed learning capabilities to develop accurate models and representations. The methodsshould also be able to describe these representations in effective natural language consistent with human understandingand reasoning.5.1.6 Causal explanationsDeveloping causal explanations for AI algorithms (i.e., why they made those predictions instead of how they arrived atthose predictions) can help increasing human understanding [36]. In addition, causal explanations strengthen models’resistance to adversarial attacks, and they gain more value when they become part of decision-making [32]. However,there can be conflicts between predicting performance and causality [32]. For example, when the confounder, which isa variable that influences both the dependent variable and independent variable, is missing from the model [32].Causal explanations are anticipated to be the next frontier of ML research and to become an essential part of the XAIliterature [26, 53]. There is a need for further research to determine when causal explanations can be made from an MLmodel [32]. In addition, according to a recent survey on causal interpretability for ML [55], it has been highlighted theabsence of ground truth data for causal explanations and verification of causal relationships make evaluating causal10Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTinterpretability more challenging. Therefore, more research is needed to guide on how to evaluate causal interpretabilitymodels [55].5.1.7 Contrastive and counterfactual explanationsContrastive explanations describe why one event occurred but not another, while counterfactual explanations describewhat is needed to produce a contrastive output with minimal changes in the input [56]. Questions in the contrastiveform "Why x and not y?" and questions of the counterfactual form "What if?" and "What would happen if?" [43].In a recent survey of contrastive and counterfactual explanations [56], it has been found that contrastive and counter-factual explanations help improve the interaction between humans and machines and personalize the explanation ofalgorithms. A further important point as observed by [56] that one of the significant barriers towards a fair assessmentof new frameworks is the lack of standardization of evaluation methods. The theoretical frameworks are also foundinadequate for applying to XAI as a result of the disconnect between the philosophical accounts of counterfactualexplanation to scientific modeling as well as ML-related concepts. Furthermore, it has been found that different domainsof science define counterfactual explanations differently, as do the approaches used to solve specific tasks.In the light of possible research directions in this point, it has been suggested in [56] the importance of includingend-users in the evaluation of generated explanations since these explanations are designed to be user-oriented. Inaddition, since contrastive and counterfactual explanations address causal and non-causal relationships, new horizonsopen to the XAI community by unifying causal and non-causal explanatory engines within a contfactually-drivenframework. Furthermore, bringing together researchers from the humanities and the computational sciences couldcontribute to further development for contrastive and counterfactual explanations generation.5.1.8 XAI for non-image, non-text, and heterogeneous dataThe focus of XAI works is mainly on image and text data. However, other data types exist but until now not wellexplained, such as sequences, graphs, and Spatio-temporal data.Using visualization to transform non-image data into images creates opportunities to discover explanations throughsalient pixels and features [48]. However, this should not be the only way for explainability for non-image or non-textdata. For example, existing explanation approaches for image or text data need to be adjusted to be used with graphdata (without any transformation) [57]. Additionally, there is a need to develop new approaches for explaining theinformation that exists with non-image or non-text data, e.g., structural information for graph data [57].Finally, with the advent of AI systems that use various types of data, explainability approaches that can handle suchheterogeneity of information are more promising [34]. For example, such systems can simulate clinicians’ diagnosticprocesses in the medical domain where both images and physical parameters are utilized to make decisions [42]. Thus,they can enhance the diagnostic effectiveness of the systems as well as explain phenomena more thoroughly [42].5.1.9 Explainability methods compositionFor specific applications in healthcare (e.g., predicting disease progression), several types of explanations at differentlevels are needed (e.g., local and global explanations) [26] in order to provide the most complete and diverse explanationswe can [42]. This is derived from the way clinicians communicate decisions utilizing visualizations and temporalcoherence as well as textual descriptions [42].Some overlap exists between explainability methods, but for the most part, each seems to address a different question[53]. According to [1], combining various methods to obtain more powerful explanations is rarely considered. Inaddition, rather than using disparate methods separately, we should investigate how we can use them as basic componentsthat can be linked and synergized to develop innovative technologies [1]. It is argued that enabling composability inXAI may contribute to enhancing both explainability and accuracy [1]. Furthermore, it could help to provide answers ina simple human interpretable language [30]. First efforts, as cited in [42], have been recently made as in [74] where theauthors proposed a model that can provide visual relevance and textual explanations.5.1.10 Challenges in the existing XAI models/methodsThere are some challenges in the existing XAI models/methods that have been discussed in the literature. Starting withscalability, which is a challenge that exists in explainable models as discussed in [26]. For example, each case requiringan explanation entails creating a local model using LIME explainable model [75]. The scalability can be an issue whenthere is a huge number of cases for which prediction and explanation are needed. Likewise, when computing Shapley11Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTvalues [76], all combinations of variables must be considered when computing variable contributions. Therefore, suchcomputations can be costly for problems that have lots of variables.Feature dependence presents problems in attribution and extrapolation [32]. If features are correlated, attribution ofimportance and features effects becomes challenging. For sensitivity analyses that permute features, when the permutedfeature has some dependence on another feature, the association breaks, resulting in data points outside the distribution,which could cause misleading explanations.In [8], the authors discussed some limitations with heatmaps explanations. Heatmaps explanations visualize whatfeatures are relevant for making predictions. However, the relation between these features, either individually or incombination, remains unclear. Low abstraction levels of explanations are another limitation. Heatmaps highlight thatspecific pixels are significant without indicating how the relevance values relate to abstract concepts in the image,such as objects or scenes. The model’s behavior can be explained in more abstract, more easily understood waysby meta-explanations that combine evidence from low-level heatmaps. Therefore, further research is needed onmeta-explanations.Model-based (i.e., ante-hoc models) and post-hoc explainability models have some challenges, as have been discussedin [77]. When model-based methods cannot predict with reasonable accuracy, practitioners start the search for moreaccurate models. Therefore, one way to increase the usage of model-based methods is to develop new modeling methodsthat maintain the model’s interpretability and render more accurate predictions. More details about this direction areprovided in [73]. Further, for model-based methods, there is a need to develop more tools for feature engineering. Itis possible to achieve comparable predictive accuracy in some applications when the features are more informativeand meaningful. Therefore, enhancing the possibility of model-based methods can be accomplished by producingmore useful features. Two categories of works can help achieve that: improve tools for exploratory data analysisand improve unsupervised techniques. The former helps to understand the data, and domain knowledge could helpto identify helpful features. The latter is needed because unsupervised techniques are often used to identify relevantstructures automatically, so advances in unsupervised techniques may result in better features.The authors in [77] have also discussed some challenges for post-hoc explainability models. According to the authors,it is challenging to determine what format or combination of formats will adequately describe the model’s behavior.Furthermore, there is uncertainty over whether the current explanation methods are adequate to capture a model’sbehavior or novel methods still needed. Another challenge is if post-hoc explanations methods identify learnedrelationships by the model that practitioners know to be incorrect, is it possible that practitioners fix these relationshipslearned and increase the predictive accuracy? Further research in pos-thoc explanations can help exploit prior knowledgeto improve the predictive accuracy of the models.5.1.11 Natural language generationExplaining in natural language needs to be accurate, useful, and easy to understand [78]. Furthermore, in order toproduce good quality explanations, the generated explanations need to be tailored to a specific purpose and audience, benarrative and structured, and communicate uncertainty and data quality that could affect the system’s output [78].Four challenges that are crucial in generating good quality explanations have been discussed in [78]:• Evaluation challenge: Develop inexpensive but reliable ways of estimating scrutability, trust, etc. Do wehave a chance to obtain reliable results if we ask users to read explanations and estimate, for example,scrupability? What experimental design gives the best results? Before we do these steps, should we make surethe explanations are accurate?• Vague Language challenge: Using vague terms in explanations is much easier to understand by humansbecause they think in qualitative terms [79]. However, how can vague language be used in explanations, suchthat the user does not interpret it in a way that will lead to a misunderstanding of the situation? In addition,setting the priority of messages based upon features and concepts that the user is aware of would be helpful.Furthermore, phrasing and terminology used should be intuitive to users.• Narrative challenge: Explaining symbolic reasoning narratively is more straightforward to comprehend thannumbers and probabilities [80]. Therefore, we need to develop algorithms for creating narrative explanationsto present the reasoning.• Communicating data quality challenge: Techniques should be developed to keep users informed when dataproblems affect results. We have discussed this issue in detail in Communicating Data Quality Section.Another challenge has been discussed in [42]. In some medical domains, it could be necessary for AI systems to generatelong textual coherent reports to mimic the behavior of doctors. The challenge here is that after generating a few coherent12Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTsentences, language generation models usually start producing seemingly random words that have no connection topreviously generated words. One of the solutions to this problem would be to use transformer networks [81] as languagemodel decoders, which can capture word relationships in a longer sentence. In order to evaluate the generated reports,it is essential to compare them with human-generated reports. However, since human-generated reports are usuallyfree-text reports (i.e., not following any specific template), it is important to first eliminate unnecessary information forthe final diagnosis from human-generated reports then conduct the comparison.5.1.12 Analyzing models, not dataThe author in [11] has discussed that analyzing models instead of data is a possible future of ML interpretability. Ithas been mentioned that a possible way to extract knowledge from data is through interpretable ML. That is becausean ML model can automatically identify if and how features are important to predicting outcomes and recognize howrelationships are represented.He added that there is a need to move from analyzing assumption-based data models to analyzing assumption-freeblack-box AI models. That’s because making assumptions about the data (i.e., distribution assumptions) is problematic.Typically, they are wrong (do not follow the Gaussian distribution), hard to check, extremely inflexible, and difficultto automate. Further, assumption-based data models in many domains are typically less predictive than black-box AImodels (i.e., generalization) when having lots of data, which is available due to digitization. Therefore, the author hasargued that there should be a development of all the tools that statistics offer for answering questions (e.g., hypothesistests, correlation measures, interaction measures) and rewrite them for black-box AI models. To some extent, this isalready taking place. For example, in a linear model, the coefficients quantify the effects of an individual feature on theresult. The partial dependent plot [82] represents this idea in a more generalized form.5.1.13 Communicating uncertaintiesCommunicating uncertainty is an important research direction because it can help to inform the users about theunderlying uncertainties in the model and explanations. According to [24], there are already inherent uncertainties in MLmodels; and model refinement efforts by developers may introduce new uncertainties (e.g., overfitting). Furthermore,some explanation methods such as permutation feature importance and Shapley value give explanations withoutmeasuring the uncertainty implied by the explanations [32].Quantifying uncertainty is an open research topic [24]. However, some works exist towards quantifying uncertaintyas discussed in [32]. The uncertainty surrounding ML models can take many forms and occur throughout the MLlife phases [24]. Therefore, in order to make progress, it is needed to become more rigorous in studying and reliablyquantifying uncertainties at model’s various phases and with the explanation methods and communicate them to theusers, then users can respond accordingly [24, 32].5.1.14 Time constraintsTime is an essential factor in producing explanations and in interpretation. Some explanations must be producedpromptly to let the user react to the decision [41]. Producing explanations efficiently can save computing resources,thereby making it useful for industrial use or in environments with limited computing capability [41]. In some situations(e.g., plant operation application), the provided explanations need to be understood quickly to help the end-user to makea decision [5]. On the other hand, in some situations (e.g., scientific applications), users would likely be willing todevote considerable time understanding the provided explanation [5]. Therefore, time is an essential factor consideringthe situation, available resources, and end-users.5.1.15 ReproducibilityIn a recent review of XAI models based on electronic health records, it has been found that research reproducibility wasnot stressed well in the reviewed literature, though it is paramount [9]. In order to facilitate comparisons between newideas and existing works, researchers should use open data, describe the methodology and infrastructure they used, andshare their code [9]. In addition, it has been suggested that publication venues should establish reproducibility standardsthat authors must follow as part of their publication process [9].5.1.16 The economics of explanationsResearch into the economic perspective of XAI is sparse, but it is essential [1]. With the pressures of social and ethicalconcerns about trusting black-box AI models, XAI has the potential to drive a real business value [1]. XAI, however,comes at a cost [83].13Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTRecently, the work in [84] identified costs of explanations in seven main categories (1) costs of explanations design, (2)costs of creating and storing audit logs, (3) costs of trade secrets violation (e.g., the forced disclosure of source code),(4) costs of slowing down innovation (e.g., increasing time-to-market), (5) costs of reducing decisional flexibility if thefuture situation does not justify the previous explanation, (6) cost of conflict with security and privacy matters, and (7)costs of using less efficient models for their interpretability. Therefore, costs associated with algorithmic explanationsshould be incurred when the benefits of the explanations outweigh the costs [83].Cost estimation is one of the issues that should be addressed by encouraging economic interpretations. Other issuesinclude algorithms proprietary, revealing trade secrets, and predicting XAI market evolution [1].5.2 Challenges and Research Directions of XAI in the Design PhaseIn this phase, the data is collected from at least one source. Then, the data preparation step is done to prepare thecollected data for the training phase. By grouping what was collected from the selected papers, we identified two mainchallenges that need further research: communicating data quality and data sharing.5.2.1 Communicating data qualityThe provided explanations for the AI system or its outcomes depend on the data used to build the system. Data bias,data incompleteness, and data incorrectness are issues that affect the quality of the data. Training AI systems usinglow-quality data will be reflected in their outcomes [78]. For example, an AI system developed for lung cancer risksprediction using data from Americans may not accurately estimate risks for a resident of Delhi due to the differences inpolluted environments in which they are living at [78]. So, what can be of high quality for a particular purpose canbe of low quality for another [20]. Reducing system accuracy is not the only consequence of building an AI systemusing low-quality data; producing unfair decisions and degrading the explainability of the AI system are other possibleconsequences.With this in mind, it has been suggested to be aware of how data was collected and any limitations associated withthe collected data [71]. Further, it has been highlighted the importance of clarifying any data issues that can reduceaccuracy when producing explanations [78]. However, how can we communicate data quality to users to let them knowhow the results are influenced by data used.In [85], the authors discussed several issues that arise when producing explanations for AI models that use imputationof missing data. They recommended disclaimers accompanied by the derived explanations and educating end-usersabout the risks involved of incorrect explanations. Even though it is good to come with appropriate disclaimers, webelieve that future studies should be undertaken to develop a practical and measurable way to communicate data qualityto users. Proposing dimensions of data quality could be the basis for that. We recommend starting with the followingquestions which is inspired from the work in [86]:• Which essential dimensions of data quality are wanted?• What are the definitions of those dimensions? and how to measure them?• How to deal with them to improve the AI model and hence its explanations?• How to communicate them (and highlight any possible risks)?According to [18], there is a variety of data quality dimensions such as completeness, accuracy, and consistency. For anextensive list of dimensions of data quality that occur in information systems, the reader may refer to the research paperin [86]. The fairness dimension can also be included, which may include demographic parity differences. It is essentialto highlight that the way that can be used to communicate data quality can vary based on the type of users.5.2.2 Data sharingData privacy and data security are two major issues concerning XAI. Since AI is used as a data-driven method andbecause any requested explanations depend on data used to build AI systems, two main aspects related to data shouldbe considered: data sharing and data preservation.Data sharing in this context means making raw data available to be used by other partners [58]. Data preservation is theretention of raw data, at least until we stop using the AI solution. The discussion here focuses on data sharing, which isrelated to the data collection and preparation phase. Challenges related to data preservation are discussed later in XAIand Privacy Section.Data sharing is a significant challenge in many data-driven solutions [58]. Two common strategies are used for datasharing between the partners: share raw data directly or send it to a central analysis repository [58]. At this phase,14Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTno explanations that may violate privacy are demanded because the solutions are still not developed yet. However,it is essential for users (e.g., patients) to feel confident that their data is secured and protected from unauthorizedaccess and misuse, and any processes are limited to the part of data that they have consented to [58]. According to[58], the implementation of watermarking or fingerprinting are typical reactive techniques used to deal with this issue.Watermarking techniques prove the authenticity and ownership of a dataset, while fingerprinting techniques help toidentify the data leak because partners receive the same basic set but marked differently with their fingerprints [58].Federated learning can be a possible solution to avoid raw data sharing. Federated learning allows building ML modelsusing raw data distributed across multiple devices or servers [87, 88]. As described in [88], the training using federatedlearning starts by sending initial model parameters by the central server, which are obtained after a few trainingiterations, to a set of clients. Then, each client uses its resources to train an ML model locally on its own datasetusing the shared parameters. Afterward, each client sends the server an updated version of the parameters. As a resultof aggregating clients’ parameters, the server creates a global model. As soon as the global model reaches a certainaccuracy level, the training process is stopped.Even though the data never leaves the user’s device, increasing the number of clients involved in a collaborative modelmakes it more susceptible to inference attacks intended to infer sensitive information from training data [89, 88].Possible research directions to deal with privacy challenges of federated learning have been discussed in [88] such asprivacy-preserving security assurance, defining optimal bounds of noise ratio, and proposing granular and adaptiveprivacy solutions.5.3 Challenges and Research Directions of XAI in the Development PhaseThere are three main types of learning in ML: supervised, unsupervised, and reinforcement learning. In supervisedlearning, a learning algorithm is used to train an ML model to capture patterns in the training data that map inputsto outputs. With unsupervised learning, which is used when only the input data is available, an ML model is trainedto describe or extract relationships in the training data. For reinforcement learning, an ML model is trained to makedecisions in a dynamic environment to perform a task to maximize a reward function. In the following subsections, wediscuss the challenges and research directions during developing ML models.5.3.1 Knowledge infusionA promising research direction is incorporating human domain knowledge into the learning process (e.g., to capturedesired patterns in the data). According to [46], understanding how experts analyze images and which regions of theimage are essential to reaching a decision could be helpful to come with novel model architectures that mimic thatprocess. Furthermore, our explanations can be better interpretable and more informative if we use more domain/task-specific terms [23].Recently, the work in [90] highlights various ways of incorporating approaches for medical domain knowledge withDL models such as transfer learning, curriculum learning, decision level fusion, and feature level fusion. Accordingto that survey, it was seen that with appropriate integrating methods, different kinds of domain knowledge could beutilized to improve the effectiveness of DL models. A review focused on knowledge-aware methods for XAI is givenby [38]. Based on the knowledge source, two categories are identified: knowledge methods and knowledge-basedmethods. Unstructured data is used as a knowledge source in knowledge methods, while knowledge-based methods usestructured knowledge to build explanations. According to [38], when we use external domain knowledge, we are able toproduce explanations that identify important features and why they matter. As concluded in that survey, many questionsremain unanswered regarding utilizing external knowledge effectively. For instance, in a vast knowledge space, howcan relevant knowledge be obtained or retrieved? To demonstrate this point, let us take the Human-in-the-loop approachas an example. Typically, a user has a wide range of knowledge in multiple domains; thus, the XAI system must ensurethat the knowledge provided to the user is desirable.Recent works in the knowledge that can be incorporated during training ML are given in [91, 92, 60]. In [91], a one-shotlearning technique was presented for incorporating knowledge about object categories, which may be obtained frompreviously learned models, to predict new objects when very few examples are available from a given class. Anotherwork in [92] has shown how knowledge graph is integrated into DL using knowledge-infused learning and presentedexamples on how to utilize knowledge-infused learning towards interpretability and explainability in education andhealthcare. The work in [60] has mentioned that the middle-to-end learning of neural networks with weak supervisionvia human-computer interaction is believed to be a fundamental research direction in the future.Based on all that, it can be seen that using XAI to explain the outcomes of the models (e.g., pointing which regions ofthe image were used to reach the decision) can help to understand better what was learned from the incorporated humanknowledge. Thus, it would help to adjust the way used in incorporating the knowledge or come with innovations in15Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTmodel architectures. Furthermore, it could be used to confirm whether a model follows the injected knowledge andrules, especially with critical applications, e.g., autonomous driving model [59]. Therefore, more research is needed toinvestigate how experts can interact with ML models to understand them and improve their abilities, which would be apromising direction in which XAI can contribute.5.3.2 Developing approaches supporting explaining the training processTraining ML models, especially DL, is a lengthy process that usually takes hours to days to finish, mainly because ofthe large datasets used to train the models [59]. Therefore, researchers and practitioners have contributed to developingsystems that could help steer the training process and develop better models.Examples of progressive visual analytics systems are cited in [59]. For example, DeepEyes [93] is an example of aprogressive visual analytics system that enables advanced analysis of DNN models during training. The system canidentify stable layers, identify degenerated filters that are worthless, identify inputs that are not processed by any filterin the network, reasons on the size of a layer, and it helps to decide whether more layers are needed or eliminateunnecessary layers. DGMTracker is another example [94] which is developed for better understanding and diagnosingthe training process of deep generative models (DGMs). In addition, big tech companies such as Google and Amazonhave developed toolkits to debug and improve the performance of ML models such as Tensor-Board1 and SageMakerDebugger2.Future studies to deal with this challenge are therefore recommended in order to develop XAI approaches supportingthe online training monitoring to get insights that could help to steer the training process by the experts, which couldhelp in developing better models and minimizing time and resources [24, 59].5.3.3 Developing model debugging techniquesThe model is already trained at this stage, and we want to discover any problems that can limit its predictions. Thedebugging of ML models is paramount for promoting trust in the processes and predictions, which could result increating new applications [95, 60], e.g., visual applications for CNN. A variety of debugging techniques exists, includingmodel assertion, security audit, variants of residual analysis and residual explanation, and unit tests [95]. According to[41], understanding what causes errors in the model can form the foundation for developing interpretable explanations.The next step is developing more model debugging techniques and combining them with explanatory techniques toprovide insight into the model’s behavior, enhance its performance, and promote trust [95].5.3.4 Using interpretability/explainability for models/architectures comparisonIt is widely known that the performance of ML models/architectures varies from one dataset/task to another [24].Usually, error performance metrics are used for the comparison to choose the suitable model/architecture for the givendataset/task and to decide how to combine models/architectures for better performance [24]. However, even if themodels may have the same performance, they can use different features to reach the decisions [14]. Therefore, theinterpretability/explainability of models can be helpful for models/architectures comparison [14]. It could even besaid that the better we understand models’ behavior and why they fail in some situations, the more we can use thoseinsights to enhance them [14]. In the future, it is expected that explanations will be an essential part of a more extensiveoptimization process to achieve some goals such as improving a model’s performance or reducing its complexity [8].Further, XAI can be utilized in models/architectures comparison.5.3.5 Developing visual analytics approaches for advanced DL architecturesWhile visual analytic approaches have been developed for basic DL architectures (e.g., CNNs and RNNs), advancedDL architectures have yet to be addressed in this way (e.g., ResNet [96] and DenseNet [97]) [24, 59]. AdvancedDL architectures pose several challenges for visual analytic and information visualization communities due to theirlarge number of layers, the complexity of network design for each layer, and the highly connected structure betweenlayers [59]. Therefore, developing efficient visual analytics approaches for such architectures in order to increase theirinterpretability as well as the explainability of their results is needed [24, 59].5.3.6 Sparsity of analysisInterpreting and validating the reasoning behind a neural network classifier requires examining the saliency maps ofvarious samples from input data, which can be a challenging task if there are a vast number of samples [61]. Therefore,1https://www.tensorflow.org/tensorboard2https://aws.amazon.com/sagemaker/debugger/16Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTthe number of visualizations that a user has to analyze should be as small as possible to reduce the sparsity of the analysis[61]. A way to achieve that can be developing novel methods to identify a meaningful subset of the entire datasetto interpret; then, by using this meaningful subset, it is needed to come up with an interpretation of the relationshipbetween various samples and various subsets [61].5.3.7 Model innovationBy explaining DL models, we can gain a deeper understanding of their internal structure and can lead to the emergenceof new models (e.g., ZFNet [98]) [62]. Therefore, in the future, the development of explanation methods for DL andnew DL models are expected to complement each other [62].Another research area is developing new hybrid models where the expressiveness of opaque models is combined withthe apparent semantics of transparent models (e.g., combining a neural network with a linear regression) [53]. Thisresearch area can be helpful for bridging the gap between opaque and transparent models and could help in developinghighly efficient explainable models [53].5.3.8 Rules extractionHistorically, the need for explanations dates back to the early works in explaining expert systems and Bayesiannetworks [4]. Rule extraction from ML models has been studied for a long time [99, 100, 101]. However, there is stillan increasing interest in utilizing rule extraction for explainability/interpretability [102, 63]. Therefore, to discovermethods that may work for explainability/interpretability, we should revisit the past research works [50].According to [101, 63], there are three main approaches for rule extraction: (1) Decomposition approach based onthe principle that the rules are extracted at the neuron level, such as visualizing a network’s structure, (2) Pedagogicalapproach that extracts rules that map inputs directly to outputs regardless of their underlying structure, such as computinggradient, (3) Eclectics approach, which is the combination of both decompositional and pedagogical approaches.Further research for rules extraction is needed, which has been discussed in [63]. First, visualize neural networks’internal structure. Through visualizing each activated weight connection/neuron/filter from input to output, one canunderstand how the network works internally and produce the output from the input. Second, transform a complexneural network into an interpretable structure by pruning unimportant or aggregating connections with similar functions.By doing this, the overfitting issue can be reduced, and the model’s structure becomes easier to interpret. Third, explorethe correspondence between inputs and outputs, for example, by modifying the inputs and observing their effects on theoutput. Fourth, calculate the gradient of the output to the inputs to know their contributions. It has also been suggestedto combine the best of DL and fuzzy logic toward an enhanced interpretability [49].5.3.9 Bayesian approach to interpretabilityThe work in [22] has discussed that there exist elements in DL and Bayesian reasoning that complement each other.Comparing Bayesian reasoning with DL, Bayesian reasoning offers a unified framework for modeling, inference,prediction, and decision making. Furthermore, uncertainty and variability of outcomes are explicitly accounted for. Inaddition, the framework has an "Occam’s Razor" effect that penalizes overcomplicated models, which makes it robustto model overfitting. However, to ensure computational tractability, Bayesian reasoning is typically limited to conjugateand linear models.In a recent survey on Bayesian DL (BDL) [103] this complement observation has been exploited, and a generalframework for BDL within a uniform probabilistic framework has been proposed. Further research is needed to be doneto exploit this complement observation because it could improve model transparency and functionality [22].5.3.10 Explaining competenciesThere is a need for users to gain a deeper understanding of the competencies of the AI system, which includes knowingwhat competencies it possesses, how its competencies can be measured, as well as whether or not it has blind spots (i.e.,classes of solutions it never finds) [28]. Through knowledge and competency research, XAI could play a significant rolein society. Besides explaining to individuals, other roles include leveraging existing knowledge for further knowledgediscovery and applications and teaching both agents and humans [28].5.4 Challenges and Research Directions of XAI in the Deployment PhaseThe following subsections are dedicated to challenges and research directions during the deployment of AI systems.The deployment phase starts from deploying ML solutions until we stop using the solutions (or maybe after that).17Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINT5.4.1 Improving explanations with ontologiesAn ontology is defined as "an explicit specification of a conceptualization" [104]. The use of ontologies for representingknowledge of the relationships between data is helpful for understanding complex data structures [18]. Therefore, theuse of ontologies can help to produce better explanations as found in [105, 106].The work in [18] has discussed some recent works of the literature on this topic such as [105, 106]. In [105], DoctorXAI was introduced as a model-agnostic explainer that focused on explaining the diagnosis prediction task of DoctorAI [107], which is a black-box AI model that predicts the patient’s next visit time. It was shown that taking advantageof the temporal dimension in the data and incorporating the domain knowledge into the ontology helped improve theexplanations’ quality. Another work in [106] showed that ontologies can enhance human comprehension of globalpost-hoc explanations, expressed in decision trees.It should be noted that ontologies are thought of as contributing a lot to explaining AI systems because they provide auser’s conceptualization of the domain, which could be used as a basis for explanations or debugging [108]. Towardthat goal, new design patterns, new methodologies for creating ontologies that can support explainable systems, andnew methods for defining the interplay between ontologies and AI techniques are needed [108]. Furthermore, it isessential to conduct several user studies to determine the benefits of combining ontologies with explanations [18].5.4.2 XAI and privacyWhen individuals are affected by automated decision-making systems, two rights conflict: the right to privacy and theright to an explanation [109]. At this stage, it could be a demand to disclose the raw training data and thus violatethe privacy rights of the individuals from whom the raw training data came [109]. Another legal challenge has beendiscussed in [35], which is the right to be forgotten [110]. By this right, individuals can claim to delete specific data sothat they cannot be traced by a third party [35]. Data preservation is another related issue because to use XAI to justifya decision reached by automated decision-making, the raw data used for training must be kept, at least until we stopusing the AI solution.One of the key challenges is establishing trust in the handling of personal data, particularly in cases where the algorithmsused are challenging to understand [58]. This can pose a significant risk for acceptance to end-users and experts alike[58]. For example, end-users need to trust that their personal information is secured and protected as well as only theirconsented data is used, while experts need to trust that their input is not altered later [58].Anonymization of data can be used to obscure the identity of people. However, privacy cannot always be protected byanonymization [109]. According to [109], the more information in a data set, the greater the risk of de-anonymization,even if the information is not immediately visible. Asserting that anonymization helps conceal who supplied the data totrain the automated decision-making system might be comforting for the individuals whom the training data came from,but this does not the case with individuals who are entitled to an explanation of the results produced by the system[109].In order to address some issues with anonymization techniques, it is recommended that further research should beundertaken in privacy-aware ML, which is the intersection between ML and security areas [58]. XAI can play anessential role in this matter because to develop new techniques to ensure privacy and security, it will be essential tolearn more about the inner workings of the system they are meant to protect [58]. In addition, in the future, to promotethe acceptance of AI and increase privacy protection, XAI needs to provide information on how the personal data of aparticular individual was utilized in a data analysis workflow [35]. However, according to [109], what if it is needed toreview the data of many individuals and they may not have consented to review their data in litigation. In such cases, apath to review data for which individuals have not consented would be demanded, but it would be difficult to find such apath [109].5.4.3 XAI and securityTwo main concerns have been discussed for XAI and security: confidentiality and adversarial attacks [2, 62, 30, 64].For the confidentiality concern, several aspects of a model may possess the property of confidentiality [2]. As anexample given by [2], think of a company invested in a multi-year research project to develop an AI model. The model’ssynthesized knowledge may be regarded as confidential, and hence if only inputs and outputs are made available, onemay compromise this knowledge [111]. The work in [112] presented the first results on how to protect private contentfrom automatic recognition models. Further research is recommended to develop XAI tools that explain ML modelswhile maintaining models’ confidentiality [2].Turning now to the adversarial attacks concern, the information revealed by XAI can be utilized in generating efficientadversarial attacks to cause security violations, confusing the model and cause it to produce a specific output, and18Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTmanipulation of explanations [2, 64]. In adversarial ML, three types of security violations can be caused by attackersusing adversarial examples [113]: integrity attacks (i.e., the system identifies intrusion points as normal), availabilityattacks (i.e., the system makes multiple classification errors, making it practically useless), and privacy violation (i.e.,violating the privacy of system users). Attackers can do such security violations because an AI model can be built basedon training data influenced by them, or they might send carefully crafted inputs to the model and see its results [113].According to [62], existing solutions to handle perturbations still suffer from some issues, including instabilities andlack of variability. Therefore, it is necessary to develop new methods to handle perturbations more robustly [62, 30].The information uncovered by XAI can also be utilized in developing techniques for protecting private data, e.g.,utilizing generative models to explain data-driven decisions [2]. Two recent research directions have been highlightedin this context [2]: using generative models as an attribution method to show a direct relationship between a particularoutput and its input variables [114]. The second is creating counterfactuals through generative models [115]. It isexpected that generative models will play an essential role in scenarios requiring understandable machine decisions [2].5.4.4 XAI and safetyTrust and acceptance are benefits of explainability/interpretability [66]. However, focusing on benefits withoutconsidering the potential risks may have severe consequences (e.g., relying too much or too little on the advice providedby the prescription recommendation system) [66]. Several studies have been conducted to evaluate the safety ofprocesses that depend on model outputs because erroneous outputs can lead to harmful consequences in some domains[2]. Therefore, possible risks must be at the top priority when designing the presented explanations [66].Many techniques have been proposed to minimize the risk and uncertainty of adverse effects of decisions made usingmodel outputs [2]. As an example, the model’s output confidence technique can examine the extent of uncertaintyresulting from lack of knowledge regarding the inputs and the corresponding output confidence of the model to notifythe user and cause them to reject the output produced by the model [2]. In order to achieve this, explaining whatregion of the inputs was used by the model to arrive at the outcome can be used for separating out such uncertaintythat may exist within the input domain [2]. Additionally, as has been suggested in [66], it is important to developexplanations that evolve with time, keeping in mind past explanations for long-term interactions with end-users andidentifying ways to minimize risks. Developing evaluation metrics and questionnaires would be essential to integratethe user-centric aspects of explanations as well as evaluating error-proneness and any possible risks [66]. Finally,in [65], some major challenges have been discussed, including developing distance metrics that more closely reflecthuman perception, improvement to robustness by designing a set of measurable metrics for comparing the robustnessof black-box AI models across various architectures, verification completeness using various verification techniques,scalable verification with tighter bounds, and unifying formulation of interpretability.5.4.5 Human-machine teamingMost provided explanations for AI systems are typically static and carry one message per explanation [50]. Explanationsalone do not translate to understanding [1]. Therefore, for a better understanding of the system, users should be able toexplore the system via interactive explanations, which is a promising research direction to advance the XAI field [50, 1].Even though there are already some works in this research direction as has been reported in [50], much work is stillneeded to tailor interfaces to different audiences, exploit interactivity, and choose appropriate interactions for bettervisualization designs [50, 24]. Various works have also been suggested to go beyond static explanations and enhancehuman-machine teaming. In [46], open-ended visual question answering (VQA) has been suggested to be used ratherthan providing a report with too many details. Here, an user queries (or make follow-up questions), and the systemanswers. Achieving that would provide better interaction between the system and the expert user. In another work [59],it has been mentioned that generative models can allow for interactive DL steering because they allow for multipleanswers. They highlighted that developing new DL models capable of adapting to various user inputs and generatingoutputs accordingly as well as developing visualization-based interfaces that enable effective interaction with DLsystems are promising research areas in the future.In [50], rather than providing static explanations, the authors have suggested building on existing intelligibility workfor context-aware systems (e.g., design space explorations, conceptual models for implicit interaction, and intelligibleinterfaces for various scenarios and using a variety of modalities). Additionally, they have highlighted a research areathat is effectively interacting with AI augmentation tools. In [1], it has been emphasized the importance of bridgingHCI empirical studies with human sciences theories to make explainability models more human-centered models. Inthis way, adaptive explainable models would emerge by providing context-aware explanations that could be adaptedto any changes in the parameters of their environment, such as user profile (e.g., expertise level, domain knowledge,cultural background, interests and preferences) and the explanation request setting (e.g., justification).19Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTThe authors in [24] have mentioned that extracting, visualizing, and keeping track of the history of interaction databetween users and systems can allow users to undo certain actions and examine them interactively would help toaddress some common challenges (e.g., hyperparameter exploration). Finally, the authors in [66] have highlighted thatuser-friendliness and intelligent interface modalities need to take into account the type of explanations that meet users’goals and needs. For example, the system can ask for feedback from the users to know how good was the providedexplanations (e.g., “explain more”, “redundant explanation”, or “different explanation”). Such interaction can help toimprove future explanations.Taken together, it seems that different ways are needed to enhance human-machine teaming. Approaching HCI andother related studies can contribute to making explainability models more human-centered. In addition, humans canprovide feedback on the provided explanations, which can help in improving future explanations.5.4.6 Explainable agencyExplainable agency refers to a general capability in which autonomous agents must provide explanations for theirdecisions and the reasons leading to these decisions [116]. Based on the three explanation phases proposed in [117], theauthors in [67] presents a research roadmap for the explainable agency.The first phase is explanation generation which is intended to explain why an action/result was taken/achieved [67].This phase of research focuses on the following key research directions: (1) there is a need to connect the internalAI mechanism of the agent/robot with the explanation generation module, (2) to produce dynamic explanations, newmechanisms are required for identifying relevant explanation elements, identifying its rationales, and combining theseelements to form a coherent explanation.The second phase is the explanation communication phase. Here, the focus is on what content end users will receiveand how to present that content [117]. According to [67], explainable agents/robots may be deployed in a variety ofenvironments. Therefore, for some cases, multimodal explanation presentations (e.g., visual, audio, and expressive)could be a useful explanation communication approach for enabling efficient explainable agency communication.For the last phase, explanation reception, the focus is on the human’s understanding of explanations. Some considerationsshould be taken into account to ensure an accurate reception [67]. It is important to develop metrics to measure theexplanations’ effectiveness and the users’ reaction to the provided explanations. In addition, the agent/robot shouldmaintain a model of user knowledge and keep updating it based on the evolution of user expertise and the user’sperception of the State of Mind (SoM) of the agent/robot, i.e., an internal representation of how the agent/robot treatsthe outer world.5.4.7 Machine-to-machine explanationA promising area of research is enabling machine-to-machine communication and understanding [68]. Furthermore, itis an important research area because of the increasing adoption of the Internet of Things (IoT) in different industries.A growing body of research has begun exploring how multiple agents can efficiently cooperate and exploring thedifference between explanations intended for humans and those intended for machines [29, 68].According to [29], future explainable approaches are likely to provide both human and machine explanations, especiallyadaptive explainable approaches [1]. For machine explanations, complex structures that are beyond the comprehensionof humans may be developed [68]. However, how is it possible to measure the success of “transfer of understanding”between agents? The work in [68] has suggested a metric for that, which is measuring the improvement of agent B’sperformance on a particular task, or set of tasks, as a result of the information obtained from agent A - though it will becrucial to determine some key details, such as the bandwidth constraints and already existing knowledge with agent A.Based on what has been mentioned above, it is expected that much work is going to be done on how to constructmachine explanations, how to communicate these explanations, and which metrics we need to measure as a successof the transfer of understanding between agents and how to measure them. With more research into how machinescommunicate/explain themselves, we will be able to understand intelligence better and creating intelligent machines[11].5.4.8 XAI and reinforcement learningThe use of DL by reinforcement learning (RL) has been applied successfully to many areas [61]. Through the explicitmodeling of the interaction between models and environments, RL can directly address some of the interpretabilityobjectives [19]. Despite that, unexplained or non-understandable behavior makes it difficult to users to trust RLagents in a real environment, especially when it comes to human safety or failure costs [61]. Additionally, we lacka clear understanding of why an RL agent decides to perform an action and what it learns during training [61]. RL’s20Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTinterpretability can help in exploring various approaches to solving problems [61]. For instance, understanding whythe RL AlphaFold system [118] is capable of making accurate predictions can assist bioinformatics scientists inunderstanding and improving the existing techniques in protein structures to speed produce better treatment before newoutbreaks happen [61].Recently, the work in [69] highlighted several issues that need to be addressed and potential research directions in thearea of XAI for RL. The authors find that the selected studies used "toy" examples or case studies that were intentionallylimited in scope mainly to prevent the combinatorial explosion problem in the number of combinations of states andactions. Therefore, more focus on real-world applications has been suggested. It has also been mentioned that thereis a lack of new algorithms in the area. Therefore, the design of RL algorithms with an emphasis on explainability isessential. Symbolic representations can be utilized so RL agents can inherently be explained and verified. Another issueis highlighted, which is the lack of user testing with the existing approaches, which is in line with what was mentionedin [47]. As for the complexity of the provided explanations, it has been found that the current focus is presentingexplanations for users with a background in AI. Therefore, it has been suggested to conduct further research to presentthe explanations for those who might interact with the agents, which may have no background in AI. For example,providing more visceral explanations, e.g., annotations in a virtual environment. Additionally, enriching visualizationtechniques by considering the temporal dimensions of RL and multi-modal forms of visualization, e.g., virtual oraugmented reality. Lastly, it has been emphasized the importance of open-source code sharing for the academiccommunity.Another interesting point for consideration has been highlighted in [10], which is learning from explanations. Thework in [119] provides a starting point, which presents an agent who trained to simulates the Mario Bros. game usingexplanations instead of prior play logs.5.4.9 Explainable AI planning (XAIP)Existing literature focuses mainly on explainability in ML, though similar challenges apply to other areas in AI as well[1]. AI planning is an example of such an area that is important in applications where learning is not an option [27].Recent years have seen increased interest in research on explainable AI planning (XAIP) [120]. XAIP includes a varietyof topics from epistemic logic to ML, and techniques including domain analysis, plan generation, and goal recognition[120]. There are, however, some major trends that have emerged, such as plan explanations, contrastive explanations,human factors, and model reconciliation [120].Recently, the work in [27] has explored the explainability opportunities that arise in AI planning. They have providedsome of the questions requiring explanation. They also have described initial results and a roadmap toward achievingthe goal of generating effective explanations. Additionally, they have suggested several future directions in both planexplanations and executions. Temporal planning, for instance, can open up interesting choices regarding the order ofachieving (sub)goals. It is also interesting to consider whether giving the planner extra time to plan would improve theperformance. In addition, one of the challenges in plan execution is explaining what has been observed at the executiontime that prompts the planner to make a specific choice. As with XAI, it is crucial to have a good metric for XAIP thatdefines what constitutes a good explanation. Finally, it is imperative that the existing works on XAIP be reconsideredand leveraged so that XAIP will be more effective and efficient when used in critical domains.5.4.10 Explainable recommendationExplainable recommendation aims to build models that produce high quality recommendations as well as provideintuitive explanations that can help to enhance the transparency, persuasiveness, effectiveness, trustworthiness, andsatisfaction of recommendation systems [70].The work in [70] conducted a comprehensive survey of explainable recommendations, and they discussed potentialfuture directions to promote explainable recommendations. With regards to the methodology perspective, it has beensuggested that (1) further research is needed to make deep models explainable for recommendations because we stilldo not fully understand what makes something recommended versus other options, (2) develop knowledge-enhancedexplainable recommendation which allows the system to make recommendations based on domain knowledge, e.g.,combine graph embedding learning with recommendation models, (3) use heterogeneous information for explainabilitysuch as multi-modal explanations, transfer learning over heterogeneous information sources, information retrievaland recommendation cross-domain explanations, and the impact that specific information modalities have on userreceptiveness on the explanations, (4) develop context-aware explainable recommendations, (5) aggregate differentexplanations, (6) integrate symbolic reasoning and ML to make recommendations and explainability better by advancingcollaborative filtering to collaborative reasoning, (7) further research is needed to help machines explain themselvesusing natural language, and (8) with the evolution of conversational recommendations powered by smart agent devices,21Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTusers may ask “why” questions to get explanations when a recommendation does not make sense. Therefore, it isessential to answer the “why” in conversations which could help to improve system efficiency, transparency, andtrustworthiness.For the evaluation perspective, the authors in [70] have been suggested that the importance of developing reliableand easily implemented evaluation metrics for different evaluation perspectives (i.e., user perspective and algorithmperspective). Additionally, evaluating explainable recommendation systems using user behavior perspectives may bebeneficial as well. Lastly, it has been highlighted that explanations should have broader effects than just persuasion.For example, investigate how explanations can make the system more trustworthy, efficient, diverse, satisfying, andscrutable.In [66], the authors have presented several research challenges in delivery methods and modalities in user experience.As mentioned in that paper, for the delivery method, the current focus in the literature is on providing the explanation tothe users while they are working on a task or looking for recommendations. However, more focus should be done on thelong-term retrieval of such explanations, for example, through a digital archive, and their implications for accountability,traceability, and users’ trust and adoption. That could increase the adoption of intelligent human-agent systems incritical domains. Another challenge is designing autonomous delivery capable of considering the context and situationin which users may need explanations and suitable explanations for them. It is worth mentioning that privacy mattersshould be taken into account when deriving the recommendations.It has also been highlighted in [66] that users’ goals and needs would have to be met by user-friendly and intelligentinterface modalities that provide appropriate explanations. Further, interaction with the system is needed and could helpto improve future generated explanations. Finally, focusing on the benefits of explainability without considering thepotential risks may have severe consequences. Therefore, when designing explanations, possible risks should be thefirst priority.5.4.11 XAI as a serviceThere is an increasing trend in developing automated ML (AutoML) tools [11]. AutoML tool is an end-to-end pipelinestarting with raw data and going all the way to a deployable ML model. Model-agnostic explanation methods areapplicable to any ML model resulting from automated ML [11]. Similarly, we can automate the explanation step:calculate the importance of each feature, plot the partial dependence, construct a surrogate model, etc [11]. Someexisting AutoML tools provide automatic generated explanations, e.g., AutoML H2O [121] and MLJAR AutoML[122]. We expect that more Auto XAI tools will be available in the future, either incorporated with AutoML tools or asservices.6 ConclusionsIn this systematic meta-survey paper, we present two main contributions to the literature of XAI. First, we proposean attempt to present a distinction between explainability and interpretability terms. Second, we shed light on thesignificant challenges and future research directions of XAI resulting from the selected 58 papers, which guide futureexploration in the XAI area. Even though they are presented individually in 39 points, they can overlap and combinethem based on researchers’ backgrounds and interests, resulting in new research opportunities where XAI can play animportant role. This meta-survey has three limitations. First, because we cannot ensure that the selected keywords arecomplete, we could miss some very recent papers. Second, to avoid listing the challenges and future research directionsper each paper, we come up with the reported 39 points, which are the results of combining what was reported inthe selected papers based on the authors’ point of view. Third, we believe that more challenges and future researchdirections can be added where XAI can play an important role in some domains, such as IoT, 5G, and digital forensic.However, related surveys did not exist at the time of writing this meta-survey.References[1] Amina Adadi and Mohammed Berrada. Peeking inside the black-box: A survey on explainable artificialintelligence (XAI). IEEE Access, 6:52138–52160, 2018. doi:10.1109/ACCESS.2018.2870052.[2] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Bar-bado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera.Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsibleai. Information Fusion, 58:82–115, 2020. ISSN 1566-2535. doi:https://doi.org/10.1016/j.inffus.2019.12.012.[3] David Gunning. Broad agency announcement explainable artificial intelligence (xai). Technical report, Technicalreport, 2016.22Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINT[4] Or Biran and Courtenay Cotton. Explanation and justification in machine learning: A survey. In IJCAI-17workshop on explainable AI (XAI), volume 8, pages 8–13, 2017.[5] Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning. arXiv preprintarXiv:1702.08608, 2017.[6] Krishna Gade, Sahin Cem Geyik, Krishnaram Kenthapadi, Varun Mithal, and Ankur Taly. Explainable ai inindustry: Practical challenges and lessons learned: Implications tutorial. In Proceedings of the 2020 Conferenceon Fairness, Accountability, and Transparency, FAT* ’20, page 699, New York, NY, USA, 2020. Association forComputing Machinery. ISBN 9781450369367. doi:10.1145/3351095.3375664.[7] Bryce Goodman and Seth Flaxman. European union regulations on algorithmic decision-making and a “right toexplanation”. AI Magazine, 38(3):50–57, Oct. 2017. doi:10.1609/aimag.v38i3.2741.[8] Wojciech Samek and Klaus-Robert Müller. Towards Explainable Artificial Intelligence, pages 5–22. SpringerInternational Publishing, Cham, 2019. ISBN 978-3-030-28954-6. doi:10.1007/978-3-030-28954-6_1.[9] Seyedeh Neelufar Payrovnaziri, Zhaoyi Chen, Pablo Rengifo-Moreno, Tim Miller, Jiang Bian, Jonathan H Chen,Xiuwen Liu, and Zhe He. Explainable artificial intelligence models using real-world electronic health recorddata: a systematic scoping review. Journal of the American Medical Informatics Association, 27(7):1173–1185,05 2020. ISSN 1527-974X. doi:10.1093/jamia/ocaa053.[10] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi.A survey of methods for explaining black box models. ACM Comput. Surv., 51(5), 2018. ISSN 0360-0300.doi:10.1145/3236009.[11] Christoph Molnar.Interpretable Machine Learning.2019.https://christophm.github.io/interpretable-ml-book/.[12] Lisa Veiber, Kevin Allix, Yusuf Arslan, Tegawendé F Bissyandé, and Jacques Klein. Challenges towardsIn 2020 {USENIX} Conference on Operational Machineproduction-ready explainable machine learning.Learning (OpML 20), 2020.[13] Roberto Confalonieri, Ludovik Coba, Benedikt Wagner, and Tarek R. Besold. A historical perspectiveof explainable artificial intelligence. WIREs Data Mining and Knowledge Discovery, 11(1):e1391, 2021.doi:https://doi.org/10.1002/widm.1391.[14] Wojciech Samek, Thomas Wiegand, and Klaus-Robert Müller. Explainable artificial intelligence: Understanding,visualizing and interpreting deep learning models. arXiv preprint arXiv:1708.08296, 2017.[15] Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. " what is relevantin a text document?": An interpretable machine learning approach. PloS one, 12(8):e0181142, 2017.[16] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and RobFergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.[17] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidencepredictions for unrecognizable images. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition (CVPR), pages 427–436. IEEE, 2015.[18] Nadia Burkart and Marco F Huber. A survey on the explainability of supervised machine learning. Journal ofArtificial Intelligence Research, 70:245–317, 2021. doi:10.1613/jair.1.12228.[19] Zachary C. Lipton. The mythos of model interpretability: In machine learning, the concept of interpretability isboth important and slippery. Queue, 16(3):31–57, 2018. ISSN 1542-7730. doi:10.1145/3236386.3241340.[20] Aniek F. Markus, Jan A. Kors, and Peter R. Rijnbeek. The role of explainability in creating trustwor-thy artificial intelligence for health care: A comprehensive survey of the terminology, design choices,and evaluation strategies.ISSN 1532-0464.doi:https://doi.org/10.1016/j.jbi.2020.103655.Journal of Biomedical Informatics, 113:103655, 2021.[21] Zeynep Akata, Dan Balliet, Maarten de Rijke, Frank Dignum, Virginia Dignum, Guszti Eiben, Antske Fokkens,Davide Grossi, Koen Hindriks, Holger Hoos, Hayley Hung, Catholijn Jonker, Christof Monz, Mark Neerincx,Frans Oliehoek, Henry Prakken, Stefan Schlobach, Linda van der Gaag, Frank van Harmelen, Herke van Hoof,Birna van Riemsdijk, Aimee van Wynsberghe, Rineke Verbrugge, Bart Verheij, Piek Vossen, and Max Welling.A research agenda for hybrid intelligence: Augmenting human intellect with collaborative, adaptive, responsible,and explainable artificial intelligence. Computer, 53(8):18–28, 2020. doi:10.1109/MC.2020.2996587.[22] Supriyo Chakraborty, Richard Tomsett, Ramya Raghavendra, Daniel Harborne, Moustafa Alzantot, FedericoCerutti, Mani Srivastava, Alun Preece, Simon Julier, Raghuveer M. Rao, Troy D. Kelley, Dave Braines, MuratSensoy, Christopher J. Willis, and Prudhvi Gurram. Interpretability of deep learning models: A survey of23Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINTresults. In 2017 IEEE SmartWorld, Ubiquitous Intelligence Computing, Advanced Trusted Computed, Scal-able Computing Communications, Cloud Big Data Computing, Internet of People and Smart City Innovation(SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), pages 1–6, 2017. doi:10.1109/UIC-ATC.2017.8397411.[23] Yu Zhang, Peter Tiˇno, Aleš Leonardis, and Ke Tang. A survey on neural network interpretability. arXiv preprintarXiv:2012.14261, 2020.[24] Angelos Chatzimparmpas, Rafael M. Martins, Ilir Jusufi, and Andreas Kerren. A survey of surveys on the useof visualization for interpreting machine learning models. Information Visualization, 19(3):207–233, 2020.doi:10.1177/1473871620904671.[25] Staffs Keele et al. Guidelines for performing systematic literature reviews in software engineering. Technicalreport, Citeseer, 2007.[26] M. Ahmad, A. Teredesai, and C. Eckert. Interpretable machine learning in healthcare. In 2018 IEEE InternationalConference on Healthcare Informatics (ICHI), pages 447–447, Los Alamitos, CA, USA, jun 2018. IEEEComputer Society. doi:10.1109/ICHI.2018.00095.[27] Maria Fox, Derek Long, and Daniele Magazzeni. Explainable planning. arXiv preprint arXiv:1709.10256, 2017.[28] David Gunning, Mark Stefik, Jaesik Choi, Timothy Miller, Simone Stumpf, and Guang-Zhong Yang.Xai—explainable artificial intelligence. Science Robotics, 4(37), 2019.[29] Alun Preece. Asking ‘why’ in ai: Explainability of intelligent systems – perspectives and challenges. IntelligentSystems in Accounting, Finance and Management, 25(2):63–72, 2018. doi:https://doi.org/10.1002/isaf.1422.[30] Gabriëlle Ras, Marcel van Gerven, and Pim Haselager. Explanation Methods in Deep Learning: Users, Values,Concerns and Challenges, pages 19–36. Springer International Publishing, Cham, 2018. ISBN 978-3-319-98131-4. doi:10.1007/978-3-319-98131-4_2.[31] Filip Karlo Došilovi´c, Mario Brˇci´c, and Nikica Hlupi´c. Explainable artificial intelligence: A survey. In 201841st International Convention on Information and Communication Technology, Electronics and Microelectronics(MIPRO), pages 0210–0215, 2018. doi:10.23919/MIPRO.2018.8400040.[32] Christoph Molnar, Giuseppe Casalicchio, and Bernd Bischl. Interpretable machine learning – a brief history,state-of-the-art and challenges. In ECML PKDD 2020 Workshops, pages 417–431, Cham, 2020. SpringerInternational Publishing. ISBN 978-3-030-65965-3.[33] Diogo V. Carvalho, Eduardo M. Pereira, and Jaime S. Cardoso. Machine learning interpretability: A survey onmethods and metrics. Electronics, 8(8), 2019. ISSN 2079-9292. doi:10.3390/electronics8080832.[34] Mauricio Reyes, Raphael Meier, Sérgio Pereira, Carlos A. Silva, Fried-Michael Dahlweid, Hendrik vonTengg-Kobligk, Ronald M. Summers, and Roland Wiest. On the interpretability of artificial intelli-gence in radiology: Challenges and opportunities. Radiology: Artificial Intelligence, 2(3):e190043, 2020.doi:10.1148/ryai.2020190043. PMID: 32510054.[35] Luca Longo, Randy Goebel, Freddy Lecue, Peter Kieseberg, and Andreas Holzinger. Explainable artificialintelligence: Concepts, applications, research challenges and visions. In Machine Learning and KnowledgeExtraction, pages 1–16, Cham, 2020. Springer International Publishing. ISBN 978-3-030-57321-8.[36] Milda Poceviˇci¯ut˙e, Gabriel Eilertsen, and Claes Lundström. Survey of XAI in Digital Pathology, pages 56–88.Springer International Publishing, Cham, 2020. ISBN 978-3-030-50402-1. doi:10.1007/978-3-030-50402-1_4.[37] Jian-Xun Mi, An-Di Li, and Li-Fang Zhou. Review study of interpretation methods for future interpretablemachine learning. IEEE Access, 8:191969–191985, 2020. doi:10.1109/ACCESS.2020.3032756.[38] Xiao-Hui Li, Caleb Chen Cao, Yuhan Shi, Wei Bai, Han Gao, Luyu Qiu, Cong Wang, Yuanyuan Gao, ShenjiaZhang, Xun Xue, and Lei Chen. A survey of data-driven and knowledge-aware explainable ai. IEEE Transactionson Knowledge and Data Engineering, 2020. doi:10.1109/TKDE.2020.2983930.[39] Ingrid Nunes and Dietmar Jannach. A systematic review and taxonomy of explanations in decision support andrecommender systems. User Modeling and User-Adapted Interaction, 27(3):393–444, 2017.[40] Arne Seeliger, Matthias Pfaff, and Helmut Krcmar. Semantic web technologies for explainable machine learningmodels: A literature review. PROFILES/SEMEX@ ISWC, 2465:1–16, 2019.[41] Ning Xie, Gabrielle Ras, Marcel van Gerven, and Derek Doran. Explainable deep learning: A field guide for theuninitiated. arXiv preprint arXiv:2004.14545, 2020.[42] Adriano Lucieri, Muhammad Naseer Bajwa, Andreas Dengel, and Sheraz Ahmed. Achievements and challengesin explaining deep learning based computer-aided diagnosis systems. arXiv preprint arXiv:2011.13169, 2020.24Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINT[43] Shane T Mueller, Robert R Hoffman, William Clancey, Abigail Emrey, and Gary Klein. Explanation in human-aisystems: A literature meta-review, synopsis of key ideas and publications, and bibliography for explainable ai.arXiv preprint arXiv:1902.01876, 2019.[44] Vanessa Buhrmester, David Münch, and Michael Arens. Analysis of explainers of black box deep neuralnetworks for computer vision: A survey. arXiv preprint arXiv:1911.12116, 2019.[45] Sheikh Rabiul Islam, William Eberle, Sheikh Khaled Ghafoor, and Mohiuddin Ahmed. Explainable artificialintelligence approaches: A survey. arXiv preprint arXiv:2101.09429, 2021.[46] Pablo Messina, Pablo Pino, Denis Parra, Alvaro Soto, Cecilia Besa, Sergio Uribe, Cristian Tejos, Claudia Prieto,Daniel Capurro, et al. A survey on deep learning and explainability for automatic image-based medical reportgeneration. arXiv preprint arXiv:2010.10563, 2020.[47] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence, 267:1–38, 2019. ISSN 0004-3702. doi:https://doi.org/10.1016/j.artint.2018.07.007.[48] Boris Kovalerchuk, Muhammad Aurangzeb Ahmad, and Ankur Teredesai. Survey of Explainable MachineLearning with Visual and Granular Methods Beyond Quasi-Explanations, pages 217–267. Springer InternationalPublishing, Cham, 2021. ISBN 978-3-030-64949-4. doi:10.1007/978-3-030-64949-4_8.[49] Feng-Lei Fan, Jinjun Xiong, Mengzhou Li, and Ge Wang. On interpretability of artificial neural networks: A sur-vey. IEEE Transactions on Radiation and Plasma Medical Sciences, 2021. doi:10.1109/TRPMS.2021.3066428.[50] Ashraf Abdul, Jo Vermeulen, Danding Wang, Brian Y. Lim, and Mohan Kankanhalli. Trends and Trajectoriesfor Explainable, Accountable and Intelligible Systems: An HCI Research Agenda, page 1–18. Association forComputing Machinery, New York, NY, USA, 2018. ISBN 9781450356206.[51] Giulia Vilone and Luca Longo. Explainable artificial intelligence: a systematic review. arXiv preprintarXiv:2006.00093, 2020.[52] Mohammad Naiseh, Nan Jiang, Jianbing Ma, and Raian Ali. Personalising explainable recommendations:Literature and conceptualisation. In Álvaro Rocha, Hojjat Adeli, Luís Paulo Reis, Sandra Costanzo, Irena Orovic,and Fernando Moreira, editors, Trends and Innovations in Information Systems and Technologies, pages 518–533,Cham, 2020. Springer International Publishing. ISBN 978-3-030-45691-7.[53] Vaishak Belle and Ioannis Papantonis. Principles and practice of explainable machine learning. Frontiers in bigData, page 39, 2021.[54] Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey of thestate of explainable ai for natural language processing. In Proceedings of the 1st Conference of the Asia-PacificChapter of the Association for Computational Linguistics and the 10th International Joint Conference on NaturalLanguage Processing, pages 447–459, 2020.[55] Raha Moraffah, Mansooreh Karami, Ruocheng Guo, Adrienne Raglin, and Huan Liu. Causal interpretability formachine learning - problems, methods and evaluation. SIGKDD Explor. Newsl., 22(1):18–33, May 2020. ISSN1931-0145. doi:10.1145/3400051.3400058.[56] Ilia Stepin, Jose M. Alonso, Alejandro Catala, and Martín Pereira-Fariña. A survey of contrastive and counterfac-tual explanation generation methods for explainable artificial intelligence. IEEE Access, 9:11974–12001, 2021.doi:10.1109/ACCESS.2021.3051315.[57] Hao Yuan, Haiyang Yu, Shurui Gui, and Shuiwang Ji. Explainability in graph neural networks: A taxonomicsurvey. arXiv preprint arXiv:2012.15445, 2020.[58] Andreas Holzinger, Peter Kieseberg, Edgar Weippl, and A. Min Tjoa. Current advances, trends and challenges ofmachine learning and knowledge extraction: From machine learning to explainable ai. In Andreas Holzinger,Peter Kieseberg, A Min Tjoa, and Edgar Weippl, editors, Machine Learning and Knowledge Extraction, pages1–8, Cham, 2018. Springer International Publishing. ISBN 978-3-319-99740-7.[59] J. Choo and S. Liu. Visual analytics for explainable deep learning. IEEE Computer Graphics and Applications,38(04):84–92, 2018. ISSN 1558-1756. doi:10.1109/MCG.2018.042731661.[60] Quanshi Zhang and Song-Chun Zhu. Visual interpretability for deep learning: a survey. Frontiers of InformationTechnology & Electronic Engineering, 19:27–39, 2018. doi:10.1631/FITEE.1700808.[61] Giang Dao and Minwoo Lee. Demystifying deep neural networks through interpretation: A survey. arXivpreprint arXiv:2012.07119, 2020.[62] Yu Liang, Siguang Li, Chungang Yan, Maozhen Li, and Changjun Jiang. Explaining the black-box model: Asurvey of local interpretation methods for deep neural networks. Neurocomputing, 419:168–182, 2021. ISSN0925-2312. doi:10.1016/j.neucom.2020.08.011.25Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINT[63] Congjie He, Meng Ma, and Ping Wang.tificial neural networks: A review.doi:https://doi.org/10.1016/j.neucom.2020.01.036.ExtractNeurocomputing, 387:346–358, 2020.interpretability-accuracy balanced rules from ar-ISSN 0925-2312.[64] Erico Tjoa and Cuntai Guan. A survey on explainable artificial intelligence (xai): Toward medical xai. IEEETransactions on Neural Networks and Learning Systems, pages 1–21, 2020. doi:10.1109/TNNLS.2020.3027314.[65] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, andXinping Yi. A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarialattack and defence, and interpretability. Computer Science Review, 37:100270, 2020.ISSN 1574-0137.doi:https://doi.org/10.1016/j.cosrev.2020.100270.[66] Mohammad Naiseh, Nan Jiang, Jianbing Ma, and Raian Ali. Explainable recommendations in intelligent systems:Delivery methods, modalities and risks. In Research Challenges in Information Science, pages 212–228, Cham,2020. Springer International Publishing. ISBN 978-3-030-50316-1.[67] Sule Anjomshoae, Amro Najjar, Davide Calvaresi, and Kary Främling. Explainable agents and robots: Resultsfrom a systematic literature review. In Proceedings of the 18th International Conference on Autonomous Agentsand MultiAgent Systems, AAMAS ’19, page 1078–1088, Richland, SC, 2019. International Foundation forAutonomous Agents and Multiagent Systems. ISBN 9781450363099.[68] Adrian Weller. Transparency: Motivations and Challenges, pages 23–40. Springer International Publishing,Cham, 2019. ISBN 978-3-030-28954-6. doi:10.1007/978-3-030-28954-6_2.[69] Lindsay Wells and Tomasz Bednarz. Explainable ai and reinforcement learning—a systematic reviewISSN 2624-8212.of current approaches and trends. Frontiers in Artificial Intelligence, 4:48, 2021.doi:10.3389/frai.2021.550030.[70] Yongfeng Zhang and Xu Chen. Explainable recommendation: A survey and new perspectives. Foundations andTrends® in Information Retrieval, 14(1):1–101, 2020. ISSN 1554-0669. doi:10.1561/1500000066.[71] The Royal Society. Explainable ai: The basics, 2019. URL https://royalsociety.org/-/media/policy/projects/explainable-ai/AI-and-interpretability-policy-briefing.pdf.[72] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. Fooling lime and shap:Adversarial attacks on post hoc explanation methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics,and Society, pages 180–186, 2020.[73] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretablemodels instead. Nature Machine Intelligence, 1(5):206–215, 2019.[74] Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and MarcusRohrbach. Multimodal explanations: Justifying decisions and pointing to the evidence. In Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.[75] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining the predictionsof any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discoveryand data mining, pages 1135–1144, 2016.[76] Erik Štrumbelj and Igor Kononenko. Explaining prediction models and individual predictions with featurecontributions. Knowledge and information systems, 41(3):647–665, 2014.[77] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods, andapplications in interpretable machine learning. Proceedings of the National Academy of Sciences, 116(44):22071–22080, 2019. ISSN 0027-8424. doi:10.1073/pnas.1900654116.[78] Ehud Reiter. Natural language generation challenges for explainable AI. In Proceedings of the 1st Workshopon Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019), pages 3–7.Association for Computational Linguistics, 2019. doi:10.18653/v1/W19-8402.[79] Kees Van Deemter. Not exactly: In praise of vagueness. Oxford University Press, 2012.[80] Kahneman Daniel. Thinking, fast and slow, 2017.[81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages5998–6008, 2017.[82] Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages1189–1232, 2001.26Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINT[83] Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O’Brien, Kate Scott, StuartSchieber, James Waldo, David Weinberger, et al. Accountability of ai under the law: The role of explanation.arXiv preprint arXiv:1711.01134, 2017.[84] Valérie Beaudouin, Isabelle Bloch, David Bounie, Stéphan Clémençon, Florence d’Alché Buc, James Eagan,Winston Maxwell, Pavlo Mozharovskyi, and Jayneel Parekh. Flexible and context-specific ai explainability: amultidisciplinary approach. Available at SSRN 3559477, 2020.[85] Muhammad Aurangzeb Ahmad, Carly Eckert, and Ankur Teredesai. The challenge of imputation in explainableartificial intelligence models. In Proceedings of the Workshop on Artificial Intelligence Safety, 2019. URLhttp://ceur-ws.org/Vol-2419/paper_26.pdf.[86] AndrewBlackandPeter2020.DDQ-Dimensions-of-Data-Quality-Research-Paper-version-1.2-d.d.-3-Sept-2020.pdf.URL(DDQ),ofhttp://www.dama-nl.org/wp-content/uploads/2020/09/DimensionsQualityDataNederpelt.[87] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages1273–1282. PMLR, 2017.[88] Omar Abdel Wahab, Azzam Mourad, Hadi Otrok, and Tarik Taleb. Federated machine learning: Survey, multi-level classification, desirable criteria and future directions in communication and networking systems. IEEECommunications Surveys Tutorials, 23(2):1342–1397, 2021. doi:10.1109/COMST.2021.3058573.[89] J. Koneˇcný, H.B. McMahan, F.X. Yu, P. Richtárik, A.T. Suresh, and D. Bacon. Federated learning: Strategies forimproving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.[90] Xiaozheng Xie, Jianwei Niu, Xuefeng Liu, Zhengsu Chen, Shaojie Tang, and Shui Yu. A survey on incorporatingdomain knowledge into deep learning for medical image analysis. Medical Image Analysis, 69:101985, 2021.ISSN 1361-8415. doi:https://doi.org/10.1016/j.media.2021.101985.[91] Li Fe-Fei, Fergus, and Perona. A bayesian approach to unsupervised one-shot learning of object categories. InProceedings Ninth IEEE International Conference on Computer Vision, pages 1134–1141 vol.2. IEEE, 2003.doi:10.1109/ICCV.2003.1238476.[92] Manas Gaur, Keyur Faldu, and Amit Sheth. Semantics of the black-box: Can knowledge graphs help makedeep learning systems more interpretable and explainable? IEEE Internet Computing, 25(1):51–59, 2021.doi:10.1109/MIC.2020.3031769.[93] Nicola Pezzotti, Thomas Höllt, Jan Van Gemert, Boudewijn P.F. Lelieveldt, Elmar Eisemann, and Anna Vilanova.Deepeyes: Progressive visual analytics for designing deep neural networks. IEEE Transactions on Visualizationand Computer Graphics, 24(1):98–108, 2018. doi:10.1109/TVCG.2017.2744358.[94] Mengchen Liu, Jiaxin Shi, Kelei Cao, Jun Zhu, and Shixia Liu. Analyzing the training processes ofdeep generative models. IEEE Transactions on Visualization and Computer Graphics, 24(1):77–87, 2018.doi:10.1109/TVCG.2017.2744938.[95] Patrick Hall, Navdeep Gill, and Nicholas Schmidt. Proposed guidelines for the responsible use of explainablemachine learning. arXiv preprint arXiv:1906.03533, 2019.[96] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.[97] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutionalnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708,2017.[98] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In David Fleet,Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision – ECCV 2014, pages 818–833,Cham, 2014. Springer International Publishing. ISBN 978-3-319-10590-1.[99] Geoffrey G Towell and Jude W Shavlik. Extracting refined rules from knowledge-based neural networks.Machine learning, 13(1):71–101, 1993.[100] Christian W. Omlin and C.Lee Giles. Extraction of rules from discrete-time recurrent neural networks. NeuralNetworks, 9(1):41–52, 1996. ISSN 0893-6080. doi:https://doi.org/10.1016/0893-6080(95)00086-0.[101] Robert Andrews, Joachim Diederich, and Alan B. Tickle. Survey and critique of techniques for extractingrules from trained artificial neural networks. Knowledge-Based Systems, 8(6):373–389, 1995. ISSN 0950-7051.doi:https://doi.org/10.1016/0950-7051(96)81920-4. Knowledge-based neural networks.27Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINT[102] Jan Ruben Zilke, Eneldo Loza Mencía, and Frederik Janssen. Deepred – rule extraction from deep neuralISBNIn Discovery Science, pages 457–473, Cham, 2016. Springer International Publishing.networks.978-3-319-46307-0.[103] Hao Wang and Dit-Yan Yeung. Towards bayesian deep learning: A framework and some existing methods. IEEETransactions on Knowledge and Data Engineering, 28(12):3395–3408, 2016. doi:10.1109/TKDE.2016.2606428.[104] Thomas R. Gruber. A translation approach to portable ontology specifications. Knowledge Acquisition, 5(2):199–220, 1993. ISSN 1042-8143. doi:10.1006/knac.1993.1008.[105] Cecilia Panigutti, Alan Perotti, and Dino Pedreschi. Doctor xai: An ontology-based approach to black-boxsequential data classification explanations. In Proceedings of the 2020 Conference on Fairness, Accountability,and Transparency, FAT* ’20, page 629–639, New York, NY, USA, 2020. Association for Computing Machinery.ISBN 9781450369367. doi:10.1145/3351095.3372855.[106] Roberto Confalonieri, Tillman Weyde, Tarek R. Besold, and Fermín Moscoso del Prado Martín. Using ontologiesto enhance human understandability of global post-hoc explanations of black-box models. Artificial Intelligence,296:103471, 2021. ISSN 0004-3702. doi:10.1016/j.artint.2021.103471.[107] Edward Choi, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. Doctor ai:In Proceedings of the 1st Machine Learning forPredicting clinical events via recurrent neural networks.Healthcare Conference, volume 56 of Proceedings of Machine Learning Research, pages 301–318, NortheasternUniversity, Boston, MA, USA, 18–19 Aug 2016. PMLR.[108] Tania Tudorache. Ontology engineering: Current state, challenges, and future directions. Semantic Web, 11(1):125–138, 2020.[109] Thomas D Grant and Damon J Wischik. Show us the data: Privacy, explainability, and why the law can’t haveboth. Geo. Wash. L. Rev., 88:1350, 2020.[110] Eduard Fosch Villaronga, Peter Kieseberg, and Tiffany Li. Humans forget, machines remember: Artificialintelligence and the right to be forgotten. Computer Law & Security Review, 34(2):304–313, 2018. ISSN0267-3649. doi:10.1016/j.clsr.2017.08.007.[111] Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-boxmodels. In Conference on Computer Vision and Pattern Recognition, 2019.[112] Seong Joon Oh, Max Augustin, Bernt Schiele, and Mario Fritz. Towards reverse-engineering black-box neuralnetworks. In International Conference on Learning Representations, 2018.[113] Ling Huang, Anthony D. Joseph, Blaine Nelson, Benjamin I.P. Rubinstein, and J. D. Tygar. Adversarialmachine learning. In Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence, AISec’11, page 43–58, New York, NY, USA, 2011. Association for Computing Machinery. ISBN 9781450310031.doi:10.1145/2046684.2046692.[114] Christian F Baumgartner, Lisa M Koch, Kerem Can Tezcan, Jia Xi Ang, and Ender Konukoglu. Visual featureattribution using wasserstein gans. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pages 8309–8319, 2018.[115] Shusen Liu, Bhavya Kailkhura, Donald Loveland, and Yong Han. Generative counterfactual introspection forexplainable deep learning. In 2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP),pages 1–5, 2019. doi:10.1109/GlobalSIP45357.2019.8969491.[116] Pat Langley, Ben Meadows, Mohan Sridharan, and Dongkyu Choi. Explainable agency for intelligent au-tonomous systems. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17,page 4762–4763. AAAI Press, 2017.[117] Mark A. Neerincx, Jasper van der Waa, Frank Kaptein, and Jurriaan van Diggelen. Using perceptual andIn Don Harris, editor, Engineeringcognitive explanations for enhanced human-agent team performance.Psychology and Cognitive Ergonomics, pages 204–214, Cham, 2018. Springer International Publishing. ISBN978-3-319-91122-9.[118] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin,Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein structure prediction usingpotentials from deep learning. Nature, 577(7792):706–710, 2020.[119] Samantha Krening, Brent Harrison, Karen M. Feigh, Charles Lee Isbell, Mark Riedl, and Andrea Thomaz.Learning from explanations using sentiment and advice in rl. IEEE Transactions on Cognitive and DevelopmentalSystems, 9(1):44–55, 2017. doi:10.1109/TCDS.2016.2628365.28Explainable AI: A Systematic Meta-Survey of Current Challenges and Future Opportunities A PREPRINT[120] Jörg Hoffmann and Daniele Magazzeni. Explainable AI Planning (XAIP): Overview and the Case of ContrastiveExplanation (Extended Abstract), pages 277–282. Springer International Publishing, Cham, 2019.ISBN978-3-030-31423-1. doi:10.1007/978-3-030-31423-1_9.[121] Erin LeDell and Sebastien Poirier. H2O AutoML: Scalable automatic machine learning. 7th ICML Workshopon Automated Machine Learning (AutoML), July 2020. URL https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf.[122] Aleksandra Pło´nska and Piotr Pło´nski. Mljar: State-of-the-art automated machine learning framework for tabulardata. version 0.10.3, 2021. URL https://github.com/mljar/mljar-supervised.29