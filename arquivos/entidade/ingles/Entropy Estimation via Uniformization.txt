Journal Pre-proofEntropy Estimation via UniformizationZiqiao Ao and Jinglai LiPII:DOI:S0004-3702(23)00100-5https://doi.org/10.1016/j.artint.2023.103954Reference:ARTINT 103954To appear in:Artificial IntelligenceReceived date:4 June 2022Revised date:1 June 2023Accepted date:3 June 2023Please cite this article as: Z. Ao and J. Li, Entropy Estimation via Uniformization, Artificial Intelligence, 103954,doi: https://doi.org/10.1016/j.artint.2023.103954.This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, andformatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting andreview before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that,during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journalpertain.© 2023 Published by Elsevier.Entropy Estimation via UniformizationSchool of Mathematics, University of Birmingham, Birmingham B15 2TT, UKZiqiao Ao, Jinglai LiAbstractEntropy estimation is of practical importance in information theory and statis-tical science. Many existing entropy estimators suffer from fast growing esti-mation bias with respect to dimensionality, rendering them unsuitable for high-dimensional problems. In this work we propose a transform-based method forhigh-dimensional entropy estimation, which consists of the following two mainingredients. Firstly, we provide a modified k-nearest neighbors (k-NN) entropyestimator that can reduce estimation bias for samples closely resembling a uni-form distribution. Second we design a normalizing flow based mapping thatpushes samples toward the uniform distribution, and the relation between theentropy of the original samples and the transformed ones is also derived. As aresult the entropy of a given set of samples is estimated by first transformingthem toward the uniform distribution and then applying the proposed estima-tor to the transformed samples. The performance of the proposed method iscompared against several existing entropy estimators, with both mathematicalexamples and real-world applications.Keywords: entropy estimation, k nearest neighbor estimator, normalizing flow,uniformization2010 MSC: 00-01, 99-001. Introduction5Entropy, a fundamental concept in information theory, has found applica-tions in various fields such as physics, statistics, signal processing, and machinelearning. For example, in the statistics and data science contexts, various ap-plications rely critically on the estimation of entropy, including goodness-of-fittesting [1, 2], sensitivity analysis [3], parameter estimation [4, 5], and Bayesianexperimental design [6, 7].In this work we focus on the continuous version of entropy that takes theform,(cid:2)H(X) = −log[px(x)]px(x)dx,(1)Email addresses: zxa029@bham.ac.uk (Ziqiao Ao), j.li.10@bham.ac.uk (Jinglai Li)Preprint submitted to Journal of LATEX TemplatesJune 7, 2023101520253035404550where px(x) is the probability density function (PDF) of random variable X.Despite the rather simple definition, entropy only admits an analytical expres-sion for a limited family of distributions and needs to be evaluated numericallyin general. When the distribution of interest is analytically available, in princi-ple its entropy can be estimated by numerical integration schemes such as theMonte Carlo method. However, in many real-world applications, the distribu-tion of interest is not analytically available, and one has to estimate the entropyfrom the realizations drawn from the target distribution, which makes it difficultor even impossible to directly compute the entropy via numerical integration.Entropy estimation has attracted considerable attention from various com-munities in the last a few decades, and numerous methods have been developedto directly estimate entropy from realizations. In this work we only considernon-parametric approaches which do not assume any parametric model of thetarget distribution, and those methods can be broadly classified into two cate-gories. The first class of methods, are known as the plug-in estimators, whichfirst estimate the underlying probability density, and then compute the integralin Eq. (1) using numerical integration or Monte Carlo (see [8] for a detaileddescription). Some examples of density estimation approaches that have beenstudied for plug-in methods are kernel density estimator [9, 10, 11, 12], his-togram estimator [13, 10] and field-theoretic approach [14]. A major limitationof this type of methods is that they rely on an effective density estimation, whichis a difficult problem in its own right, especially when the dimensionality of theproblem is high. A different strategy is to directly estimate the entropy from theindependent samples of the random variable. Popular methods falling in thiscategory include the sample-spacing [15] and the k-nearest neighbors (k-NN)[16, 17] based estimators. The latter is particularly appealing among the exist-ing estimation methods thanks to its theoretical and computational advantagesand has been widely used in practical problems. Efforts have been constantly de-voted to extending and improving the k-NN methods, and some recent variantsand extensions of the methods are [18, 19, 20]. It is also worth mentioning thatthere are many other types of direct entropy estimators available. For example,Ariel and Louzoun [21] decoupled the target entropy to a sum of the entropy ofmarginals, which is estimated using one-dimensional methods, and the entropyof copula, which is estimated recursively by splitting the data along statisticallydependent dimensions. Kandasamy et al.[22] suggested a leave-one-out tech-nique for the von Mises expansion based estimator [23]. We also note that incertain applications the main purpose is to minimize or maximize the quantityof entropy, and in this case entropy gradient estimation strategies [24, 25] havebeen explored to avoid direct entropy estimation.It is well known that, entropy estimation becomes increasingly more diffi-cult as the dimensionality grows, and such difficulty is mainly due to the es-timation bias, which decays very slowly with respect to sample size for high-dimensional problems. For example in many popular approaches including thek-NN method [16], the estimation bias decays at the rate of O(N −γ/d) whereN is the sample size, d is the dimensionality, and γ is a positive constant[26, 22, 27, 28]. As a result, very few, if not none, of the existing entropy25560657075808590estimation methods can effectively handle high-dimensional problems withoutmaking strong assumptions about the smoothness of the underlying distribu-tion [22]. Indeed, the well-known minimax bias results (e.g., [29, 30]) indicatethat without the strong smoothness assumption [22], the curse of dimensional-ity is unavoidable. However, efforts can still be made to reduce the differencebetween the actual estimation bias and the theoretical bound.The main goal of this work is to provide an effective entropy estimationapproach which can achieve faster bias decaying rate under mild smoothnessassumption, and thus can effectively deal with high-dimensional problems. Themethod presented here consists of two main ingredients. First propose two trun-cated k-NN estimators based on those by [16] and [17] respectively, and alsoprovide the bounds of the estimation bias in these estimators. Interestingly ourtheoretical results suggest that the estimators achieve zero bias for uniform dis-tributions, while there is no such a result for any existing k-NN based estimators,according to the bias analysis available to date [27, 31, 32]. This property offersthe possibility to significantly improve the performance of entropy estimationby mapping the data points toward a uniform distribution, a procedure that werefer to as uniformization. Therefore the second main ingredient of the methodis to conduct the uniformization of the data points, with the normalizing flow(NF) technique [33, 34]. Simply speaking, NF constructs a sequence of invertibleand differentiable mappings that transform a simple base distribution such asstandard Gaussian into a more complicated distribution whose density functionmay not be available. Specifically we use the Masked Autoregressive Flow [35],a NF algorithm originally developed for density estimation, combined with theprobability integral transform, to push the original data points towards the uni-form distribution. We then estimate the entropy of the resulting near-uniformdata points with the proposed truncated k-NN estimators, and derive that ofthe original ones accordingly (by adding an entropic correction term due to thetransformation). Therefore, by combining the truncated k-NN estimators andthe normalizing flow model, we are able to decode a complex high-dimensionaldistribution represented by the realizations, and obtain an accurate estimationof its entropy.The rest of the paper is organized as follows. In Section 2, we describe thetraditional k-NN based methods of entropy estimation and their convergenceproperties. In Section 3, we introduce the truncated k-NN estimators for dis-tributions with compact support, and then show how to combine these newestimators with the NF-based uniformization procedure to estimate the entropyof general distributions. Numerical examples and applications are presented inSections 4 and Section 5 respectively to demonstrate the effectiveness of theproposed methods. Finally, in Section 6, we summarize our findings and discusssome future research directions.952. k-NN Based Entropy EstimationWe provide a brief introduction to two commonly used k-NN based entropyestimators in this section. We start with the original k-NN entropy estimator3100proposed in [16], where the k-th nearest neighbor is contained in the smallestpossible closed ball. Next, we introduce a popular variant of the k-NN estimatorproposed in [17], and this method uses the smallest possible hyper-rectangle tocover at least k points. We finally discuss some theoretical analysis of estimationerrors in the estimators.2.1. Kozachenko-Leonenko EstimatorRecall the definition of entropy in Eq. (1). Given a density estimator (cid:3)px(x)i=1 drawn from px(x), thefor px(x) and a set of N i.i.d. samples S = {x(i)}Nentropy of the random variable X can be estimated as follows:(cid:3)H(X) =− N −1N(cid:4)i=1log (cid:3)px(x(i)).(2)The Kozachenko-Leonenko (KL) estimator depends on a local uniformity as-sumption to obtain the estimate (cid:3)px(x). For each x(i), one first identifies thek-nearest neighbors (in terms of the p-norm distance) of it, and defines thesmallest closed ball covering all these k neighbors as:B(x(i), (cid:3)i/2) = {x ∈ Rd(cid:5)(cid:5) (cid:3)x − x(i)(cid:3)p ≤ (cid:3)i/2},where (cid:3)i be twice the distance between x(i) and its k-th nearest neighbor amongthe set S. We shall refer to the closed ball B(x(i), (cid:3)i/2) as a cell centered at x(i),and let qi be the mass of the cell B(x(i), (cid:3)i/2) , i.e.,(cid:2)qi((cid:3)i) =x∈B(x(i),(cid:3)i/2)px(x)dx.It can be derived that the expectation value of log qi over (cid:3)i is given byE(log qi) = ψ(k) − ψ(N ),(3)where ψ(x) = Γ(cid:2)(x)then assumes that the density is constant in B(x(i), (cid:3)i), which givesΓ(x) with Γ(x) being the Gamma function [17]. KL estimatorqi((cid:3)i) ≈ cd(cid:3)di px(x(i)),(4)where d is the dimension of X andcd = Γ(1 +1p)d/Γ(1 +dp),is the volume of the d-dimensional unit ball with respect to p-norm. Combining(3) and (4) one can get an estimate of the log-density at each sample point,log (cid:3)px(x(i)) = ψ(k) − ψ(N ) − log cd − d log (cid:3)i.(5)Plugging the above estimates for i = 1, ..., N into (2) yields the KL estimator:(cid:3)HKL(X) =− ψ(k) +ψ (N ) + log cd +dNN(cid:4)i=1log (cid:3)i.(6)42.2. KSG EstimatorAs is mentioned earlier, the Kraskov-St¨ogbauer-Grassberger (KSG) estima-tor is an important variant of ˆHKL. Unlike KL estimator that is based on closedballs, KSG estimator uses hyper-rectangles to form the cells at each data point.Namely one chooses the ∞-norm as the distance metric (i.e p = ∞), and asa result the cell B(x(i), (cid:3)i/2) becomes a hyper-cube with side length (cid:3)i. Next,we allow the hyper-cube to become a hyper-rectangle: i.e., the cells admit dif-ferent side lengths along different dimensions. Specifically, for j = 1, ..., d, wedefine (cid:3)i,j to be twice of the distance between x(i) and its k-th nearest neighboralong dimension j, and the cell centered at x(i) covering its k-nearest neighborsbecomesB(x(i), (cid:3)i,1:d/2) = {x = (x1, ..., xd) | |xj − x(i)j| ≤(cid:3) i,j/2,for j = 1, ..., d},(7)where (cid:3)i,1:d = ((cid:3)i,1, ..., (cid:3)i,d). This change leads to a different formula for comput-ing the mass of the cell B(x(i), (cid:3)i,1:d/2),E(log qi) ≈ ψ(k) −d − 1k− ψ(N ).(8)It is worth noting that the equality in Eq. (3) is replaced by approximate equalityin Eq. (8), because a uniform density within the rectangle has to be assumedto obtain Eq. (8) (see Lemma 2 in Appendix A.2 for details). Using a similarlocal assumption as Eq. (4), the KSG estimator is derived as,(cid:3)HKSG(X) =− ψ(k) +ψ (N ) +d − 1k+1NN(cid:4)d(cid:4)i=1j=1log (cid:3)i,j.(9)105110115120We note that the KSG method was actually developed in the context of esti-mating mutual information [17], and has been reported to outperform the KLestimator in a wide range of problems [27]. As has been shown above, it isstraightforward to extend it to entropy estimation, and our numerical experi-ments also suggest that it has competitive performance as an entropy estimator,which will be demonstrated in Section 4.2.3. Convergence AnalysisAnother important issue is to analyze the estimation errors in these entropyestimators and especially how they behave as the sample size increases. In mostof the k-NN based estimators including the two mentioned above, the variance isgenerally well controlled, decaying at a rate of O(N −1) with N being the samplesize, while the main issue lies on the estimation bias. In fact, the bias of estima-tor (cid:3)HKL has been well studied, but that of (cid:3)HKSG receives very little attention.Previous results related to the former are listed as follows. The original [16]paper established the asymptotic unbiasedness for k = 1 while [36] obtained thesame result for general k. For distributions with unbounded support, [37] proved5Figure 1: The schematic illustration of the truncated estimator. The shaded area is thatremoved from the k-NN cell.) ford = 1.that the bias bound decays at a rate of O( 1√Nto higher dimensions, obtaining a bias bound of O(N − 1d ) up to polylogarithmicfactors. For distributions compactly supported, usually densities satisfying theβ-H¨older condition are considered. [32] gave a quick-and-dirty upper bound ofbias, O(N −β), for a simple class of univariate densities supported on [0, 1] andd ) (β ∈ (0, 2])bounded away from zero.for general d with some additional conditions on the boundary of support. Wereinstate that all these works obtained a variance bound of O(N −1).[31] proved the bias is around O(N − β[27] generalized itIt should be noted that the bias bounds given by previous studies typicallydepend on some properties of target densities, such as smoothness parameterand Hessian matrix, providing insights that these estimators perform well oncertain distributions. This motivates the idea that one can transform the givendata points toward a desired distribution for a more accurate entropy estimation,which is detailed in next section.1251301353. Uniformizing Mapping Based Entropy EstimationIn this section, we present the proposed approach in detail. As is mentionedearlier, it consists of two main ingredients: a truncated version of the k-NNentropy estimators, and a transformation that can map data points toward auniform distribution.1403.1. Truncated KL/KSG EstimatorsFor compactly supported distributions, a significant source of bias comesfrom the boundary of the support, where the k-NN cells are constructed in-cluding areas outside of the support of the distribution density [31]. Intuitively6speaking, incorrectly including such areas results in an underestimate of the den-sities, leading to bias in the estimator. We thus propose a method to reduce theestimation bias by excluding the areas outside of the distribution support, andremarkably the resulting estimator enjoy certain convergence properties whichenable us to design the NF based estimation approach. The only additionalrequirement for using these estimators is that the bound of support of densityshould be specified. Without loss of generality, we suppose the target densityis supported on the unit cube Q := [0, 1]d in Rd. The procedure of our methodis as follows: we first determine all the cells using either KL or KSG, then ex-amine whether each k-NN cell covers area out of the distribution support, andif so, truncate the cell at the boundary to exclude such area (see Fig. 1 fora schematic illustration). Mathematically the truncated KL (tKL) estimator(with ∞-norm), is given by(cid:3)HtKL(X) = −ψ(k) + ψ(N ) +1NN(cid:4)d(cid:4)i=1j=1log ξi,j,(10)wherej + (cid:3)i/2, 1} − max{x(i)and the truncated KSG (tKSG) esitmator is given byξi,j = min{x(i)j− (cid:3)i/2, 0};(cid:3)HtKSG(X) =− ψ(k) +ψ (N ) + (d − 1)/k +1NN(cid:4)d(cid:4)i=1j=1log ζi,j,(11)whereζi,j = min{x(i)j + (cid:3)i,j/2, 1} − max{x(i)j− (cid:3)i,j/2, 0}.Next we shall theoretically analyze the bias of the truncated estimators.Our analysis relies on some assumptions on the density function px, which aresummarized as below:Assumption 1. The distribution px satisfies:(a) px is continuous and supported on Q;(b) px is bounded away from 0, i.e., C1 = infx∈Q(c) The gradient of px is uniformly bounded on Qo, i.e., C2 = supx∈Qopx(x) > 0;∞.||(cid:2)px(x)||1 <First we consider the bias of estimator (cid:3)HtKL and the following theorem statesthat, the bias in (cid:3)HtKL is bounded and vanishes at the rate of O(N − 1d ).145150Theorem 1. Under Assumption 1 and for any finite k and d, the bias of thetruncated KL estimator is bounded by(cid:5)(cid:5)E[ (cid:3)HtKL(X)] − H(X)(cid:5)(cid:5) ≤C2C 1+1/d1(cid:7) 1d .(cid:6) kN7The variance of the truncated KL estimator is bounded byVar[ (cid:3)HtKL(X)] ≤ C1N,for some C > 0.Proof. We provide a skeleton proof here, where the complete proof includingthe notations is detailed in Appendix A.3 and Appendix A.4.Proof of the bias bound for the truncated KL estimator proceeds as follows.1. Show thatE[ (cid:3)HtKL(X)] = −E(cid:8)logP (B(x; (cid:3)k/2))μ(B(x; (cid:3)k/2))(cid:9).2. Bound the following difference by(cid:5)(cid:5)(cid:5)(cid:5) log p(x) − logP (B(x; (cid:3)k/2))μ(B(x; (cid:3)k/2))(cid:5)(cid:5)(cid:5) ≤ C2(cid:5)2C1(cid:3)k.(12)(13)3. Note that H(X) = −E(log p(x)), and using Eq. (12), Eq. (13) and theupper bound of E((cid:3)k) obtained from Lemma 4, we can derive that thebias E[ (cid:3)HtKL(X)] is bounded by(cid:5)(cid:5)E[ (cid:3)HtKL(X)] − H(X)(cid:7) 1d .(cid:5)(cid:5) ≤(14)C2C 1+1/d1(cid:6) kN155Proof of the variance bound for the truncated KL estimator proceeds as fol-lows.1. Let αi =(cid:10)dj=1 log ξi,j and let α∗i (for i = 2, ..., N ) be the estimators withsample x(1) removed. Then, by the Efron-Stein inequality [38],Var[ (cid:3)HtKL(X)] = Var(cid:11)1NN(cid:4)i=1(cid:12)(cid:11)(cid:13)αi≤ 2N E1NN(cid:4)i=1αi − 1NN(cid:4)i=2α∗i(cid:14)2(cid:12).2. Let 1Ei be the indicator function of the event Ei = {(cid:3)k(x(1)) (cid:7)= (cid:3)∗k(x(1)) is twice the k-NN distance of x(1) when α∗(15)k(x(1))},i are used. Thenαi −(cid:14)21NN(cid:4)i=2α∗i(cid:13)≤ (1 + Ck,d)α21 + 2(cid:14)1Ei(α2i + α∗2i ),N(cid:4)i=2(16)where Ck,d is a constant.3. Since αi and α∗upper bounds of the following three expectations: E[α2and (N − 1)E[1E2α∗22 ].i are identically distributed, we only need to derive the1], (N − 1)E[1E2α22]1608where (cid:3)∗we show that(cid:13)N(cid:4)N 21Ni=14. Finally we obtain the bound of the variance of (cid:3)HtKL(X)Var[ (cid:3)HtKL(X)] ≤ C1N,(17)for some C > 0.Note that C2 = 0 when px is uniform on Q, and the following corollaryfollows directly:165Corollary 1. Under the assumption in Theorem 1, if X is uniformly distributedon Q, then the truncated KL estimator is unbiased.170175This corollary is the theoretical foundation of the proposed method, as itsuggests that if one can transform the data points into a uniform distribution,the tKL method can yield an unbiased estimate.In reality, it is usually im-possible to map the data point exactly into a uniform distribution to achievethe unbiased estimate. To this end, Theorem 1 suggests that, as long as thetransformed samples are close to a uniform distribution in the sense that C2 issmall, the transformation can still significantly reduce the bias. Since the maincontribution of the mean-square estimation error comes from the bias (as thevariance decays at the rate of O(N −1)), reducing the bias therefore leads muchmore accurate estimation of the entropy.We next consider the bias of the tKSG estimator. The second theoremshows that the expectation of (cid:3)HtKSG has the same limiting behavior up to apolylogarithmic factor in N .Theorem 2. Under Assumption 1 and for any finite k and d, the bias of thetruncated KSG estimator is bounded by(cid:5)(cid:5)E[ (cid:3)HtKSG(X)] − H(X)(cid:5)(cid:5) ≤ C(log N )k+2C k+11 N1dfor some C > 0. The variance of the truncated KSG estimator is bounded byVar[ (cid:3)HtKSG(X)] ≤ C (cid:5) (log N )k+2,N180for some C (cid:5) > 0.Proof. Again, we only provide a skeleton proof here, with the complete detailsgiven in Appendix A.5 and Appendix A.6.Proof of the bias bound for the truncated KSG estimator proceeds as follows.1. Suppose that (cid:15)P , (cid:15)p, and (cid:15)q(cid:3)x1k ,...,(cid:3)xdk(x) are defined as in Lemma 2 withdj=1 log ζi,j are identically(cid:10)d , and by Lemma 2 and the fact thatl = p(x)− 1distributed, we haveE[ (cid:3)HtKSG(X)] = Ex∼p(cid:8)EPlog ζ x1k· · ·ζ xdk(cid:9)− Ex∼pE(cid:2)P(cid:6)(cid:8)logp(x)(cid:3)x1k· · ·(cid:3) xdk(cid:7)(cid:9). (18)92. We separate the d-dimensional unit cube Q into two subsets, Q = Q1 +Q2,(cid:6)(cid:7) 1d , andQ 2 = Q − Q1.185where Q1 := [ aN2 , 1 − aN2 ]d, aN =2k log NC1N3. Note that H(X) =− E(log p(x)), and we can then decompose the biasinto three terms according to the above separation of unit cube:(cid:5)(cid:5)E[ (cid:3)HtKSG(X)] − H(X)(cid:5)(cid:5)(cid:7)(cid:9)(cid:6)(cid:5)(cid:5) Ex∼p· · ·ζ xdkζ x1klog(cid:8)=EP≤I1 + I2 + I3,(cid:5)(cid:5)(cid:6)(cid:8)log(cid:3)x1k· · ·(cid:3) xdk(cid:7)(cid:9)(cid:5)(cid:5)(cid:5)(cid:5)− Ex∼pE(cid:2)P(19)withI1 =I2 =I3 =(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q2(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q1(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q(cid:8)logEP :(cid:3)k<aN(cid:6)(cid:6)ζ x1k· · ·ζ xdkζ x1k· · ·ζ xdk(cid:7)(cid:9)(cid:7)(cid:9)(cid:8)log(cid:6)logζ x1k· · · ζ xdk(cid:7)(cid:9)EP :(cid:3)k<aN(cid:8)EP :(cid:3)k≥aN(cid:5)(cid:5)(cid:5)(cid:5) +(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q2E(cid:2)P :(cid:3)k<aN(cid:8)− Ex∈Q1(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q(cid:5)(cid:5)(cid:5)(cid:5) +E(cid:2)P :(cid:3)k<aN(cid:8)E(cid:2)P :(cid:3)k≥aN(cid:6)(cid:8)log(cid:3)x1k· · ·(cid:3) xdk(cid:7)(cid:9)(cid:5)(cid:5)(cid:5)(cid:5),(cid:6)log(cid:3)x1k· · · (cid:3)xdk(cid:7)(cid:9)(cid:6)log(cid:3)x1k· · ·(cid:3) xdk(cid:7)(cid:9)(cid:5)(cid:5)(cid:5)(cid:5),(cid:5)(cid:5)(cid:5)(cid:5),(20)means taking expectation under the probability measure Pwhereover (cid:3)xjEP :(cid:3)k<aNk < aN , j = 1, ..., d.4. Finally, by bounding the three terms separately, we obtain(cid:5)(cid:5)E[ (cid:3)HtKSG(X)] − H(X)(cid:5)(cid:5) ≤ C(log N )k+2C k+11 N1d,(21)for some C > 0.1901. Let βi =(cid:10)Proof of variance bound for the truncated KSG estimator proceeds as fllows.dj=1 log ζi,j, and define β∗with sample x(1) removed. Next we show that (N − 1)E[1E2β21)E[1E2β∗2that E[β2(cid:8)2. Separate E2 ] are of the same order as E[β21 ] =O ((log N )k+2), which is done in Steps 2 and 3.(cid:9)i (for i = 2, ..., N ) to be the estimators2 ] and (N −1 ]. As such we only need to proveβ21into two parts,(cid:8)(cid:9)Eβ21= Ex∈QEP :(cid:3)k<aN(cid:9)(cid:8)β21+ Ex∈QEP :(cid:3)k≥aN(cid:8)(cid:9),β21(22)where aN =(cid:6)(cid:7) 1d .2k log NC1N3. By bounding the two parts separately, we obtain the bound of the expec-tation of β21195for some C9 > 0.E[β21 ] ≤ C9(log N )k+2,(23)104. With the above bound, we can obtain the bound of the variance of (cid:3)HtKSG(X)Var[ (cid:3)HtKSG(X)] ≤ C (cid:5) (log N )k+2(24),Nfor some C (cid:5) > 0.200205210215As one can see from Theorem 2, while the uniform distribution leads to zerobias for (cid:3)HtKL, we can not obtain the same result for (cid:3)HtKSG, which means notheoretical justification for mapping the data points toward a uniform distri-bution for this estimator. That said, the tKSG estimator and Theorem 2 arestill useful, and the reason for that is two-fold. First as is mentioned earlier, noexisting result on the bound of bias is available for the KSG estimator to thebest of our knowledge, and to this end our analysis on tKSG is the first knownbias bound for this type of estimators, and may provide useful information forunderstanding the convergence property of them. More importantly, our numer-ical experiments demonstrate that mapping the data points toward a uniformdistribution does significantly improve the performance of tKSG as well. In fact,we have found that tKSG can achieve the same or slightly better results thantKL on the transformed samples in our test cases.3.2. Estimating Entropy via TransformationAs is mentioned earlier, based on the interesting convergence properties ofthe truncated estimators in particularly tKL, we want to estimate the entropyof a given set of samples by mapping them toward a uniform distribution. Toimplement this idea, an essential question to ask is that, how the entropy of thetransformed samples relates to that of the original ones. Proposition 1 providesan answer to this question.Proposition 1 ([39]). Let f be a mapping: Rd → Rd, X be random variabledefined on Rd following distribution px, and Z = f (X). If f is bijective anddifferentiable, we have(cid:2)H(X) =H (Z) +pz(z) log(cid:5)(cid:5)(cid:5)(cid:5) det(cid:5)(cid:5)(cid:5)(cid:5)dz,∂f −1(z)∂z(25)where pz(z) is the distribution of Z.Therefore given a data set S = {x(i))}Ni=1 and a mapping Z = f (X), fromEq. (25) we can construct an entropy estimator of X as,(cid:3)H(X) = (cid:3)H(Z) +(cid:5)(cid:5)(cid:5)(cid:5) detlog∂f −1(z(i))∂z(cid:5)(cid:5)(cid:5)(cid:5),1nn(cid:4)i=1(26)where (cid:3)H(Z) is an entropy estimator ofZ (either tKL or tKSG) based on thetransformed samples SZ = {z(i) = f (x(i))}ni=1.22011We refer to such a mapping f (·) as a uniformizing mapping (UM) and theresulting methods as UM based entropy estimators where the main procedureis outlined in Algorithm 1. A central question in the implementation of Algo-rithm 1 is obviously how to construct a UM which can push the samples towarda uniform distribution, which is discussed in next section.The bias of the UM based estimators rely on the property of the UM (orequivalently the NF), on which we make the following assumption:225Assumption 2. Let S = {x(i)}Nstruct the UM and pSsupz∈Qoa positive integer M and a positive real number ¯C < 1 such that:i=1 be the set of i.i.d samples used to con-2 =0; (2) There existz be the resulting density of Z in Eq. (26). Denote C Nz (z)||1, and assume that C N2 satisfies: (1) C N2P−→N→∞||(cid:2)pS∀N > M, C N2≤ ¯C, a.s.Based on Theorem 1 and Theorem 2, we can obtain the bias bounds and theMSE bounds of the UM based estimators.Corollary 2. Suppose that the density function of the original distribution isdifferentiable and the UM satisfies Assumption 2. The bias of UM-tKL estimatoris bounded by(cid:5)(cid:5)E[ (cid:3)HUM−tKL(X)] − H(X)(cid:5)(cid:5) ≤ C NUM−tKL(cid:7) 1d ,(cid:6) kNwhere limN→∞C NUM−tKL = 0. The MSE of UM-tKL estimator is bounded byE[( (cid:3)HUM−tKL(X) − H(X))2] ≤ C11N+ DNUM− tKL(cid:7) 2d ,(cid:6) kN(27)(28)230where C1 is a positive constant and limN→∞DNUM− tKL = 0.Proof. See Appendix B.Corollary 3. Suppose that the density function of the original distribution isdifferentiable and the UM satisfies Assumption 2. The bias of UM-tKSG esti-mator is bounded by(cid:5)(cid:5)E[ (cid:3)HUM−tKSG(X)] − H(X)(cid:7)(1+ ¯C)d+1(1+ ¯C)(cid:6)where CUM− tKSG = Cof UM-tKSG estimator is bounded by(1− ¯C)k+1(cid:5)(cid:5) ≤ CUM− tKSG(log N )k+2N1d,(29)and C is a positive constant. The MSEE[( (cid:3)HUM−tKSG(X) − H(X))2] ≤ C2(log N )k+2Nwhere C2 is a positive constant and DNUM− tKSG =(cid:16)C12+ DNUM− tKSG(log N )2(k+2)N(cid:7)(1+ ¯C)d+1(cid:17)22d,(30).(1− ¯C)k+1(cid:6)(1+ ¯C)Proof. See Appendix C.Algorithm 1 UM based entropy estimatorInput: a set of i.i.d samples: SX = {x(i)};Output: an entropy estimate (cid:3)H(X);• compute a uniformizing map f (·);• let SZ = {z(i) = f (x(i)), i = 1, ..., n};• estimate (cid:3)H(Z) from SZ using Eq. (10) or Eq. (11);• compute (cid:3)H(X) using Eq. (26).3.3. Constructing UM via Normalizing FlowWe discuss in this section how to construct a UM via the NF method. Firstsince the image of f is [0, 1]d, we assume that f is in the form of f = Φ ◦ g whereg : Rd → Rd is learned and Φ : Rd → [0, 1]d is prescribed. Recall that pz is thedistribution of Z = f (X) withX following px, and we want the function g byminimize the Kullback-Leibler divergence (KLD) between pz and the uniformdistribution pu:(cid:2)D(pz|pu) :=pz(z) logming∈Ω(cid:11)pz(z)pu(z)(cid:12)dz,(31)235where z = Φ ◦ g(x) and Ω is a suitable function space. Solving Eq. (31) directlyposes some computational difficulty as the calculation involves the function Φ,the choice of which may affect the computational efficiency. To simplify thecomputation, we recall the following proposition:Proposition 2 ([34]). Let T : Y → Z be a bijective and differentiable transfor-mation, pz(z) be the distribution obtained by passing py(y) through T , and πz(z)be the distribution obtained by passing πy(y) through T . Then the equalityD(πy(y)||py(y)) = D(πz(z)||pz(z))(32)holds.We now construct the mapping Φ with the cumulative distribution func-tion of the standard normal distribution, a technique known as the probabilityintegral transform, yielding, for a given y ∈ Rd,Φ(y) = (φ1(y1), ..., φd(yd)), φi(yi) =12(1 + erf(y√2)),where erf(·) is the error function. It should be clear that if y follows a standardnormal distribution, z = Φ(y) follows a uniform distribution in [0, 1]d, and vice13versa. Now applying Proposition 2, we can show that Eq. (31) is equivalent toD(py(y)|q(y)),ming∈Ω(33)where y = g(x) follows distribution py(·) andq (·) is the standard normal distri-bution. Now assume that g(·) is invertible and let its inverse be h = g−1. Wealso assume that both g and h are differentiable. Applying Proposition 2 toEq. (33) with T = h, we find that Eq. (33) is equivalent tominh∈Ω−1D(px(x)|qh(x)),(34)where Ω−1 = {g−1|g ∈ Ω} and qh is the distribution obtained by passing qthrough the mapping h:qh(x) = q(cid:6)h−1(x)(cid:13)(cid:7)(cid:5)(cid:5)(cid:5)(cid:5) det(cid:14) (cid:5)(cid:5)(cid:5)(cid:5).∂h−1∂x(35)Eq. (34) essentially says that we want to push a standard normal distributionq toward a target distribution px, and therefore solving Eq. (34) falls naturallyinto the framework of NF. Specifically, NF aims to build such a mapping hby composing multiple simple mappings: h = h1 ◦ ... ◦ hK . Each hk needs tobe a diffeomorphism: namely it is invertible and both it and its inverse aredifferentiable, which ensures that their composition h is also a diffeomorphism.Next by plugging in the data, we can rewrite Eq. (34) as a maximum likelihoodproblem:maxh=(h1,...,hK )Epx[log qh(x)] :≈1NN(cid:4)i=1log qh(x(i)).(36)As is mentioned earlier, the intermediate mapping hi is usually taken to be ofa simple parametrized form and so that its gradient and inverse are easy tocompute. Once h1, ..., hK are computed, the function g can be obtained asg = (h1 ◦ · · · ◦ hK )−1 = h−1K◦ · · · ◦ h−11 ,(37)and recall that in Eq. (13) in the main paper we also need the det-Jacobian ofmapping g−1 (i.e., h), which can be calculated as,det∂g−1(y)∂y= det∂h1(y1)∂y1◦ · · · ◦ det∂hK(yK)∂yK,(38)240where yK = y, y0 = x and yk−1 = hk(yk) for k = 1, ..., K.The NF methods depend critically on the component layers, the choice ofwhich has to be balanced between computational efficiency and representingflexibility. In this paper, we use a special version of NF, the Masked Autoregres-sive Flow (MAF) [35] that is originally designed for density estimation. Sincethe purpose of MAF is to estimate the density px, it is specifically designed toefficiently evaluate the inverse mappings, which is thus particularly useful for24514250255260265our application. We note, however, our method does not rely on any specificimplementation of NF.Once the mapping h(·) (or equivalently g−1(·)) is obtained, it can be inserteddirectly into Algorithm 1 to estimate the sought entropy. In practice, the sam-ples are split into two sets, where one of them is used to construct the UM andthe other is used to estimate the entropy.4. Numerical ExperimentsBefore diving into the applications, we conduct several numerical compar-isons of the proposed estimators using mathematical examples. The code for re-producing these examples can be found in https://github.com/ziq-ao/NFEE.4.1. An Illustrating Example for the Truncated EstimatorsHere we use a toy example to demonstrate the improvement of the truncatedestimators over the na¨ıve version. Specifically, the test example is an indepen-dent multivariate Beta distributions B(b, b) with dimensionality d and shapeparameter b. In the numerical experiments, the dimensionality is varied from 1to 40 and the parameter b takes three values 1, 1.5 and 2. In each setup, wegenerate 1000 samples from the distribution and use KL, KSG, tKL and tKSGto estimate the entropy. All experiments are repeated 100 times and the Root-mean-square-error (RMSE) of estimates are computed. In Fig. 2, we plot theRMSE (on a logarithmic scale) against the dimensionality d. From this figure,we can see that the truncated methods (blue lines) significantly outperform thena¨ıve ones (red lines) in all cases, indicating that the truncation technique canimprove the performance of the KL/KSG estimators for compactly supporteddistributions.ESMR10310210110010-110-2tKL, b=1KL, b=1tKL, b=1.5KL, b=1.5tKL, b=2KL, b=2tKSG, b=1KSG, b=1tKSG, b=1.5KSG, b=1.5tKSG, b=2KSG, b=25101520d25303540Figure 2: truncated estimators vs non-truncated estimators for multidimensional Beta distri-butions with various shape parameters b.27015UM-tKLUM-tKSGKLKSG101100ESMR10-11020d3040UM-tKLUM-tKSGKLKSG10110010-1ESMR10-20.511.52N2.5 3104Figure 3: Left: RMSE plotted against the dimensionality d. Right: RMSE (on a logarithmicscale) plotted against the sample size N .4.2. Multivariate Normal DistributionTo validate the idea of UM based entropy estimator, a natural question toask is that how it works with a perfect NF transformation, that yields exactlynormally distributed samples. To answer this question, we first conduct the nu-merical tests with the standard multivariate normal distribution, correspondingto the situation that one has done a perfect NF (in this case the function g inSection 3.3 is chosen to be identity map).Specifically we test the four methods: KL, KSG, UM-tKL and UM-tKSG,and we conduct two sets of tests: in the first one we fix the sample size to be 1000and vary the dimensionality, while in the second one we fix the dimensionalityto be 40 and vary the sample size. All the tests are repeated 100 times and theRMSE of the estimates are calculated. In Fig. 3 (left), we plot the RMSE (ona logarithmic scale) as a function of the dimensionality. One can see from thisfigure that, as the dimensionality increases, the estimation error in KL and KSGgrows significantly faster than that in the two UM based ones, with the error inKL being particularly large. Next in Fig. 3 (right) we plot the RMSE againstthe sample size N (note that the plot is on a log-log scale) for d = 40, whichshows that for this high-dimensional case, the two UM based estimators yieldmuch lower and faster-decaying RMSE than those two estimators on the originalsamples. Overall these results support the theoretical findings in Section 3.1 thatthe estimation error can be significantly reduced by mapping the target samplestoward a uniform distribution.4.3. Multivariate Rosenbrock DistributionIn this example we shall see how the proposed method performs when NFis included. Specifically our example is the Rosenbrock type of distributions –the standard Rosenbrock distribution is 2-D and widely used as a testing exam-ple for various of statistical methods. Here we consider two high-dimensionalextensions of the 2-D Rosenbrock [40]: the hybrid Rosenbrock (HR) and theeven Rosenbrock (ER) distributions. The details of the two distributions in-cluding their density functions are provided in Appendix D.2. The Rosenbrock27528028529029530016distribution is strongly non-Gaussian, and that can be demonstrated by Fig. 4(left) which shows the samples drawn from 2-D Rosenbrock. As a comparison,Fig. 4 (right) shows the samples that have been transformed toward a uniformdistribution and used in entropy estimation.Original samplesUM-transformed samples10.500050.5120100-5305310315320325Figure 4: Left: the original samples drawn from a 2-D Rosenbrock distribution; Right: theUM-transformed samples used in the entropy estimation.In this example we compare the performance of seven estimators: in additionto the four used in the previous example, we include an estimator only usingNF (details in SI) as well as two state-of-the-art entropy estimators: CADEE[21] and the von-Mises based estimator [22]. First we test how the estimatorsscale with respect to dimensionality, where the sample size is taken to be N =500d. With each method, the experiment is repeated 20 times and the RMSE iscalculated. The RMSE against the dimensionality d for both test distributions isplotted in Figs. 5 (a) and (b). One can observe here that in most cases, the UMbased methods (especially UM-tKSG) offer the best performance. An exceptionis that CADEE performs better in low dimensional cases for ER, but its RMSEgrows much higher than that of the UM methods in the high-dimensional regime(d >15). Our second experiment is to fix the dimensionality at d = 10 and varythe sample size, where the RMSE is plotted against the sample size for both HRand ER in Figs. 5 (c) and (d). The figures show clearly that the RMSE of theUM based estimators decays faster than other methods in both examples, withthe only exception being CADEE in the small sample (≤ 104) regime of ER.It is also worth noting that, though it is not justified theoretically, UM-tKSGseems to perform slightly better than UM-tKL in all the cases.4.4. Multivariate Rosenbrock Distribution with Discontinuous DensityRecall that Corollaries 2 and 3 assume the differentiability of the originaldensity functions, which is often not satisfied by practice. Thus, it is also of inter-est to examine the performance of the proposed methods for distributions withdiscontinuous densities. To this end, we modify the multivariate Rosenbrockdistributions studied in Section 4.3, so that their densities are discontinuous onthe boundaries of their supports (see Appendix D.2 for the details), and repeat17ESMR10210110010-1101ESMR(a)(b)ESMR100NFCADEEvon-Mises20UM-tKLUM-tKSGKLKSG10d(c)5101520d(d)101100ESMR10-1103104N100103104NFigure 5: Top: RMSE vs. dimensionality for HR (a) and ER (b); Bottom: RMSE vs. samplesize for HR (c) and ER (d).330335the comparisons conducted in Section 4.3. The results are shown in Figs. 6. Forthe modified HR (in Fig. 6 (a) and (c)), only the von-Mises estimator achieves asmaller RMSE than the UM based ones in the low-dimensional regime (d≤10),while the UM based estimators perform the best in the high-dimensional regime.For modified ER (in Fig. 6 (b) and (d)), the UM based estimators are inferiorto CADEE but outperform any other methods in most cases.5. Application ExamplesIn this section, we consider two applications involving entropy estimation,in which our methods are compared with the existing ones.5.1. Application to Entropy Rate EstimationOur first application example is to estimate the differential entropy rateof a continuous-valued time series. Shannon entropy rate [41] measures theuncertainty of a stochastic process X = {Xi}i∈N. For a stationary process, it isdefined as,¯H(X ) = limt→∞H(Xt | Xt−1, ..., X1),(39)340where H(· | ·) is the conditional entropy of two random variables.In thisexample, we consider the stochastic processes that satisfy the following twoassumptions:18101ESMR10010-1654321ESMR103(a)(b)NFCADEEvon-Mises20UM-tKLUM-tKSGKLKSG10d(c)101ESMR10010-1ESMR1002010d(d)104N103104NFigure 6: Top: RMSE vs. dimensionality for modified HR (a) and ER (b); Bottom: RMSEvs. sample size for modified HR (c) and ER (d).• First X is a conditionally stationary process of order p: there exists a fixedpositive integer p such that, for any integer t > p, the conditional densityfunction of Xt given Xt−1 = xt−1, ..., Xt−p = xt−p satisfiesp(Xt = xt | Xt−1 = xt−1, ..., Xt−p = xt−p) = f (xt | xt−1, ..., xt−p),(40)where f is a fixed conditional density function independent from t.• Second X is a Markov process of order p: there exists a positive integer psuch that, for any integer t > p,p(Xt = xt | Xt−1 = xt−1, ..., X1 = x1)= p(Xt = xt | Xt−1 = xt−1, ..., Xt−p = xt−p.)(41)Under these assumptions, the entropy rate of X can be calculated as,¯H = H(Xt | X(t−1):(t−p)) = H(Xt:(t−p)) − H(X(t−1):(t−p)),(42)where Xt:(t−p) = (Xt, Xt−1, ..., Xt−p) and so on. Note here that t can be takento be any integer > p, and for simplicity we can take it to be t = p + 1, and asa result Eq. (42) is simplified to,¯H = H(Xt | X(t−1):(t−p)) =H (X(p+1):1) − H(Xp:1).19Suppose that we have a T -step (with T > p) observation of X : {xt}Tcan compute its entropy rate as follows [42]:t=1, and weˆH = (cid:3)H(X(p+1):1) − (cid:3)H(Xp:1),345where (cid:3)H(X(p+1):1) and (cid:3)H(Xp:1) are estimated with a desired estimator fromthe observation {xt}Tt=1.In this example, we consider three autoregressive models of orders 3, 7 and15 respectively, which are given byAR(3) : Xt = −1.35 + 0.5Xt−1 + 0.4X 2AR(7) : Xt = −1.35 + 0.5Xt−1 + 0.3X 2− 0.3Xt−3 + (cid:3)t,− 0.3Xt−7 + (cid:3)t,AR(15) : Xt = −1.35 + 0.5Xt−1 + 0.05(Xt−5 + Xt−6 + Xt−7)2−0.005(Xt−11 + Xt−12 + Xt−13)2 − 0.1Xt−15 + (cid:3)t,t−2t−5(43a)(43b)(43c)where (cid:3)t ∼ N (0, (0.03)2) is white noise. Fig. 7 shows the simulated snapshots ofthe three models. We implemented the procedure described above to estimatethe entropy rate of these three models where the entropy is estimated with theseven estimators used in Section 4. On the other hand, since the conditionaldensity functions are analytically available in this example, the entropy rate canalso be directly estimated via the standard Monte Carlo integration, which willbe used as the ground truth. We apply the aforementioned entropy estimatorsto compute the entropy rate with a simulated sequence of 10, 000 steps. Witheach method, 20 repeated trials are conducted and the RMSE is calculated. Theresults are reported in Table 1, from which we make the following observations.The performance of the von-Mises estimator appears to be the best for theAR(3) model, however, all estimators yield very small Root Mean Squared Error(RMSE) suggesting that this problem is not particularly challenging. For theAR(7) model, the UM-based methods have smaller RMSE than the others, andfor the AR(15) model, the two UM-based methods and KSG perform betterthan the other three. Overall, UM-KSG results in the smallest RMSE for bothAR(7) and AR(15).350355360Method UM-tKL UM-tKSGAR(3)0.029AR(7)0.67AR(15)1.150.0510.430.68KL0.0271.231.51KSG0.0320.900.98NF0.120.951.61CADEE0.312.404.14von-Mises0.0160.701.42Table 1: RMSE of entropy rate estimations based on entropy estimators for the autoregressivemodel. The smallest (best) RMSE value is shown in bold.5.2. Application to Optimal Experimental DesignIn this section, we apply entropy estimation to an optimal experimentaldesign (OED) problem. Simply put, the goal of OED is to determine the optimalexperimental conditions (e.g., locations of sensors) that maximize certain utility20tXtX0-1-2420-2050050AR(3)100tAR(7)100tAR(15)150200150200tX420-2050100t150200Figure 7: Snapshots of the simulated time series.function associated with the experiments. Mathematically let λ ∈ D be designparameters representing experimental conditions, θ be the parameter of interest,and Y be the observed data. An often used utility function is the entropy of thedata Y , resulting in the so-called maximum entropy sampling method (MES) [6]:U (λ) :=H (Y |λ),maxλ∈D(44)365370375and therefore evaluating U (λ) becomes an entropy estimation problem. Thisutility function is equivalent to the mutual entropy criterion under certain con-ditions [43]. This formulation is particularly useful for problems with expensiveor intractable likelihoods, as the likelihoods are not needed if the utility functionis computed via entropy estimation. A common application of OED is to deter-mine the observation times for stochastic processes so that one can accuratelyestimate the model parameters and here we provide such an example, arisingfrom the field of population dynamics.Specifically we consider the Lotka-Volterra (LV) predator-prey model [44, 45].Let x and y be the populations of prey and predator respectively, and the LVmodel is given by˙x = ax − xy,˙y = bxy − y,where a and b are respectively the growth rates of the prey and the predator.In practice, often the parameters a and b are not known and need to be esti-mated from the population data. In a Bayesian framework, one can assign aprior distribution on a and b, and infer them from measurements made on the21Figure 8: Top: some sample data paths of (x, y); Bottom: the optimal observation timesobtained by the eight methods.380385population (x, y). Here we assume that the prior for both a and b is a uniformdistribution U [0.5, 4].In particular we assume that the pair (x + (cid:3)x, y + (cid:3)y),where (cid:3)x, (cid:3)y ∼ N (0, 0.01) are independent observation noises, is measured atd = 5 time points located within the interval [0, 10], and the goal is to deter-mine the observation times for the experiments. As is mentioned earlier, we shalldetermine the observation times using the MES method. Namely, the designparameter in this example is λ = (t1, ..., td), the data Y is the pair (x+(cid:3)x, y +(cid:3)y)measured at t1, ..., td, and we want to find λ that maximizes the entropy H(Y |λ).Method UM-tKL UM-tKSG CADEENMC(SE)RMSE-1.45(0.0073)0.480.730.86Equidistant-2.73(0.0074)—KL-1.65(0.0072)3.60KSG-1.56(0.0076)1.05NF-1.48(0.0072)0.88von-Mises-1.81(0.0049)1.31Table 2: The reference entropy values of the observation time placements obtained by usingall the methods. The smallest (best) entropy value is shown in bold.390395A common practice in such problems is not to optimize the observation timesdirectly and instead parametrize them using the percentiles of a prescribed dis-tribution to reduce the optimization dimensionality [46]. Here we use a Betadistribution, resulting in two distribution parameters to be optimized (see [46]and Appendix D.4 for further details). We solve the resulting optimizationproblem with a grid search where the entropy is evaluated by the seven afore-mentioned estimators each with 10,000 samples. We plot in Fig. 8 the optimalobservation time placements computed with the seven aforementioned estima-tors, as well as the equidistant placement for a comparison purpose. Also shownin the figure are some sample paths of the population (x, y) where we can seethat the population samples are generally subject to larger variations near thetwo ends and relative smaller ones in the middle. Regarding the optimization22400405410415420425results, we see that the optimal time placements obtained by the two UM basedestimators and CADEE are the same, while they are different from the resultsof other methods. To validate the optimization results, we compute a referenceentropy value for the optimal placement obtained by each method, using NestedMonte Carlo (NMC) (see [47] and Appendix D.5 for details) with a large samplesize (105 ×105), and show the results in Table 2. Note that though the NMC canproduce a rather accurate entropy estimate, it is too expensive to use directly inthis OED problem. Using the reference values as the ground truth, we can fur-ther compute the RMSE of these estimates (over 20 repetitions), which are alsoreported in Table 2. From the table one observes that the placement of obser-vation times computed by the two UM methods and CADEE yields the largestentropy values, which indicates that these three methods clearly outperform allthe other estimators in this OED problem. Moreover, from the RMSE resultswe can see that the UM based methods (especially UM-tKSG) yield smallerRMSE than CADEE, suggesting that they are more statistically reliable thanCADEE.6. ConclusionIn summary, we have presented a uniformization based entropy estimator,and also provided some theoretical analysis of it. We believe the proposedentropy estimator can be useful for a wide range of real-world applications.Some improvements and extensions of the method are possible. First while ourtheoretical results provide some justification for the method, further analysisis needed to establish the convergence rate and understand the estimation bias.Additionally, the method may be extended to estimate other density functionals,such as the Renyi entropy and the Kullback-Leibler divergence. Finally in thiswork the proposed method is demonstrated only with synthetic data, and it istherefore sensible to further examine the method with real-world data sets. Wewill explore these research problems in future studies.7. Acknowledgments430This work was partially supported by the China Scholarship Council (CSC).The authors would also like to thank Dr. Alexander Kraskov for discussionabout the KSG estimator.References[1] O. Vasicek, A test for normality based on sample entropy, Journal of theRoyal Statistical Society: Series B (Methodological) 38 (1) (1976) 54–59.435[2] M. N. Goria, N. N. Leonenko, V. V. Mergel, P. L. Novi Inverardi, A newclass of random vector entropy estimators and its applications in testingstatistical hypotheses, Journal of Nonparametric Statistics 17 (3) (2005)277–297.23440[3] S. Azzi, B. Sudret, J. Wiart, Sensitivity analysis for stochastic simulatorsusing differential entropy, International Journal for Uncertainty Quantifi-cation 10 (1).[4] B. Ranneby, The maximum spacing method. an estimation method relatedto the maximum likelihood method, Scandinavian Journal of Statistics(1984) 93–112.445[5] E. Wolsztynski, E. Thierry, L. Pronzato, Minimum-entropy estimation insemi-parametric models, Signal Processing 85 (5) (2005) 937–949.[6] P. Sebastiani, H. P. Wynn, Maximum entropy sampling and optimalbayesian experimental design, Journal of the Royal Statistical Society: Se-ries B (Statistical Methodology) 62 (1) (2000) 145–157.450455[7] Z. Ao, J. Li, An approximate KLD based experimental design for mod-els with intractable likelihoods, in: International Conference on ArtificialIntelligence and Statistics, PMLR, 2020, pp. 3241–3251.[8] J. Beirlant, E. J. Dudewicz, L. Gy¨orfi, E. C. Van der Meulen, Nonparamet-ric entropy estimation: An overview, International Journal of Mathematicaland Statistical Sciences 6 (1) (1997) 17–39.[9] H. Joe, Estimation of entropy and other functionals of a multivariate den-sity, Annals of the Institute of Statistical Mathematics 41 (4) (1989) 683–697.[10] P. Hall, S. C. Morton, On the estimation of entropy, Annals of the Institute460of Statistical Mathematics 45 (1) (1993) 69–88.[11] K. R. Moon, K. Sricharan, K. Greenewald, A. O. Hero III, Ensemble esti-mation of information divergence, Entropy 20 (8) (2018) 560.[12] G. Pichler, P. J. A. Colombo, M. Boudiaf, G. Koliander, P. Piantanida, Adifferential entropy estimator for training neural networks, in: InternationalConference on Machine Learning, PMLR, 2022, pp. 17691–17715.465[13] L. Gy¨orfi, E. C. Van der Meulen, Density-free convergence properties ofvarious estimators of entropy, Computational Statistics & Data Analysis5 (4) (1987) 425–436.[14] W.-C. Chen, A. Tareen, J. B. Kinney, Density estimation on small data470sets, Physical review letters 121 (16) (2018) 160605.[15] E. G. Miller, A new class of entropy estimators for multi-dimensional den-sities, in: 2003 IEEE International Conference on Acoustics, Speech, andSignal Processing, 2003. Proceedings.(ICASSP’03)., Vol. 3, IEEE, 2003, pp.III–297.475[16] L. Kozachenko, N. N. Leonenko, Sample estimate of the entropy of a ran-dom vector, Problemy Peredachi Informatsii 23 (2) (1987) 9–16.24480485490495500505[17] A. Kraskov, H. St¨ogbauer, P. Grassberger, Estimating mutual information,Physical review E 69 (6) (2004) 066138.[18] S. Gao, G. Ver Steeg, A. Galstyan, Efficient estimation of mutual informa-tion for strongly dependent variables, in: Artificial intelligence and statis-tics, 2015, pp. 277–286.[19] W. M. Lord, J. Sun, E. M. Bollt, Geometric k-nearest neighbor estimationof entropy and mutual information, Chaos: An Interdisciplinary Journal ofNonlinear Science 28 (3) (2018) 033114.[20] T. B. Berrett, R. J. Samworth, M. Yuan, et al., Efficient multivariate en-tropy estimation via k-nearest neighbour distances, Annals of Statistics47 (1) (2019) 288–318.[21] G. Ariel, Y. Louzoun, Estimating differential entropy using recursive copulasplitting, Entropy 22 (2) (2020) 236.[22] K. Kandasamy, A. Krishnamurthy, B. Poczos, L. A. Wasserman, J. M.Robins, Nonparametric von mises estimators for entropies, divergences andmutual informations., in: NIPS, Vol. 15, 2015, pp. 397–405.[23] L. T. Fernholz, Von Mises calculus for statistical functionals, Vol. 19,Springer Science & Business Media, 2012.[24] L. Wen, H. Bai, L. He, Y. Zhou, M. Zhou, Z. Xu, Gradient estimationof information measures in deep learning, Knowledge-Based Systems 224(2021) 107046.[25] J. H. Lim, A. Courville, C. Pal, C.-W. Huang, Ar-dae: towards unbiasedneural entropy gradient estimation, in: International Conference on Ma-chine Learning, PMLR, 2020, pp. 6061–6071.[26] A. Krishnamurthy, K. Kandasamy, B. Poczos, L. Wasserman, Nonparamet-ric estimation of renyi divergence and friends, in: International Conferenceon Machine Learning, PMLR, 2014, pp. 919–927.[27] W. Gao, S. Oh, P. Viswanath, Demystifying fixed k-nearest neighbor infor-mation estimators, IEEE Transactions on Information Theory 64 (8) (2018)5629–5661.[28] K. Sricharan, D. Wei, A. O. Hero, Ensemble estimators for multivariateentropy estimation, IEEE transactions on information theory 59 (7) (2013)4374–4388.510[29] Y. Han, J. Jiao, T. Weissman, Y. Wu, Optimal rates of entropy estimationover lipschitz balls, The Annals of Statistics 48 (6) (2020) 3228–3250.[30] L. Birg´e, P. Massart, Estimation of integral functionals of a density, TheAnnals of Statistics (1995) 11–29.25515520525530[31] S. Singh, B. P´oczos, Finite-sample analysis of fixed-k nearest neighbor den-sity functional estimators, in: Advances in neural information processingsystems, 2016, pp. 1217–1225.[32] G. Biau, L. Devroye, Lectures on the nearest neighbor method, Vol. 246,Springer, 2015.[33] D. Rezende, S. Mohamed, Variational inference with normalizing flows, in:International Conference on Machine Learning, PMLR, 2015, pp. 1530–1538.[34] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, B. Laksh-minarayanan, Normalizing flows for probabilistic modeling and inference,Journal of Machine Learning Research 22 (57) (2021) 1–64.[35] G. Papamakarios, T. Pavlakou, I. Murray, Masked autoregressive flow fordensity estimation, in: Advances in Neural Information Processing Systems,2017, pp. 2338–2347.[36] H. Singh, N. Misra, V. Hnizdo, A. Fedorowicz, E. Demchuk, Nearest neigh-bor estimates of entropy, American journal of mathematical and manage-ment sciences 23 (3-4) (2003) 301–321.[37] A. B. Tsybakov, E. Van der Meulen, Root-n consistent estimators of en-tropy for densities with unbounded support, Scandinavian Journal of Statis-tics (1996) 75–83.[38] B. Efron, C. Stein, The jackknife estimate of variance, The Annals of Statis-535tics (1981) 586–596.[39] S. Ihara, Information theory for continuous systems, Vol. 2, World Scientific,1993.[40] F. Pagani, M. Wiegand, S. Nadarajah, An n-dimensional rosenbrock distri-bution for mcmc testing, arXiv preprint arXiv:1903.09556.540[41] C. E. Shannon, A mathematical theory of communication, The Bell systemtechnical journal 27 (3) (1948) 379–423.[42] D. Darmon, Specific differential entropy rate estimation for continuous-valued time series, Entropy 18 (5) (2016) 190.[43] M. C. Shewry, H. P. Wynn, Maximum entropy sampling, Journal of applied545statistics 14 (2) (1987) 165–170.[44] A. J. Lotka, Elements of physical biology, Williams & Wilkins, 1925.[45] V. Volterra, Variazioni e fluttuazioni del numero d’individui in specie ani-mali conviventi, C. Ferrari, 1927.26550555560565570575580585[46] E. G. Ryan, C. C. Drovandi, M. H. Thompson, A. N. Pettitt, Towardsbayesian experimental design for nonlinear models that require a large num-ber of sampling times, Computational Statistics & Data Analysis 70 (2014)45–60.[47] K. J. Ryan, Estimating expected information gains for experimental designswith application to the random fatigue-limit model, Journal of Computa-tional and Graphical Statistics 12 (3) (2003) 585–603.[48] M. Hardy, Combinatoricsof partial derivatives,arXiv preprintmath/0601149.[49] L. Dinh, J. Sohl-Dickstein, S. Bengio, Density estimation using real NVP,in: 5th International Conference on Learning Representations, ICLR 2017,Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.[50] M. Germain, K. Gregor, I. Murray, H. Larochelle, Made: Masked autoen-coder for distribution estimation, in: International Conference on MachineLearning, PMLR, 2015, pp. 881–889.[51] G. Loaiza-Ganem, Y. Gao, J. P. Cunningham, Maximum entropy flow net-works, in: 5th International Conference on Learning Representations, ICLR2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings,OpenReview.net, 2017.[52] T. Rainforth, R. Cornish, H. Yang, A. Warrington, F. Wood, On nestingmonte carlo estimators, in: International Conference on Machine Learning,PMLR, 2018, pp. 4267–4276.Appendix A. Proofs of Theorem 1 and Theorem 2Here we provide proofs of Theorems 1&2. We follow closely the frameworkfrom [31] and [27] of finite-sample analysis of fixed k nearest neighbor entropyestimators. They both gave a bias bound of roughly O() (γ is some posi-tive constant) and a variance bound of roughly O( 1N ) for the entropy estimator(cid:3)HKL, under some mild assumptions. Similarly here we prove that the proposed(cid:3)HtKL and (cid:3)HtKSG also have such bias and variance bounds. More interestingly,our analysis relates the bias bound of (cid:3)HtKL to the gradient of density function.(cid:7)γ/d1N(cid:6)Appendix A.1. Definitions and assumptionsIn this section, we introduce some notations and assumptions that the proofsrely on. As is mentioned in the main paper, we only consider distributions withdensities supported on the unit cube in Rd. Let Q := [0, 1]d denote the unit cubein d-dimensional Euclidean space Rd and P denote an unknown μ-absolutelycontinuous Borel probability measure, where μ is the Lebesgue measure. Letp : Q → [0, ∞) be the density of P .27590595600605610615Definition 1 (Twice the k-NN distance for cubes). Suppose {x(i)}N −1is seti=1of N − 1 i.i.d. samples from P . We define twice the maximum-norm k-NNdistance for cubes by (cid:3)k(x) = 2||x − x∗||∞, where x∗ is the k-nearest elementamongst {x(i)}N −1i=1to x with respect to ∞-norm.Definition 2 (Twice the k-NN distance for rectangles). Suppose {x(1(cid:2)), ..., x(k(cid:2))}is set of the k nearest elements amongst {x(i)}N −1to x with respect to ∞-norm.i=1We define twice the k-NN distance in the marginal direction xj by (cid:3)xjk (x) =|, where x∗j is the k-nearest element amongst {x(1(cid:2)), ..., x(k(cid:2))} in the2|xj − xmarginal direction xj to x. It should be noted that (cid:3)k(x) = max1≤j≤d(cid:3)xjk (x).∗jjDefinition 3 (Truncated twice the k-NN distance). Since we only considerdensities supported on the unit cube, we define so-called truncated distance forconvenience. In the cubic case, we define truncated twice the k-NN distance inthe marginal direction xj by ξxjk (x) = min{xj +(cid:3)k(x)/2, 1}−max{xj −(cid:3)k(x)/2, 0}.In the rectangular case, such distance in the marginal direction xj is defined byk (x) = min{xj + (cid:3)xjζ xjDefinition 4 (r-cell). We define the r-cell centered at x by B(x; r) ={x (cid:5) ∈ Rd :||x(cid:5) − x||∞ < r} in the cubic case, and by B(x; r1:d) =k (x)/2, 1} −max {xj − (cid:3)xjk (x)/2, 0}.{x(cid:5) ∈ Rd : |x(cid:5)j− xj| <d(cid:18)rj} in the rectangular case.j=1Definition 5 (Truncated r-cell). We define the truncated r-ball centered at xby B(x; r) = Q ∩B (x; r) in the cubic case, and by B(x; r1:d) = Q ∩B (x; r1:d) inthe rectangular case.Definition 6 (Mass function). We define the mass of the cell B(x; r/2) as afunction with respect to r, which is given by pr(x) = P (B(x; r/2)), and definethe mass of the cell B(x; r1:d/2) as a function with respect to r1, ..., rd, which isgiven by qr1,...,rd (x) = P (B(x; r1:d/2)).Assumption 3. We make the following assumptions:(a) p is continuous and supported on Q;(b) p is bounded away from 0, i.e., C1 = infx∈Q(c) The gradient of p is uniformly bounded on Qo, i.e., C2 = supx∈Qop(x) > 0;∞.||(cid:2)p(x)||1 <Appendix A.2. Preliminary lemmasHere, we present some lemmas that support the proofs of the main results.Lemma 1 ([17]). The expectation of log p(cid:3)k (x) satisfiesE[log p(cid:3)k (x)] = ψ(k) − ψ(N ).28Lemma 2. Let (cid:15)P be the probability measure of a uniform distribution supportedld , x ∈ S beon a d-dimensional (hyper-)cubic area S := B(x; l/2), and (cid:15)p(x) = 1the density function. Define (cid:15)qr1,...,rd (x) = (cid:15)P (B(x; r1/2, ..., rd/2)) and (cid:15)pr(x) =(cid:15)P (B(x; r/2)). Then, we haveE[log (cid:15)q(cid:3)x1k ,...,(cid:3)xdk(x)] = ψ(k) − d − 1k− ψ(N ),where (cid:3)xjk , j = 1, ..., d are defined as Definition 2 after replacing P by (cid:15)P .Proof. The probability density function for ((cid:3)x1k , ..., (cid:3)xdk ) is given by,fN,k(r1, ..., rd) =(N − 1)!k!(N − k − 1)!×∂d((cid:15)qkr1,...,rd )∂r1 · · ·∂r d× (1 − (cid:15)prm )N −k−1,(A.1)rj [17]. Then we havewhere (cid:15)pr = (cid:15)P (B(x; r/2)), and rm = max1≤j≤d(cid:14)(cid:13)(cid:2)(cid:2)E[log (cid:15)q(cid:3)(cid:2)lx1k ,...,(cid:3)(cid:2)xdk(cid:13)l==· · ·00(cid:14)(cid:13)N − 1kN − 1k(cid:13)(cid:14)(x)] =N − 1k(cid:2)l(cid:14)0·kd 1ld(cid:2)· · ·(cid:2)01l· · ·(cid:6)∂dl·(cid:7)∂d((cid:15)qkr1,...,rd )∂r1 · · · ∂rd1ld rd1ld rdN − 1k0ld r1 · · · rd)k( 1∂r1 · · ·∂r d1ld r1 · · ·r d)k−1(1 −(u1 · · ·u d)k−1(1 − ud· (1 −(l0(cid:2)10=kd· · ·m)N −k−1 log(u1 · · ·u d)du1 · · ·du d,(A.2)where the last equality comes from the change of variables ui = 1l ri, i = 1, ..., d.Note that the integrand is symmetric under a permutation of the labels 1, ..., d,and so we have0· (1 − (cid:15)prm )N −k−1 log (cid:15)qr1,...,rd dr1 · · ·dr dm)N −k−1 log(m)N −k−1 log(1ld r1 · · ·r d)dr1 · · ·dr d1ld r1 · · · rd)dr1 · · ·dr dE[log (cid:15)q(cid:3)(cid:13)x1k ,...,(cid:3)N − 1k=dkd(x)]xdk(cid:14) (cid:2)1(cid:13)0duduk−1d(1 − udd)N −k−100(cid:2)ud(cid:2)ud· · ·(u1 · · ·u d−1)k−1 log(u1 · · ·u d)du1 · · ·du d−1(cid:14)(A.3)Computing the integral over u1, ..., ud−1 using the symmetry again, we obtain(cid:2)ud0(cid:2)· · ·(cid:2)=(d − 1)=I1 + I2,ud(u1 · · ·u d−1)k−1 log(u1 · · ·u d)du1 · · ·du d−1(cid:2)ud0ud· · ·(u1 · · · ud−1)k−1 log u1du1 · · ·du d−1 + log um0029(cid:2)ud(cid:2)ud· · ·00(u1 · · ·u d−1)k−1du1 · · ·du d−1(A.4)where I1 is the first term and I2 is the second term. By basic calculus, we have(cid:13) (cid:2)ud(cid:14)d−2(u2)k−1du2log u1du1I1 = (d − 1)= (d − 1)(cid:2)ud0(cid:6) 1kukduk−11(cid:7)d−1(log ud − 1k0),andwhich yield I1 + I2 = ( 1change the variables by t = udk ukd. Plug this into Eq (A.3) and(cid:7)d−1ukdI2 = log ud((cid:7)d−1(cid:6)1kd log ud − d−1kd, and we finally have,(cid:7)(A.5)(A.6)E[log (cid:15)q(cid:3)(cid:13)x1k ,...,(cid:3)xdk(cid:14) (cid:2)(x)]=dk(cid:13)=kN − 1kN − 1k(cid:14) (cid:2)01=ψ(k) −− ψ(N ).0d − 1k1ukd−1d(1 − udd)N −k−1(cid:6)(cid:6)d log ud − d − 1k(cid:7)d − 1kdt(cid:7)dud(A.7)tk−1(1 − t)N −k−1log t −Lemma 3 (Lemma 3 in [31]). Suppose p satisfies Assumption (a) and (b).Then, for any x ∈ Q and r >, we have(cid:7)1/d(cid:6)kC1NP((cid:3)k(x) > r) ≤ e−C1rdN(cid:6) eC1rdNk(cid:7)k.Lemma 4 (Lemma 4 in [31]). Suppose p satisfies Assumption (a) and (b).Then, for any x ∈ Q and α > 0, we haveE[(cid:3)αk (x)] ≤ (1 +αd(cid:6) kC1N)(cid:7) αd .Lemma 5. Suppose p satisfies Assumption 3, then, for any x ∈ Q and array(r1, ..., rd) that satisfy(cid:19)xj + rj2xj − rj2≤ 1, if xj ≤ 12≥ 0, if xj > 12for j = 1, ..., d, we have(cid:5)(cid:5)(cid:5)(cid:5)∂dqr1,...,rd (x)∂r1 · · ·∂r d−1(cid:3)dj=1 1j2(cid:5)(cid:5)(cid:5)(cid:5) ≤p(x)1(cid:3)d2j=1 1j +1and(cid:5)(cid:5)(cid:5)(cid:5)∂uqr1,...,rd (x)∂r1 · · · ∂ru−1(cid:3)uj=1 1j2(cid:6)B(xu+1:d;p(x)μru+12, ...,rd2(cid:7)(cid:5)(cid:5)(cid:5)(cid:5) ≤)30C2rm,1(cid:3)u2j=1 1j +1(cid:6)C2rmμB(xu+1:d;ru+12, ...,(cid:7),)rd26202 , xj + rjrj and 1j is the indicator function admitting the valuewhere u < d, rm = max1≤j≤d1 if [xj − rj2 ] intersects [0, 1] and 0 otherwisely.Proof. For the sake of convenience, we only discuss the case when x ∈ [0, 12 ]dand 1j = 1 for j = 1, ..., n ≤ u. The proof for other cases can be obtained bypermuting the labels 1, ..., d. By the definition of qr1,...,rd (x), we haveqr1,...,rd (x) =(cid:2)(cid:2)x1+r1/2x1−r1/2x1+r1/2=0(cid:2)(cid:2)· · ·· · ·xd+rd/2xd−rd/2xn+rn/2p(x(cid:5)1, ..., x(cid:5)d)dx(cid:5)d· · ·dx (cid:5)1(cid:2)xn+1+rn+12(cid:2)· · ·xd+rd/2xd−rd/2p(x(cid:5)1, ..., x(cid:5)d)dx(cid:5)d· · · dx(cid:5)1,0xn+1−rn+12(A.8)and the partial derivative of it with respect to the first n variables is given by∂nqr1,...,rd (x)∂r1 · · ·∂r nxn+1+12nxn+1−(cid:2)rn+12rn+12=(cid:2)· · ·xd+rd/2xd−rd/2p(x1 +r12, ..., xn +rn2, x(cid:5)n+1, ..., x(cid:5)d)dx(cid:5)d· · · dx(cid:5)n+1.(A.9)Next we obtain the partial derivative of qr1,...,rd (x) with respect to the first uvariablesxu+1+ru+1/2(cid:2)∂uqr1,...,rd (x)∂r1 · · ·∂r u12u12uB(xu+1:d;(cid:2)==xu+1−ru+1/2ru+12(cid:2)· · ·xd+rd/2xd−rd/2p(x1 +r12, ..., xn +rn2, xn+1 ±rn+12, ..., xu ±ru2, x(cid:5)u+1, ..., x(cid:5)d)dx(cid:5)u+1· · ·dx (cid:5)dp(x1 +r12, ..., xn +rn2, xn+1 ±rn+12, ..., xu ±ru2,...,rd2 ), x(cid:5)u+1, ..., x(cid:5)d)dx(cid:5)u+1· · ·dx (cid:5)d,(A.10)where the notation p(..., x ± r2 , ...) = p(..., x + r2 , ...) +p( ..., x − r2 , ...).Finally, we have∂uqr1,...,rd (x)∂r1 · · ·∂r u(cid:2)−B(xu+1:d;ru+12,...,rd2 )p(x)μ1(cid:3)uj=1 1j2(cid:5)(cid:5)(cid:5)(cid:5)p(x1 +(cid:6)B(xu+1:d;r12, ..., xn +ru+12rn2(cid:7)(cid:5)(cid:5)(cid:5)(cid:5))rd2, ...,, xn+1 ±rn+12, ..., xu ±ru2, x(cid:5)u+1, ..., x(cid:5)d)− 2u−np(x)(cid:5)(cid:5)(cid:5)(cid:5)dx(cid:5)u+1· · ·dx (cid:5)d(cid:2)2u−n2u12n+1 C2rmμ(cid:6)B(xu+1:d;ru+12,...,B(xu+1:d;C2rd2 )ru+12rm2dx(cid:5)u+1· · · dx(cid:5)d, ...,(cid:7),)rd231(A.11)(cid:5)(cid:5)(cid:5)(cid:5)12u≤≤=which completes the proof for u < d.Particularly, we have(cid:5)(cid:5)(cid:5)(cid:5)∂dqr1,...,rd (x)∂r1 · · ·∂r d(cid:5)(cid:5)(cid:5)(cid:5) ≤p(x)−1(cid:3)dj=1 1j21(cid:3)d2j=1 1j +1C2rm.(A.12)Lemma 6. Suppose p satisfies Assumption 3, then, for any x ∈ Q and r thatsatisfy(cid:19)xj + r2xj − r2≤ 1, if x ≤ 12≥ 0, if x > 12for j = 1, ..., d, we have(cid:5)(cid:5)(cid:5)(cid:5)pr(x) − p(x)μ(cid:6)B(x;(cid:7)(cid:5)(cid:5)(cid:5)(cid:5) ≤ C2r2)r2B(x;r2),and(cid:5)(cid:5)(cid:5)(cid:5)dpr(x)dr−d(cid:4)j=1121j(cid:6)p(x)μB(xˆj;(cid:7)(cid:5)(cid:5)(cid:5)(cid:5) ≤r2)d(cid:4)j=1(cid:6)121j +1 C2rμB(xˆj;(cid:7),)r2625where m < d and 1j is the indicator function admitting the value 1 if [xj −2 , xj + rrProof. By the definition of pr(x), we have2 ] intersects [0, 1] and 0 otherwiesly.(cid:2)pr(x) =B(x; r2 )It then follows that,p(x(cid:5)1, ..., x(cid:5)d)dx(cid:5)d· · ·dx (cid:5)1.(A.13)(cid:5)(cid:5)(cid:5)(cid:5)pr(x) − p(x)μ(cid:2)(cid:5)(cid:5)p(x(cid:5)B(x; r2 )(cid:2)(cid:6)B(x;(cid:7)(cid:5)(cid:5)(cid:5)(cid:5)r2)1, ..., x(cid:5)d) − p(x)C2r2dx(cid:5)d· · ·dx (cid:5)1≤≤2 )B(x; rr2B(x;r2),=C2(cid:5)(cid:5)dx(cid:5)d· · · dx(cid:5)1(A.14)which completes proof of the first inequality. For the second inequality, one caneasily see thatpr(x) = qr,...,r(x).(A.15)32Now using Lemma 5, we obtain(cid:5)(cid:5)(cid:5)(cid:5)dpr(x)dr(cid:5)(cid:5)(cid:5)(cid:5)d(cid:4)≤≤j=1d(cid:4)j=1−d(cid:4)j=1121j(cid:6)p(x)μB(xˆj;(cid:7)(cid:5)(cid:5)(cid:5)(cid:5)r2)∂qr1,...,rd (x)∂rj(cid:5)(cid:5)(cid:5)− 121j(cid:6)p(x)μB(xˆj;r1:d=r(cid:7)(cid:5)(cid:5)(cid:5)(cid:5)r2)(A.16)(cid:6)121j +1 C2rμB(xˆj;(cid:7).)r2Appendix A.3. Proof of bias bound for the truncated KL estimatorProof. Note thatdj=1 log ξi,j are identically distributed, then we have(cid:10)E[ (cid:3)HtKL(X)] = −ψ(k) + ψ(N ) += −ψ(k) + ψ(N ) +EN(cid:4)(cid:8) d(cid:4)E(cid:9)log ξi,j1Ni=1(cid:8) d(cid:4)j=1(cid:9)log ξxjk (x)j=1= −E[log p(cid:3)k (x)] + E[log μ(B(x; ξx1(cid:9)(cid:8)k /2, ..., ξxdk /2))]= −Elog= −E(cid:8)logP (B(x; (cid:3)k/2))k /2, ..., ξxd(cid:9)μ(B(x; ξx1P (B(x; (cid:3)k/2))μ(B(x; (cid:3)k/2)),k /2))(A.17)where the third equality is from Lemma 1 and the fifth equality is due to thefact that p is supported on Q. Note thatC1 ≤P (B(x; (cid:3)k/2))μ(B(x; (cid:3)k/2))≤ supx∈Qp(x) < ∞,(A.18)and we have(cid:5)(cid:5)(cid:5)(cid:5) log p(x) − logP (B(x; (cid:3)k/2))μ(B(x; (cid:3)k/2))(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)(cid:5)p(x) −1C1P (B(x; (cid:3)k/2))μ(B(x; (cid:3)k/2))(cid:2)(cid:5)(cid:5)(cid:5)(cid:5)≤≤≤1C1μ(B(x; (cid:3)k/2))1C1μ(B(x; (cid:3)k/2))|p(x) − p(x(cid:5))|dx(cid:5)(A.19)B(x;(cid:3)k/2)(cid:2)B(x;(cid:3)k/2)C2||x − x(cid:5)||∞dx(cid:5)≤ C22C1(cid:3)k.33Finally, using Lemma 4, the bias bound of E[ (cid:3)HtKL(X)] can be obtained by(cid:5)(cid:5)E[ (cid:3)HtKL(X)] − H(X)(cid:5)(cid:5)(cid:8)(cid:5)(cid:5)(cid:5)(cid:5) log p(x) − logE≤ Ex∼p≤ C22C1≤ C2Ex∼pE[(cid:3)k](cid:7) 1(cid:6) kd ,NC 1+1/d1P (B(x; (cid:3)k/2))μ(B(x; (cid:3)k/2))(cid:9)(cid:5)(cid:5)(cid:5)(cid:5)(A.20)which completes the proof.630Appendix A.4. Proof of variance bound for the truncated KL estimatordj=1 log ξi,j. We then defineProof. For the sake of convenience, we define αi =α(cid:5)i, i = 1, ..., N as the estimators after x(1) is resampled and α∗i , i = 2, ..., N asthe estimators after x(1) is removed. Then, by the Efron-Stein inequality [38],(cid:10)(cid:11)1N(cid:11)(cid:13)Var[ (cid:3)HtKL(X)] = Var≤EN21N(cid:12)αiN(cid:4)i=1N(cid:4)αi −1Nαi − 1N(cid:14)2(cid:12)N(cid:4)α(cid:5)ii=1N(cid:4)α∗ii=2N(cid:4)(cid:14)2(cid:13)+(cid:14)2(cid:12).α∗i1Nαi − 1Ni=1i=2(cid:11)(cid:13)1N(cid:11)(cid:13)i=1N(cid:4)i=1N(cid:4)≤ N E= 2N E1NN(cid:4)i=1α(cid:5)i− 1NN(cid:4)i=2α∗i(cid:14)2(cid:12)Let 1Ei be the indicator function of the event Ei = {(cid:3)k(x(1)) (cid:7)= (cid:3)∗k(x(1)) is twice thek -NN distance of x(1) when α∗(cid:3)∗N(cid:4)i are used. Then,N(cid:4)N(cid:4)(cid:13)(cid:14)N1Nαi −1Ni=1i=2α∗i= α1 +1Ei (αi − α∗i ).(A.22)(A.21)k(x(1))}, whereBy Cauchy-Schwarz inequality, we have(cid:13)N 21NN(cid:4)i=1αi − 1NN(cid:4)i=2α∗i(cid:14)2(cid:13)≤1 +N(cid:4)i=21Ei(cid:13)i=2(cid:14)(cid:13)α21 +N(cid:4)i=2(cid:14)1Ei (αi − α∗i )2N(cid:4)i=21Ei (αi − α∗i )2(cid:14)(cid:14)N(cid:4)i=21Ei(α2i + α∗2i ),(A.23)≤ (1 + Ck,d)α21 +(cid:13)≤ (1 + Ck,d)α21 + 234where Ck,d is the constant such that x1 is amongst the k-nearest neighbors ofat most Ck,d other samples. Note that αi and α∗i are identically distributed, weonly need to boundE[α21],(N − 1)E[1E2α22],(N − 1)E[1E2α∗22 ].Bound of (A.24a):We separate (A.24a) into two parts,(cid:8)(cid:9)(cid:8)Eα21= Ex∈QEP :(cid:3)k<aNα21(cid:9)+ Ex∈QEP :(cid:3)k≥aN(cid:8)(cid:9),α21(A.24a)(A.24b)(A.24c)(A.25)where aN =(cid:6)(cid:7) 1d .2k log NC1NFirst, we consider the bound of the first term in Eq (A.25). For any x ∈ Q,(cid:6)(cid:8)logξx1k· · ·ξ xdk(cid:7)(cid:9)2dr.(A.26)(cid:9)(cid:8)α21EP :(cid:3)k<aN(cid:2)aNfN,k(r)(cid:14)(cid:13)=0N − 1kwhere fN,k(r) =k· dprdr· pk−1r· (1 − pr)N −k−1 [17]. Note that forsufficiently large N ,(cid:2)aN(cid:6)[log0(cid:2)aN(cid:8)≤log(cid:7)(cid:9)2drξx1k(cid:6) r2· · ·ξ xdk(cid:7)(cid:9)2· · · r20≤C3(log N )3N 1/d,dr(A.27)for some C3 > 0, we now focus on bounding fN,k(r). By basic calculus, we cansee that(cid:13)(cid:14)kN − 1k· pk−1r· (1 − pr)N −k−1 ≤ C4N,(A.28)for some C4 > 0 andp r ∈ (0, 1). Also, by Lemma 6, we have dpr≤ C5drsome C5 > 0 and r < aN . Therefore, the pdf term can be bounded bylog NN forfN,k(r) ≤ C4C5 log N.Combining Eq (A.27) and Eq (A.29), we can bound Eq (A.26) by:(cid:9)(cid:8)α21EP :(cid:3)k<aN≤ C3C4C5(log N )4N 1/d≤ C6,(A.29)(A.30)35for some C6 > 0. Thus, the first term in Eq (A.25) is bounded byEx∈QEP :(cid:3)k<aN(cid:8)(cid:9)α21≤ C6.(A.31)Now we consider the second term in Eq (A.25). For (cid:3)k ≥ aN and sufficientlylarge N , we have(cid:6)(cid:8)logξx1k· · ·ξ xdk(cid:7)(cid:9)2 ≤(cid:8)(cid:6)≤ d2log(cid:8)(cid:3)k/2 · · · (cid:3)k/2(cid:7)(cid:9)2(cid:6) aN2≤ C7(log N )2,log(cid:7)(cid:9)2(A.32)for some C7 > 0. Using Lemma 3 and Eq (A.32), the second term in Eq (A.25)can be bounded byEx∈QEP :(cid:3)k≥aN(cid:9)(cid:8)α21= Ex∈QEP :(cid:3)k≥aN(cid:6)(cid:8)logξx1k· · ·ξ xdk(cid:7)(cid:9)2(cid:11)(cid:12)≤ C7(log N )2 · P ((cid:3)k ≥ aN )(A.33)≤ C8(log N )k+2N 2k,for some C8 > 0.Combining Eq (A.31) and Eq (A.33), the expectation of α21 is bounded byE[α21] ≤ C9,(A.34)for some C9 > 0.635Bound of (A.24b):Since the event E2 is equivalent to the event that x(1) is amongst the k-NNN −1 . Additionally, since E2 isof x(2), E[1E2] = P{x(1) ∈ B(x(2); (cid:3)k(x(2))} = kindependent of (cid:3)k(x(2)), (A.24b) is therefore bounded as(N − 1)E[1E2α22] ≤ (N − 1)E[1E2]E[α22] ≤ kC9,(A.35)where the second inequality is from Eq (A.34).Bound of (A.24c):Using the independence between E2 and (cid:3)∗of x(2) after x(1) is removed), we can bound (A.24c) as2 ] ≤ (N − 1)E[1E2]E[α∗2(N − 1)E[1E2α∗2k(x(2)) (twice thek -NN distance2 ] ≤ kC10,(A.36)for some C10 > 0, where the second inequality is obtained from Eq (A.34) whenthe sample size is reduced to N − 1.Finally we obtain the bound of the variance of (cid:3)HtKL(X)Var[ (cid:3)HtKL(X)] ≤ C111N,(A.37)640for some C11 > 0.36Appendix A.5. Proof of bias bound for the truncated KSG estimatorProof. We separate the d-dimensional unit cube Q into two subsets, Q = Q1 +(cid:7) 1(cid:6)Q2, where Q1 := [ aNd , and Q2 = Q − Q1. Supposethat (cid:15)P , (cid:15)p, and (cid:15)q(cid:3)d , and byLemma 2 and the fact that(x) are defined as in Lemma 2 with l = p(x)− 1dj=1 log ζi,j are identically distributed, we have2 ]d, aN =2 , 1 − aN2k log NC1Nx1k ,...,(cid:3)(cid:10)xdkE[ (cid:3)HtKSG(X)] = −ψ(k) +ψ (N ) + (d − 1)/k +(cid:8)(cid:8)(cid:9)= Ex∼p= Ex∼pEPEP(cid:8)log ζ x1k· · ·ζ xdklog ζ x1k· · ·ζ xdk(cid:9)− Ex∼p− Ex∼pE(cid:2)PE(cid:2)P(cid:8)log (cid:15)q(cid:3)(cid:6)logN(cid:4)(cid:8) d(cid:4)E(cid:9)log ζi,j1Ni=1j=1(cid:9)x1k ,...,(cid:3)xdkp(x)(cid:3)x1k· · ·(cid:3) xdk(A.38)(cid:7)(cid:9).We decompose the bias into three terms and bound them separately:(cid:5)(cid:5)E[ (cid:3)HtKSG(X)] − H(X)(cid:5)(cid:5)(cid:7)(cid:9)(cid:6)(cid:5)(cid:5) Ex∼p· · ·ζ xdkζ x1klog(cid:8)=EP≤I1 + I2 + I3,(cid:5)(cid:5)(cid:6)(cid:8)log(cid:3)x1k· · ·(cid:3) xdk(cid:7)(cid:9)(cid:5)(cid:5)(cid:5)(cid:5)− Ex∼pE(cid:2)P(A.39)withI1 =I2 =I3 =(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q2(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q1(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q(cid:6)(cid:8)logζ x1k· · · ζ xdkEP :(cid:3)k<aNEP :(cid:3)k<aN(cid:8)EP :(cid:3)k≥aN(cid:8)(cid:6)logζ x1k· · · ζ xdk(cid:6)logζ x1k· · ·ζ xdk(cid:7)(cid:9)(cid:5)(cid:5)(cid:5)(cid:5) +(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q2(cid:7)(cid:9)(cid:7)(cid:9)E(cid:2)P :(cid:3)k<aN(cid:8)− Ex∈Q1(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q(cid:5)(cid:5)(cid:5)(cid:5) +E(cid:2)P :(cid:3)k<aN(cid:8)E(cid:2)P :(cid:3)k≥aN(cid:6)(cid:8)log(cid:3)x1k· · ·(cid:3) xdk(cid:7)(cid:9)(cid:5)(cid:5)(cid:5)(cid:5),(cid:6)log(cid:3)x1k· · ·(cid:3) xdk(cid:7)(cid:9)(cid:6)log(cid:3)x1k· · ·(cid:3) xdk(cid:7)(cid:9)(cid:5)(cid:5)(cid:5)(cid:5),(cid:5)(cid:5)(cid:5)(cid:5),(A.40)means taking expectation under the probability measure P overEwhereP :(cid:3)k<aN(cid:3)xjk < aN , j = 1, ..., d.Bound of I1:For any x ∈ Q2,(cid:8)(cid:6)ζ x1k· · · ζ xdk(cid:7)(cid:9)EP :(cid:3)k<aN(cid:2)aNlog(cid:2)aN=· · ·00fN,k(r1, ..., rd) log(cid:6)ζ x1k· · ·ζ xdk(cid:7)dr1 · · ·dr d.(A.41)where fN,k(r1, ..., rd) =(cid:13)(cid:14)N − 1k· ∂d(qkr1,...,rd∂r1···∂rd)· (1 − prm )N −k−1, and rm =37max1≤j≤d(cid:2)aN0(cid:2)aN0(cid:2)aN0≤≤rj [17]. Note that for sufficiently large N , we have,(cid:2)aN0(cid:2)aN0(cid:2)aN· · ·· · ·· · ·0(cid:2)(cid:5)(cid:5) log(cid:5)(cid:5) log(cid:5)(cid:5) logaN(cid:6)ζ x1k· · ·ζ xdk(cid:7)(cid:5)(cid:5)dr1 · · ·dr d(cid:6) r12(cid:6)(cid:7)(cid:5)(cid:5)dr1 · · ·dr d· · · rd2(cid:7)(cid:5)(cid:5)dr1 · · ·dr d +(cid:13) (cid:2)aNr1 · · ·r d· · ·(cid:2)aN0(cid:2)aN0(cid:14)dd log 2dr1 · · · drd= − d(aN )d−1(cid:7)2(cid:6)≤C3log NC1Nlog rdr + d log 2dr00,(A.44)(A.45)(A.42)for some C3 > 0. We now focus on bounding fN,k(r1, ..., rd). We omit thesubscripts of qr1,...,rd for simplicity from now. By the multivariate version ofFa`a di Bruno’s formula [48], one obtains∂d(qk)∂r1 · · ·∂r d=(cid:4)π∈Πd|π|qk(dq)|π|·(cid:20)B∈π∂|B|q(cid:21)j∈B ∂rj,(A.43)where π runs through the set Π of all partitions of the set 1, ..., d. By Lemma 5,we have∂|B|q(cid:21)j∈B ∂rj≤ p(x)rd−|B|m+ C2rd−|B|+1m,which implies that(cid:20)B∈π∂|B|q(cid:21)j∈B ∂rj≤ M r(|π|−1)dm,where M = p∗d + 1 and p∗ = supx∈Qp(x). Therefore, for |π| ≤k and rm ≤ aN we38can bound fN,k(r1, ..., rd) as(cid:13)(cid:4)fN,k(r1, ..., rd) =(cid:14)N − 1k· d|π|qk(dq)|π|·(cid:20)B∈π∂|B|q(cid:21)j∈B ∂rj· (1 − prm )N −k−1(N − 1)!(k − |π|)!(N − k − 1)!qk−|π|(1 − prm )N −k−1M r(|π|−1)dmM · N kpk−|π|rm(1 − prm )N −k−1r(|π|−1)dmCM · N |π|r(|π|−1)dmπ∈Π(cid:4)π∈Π(cid:4)π∈Π(cid:4)π∈Π(cid:4)π∈Π≤≤≤≤CM(cid:13)(cid:13)(cid:14)|π|−1N2k log NC1(cid:14)k−1N,2k log NC1≤|Π|CM(A.46)where the third inequality is due to the fact that pk−|π|(1−p)N −k−1 ≤ CN −k+|π|for p ∈ [0, 1]. Combining Eq (A.46) and Eq (A.42), we can bound the expecta-tion in Eq (A.41) by(cid:5)(cid:5)(cid:5)(cid:5) EP :(cid:3)k<aN(cid:8)(cid:6)logζ x1k· · ·ζ xdk(cid:6)(cid:7)(cid:9)(cid:5)(cid:5)(cid:5)(cid:5) ≤ C4(cid:7)k+1log NC k1(A.47)for some C4 > 0. It follows that the first term of I1 is bounded by(cid:5)(cid:5)(cid:5)(cid:5) Ex∈Q2EP :(cid:3)k<aN(cid:6)(cid:8)logζ x1k· · ·ζ xdk(cid:7)(cid:9)(cid:5)(cid:5)(cid:5)(cid:5) ≤ C4(cid:6)(cid:6)≤ C4Ex∈Q2[1]p∗μ(x ∈ Q2)(cid:7)k+1(cid:7)k+1log NC k1log NC k1(cid:6)(cid:7)k+1log NC k1(cid:6)≤ p∗C4= (d + 1)p∗C4(d + 1)aN(cid:7)k+1log NC k1(cid:6) 2k log NC1N(cid:7) 1d .(A.48)Since (cid:15)P is a sepcial case of P , the second term of I1 can also be bounded by thesame order. Thus, I1 is bounded by(cid:6)|I1| ≤C 5(cid:7)k+2log NC k+11 N,1d(A.49)645for some C5 > 0.Bound of I2:39For any x ∈ Q1 and (cid:3)xjk < aN , j = 1, ..., d, it is easy to see that ζ xjk = (cid:3)xjk .Thus, I2 can be bounded and rewritten asI2 ≤ Ex∈Q1= Ex∈Q1(cid:8)(cid:5)(cid:5)(cid:5)(cid:5) EP :(cid:3)k<aN(cid:5)(cid:2)(cid:5)(cid:5)(cid:5)aN· · ·00(cid:6)log(cid:2)aN(cid:6)(cid:7)(cid:9)(cid:8)(cid:6)ζ x1k· · ·ζ xdk− Elog(cid:3)x1k· · ·(cid:3) xdk(cid:2)P :(cid:3)k<aNfN,k(r1, ..., rd) − (cid:15)fN,k(r1, ..., rd)(cid:7)(cid:9)(cid:5)(cid:5)(cid:5)(cid:5)(cid:6)(cid:7)logr1 · · ·r d(cid:7)dr1 · · ·dr d(cid:5)(cid:5)(cid:5)(cid:5),where (cid:15)fN,k(r1, ..., rd) =the subscripts of (cid:15)qr1,...,rd in the following analysis. Since we haver1,...,rd∂r1···∂rd)∂d((cid:2)qk(cid:13)(cid:14)N − 1k(A.50)· (1 − (cid:15)prm )N −k−1. Again, we omit(cid:2)aN(cid:2)aN· · ·0(cid:6)≤C30(cid:7)2,log NC1N(cid:6)(cid:5)(cid:5) logr1 · · · rd(cid:7)(cid:5)(cid:5)dr1 · · ·dr d(A.51)from (A.42), we now focus on bounding fN,k(r1, ..., rd) − (cid:15)fN,k(r1, ..., rd). Recallthe Fa`a di Bruno’s formula in Eq (A.43), and we have∂|B|q(cid:21)j∈B ∂rj(1 − prm )N −k−1p(x)r1 · · ·r d + O(r1 · · · rdrm)(cid:20)(cid:20)(cid:7)(cid:6)p(x)rj + O(rmrj)1 − p(x)rdm(cid:7)k−|π|(cid:7)N −k−1− O(rd+1m )B∈π(cid:14)j∈ (cid:4)B(cid:6)j∈ (cid:4)B(cid:7)k−|π|(cid:6)1 + O(rm)(cid:7)k−|π| (cid:20)(cid:6)(cid:20)(cid:7)rjp(x)p(x)r1 · · ·r d(cid:7)(cid:6)(cid:7)N −k−1(cid:6)1 + O(rm)(cid:6)p(x)r1 · · ·r d1 − p(x)rdm(cid:20)(cid:7)k−|π| ·(cid:6)p(x)1 − O(rd+1m )(cid:20)(cid:7)rjB∈πj∈ (cid:4)B(cid:7)N −k−1====fN,k(r1, ..., rd)(cid:14)(cid:4)(cid:13)∂|π|qk(∂q)|π|(cid:20)B∈π(cid:6)k!(k − |π|)!(cid:20)(cid:6)N − 1k(cid:14)N − 1k×π∈Π(cid:4)(cid:13)π∈Π(cid:13)(cid:4)π∈ΠN − 1k(cid:13)(cid:4)π∈Π(cid:14)N − 1k(cid:13)(cid:4)=π∈Π(cid:14)N − 1kk!(k − |π|)!(cid:6)×k!(k − |π|)!×∂|π|(cid:15)qk(∂ (cid:15)q)|π|(cid:6)(cid:6)1 − p(x)rdm(cid:7)N −k−1 ·(cid:6)B∈π1 + O(rm)j∈ (cid:4)B(cid:7)k(cid:6)(cid:20)·B∈π∂|B|(cid:15)q(cid:21)j∈B ∂rj(cid:7)k(cid:6)· (1 − (cid:15)prm )N −k−1 ·(cid:7)N −k−11 − O(rd+1m )(cid:6)1 + O(rm)(cid:7)N −k−1(cid:7)k(cid:6)1 − O(rd+1m )(cid:7)N −k−1= (cid:15)fN,k(r1, ..., rd) ·1 +O (rm)1 − O(rd+1m )(A.52)where the second equality is from Lemma 5 and Lemma 6 and the fifth equalityis from the fact that (cid:15)q = p(x)r1 · · ·r d and (cid:15)prm = p(x)rdm for x ∈ Q1 and rm ≤ aN .40By Eq (A.52), we obtain the bound of the difference fN,k(r1, ..., rd)− (cid:15)fN,k(r1, ..., rd)|fN,k(r1, ..., rd) − (cid:15)fN,k(r1, ..., rd)|(cid:5)(cid:5)(cid:6)(cid:5)(cid:5)1 − O(rd+1m )1 + O(rm)(cid:7)k(cid:6)(cid:7)N −k−1 − 1=≤C6rm(cid:13)≤C6(cid:15)fN,k(r1, ..., rd)2k log NC1N(cid:14) 1d|Π|CM(cid:13)2k log NC1(cid:14)k−1N,(cid:5)(cid:5)(cid:5)(cid:5)(cid:15)fN,k(r1, ..., rd)(A.53)for some C6 > 0, where the last inequality is from Eq (A.46) and the fact that(cid:15)P is a special case of P . Combining Eq (A.53) and Eq (A.51), we obtain thebound of I2I2 ≤ C3C6≤ C7(cid:13)2k log NC1N(log N )k+2C k+11 N1d,(cid:14) 1d|Π|CM(cid:13)2k log NC1(cid:14)k−1(cid:6)(cid:7)2log NC1Ex∈Q1[1](A.54)for some C7 > 0, as Ex∈Q1[1] ≤ 1.650Bound of I3:To bound the first term of I3, we need to bound(cid:6)(cid:8)(cid:5)(cid:5) logζ x1k· · ·ζ xdk(cid:9)(cid:7)(cid:5)(cid:5)EP :(cid:3)k≥aNfirst. Note that the event {(cid:3)k ≥ aN } is equivalent to that there is at least onej ∈ {1, ..., d} such that (cid:3)xj≥ aN , and by the symmetry of the equation, thekexpectation over this set can be rewritten as(cid:6)(cid:8)(cid:5)(cid:5) logζ x1k· · · ζ xdkEP :(cid:3)k≥aN(cid:9)(cid:7)(cid:5)(cid:5)=d(cid:4)i=1C idP :Consider each term in Eq (A.55)(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aN(cid:6)(cid:8)(cid:5)(cid:5) logζ x1k· · ·ζ xdk(cid:9)(cid:7)(cid:5)(cid:5).(A.55)(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aN(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aN≤P :P :(cid:6)(cid:8)(cid:5)(cid:5) logζ x1k· · ·ζ xdk(cid:9)(cid:7)(cid:5)(cid:5)(cid:6)(cid:8)(cid:5)(cid:5) logζ x1k· · ·ζ xik(cid:9)(cid:7)(cid:5)(cid:5)+P :(cid:6)(cid:8)(cid:5)(cid:5) logζ xi+1k· · ·ζ xdk(cid:9)(cid:7)(cid:5)(cid:5).(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aNFor (cid:3)xjk≥ aN , j = 1, ..., i and sufficiently large N , we have(cid:6)(cid:5)(cid:5) logζ x1k· · ·ζ xik(cid:7)(cid:5)(cid:5) ≤k /2(cid:6)(cid:5)(cid:5) log(cid:5)(cid:5) log≤k /2 · · · (cid:3)xi(cid:3)x1(cid:5)(cid:6) aN(cid:5)2≤ C8 log N,(cid:7)i(cid:7)(cid:5)(cid:5)(A.56)(A.57)41for some C8 > 0. Using Lemma 3 and Eq (A.57), the first term of Eq (A.56)can be bounded by(cid:8)(cid:5)(cid:5) log≤ C8 log N · P{(cid:3)k,1:i ≥ aN , (cid:3)k,i:d < aN }(cid:7)(cid:5)(cid:5)(cid:19)E(cid:6)(cid:9)ζ x1k· · ·ζ xikP :(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aN≤ C8 log N · P {(cid:3)k ≥ aN }≤ C9(log N )k+1N 2k,(A.58)For some C9 > 0.Now consider the second term of Eq (A.56). Like Eq (A.42), the integrationwith respect to Lebesgue measure can be bounded as(cid:2)1· · ·(cid:2)1(cid:13) (cid:2)aNaNaN≤ −( d − i)(aN )d−i−10(cid:2)aN· · ·(cid:2)0aN(cid:6)(cid:5)(cid:5) logζ xi+1k· · ·ζ xdklog rdr + (d − i) log 2((cid:14)drd · · ·dr i(cid:7)(cid:5)(cid:5)dri+1 · · ·dr d(cid:2)aNdr)d−i≤C10 log N,00(A.59)for some C10 > 0. Again using the multivariate version of Fa`a di Bruno’sformula, we can bound fN,k(r1, ..., rd) for |π| ≤k and rm ≥ aN as(cid:13)(cid:4)fN,k(r1, ..., rd) =(cid:14)N − 1k·d|π|qk(dq)|π|·(cid:20)B∈π∂|B|q(cid:21)j∈B ∂rj· (1 − prm )N −k−1(N − 1)!(k − |π|)!(N − k − 1)!(N − 1)!(k − |π|)!(N − k − 1)!qk−|π|(1 − prm )N −k−1M r(|π|−1)dm(1 − C1adN )N −k−1Mπ∈Π(cid:4)π∈Π(cid:4)π∈Π≤≤≤C111N k ,(A.60)for some C11 > 0. Therefore, combining Eq (A.59) and Eq (A.60) leads to thebound of the second term of Eq (A.56)(cid:6)(cid:8)(cid:5)(cid:5) logζ xi+1k· · ·ζ xdk(cid:9)(cid:7)(cid:5)(cid:5)≤ C10C11log NN k ,(A.61)(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aNP :which is a larger bound then Eq (A.58). As a result we can bound Eq (A.56) by(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aNP :(cid:6)(cid:8)(cid:5)(cid:5) logζ x1k· · ·ζ xdk(cid:9)(cid:7)(cid:5)(cid:5)≤ C10C11log NN k .(A.62)42Given Eq (A.62), we are now able to estimate Eq (A.55) and then the first termof I3 by the same bound up to a constant. Similarly, we can also bound thesecond term of I3 by O. Thus, I3 can be bounded by(cid:6)(cid:7)log NN kI3 ≤ C12log NN k ,(A.63)for some C12 > 0.Finally, combining the upper bounds of I1, I2 and I3, we obtain that thebias is bounded by(cid:5)(cid:5)E[ (cid:3)HtKSG(X)] − H(X)(cid:5)(cid:5) ≤ C13(log N )k+2C k+11 N1d,(A.64)for some C13 > 0.d(cid:10)Appendix A.6. Proof of variance bound for the truncated KSG estimatorj=1 log ζi,j, and define β(cid:5)Proof. We let βi =i, i = 1, ..., N as the estimatorsafter x(1) is resampled and β∗i , i = 2, ..., N as the estimators after x(1) is removed.It should be noted that this proof can be completed by following the roadmapin Appendix A.4, and the only issue that needs to be validated here is thatE[β21 ] =O ((log N )k+2).Again, we separate E(cid:9)(cid:8)(cid:8)(cid:9)β21into two parts,Eβ21= Ex∈Q(cid:9)(cid:8)β21EP :(cid:3)k<aN+ Ex∈QEP :(cid:3)k≥aN(cid:8)(cid:9),β21(A.65)where aN is defined as in Appendix A.5.First, we consider the bound of the first term in Eq (A.65). For any x ∈ Q,EP :(cid:3)k<aN(cid:2)aN(cid:8)β21(cid:2)(cid:9)aNfN,k(r1, ..., rd)(cid:6)(cid:8)logζ x1k· · ·ζ xdk(cid:7)(cid:9)2dr1 · · · drd,(A.66)· · ·=00(cid:14)(cid:13)N − 1k· ∂d(qkr1,...,rd∂r1···∂rd)·(1−prm)N −k−1, and rm = max1≤j≤drj655660Note that for sufficiently large N , we have,aN(cid:8)(cid:6)logζ x1k· · ·ζ xdk(cid:7)(cid:9)2dr1 · · ·dr dwhere fN,k(r1, ..., rd) =[17].(cid:2)aN(cid:2)· · ·· · ·0(cid:2)aN≤0(cid:2)aN(cid:8)log0(cid:2)aN0(cid:2)aN(cid:8)· · ·log=d≤C30(log N )3N0,(cid:6) r12(cid:6) r12(cid:7)(cid:9)2dr1 · · ·dr d· · · rd2(cid:7)(cid:9)2dr1 · · ·dr d + d(d − 1)43(cid:2)aN(cid:2)aN· · ·00log(cid:7)(cid:6) r12log(cid:7)(cid:6) r22dr1 · · · drd(A.67)for some C3 > 0. Recall Eq (A.46), and we can bound Eq (A.66) as:(cid:8)(cid:9)EP :(cid:3)k<aNβ21≤ C4(log N )k+2,for some C4 > 0. Thus, the first term in Eq (A.65) is bounded byEx∈QEP :(cid:3)k<aN(cid:9)(cid:8)β21≤ C4(log N )k+2.Now we consider the second term in Eq (A.65).Like the bound analysis of I3 in Appendix A.5, we can rewriteas(cid:9)(cid:8)β21=EP :(cid:3)k≥aNd(cid:4)i=1C idP :(cid:8)(cid:9).β21(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aN(A.68)(A.69)(cid:9)(cid:8)β21EP :(cid:3)k≥aN(A.70)Consider each term of Eq (A.55)(cid:8)(cid:9)β21(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aNP :(cid:13)≤2P :For (cid:3)xjk(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aN(cid:6)(cid:8)(cid:5)(cid:5) logζ x1k· · ·ζ xik(cid:7)(cid:5)(cid:5)2(cid:9)+P :(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aN(cid:6)(cid:8)(cid:5)(cid:5) logζ xi+1k· · ·ζ xdk(cid:14)(cid:7)(cid:5)(cid:5)2(cid:9)≥ aN , j = 1, ..., i and sufficiently large N , we have(cid:7)(cid:5)(cid:5)2 ≤(cid:6)(cid:6)(cid:5)(cid:5) logζ x1k· · ·ζ xikk /2(cid:5)(cid:5) log(cid:5)(cid:5) log≤k /2 · · · (cid:3)xi(cid:3)x1(cid:5)(cid:6) aN(cid:5)22(cid:7)i(cid:7)(cid:5)(cid:5)2(A.71)(A.72)≤ C5(log N )2,for some C5 > 0. Using Lemma 3 and Eq (A.72), the first term of Eq (A.71)can be bounded by(cid:6)(cid:8)(cid:5)(cid:5) logζ x1k· · · ζ xik(cid:7)(cid:5)(cid:5)2(cid:9)≤ C5(log N )2 · P{(cid:3)k,1:i ≥ aN , (cid:3)k,i:d < aN }(cid:19)P :E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aN≤ C5(log N )2 · P {(cid:3)k ≥ aN }≤ C6,(A.73)for some C6 > 0.Now consider the second term of Eq (A.71). Like Eq (A.67), the integration44with respect to Lebesgue measure is bounded as(cid:2)1· · ·(cid:2)1(cid:13) (cid:2)aN(cid:2)aN· · ·(cid:6)(cid:5)(cid:5) logaN≤C7,aN00ζ xi+1k· · ·ζ xdk(cid:7)(cid:5)(cid:5)2(cid:14)dri+1 · · · drddrd · · ·dr i(A.74)for some C7 > 0. Therefore, combining Eq (A.74) and the PDF bound inEq (A.60) leads to the bound of the second term of Eq (A.71)(cid:6)(cid:8)(cid:5)(cid:5) logζ xi+1k· · ·ζ xdk(cid:7)(cid:5)(cid:5)2(cid:9)≤ C8,(A.75)(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aNP :for some C8 > 0. As a result we can bound Eq (A.71) by(cid:6)(cid:8)(cid:5)(cid:5) logζ x1k· · ·ζ xdk(cid:9)(cid:7)(cid:5)(cid:5)≤ C6 + C8.(A.76)(cid:19)E(cid:3)k,1:i ≥ aN(cid:3)k,i:d < aNP :665Given Eq (A.76), we are now able to estimate Eq (A.70) and then the secondterm of Eq (A.65) by the same bound up to a constant.Finally, the expectation of β21 is bounded asE[β21 ] ≤ C9(log N )k+2,(A.77)for some C9 > 0. Following the same procedure in Appendix A.4, we can obtainthe bound of the variance of (cid:3)HtKSG(X)Var[ (cid:3)HtKSG(X)] ≤ C10(log N )k+2N,(A.78)for some C10 > 0.Appendix B. Proof of Corollary 2Proof. Given a UM f , the density of the original distribution satisfies the changeof variable formula,px(x) = pz(f (x))g(x),(B.1)(cid:5)(cid:5)(cid:5)det ∂f (x)∂x(cid:5)(cid:5)(cid:5) is differentiable and positive for any x ∈ Rd ([35, 49]).where g(x) =Recall that px is differentiable, and it follows that,pz(z) =px(f −1(z))g(f −1(z))),(B.2)is also differentiable for any z ∈ Qo. Thus, the supreme C N2random variable.670is a well defined45Since pSz is a differentiable density function defined on Q, there exists az∗ ∈ Q such that pSz (z∗) = 1. By mean value theorem, we havez (z)|z (ξ) · (z∗ − z)|z (ξ)||1 · ||z∗ − z||∞|1 − pS≤|(cid:2)pS≤||(cid:2)pS≤C N2 ,(B.3)where ξ is some vector in Q. Thus, we have1 − C N2≤ pNx (x) ≤ 1 + C N2 .(B.4)Now define C N1 = infz∈QpSz (z). For N > M , the bias can then be bounded by(cid:5)(cid:5)E[ (cid:3)HUM−tKL(X)] − H(X)(cid:5)(cid:5)≤EUM(cid:8)≤E(C N(cid:5)(cid:5)(cid:5)(cid:5)EX [ (cid:3)HUM−tKL(X)] − H(X)(cid:7) 1C N21 )1+1/d(cid:6) kN(cid:9)(cid:6) kN(cid:7) 1d ,d≤C NUM− tKL(B.5)≤ ¯C, a.s.where C NUM− tKL =1(1− ¯C)1+1/d E[C N2 ]. Note that C N2E[C N2 ] = 0 and therefore limN→∞UM− tKL = 0.P−→N→∞0 andC N2C Nfor any N > M , we have limN→∞The MSE can be bounded byE[( (cid:3)HUM−tKL(X) − H(X))2]≤2E[( (cid:3)HUM−tKL(X) − EX [ (cid:3)HUM−tKL(X)])2] + 2E[(EX [ (cid:3)HUM−tKL(X)] − H(X))2]=2EUMEX [( (cid:3)HUM−tKL(X) − EX [ (cid:3)HUM−tKL(X)])2] + 2EUM[(EX [ (cid:3)HUM−tKL(X)] − H(X))2]Note that when N > M , C Nwe can bound the first term of Eq. (B.6) by1 and C N(B.6)2 satisfy Assumption 3. Then by Theorem 1,2EUMEX [( (cid:3)HUM−tKL(X) − EX [ (cid:3)HUM−tKL(X)])2] ≤ C11N,(B.7)for some C1 > 0. The second term of Eq. (B.6) can be bounded by2EUM[(EX [ (cid:3)HUM−tKL(X)] − H(X))2](cid:9)(cid:6) kN(C N(cid:7) 2d(cid:8)≤2E2 )2(C N1 )2(1+1/d)(cid:7) 2(cid:6) kdN≤DNUM− tKL46(B.8)where DNfor any N > M . Thus, the MSE is bounded by(1− ¯C)2(1+1/d) E[(C NUM− tKL =22 )2]. Again, we have, limN→∞DNUM− tKL = 0E[( (cid:3)HUM−tKL(X) − H(X))2] ≤ C11N+ DNUM− tKL(cid:7) 2d .(cid:6) kN(B.9)Appendix C. Proof of Corollary 3Proof. For N > M , the bias can be bounded by(cid:6)(cid:5)(cid:5)E[ (cid:3)HUM−tKSG(X)] − H(X)(¯pSz )d + 1C k+11(cid:9) (log N )k+2N(cid:8) ¯pSz(cid:5)(cid:5)(cid:7)1d≤CE≤CUM− tKSG(log N )k+2N1d,(C.1)(cid:6)(cid:7)(1+ ¯C)d+1(1+ ¯C)where C is a positive constant, ¯pS(1− ¯C)k+1Similarly as the proof of Corollary 2 and by Theorem 2, we can bound the MSEbypSz (z) and CUM− tKSG = Cz = supz∈Q.E[( (cid:3)HUM−tKSG(X) − H(X))2] ≤ C2(log N )k+2Nwhere C2 is a positive constant and DNUM− tKSG =(cid:16)C+ DNUM− tKSG(log N )2(k+2)N(cid:7)(1+ ¯C)d+1(cid:17)22d,(C.2).(1− ¯C)k+1(cid:6)(1+ ¯C)Appendix D. Further details of the numerical examples675Appendix D.1. Implementation details of the estimators680The setup of MAF: We use a MAF built by 10 autoregressive layers [50]for Hybrid Rosenbrock distribution and one built by 5 autoregressive layersfor Even Rosenbrock distribution and the application of experimental design.Each layer has two hidden layers of 50 units and tanh nonlinearities. In eachexperiment, half of the samples are used to train the MAF model and the otherhalf are used to estimate the entropy.The implementation of CADEE and non-Mises estimator: The twoestimators are implemented using the code provided by [21] and [22] with thedefault parameters.47685Appendix D.2. The two multivariate Rosenbrock distributionsHybrid Rosenbrock Distribution. The density of the hybrid Rosenbrockdistribution is given byπ(x) ∝ exp⎧⎨⎩−a(x1 − μ)2 −n2(cid:4)n1(cid:4)j=1i=2bj,i(xj,i − x2j,i−1)2⎫⎬⎭ ,(D.1)where the dimensionality of x is d = (n1 − 1)n2 + 1. The variable xj,1 = x1 forj = 1, ..., n2. The normalization constant of Eq. (D.1) is(cid:21)√a(cid:28)bj,i.n1,n2i=2,j=1πd/2(D.2)In this experiment, we set μ = 1.0, a = 1.0, bj,i = 0.1 for all i and j, n1 = 4and n2 ranging from 1 to 7. This setting forms a class of distributions withdimensions ranging from 4 to 22.Even Rosenbrock Distribution. The density of the even Rosenbrockdistribution is given byπ(x) ∝ exp⎧⎨⎩−d/2(cid:4)(cid:29)i=1(x2i−1 − μ2i−1)2 − ci(cid:6)x2i − x22i−1(cid:30)(cid:7)2⎫⎬⎭ ,(D.3)690695where the dimensionality d must be an even number. The normalization con-stant for Eq. (D.3) isd/2i=1πd/2In this experiment, we set μ2i−1 = 0, ci = 12.5 fori = 1, ..., d/2 withd rangingfrom 2 to 22. This setting forms a class of distributions with dimensions rangingfrom 2 to 22.(D.4)ci.(cid:21)√Hybrid Rosenbrock Distribution with Discontinuous Density. Thedensity of the hybrid Rosenbrock distribution with discontinuous density is givenbyπ(x) = unifpdf(x1, μ,(cid:31)18a) ×n2(cid:20)n1(cid:20)j=1i=2unifpdf(xj,i, x2j,i−1,(cid:31)18b)(D.5)where unifpdf(x, α, β) is the pdf of the continuous uniform distribution on theinterval [α−β, α+β], evaluated at the values in x, and where the dimensionalityof x is d = (n1 − 1)n2 + 1. The variable xj,1 = x1 for j = 1, ..., n2.In this experiment, we set μ = 1.0, a = 1.0, bj,i = 0.1 for all i and j, n1 = 4and n2 ranging from 1 to 7. This setting forms a class of distributions withdimensions ranging from 4 to 22.Even Rosenbrock Distribution with Discontinuous Density. Thedensity of the even Rosenbrock distribution with discontinuous density is given48byπ(x) =d/2(cid:20)(cid:8)i=1unifpdf(x2i−1, μ2i−1, 0.5) × unifpdf(x2i, x22i−1, ci)(cid:9),(D.6)where the dimensionality d must be an even number.700In this experiment, we set μ2i−1 = 0, ci = 0.025 for i = 1, ..., d/2 withdranging from 2 to 22. This setting forms a class of distributions with dimensionsranging from 2 to 22.Appendix D.3. Entropy estimator only using NFIn this section we describe a simplified version of the proposed method, whichestimate the entropy only using NF (without the truncated entropy estimators).To start with, we recall Eq. (12) in the main paper,(cid:2)H(X) =H (Z) +pz(z) log(cid:5)(cid:5)(cid:5)(cid:5) det(cid:5)(cid:5)(cid:5)(cid:5)dz.∂f −1(z)∂z(D.7)The main idea of this simplified method is to assume that the transformedrandom variable Z exactly follows a uniform distribution and as a result H(Z) =0. Therefore the entropy of X is estimated as,ˆHN F (X) =(cid:5)(cid:5)(cid:5)(cid:5) detlog∂f −1(z(i))∂z(cid:5)(cid:5)(cid:5)(cid:5),1nn(cid:4)i=1(D.8)705where z(i) = f (x(i)). A limitation of this method is quite obvious – the trans-formed random variable Z is usually not uniformly distributed and simply takingits entropy to be zero will undoubtedly introduce bias, which is demonstratedby the numerical examples in the main paper.It should also be noted that,while not in the context of entropy estimation, a NF based approach has beenused for maximum entropy modeling [51].Appendix D.4. The Beta scheme for parametrizing the observation timesIn the optimal experimental design (OED) example, we use a lower dimen-sional parameterization scheme to reduce the dimensionality of the optimiza-tion problem [46]. In particular we use the Beta scheme [46] to allocate theplacements of the observation times. Specifically, let Q(·, α, β) be the quantilefunction of the beta distribution with shape parameters α and β, and the dobservation times λ = (t1, ..., td) in the time interval [0, T ] are allocated as,ti = T · Q(id + 1, α, β),i = 1, ..., d.(D.9)710As such the d-dimensional variable λ is parametrized by α > 0 and β > 0.49Appendix D.5. Nested Monte CarloHere we describe the Nested Monte Carlo (NMC) approach that is usedto estimate the entropy in the experimental design example. Recall that theentropy of interest is H(Y ) (here for simplicity we omit the design parameterλ):(cid:2)H(Y ) =log p(y)p(y)dy,(D.10)which can be estimated via Monte Carlo (MC):H(Y ) ≈ − 1MM(cid:4)i=1log p(y(i)),(D.11)where y(i) are drawn from p(y). A difficulty here is that we do not have anexplicit expression of p(y). Note however that in this example the likelihoodp(y|θ) and the prior p(θ) are available and we can therefore write(cid:2)p(y) =p(y|θ)p(θ)dθ.(D.12)It follows that p(y) can also be estimated via MC:p(y(i)) ≈1NN(cid:4)j=1p(y(i)|θ(j)),(D.13)where θ(j) are drawn from p(θ). Combining Eq. (D.13) and Eq. (D.11), weobtain an estimator of H(Y ), which is referred to as the NMC method [47]. Inparticular, Eq. (D.13) is usually referred to as the inner MC and Eq. (D.11) isreferred to as the outer one. Since the theoretical results in [47, 52] show thatthe mean squared error of NMC estimator decays at a rate of O( 1N ), wecan obtain an accurate evaluation of H(Y ) with a sufficiently large number ofsamples, and in the numerical example we use M = N = 1 × 105. We emphasizethat such a large number of samples is not computationally feasible to use inthe experimental design procedure, and thus in the example we have to resortto other entropy estimation methods.M + 171572050(cid:24)(cid:286)(cid:272)(cid:367)(cid:258)(cid:396)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:381)(cid:296)(cid:3)(cid:349)(cid:374)(cid:410)(cid:286)(cid:396)(cid:286)(cid:400)(cid:410)(cid:400)(cid:3)(cid:3)(cid:3)(cid:1409)(cid:3)(cid:100)(cid:346)(cid:286)(cid:3)(cid:258)(cid:437)(cid:410)(cid:346)(cid:381)(cid:396)(cid:400)(cid:3)(cid:282)(cid:286)(cid:272)(cid:367)(cid:258)(cid:396)(cid:286)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:410)(cid:346)(cid:286)(cid:455)(cid:3)(cid:346)(cid:258)(cid:448)(cid:286)(cid:3)(cid:374)(cid:381)(cid:3)(cid:364)(cid:374)(cid:381)(cid:449)(cid:374)(cid:3)(cid:272)(cid:381)(cid:373)(cid:393)(cid:286)(cid:410)(cid:349)(cid:374)(cid:336)(cid:3)(cid:296)(cid:349)(cid:374)(cid:258)(cid:374)(cid:272)(cid:349)(cid:258)(cid:367)(cid:3)(cid:349)(cid:374)(cid:410)(cid:286)(cid:396)(cid:286)(cid:400)(cid:410)(cid:400)(cid:3)(cid:381)(cid:396)(cid:3)(cid:393)(cid:286)(cid:396)(cid:400)(cid:381)(cid:374)(cid:258)(cid:367)(cid:3)(cid:396)(cid:286)(cid:367)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:400)(cid:346)(cid:349)(cid:393)(cid:400)(cid:3)(cid:410)(cid:346)(cid:258)(cid:410)(cid:3)(cid:272)(cid:381)(cid:437)(cid:367)(cid:282)(cid:3)(cid:346)(cid:258)(cid:448)(cid:286)(cid:3)(cid:258)(cid:393)(cid:393)(cid:286)(cid:258)(cid:396)(cid:286)(cid:282)(cid:3)(cid:410)(cid:381)(cid:3)(cid:349)(cid:374)(cid:296)(cid:367)(cid:437)(cid:286)(cid:374)(cid:272)(cid:286)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:449)(cid:381)(cid:396)(cid:364)(cid:3)(cid:396)(cid:286)(cid:393)(cid:381)(cid:396)(cid:410)(cid:286)(cid:282)(cid:3)(cid:349)(cid:374)(cid:3)(cid:410)(cid:346)(cid:349)(cid:400)(cid:3)(cid:393)(cid:258)(cid:393)(cid:286)(cid:396)(cid:856)(cid:3)(cid:3)(cid:3)(cid:1407)(cid:3)(cid:100)(cid:346)(cid:286)(cid:3)(cid:258)(cid:437)(cid:410)(cid:346)(cid:381)(cid:396)(cid:400)(cid:3)(cid:282)(cid:286)(cid:272)(cid:367)(cid:258)(cid:396)(cid:286)(cid:3)(cid:410)(cid:346)(cid:286)(cid:3)(cid:296)(cid:381)(cid:367)(cid:367)(cid:381)(cid:449)(cid:349)(cid:374)(cid:336)(cid:3)(cid:296)(cid:349)(cid:374)(cid:258)(cid:374)(cid:272)(cid:349)(cid:258)(cid:367)(cid:3)(cid:349)(cid:374)(cid:410)(cid:286)(cid:396)(cid:286)(cid:400)(cid:410)(cid:400)(cid:876)(cid:393)(cid:286)(cid:396)(cid:400)(cid:381)(cid:374)(cid:258)(cid:367)(cid:3)(cid:396)(cid:286)(cid:367)(cid:258)(cid:410)(cid:349)(cid:381)(cid:374)(cid:400)(cid:346)(cid:349)(cid:393)(cid:400)(cid:3)(cid:449)(cid:346)(cid:349)(cid:272)(cid:346)(cid:3)(cid:373)(cid:258)(cid:455)(cid:3)(cid:271)(cid:286)(cid:3)(cid:272)(cid:381)(cid:374)(cid:400)(cid:349)(cid:282)(cid:286)(cid:396)(cid:286)(cid:282)(cid:3)(cid:258)(cid:400)(cid:3)(cid:393)(cid:381)(cid:410)(cid:286)(cid:374)(cid:410)(cid:349)(cid:258)(cid:367)(cid:3)(cid:272)(cid:381)(cid:373)(cid:393)(cid:286)(cid:410)(cid:349)(cid:374)(cid:336)(cid:3)(cid:349)(cid:374)(cid:410)(cid:286)(cid:396)(cid:286)(cid:400)(cid:410)(cid:400)(cid:855)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)