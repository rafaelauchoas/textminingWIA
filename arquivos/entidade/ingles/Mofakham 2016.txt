Artificial Intelligence 237 (2016) 59–91Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintPOPPONENT: Highly accurate, individually and socially efficient opponent preference model in bilateral multi issue negotiationsFarhad Zafari a,b, Faria Nassiri-Mofakham a,∗a Department of Information Technology Engineering, Faculty of Computer Engineering, University of Isfahan, 81746-73441, Hezar Jerib Avenue, Isfahan, Iranb Faculty of Science, Engineering and Technology, Swinburne University of Technology, Melbourne, VIC 3122, Australiaa r t i c l e i n f oa b s t r a c tArticle history:Received 4 December 2014Received in revised form 27 March 2016Accepted 1 April 2016Available online 8 April 2016Keywords:Bilateral multi issue negotiationOpponent modelingBidding strategyAcceptance strategyPerceptronMulti bipartite gradient descentIn automated bilateral multi issue negotiations, two intelligent automated agents negotiate on behalf of their owners regarding many issues in order to reach an agreement. Modeling the opponent can excessively boost the performance of the agents and increase the quality of the negotiation outcome. State of the art models accomplish this by considering some assumptions about the opponent which restricts the applicability of the models in real scenarios. In this study, a less restricted technique where perceptron units (POPPONENT) are applied in modeling the preferences of the opponent is proposed. This model adopts the Multi Bipartite version of the Standard Gradient Descent search algorithm (MBGD) to find the best hypothesis, which is the best preference profile. In order to evaluate the accuracy and performance of this proposed opponent model, it is compared with the state of the art models available in the Genius repository. This results in the devised setting which approves the higher accuracy of POPPONENT compared to the most accurate state of the art model. Evaluating the model in the real world negotiation scenarios in the Genius framework also confirms its high accuracy in relation to the state of the art models in estimating the utility of offers. The findings here indicate that this proposed model is individually and socially efficient. This proposed MBGD method could also be adopted in other practical areas of Artificial Intelligence.© 2016 Elsevier B.V. All rights reserved.1. IntroductionNegotiation is the science and art of resolving any kind of disputes and reaching consensus among human parties. In an automated bilateral multi-issue version of negotiations, intelligent computer agents engage in a cooperative process on behalf of their beneficiaries with different and sometimes contradicting interests, with the objective of achieving an agree-ment on one or more issues. Recently, with the emergence of ANAC (an annual international Automated Negotiating Agents Competition) [2,3], many new negotiation strategies have been developed. Most of the existing sophisticated negotiation strategies typically consist of a set of fixed modules. In general, as observed in Fig. 1, three main components are distin-* Corresponding author. Tel.: +98 (0)31 3793 4510; fax: +98 (0)31 3668 2887.E-mail addresses: f.zafari@eng.ui.ac.ir, fzafari@swin.edu.au, f_z_uut@yahoo.com (F. Zafari), fnasiri@eng.ui.ac.ir, fnasirimofakham@yahoo.com(F. Nassiri-Mofakham).URL: http://eng.ui.ac.ir/~fnasiri/ (F. Nassiri-Mofakham).http://dx.doi.org/10.1016/j.artint.2016.04.0010004-3702/© 2016 Elsevier B.V. All rights reserved.60F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91Fig. 1. The negotiation flow (adapted from the BOA framework [1]).guished in a negotiating agent which work together within a BOA framework to accomplish the whole negotiation task in a collaborative manner [1]:1. Bidding Strategy: this module may interact with the opponent model component by sending one or more candidate offers to the opponent model, and receive the estimated utility of those offers in the utility space of the opponent. Next, the bidding strategy component decides on one of those offers, as the selected candidate offer, to be sent to the opponent as the next proposal.2. Opponent Model: this module constructs a model of the preference profile of the opponent through a learning technique. This model takes a number of offers and generates their estimated utilities.3. Acceptance Strategy: this component receives the incoming offer from the opponent and the outgoing offer chosen by the bidding strategy component, and then determines whether the incoming offer is acceptable for the agent. If the received offer from the opponent is good enough to be accepted, then an accept message is provided and sent to the opponent in response. Otherwise, the outgoing offer, previously chosen by the bidding strategy component, is forwarded to the opponent as the response.1According to the BOA framework, a negotiation strategy functions as follows: as soon as the agent receives a new offer2from the opponent, the bidding history and the opponent model are immediately updated (steps 1 and 2). This process assures that the agent has the most updated information regarding its opponent. Then in turn, the bidding strategy module generates a number of candidate bids with similar utilities for the agent and sends them to the opponent model. The oppo-nent model then calculates the estimated utilities of the received bids and renders them to the bidding strategy component in response (steps 3 and 4). Next, the bidding strategy component chooses one of the candidate bids according to an oppo-nent model strategy (for example, the best bid is chosen) and forwards this bid to the acceptance strategy module (step 5). Finally, the acceptance strategy decides on whether to accept this newly received bid from the opponent (in step 1) or to send the bid recently received from the bidding strategy component (in step 5).Identifying offers that are mutually beneficial, avoiding non-agreement offers, and earlier agreements are the benefits of applying an opponent modeling [7]. Despite the variety in opponent modeling techniques, most of the current models rely on a small and common set of learning techniques [7]. It is believed that this is due to the restricting nature of the negotiation problem. The core of the opponent modeling is a learning technique. Moreover, traditional learning techniques are mainly comprised of two non-overlapping learning and classification or prediction phases. Since in a single negotiation session all bids (i.e., training examples) are not available at the same time in advance, traditional learning techniques are not easily applied. An opponent model that is able to learn incrementally and update itself once the new training examples (i.e., offers) arrive during a negotiation session is of necessity. These learning techniques are collectively referred to as adaptive models [8], where both of the aforementioned phases are performed in parallel. Another problem in the negotiation setting is that the training instances lack the output variable (i.e., the variable which contains the utility values of the received bids in the opponent’s view). That is, since the agent is not aware of the preference profile of the opponent, it cannot calculate the values of that output variable. Therefore, specific opponent models are required that are capable of modeling the preferences of the opponent with no need for the value of the output variable. Strictly speaking, the model must either use an unsupervised learning technique or somehow pre-estimate the utility values of the received bids from the opponent 1 An elaborated analysis of the state of the art acceptance strategies and their performance as well as optimal acceptance mechanisms has been carried out by Baarslag et al. [4,5] and Baarslag and Hindriks [6], respectively.2 The terms “offer” and “bid” are used interchangeably throughout this article.F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9161and use the estimated values as the output label for the training examples. In order to overcome this limitation, all the existing opponent models use a subset of the following 13 assumptions to extract the preferences of the opponent [9]:Assumption 1. The bidding strategy of the opponent follows a concession based pattern. That is, the bidding strategy of the agent generates a sequence of bids in the order of the agent’s preference over those bids. This assumption reduces the opponent modeling problem to the problem of estimating the utility of all the bids in the outcome space, so that the estimated utilities are as close to the real utilities as possible.Assumption 2. The first bid sent by the opponent has the highest possible utility in the opponent’s utility space. That is, the opponent sends its most preferable bid at the beginning of the negotiation session. The first bid embodies the best possible values for each of the negotiable issues. As all rational agents try to maximize their own utilities, it is not surprising that this assumption holds true for most of the rational agents.Assumption 3. There is an inverse relation between the importance of an issue and the number of times its value changes throughout the negotiation session. In other words, agents rarely change the value of issues of great importance; the more important the issue, the lower the tendency of the agents to change its value [10,11]. Obviously, the validity of this assumption depends on the agent’s bidding strategy and domain.Assumption 4. There is a direct relation between the importance of an issue value and the number of times it is offered. Here, the agents seek to offer an issue value of greater importance (greater evaluation value) in the successive offers they send in a negotiation session.Here, too, the validity of this assumption depends on the agent’s bidding strategy and domain.Assumption 5. The evaluation functions of the negotiation issues are defined by a number of functions with pre-defined shapes (e.g., as in the model proposed by Hinkriks and Tykhonov [12]). Clearly, the objective of applying this assumption is to limit the number of hypotheses in the opponent’s hypothesis space.Assumption 6. The negotiation issue weight values are simply calculated according to their rankings among all negotiation issue weights (e.g., as in the model proposed by Hinkriks and Tykhonov [12]). Obviously, like Assumption 5, the objective here is to reduce the opponent’s hypothesis space size.Assumption 7. The utility values of the bids offered by the opponent throughout a negotiation session are distributed around a constant and specific number (such as 1). Since the rational agents’ objective is to maximize their utilities, this assumption is assumed to be valid for rational agents.Assumption 8. All negotiation issues have equal importance for the opponent. In other words, all the negotiation issues are of equal weight values [13]. Obviously, the purpose of making this assumption is to limit the size of the opponent’s hypothesis space.Assumption 9. The negotiation issues are conflicting between the two agents. Here, the evaluation of a negotiation issue value for the opponent is given by 1 minus the evaluation of that negotiation issue value for the agent (e.g., the models proposed by Jazayeriy et al. [14] and Zhang et al. [15]). Obviously, the objective of this assumption is to limit the space size of the opponent’s hypothesis.Assumption 10. The utility values of the offers received from the opponent are known for the agent (e.g., the models proposed by Hinkriks and Tykhonov [12] and Williams et al. [16]).Assumption 11. The utility function of the opponent is completely known for the agent. In other words, the agent has perfect information regarding the preference profile of the opponent.Assumption 12. The utility value of an offer received from the opponent equals one minus the utility of that offer for the agent. In other words, the preference profile of the opponent is the opposite of the agent’s preference profile.Assumption 13. The utility value of an offer for the opponent equals the utility of that offer for the agent. In other words, the preference profile of the opponent is the same as that of the agent.Another difficulty with modeling an opponent’s preferences in bilateral negotiations is related to the time factor. In discounted negotiations [2,3,17], where the utility of an offer is decreased as the negotiation time passes, reaching an agreement as early as possible is of extreme importance. Since constructing an opponent model is costly in the computa-tional sense, the agent should make a trade-off between “non-application of an opponent model, hence saving time” and 62F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91“applying one and increasing its utility gained by using the opponent model”. In other words, by non-application the agent would save more time to better explore the outcome space, by looking for an agreement which would result in a gain in his utility. On the other hand, applying an opponent model would assist the agent to make more appropriate agreements. This trade-off is referred to as the time/exploration trade-off [9]. The post event analysis of ANAC tournaments [2,3,17] also confirms that the computational complexity of the opponent models and the poor accuracy are the two main factors that degrade the performance of the agents applying these models [9]. In particular, the time factor is of paramount importance in online opponent models. In these models, the participating agents usually exchange a limited number of offers before the negotiation deadline is met, therefore, they do not contain enough information to accurately train an opponent model [12,18]. Consequently, the ability of the model to extract the most information possible from the training bids it receives is highly essential. Due to the time restriction and the limited number of offers the agents exchange in their negotiations, a proper opponent model is one with the following features:1. Functionality based on the least assumptions, which would make it more robust against the opponents not following these assumptions.2. Having the lowest possible computational cost (time/exploration trade-off).3. Extracting the most information from the least bids (especially important in online opponent models).4. Incremental training capability.To learn the issue weight values and individual utility function, a two layered architecture would be essential. Thus, in order to overcome the aforementioned difficulties and to justify the features outlined above, a new model based on perceptron units is proposed here for an agent with incomplete information, in order to model the preference profile of the opponent in bilateral multi issue negotiations. Moreover, to be more applicable in real world negotiations, fewer and more realistic assumptions than that of the state of the art models are applied in this study. This is obtained by proposing an opponent model named POPPONENT, based on an adapted version of the standard gradient decent search (named the Multi Bipartite Gradient Decent Search), which applies fewer assumptions. The model shows success in the practical AI area of modeling the user’s preferences in bilateral multi issue negotiations with incomplete information.The remainder of this paper is organized as follows: Section 2 contains the literature review; in Section 3, the general negotiation setting is described; in Section 4, the mathematical concepts underlying perceptron based learning, and the proposed opponent model based on perceptron units are introduced; in Section 5, the experimental setup used in evaluating the proposed model, the carried-out examination, is discussed and the accuracy of this is compared proposed model with some of both the classic and the state of the art opponent models. Next, the efficiency of this model in practice (with real examples) is presented. Finally, in Section 6, the paper is concluded.2. Literature reviewIn this section, the opponent models from the general and technical perspectives are reviewed in Sections 2.1 and 2.2, respectively. State of the art opponent models currently available in the Genius3 Repository will be discussed in Section 2.3.2.1. General classification of opponent modelsThere are many aspects related to the opponent which could be learnt in a negotiation session. Actually, an opponent considers several attributes, based on which they try to learn the existing opponent modeling techniques. These are gener-ally categorized into the following [7]:Category 1. Models that try to estimate the reservation value of the opponent [19,20].Category 2. Models that try to estimate the deadline of the opponent [19,20].Category 3. Models that try to estimate the order of the opponent’s preferences over different negotiation issues (the exact value or at least the order of issue weights) [14,15,21,22].Category 4. Models that try to estimate the order of the opponent’s preferences over different negotiation outcomes (the exact value or at least the order of the utilities of the offers) [12,16,23–26].Category 5. Models that try to learn the opponent’s bidding strategy [20,27–29].Category 6. Models that try to learn the opponent’s acceptance strategy [30,31].The function of the models in the first category is based on the assumption that the agents usually stop conceding close to their reservation values (i.e., the minimum acceptable utility for the agent) and that they exhibit this behavior when approaching negotiation deadline. This means that in the beginning of the negotiation, the agents will offer bids with F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9163far greater utilities than their reservation values [7]. For example, in the model proposed by Gwak and Sim [19] where a simple single issue negotiation is only based on price, the Bayesian method is adopted to estimate the reservation price of the opponent. They assume reservation prices and negotiation deadlines of the agents to be private knowledge. In their model, a number of hypotheses (in the form of r v = v i ) are considered as possible values (v i ) for the reservation price (r v) of the opponent. Next, the Bayesian learning process is applied to update the probability values of the hypotheses in the hypothesis space3 using the offers received from the opponent during the negotiation session. Then, the weighted average of all hypotheses is calculated and considered as the estimated reservation value of the opponent. In this model, the bidding strategy of the opponent is assumed to follow some kind of a concession function. Using the opponent’s estimated reservation price (reservation value), the buyer/seller bidding prices are generated in ascending/descending order. Assuming that the opponent uses a pre-known bidding strategy, it is illustrated that the agent could easily compute the opponent’s reservation price given its deadline, and vice versa. Given these two value estimations, the agent would then be able to determine the optimal bidding strategy. In their model, the opponent is assumed to have discrete reservation prices, among which each one is a fraction of the difference between the maximum and minimum possible prices considered in that negotiation. This model cannot be adopted in the real world, because real world negotiations are more complex, and often contain more issues than just one price issue. Furthermore, the agents in more complex domains may exhibit more complicated behavior than just simply following a typical concession strategy; accordingly, we need to design a model to operate in more complex and realistic scenarios. As another example, in the model proposed by Yu et al. [20] (similar to the model by Gwak and Sim [19]) the opponent model is used for learning the reservation value and deadline of the opponent (which is assumed to be private information) in a single issue bilateral negotiation. In their article, a model is proposed based on Bayesian learning and regression analysis. This shows how the method is adopted in adapting the agent’s strategy to the opponent. Similar to the previous model, this model assumes that the opponent follows a pre-known concession based strategy with unknown parameters; that is, this model is not applicable in realistic scenarios where multiple issues are subject to negotiation and the opponent follows more complex bidding strategies.The second category is run on models that try to estimate the deadline of the opponent. Obviously these models are specifically adopted in scenarios where the opponent has a private deadline. However, in the famous settings like ANAC, it is assumed that the negotiating parties have a common deadline to reach an agreement. For example, both the models proposed by Gwak and Sim [19] and Yu et al. [20] are of this category.The third category includes the models that try to learn the order of the opponent’s preferences over different negotia-tion issues. In other words, they try to estimate the weights of the issues according to the opponent’s view. Faratin et al. [21] believe that obtaining information about the preference order of the negotiation issues from the opponent’s perspec-tive would be sufficient to improve the outcome in a negotiation session. Therefore, the problem of learning the opponent’s utility function is simply reduced to learning the order of issue weights in the opponent’s context. In these models, each possible permutation of the issue weight order is considered as a hypothesis in the hypothesis space, and then the Bayesian formula is applied to update the probability of each hypothesis once a new bid is received from the opponent. For example, the models devised by Niemann and Lang [22], Jazayeriy et al. [14], and Zhang et al. [15] fall into this category. These mod-els also assume restricted types of bidding behaviors for the opponent. For example, the model proposed by Jazayeriy et al. assumes that the opponent follows one of the three following concession strategies: (1) Boulware, (2) Fixed Concession, and (3) Conceder. Moreover, they assume that the agent knows which one of the three aforementioned strategies its opponent follows. Here it is assumed that all of the negotiation issues are monotonic and conflicting, meaning that increasing the util-ity value of the agent for an issue would cause the opponent’s utility for that issue to decline. Obviously, these assumptions do not generally hold true in the real world of negotiations. Contrary to the model presented by Jazayeriy et al. [14], which assumes all the issues are conflicting, in the model by Gwak and Sim [19], it is rather assumed that the agents announce their preference direction as well as the acceptable value ranges for each issue before starting the negotiation session. In other words, the agents’ preference directions are considered as common knowledge information in the negotiation. For example, the buyer agent announces that the range of its acceptable prices is from $100 to $200. It also declares that there is an inverse relation between the price and its utility. In other words, higher price values will have lower utility values, and vice versa; consequently, both negotiating parties would know the conflicting issues, and would only negotiate on these issues. That is, the agents would be able to easily pick the values with maximum utilities for non-conflicting issues. Thus, there would be no need to negotiate on the values of such issues. It is also assumed that there is an opposite correlation between the weight of an issue and the number of times its value is changed. In the model proposed by Zhang et al. [15], the agent analyzes the history containing the offers received from the opponent and applies Bayesian Learning to learn the opponent’s preferences over negotiation issues. The learnt orders of the issue weights are then incorporated into a counter-offer proposition algorithm to enable the agent to propose the offers which are mutually beneficial for both the agent and the opponent. Similar to the model proposed by Jazayeriy et al. [14], in this model it is also assumed that all issues are conflicting. With this assumption, the hypothesis space of possible utility functions will be limited to the possible orders of negotiation issue weights. It is also assumed that the opponent follows a time-dependent concession strategy [33] with known parameters. The counter-offer proposition mechanism used by the agent compensates for the issues of high impor-tance for the opponent. By using this trade-off mechanism, the agent tries to find bids with greater utilities than both the 3 The term “hypothesis space” is used by Anderson et al. in their Machine Learning book [32] in reference to the space containing all the hypotheses.64F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91agent and the opponent’s target utilities. In other words, the agent will try to propose offers that are mutually acceptable for both the agent and the opponent. It is obvious that these assumptions do not necessarily hold true in the real world, so when the opponent does not follow these assumptions, these models are prone to failure.The models in the fourth category estimate the preference order of the existing bids from the outcome space in the opponent’s view. In fact, the models in this category try to estimate the utility function of the opponent and then use this function to calculate the utility of a given bid in the opponent’s utility space. Some of the models (e.g. [12,16,23–26]) that are based on the Bayesian formula [34] are classified in this category. These models consider the hypothesis space as the preference order of the negotiation issues as well as evaluation values of each negotiation issue. They update the probability of each hypothesis using the Bayesian formula in a consecutive manner as each offer is received from the opponent through the negotiation session. For example, the model proposed by Hindriks et al. [12] that in fact is the basic model underlying some other Bayesian models (e.g. [16,24,26]) estimates the utility function of the opponent (which includes the estimated values for the weights of negotiation issues and estimated evaluation values for each negotiation issue value) through a Bayesian formula. This model is extremely similar to the model proposed by Zhang et al. [15] but it is different in two ways: first, the hypothesis space contains both the evaluation functions and orders of issue weights. However, in the model proposed by Zhang et al. [15] the hypothesis space is reduced by assuming that all negotiation issues are conflicting, so only the possible ranking of issue weights is assumed as the hypothesis space. Second, in the model proposed by Hindriks et al. [12], a normal distribution is used to calculate the probability of observing a bid given the condition that a hypothesis holds (or P (D|h)), but in the model proposed by Zhang et al. [15], upon receipt of a new offer, the best hypothesis which most accurately estimates the utility of this new offer is calculated, and the resultant value is used to estimate the value of P (D|h) for each of the hypotheses.The fifth category comprises the models that try to learn the bidding strategy of the opponent. The bidding strategy is defined as a specific function which maps a negotiation state to a bid [7]. In other words, a bidding strategy generates a sequence of successive bids, which are going to be presented to the opponent in every round of the negotiation. This function could be either a time dependent or behavior dependent (or imitative) function [33]. The strategy proposed by Krimpen et al. [10] is an example of a time dependent function in the bidding strategy presented by Fatima et al. [35]. Moreover, the bidding strategy used by Baarslag et al. [24] is of the behavioral function types that compensates every nice move from the opponent. The models in this category are classified in two classes: (1) Regression analysis models, and (2) Time series forecasting models [7]. Regression analysis models assume that the bidding strategy of the opponent could be defined by a formula with unknown parameters; that is, the problem of estimating the bidding strategy of the opponent could be easily reduced to the regression analysis on the utility values of the received offers from the opponent in the agent’s own utility space. The model proposed by Yu et al. [20] falls into this category. On the other hand, in time series forecasting models, the bidding strategy of the opponent is totally unknown; that is, the agent does not even have any information on the bidding strategy formula. Therefore, the agent uses a time series in order to forecast the utility values of the upcoming received offers in the agent’s own utility space. In other words, these models receive a list of timely ordered utility values as the input and generate as the output the probable next utility values. For example, the models devised by Williams et al. [27,29] and Carbonneau et al. [28] fall in this category.The models in the sixth category try to estimate the acceptance strategy of the opponent. The acceptance strategy of an agent is defined as a Boolean function, which receives a typical bid as the input and produces a Boolean value showing whether the bid would be acceptable by that agent or not. It is important for the agent to learn this function for improving the negotiation outcome, since it could determine the best bid for the agent which is still acceptable for the opponent [7]. One of the works that deals with proposing an acceptance strategy learning method is the article by Lau et al. [30]. In this model, the offers sent by the agent to the opponent are considered as unacceptable bids and the offers received from the opponent are considered as acceptable ones. These bids are then fed into a Bayesian learning model as training examples through which the probability of accepting a candidate bid by the opponent would be determined. The model proposed by Aydogan and Yolum [31] also falls in this category. This model estimates the acceptability of a model through inductive learning. In their model, the sent offers are considered as the negative instances and the received offers are considered as the positive instances, and both are applied in learning the target function.2.2. Technical classification of opponent modelsIn the general classification, the models are classified based on the aspects of the opponent which are to be learnt. However, according to the underlying scheme in which they are applied to extract the opponent’s preferences, the existing state of the art opponent models (which use a common negotiation setting for estimating the preference order of the negotiation outcomes and are designed and implemented in the Genius framework [36,37] in the ANAC tournaments) could be categorized into the following [38]:1. The Bayesian models: estimate the preference profile of the opponent by generating a set of candidate profiles (hy-potheses) and then using Bayesian learning to update the probabilities of each hypothesis. The models here make some assumptions on the bidding strategy of the opponent.2. The Frequency models: estimate the issue weights and evaluation values by considering the changing frequency of the value of each issue between successive bids and the frequency at which each issue value is offered, respectively. Unlike F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9165the Bayesian models, which make strong assumptions about the bidding strategy of the opponent, the models of this group make assumptions on the value distribution of the issues in the opponent’s preference profile and impose weak restrictions on the bidding strategy of the opponent [9].3. The Value models: are the same as frequency models except that they assume equal and constant values for issue weights and focus on estimating the evaluation values of each issue value instead. Though assuming constant and equal values for issue weights would degrade the model’s accuracy, it would also make the agent free of the need for estimating the issue weights.Bayesian opponent modeling techniques, allegedly the most prominent and popular probabilistic approaches in opponent modeling [7], generate hypotheses on the opponent’s preferences. Upon receiving a new offer from the opponent, they update the probabilities associated with each one of these hypotheses [9,12]. These models are mostly based on the first assumption, that is, these models make strong assumptions on the bidding strategy of the opponent.On the other hand, frequency models (and value models) are mostly reliant on the third and fourth assumptions (and in some cases on the second assumption), and rely less on the first assumption. That is, these models usually make as-sumptions on the value distribution of the issues in the opponent’s preference profile and depend less on the opponent’s bidding strategy. However, it is not clear whether the opponent would stick to the underlying assumptions of these models. Therefore, due to the uncertainty about the validity of these assumptions through negotiations with different opponents, these modeling techniques are potentially subject to failure, since they strongly count on the opponent to somehow follow a subset of these assumptions [9].2.3. State of the art opponent modelsAll three classes in the technical classification try to estimate the utility function of the opponent and correspond to the fourth category in the general classification (see Section 2.1). In this article, the frequency models and value models are not differentiated, and the value models are considered as a part of the frequency models. In fact, frequency models are value models which also extract the issue weight values from the opponent’s behavioral and verbal reflection. In Section 5, the accuracy of this newly proposed model is compared with the state of the art frequency (and value) models together with Bayesian and classic models, which are available in the Genius repository. These models are briefly explained below:1. AgentLG Freq. Model: each evaluation value is simply estimated based on the frequency at which that value is offered. Issue weights are simply assumed to be uniform [36,37].2. AgentX Freq. Model: a more complex variant of HardHeaded Freq. model, where the tendency of the opponent to repeat bids is of concern. The weight of an issue is computed based on the number of times its value changes [36,37].3. CUHKAgent Freq. Model: the evaluation values are computed based on the frequency at which they are offered. The utility value of a bid is estimated by calculating the sum of the evaluation values for all issues of that bid and dividing it by the best possible score to normalize it into a utility value. Only the first 100 unique bids are used in this model for its estimation. Issue weights are simply assumed to be uniform [39].4. HardHeaded Freq. Model: the evaluation values are calculated based on the frequency at which they are offered. The weight of an issue is calculated based on the number of times its value changes [10].5. InoxAgent Freq. Model: the evaluation values are calculated based on the frequency at which they are offered. The more important an issue value is, the more it is repeated in successive bids. The weight of an issue is calculated based on the number of times its value changes. The higher the importance of an issue, the more unchanged its value remained [36,37].6. Nash Freq. Model: computes the evaluation values based on the frequency at which they are offered. The weight of an issue is calculated based on the number of times the best assumed value of the issue is changed [36,37].7. Smith Freq. Model: the evaluation values are computed based on the frequency at which they are offered. The weight of an issue is calculated based on the distribution of the values of that issue [11].8. The Fawkes Freq. Model: the evaluation values are determined based on their frequency through successive bids. The issue weights are determined based on the number of times their value changes. This model is very similar to InoxAgent Freq. Model with a slight difference: the unchanged issue values are determined by comparing the values of the last and first bids received from the opponent [40].9. IAMHaggler Bayes. Model: is an efficient implementation of the Bayesian Scalable model, and is similar to the Scalable Bayesian model; here, it is also assumed that the opponent uses a particular time-dependent concession function [16].10. TheNegotiatorReloaded Bayes. Model: is very similar to the IAMHaggler Bayes. Model with a slight difference: it uses differ-ent parameters for the supposed concession function of the opponent [40].11. Scalable Bayes. Model: the issue weights and evaluation values are estimated using Bayesian learning. First, it initializes the hypothesis space of all possible preference profiles of the opponent. Next, it repeatedly updates each probability value based on the assumption that the opponent concedes according to a linear function [12].12. Perfect IAMHaggler Bayes. Model: the same as the IAMHaggler Bayes. Model except that it is equipped with perfect infor-mation [16].13. Perfect Scalable Bayes. Model: the same as the Scalable Bayes. Model except that it is equipped with perfect information [12].66F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91Table 1Classification of the state of the art opponent models in Genius repository.Opponent modelTypeEstimatingIssue weightsEvaluation valuesAssumptionsAgentLG Freq.AgentX Freq.CUHKAgent Freq.HardHeaded Freq.InoxAgent Freq.Nash Freq.Smith Freq.TheFawkes Freq.IAMHaggler Bayes.TheNegotiatorReloaded Bayes.Scalable Bayes.Perfect IAMHaggler Bayes.Perfect Scalable Bayes.Perfect ModelNo ModelWorst ModelOpposite Modela Note: F, B, and C stand for Frequency based, Bayesian, and Classic.FaFFFFFFFBBBBBCCCCNoYesNoYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYes8, 44, 38, 44, 34, 34, 3, 24, 34, 3, 26, 5, 16, 5, 16, 5, 110, 6, 5, 110, 6, 5, 11113111214. Perfect Model: The agent has perfect knowledge regarding the preferences of the opponent. In other words, the oppo-nent’s utility function is known to the agent [36,37].15. No Model: the agent is without any model, hence the estimated utility of a given bid equals the utility of that bid in the opponent’s utility space [36,37].16. Worst Model: This model is the opposite of the Perfect Model, that is, the utility of a given bid using the Worst Model equals 1 minus the real utility of the same bid obtained through the Perfect Model [36,37].17. Opposite Model: it is assumed that the estimated utility of a given bid equals 1 minus the utility of the same bid in the agent’s own utility space [36,37].All the models in the Genius repository, together with the assumptions that each adopts in modeling the opponent’s preferences, are tabulated in Table 1 (see Section 1). As observed, eight out of the seventeen opponent models are Frequency based. Moreover, except AgentLG and CHUKAgent which only estimate evaluation values and not issue weights, all others estimate both the issue weights and evaluation values.With respect to the technical classification (Section 2.2), this newly proposed model does not fit into any of the three categories. Hence, a new class is defined, named Perceptron-Based Models, where this proposed model could fit in. More-over, this proposed model fits best into the fourth category described in the general classification (Section 2.1). Nonetheless, to the best of our knowledge, there is no proposed model in the fourth category which specifically uses neural networks (or neuron units) as the underlying learning technique. Our model is the first proposed model in the fourth category that uses perceptron units as the underlying mathematical basis for opponent modeling.3. Negotiation settingThe negotiation setting here corresponds to the state of the art models’ setting in the field of automated negotiations (e.g., [1–3,17,36,38]) and that of the ANAC 2010–2013.4 Automated agents alternatively exchange offers and compete against each other to reach a joint agreement on a set of issues in bilateral negotiations. The issues and possible values for each issue constitute a domain. For each domain there could be two preference profiles (one for each side of the negotiation) which together with the domain construct a negotiation scenario.The interaction between negotiating parties is regulated by a negotiation protocol that defines the rules of how and when proposals can be exchanged [17]. In this setting, the alternating offers protocol is applied [42]. Negotiation is bilateral, that is, exactly two parties are negotiating over one or a set of issues. Each issue is associated with a set of possible values. The agents repeatedly exchange offers in successive rounds, so as to reach a mutually acceptable outcome. The negotiation deadline is reached after a specified number of rounds N are passed. This type of negotiation setting is commonly referred to as a round based setting. Each agent tries to take advantage of the other party for gaining a maximum utility for its own. A negotiation break-off causes both negotiating parties to obtain their reservation values. Therefore, the agents try to reach an agreement before the deadline. A negotiation session takes place in a negotiation scenario, which consists of a negotiation domain (or, alternatively, an outcome space) and two preference profiles (or, alternatively, utility space) one for each negotiating agent.4 In 2014, another setting has been added for negotiations under nonlinear utilities and in ultra large domains. ANAC 2015 considered multi-party negotiations [41].F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9167The negotiation domain Ω specifies all possible offers −→ω that the agents can send or receive. Each offer or possible outcome is a vector (cid:4)ω1, · · · , ωn(cid:5), where each component is the mapping of every issue i to a value ωi ∈ {v i1, · · · , v imi}, where, mi is the number of possible values for issue i, i = 1, . . . , n [17]. A preference profile {(cid:4)−→ω, U (−→ω)(cid:5) | −→ω ∈ Ω}, on the −→ω) which maps each possible offer −→ω ∈ Ω to a value in the [0, 1] range based other hand, consists of a utility function U (on the overall relative value of that offer for the agent. In multi-issue negotiations, the common assumption is that the utility of an offer can be computed as a weighted sum of the utilities associated with the values for each issue [12,43–47]. Accordingly, in the negotiation setting here, the agents use the linear additive utility function shown in Equation (1),5defined by a set of weights w i , and the corresponding evaluation functions or evaluation values evali(ωi), i = 1, . . . , n, for the issue value ωi of a given offer −→ω:−→ω) =U (n(cid:2)i=1w i.evali(ωi)(1)In other words, the preference profile of an agent is modeled as a linear combination of a set of weights (which measures the relative importance of the negotiation issues) and a number of individual utility functions (or evaluation functions) which calculate the utility of a possible value for a negotiation issue. Unlike the negotiation domain which is publicly known for both the negotiating parties, the preference profile is private for each agent, so the agents are not aware of the weights and evaluation values associated with the preference profile of one another. Without losing the applicability of this newly proposed model, for the sake of simplicity in the negotiation setting here it is assumed that each issue value ωi in an offer only takes a finite set of discrete values.This negotiation setting is online, that is, the agent is only allowed to use the offers exchanged during a single negotiation session to model the preferences of the opponent, so that learning between sessions is not authorized. Unlike offline opponent models where negotiation information from different negotiation sessions with similar opponents is applied in modeling the preferences of an opponent, in online models (e.g., [10,12]) no history of the previous negotiations is provided for the opponent model [7,9].4. POPPONENT: perceptron-based opponent modelAs explained in Section 2, state of the art models fall into the categories of either Frequency-based or Bayesian models, and they suffer from the dependency on the category-based restricting set of assumptions to model the opponent. In this study, an efficient model is designed based on perceptron units which are less dependent on the assumptions of the opponent. In fact, unlike non-classic state of the art models (Table 1) which depend on a number of restricting assumptions, this model relies only on one assumption, either 1 or 10. In real negotiation problems, all of the training examples (i.e., bids) are not always given in advance, but are gradually acquired one at a time. Therefore, a model is required which is updated right after each bid is encountered. For this purpose, the proposed perceptron-based opponent model (POPPONENT) adopts an incremental gradient descent or stochastic gradient descent algorithm to efficiently learn the preference profile of the opponent in linear scenarios. The mathematical details behind this approach are presented in Section 4.1. The algorithm of the perceptron-based opponent model is proposed in Section 4.2.4.1. Mathematical justifications4.1.1. Perceptron-based learningIn order to enable the agent to learn the preference (issue weight values and individual utility functions) of an opponent during a negotiation session in bilateral multi-issue negotiations, the standard simple perceptron units are adopted, which are used as the basic units that construct the ANN [32,48–50], Fig. 2.A simple perceptron unit takes a vector of real valued inputs (x1, . . . , xn) and calculates a linear combination of their values through a linear additive function as Equation (3),(cid:3)O (x1, . . . , xn) =if w 0 + w 1x1 + w 2x2 + · · · + wnxn > 01−1 otherwise(3)where edge weights w i are the real valued parameters that determine the contribution of inputs xi to the perceptron output O , and the quantity −w 0 is a specific threshold (such as 0). When all the inputs are received in a perceptron unit, if the weighted inputs’ combination exceeds the threshold, the perceptron produces 1 as an output; otherwise, it produces −1. The weights are not known in advance, so they have to be somehow adjusted through some training instances, a specific learning algorithm (like gradient descent) and a specific training rule (like the delta rule) [32]. The error between the real 5 In discounted scenarios [2,3,17], the utility of an offer decreases over time according to a discount factor. Discounted utility U D could be as follows:U D (−→ω) = U (−→ω) × dtwhere, d (0 ≤ d ≤ 1) and t (0 ≤ t ≤ 1) are the discount factor and the negotiation time, respectively.(2)68F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91Fig. 2. A simple unit of perceptron [32].Fig. 3. The Perceptron unit: (a) to learn the issue weights, and (b) to learn the evaluation values.output and the output produced by perceptron unit is minimized by giving each training example in the training set into the perceptron unit, and adjusting the edge weights according to a training rule (like the delta rule) in a successive manner.4.1.2. Multi bipartite incremental gradient descent searchor priorities as well as the evaluations evalOPi (ωi) corresponding to each possible value ωi ∈ {v i1, · · ·, v imiAs mentioned in Section 3, in bilateral multi-issue negotiations, the preference profile of an opponent is private to the agent. Therefore, the agent has to learn it through offers as training instances received from the opponent and through a specific learning algorithm. In linear scenarios, a preference profile is comprised of a number of negotiation issue weights }, where, miw OPiis the number of possible values for issue i (i = 1, . . . , n). Since both the issue weights and evaluations of the opponent’s preference profile are unknown to the agent, this proposed model applies two types of perceptron units [32] for learning the opponent’s preferences through a multi bipartite incremental gradient descent search. The two constituting parts of this algorithm execute two separate overlapping steps: in the first step, Fig. 3(a), the evaluation values and the issue weights are assumed to have become the inputs to the perceptron unit and the weight vector elements of the perceptron, respectively. Next, the gradient descent is applied to adjust the issue weights (perceptron type 1); in the second step shown in Fig. 3(b), the issue weights and the evaluation values are assumed to have become the inputs to multiple copies of the perceptron unit and the weight vector of the perceptron units, respectively. Next, the evaluation values are adjusted by feeding the issue weight values as the input into the perceptron units (perceptron type 2). In other words, the objective of these perceptron units is to efficiently learn both the issue weight values w OP1 (ω1) through 1} of each issue i in a given negotiation domain, since for each issue i, evalOPωi ∈ {v i1, · · ·, v imi}, there are mi possible evaluation values for issue i. This means that the total number of perceptron units (of type 2) for learning the evaluation values is calculated through Equation (4):n (ωn) for all possible values ωi ∈ {v i1, · · ·, v imiand the evaluation values evalOPthrough w OPnn2 =n(cid:4)i=1mi(4)Moreover, in order to learn the negotiation weight values, only one perceptron unit (of type 1) would suffice, that is, in total, n2 + 1 perceptron units are needed to learn the preference profile of the opponent. Here, two types of perceptrons work next to one another in a manner in which at any given time the perceptron type 1 works together with one single perceptron from type 2.Upon receipt of a new offer (cid:4)ω1, · · ·, ωn(cid:5) from the opponent, the perceptron unit calculates the utility of that offer, and the perceptron learning algorithm is used to learn the preference profile of the opponent. By inserting each training example into the perceptron, the algorithm tries to estimate the real preference profile of the opponent by minimizing the error between the produced and real output, by adjusting the weight vector values of the perceptron unit in an iterative manner.The learning problem in Fig. 3(a) determines the weights w OP1n , in the sense that the training error of the learnt hypothesis (which is a vector of weight values) is minimized. The learning problem in Fig. 3(b) determines the evaluation values evalOPi (ωi) in the sense that the training error of the learnt hypothesis (which is a vector of evaluation values) is minimized.through w OPF. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9169Fig. 4. Moves for locating the global minimum in the plane of error values associated with different hypotheses (i.e., weight vectors) using (a) gradient descent, and (b) incremental gradient descent [32].The hypothesis space of possible weights (w 1 and w 2 in a sample 2-issue domain) and the associated error values are presented in Fig. 4. To find a minimum point, first, the incremental gradient descent or stochastic gradient descent optimization algorithm begins with an initial point on the error surface, and next, it repeatedly updates the weight values in the weight vector so that the search is directed towards the steepest descent along the error surface. In this algorithm, the weight values are updated right after each single training example is encountered in an incremental manner. This process continues until the global (or local) minimum is achieved.The error in modeling the opponent’s preference profile is formulated as follows:−→k ) =E((cid:2)−→k )Ed(d(ω)∈D(5)where Ed(−→k ) is the individual error for the training instance −−−−→d(ω), the value of which is calculated by Equation (6):−→Ed(−→k = (cid:4)(td − od)2−−−−−−→evalOP(cid:5) is the opponent preference profile, n (ωn)(cid:5), ωi ∈ {v i1, · · ·, v imik ) = 12−−−−→−−−−→d(ω) |w OP, where −−−−→}, i = 1, . . . , n} is the set of training examples, td is the target output d(ω) = (cid:4)evalOP1 (ω1), · · ·, evalOP−−−−→d(ω). The (real output value) for the training example n (vnmn )(cid:5) contains all the evaluation values for all the n (vn1), · · ·, evalOPvector possible values for all issues. The set of training examples or D includes the evaluation values for all possible offers in the outcome space, its cardinality is calculated through Equation (4).−−−−→d(ω), and od is the output of the linear unit for the training example (cid:5) is the issue weights vector, D = {−−−−−−→evalOP = (cid:4)evalOP1 (v 1m1 ), . . . , evalOP1 (v 11), · · ·, evalOP−−−−→w OP = (cid:4)w OP1 , . . . , w OP(6)nAccording to the function which delivers the utility value of a bid (Equation (1)), the utility value of a given training instance −−−−→d(ω) is calculated through Equation (7):od =n(cid:2)i=1evalOPi (ωi).w OPiBy inserting Equation (7) into (6) and (6) into (5), Equation (8) is yielded:E(−→k ) = 12(cid:2)d(ω)∈D(cid:5)td −n(cid:2)i=1(cid:6)2evalOPi (ωi).w OPi(7)(8)which represents the training error of all training examples. In Equation (8), ωi is the value of issue i in the training instance −−−−→d(ω).The objective here is to minimize the error E by searching for the best preference profile which yields the minimum error between the estimated offer utility values, according to the bidding behavior of the opponent (td) and the estimated ) for all offer utility values according to the current values of the preference profile of the opponent ((cid:7)ni=1 evalOPi (ωi).w OPi70F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91the training examples in D. The direction of steepest descent along the error surface can be determined by computing the −→gradient or derivative of E with respect to each component of the opponent profile vector k . This derivative vector, with respect to the vector −→k , can be calculated through Equation (9):∇ E(−→k ) = ∇ E(cid:8)−−−−→w OP,−−−−−−→evalOP(cid:9)=(cid:10)(cid:11)∂ E−−−−→w OP,∂∂ E−−−−−−→evalOP∂(9)Since the gradient calculated based on Equation (9) specifies the direction that yields the steepest increase in E, the negative of this value yields the direction of steepest decrease. The training rule for gradient descent search is expressed through Equation (10):−→k ) and−→k = −η∇ E(−→k + (cid:5)−→k =(10)−→k(cid:5)where the positive constant η is the learning rate which determines the step size in the gradient descent search. Writing Equation (10) in the componential form would yield:and ki = ki + (cid:5)ki(11)(cid:5)ki = −η∂ E∂ki(cid:7)nwhere i is between 1 and issues, that is, the total number of the components in vector total possible values of all issues (−−−−−−→evalOP). Provided that component ki is a weight value, the following equations are yielded.the evaluation values vector (i=1 mi +n, mi is the number of possible values for the issue i, and n is the number of negotiation −→k equals the total number of negotiation issues (n) plus the −−−−→w OP) and −→k in Equation (11) contains both the issue weights vector ((cid:7)ni=1 mi ). The vector (cid:5)w OPi= −η∂ E∂ w OPiand wOPi= w OPi+ (cid:5)w OPiIn order to achieve the steepest descent each component w OPiof the weight vector −−−−→w OP in proportion to ∂ E∂ wOPi(12)should be altered. Applying Equation (8), the gradient of E with respect to w OPiin Equation (12) is calculated through Equation (13).(cid:2)1(td − od)2= ∂∂ E∂ w OPi∂ w OP2i(cid:2)d(ω)∈D∂∂ w i(td − od)22(td − od)∂∂ w OPi(td − od)(td − od)∂∂ w OPi(td − od)∂∂ w OPi(cid:8)(td − od)−evalOP(td − od)(cid:5)n(cid:2)td −i=0(cid:9)i (ωi)= 12d(ω)∈D(cid:2)= 12d(ω)∈D(cid:2)=d(ω)∈D(cid:2)d(ω)∈D(cid:2)d(ω)∈D==(cid:6)evalOPi (ωi).w OPiNow, by combining Equations (12) and (13), the training rule for updating the weight values is obtained as follows:w OPi= w OPi+ η(td − od).evalOPi (ωi)(cid:2)d(ω)∈DOnce more, provided that component ki is an evaluation value, Equation (11) will be modified into:(cid:5)evalOPi (v i j) = −η∂ E∂evalOPi (v i j)and evalOPi (v i j) = evalOPi (v i j) + (cid:5)evalOPi (v i j)(13)(14)(15)where j is between 1 and mi (mi is the number of possible values for issue i), and i is between 1 and n (n is the number of issues).Similar to Equation (13), the training rule applied in learning the evaluation values is expressed as Equation (16).∂ E∂evalOPi (v i j)=∂∂evalOPi (v i j)12(cid:2)d(ω)∈D(td − od)2(16)F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9171∂∂evalOPi (v i j)(td − od)22(td − od)∂∂evalOPi (v i j)(td − od)(cid:2)= 12d(ω)∈D(cid:2)= 12d(ω)∈D(cid:2)=(td − od)d(ω)∈D(cid:2)d(ω)∈D=(td − od)∂∂evalOPi (v i j)∂∂evalOPi (v i j)(td − od)(cid:5)td −n(cid:2)i=0(cid:6)evalOPi (ωi).w OPiIf there exists d(ω) ∈ D, that ωi = v i j , then Equation (17) is yielded:∂ E∂evalOPi (v i j)=(cid:2)(cid:8)(td − od)−w OPi(cid:9)d(ω)∈DIf not, Equation (18) is yielded:∂ E∂evalOPi (v i j)= 0Combining Equations (15), (17), and (18) would yield:evalOPi(v i j) =(cid:12)evalOPievalOPi(v i j) + η(v i j)(cid:7)d(ω)∈D (td − od).w OPi∀d(ω) ∈ D | ωi = v i jotherwise(17)(18)(19)Equation (19) means that the evaluation value for the value v i j (i.e., evalOPi (v i j)) remains unchanged as long as it is not met in a training example.At this point, it could be deduced that in order to make a movement in the steepest descent direction in the error surface values according to Equations (14) and (19), respectively, of different preference profiles, calculating the evalOPwould suffice, hence the updating of the opponent preference profile.i (v i j) and w OPiWhen a sufficiently small learning rate η is used, the gradient descent search assures convergence into a vector with minimum error, regardless of whether the training examples are linearly separable or not. If η is too large, this search approach is at risk of exceeding the vector with minimum error in the error surface. Therefore, using an appropriate learning rate is of the essence [32].This gradient descent-based opponent modeling algorithm could be subject to two major drawbacks, namely: (1) con-verging to a local minimum can sometimes be quite slow, and (2) if there are multiple local minima in the error surface, then there is no guarantee that the procedure will find the global minimum [32]. Besides this, in real world negotiation problems, all of the offers (training examples) are not always provided in advance. If all training examples were available in advance, reaching one of the global minimums would mean obtaining 100 percent accuracy (or zero error) in predicting the output values through the input values for the set of available training instances.These difficulties are alleviated by the incremental or stochastic version of the gradient descent. As mentioned, non-i (v i j) values separately for all training instances through −−−−→w OP, and then applies i (v i j) values, respectively (through Equations (14) and (19)). However, in incremental values are updated right after each offer is received, according to the incremental gradient descent first calculates (cid:5)w OPthe target values provided by the current evaluation values, vector and evalOPthem in calculating the w OPor stochastic gradient descent, evalOPfollowing delta equations:−−−−−−→evalOP, and the weight vector, i (v i j) and w OPiand (cid:5)evalOPii(cid:12)evalOPi (v i j) =w OPi= w OPievalOPevalOPi (v i j) + η(td − od).w OPi (v i j)+ η(td − od).evalOP(ωi)iiif ωi = v i jotherwise(20)(21)Equation (20) reveals that in the incremental gradient descent, not all the evaluation values are updated, but only those that have been met in the training instance −−−−→d(ω), hence, Equation (20) can be rewritten as Equation (22).e valOPi (ωi) = e valOPi(ωi) + η(td − od).w OPi(22)Now, in order to find the best preference profile, in the incremental gradient descent algorithm, after each training −−−−→d(ω) is met, Equations (21) and (22) are applied, and the evaluation values for the values that have been observed instance in the bid ((cid:4)ω1, · · ·, ωn(cid:5)) and their weight values are updated.Having introduced the mathematical basics of this algorithm, in Section 4.2 we present the algorithm based on the incremental gradient descent learning to extract the preference profile of the opponent.72F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–914.2. POPPONENT algorithmThe newly introduced multi bipartite incremental gradient descent or stochastic gradient descent search (Section 4.1.2) is applied here to learn the preference profile of the opponent in linear scenarios. To serve the purpose, it learns the issue }i (ωi) for all possible values ωi ∈ {v i1, · · · , v imipriorities or weight values w OP1(where, mi is the number of possible values for issue i) and all negotiation issues i (i = 1, . . . , n) in that negotiation domain.This proposed algorithm is an incremental version of the perceptron-based learning method (Section 4.1) and applies two parameters of η and N as the input. Parameter η represents the learning rate which determines the step size in the gradient descent search. Parameter N represents the number of training repeats for each training instance. This algorithm includes two separate functions of initializer and updater.n and the evaluation values evalOPthrough w OPiThe first function is invoked just once when the model is generated and the essential parameters of the proposed model, most importantly the preference profile of the opponent (that is, the issue weights w OPi (ωi) for issue values) is initialized. By trying different initial points, it is realized that 0.5, the midpoint in the hypothesis space, is the }, i = 1, . . . , n (Algorithm: line 13). Similarly, for the best point for initializing evalOPweight values w i , equal weights 1i (ωi) values for each ωi ∈ {v i1, · · · , v imiThe second function receives an offer vector −→ω which specifies the issue values for all the negotiation issues of a new offer recently received from the opponent. That is, as soon as a new offer is received from the opponent, this function is invoked to update the model based on this newly received bid. It updates the estimated preference profile of the opponent by adjusting evalOPi (ωi) and w i values (Algorithm: lines 18 and 21). Whenever a new offer is received from the opponent, −→ω) is a function the perceptron learning delta rules (Algorithm: lines 18 and 21) are repeated N times. The EstimatedUtilityOP(which receives an offer as the input and returns the estimated utility value of that offer in the opponent’s utility space as the output.n are chosen for all issues (Algorithm: line 14).and evaluations evalOPIn this algorithm, instead of updating each evalOPvalues when all training examples are met, each evalOPvalue is modified using training delta rules right after each single training instance is met in an incremental manner (Algorithm: lines 18, 21). Therefore, this algorithm can easily be applied in more realistic negotiation scenarios in which training examples (opponent offers) are gradually met one at a time.value after calculating all (cid:5)w i and (cid:5)evalOPii (ωi) and w OPii (ωi) and w OPiThe algorithm:Algorithm: Perceptron-based opponent model−→ω = (ω1, · · ·, ωn) is the offer vector containing the values of a new offer received from the opponent.1: Where,2: η is the learning rate.3: N is the maximum number of training repeats for each training instance.4:5: n is the number of issues.6: mi is the number of possible values for i-th issue.7: ωi denotes a possible value for the i-th issue.8: w OPis the weight of the i-th issue.9:10: ωi is the value of i-th issue for the offer −→ω.11: PerceptronUtilityOP(is the evaluation function of the opponent for the i-th issue.ievalOPi−→ω) is the estimated utility of the offer −→ω in the opponent’s utility space, which is obtained by feeding the offer into the perceptron unit and getting the output value.12: EstimatedUtilityOP(−→ω) is a function which receives an offer −→ω as the input and returns the estimated utility of that bid in the opponent’s utility space as the output. This function value is calculated using the estimated bidding behavior of the opponent.Initializer13:14:i (ωi) to 0.5, for each ωi ∈ {v i1, · · · , v imiInitialize all evaluation values evalOPto 1Initialize each weight value w OPn .i} (i = 1, . . . , n).For each evaluation value evalOPUpdater15: Repeat the following process N times16:17:18:19:20:21:For each issue weight value wi, Doi (ωi), DoInput the instance −→ω to the unit and compute the output PerceptronUtilityOP(evalOP−→ω) − PerceptronUtilityOP(i (ωi) + η.(EstimatedUtilityOP(i (ωi) ← evalOP−→ω).−→ω)). w OP.iInput the instance −→ω to the unit and compute the output PerceptronUtilityOP(w OPi−→ω) − PerceptronUtilityOP(+ η.(EstimatedUtilityOP(−→ω)). evalOPi (ωi).← w OPi−→ω).F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9173This proposed model is a supervised algorithm, thus it needs output labels for training instances. The problem of preference modeling in bilateral multi issue negotiations through supervised learning methods can be separated in two sub-problems: 1) estimating the utility values of the opponent’s offer history (the history of the offers received from the opponent through the negotiation session) and 2) extracting the estimated utility function (or the preference profile) of the opponent from the opponent’s offer history. By solving the first problem, now the opponent’s offer history contains all the −→ω) function which deals with estimated offer utilities for each offer in the opponent’s offer history. The EstimatedUtilityOP(the first sub-problem, can be estimated according to the perceived bidding behavior of the opponent.In this article, four different values – three constant (0.6, 0.8, and 1) and one adaptive – are applied in order to estimate the utilities of the offers proposed by the opponent. Applying the constant values for estimating the utility value of the bids received from the opponent may seem simplistic at first, but actually it is not. As explained in Section 1, the opponent modeling problem in negotiation is a specific type of a learning problem, where all of the training examples are not provided in advance, but are provided throughout the negotiation session in an incremental manner. In fact, designing a preference model which is able to update in an incremental manner is one of the important challenges in modeling the user preferences in bilateral multi issue negotiations. More importantly, in such models the number of bids that the agents exchange before reaching deadline is limited [2], and this is another important challenge that an opponent model should overcome. Although estimating the utilities of the bids received by the opponent through a constant function may not be the best choice, it could at least be a reasonable assumption, whereas the rational agents try to reach the highest utility by following a concession-based bidding strategy [2]. For example, 1) assuming “most of the bids that the opponent sends throughout the negotiation session have utilities around a value such as 1” would not be an unreasonable assumption, and 2) in the Scalable Bayesian Model proposed by Hindriks et al. [2], they assume that the utility of the bids sent by the opponent follow the 1 − 0.05t (0 ≤ t ≤ 1) function which places the utility of the bids received from the opponent into the [0.95, 1] interval. Obviously, this assumption would not be very different from assuming that the opponent mostly sends the bids with the utility value 1.For the fourth value, we use the adaptive method where the agent estimates the bids that the opponent will offer in future, based on the opponent’s bid history [51–55]. At time t, the opponent will offer a bid with the utility target(t) the value of which is calculated based on Equations (23) and (24):emax(t) = μ(t) +(cid:8)target(t) = 1 −(cid:9)(cid:8)1 − μ(t)d(t)(cid:9)tα1 − emax(t)(23)(24)where, emax(t) estimates the maximum utility of the bid that the opponent will offer in the future, μ(t) is the average utility of the bids proposed by the opponent in the agent’s own utility space, and d(t) determines the width of the bids received from the opponent and is calculated based on deviation.Similar to the approach applied by CHUKAgent (see Section 2) [39], this adaptive model applies only the first 1000 (instead of all 5000) unique bids in a negotiation session for its estimation in order to prevent its accuracy decline (see Section 5.3.1).The computational complexity of POPPONENT Algorithm is linear (O (n)), and is analyzed in Appendix A.5. ExperimentsTo evaluate the proposed POPPONENT model, two separate experimental settings are applied for assessing its accuracy and performance (in real world negotiation examples) compared to the available opponent models.5.1. Experimental settingAs reviewed in Section 2, different settings and measures are applied by the researchers for evaluating their proposed models. The Genius framework, provided by ANAC, considers all the measures available in this field (see Appendix B) and presents the most extensive and comprehensive settings. The Genius framework facilitates development and evaluation of negotiation agents and their constituting components through a repository of the largest comparable set of models consistent with ANAC settings6 [38]. Accordingly, a similar experimental setting is adopted here to assess the accuracy and performance of this POPPONENT model.The first setting, introduced in Section 5.1.1, evaluates the accuracy of POPPONENT through the Pearson Correlation measure. The second setting, introduced in Section 5.1.2, evaluates this proposed model by measuring the real performance of the agents applying this model in real world experimental negotiation scenarios.5.1.1. Experiment I: evaluating the accuracy of POPPONENTThe experimental setting applied by Baarslag et al. for automated bilateral multi issue negotiations [38] is also applied here to evaluate the accuracy of this proposed POPPONENT model versus the state of the art opponent models.6 Baarslag et al. proposed a setting for evaluating and comparing the accuracy and performance of the set of state of the art opponent models currently available in the Genius repository [38].74F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91According to this setting, 5 variations of POPPONENT (Section 4.2) are compared with a total of 15 opponent models including 8 Frequency-based opponent models, 5 Bayesian opponent models and 1 classic model (Opposite model7) together with a No Model. These models are tabulated in Table 1.Each one of these 20 (= 8 + 5 + 1 + 1 + 5) opponent models is equipped with a bidding strategy to compete with an opponent agent. The bidding behavior of an opponent could be influenced by the opponent model of the agent in two manners: first, as the opponent model helps the agent to make better offers, it may cause the negotiation to end earlier with a high quality agreement, hence a shorter agreement, and second, since applying an opponent model would change the bidding sequence of the agent, it may also cause the opponent to change its bidding behavior. For example, if the opponent applies some kind of an imitative bidding strategy [33], different bidding sequences for the agent will definitely result in different bidding sequences from the opponent. Therefore, to be able to compare the different opponent models, all the models being trained under the same circumstances should be confirmed to allow the non-adaptive opponents to be applied for training this proposed opponent model as well as the other models.Here the acceptance capability from the negotiation strategy of the opponent is removed, that is, because an equal number of bids are to be exchanged through all the sessions. This ensures that all the opponent modeling techniques are actually being executed under the same conditions.5.1.1.1. Bidding behaviors of the agents Once more, in accordance with the setting suggested by Baarslag et al. [38], we use four classes of different opponents, with different bidding behaviors applied here as follows:1. Conceding agents apply a time dependent concession strategy [33] to make the bids during the negotiation session, that is, the target utility is calculated through Equation (25):(cid:9)(cid:8)ut = P max.1 − t1/e(25)where, for concession rate e, four different values of 0.1, 0.2, 1, and 2 are applied with a fixed starting bid utility P max = 1.2. Random agents generate a bid with a utility above a fixed threshold m in a random manner. Four different values 0, 0.25, 0.5, and 0.75 are selected for the threshold.3. Conceding agents with an offset are time-dependent agents which do not start the negotiation with their best bids. For this purpose, a linear concession rate (e = 1), with different values of 0.7, 0.8, and 0.9 is applied in the starting bid utility P max.4. Non-conceding agents, which begin with a minimum target utility that increases to its maximum utility over time. The target utility is calculated through Equation (26):ut = P min + (1 − P min).t(26)where four different values of 0, 0.25, 0.5, and 0.75 are applied in the minimum acceptable utility P min for the agent.In accordance with these values, 4 types of conceding agents, 4 types of random agents, 3 types of conceding agents with an offset, and 4 types of non-conceding agents are obtained. Moreover, it is worth noting that the first class reveals a predictable bidding behavior, since it always begins with its best offer, always concedes, and makes a unique trace, while the other three classes are all considered to be unpredictable, since they make random offers or do not concede or begin with their best offers. From the four classes above, the second contains opponents with random or non-deterministic behavior, while the others comprise deterministic opponents. Consequently, the experiment involves a total of 15 opponents, among which 4 opponents (in the first class) are predictable, and the remaining 11 are unpredictable.As mentioned in Section 1, all of the existing models apply a subset of assumptions on the bidding strategy or on the value distribution of the issues of the opponent’s preference profile. Here, different opponents with different bidding strategies are applied in training the opponent model and to see how accurate each opponent model actually would be, when these assumptions would not hold true in the real world.5.1.1.2. Negotiation scenarios According to Baarslag et al. [38], there exist the following three features of a negotiation sce-nario that significantly influence the ability of the opponent model in estimating the opponent’s preferences in an accurate manner [38]:1. The Domain size can be small, medium, or large based on its total number of possible offers and depending on the amount of parameters of the preference profile.2. The Bid distribution relates to the average distance of outcomes of a scenario to the nearest Pareto optimal offers. In a high bid distribution domain, a high percentage of outcomes are distanced far from the Pareto frontier.87 ANAC organizers did not consider the Perfect Model and Worst Model in the setting, since it is already clear what their Pearson Correlation values would be. According to their definitions, the Perfect Model and Worst Model always yield accuracy values of 1 and −1, respectively.8 Pareto frontier: The collection of all Pareto Bids in an outcome space [44,56–59].F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9175Table 2Negotiation setting for Experiment I.OpponentsBidding behaviorConceding(Deterministic and predictable)ParametersFour opponents with e = 0.1, 0.2, 1, and 2 in Equation (25)Random(Non-deterministic and unpredictable)Four opponents with thresholds of 0, 0.25, 0.5, 0.75, and 1Conceding with an offset(Deterministic and unpredictable)Non-conceding(Deterministic and unpredictable)Three opponents with P max = 0.7, 0.8, and 0.9(e = 1) in Equation (25)Four opponents with P min = 0, 0.25, 0.5, 0.75, and 1 in Equation (26)a Note: L, M, and H stand for Low, Medium, and High, respectively.Itex vs cypressSize: SmallOpposition: L, M, and HaBid Distribution: L, M, and HEmploymentSize: SmallOpposition: L, M, and HBid Distribution: L, M, and HsoiranecSCarSize: MediumOpposition: L, M, and HBid Distribution: L, M, and HSupermarketSize: LargeOpposition: L, M, and HBid Distribution: L, M, and HTravelSize: LargeOpposition: L, M, and HBid Distribution: L, M, and H3. The Opposition defines the competitiveness of the domain and is determined by the minimum of distances of all points (i.e., the distance of Kalai–Smorodinsky point, a unique point on Pareto Frontier with equal utilities for both parties) in the outcome space to the point of perfect mutual satisfaction (i.e., maximum utility for both parties).Now, in order to evaluate the proposed model in a complete setting, all the possible combinations of these three features should be tested. Due to the considerable computational limitations of testing all domains available in the literature and the Genius, Baarslag et al. [38] have carefully selected 5 domains and have considered all levels of those three domain features. The same domains are adopted here for all the aspects affecting the performance of opponent models to be considered. For this purpose, 45 scenarios containing 3 opposition levels and 3 bid distributions levels in 5 domains are applied. The summary of this setting including 15 bidding strategies over 5 negotiation domains applied in interactions among the agents are tabulated in Table 2.5.1.1.3. Interactions This proposed opponent model is trained to perform against all deterministic opponent agents in 45 aforementioned scenarios, a total of 495 (= 11 × 45) negotiation sessions. As for random agents, each negotiation session is repeated 5 times to eliminate the randomness in the results and to ensure that the results are reliable. Consequently, a total of 900 (= 5 × 4 × 45) negotiation sessions are run against 4 types of random opponents, making a total of 1395 negotiation sessions to be executed. All negotiation sessions are executed in the round-based setting for an equal number of 5000 rounds, where each round contains exactly two bids: an offer sent by the agent and the associated counter offerreceived from the opponent, a total of 6975000 (= 1395 × 5000) training bids which can be fed into each one of the 20 (= 8 + 5 + 1 + 1 + 5) opponent models for training purposes.The interaction among agents in the first experiment, in which the BOA framework is applied (Section 1), is presented in Fig. 5. Agents in Side B include a bidding strategy, no opponent model, and do not follow an acceptance strategy. In each interaction, an opponent model (equipped with an arbitrary bidding strategy and without an acceptance strategy) embedded in an agent in Side A will be trained by competing against opponent agents in Side B in 45 scenarios. Both sides actually have an acceptance module, but that module never accepts. This is because it is accepted for each opponent model in order to be trained by the same amount of training instances or bids (i.e., 6975000), in order to preserve equal conditions.In each one of the negotiation sessions, the agent in Side B applies one of 15 bidding strategies in Table 2, while the bidding strategy for the agent in Side A is not important; therefore it applies an arbitrary bidding strategy from Table 2. The strategies chosen in this setting are non-adaptive, because it is required that all the models apply the same bid sequences. If the bidding strategy in Side B were to be adaptive, it would change according to the bidding strategies in Side A. When the models apply different training instances or bids, they will not be compared in equal conditions. Since the bidding strategy in Side B (Opponent Side) is non-adaptive, it is not affected by the bidding strategy in Side A (the side which uses those 20 opponent models including POPPONENT variations). Therefore, it is actually not important which bidding strategy is applied Pareto bid: A point is a Pareto Bid, if and only if it is not possible to make any Pareto improvement at that point.Pareto improvement: The movement from one bid in the outcome space to another bid, which at least increases the utility of one agent, without decreasing the utility of the other agent.76F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91Fig. 5. Opponent models learning process during interactions in Experiment I.on Side A, because it will not have any effect on the opponent model applied on Side A. This serves the objective of this experiment, which is to train the opponent models in Side A.5.1.2. Experiment II: evaluating performance of POPPONENTTo evaluate the real world performance of this proposed model in a realistic setting, a similar setting to that applied by the ANAC organizers [38] is designed here. To accomplish this, a BOA framework is applied (Section 1) to embed each opponent model into an agent framework, the bidding strategy of which (together with its associated acceptance strategy) is chosen from the state of the art agents. Next, the average performance of each constructed agent is assessed through different types of components in competition with each other.5.1.2.1. Agents When the settings where each model would be trained are determined, the POPPONENT is compared with the following 6 state of the art opponent models together with the Perfect Model, Worst Model, and No Model (see Section 2.3) applied in the Automated Negotiating Agents Competitions (ANAC):• Agent X Freq. Model• Agent LG Freq. Model• CUHK Freq. Model• Smith Freq. Model• HardHeaded Freq. Model• InoxAgent Freq. Model• Perfect Model• Worst Model• No ModelRegarding other components, the top bidding strategies together with their associated acceptance strategies from ANAC 2010, ANAC 2011, ANAC 2012, and ANAC 2013, and the four time-dependent bidding strategies together with a simple acceptance strategy are collected here. The agents equipped with these strategies are: AgentK [51], Yushu [60], Nozomi [2], HardHeaded [10], Gahbonio [61], IAMHaggler2011 [27], TheNegotiatorReloaded [40], BRAMAgent2 [40], and InoxAgent [62], and the four Time Dependent Conceding Agents from Experiment I (Table 2).5.1.2.2. Negotiation scenarios Each agent competed 5 times, against all opponents on seven scenarios: Grocery [17], Thompson Employment [63], Travel [2], Small Energy, Supermarket (from ANAC 2012)9 [40], Camera [62], and ItexVsCypress [2]. The size, opposition, and bid distribution values (Section 5.1.1.2) of these scenarios are tabulated in the Table 3, sorted by bid distribution.5.1.2.3. Interactions This newly proposed model of opponents (AP and P1) together with other 9 opponent models (6 state of the art, Perfect Model, Worst Model and No Model), is evaluated according to the model in Fig. 5 by competing in both 9 It is worth noting that agents that participated in ANAC 2014 and ANAC 2015 respectively proposed models for nonlinear and multi-party scenarios which fall outside the scope of this paper.F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9177Table 3Negotiation scenarios applied in Experiment II.DomainGroceryEnergy smallItex vs. cypressCameraThompsonSupermarketTravelSize16001562518036003125112896188160IssuesOppositionBid distribution56465670.8060.4320.4310.8910.2670.3470.230.1910.2170.2220.2520.3250.3470.416Fig. 6. Opponent models’ learning process during interactions in Experiment II.sides of A and B (except that in Side A agents have all three BOA modules while agents in Side B have no opponent model). In both sides, the agents apply 13 bidding and acceptance strategies, consisting of 9 agents from ANAC competitions and 4 time-dependent agents. In Side A, there exist 7 opponent models combined with the same 13 bidding and acceptance strategies. Each agent competes 5 times over 7 negotiation scenarios for both preference profiles (i.e., each agent acts in both sides of a scenario); hence, for each model 11830 (= 7 × 5 × 2 × 13 × 13) sessions are executed. In other words, a total number of 82810 (= 11830 × 7) sessions are executed in the second experiment to evaluate this newly proposed model together with the state of the art opponent models in a fair realistic competition. All of the sessions are executed in Genius 4.2 [36,37] using the round-based protocol for an equal number of 1000 rounds. The interaction among agents in the second experiment is expressed in Fig. 6, using the BOA framework (Section 1).5.2. ExamplesBefore evaluating the overall results of experiment settings I and II, in this section an example of negotiations in course between agents including POPPONENT or No Model is presented. The performance of this proposed POPPONENT model applied by a Linear Time Dependent Agent (Section 5.1.1.1) against another Liner Time Dependent with No Model, Figs. 5and 6, in 50 rounds (as the negotiation deadline) is presented in Fig. 7. In this figure, (a) especially shows how the agent applying POPPONENT performs in a negotiation session against another agent with no model. How the same agent with No Model performs against another agent of its kind is shown in Fig. 7(b). Finally, the utility of the bids that agent A sends to itself (side A) and to the opponent (side B), 1) when applying POPPONENT, and 2) when applying No Model are shown Fig. 7(c). When the agent A applies POPPONENT, a rapid agreement is achieved with a higher utility for both sides.In Fig. 8, we show how the MBGD search method seeks to minimize Standard Error (Equation (5)) by converging towards zero, while this proposed POPPONENT model is being trained against a simple Time-Dependent Agent in a total of 5000 rounds. As the first 1000 rounds indicate, in this proposed model the algorithm moves towards the minimum point in the error surface, Fig. 4.5.3. Experimental resultsThe detailed experimental results comparing the accuracy and performance of POPPONENT against state of the art models are presented in Sections 5.3.1 and 5.3.2. The major findings are summarized in Section 5.3.3.78F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91Fig. 7. Performance gained against a time dependent agent by an agent equipped with (a) POPPONENT, (b) No Model, and (c) Their bid utilities vs. negotia-tion round.Fig. 8. Standard error vs. negotiation round, for training POPPONENT against a time dependent opponent.5.3.1. Experiment results I: accuracy of POPPONENTBased on the setting designed for Experiment I (Section 5.1.1), the accuracy of POPPONENT is compared with state of the art opponent models using the Pearson Correlation between the estimated and real bid utilities (Section B.1). For each model, the Pearson Correlation is in 11 points in time with equal distances (i.e., 10 time slots). The accuracies of all 20 models F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9179Table 4The list of the opponent models.AbbreviationOpponent modelParticipated inRankP6P8P1APPPLGFXFCKFHHFIXFNFSFFFIHBPIHBNRBSBPSBOMNMPMWMa Note: F and Q respectively stand for the final and qualifying rounds (on ANAC) the agent employing the model participated in.Perceptron Based Model (Constant 0.6)Perceptron Based Model (Constant 0.8)Perceptron Based Model (Constant 1)Adaptive Perceptron Based ModelPerfect Perceptron Based ModelAgentLG Frequency ModelAgentX Frequency ModelCUHKAgent Frequency ModelHardHeaded Frequency ModelInoxAgent Frequency ModelNash Frequency ModelSmith Frequency ModelThe Fawkes Frequency ModelIAMHaggler Bayesian ModelPerfect IAMHaggler Bayesian ModelThe Negotiator Reloaded Bayesian ModelScalable Bayesian ModelPerfect Scalable BayesianOpposite ModelNo ModelPerfect ModelWorst Model–––––ANAC 2012 (Q, F)aANAC 2012 (Q)ANAC 2012 (Q, F)ANAC 2011 (Q, F)ANAC 2013 (Q, F)–ANAC 2010 (Q, F)ANAC 2013 (Q, F)ANAC 2010 (Q, F)–ANAC 2012 (Q, F)–––––––––––2F13Q1F1F6Q–7F1F4F–3F––––––Fig. 9. Average accuracy obtained through all models against all opponent agents.including 5 variations of POPPONENT (Section 4.2) together with one classic and 13 state of the art models (Section 5.1.1) and No Model10 are assessed here. The abbreviations of these models are listed in Table 4.The average accuracy and respective standard deviations of all the opponent models against all opponent agents (pre-dictable and unpredictable opponents) are expressed in Fig. 9, where at the perfect information state (PP), this proposed model outperforms the state of the art models by a large average accuracy. As observed, the other three variations of POPPONENT (i.e., P6, P8, and P1 except PP) outperform all state of the art models with respect to the average accuracy over all opponents. Here, unlike most state of the art models, the accuracy of this model in its four variations increases over time in a monotonic manner. The accuracies of POPPONENT variations and CKF (which have the highest and the closest accuracy compared with POPPONENT variations) are compared in Fig. 10. This figure shows that all POPPONENT variations (except AP) exceed CKF (and all the other models) in terms of average accuracy.As observed in Fig. 9, most of the state of the art models lose their accuracy over time, since they handle the later bids in an incorrect manner. This is unique to the Bayesian models, since these models make some assumptions on the bidding behavior of the opponent, which become invalid as time passes. Most of the frequency models (and value models) are subject to this phenomenon as well, since they make some assumptions on the opponent’s preference profile, which 10 As also mentioned in footnote 6, there is no need to execute Perfect Model and Worst Model, since their accuracies were already clear given their definition.80F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91Fig. 10. Average accuracy against all opponent agents obtained through POPPONENT variations and the top state of the art opponent model.do not necessarily hold true. However, this is not true for the CUHKAgent frequency model, since it only uses 100 initial unique bids received from the opponent in order to update it. The accuracy of this model does not change after a few initial rounds [38].It is obvious that the POPPONENT in its perfect information state (PP) is not affected by the later bids, since it does not make any assumption on the opponent at all. However, in the other three constant-value variations of POPPONENT, one assumption is made on the opponent’s bidding behavior, which is why the accuracy of this proposed model in these three variations is low compared to the PP state. However, even in these three states, the accuracy of POPPONENT in-creases gradually. This could be attributed to the fact that compared to the other models, this model works based on a lower number of assumptions on the opponent or on the value distribution of the issues of the opponent’s preference profile.At first, it may not look surprising if PP reaches a great overall accuracy, because it uses Perfect Information to estimate the utility values of the bids received from the opponent; in fact it is, for several reasons. First, as observed in Fig. 9, it is evident that the accuracy that the two other Perfect Information models of PSB and PIHB achieve is nothing in comparison with that of the PP. In fact, PP doubles the accuracy values of PSB and PIHB. Second, adding perfect information to these models (PP, PSB, PIHB) would mean that an unsupervised learning problem is converted into a supervised learning prob-lem. Obviously, supervised learning problems are still one of the most challenging problems in data mining and machine learning. Finally, as mentioned in Section 1, in automated negotiations, the agents usually exchange only a limited num-ber of bids before they reach the deadline. However, as observed in this figure, this challenge does not prevent PP from achieving high accuracies. By the time only 10 percent of the negotiation time has passed, the accuracy of PP exceeds 60 percent.By comparing the accuracy of AP with CKF in Fig. 10, it is revealed that AP outperforms CKF. It is noticed that the accuracy of AP when it is trained using all the bids it receives in a negotiation session reaches its maximum at t = 0.2(round 1000) and then it begins to decrease. Similar to the approach applied by CKF in preventing a decline in its accuracy, 1000 initial bids are used here to train AP (instead of all 5000 bids).The accuracy of the models in Experiment I in terms of different features of the domains (Size, Opposition, and Distri-bution) is presented in Fig. 11, where the following properties are induced:Property 1. POPPONENT model works better in medium to large negotiation domains. (cid:2)Property 2. Accuracy of POPPONENT model improves with an increase in the distribution level of the negotiation scenario. (cid:2)Property 3. Most models (including the POPPONENT variations) generally perform better in scenarios with a medium size, high level of opposition, and high level of distribution. (cid:2)5.3.2. Experiment results II: performance of POPPONENTThe performance of each model is measured and revealed in Experiment II according to the setting explained in Sec-tion 5.1.2. Six performance measures of Avg. Utility, Avg. Time of Agreement, Avg. Pareto Distance of Agreement, Avg. Kalai Distance of Agreement, Avg. Nash Distance of Agreement and Avg. Percentage of Pareto Bids are applied for this purpose (see details in Ta-ble B.1).The values of the aforementioned performance measures and the final accuracy (accuracy at the final round of the negotiation) of this proposed POPPONENT model, together with the state of the art models in Experiment II are tabulated in Tables C.1 through C.7 (Appendix C); all the results are presented in Fig. 12.The content of Tables C.1 through C.7 and Fig. 12 indicate that POPPONENT is the most accurate model compared with other models available. This is consistent with the results of the first experiment, where it is found that the accuracy of F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9181Fig. 11. Summary of the results of average accuracy of all models over all opponents in terms of different levels of (a) domain size, (b) domain opposition, and (c) domain distribution.POPPONENT exceeds state of the art models. By considering the content of Tables C.1–C.7, it is found that POPPONENT (AP) and IXF models are the only two models which can outperform all other models in at least one domain with respect to all performance/accuracy measures; the proposed model’s properties consist of:Property 4. In terms of the Pearson Correlation measure, on average, POPPONENT is the most accurate opponent model in linear bilateral multi-issue negotiations against all types of opponents. (cid:2)Property 5. Among the models in Experiment II, POPPONENT is the most efficient model in at least one domain, with respect to all the measures. (cid:2)5.3.3. Key findingsThe results of Experiment I (Section 5.3.1) are tabulated in Tables 5 and 6. These results show the accuracies of all POPPONENT variations, the nearest opponent models (in accuracy) to the POPPONENT accuracy, and the opponent models applied by the winners of ANAC 2012 and 2013. As observed in these tables, this proposed POPPONENT model in the perfect information state (PP column) reaches higher accuracy levels than the state of the art models, as well as the models 82F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91Fig. 12. Summary of the results of Experiment II.Table 5POPPONENT’s accuracy compared with that of the state of the art and ANAC winners’ opponent model against different opponent agents.Opponent categoryAll opponentsPredictable opponentsUnpredictable opponentsRandom opponentsConceding opponents with an offsetNon-conceding opponentsAccuracy of POPPONENTState of the art opponent modelANAC winnerP6P8P1APPPModelAccuracy0.38720.79610.23840.38580.76630.38950.80360.23890.36650.78990.38990.80770.23790.35330.80100.38850.73250.26340.37670.76440.72990.76450.71730.84570.7444CKFCKFPSBLGFCKF0.38480.705430.33170.54060.59912012CKF0.38480.70540.26820.44130.59922013FF0.31600.46180.26300.51350.1645−0.3048−0.3020−0.2997−0.22560.5686OM0.2158−0.15300.0864from top ANAC agents. Besides this, as Table 5 shows, this proposed model in the P1 variation (the fourth column) is ranked second (after the PP variation) among all the state of the art models. This includes the models applied by top ANAC 2012 and 2013 agents against all opponents, predictable or conceding opponents, and conceding-opponents-with-an-offset (with accuracies of 0.3899, 0.8077, and 0.8010, respectively). However, the state of the art models of PSB, LGF, and OM with accuracies of 0.3317, 0.5406, and 0.2158 are ranked second against unpredictable opponents, random opponents, and non-conceding opponents, respectively. The opponent model applied by Agent Fawkes (FF) is weak in accuracy, meaning that as the winner of ANAC 2013, this agent must have been applying a very efficient bidding strategy or acceptance strategy or both, to compensate for the very poor accuracy of its opponent model.That this proposed POPPONENT model in PP variation has the highest accuracy in terms of different levels of scenario features is observed in Table 6. Moreover, when the scenario size is medium and high, this proposed model in P6 and P8 variations, respectively, has ranked second among all the models from the state of the art. Another intriguing observation in Table 6 is the fact that for all levels of scenario features, CUHKAgent Freq. Model has an excellent accuracy compared with F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9183Table 6POPPONENT’s accuracy compared with that of the state of the art and ANAC winners’ opponent model regarding different scenario features.Scenario featureFeature levelSizeDistributionOppositionLowMidHighLowMidHighLowMidHighAccuracy of POPPONENTState of the art opponent modelANAC winnerP1P6P8APPPModelAccuracy0.35730.42860.40310.27720.40550.48690.31570.41510.43880.35070.44230.39600.27810.40150.48180.31920.40670.43560.35520.43010.40340.28250.40520.48080.32240.40990.43610.35750.42990.39880.29300.40250.47000.31210.41100.44240.67570.81580.74120.65830.75310.77840.67340.76060.7557PSBCKFCKFOMCKFNFPSBCKFCKF0.40110.42360.37050.44420.39790.52430.33760.41410.44922012CKF0.37980.42360.37050.25670.39790.49990.29120.41410.44922013FF0.29900.38010.30100.12880.31370.50560.19350.33150.4231Table 7Summary of Experiment I results: the most accurate realistic model.AgainstAll opponentsPredictable opponentsP1 (and P8, AP, P6)SizeP8 (and P1, P6, AP)Top modelScenario featureFeature levelTop modelUnpredictable opponentsRandom opponentsNFLGFDistributionConceding opponents with an offsetNon-conceding opponentsP1 (and P8, P6, AP)OMOppositionLowMidHighLowMidHighLowMidHighCKFP6 (and P8, AP, P1)P8 (and P1, AP, P6)OMP1 (and P8, AP, P6)NFCKFP1CKFTable 8Top performing real model in each domain for all performance/accuracy measures in Experiment II.DomainUtilityPearson correlationNash distancePareto distanceKalai distanceTimePerc. of Pareto bidsSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverageIXF (and LGF)IXF (and AP)IXF (and SF)AP (and IXF)HHF (and SF)LGF (and SF)IXF (and LGF)IXF (and LGF)P1 (and AP)P1 (and AP)P1 (and AP)IXF (and AP)P1 (and CKF)P1 (and HHF)P1 (and AP)P1 (and AP)IXF (and XF)IXF (and HHF)CKF (and LGF)CKF (and IXF)IXF (and AP)CKF (and AP)AP (and LGF)IXF (and CKF)XF (and IXF)IXF (and HHF)IXF (and SF)AP (and CKF)HHF (and IXF)LGF (and SF)IXF (and LGF)IXF (and LGF)IXF (and XF)IXF (and CKF)CKF (and LGF)CKF (and IXFIXF (and AP)CKF (and SF)AP (and LGF)IXF (and CKF)XF (and AP)AP (and XF)CKF (and LGF)SF (and LGF)IXF (and HHF)LGF (and HHF)AP (and P1)LGF (and SF)XF (and P1)XF (and LGF)P1 (and AP)LGF (and SF)XF (and P1)LGF (and IXF)IXF (and XF)XF (and P1)the other state of the art models. Since the agent applied here is the winner of ANAC 2012, it could be deduced that part of this success must have been due to the high accuracy of the opponent models’ module of this agent.As tabulated in Table 7, in average the POPPONENT (P1 variation) is the most accurate model vs. all opponents. Moreover, here, it is observed that for five levels of scenario features out of a total of nine, POPPONENT exhibited an excellent accuracy in comparison with all other top models, thus recommending the application of POPPONENT as the most accurate model in scenarios with medium or large size, medium distribution, and low or medium level of opposition.Both Experiments I and II reveal that POPPONENT is undoubtedly the most accurate model among all its counterparts. Besides this, as expressed in Tables C.1–C.7, POPPONENT achieves the highest performance in the Grocery domain for the average utility measure (through AP), the highest performance in the Travel and ItexVsCypress domains for the average time of agreements measure (through AP), the highest performance in the ItexVsCypress for the Nash and Kalai distance mea-sures (through AP), the highest performance in the Grocery domain for the Pareto distance measure (through AP), the best performance in the Supermarket, Travel, Thompson, Energy, Camera, and ItexVsCypress, for the Pearson Correlation measures (through P1), and the best performing model in the Thompson domain for the percentage of Pareto bids measure (through P1). These results are tabulated in Table 8.According to Tables C.1–C.7 and Table 8, it is observed that P1 has not performed well in terms of the measures, except for a rather intriguing performance value in the Pearson Correlation and a very high performance value in the Percentage of Pareto Bids. Although it is the most accurate model (high Pearson Correlation value) and is successful in making Pareto Bids (high Percentage of Pareto Bids value), it is not successful in obtaining favorable outcomes. This phenomenon can be observed for XF as well. However, AP (with lower Pearson Correlation and Percentage of Pareto Bids values) has been more successful in getting favorable utilities; thus, it could be concluded that making more Pareto Bids does not necessarily make an opponent 84F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91model successful. Rather, it seems that there is an optimal point in the Percentage of Pareto Bids measure, which guides the agent to achieve the highest performance.5.3.4. SummaryIn brief, by considering PP, POPPONENT is the most accurate model against predictable, unpredictable, random, conced-ing opponents with an offset, non-conceding, and all types of opponents, and also in terms of different levels of scenario features (size, opposition, distribution). Excluding PP, POPPONENT variations are the top performing models against all op-ponents, predictable opponents, and conceding opponents with an offset. POPPONENT variations rank first in the scenarios with medium opposition, medium distribution, and medium to large size. The results of Experiment II reveal that for all the measures, POPPONENT reaches the highest performance in at least one of the domains.6. ConclusionsNegotiation is the science and art of resolving disputes and reaching consensus between human parties. Automated bilateral multi issue negotiations are a special kind of traditional negotiations where two intelligent automated negotiating agents undertake the task of making an agreement on multiple issues on behalf of their human clients. Each automated negotiating agent strategy can be of the following subcomponents [1]: (1) Opponent Modeling, (2) Bidding Strategy, and (3) Acceptance Strategy. Thus, the negotiation strategy could be thought of as a larger component, which embodies the aforementioned subcomponents, and manages an efficient collaboration and interaction between them to accomplish the whole negotiation task. Opponent modeling techniques, as one of the constituent components of a negotiation strategy, are highly contributive in the success of a negotiating agent, in terms of both obtaining better individual utilities and achieving higher social welfare values. Therefore they are widely studied in the literature of Automated Negotiations.In this article, a new technique is proposed based on perceptron units, called POPPONENT, in order to model the pref-erences of the opponent in bilateral multi issue negotiations with linear utility functions. In fact, POPPONENT is a very successful implementation of a generalized version of the Standard Gradient Descent search algorithm (GD), referred to as the Multi Bipartite Gradient Descent search (MBGD) in a practical AI problem. The performance of the POPPONENT model is compared with the state of the art models from the Genius repository [36,37]. The POPPONENT is evaluated in 5 separate states which include the perfect information state (PP), the constant value states (with constant values of 0.6 (P6), 0.8 (P8), 1 (P1)), and the Adaptive state (AP). In the perfect information state, it is revealed that this outperforms the state of the art models in average accuracy by a large margin, when the accuracy is measured over all opponents including predictable (conceding opponents), unpredictable, random, conceding opponents with an offset, and non-conceding. It is found in the experiments that the accuracy of POPPONENT (in all the three constant value states) exceeds the accuracy of the most accurate state of the art model against all opponents in average and most other scenarios. Evaluating the performance of POPPONENT through Genius indicates that it overcomes the most accurate state of the art opponent models. The results indicate that POPPONENT overcomes all the state of the art models in at least one domain for all the performance/accuracy measures.In Experiment I, three constant values (together with perfect information) are applied to estimate the utility of a received bid in the opponent’s utility space. In Experiment II, an adaptive method is applied to estimate the bidding behavior of the opponent.It is believed that there is still a possibility for more improvements in accuracy and performance through adaptive methods. In the future, we plan to work further on the estimation of the opponent bids utilities, to design and implement highly efficient adaptive methods in order to estimate bid utilities through the history of bids which are received from the opponent in an incremental manner, and to evaluate the accuracy of this model while employing such adaptive methods. Another interesting future study direction would be the evaluation of this proposed model through different parameter values (i.e. learning rate, number of repeats, etc.). It is unknown how the accuracy and performance of this model is affected by the changes made to its parameters, and whether the model accuracy and performance could be further improved with proper parameter values. POPPONENT is a successful implementation of this new search method, with MBGD in the area of preference modeling automated negotiations. We are interested in the applicability of this new method to other practical areas of Artificial Intelligence.AcknowledgementsThe authors would like to warmly thank Dr. Tim Baarslag for providing the necessary scenario files for the experiments. The authors would also like to thank him for his compassion, and his continued support through the whole process of this research.Appendix A. Complexity of the POPPONENT algorithmComplexity analysis of this proposed POPPONENT algorithm is run easily. According to Equations (21) and (22), upon receiving a new bid from the opponent, POPPONENT applies delta rules to update the current estimated preference profile of the opponent. Let n represent the number of negotiation issues, and N represent the number of training repeats for each F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9185Table B.1Summary of the measures used in the experiments.Experiment IPearson correlationExperiment IIAvg. UtilityAvg. Nash DistanceAvg. Kalai DistanceAvg. Pareto Distancedp (uOP, u(cid:10)OP) =(cid:7)−→ω∈Ω (uOP ((cid:13)(cid:7)−→ω∈Ω (uOP (−→ω)−uOP )(u(cid:10)OP (−→ω∈Ω (u−→ω)−u(cid:10)OP ((cid:10)OP )−→ω)−u(cid:7)−→ω)−uOP )2(cid:10)OP )2AvgUtilityA=(cid:7)Ni=1 utilityiANAvgNashDist A,B=(cid:13)(cid:7)Ni=1 NashDistiNA,BNashDistiA,B=(utilityiA− NashUtilityiA )2 + (utilityiBAvgKalaiDist A,B=(cid:13)(cid:7)Ni=1 KalaiDistiNA,BKalaiDistiA,B=(utilityiA− KalaiUtilityiA )2 + (utilityiBAvgParetoDist A,B=(cid:13)(cid:7)Ni=1 ParetoDistiNA,B− NashUtilityiB )2− KalaiUtilityiB )2ParetoDistiA,B=(utilityiA− ParetoUtilityiA )2 + (utilityiB− ParetoUtilityiB )2Avg. TimeAvgTime =(cid:7)Ni=1 TimeOfAgreementiNAvg. Perc. of Pareto BidsAvgPercOfParetoBids =(cid:7)Ni=1( TotalParetoBidsiNTotalBidsOfferedi )training example; next, for each training instance, the delta rule in Equation (22) (Algorithm: line 18) repeats n times, the delta rule in Equation (21) (Algorithm: line 21) repeats n times, and the whole process is repeated N times for each training instance (Algorithm: line 15), hence, the total number of executions for the delta rule:ENDelta Rule = N(n + n) = 2Nn(A.1)Accordingly, the computational complexity of the algorithm only depends on the number of negotiation issues, therefore the computational complexity of the algorithm is linear (O (n)).Appendix B. MeasuresThis section explains different measures provided by ANAC in the Genius framework [38], applied here to assess the accuracy and performance of POPPONENT in both of the aforementioned experiments. These measures are tabulated in Table B.1 and explained in Sections B.1 and B.2.B.1. The measures applied in Experiment IIn the setting adopted in Experiment I (Section 5.1.1), the accuracy of this proposed model is evaluated together with other opponent models through a Pearson Correlation between the estimated and real bid utilities [38]:(cid:8)dpuOP, u(cid:9)(cid:10)OP=(cid:13)(cid:7)(cid:7)−→ω∈Ω (uOP(−→ω) − uOP)(u(cid:10)OP(−→ω∈Ω (uOP(−→ω) − uOP)2−→ω∈Ω (u(cid:7)−→ω) − u(cid:10)OP((cid:10)OP)−→ω) − u(cid:10)OP)2(B.1)where, uOP is the real utility space or preference profile of the opponent, uprofile of the opponent, uOP(is the estimated utility of the negotiation outcome −→ω in the opponent’s utility space.−→ω) is the real utility of the negotiation outcome −→ω in the opponent’s utility space, and u(cid:10)OP is the estimated utility space or preference −→ω)(cid:10)OP(Since this measure evaluates the extent to which a model can accurately predict the ranking of the bids in outcome space and not their exact utilities, it best fits our purpose. Awareness regarding the rankings of the bids in the outcome space is sufficient to make bids on the Pareto Frontier11 (or at least as close to Pareto Frontier as possible). It is worth noting that in order to make conceding moves in further approaching the Pareto Frontier, knowing the exact utility values of bids is non-essential. Instead, we only need to know the best bid among a number of bids with equal utilities for the 11 See footnote 7.86F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91agent (i.e., Iso-curve bids). If the best bid is known, a move can happen in a negotiation round, which would get us as close to the Pareto Frontier as possible.B.2. The measures applied in Experiment IIIn Experiment II (Section 5.1.2), the performance of this proposed model is evaluated by measuring the real performance of agents applying opponent models in real world experimental negotiation scenarios. For this purpose, the six performance measures [9], which are explained below are applied here:Average utility [12,17,64,65] of the agents in this designed setting is computed through Equation (B.1):(cid:7)AvgUtility A=Ni=1 utilityiNA(B.2)where N is the total number of sessions, AvgUtility A is the average utility of agent A in those N sessions, and utilityiutility of agent A in the ith session.A is the Average Nash distance of agreements [65,66] specifies the average distance to the Nash point12:(cid:7)AvgNashDist A,B=Ni=1 NashDistiNA,B(B.3)thus(cid:13)(cid:8)NashDistiA,B=utilityiA− NashUtilityiA(cid:8)(cid:9)2 +utilityiB− NashUtilityiB(cid:9)2where N is the total number of sessions, AvgNashDist A,B is the average Nash distance of agreements for agents A and Bin those N negotiation sessions, NashDistiA is the utility of the Nash point for agent A in the ith session, and NashUtilityiB is the utility of the Nash point for agent B in the ith session.A,B is the Nash distance of the agreement in the ith session, NashUtilityiThe Average Kalai distance of agreements [65,66] specifies the average distance to the Kalai point13:(cid:7)AvgKalaiDist A,B=Ni=1 KalaiDistiNA,B(B.4)so that,KalaiDistiA,B=(cid:13)(cid:8)utilityiA− KalaiUtilityiA(cid:8)(cid:9)2 +utilityiB− KalaiUtilityiB(cid:9)2where N is the total number of sessions, AvgKalaiDist A,B is the average Kalai distance of agreements for agents A and Bin those N negotiation sessions, KalaiDistiA is the utility of the Kalai point for agent A in the ith session, and KalaiUtilityiB is the utility of the Kalai point for agent B in the ith session.A,B is the Kalai distance of the agreement in the ith session, KalaiUtilityiAverage Pareto distance of agreements [17,65,66] specifies the average minimal distance of agreements from the Pareto Frontier:(cid:7)AvgParetoDist A,B=Ni=1 ParetoDistiNA,B(B.5)here,(cid:13)(cid:8)(cid:8)(cid:9)2 +(cid:9)2=A,ButilityiAParetoDisti− ParetoUtilityiAutilityiBwhere N is the total number of sessions, AvgParetoDist A,B is the average Pareto distance of agreements for agents A and Bin the N negotiation sessions, ParetoDistiA is the utility of the Pareto point for agent A in the ith session, and ParetoUtilityiB is the utility of the Pareto point for agent B in the ith session.A,B is the Pareto distance of the agreement in the ith session, ParetoUtilityi− ParetoUtilityiBThe Average time of agreement [1,65] specifies the average amount of time to reach an agreement:(cid:7)AvgTime =Ni=1 TimeOfAgreementiN(B.6)12 Nash: a unique point on Pareto Frontier at which the product of the utilities of both parties is maximized [43,56,59].13 Kalai–Smorodinsky: the point intersecting Pareto Frontier and the line linking Conflict point (0, 0) to Utopia Point (1, 1) [57,58].F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9187where N is the total number of sessions, AvgTime is the average time of agreements in those N sessions, and TimeOfAgreementi is amount of time elapsed before reaching an agreement in the i-th session.Average Percentage of Pareto Bids [36,62], specifies the average percentage of bids offered in a negotiation, which reside on the Pareto Frontier as Equation (B.7):AvgPercOfParetoBids =(cid:7)Ni=1( TotalParetoBidsiTotalBidsOfferedi )N(B.7)where N is the total number of sessions, TotalParetoBidsi and TotalBidsOfferedi are the total number of Pareto bids and the total number of bids that the agent has offered in the ith session.The Nash Distance, Kalai Distance, and Pareto Distance measures determine the social efficiency of the model; the lower these numbers, the more socially efficient the model in terms of these performance measures.Appendix C. Tabulated Experiment results IITable C.1POPPONENT (a) average utility, (b) standard deviation compared with that of the state-of-the art opponent models in different domains.DomainLGFCKFSFXFHHFIXFAPSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverage0.5735090.6475010.5387190.6960390.2973340.6478620.5142220.5593120.5689590.6495890.5383280.6972890.2992390.6472050.5139110.5592170.5696480.6496050.5409290.6970680.300330.6475220.5099940.5593DomainLGFCKFSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverage0.0213687670.0112876490.0214941880.0032723820.0151296710.0050673260.0215136130.0141619420.0250837180.0155701610.0182362180.0022987870.014434950.0052884280.0165429510.013922174SF0.0253620.019210.0237430.0031680.0144470.0042660.0193170.0156450.5860.6389580.5281380.6912760.2941040.6474750.5021880.555448(a)XF0.0233890.0220770.0233430.004110.0167170.0071610.0205310.016761(b)0.5656950.6519740.5375640.6957580.301180.6457140.5096970.5582260.5808670.6567950.5413210.6984180.3006020.6448660.5168570.5628180.5710502960.6504981720.5351375740.6987184390.2971274660.6460401190.5119439720.558645148HHFIXFAP0.0262930.0168140.0200750.0023250.0127610.005690.0170270.0144270.0264380.0203380.0190650.0027780.0134590.0051160.0203970.015370.0282047620.0191923850.0235710660.0022560750.0175964070.0062883190.0178748460.016426266P10.5691390.638120.5159080.6898380.2950670.6429420.5121190.551876P10.0277570.0191240.0239230.0061590.0148090.006330.0202250.016904Table C.2POPPONENT (a) average time of agreement, (b) standard deviation compared with that of the state of the art opponent models in different domains.DomainLGFCKFSFXFHHFIXFAPSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverageDomainSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverage0.8087240.760540.8355280.6081070.7733510.6373470.8508590.753494LGF0.0073240.0059820.0063360.0062040.0107440.0097880.0056680.0074350.8101610.7599060.8351660.6090990.7743350.6395230.8512920.754212CKF0.0077260.0067180.0057190.006620.0093760.0088420.0042430.0070350.8095340.7597730.836460.6077770.7732010.6405860.8514570.754113SF0.0077780.0062450.0059780.0057220.0104040.0073870.0042350.0068220.8040340.7589980.8386320.6182180.7789950.6408410.8525340.756036(a)XF0.0061910.0073090.0068090.0078250.0101180.0097650.0049820.007571(b)0.808660.759410.8384020.6111780.772640.6390890.8511070.7543550.8069340.7593640.8371650.6124350.7725880.6414490.8502140.7543070.8064249360.7579255060.8361017680.6149785130.7801736720.6418220830.8492010950.755232511HHF0.0069070.0063220.0060680.0064490.0096390.009780.0048770.007149IXF0.0070420.0066930.0062180.0061730.0118470.0081930.0037270.007128AP0.0061460.007030.005740.0051650.0122910.0105820.0048490.0074P10.8078540.7600310.8390770.6199990.7811780.6462370.8496060.757712P10.0057470.0054440.0062330.0060890.009840.0094510.0042390.0067288F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91Table C.3POPPONENT (a) average Nash Distance, (b) standard deviation compared with that of the state of the art opponent models in different domains.DomainLGFCKFSFXFHHFIXFAPSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverageDomainSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverage0.44280.395980.4573660.1956030.4271810.1955730.4375580.36458LGF0.0250640.0156790.0270380.0048420.0303150.0068460.029080.0198380.4491040.3897990.4560090.1945970.4248180.1938790.4398730.364011CKF0.030040.021390.024370.0031240.0288860.007250.0220340.0195850.4473910.3901450.4579190.1955510.4249030.1941740.4436110.364814SF0.0306870.0248040.0287230.0046460.0280730.0055270.0250280.021070.4369570.4097630.4801580.2091790.4316430.2028290.4622620.376113(a)XF0.0281390.0284050.0295270.0060040.0361760.0093970.0265970.023464(b)0.4544140.3884110.4611350.199010.419750.2014050.4427420.3666950.4356190.3819270.4577730.1947920.4192630.2033920.4392080.3617110.4469680280.3964828240.4658703840.1968301370.4246124680.2000304980.4372237550.366859728HHF0.0311510.0232340.0248180.0034580.0259670.008190.0219750.019828IXF0.0325860.0265970.0238940.0041930.0262340.0070340.02530.020834AP0.033960.0249610.0293720.0036130.0353260.0084690.022960.022666Table C.4POPPONENT (a) average Pareto Distance, (b) standard deviation compared with that of the state of the art opponent models in different domains.DomainLGFCKFSFXFHHFIXFAPSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverageDomainSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverage0.1872340.1668090.1902550.0490980.245040.0185990.1681170.14645LGF0.0212190.0115410.0242430.002050.0256250.000420.0231750.0154680.1969620.1610850.1917630.0478640.239550.0193410.1697870.146622CKF0.0264040.0161710.0191440.0003950.0250890.0015110.0169760.0150990.1952590.1608080.1897580.0486420.2385790.0187320.1744840.146609SF0.0261570.0198360.0273770.001890.0249710.0002670.0205160.0172880.175870.1757920.2042190.059760.24820.0209880.1865010.153047(a)XF0.0246010.0201060.0256870.0024530.0320850.0030720.0211040.018444(b)0.2029490.1590090.1909130.0510050.2349170.021420.1742470.147780.1822630.153250.1892650.0484150.235160.0235090.1666010.1426370.191709050.1631447610.1963552870.047828860.2378924090.0216368590.1724475190.147287821HHF0.0282040.0169960.0197480.0004740.0216640.0009980.0177710.015122IXF0.0276740.0210530.018790.0007860.0204350.0012440.0215370.015931AP0.0314290.0196620.0241680.000430.0288610.0010620.0183150.017704Table C.5POPPONENT (a) average Kalai Distance, (b) standard deviation compared with that of the state of the art opponent models in different domains.DomainLGFCKFSFXFHHFIXFAPSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverage0.4153580.395980.4573660.1956030.4261420.1898140.4375580.3596890.4227110.3897990.4560090.1945970.4237330.1892490.4398730.3594250.4222810.3901450.4579190.1955510.4237050.189790.4436110.3604290.4139410.4097630.4801580.2091790.430420.19740.4622620.371875(a)0.4288220.3884110.4611350.199010.4184110.1949160.4427420.3619210.410370.3819270.4577730.1947920.4180650.1969880.4392080.3570180.4247575480.3964828240.4658703840.1968301370.423091110.1960601750.4372237550.362902276P10.4681080.4206870.5156480.2152730.4398930.2116670.4615180.390399P10.0330230.0230430.030460.0091060.0304040.0085250.0247370.022757P10.1990270.179020.2285920.0630340.2467910.0282430.180980.160812P10.0305290.017640.0240880.0061470.0229820.0020880.0209950.017781P10.4464630.4206870.5156480.2152730.4384330.2057570.4615180.446463F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9189Table C.5 (continued)DomainSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverageLGF0.0255490.0156790.0270380.0048420.0301460.005360.029080.019671CKF0.0292850.021390.024370.0031240.0288890.005940.0220340.01929SF0.0301560.0248040.0287230.0046460.0280210.004250.0250280.020804XF0.0274010.0284050.0295270.0060040.0361010.0074970.0265970.023076(b)HHF0.0305560.0232340.0248180.0034580.0258650.0067670.0219750.019525IXF0.0322660.0265970.0238940.0041930.0260840.0063770.02530.020673AP0.0333890.0249610.0293720.0036130.0351410.0068080.022960.022321P10.0321310.0230430.030460.0091060.0303980.0070480.0247370.032131Table C.6POPPONENT (a) average Pearson Correlation of Bids, (b) standard deviation compared with that of the state of the art opponent models in different domains.DomainLGFCKFSFXFHHFIXFAPSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverage0.7082620.5862560.576810.7990050.6915010.8017990.76850.704590.7585020.6083880.6770930.8246150.7354250.8053550.8078520.7453190.7413860.6238080.6041070.8252720.7103310.8071660.7688020.7258390.720820.5731110.636780.7967310.675370.6447010.7607160.68689(a)0.7324250.6218010.6077540.8374420.7004080.8220570.7602330.7260170.7303470.6134230.6050260.8490780.6951290.8162160.751480.7229570.8217745770.6841659350.6807359770.8471876270.6802667470.8214614180.8624967430.771155575DomainLGFCKFSFXFHHFIXFAPSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverage0.0032490.0031410.002410.0014680.007960.0027290.0022190.0033110.0023860.0029530.0021720.0016040.0036550.0020560.0015540.002340.0030460.0031040.0026950.0014850.006660.0019140.0017730.0029540.0134250.0148340.011010.00480.0150970.0094810.0084290.011011(b)0.0086490.0090440.0082880.0036520.0099440.0082390.0076470.0079230.0110860.0118570.0090240.0053770.0141620.0097590.0123030.010510.0123844320.008964040.0209416020.0029674730.0309669490.0058691840.0128368450.013561504P10.8534680.6994240.7658180.8420550.8521730.8424270.8924690.821119P10.0052360.0050790.0042330.0033010.0049150.0035110.0037180.004285Table C.7POPPONENT (a) average Percentage of Pareto Bids, (b) standard deviation compared with that of the state of the art opponent models in different domains.DomainSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverageDomainSupermarketTravelThompsonGroceryEnergyCameraItexVsCypressAverageLGF0.3948250.3733310.6449550.6984180.3784520.7388290.8126330.577349LGF0.0214320.0173610.0140050.0088980.0262580.0108740.0071820.015144CKF0.3958040.3726820.6502020.6963470.3755720.7330520.8132820.576706CKF0.0233840.0222110.0116950.0101650.0215650.0113280.0054270.015111SF0.3984580.3728520.6490680.6967480.3864530.7337890.8130510.578631SF0.0214730.0190320.0118860.0089920.0247160.0103190.0055120.014561XF0.4624660.3760190.6473090.6892480.4402110.71230.8188360.592341(a)XF0.0277820.0230110.0138290.0109190.0271320.0112610.0070920.017289(b)HHF0.4001530.3719450.6469740.6870820.3945580.7355780.8138480.578591HHF0.0230610.0213390.0126290.0106220.0224330.0122540.0072570.015656IXF0.4068190.3712990.6483220.6893350.3966350.7384230.8196940.581504IXF0.0242890.0206960.0127910.011380.0214970.0126130.0043410.015373AP0.4151590.3708990.6618950.6950650.3905690.723820.813040.581492AP0.0216180.0221120.0135030.0075560.0207320.0151860.0069290.015376P10.4177630.3725230.6643350.6951240.4025410.7264470.8148070.584791P10.0240820.0224960.0132690.0079540.0236110.0129130.0055120.01569190F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–91References[1] T. Baarslag, K. Hindriks, M. Hendrikx, A. Dirkzwager, C. Jonker, Decoupling negotiating agents to explore the space of negotiation strategies, in: Pro-ceedings of the 5th International Workshop on Agent-Based Complex Automated Negotiations, ACAN, 2012.[2] T. Baarslag, K. Hindriks, C. Jonker, S. Kraus, R. Lin, The first automated negotiating agents competition (ANAC 2010), in: T. Ito, M. Zhang, V. Robu, S. Fatima, T. Matsuo (Eds.), New Trends in Agent-Based Complex Automated Negotiations, vol. 383, Springer, Berlin, Heidelberg, 2012, pp. 113–135.[3] K. Fujita, T. Ito, T. Baarslag, K. Hindriks, C. Jonker, S. Kraus, et al., The second automated negotiating agents competition (ANAC2011), in: T. Ito, M. Zhang, V. Robu, T. Matsuo (Eds.), Complex Automated Negotiations: Theories, Models, and Software Competitions, vol. 435, Springer, Berlin, Heidelberg, 2013, pp. 183–197.[4] T. Baarslag, K. Hindriks, C. Jonker, Acceptance conditions in automated negotiation, in: T. Ito, M. Zhang, V. Robu, T. Matsuo (Eds.), Complex Automated Negotiations: Theories, Models, and Software Competitions, vol. 435, Springer, Berlin, Heidelberg, 2013, pp. 95–111.[5] T. Baarslag, K. Hindriks, C. Jonker, Effective acceptance conditions in real-time automated negotiation, Decis. Support Syst. 60 (2014) 68–77.[6] T. Baarslag, K.V. Hindriks, Accepting optimally in automated negotiation with incomplete information, presented at the in: Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems, St. Paul, MN, USA, 2013.[7] T. Baarslag, M.C. Hendrikx, K. Hindriks, C. Jonker, Learning about the opponent in automated bilateral negotiation: a comprehensive survey of opponent modeling techniques, Auton. Agents Multi-Agent Syst. (2015) 1–50, 2015/09/07.[8] S.Z. Li, A.K. Jain, Encyclopedia of Biometrics: I–Z, vol. 2, Springer, 2009.[9] T. Baarslag, M. Hendrikx, K. Hindriks, C. Jonker, Measuring the performance of online opponent models in automated bilateral negotiation, in: M. Thielscher, D. Zhang (Eds.), AI 2012: Advances in Artificial Intelligence, vol. 7691, Springer, Berlin, Heidelberg, 2012, pp. 1–14.[10] T. Krimpen, D. Looije, S. Hajizadeh, HardHeaded, in: T. Ito, M. Zhang, V. Robu, T. Matsuo (Eds.), Complex Automated Negotiations: Theories, Models, and Software Competitions, vol. 435, Springer, Berlin, Heidelberg, 2013, pp. 223–227.[11] N. Galen Last, Agent Smith: opponent model estimation in bilateral multi-issue negotiation, in: T. Ito, M. Zhang, V. Robu, S. Fatima, T. Matsuo (Eds.), New Trends in Agent-Based Complex Automated Negotiations, vol. 383, Springer, Berlin, Heidelberg, 2012, pp. 167–174.[12] K. Hindriks, D. Tykhonov, Opponent modelling in automated multi-issue negotiation using Bayesian learning, presented at the in: Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems, Estoril, Portugal, 2008.[13] A. Frieder, G. Miller, Value model agent: a novel preference profiler for negotiation with agents, in: T. Ito, M. Zhang, V. Robu, T. Matsuo (Eds.), Complex Automated Negotiations: Theories, Models, and Software Competitions, vol. 435, Springer, Berlin, Heidelberg, 2013, pp. 199–203.[14] H. Jazayeriy, M. Azmi-Murad, N. Sulaiman, N. Izura Udizir, The learning of an opponent’s approximate preferences in bilateral automated negotiation, J. Theor. Appl. Electron. Commer. Res. 6 (2011) 65–84.[15] J. Zhang, F. Ren, M. Zhang, Prediction of the opponent’s preference in bilateral multi-issue negotiation through Bayesian learning, in: Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems, Paris, France, 2014, pp. 331–338.[16] C. Williams, V. Robu, E. Gerding, N. Jennings, IAMhaggler: a negotiation agent for complex environments, in: T. Ito, M. Zhang, V. Robu, S. Fatima, T. Matsuo (Eds.), New Trends in Agent-Based Complex Automated Negotiations, vol. 383, Springer, Berlin, Heidelberg, 2012, pp. 151–158.[17] T. Baarslag, K. Fujita, E.H. Gerding, K. Hindriks, T. Ito, N.R. Jennings, et al., Evaluating practical negotiating agents: results and analysis of the 2011 international competition, Artif. Intell. 198 (2013) 73–103.[18] K. Sycara, D. Zeng, Benefits of learning in negotiation, in: Proceedings of the 14th National Conference on Artificial Intelligence and 9th Innovative Applications of Artificial Intelligence Conference, AAAI-97/IAAI-97, Menlo Park, CA, USA, 1997, pp. 36–42.[19] J. Gwak, K.M. Sim, Bayesian learning based negotiation agents for supporting negotiation with incomplete information, in: Proceedings of the Interna-tional MultiConference of Engineers and Computer Scientists, Hong Kong, 2011, pp. 163–168.[20] C. Yu, F. Ren, M. Zhang, An adaptive bilateral negotiation model based on Bayesian learning, in: T. Ito, M. Zhang, V. Robu, T. Matsuo (Eds.), Complex Automated Negotiations: Theories, Models, and Software Competitions, vol. 435, Springer, Berlin, Heidelberg, 2013, pp. 75–93.[21] P. Faratin, C. Sierra, N.R. Jennings, Using similarity criteria to make issue trade-offs in automated negotiations, Artif. Intell. 142 (2002) 205–237.[22] C. Niemann, F. Lang, Assess your opponent: a Bayesian process for preference observation in multi-attribute negotiations, in: T. Ito, M. Zhang, V. Robu, S. Fatima, T. Matsuo (Eds.), Advances in Agent-Based Complex Automated Negotiations, vol. 233, Springer, Berlin, Heidelberg, 2009, pp. 119–137.[23] R. Lin, S. Kraus, J. Wilkenfeld, J. Barry, Negotiating with bounded rational agents in environments with incomplete information using an automated agent, Artif. Intell. 172 (2008) 823–851.[24] T. Baarslag, K. Hindriks, C. Jonker, A Tit for Tat negotiation strategy for real-time bilateral negotiations, in: T. Ito, M. Zhang, V. Robu, T. Matsuo (Eds.), Complex Automated Negotiations: Theories, Models, and Software Competitions, vol. 435, Springer, Berlin, Heidelberg, 2013, pp. 229–233.[25] R. Lin, S. Kraus, J. Wilkenfeld, J. Barry, An automated agent for bilateral negotiation with bounded rational agents with incomplete information, Front. Artif. Intell. Appl. 141 (2006) 270–274.[26] L. ¸Serban, G. Silaghi, C. Litan, AgentFSEGA: time constrained reasoning model for bilateral multi-issue negotiations, in: T. Ito, M. Zhang, V. Robu, S. Fatima, T. Matsuo (Eds.), New Trends in Agent-Based Complex Automated Negotiations, vol. 383, Springer, Berlin, Heidelberg, 2012, pp. 159–165.[27] C. Williams, V. Robu, E. Gerding, N. Jennings, IAMhaggler2011: a Gaussian process regression based negotiation agent, in: T. Ito, M. Zhang, V. Robu, T. Matsuo (Eds.), Complex Automated Negotiations: Theories, Models, and Software Competitions, vol. 435, Springer, Berlin, Heidelberg, 2013, pp. 209–212.[28] R. Carbonneau, G.E. Kersten, R. Vahidov, Predicting opponent’s moves in electronic negotiations using neural networks, Expert Syst. Appl. 34 (2008) 1266–1273.[29] C.R. Williams, V. Robu, E.H. Gerding, N.R. Jennings, Using Gaussian processes to optimise concession in complex negotiations against unknown op-ponents, presented at the in: Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, 2011.[30] R.Y.K. Lau, Y. Li, D. Song, R.C.W. Kwok, Knowledge discovery for adaptive negotiation agents in e-marketplaces, Decis. Support Syst. 45 (2008) 310–323.[31] R. Aydo˘gan, P. Yolum, Learning opponent’s preferences for effective negotiation: an approach based on concept learning, Auton. Agents Multi-Agent Syst. 24 (2012) 104–140.mann, 1986.[32] J.R. Anderson, R.S. Michalski, R.S. Michalski, J.G. Carbonell, T.M. Mitchell, Machine Learning: An Artificial Intelligence Approach, vol. 2, Morgan Kauf-[33] P. Faratin, C. Sierra, N.R. Jennings, Negotiation decision functions for autonomous agents, Robot. Auton. Syst. 24 (1998) 159–182.[34] T.M. Mitchell, Machine Learning, vol. 45, McGraw Hill, Ridge, IL, 1997.[35] S.S. Fatima, M. Wooldridge, N. Jennings, Optimal negotiation strategies for agents with incomplete information, in: J.-J. Meyer, M. Tambe (Eds.), Intelli-gent Agents VIII, vol. 2333, Springer, Berlin, Heidelberg, 2002, pp. 377–392.[36] R. Lin, S. Kraus, T. Baarslag, D. Tykhonov, K. Hindriks, C.M. Jonker, Genius: an integrated environment for supporting the design of generic automated negotiators, Comput. Intell. 30 (2012) 48–70.[37] K. Hindriks, C.M. Jonker, S. Kraus, R. Lin, D. Tykhonov, Genius: negotiation environment for heterogeneous agents, presented at the in: Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems, Budapest, Hungary, 2009.[38] T. Baarslag, M. Hendrikx, K. Hindriks, C. Jonker, Predicting the performance of opponent models in automated negotiation, in: Proceedings of the 7th International Conference on Advanced Information Technologies, St. Paul, MN, USA, 2013, pp. 59–66.F. Zafari, F. Nassiri-Mofakham / Artificial Intelligence 237 (2016) 59–9191[39] J. Hao, H.-f. Leung, CUHKAgent: an adaptive negotiation strategy for bilateral negotiations over multiple items, in: I. Marsa-Maestre, M.A. Lopez-Carmona, T. Ito, M. Zhang, Q. Bai, K. Fujita (Eds.), Novel Insights in Agent-Based Complex Automated Negotiation, vol. 535, Springer, Japan, 2014, pp. 171–179.[40] C. Williams, V. Robu, E. Gerding, N. Jennings, An overview of the results and insights from the third automated negotiating agents competition (ANAC2012), in: I. Marsa-Maestre, M.A. Lopez-Carmona, T. Ito, M. Zhang, Q. Bai, K. Fujita (Eds.), Novel Insights in Agent-Based Complex Automated Negotiation, vol. 535, Springer, Japan, 2014, pp. 151–162.[41] F. Zafari, F. Nassiri-Mofakham, BraveCat: iterative deepening distance-based opponent modeling and hybrid bidding in nonlinear ultra large bilateral multi issue negotiation domains, in: N. Fukuta, T. Ito, M. Zhang, K. Fujita, V. Robu (Eds.), Recent Advances in Agent-Based Complex Automated Negoti-ation, vol. 638, Springer International Publishing, Switzerland, 2016, pp. 285–293.[42] A. Rubinstein, Perfect equilibrium in a bargaining model, Econometrica: J. Econom. Soc. 50 (1982) 97–109.[43] G. Weiss, Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence, MIT Press, 1999.[44] H. Raiffa, The Art and Science of Negotiation, Harvard University Press, 1982.[45] F. Nassiri-Mofakham, M.A. Nematbakhsh, N. Ghasem-Aghaee, A. Baraani-Dastjerdi, A heuristic personality-based bilateral multi-issue bargaining model in electronic commerce, Int. J. Hum.-Comput. Stud. 67 (2009) 1–35.[46] F. Nassiri-Mofakham, N. Ghasem-Aghaee, M.A. Nematbakhsh, A. Baraani-Dastjerdi, A personality-based simulation of bargaining in e-commerce, Simul. Gaming 39 (2008) 83–100.[47] F. Nassiri-Mofakham, M.A. Nematbakhsh, A. Baraani-Dastjerdi, N. Ghasem-Aghaee, R. Kowalczyk, Bidding strategy for agents in multi-attribute combi-natorial double auction, Expert Syst. Appl. 42 (2015) 3268–3295.[48] E. Alpaydin, Introduction to Machine Learning, MIT Press, 2004.[49] D.T. Larose, Discovering Knowledge in Data: An Introduction to Data Mining, John Wiley & Sons, 2014.[50] I.H. Witten, E. Frank, Data Mining: Practical Machine Learning Tools and Techniques, Morgan Kaufmann, 2005.[51] S. Kawaguchi, K. Fujita, T. Ito, AgentK: compromising strategy based on estimated maximum utility for automated negotiating agents, in: T. Ito, M. Zhang, V. Robu, S. Fatima, T. Matsuo (Eds.), New Trends in Agent-Based Complex Automated Negotiations, vol. 383, Springer, Berlin, Heidelberg, 2012, pp. 137–144.[52] S. Kawaguchi, K. Fujita, T. Ito, AgentK2: compromising strategy based on estimated maximum utility for automated negotiating agents, in: T. Ito, M. Zhang, V. Robu, T. Matsuo (Eds.), Complex Automated Negotiations: Theories, Models, and Software Competitions, vol. 435, Springer, Berlin, Heidelberg, 2013, pp. 235–241.[53] S. Kawaguchi, K. Fujita, T. Ito, Compromising strategy based on estimated maximum utility for automated negotiation agents competition (ANAC-10), in: K. Mehrotra, C. Mohan, J. Oh, P. Varshney, M. Ali (Eds.), Modern Approaches in Applied Intelligence, vol. 6704, Springer, Berlin, Heidelberg, 2011, pp. 501–510.[54] M. Ikrashi, K. Fujita, Compromising strategy using weighted counting in multi-times negotiations, in: 2014 IIAI 3rd International Conference on Ad-[55] K. Fujita, Efficient strategy adaptation for complex multi-times bilateral negotiations, in: 2014 IEEE 7th International Conference on Service-Oriented vanced Applied Informatics, IIAIAAI, 2014, pp. 453–458.Computing and Applications, SOCA, 2014, pp. 207–214.[56] M. Fasli, Agent Technology for E-Commerce, Wiley Press, Chichester, 2007.[57] H. Gimpel, J. Mäkiö, Towards multi-attribute double auctions for financial markets, EM 16 (2006) 130–139.[58] E. Kalai, M. Smorodinsky, Other solutions to Nash’s bargaining problem, Econometrica: J. Econom. Soc. (1975) 513–518.[59] J.F. Nash Jr., The bargaining problem, Econometrica: J. Econom. Soc. 18 (1950) 155–162.[60] B. An, V. Lesser, Yushu: a heuristic-based agent for automated negotiating competition, in: T. Ito, M. Zhang, V. Robu, S. Fatima, T. Matsuo (Eds.), New Trends in Agent-Based Complex Automated Negotiations, vol. 383, Springer, Berlin, Heidelberg, 2012, pp. 145–149.[61] M. Ben Adar, N. Sofy, A. Elimelech, Gahboninho: strategy for balancing pressure and compromise in automated negotiation, in: T. Ito, M. Zhang, V. Robu, T. Matsuo (Eds.), Complex Automated Negotiations: Theories, Models, and Software Competitions, vol. 435, Springer, Berlin, Heidelberg, 2013, pp. 205–208.[62] K. Gal, L. Ilany, The fourth automated negotiation competition, in: K. Fujita, T. Ito, M. Zhang, V. Robu (Eds.), Next Frontier in Agent-Based Complex Automated Negotiation, vol. 596, Springer, Japan, 2015, pp. 129–136.[63] L. Thompson, The Heart and Mind of the Negotiator, 3rd ed., Prentice Hall, Upper Saddle River, New Jersey, 2000.[64] T. Klos, K. Somefun, H. La Poutré, Automated interactive sales processes, IEEE Intell. Syst. 26 (2011) 54–61.[65] F. Zafari, F. Nassiri-Mofakham, A.Z. Hamadani, DOPPONENT: a socially efficient preference model of opponent in bilateral multi issue negotiations, J. Comput. Secur. 1 (4) (2015) 283–292.[66] K. Hindriks, C.M. Jonker, D. Tykhonov, Negotiation dynamics: analysis, concession tactics, and outcomes, presented at the in: Proceedings of the 2007 IEEE/WIC/ACM International Conference on Intelligent Agent Technology, Silicon Valley, San Francisco, CA, USA, 2007.