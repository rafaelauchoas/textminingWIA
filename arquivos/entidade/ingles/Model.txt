ELSEVIER Artificial Intelligence 100 ( 1998) 177-224 Artificial Intelligence Model-based average reward reinforcement learning * Prasad Tadepalli ‘,*, DoKyeong Ok b*2 ’ Department of Computer Science, Oregon State University, Corvallis, OR 97331, USA ’ Korean Army Computer Center, NonSanSi DuMaMyeon BuNamRi, i?O.Box 29, ChungNam 320-919, South Korea Received 27 September 1996; revised 15 December 1997 Abstract from (RL) Reinforcement that improve their performance in many domains, than its discounted the natural criterion is the study of programs the environment. Most RL methods optimize Learning rewards and punishments by the dis- receiving is to total reward received by an agent, while, counted optimize the average reward per time step. In this paper, we introduce a model-based Average- reward Reinforcement Learning method called H-learning and show that it converges more quickly in the domain of scheduling a simulated Automatic and robustly Guided Vehicle explores to the the unexplored parts of the state space, while always choosing greedy actions with respect current value function. We show that this “Auto-exploratory H-Learning” performs better than the previously to larger state spaces, we extend it to learn action models and reward functions in the form of dynamic Bayesian networks, and approximate are effective faster in some AGV scheduling its value function using local linear regression. We show that both of these extensions it converge in significantly and making tasks. @ 1998 Published by Elsevier Science B.V. (AGV). We also introduce a version of H-learning strategies. To scale H-learning studied exploration that automatically counterpart of H-learning requirement the space reducing Keywords: Machine networks; Linear regression; AGV scheduling learning; Reinforcement learning; Average reward; Model-based; Exploration; Bayesian author. Email: * Corresponding ’ Most of the work was done when both the authors were at Oregon State University. ’ Email: okdo@unitel.co.kr. tadepall@cs.orst.edu. 0004-3702/98/$19.00 PII SOOO4-3702(98)00002-2 @ 1998 Published by Elsevier Science B.V. All rights reserved. 178 P: 7hdepulli. D. Ok/Art$ificial Intelligence 100 (1998) 177-224 1. Introduction receives equivalent scheduling the learner is the study of programs rewards and punishments Reinforcement Learning (EU) task by receiving that improve their perfor- from the environment. RL tasks, for many and elevator scheduling optimization as money learning of good procedures including Q-learning [ 31, optimize in automatic tasks such as job-shop learning, (ARTDP) [ 181. In other words, a reward mance at some has been quite successful including some real-world [ 1 1,44,47]. Most approaches to reinforcement and Adaptive Real-Time Dynamic Programming counted reward after one time step is considered immediately. Discounted ward can be interpreted situation run will be terminated in which we would etary aspect or the probability in which we are is the average counted the average ward action sequences one. [ 461 the total dis- that is received to a fraction of the same reward received in which re- time step. Another that the at any given time for whatever reason. However, many domains the mon- in the time scales in such domains received per time step. Even so, many people have used dis- to optimize learning total re- two such the better algorithms reason is that the discounted sequence of actions and rewards. Hence, to choose to use reinforcement of immediate in. The natural in such domains, while aiming to do this criterion that can earn is when from a state can be compared by this criterion is finite even for an infinite learning do not have either at least to optimize termination, criterion is motivated by domains is a fixed probability that it is well-suited interest there [ 21,261. One reinforcement to model interested in each reward reward like with encourages the learner factor, discounting While mathematically convenient, in domains where these isn’t a natural interpretation long-term benefits reward decreases optimization when average-reward to sacrifice for the discount for short-term gains, since the impact of an action choice on long-term exponentially time. Hence, using discounted optimization be argued optimizes to 1 [ 21,261. This raises appropriate it can if that also nearly close the question whether and when discounted RL methods are that it is appropriate the average lead to suboptimal policies. Nevertheless, reward by using a discount is what is required could to optimize discounted the average reward. to use to optimize is sufficiently factor which total reward version the short-term is an undiscounted [ 31. We compare H-learning with In this paper, we describe an Average-reward RL (ARL) method called H-learning, Programming of Adaptive Real-Time Dynamic its discounted a simulated Automatic Guided Vehicle which (ARTDP) the task of scheduling dling robot used in manufacturing. Our results show that H-learning ARTDP when icy also optimizes are different, ARTDP either fails to converge converges counterpart, ARTDP, in (AGV) , a material han- is competitive with factor) optimal pol- the average reward. When short-term and long-term optimal policies policy or factor is high. Like ARTDP, and unlike Schwartz’s R-learning is model-based, [ 381, H-learning models. We show that in the AGV scheduling domain H-learning steps than R-learning [37] and Singh’s ARL algorithms in that it learns and uses explicit action and reward in fewer and is competitive with it in CPU time. This is consistent with the (discounted with a small discount to the optimal average-reward if the discount too slowly converges I? Tadepdli, D. Ok/Artificiul Intelligence 100 (1998) 177-224 179 previous RL [3,28]. results on the comparisons between model-based and model-free discounted Like most other RL methods, H-learning needs exploration to find an optimal policy. including (recency-based) [45]. Other meth- random actions, preferring (IE) method of Kaelbling strategies have been studied that are least recently executed occasionally in RL, to visit states that are least visited (counter-based) of reward functions used by Koenig and Simmons under uncertainty” A number of exploration executing or executing actions ods such as the Interval Estimation the idea incorporate representation the value function of states of “optimism with high values, and gradually decreasing to explore automatically, while always executing greedy actions. We introduce a version of H-learning exploring respect converges more quickly random, counter-based, to the current value function. We show that this “Auto-exploratory H-learning” to H-learning under strategies. the unexplored parts of the state space while always taking a greedy action with to a better average reward when compared and Boltzmann recency-based, in which the value function that uses optimism under uncertainty, and has the property of automatically them, these methods encourage [ 17,201. By initializing and the action-penalty ARL methods exploration the learner is stored as a table require to scale to large state spaces. Model-based methods too much space like H-learning time and training also have the additional the reward essential problem of having to explicitly functions, which is space-intensive. To scale ARL in a more compact store their action models and it is to such domains, form. the to design in such a these networks to fully specify the domain models. the to approximate its action models and value function Dynamic Bayesian networks have been successfully used in the past to represent [ 12,351. In many cases, it is possible action models way that a small number of parameters are sufficient We extended H-learning conditional probabilities for our method but also increases the action models dominates the learning time. so that it takes the network structure as input and learns in the network. This not only reduces the space requirements in domains where learning the speed of convergence learning is piecewise reinforcement of H-learning tasks, especially the performance linear. To take advantage of this, we implemented combines with learning of Bayesian network-based that can be described by small Bayesian networks, have a uniform those with action models and reward Many reward functions the optimal value func- structure over large regions of the state space. In such domains, tion of H-learning a value function approximation method based on local linear regression. Local linear regression action models and synergistically improves tasks. Combining Auto-exploratory H-learning with action model and value function approximation to even faster convergence The rest of the paper leads learning method. as follows: Section 2 introduces Markov Deci- that form the basis for RL, and motivates Average-reward RL. Section 3 it with ARTDP and R-learning sion Problems introduces H-learning and compares ing task. Section 4 introduces Auto-exploratory H-learning, previously Bayesian networks and local linear regression value function issues, and Section 7 is a summary. in an AGV schedul- it with some the use of dynamic the action models and the respectively. Section 6 is a discussion of related work and future research in some domains, producing a very effective is organized schemes. Section 5 demonstrates in many AGV scheduling studied exploration to approximate and compares 180 P 72uiepdii. D. Ok/Artijicitrl Irltelligencr 100 (1998) 177-224 2. Background We assume that are applicable is described by a discrete that the learner’s environment is modeled by a Markov Decision Process (MDP). An MDP set S of n states, and a discrete set of i are denoted by U(i) and actions, A. The set of actions are called admissible. The Markovian assumption means that an action u in a given state i E S results in state j with some fixed probability P;,,i (u). There is a finite immediate reward r;,,;(u) a sequence of discrete steps. A policy p is a mapping p(i) E U(i). We only consider policies “stationary policies”. in state j. Time is treated as from states to actions, such that that do not change over time, which are called for executing an action u in state i resulting in a state 2. I. Discounted reinforcement learning Suppose is stochastic, accumulating rp(so, is the expected value of this variable, that an agent using a policy p goes through states SO,. . . , st in time 0 thru a total reward rp(so, t) is a random variable. Hence, one candidate to total reward received t starting time i.e., as t tends to co, this sum can be unbounded. The discounted RL methods factor y, where total reward given t, with some probability, If the environment optimize in time horizon, it finite by multiplying make 0 < y < 1. In other words, by: from SO, E( rp( SO, t)). Unfortunately, however, over an infinite the expected discounted reward by a discount i.e., the expected each successive rsk,sk,, (,u(sk)). they optimize t) = cL:i ffi(sO) =~~~E(Cykr,,,.,,,,(~(Sk))). I-1 k=O (1) It is known that there exists an optimal discounted policy p* the above value function over all starting states SO and policies pu. It can be shown to satisfy the following that maximizes recurrence [ 3,6] : relation (2) relation by side of the f'"*(i) =u~;;i{ri(u) +r~pi..j(~)P*(~)}. ./ES Real-Time Dynamic Programming (RTDP) solves the above recurrence i in each step by the right-hand state the value of the current updating above equation. RTDP assumes r* (*), are given. Adaptive Real-time Dynamic Programming action model probabilities these estimates by the right-hand literature, this method based method because to simultaneously is called it learns the value function. as real values while updating side of the recurrence and reward functions relation above the certainty learn that the action models p+,* (*) and the reward functions the and uses state the value function of the current through on-line experience, (ARTDP) estimates [ 31. In dynamic programming It is a model- the action and reward models explicitly, and uses them equivalence control [6]. The model-free RL methods, such as Q-learning, the model learning and the combine into one step by learning a value function over state-action pairs. value function learning I? 7irdepulli. D. Ok/Artificial Intelligence 100 (1998) 177-224 181 i and action u represents The value of Q( i, u) for state executing action u in state i and from then on following correct Q-values must satisfy the following relationship with fp*. the discounted the optimal policy. Hence, total reward of the Q(LlO = r;(u) +r~p;~.,(~).f’*(j). ,iES (3) In Q-learning, Q( i, u) is updated every time an action u is executed execution results is updated using in an immediate the following rule, reward r. ,nl,F1 and a transition in state i. If this into state j, then Q( i, u) Q<i,u) + Q(i,u> +P(r;,,l,n +rr/,(j> - Q(i,u>), (4) where 0 < y < 1 is the discount is the value of state j, detined as max,,Eu(,i) Q( j, a). an exploration state infinitely guaranteed strategy often, and to an optimal policy that ensures the learning to converge [46]. that factor, 0 < /? < 1 is the learning If the learning it executes each admissible rate, and up(j) algorithm action uses in each is rate /3 is appropriately decayed, Q-learning 2.2. The problems of discounting Discounted reinforcement learning is well-studied, and methods such as Q-learning optimization at any given that can earn interest, or where there is a fixed probability and ARTDP are shown to converge under suitable conditions both in theory and in prac- is motivated by domains where reward can be interpreted tice. Discounted that a run will as money be terminated like to learning do not have either of these properties. As Schwartz pointed use reinforcement out, even researchers who use learning methods totals in such domains expected bigger in many cases. The following example evaluate reward per time step rewards [37]. Discounting in favor of smaller short-term their systems using a different, but more natural, measure-average in such domains tends to sacrifice time. However, many domains that optimize discounted in which we would rewards, which is undesirable long-term illustrates this. shown domain In the Multi-loop lengths. The agent has to choose one of the four loops in state S. The average reward per step I, 6/5 = 1.2 in loop 2, 9/7 = I .29 in loop 3, and 12/9 = 1.33 in is 3/3 = 1 in loop loop 4. According is to take to the average-reward loop 4, getting optimality the highest average reward of 1.33. in Fig. I, there are four loops of different the best policy criterion, But the discounted optimal policy is different based on the value of the discount factor y. Let Reward; be the reward at the end of loop i and Length, be the total number of loop i steps in loop i. Then, is total reward for the policy ,u; of following the discounted 0 + . . + yL”“h’ms-‘Rewa,yji + 0 + . . + y2bn~thn-‘Re~~&j + (J + . . and can be simplified to 182 P Tudepolll. 0. Ok/Am$cirrl Intelligence 100 (I 998) 177-224 loop 1 loop 2 loop 3 loop 4 Fig. I. The Multi-loop domain. Discounted learning methods may converge to a suboptimal policy. E 2 d 8 9 ZJ a I ‘- s 1.3 - 1.2 - 1.1 - 1 - 0.9 - 0.8 0.7 - - ARTDP w/ gamma=0.98 ARTDP w/ changing gamma ARTDP WI gamma=06 A-- 0.6 ’ ‘f 0 I 1 I I I 20000 40000 60000 80000 100000 Steps Fig. 2. On-line average reward per step with 10% random exploration in the Multi-loop domain. Each point is the mean of 30 trials over the last 8000 steps. In particular, the rewards in each loop are such that the optimal discounted policy loop 3 when than 0.97, the loop 1 when y < 0.85, loop 2 when 0.85 f y < 0.94, is to follow 0.94 < y < 0.97, and loop 4 when y 3 0.97. Hence when y is greater policy for optimizing Fig. 2 shows discountin g total reward also optimizes the results of running ARTDP of y. On the X-axis average reward for the previous 8000 steps averaged over 30 trials. For comparison, average reward of our model-based ARL method called H-learning section) in this domain with different values is the on-line the in the next in the same way and is shown as the solid line. For exploration, taken, and on the Y-axis is the number of steps the average reward. is computed (described the average the convergence I? Tudepullr. D. Ok/Art+&1 Intelligence 100 (1998) 177-224 183 in state S. ARTDP converged to loop 1 random actions were taken with 0.1 probability when y=O.8, loop 2 when y=O.9, loop 3 when y=O.95, and loop 4 when y=O.98. The H-learning loop 4 in the fewest number of steps (shown with a solid algorithm line in Fig. 2). This experimental discounted When suboptimal is different depending is too small, factor. policy may yield a that the optima1 policy for maximizing factor average reward. on the value of the discounting the optimal discounted result confirms total reward the discount selected As we see in this domain, discounted learning methods can optimize increased increasing to 0.9 when ARTDP converged to loop 2, and to 0.98 when to a loop if it selected reward with a high value of y (3 0.97). But a high value of y causes learning methods with a low to be too slow as can be seen in Fig. 2. Since discounted to converge than those with a high value of y, we may value of y need fewer steps to an optimal average-reward policy faster by starting with expect that they can converge it. To see whether this approach works, the value a low value of y and slowly from 0.8 to 0.98, while running ARTDP. y was started of y was gradually I, to 0.95 when with 0.8 and was increased that it converged steps. ARTDP converged The result of Fig. 2 shows that changing y makes ARTDP even slower than when y is fixed at the highest value, 0.98. In summary, using discounted it converged that loop for a thousand consecutive is to the gain or average reward leads to short-sighted policies, and to poor perfor- the discounted they converge the maximize mance learning methods can find the optimal average-reward too slowly. Moreover, starting convergence naturally Q-learning even further. We will later show that these problems due to discounting factor is high enough, policy; but then factor is low. If the discount from a small y and slowly to loop 3. It was assumed lead to poor performance in real world domains the actual optimization of ARTDP and in some cases. learning when it slows down if the discount increasing criterion to loop arise and 3. Average-reward reinforcement learning We start with the standard Markov Decision Problems that denotes that r@ ( SO, t) is a random variable the agent uses the policy p starting Recall t when Learning reward per step over time t as t ---f co. For a given starting state SO, and policy EL, this is denoted by p”( SO) and is defined as from SO. In Average-reward Reinforcement (ARL), we seek to optimize the average expected (MDP) the total reward received introduced in Section 2. in time pp(sO) = ,fimm fE(?(sa, t)). (5) We say that two states communicate each state from of reaching set of states not in that set. Non-recurrent states form a single recurrent is a that communicate with each other and do not communicate with states if its if every set under each stationary policy. It is a unichain that policy. A recurrent set of states transient. An MDP states are called the other using is ergo&c under a policy if there is a positive probability 184 t? fideptrlli, D. Ok/Art$rinl Intelligence 100 (1998) 177-224 stationary policy gives rise to a single recurrent set of states and possibly some transient states [ 341. For unichain MDPs /.L is independent policy denoted by p(p), maximizes p(p). “the optimal policy”, we mean long-term the expected reward per time step for any of the starting state SO. We call it the “gain” of the policy p, policy”, p*, that and consider From now on, unless otherwise specified, whenever we use the term the problem of finding a “gain-optimal average the gain-optimal policy. 3.1. Derivation of H-learning the gain of a policy, p(p), is independent in time t may not be so. The total reward ,X can be conveniently denoted by p(p) for a starting t + q(s), where Ed of the starting state, the total state s in is a the always exists, and is denoted by offset. Although lim ,+oo E,(S) may not exist for periodic policies, of E,(S), defined as liml,, f cl, e,(s), [ 61. It is called the bias of state s and can be interpreted as the expected in total reward for starting in state s over and above p( ,u) t, the expected long-term total though reward Even expected time t for a policy time-dependent Cesaro-limit h(s) advantage reward in time t on the average. Suppose the system goes from state i to j using a policy p. In so doing, it used up a time step that is worth a reward of p(p) reward of r; ( /A( i) ) . Hence, the following equation. on the average, but gained an immediate the bias values of state i and j for the policy /J must satisfy The gain-optimal policy p* maximizes the right-hand side of the above equation for each state i [ 61. Theorem 1 (Howard). h over S that satisfy For any MDP, the recurrence relation there exist a scalar p and a real-valued function ‘v’i E S, h(i) = .F;;,{ r;(u) + ep;.,;(u)h(j)} ./=I - p, Further, gain. the optimal policy ,u* attains the above maximum for each state i, and p is its as follows. is the Bellman equation In going reward ri (u) action, Eq. (7) be explained gained an immediate if u is the optimal opposed between equal r;(u) - p. this can for Average-reward RL problem. the system from a state instead of the average reward p. After convergence, i to the best next state j, Intuitively, long-term to state j must equal the difference between the bias value of state in state the difference i and the expected bias value of the next state j must for being and p. Hence advantage ri(u) the expected i as I? Ttrdeprrlli, D. Ok/Arri$ciul Intelligence 100 (1998) 177-224 185 h( l)=O Fig. 3. A simple MDP that illustrates the Bellman equation. I. 2. 3. 4. 5. 6. in the current state i. Let a be the state, and Yi,,ml be the immediate reward received. taken, k be the resulting action or a greedy action Take an exploratory action N( i, a) + N(i, a) + I ; N(i, n, k) + N(i, a, k) + 1 p;,k(a) - N(i, n, k)/N(i, a) r;(a) +- c(u) + (~inwr - r;(a) )/N(k a) i) + All actions u E U(i) GreedyActions( that maximize {r;(u) + CT=, Pi,.;(0G)1 If a E GreedyActions( (a) P+ (1 -a>p+a(r;(a) then -h(i) +h(k)) (b) a - $ 7. h(i) + mau~(i){ri(u) 8. i+k + Cy=, pi.j(u)W) -P Fig. 4. The H-learning algorithm. The agent executes steps 1-8 when in state i. Notice that any one solution adding the same constant in the same set of optimal policies only by the relative differences arbitrary MDPs. “reference” recurrent to Eq. (7) yields an infinite number of solutions by to all h-values. However, all these sets of h-values will result in a state is determined ,u*, since the optimal action between the values of h. Setting state to 0, guarantees a unique the h-value of an for unichain solution For example, in Fig. 3, the agent has to select between and in state 0. For this domain, p = 1 for the optimal policy of choosing good- to 0, then h( 1) = 0, h(2) = 1, and h(3) = 2 the actions good-move set h(O) the recurrence bad-move move in state 0. If we arbitrarily satisfy and h( 1) is 2, which equals the difference between action in state 3 and the optimal average reward 1. relative value iteration method, In White’s relations in Eq. (7). For example, ence state is set to 0 and the resulting equations are solved by synchronous the h-value of an arbitrarily chosen refer- successive the difference between h(3) reward for the optimal the immediate 186 P Ttrdeprrlli. D. Ok/Artificial Intelligence 100 (1998) 177-224 [ 61. Unfortunately, approximation dates p using Eq. (7) does not always converge to solve for p, H-learning estimates it from on-line the asynchronous [ 51. Hence, rewards version of this algorithm that up- instead of using Eq. (7) (see Fig. 4). The algorithm in Fig. 4 is executed the number of times ~1 was executed in each step, where i is the current state, N( i, u) is the number of times the current greedy policy (Y to 1, and all initializes in i, and N( i, u, j) explicitly stores the algorithm implementation Before starting, to 0. GreedyActions in each state are initialized to the set of admissible denotes it resulted in state j. Our in the array GreedyActions. other variables actions in that state. H-learning can be seen as a cross between Schwartz’s R-learning discounted average-reward model-free is a model-based probabilities mates. It then employs mates as the true values while updating the equation the “certainty pi,,i (a) learning method, and Adaptive RTDP (ARTDP) learning method. Like ARTDP, H-learning and rewards T; (a) by straightforward maximum principle” by using equivalence the h-value of the current state i according [ 371, which computes is a [ 31, which the esti- likelihood the current esti- to (8) One of the nice properties of H-learning, the optimal policy no matter what exploration is executed as long as every action of every state is that it learns learning, exploration the optimality such as TD-A which are designed executed during learning of the learned policy. This to learn that is shared by Q-learning and ARTDP, is used during often. The is learned, not is unlike some temporal difference methods that is strategy sufficiently for the policy the value function strategy only effects the speed with which the optimal policy [ 391. in the model-free R-learning Eq. (7) is split into two parts by Just as in Q-learning, defining h(j) = m;x R(j, u). (10) R( i, u) represents gain-optimal When action u is executed equation policy the expected bias value when action u is executed is followed from then on. Initially in state i and the all the R-values are set to 0. the update in state i, the value of R( i, u) is updated using R(i, u) + (I - P)R(L u> + P(ri,,l,,l + h(j) - P>, (11) /3 is the learning where state, and p is the estimate of the average rate, Y;,,~,,~ is the immediate reward of the current greedy policy. reward obtained, j is the next In I? Tudeplli, D. Ok/Arti&iuI Intelligence 100 (1998) 177-224 187 any state need pi,.;(u), R-values. to explicitly since i, the greedy action u maximizes the immediate learn reward the value R(i, u); so R-learning does not functions Ti(U) or the action models the it does not need them either for the action selection or for updating As in most RL methods, while using H-learning, the agent makes some exploratory the estimation of p slightly complicated. Simply averaging (greedy) moves would not do, because that do not necessarily maximize is visited could converge that every state moves-moves intended to ensure exploratory moves, H-learning moves make rewards over non-exploratory moves could make it were always making greedy moves. R-learning greedy action u that maximizes Hence, p can be estimated by cumulatively greedy action u is executed following equation, where cy is the learning the system accumulate the average to estimate reward the right-hand in state i resulting rate, the right-hand infinitely often during to a suboptimal side of Eq. (7) and are training. Without these the immediate the exploratory policy. However, that it never visits if to that of in any state i, for any similar rewards from states Instead, we use a method [ 371. From Eq. (7), side, p = T;(U) -h(i) fan=, p;,j(u)h(j). averaging Y;(U) - h(i) + h(j), whenever a the in state j. Thus, p is updated using p + p + cu(r;(u) - h(i) + /z(j) - p), (12) rather Indeed, to converge in H-learning is very similar the two algorithms to apply this algorithm the role of exploration reward T;(U) for each action over to the gain-optima1 policy in are non-ergodic, the immediate than averagin g the “adjusted H-learning was proved mains that we are interested we need to add exploration. form the original MDP infinitely often. Another difference between updates p by averaging sequence, as we do. Later, we present experimental reason Thirdly, reference slows down H-learning gorithm B to our case, we have rectness under suitable conditions convergence is quite be easier theory. to Jalali and Ferguson’s Algorithm B [ 161. This algorithm for ergodic MDPs. Since most do- to such domains, is to trans- into an ergodic one by making sure that every state is visited is that the B-algorithm the action ri,; - h(i) + h(j), that this is the crucial of the two algorithms. recurrent that this change of Al- its cor- the original proof of it may In any case, the h-values bounded, Algorithm B chooses an arbitrary its h-value in many cases. To extend to 0. We found the proof of convergence that the above changes preserve to show of exploration. Unfortunately, is disputed.3 proof based on stochastic approximation to give an independent state and permanently difference between for the significant the performances its correctness convergence that shows immediate to make evidence involved reward” grounds and 3.2. AGV Scheduling Automatic Guided vehicles to of learning algorithms, a small AGV domain called the “Delivery domain” shown in plants the performance in modern manufacturing from one location [27]. To compare transport materials various to another are used (AGVs) 3 D. Bertsekas, private communication 188 I? Tadeplli, D. Ok/Artificial Inrellipxce 100 (1998) 177-224 elt 1 cl Job Package Moving Obstacle AGV Job generator 2 Conveyor-belt 2 Fig. 5. The Delivery domain. Fig. 5 was used. There are two job generators on the left, one AGV, and two destination conveyor belts on the right. Each job generator produces its queue as soon as it is empty. The AGV loads and carries a single job at a time to its conveyor belt. Each job generator can generate either a type 1 job with K destination to belt 1, or a type 2 job with 1 unit of reward when units of reward when delivered job 1 is p for generator 1, and 4 for delivered generator 2. to belt 2. The probability jobs and puts of generating them on load, move-up, move-down, The AGV moves on two lanes of 5 positions each, and can take one of six actions at a time: do-nothing, change-lane, and unload. To load a job, the AGV must be in the position next to the queue. To unload a job, it must be next to the proper conveyor belt. To make this domain more interesting, is added. It randomly moves up or down in each instant, but can only stay in the right lane and cannot stand still, The AGV and the obstacle can both move in a single time step. a job or is standing If the obstacle collides with the AGV when the AGV is delivering still, the state remains unchanged. There for all collisions with the obstacle. is a penalty of -5 a moving obstacle is specified by the two job numbers (X-Y of the AGV and the obstacle, and the job number on the AGV. We assume to the learning states in this domain. The goal of the AGV A state coordinates) that each queue can hold a single job and the complete state is observable system. There are a total of 540 different is to maximize policy. i.e., find the gain-optimal received per unit in the queues, the locations the average reward time, By varying ratio of the jobs and/or the reward the optimal policy generators, generators produce jobs from queue 2 much more frequently time steps needed that needed produce queue 1 much more frequently more than compensates for the extra distance. policy given different values of p, q, and K. the job mixes produced by the job is changed. For example, when K = 1, and both the job type 1 jobs with very low rates p and q, the AGV should unload the number of than them from queue 1 to belt 2. But, when both the job generators jobs from the increased value of job 1 the best jobs of type 1 with a high rate, and K = 5, the AGV should unload type 2 jobs from queue 2 to belt 2 is much smaller It is, in general, hard to predict than from queue 1 because than from queue 2, because to transport to move learning, For P Tadepalli. D. Ok/AmjScial Intelligence 100 (1998) 177-224 189 3.3. Experimental results Our experiments are based on comparing H-learning with ARTDP, Q-learning, R- and the B-algorithm of Jalali and Ferguson in the Delivery domain. these experiments, the Delivery domain is used with p = 0.5, and q = 0.0. In other words, generator 1 produces both types of jobs with equal probability, while generator 2 always produces the result of comparing H-learning with ARTDP, Q-learning, and R-learning and K = 5. We chose these two sets of domain parameters because situations. Experiments qualitatively different [ 3 I]. are reported elsewhere in two situations of the AGV domain: K = 1 two on a wider range of domain parameters type 2 jobs. We present they illustrate is chosen uniformly a random exploration to tune. The parameters initial state. In all our experiments, 1 - 7, a greedy action randomly Each experiment was repeated for 30 trials for each algorithm. Every trial started from strategy was used, is chosen, and with a probability actions. While over all available the average reward per step is computed over the last 10,000 steps for K = 1, do not have learning methods are tuned by to get the best performance. Strictly speaking, y is part of the problem algorithm. is to see how well the discounted methods can it as an adjustable parameter. For a random in which with a probability 77 = 0.1, an action training, and over the last 40,000 steps for K = 5. H-learning any parameters trial and error definition However, since our goal in this domain a gain-optimal approach K = 1 case, for ARTDP the only parameter are p = 0.05 and y = 0.9, and for R-learning, ARTDP y = 0.99, for Q-learning cy = 0.005. The results are shown ,l3 = 0.05 and y = 0.99, and for R-learning, in Fig. 6. p = 0.01, cy = 0.05. For K = 5 case, for p = 0.01, and not a parameter of the learning is y = 0.9, the parameters and the B-algorithm policy, we treated for Q-learning for the other of discounted optimization, learning policy is closer the gain-optimal than type 1 jobs, for a small value of y coincides with the gain-optimal When K = 1, since both jobs have the same reward, always serve generator 2 that produces only type 2 jobs. Since the destination to their generator jobs icy. We call this type of domains “short-range policy ter setting of the delivery domain, well as the long-term optimal policy. In this case, the model-based ARTDP, converges the difference formance. is to of these it is also a discounted optimal pol- domains” where the discounted optimal policy. For this parame- serving queue 2 is the short-term optimal policy as discounted method, although the same per- faster than H-learning, show almost is negligible. All methods except R-learning to the gain-optimal policy slightly I than the jobs of type 2. The gain-optimal When K is set to 5, the AGV receives five times more reward by unloading policy here is to serve the the from queue 1 all of the time except when both of the queues have type 2 jobs the dis- the obstacle 1, it is in serving generator 2 and does it cannot this difficulty, y is set to jobs of type jobs and counted optimal policy when y = 0.9. Even when because close not return find jobs to belt 2, ARTDP sees a short-term opportunity for belt 2, it often has to go there. Whenever to transport high reward jobs. Hence policy when y = 0.9. To overcome 1. This policy conflicts with is located near conveyor-belt it also generates the AGV serves the gain-optimal 1, thus failing the generator to generator 190 P 7&%&/i. D. Ok/Artijicid Intelligence 100 (1998) 177-224 E g I% 5 $ 2 z 0’ 0.25 0.2 0.15 0.1 0.05 0.25 0.05 0 0 Q w/beta&05 R w/beta=O.Ol & alpha =0.05 b--- & gamma=&9 --x.---. Steps 500000 1 e+06 Steps 1.5e+06 2e+06 Fig. 6. Average reward per step for H-learning, B-algorithm, ARTDP, Q-learning, and R-learning in the Delivery domain estimated over 30 trials with random exploration with 7 = 0.1. Top: p = 0.5, y = 0.0, and K = 1. Average reward is estimated over the last 10,000 steps. Bottom: p = 0.5, q = 0.0, and K = 5. Average reward is estimated over the last 40,000 steps. this value of y, the discounted optimal policy is the same as the gain- policy 0.99. With optimal policy. Even as well as the undiscounted gain-optimal discounted methods need optimal policy. As we can Q-learning, less than 0.1, while H-learning than 0.18. Thus the average-reward and the B-algorithm in 2 million longer infer so, the discounted B-algorithm the discount learning methods, ARTDP and Q-learning, of Jalali and Ferguson steps. Since training from Fig. 6, in this “long-range” the could not find the is high, rate to learn the domain, ARTDP, for all trials getting a gain and R-learning were able to find a policy of gain higher time or higher exploration served queue 2 exclusively factor y=O.99 outperformed the discounted learning methods, ARTDP and Q-learning, in finding the learning methods, H-learning and R-learning, significantly P Tudepdli. D. Ok/Art$cial Intelligence 100 (1998) 177-224 191 the average reward, policy. H-learnin g and R-learning took more training steps to converge gain-optimal R-learning less gain than H-learning did. Somewhat surprisingly, to optimize also allowed the h-value of a reference B-algorithm B-algorithm while updating p. and the H-learning suggests that it is crucial the B-algorithm is also unable to do random exploration served both queues for all 30 trials. But, and learned a policy of than H-learning the B-algorithm, which is designed to find the gain-optimal and prevented policy. Since we it from grounding this version of the of the the Eq. 12 state to 0, the only difference between is the way p is updated. The poor performance to adjust the immediate rewards using in low average reward. that model-based that ARTDP could not find the gain-optimal The main even though of y reduces for optimal action selection. Since to the initial steps, true optimum. Meanwhile and result reason policy when K = 5, it is also the discounted optimal policy when y = 0.99, is that a high value far off rewards relevant and makes the temporally the effect of discounting these rewards back it takes a long time to propagate to the to converge it takes a long time for the discounted methods the action rewards still dominate the short-term in selecting The reason the model-based learning methods converge is that they propagate more information in fewer steps than model- in each step by taking the in each update. This also to learn and store the action models explicitly, free learning methods expectation over all the possible next states for a given action requires the CPU-time and increases learning with that of ARTDP, Q-learning, Fig. 7 shows the last 40,000 steps with random exploration with 71 = 0.1. All parameters methods are the same as those in Fig. 6. of H- time. is the on-line average reward of 30 trials over for learning for each update. So, we compared these results. Each point as a function of CPU learning methods the performance and R-learning When K = 1, Q-learning converged R-learning was the slowest. H-learning mance. When K = 5, not converge ods, H-learning in the beginning, R-learning H-learning. to the gain-optimal and R-learning, the discounted converged to the gain-optimal policy and ARTDP showed almost learning methods, ARTDP and Q-learning, policy, whereas did. Even though H-learning the two average-reward in the shortest time. the same perfor- could learning meth- had good performance to the gain-optimal policy slightly faster than The results of this experiment show that in short-range domains where discounted and better policy, H-learning than R-learning with respect optimal policy coincides with the gain-optimal ARTDP and Q-learning, However, time. But gain-optimal long to converge gain. H-learning in such cases with no parameter performs as well as to number of steps. the model-free methods show slightly better performance with respect to CPU the in long-range either take too to the gain-optimal to a policy with less policy or, if y is low, converge achieves higher average reward in fewer steps than the other methods policy, discounted methods such as ARTDP and Q-learning optimal policy conflicts with domains where discounted In a different the robustness changes in the domain parameters p, 4, and K in the Delivery domain. We experimented with a total of 75 different domain parameter settings, by varying p, q, and K. H-learning was compared to ARTDP with y = 0.9, 0.99, and 0.999. of H-learning with respect to tuning. experiment, we tested 192 I? Ttrdeptrlli. D. Ok/Ar/iJicitr/ Intelligence 100 (I 998) 177-224 ARTDP WI gamma=0.9 Q WI beta=0.05 & gamma=0.9 R wl beta=O.Ol & alpha=0.05 -+-- -a-- x ... x 0 0 I I I I I I I I 10000 20000 CPU Time (0.01 sec.) 30000 40000 50000 0.3 0.25 0.2 0.15 0.1 E g d 8 E : a E z 6 ARTDP WI gamma=0.99 Q WI beta=0.05 & gammaz0.99 R w/ beta=O.Ol & alpha=0.005 -+-- .o-- .-x -.. I 50000 I 100000 CPU Time (0.01 sec.) 8 150000 II 200000 Fig. 7. Average in the Delivery domain estimated over 30 trials with rewards per step plotted against CPU time for H-learning, ARTDP, Q-learning, and R-learning random exploration with 7 = 0. I. Top: 17 = 0.5, q = 0.0, is estimated over the last 10 seconds of CPU time. Bottom: p = 0.5, q = 0.0, and reward and K = 1. Average K = 5. Average reward is estimated over the last 40 seconds of CPU time. H-learning to find configurations the optimal policy in all 75 cases, whereas ARTDP in 61 cases. We into was able the best y value that these 75 different configurations to find (0.99) was able can be considered the optimal policy of the domain can be roughly divided with found In these configurations, two groups-50 ARTDP with y = 0.9 and H-learning were comparable. They both found the gain-optimal policy, and ARTDP sometimes remaining 25 “long-range” the gain-optimal but much more slowly success too drastically In the fewer steps than H-learning. however, ARTDP with y = 0.9 could not find policy. Increasing y to 0.99 helped ARTDP find the gain-optimal policy, the Increasing y to 0.999, however, decreased it slowed down found configurations, (in 300,000 steps), because rate of ARTDP the convergence than H-learning. “short-range”. it in slightly [ 3 I 1. I? Tudepalli. D. Ok/Art$cial Intelligence 100 (1998) 177-224 193 In summary, our experiments in the domain parameters, indicate than its discounted to changes the gain-optimal policy that if our goal is to find the gain-optimal Learning methods are preferable with those of Mahadevan who compared Q-learning lator domain [ 241. better and a maze domain and found policies, that H-learning and in many cases, converges is more robust with respect in fewer steps to suggest counterpart. Thus our experiments then Average-reward Reinforcement to the discounted methods. Our results are consistent in a robot simu- to perform that R-learning and R-learning can be tuned 4. Exploration Recall that H-learning infinitely often during fortunately, reward, because maximize its reward. needs exploratory training actions executed exclusively in order to avoid converging actions to ensure that every state is visited to suboptimal policies. Un- for exploratory purpose could lead to decreased the agent’s current knowledge of how to they do not fully exploit In this section, we will describe a version of H-learning learning while always executing “optimism algorithm, using action-penalty (AH), which automatically explores current greedy actions. Our approach the promising under uncertainty”, and Koenig and Simmons’s method of representing to Kaelbling’s is similar and scheme [ 17,18,20]. called Auto-exploratory H- parts of the state space is based on the idea of (IE) functions Interval Estimation the reward 4. I. Auto-exploratory H-Learning Recall that in ergodic MDPs, every stationary policy is guaranteed to visit all states. to ensures domains interested it can be shown sufficient exploration, in this section. Unfortunately, although a better exploration even further. Hence, we are primarily that always choosing a greedy action with respect this with ergodic MDPs, where every pair of states communicate in the gain of a stationary policy for a (non-unichain) MDP is not constant but depends on the initial state In these MDPs, the current value function strategy might speed up convergence non-ergodic general multichain [ 341. Hence we consider some restricted classes of MDPs. An MDP is communicating if for every pair of states i, j, there is a stationary policy under which they communicate. Contrast stationary policy. For example, our Delivery domain Serving only one of the generators states. A weakly communicating MDP and also allows a set of states which are transient under every stationary policy Although on the initial state, the gain of an optimal policy does not. AH-learning and works by using p as an upper bound on the optimal gain. It does this by initializing p to a high value and by slowly it to the gain of the optimal policy. Instead of as an estimate of average reward of the current greedy policy, we now interpret p as the aspired average reward of the learner. The aspired average reward decreases slowly, under every but not ergodic. the AGV from visiting some than a communicating MDP [34]. the gain of a stationary policy for a weakly communicating MDP also depends this fact, all the time prevents is more general is communicating, reducing exploits 194 P Eldeplli, D. Ok/Art+&1 Intelligence 100 (1998) 177-224 move(0,O.S) move(0,O.S) &move(o,o.& Fig. 8. The Two-state domain. The notation action( r, p) on the arc from a node I to 2 indicates that, when action I, p is the probability of the next state being 2 and r is the immediate reward. is executed from them decreases with time. When while the actual average reward of the current greedy policy increases, and the difference the aspired value is the same as the average between reward of the current greedy policy, AH-learning converges average any suboptimal for weakly communicating MDPs, a strict superset of unichains. to a globally optimal policy, we have to adjust the initial value of the aspired the average reward of rate so that it never falls below to find gain-optima1 policies greedy policy. AH-learning reward and its learning converges. To ensure that AH-learning is applicable There are two reasons why H-learning needs exploration: Inadequate reward models, and to learn correct h values. the accuracy of either of these, making affect policy. to learn accurate action and could adversely exploration to a suboptimal the system converge the changes are updated The key observation in the design of AH-learning greedy policy, and p(,u) be its gain. Consider what happens to be max,Eu(i){r;( is that the current value of p affects for the states in the current greedy policy. Let p be the if the u) + for states in the how the h-values current suboptimal current value of p is less than p(p). Recall that h(i) - p. Ignoring C’,‘=, p;,;(u)h(j)} the sum of immediate tend current greedy policy to be higher than np (since p < p( ,z)). in any n steps is likely rewards for this policy that the h-values of all states in the current It is possible, under the h-values of states not visited by this policy policy do not change, the system may never be able to get out of the set of states in the current greedy policy. If the optimal policy it will never be learned. involves going in the Two-state MDP in Fig. 8, which is a communicating through states not visited by the greedy policy, to increase on the average, because increase or stay the same. Since these circumstances, the greedy policy, This is illustrated that by executing the h-values this implies to p itself, is updated clearly In each of the two states, multichain. stay always keeps the system with 50% probability. There is no immediate There is a reward of 1 for the stay action in state 2. In this domain, and stay in state 2 with p(p*) = 2. in the same state as it is currently there are two actions available: stay and move. it in either state. in state 1 and a reward of 2 for the stay action reward for the move action in; but move changes the optimal policy y* is taking the action move in state 1 When actions are always chosen greedily, H-learning in approximately in state 2 is executed before the stay action in state 1. If the stay action in state 1 is executed that in state 2, it receives a reward of +I and updates h( 1) to 1 + h( 1) - p. before finds the gain-optima1 policy trials in which the stay action half of the trials for this domain-those I? Edepalli, D. Ok/Art$cial Intelligence 100 (1998) 177-224 195 is to continue to execute stay The greedy policy p increases action choice always results and therefore converges in the stay action to a suboptimal policy. the value of h( 1) in every update until finally p converges in state 1. If p < p(p) = 1, this to 1. Since greedy in state 1, H-learning never visits state 2 as before, the MDP is assumed Now consider what happens and eventually will be visited. Thus, states with higher h-values are reachable ignoring if p > p(p), where ,u is a current greedy policy. In this the h-values of the states in the current greedy case, by the same argument policy must decrease on the average. This means that eventually the states outside the set of states visited by the greedy policy will have their h-values higher than some of those visited by the greedy policy. Since the recurrent h-values, affect the gain, as long as p > p(p), policy p. This suggests changing H-learning pa, which to be weakly communicating, from the states with decreasing the transient there is no danger of getting stuck in a suboptimal so that it starts with a high initial p-value, is high enough so that it never gets below the gain of any suboptimal policy. itself. In fact, p is the changes changing at a rate determined by a. Hence, even though p was initially higher after the previous argument work, we have to adjust LY so that p changes low to the h-values. This can be done by starting with a sufficiently the initial constantly than p(p), a while. To make slowly compared initial a-value, values pe and ‘~0 by Hfil@“. Hence, the H-learning we used until now is Ho,‘. it gradually. We denote H-learning with In the preceding discussion, we ignored it decreases continuously, it can become smaller states that do not (~0, and decaying to the p-value than p(,u) because So far, we have considered turn to its effect on the accuracy of action models. For the rest of the discussion, useful the utility R( i, n) of a state action pair (i, a) to be to define the effect of lack of exploration on the h-values. We now it is to 1 -p the R-value that maximize the system assumes in state i are actions that move takes it to state 1 because the greedy actions the following run of H 6.0.2 in the Two-state domain, where, the action stay in state 1. It reduces h( 1) = R( 1, stay) in state i. Hence, in step 1, the Consider and takes agent executes it has the action move in the next step. Assume 50% failure rate. With this limited experience, that both the actions have the same next state in state 1, and stay has a reward of 1 while move has 0. Hence, that R( 1, stay) = I + h( 1) - p > 0+ h( 1) - p = R( 1, move) and continues it determines to execute stay, and keeps decreasing agent cannot get to state 2 because Therefore, model of move. Unfortunately, The solution we have the value of h( 1) . Even though h( 2) > h( 1) , the it does not have the correct action model for move. to learn the correct it keeps executing stay, which in turn makes it impossible this problem cannot be fixed by changing pu or (~0. (AH- starts with a high pe and low (~0 (AH~‘T~[’ ) , and stores the R-values explicitly. all R-values of the same state are effectively updated at the same time by In (cf. Eq. (13)) only is updated by the following update equation the h-value, which sometimes makes it converge learning), In H-learning, updating AH-learning, R(i, a) when action u is taken in state i. to incorrect action models. called “Auto-exploratory implemented, H-Learning” 196 P 7kie/d/i. D. Ok/Art&G1 Intelligence 100 (1998) 177-224 1. Take a greedy action, i.e., an action a E U(i) that maximizes R( i, a) in the current state, and rinrnl be the immediate reward received. state i. Let k be the resulting 2. N(i,a)~N(i,a)+l;N(i,a,k)~N(i,u,k)+l 3. p;,/t(u) + N(L a, k)/N(i, 4. r;(u) + ri(u) + (rifll,,l - ri(u))/N(i,u) 5. p+- 6.at^ (1 -a)p+ar;(u) a) a+l 7. R(i,a) +- r;(u) + ~~=,{p;.,i(u)h(j)} 8. itk - p, where h(j) = maxu R(j,u). Fig. 9. The AH-learning algorithm. The agent executes steps l-8 when in state i. R(i,u) +- r;(a) + ~~;,.i(u)~(j) - P. .&I (14) In AH-learning, when p is higher value of the executed action the same. Therefore, current state, forcing execute all actions, greedy actions. the R- than the gain of the current greedy policy, is decreased, while the R-values of the other actions remain in the actions appear the un-executed is forced to to explore such actions. Thus, AH-learning learn correct models and find the optimal policy by executing only eventually, the system to be the best set to user- of AH-learning in Fig. 9. p and cy are initially our implementation is shown The algorithm for AH-learning defined parameters po and LYO. Unlike H-learning, does not explicitly an executed action p can be updated simply by taking are no non-greedy rather than the h-values, and are updated by Eq. (14). shows the plot of R-values that distort Fig. 10(a) actions if store the current greedy policy. There is greedy before updating p, since all executed actions are greedy. reward, since there this estimate. The R-values are stored explicitly the average with the immediate is also no need to check the action the immediate domain. All initial R-values are 0. Because the initial value of p, updating of R-values lower than beginning. Thus, just executed time. Therefore, AH6,0.2 can learn accurate action models by executing each state many reduces very the “error measure”, will be decreased than higher the Two-state domain. from a single run of AH6,0,2 in the Two-state is much in the the next in each action little, because for other actions they are executed, because p is significantly to 2, R( 2, stay) - p is almost 0. But the R-values reward of any action rapidly is rarely chosen as the best action times. As p gets close significantly whenever finds the gain-optimal their immediate reward. Thus, the system r(2, stay) policy in reduces them Fig. IO(b) shows the on-line average reward for 100 trials of AH6,0.2 H6,0.2 and Ho,’ in Fig. 8. When actions are chosen greedily, AH6,0.2 the that AH- the search space effectively while always executing greedy actions. On learning methods except AH- in the Two-state domain the optimal policy found optimal policy learning explores this very simple Two-state domain, all previously discussed in 57 and 69 trials respectively. This confirms our hypothesis shown in all 100 trials tested, whereas Ho,‘, and H6,o.2 found I? Tadepalli. D. Ok/Artijicial Intelligence 100 (1998) 177-224 197 20 40 60 80 100 Steps 200 400 600 800 1000 Steps E s d w E $ a s! ‘- s 2.2 2 1.8 1.6 1.4 1.2 1 0 Fig. IO. (a) The R values and p values of AHh.“.* in a single trial (top), and (b) of last 20 steps of AHh,“.‘, H”.“.2, and Ho,’ mean-averaged over 100 trials (bottom) the on-line mean rewards in the Two-State domain. learning, execute non-greedy including H-learning, ARTDP, Q-learning, to find the gain-optimal actions results and informal the gain of any suboptimal high value and LY to a sufficiently and R-learning need to occasionally policy. reasoning, we conjecture policy during that if p is main- learning, by initializing then AH-learning than the gain of to that lower small value, to the gain-optimal policy. However, if p becomes policy any time during learning, AH-learning might converge From our empirical tained higher than it to a sufficiently converges some suboptimal policy. 4.2. Experimental results on AH-learning In this section, we present more empirical evidence of in finding a policy with a higher average reward, and in converging quickly the effectiveness to illustrate AH-learning 198 I? Tudepdli. D. CJk/Art~fir~ict/ Intellipm 100 (I 998) 177-224 to other previously studied exploration methods. We use the Delivery when compared domain of Fig. 5 to do this. We compared AH-learning to four other exploration methods: exploration, Boltzmann counter-based In random exploration, actions with a small probability greedy action, exploration, an action u is chosen 1 - 7, i.e., one that maximizes R( i, a) over all a, is chosen. that maximizes 7. With a high probability a random action is selected uniformly exploration, and recency-based random exploration, [45]. from among the admissible exploration in any state In counter-based i, a R(i, a) + 6 c(i) c:‘=, pi,,j(U)C(j) ’ where c(i) In Boltzmann is the number of times state i is visited, and 6 is a small positive constant. exploration, an action a is selected in state i with probability eK(i,tr)lP Cue R(i.u)/P ’ parameter. that maximizes R(i, a) exploration, an is the number in state i last, and E is a small positive constant. rate. initial values and the rates of decay were tuned by trial and error to give the best where /3 is the temperature or randomness action a is selected of steps since In all the four cases, Their performance. the parameters 7, 6, /3, and E were decayed at a constant the action a is executed In recency-based where n(i, a) f q/m, The parameters for the Delivery domain, p, q and K, were set to 0.5, 0.0 and 5 as in is particularly first, the domain Fig. 6(b). Proper exploration reasons: rewards; and third, gain. For all these reasons, of any suboptimal policy. It gave the best performance with pa = 2 and (~0 = 0.0002. is stochastic; there are many suboptimal it is difficult to maintain p consistently for AH-learning for the following it takes many steps to propagate high to the optimal policies with gain close higher than the gain to find the gain-optimal in this domain policy, which is important important second, Fig. 11 shows the on-line average rewards over 30 trials of AH2,0.0002, H’*O.OO’, and Ho,‘, with only greedy actions, and of Ho,’ with 4 different exploration methods: random exploration with an initial 77 = 0.14 that is decayed by 0.00031 every 1000 steps; counter- based exploration with an initial S = 0.07 that is decayed at the rate of 0.00021 every 1000 steps; Boltzmann /? = 0.3 that is decreased by 0.0003 every 1000 steps; and recency-based exploration with an initial E = 0.05 that is reduced by 0.0004 every 1000 steps. exploration with an initial tuning of po and a~, it improved When actions are always greedily chosen, Ho*’ could not find the optimal policy even and was only slightly appears much and Boltzmann is faster than other exploration methods, although to its than AH and recency-based methods. Recency-based than random exploration once. By proper worse better explorations it dives down optimism under uncertainty. to a very low value in the very beginning, which can be attributed for this domain, while counter-based seem worse. AH-learning significantly, exploration I? Tadepulli, D. OklAmjCul Inrelligence 100 (1998) 177-224 194 0.25 0-l 0.05 0 -0.05 -0.1 -0.15 0 50000 100000 150000 200000 250000 300000 350000 400000 450000 500000 Fig. Il. The on-line mean rewards of last IOK steps averaged over 30 trials for AH2,“.‘WW2, H’,“.‘““, and Ho,’ with only greedy action-selection, and for H”.’ with recency-based, counter-based. random, and Boltzman exploration strategies in the Delivery domain with 11 = 0.5, y = 0.0, and K = 5. It is interesting curves of all methods other than AH-learning to compare AH-learning with other RL methods with respect to CPU in Fig. 12 were to time. The performance copied from Fig 7. We tested AH-learning with K = 1 and 5, and added these results Fig. 12. Since all methods other than AH-learning with 7 = 0.1, their final on-line average AH-learning. Perhaps more less CPU time than the other methods in much unlike H-learning, AH-learning was significantly random exploration lower than that of to the optimal policy In particular, in the long-range domain. faster than R-learning. used un-decayed rewards were significantly importantly, AH-learning converged that with proper These results suggest the state space much more effectively AH-learning does involve be automatically maximum p to something higher explores schemes. Although that at least p can adjusted. One way to do this is to keep track of the currently known reward over all state-action pairs, i.e., maxi,, R;(u), and reinitialize it changes. two parameters p and LY, it appears initialization than than this value whenever of p and a, AH-learning the other exploration immediate tuning 5, Scaling-up average-reward reinforcement learning table-based RL methods including H-learning programming and need space proportional function. For interesting causing a combinatorial real-world domains, explosion All dynamic the value enormous, these algorithms. to the number of states and AH-learning are based on to store the number of states can be for in the in the time and space requirements In fact, all table-based RL methods need space exponential 200 F! Tudepalli. D. Ok/Artijiciul Intelligence 100 (1998) 177-224 Q w. beta=0.05 & gamma=0.9 R wl beta=O.Ol & alpha=0.05 .-x---- 0 0 10000 20000 30000 CPU Time (0.01 sec.) 40000 50000 0 50000 100000 CPU Time (0.01 sec.) 150000 200000 Fig. 12. (a) Average rewards versus training time for AH ‘~“~“(M)s, H".' , ARTDP, Q-learning, and R-learning with K = 1 (top), and (b) average rewards versus training time for AH *,“.‘xK1*, Ho,‘, ARTDP, Q-learning, and R-learning with K = 5 (bottom). Each point is the mean of the average reward over the last 40,000 steps for 30 trials with p = 0.5 and y = 0.0. All methods except AH-learning use random exploration with 7 = 0. I. function. to store (number of machines, In addition, states. the value the values of individual jobs to be transported, number of AGVs, completely number of state variables etc.) just compartmentalize in a particular state, it has absolutely no influence on the action they choose in a similar state. the state spaces are so huge that an agent can never expect to have In realistic domains, enough experience with each state to learn the appropriate action. Thus, it is important to states that have not been experienced by the system, from similar states to generalize that have been experienced. This for the from a hypothesized value function is usually done by finding an approximation table-based If they space of functions. the best action algorithms learn There have been several function approximation methods studied including neural network learning [ 8,211, clustering in the discounted [ 261, memory-based [ 281, and locally weighted regression [ 29,361. Two characteristics of the AGV RL literature, methods F! Tudepalli, D. Ok/AmjSciul Intelligence 100 (1998) 177-224 201 specific this implies for H-learning the value function that the value function scheme must be able to generalize In the AGV domain, -..,I ” scheduling domain attracted us to local linear regression as the method of choice. First, features of the state in this domain. the location of the AGV is one of the most important locations of the Any function approximation varies linearly over a AGV to large regions. Second, reward region of state space, when the optimal action and its effects, i.e., the immediate this and the changes region. in +L- v L11e A- allU I - L”UI”IIILLLc;s “1 LIIC; fiiu v. I ~11s rb uue vecause me upmuai action it changes only the coordinates same in large geometrically AGV and by a constant amount, and it gives a constant linear ^^_I ,J:-,.*-,. ,c rL_. AC%, -rPL:” :- &....^ I-^^_..“^ rL_ _-r:-_ 1 __r:_- . in the feature values of the current state, are constant iS the of the reward (usually 0). for each also need space to of the action models and reward in Section 2. The space requirement the domain model in the number of state variables. Dynamic Bayesian networks have In many the domain models in such a way that a small number of state-action store models as defined is also exponential been successfully cases, parameters learning methods such as H-learning to fully specify the domain models. learning methods need only to store the value function the domain model, which in the past to represent is the combination While model-free pair, model-based these networks it is possible are sufficient is piecewise for storing throughout contiguous immediate to design [ 12,351. regions, used 5. I. Model generalization using Bayesian networks One of the disadvantages of model-based methods like H-learning is that explicitly a lot of space. The space requirement from O(nm) on the of the domain, where n is the number of states and 112 is the number of to large domains, we represent the In this section, we describe how that the network learning methods networks. for them, assuming the parameters depending to 0(&n) to learn its action and reward models consumes the models can be anywhere storing for storing stochasticity actions. To scale the model-based domain models using dynamic Bayesian we can adapt H-learning str~rrt~~re is criven -_.- _____ _I D _._... We assume (DBN) Bayesian network t, and the action at time directed acyclic graph whose nodes represent probability the probabilities The probability associated CPTs, and there are many algorithms learning is given as prior knowledge, structure CPTs. table that a state is described by a set of discrete valued features. A dynamic the feature values and is a represents the relationships the feature values at time t + 1. A Bayesian network between (CPT) of different values for a node conditioned of any event given some evidence random variables, along with a conditional associated with every node. The CPT at each node describes on the values of its parents. is determined by the network and the this [ 351. Since the network the to compute action models to learning reduces we mustrate the dynamic Bayesian network representation is a job generator on the left and a conveyor-belt using the Siippery-iane in Fig. 13(a). There domain right. The job generator generates a new job The goal of the AGV in the unloading (Job-on-AGV on the loads a job, it zone. A state of this domain can be described by the job on AGV is 0 or 1) and the AGV location in the loading zone and unload is to repeatedly after the AGV immediately load a job (AGV-Lot). (which 202 P: fildeptlli. D. Ok/Arttj‘ickzl lntellipnce IOU (1998) 177-224 Loading Zone AGV Lane Unloading Zone Job generator Conveyor-belt 0 Job Package Fig. I?. (a) The Slippery-lane domain (fop) and (b) its dynamic Bayesian network (bottom) stay, load, unload, the unloading the lane from is for entering There are six actions: forward, backward, and move. The action is for entering forward its correct the lane from 1 -P,,l,,ve destination with probability P,,,,,,,, and in the incorrect direction with probability is slippery). The other actions have the obvious meanings. The AGV (since gets a positive reward of 5 when it unloads a job, and gets a penalty of -0.005 whenever it moves with a job. In all other cases, the reward is 0. the loading zone, and backward is executed, it moves If move the lane towards zone. shows a DBN for this domain. Typically, Fig. 13(b) the first set corresponds of nodes, where to the features of the next state under a given action. representation explicitly differences the differences between to the corresnnndino y_..-...D in which only of [he currefit ff3tilrPS ._- __._- state. a DBN consists of two sets to the features of a state and the second set the features Instead, we used a simplified in the two states are these shown. The features of the next state can be easily computed by adding fl represent for moving the changes it is, and -1 left. Because is independent for unloading, of Job-on-AGV only when Job-on-AGV and AAGV-Lot respectively. In this domain, AJob-on-AGV and 0 for other actions, and AAGV-Lot In Fig. 13, AJob-on-AGV and AGV-Lot, -1 right, 0 for staying where Job-on-AGV for loading, for moving and unload actions are admissible AJob-on-AGV on-AGV and Action are both parents of AAGV-Lot because dependent on Job-on-AGV reward only when immediate iearning to the values of is either is either + 1 the load value, or AGV-Lot, given the Action. But Job- is as well as Action. Since the action move receives a negative the expected the AGV has a job, reward has both Job-on-AGV tine CPTs of the dynamic Bayesian network from exampies is straightforward in the network are directly observable and the network structure probability because all the features is known. Consider by the fraction of state-action pairs in P(f=UIf,=L’r.. which f has the value u out of all cases in which the parents have the desired values. For of AAGV-Lot = + I given Job-on-AGV = 1 and Action = move example, ,f that has parents a node . , ,fk = ok) is approximated and Action as its parents. the direction of move the node Exp-Reward , fk. The conditional has the appropriate the probability that denotes fi , P Tudepnlli, D. Ok/Arrificiui Intelligence 100 (1998) 177-224 203 is the fraction of the cases in which the AGV moved right when it had a job and move was executed. reduces consequence If n is the number of different AGV locations, repre- of the domain models from 10n - 4 to 32. An sentation is that learning of domain models can now be important much faster. Unfortunately, time is often dominated by the time it takes to learn the value function, and not by the time it takes to iearn iiiodeis. This is true in our Deiivery domain. But in domains the space requirement of this reduction this is not always easy to see because the above Bayesian network the learning tiie domain such as the Slippery-lane, where accurate domain model mance, Bayesian network-based models can demonstrably will be shown in Section 5.3.1. is crucial for perfor- learning expedite policy learning. This 5.2. Value function approximation In this section, we describe our value function approximation method that is based on local linear regression. like ARTDP it is important these conditions, to other “nonlinear” in an AGV scheduling that the value function to be able to generalize the h function. Second, In what follows, we assume task such as the Slippery-lane We chose local linear regression features of the state is the location of the AGV. Since (LLR) as an approximation method for two reasons. domain of the last section, the AGV is the in AGV approximate independent of the exact location of the AGV the h linear with respect to the AGV location when the other features of in one of a large set of locations, to large regions the immediate First, one of the important typically to effectively locations domains reward is usually given some other features of the state and the action. Under function is piecewise the state are constant. linear with respect to a small number of such “linear” with respect features. However, learning methods ~nnmuim~tinn with disco~unted learnin “yy” ,..... -...,.. more memory In linear is piecewise features and can change arbitrarily the value function of discounted linear so that the sum of the squares of the errors of these points with respect surface the neighborhood state is represented Our value function features. This the h features function with a set of piecewise the value function using a set of exempiars”, which are a seiect set of states and their h-vaiues, picked by the learning algorithm. The value function the stored exemplars. to a set of m data points to the output in that the features. the values of the k linear the nonlinear to store the value function. regression, we fit a linear surface features and n - k “nonlinear” to generalizing by a set of k “linear” is limited approximation of the point where a prediction to Locally Weighted Regression linear. Hence using a piecewise linear functions, we represent the data points are chosen is needed. Let us assume Instead of representing for the points between is not piecewise in k dimensions [ IO]. In local (LWR) where large weights is interpolated is minimized regression, are given is similar [2,29,36]. introduce infinitely linear rn:ilrl r :< e ---.- ----_- -..- D-’ !apPr ~rrnr~ 2nd ~QIJ!~ ren,~gire Suppose we need an estimate of h(p), where the state p has values x,)1, . . . , x,,,’ for If p is one of the exemplars, that first selects its n features, where its stored value features. the system the first k are the linear is its estimate. Otherwise, the exemplars 204 I? Trrdeplli, D. Ok/Artijiciul Intelligence 100 (1998) 177-224 features. of the k-dimensional that selecting neighbors the h-value features. Out of these, (subspaces) the nearest neighbor on each side of p in its first dimension this way reduces In other words, these exemplars it picks one nearest exemplar of p space centered at p. If is large errors from states that are far off from p but are close to each is measured by the have the same values as p for all its nonlinear only differ in the k linear in each of the 2k orthants k = 1, for example, selected. We found due to extrapolating other. The distance between states that differ in their linear features Euciidean on the values of the first k features of these exemplars uses the values of all its selected neighbors, or is less than it is set to the maximum in reducing from p. the system uses iinear regression fit and of than of their values, the minimum of their values, respectively. This step is useful in the distances of different neighbors the errors due to large differences distance. After seiecting to find a least squares If the predicted value the 2’ neighbors, the maximum the potentially it to predict or minimum its h-value. is greater If p does not have neighbors in all the 2k orthants then the number of dimensions nonlinear neighbors continued estimated features, in 2k-’ orthants are selected until k = I. If this also fails to be 0. At any time, an exemplar the currently is stored that share its values for all their k is reduced by 1, and the nearest is is holds. This the h-value such to find such neighbors, that the above condition then from the h-values of adjacent stored exemplars. Since there tolerance in the state space differ by p when normalized by multiplying if its updated h-value neighbors. Whenever of its nearest neighbors from the exemplar h(j)-value iarop nllmhpr nf ~x~mnlnrc w$ be stored ifi the ~~oinnino 2nd &.y ze ~PVP,: &i&d if its h-value cannot be estimated within a given states is is also stored the h-values of all its selected nearest to see if any say j, can be safely deleted the stored a it may now be possible from j’s nearest neighbors, which may include a constant parameter E with p. An exemplar a new exemplar (selected to approximate i. Without is greater or less than the system checks is no immediate (i, u) is stored, this heuristic, the tolerance set, because as described reward, above), av .LY.L.““. -1 -“w”‘y...‘” b*n*‘=-**a) afterwards. We call approximates the value function using the version of H-learning Fig. 14 shows the algorithm of LBH-learning. that learns Bayesian network action models and “LBH-learning”. local linear regression, the exemplar set to empty, It initializes than the expected optimum gain, and (Y to a low value. The ex- p to a value higher plicitly stored h-values of the exemplars are denoted by h( .), and the values estimated by LLR are denoted by A( .) . The prob- abilities p;,.j (u) can be inferred the standard Bayesian algorithms. The parameters of the Bayesian networks are updated network in Section 5.1. The exemplars are up- incrementally dated and used in the prediction of h-vaiues of other states as descriiied in the previous paragraphs. from Bayesian networks using ri( u) and the transition by Update-BN-model as described immediate inference rewards Our algorithm is similar to the edited nearest neighbor that improve the size of the representation exemplars keep nearest neighbor approaches small [ 1,13,14]. One problem with is that they are highly sensitive their predictive accuracy and prune the unnecessary algorithms, which collect ones to the edited to noise, since they tend to P Tadepalli, R. Ok/Artificial Intelligence 100 (1998) 177-224 205 1. Take an exploratory action n or a greedy action a that maximizes CT=, p;,,j(a)h(j). received. Let 1 be the resulting state, and rinlm be the immediate ri(a) + reward 2. Update-BN-model 3. If a is a greedy action, (i, a, 1, r;,,I,,l) then (a) p +- (1 - a>p + Ly(r;(a) + h(I) (b) cy +- 2 - h(i)) 4. i i 5. max,{ri(u> -P If (i, h(i)) E Exemplars, delete it. Let Neighbors be the nearest neighbors of i in the 2k orthants surrounding i that differ from it only in the values of k linear features. Pi,,j(U)h(j))} CC;=, + 6. If lu - h(i) 1 > EP or u > maxjENeighbors h(j) or u < minjcNeighbors h(j) (a) Add (i, u) to Exemplars. (b) For any j E Neighbors, plars. (Jz( j) 7. i+l if /h(j) is computed after temporarily -h(j)/ < EP then delete (j, h(j)) from Exem- removing (j, h(j)) from Exemplars.) Fig. 14. LBH-learning, which uses Bayesian networks for representing the action models and local linear rerrrewinn for lnnroximating __o__ _.__ _.. __. Lrr__ _ ..~ the value function. Steps 1-7 are executed when the agent is in state i. [ 11. from the remaining points linear and has no noise. Even though the intermediate store all the noise points, which cannot be interpolated Our algorithm does not seem to suffer from this problem, since the target value function that is in fact piecewise the learner sees are indeed noisy, linear stages of learning. This is (with a small number of “pieces”) is updated by backing up the values one step at a time from because the value function the adjacent in almost and hence all places. 4 states, which keeps it locally consistent even in the intermediate the value function to be piecewise still appears examples T ‘1.. TT I__--:-- J_,IKt: l-l-,ea,r,,r,g, LDlYl-ItXilLI111g ‘ill, “t; G,%LGI,“GU L” fi”L”-r;ny‘“,ar”ly r TlTT ,____:__ _^_ L_ ,..,t,_,4,l ( ALBH-learning) by initializing of h-values. Thus ALBH-learning approximation It maintains In the next section, we evaluate different versions of H-learning in several domains. and ALBH-learning a separate set of exemplars to the gain-optimal and converges p to a high value and explicitly storing R-values uses Bayesian networks and local linear regression l ,. “‘+..t,. _“_ I,..., + _..., T DLI ,*-....:..-” ~~II-IGL(IIIIIIg instead for policy by taking only greedy actions. for each action a to store the tuples (i, R(i, a)}. including LBH-learning locally linear 5.3. Experimental results In this section, we show experimental do- mains. We compare is named by its extensions: A for using auto-exploration, B for learning Bayesian network of six versions of H-learning. Each version in three different AGV-scheduling the performance results 4 There are, in fact, two kinds of locality: locality in the state space and locality in the Cartesian space inhabited by the AGV. The fact that these two notions coincide in our domain is an important premise behind the above argument. 206 t? Tudeplli, D. Ok/Artijicial Intellipme 100 (I 998) 177-224 action models, and L for approximating sion. the value function using local linear regres- We also compare the performance of ARTDP with Bayesian network action models regression and local linear Since LBARTDP does not have the parameter p, its tolerance was normalized where pi is the smallest dimensions. This gave better performance (ARTDP) . to be &Pi, in state i among all its linear local slope of the value function than using a constant these two extensions , and without (LBARTDP) tolerance. Because ~-learning -with a iiigh valile of p gives good perfoimance, all algorithms that do not have based on H-learning gorithms v = 0.1 probability while The experiments regression, linear their applicability demonstrate the scalability to domains with multiple start with a high value of p. In all these experiments, the auto-exploratory the al- take random actions with take greedy actions. the synergy between Bayesian network models and local in both space and time, and of the learning methods component algorithms always the auto-exploratory linear features. 5.3.1. Improving the performance of H-learning is to demonstrate the synergy between local linear The goal of the first experiment regression and the Bayesian network domain We use the Slippery-lane in Fig. 13(b) learning shown are used for representing works locations tant, P,,,,,,l, was set to 0.6, i.e., with 60% probability, zone if it has a job and to the loading zone if it has no job. for the AGV was set to 30. To make the model in Fig. 13(a). The dynamic Bayesian net- the domain models. The number of impor- to the unloading learning the AGV moves significantly in scaling H-learning. The parameters of each method were tuned by trial and error. The p-values were to 0.1, 0.05, 0.02, 0.01, 0.01, and 0.01 respectively LH-learning, BH-learning, AH-learning, for ALBH-learning, and H-learning. The a-values initialized LH-learning fQrtnr fnr I RARTnP au. YuI_I.Lu_ .U”L”l initialized LBH-learning, were LBH-learning, rlicrnllnt UL.,““....L 30 trials for each method, ted the off-line of 1OOK steps from 3 random convergence main. is too fast average Since P,,,,,,, is close to 0.5, model stored . _ __ . its parameterized but due to a different BH-learning, with learning which H-learning, interesting was the performance -__ . BH-learning of approximations and AH-learning, learning. Since AH-learning BAH-learning updates one R-value of state-action and LH-learning, to 0.001, 0.005, 0.0001, 0.001, 0.001, and 0.001 For and LBARTDP, E was set to 1, 1, and 2 respectively. The for respectively. The _..- ~urwrimmtc ..,.y~.-‘..-“.U WPW .I--- rewatwl ‘-r--‘-- WRE QPt tn 0 95 .,..-. . . ..- “_. from a random .., starting state. In Fig. 15, we plot- initial reward over 30 trials, each estimate being based on 3 runs because the in this do- start states. We chose off-line estimation the on-line to reliably measure average reward learning domain models converged more quickly is very important its models as tables. LH-learning also converged reason, namely value function of LBH-learning which was clearly superior t’hus demonstrating the synergy between in this domain. Thus, than H- than approximation. Most to both the two kinds faster used. Also ALBH-learning and both BAH-learning converged and AH-learning faster than both BAH-learning converged faster explored the domain effectively and learned models than H- faster, converged only slightly faster than AH-learning. Because ALBH-learning pair at every step while LBH-learning updates P Tudepalli, D. Ok/Art$icial Intelligence 100 (1998) 177-224 207 0.02 0.018 t I I 4 40000 steps 60000 80000 100000 Fig. IS. Off-line average and LBHu.‘.“.‘X’2 BH”.“‘.“.‘“” LH”.‘.“.‘n”‘~ reward of ALBH”.‘.“.‘““, BAH”~“2~“~‘K”, and AH”~“‘~O~W’ without random exploration ,.,“.“‘.&““’ , and LBARTDP with random exploration with 7 = 0.1, runs of 1OOK steps each from 3 random is over 3 off-line trial, evaluation In each averaged over 30 trials. start states at each point. although shows one h-value of state at every step, ALBH-learning learning. LBARTDP was slower than LBH-learning, ALBH-learning, BAH-learning, AH-learning, converged a little slower than LBH- and it was faster than BH-learning and H-learning. size of all methods stores only 4 states that do not use auto- to the end the value also stores 6 or 7 states because the Bayesian network models. BH- on the other hand, store values for all 60 states. Fig. 16(b) store stores 6 or 7 states because linear. LH-learning that correspond These m&n& meth~& af a!! ~~~@-~~nlnmtnrv Y’-----‘, Fig. 16(a) the average exemplar cannot be very smooth without is not piecewise In the end, LBH-learning exploration. locations of the slippery-lane, while LBARTDP function of ARTDP its value function learning chnwc the ~vprzo~ “.__ ..” .*._ more exemplars R-value for each state and each action. However, 17 exemplars of R-values while BAH-learning of R-values. p~pmnlar b- W”“‘-.y’“’ than the methods and H-learning, ci~ .,.-_ in Fig. 16(a) because in the end, ALBH-learning they have to approximate the stores only store all 122 exemplars and AH-learning stores take only to make LBH-learning stores 6.7% of all exemplars of h-values and ALBH-learning functions actions may not be taken frequently for those actions smooth. Therefore, 13.6% of all exemplars of R-values. Because greedy actions, the suboptimal the value uses the same approximation methods as LBH-learning, ALBH-learning proportion of the total exemplars iinear regression function may not be piecewise need more exemplars stores a greater than LBH-learning. All H-learning methods using local the vaiue they the auto-exploratory methods enough though ALBH-learning it is not fully converged, than in the end because store more exempiars to store it accurately. in the beginning linear when and hence even These results suggest that local linear regression prove both the time and space requirements faster than its discounted counterpart. of H-learning, and Bayesian network models im- and make it converge much 208 I? Tudepdli. D. Ok/Artificitrl Intelligence 100 (1998) 177-224 40000 60000 80000 100000 Steps 80 60 0 0 20000 40000 60000 80000 100000 Steps Fig. 16. Number of exemplars for (a) BH (top), and (b) AH”.“‘.“,‘W” ,, ,, , _,,. t”l, . ,+“‘.tktxb, ,JJH”.““.“.‘K’s ,J+%0.‘“1, and LBARTDP , fjAH”.“2.“.‘nl~, and ALBH”~‘~“~‘X” (bottom). All methods used random exploration with 4 = 0.1. Each point is the mean of 30 trials. 5.3.2. Scaling of LBH-learning with dotnain size The goal of the experiment of this section learning rlnmoin” of Fig. 17/a\ UVIIIUI.. and ALBH-learning 1 I \u, LV U” t&a‘“. tn An thic by varying is to demonstrate the scaling of LBH- the size of the domain. We use the “Loop There are one AGV, two job generators 1 and 2, and for each destination AGV-Lot, Job-on-AGV, jobs generates 2. Each generator state is described by 5 features-Lane, at-Gen2-with the denotes tal number of possible tions from 1, . . . , 2n/5 in each of the short lane number locations of the AGV lanes and 2n/5 for lane 1 but from 1,. . . , n/5 locations Job-at-Genl, two conveyor belts 1 and belt with 0.5 probability. A and Job- from 1 to 4 and to- loca- takes values lanes. Job-on- in Fig. 17(a). The three in lane 1. AGV-Lot for the other is denoted by n. There are n/5 obvious meanings. The variable Lane takes values of the AGV’s location as shown I? Tudepulli, D. Ok/Artificial Intelligence 100 (1998) 177-224 209 14 59 60 . . job generator i I Lane 3 ’ 7~oG&b_it . \\ I Conveyor-belt 2 Job generator 2 Fig. 17. (a) The Loop domain (top) and (b) its dynamic Bayesian network (bottom). indicates of the job the destination and Job-at-Gen2 are 1 or 2 depending the size of the state space is 0 if the AGV has no job and If the AGV does not have a job (1 or 2) on the destina- take the move-forward it can AGV if it has a job. Job-at-Genl tion of the job waiting at the generators. Therefore, n x 3 x 2 x 2 = 12n. The AGV has 4 actions move-forward, move-backward, and, unload. The AGV can always action. if it has a job zones, and the optimal average make livery is made proportional receives a reward of +O.ln. ward of +O.O2n. The goal of the AGV conveyor-belts. Whenever ated. Fin action or the move-backward load at the loading zones. To for de- it re- to proper the AGV loads a job from job generator, a new job is gener- take the action unload at the unloading the same for all n, the immediate reward to its correct belt, it gets a smaller to n. If the AGV delivers If it delivers 6. L I \ “, Uaa”..” CnI” u, nnc..,..u YUJVUICI.. I,VI..“L.. 1”1 -,a.., U”,LLULaI. “ZSV ,,“.I ~PW is to move jobs from job generators it to the wrong belt, it can rewards take the action is load thir Anmain the Avn~mir a job RIVPC~DII n~twnrli rhnwc 17/h) flne for IUUCUIU) footllro the AGV-Lot Link, abstracts end locations of the lane, and middle for the other locations between feature distinguishes succinctly model AGV’s motion AGV-Lot, given Link, Lane and Action, to store the domain models ments feature. Link takes 3 values: end1 and end2 for the two the two ends. This to of the space require- the end locations of each lane from the rest, which in the loop. Since AAGV-Lot this representation from 4%~ + 52 to 1056. is useful is now independent reduces We used random exploration with 7 = 0.1 LBH-learning. AH-learning and ALBH-learning in this experiment and use auto-exploration. We set E to 1 and for H-learning 210 P. 7i~deIdli, D. Ok/Arti$cinl Intelligence 100 (1998) 177-224 0.25 0.2 i H(25 0 20000 40000 60000 Steps 0.25 c 0.2 0.15 0.1 0.05 0 L- 40000 Steps _I ~~ 60000 80000 100000 rewards per step for H and LBH average Fig. 18. On-line 5000 steps. Each point is the mean of 30 trials. H’.C~‘J.‘n’2( 125) LBH2.“.‘W”( 125) (bottom), where total number of AGV locations. E for ALBH-learning (top), and (b) LBH 1.(1.(1* (25), LBH1.“.“.‘nl7(50), LBH’.%o.‘m(75), are p,, and czo, and the number the superscripts in the Loop domain estimated over last (a) H’.“.“*(25), H i.().(Ms(50), H1,0.002(75), Hh%O.“~“( loo), LBH’.%o.tW( 100). and the in the parentheses is the is set to 1. sm= chnwn ad II U&V LU.L”U L’, CLIUL U..U “L.“L UL..4 . .._ “.._..._ varied n from 25 to 125 in steps of 25. The parameters of each algorithm nf M SW ~IWWWI hxr ad “I The on-line average rewards of H-learning those of AH-learning estimated on-line over the last 5000 steps and are averaged over 30 trials. in . . . and LBH-learning are shown and ALBH-learning the . .._ “..y_ ^__.... __ _ ‘O-. nf Fioc rnntinnc ~rrnr trial are shown in Fig. 18 and in Fig. 19. The values shown are for each value 19. 18 and As we can see, the convergence than those of H-learning longer sistently better with n. The convergence to travel over longer distances ALBH-learning, takes speeds of LBH-learning and AH-learning, for larger values of n, because and ALBH-learning and the difference to get new information. However, in LBH-learning the number of steps for convergence grows much more slowly are con- increases the AGV has and than in E Tadepalli, D. Ok/Art@cial Intelligence 100 (1998) 177-224 211 0 20000 40000 60000 a0000 100000 Steps 0.25 0 0 20000 40000 60000 80000 100000 Steps average rewards last 5000 i9. On-iine the estimated Fig. for AH over AH’,n.0008(50), the mean of 30 AH’~‘,“.‘WX”( 75). AH’~“~“.“a2( IOO), AH’.2.“~‘X”’ ( 125) (top), and (b) ALBH ‘.o.m3(25), ALBH’.%“.m”(50), ALBH’.7.“.W2 ( 75). ALBH2*“.“‘2 ( 100). and ALBH2.“.002( 125) (bottom), where a”, and the number the superscripts is the total number of AGV locations. E for ALBH-learning steps. Each point are pa and is set to 1. (a) AH’.“.002(25), in the parentheses and ALBH the Loop domain trials. step per in is average the on-line is attributable H and AH. Also higher than those of H-learning difference LBH-learning ods have a buiit-in decay mechanism. in Section 4.2 illustrate, potentially are not decayed useful parts of the state space. rewards of AH-learning respectively and LBH-learning to two factors. First, the rates of exploration of H-learning in this experiment, whereas Perhaps more auto-exploratory methods are more effective importantiy, and ALBH-learning are for each value of n. This and the auto-exploratory meth- as the experiments in exploring only The numbers of the stored exemplars of LBH-learning over 30 trials, are shown in Fig. 20 in comparison The bigger and AH-learning. the value of n, the larger the number of exemplars and ALBH-learning, averaged to the exemplars stored by H-learning stored 212 I? Tadeplli, D. Ok/Arrifcial Intelligence 100 (1998) 177-224 a00 600 2500 Steps 0 0 20000 40000 60000 60000 100000 Steps Fig. 20. Number of exemplars of (a) H-learning and LBH-learning (top) with random exploration with (bottom) with oniy auto-expioration in <he ioop domain. 7) = 0.1, and (b) AH-learning and ALBH-learning The results are mean averages over 30 trials. and AH-learning. For LBH-learning, by H-learning almost remains constant with increasing n. ALBH-learning but the absolute number of exemplars increasing especially n. This for suboptimal its R-value is because increases function actions. the absolute number of exemplars like LBH-learning performs slightly more than LBH-learning by is less smooth than the h function, F! Tudepalli, D. Ok/Art#cial Intelligence 100 (I 998) 177-224 213 gc LI m 50 45 - 2 z 5 0 r s a (rl E k? a, D s! 0 (0 -6 35 30 25 20 15 10 5 I I I I I I 1 ALElH.max.ralio - ALEiH.final.ratio -+- . _ LBH.max.ratio LBH.final.ratio -+--- ----. _ * ‘\ t “0 *,\ \\ *\ \\ \\ \’ \’ \’ \\\ ‘\‘.._ \ *.Q::::::~ “-__:y____ --t--X:~~~,,_. + 01 0 I 20 I I Tit total nt?nber , 80 I I I 100 120 140 of AGV locations(n) Fig. 21. The maximum ALBH-learning 30 trials. and final ratios of stored exemplars and as a function of the domain size II in the Loop domain. The results are mean averages over to all possible exemplars for LBH-leaming (usually and ALBH-learning during indicate and ALBH.max.ratio and ALBH.final.ratio Fig. 21 shows the percentage of the number of stored exemplars out of the total locations (n) of the AGV in this the maximum values of this ratio and exemplars as a function of the total number of possible indicate domain. LBH.max.ratio learning for LBH-learning LBH.final.ratio the values of this ratio for LBH-learning and ALBH-learning after the last step. The ratio is maximum at 38.8% for LBH-learning and 44.3% gradually n = 125. The final ratios are lower than the maximum at the end is a lot smoother tal0=r greedy artinnr ..rrfiu..u, 1.u 1. . . . ..I” it stores more exemplars and ALBH-learning increases. locations n = 25, and when the value function always nrtinnc SWP not fully converged. Hence than it is in the beginning. Because ALBH-learning Fig. 21 clearly shows that LBH-learning and to 15.9% for ALBH-learning the total number of AGV itc I?-vajues for subontimsl store far fewer exemplars to 9.4% for LBH-learning Y’--“-- __..V..U _-_ than LBH-learning. for ALBH-learning at the beginning) and AH-learning than H-learning ratios because reduces when as IZ are smoother and LBH-learning and LBH-learning because H-learning The value functions for H-learning, LBH-learning, AH-learning, and ALBH-learning to the optimal value function. than those for AH- take random actions the state space more evenly, while AH-learning and thus explore take only greedy actions and thus do not thoroughly at the end of training are shown in Fig. 22 in comparison The value functions for H-learning learning and ALBH-learning with 0.1 probability the and ALBH-learning usually suboptimal has a smooth piecewise random expioration, which iinear Vaiue function when using forces the agent to visit all states more or less uniformly. Due to local linear regression, than H-learning’s value function and ALBH- LBH-learning’s value function. Thus, LBH- learning’s value learning’s in solid value lines in Fig. 22. regions of the state space. A learning method based on H-learning value function function function is smoother is the closest to the optimal value function, than AH-learning’s is smoother explore shown 214 P Tudel~ulli. D. Ok/Art@cicll Intelli~enue 100 (1998) 177-224 ptimal Value Function 0 10 20 30 40 AGV 50 location 60 70 80 90 al 5 i 12 8 6 0 0 10 20 30 40 AGV 60 50 location 70 80 90 (top) and (b) AH and ALBH Fig. 22. The kvalues as a function of AGV (bottom), when AGV does not have a job, generator I has job 2 and generator 2 has job 1, and n = 100. The location is as shown in Fig. 17(a). The optimal value function is shown in solid lines in the two plots. AGV location for (a) H and LBH The results than H-learning and memory use. in this domain show that LBH-learning and ALBH-learning and AH-learning with the domain size, both in terms of learning scale better speed 5.3.3. Scaling to multiple linear features In this section, we demonstrate LBH-learning and ALBH-learning in the “Grid” domain shown in Fig. 23(aj, which has two iinear dimensions. At one corner of the 15 x 15 grid, there is a job generator and at the opposite corner, conveyor-belt. The AGV can take any action among four admissible there is no obstacle in the middle the obstacles. The AGV also has the load or the unload action available at the there is a destination actions -move-north, or wall represent move-south, move-east, to. The dark squares and move-west-if the AGV wants in the location to move P Tadepalli, D. Ok/Art@cial Intelligence 100 (1998) 177-224 215 Job AGV obstacle Job generator <zC--~z-> ma(L-$Ez$e Fig. 23. (a) The Grid domain (top) and (b) its dynamic Bayesian network (bottom). reward per step is 1. Fig. 23(b) appropriate average Grid domain, which only on the Action variable. domain model from 3076 to 24. corner. It receives a reward of 58 when it delivers a job, so that the optimal for the the effect of any available action depends the shows the dynamic Bayesian network the space requirements In fact, it reduces is very compact for storing since Each point in Fig. 24 is the on-line average reward for 30 trials calculated over the effect of setting E to 0.4, 0.7, 1.0, and 1.3 on the the last 5000 steps. It shows reward of LBH-learning with random exploration with v = 0.1, and on the average finds the reward of ALBH-learning. With E = 0.4, 0.7, and 1.0, LBH-learning average policy much faster than H-learning does. With E = 1.3, its speed decreases, gain-optimal policy. because policy much Similarly, ALBH-learning faster to a than AH-learning suboptimal policy, as e is too high and it does not take any random exploratory actions. Thus its average with E = 0.4, 0.7, and 1.0 finds the gain-optimal too high a value of E sometimes makes does. However ALBH-learning with e = 1.3 converges reward remains zero. to a suboptimal it converge Fig. 25 shows the number of exemplars stored by LBH-learning and ALBH-learning T -IT 1~ Lam-learning of E, except when E is too high, ing methods converge space. The value functions of LBH-learning sufficient exemplars. to a suboptimal exploration usuaiiy i.e., 1.3 in this case. When E is too high, and ALBH -learning. store fewer exempiars with higher vaiues learn- the state are not smooth without them store a large number of policy and do not sufficiently and ALBH-learning explore of the state space, which makes 216 P. Tadepalli, II. Ok/Art$cial Intelligence 100 (1998) 177-224 0.6 LBH WI epsilon=O.rl -+ LBH wl epsilork0.7 -+--. LBH w/ epsilon=1 .O -a-- LBH w/ epsilon=1 .3 .++. H- 40000 60000 80000 100000 Steps ALBH w/ epsilon=0.4 +- ALBH WI epsilon=0.7 -+-. ALBH w/ epsilon=1 .O -o-- ALBH w/ epsilon=l.3 .++... _ AH - 0 20000 40000 60000 80000 100000 Steps Fig. 24. On-line average rewx& v = 0.1, and (b) AH8,'J.'KW'3 is the mean of 30 trials over the last 5,000 steps. per step iJJH3.!!.!%!2 (top j with random expioration with and ALBH 5~(‘~‘n”‘7 (bottom) without exploration in the Grid domain. Each point for taj @.!!.%!3 and The learning domain, to AGV’s for multiple final value of H-learning, AH-learning, functions in Fig. 26 in comparison are shown the true optimal value function of H-learning and ALBH-learning location. LBH-learning LBH-learning, and ALBH- to the optimal value function. For this linear with respect this value function is piecewise approximate Since the obstacles linear features successfully using local linear regression. in the middle of the grid are not states, they do not have h-values. in the plots of Fig. 26. Like the experimental Some arbitrary vaiues iower tnan tine h-vaiues of states surrounding their locations (Fig. 22)) the value functions than the corresponding for LBH and ALBH AH, respectively). them are assigned to in the Loop domain for H and LBH (in the left half of Fig. 26) are smoother (in the right half). The value functions (H and than those without approximation auto-exploratory are smoother versions learning results P Tudepalli. D. Ok/Artificial Intelligence 100 (1998) 177-224 217 500 , I I LBH w/ epsilon=0.4 + LBH WI epsilon=0.7 -+-. LBH wl epsilon=1 .O -o-- -x- LBH w/ epsilon=l.3 200 150 100 50 0 0 20000 40000 60000 80000 IOOGQO Steps 1800 , I I 1 1 ALBH w/ epsilow0.4 +- ALBH w/ epsilon-O.7 -+-- ALBH w/ epsilon=1 .O -0.- a--- ALBH wl epsilond.3 AH - 600 400 200 I I 0 20000 40000 I 60000 # 80000 I 100000 Steps Fig. 25. Average (9 = exploration domain. number of exemplars of 30 trials of (a) H”.“N’3 and LBH”v”~* 0.1) and (b) AH’.“.lxx” and ALBHS,0.0007 (bottom) without exploration (top) with random in the Grid The results of this section demonstrate that local linear regression and Bayesian network model learning extend to two-dimensional spaces, and significantly reduce the learning time and memory requirements. 6. Discussion and future work The basic premise of this paper is that many real-world domains demand optimizing average reward per time step, while most work in Reinforcement Learning is focused on optimizing discounted total reward. Because discounting encourages the learner to sacrifice long-term benefits for short-term gains, using discounted RL in these domains could lead to suboptimal policies. We presented a variety of algorithms based on H- 218 P Tudepctlli, D. Ok/Artificial lntelligencr 100 (1998) 177-224 Fig. 26. The value functions of H-learning and ALBH-learning H-learning isset (middle and LBH-learning right), to I. (top in comparison use random exploration with 7 = 0. I E for LBH-learning left), AH-learning (top right) LBH-learning to the optimal value function (bottom) (middle left), in the Grid domain. and ALBH-learning learning, a model-based method designed step, and demonstrated of parts of this work include their usefulness [ 411, [ 321, and [ 421. to optimize in AGV scheduling the gain or average reward per time tasks. Earlier presentations learning, Fig. 27 shows the family of algorithms obtained by adding auto-exploration, Bayesian to H-learning. We can choose any on the domain, our needs, and the available. Based on our experiments, we can make network model combination of these resources and the prior knowledge the following recommendations: three extensions, and local linear regression depending I? Tadepulli, D. Ok/ArtiJicial Intelligence 100 (1998) 177-224 219 Fig. 27. The family of algorithms based on H-learning l When we know an upper bound pmax on the gain, - use auto-exploration with p initialized to something higher than pmax. l When we cannot afford tuning the exploration parameters, - use H-learning with a small p > 0 and LY = 1 with some exploration strategy. l When we have only a limited amount of space and time, - use local linear regression. l When the structures of the Bayesian network action models are available, for the Bayesian network action models. - use them to learn parameters “y”““‘..““” “‘~““C..“‘” 117 ‘Ml L4, )“_,. approaches reinforcement learning methods Learning point of view fnr SWP~~OP-I-PWW~ nntimintinn I”. U.w.Ubv ._.,..... There is an extensive body of literature on average-reward optimization using dynamic [ 6,15,34]. Mahadevan gives a useful survey of this literature [ 241. Schwartz and Singh present model- There RW nt bact twn _.._-- ..&” . . .w..v. ..,., that have been proved to converge under [ 4,161, Bertsekas’s algorithm the Average- into a stochastic shortest path algorithm with slowly changing edge immediate programming from Reinforcement frpp RT alonrithmc 1,-v I\_ average-reward suitable conditions reward RL problem costs rewards, relation the immediate that is equivalent the h values. p is estimated by on-line averaging of the h value of a reference state rather than by on-line averaging of is very rewards, as in H-learning. The basic H-learning adjusted similar is due to expioration, which i.e., p - R;(u), where p is the gain of the current greedy policy and Ri(u) reward of executing action u in state i. Hence, it uses a recurrence to the Algorithm B of Jalali and Ferguson [4,6]. The edge costs are essentially is ignored by Jaiaii and Ferguson. the negated average-adjusted [ 161. The main difference in Section 3 for updating is based on converting to Eq. (7) immediate algorithm is In this paper, we have been mainly Bias-optimality, gain-optimality. than gain-optimality total reward obtained before entering a recurrent All gain-optimal It seeks [34,37]. or Schwartz’s T-optimality, concerned with average-reward optimality or is a more refined notion the expected state, while also being gain-optimal. that maximizes to find a policy policies are not bias-optimal. H-learning and R-learning can find the 220 P. Tndepulli, D. Ok/Art$cial Intelligence 100 (1998) 177-224 for unichain MDPs only set of states, and all states, if all gain-optimal including the non-recurrent infinitely often using some exploration strategy. To find the bias-optimal it is necessary to select bias-optimal in every state using more refined criteria. Mahadevan to find the bias-optimal policies is based on solving a set of nested recurrence relations iz and ‘W for each state, since the iz vaiues aione are not sufficient policy. Bias optimal policies appear to have a significant advantage in operations systems studied control queuing policies ones and R-learning bias-optimal the same recurrent visited for more general unichains, the gain-optimal both H-learning [ 23,251. His method vaiues bias-optimal domains literature such as the admission [ 23,251. Auto-exploratory H-learning policies give rise to states, are policies from among extends for general unichains actions for two to determine a in some research belongs to the set of exploration The general techniques idea here is to initialize so that a state (or state-action than a well-explored that can be the pair) that is if the latter this effect for each domains, or when the updates are done using the minimax their high utility. Koenig and Simmons the value function and by giving a negative penalty pairs are never state, even achieve that the Q-values of state-action less than the “admissibility” to the A* algorithms, this is called is executed, the more closely its Q-value to execute an action with the highest Q-value in one case, its Q-value may forever remain higher than in the same state. Since the Q-values of other actions are the best. In the those of the other the to execute that the executed action decrease to a value below is actually a chance an action the reward under uncertainty”. [ 201. In deterministic functions appears better as “optimism and explored to have a relatively classified value function not sufficiently is known simply by zero-initializing action scheme, this ensures true optimal values. condition. The more approaches will have one of two effects: the Q-values of other actions non-underestimating, other case, its Q-value would eventually available other potentially Kwlhlinm’r L.UV.“L”‘& In analogy frequently its real value. Hence choosing in the same state. This gives best actions, Tntmwal E’rtimatinn 0 *.mLVL .U, YY..LI.U.L”.. of the same idea, and is applicable interval of the value function bounds of their confidence in the confidence the confidence Hence, picking actions using effect. intervals satisfy this implies intervals. actions L.L”.LI”Y au yuuI... “.. thus encouraging ITE’\ methnrl \A_, to stochastic domains for each state, and picks actions nn 2 tnnw mnhictirntd vm-cinn ““y”‘““~“.“.. .vIY.V.. [ 171. It maintains a confidence the upper- is interval with a high probability, we can say that the upper bounds of If the true value of a state (or state-action pair) that maximize .ll”Iv . property with a high probability. the above admissibility this upper bound will have the desired auto-exploratory the algorithm exploration. ic hacd In AH-learning, in the right-hand to giving a high penalty to OS. Since p is subtracted it is equivalent . _^^_ lzul the same effect is achieved by initializing R values AH-learning, Simmons method the optimal gain, and the system stops exploring ically characterizing problem. The idea of Auto-exploratory well. However, of p fluctuates much more in R-learning p to a high value and the side of the update equation of for each action as in Koenig and to loop. Theoret- is an important open learning as experiments with R-learning, we found that the value to be the conditions of convergence of AH-learning learning can be adapted to model-free to the optimai poiicy, p converges . As the system converges states not in the optimal unless (Y is initialized than in H-learning, in our preliminary R Tudepalli, D. Ok/Arrijicial Intelligence 100 (1998) 177-224 221 to update learning work very small. However, a small LY will have the consequence Another possibility of the confidence is to maintain a confidence interval interval the R values as in the IE method. of slowing down learning. for p and to use the upper bound algorithms [ 3,28 j . Once they can be used to propagate more to converge because it has been observed is based on model-free of the controi poiicy and can be iearned such as Q- they have simpler that they do need more real-time reinforcement [46]. Model-free algorithms are easier to implement, because Most learning update procedures. However, experience independent action models are learned, update of the value function by considering than the only next state that was actually to learn of the stumbling that representing practical. Dynamic Bayesian networks have been used by a number of researchers represent action models they can also be useful learning and to shorten dynamic Bayesian network as prior knowledge and learns only the conditional probability tables. One of the important networks automatically. they do not learn explicit action models, which are the fairiy quickiy in each all possible next states of an action rather reached. They can also be used to plan and [40]. However, one is too much space to be to that from simulated blocks them explicitly time. Our current method uses the structure of the experience for the model-based theoretic planning represent is to learn the structure of these in decision to compactly as transition matrices consumes as in the Dyna architecture [7,12,19,30]. We showed to be more widely used future research problems the action models for reinforcement the learning information algorithms We showed that the piecewise linearity of the h-function It is a member of a family of regression the name of Locally Weighted Regression technique with a sound statistical basis that also takes into account (LLR). using Local Linear Regression niques that go under is a regression locality of the target function. As a result, places, but complex LWR in reinforcement including learning, (LWR) can be effectively exploited tech- [2,29]. LWR the in some of [36]. Our results suggest that are smooth applications it can fit functions a juggling robot in other places. There have been many successful (1 1 R\ \---., .wneriallv -Vrv-“.., lint=nr rwmwcinn ,... “.._ .“6’-‘““” is 2 pr~p_isincr nnnrnarh tn nnnrnvimatinn -~y-“.--A’“‘A”’ -- e -l-r-‘--” combines with approxi- that it synergistically it does the that lnral ._...& ..,“... for Average-reward RL. We also showed mating domain models using Bayesian networks. However, being a local method, not scale well with the number of dimensions value function or the conveyor belts feature selection methods, or instead use more aggressive approximation methods the neural networks or regression Another as the location of the AGV, can be real-valued. e.g., the number of AGVs, machines, it with other to combine like is to extend our work to domains where some features, such of the state space, especially when trees to scale learning It may be necessary in these dimensions, important problem to larger domains in our domains. is nonlinear [ 9,11,21]. for a given To apply our methods to the full-scale AGV scheduling, we need to be able to handle ,- .__* the number 01 AtiVs muitipie AGVs. Since AGVs are stiii very expensive, minimizing needed floor AGVs as a single agent does not scale because system some positive scheduling all the to the AGV of the sets of actions of all the AGVs. There have been including Crites’s work on [ 11,431. reinforcement a bank of elevators, and Tan’s results is the cross-product results is an important practical problem. Treating learning, in a hunter-prey the set of actions available in multi-agent simulation factory 222 P Tadepcdli. D. Ok/Arrijicinl intelligence 100 (1998) 177-224 experiments Our preliminary policy can be learned as a mapping Both AGVs share and update policy. This approach converges AGVs as a single agent. in a simple domain with 2 AGVs indicate that an optimal from global state space to actions of a single AGV. the same optimal the two faster to a global optimal policy the same value than treating and follow function Another important assumption n _I_,~. rromems /nA1sT\n (rumors) observable. Recently, Markov Decision for solving POMDPs currently appear that building the form of high-level in this area. progress in more domain-specific that we made that needs relaxing is that the state is fully there has been some work to extend RL to Partially Observable even the best aigorithms for large problems. We believe in into the learning algorithms to further ILL, 53J. unrortunateiy, to be impractical prior knowledge features or constraints on the value function \ mm 0’11 7, 1.~ is essential 7. Conclusions Reinforcement learning has proved successful reinforcement in a number of domains learning methods including that are currently sensitive is prohibitively total reward, whereas a family of algorithms the most natural criterion average-reward RL method called H-learning, lead to suboptimal the exploration the average to optimize average policies, and strategy. We presented is to optimize discounted RL methods such as the AGV scheduling that employing in reward per re- to the discount some real-world domains. However, most popular optimize discounted many domains time step. We showed ward could factor and and empirically demonstrated model-based learning method, their usefulness. We presented that other previously which outperforms in the form learning method can exploit prior knowledge our structure, network time and can of dynamic Bayesian npprlprl tn learn th,=m WP 11cn rhnwd that the va11w fmnctinnc nf H-l~nmino cm ,,V”U,,” C” LVULI, LI,VLLB. ,.V c..“, .,.L”..“.. L..... .a-_ ...l..v _” ..vC.V.I y .,. _- .__..--.. b --.. re- linear be effectively to domains with large number of dimensions, multi-agent RL, main, and these prob- lems and to a variety of real-world tasks. of its action models improve both scaling analysis. We believe an auto-exploratory studied regression. Several open problems that we laid a foundation to apply average-reward including theoretical strategies. We showed version of our the space and approximated reinforcement exploration to study learning based on a using local Acknowledgments n -__- acknowledge We gratefully the support of NSF under grant number and .*r the support of UNK under grant number N666i4-95-l-UXI. we thank Chris Aikeson, Leemon Baird, Andrew Barto, Dimitri Bertsekas, Tom Dietterich, Leslie Kaelbling, Jude Shavlik, Sridhar Mahadevan, Toshi Minoura, Andrew Moore, Martin Puterman, Satinder Singh, and Rich Sutton topic. We thank Sandeep Seri for his help with testing our programs. We thank the reviewers of suggestions. this paper for their excellent IRI-9520243 discussions interesting for many on this . ,x-r_ I? i%depnlli, D. Ok/ArtificiuI Intelligence 100 (1998) 177-224 223 References 11 I D.W. Aha, D. Kibler, M.K. Albert. Instance-based learning algorithms, Machine Learning 6 (1991) 37-66. 121 C.G. Atkeson, A.W. Moore, S. Schaal, Locally weighted learning, Artificial Intelligence Review I I (1997) I l-73. 131 A.G. Barto, S.J. Bradtke, S.P. Singh, Learning to act using real-time dynamic programming, Artificial Intelligence 73 (1995) 81-138. IdI D Rprtsekns , . , I I _ . ..I_.._. , .A_ new value-iteration method for the averaop aI Technical Report LIDS-P-2307, MIT, Boston, MA, 1995. rnst _“L.. “-‘.a. . . . . v r’vb’.. rlvn.lmir nrnornmmino . . . . . . ‘..b F..,“.” nmhlmm . . . . [ 5 ) D.P. Bertsekas. Distributed dynamic programming, 161 D.P. Bertsekas. Dynamic Programming and Optimal Control, Athena Scientific, Belmont, MA, 1995. IEEE Trans. Automatic Control 27 (3) ( 1982). 171 C. Boutilier, R. Dearden, M. Goldszmidt, Exploiting structure in policy construction, in: Proceedings 14th International Joint Conference on Artificial Intelligence (IJCAI-95). Montreal, Que., 1995. 18 1 J. Boyan, A.W. Moore, Generalizing reinforcement learning: safely approximating the value function, in: Proceedings Neural Information Processing Systems, 1994. 191 L. Brieman, J.H. Friedman, R.A. Olshen, C.J. Stone, Classification and Regression Trees, Wadsworth International Group, Belmont, MA, 1984. [ IO) G.C. Canavos, Applied Probability and Statistical Methods, Little, Brown and Company, Boston, MA, 1984. 1 1 I ] R.H. Crites, A.G. Barto, hl^.._ “I r..F^.....“r:,... D..-,.,.“l:..” C..*+,..“^ r r”~en>lrl?$ 0~.\rcrllJ, ,*z;u,a, ,,II”,,lldLI”II Improving elevator performance using reinforcement &“A ,*,m, 17jr”. \“,oJ. 8 h”lT D..,.rr. 0^...I._rl”0 , 1*111 r ,EiJJ, \-Urr”rruge, learning, ronr in: Advances in I 121 T. Dean, K. Kanazawa, A model for reasoning about persistence and causation, Computational intelligence 5 (3) (1989) 142-150. [ 13 I C.W. Gates, The reduced nearest neighbor L 14 1 P.E. Hart, The condensed nearest neighbor 1 IS] R.A. Howard, Dynamic Programming rule, IEEE Trans. Inform. Theory (1972) 431-433. rule, IEEE Trans. Inform. Theory 14 ( 1968) 515-516. and Markov Processes, MIT Press and Wiley, Cambridge, MA, 1960. [ 16) A. Jalali, M. Ferguson, Computationally efficient adaptive control algorithms for markov chains, in: IEEE Proceedings 28th Conference on Decision and Control, Tampa, FL, 1989. [ I7 I L.P. Kaelbling, Learning 1181 L.P. Kaelbling, M.L. Littman, A.W. Moore. Reinforcement in Embedded Systems, MIT Press, Cambridge, MA, 1990. learning: a survey, J. Artificial Jntelligence Research 4 ( 1996) 237-285. [ 191 U. Kjaerulff, A computational scheme for reasoning in dynamic probabilistic networks, in: Proceedings 8th Conference on Uncertainty in Aniticiai inteiiigence ( i992 j i ii-i29. 1201 S. Koenig, R.G. Simmons, The effect of representation and knowledge on goal-directed exploration with reinforcement-learning algorithms, Machine Learning 22 ( 1996) 227-250. [21 I L-J. Lin, Self-improving reactive agents based on reinforcement learning, planning, and teaching, Machine Learning 8 ( 1992) 293-32 I. 122) M.L. Littman, A. Cassandra, L.P. Kaelbling, Learning policies for partially observable environments: scaling up, in: Proceedings of International Machine Learning Conference, San Fransisco, CA, 1995, pp. 362-370. I23 I S. Mahadevan, An average reward reinforcement learning algorithm for computing bias-optimal policies, in: Proceedings National Conference on Artificial Intelligence (AAAI-96). Portland, OR, 1996. 124) S. Mahadevan. Average reward reinforcement learning: foundations, algorithms, and empirical results, Machine Learning 22 ( 1996) lS9- 195. 125 I S. Mahadevan. Sensitive discount optimality: Unifying discounted and average reward reinforcement learning, in: Proceedings international Machine Learning Conference, Bari, Italy, 1996. [26] S. Mahadevan, J. Connell, Automatic programming of behavior-based Intelligence 55 ( 1992) 3 I I-365. learning, Artificial robots using reinforcement I271 W.L. Maxwell, J.A. Muckstadt, Design of automatic guided vehicle systems, Institute of Industrial Engineers Trans. 14 (2) ( 1982) I 14-124. 1281 A.W. Moore. A.G. Atkeson. Prioritized sweeping: Reinforcement learning with less data and less time, Machine Learning J. 13 (1993) 1033130. 224 P Ttrdeplli. D. Ok/Artijicicd Intelligence 100 (1998) 177-224 1291 A.W. Moore, C.G. Atkeson, S. Schaal, Locally weighted learning for control, Artificial Intelligence Review I1 (1997) 75-I 13. I30 1 A.E. Nicholson, J.M. Brady, The data association problem when monitoring robot vehicles using dynamic belief networks, in: ECAI 92: 10th European Conference on Artificial Intelligence Proceedings, Vienna, Austria, Wiley, New York, 1992, pp. 689-693. 13 I ] D. Ok, A study of model-based average reward reinforcement learning, Ph.D. Thesis, Technical Report, 96-30-2, Department of Computer Science, Oregon State University, Corvallis, OR, 1996. 1321 D. Ok, P. Tadepalli, Auto-exploratory average reward reinforcement learning, in: Proceedings National Conference on Artificial Intelligence (AAAI-96); Portland, OR; 1996. I33 I R. Parr, S. Russell, Approximating optimal policies for partially observable stochastic domains, in: Proceedings National Conference on Artificial Intelligence (AAAI-94). Seattle, WA, 1994, pp. IO88- 1093. I34 1 M.L. Puterman, Markov Decision Processes: Discrete Dynamic Stochastic Programming, John Wiley, 1351 L361 1371 I381 1391 1401 1411 1421 1431 1441 I451 1461 1471 New York, 1994. S. Russell, P. Norvig, Artihcial Intelligence: A Modern Approach, Prentice-Hall, Englewood Cliffs, NJ, 1995. S. Schaal, C. Atkeson, Robot juggling: an implementation of memory-based learning, in: IEEE Control Systems. Vol. 14, 1994, pp. 57-71. A. Schwartz, A reinforcement learning method for maximizing undiscounted rewards, in: Proceedings 10th International Conference on Machine Learning, Morgan Kaufmann, San Mateo, CA, 1993. S.P. Singh, Reinforcement “r,.^,.,.,l:..rrr rlvceeu~ngs L*ULIVIILLI L-UIII~I~~LC “11 T\LLIIICILLI uar;rrlg,r;rrcr; \rrr\r\r-7-1, hTnr:-..nl rrr"t>r‘."^a rn+rrll:",...^r IA n n, CIA\ _.. A.&c,:", learning algorithms for average-payoff markovian decision processes, in: Seatt!e, WA, MIT PiESS, Cambridge, MA, 1994. R.S. Sutton, Learning to predict by the methods of temporal differences, Machine Learning 3 ( 1988) 9-44. R.S. Sutton, Integrating architectures for learning, planning and reacting based on approximating dynamic programming, in: Proceedings Seventh International Conference on Machine Learning, Austin, TX, 1990. P. Tadepalli, D. Ok, H-learning: A reinforcement learning method for optimizing undiscounted average reward, Technical Report 94-30-l. Department of Computer Science, Oregon State University, 1994. P. Tadepalli, D. Ok, Scaling up average reward reinforcement learning by approximating the domain models and the value function, in: Proceedings 13th International Conference on Machine Learning, 1996. M. Tan. Multi-agent reinforcement learning: independent vs. cooperative agents, in: Proceedings 10th International Conference on Machine Learning, Morgan Kaufmann, San Mateo, CA, 1993. G. Tesauro, Practical issues in temporal difference learning, Machine Learning 8 (3-4) __- _-_ ( 1992) 257-277. S. Thrun, The role of exploration in learning control, in: Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, Van Nostrand Reinhold, New York, 1994. C.J.C.H. Watkins, P. Dayan, Q-learning, Machine Learning 8 ( 1992) 279-292. W. Zhang, T. Dietterich, A reinforcement learning approach to job-shop scheduling, in: Proceedings 14th International Joint Conference on Artificial Intelligence (IJCAI-95). Montreal, Que., 1995. 