Artificial Intelligence 170 (2006) 1137–1174www.elsevier.com/locate/artintControlled generation of hard and easy Bayesian networks:Impact on maximal clique size in tree clusteringOle J. Mengshoel a,∗, David C. Wilkins b, Dan Roth ca RIACS, NASA Ames Research Center, Mail Stop 269-3, Moffett Field, CA 94035, USAb Center for the Study of Language and Information, Stanford University, Stanford, CA 94305, USAc Department of Computer Science, University of Illinois, Urbana-Champaign, 201 N. Goodwin, Urbana, IL 61801, USAReceived 1 July 2005; received in revised form 20 September 2006; accepted 24 September 2006Available online 30 October 2006AbstractThis article presents and analyzes algorithms that systematically generate random Bayesian networks of varying difficulty levels,with respect to inference using tree clustering. The results are relevant to research on efficient Bayesian network inference, suchas computing a most probable explanation or belief updating, since they allow controlled experimentation to determine the impactof improvements to inference algorithms. The results are also relevant to research on machine learning of Bayesian networks,since they support controlled generation of a large number of data sets at a given difficulty level. Our generation algorithms, calledBPART and MPART, support controlled but random construction of bipartite and multipartite Bayesian networks. The Bayesiannetwork parameters that we vary are the total number of nodes, degree of connectivity, the ratio of the number of non-root nodesto the number of root nodes, regularity of the underlying graph, and characteristics of the conditional probability tables. The maindependent parameter is the size of the maximal clique as generated by tree clustering. This article presents extensive empiricalanalysis using the HUGIN tree clustering approach as well as theoretical analysis related to the random generation of Bayesiannetworks using BPART and MPART.© 2006 Elsevier B.V. All rights reserved.Keywords: Probabilistic reasoning; Bayesian networks; Tree clustering inference; Maximal clique size; C/V -ratio; Random generation;Controlled experiments1. IntroductionEssentially all inference problems studied using the Bayesian network (BN) formalism are known to be computa-tionally hard in the general case [14,60,66]. Given the central role of BNs in a wide range of automated reasoning ap-plications, for example in medical diagnosis [3,43,67], probabilistic risk analysis [9,45], language understanding [10,12], intelligent data analysis [40,54,61], error correction coding [27,28,48,49], and biological pedigree analysis [68],developing efficient algorithms for these inference problems is an important research problem. The performance ofexact Bayesian network inference algorithms—including tree clustering algorithms [2,33,38,39,46,65], conditioningalgorithms [16,17,22,32,57,58,64], and elimination algorithms [19,47,72]—depends on the treewidth or the optimal* Corresponding author.E-mail addresses: omengshoel@riacs.edu (O.J. Mengshoel), dwilkins@stanford.edu (D.C. Wilkins), danr@cs.uiuc.edu (D. Roth).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.09.0031138O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174maximal clique size of a BN’s induced clique tree [5,17,20,21]. Treewidth was initially a theoretical concept relatedto graph minors [59]; it has more recently been established that the notion of treewidth plays a key role in the analysisof algorithms [8,20,44].A significant component of research on inference in BNs has to be experimental and rely on the use of BN instances.Similar experiments are needed and have indeed been performed for other problems, including the satisfiability prob-lem (SAT) [1,13,26,55,62,63]. For SAT, it has been established empirically that there is a phase transition in theprobability of satisfiability of an instance drawn from a certain distribution [55]. This phase transition phenomenonhas been found to be closely related to a parameter describing the constrainedness of instances, namely the ratio be-tween the number of variables V and the number of clauses C, denoted the C/V -ratio. Interestingly, it has been foundthat algorithmic hardness also varies with the C/V -ratio, at least for certain algorithms [55]. As the C/V -ratio is var-ied, there is a variation in problem difficulty (or hardness), as measured in mean or median inference time for certainalgorithms across a sample of problems. Maximal hardness for several algorithms occurs in the phase transition region.Experimental work in Bayesian network inference can also be performed using randomly generated instances.In this article, we investigate the following research questions: How should BNs for experimentation be randomlygenerated, such that their computational hardness can be understood, analyzed, and controlled? More specifically, isit fruitful to generalize the C/V -ratio from SAT to a BN setting? If it is, what is the relationship between the C/V -ratio and treewidth or maximal clique size? Answering these research questions is important for several reasons.Generating problem instances randomly, a common practice in the BN community [7,17,34,35,41,56,69,70], mayresult in easy inference problems that do not present a challenge to inference algorithms [4,11,25], even thoughworst case complexity results show that both exact and approximate MPE computation is NP-hard [1,66]. In thisarticle we extend previous research on randomly generating BN instances and present an experimental paradigmfor systematically generating increasingly hard random Bayesian network instances for tree clustering. We describetwo algorithms for controlled generation of BNs, the bipartite (BPART) and multipartite (MPART) constructionalgorithms, and prove several properties for the BNs that they construct. For the BPART case, this includes thedistribution over the root node out-degrees and the minimum out-degree as well as the (small) probability that anirregular BN is also regular. For MPART networks [41] we analyze the relationship to BPART BNs and in particularpresent a formula for the probability that an MPART BN is bipartite. We characterize properties of the BN generationalgorithms in order to better understand the factors that in turn contribute to the hardness of inference, so that thoroughbenchmarking and comparison of algorithms can be performed.The inference approach we focus on, tree clustering as implemented in the HUGIN algorithm, was introduced as abelief updating algorithm [46], and was later extended to encompass belief revision [18]. Thus, in the tree clusteringapproach, computing marginal distributions and most probable explanations (MPEs) are closely related. In particular,they both depend on the total clique tree size as well as the maximal clique size of a BN’s clique tree. In a BN, let V bethe number of root nodes and C the number of non-root nodes. We show that the C/V -ratio is a key parameter for BNinference hardness, as it is for SAT [11,55]. Analytically, we provide a conservative lower bound on total clique treesize and introduce a new class of BNs, only-child BNs, for which we give sufficient conditions for Hamiltonicity andlongest cycle. Formation of cycles, including Hamiltonian cycles, is important because they often need fill-in edges inorder for a triangulated graph to be constructed, and cycles thus significantly contribute to clique tree size.Even when the topology is restricted to the BPART or MPART types, we identify several input parameters thatcan be varied when randomly generating BNs. We empirically study a few of these parameters in detail and showhow changing them affects properties of the generated BNs which again can increase computational hardness for treeclustering. For both the BPART and MPART constructions, generating random networks may result in very easyinstances, but a careful selection of the parameters along the dimensions we discuss, even while keeping the size ofthe networks fixed, gradually increases the complexity of inference and results in networks that the tree clusteringalgorithm cannot handle. A main empirical result is that the C/V -ratio can be used to predict an upper bound on thetreewidth (or optimal maximal clique size) of the induced clique trees for samples of BPART and MPART BNs. Ourselection of families of hard networks extends research on generating hard instances for the satisfiability problem [4,11,25,55] as well as existing research in the BN community [34,41,70]. Increasing the C/V -ratio causes, for certainvalues for C and V , an approximately linear increase in the upper bound on treewidth or the number of nodes in thelargest clique. In other words, we obtain an easy-hard-harder pattern for tree clustering algorithms including HUGIN,which contrasts with the easy-hard-easy pattern observed for SAT formulas using the Davis–Putnam algorithm [55].Experimenters may thus use the C/V -ratio directly, instead of or as a complement to maximal clique size or treewidth.O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741139In addition to the C/V -ratio, we study the regularity of a BNs underlying graph and the distributional nature ofconditional probability tables.The rest of this article is organized as follows. Section 2 introduces Bayesian network definitions and notation aswell as the MPE problem. In Section 3 we briefly describe inference and in particular tree clustering and the HUGINalgorithm as well as the concepts of maximal clique size and treewidth. Section 4 discusses the use of applicationBNs and randomly generated BNs for experimentation. In particular, Bayesian networks generated by the BPARTalgorithm as well as the MPART algorithm are presented and analyzed; there are also results on their relationship.Section 5 discusses the interaction between properties of randomly generated BNs and their hardness for tree cluster-ing algorithms and HUGIN in particular. In Section 6 we turn to the experimental part of the article, with experimentalresults for BPART and MPART networks, using the state-of-the-art inference system HUGIN to study characteristicsof the maximal clique sizes generated as well as inference times. Section 7 concludes and discusses future work.Earlier versions of this research have been reported previously [51,52]. In closely related work we have developedand investigated a stochastic local search approach to computing MPEs and compared it to tree clustering for varyingC/V -ratios [51,53].2. PreliminariesA Bayesian network (BN) represents a multi-variate probability distribution as a directed acyclic graph (DAG),where the nodes represent random variables.Definition 1 (Directed acyclic graph (DAG)). Let G = (X, E) be a directed acyclic graph (DAG) with nodes X ={X1, . . . , Xn} and edges E = {E1, . . . , Em}. An ordered tuple Ei = (Y, X), where 1 (cid:2) i (cid:2) m and X, Y ∈ X, representsa directed edge from Y to X. Here, ΠX denotes the parents of X: ΠX = {Y | (Y, X) ∈ E}. Similarly, ΨX denotes thechildren of X: ΨX = {Z | (X, Z) ∈ E}. The out-degree and in-degree of a node X is o(X) = |ΨX| and i(X) = |ΠX|respectively. The minimal non-zero out-degree of any node in G is denoted δo(G) and the minimal non-zero in-degreeof any node in G is denoted δi(G); n(G) = |X| is the number of nodes in G.The following characterization of graphs in general and BNs in particular turns out to be fruitful when analyzingthe performance of inference algorithms on BNs.Definition 2 (Root node, non-root node, leaf node). Let G be a non-empty DAG and let X be a node in G. If i(X) = 0then X is a root node. If i(X) > 0 then X is a non-root node. If i(X) > 0 and o(X) = 0 then X is a leaf node.Any non-empty DAG G has at least one root node so V (cid:3) 1 and the C/V -ratio is always well-defined for non-empty DAGs according to Definition 2. Only non-empty graphs are considered in the rest of this article. In theimportant special case of bipartite DAGs, which we formally introduce below, the C/V -ratio is the ratio of the numberof leaf nodes to the number of root nodes.Definition 3 (Bipartite DAG). Let G = (X, E) be a DAG. If X can be split into partite sets V = {X ∈ X | i(X) = 0}(the root nodes) and C = {X ∈ X | i(X) > 0} (the leaf nodes) such that any (V , C) ∈ E is such that V ∈ V and C ∈ Cthen G is a bipartite DAG; B is the set of all bipartite DAGs.In tree clustering algorithms, which we return to in Section 3, a directed graph (BN) is transformed into an undi-rected graph (a clique tree) over which inference is performed.Definition 4 (Undirected graph). Let G = (X, E) be an undirected graph with nodes X = {X1, . . . , Xn} and edgesE = {E1, . . . , Em}. An undirected edge Ei = {X, Y } is a set where 1 (cid:2) i (cid:2) m and X, Y ∈ X. The set of adjacent (orneighbor) nodes of a node X is denoted a(X) = {Y | {X, Y } ∈ E} while its degree, d(X) = |a(X)|, is the number ofneighbors X has in G. Finally, δ(G) is the minimal degree of all nodes in G and n(G) = |X| is the number of nodesin G.1140O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174In this article we will usually not distinguish, in BNs, between a graph node and the corresponding random variable.For the purpose of this article we also focus exclusively on labelled graphs—graphs with distinguishable vertices—both in the directed and the undirected cases. When parts of a graph (BN) are studied, the following notion of aninduced subgraph is useful.Definition 5 (Induced subgraph). Let X be the nodes in a directed graph. The product X × X is defined as {(Xi, Xj ) |Xi, Xj ∈ X}. Let Y be the nodes in an undirected graph. The product Y × Y is defined as {{Yi, Yj } | Yi, Yj ∈ Y }. LetG = (Z, E) be a (directed or undirected) graph. The induced subgraph G[W ] = (W , E[W ]) is a graph with nodesW ⊆ Z and edges E[W ] = (W × W ) ∩ E.We also extend the graph notation and definitions to BNs, understanding that they apply to the graph part of theformal definition of BNs, which follows. First, we have the following definition.Definition 6 (BN node). A discrete BN node X is a random variable with a discrete, finite state space ΩX ={x1, . . . , xk}.While BN nodes can also be continuous, this article is restricted to the discrete case and we will take “BN node”to mean “discrete BN node” in the following.Definition 7 (Bayesian network). A Bayesian network is a tuple β = (X, E, P ), where (X, E) is a DAG with anassociated set of conditional probability distributions P = {Pr(X1 | ΠX1), . . . , Pr(Xn | ΠXn)}. Here, Pr(Xi | ΠXi ) isthe conditional probability distribution for Xi ∈ X. Further, let πXi represent the instantiation of the parents ΠXi ofXi . The independence assumptions encoded in (X, E) imply the joint probability distributionPr(x) = Pr(x1, . . . , xn) = Pr(X1 = x1, . . . , Xn = xn) =n(cid:2)i=1Pr(xi | πXi ).(1)Bayesian networks are also known as belief networks, Bayesian belief networks, or probabilistic networks; a con-ditional probability distribution Pr(Xi | ΠXi ) is also known as a conditional probability table (CPT).Sometimes a BN is provided with observations or evidence by setting or clamping m nodes {O1, . . . , Om} toknown states o = {O1 = o1, . . . , Om = om} = {o1, . . . , om}. These nodes are called observation nodes and need to beconsidered when computing an explanation, which is defined below.Definition 8 (Explanation). Consider a BN β = (X, E, P ) with X = {X1, . . . , Xn} and observations o = {o1, . . . , om}for m (cid:2) n. An explanation x assigns states to all non-evidence nodes {Xm+1, . . . , Xn}: x = {xm+1, . . . , xn} ={Xm+1 = xm+1, . . . , Xn = xn}.When discussing an explanation x, the BN β for which x is an explanation is easily understood and therefore leftimplicit. Among explanations, the u most probable ones are of particular interest.Definition 9 (Most probable explanation (MPE)). Let x range over all explanations in a BN β. Finding a most probableexplanation (MPE) in β is the problem of computing an explanation x∗ such that Pr(x∗) (cid:3) Pr(x). The u most probableexplanations is X∗ = {x∗} where Pr(x∗) = Pr(x∗i ) (cid:3) Pr(x) for 1 (cid:2) i (cid:2) u.1) = · · · = Pr(x∗u) and Pr(x∗1, . . . , x∗uHere, u = |X∗| is simply the number of MPEs in a BN and no other explanation has higher probability thanx∗ ∈ X∗. Since there might be u > 1 MPEs with the same probability we say “an” or “one” MPE rather than “the”MPE. As is common, we compute just one MPE x∗ even when multiple MPEs exist in a BN. Following Pearl wesometimes denote computing an MPE as belief revision, while computing the marginal distribution over a BN node isalso denoted belief updating [58].It has been shown that exact MPE computation is NP-hard [66]. The problem of relative approximation of an MPEis to find an assignment with probability close to that of an MPE to within a small ratio. This problem has also beenproven to be NP-hard [1]. Belief updating is computationally hard also [14,60].O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–117411413. Inference in Bayesian networksIn addition to the arguments referring to the mapping from SAT, the claim that we have a way to generate hard andeasy BNs for the inference task needs to be supported analytically and experimentally by considering one or moreBN inference algorithms. BN inference algorithms can be classified as exact or approximate. Exact BN inferencealgorithms are the main focus in this article and include tree clustering algorithms [2,33,38,39,46,65], conditioningalgorithms [16,17,22,32,57,58,64], elimination algorithms [19,47,72], and hybrid exact methods [20].For the purpose of this article, we study in detail one of the most prominent inference approaches—the tree clus-tering approach and more specifically the HUGIN algorithm.1 Tree clustering is further discussed in Section 3.1.Section 3.2 briefly discusses other exact BN inference algorithms.3.1. Inference by tree clustering: The role of maximal clique sizeTree clustering is currently one of the major approaches to inference in multiply connected Bayesian networks [58].Like other tree clustering algorithms, the HUGIN algorithm employs two phases: a compilation (or clustering) phaseand a propagation (or run-time) phase [2,37,39,46]. During compilation, a Bayesian network is transformed intocliques organized in a clique tree. During propagation, evidence is propagated in the clique tree, leading to beliefupdating or belief revision computations as appropriate.A clique (or junction) tree is constructed from a Bayesian network in the following way by the HUGIN algorithm.First, an initial moral graph β(cid:6) is constructed by making an undirected copy of β and then augmenting it as follows. LetX systematically range over all nodes in β. For each node X, HUGIN adds to β(cid:6) an edge between each pair of nodesin ΠX if no such edge already exists in β(cid:6). Second, HUGIN triangulates the moral graph β(cid:6), creating a triangulatedgraph β(cid:6)(cid:6). Triangulation amounts to adding fill-in edges to the moral graph β(cid:6) such that no chordless cycle of lengthgreater than three exists. Third, a clique tree β(cid:6)(cid:6)(cid:6) is created from the triangulated graph β(cid:6)(cid:6).2 This clique tree—whichconsists of cliques and separators—must exhibit the property that for any two clique nodes F and H in the tree, allnodes between them contain F ∩ H . In β(cid:6)(cid:6)(cid:6), where both cliques and separators have belief tables associated with them,the joint probability Pr(X) is the product of the clique belief tables divided by separator belief tables.The following quantities are important for characterizing computation in the clique tree [46].Definition 10 (Clique tree parameters). Let Γ be the set of cliques in the clique tree β(cid:6)(cid:6)(cid:6), created from a BN β usingtree clustering. The state space size of a clique H in β(cid:6)(cid:6)(cid:6), g, is defined as(cid:2)g = |ΩH | =|ΩX|,X∈Hwhere X is a node in β. The maximal number of nodes in a clique in β(cid:6)(cid:6)(cid:6), h, is defined ash = supH ∈Γ|H |.The total clique tree size (or total state space size) k of β(cid:6)(cid:6)(cid:6) is defined as(cid:3)k =|ΩH |,H ∈Γwhile (cid:8), the maximal clique size (or maximal state space size of a clique) in β(cid:6)(cid:6)(cid:6), is(cid:8) = supH ∈Γ|ΩH |.(2)(3)(4)(5)1 For the sake of simplicity, we generally do not distinguish between (i) the HUGIN algorithm and (ii) the HUGIN system, namely a softwareimplementation of the HUGIN algorithm. In general, this article discusses the HUGIN system in the experimental parts of the article, and the HUGINalgorithm elsewhere.2 Some tree clustering algorithms, but not HUGIN, employ an intermediate step right before clique tree construction. This intermediate stepcreates a junction graph or system of cliques from the triangulated graph.1142O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174A functional notation, for example k(β(cid:6)(cid:6)(cid:6)) or (cid:8)(β(cid:6)(cid:6)(cid:6)), is sometimes used in this article in order to make β(cid:6)(cid:6)(cid:6) explicit.For the optimal (minimal) values, h∗, k∗, and (cid:8)∗ are used for h, k, and (cid:8) respectively. When the above parameters areconsidered random variables, the letters G, H, K, and L are used.Some of our investigations are restricted to nodes with two states, S = 2, and in this case the state space size g ofa clique (2) simplifies tog = |ΩH | =(cid:2)|ΩX| = S|H | = 2|H |.X∈HWhen all BN nodes have S states the maximal number h of nodes in a clique is the same as the number of nodesin a clique of maximal size (cid:8). Consequently, we do not distinguish between these two quantities in this article, eventhough in general they need to be kept distinct. It is also easy to see that(cid:8) = Sh = 2h.(6)When a BN is highly connected, as in some of the networks considered in this article, the cliques in the cliquetree become very large, thus making tree clustering inference slow. A crucial step in the process of creating aclique tree from a Bayesian network is triangulation—the construction of a triangulated moral graph β(cid:6)(cid:6). Triangu-lation determines g, h, k, and (cid:8). Optimal triangulation including the computation of h∗ is unfortunately known tobe NP-hard, but there are heuristic algorithms such as MINIMUMFILLINWEIGHT, MINIMUMFILLINSIZE, MINI-MUMCLIQUEWEIGHT, MINIMUMCLIQUESIZE that compute upper bounds on h∗, k∗, and (cid:8)∗ and in practice performtriangulation quite well [33,37,42].HUGIN was introduced as a belief updating algorithm [46], and was later extended to MPE computation (beliefrevision) [18], using essentially the same clique tree β(cid:6)(cid:6)(cid:6) in both cases. Thus, in the HUGIN approach, computingmarginal distributions and computing MPEs are closely related. There are two main algorithmic differences: First,when computing an MPE x∗ ∈ X∗, maximization is performed, while in belief updating, summation is performed.For our purposes, this step has essentially the same performance in both cases. Second, HUGIN belief revision must,in cases of multiple most probable explanations |X∗| > 1, perform propagation several times [50]. On the other hand,one propagation is sufficient in HUGIN belief updating. Of these two differences, the latter has a more significantimpact on the computational cost of propagation and is further discussed in Section 5.6.3.2. Inference, maximal clique size, and treewidthThe complexity of most exact Bayesian network inference algorithms—including tree clustering algorithms, con-ditioning algorithms, and elimination algorithms—has been found to depend on treewidth (cid:9) ∗ or on optimal maximalclique size h∗, where (cid:9) ∗ = h∗ − 1 [20,46]. Time and space complexity for tree clustering is exponential in thetreewidth of the clique tree. Conditioning algorithms [16,17,22,32,57,58,64] transform a multiply connected graphinto several singly connected graphs by introducing cycle cutsets, and perform computations over each singly con-nected graph. The time complexity of conditioning, for a minimal cycle cutset of size c, has been bounded from belowby treewidth (cid:9) ∗ in the inequality (cid:9) ∗ (cid:2) c + 1 [8]. The time complexity of elimination is closely related to that oftree clustering, and also depends on the treewidth (cid:9) ∗ [5,21]. Finally, there are hybrid algorithms—combining treeclustering, conditioning, and elimination—that trade off space- and time-complexity and again there is a dependencyon treewidth [17,20]. In one such hybrid algorithm it is possible to move from O(n) space and O(n exp((cid:9) ∗ log n))time to O(n exp((cid:9) ∗)) space and O(n exp((cid:9) ∗)) time in a gradual fashion [17].Unfortunately, computing treewidth (cid:9) ∗ for a graph is in itself computationally hard. In particular, the problem ofdetermining whether the treewidth of a given graph is bounded by an integer k has been shown to be NP-complete [6].However, it is possible to empirically establish lower bounds for treewidth as well as upper bounds, computed usingheuristics, for treewidth in polynomial time [44]. Optimal triangulation is closely related to computing treewidth, andtriangulation heuristics play a key role in tree clustering algorithms, as discussed above and further investigated in theexperiments in Section 6.4. Bayesian networks for experimentation and benchmarkingThere are several ways to experiment with inference algorithms using BNs. In this section we discuss the twomain classes of BNs used for experimentation in the literature: application BNs and randomly generated BNs. WhileO.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741143essential, we argue that application BNs also have limitations. It is non-trivial, for application BNs, to understand howdifferent BN parameters interact and contribute to inference complexity. Using randomly generated BNs, we can startaddressing those problems, but still need to make sure that the BNs generated are such that inference hardness can—atleast to some extent—be controlled and predicted.In Section 4.1 we define subsets of the set of Bayesian networks. Section 4.2 briefly discusses BNs from applica-tions. In Section 4.3 we discuss our two approaches to randomly generating Bayesian networks, the BPART algorithmand the MPART algorithm.4.1. Classes of Bayesian networksIt turns out that the regularity of a BN’s underlying graph varies between applications and also has a major impacton maximal clique size and thus on BN inference times. In order to discuss the effect of graph regularity on BNinference, we introduce the following terminology which applies to directed graphs in general.Definition 11 (Regularity). Let G = (X, E) be a directed graph. If any two nodes X, Y ∈ X with in-degreesi(X), i(Y ) > 0 have the same number of parents i(X) = i(Y ) then we say that G is parent-regular, or G ∈ UPR.If G is not parent-regular then it is parent-non-regular, or G ∈ UPN, with UPR ∩ UPN = ∅. If G is either parent-regularor parent-non-regular then it is parent-irregular, or G ∈ UPI, where UPI := UPR ∪ UPN. If any two nodes X, Y ∈ Xwith out-degrees o(X), o(Y ) > 0 have the same number of children o(X) = o(Y ) then G is child-regular, or G ∈ UCR.If G is not child-regular then it is child-non-regular, or UCN, with UCR ∩ UCN = ∅. If G is either child-regular orchild-non-regular then it is child-irregular: G ∈ UCI where UCI := UCR ∪ UCN.We note that the above definitions allow a non-trivial graph G to be both parent regular and parent-irregular orchild-regular and child-irregular: G ∈ UPI ∩ UPR or G ∈ UCI ∩ UCR, where UPI ∩ UPR as well as UCI ∩ UCR are non-empty. This turns out to simplify the construction algorithms along with their analysis as we will see in Section 4.3.4.We will also see that the probability of a BN that is an element of UCI is also an element of UCR, in other words thesize of UCI ∩ UCR, is extremely small for the constructions and parameter values considered there.Based on the regularity concepts introduced in Definition 11, the following four classes of directed graphs (andBNs) are identified.Definition 12 (Class A, Class B, Class C, Class D directed graphs). A directed graph G is a Class A (or “regular”)graph if it is child-regular and parent-regular: G ∈ UA where UA := UCR∩ UPR. If G is child-irregular and parent-regular then it is a Class B (or “irregular”) graph: G ∈ UB where UB := UCI ∩ UPR. If G is child-regular and parent-irregular then it is a Class C graph: G ∈ UC where UC := UCR ∩ UPI. If G is child-irregular and parent-irregular then itis a Class D (or unconstrained) graph: G ∈ UD where UD := UCI ∩ UPI. The sets of all Class A, Class B, Class C andTable 1An informal presentation of directed graphs, including Bayesian networks, along the two orthogonal dimensions of child-regularity and parent-regularity, leading to the following four classes. Class A: Parent-regular and child-regular; Class B: Parent-regular and child-irregular; Class C:Parent-irregular and child-regular; and Class D: Parent-irregular and child-irregular. Class D is unconstrainedParent-regular set UPR: Non-root nodeshave the same number of parentsParent-irregular set UPI: Non-root nodestypically have different number of parentsChild-regular set UCR: Non-leaf nodes havethe same number of childrenChild-irregular set UCI: Non-leaf nodes typi-cally have different number of childrenClass A: Parent-regular and Child-regularClass B: Parent-regular and Child-irregularUA := UPR ∩ UCRClassical Gallager codes [28,49]Regular kCNFRead-(cid:8) formulasRegular multipartite graphsUB := UPR ∩ UCIModern Gallager codes [48,49]Irregular kCNF formulas [55]Irregular multipartite graphsBiological pedigrees [68]Class C: Parent-irregular and Child-regularClass D: Parent-irregular and Child-irregularUC := UPI ∩ UCRRead-(cid:8) formulasUD := UPI ∩ UCIMany application BNsMixed CNF formulas1144O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174Fig. 1. Examples of Class A, Class B, Class C, and Class D bipartite Bayesian networks (BNs). The Class A BN is parent-regular andchild-regular (a “regular” BN); the Class B BN is parent-regular and child-non-regular and thus child-irregular (an “irregular” BN); the ClassC BN is parent-non-regular (thus parent-irregular) and child-regular; and the Class D BN is parent-non-regular (thus parent-irregular) as well aschild-non-regular (thus child-irregular) and therefore unconstrained.Class D bipartite graphs (BNs) are respectively introduced as follows: BA := UA ∩ B, BB := UB ∩ B, BC := UC ∩ B,and BD := UD ∩ B.Table 1 summarizes some families of BNs, including BNs for error correction coding [27,28,48,49], and how theycan be classified into our four classes UA, UB , UC , and UD. The following example and Fig. 1 illustrate the classes ofnetworks presented in Definition 12 and Table 1.Example 13. Fig. 1 contains examples of Class A, Class B, Class C, and Class D BNs.In propositional logic, the notion of read-(cid:8) means that a variable is used or “read” (cid:8) times in the clauses of a formula.This concept is used in Table 1. Also, the development in information theory from classical Gallager codes to modernGallager codes fits into our framework as presented in Table 1. Gallager’s original codes [28] can be encoded as ClassA BNs according to our terminology. Modern Gallager codes [48], on the other hand, correspond to Class B BNs.Table 1 also includes biological pedigree BNs [68], which are typically also Class B BNs. In a BN representing apedigree, non-root nodes typically have two parents, but the number of children per non-leaf node can vary.Note that regularity can easily be made more gradual than in the framework presented in Table 1. For example, onecould use the variance in in-degree and out-degree as a measure of regularity. With this more general measure, highvariance means irregular, low variance means regular. In this article, however, we are concerned with two extremecases and leave other variations for future work. We investigate, under a minor relaxation introduced in Definition 16,the effect of regularity by considering Class A BNs as well as Class B BNs. We denote the former regular and thelatter irregular BNs when there is no chance of confusion.4.2. Bayesian networks from applicationsBN inference algorithms may be studied empirically by evaluating their performance on one or several BNsfrom applications [41,51,56]. For example, BNs may be taken from Friedman’s Bayesian Network Repository athttp://www.cs.huji.ac.il/labs/compbio/Repository/. Application BNs are obviously very important when performingexperimental studies. However, we believe that it is difficult to understand the performance of a BN inference al-gorithm by studying only application BNs. First, there is a problem of dimensionality in that application BNs varyconsiderably in their many topological and distributional parameters. It is therefore unclear how much one can learnfrom pooling BNs from different applications. Second, the inference times vary significantly between application BNs,and there is in general no clear correlation between any of the BN parameters and the inference times [51]. Third, theO.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741145number of BNs per application is most often very small—typically there is one BN per application. Restricting oneselfto BNs from one application is thus not desirable: It is very difficult to obtain good statistics using small samples.There is also a more fundamental limitation associated with the use of application BNs as the “gold standard” forperformance. Some application BNs might have been fine-tuned to give adequate performance using existing inferencealgorithms. It seems very valuable to construct BNs that are not biased in this way, in order to thoroughly characterizeexisting algorithms as well as lay the groundwork for novel algorithms and more challenging applications.4.3. Bayesian network generation algorithmsA potential solution to some of the limitations associated with using application BNs for empirical research is torandomly generate BN instances [7,17,34,41,56,69,70]. Using randomly generated BNs, one can create as many BNsas needed to provide a significant evaluation. This approach also reduces the problem of dimensionality, since onemay vary the BNs generated along just one or a few dimensions at a time.Issues related to randomly generating BNs are addressed in the remainder of this section. In Section 4.3.1 wepresent the parameters we have used to (partly) control the process of randomly generating Bayesian networks. InSections 4.3.2, 4.3.3, and 4.3.4 we present the BPART algorithm used to generate a certain class of bipartite networks,for which the mapping from the satisfiability problem (SAT) is fairly direct. In Section 4.3.5 we discuss the MPARTconstruction, which we show to be related to the BPART construction and, as a result, can be studied from a similarpoint of view. In Section 4.3.6 we discuss the MPART and BPART constructions as well as related work.4.3.1. Input parameters for Bayesian network generation algorithmsMany parameters might be varied when randomly generating Bayesian networks. The following parameters, whichcover both topological and distributional issues and whose impact on inferential hardness for tree clustering is furtherdiscussed in Section 5, correspond to the input parameters of the BPART and MPART algorithms presented later inthis section:• Number of root nodes V in BN: The range of this integer is 1 to ∞; the default value is 30. For the special case ofSAT-like BNs (formally defined in Section 4.3.2), root nodes correspond to variables in conjunctive normal form(CNF) formulas.• Number of non-root nodes C in BN: The range of this integer is 0 to ∞; the default value is 90. For SAT-like BNs(Section 4.3.2), non-root nodes are leaf nodes and correspond to clauses in CNF formulas.• Conditional probability table (CPT) type Q and F for BN root and non-root nodes respectively: The choices aredeterministic (or, and, and xor), uniform, and random; the default value for root nodes is uniform whileit is or for non-root nodes. Section 5.6 contains further discussion of CPTs. For experimentation in this article,or non-root CPTs and uniform root CPTs are employed except in Section 6.5 where all CPTs are random.• Child-regularity R of BN: The choices are Class B child-irregular (R = false) or Class A child-regular (R = true).The default value is child-irregular, or R = false. Four classes Class A, Class B, Class C, and Class D of BNs ofvarying regularity have been identified; in this article we consider parent-regular BNs in detail. Section 5.4 furtheranalyzes the child-regular case R = true; Section 6.4 presents experimental results for R = true, the remainingexperiments in Section 6 focus on the child-irregular case and use R = false.• The number of states for a node X in the BN, S = |ΩX|. The range of this integer is 1 to ∞; the default value isS = 2 (boolean nodes). For boolean nodes, the states may without loss of generality be called 0 and 1 or false andtrue respectively. Experiments in Section 6 are restricted to S = 2.• The in-degree or number of parents P for each non-root node X in the BN: Given our focus on parent-regularBNs, the number of parents P for each non-root node X is a parameter: P = i(X) = |ΠX|. The range of thisinteger is 1 (cid:2) P (cid:2) V ; the default value is P = 3. Experiments in Section 6 use P = 3 except in Section 6.5 whereP = 2 is used.In the following we will always assume that the constraints for V , C, Q, F , R, S, and P as presented above are allsatisfied. As an example, the default values V = 30, C = 90, Q = uniform, F = or, R = false, S = 2, and P = 3make up a valid set of input parameters. These default values give a specific signature for, say, the BPART algorithm,namely BPART(uniform, or, 30, 90, 2, false, 3).1146O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174The following quantities can easily be derived from the input parameters presented above:• The total number of BN nodes is N = C + V .• From V and C the C/V -ratio can be obtained: The range is 0 to ∞; the default value is C/V = 3. Since V (cid:3) 1,the C/V -ratio is always well-defined. See Section 5.2 for further discussions of the C/V -ratio; experiments inSection 6 use C/V -ratios varying from C/V = 0.75 to C/V = 3.4.• Given C and P , the total number of BN edges is: E = C × P , giving E/V = C × P /V . Since V (cid:3) 1, the E/V -ratio is always well-defined. The E/V -ratio generalizes the C/V -ratio; we generally fix P and use C/V -ratio inthis article but E/V shows up in analytical results in Section 5.4.2.To be consistent with the existing research literature on randomly generating problem instances in the areas ofsatisfiability and constraint satisfaction, we use upper-case V to denote the number of root nodes and upper-case C todenote the number of non-root nodes in a BN. Neither V nor C are nodes or random variables in a Bayesian network,even though upper-case letters are also used to represent these concepts.4.3.2. The BPART network generation algorithm: A synthetic bipartite constructionFor many NP-hard problems, simply generating random instances in an undiscriminating fashion has sometimesresulted in fairly easy problem instances [4,11,25]. This problem has been addressed in the context of satisfiability,in the seminal work of Mitchell, Selman, and Levesque [55], where it was shown how to generate hard instancesfor 3SAT. Here we show how these ideas can be generalized and used to generate, as it turns out, hard instances forbelief revision and belief updating when using tree clustering. In this section we present our approach to randomlygenerating bipartite BNs of varying hardness.Fig. 2 presents our BPART construction algorithm which generates SAT-like Bayesian networks as a special case.When BPART is invoked using the following signature, it creates SAT-like BNs.Definition 14 (SAT-like BN). Let β ← BPART(uniform, or, V , C, 2, false, P ). The BN β is SAT-like.The BPART algorithm can also construct more general BNs as reflected in the following definition.BPART(Q, F, V , C, S, R, P )Input: Q conditional probability table (CPT) type, root nodesFVCSRPCPT type, non-root nodesnumber of “variables” (root nodes)number of “clauses” (leaf or non-root nodes)number of states per nodecreate regular BN—true or falsenumber of parents of clauses (non-root nodes)Output:βBayesian networkbeginβ ← CREATEBN()ADDLAYER(β, V , 0, S, false){First, add layer of root nodes—V variables}ADDLAYER(β, C, P , S, R){Second, add layer of child nodes—C clauses}for i ← 1 to V + Cnode ← GETNODE(β, i)if ROOTNODE(node) thenSETDISTRIBUTION(node, S, Q) {Set CPT of root node}thenSETDISTRIBUTION(node, S, F ) {Set CPT of non-root node}endendreturn βendFig. 2. The BPART algorithm for constructing synthetic, bipartite BNs. The input parameters Q, F , V , C, S, R and P are used to create dif-ferent variants of BPART networks. The BPART algorithm creates classical, irregular SAT-like BNs when it is invoked with the parametersQ = uniform, F = or, S = 2, R = false, and P = 2; in other words as follows: BPART(uniform, or, V, C, 2, false, 2).O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741147Table 2Algorithms used by the Bayesian network construction algorithms. Possible values of the parameter F in SETDISTRIBUTION and details ofCHOOSEFEWESTCHILDREN are discussed in the textFunction nameADD(X, X)ADDPARENT(X, Y )CHOOSEFEWESTCHILDREN(Y )CREATEBN()CREATENODE(β)GETLEAFNODES(β)GETNODE(β, i)GETNUMBEROFCHILDREN(X)GETNUMBEROFNODES(X)RANDOMINT(L, H )ROOTNODE(X)SETDISTRIBUTION(X, S, F )SETNUMBEROFSTATES(X, S)DescriptionAdds node X to the set of nodes X; sets X ← {X} ∪ X.If possible, adds Y to ΠX and returns true, else returns false.Randomly chooses and returns appropriate node among nodes Y .Returns a new, empty BN.Returns a newly created node in BN β.Returns nodes without any children in BN β.Returns the ith node in the BN β, assuming some node ordering.Returns the number of children for BN node X.Returns the number of BN nodes in the set of nodes X.Returns, uniformly at random, a natural number in the interval [L, H ].Returns true if X is a root node, else returns false.Sets distribution of BN node X, with S states, to CPT of type F.Creates S states in the input BN node X.Definition 15 (BPART BNs). The set of all BNs generated by BPART is defined as UBPART = {β | β ←BPART(Q, F, V , C, S, R, P )}. The set of regular and irregular BPART BNs are respectively denoted U r= {β |β ← BPART(Q, F, V , C, S, true, P )} and U i= {β | β ← BPART(Q, F, V , C, S, false, P )}.BPARTBPART(cid:4)mi=1 Ci with clauses Ci = Xi1Building on an existing construction [14,60], the basic idea is to generalize from a conjunctive normal form (CNF)formula and generate a Bayesian network for which an MPE corresponds to a satisfying assignment of the formula.For a SAT-like BN, given a 3CNF formula f =∨ Xi3 , one can construct abipartite Bayesian network, in which one layer of nodes (the root nodes X) corresponds to the variables and a secondlayer (the leaf nodes C) corresponds to the clauses. A variable Xj ∈ X has an edge directed toward Ci ∈ C iff Xjoccurs in the clause Ci . Clause nodes are clamped during inference. The conditional probability tables are set so thatPr(f = 1) > 0 iff the assignment to the Xj ’s satisfies the 3CNF formula. It is easy to see that this happens if for alli, the conditional probability table associated with the node Ci simulates an or gate of three inputs. To generate aBN that corresponds to a non-monotone CNF, the CPTs need to be generalized accordingly in a straight forward way.It is easy to verify that an MPE x∗—an assignment of values to the Xj ’s—has a positive probability iff it satisfiesthe corresponding 3CNF formula [14,60]. There may be many satisfying assignments, all with the same probability,making them all MPEs.∨ Xi2There are several ways to generate random BNs corresponding to CNF formulas. Our BPART approach, presentedin Fig. 2, is based on the following policy: work with V variables and C clauses; generate the clauses by selectingvariables uniformly into clauses and negate each variable with probability p = 0.5 [55]. Subroutines used by BPARTinclude ADDLAYER (see Fig. 3), CHOOSEFEWESTCHILDREN, and SETDISTRIBUTION (see Table 2). Turning toBPART in Fig. 2, CREATEBN first creates a new, empty BN β. ADDLAYER(β, V , 0, S, false) then adds to β a layerof V root nodes, where each root node has S states. Next, ADDLAYER(β, C, P , S, R) adds to β the C non-root (orleaf) nodes. Each leaf node has P parents, chosen among the V root nodes. The nature of a leaf node’s parent selectionprocess, which takes place in ADDLAYER, is controlled by the regularity parameter R ∈ {true, false}. Fig. 3 presentshow a layer in the BPART construction is added by the ADDLAYER procedure. ADDLAYER, which returns the BNβ with a new layer of M nodes added to it, works differently for the regular (R = true) and the irregular (R = false)cases as discussed further in Section 4.3.3 and Section 4.3.4 respectively.The CPTs of all nodes are constructed in the top level of BPART (Fig. 2). Non-root node CPTs are determined bythe input parameter F , with F ∈ {or, and, xor, uniform, random}. Similarly, root node CPTs are determined byQ ∈ {or, and, xor, uniform, random}. In a deterministic CPT (where CPTs are or, and, or xor), all entriesare either 0 or 1. In a uniform CPT of a node X with |ΩX| = S states, the probability mass for a node state for agiven parent instantiation is 1/S. In a random CPT, the probabilities are first picked from a uniform random U (0, 1)distribution, and then normalized to make sure that the conditional probabilities for a given parent instantiation sumto 1. These types cover the CPTs that are required to provide an exact mapping from SAT problems to corresponding1148O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174Input:BN to which new layer is addedADDLAYER(β, M, P , S, R)βM number of new nodes to create in the new layerPnumber of parent nodes to assign to a new nodeSnumber of states per new nodetrue if regular BN layer is to be created, else falseRBayesian network with layer of nodes addedOutput:βbeginparents ← GETLEAFNODES(β)for i ← 1 to MXi ← CREATENODE(β)SETNUMBEROFSTATES(Xi , S)c ← 0while c < P {The new node Xi is given parents}if not R then {The irregular case R = false}j ← RANDOMINT(1, GETNUMBEROFNODES(parents))parent ← parents[j ]else {The regular case R = true}parent ← CHOOSEFEWESTCHILDREN(parents) {Pick parent with fewest children}endsuccess ← ADDPARENT(Xi , parent) {false if parent among Xi ’s parents already}if success then c ← c + 1 endendendreturn βendFig. 3. The function ADDLAYER adds a layer consisting of M nodes to the Bayesian network β. ADDLAYER is invoked by BPART.BNs as well as idealizations of CPTs that might occur in some applications. In Section 5.6, we discuss the relationshipbetween these CPTs and hardness for inference.There are two related but slightly different perspectives on BPART’s signature. The first perspective is to regardit as a parametrized probability distribution B over BNs—as in B ∼ BPART(uniform, or, 30, 90, 2, false, 3).The second perspective is to regard the signature as an assignment that generates one sample β from the probabilitydistribution B: The assignment β ← BPART(uniform, or, 30, 90, 2, false, 3) will, under reasonable assumptionsregarding the periodicity of the pseudo-random number generator, create a different BN β each time BPART isinvoked. In general, the former perspective is taken in this section as well as in Section 5, while the latter perspectiveis taken in the experimental part of the article, Section 6, where we sample from B.The BPART and MPART construction algorithms presented in this section can construct irregular or regular BNs.The setting R = true gives a regular BN, while R = false gives an irregular BN. We now discuss these two casesseparately.4.3.3. The BPART regular case: R = trueIn the ADDLAYER procedure, if the new layer of nodes is to be regular (so R = true), the CHOOSEFEWESTCHIL-DREN procedure is invoked as shown in Fig. 3. CHOOSEFEWESTCHILDREN selects a node X among its input nodesY such that no other input node has fewer children. By doing this, the algorithm ensures that the parent layer is child-regular or “close to” child-regular. More formally, it is ensured that the BN has an underlying relaxed Class A graph,as defined below.We now introduce concepts closely related to Definitions 11 and 12, in order to make the notion of regularity morewidely applicable without losing the essence of regularity.Definition 16 (Relaxed Class A directed graph). Consider a directed graph G with V non-leaf nodes and E edges. Ifany non-leaf node Xi where i ∈ {1, . . . , V } has o(Xi) = (cid:13) EV isVused and we say that G is relaxed child-regular. If a directed graph G is relaxed child-regular and parent-regular, thenG is a relaxed Class A directed graph. The set of relaxed Class A graphs is UA∗ ; the set of bipartite relaxed Class Agraphs is BA∗ := UA∗ ∩ B.(cid:16) children, then the notation o(Xi) ≈ E(cid:14) or o(Xi) = (cid:15) EVO.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741149The parameter value R = true controls the number of children (the out-degree) of BPART root nodes as presentedin the following theorem and corollary.Theorem 17. Suppose the BPART algorithm is called with R = true. If F edges have been distributed during the BNconstruction process of BPART, then o(X) ≈ FV for any root node X.Proof. Suppose it were not the case. This means that there exists at least one root node X where |ΨX| > (cid:13) F(cid:14) orV|ΨX| < (cid:15) F(cid:16) (or fewer)Vchildren. Assume, for the purpose of contradiction, that all root nodes excluding X have (cid:13) F(cid:14)V(cid:3) F(cid:14). This means that there exists at least one root node Y with (cid:15) FV(cid:14) children. Since (cid:13) FV(cid:16). Consider the case |ΨX| > (cid:13) FVV , the number of children (edges) for these (V − 1) root nodes is(cid:5)(cid:6)(V − 1)FV(cid:3) (V − 1)FV= F − FV.(cid:14) (cid:3) FSince the number of children for X is |ΨX|, the total number of edges in the BN is F − F+ |ΨX|, which is notVpossible since |ΨX| > (cid:13) FV and there is a contradiction. Consequently, there exists at least one root node Y withV(cid:15) F(cid:16) edges. However, this fact contradicts how CHOOSEFEWESTCHILDREN operates. CHOOSEFEWESTCHILDRENValways picks, for a leaf node, a parent with fewest children. However, the fact that X now has |ΨX| > (cid:13) F(cid:14) implies thatVat some earlier stage X was picked by CHOOSEFEWESTCHILDREN while having (cid:13) F(cid:14) children. At the same time,Vthere must have been a node Y with |ΨY | (cid:2) (cid:15) F(cid:16), and so CHOOSEFEWESTCHILDREN should have chosen Y . This isVa contradiction. The proof for the case |ΨX| < (cid:13) FV(cid:14) is similar. (cid:2)Using Theorem 17, we formally characterize the output of the BPART algorithm in Fig. 2.Corollary 18. With input parameter R = true, the BPART algorithm creates bipartite relaxed Class A BNs:U r⊂ BA∗ .BPARTProof. Use F = CP in Theorem 17 and apply Definition 16. (cid:2)Unless otherwise noted, we do not distinguish between relaxed Class A BNs and Class A BNs in the following,and denote both as Class A BNs. In particular, we shall say that BPART, given input parameter R = true, createsClass A BNs even though this is true, strictly speaking according to Definition 11, only in the special case where(cid:13) CP(cid:16) (see Corollary 18). The notion of regularity and its impact on inference hardness is further discussedVin Section 5.4, and we turn now to the irregular case.(cid:14) = (cid:15) CPV4.3.4. The BPART irregular case: R = falseIf R = false, parent nodes are chosen uniformly at random without replacement in the ADDLAYER procedure (seeFig. 3). It is easy to show that bipartite Class B BNs are generated.Theorem 19. If R = false, the BPART algorithm creates bipartite Class B BNs: U iBPART⊂ BB .In any BN, the parents of a non-root node must be distinct. Consequently, for a given BPART leaf node, ADD-LAYER selects among the |V | = V root nodes without replacement and we obtain the following.Theorem 20 (Exact child distribution). Consider a BN with root nodes V and leaf nodes C created using BPARTwith input parameter R = false. For any X ∈ V , let the number of children be N = |ΨX|, and let b be the binomialdistribution. It is the case that N ∼ b(C, P /V ).Proof. Using the V root nodes, formnodes {Vi,1, . . . , Vi,P }, where Vi,j ∈ V for 1 (cid:2) i (cid:2)among thesedistinct super-nodes, where each super-node Wi contains P different rootand 1 (cid:2) j (cid:2) P . Clearly, BPART performs C Bernoulli trialssuper-nodes. One Bernoulli trial is, for an arbitrary node X ∈ V , considered a success if X is(cid:7)VP(cid:8)(cid:8)(cid:7)VP(cid:8)(cid:7)VP1150O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174an element of the chosen super-node. There areprobability of Bernoulli trial success for X is(cid:7)(cid:8)V −1P −1super-nodes in which X is an element. Consequently, thep =(cid:8)(cid:7)V −1(cid:8) = PP −1(cid:7)VVP,and since there are C trials we obtain a binomial distribution b(C, P /V ). (cid:2)In the following theorem we introduce a simplifying assumption, further justified below, that parent selection inADDLAYER is made with replacement even though we assumed differently in Theorem 20.Theorem 21 (Approximate child distribution). Consider a BN with root nodes V and leaf nodes C created usingBPART with R = false. As an approximation, suppose, for any Y ∈ C, that each of Y ’s P parents is picked indepen-dently and uniformly at random among V . For any X ∈ V , let the number of children be N = |ΨX|, and let b be thebinomial distribution. It is the case that N ∼ b(CP , 1/V ).Proof. Consider a BN β with root nodes V created using the BPART algorithm with input R = false. In ADDLAYER,which is invoked by the BPART algorithm, the ith leaf node Yi ∈ C selects P parent nodes. As an approxima-tion, assume that each parent is picked independently. Now consider one particular root node Xk ∈ V . Call it asuccess if, for the selection of the j th parent by the ith leaf node Yi , Xk gets picked as a parent node, else call ita failure. Let Ii,j be an indicator random variable for this trial. By assumption, each selection of a root node is anindependent Bernoulli trial with probability of success p = 1/V , and we obtain a sequence of Bernoulli random vari-ables I1,1, . . . , I1,P , . . . , Ii,1, . . . , Ii,P , . . . , IC,1, . . . , IC,P . The number of times that Xk is picked is a random variable(cid:9)CPN =Ii,j . Clearly, N has a binomial distribution b(n, p) with n = CP trials and probability of successi=1j =1p = 1/V . (cid:2)(cid:9)The approximating assumption of independence in Theorem 21 is justified as follows. As V → ∞, the randomparent selection process in ADDLAYER approaches drawing independently with replacement, since the probability ofpicking the same root node twice or more tends to zero.Having introduced and analyzed the BPART construction, we return to a discussion of Definition 11 and the classesUCI, UCR, and UCN. As an example, suppose for β ∈ U iBPART that each child node has three parents. Clearly, β ∈ UCIand typically β ∈ UCN also. We say “typically” because it could happen that parents are randomly picked such thatthe graph ends up being child-regular “by chance”, or β ∈ UCR. As an example, each root node in β could end upwith, say, exactly six children, assuming there are twice as many leaf nodes as root nodes. However, as we will seein Theorem 22 and Example 23, the probability of β ∈ UCR is extremely small for β ∈ U iBPART when β is generatedusing parameter values for P , V , and C of the order of magnitude used in the experimental part of this article.Theorem 22. Consider a BN β ∈ UCI with root nodes V and leaf nodes C created using BPART with input parameterR = false. Suppose for any Y ∈ C that each of Y ’s P parents is picked independently and uniformly at random fromV . Also assume that k := CPV is an integer. It is the case thatPr(β ∈ UCR) = CP !( CP!)VV(cid:11)CP.(cid:10)1V(7)Proof. The desired joint distribution over root nodes V is clearly multinomial, givingPr(β ∈ UCR) = Pr(N1 = k, . . . , NV = k) = n!(k!)V× pn.Using Theorem 21 we have p = 1/V and n = CP ; therefore (7) follows. (cid:2)O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741151Unfortunately, it is not easy to see what happens when CP → ∞ in (7). However, by putting n := CP and usingStirling’s formula to approximate the factorial function in (7), we getPr(β ∈ UCR) ≈√(√2πnnne−n2πn/V )V (n/V )ne−n(cid:12)(cid:11)n=(cid:10)1VV V(2πn)V −1,(8)from which it is easy to see that limn→∞ Pr(β ∈ UCR) = 0. In words, when a child-irregular BPART BN β ∈ UCIis generated, β is also child-non-regular with “high” probability Pr(β ∈ UCN) = 1 − ε, where ε := Pr(β ∈ UCR) ischaracterized by (7) or (8). Here is an example.Example 23. Consider a BPART BN β ∈ UCI constructed using parameters R = false, P = 3, V = 30, and C = 90.Using Theorem 22 we obtain Pr(β ∈ UCR) ≈ 1.61 × 10−25, a very small probability indeed. Using (8) gives theapproximation Pr(β ∈ UCR) ≈ 2.130 × 10−25.We now turn to a related but different question. How can the minimum out-degrees of root nodes in an irregularBPART BN β be characterized? This question is answered in Theorem 25 below, using the fact that parents of a leafnode are picked at random in the BPART algorithm. Before stating the result, we formally define minimal root nodeout-degree, which is a random variable M.Definition 24 (Minimum out-degree). Let X = {X1, . . . , Xn} be a set of n BN nodes with randomly distributed chil-dren, and let Ni ∼ o(Xi). The minimum out-degree random variable M is defined as M = min(N1, . . . , Nn) andabbreviated as M = min(X).In the following, Mi is used for the irregular case (R = false), Mr for the regular case (R = true).Theorem 25 (Expected minimum out-degree in BPART). Consider a BN β with root nodes V and leaf nodes Ccreated using BPART with R = false. Suppose, for any Cj ∈ C, that Cj ’s P parents are picked independently anduniformly at random among V . Let Mi = min(V ). The expectation E(Mi) isE(Mi) =(cid:15) CP(cid:16)(cid:3)Vj =1(cid:8)(cid:7)V ,1 − B(j − 1; CP , 1/V )(9)where B(k; n, p) is the cumulative binomial distribution function Pr(X (cid:2) k), where X ∼ b(n, p).Proof. The probability that the value of some Ni is k or greater isPr(Ni (cid:3) k) = 1 − Pr(Ni < k) = 1 − B(k − 1; n, p),(10)where B is the cumulative binomial distribution. (Recall that Theorem 21 uses the binomial distribution when R =false.) Considering all V root nodes in β, for the minimum to be k, all V nodes need to be k or greater: Pr(Mi (cid:3) k) =Pr(N1 (cid:3) k, N2 (cid:3) k, . . . , NV (cid:3) k). Since, as an approximation, root nodes are assumed to be picked independently anduniformly at random, there is independence between N1, . . . , NV . By introducing N = N1 = · · · = NV as well as (10)we obtainPr(Mi (cid:3) k) =V(cid:2)i=1Pr(Ni (cid:3) k) =(cid:8)(cid:7)V =1 − Pr(N (cid:2) k − 1)(cid:8)(cid:7)V .1 − B(k − 1; n, p)Given a random variable Mi with possible values {0, 1, . . . , m}, the tail sum formula for expectation isE(Mi) =m(cid:3)j =1Pr(Mi (cid:3) j )which by combining (11) and (12) yieldsm(cid:3)(cid:8)(cid:7)V .1 − B(j − 1; n, p)E(Mi) =j =1(11)(12)1152O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174By substituting in m = (cid:15) CPV(cid:16), n = CP and p = 1V we obtain (9) as desired. (cid:2)The following example illustrates Theorem 25 using parameter values from experiments in this article.Example 26 (Expected minimum out-degree in BPART). Consider a BPART network constructed using input pa-rameters C = 60, V = 30, P = 3, and R = false. Using Theorem 25, we obtain E(Mi) ≈ 1.72. If instead we useC = 102, while the other parameters stay the same, we obtain E(Mi) ≈ 4.39.While Eq. (9) can be used to compute values for E(Mi) as shown in Example 26, it is unfortunately not obvious howE(Mi) changes when the C/V -ratio changes. However, based on Theorem 21 we may use the binomial distributionb(CP , 1/V ) with ˆE(Mi, a) = μ − aσ as an approximation to E(Mi), where a is the number of standard deviations.A more explicit form is thus obtained:ˆE(Mi, a) = μ − aσ = CPV− aCP (V − 1)V 2= CP − aCP (V − 1)V(cid:13)√.(13)The impact of an increasing C/V -ratio can now be observed from Eq. (13): For a fixed number of standard deviationsa, when the C/V -ratio increases due to an increase in C, it is clear that ˆE(Mi, a) in Eq. (13) will increase as well.Table 3 provides, by means of example, some insight into the difference between the irregular and regular cases. Thedifference in the expectations of the minimal degrees, E(Mr ) versus E(Mi), is quite dramatic. Table 3 also comparesEq. (13) with Eq. (9) for V = 30, P = 3, C varying from 60 to 102, and a = 1 or a = 2 standard deviations. Here,E(Mi) is bounded as follows: ˆE(Mi, 2) < E(Mi) < ˆE(Mi, 1); ˆE(Mi, 1) provides a conservative upper bound.4.3.5. The MPART network generation algorithm: A synthetic multipartite constructionAre the patterns of hard and easy restricted to the BPART construction? If not, then what is construction-specific,and what is general across constructions? In order to explore these questions, we investigated a different but re-lated algorithm for generating random Bayesian networks, the MPART construction. MPART is closely related toTable 3These results are relevant for BPART BNs with V = 30, P = 3, and C varying from 60 to 102. The table shows the expectation of minimumout-degrees E(Mr ) in Class A (regular) BNs and E(Mi ) in Class B (irregular) BNs. For the irregular case, the approximation ˆE(Mi , b) = μ − aσfor a = 1 and a = 2 standard deviations is also displayedC/VClass A: E(Mr )Class B: ˆE(Mi , 1)Class B: E(Mi )Class B: ˆE(Mi , 2)2.063.591.721.182.264.072.071.552.474.562.441.922.675.052.812.312.885.553.202.703.096.053.593.103.296.553.993.513.4107.064.393.92Fig. 4. Example Bayesian network generated by the MPART algorithm. This BN has four root nodes {X1, . . . , X4} and seven non-root nodes{C1, . . . , C7}. Among the non-root nodes, only C3 could have appeared in a BPART network; the remaining non-root nodes are MPART-specificsince they either have at least one non-root child or parent. For instance, C1 has non-root C4 as parent and C5 as child.O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741153Fig. 5. Generation of Bayesian networks using the BPART and MPART constructions. Edges between nodes are, for simplicity, omitted. In bothconstructions, nodes are treated sequentially. Let Xi be the BN node currently being processed. For Xi , a fixed number of P parent nodes israndomly selected. For BPART, parent nodes for Xi are chosen among the root nodes {XC+1, . . . , XN }, while they are chosen among the nodes{Xi+1, . . . , XN } for MPART. The MPART construction is a generalization of the BPART construction: When it so happens that all parent nodesfor an MPART BN are picked from the root nodes, the BN could also have been generated by BPART.an approach of Kask and Dechter [41]. The procedure they describe can be viewed as follows. Choose these threeparameters: N —the number of nodes in the network; V —the number of root nodes; and P —the number of parentsof a non-root node. Construct a network as follows: Index the nodes from 1 to N , and iterate from the Cth node XC .At the ith step, process the ith node Xi , with 1 (cid:2) i (cid:2) C, and pick, uniformly at random, P parents among the nodesindexed from i + 1 to N . Repeat until all C non-root nodes {X1, . . . , XC} have been assigned parents.Example 27. An example MPART BN is shown in Fig. 4.The MPART and BPART constructions are compared in Fig. 5. The essential difference is that MPART BNs allownon-root nodes to have other non-root nodes as parents, while this is not allowed in BPART networks. Essentially, theMPART algorithm is similar to the BPART algorithm except a slight variation on BPART’s ADDLAYER algorithm(ADDLAYER is presented in Fig. 3). Instead of BPART’s ADDLAYER statement “if success then c ← c +1”, MPARTsequentially uses the statements “if success then c ← c + 1” and “ADD(node, parents)”. The signature of MPARTmirrors BPART’s signature as presented in Fig. 2 and is MPART(Q, F , V , C, S, R, P ).Definition 28 (MPART BNs). The set of all BNs generated by MPART is UMPART = {β | β ← MPART(Q, F, V ,C, S, R, P )}. The sets of regular and irregular MPART BNs are respectively defined as U r= {β | β ←MPART(Q, F, V , C, S, true, P )} and U i= {β | β ← MPART(Q, F, V , C, S, false, P )}.MPARTMPARTThis multipartite construction algorithm typically creates BNs with a tree-like topology. However, if the numberof non-root nodes C = N − V is much smaller than V , the graph may be bipartite or “close to” bipartite. To reflectdifferent types of MPART BNs, we introduce the following terminology.Definition 29. Let an MPART BN β have root nodes V and non-root nodes X = {X1, . . . , XC} with C = |C|. Wedefine the bipartite subset of U i⊆ V } ∩ U i= B ∩ U i⊆ V , . . . , ΠXC= {β | ΠX1MPART as BiMPART.MPARTMPARTGiven this terminology, we show in the following theorems that the MPART construction is a generalization of theBPART construction in the case of irregular MPART BNs.1154O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174Theorem 30. Irregular BPART BNs are a subset of irregular MPART BNs: U iBPART⊂ U iMPART.BPART and β /∈ U iBPART then β ∈ U iMPART, and thus that U iProof. First, we show that if β ∈ U iMPART. Assume, for purposesBPARTof contradiction, that β ∈ U iMPART. For this to happen, there must exist a node X in β with ΠX con-structed by BPART such that ΠX cannot be constructed by MPART. However, this is not possible, since the candidateset for ΠX in BPART is a subset of that in MPART. This follows from the structure of the MPART constructionalgorithm, and in particular the statement ADD(node, parents) which adds “node” to the candidate set “parents”. Thisstatement in the MPART algorithm is lacking in the BPART algorithm. Second, we exhibit a β ∈ U iMPART such thatβ /∈ U i(cid:2) U iMPART. Consider the three-node chain β with nodes {X1, X2, X3}, and edgesBPART, and therefore U i{(X1, X2), (X2, X3)}. Clearly, β /∈ U iBPART since β is not bipartite. However, β ∈ U iMPART since β may be constructedby the MPART construction algorithm using the signature MPART(Q, F, V , C, S, R, P ) with V = 1 and C = 2. (cid:2)⊆ U iBPARTTheorem 31. Irregular BPART BNs are the same as irregular bipartite MPART BNs: U iBPART= BiMPART.Proof. We need to show that (i) U ihave U i⊂ U iBPARTMPART, which gives (i) as follows:BPART⊆ BiMPART and (ii) U iBPART⊇ BiMPART. From the proof of Theorem 30 weU iBPART∩ B ⊆ U iMPART∩ B,U iBPART⊆ BiMPART.For (ii), consider a BN β ∈ BiMPART. Since β is bipartite, the MPART statement ADD(node, parents) had no effecton the structure of β, and could have been left out. In that case we have, by construction, the BPART algorithm andclearly β ∈ U iBPART. (cid:2)Finally, we provide the probability that a randomly generated irregular MPART BN will be bipartite, and in par-ticular an irregular BPART BN.Theorem 32. Let β ∈ U iMPART. For P (cid:3) 1 and C (cid:3) 1, the probability of the event β ∈ BiC(cid:2)P −1(cid:2)(cid:10)(cid:11)MPART isPr(β ∈ BiMPART) =i=1j =0V − jC + V − i − j.Proof. Let C be the set of C non-root nodes and let Xi ∈ C be the node in β currently processed by the MPART al-gorithm. For Xi , parents are picked among {Xi+1, . . . , XC} ∪ {XC+1, . . . , XN }; the non-root nodes already processedand all root nodes. Clearly, Xi ’s parents ΠXi must all be picked among the root nodes V = {XC+1, . . . , XN } for theevent β ∈ BiMPART to take place. For MPART’s first pick of a parent Y for Xi there are k = C − i non-root nodes{Xi+1, . . . , XC} to avoid. For R = false, MPART’s selection distribution is uniform, giving a probability of successfor Xi of Pr(Y ∈ V ) = Vk+V . The nodes in ΠXi need to be distinct but are otherwise picked independently and themultiplication principle can be applied, giving(cid:10)(cid:10)(cid:11)(cid:11)(cid:10)(cid:10)(cid:11)(cid:11)Pr(ΠXi⊆ V ) =Vk + V×V − 1k + V − 1× · · · ×V − P + 1k + V − P + 1=P −1(cid:2)j =0V − jk + V − j.(14)For β ∈ BiMPART to occur, a condition similar to (14) needs to hold for all C non-root nodes; by Definition 29 we getPr(β ∈ Bi⊆ V ). Since the selection of parents for Xi is independent of the selectionMPART) = Pr(ΠX1of parents for Xj , for i (cid:22)= j , the multiplication principle applies again and we obtain for non-root nodes {X1, . . . , XC}in β⊆ V , . . . , ΠXCPr(ΠX1⊆ V , . . . , ΠXC⊆ V ) =C(cid:2)i=1Pr(ΠXi⊆ V ).(15)O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741155Substituting (14) into (15) and using the fact that k = C − i givesPr(β ∈ BiMPART) =C(cid:2)i=1Pr(ΠXi⊆ V ) =C(cid:2)P −1(cid:2)(cid:10)i=1j =0V − jk + V − jC(cid:2)P −1(cid:2)(cid:10)(cid:11)=i=1j =0V − jC + V − i − j(cid:11).Since P (cid:3) 1 and C (cid:3) 1 by assumption, 0 (cid:2) Pr(β ∈ BiMPART) (cid:2) 1 and is thus a well-defined probability. (cid:2)A reader might ask how our MPART algorithm relates to the work of Kask and Dechter [41]. MPART is based ontheir approach and the topologies of the BNs generated are quite similar. There are some minor differences, includingsome details in the algorithms; the CPTs generated; and the clamping of evidence. Our main contribution is notMPART itself but rather the following: The observation that BPART is a special case of MPART as characterizedin Theorems 30, 31, and 32; the fact that C/V -ratio is key also for MPART; and finally the fact that MPART BNsgenerated using C/V (cid:2) 0.75 turn out to be relatively easy for tree clustering as further investigated in Section 6.5. Thereason for our interest in C/V (cid:2) 0.75 is that this inequality holds for the BNs investigated by Kask and Dechter [41].4.3.6. Discussion and related workThe BPART construction is, as already mentioned, a generalization of the random generation of problem instancesfor the satisfiability problem (SAT) [55]. Satisfiability might seem like a limited problem to consider since (i) it isa decision problem rather than an optimization problem like the MPE problem (see Definition 9) and (ii) it gives aparticular bipartite BN topology. Before discussing these possible concerns, we note that the BPART algorithm cangenerate Bayesian networks that are not SAT-like, using parameter settings such as F = random or S > 2.Concerning (i), even though satisfiability is obviously a decision problem, decision problems are special casesof optimization problems. For instance, it has proven fruitful to view the SAT decision problem as the MAXSAToptimization problem [15,30]. MAXSAT is an optimization problem where one maximizes the number of satisfiedclauses. So even though satisfiability is a decision problem, there is a strong connection to an optimization problem.Concerning (ii), we argue that the bipartite topology is interesting in its own right and inference algorithms havebeen specifically designed for bipartite BNs [31]. Important classes of application BNs, including medical diagnosisBNs such as the QMR-DT BN [67] and information theory BNs [27,28], are essentially bipartite.3 Bipartite medicalBNs typically model diseases and symptoms—diseases are root nodes, symptoms are leaf nodes [43,67]. These BNsmay be used to compute an MPE over the disease nodes given known symptoms. Bipartite information theory BNsare used for coding and decoding in the presence of noisy transmission [27,28]. Two classes of information theoryBayesian networks are Hamming code BNs and low density parity code BNs, and for both there is a close relationshipto BPART networks in general and SAT-like BNs in particular. The fact that “traditional” information theory BNs[28] have a fixed number of children per root node corresponds to a BPART parameter value R = true in Fig. 2.A parameter value R = false gives “modern” Gallager codes [48].The BPART topology also provides a well-understood stepping stone towards other topologies, and in particularit is a component in multi-partite BNs. Furthermore, as we detail in the MPART construction above, one can regardthe leaf nodes in a BPART BN as corresponding to the non-root nodes in an arbitrary, non-bipartite BN such as anMPART BN.Finally, we note that our approach is independent of and complementary to related research which relies on Markovchain convergence in order to randomly sample BNs [34,35]. While the use of a Markov chain is different from ourapproach, there is some similarity between their use of heuristic width [35] and our use of heuristics for cliquetree optimization. However, there is an important difference between these two approaches. While we generate BNsrandomly without directly controlling maximal clique size or total clique tree size, Ide et al. enforce a constraint onheuristic width as part of the random BN generation process [35]. This makes their approach more general but alsopotentially slower and more difficult to analyze compared to our approach.3 The QMR-DT BN is computationally challenging, but is unfortunately not publicly available and is consequently not used for experimentationhere.1156O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11745. Analysis of hard and easy synthetic networksThis section synthesizes the discussion, in Section 4, of parameters of randomly generated BNs and the presenta-tion of tree clustering in Section 3. In particular, we discuss how varying some of these BN parameters might affect theperformance of tree clustering inference. This discussion motivates the experiments performed in Section 6 and justi-fies our expectations regarding the effect of varying different parameters. Section 6 also provides further quantitativedetails regarding how clique sizes and inference times increase with the C/V -ratio and when different triangulationheuristics are used.The rest of this section is organized as follows. In Section 5.1 we relate our focus on the C/V -ratio to previousresearch. The following few sections discuss topological issues. In Section 5.2 we identify a conservative lower boundon clique tree size. Qualitatively, this lower bound gives an easy-hard-harder pattern, with increasing C/V -ratio, forBN inference using tree clustering. Section 5.3 introduces the structural concept of only-child BN as well as resultsused in the following. Section 5.4 focuses on cycle formation in the moral graphs of Regular Class A BNs. The reasonfor our analysis of cycles in general and Hamiltonian cycles in particular is that cycles in the moral graph force treeclustering (including HUGIN) to add fill-in edges to the moral graph, which again significantly impacts the size of theclique tree and thus inference time. In Section 5.5, we argue that regular (Class A) BNs should be harder than irregular(Class B) BNs based on insights regarding cycle formation. The final section, Section 5.6, discusses how the natureof CPTs can impact tree clustering inference.5.1. Previous research and the C/V -ratioFor the problem of solving SAT instances, a phase transition phenomenon has been observed for the probabilityof satisfiability [11,55]. This phase transition phenomenon has been studied by controlling the ratio C/V betweenthe number of variables V and the number of clauses C in a CNF formula, generating problem instances, and ex-perimentally observing the resulting probability of satisfiability [55]. Through extensive experimentation it has beenestablished that for large 3CNF formulas the phase transition occurs for C/V ≈ 4.25. For smaller instances, the phasetransition is at higher C/V values. For example, for V = 20 the transition is at C/V ≈ 4.55. Interestingly, it has beenfound that the computational cost of finding a satisfying instantiation is correlated with the probability of satisfiabil-ity [11,55]. For instance, when using the Davis–Putnam algorithm to search for satisfying assignments, there is aneasy-hard-easy pattern for SAT. The hardest instances are found in the region around the phase transition, the hardregion, of the easy-hard-easy pattern [13,26,55,71].The nature of the hardness pattern depends on the algorithmic approach investigated, as has been clearly recognizedfor SAT [55]. Similar to the results for SAT, there is a need for investigations of synthetically generated BPART andMPART networks using a BN inference algorithm such as HUGIN. This is what we do in this section. It should benoted that we are not aiming to establish a phase transition exactly like the one established for SAT; rather our goal isto develop an approach to systematically generate BNs of varying and predictable hardness.5.2. A lower boundIn the BPART and MPART constructions, the C/V -ratio is controlled by varying the values of the input parame-ters C and V .Via the construction described in Section 4.3.2 and the discussion in Section 5.1, we hypothesized that the im-portance of the C/V -ratio carries over to BPART BNs. Given the way we have generated the SAT-like networkscorresponding to generation of CNF formulas, we have mapped V to be the number of root nodes in a BN and C tobe the number of non-root nodes in the BN. When limiting our attention to SAT-like BNs, the problem is exactly thesame as the SAT problem, so one might on first thought expect very similar result. However, in previous work theDavis–Putnam algorithm, a recursive splitting approach [30], was used [55]. The Davis–Putnam algorithm is quitedifferent from tree clustering algorithms including HUGIN. Consequently, while an easy-hard-easy pattern has beenO.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741157observed for SAT formulas using Davis–Putnam [55], there is a different pattern for tree clustering.4 Our reasoning,which is summarized below in Theorem 34, is based on the following definitions.Definition 33 (Clique tree metrics). Let β(cid:6)(cid:6)(cid:6) be a clique tree constructed from a BN β by tree clustering. The total sizeof cliques containing only root nodes from β is defined as kR(β(cid:6)(cid:6)(cid:6)). The total size of cliques containing a mixture ofroot nodes and non-root nodes from β is defined as kM (β(cid:6)(cid:6)(cid:6)).Clearly, no cliques contain leaf nodes only, and thus k(β(cid:6)(cid:6)(cid:6)) = kM (β(cid:6)(cid:6)(cid:6)) + kR(β(cid:6)(cid:6)(cid:6)). For BPART in particular thereare two types of cliques: Cliques with root nodes only (with total size kR), and cliques with leaf nodes and root nodes(total size kM ). The total size of cliques with root nodes only, kR(β(cid:6)(cid:6)(cid:6)), turns out to be important in the experimentalpart of this article; we first focus on kM (β(cid:6)(cid:6)(cid:6)) below.Theorem 34 (Lower bound). Consider a BN β with C non-root nodes, each with P (cid:3) 0 parents. Let S = |ΩX| (cid:3) 2for all nodes X in β. Then, for β’s clique tree β(cid:6)(cid:6)(cid:6), kM (β(cid:6)(cid:6)(cid:6)) (cid:3) CSP +1.Proof. Suppose the m mixed node cliques in the clique tree are enumerated from 1 to m. Let the number of rootnodes and non-root nodes in the ith such clique be denoted V (i) and C(i) respectively, where V (i) (cid:3) P and C(i) (cid:3) 1.Clearly, SC(i)+V (i) (cid:3) SC(i)+P (cid:3) C(i)SP +1. The last inequality follows because it is easy to show, for S (cid:3) 2, P (cid:3) 0and C(i) (cid:3) 1, that SC(i)+P (cid:3) C(i)SP +1. The total size is thereforekM (β(cid:6)(cid:6)(cid:6)) =m(cid:3)i=1SC(i)+V (i) (cid:3)m(cid:3)i=1where the last inequality follows becausem(cid:3)i=1C(i) (cid:3) CSP +1,C(i)SP +1 = SP +1(cid:9)mi=1 C(i) (cid:3) C. (cid:2)An informal explanation of this result is as follows: By construction, each non-root node Ci in β has P parentsand each node in β has S states. Recall from Section 3.1 that as a result of moralization, a non-root node Ci and itsparents ΠCi always end up in the same clique. After moralization, a given non-root node Ci will therefore belongto a clique whose size is at least SP +1. As stated in Theorem 34, one can as a lower bound argue that this is truefor all C non-root nodes, thus the total size of β(cid:6) after moralization is at least CSP +1. Since edges are potentiallyadded but never deleted in subsequent steps of tree clustering, CSP +1 is a lower bound for the total size k(β(cid:6)(cid:6)(cid:6)) ofthe clique tree β(cid:6)(cid:6)(cid:6). Clearly, this space complexity lower bound implies a time complexity lower bound of CSP +1 aswell. Given Theorem 34 and our focus on C/V , how may one make C/V increase? If we hold V constant, then Cneeds to be increasing, and consequently CSP +1 increases. Therefore, the HUGIN compilation and propagation timeswill typically increase, too. An argument for an easy-hard-harder pattern can therefore be made based on HUGIN’smoralization step only, and considering all non-root nodes in BNs (including BPART and MPART BNs). In otherwords, when considering moralization only, one would for tree clustering expect an easy-hard-harder pattern withincreasing C/V -ratio when V is kept constant and C is increased. This expectation is confirmed in experiments inSection 6.5.3. Only-child BNsFor a subclass of Class A BNs, namely only-child BNs (see Definition 36), we can show Hamiltonicity (see The-orem 42). In preparation for that result, we introduce a few definitions that apply to BNs in general. Intuitively, twoBN nodes X and Y are q-siblings if they have q parents in common.Definition 35 (q-siblings). Let q be a non-negative integer. Consider two nodes X and Y in a directed acyclic graph(BN). If |ΠX ∩ ΠY | = q, then X and Y are q-siblings.4 For Davis–Putnam, SAT formulas become easier with a very high C/V -ratio, C/V > 4.25, due to the overwhelming proportion of unsatisfiableinstances where the Davis–Putnam procedure encounters inconsistencies and can prune the search space. Suppose there is an inconsistency for aBN when employing Davis–Putnam. When using tree clustering on the same BN, this inconsistency is not detected until during the propagationphase, which is where the CPT values come into play.1158O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174Based on the concept of q-siblings, the notion of “only (common) child” is introduced below. Informally, a nodeX is an only-child node if its parents ΠX do not have any other children in common.Definition 36 (Only-child). Let X be the nodes in a directed graph (BN). A node X is an only-child node if, for allY ∈ X − {X}, X and Y are k-siblings with k < 2, where k is a non-negative integer. A graph (BN) is an only-childgraph (BN) if all nodes are only-child nodes.We hypothesize that the notion of only-child provides a good approximation to the BNs constructed by BPARTand MPART in the following sense: When the number of root nodes V is “large”, both BPART and MPART will“often” construct a node that is an only-child, as long as the number of non-root nodes C is “not too large” comparedto V . Stated differently, a “high” proportion of the children will be only-children, given “reasonable” assumptions onthe number of non-root nodes and root nodes. We believe that these ideas can be further formalized, and leave this forfuture work.The notion of only-child is not, in general, valid in application BNs and we have not built the assumption into theBPART and MPART algorithms either. However, the notion of only-child paves the way for a formal analysis basedon filtering of clique trees, which we now introduce.Definition 37 (Filtering). Let β(cid:6)(cid:6)(cid:6) be an undirected graph (X(cid:6)(cid:6)(cid:6), E(cid:6)(cid:6)(cid:6)) that is a clique tree. The filtering of clique nodesX(cid:6)(cid:6)(cid:6) using nodes V is defined as X(cid:6)(cid:6)(cid:6)(cid:23)V (cid:24) = X(cid:6)(cid:6)(cid:6) ∩ P(V ), where P(V ) is the power set of V .Given a filtered set of nodes, one can construct an induced subgraph (Definition 5) of the clique tree β(cid:6)(cid:6)(cid:6) as follows:First, consider the tuple of clique tree nodes and edges (X(cid:6)(cid:6)(cid:6), E(cid:6)(cid:6)(cid:6)) in β(cid:6)(cid:6)(cid:6), and form Y (cid:6)(cid:6)(cid:6) ← X(cid:6)(cid:6)(cid:6)(cid:23)V (cid:24). Second, formthe induced subgraph Z(cid:6)(cid:6)(cid:6) ← β(cid:6)(cid:6)(cid:6)[Y (cid:6)(cid:6)(cid:6)] by using the clique tree nodes Y (cid:6)(cid:6)(cid:6). This process, where we define β(cid:6)(cid:6)(cid:6)[V ] =β(cid:6)(cid:6)(cid:6)[X(cid:6)(cid:6)(cid:6)(cid:23)V (cid:24)], is illustrated below. For simplicity we say, for example, V1V2C1 rather than {V1, V2, C1} to represent aclique tree node containing the BN nodes V1, V2, and C1. The difference between Y (cid:6)(cid:6)(cid:6) and β(cid:6)(cid:6)(cid:6)[Y (cid:6)(cid:6)(cid:6)] is that the formerFig. 6. An example of compiling an only-child BPART BN. Only the subgraph induced by the BN root nodes, V , are shown for the moral graph,the triangulated moral graph, and the junction (or clique) tree. This is an only-child BN because no leaf node Ci has, for all i (cid:22)= j , more than onecommon parent with another leaf node Cj . For instance, |ΠC1| = |{V1, V3} ∩ {V2, V3}| = 1.∩ ΠC2O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741159is a set of nodes while the latter is a graph. In Example 38 the notions of only-child, induced subgraph, and filteringare brought together.Example 38 (Only-child BN). Fig. 6 shows a bipartite only-child BN β as well as the induced subgraphs β(cid:6)[V ],β(cid:6)(cid:6)[V ], and β(cid:6)(cid:6)(cid:6)[V ] of β(cid:6), β(cid:6)(cid:6), and β(cid:6)(cid:6)(cid:6) respectively. This is an only-child BN because no leaf node Ci has more thanone parent in common with another leaf node Cj for any i (cid:22)= j . There is a loop (V5, V7, V6, V4, V2, V3, V1) in themoral subgraph β(cid:6)[V ], which again causes fill-in edges (V5, V6), (V5, V4), (V1, V4), and (V1, V2) in the triangulatedsubgraph β(cid:6)(cid:6)[V ] and cliques as shown in the clique subtree β(cid:6)(cid:6)(cid:6)[V ].In Example 38 there is a Hamiltonian cycle in the moral graph for the root nodes, which again causes four fill-inedges and five cliques in the subgraph of the junction tree induced by the BN root nodes.The notion of only-child is useful when we analyze how moralization affects the non-leaf nodes in a BN. Inparticular, the only-child definition is used in the following theorem.Theorem 39 (Degree of moralized only-child BNs). Let C be the non-root nodes in a BPART only-child BN β(Definition 36), and let V be the root nodes. If P = |ΠC| (cid:3) 2 for all C ∈ C, then any V ∈ V has degree d(V ) =(P − 1)|ΨV | in the subgraph γ (cid:6) = β(cid:6)[V ] of the moralized graph β(cid:6) induced by V .Proof. There are three cases: |ΨV | = 0, |ΨV | = 1, and |ΨV | (cid:3) 2. For the first two cases, it holds true for V ∈ Vthat d(V ) = 0 and d(V ) = P − 1 respectively and so the formula is correct. We now consider the case of |ΨV | (cid:3) 2.Since each C ∈ C has P parents, moralization ensures that each parent V ∈ ΠC has at most (P − 1) neighbors in themoralized graph β(cid:6) due to C. Consider arbitrary Ci, Cj ∈ C such that Ci, Cj ∈ ΨV , where i (cid:22)= j . By assumption, each− V ) = ∅ for any V ∈ V . Each ofC ∈ C is an only-child. Due to the assumed only-child property, (ΠCiCi and Cj consequently give (P − 1) neighbors for V in the graph γ (cid:6) = β(cid:6)[V ] induced by V in the moralized graphβ(cid:6). This holds for any Ci, Cj ∈ ΨV , giving a total of d(V ) = (P − 1)o(V ) = (P − 1)|ΨV |. (cid:2)− V ) ∩ (ΠCjAn example of applying Theorem 39 follows.Example 40 (Degree of moralized only-child BNs). Consider Fig. 6 again, and in particular the subgraph γ (cid:6) = β(cid:6)[V ]induced by the BNs root nodes V = {Vi | 1 (cid:2) i (cid:2) 7}. (The second graph from the top in Fig. 6 is γ (cid:6).) For all Vi ∈ V ,d(Vi) = (P − 1)|ΠC| = 2 as predicted by Theorem 39.5.4. Regular BNsIn BPART and MPART, setting R = true creates regular BNs. In this section, the focus is on how regularityimpacts cycles in the moralized graph of a BN. Long, undirected cycles (loops) in the moralized graph are one of themain factors causing large maximal clique sizes. Consequently, a key question for tree clustering is the impact of themoralization and triangulation steps in terms of cycles. We focus on the longest cycle as well as the extreme case ofcycles visiting all nodes exactly once, Hamiltonian cycles.5.4.1. Regular Class A BNsThe issue of the regularity of the underlying graph of a Bayesian network has received some attention in informationtheory [27,28,48]. Gallager’s original codes, which are denoted classical Gallager codes in Table 1, require each rootnode to have the same number of children and each leaf node to have the same number of parents. They are thereforeof the Class A type [28]. Recently, a compelling argument has been provided for why lifting the Class A regularityconstraints may be beneficial when computing the MPE in BNs for decoding [48]. In particular, when using iteratedbelief propagation to compute the MPE given a codeword transmitted over a noisy channel, irregular BNs have beenfound to perform better than regular ones [29].The intuition is that, using iterated belief propagation, high degree root nodes may tend to get quicker to the “right”setting, given that they need to satisfy more constraints. This leads to a “wave effect” that helps lower degree rootnodes find their “right” setting. For belief propagation it is therefore beneficial to have a mixture of high-degree andlow-degree (“low regularity”) root nodes in information theory BNs. These observations raise the question of how BN1160O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174regularity might impact tree clustering. In the following we provide further insight into this question by consideringcycle formation; some related work exists [29].5.4.2. Regular only-child BNsWe now turn to cycle formation in the moralized graph of only-child BNs. Example 38 and Fig. 6 illustrate theimportance of Hamiltonian cycles. The following Hamiltonicity result by Jackson [36] is used below.Theorem 41 (Hamiltonicity, Jackson). Let G be an undirected graph where all vertices have degree δ(G) = d(G) = k.If d(G) (cid:3) n(G)/3 then G is Hamiltonian.In Theorem 42 we apply Theorem 41 to a BNs moral graph β(cid:6).(cid:14)(cid:15)Theorem 42 (Hamiltonicity of BPART only-child BN). Consider an only-child BPART BN β constructed withR = true, let β(cid:6) be the moralized graph of β, and let γ (cid:6) = β(cid:6)[V ] be the subgraph in β(cid:6) induced by the root nodes Vof β. If(P − 1)CPV(cid:3) V3(16)then γ (cid:6) is Hamiltonian.Proof. The theorem follows from Theorem 41, Corollary 18, and Theorem 39. Consider the subgraph γ (cid:6) induced bythe root nodes V in the moralized graph β(cid:6), γ (cid:6) = β(cid:6)[V ]. Since β is an only-child BN it is known from Theorem 39 thatfor a root node X ∈ V (cid:6), where γ (cid:6) = (V (cid:6), E(cid:6)), d(X) = (P − 1)|ΨX|. Due to the BPART BNs regularity, Corollary 18V . Consequently, each root node X from the BN has, in γ (cid:6), the degree d(X) = (P − 1)(cid:15) CPapplies and |ΨX| ≈ CP(cid:16) ord(X) = (P − 1)(cid:13) CP(cid:14). For the purposes of this proof we drop edges for nodes with (P − 1)(cid:13) CP(cid:14) edges, if any, thusVVconstructing γ (cid:6)(cid:6) where all nodes have degree d(γ (cid:6)(cid:6)) = (P − 1)(cid:15) C(P −1)(cid:16). Since γ (cid:6)(cid:6) is a regular graph, we can applyTheorem 41 with n(γ (cid:6)) = n(γ (cid:6)(cid:6)) = V and obtain (16). (cid:2)VVWe note how, for constant P , the ratio C/V plays a prominent role in Theorem 42, specifically in the expression(cid:16). And, as the following example illustrates, the C/V -ratio does not need to be very high before a Hamiltonian(cid:15) CPVcycle is guaranteed.Example 43 (Hamiltonicity of BPART only-child BN). Consider an only-child BPART network with C = 90, V =30, P = 3, and R = true. In particular, consider the undirected graph γ (cid:6) induced by the root nodes V in β, aftermoralization to β(cid:6). Using Theorem 42, we obtain (P − 1) (cid:15)CP /V (cid:16) = 18, while n(γ (cid:6))/3 = 10, so a Hamiltoniancycle is guaranteed.There are cases where one might not be able to show that a Hamiltonian cycle must exist, but one can compute thelongest cycle c(γ (cid:6)) and apply the following theorem due to Dirac [23].Theorem 44 (Longest cycle, Dirac). Let c(G) be the length of the longest cycle of an undirected graph G. If G is2-connected then c(G) (cid:3) min(n(G), 2δ(G)).The longest cycle is determined by the following result when the underlying BN β is of a certain type.Corollary 45 (Longest cycle in BPART only-child BN). Consider an only-child BPART BN β constructed withR = true, and let V be the root nodes in β. Further, let β(cid:6) be the moralized graph of β, and consider γ (cid:6) = β(cid:6)[V ], thesubgraph of β(cid:6) induced by V . If γ (cid:6) is 2-connected then(cid:10)(cid:14)(cid:15)(cid:11)c(γ(cid:6)) (cid:3) minV , 2(P − 1)CPV.(17)O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741161Proof. Theorem 44 applies with n = V = |V | and δ(G) (cid:3) (P − 1)(cid:15) CPV(cid:16); the result follows. (cid:2)Again, we see that the C/V -ratio is part of a lower bound (17), here for the longest cycle c(γ (cid:6)).Example 46. Consider a 2-connected undirected graph γ (cid:6) = β(cid:6)[V ] constructed, using tree clustering, from an only-child BPART BN β created using input parameters R = true, P = 3, V = 30, and C = 60. In this case, Corollary 45applies and the longest cycle is c(γ (cid:6)) = min(30, 24) = 24.While only the longest cycle c(γ (cid:6)) is mentioned in the corollary and example above, it is clear that there may beseveral cycles, of varying length, in a moralized BN. Due to the limited number of root nodes V these cycles are likelyto interact, increasing the chances of fill-in edges, leading to larger cliques and larger maximal clique sizes.5.5. Irregular BNsSo far, the role of the C/V -ratio in regular Class A BNs has been established. We now turn to irregular Class B BNs,which are generated by setting R = false when invoking BPART and MPART. How do Class A regular (R = true)BPART BNs compare to Class B irregular (R = false) BPART BNs, specifically with regard to the Hamiltonicityof the moralized graph as induced by the root nodes of the BN? The following theorem addresses this question byconsidering random variables Mi and Mr representing the minimal root node out-degree in the BPART irregularand regular case respectively (see Section 4.3.4). Using expectations, we consider undirected graphs Gr and Gi , bothinduced over the moralized graphs using the root nodes of the respective Class A and Class B BPART networks.Theorem 47. Let V r and V i be root nodes for only-child BPART BNs α and β generated using R = true andR = false respectively, and put Mr = min(V r ) and Mi = min(V i). Let Gr = α(cid:6)[V r ] and Gi = β(cid:6)[V i]. Further, forb > 0, let d(Gr ) := bE(Mr ) and d(Gi) := bE(Mi) respectively and assume that there exist BPART input parametersfor any E(Mr ) where 0 (cid:2) E(Mr ) < C. There exist BPART input parameter values (excluding for R) such that Gr isHamiltonian while Gi is not necessarily Hamiltonian.Proof. For R = true, set the other input parameter values to BPART such that E(Mr ) = V3(P −1) , which according toTheorem 39 gives d(Gr ) = V3 . Using Theorem 41, Gr is Hamiltonian. For R = false, suppose that the same BPARTinput parameters values are used as for the R = true case (except for R). Clearly, E(Mi) < E(Mr ) and consequentlyd(Gi) < d(Gr ) and d(Gi) < V3 and as a result Gi is not necessarily Hamiltonian according to Theorem 41. (cid:2)Theorem 47 says that, for certain BPART input parameter values excluding R’s, the induced subgraph Gr con-structed from a regular BPART BN is Hamiltonian, while there is no such guarantee for Gi constructed from anirregular BPART BN.The result above and the analysis in Section 4.3.4 suggest that regular Class A BPART networks are, for givenvalues of the input parameters excluding R, more likely to have cycles that need fill-in edges than irregular Class BBPART networks. This again makes the inference problem harder for tree clustering in the Class A BPART case.Further insight is provided by the experiments discussed in Section 6.4, where we consider the effect of regularity onthe hardness of tree clustering in terms of maximal clique sizes and inference times.5.6. Hardness and conditional probability tablesTo investigate the effect of different conditional probability tables (CPTs), we consider, as described in Sec-tion 4.3.2, three CPT types: deterministic CPTs (or, and, and xor), uniform CPTs, and random CPTs. In theBPART and MPART algorithms, CPTs are controlled using the Q and F input parameters.The values of CPTs have, with a few notable exceptions discussed below, little significance during tree clustering’scompilation phase, since this phase is primarily impacted by a BNs topology. The exceptions include the two tech-niques of zero-compression and approximation, techniques that also take numerical CPT values into account [24].These techniques are not used or investigated in this article; we leave this for future research. In the remainder of thissection, we focus on tree clustering’s propagation phase.1162O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11745.6.1. Random root nodes and random non-root nodesThe nature of a BNs CPT impacts the number of MPEs, which we formally define as follows in the case of randomlygenerated BNs.Definition 48 (Number of MPEs). Let β be a randomly generated BN with MPEs X∗ = {x∗MPEs in β is defined as a random variable U = |X∗| = |{x∗}|.1, . . . , x∗u1, . . . , x∗u}. The number ofFor both BPART and MPART BNs with random CPTs there will be very few MPEs, typically one, or E(U) ≈ 1.The reason for this is that it is very unlikely that two different explanations will have exactly the same probability,when all conditional probabilities are sampled from a continuous distribution as is done when Q = random andF = random. The existence of one MPE means that just one propagation in the clique tree β(cid:6)(cid:6)(cid:6) is required. The timerequired for one clique tree propagation increases with the C/V -ratio, on average, due to increased clique sizes, asargued earlier in this section.5.6.2. Uniform root nodes and deterministic non-root nodesWe consider now the case of uniform root nodes and boolean leaf or-nodes, or Q = uniform and F = or.For these SAT-like BPART BNs there are lessons to be learned from the extensive research on SAT and constraintsatisfaction problems using search algorithms [11,13,26,71]. Specifically, an approximately log-linear relationshipbetween the C/V -ratio and the expected number of solutions E(U) has been empirically established, with for exampleˆE(U) ≈ 1000 for C/V = 3 and ˆE(U) ≈ 11.5 for C/V = 4.67 for the case of V = 24 variables and satisfiable probleminstances [71]. In other words, as the C/V -ratio increases the number of solutions decreases on average. While othercharacteristics of search space structure, such as the size variability of global minima [26] and the decrease in thenumber of local minima with the C/V -ratio [71] are important, it is clear that the drop-off in E(U) as a function ofthe C/V -ratio is prominent.For SAT-like BNs the number of MPEs would depend on the C/V -ratio exactly as for SAT. The propagation partof the inference problem should thus be relatively harder for HUGIN belief revision to handle at low C/V -ratios sincerepeated propagations are required to arrive at an MPE x∗ [50]. As the C/V -ratio is increased, a question is how theresulting decrease in E(U)—suggesting decreased inference time due to fewer propagations—will interact with theincrease in inference time due to larger cliques in the clique tree. The experimental part of this article sheds light onthis question.6. Experiments with hard and easy synthetic networksIn this section we report on experiments performed with the HUGIN system using BNs generated by implemen-tations of the BPART and MPART algorithms. Different parameters, as presented in Section 4.3.1, that control thenature of BNs generated by BPART and MPART have been varied in the experiments. Which parameters shouldbe systematically varied? Our main concern in answering this question is to make sure that inference hardness, interms of tree clustering’s maximal clique size, can be varied in an interesting fashion. Some of the parameters, suchas total number of BN nodes N = V + C and number of states per BN node S, are obviously tied to the complexityof inference, and are somewhat less interesting. Other structural and distributional effects may not be as obvious andare therefore the main focus in the experiments below. Specifically, we study the effect—on maximal clique size andinference times—of varying the C/V -ratio, the graph regularity parameter R, and the CPT value parameters F and Q.In the following, Section 6.1 outlines the methodology used. In Section 6.2, we present results from computa-tional experiments that show how maximal clique size and inference time vary with the C/V -ratio for SAT-likeBNs generated using the BPART construction. Section 6.3 follows up on Section 6.2 and compares, using thesame BPART instances, the MINIMUMCLIQUEWEIGHT and MINIMUMFILLINWEIGHT triangulation heuristics.Section 6.4 presents experiments for BNs of varying regularity, again for the BPART construction. Section 6.5 pro-vides experimental results for the MPART construction—unlike for the other experiments we here used randomCPTs and also kept the total number of BN nodes constant.O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–117411636.1. MethodologyIn the experiments reported here, we generated random samples of BN instances according to the BPART orMPART constructions, ran the HUGIN system [24] on the samples, and recorded key clique tree characteristics asintroduced in Definition 10. In particular, we focused on the closely related statistics for maximal clique size (cid:8) (5)and maximal number of nodes in a clique h (3). Given the random generation of instances, these characteristics maybe considered random meta-variables, and we investigated maximal clique size L and number of nodes in a maximalclique H, with corresponding sample statistics such as maximal clique size sample mean ¯xL and maximal cliquesize sample standard deviation sL.5 For tree clustering, the maximal clique size results obviously impact both beliefupdating and belief revision since both rely on clique trees.While we focus primarily on maximal clique size results, we also present some inference time results for beliefrevision. Specifically, we present inference time statistics such as sample median m, sample mean ¯x, and samplestandard deviation s for the time (in seconds) to compute an MPE x∗ ∈ {x∗}—all as a function of the C/V -ratio. These results are, we believe, interesting in their own right and also upper bound the inference time for beliefupdating. For the timing experiments, a Dell 410 700 MHz Pentium III CPU with 1 GB of RAM and using up to 9 GBof swap space was used.1, . . . , x∗The number of parents of a non-root node was P = 3 in all experiments except for those reported in Section 6.5,where non-root nodes have two parents, so P = 2. Note that state space sizes of cliques including (cid:8) (6), “cliquesizes” for short, are given as numbers indicating memory requirements, where the amount of primary memory (RAM)required for storage is implementation-dependent. For instance, an implementation of tree clustering might use adouble data type requiring 8 bytes, in which case the amount of RAM r needed to store a 24-node clique consistingof binary (S = 2) nodes is r = (cid:8) × 8 bytes = Sh × 8 bytes = 224 × 8 bytes = 128 MB.uWe do not report on experiments with as large SAT-like networks as the propositional formulas used in earlier ex-perimental research on SAT—see, for example, [55] or repositories such as SATLIB at http://www.satlib.org. HUGINwas not able to process these large networks for non-trivial C/V -ratios. For the same reason, many of our results donot approach or go beyond the region where C/V ≈ 4.25, even though this is the phase-transition region for SAT for-mulas [55]. This is in line with our goal of constructing BNs for benchmarking and understanding BN computationalhardness rather than focusing on the C/V ≈ 4.25 phase transition. We should also note that HUGIN’s default settingswere generally used in these experiments. The MINIMUMFILLINWEIGHT triangulation heuristic was used, except inSection 6.3. HUGIN’s default “off” settings were used for compression and approximation. As far as evidence for leafnodes, the default was to clamp binary nodes to 1 (or true).6.2. BPART Class B networks: Hardness and the C/V phenomenonWhat is the empirical impact, on maximal clique sizes and inference times, of varying the C/V -ratio when gen-erating irregular (R = false) BPART BNs? In order to investigate this question, 800 SAT-like BNs were generatedusing the signature BPART(uniform, or, 30, C, 2, false, 3), varying the number of leaf nodes from C = 60 to C =102. In other words, the C/V -ratio was varied from C/V = 2.0 to C/V = 3.4. The leaf nodes were clamped to 1during HUGIN inference. In Table 4, Fig. 7, and parts of Table 5, maximal clique size and inference time results fromthese experiments are reported.From the results and analysis provided in Table 4, we conclude that for BPART the maximal clique size increases,on average, with the C/V -ratio. This is in correspondence with the theoretical results earlier in the article. Fig. 7shows, using linear regression, how the number of nodes in the maximal clique varies with the C/V -ratio. This linearregression result shows that for BPART, given the parameters used here, the sample mean number of nodes in themaximal clique, ¯xH, grows linearly with C/V -ratio. We obtain the following empirical expressions for the maximalnumber of nodes in a clique h and maximal clique size (cid:8): h = 3.06 × C/V + 10.0 and (cid:8) = 2h = 23.06×C/V +10.0. Theregression is statistically significant, with an R2 = 0.716, an F -ratio of 2013, and a p-value of 2.10 × 10−220. The95% confidence interval for the slope of the regression line is (2.93, 3.20).Let us now discuss the inference time results presented in Fig. 7 and Table 5. The linear growth of ¯xH and hwith C/V translates into exponential growth of ¯xL and (cid:8) with C/V . This exponential growth of (cid:8) with C/V in turn5 The term “meta-variable” is used to distinguish these random variables from the random variables or nodes making up the BNs themselves.1164O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174Table 4Experimental results showing statistics for BNs generated by the BPART algorithm with R = false (the irregular case), with V = 30 and C rangingfrom 60 to 102. The number of nodes in and sizes of the maximal cliques are shown for instances with varying C/V ratios. The smallest maximalclique contained h = 14 nodes (for 5 instances with C/V = 2.0), while the largest maximal clique contained h = 22 nodes (for 7 instances withC/V = 3.4)Maximal clique sizeNodes HState space LC/V -ratio of BPART Class B (irregular) BNs2.02.22.42.62.83.03.23.4Total14151617181920212216,38432,76865,536131,072262,144524,2881,048,5762,097,1524,194,304526481921728372341035411311235020610334114221046411Number of instancesMean number of nodes ¯xHMean size (in 1000s) ¯xL10015.8770.9410016.86150.610017.60238.610018.07329.710018.65503.310019.29720.96338712616717014360880018.31582.07304220110019.781,0771163937710020.331,565Fig. 7. These results are for Class B (irregular) BPART BNs with V = 30 root nodes and a C/V -ratio ranging from C/V = 2.0 to C/V = 3.4.Left: The number of nodes in the clique tree’s maximal clique is plotted as a function of the C/V -ratio. In this scatter plot, points representing BNinstances as well as linear regression results are displayed. Right: In this graph of averages and medians, the MPE computation time sample means¯xT and sample medians mT are shown.explains the approximately exponential growth in inference times reported in Fig. 7. This result provides an empiricalanswer to the question raised in Section 5.6: On the one hand, we recall that fewer propagations due to a decrease inE(U) should cause a decrease in the total inference time. On the other hand, an increase in expected maximal cliquesize E(H) should cause an increase in inference time. Clearly, the latter effect outweighs the former here.We note, in Table 5, that the standard deviations siT for the inference times are substantial. The high standarddeviation is typical for this area of research and has been observed also for SAT [55,62,63]. Both Table 4 and Fig. 7provide evidence for why the inference time standard deviation siT is substantial: Inference time clearly depends on themaximal clique size L, which is exponential in the number of nodes in the maximal clique H. A change in H has anexponential effect on L, which again has an approximately linear effect on inference time. In the column C/V = 2.0in Table 4, H ranges from 14 to 18 giving a 28.6% increase from the smallest (H = 14) to the largest value (H = 18).This causes L to range from 16,384 to 262,144, a 1500% increase from the smallest (L = 16,384) to the largest(L = 262,144) value.O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741165Table 5The effect of regular and irregular BPART BNs on HUGIN maximal clique sizes and computation times (in seconds) is shown. The followingsample statistics are presented for the regular case: median computation time mrT, standard deviation for computationtime srL. Similar statistics are presented, with “i” superscripts, for the irregular caseT, and mean maximal clique size ¯xrT, mean computation time ¯xrBN parametersV3030303030303030C60667278849096102C/V2.02.22.42.62.83.03.23.4BPART Class A (regular) BNs¯xrmrTTsrT16.7029.0042.8458.41101.45121.05156.45168.3019.433.145.665.8109.8131.0158.4192.011.516.419.831.859.766.365.497.4¯xrL382.1655.4923.01,3762,0812,6423,3244,708BPART Class B (irregular) BNsmiT¯xiTsiT3.677.119.9414.6021.4329.4145.6458.743.108.2111.6118.3826.3135.3253.7867.362.105.016.7512.5217.9821.3633.9042.79¯xiL70.94150.6238.6329.7503.3720.91,0771,565RatiosT/ ¯xi¯xrT3.134.043.933.584.173.712.952.85L/ ¯xi¯xrL5.394.353.874.174.133.663.093.01Table 6Randomly generated BPART Class B instances, for V = 30 and C = 60, showing the seven instances with the smallest and largest maximal clique(cid:6)(cid:6)(cid:6)i of a BN instance βi , the number of cliques it contains for different clique sizes. From a samplesizes. A column contains, for the clique tree βof 100 BNs, the five columns β0, β12, β73, β89, and β92 present the BN instances with smallest maximal clique size (cid:8) = 16, 384, while the twocolumns β6 and β80 present the BN instances with largest maximal clique size (cid:8) = 262,144. Of these instances, β6 has the largest total clique sizek = 39,776, while β92 has the smallest total clique size k = 53,344Clique sizeClique trees for BPART Class B BNs, C/V = 2.0(cid:6)(cid:6)(cid:6)89(cid:6)(cid:6)(cid:6)12(cid:6)(cid:6)(cid:6)73(cid:6)(cid:6)(cid:6)0βββββ(cid:6)(cid:6)(cid:6)92β(cid:6)(cid:6)(cid:6)6β(cid:6)(cid:6)(cid:6)80Nodes hState space (cid:8)4567891011121314151617181632641282565121,0242,0484,0968,19216,38432,76865,536131,072262,1446022212122621142214613212111146032211346031431212601222211Maximal clique size (cid:8)Total clique tree size kMaximal number of nodes hMaximal clique (cid:8)kTop three cliques (%)× 100% (%)16,38461,0561426.860.416,38475,8401421.664.816,38481,2001420.260.516,38499,1381416.549.616,38453,3441430.776.8262,144439,7761859.693.1601121211211262,144284,1921892.296.6Table 6 contains further details for the seven extreme instances listed in the column C/V = 2.0 in Table 4. Table 6displays the five instances with smallest maximal clique size (cid:8)(β0) = (cid:8)(β12) = (cid:8)(β73) = (cid:8)(β89) = (cid:8)(β92) = 16,384and the two instances with largest maximal clique size (cid:8)(β6) = (cid:8)(β80) = 262,144. The following example illustrateshow total clique tree size k were distributed among kM and kR (Definition 33) for one of these example BPARTinstances.Example 49. Table 6 contains a BPART(uniform, or, 30, 60, 2, false, 3) instance β12. The total state space sizefor the clique tree β(cid:6)(cid:6)(cid:6)12) + kR(β(cid:6)(cid:6)(cid:6)12) = 60 × 24 = 960 andkR(β(cid:6)(cid:6)(cid:6)12) = 2 × 24 + 1 × 25 + · · · + 1 × 213 + 4 × 214 = 73,792.12) = 75,840, where kM (β(cid:6)(cid:6)(cid:6)12 is given by k(β(cid:6)(cid:6)(cid:6)12) = kM (β(cid:6)(cid:6)(cid:6)1166O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174In terms of number of cliques in the above example, the cliques of size Si = 24 are clearly dominant, with qi = 60cliques contributing towards kM (β(cid:6)(cid:6)(cid:6)12): 60×24 = 960. However, in terms of impact on k(β(cid:6)(cid:6)(cid:6)12) and in particularthe maximal cliques of size Si = 214 are much more important, with contribution 4 × 214 = 65,536. Several pointscan be made regarding these seven extreme instances. First, while most cliques have size |ΩH | = 16 as indicated bythe lower bound of Theorem 34, there is a strong heavy tail effect present, in particular in β6 with total clique sizek = 439,776 and in β80 with k = 284,192. The three largest cliques make up 93.1% and 96.6% of total clique sizein BNs β6 and β80 respectively. In fact, β80 is extreme; the size (cid:8) of the largest clique is 32 times that of the secondlargest clique. Clearly, whether cliques of size 217 and 218 are present (for β6 and β80) or absent (for the rest) has,in this example, a dramatic impact on total clique tree size and thereby on variation in total clique tree size betweenthe different instances. This is a pattern that may in part explain the large standard deviations for inference times inTable 5.12), kR(β(cid:6)(cid:6)(cid:6)In related experiments, details of which are omitted due to space restrictions, the randomly generated BNs consistedof V ∈ {20, 30, 35, 40} root nodes, the number of leaf nodes C was varied from 40 to 140, and the C/V -ratio wasranging from 2.0 to 4.0. The results were similar to what was reported above: As the C/V -ratio increased, the meaninference time increased at an approximately exponential rate.Stated qualitatively, our experiments confirm the hypothesis that a “high” C/V -ratio implies a “large” maximalclique size which again implies a “long” inference time. There is a strong causal relationship from a high C/V -ratio toa large maximal clique size. Of course, this is for the BPART topology, using certain ranges for the input parametersof BPART, and for a particular tree clustering triangulation heuristic MINIMUMFILLINWEIGHT. In the next sectionwe turn to a more detailed study of triangulation heuristics.6.3. BPART Class B networks: Triangulation heuristicsHere, we report on experiments with the MINIMUMCLIQUEWEIGHT triangulation heuristic. A first question iswhether varying the C/V -ratio has a similar impact when using the MINIMUMCLIQUEWEIGHT heuristic as wasobserved for the MINIMUMFILLINWEIGHT heuristic in Section 6.2. In order to answer this question, additionalexperiments were performed using the BPART Class B instances from Section 6.2, but this time compiled by treeclustering using the MINIMUMCLIQUEWEIGHT heuristic. The maximal clique sizes resulting from this set of experi-ments were analyzed using linear regression. The regression results for MINIMUMCLIQUEWEIGHT were statisticallysignificant, with an R2 = 0.695, a standard error of 0.906, and an F -ratio of 1822. This regression gives the followingempirical results for the maximal number of nodes in a clique h and maximal clique size (cid:8): h = 2.98×C/V +10.5 and(cid:8) = 2h = 22.98×C/V +10.5. The 95% confidence interval for the slope of h is (2.85, 3.12) with a p-value of 3.3×10−208.The 95% confidence interval for the intercept of the regression line is (10.13, 10.89) with a p-value of 2.9 × 10−273.These results show that the number of nodes in the maximal clique h grows linearly with C/V -ratio also when theMINIMUMCLIQUEWEIGHT heuristic is used. This gives an exponential growth in (cid:8) with implications similar to whatwas discussed in Section 6.2 for MINIMUMFILLINWEIGHT.A second question is whether MINIMUMCLIQUEWEIGHT and MINIMUMFILLINWEIGHT produce very differentmaximal clique sizes. Comparing the respective regression lines, it is clear that MINIMUMFILLINWEIGHT is, onaverage, slightly superior to MINIMUMCLIQUEWEIGHT for the C/V range considered here. One might ask whetherthe difference is statistically significant. Out of the 800 BN samples, there were 525 instances in which the maximalclique sizes were the same for the two heuristics, 231 instances in which the MINIMUMFILLINWEIGHT was better,and 44 instance in which MINIMUMCLIQUEWEIGHT was better. Doing a paired comparison using the sign test, wefound a difference between the two heuristics at the 1% significance level; MINIMUMFILLINWEIGHT was indeedbetter according to this statistical test.6.4. BPART Class A networks: Hardness and graph regularityWhat is the impact, on maximal clique sizes and inference times, of varying the C/V -ratio when generatingregular (R = true) BPART BNs? To answer this question and to complement our analysis of regularity, as presentedin Section 5.4, we created BNs using the BPART construction while varying the C/V -ratio, as earlier, but generatingClass A (regular) BNs. Specifically, the signature used for BN construction was BPART(uniform, or, V, C, 2,true, 3). A similar signature BPART(uniform, or, V, C, 2, false, 3) was used to construct Class B (irregular) BNsO.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741167Fig. 8. Tree clustering results for BPART BNs with V = 30 root nodes and C/V -ratios from C/V = 2.0 to C/V = 3.4 are shown. The plots reflectboth Class B irregular and Class A regular BNs. Left: The number of nodes in the maximal clique is shown as a function of the C/V -ratio. Datapoints representing Class A regular BPART instances as well as regression results are displayed. The regression line for Class B irregular BPARTinstances is also included. Right: The mean computation times (in seconds) is plotted as a function of C/V -ratio. Specifically, the sample mean ¯xrTfor the Class A regular BNs and the sample mean ¯xiT for the Class B irregular BNs are shown for varying levels of the C/V -ratio.Table 7Experimental tree clustering results showing statistics for BNs generated using BPART with input parameters R = true (Class A regular case),V = 30, and C ranging from 60 to 102. The number of nodes and size of the maximal clique is shown for varying C/V ratios. The smallestmaximal clique contains h = 16 nodes (for C/V = 2.0), while the largest maximal clique contains h = 23 nodes (for C/V = 3.4)C/V -ratio of BPART Class A (regular) BNs2.02.22.42.62.83.03.23.4TotalMaximal clique sizeNodes HState space L161718192021222365,536131,072262,144524,2881,048,5762,097,1524,194,3048,388,6083144431841644361031481111448361Number of BN instancesMean number of nodes ¯xHMean size (in 1000s) ¯xL10018.27382.110019.12655.410019.6092310020.221,376128571410020.842,0812135331110021.162,64234548410021.533,324317572310022.004,708318711231872191512880020.342011in Section 6.2. The essential difference between the two signatures is that in the relaxed Class A regular case, eachroot node has essentially the same number of children, while in the Class B irregular case, the number of children isexactly distributed as b(C, P /V ) (see Theorem 20) and approximately distributed as b(CP , 1/V ) (see Theorem 21).Maximal clique size and inference time results for these experiments, where leaf nodes again were clamped to 1during HUGIN inference, are presented in Table 5 and in Fig. 8. Table 5’s columns ¯xrL summarize the bottomrows of Tables 7 and 4, respectively. Table 7 presents in detail how the maximal clique size and number of nodes inthe maximal clique vary with the C/V -ratio.L and ¯xiFig. 8 summarizes linear regression results, showing how the number of nodes in the maximal clique, ¯xH, varieslinearly with the C/V -ratio. The regression gives these results for the maximal number of nodes in a clique h andmaximal clique size (cid:8): h = 2.59 × C/V + 13.4 and (cid:8) = 2h = 22.59×C/V +13.4. The regression, which is based on 800observations, is statistically significant, with R2 = 0.703, an F -ratio of 1893, and a p-value of 7.5 × 10−213. The95% confidence interval for the slope of the regression line is (2.47, 2.70). For the parameters used here, the result1168O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174for h shows that for the BPART Class A construction, the number of nodes in the maximal clique grows linearlywith C/V -ratio. This linear growth of h explains, similar to what was discussed in Section 6.2, the approximatelyexponential growth of (cid:8) and increasing inference times.Can we conclude that the maximal clique sizes for BPART Class A and Class B BNs are significantly different?Comparing with the corresponding results for Class B BNs in Table 4, we note that the regular Class A instancesin Table 7 are skewed towards higher maximal clique sizes. A statistical t-test for two population means, assumingunknown population means and variances, was performed for the maximal clique sizes of the Class A and Class Bsamples. With t = 26.5, and critical values for t of 1.65 (one-tail) and 1.96 (two-tail), the null-hypothesis that thetwo population means are equal, μiL, is rejected. This result, along with Table 7 and Fig. 8, sheds additionallight on the difference in computation times between Class A and Class B BNs. This result is also consistent with thetheoretical analysis in Section 5.4.L = μrT. From the ratios ¯xrThe regular BPART BNs on average required three to four times more time for computation compared to irregularT, versus the irreg-T one can easily determine that HUGIN consistently was faster on irregular thanL, wereL, we see that there was aL (cid:2) 5.39 inL/ ¯xiBPART BNs. More formally, consider the sample mean computation times for the regular case, ¯xrular case, ¯xiT/ ¯xion regular BNs: 2.85 (cid:2) ¯xrin line with the results for ¯xrcorresponding larger sample mean for Class A regular BNs than for Class B irregular BNs: 3.01 (cid:2) ¯xrTable 5.T (cid:2) 4.17 in Table 5. The results for maximal clique size sample means, ¯xrT and ¯xiT. From the ratios between these sample means, ¯xrL and ¯xiL/ ¯xiT/ ¯xiIn summary, we have shown that regularity is a factor not only for iterated belief propagation when used oninformation theory BNs [28,48], but also for tree clustering on closely related BNs. Confirming and adding detail tothe analytical results, we have shown empirically that regular Class A BNs are harder, on average, than irregular ClassB BNs due to their larger maximal cliques.6.5. MPART Class B networks: Hardness and conditional probabilitiesIn Sections 6.2, 6.3, and 6.4 we reported results for BPART BNs. What happens if several of the parameters aswell as the topology are changed such that quite different BNs are generated? Is the C/V -ratio still important whenBNs are generated using the MPARTconstruction? These questions are investigated empirically in this section, usinga data set containing 800 MPART samples. The signature used to generate BNs was MPART(random, random, V,C, 2, false, 2) with C ∈ {110, . . . , 156} and V = 256 − C, giving a C/V -ratio ranging from 0.75 to 1.56. FollowingKask and Dechter [41], we kept N constant, varied V , and determined C by setting C = N − V . The effect of this is anon-linear change in the C/V -ratio as a function of change in C. Also note that there are P = 2 parents per non-rootnode rather than P = 3 as used in the experiments above; other differences are that random CPTs were used and leafnodes were not clamped during inference.Fig. 9 plots results in the form of individual data points—number of nodes in the maximal clique of a sample BNas function of C/V -ratio—and also displays linear regression results. The following empirical results for h and (cid:8)were obtained: h = 17.7 × C/V − 5.87 and (cid:8) = 2h = 217.7×C/V −5.87. The regression is statistically significant, withTable 8Inference times (in seconds) for a total of 800 BNs generated using the MPART construction are shown. Median computation time mT, meancompilation time ¯xC with standard deviation sC, mean propagation time ¯xP with standard deviation sP, mean computation time ¯xT with standarddeviation sT, and the mean size of the maximal clique ¯xL are also presentedBN parametersV146136126116106104102100C110120130140150152154156V + C256256256256256256256256C/V0.750.881.031.211.421.461.511.56Statistics for MPART BNs¯xCmTsC0.1560.2030.2810.6884.2048.03912.08020.4200.1480.1950.2680.5663.4395.56510.03017.8110.0130.0470.0710.3593.5945.12110.93522.477¯xP0.0100.0140.0470.3003.1745.3659.85217.585sP0.0080.0090.0460.3523.6175.16311.10322.568¯xT0.1580.2090.3150.8676.61410.92919.88235.396sT0.0140.0500.1090.7107.20810.28122.03545.044¯xL0.211.3613.4097.081,1861,9513,5097,229O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741169Fig. 9. For BNs generated using the MPART construction, these plots display tree clustering results as function of C/V -ratio. Here, the C/V -ratioranges from C/V = 0.75 to C/V = 1.56. These results are for non-root nodes C ranging from C = 110 to C = 156, and for root nodes V rangingfrom V = 146 to V = 100. Left: The number of nodes in the maximal clique is plotted as a function of the C/V -ratio. This scatter plot showsBN instances as well as linear regression results. Right: The sample mean computation times ¯xT and sample median computation times mT aredisplayed in this log-plot.Table 9Experimental results for tree clustering on BNs generated using the MPART construction. Here, C + V = 256 for all C/V ratios, with C rangingfrom 110 to 156 and V ranging from 100 to 146. The number of nodes in and sizes of the maximal cliques are shown for a number of instances forvarying C/V ratios. The smallest maximal clique contains h = 4 nodes (for C/V = 0.75), while the largest maximal clique contains h = 26 nodes(for C/V = 1.56)Maximal clique sizeNodes HState space LC/V -ratio of MPART BNs0.750.881.031.211.421.461.511.561925321712313172329187337181831787145678910111213141516171819202122232425261632641282565121,0242,0484,0968,19216,38432,76865,536131,072262,144524,2881,048,5762,097,1524,194,3048,388,60816,777,21633,554,43267,108,864431026222310111411221825105414101633219421391021212095113717222018543Number of BN instancesMean number of nodes ¯xHMean size (in 1000s) ¯xL1007.080.211009.751.3610012.6113.4010015.7997.0810019.151,18610020.071,95110020.723,50910021.587,2291170O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174an R2 = 0.908, an F -ratio of 7882, and a p-value of 0. The 95% confidence interval for the slope of the regressionline for h is (17.3, 18.1). As before, the regression results show that h increases linearly with C/V -ratio, giving anapproximately exponential growth in (cid:8) with the C/V -ratio. An approximately exponential growth in maximal cliquesize (cid:8) explains, to a large extent, the inference times reported in Table 8 and to the right in Fig. 9. Table 9 providesfurther details on how statistics of the maximal clique size, including ¯xH and ¯xL, increase with the C/V -ratio.The results for MPART were in many respects similar to the results for BPART. However, there was slightlygreater variation in the MPART case, as can be seen by comparing the sample standard deviations or by comparingTables 4 and 9. Also, the growth in maximal clique size and thus in computation time as a function of C/V -ratiowas in fact stronger for MPART than for BPART. This is clearly in part due to the different CPTs used, followingthe discussion in Section 5.6. For MPART, CPT values were picked from a random distribution, giving just onepropagation, while for BPART multiple propagations were in general needed.We also note that these MPART BNs are similar to the multipartite BNs investigated by Kask and Dechter [41].They also randomly generated BNs with C/V (cid:2) 0.75 and N = V + C = 256 nodes. For C/V (cid:2) 0.75, our MPARTsamples were relatively easy for HUGIN to solve on the average. For example, the highest C/V -ratio used by Kaskand Dechter, C/V = 0.75, had for MPART a sample mean inference time of ¯xT = 0.1579 seconds (see Table 8, rowC/V = 0.75) and a maximal clique size sample mean ¯xL = 210 (see Table 9, column C/V = 0.75). For MPART, thelargest maximal clique size in the C/V = 0.75 subsample is (cid:8) = 2, 048. When using MPART BNs with C/V (cid:2) 0.75in experiments with other algorithms, one should keep in mind that these BNs are in fact relatively easy for HUGIN tosolve on average. On the other hand, with C/V (cid:3) 1.50 these MPART BNs are typically quite challenging.7. Conclusion and future workThe performance of Bayesian network (BN) inference algorithms has in previous research been empirically evalu-ated using BN instances from applications. To complement such experiments, randomly generated BNs have also beenused [7,17,34,41,56,69,70]. We believe that a certain amount of care is required when randomly generating BNs. Thegeneration algorithms need to provide “knobs” for controlling the difficulty of the generated instances. The syntheticBNs need to be idealized, to support analysis, but at the same time they also need to be somewhat realistic and relevantto applications.We have developed, based on previous research, a paradigm for systematically generating increasingly hard randominstances for BN inference. One of the classes of BNs, the bipartite BPART networks generated by the BPARTalgorithm, extends research on generating hard instances for satisfiability problems [55]. Here, we have exploitedthe relationship between computing an MPE and finding a satisfying assignment of a corresponding CNF formula toconstruct hard instances for the MPE problem. These BPART networks are also similar in structure to applicationBNs from medicine [67] and information theory [27,28,49], and our results should be relevant to these two areas ofresearch. The other class of BNs, the MPART networks, is closely related to an approach of Kask and Dechter [41].For these MPART networks we have analyzed the relationship to the BPART BNs.Different algorithms for BN inference in general and MPE computation in particular have been investigated usingour experimental paradigm [51]. In this article, we have focused on the HUGIN tree clustering algorithm [2,37,39,46].HUGIN is one of the best probabilistic inference algorithms available, and using this algorithm, and systematicallyvarying some structural and distributional parameters of the synthetic Bayesian networks, we have shown how HUGINinference is impacted. In particular, we randomly generated BNs by using the BPART and MPART algorithms andvarying these parameters:• the ratio C/V of the number of non-root nodes, C, to the number of root nodes, V , in the BN,• the regularity structure of the BN’s underlying graph, and• the conditional distribution tables (CPTs) of the nodes in the BN.We have carefully studied how these parameters impact inference hardness, expressed in terms of maximal cliquesize or inference time, in tree clustering. We identified, for HUGIN, an easy-hard-harder pattern for both the BPARTand MPART networks. For both classes, generating random networks can result in very easy instances. On the otherhand, by carefully varying parameters along certain dimensions, one can construct BNs that existing tree clusteringalgorithms cannot handle. Specifically, we have found that the C/V -ratio, through its impact on maximal clique size,O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741171is an indication of the inferential hardness of the network under suitable structural and parametric assumptions. As theC/V -ratio grows, even when fixing the number N = C + V of nodes in the network, the inference problem becomesharder, on average, due to an increasing maximal clique size. We now summarize our results in more detail.Four structurally distinct classes of BNs were identified in this article—Class A, Class B, Class C, and Class DBNs—of which we more closely investigated Class A (regular) and Class B (irregular) BNs. For Class B irregularBPART BNs, our analysis showed an easy-hard-harder pattern with increasing C/V -ratio. Through regression analy-sis, a linear relationship was established between the C/V -ratio and the mean number of nodes in the maximal clique,giving an approximately exponential growth in maximal clique size which was also reflected in HUGIN tree clusteringinference time. Results were similar for tree clustering’s MINIMUMFILLINWEIGHT and MINIMUMCLIQUEWEIGHTtriangulation heuristics, with the former being slightly but significantly better in terms of optimizing maximal cliquesize.A second structural parameter we considered, again using the BPART construction, was the regularity of theunderlying graph of the BN. Our analysis showed that regular Class A BPART BNs should be harder than irregularClass B BPART BNs, and this expectation was confirmed in experiments. A regression analysis exhibited exponentialgrowth in maximal clique size as a function of C/V -ratio, similar to the irregular case. We also showed experimentally,while keeping other parameters of the generated networks fixed, that maximal clique size sample means were from 3.0to 5.4 times greater for Class A BNs compared to Class B BNs. These results also shed new light on the computationalbenefit of irregularity in information theory BNs [27,28,48].Our studies with MPART used quite different input parameters for BN sample generation. Still, the regressionresults bear resemblance to BPART’s regression results. There turned out to be approximately linear growth in themean number of nodes in the maximal clique as a function of C/V , giving an approximately exponential growth inmaximal state space size, from which one can expect an approximately exponential growth in inference times. In fact,the graph of averages showed slightly stronger than exponential growth. It also turned out that previous work with asimilar class of BNs, using C/V (cid:2) 0.75 [41], correspond to a relatively easy region of the MPART distribution.Since the complexity of most exact Bayesian network inference algorithms—including tree clustering algorithms,conditioning algorithms, and elimination algorithms—depend on treewidth or optimal (minimal) maximal clique size[5,17,20,21], we believe that our results are of interest to researchers investigating exact Bayesian network inferencealgorithms. Our empirical results are limited by the fact that we employed the suboptimal MINIMUMFILLINWEIGHTand MINIMUMCLIQUEWEIGHT triangulation heuristics; however due to the hardness of BN inference problems [1,14,60,66] and the widespread adoption of these and similar heuristics we believe that our results are of significantinterest.In addition to showing some interesting aspects of HUGIN and the tree clustering approach, we believe this line ofresearch to be essential so that valid experimental evaluation of other algorithms can be performed. For instance, inrelated work we have used BPART and MPARTnetworks to benchmark stochastic local search and have found strongdependence of inference time on the C/V -ratio, qualitatively similar to the results reported here [51,53]. The approachof constructing synthetic BNs has guided us in developing a stochastic local search approach to the point where itoutperforms HUGIN on certain synthetic instances as well as on certain application BNs [51,53]. Similar researchwould help in developing a better understanding of different algorithms under varying conditions. In particular, ourresults can be used by other researchers to focus their work on areas in the space of Bayesian networks where wefound time- or space-consumption to be relatively high for tree clustering.This research can be extended in several other directions. It is important to develop a better understanding of otherdimensions (beyond C/V , regularity and different CPT types), intermediate cases, and other inference algorithms.While we have studied extreme cases in a few dimensions, it is essential to perform similar studies for other dimen-sions. Other important areas include improved analytical models for clique tree cluster formation and growth, loopformation, interactions between loops, and the placement of fill-in edges in the moral graphs induced by BNs. Improv-ing the understanding of the relationship between synthetically generated networks and networks from applicationswould also be a natural extension of this research.AcknowledgementsThe research reported here was largely conducted while Ole J. Mengshoel was at the University of Illinois, Urbana-Champaign. Ole J. Mengshoel and David C. Wilkins gratefully acknowledge support in part by ONR Grant N00014-1172O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–117495-1-0749, ARL Grant DAAL01-96-2-0003, and NRL Grant N00014-97-C-2061. Dan Roth gratefully acknowledgesthe support of NSF grants IIS-9801638 and SBR-987345. Vadim Bulitko, David Fried, Song Han, William Hsu, BrentSpillner, and anonymous reviewers are acknowledged for comments related to this work. David Fried, Song Han, andMisha Voloshin are acknowledged for their co-development of the software used in the experiments.References[1] A.M. Abdelbar, S.M. Hedetnieme, Approximating MAPs for belief networks is NP-hard and other theorems, Artificial Intelligence 102 (1998)21–38.[2] S.K. Andersen, K.G. Olesen, F.V. Jensen, F. Jensen, HUGIN—a shell for building Bayesian belief universes for expert systems, in: Proceedingsof the Eleventh International Joint Conference on Artificial Intelligence, vol. 2, Detroit, MI, August 1989, pp. 1080–1085.[3] S. Andreassen, M. Woldbye, B. Falck, S.K. Andersen, MUNIN—A causal probabilistic network for interpretation of electromyographicfindings, in: Proceedings of the Tenth International Joint Conference on Artificial Intelligence, Milan, Italy, August 1987, pp. 366–372.[4] D. Angluin, L.G. Valiant, Fast probabilistic algorithms for Hamiltonian circuits and matchings, Journal of Computer and System Sci-ences 18 (2) (1979) 155–193.[5] S. Arnborg, Efficient algorithms for combinatorial problems on graphs with bounded decomposability—a survey, BIT 25 (1985) 2–23.[6] S. Arnborg, D.G. Corneil, A. Proskurowski, Complexity of finding embeddings in a k-tree, SIAM Journal of Algebraic and Discrete Methods 8(1987) 277–284.[7] A. Becker, D. Geiger, Approximation algorithms for the loop cutset problem, in: Proceedings of the Tenth Annual Conference on Uncertaintyin Artificial Intelligence (UAI-94), San Francisco, CA, 1994, pp. 60–68.[8] U. Bertele, F. Brioschi, Nonserial Dynamic Programming, Academic Press, New York, 1972.[9] A. Bobbio, L. Portinale, M. Minichino, E. Ciancamerla, Improving the analysis of dependable systems by mapping fault trees into Bayesiannetworks, Reliability Engineering and System Safety 71 (3) (2001) 249–260.[10] N. Chater, C.D. Manning, Probabilistic models of language processing and acquisition, Trends in Cognitive Sciences 10 (7) (2006) 335–344.[11] P. Cheeseman, B. Kanefsky, W.M. Taylor, Where the really hard problems are, in: Proceedings of the Twelfth International Joint Conferenceon Artificial Intelligence, Sidney, Australia, 1991, pp. 331–337.[12] M. Ciaramita, M. Johnson, Explaining away ambiguity: Learning verb selectional preference with Bayesian networks, in: 18th InternationalConference on Computational Linguistics (COLING-00), Saarbrücken, Germany, 2000, pp. 187–193.[13] D. Clark, J. Frank, I. Gent, E. MacIntyre, N. Tomov, T. Walsh, Local search and the number of solutions, in: Proceedings of the SecondInternational Conference on Principles and Practices of Constraint Programming, in: Lecture Notes in Computer Science, vol. 1118, Springer,Berlin, 1996, pp. 119–133.[14] F.G. Cooper, The computational complexity of probabilistic inference using Bayesian belief networks, Artificial Intelligence 42 (1990) 393–405.[15] P. Crescenzi, V. Kann, A compendium of NP optimization problems, Technical Report SI/RR-95/02, Dipartimento di Scienze dell’Infor-mazione, Universita di Roma “La Sapienza”, Roma, Italy, 1995.[16] A. Darwiche, Conditioning methods for exact and approximate inference in causal networks, in: Proceedings of the Eleventh Annual Confer-ence on Uncertainty in Artificial Intelligence (UAI-95), Montreal, Canada, 1995, pp. 99–107.[17] A. Darwiche, Recursive conditioning, Artificial Intelligence 126 (12) (2001) 5–41.[18] A.P. Dawid, Applications of a general propagation algorithm for probabilistic expert systems, Statistics and Computing 2 (1992) 25–36.[19] R. Dechter, Bucket elimination: A unifying framework for reasoning, Artificial Intelligence 113 (1–2) (1999) 41–85.[20] R. Dechter, Y. El Fattah, Topological parameters for timespace tradeoff, Artificial Intelligence 125 (1–2) (2001) 93–118.[21] R. Dechter, J. Pearl, Network-based heuristics for constraint satisfaction problems, Artificial Intelligence 34 (1) (1987) 1–38.[22] F.J. Diez, Local conditioning in Bayesian networks, Artificial Intelligence 87 (1–2) (1996) 1–20.[23] G.A. Dirac, Some theorems on abstract graphs, in: Proc. London Math. Soc. 2 (1952) 69–81.[24] Hugin Expert, Hugin API: Reference Manual, Hugin Expert, 2004.[25] J. Franco, M. Paull, Probabilistic analysis of the Davis Putnam procedure for solving the satisfiability problem, Discrete Applied Mathematics 5(1983) 77–87.[26] J. Frank, P. Cheeseman, J. Stutz, When gravity fails: Local search topology, Journal of Artificial Intelligence Research 7 (1997) 249–281.[27] B.J. Frey, Graphical Models for Machine Learning and Digital Communication, MIT Press, Cambridge, MA, 1998.[28] R.G. Gallager, Low density parity check codes, IRE Transactions on Information Theory 8 (1962) 21–28.[29] X. Ge, D. Eppstein, P. Smyth, The distribution of loop lengths in graphical models for turbo decoding, IEEE Transactions on InformationTheory 47 (6) (2001) 2549–2553.[30] P.W. Gu, J. Purdom, J. Franco, B.W. Wah, Algorithms for the satisfiability SAT problem: A survey, in: Satisfiability Problem: Theory andApplications, in: DIMACS Series in Discrete Mathematics and Theoretical Computer Science, American Mathematical Society, Providence,RI, 1997, pp. 19–152.[31] M. Henrion, Search-based methods to bound diagnostic probabilities in very large belief networks, in: Proceedings of the Seventh AnnualConference on Uncertainty in Artificial Intelligence (UAI-91), University of California at Los Angeles, CA, 1991, pp. 142–150.[32] E.J. Horvitz, H.J. Suermondt, G.F. Cooper, Bounded conditioning: Flexible inference for decisions under scarce resources, in: Proceedings ofthe Fifth Conference on Uncertainty in Artificial Intelligence (UAI-89), Windsor, Ontario, Morgan Kaufmann, 1989, pp. 182–193.[33] C. Huang, A. Darwiche, Inference in belief networks: A procedural guide, International Journal of Approximate Reasoning 15 (1996) 225–263.O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–11741173[34] J.S. Ide, F.G. Cozman, Generating random Bayesian networks, in: Proceedings on 16th Brazilian Symposium on Artificial Intelligence, Portode Galinhas, Brazil, November 2002, pp. 366–375.[35] J.S. Ide, F.G. Cozman, F.T. Ramos, Generating random Bayesian networks with constraints on induced width, in: Proceedings of the 16thEuropean Conference on Artificial Intelligence, 2004, pp. 323–327.[36] W. Jackson, Hamilton cycles in regular 2-connected graphs, Journal of Combinatorial Theory Series B 29 (1980) 27–46.[37] F.V. Jensen, An Introduction to Bayesian Networks, Springer, New York, 1996.[38] F.V. Jensen, S.L. Lauritzen, K.G. Olesen, Bayesian updating in causal probabilistic networks by local computations, SIAM Journal on Com-puting 4 (1990) 269–282.[39] F.V. Jensen, K.G. Olesen, S.K. Andersen, An algebra of Bayesian belief universes for knowledge-based systems, Networks 20 (5) (1990)637–659.[40] P. Jones, C. Hayes, D. Wilkins, R. Bargar, J. Sniezek, P. Asaro, O.J. Mengshoel, D. Kessler, M. Lucenti, I. Choi, N. Tu, J. Schlabach,CoRAVEN: Modeling and design of a multimedia intelligent infrastructure for collaborative intelligence analysis, in: Proceedings of theInternational Conference on Systems, Man, and Cybernetics, San Diego, CA, October 1998, pp. 914–919.[41] K. Kask, R. Dechter, Stochastic local search for Bayesian networks, in: Proceedings Seventh International Workshop on Artificial Intelligenceand Statistics, Fort Lauderdale, FL, Morgan Kaufmann, January 1999.[42] U. Kjaerulff, Optimal decomposition of probabilistic networks by simulated annealing, Statistics and Computing 2 (1992) 7–17.[43] I. Kononenko, Inductive and Bayesian learning in medical diagnosis, Applied Artificial Intelligence 7 (1993) 317–337.[44] A.M.C.A. Koster, H.L. Bodlaender, S.P.M. van Hoesel, Treewidth: Computational experiments, in: H. Broersma, U. Faigle, J. Hurink, S. Pickl(Eds.), Electronic Notes in Discrete Mathematics, vol. 8, Elsevier Science Publishers, Amsterdam, 2001.[45] H. Langseth, L. Portinale, Bayesian networks in reliability, Reliability Engineering and System Safety 92 (1) (2007) 92–108.[46] S. Lauritzen, D.J. Spiegelhalter, Local computations with probabilities on graphical structures and their application to expert systems (withdiscussion), Journal of the Royal Statistical Society Series B 50 (2) (1988) 157–224.[47] Z. Li, B. D’Ambrosio, Efficient inference in Bayes nets as a combinatorial optimization problem, International Journal of ApproximateReasoning 11 (1) (1994) 55–81.[48] M.G. Luby, M. Mitzenmacher, M.A. Shokrollahi, D.A. Spielman, Improved low-density parity-check codes using irregular graphs and beliefpropagation, in: International Symposium on Information Theory, Cambridge, MA, August 1998.[49] D.J.C. MacKay, Information Theory, Inference and Learning Algorithms, Cambridge University Press, Cambridge, UK, 2002.[50] A. Madsen, Computing MPEs in Hugin. Personal communication, April 2003.[51] O.J. Mengshoel, Efficient Bayesian network inference: Genetic algorithms, stochastic local search, and abstraction, PhD thesis, Departmentof Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, April 1999.[52] O.J. Mengshoel, D. Roth, D.C. Wilkins, Hard and easy Bayesian networks for computing the most probable explanation, Technical ReportUIUCDCS-R-2000-2147, Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, January 2000.[53] O.J. Mengshoel, D. Roth, D.C. Wilkins, Stochastic greedy search: Computing the most probable explanation in Bayesian networks, TechnicalReport UIUCDCS-R-2000-2150, Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, February 2000.[54] O.J. Mengshoel, D.C. Wilkins, Raven: Bayesian networks for human-computer intelligent interaction, in: M.S. Vassiliou, T.S. Huang (Eds.),Computer Science Handbook for Displays, Rockwell Scientific Company, 2001, pp. 209–219.[55] D. Mitchell, B. Selman, H.J. Levesque, Hard and easy distributions of SAT problems, in: Proceedings of the Tenth National Conference onArtificial Intelligence, San Jose, CA, 1992, pp. 459–465.[56] J.D. Park, A. Darwiche, Approximating MAP using local search, in: Proceedings of the Seventeenth Conference on Uncertainty in ArtificialIntelligence (UAI-01), Seattle, WA, 2001, pp. 403–410.[57] J. Pearl, A constraint-propagation approach to probabilistic reasoning, in: L.N. Kanal, J.F. Lemmer (Eds.), Uncertainty in Artificial Intelli-gence, Elsevier, Amsterdam, 1986, pp. 357–369.[58] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, San Mateo, CA, 1988.[59] N. Robertson, P. Seymour, Graph minors. ii. Algorithmic aspects of treewidth, Journal of Algorithms 7 (1986) 309–322.[60] D. Roth, On the hardness of approximate reasoning, Artificial Intelligence 82 (1996) 273–302.[61] C.C. Ruokangas, O.J. Mengshoel, Information filtering using Bayesian networks: effective user interfaces for aviation weather data, in: Pro-ceedings of the 2003 International Conference on Intelligent User Interfaces, Miami, FL, 2003, pp. 280–283.[62] B. Selman, H.A. Kautz, B. Cohen, Noise strategies for improving local search, in: Proceedings of the Twelfth National Conference on ArtificialIntelligence, Seattle, WA, 1994, pp. 337–343.[63] B. Selman, H. Levesque, D. Mitchell, A new method for solving hard satisfiability problems, in: Proceedings of the Tenth National Conferenceon Artificial Intelligence, San Jose, CA, July 1992, pp. 440–446.[64] R.D. Shachter, S.K. Andersen, P. Szolovits. Global conditioning for probabilistic inference in belief networks, in: Proceedings of the TenthAnnual Conference on Uncertainty in Artificial Intelligence (UAI-94), Seattle, WA, 1994, pp. 514–522.[65] P.P. Shenoy, A valuation-based language for expert systems, International Journal of Approximate Reasoning 5 (3) (1989) 383–411.[66] E. Shimony, Finding MAPs for belief networks is NP-hard, Artificial Intelligence 68 (1994) 399–410.[67] M.A. Shwe, B. Middleton, D.E. Heckerman, M. Henrion, E.J. Horvitz, H.P. Lehmann, G.F. Cooper, Probabilistic diagnosis using a re-formulation of the INTERNIST-1/QMR knowledge base: I. The probabilistic model and inference algorithms, Methods of Information inMedicine 30 (4) (1991) 241–255.[68] C. Skaaning Jensen, A. Kong, Blocking Gibbs sampling for linkage analysis in large pedigrees with many loops, Research Report R-96-2048,Department of Computer Science, Aalborg University, Denmark, 1996.[69] H.J. Suermondt, G.F. Cooper, Probabilistic inference in multiply connected belief networks using loop cutsets, International Journal of Ap-proximate Reasoning 4 (1990) 283–306.1174O.J. Mengshoel et al. / Artificial Intelligence 170 (2006) 1137–1174[70] R.L. Welch, Real time estimation of Bayesian networks, in: Proceedings of the Twelfth Annual Conference on Uncertainty in ArtificialIntelligence (UAI-96), Portland, OR, 1996, pp. 533–544.[71] M. Yokoo, Why adding more constraints makes a problem easier for hill-climbing algorithms: Analyzing landscapes of CSPs, in: Proceedingsof the Third International Conference on Principles and Practice of Constraint Programming, in: Lecture Notes in Computer Science, vol. 1330,Springer, Berlin, 1997, pp. 357–370.[72] N.L. Zhang, D. Poole, Exploiting causal independence in Bayesian network inference, Journal of Artificial Intelligence Research 5 (1996)301–328.