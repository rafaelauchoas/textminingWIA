Artificial Intelligence 175 (2011) 1951–1983Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintItemset mining: A constraint programming perspectiveTias Guns∗, Siegfried Nijssen, Luc De RaedtKatholieke Universiteit Leuven, Celestijnenlaan 200A, 3001 Leuven, Belgiuma r t i c l ei n f oa b s t r a c tArticle history:Received 31 May 2010Received in revised form 5 May 2011Accepted 6 May 2011Available online 11 May 2011Keywords:Data miningItemset miningConstraint programmingThe field of data mining has become accustomed to specifying constraints on patternsof interest. A large number of systems and techniques has been developed for solvingsuch constraint-based mining problems, especially for mining itemsets. The approachtaken in the field of data mining contrasts with the constraint programming principlesdeveloped within the artificial intelligence community. While most data mining researchfocuses on algorithmic issues and aims at developing highly optimized and scalableimplementations that are tailored towards specific tasks, constraint programming employsa more declarative approach. The emphasis lies on developing high-level modelinglanguages and general solvers that specify what the problem is, rather than outlining howa solution should be computed, yet are powerful enough to be used across a wide varietyof applications and application domains.This paper contributes a declarative constraint programming approach to data mining.More specifically, we show that it is possible to employ off-the-shelf constraint program-ming techniques for modeling and solving a wide variety of constraint-based itemsetmining tasks, such as frequent, closed, discriminative, and cost-based itemset mining.In particular, we develop a basic constraint programming model for specifying frequentitemsets and show that this model can easily be extended to realize the other settings. Thiscontrasts with typical procedural data mining systems where the underlying proceduresneed to be modified in order to accommodate new types of constraint, or novelcombinations thereof. Even though the performance of state-of-the-art data miningsystems outperforms that of the constraint programming approach on some standard tasks,we also show that there exist problems where the constraint programming approach leadsto significant performance improvements over state-of-the-art methods in data mining andas well as to new insights into the underlying data mining problems. Many such insightscan be obtained by relating the underlying search algorithms of data mining and constraintprogramming systems to one another. We discuss a number of interesting new researchquestions and challenges raised by the declarative constraint programming approach todata mining.© 2011 Elsevier B.V. All rights reserved.1. IntroductionItemset mining is probably the best studied problem in the data mining literature. Originally applied in a supermarketsetting, it involved finding frequent itemsets, that is, sets of items that are frequently bought together in transactions ofcustomers [1]. The introduction of a wide variety of other constraints and a range of algorithms for solving these constraint-based itemset mining problems [33,5,41,42,11,31,50,9] has enabled the application of itemset mining to numerous other* Corresponding author. Tel.: +32 16 32 75 67; fax: +32 16 32 79 96.E-mail address: tias.guns@cs.kuleuven.be (T. Guns).0004-3702/$ – see front matter © 2011 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2011.05.0021952T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983problems, ranging from web mining to bioinformatics [31]; for instance, whereas early itemset mining algorithms focusedon finding itemsets in unsupervised, sparse data, nowadays closed itemset mining algorithms enable the application ofitemset mining on dense data [40,43], while discriminative itemset mining algorithms allow for their application on su-pervised data [35,13]. This progress has resulted in many effective and scalable itemset mining systems and algorithms,usually optimized to specific tasks and constraints. This procedural and algorithmic focus can make it non-trivial to extendsuch systems to accommodate new constraints or combinations thereof. The need to allow user-specified combinations ofconstraints is recognized in the data mining community, as witnessed by the development of a theoretical framework basedon (anti-)monotonicity [33,41,11] and systems such as ConQueSt [9], MusicDFS [50] and Molfea [18]. These systems supporta predefined number of (anti-)monotonicity based constraints, making them well suited for a number of typical data miningtasks.These approaches contrast with those of constraint programming. Constraint programming is a general declarativemethodology for solving constraint satisfaction problems, meaning that constraint programs specify what the problem is,rather than outline how the solution should be computed; it does not focus on a particular application. Constraint program-ming systems provide declarative modeling languages in which many types of constraints can be expressed and combined;they often support a much wider range of constraints than more specialized systems such as satisfiability (SAT) and integerlinear programming (ILP) solvers [10]. To realize this, the model is separated as much as possible from the solver. In the pasttwo decades, constraint programming has developed expressive high-level modeling languages as well as solvers that arepowerful enough to be used across a wide variety of applications and domains such as scheduling and planning [45].The question that arises in this context is whether these constraint programming principles can also be applied to itemsetmining. As compared to the more traditional constraint-based mining approach, this approach would specify data miningmodels using general and declarative constraint satisfaction primitives, instead of specialized primitives; this should makeit easy to incorporate new constraints and combinations thereof as – in principle – only the model needs to be extended tospecify the problem and general purpose solvers can be used for computing solutions.The contribution of this article is that we answer the above question positively by showing that the general, off-the-shelfconstraint programming methodology can indeed be applied to the specific problems of constraint-based itemset mining.1We show how a wide variety of itemset mining problems (such as frequent, closed and cost-based) can be modeled in aconstraint programming language and that general purpose out-of-the-box constraint programming systems can effectivelydeal with these problems.While frequent, closed and cost-based itemset mining are ideal cases, for which the existing constraint programmingmodeling language used suffices to tackle the problems, this cannot be expected in all cases. Indeed, in our formulation ofdiscriminative itemset mining, we introduce a novel primitive by means of a global constraint. This is common practice inconstraint programming, and the identification and study of global constraints that can effectively solve specific subproblemshas become a branch of research on its own [6]. Here, we have exploited the ability of constraint programming to serveas an integration platform, allowing for the free combination of new primitives with existing ones. This property allowsto find closed discriminative itemsets effectively, as well as discriminative patterns adhering to any other constraint(s).Furthermore, casting the problem within a constraint programming setting also provides us with new insights in howto solve discriminative pattern mining problems that lead to important performance improvements over state-of-the-artdiscriminative data mining systems.A final contribution is that we compare the resulting declarative constraint programming framework to well-knownstate-of-the-art algorithms in data mining. It should be realized that any such comparison is difficult to perform; thisalready holds when comparing different data mining (resp. constraint programming) systems to one another. In our com-parison we focus on high-level concepts rather than on specific implementation issues. Nevertheless, we demonstrate thefeasibility of our approach using our CP4IM implementation that employs the state-of-the-art constraint programming li-brary Gecode [47], which was developed for solving general constraint satisfaction problems. While our analysis revealssome weaknesses when applying this particular library to some itemset mining problem, it also reveals that Gecode canalready outperform state-of-the-art data mining systems on some tasks. Although outside the scope of the present paper, itis an interesting topic of ongoing research [37] to optimize constraint programming systems for use in data mining.The article is organized as follows. Section 2 provides an introduction to the main principles of constraint programming.Section 3 introduces the basic problem of frequent itemset mining and discusses how this problem can be addressed us-ing constraint programming techniques. The following sections then show how alternative itemset mining constraints andproblems can be dealt with using constraint programming: Section 4 studies closed itemset mining, Section 5 considersdiscriminative itemset mining, and Section 6 shows that the typical monotonicity-based problems studied in the literaturecan also be addressed in the constraint programming framework. We also study in these sections how the search of theconstraint programming approach compares to that of the more specialized approaches. The CP4IM approach is then evalu-ated in Section 7, which provides an overview of the choices made when modeling frequent itemset mining in a concreteconstraint programming system and compares the performance of this constraint programming system to specialized datamining systems. Finally, Section 8 concludes.1 We studied this problem in two conference papers [16,38] and brought it to the attention of the AI community [17]. This article extends these earlierpapers with proofs, experiments and comprehensive comparisons with related work in the literature.T. Guns et al. / Artificial Intelligence 175 (2011) 1951–198319532. Constraint programmingIn this section we provide a brief summary of the most common approach to constraint programming. More details canbe found in text books [45,3]; we focus on high-level principles and omit implementation issues.Constraint programming (CP) is a declarative programming paradigm: the user specifies a problem in terms of its con-straints, and the system is responsible for finding solutions that adhere to the constraints. The class of problems thatconstraint programming systems focus on are constraint satisfaction problems.Definition 1 (Constraint Satisfaction Problem (CSP)). A CSP P = (V, D, C) is specified by• a finite set of variables V ;• an initial domain D, which maps every variable v ∈ V to a set of possible values D(v);• a finite set of constraints C.(cid:5)A variable x ∈ V is called fixed if |D(x)| = 1; a domain D is fixed if all its variables are fixed, ∀x ∈ V : |D(x)| = 1. A domain D(cid:5)(x) ⊆ D(x) for all x ∈ V ; a domain is false if there exists an x ∈ V such that D(x) = ∅.is called stronger than domain D if DA constraint C(x1, . . . , xk) ∈ C is an arbitrary boolean function on variables {x1, . . . , xk} ⊆ V .A solution to a CSP is a fixed domain Dstronger than the initial domain D that satisfies all constraints. Abusing notation(cid:5)for a fixed domain, we must have that ∀C(x1, . . . , xk) ∈ C: C(D(cid:5)(x1), . . . , D(cid:5)(xk)) = true.A distinguishing feature of CP is that it does not focus on a specific set of constraint types. Instead it provides generalprinciples for solving problems with any type of variable or constraint. This sets it apart from satisfiability (SAT) solving,which focuses mainly on boolean formulas, and from integer linear programming (ILP), which focuses on linear constraintson integer variables.Example 1. Assume we have four people that we want to allocate to two offices, and that every person has a list of otherpeople that he does not want to share an office with. Furthermore, every person has identified rooms he does not wantto occupy. We can represent an instance of this problem with four variables which represent the persons, and inequalityconstraints which encode the room-sharing constraints:D(x1) = D(x2) = D(x3) = D(x4) = {1, 2},C = {x1 (cid:8)= 2, x1 (cid:8)= x2, x3 (cid:8)= x4}.The simplest algorithm to solve CSPs enumerates all possible fixed domains, and evaluates all constraints on each ofthese domains; clearly this approach is inefficient. Most CP systems perform a more intelligent type of depth-first search, asgiven in Algorithm 1 [47,45]:returnAlgorithm 1 Constraint-Search(D)1: D := propagate(D)2: if D is a false domain then3:4: end if5: if ∃x ∈ V : |D(x)| > 1 then6:7:8:9:10: else11:12: end ifx := arg minx∈V,D(x)>1 f (x)for all d ∈ D(x) doOutput solutionend forConstraint-Search(D ∪ {x (cid:11)→ {d}})Other search strategies have been investigated as well [53,44], but we restrict ourselves here to the most common case.In each node of the search tree the algorithm branches by restricting the domain of one of the variables not yet fixed (line 7in Algorithm 1). It backtracks when a violation of a constraint is found (line 2). The search is further optimized by carefullychoosing the variable that is fixed next (line 6); here a function f (x) ranks variables, for instance, by determining whichvariable is involved in the highest number of constraints.The main concept used to speed up the search is constraint propagation (line 1). Propagation reduces the domains ofvariables such that the domain remains locally consistent. One can define many types of local consistencies, such as nodeconsistency, arc consistency and path consistency; see [45]. In general, in a locally consistent domain, a value d does not(cid:5)(x) = {d}. The mainoccur in the domain of a variable x if it can be determined that there is no solution Din which D(cid:5)1954T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983motivation for maintaining local consistencies is to ensure that the backtracking search does not unnecessarily branch oversuch values, thereby significantly speeding up the search.To maintain local consistencies, propagators or propagation rules are used. Each constraint is implemented by a propaga-tor. Such a propagator is activated when the domain of one of the variables of the constraint changes. A propagator takesthe domain as input and outputs a failed domain in case the constraint can no longer be satisfied, i.e. if there exists no(cid:3)(xk)stronger than D with C(x1), . . . , D= true.fixed D(1)D(cid:2)(cid:5)(cid:5)(cid:5)When possible, the propagator will remove values from the domain that can never satisfy the constraint, giving as outputa stronger, locally consistent domain. More formally, a value c should be removed from the domain of a variable ˜x if thereis no(cid:5)fixed Dstronger than D with D(cid:5)(˜x) = c and C(cid:2)(cid:5)D(x1), . . . , D(cid:5)(cid:3)(xk)= true.(2)This is referred to as propagation; propagation ensures domain consistency. The repeated application of propagators can leadto increasingly stronger domains. Propagators are repeatedly applied until a fixed point is reached in which the domain doesnot change any more.Consider the constraint x1 (cid:8)= x2, the corresponding propagator is given in Algorithm 2:D(x2) = D(x2) \ D(x1)Algorithm 2 Conceptual propagator for x1 (cid:8)= x21: if |D(x1)| = 1 then2:3: end if4: if |D(x2)| = 1 then5:6: end ifD(x1) = D(x1) \ D(x2)The propagator can only propagate when x1 or x2 is fixed (lines 1 and 4). If one of them is, its value is removed from thedomain of the other variable (lines 2 and 5). In this propagator there is no need to explicitly check whether the constraintis violated, as a violation results in an empty and thus false domain in line 2.Example 2 (Example 1 continued). The initial domain of this problem is not consistent: the constraint x1 (cid:8)= 2 cannot besatisfied when D(x1) = {2} so value 2 is removed from the domain of x1. Subsequently, the propagator for the constraintx1 (cid:8)= x2 is activated, which removes value 1 from D(x2). At this point, we obtain a fixed point with D(x1) = {1}, D(x2) = {2},D(x3) = D(x4) = {1, 2}. Persons 1 and 2 have now each been allocated to an office, and two rooms are possible for persons 3and 4. The search branches over x3 and for each branch, constraint x3 (cid:8)= x4 is propagated; a fixed point is then reached inwhich every variable is fixed, and a solution is found.In the above example for every variable its entire domain D(x) is maintained. In constraint programming many types ofconsistency and algorithms for maintaining consistency have been studied. A popular type of consistency is bound con-sistency. In this case, for each variable only a lower- and an upper-bound on the values in its domain is maintained.A propagator will try to narrow the domain of a variable to that range of values for which it still believes a solutioncan be found, but does not maintain consistency for individual values. To formulate itemset mining problems as constraintprogramming models, we mostly use variables with binary domains, i.e. D(x) = {0, 1} with x ∈ V . For such variables thereis no difference between bound and domain consistency.Furthermore, we make extensive use of two types of constraints over boolean variables, namely the summation constraint,Eq. (3), and reified summation constraint, Eq. (6), which are introduced below.2.1. Summation constraintGiven a set of variables V ⊆ V and weights w x for each variable x ∈ V , the general form of the summation constraint is:(cid:4)x∈Vw xx (cid:2) θ.(3)The first task of the propagator is to discover as early as possible whether the constraint is violated. To this aim, thepropagator needs to determine whether the upper-bound of the sum is still above the required threshold; filling in theconstraint of Eq. (1), this means we need to check whether:(cid:5)(cid:4)(cid:6)(4)maxfixed D(cid:5) stronger than Dw x Dx∈V(cid:5)(x)(cid:2) θ.T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831955(cid:5)(cid:4)(cid:6)A more intelligent method to evaluate this property works as follows. We denote the maximum value of a variable x byxmax = maxd∈D(x) d, and the minimum value by xmin = mind∈D(x) d. Denoting the set of variables with a positive, respectively+ = {x ∈ V | w x (cid:2) 0} and Vnegative, weight by V(cid:6)− = {x ∈ V | w x < 0}, the bounds of the sum are now defined as:(cid:4)(cid:5)(cid:4)(cid:5)(cid:4)(cid:4)(cid:4)(cid:4)(cid:6)maxw xx=w xxmax +w xxmin,x∈Vx∈V +x∈V −minw xxx∈V(cid:7)=w xxmin +w xxmax.x∈V +x∈V −x∈V w xx (cid:2) θ can still be satisfied.These bounds allow one to determine whether an inequality constraintThe second task of the propagator is to maintain the bounds of the variables in the constraint, which in this case are thevariables in V . In general, for every variable ˜x ∈ V , we need to update ˜xmin to the lowest value c for which there exists adomain Dwith(cid:5)(cid:5)fixed Dstronger than D, D(cid:5)(˜x) = candw x D(cid:5)(x)(cid:2) θ.(5)x∈VAlso this can be computed efficiently; essentially, for binary variables x ∈ V we can update all domains as follows:• D(x) ← D(x) \ {0} if w x ∈ V• D(x) ← D(x) \ {1} if w x ∈ V+−and θ (cid:3) max(and θ (cid:3) max((cid:7)(cid:7)x∈V w xx) < θ + w x;x∈V w xx) < θ − w x.Example 3. Let us illustrate the propagation of the summation constraint. GivenD(x1) = {1},2 ∗ x1 + 4 ∗ x2 + 8 ∗ x3 (cid:2) 3;D(x2) = D(x3) = {0, 1},we know that at least one of x2 and x3 must have value 1, but we cannot conclude that either one of these variables iscertainly zero or one. The propagator does not change any domains. On the other hand, givenD(x1) = {1},2 ∗ x1 + 4 ∗ x2 + 8 ∗ x3 (cid:2) 7;D(x2) = D(x3) = {0, 1},the propagator determines that the constraint can never be satisfied if x3 is false, so D(x3) = {1}.2.2. Reified summation constraintThe second type of constraints we will use extensively is the reified summation constraint. Reified constraints are acommon construct in constraint programming [52,54]. Essentially, a reified constraint binds the truth value of a constraintCto a binary variable b:(cid:5)b ↔ C(cid:5).(cid:5)In principle, Cof a reified summation constraint:can be any boolean constraint. In this article, C will usually be a constraint on a sum. In this case we speakb ↔(cid:4)x∈Vw xx (cid:2) θ.(6)This constraint states that b is true if and only if the weighted sum of the variables in V is higher than θ . The mostimportant constraint propagation that occurs for this constraint is the one that updates the domain of variable b. Essentially,the domain of this variable is updated as follows:• D(b) ← D(b) \ {1} if max(• D(b) ← D(b) \ {0} if min(x∈V w xx) < θ ;x∈V w xx) (cid:2) θ .(cid:7)(cid:7)In addition, in some constraint programming systems, constraint propagators can also simplify constraints. In this case, ifD(b) = {1}, the reified constraint can be simplified to the constraintx∈V w xx (cid:2) θ ; if D(b) = {0}, the simplified constraintbecomes(cid:7)(cid:7)x∈V w xx < θ .Many different constraint programming systems exist. They differ in the types of variables they support, the constraintsthey implement, the way backtracking is handled and the data structures that are used to store constraints and propagators.Furthermore, in some systems constraints are specified in logic (for instance, in the constraint logic programming systemECLiPSe [3]), while in others the declarative primitives are embedded in an imperative programming language. An exampleof the latter is Gecode [47], which we use in the experimental section of this article.1956T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983TidT 1T 2T 3T 4T 5T 6T 7T 8T 9T 10Itemset{B}{E}{A,C}{A,E}{B,C}{D,E}{C,D,E}{A,B,C}{A,B,E}{A,B,C,E}Tid12345678910A0011000111B1000100111C0010101101D0000011000E0101011011Fig. 1. A small example of an itemset database, in multiset notation (left) and in binary matrix notation (right).3. Frequent itemset miningNow that we have introduced the key concepts underlying constraint programming (CP), we study various itemset miningproblems within this framework. We start with frequent itemset mining in the present section, and then discuss closed,discriminative and cost-based itemset mining in the following sections. For every problem, we provide a formal definition,then introduce a constraint programming model that shows how the itemset mining problem can be formalized as a CPproblem, and then compare the search strategy obtained by the constraint programming approach to that of existing itemsetmining algorithms.We start with the problem of frequent itemset mining and we formulate two CP models for this case. The differencebetween the initial model and the improved one is that the later uses the notion of reified constraints, which yields betterpropagation as shown by an analysis of the resulting search strategies.3.1. Problem definitionThe problem of frequent itemset mining was proposed in 1993 by Agrawal et al. [1]. Given is a database with a set oftransactions.2 Let I = {1, . . . , m} be a set of items and A = {1, . . . , n} be a set of transaction identifiers. An itemset databaseD is as a binary matrix of size n × m with Dti ∈ {0, 1}, or, equivalently, a multi-set of itemsets I ⊆ I, such that(cid:8)D(cid:5) =(t, I)(cid:9)(cid:9) t ∈ A, I ⊆ I, ∀i ∈ I: Dti = 1(cid:10).A small example of an itemset database is given in Fig. 1, where for convenience every item is represented as a letter.There are many databases that can be converted into an itemset database. The traditional example is a supermarketdatabase, in which each transaction corresponds to a customer and every item in the transaction to a product bought bythe customer. Attribute–value tables can be converted into an itemset database as well. For categorical data, every attribute–value pair corresponds to an item and every row is converted into a transaction.The coverage ϕD(I) of an itemset I consists of all transactions in which the itemset occurs:ϕD(I) = {t ∈ A | ∀i ∈ I: Dti = 1}.The support of an itemset I , which is denoted as supportD(I), is the size of the coverage:supportD(I) =(cid:9)(cid:9)(cid:9).(cid:9)ϕD(I)In the example database we have ϕD({D, E}) = {T 6, T 7} and supportD({D, E}) = |{T 6, T 7}| = 2.Definition 2 (Frequent itemset mining). Given an itemset database D and a threshold θ , the frequent itemset mining problemconsists of computing the set(cid:8)I(cid:9)(cid:9) I ⊆ I, supportD(I) (cid:2) θ(cid:10).The threshold θ is called the minimum support threshold. An itemset with supportD(I) (cid:2) θ is called a frequent itemset.Note that we are interested in finding all itemsets satisfying the frequency constraint.The subset relation between itemsets defines a partial order. This is illustrated in Fig. 2 for the example database ofFig. 1; the frequent itemsets are visualized in a Hasse diagram: a line is drawn between two itemsets I1 and I2 iff I1 ⊂ I2and |I2| = |I1| + 1.By changing the support threshold, an analyst can influence the number of patterns that is returned by the data miningsystem: the lower the support threshold, the larger the number of frequent patterns.2 Itemset mining was first applied in a supermarket setting; the terminology still reflects this.T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831957Fig. 2. A visualization of the search space for the database of Fig. 1; frequent itemsets for θ = 2 are highlighted. Frequent closed itemsets are highlightedblack; non-closed frequent itemsets are grey.3.2. Initial constraint programming modelOur model of the frequent itemset mining problem in constraint programming is based on the observation that we canformalize the frequent itemset mining problem also as finding the set:(cid:8)(I, T )(cid:9)(cid:9) I ⊆ I, T ⊆ A, T = ϕD(I), |T | (cid:2) θ(cid:10).Here we make the set of transactions T = ϕD(I) explicit. This yields the same solutions as the original problem because theset of transactions T is completely determined by the itemset I . We will refer to T = ϕD(I) as the coverage constraint while|T | (cid:2) θ expresses a support constraint.To model this formalization in CP, we need to represent the set of items I and the set of transactions T . In our modelwe use a boolean variable I i for every individual item i; furthermore we use a boolean variable Tt for every transaction t.An itemset I is represented by setting I i = 1 for all i ∈ I and I i = 0 for all i /∈ I . The variables Tt represent the transactionsthat are covered by the itemset, i.e. T = ϕ(I); Tt = 1 iff t ∈ ϕ(I). One assignment of values to all I i and Tt corresponds toone itemset and its corresponding transaction set.We now show that the coverage constraint can be formulated as follows.Property 1 (Coverage constraint). Given a database D, an itemset I and a transaction set T , then(cid:5)(cid:6)T = ϕD(I) ⇐⇒∀t ∈ A: Tt = 1 ↔I i(1 − Dti) = 0,(cid:4)i∈Ior equivalently,(cid:5)T = ϕD(I) ⇐⇒∀t ∈ A: Tt = 1 ↔(cid:6)Dti = 1 ∨ I i = 0,(cid:11)i∈Iwhere I i, Tt ∈ {0, 1} and I i = 1 iff i ∈ I and Tt = 1 iff t ∈ T .(7)(8)Proof. Essentially, the constraint states that for one transaction t, all items i should either be included in the transaction(Dti = 1) or not be included in the itemset (I i = 0):T = ϕD(I) = {t ∈ A | ∀i ∈ I: Dti = 1}⇐⇒ ∀t ∈ A:⇐⇒ ∀t ∈ A:Tt = 1 ↔ ∀i ∈ I: Dti = 1Tt = 1 ↔ ∀i ∈ I: 1 − Dti = 01958T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983⇐⇒ ∀t ∈ A:Tt = 1 ↔(cid:4)i∈II i(1 − Dti) = 0.The representation as a clause in Eq. (8) follows from this. (cid:2)It is quite common in constraint programming to encounter different ways to model the same problem or even the sameconceptual constraint, as above. How propagation is implemented for these constraints can change from solver to solver. Forexample, watched literals could be used for the clause constraint, leading to different runtime and memory characteristicscompared to a setting where no watched literals are used. We defer the study of such characteristics to Section 7.Under the coverage constraint, a transaction variable will only be true if the corresponding transaction covers the itemset.Counting the frequency of the itemset can now be achieved by counting the number of transactions for which Tt = 1.Property 2 (Frequency constraint). Given a database D, a transaction set T and a threshold θ , then|T | (cid:2) θ ⇐⇒(cid:4)t∈ATt (cid:2) θ,where Tt ∈ {0, 1} and Tt = 1 iff t ∈ T .(9)We can now model the frequent itemset mining problem as a combination of the coverage constraint (7) and the fre-quency constraint (9). To illustrate this, we provide an example of our model in the Essence’ language in Algorithm 3:Algorithm 3 Fim_cp’s frequent itemset mining model, in Essence’1: given NrT, NrI : int2: given TDB : matrix indexed by [int(1..NrT),int(1..NrI)] of int(0..1)3: given Freq : int4: find Items : matrix indexed by [int(1..NrI)] of bool5: find Trans : matrix indexed by [int(1..NrT)] of bool6: such that7: $ Coverage Constraint (Eq. (7))8: forall t: int(1..NrT).9:Trans[t] <=> ((sum i: int(1..NrI). (1-TDB[t,i])∗Items[i]) <= 0),10: $ Frequency Constraint (Eq. (9))11: (sum t: int(1..NrT). Trans[t]) >= Freq.Essence’ is a solver-independent modeling language; it was developed to support intuitive modeling, abstracting away fromthe underlying solver technology [22].We will now study how a constraint programming solver will search for the solutions given the above model. A firstobservation is that the set of transactions is completely determined by the itemset, so we need only to search over the itemvariables.When an item variable is set (D(I i) = {1}) by the search, only the constraints that contain this item will be activated.In other words, the frequency constraint will not be activated, but every coverage constraint that contains this item will be.A coverage constraint is a reified summation constraint, for which the propagator was explained in Section 2. In summary,when an item variable is set, the following propagation is possible for the coverage constraint:• if for some t:• if for some t:(cid:7)(cid:7)i∈I (1 − Dti) ∗ I mini∈I (1 − Dti) ∗ I maxi > 0 then remove 1 from D(Tt );= 0 then remove 0 from D(Tt ).iOnce the domain of a variable Tt is changed, the support constraint will be activated. The support constraint is a summationconstraint, which will check whether:(cid:4)t∈AT maxt(cid:2) θ.If this constraint fails, we do not need to branch further and we can backtrack.Example 4. Fig. 3(a) shows part of a search tree for a small example with a minimum frequency threshold of 2. Essentially,the search first tries to add an item to an itemset and after backtracking it will only consider itemsets not including it. Aftera search step (indicated in green), the propagators are activated. The coverage propagators can set transactions to 0 or 1,while the frequency constraint can cause failure when the desired frequency can no longer be obtained (indicated by a redcross in the two left-most branches).T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831959Fig. 3. Search-propagation interaction of the non-reified frequent itemset model (left) and the reified frequent itemset model (right). A propagation step iscolored in blue, a search step in green. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of thisarticle.)1960T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983Observe that in the example we branched on item 2 first. This follows the generic ‘most constrained’ variable orderheuristic, which branches over the variable contained in most constraints first (remember that the coverage constraints areposted on items that have a 0 in the matrix). If item 1 would be branched over first, the search tree would be larger,as both branches would have to determine separately that I2 = 1 does not result in a frequent itemset. An experimentalinvestigation of different branching heuristics is done in Section 7.3.3. Improved modelInspired by observations in traditional itemset mining algorithms, we propose an alternative model that substantiallyreduces the size of the search tree by introducing fine-grained constraints. The main observation is that we can formulatethe frequency constraint on each item individually:Property 3 (Reified frequency constraint). Given a database D, an itemset I (cid:8)= ∅ and a transaction set T , such that T = ϕD(I), then|T | (cid:2) θ ⇐⇒ ∀i ∈ I : Ii = 1 →(cid:4)t∈ATt Dti (cid:2) θ,where I i, Tt ∈ {0, 1} and I i = 1 iff i ∈ I and Tt = 1 iff t ∈ T .Proof. We observe that we can rewrite ϕD(I) as follows:ϕD(I) = {t ∈ A | ∀i ∈ I: Dti = 1} =(cid:2)(cid:3).{i}ϕD(cid:12)i∈I(10)Using this observation, it follows that:(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)|T | (cid:2) θ ⇐⇒(cid:9)(cid:9)(cid:3)(cid:9)(cid:9)(cid:9)(cid:2) θ{ j}ϕD(cid:12)(cid:2)j∈I⇐⇒ ∀i ∈ I:⇐⇒ ∀i ∈ I:(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)ϕD(cid:9)(cid:9)ϕD(cid:2)(cid:2)(cid:3)(cid:3){i}{i}(cid:12)∩(cid:2){ j}ϕD(cid:9)(cid:9)(cid:3)(cid:9)(cid:9)(cid:9)(cid:2) θj∈I(cid:9)(cid:9) (cid:2) θ∩ T(cid:4)⇐⇒ ∀i ∈ I: Ii = 1 →Tt Dti (cid:2) θ.(cid:2)t∈AThe improved model consists of the coverage constraint (Eq. (7)) and the newly introduced reified frequency constraint(Eq. (10)). This model is equivalent to the original model, and also finds all frequent itemsets.The reified frequency constraint is posted on every item separately, resulting in more fine-grained search-propagationinteractions. Essentially, the reified frequency constraint performs a kind of look-ahead; for each item, a propagator willcheck whether that item is still frequent given the current itemset. If it is not, it will be removed from further consideration,as its inclusion would make the itemset infrequent. In summary, the main additional propagation allowed by the reifiedconstraint is the following:• if for some i:(cid:7)t∈A Dti ∗ T maxt < θ then remove 1 from D(I i), i.e. I i = 0.Example 5. Fig. 3 shows the search trees of both the original non-reified model as well as the improved model using thereified frequency constraint.In the original model (Fig. 3(a)), the search branches over I2 = 1, after which the propagation detects that this makesthe itemset infrequent and fails (left-most branch). In the reified model (Fig. 3(b)) the reified frequency propagator for I2detects that this item is infrequent. When evaluating the sum (0 ∗ T max), it is easy to see that themaximum is 1 < 2, leading to I2 = 0 (second level). The same situation occurs for I3 near the bottom of the figure. Thistime, the propagator takes into account that at this point T 3 = 0 and hence T max+ 1 ∗ T max+ 0 ∗ T max= 0.2313The reified propagations avoid creating branches that can only fail. In fact, using the reified model, the search becomesfailure-free: every branch will lead to a solution, namely a frequent itemset. This comes at the cost of a larger number ofpropagations. In Section 7 we experimentally investigate the difference in efficiency between the two formulations.T. Guns et al. / Artificial Intelligence 175 (2011) 1951–198319613.4. ComparisonLet us now study how the proposed CP-based approach compares to traditional itemset mining algorithms. In order tounderstand this relationship, let us first provide a short introduction to these traditional algorithms.The most important property exploited in traditional algorithms is anti-monotonicity.Definition 3 (Anti-monotonic constraints). Assume given two itemsets I1 and I2, and a predicate p(I, D) expressing a con-straint that itemset I should satisfy on database D. Then the constraint is anti-monotonic iff ∀I1 ⊆ I2 : p(I2, D) (cid:20)⇒ p(I1, D).Indeed, if an itemset I2 is frequent, any itemset I1 ⊆ I2 is also frequent, as it must be included in at least the sametransactions as I2. This property allows one to develop search algorithms that do not need to consider all possible itemsets.Essentially, no itemset I2 ⊃ I1 needs to be considered any more once it has been found that I1 is infrequent.Starting a search from the empty itemset, there are many ways in which one could traverse the search space, the mostimportant ones being breadth-first search and depth-first search. Initial algorithms for itemset mining were mostly breadth-first search algorithms, of which the Apriori algorithm is the main representative [2]. However, more recent algorithmsmostly use depth-first search. Given that most CP systems also perform depth-first search, the similarities between CP anddepth-first itemset mining algorithms are much larger than between CP and breadth-first mining algorithms. An outline ofa general depth-first frequent itemset mining algorithm is given in Algorithm 4. The main observations are the following:• if an item is infrequent in a database, we can remove the corresponding column from the database, as no itemset willcontain this item and hence the column is redundant;• once an item is added to an itemset, all transactions not containing this item become irrelevant for the search treebelow this itemset; hence we can remove the corresponding row from the database.The resulting database, which contains a smaller number of transactions having a smaller number of items, is often calleda projected database. Hence, every time that we add an item to an itemset, we determine which items and transactionsbecome irrelevant, and continue the search for the resulting database, which only contains frequent items and transactionscovered by the current itemset. Important benefits are that the search procedure will never try to add items once they havebeen found to be infrequent; transactions no longer relevant can similarly be ignored.Please note the following detail: in the projected database, we only include items which are strictly higher in orderthan the highest item currently in the itemset. The reason for this is that we wish to avoid that the same itemset isfound multiple times; for instance, we wish to find itemset {1, 2} only as a child of itemset {1} and not also as a child ofitemset {2}.Algorithm 4 Depth-First-Search (Itemset I , Database D)1: F := {I}2: determine a total order R on the items in D3: for all items i occurring in D do4:5:6:7: F := F ∪ Depth-First-Search(I ∪ {i}, Di );8: end for9: return Fcreate from D projected database Di , containing:- only transactions in D that contain i- only items in Di that are frequent and higher than i in chosen order RAn important choice in this general algorithm is how the projected databases are stored. A very large number of strate-gies have been explored, among which tid-lists and FP-trees [30]. Tid-lists are most relevant here, as they compare best tostrategies chosen in CP systems. Given an item i, its tid-list in a database D is ϕD({i}). We can store this list as a list ofintegers [56], a binary vector, or using variations of run-length encoding [49]. The projected database of a given itemset I isthus the set(cid:8)(cid:2)(cid:2)I ∪ {i}(cid:3)(cid:3) (cid:9)(cid:9)(cid:2)(cid:9)(cid:9)ϕD(cid:10)(cid:3)(cid:9)(cid:9) (cid:2) θ.I ∪ {i}i, ϕDThe interesting property of tid-lists is that they can easily be updated incrementally: if we wish to obtain a tid-listfor item j in the projected database of itemset {i}, this can be obtained by computing ϕD({i}) ∩ ϕD({ j}), where D is theoriginal database; for instance, in the case bit vectors are used this is a binary AND operation, which most CPUs evaluateefficiently. The most well-known algorithm using this approach is the Eclat algorithm [56].An example of a depth-first search tree is given in Fig. 4, using the same database as in Fig. 3; we represent the projecteddatabase using tid-lists. The order of the items is assumed to be the usual order between integers. In the initial projecteddatabase, item 2 does not occur as it is not frequent. Each child of the root corresponds to an itemset with one item.1962T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983Fig. 4. The search tree for depth-first frequent itemset miners, for the same example as in Fig. 3, where the items are ordered by the natural orderon integers. Each itemset has a corresponding projected database containing only frequent items higher than the items chosen so far. For instance,the projected database for itemset {4} is empty as items 1 and 3 are lower than 4; the database of {1} does not contain item i3 as {1, 3} is not fre-quent.3.4.1. Comparison with search using the CP modelWe now compare the above descriptions of itemset mining algorithms and constraint programming systems. Necessarilywe need to restrict this discussion to a comparison of high-level principles; a detailed comparison of both approaches isnot possible without studying the data structures of specific constraint programming systems in detail, which we considerto be out of the scope of this article; see [37] for a first attempt in that direction.We first consider the differences in the search trees when using our CP model as compared to traditional mining algo-rithms. These differences can be understood by comparing the trees in Figs. 3 and 4. In depth-first itemset mining, eachnode in the search tree corresponds to an itemset. Search proceeds by adding items to it; nodes in the search tree canhave an arbitrary number of children. In CP, each node in the search tree corresponds to a domain, which in our modelrepresents a choice for possible values of items and transactions. Search proceeds by restricting the domain of a variable.The resulting search tree is always binary, as every item is represented by a boolean variable that can either be true or false(include or exclude the item).We can identify the following relationship between nodes in the search tree of a CP system and nodes in the search treeof itemset miners. Denoting by D(I i) the domain of item variable I i in the state of the CP system, we can map each stateto an itemset as follows:(cid:9)(cid:9) D(I i) = {1}i ∈ I(cid:10).(cid:8)Essentially, in CP some branches in the search tree correspond to an assignment D(I i) = {0} for an item i (i.e. the item i isremoved from consideration). All nodes across a path of such branches are collapsed in one node of the search tree of theitemset miner, turning the binary tree in an n-ary tree.Even though it might seem that this different perception of the search tree leads to a higher memory use in CP systems,this is not necessarily the case. If the search tree is traversed in the order indicated in Fig. 3(b), once we have assignedvalue D(I1) = {0} and generated the corresponding child node, we no longer need to store the original domain D withD(I1) = {0, 1}. The reason is that there are no further children to generate for this original node in the search tree; if thesearch returns to this node, we can immediately backtrack further to its parent (if any). Hence, additional memory onlyneeds to be consumed by branches corresponding to D(I i) = {1} assignments. This implies that in practice the efficiencydepends on the implementation of the CP system; it does not depend on the theoretically different shape of the searchtree.In more detail these are the possible domains for the variables representing items during the search of the CP system:• D(I i) = {0, 1}: this represents an item that can still be added to the itemset, but that currently is not included; intraditional itemset mining algorithms, these are the items included in the projected database;• D(I i) = {0}: this represents an item that will not be added to the itemset. In the case of traditional itemset miningalgorithms, these are items that are neither part of the projected database nor part of the current itemset;• D(I i) = {1}: this represents an item that will be part of all itemsets deeper down the search tree; in the case oftraditional algorithms, this item is part of the itemset represented in the search tree node.T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831963Similarly, we have the transaction variables:• D(Tt ) = {0, 1}: this represents a transaction that is covered by the current itemset (since 1 is still part of its domain),but may still be removed from the coverage later on; in traditional algorithms, this transaction is part of the projecteddatabase;• D(Tt ) = {0}: this represents a transaction that is not covered by the current itemset; in traditional algorithms, thistransaction is not part of the projected database;• D(Tt ) = {1}: this represents a transaction that is covered by the current itemset and that will be covered by all itemsetsdeeper down the search tree, as the transaction contains all items that can still be added to the itemset; in traditionalitemset mining algorithms, this transaction is part of the projected database.A second difference is hence which information is available about transactions during the search. In our CP formalization, wedistinguish transactions with domain D(Tt ) = {0, 1} and D(Tt ) = {1}. Frequent itemset mining algorithms do not make thisdistinction. This difference allows one to determine when transactions are unavoidable: A transaction becomes unavoidable(D(Tt ) = {1}) if all remaining items (1 ∈ D(I i)) are included in it; the propagation to detect this occurs in branches whereitems are removed from consideration. Such branches are not present in the itemset mining algorithms; avoiding thispropagation could be important in the development of new constraint programming systems.Thirdly, to evaluate the constraints, CP systems store constraints or propagators during the search. Essentially, to everynode in the search tree a state is associated that reflects active constraints, propagators and variables. Such a state corre-sponds to the concept of a projected database in itemset mining algorithms. The data structures for storing and maintainingpropagators in CP systems and in itemset mining algorithms are however often very different. For example, in itemset min-ing efficient data representations such as tid-lists and fp-trees have been developed; CP systems use data structures forstoring both propagators and constraints, which may be redundant in this problem setting. For instance, while in depth-firstitemset mining, a popular approach is to store a tid-list in an integer array, CP systems may use both an array to storethe indexes of variables in a constraint, and use an array to store a list of constraints watching a variable. Resolving thesedifferences however requires a closer study of particular constraint programming systems, which is outside the scope of thispaper.Overall, this comparison shows that there are many high-level similarities between itemset mining and constraint pro-gramming systems, but that in many cases one can also expect lower-level differences. Our experiments will show that theselow-level differences can have a significant practical impact, and hence that an interesting direction for future research is tobridge the gap between these systems.4. Closed itemset miningEven though the frequency constraint can be used to limit the number of patterns, the constraint is often not restrictiveenough to find useful patterns. A high support threshold usually has as effect that only well-known itemsets are found; fora low threshold the number of patterns is often too large. To alleviate this problem, many additional types of constraintshave been introduced. In this and the following sections, we will study how three further representative types of constraintscan be formalized as constraint programming problems. Closed itemset mining aims at avoiding redundant itemsets, dis-criminative itemset mining wants to find itemsets that discriminate two classes of transactions, and cost-based constraintsare representative for a fairly general class of constraints in the monotonicity framework.4.1. Problem definitionCondensed representations aim at avoiding redundant itemsets, which are itemsets whose necessary presence in the fullsolution set may be derived from other itemsets found by the algorithm. The closedness constraint is a typical constraintthat is used to find such a condensed representation [40]. We now introduce the closedness constraint more formally.One way to interpret itemsets, is by seeing them as rectangles of ones in a binary matrix. For instance, in our exampledatabase of Fig. 1 on page 1956, for itemset {D} we have corresponding transactions {T 6, T 7}. The itemset {D} and thetransaction set {T 6, T 7} select a submatrix which can be seen as a rectangle in the matrix. Observe that due to the waythat we calculate the set of transactions from the set of items, we cannot add a transaction to the set of transactionswithout including a zero element in the rectangle. However, this is not the case for the columns. In this case, we haveϕ({D}) = ϕ({D, E}) = {T 6, T 7}; we can add item E and still obtain a rectangle containing only ones. Closed itemset miningcan be seen as the problem of finding maximal rectangles of ones in the matrix.An essential property of ‘maximal’ rectangles is that if we consider its transactions, we can derive the corresponding setof items: the largest itemset shared by all transactions must define all columns included in the rectangle. This leads us tothe following definition of closed itemset mining.Definition 4 (Frequent closed itemset mining). Given a database D, let ψD(T ) be defined asψD(T ) = {i ∈ I | ∀t ∈ T : Dti = 1}.1964T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983Given a threshold θ , the frequent closed itemsets are the itemsets in(cid:8)I(cid:9)(cid:9) I ⊆ I, supportD(I) (cid:2) θ, ψD(cid:2)(cid:3)ϕD(I)(cid:10).= IGiven an itemset I , the itemset ψD(ϕD(I)) is called the closure of I . Closed itemsets are those which equal their closure.If an itemset is not equal to its closure, this means that we can add an item to the itemset without changing its support.Closed itemsets for our example database are highlighted in black in Fig. 2.The idea behind closed itemsets has also been studied in other communities; closed itemset mining is in particularclosely related to the problem of finding formal concepts in formal contexts [24]. Essentially, formal concepts can be thoughtof as closed itemsets that are found without applying a support threshold. In formal concept analysis, the operators ϕand ψ are called Galois operators. These operators define a Galois connection between the partial orders for itemsets andtransaction sets, respectively.4.2. Constraint programming modelCompared to frequent itemset mining, the additional constraint that we need to express is the closedness constraint.We can deal with this constraint in a similar way as with the coverage constraint. Assuming that T represents the set oftransactions covered by an itemset I , the constraint that we need to check is the following:ψD(T ) = I,(11)as in this case ψD(ϕD(I)) = I . This leads to the following constraint in the CP model, which should be posted together withthe constraints in Eqs. (7) and (10).Property 4 (Closure constraint). Given a database D, an itemset I and a transaction set T , then(cid:5)(cid:6)I = ψD(T ) ⇐⇒∀i ∈ I: I i = 1 ↔Tt(1 − Dti) = 0,(12)(cid:4)t∈Awhere I i, Tt ∈ {0, 1} and I i = 1 iff i ∈ I and Tt = 1 iff t ∈ T .The proof is similar to the proof for the coverage constraint.4.3. ComparisonSeveral classes of algorithms have been proposed for closed itemset mining, each being either an extension of a breadth-first algorithm such as Apriori, or a depth-first algorithm, operating on tid-lists or fp-trees. We limit ourselves here todepth-first mining algorithms once more.Initial algorithms for mining closed itemsets were based on a repository in which closed itemsets were stored. The searchis performed by a depth-first frequent itemset miner which is modified as follows:• when it is found that all transactions in a projected database contain the same item, this item is immediately added tothe itemset as without it the itemset cannot be closed;• for each itemset I1 found in this way, it is checked in the repository whether an itemset I2 ⊇ I1 has been found earlierwhich has the same support; if not, the itemset is stored in the repository and the search continues; otherwise theclosed supersets, starting with I2, have already been found earlier as children of I2 so this branch of the search tree ispruned.The first modification only checks items that are in the projected database, which are by construction items i > max(I) thatare higher in the lexicographic order. The repository is needed to check whether there is no superset with an additional itemi < max(I); this is what the second modification does. With an appropriate search order only closed itemsets are stored [43].This procedure works well if the number of closed sets is small and the database is large. When the number of closeditemsets is large storing itemsets and searching in them can be costly. The LCM algorithm addresses this problem [51]. Inthis algorithm also for the items i < max(I) it is checked in the data whether they should be part of the closure, eventhough the depth-first search procedure does not recurse on such items.4.3.1. Constraint propagationThe additional constraint (12) for closed itemset mining is similar to the coverage constraint and hence its propagationis also similar. When all remaining transactions (i.e. those for which 1 ∈ D(Tt )) contain a certain item, the propagator will:• change the domain of the item i to D(I i) = {1} if 1 ∈ D(I i);• fail if 1 /∈ D(I i).T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831965TidT 1T 2T 3T 4T 5T 6T 7T 8T 9T 10Itemset{B}{E}{A, C}{A, E}{B, C}{D, E}{C, D, E}{A, B, C}{A, B, E}{A, B, C, E}Class−−−−+−−+−+Fig. 5. A small example of a class-labeled itemset database, in multiset notation.Hence, in this case we do not have failure-free search; if the closure constraint requires the inclusion of an item in theclosure that cannot be included, the search will backtrack.Overall this behavior is very similar to that of the LCM algorithm: essentially we are performing a backtracking searchwithout storing solutions, in which items in the closure are immediately added and some branches are pruned as they failto satisfy an order constraint. The main difference between LCM and the CP system is as in the previous section: other datastructures are used and the search tree is differently organized.5. Discriminative itemset miningItemset mining was initially motivated by the need to find rules, namely association rules. However, in the problemsettings discussed till now, no rules were found; instead we only found conditions and no consequents. In this section westudy the discovery of rules in a special type of transactional data, namely, data in which every transaction is labeled with a(binary) class label. The task is here to find itemsets that allow one to discriminate the transactions belonging to one classfrom those belonging to the other class. As it turns out, integrating this constraint efficiently in constraint programmingrequires the addition of a new primitive to the constraint programming system that we used till now. On the one hand thisshows the limits of the declarative approach presented till now; on the other hand, our results demonstrate the feasibilityof adding new data mining primitives as global constraints. Furthermore, as we will see, the application of the CP principlesin the development of a new constraint propagator turns out to be crucial in improving the performance of existing miningsystems.5.1. Problem definitionTo illustrate the problem of discriminative itemset mining, consider the database given in Fig. 5. We are interested infinding itemsets that discriminate transactions in different classes from one another. In the example database, for instance,the itemset {B, C} almost only occurs in the positive examples, and hence can be thought to discriminate positive transac-tions from negative ones, leading to the rule {B, C} → +.Whereas we will refer to this mining problem here as the problem of discriminative itemset mining [35,13,21], it is alsoknown under many other names, such as correlated itemset mining [48,39], interesting itemset mining [5], contrast setmining [4], emerging itemset mining [20] and subgroup discovery [55,32,28]. The problem is also highly related to that ofrule learning in machine learning [23]. The key difference is that rule learning in machine learning usually uses heuristictechniques, while in itemset mining typically exhaustive techniques are used to find the global optimum., and the part for which the target attribute is negative as D−Even though in the general case a target attribute may have multiple values, we will restrict ourselves to the casewhere the target attribute has two values: positive and negative. We refer to the part of the database for which the targetattribute is positive as D+. The set of transactions identifiersappearing in the corresponding parts is indicated by A+. We define the stamp point of an itemset I as σ (I) =(|supportD+ (I)|, |supportD− (I)|). Hence the stamp point of an itemset is a vector (p, n) where p is the frequency of thisitemset in D+. Given these numbers, we can compute a discrimination scoref (p, n). For itemsets, the stamp point σ (I) = (p, n) is used to calculate the score f (σ (I)), written f (I) in short. Examplesinclude χ 2, information gain, Gini index, Fisher score and others. For example χ 2 is a well-of discrimination measures fknown measure of correlation in statistics:and n is the frequency of this itemset in D−and A−χ 2(p, n) =+(p − (p+n)|D|(p+n)|D|· |D+|)2· |D+|(|D+| − p − |D|−(p+n)|D||D|−(p+n)· |D+||D|(n − (p+n)|D|(p+n)|D|· |D+|)2· |D−|)2· |D−|++(|D−| − n − |D|−(p+n)|D|−(p+n)· |D−||D||D|· |D−|)2(13)where it is assumed that 0/0 = 0. An illustration of this measure is given in Fig. 6. The domain of stamp points [0, |D+|] ×[0, |D−|] is often called PN-space.1966T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983Fig. 6. Left: Plot of the χ 2 measure and a threshold at χ 2 = 20. Right: isometric of the threshold in PN space.Essentially, we are interested in finding itemsets which are as close as possible to the maxima in one of the oppositecorners; the χ 2 measure scores higher the closer we are to these maxima.A discrimination measure can be used in a constraint in several ways. We will limit ourselves to the following two cases.Definition 5 (Discriminative itemset mining). Given a database D, a discrimination measure f and a parameter θ , the dis-criminative itemset mining problem is that of finding all itemsets in(cid:8)I(cid:9)(cid:9) I ⊆ I, f (I) (cid:2) θ(cid:10).Definition 6 (Top-k discriminative itemset mining). Given a database D, a discrimination measure f and a value k, the top-kdiscriminative itemset mining problem is the problem of finding the first k elements of the list [I1, I2, . . . , In] consisting ofall itemsets I ⊆ I downward sorted by their f (I) values.In other words, the set of k patterns that score highest according to the discrimination measure. For k = 1 this corre-sponds to finding arg maxI⊆I f (I).5.2. Constraint programming modelThe discrimination constraint can be expressed in a straightforward way. In addition to the variables Tt and I i weintroduce two new variables p and n, calculated as follows:(cid:4)(cid:4)p =Tt,t∈A+n =Tt.(14)t∈A−represent the set of transaction identifiers in the positive database D+and A−and negative databaseRemember that A+D−respectively. The discrimination constraint is now expressed as follows.Property 5 (Discrimination constraint). Given a database D, a transaction set T , a discrimination measure f and a threshold θ , anitemset is discriminative ifff (p, n) (cid:2) θ,where p and n are defined as described in Eq. (14).Such a constraint could be readily expressed in CP systems; essentially, a discrimination measure such as χ 2 is composedof a large number of mathematical operations on the variables p, n, |A−| and |A+|. By carefully decomposing the measureinto simple operations using intermediate variables, CP systems may be able to maintain bound consistency. This approachwould however be cumbersome (for instance, in the case of the χ 2 function we would need to rewrite the formula to takecare of the division by zero) and it is not guaranteed that rewriting its formula leads to an efficient computation strategyfor all discrimination measures.Hence, we propose a more robust approach here, which requires the addition of a new constraint in a CP system toenable the maintenance of tighter bounds for discrimination measures with ‘nice’ properties. Adding specialized globalT. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831967Fig. 7. Illustration of a rectangle of stamp points in PN space; within the rectangle [p1, p2] × [n1, n2], a ZDC measure reaches its highest value in one of thetwo highlighted stamp points.constraints is common practice in CP [45] and hence well supported in systems such as Gecode. The main observation thatwe use in this case is that many discrimination measures, such as χ 2, are zero on the diagonal and convex (ZDC).Definition 7. A scoring function fis zero diagonal convex (ZDC) if it has the following two properties:• the function reaches its minimum in all stamp points on the diagonal in PN-space, i.e.,∀0 (cid:3) α (cid:3) 1: f(cid:9)(cid:9), α• the function is convex, i.e., for every pair of stamp points σ (cid:8)= σ (cid:5)(cid:2)(cid:9)(cid:9)A−(cid:9)(cid:9)A+= 0;(cid:2)α(cid:9)(cid:3)(cid:9)(cid:3)∀0 (cid:3) α (cid:3) 1: f(cid:2)ασ + (1 − α)σ (cid:5)(cid:3) α f (σ ) + (1 − α) fit holds that(cid:3).σ (cid:5)Theorem 1. Fisher score, information gain, Gini index, and χ 2 are ZDC measures.Definitions, as well as independent, alternative proofs of this theorem, can be found in [13,34,35]. The plot of χ 2 inFig. 6 illustrates these two properties: the function is zero on the diagonal and convex.For a ZDC measure the following can be proved.Theorem 2 (Maximum for ZDC measures). Let f be a ZDC measure and 0 (cid:3) p1 (cid:3) p2 and 0 (cid:3) n1 (cid:3) n2. Thenmax(σ1,σ2)∈[p1,p2]×[n1,n2]f (σ1, σ2) = max(cid:8)(cid:10)f (p1, n2), f (p2, n1).Proof. The proof is similar to that of [35]. First, we observe that the function is convex. Hence, we know that the maximumin a space [p1, p2] × [n1, n2] is reached in one of the points (p1, n1), (p1, n2), (p2, n1) and (p2, n2). Next, we need to showthat we can ignore the corners (p1, n1) and (p2, n2). Observing that the minimum is reached on the diagonal, we candistinguish several cases.If n1/|A−| < p1/|A+|, the point (p1, n1) is ‘below’ the diagonal. We know for the point (|A+||A−| n1, n1) on the diagonalthat f (|A+||A−| n1, n1) = 0. Due to the convexity we know then that f (|A+||A−| n1, n1) = 0 (cid:3) f (p1, n1) (cid:3) f (p2, n1).Similarly, we can show that if (p1, n1) is above the diagonal that f (p1, n1) (cid:3) f (p1, n2); that f (p2, n2) (cid:3) f (p2, n1) if(p2, n2) is below the diagonal; and that f (p2, n2) (cid:3) f (p1, n2) if (p2, n2) is above the diagonal. (cid:2)The bound states that to find the highest possible score in a rectangle of points, it suffices to check two corners of therectangle. This is illustrated in Fig. 7, where a rectangle [p1, p2] × [n1, n2] is highlighted; the maximum on a ZDC measureis reached in one of the two corners (p2, n1) and (p1, n2). This property can be used to implement a propagator for adiscrimination constraint.Similar to the model for standard frequent itemset mining, we can improve the model by posting the discriminationconstraint on each item individually, leading to the reified discrimination constraint:(cid:4)(cid:5) (cid:4)(cid:6)∀i ∈ I : I i = 1 → fTt Dti,Tt Dti(cid:2) θ.(15)t∈A+t∈A−Our CP model of discriminative itemset mining is a combination of this constraint and the coverage constraint in Eq. (7).The propagator for the above constraint is obtained by applying Theorem 2 (see Algorithm 5).To understand this propagator, consider the example in Fig. 7, where we have marked the curve f (p, n) = θ for aparticular value of θ . Due to the convexity of the function f , stamp points for which f (p, n) (cid:2) θ can be found in the1968T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983(cid:7)t∈A+ Tt Dti ,(cid:7)t∈A− Tt Dti ) (cid:2) θ(cid:7)(cid:7)t∈A+ Tt Dti ,post constraint f ((cid:7)Algorithm 5 Conceptual propagator for Ii = 1 → f (1: if D(Ii ) = {1} then2:3: else4:5:6:7:8:9: end ifDti ,t∈A+ T max(cid:7)tt∈A+ T minf (tD(Ii ) = D(Ii ) \ {1}upper = max{ f (if upper < θ thenend if(cid:7)t∈A− Tt Dti ) (cid:2) θDti ),t∈A− T min(cid:7)tDti ,t∈A− T maxtDti )}Fig. 8. Stamp points (p2, 0) and (0, n2) are upper bounds for the itemset I with (p2, n2) = σ (I).(cid:7)lower-right and upper-left corner. None of the stamp points in (p, n) ∈ [p1, p2] × [n1, n2], where p1 =(cid:7)p2 =checked by the propagator by determining that f (p1, n2) < θ and f (p2, n1) < θ .Dti ,Dti , satisfies f (p, n) (cid:2) θ in the figure; this can easily beDti and n2 =t∈A− T maxt∈A+ T maxt∈A+ T mint∈A− T minDti , n1 =(cid:7)tttt(cid:7)5.3. ComparisonTraditional discriminative itemset mining algorithms essentially proceed by updating frequent itemset mining algorithmssuch that a different anti-monotonic constraint is used during the search. This constraint is based on the derivation of anupper-bound on the discrimination measure [35].Definition 8 (Upper-bound). Given a function f (p, n), function g(p, n) is an upper-bound for fiff ∀p, n: f (p, n) (cid:3) g(p, n).In the case of itemsets it is said that the upper-bound is anti-monotonic, if the constraint g(I) (cid:2) θ is anti-monotonic.The following upper-bound was presented by Morishita and Sese for ZDC measures [35].Theorem 3 (Upper-bound for ZDC measures). Let f (p, n) be a ZDC measure, then g(p, n) = max( f (p, 0), f (0, n)) is an upper-boundfor f (p, n) and g(I) (cid:2) θ is an anti-monotonic constraint.Proof. The fact that this function is an upper-bound follows from Theorem 2, where we take p1 = n1 = 0, p2 = σ1(I) andn2 = σ2(I). The anti-monotonicity follows from the fact that f (p, 0) and f (0, n) are monotonically increasing functions inand D−p and n, respectively. p and n represent the support of the itemset in the positive and negative databases D+respectively, which are anti-monotonic as well. (cid:2)The bound is illustrated in Fig. 8. Given the threshold θ in this figure, the itemset I with stamp point (p, n) = σ (I) willnot be pruned, as at least one of f (p, 0) or f (0, n) has a discrimination score that exceeds the threshold θ .This bound is used in an updated frequent itemset miner, of which the main differences are:• we need to be able to compute the support in the two classes of data separately. This can be achieved both using tid-listand fp-trees;• to prune items from the projected database, instead of a support constraint, a constraint on the upper-bound of thediscrimination score is used: a subtree of the search tree is pruned iff g(I) < θ , where θ is the threshold on the score(or the score of the kth best itemset found so far in top-k mining).In case we do not wish to find all discriminative patterns with a score above θ , but instead the top-k patterns with thehighest discriminative score, a branch-and-bound search strategy can be employed. In top − 1 branch-and-bound search, thebound on the discrimination score f (p, n) is increased as patterns with a higher score are found. For top − k branch-and-bound search, the bound is set to that of the kth pattern.T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831969Fig. 9. Illustration of the possible propagation for discriminative itemset mining; this propagation loop was not yet studied in specialized itemset miningalgorithms. In step 1 we have itemset {2}; we find out that itemset {2, 6} cannot reach the desired score and hence item 6 is excluded from consideration.As a result, some transactions may become unavoidable. Consequently, itemset {2, 4} may now be known not to reach the threshold and item {4} isexcluded from consideration.5.3.1. Constraint propagationIntuitively, when we compare Figs. 7 and 8, we can see that the search would continue for the itemset in Fig. 8 becausethe maximum reachable score is measured in the points (p2, 0) and (0, n2), for which the score is above the threshold θ .On the other hand the search would stop in Fig. 7 because the maximum the itemset can reach is measured in (p2, n1) and(p1, n2), for which the score is below the threshold. The difference is that in Fig. 7 p1 and n1 are taken into account, whichis the number of unavoidable transactions. As outlined on page 1963, unavoidable transactions are transactions for whichmin D(Tt ) = 1. So instead of having to use the upper-bound of Theorem 3, which does not take unavoidable transactions intoaccount, we can use Theorem 2, which offers a much tighter bound, especially in the case of many unavoidable transactions.Using the reified discrimination constraint leads to fine-grained interaction between search and propagation similar tothe reified frequency constraint in Section 3.3; Excluding an item from the itemset by reducing its domain to D(I i) = {0},can lead to the following propagation loop:1. Some transactions become unavoidable and are changed to D(Tt ) = {1}.2. D(Tt ) having changed, the reified discrimination constraints are checked; possibly a constraint detects that some itemcan no longer be included in the itemset and the item’s domain is reduced to D(I i) = {0}.3. Return to step 1.This propagation loop is illustrated in Fig. 9. It is absent in traditional discriminative itemset miners, which use the moresimple bound g(I). We will experimentally verify whether it is beneficial to perform the additional proposed propagationin Section 7.6. Itemset mining with costsIn cases where the mining process still yields a very large set of patterns, additional constraints can reduce the numberof patterns. Several papers have studied alternative constraints to the support constraint, which has lead to the concepts ofmonotonic, anti-monotonic and convertible anti-monotonic constraints. The prime example on which these concepts havebeen illustrated both in theory and in practice are constraints in which a cost, or weight, is associated with every item. Inthis section, we review these constraints and then show how they can be handled in the constraint programming approach.6.1. Problem definitionEssentially, every item i now has an associated weight c(i), often called the cost3 of the item. Let us now define the totalcost of an itemset asc(I) =(cid:4)c(i).i∈IThen we may be interested in finding itemsets for which we have a high total cost [42,11,9].Definition 9 (Frequent itemset mining with minimum total cost). Given a database D and two parameters θ and γ , the frequentitemset mining problem under a minimum cost constraint is the problem of finding the itemsets in(cid:8)I(cid:9)(cid:9) I ⊆ I, supportD(I) (cid:2) θ, c(I) (cid:2) γ(cid:10).3 This terminology is again from the supermarket setting, where the cost of an item could be its price or profit.1970T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983Similarly, we can mine under maximum cost constraints and average cost constraints.Definition 10 (Frequent itemset mining with maximum total cost). Given a database D and two parameters θ and γ , thefrequent itemset mining problem under a maximum cost constraint is the problem of finding the itemsets in(cid:8)I(cid:9)(cid:9) I ⊆ I, supportD(I) (cid:2) θ, c(I) (cid:3) γ(cid:10).Definition 11 (Frequent itemset mining with minimum average cost). Given a database D and two parameters θ and γ , thefrequent itemset mining problem under a minimum average cost constraint is the problem of finding the itemsets in(cid:8)I(cid:9)(cid:9) I ⊆ I, supportD(I) (cid:2) θ, c(I)/|I| (cid:2) γ(cid:10).Please note that a special case of cost-based itemset mining is achieved when c(i) = 1 for all i. These constraints areusually referred to as size constraints. A minimum size constraint is similar to a minimum support constraint: one is definedon the items, the other on the transactions.6.2. Constraint programming modelIn analogy to the support constraint, cost constraints can be expressed in two ways, non-reified and reified, and can beadded to the usual support and coverage constraints in the CP model.Property 6 (Non-reified minimum and maximum total cost constraint). Given a database D, an itemset I and a threshold γ , thenc(I) ≶ γ ⇐⇒(cid:6)I ic(i) ≶ γ,(cid:5)(cid:4)i∈Iwhere ≶∈ {<, (cid:3), (cid:2), >}, I i ∈ {0, 1} and I i = 1 iff i ∈ I .(16)Property 7 (Reified minimum and maximum total cost constraint). Given a database D, an itemset I and a threshold γ , ifsupportD(I) (cid:2) 1 then(cid:5)c(I) ≶ γ ⇐⇒Tt = 1 →(cid:6)I i Dtic(i) ≶ γ,(cid:4)i∈Iwhere ≶∈ {<, (cid:3), (cid:2), >}, I i ∈ {0, 1}, I i = 1 iff i ∈ I and Tt = 1 iff t ∈ T .(17)Proof. This follows from the assumption that also the coverage constraint should hold; hence if Tt = 1 we know that forall i with I i = 1 we must have Dti = 1. Because supportD(I) (cid:2) 1, we know that there is at least one transaction for whichTt = 1. (cid:2)Average cost constraints can be expressed by allowing for negative coefficients.Property 8 (Non-reified minimum and maximum average cost constraint). Given a database D, an itemset I and a transaction set T ,thenc(I)/|I| ≶ γ ⇐⇒(cid:5)(cid:4)(cid:2)c(i) − γ(cid:3)I i(cid:6)≶ 0,i∈Iwhere ≶∈ {<, (cid:3), =, (cid:8)=, (cid:2), >}, I i ∈ {0, 1} and I i = 1 iff i ∈ I .Proof. This follows from the following observation:c(I)/|I| ≶ γ ⇔ c(I) ≶ γ |I| ⇔(cid:3)(cid:2)c(I) − γ |I|≶ 0.(cid:2)The reified average cost constraints are obtained in a similar way as the reified total cost constraints.(18)T. Guns et al. / Artificial Intelligence 175 (2011) 1951–198319716.3. ComparisonAll specialized algorithms for mining under cost constraints exploit that these constraints have properties similar toanti-monotonicity.Definition 12 (Monotonic constraints). Assume given two itemsets I1 and I2, and a predicate p(I, D) expressing a constraint.Then the constraint is monotonic iff ∀I1 ⊆ I2 : p(I1, D) (cid:20)⇒ p(I2, D).Examples are maximum support and minimum cost constraints. Different approaches have been proposed for dealingwith monotonic constraints in the literature. We will discuss these approaches separately, at the same time pointing out therelation to our models in CP.6.3.1. Minimum total cost constraint: simple approachThe simplest depth-first algorithms developed in the data mining community for dealing with monotonic constraintsare based on the observation that supersets of itemsets satisfying the constraint also satisfy the constraint. Hence, duringthe depth-first search procedure, we do not need to check the monotonic constraint for children of itemsets satisfying themonotonic constraint [41]. To emulate this behavior in CP, we would only check the satisfiability of the monotone constraint,and refrain from possibly propagating over variables. This would result in branches of the search tree being cut when theycan no longer satisfy the constraint; the constraint would be disabled once it can no longer be violated.6.3.2. Minimum total cost constraint: DualMiner/non-reifiedMore advanced is the specialized DualMiner algorithm [11]. DualMiner associates a triplet (Iin, Icheck, Iout) with everynode in the depth-first search tree of an itemset miner. Element Iin represents the itemset to which the node in the searchtree corresponds; Icheck and Iout provide additional information about the search node. Items in Icheck are currently notincluded in the itemset, but may be added in itemsets deeper down the search tree; these items are part of the projecteddatabase. For items in Iout it is clear that they can no longer be added to any itemset deeper down the search tree. Addingan item of Icheck to Iin leads to a branch in the search tree. An iterative procedure is applied to determine a final triplet(Iin, Icheck, Iout) for the new search node, and to determine whether the recursion should continue:• it is checked whether the set Iin satisfies all anti-monotonic constraints. If not, stop;• it is checked which individual items in Icheck can be added to Iin and satisfy the anti-monotonic constraints. Only thosethat do satisfy the constraints are kept in Icheck, others are moved to Iout;• it is checked whether the set Iin ∪ Icheck satisfies the monotonic constraints. If not, stop. Every item i ∈ Icheck for whichitemset (Iin ∪ Icheck)\{i} does not satisfy the monotonic constraints, is added to Iin. (For instance, if the total cost istoo low without a certain item, we have to include this item in the itemset.) Finally, the procedure is iterated again todetermine whether Iin still satisfies the anti-monotonic constraints.If the loop reaches a fixed point and items are still left in Icheck the search continues, unless it also appears that Iin ∪ Ichecksatisfies the anti-monotonic constraints and Iin satisfies the monotonic constraints; in this case the sets Icheck ⊆ I ⊆ Iin ∪ Icheckcould immediately be listed.A similar search procedure is obtained when the cost constraints are formulated in a non-reified way in the CP system.As pointed out earlier, a non-reified minimum or maximum cost constraint takes the following form:(cid:4)iI ic(I) ≶ γ .Propagation for this constraint is of the following kind:• if according to current domains the constraint can only be satisfied when the CP system includes (minimum costconstraint) or excludes (maximum cost constraint) an item, then the system does so; this corresponds to moving anitem to the Iin or Iout set in DualMiner;• if according to current domains the constraint can no longer be satisfied, backtrack;• if according to current domains the constraint will always be satisfied the constraint is removed from the constraintstore.Hence, the overall search strategy for the non-reified constraint is similar to that of DualMiner. There are also some differ-ences. DualMiner does not aim at finding the transaction set for every itemset. If it finds that Iin satisfies the monotonicconstraint and Iin ∪ Icheck satisfies the anti-monotonic constraint, it does not continue searching, and outputs the corre-sponding range of itemsets explicitly or implicitly. The CP system will continue enumerating all itemsets in the range inorder to find the corresponding transaction sets explicitly.1972T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19836.3.3. Minimum total cost constraint: FP-Bonsai/reifiedIn the FP-Bonsai algorithm [8], the idea of iteration till a fixed point is reached was extended to monotonic constraints.The main observation on which this algorithm is based is that a transaction which does not satisfy the minimum cost con-straints, will never contain an itemset that satisfies the minimum cost constraint. Hence, we can remove such transactionsfrom consideration. This may reduce the support of items in the projected database and result in the removal of more itemsfrom the database. The reduction in size of some transactions may trigger a new step of propagation.If we consider the reified minimum cost constraint,Tt = 1 →(cid:4)iI ic(I)Dti (cid:2) γ ,we can observe a similar propagation. Propagation essentially removes a transaction from consideration when the constraintcan no longer be satisfied on the transaction. The removal of this transaction may affect the support of some items, requiringthe propagators for the support constraints to be re-evaluated.Note that the reified constraint is less useful for a maximum total cost constraint, c(I) (cid:3) γ . Essentially, for every trans-action only items already included in the itemset can be considered. If the sum of these items is to large, the transactionis removed from consideration. This would happen for all transactions separately, leading to a failed branch. Overall, thepropagation is expensive to evaluate (as it needs to be done for each transaction) and not as effective as the non-reifiedpropagator which can also prune items from consideration instead of only failing.Thus the reified and non-reified form are complementary to each other. We can obtain both types of propagation byposting constraints of both types in a CP system.6.3.4. Minimum and maximum average cost constraints.Average cost constraints are neither monotonic nor anti-monotonic. Still, they have a property that is related to that ofmonotonic and anti-monotonic constraints.Definition 13 (Convertible anti-monotonic constraints). Assume given two itemsets I1 and I2, a predicate p(I, D) express-ing a constraint, and an order < between items. Then the constraint is convertible anti-monotonic for this order iff∀I1 ⊆ I2, min(I2 \ I1) (cid:2) max(I1) : p(I2, D) (cid:20)⇒ p(I1, D).For example, if the items are ordered according to increasing cost c(I), when adding items that are more expensivethan the current items, the average cost can only increase. For a decreasing order in item cost, the minimum average costconstraint is convertible anti-monotonic. Different orderings will not result in anti-monotonic behavior, i.e. if after addingexpensive items an item with a low cost would be added, the average cost would go down. Note that a conjunction ofmaximum and minimum cost constraints is hence not convertible anti-monotonic, as we would need opposing orders foreach of the two constraints.Essentially our formalization in CP of average cost constraints is very similar to that of total cost constraints, the maindifference being that negative costs are allowed. Consequently, depending on the constraint (minimum or maximum) andthe weight (positive or negative) either the maximum value in the domain or the minimum value in the domain is used inthe propagator. In the non-reified form, we obtain propagation towards the items; in the reified form towards the transac-tions.This search strategy is fundamentally different from the search strategy used in specialized mining systems, in which theproperty is used that one average cost constraint is convertible anti-monotonic. Whereas in specialized systems the combi-nation of multiple convertible constraints poses problems, in the CP-based approach this combination is straightforward.6.3.5. ConclusionsInterestingly, when comparing models in CP and algorithms proposed in the data mining community, we can see thatthere are many similarities between these approaches. The different approaches can be distinguished based on whether theyrepresent a reified or an non-reified constraint. The main advantage of the constraint programming approach is that theseapproaches can also easily be combined. This advantage is also evident when combining convertible constraints; specializedalgorithms can only optimize on one such constraint at the same time.7. ExperimentsIn previous sections we concentrated on the conceptual differences between traditional algorithms for itemset miningand constraint programming; we showed that itemset mining can be modeled in constraint programming. In the presentsection, we first consider several choices that have to be made when implementing itemset mining in a constraint program-ming framework and evaluate their influence on the performance of the mining process. More specifically, we answer thefollowing two questions about such choices:T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831973Table 1Properties of the datasets used.Dataset1. Soybean2. Splice-13. Anneal4. Mushroom# transactions# itemsDensity10% freq# solutions (10% freq)6303190812812459290411190.250.210.430.19633198181212 75419631 891 712574 514QA What is the difference in performance between using reified versus non-reified constraints of itemset mining?QB What is the effect of the different variable orderings on the performance of itemset mining?Further (more technical and system dependent) choices made in our implementation are explained in Appendix A for com-pleteness and reproducibility. All used implementations are also available on our website: http://dtai.cs.kuleuven.be/CP4IM/.We use the best approach resulting from the above experiments to experimentally compare our constraint program-ming framework CP4IM to state-of-the-art itemset mining systems. More specifically, our comparative experiments focus onanswering the following questions:Q1 What is the difference in performance of CP4IM for frequent itemset mining and traditional algorithms?Q2 What is the difference in performance of CP4IM for closed itemset mining and traditional algorithms?Q3 Is the additional propagation in CP4IM for discriminative itemset mining beneficial? If so, how much?Q4 Is the alternative approach for dealing with convertible constraints in CP4IM beneficial? If so, how much?We ran experiments on PCs with Intel Core 2 Duo E6600 processors and 4 GB of RAM, running Ubuntu Linux. Theexperiments are conducted using the Gecode constraint programming system [25]. Gecode4 is an open source constraintprogramming system which is representative for the current state-of-the-art of efficient constraint programming.The starting point for our experiments was Gecode version 2.2.0. In the course of our experiments we tried severalformulations and implemented alternative propagators, explained in detail in Appendix A, some of which are now includedin Gecode by default.7.1. Data setsIn our experiments we used data from the UCI Machine Learning repository.5 To deal with missing values we prepro-cessed each dataset in the same way as [16]: we first eliminated all attributes having more than 10% of missing values andthen removed all examples (transactions) for which the remaining attributes still had missing values. Numerical attributeswere binarized by using unsupervised discretization with 4 equal-frequency bins; each item for an attribute correspondsto a threshold between two bins. These preprocessed datasets can be downloaded from our website.6 The datasets andtheir basic properties can be found in Table 1. The density is the relative amount of ones in the matrix. In many itemsetproblems, a higher density indicates that the dataset is more difficult to mine.7.2. Alternative itemset minersWe used the following state-of-the-art specialized algorithms, for which implementations are freely available, as thebasis for our comparative evaluation:LCM LCM [51] is a specialized frequent closed itemset mining algorithm based on tid-lists;Eclat Eclat [56] is a specialized depth-first frequent itemset mining based on tid-lists;Patternist Patternist [9] is a specialized breadth-first algorithm for mining under monotonic and anti-monotonic constraints;DDPMine DDPMine [13] is a specialized depth-first closed discriminative itemset mining algorithm based on fp-trees and arepository of closed itemsets; it uses a less tight bound than the bound summarized in Section 5 [38].Note that in our experiments we are not using all algorithms discussed in previous sections. The reason is that we preferredto use algorithms for which comparable implementations by the original authors were freely available (i.e. executable onthe same Linux machine).Table 2 provides an overview of the different tasks that these data mining systems support. The LCM and Eclat algorithmshave been upgraded by their original authors to support respectively frequent itemset mining and closed itemset miningtoo. Patternist is a constraint-based mining algorithm which has been carefully designed to make maximal use of monotone4 http://www.gecode.org/.5 http://archive.ics.uci.edu/ml/.6 http://dtai.cs.kuleuven.be/CP4IM/.1974T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983Table 2Comparing the mining tasks that different miners support.Frequent itemsetsClosed itemsetsCorrelated itemsetsMonotone constraintsConvertible constraintsCombinations of the aboveCP4IMXXXXXXLCMX*XEclatXX*PatternistDDPMineXXXX**XX* – Not originally designed for this task.X** – Combinations of the constraints it supports, except multiple convertible constraints.Table 3Comparing the reified versus non-reified formulation of the frequency constraint on 2 datasets for different frequencies. The reified formulation never hasfailed branches.DatasetFreq.Non-reified frequencyReified frequencyMem.Props.FailuresTime (s)Mem.Props.Time (s)AnnealAnnealAnnealMushroomMushroomMushroom5%10%15%5%10%15%26942438230917 86217 86217 862235 462221 054200 4427 737 1164 184 9402 680 47917024829810 3833376190946.319.38.4269.574.240.829502501237320 48620 22919 973236 382224 555203 7595 980 4782 853 2481 589 28944.718.98.3239.843.410.5Faster1.041.021.011.121.713.89and convertible constraints during the search. Our CP4IM system is the only system that supports all of these constraints aswell as combinations of these constraints. Furthermore, thanks to the use of a declarative constraint programming systemit can easily be extended with further types of constraints. This is what we regard as the major of advantage of the con-straint programming methodology. It is also interesting to contrast this approach with that taken by the alternative, moreprocedural systems, which were typically designed to cope with a single constraint family and were later upgraded to dealwith other ones too. This upgrade usually involves changing the algorithm dramatically and hard-coding the new constraintin it. In contrast, in CP one might need to add a new propagator (as we have done for the discrimination constraint), butthe corresponding constraint can freely be used and combined with any other current and future constraint in the system.This is essentially the difference between a declarative and a procedural approach.On the other hand, generality and flexibility also may have a price in terms of performance. Therefore, we do not expectCP4IM to perform well on each task, but we would hope its performance is competitive when averaging over a number oftasks.7.3. QA: non-reified vs reifiedIn Section 3.3 we argued that using reified frequency constraints for the standard frequent itemset mining problemcan lead to more propagation. Table 3 shows a comparison between running the CP model with non-reified and reifiedfrequency constraints. Two datasets were used, each with three different frequency thresholds. For the reasonably smallanneal dataset, it can be noted that the non-reified version needs a bit less memory and propagation, but at the costof some failed branches in the search tree. This leads to a small increase in run time. For the bigger mushroom datasethowever, the difference is larger. For higher minimum frequency thresholds the reified pruning becomes stronger while forthe non-reified formulation the cost of a failed branch increases, leading to a larger difference in runtime. In general wehave observed that using the reified frequency constraints often leads to better performance, especially for larger datasets.7.4. QB: variable orderingIn constraint programming it is known that the order in which the variables are searched over can have a large impacton the size of the search tree, and hence the efficiency of the search. This has received a lot less attention in the itemsetmining community, except in algorithms like fp-growth where a good ordering is needed to keep the fp-tree size down.We consider three possible variable ordering strategies, for the standard frequent itemset mining problem:the input order of variables is used;arbitrary:minimum degree:maximum degree:the variable occurring in the smallest number of propagators;the variable occurring in the largest number of propagators.The comparison of the three variable orderings can be found in Table 4. The experiments show that choosing the variablewith maximum degree leads to a large reduction in the number of propagations and runtime. The maximum degree heuris-T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831975Table 4Comparison of peak memory, number of propagations and time (in seconds) using different variable ordering heuristics on the 4 datasets listed in Table 1.ArbitraryMem.1860124 431211631 2361234Minimum degreeMaximum degreeProps.799 7913 188 139121 495 848344 377 153Time0.54973365Mem.1540126 927211630 148Props.3 861 0553 772 591472 448 674997 905 725Time3.1603741504Mem.1860122 511179626 244Props.137 6662 602 069577 7196 232 932Time0.2411848Fig. 10. Standard itemset mining on different datasets. (For interpretation of the references to color in this figure, the reader is referred to the web versionof this article.)Fig. 11. Closed itemset mining on different datasets.tic corresponds to choosing the item with the lowest frequency, as this item occurs in the coverage constraint (Eq. (7))of most transactions. In other words, the most efficient variable ordering strategy is a fail-first strategy that explores themost unlikely branches of the search tree first. Such a conclusion is not surprising in the constraint programming commu-nity.7.5. Q1: frequent itemset miningA comparison of specialized frequent itemset miners and CP4IM is provided in Fig. 10 for a representative number ofdatasets. In this figure we show run times for different support thresholds as it was previously found that the differencesbetween systems can highly depend on this constraint [27].The run time of all systems is correlated to the number of patterns found (the red line). Our CP4IM model implementedin Gecode, indicated by FIM_CP, is significantly slower than the other depth-first miners, but shows similar behavior. Thisindicates that indeed its search strategy is similar, but the use of alternative data structures and other overhead in the con-straint programming system introduces a lot of overhead for standard frequent itemset mining. It is remarkable that CP4IMcompares well to the breadth-first Patternist system, which does not use the concept of projected databases as pervasivelyas other systems; the compact representations developed in the specialized itemset miners for projected databases indeedexplain the performance difference.1976T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983Table 5Statistics of UCI datasets, and runtimes, in seconds, of two CP models and other systems.NameAnnealAustralian-crBreast-wiscDiabetesGerman-crHeart-clevelHypothyroidIonospherekr-vs-kpLetterMushroomPendigitsPrimary-tumSegmentSoybeanSplice-1VehicleYeastDense0.450.410.500.500.340.470.490.500.490.500.180.500.480.500.320.210.500.49Trans81265368376810002963247351319620 000812474943362310630319084614847.6. Q2: closed itemset miningItemCP4IM(4)CP4IM(2)ddpmine [14]lcm [51]931251201121129588445732241192163123550287252890.220.300.282.452.390.190.711.440.9252.6614.113.680.031.450.0530.410.855.6724.090.6313.66128.0466.792.1510.91>46.20>13.48>0.13>0.0731.11>781.6322.463.4096.75––9.49––125.60–0.09–0.26–0.051.86––7.921.2227.49697.1230.842.87>>25.62>0.03>0.08>0.020.02>185.28In Fig. 11 the runtime of all mining algorithms is shown for the problem of closed itemset mining. Again, run time iscorrelated with the number of patterns found. The difference between CP4IM and the other miners is smaller in this exper-iment. We argued in the previous section that the CP system behaves similar to the LCM system. Indeed, our experimentson both the mushroom and letter data set show that this is the case; in one case even outperforming the Eclat system,which as not originally developed for closed itemset mining.It should be noted that on sparse data, such as mushroom, the difference in performance between Gecode and specializedsystems is larger than on dense data, such as the letter data. This can be explained by the inefficient representation ofsparse data in Gecode; on dense data, as compared to Eclat, this inefficient representation is compensated by more effectivepropagation.7.7. Q3: discriminative closed itemset miningIn this experiment we compare several approaches for finding the most discriminative itemset, given labeled data. Resultsare shown in Table 5. As in this setting we do not have to determine a threshold parameter, we perform experimentson a larger number of datasets. The missing values of the datasets were preprocessed in the same way as in previousexperiments. However, the numerical attributes were binarized using unsupervised discretization with 7 binary split points(8 equal-frequency bins). This enforces a language bias on the patterns that is closer to that of rule learning and subgroupdiscovery systems [28]. In case of a non-binary class label, the largest class was labeled positive and the others negative.The properties of the datasets are summarized in Table 5; note the higher density of the datasets than in the previousexperiments, resulting from the discretization procedure.We report two types of experiments with CP4IM: using the propagator introduced in Section 5.2 (CP4IM(4)) and usinga propagator that mimics the propagation occurring in the specialized discriminative itemset miner introduced in [35](CP4IM(2)). Furthermore, we also apply the LCM algorithm; in [38] it was shown that for well-chosen support thresholds,the resulting set of frequent itemsets is guaranteed to contain all itemsets exceeding a correlation threshold. We use thecorrelation threshold of the best pattern (found using our algorithm) to compute a support threshold according to thismethod and run LCM with this support threshold. Note that by providing LCM knowledge about the best pattern to befound, the comparison is unfair to the advantage of LCM.For experiments marked by “>” in our table no solution was found within 900 seconds. In experiments marked by “–”the repository of closed itemsets runs out of memory. The experiment shows that CP4IM(4) consistently outperforms exist-ing data mining systems, where in most cases this increased performance can be attributed to the improved propagationthat was revealed in CP4IM(4).It can be noted that on one dataset, the mushroom dataset, the new propagator takes slightly more time. Our hypothesisis that this is related to the low density of the data, for which 4-bound pruning can be less effective when there is nostructure in the data which would lead to unavoidable transactions. To test this hypothesis, we performed additional exper-iments in which we gradually sparsified two dense datasets, given in Fig. 12. The sparsification was performed by randomlyremoving items uniformly from the transaction database, until a predefined sparsity threshold was reached. Averaging run-times over 10 different samples for each setting, we ran our CP4IM system using three different propagators: CP4IM(4) andCP4IM(2), as explained above, and CP4IM(1) which uses the simple frequency based propagator used in [14].T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831977Fig. 12. Correlated itemset mining when sparsifying the data.Fig. 13. Runtimes of itemset miners on Segment data under constraints. In the left figure, the suffix _m25, _m30, etc. of the name indicates the minimumsize threshold. In the right figure, the suffix _1%, etc. represent the minimum support threshold.The experiments show that when the density is decreased, and hence the sampling removes structure from the data, theadvantage of the more advanced pruning method over the more simple ones disappears. However, within the CP frameworkthe 4-bound method is often better and at worse equivalent to the 2- and 1-bound pruning.7.8. Q4: cost-based itemset miningIn this experiment we determine how CP4IM compares with other systems when additional cost constraints are em-ployed. Results for two settings are given in Fig. 13, where our system is indicated by FIM_CP. In the first experiment weemployed a (monotonic) minimum size constraint in addition to a minimum frequency constraint; in the second a (con-vertible) maximum average cost constraint. The results are positive: even though for small minimum size constraints thebrute force mining algorithms, such as LCM, outperform CP4IM, CP4IM does search very effectively when this constraintselects a small number of very large itemsets (30 items or more); in extreme cases CP4IM finishes within seconds whileother algorithms do not finish within our cut-off time of 30 minutes. Patternist, being a breadth-first algorithm, was unableto finish some of these experiments due to memory problems. This indicates that CP4IM is a competitive system whenthe constraints require the discovery of a small number of very large itemsets. The results for convertible constraints areparticularly promising, as we did not optimize the item order in any of our experiments, as is usually done when dealingwith convertible constraints.8. ConclusionsWe started this paper by raising the question as to whether constraint programming can be used for solving itemset miningproblems in a declarative way. Our results show that the answer to this question is indeed positive and that the useof constraint programming offers several advantages as well as new insights. Perhaps the more important advantage isthat constraint programming systems are general purpose systems supporting many different types of constraints. In thisregard, we showed that it is possible to incorporate many well-known constraints, such as cost constraints, closednessor discriminative measures as defined above, as well as their combinations in the constraint programming system. Theadvantage of the resulting declarative approach to data mining is that it is easy to extend or change in order to accommodatenew constraints, and that all constraints can automatically be combined with one another. Furthermore, a detailed analysisof the solution strategy of constraint programming systems showed that there are many similarities between these systemsand specialized itemset mining systems. Therefore, the constraint programming system arguably generalizes these systems,1978T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983not only from a theoretical perspective but also from a practical one. This was confirmed in our experiments: for problemssuch as frequent and closed itemset mining, for which fast implementation contests were organized, these specializedsystems usually outperform CP4IM; however the runtime behavior of our constraint programming approach is similar tothat of the specialized systems. The potential of the CP approach from a performance perspective was demonstrated on theproblem of discriminative itemset mining. We showed that by rigorously using the principles of constraint programming,more effective propagation is obtained than in alternative state-of-the-art data mining algorithms. This confirms that it isalso useful in an itemset mining context to take propagation as a guiding principle. In this regard, it might be interestingto investigate the use of alternative search strategies that have been developed in the constraint programming community[53,44].Continuing this research, we are currently studying the application of our approach to problems arising in bioinformatics.For instance, itemset mining has commonly been applied to the analysis of microarray data; our hope is that constraintprogramming may offer a more general and more flexible approach to analyze such data. Whereas the above work is stillrestricted to the discovery of patterns in binary data, the use of constraint programming in other pattern mining relatedproblems is also a promising direction of future research. A problem closely related to pattern mining is that of patternset mining [19], where one does not only impose constraints on individual patterns, but also on the overall set of patternsconstituting a solution [29]. Constraints that can be imposed include, for instance, the requirement that patterns do notoverlap too much, or that they cover the complete set of transactions together. Another related problem is that of findingpatterns in continuous data. This requirement is in particular relevant to deal with problems in bioinformatics. Likewise,there are many approaches to mining structured data, such as sequences, trees and graphs. It is an interesting open questionas to whether it is possible to represent such problems using constraint programming too. One of the challenges here isthat such structured data can no longer be represented using a fixed number of features or variables.In addition to pattern mining, other areas of machine learning and data mining may also profit from a closer study ofconstraint programming techniques. One such area is statistical machine learning, where problems are typically formulatedusing mathematical programming. Recently some results in the use of other types of solvers have already been obtainedfor certain probabilistic models [12,15]. In these approaches, however, Integer Linear Programming (ILP) or satisfiability(SAT) solvers were used. CP solvers address a more general class of problems than ILP and SAT solvers, but this generalitysometimes comes at a computational cost. Current developments in CP that aim at combining ILP and SAT with CP mayalso help in addressing these machine learning problems.Other topics of interest are constraint-based clustering and constraint-based classifier induction. In constraint-based clus-tering the challenge is to cluster examples when additional knowledge is available about these examples, for instance,prohibiting certain examples from being clustered together (so-called cannot-link constraints). Similarly, in constraint-basedclassifier induction, one may wish to find a decision tree that satisfies size and cost-constraints. A first study on the applica-tion of CP on this problem was recently performed by Bessiere, Hebrard, and O’Sullivan [7]. In data mining, the relationshipbetween itemset mining and constraint-based decision tree learning was studied [36]. It is an open question as to whetherthis relation can also be exploited in a constraint programming setting.Whereas the previous cases study how data mining could profit from constraint programming, the opposite directionis also a topic of interest: how can constraint programming systems be extended using techniques from data mining? Forexample, in constraint programming systems the data is typically spread over the constraints, and possibly multiple timesin different ways. In contrast, in data mining the data is typically centrally accessed, allowing the use of different matrixrepresentations.To summarize, we believe that the further integration of machine learning, data mining and constraint programmingmay be beneficial for all these areas.AcknowledgementsThis work was supported by a Postdoc and a project grant from the Research Foundation—Flanders, project “Principles ofPatternset Mining” as well as a grant from the Institute for the Promotion and Innovation through Science and Technologyin Flanders (IWT-Vlaanderen).Appendix A. Improved solving, continuedIn Section 7 we empirically studied the effect of non-reified versus reified formulations and of different variable orderingheuristics. In this appendix we include some additional findings, as we have experienced that making the right low-leveldecisions is necessary to be competitive with the highly optimized itemset mining implementations. We start by studyingthe differences between using boolean variables and integers with a domain of {0, 1}. We continue by studying two imple-mentation alternatives for an essential constraint shared by all models: the coverage constraint. We end with a comparisonof different value ordering heuristics; to explain and improve the results we have to provide some additional details aboutthe Gecode system.Apart from Gecode specific remarks, which are applicable only to solvers that do copying and cloning, the results pre-sented in this appendix are also valid and applicable to other solvers. In fact, parts of the work studied here are now bydefault in the aforementioned Gecode system.T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831979Table A.6Comparison in propagations, peak memory and time (in seconds) of using boolean variables and their respective constraints versus using integer variablesand constraints.DatasetOriginal booleanPeak mem.# props1. Soybean2. Splice-13. Anneal4. Mushroom2820147 280314045 6365 909 59223 708 8071 121 904 9242 989 128 466Time1.41292791447IntegersPeak mem.# props2436142 032256439 9401 839 9329 072 323273 248 618862 480 847Time0.857136508Gain1.72.32.12.9Table A.7Comparison in propagations, peak memory and time (in seconds) of the channeled integer formulation of the base model and the boolean formulation withthe dedicated propagator.DatasetIntegersPeak mem.# props1. Soybean2. Splice-13. Anneal4. Mushroom2436142 032256439 9401 839 9329 072 323273 248 618862 480 847Time0.857136508Dedicated booleanPeak mem.# props1796123 279250030 8521 058 2386 098 820121 495 848520 928 196Time0.56874387Gain1.60.81.81.3A.1. Booleans vs integersFinite domain integer solvers can choose to represent a boolean as an integer with a domain of {0, 1}, or to implementa specific boolean variable.An essential constraint in our models is the reified summation constraint. Such a sum can be expressed both on booleanand integer variables, but the reification variable always has a boolean domain. Using boolean variables should be equally ormore efficient than integer variables, especially since in our model, the integer variables need to be ‘channeled’ to booleanvariables for use in the reification part. However, in our experiments (Table A.6) the model using booleans was slower thanthe one using integers. The reason is that a given reified summation constraint on booleans B i and boolean variable C ,(cid:4)iB i (cid:2) v ↔ C(cid:7)was decomposed into two constraints: S =i B i and S (cid:2) v ↔ C , where S is an additional integer variable; separate prop-agators were used for both constraints. For integers on the other hand, a single propagator was available. Our experimentsshow that decomposing a reified sum constraints over booleans into a sum of booleans and reifying the integer variable isnot beneficial.We implemented a dedicated propagator for a reified sum of boolean variables constraint, which includes an optimizationinspired by SAT solvers [26]. A propagator is said to watch the variables on which it depends. A propagator is activatedwhen the domain of one of its watched variables changes. To improve efficiency, the number of watched variables shouldi=1 B i (cid:2) v ↔ C , where all B i and C are boolean variables, then itnot be larger than necessary. Assume we have a sumis sufficient to watch max(v, n − v + 1) (arbitrary) variables B i not fixed yet: the propagator can not succeed (v variablestrue) or fail (n − v + 1 variables false) without assigning at least one of the watched variables. In Table A.7 we comparethe formulation of the basic frequent itemset mining problem using integers and channeling, to using boolean variablesand the new dedicated propagator. The peak amount of memory needed when using only booleans is naturally lower. Theamount of propagations is also decreased significantly, leading to lower runtimes for all but one dataset. Hence it is overallrecommended to use boolean variables with dedicated propagators.(cid:7)nA.2. Coverage constraint: propagators versus advisersWhen a variable’s domain changes, the propagators that watch this variable can be called with different amounts ofinformation. To adopt the terminology of the Gecode system, we differentiate between classic ‘propagators’ and ‘advisers’:• propagators: when the domain of at least one variable changes, the entire propagator is activated and re-evaluated;• advisers: when the domain of a variable changes, an adviser is activated and informed of the new domain of thisvariable. When the adviser detects that propagation can happen, it will activate the propagator.Both techniques have their advantages and disadvantages: classic propagators are conceptually simpler but need to iterateover all its variables when activated; advisers are more fine-grained but require more bookkeeping. We implemented thecoverage constraint using both techniques, and compare them in Table A.8. Using advisers requires more memory butreduces the overall amount of propagations, the runtimes also decrease.1980T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983Table A.8Comparison in propagations, peak memory and time (in seconds) of formulating the coverage constraints using: the new reified sum constraint, the advisorversion of the new reified sum constraint and the clause constraint.Boolean sumBoolean sum, advisersClause, advisersMem.1796123 279250030 852# props1 058 2386 098 820121 495 848520 928 1961234Time0.56874387Mem.2500237 071250047 172# props799 7913 188 139121 495 848344 377 153Time0.55473372Mem.1860124 431211631 236# props799 7913 188 139121 495 848344 377 153Time0.54973365Table A.9Comparison of peak memory, propagations and time (in seconds) using the minimum or maximum value ordering heuristics on the frequent itemset miningproblem.Dataset1. Soybean2. Splice-13. Anneal4. MushroomMinimum valueMaximum valueMem.1860122 511179626 244Props.137 6662 602 069577 7196 232 932Time0.2411848Mem.89916 328141220 229Props.217 8024 961 137726 3089 989 882Time0.31091863Fig. A.14. Search tree for the first 35 variables of the mushroom dataset. Every blue circle is a branchpoint over an item, every green diamond is a solution.A branch to the left assigned 0 to the item of that branchpoint, a branch to the right assigned 1. (For interpretation of the references to color in this figurelegend, the reader is referred to the web version of this article.)A.3. Coverage constraint: clauses vs sums.As we pointed out in Property 1, Eqs. (7) and (8), on page 1957, the coverage constraint can be expressed in twoequivalent ways: using reified sums or using reified clauses. Both options are evaluated in Table A.8. Overall, we find thatthe formulation using clauses performs best.A.4. Value orderingFor boolean decision variables, two value ordering heuristics are meaningful: selecting the minimum value (0) or select-ing the maximum value (1) first. A comparison can be found in Table A.9, where the maximum degree variable ordering isused.The results are surprising: using the maximum value heuristic leads to more propagation and longer run times. This iscounter-intuitive: the search tree is equally large in both cases and because the complete tree is searched, the total amountof propagation should be identical too. The explanation can be found in how the Gecode system stores intermediate statesduring the search. Gecode uses a technique called copying and recomputation [46]. In this technique, some nodes, but notnecessarily all nodes, in the depth-first search tree are copied and stored. To backtrack, one retrieves the latest copied nodeand recomputes the propagations using the assignments leading to the desired node. This can save memory consumptionand runtime for large problems [46]. The amount of copying/recomputation is set by the copy distance parameter. In Gecode,the default is 8, meaning that a new copy is made every 8 nodes.When we consider a search tree using the minimum value first heuristic for our models (see Fig. A.14), we see that allvariables are set to zero first, creating one long branch. The copied nodes in this branch are reused throughout the rest ofthe search. When using the maximum value heuristic, more propagation is possible and shorter branches are explored first.Consequently, less nodes are copied, and a lot of recomputation needs to be done in each of the short branches. In ourexperiments this results in increased overhead.Table A.10 compares two values of the copy distance parameter, and how this influences the value ordering heuristic.With a distance of 0, every node in the search tree is copied. This results in a smaller amount of propagation comparedto a distance of 8, independent of the value ordering heuristic used. Interestingly, the amount of runtime is also decreasedcompared to a larger copy distance. Using the maximum value first heuristic is about as fast as the minimum value heuristic,T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831981Table A.10Comparison of peak memory, propagations and time (in seconds) using the minimum or maximum value ordering heuristics. The copy distance is eitherthe default (c–d 8) or zero (c–d 0).Minimum, c–d 8Minimum, c–d 0Maximum, c–d 0Mem.1860122 511179626 244Props.137 6662 602 069577 7196 232 9321234Time0.2411848Mem.7626911 8634231148 173Props.64 8231 822 064224 5552 853 248Time0.2241943Mem.179616 328250120 229Props.64 8231 822 064224 5552 853 248Time0.2231943Fig. A.15. Relative improvements from applying several heuristics.but needs significantly less memory. For our application it is thus faster and less memory intensive to clone every node inthe search tree and choose the maximum value first.A.5. SummaryAn overview of the relative improvements of each step can be found in Fig. A.15. Overall, we see that even thoughour initial model – using reified constraints – could be specified straightforwardly, the scalability of the approach is highlydepended on making the right low-level decisions, as discussed in this section. Only this modeling process as a whole canmake the CP-based approach competitive with current specialized systems for constraint-based itemset mining.References[1] Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami, Mining association rules between sets of items in large databases, in: Proceedings of the ACMSIGMOD International Conference on Management of Data, ACM Press, 1993, pp. 207–216.[2] Rakesh Agrawal, Hiekki Mannila, Ramakrishnan Srikant, Hannu Toivonen, A. Inkeri Verkamo, Fast discovery of association rules, in: Advances in Knowl-edge Discovery and Data Mining, AAAI Press, 1996, pp. 307–328.[3] Krzysztof R. Apt, Mark Wallace, Constraint Logic Programming Using Eclipse, Cambridge University Press, New York, NY, USA, 2007.[4] Stephen D. Bay, Michael J. Pazzani, Detecting change in categorical data: mining contrast sets, in: Proceedings of the Fifth International Conference onKnowledge Discovery and Data Mining, ACM Press, 1999, pp. 302–306.[5] Roberto J. Bayardo Jr., Rakesh Agrawal, Dimitrios Gunopulos, Constraint-based rule mining in large, dense databases, Data Mining and KnowledgeDiscovery 4 (2/3) (2000) 217–240.[6] Nicolas Beldiceanu, Mats Carlsson, Sophie Demassey, Thierry Petit, Global constraint catalogue: past, present and future, Constraints 12 (March 2007)21–62.[7] Christian Bessiere, Emmanuel Hebrard, Barry O’Sullivan, Minimising decision tree size as combinatorial optimisation, in: Principles and Practice ofConstraint Programming, in: Lecture Notes in Computer Science, vol. 5732, Springer, 2009, pp. 173–187.[8] Francesco Bonchi, Bart Goethals, FP-bonsai: the art of growing and pruning small fp-trees, in: Advances in Knowledge Discovery and Data Mining, in:Lecture Notes in Computer Science, vol. 3056, Springer, 2004, pp. 155–160.[9] Francesco Bonchi, Claudio Lucchese, Extending the state-of-the-art of constraint-based pattern discovery, Data and Knowledge Engineering 60 (2)(2007) 377–399.[10] Sally C. Brailsford, Chris N. Potts, Barbara M. Smith, Constraint satisfaction problems: algorithms and applications, European Journal of OperationalResearch 119 (3) (1999) 557–581.[11] Cristian Bucila, Johannes Gehrke, Daniel Kifer, Walker M. White, DualMiner: a dual-pruning algorithm for itemsets with constraints, Data Mining andKnowledge Discovery 7 (3) (2003) 241–272.[12] Ming-Wei Chang, Lev-Arie Ratinov, Nicholas Rizzolo, Dan Roth, Learning and inference with constraints, in: Proceedings of the Twenty-Third AAAIConference on Artificial Intelligence, AAAI Press, 2008, pp. 1513–1518.[13] Hong Cheng, Yan Xifeng, Jiawei Han, Chih-Wei Hsu, Discriminative frequent pattern analysis for effective classification, in: Proceedings of the 23rdInternational Conference on Data Engineering, IEEE, 2007, pp. 716–725.[14] Hong Cheng, Yan Xifeng, Jiawei Han, P.S. Yu, Direct discriminative pattern mining for effective classification, in: Proceedings of the 24th InternationalConference on Data Engineering, IEEE, 2008, pp. 169–178.[15] James Cussens, Bayesian network learning by compiling to weighted max-sat, in: Proceedings of the 24th Conference in Uncertainty in ArtificialIntelligence, AUAI Press, 2008, pp. 105–112.1982T. Guns et al. / Artificial Intelligence 175 (2011) 1951–1983[16] Luc De Raedt, Tias Guns, Siegfried Nijssen, Constraint programming for itemset mining, in: Proceeding of the 14th ACM SIGKDD International Confer-ence on Knowledge Discovery and Data Mining, ACM, 2008, pp. 204–212.[17] Luc De Raedt, Tias Guns, Siegfried Nijssen, Constraint programming for data mining and machine learning, in: Proceedings of the Twenty-Fourth AAAIConference on Artificial Intelligence, AAAI Press, 2010, pp. 1513–1518.[18] Luc De Raedt, Stefan Kramer, The levelwise version space algorithm and its application to molecular fragment finding, in: Proceedings of the Seven-teenth International Joint Conference on Artificial Intelligence, Morgan Kaufmann, 2001, pp. 853–862.[19] Luc De Raedt, Albrecht Zimmermann, Constraint-based pattern set mining, in: Proceedings of the Seventh SIAM International Conference on DataMining, SIAM, 2007, pp. 1–12.[20] Guozhu Dong, Jinyan Li, Efficient mining of emerging patterns: discovering trends and differences, in: Proceedings of the Fifth International Conferenceon Knowledge Discovery and Data Mining, ACM Press, 1999, pp. 43–52.[21] Wei Fan, Kun Zhang, Hong Cheng, Jing Gao, Yan Xifeng, Jiawei Han, Philip S. Yu, Olivier Verscheure, Direct mining of discriminative and essentialfrequent patterns via model-based search tree, in: Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and DataMining, ACM, 2008, pp. 230–238.[22] Alan M. Frisch, Matthew Grum, Christopher Jefferson, Bernadette Martínez Hernández, Ian Miguel, The design of essence: a constraint languagefor specifying combinatorial problems, in: Proceedings of the 20th International Joint Conference on Artificial Intelligence, Morgan Kaufmann, 2007,pp. 80–87.[23] Johannes Fürnkranz, Peter A. Flach, ROC ‘n’ rule learning – towards a better understanding of covering algorithms, Machine Learning 58 (1) (2005)39–77.[24] Bernhard Ganter, Gerd Stumme, Rudolf Wille (Eds.), Formal Concept Analysis, Foundations and Applications, Lecture Notes in Computer Science,vol. 3626, Springer, 2005.[25] Gecode Team, http://www.gecode.org.[26] Ian P. Gent, Christopher Jefferson, Ian Miguel, MINION: a fast scalable constraint solver, in: Proceeding of the 17th European Conference on ArtificialIntelligence, IOS Press, 2006, pp. 98–102.[27] Bart Goethals, Mohammed J. Zaki, Advances in frequent itemset mining implementations: report on FIMI’03, in: SIGKDD Explorations Newsletter, vol. 6,2004, pp. 109–117.[28] Henrik Grosskreutz, Stefan Rüping, Stefan Wrobel, Tight optimistic estimates for fast subgroup discovery, in: Machine Learning and Knowledge Discov-ery in Databases, in: Lecture Notes in Computer Science, vol. 5211, Springer, 2008, pp. 440–456.[29] Tias Guns, Siegfried Nijssen, Luc De Raedt, k-Pattern set mining under constraints, CW Reports CW596, Department of Computer Science, K.U. Leuven,October 2010.[30] J. Han, J. Pei, Y. Yin, Mining frequent patterns without candidate generation, in: Proceedings of the ACM SIGMOD International Conference on Manage-ment of Data, ACM Press, 2000, pp. 1–12.[31] Jiawei Han, Hong Cheng, Dong Xin, Xifeng Yan, Frequent pattern mining: current status and future directions, Data Mining and Knowledge Discov-ery 15 (1) (2007) 55–86.[32] Branko Kavsek, Nada Lavrac, Viktor Jovanoski, APRIORI-SD: adapting association rule learning to subgroup discovery, in: Advances in Intelligent DataAnalysis, in: Lecture Notes in Computer Science, vol. 2810, Springer, 2003, pp. 230–241.[33] Heikki Mannila, Hannu Toivonen, Levelwise search and borders of theories in knowledge discovery, Data Mining and Knowledge Discovery 1 (3) (1997)241–258.[34] Yasuhiko Morimoto, Takeshi Fukuda, Hirofumi Matsuzawa, Takeshi Tokuyama, Kunikazu Yoda, Algorithms for mining association rules for binarysegmentations of huge categorical databases, in: Proceedings of 24rd International Conference on Very Large Data Bases, Morgan Kaufmann, 1998,pp. 380–391.[35] Shinichi Morishita, Jun Sese, Traversing itemset lattice with statistical metric pruning, in: Proceedings of the Nineteenth ACM SIGMOD-SIGACT-SIGARTSymposium on Principles of Database Systems, ACM, 2000, pp. 226–236.[36] Siegfried Nijssen, Élisa Fromont, Optimal constraint-based decision tree induction from itemset lattices, Data Mining and Knowledge Discovery 21 (1)(2010) 9–51.[37] Siegfried Nijssen, Tias Guns, Integrating constraint programming and itemset mining, in: Machine Learning and Knowledge Discovery in Databases, in:Lecture Notes in Computer Science, vol. 6322, Springer, 2010, pp. 467–482.[38] Siegfried Nijssen, Tias Guns, Luc De Raedt, Correlated itemset mining in ROC space: a constraint programming approach, in: Proceedings of the 15thACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2009, pp. 647–656.[39] Siegfried Nijssen, Joost N. Kok, Multi-class correlated pattern mining, in: Knowledge Discovery in Inductive Databases, in: Lecture Notes in ComputerScience, vol. 3933, Springer, 2005, pp. 165–187.[40] Nicolas Pasquier, Yves Bastide, Rafik Taouil, Lotfi Lakhal, Discovering frequent closed itemsets for association rules, in: Database Theory, in: LectureNotes in Computer Science, vol. 1540, Springer, 1999, pp. 398–416.[41] Jian Pei, Jiawei Han, Can we push more constraints into frequent pattern mining? in: Proceedings of the sixth ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining, ACM, 2000, pp. 350–354.[42] Jian Pei, Jiawei Han, Laks V.S. Lakshmanan, Mining frequent item sets with convertible constraints, in: Proceedings of the IEEE International Conferenceon Data Engineering, IEEE, 2001, pp. 433–442.[43] Jian Pei, Jiawei Han, Runying Mao, Closet: an efficient algorithm for mining frequent closed itemsets, in: ACM SIGMOD Workshop on Research Issuesin Data Mining and Knowledge Discovery, ACM, 2000, pp. 21–30.[44] Laurent Perron, Search procedures and parallelism in constraint programming, in: Principles and Practice of Constraint Programming, in: Lecture Notesin Computer Science, vol. 1713, Springer, 1999, pp. 346–360.[45] Francesca Rossi, Peter van Beek, Toby Walsh, Handbook of Constraint Programming (Foundations of Artificial Intelligence), Elsevier Science Inc., 2006.[46] Christian Schulte, Programming Constraint Services: High-Level Programming of Standard and New Constraint Services, Lecture Notes in ComputerScience, vol. 2302, Springer, 2002.[47] Christian Schulte, Peter J. Stuckey, Efficient constraint propagation engines, Transactions on Programming Languages and Systems 31 (1) (2008) 1–43.[48] Jun Sese, Shinichi Morishita, Answering the most correlated n association rules efficiently, in: Principles of Data Mining and Knowledge Discovery, in:Lecture Notes in Computer Science, vol. 2431, Springer, 2002, pp. 410–422.[49] Pradeep Shenoy, Jayant R. Haritsa, S. Sudarshan, Gaurav Bhalotia, Mayank Bawa, Shah Devavrat, Turbo-charging vertical mining of large databases, in:Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, ACM, 2000, pp. 22–33.[50] Arnaud Soulet, Bruno Crømilleux, An efficient framework for mining flexible constraints, in: Advances in Knowledge Discovery and Data Mining, in:Lecture Notes in Computer Science, vol. 3518, Springer, 2005, pp. 43–64.[51] Takeaki Uno, Masashi Kiyomi, Hiroki Arimura, LCM ver.3: collaboration of array, bitmap and prefix tree for frequent itemset mining, in: Proceedings ofthe 1st International Workshop on Open Source Data Mining, ACM, 2005, pp. 77–86.[52] Pascal Van Hentenryck, Yves Deville, in: The Cardinality Operator: A New Logical Connective for Constraint Logic Programming, MIT Press, Cambridge,MA, USA, 1993, pp. 383–403.T. Guns et al. / Artificial Intelligence 175 (2011) 1951–19831983[53] Pascal Van Hentenryck, Laurent Perron, Jean-Francois Puget, Search and strategies in OPL, ACM Transations Computational Logic 1 (2) (2000) 285–320.[54] Pascal Van Hentenryck, Vijay A. Saraswat, Yves Deville, Design, implementation, and evaluation of the constraint language cc(FD), Journal of LogicProgramming 37 (1–3) (1998) 139–164.[55] Stefan Wrobel, An algorithm for multi-relational discovery of subgroups, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes inComputer Science, vol. 1263, Springer, 1997, pp. 78–87.[56] Mohammed Javeed Zaki, Karam Gouda, Fast vertical mining using diffsets, in: Proceedings of the Ninth ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, ACM, 2003, pp. 326–335.