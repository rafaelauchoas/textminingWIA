0202guA71]VC.sc[2v07190.6081:viXraFUSION OF COMPLEX NETWORKS AND RANDOMIZED NEURALNETWORKS FOR TEXTURE ANALYSISLucas C. Ribas1, Jarbas Joaci de Mesquita Sá Junior3, Leonardo F. S. Scabini2, and Odemir M. Bruno1,21São Carlos Institute of Physics, University of São Paulo (USP), PO Box 369, 13560-970, São Carlos, SP, Brazil.Scientific Computing Group2Institute of Mathematics and Computer Science, University of São Paulo (USP), USP, Avenida Trabalhador são-carlense, 400,13566-590, São Carlos, SP, Brazil.3Curso de Engenharia da Computação, Programa de Pós-Grad. em Eng. Elétrica e de Computação, Campus de Sobral, UniversidadeFederal do Ceará, Rua Coronel Estanislau Frota, 563, Centro, Sobral, Ceará, CEP: 62010-560, BrasilABSTRACTThis paper presents a high discriminative texture analysis method based on the fusion of complexnetworks and randomized neural networks. In this approach, the input image is modeled as a complexnetworks and its topological properties as well as the image pixels are used to train randomized neuralnetworks in order to create a signature that represents the deep characteristics of the texture. Theresults obtained surpassed the accuracy of many methods available in the literature. This performancedemonstrates that our proposed approach opens a promising source of research, which consists ofexploring the synergy of neural networks and complex networks in the texture analysis field.1IntroductionMost of computer vision applications consider texture as a key factor to image discrimination, thus texture analysis hasbeen a constant research field since the 1960s. The texture is a visual pattern related to the object surface, which in animage is represented by the pixel spatial organization. However, the interpretation of texture is ambiguous, thus there isno formal definition to the term that is widely accepted by the scientific community. This resulted in an extensive andheterogeneous literature of texture analysis methods proposed along the years [8, 39, 65]. Usually, texture descriptorsare applied in different areas such as industrial inspection [41], geology [62], medicine [14], material science [68], andso on.Classical texture analysis techniques can be grouped into four different approaches: statistical, spectral, structural,model-based methods [66]. The earlier and most diffused methods are statistical-based, such as variants of gray-level co-occurrence matrices (GLCM) [31, 49] and local binary patterns (LBP) [10, 45]. Spectral methods exploretexture in the frequency domain, some examples are Gabor filters [34] and wavelet transform [21]. On the otherhand, structural methods consider texture as a combination of smaller elements, called textons, that compose theoverall texture as a spatially organized pattern. A common approach of this kind of analysis is the Morphologicaldecomposition [37]. Finally, model-based methods represent textures through sophisticated mathematical models andestimating its parameters. Common methods of this category include Fractal models [2–4, 12, 23, 56] and stochasticmodels [50]Besides classical methods, more recent and innovative techniques are addressing texture differently, achieving promisingresults. An example is the set of techniques that use learning, such as descriptors based on a vocabulary of scaleinvariant feature transform (SIFT) [17], often called bag-of-visual-words (BOVW). Methods based on image complexityanalysis are also gaining attention such as cellular automata [19] and complex networks (CN) [4, 26, 57, 58, 64]. Inparticular, methods based on the CN theory have achieved promising results due to its capacity to represent the relationamong structural elements of texture. However, the problem of how to achieve more satisfactory modeling (i.e., a lessernumber of parameters) and new ways of characterizing the network remains a challenge to overcome.In this paper, we propose a novel approach that combines complex networks and randomized neural networks (RNN)in order to obtain a texture signature. Complex networks is attracting increasing attention due to its flexibility andgenerality for representing many real-world systems, including texture images. On the other hand, a randomized neural   network is a neural network with a unique hidden layer and a very fast learning algorithm, which has been used in manypattern recognition tasks. Here we first model the texture image as a directed network, representing the informationabout the pixels and its neighbors as vertices and edges. To characterize the texture, the topological properties from themodeled network and the image pixels are used to train a randomized neural network, and the set of output weightsis used as a feature vector that represents discriminative characteristics of the texture. Experimental results on fourdatabases demonstrated a better performance of the proposed method when compared to other methods of the literature.The remainder of this paper is organized as follows. Section 2 describes the fundamentals of complex networks andrandomized neural networks. A novel method for texture classification based on fusion of complex networks andrandomized neural networks is presented in Section 3. Section 4 describes the databases and experiments performed toevaluate the proposed method. The discussion about the results achieved and comparisons are presented in Section 5.Finally, in Section 6, we conclude the work with some remarks.2 Background2.1 Complex networksAlmost any natural phenomena can be modeled as networks by defining a set of entities and establishing a criterion ofrelation between them. Some classical examples are the internet, composed of various connected computers and routers,and a network of a cell, describing chemicals connected by chemical reactions. Complex networks are part of an areaknown as network science [6]. Network science is strongly based on graph theory. In the last decades, works haveshown patterns present in many networks or graphs, which were then understood as a structural characteristic of somemodels such as the scale-free [7] and the small-world [61]. These findings have caused increasing interest from thescientific community on the study of complex networks, creating a new multidisciplinary research field.The theoretical foundations of this area arise from of the intersection of the graph theory, physics, mathematics, statisticsand computer science. Therefore, CN has been employed as a powerful tool for pattern recognition [43], where naturalsystems of many areas are modeled as networks and then quantified through its topological structure. CN applicationsare found in various areas of science, such as, physics, social sciences, biology, mathematics, ecology, medicine,computer science, linguistic, neuroscience, among others [18].v1, ..., vn{Formally, a network or graph G is described by a tuple of vertices and edges (V, E). Let vi be a vertex of the setV =. An edge evi,vj represents a connection between two vertices vi and vj, so the set E =is}composed of all edges connecting vertices of V . The network can also be directed, in this case, the edges evi,vj havea direction from vi to vj. In most of the CN applications, the first step is to define how to model the target problemas a network, thus defining what are the vertices and what are the edges. Once G is properly built, many measurescan be computed to quantify its structure, varying from centrality, path-based measures, community structure, andmany more [15]. Moreover, the structure of a real network is the result of the continuous evolution of the forces thatformed it, and certainly affects the function of the system [9]. Therefore, the network dynamics can be analyzed by thecharacterization of its structural evolution in function of time or some modeling parameter.evi,vj , ...}{2.2 Randomized neural networksRandomized neural networks [32, 51, 52, 59] are neural networks composed of two neuron layers (hidden and outputlayer), each one with a different role in the regression/classification task. The hidden layer has its neural weightsdetermined randomly according to a probability distribution (for instance, a uniform or normal distribution). Its purposeis to project non-linearly the input data in another dimensional space where it is more likely that the feature vectors arelinearly separable, as stated in Cover’s theorem [16]. In turn, the output layer aims to linearly separate these projectedfeature vectors using the least-squares method.Mathematically, letting X = [ (cid:126)x1, (cid:126)x2, . . . , (cid:126)xN ] be a matrix of input feature vectors (including +1 for bias weight) andD = [ (cid:126)d1, (cid:126)d2, . . . , (cid:126)dN ] be the corresponding labels, the first step is to build the matrix of hidden neuron weights W ofdimensions Q(p + 1), where Q and p are the number of hidden neurons and the number of attributes in each inputfeature vector, respectively.×Next, the output of the hidden layer for all the feature vectors (cid:126)xi (i1, . . . , N ) can be obtained by Z = φ(W X), whereφ(.) is generally a sigmoid or hyperbolic tangent function. Thus, this matrix of projected vectors Z = [ (cid:126)z1, (cid:126)z2, . . . , (cid:126)zN ](including +1 for bias weight) can be used to compute the output neuron weights, according to the following equation∈M = DZ T (ZZ T )−1,(1)2where Z T (ZZ T )−1 is the Moore-Penrose pseudo-inverse [44, 54].Sometimes, the matrix ZZ T becomes singular (that is, without inverse), or close to singular, which results in unstableresults in Equation 1. In order to avoid these drawbacks, it is possible to use the Tikhonov regularization [13, 60],according toM = DZ T (ZZ T + λI)−1,(2)where 0 < λ < 1 and I is the identity matrix.3 Proposed methodIn this section, we describe the proposed method that combines a new texture modeling in complex networks andrandomized neural networks for texture characterization.3.1 Modeling texture as directed CNLet I be an image composed of pixels i, which have as Cartesian coordinates xi and yi. In gray-scale images, each[0, L], where L is the highest gray-level value. To model apixel has an intensity represented by an integer value I(i)∈V of a network R. The set of edges E istexture image as a directed network, each pixel i is mapped as a vertex vibuilt connecting two vertices vi and vj, which represent two pixels i and j, by a directed edge from vi to vj, evi,vj ∈E,if the Euclidean distance between them is less than or equal to a radius r and I(i) (cid:54) I(j), according to∈where dist(vi, vj) =w(evi,vj ) defined as(xi−(cid:112)dist(vi, vj) (cid:54) rE|∧I(i) < I(j),(3)xj)2 + (yiyj)2 is the Euclidean distance between two pixels. Each edge has a weight(cid:9)E =evi,vj ∈(cid:8)−w(evi,vj ) =(cid:40)|I(i)−I(j)|L(cid:17),+( |I(i)−I(j)|L(cid:16) dist(vi,vj )−1r−12If r = 1), Otherwise.(4)It is worth mentioning that the direction of the edges is determined by the pixel gray-levels. In other words, an edgepoints to the vertex that represents a pixel with greater intensity. If both intensities are equal, the edge is bidirectional.It is also important to stress that the value r is the unique parameter of modeling and determines the size of theneighborhood of each vertex. Thus, as r increases, the reach of connection and the degree of vertices increase as well.This procedure makes the analysis of the behavior of this evolution an interesting way of studying these networks.Figure 1 shows the modeling of a texture image as a directed network for different values of r.3.2 Proposed signature based on RNNOur method aims to use as texture signature the weights of the output layer of the RNN trained with information fromthe modeled complex networks. For this, three sources of information are considered for each vertex: out-degree,weighted out-degree, and weighted in-degree. As the out-degree is directly related to the in-degree in the modelednetworks (i.e. the sum of these two degrees is equal in all the vertices) and, therefore, provide the same information, weconsidered only the out-degree.The out-degree kvi of a vertex vi represents the number of out-edges connected to other vertices,kvi =vj ∈V (cid:26)(cid:88)1,0,Eevi,vj ∈otherwise.(5)On the other hand, the weighted out-degree ksvi is given by the sum of the weights of the out-degree edges of a vertexvi,ksvi =vj ∈V (cid:26)(cid:88)w(evi,vj ),0,Eevi,vj ∈otherwise.3(6)(a) Texture image(b) r = 1(c) r = 2Figure 1: Examples of a texture image modeled as a directed complex network.Finally, the weighted in-degree kevi is defined as the sum of the weights of the in-degree edges in vi,kevi =vj ∈V (cid:26)(cid:88)w(evj ,vi),0,evj ,vi ∈Eotherwise.(7)To build a matrix of input vector for the RNN, we adopted a strategy of analysis of the evolution of the complex networkfor different values of the modeling parameter r. In this way, the input feature vector and the corresponding labelof a vertex vi are built according to the following procedure: the gray-scale intensity of the pixel is considered asan output label dvi = I(i) and the values of out-degree of the vertex for different values of the modeling parameterr are attributes of the input feature vector (cid:126)xvi = [k1], where R is the maximum values of the modelingviparameter. A matrix of input feature vectors X(k) and a matrix of output labels D are then built for all the verticesof the complex network. Thus, it is possible to analyze the evolution of the topology of vertices that represent pixelsthat have a determined gray-scale intensity. Figure 2(a) shows an example of how to build these matrices X and D. Inaddition to building the matrix of input feature vectors X(k) for the out-degree, we also built matrices of input vectorsfor the weighted out-degree X(ks) and for the weighted in-degree X(ke).The next step is to define the weights of the matrix W of the hidden layer of the RNN. In general, these weights aredetermined randomly in each training stage. Nevertheless, because we want our method to provide the same signaturefor the same texture image, it is necessary to use the same values in the matrix W . Thus, we adopted the strategyproposed in [20] and used the Linear Congruent Generator (LCG) [38, 53] in order to obtain pseudo-random uniformvalues for the matrix W , according to the following equation, ..., kRvi, k2viV (n + 1) = (aV (n) + b) mod c,(8)(p + 1), itswhere V is a random numeric sequence and a, b and c are parameters. The sequence V has length E = Qfirst value is V (1) = E + 1, and the values of the parameters are a = E + 2, b = E + 3 and c = E2 (values adoptedin [20]). Hence, the matrix W is composed of the vector V divided into Q segments of length p + 1. Finally, all valuesof matrix W and each line of the matrix X are normalized using z-score (zero mean and unit variance).∗∗4Figure 2: Building of an input feature vector and corresponding output label for the out-degree kvi using differentvalues of r to model the complex networks.The proposed texture signature is built based on the matrix M , which becomes a vector (cid:126)f = DZ T (ZZ T +λI)−1, whereλ = 10−3 (Figure 2(b)). Notice that (cid:126)f has length Q + 1 due to the bias weight. Thus, the first step is to concatenate thevectors (cid:126)f obtained from RNNs trained with the three matrices of input data X(k), X(ks), X(ke), according to(cid:126)Υ(Q)R =(cid:126)fk, (cid:126)fks, (cid:126)fke,(9)where Q is the number of neurons of the hidden layer and R is the maximum radius for building the complex network.The vector (cid:126)Υ(Q)R is built using a single value of Q and R. These two parameters influence the weights of the neuralnetwork and, therefore, provide different characteristics for different values. Thus, initially we propose a vector(cid:126)Θ(R)(Q1,Q2,Qm) that concatenates the vectors (cid:126)Υ(Q)R for different values of Q,(cid:104)(cid:105)(cid:126)Θ(R)Q1,Q2,...,Qm =(cid:126)Υ(Q1)R, (cid:126)Υ(Q2)R, ..., (cid:126)Υ(Qm)R(cid:104)(cid:105).(10)Finally, we propose a feature vector (cid:126)Ψ(R1, R2)Q1,Q2,...,Qm that concatenates the vector (cid:126)Θ(R)Q1,Q2,...,Qm for twovalues of R,(cid:126)Ψ(R1, R2)Q1,Q2,...,Qm =(cid:126)Θ(R1)Q1,Q2,...,Qm, (cid:126)Θ(R2)Q1,Q2,...,Qm(cid:104)(cid:105).(11)4 ExperimentsIn order to validate our proposed method and compare it to other texture analysis methods, the signatures wereclassified using linear discriminant analysis [24]. This classifier was adopted due to its simplicity, which emphasizesthe characteristics obtained by the methods. The leave-one-out cross-validation scheme was used. In this validationstrategy, one sample is used for testing the model and the remainder for training it. This process is repeated N times(N is the number of samples), each time with a different sample for testing. The performance measure is the averageaccuracy of the N runnings.The gray-scale texture databases used as benchmark to evaluate our proposed method were:Brodatz [11]: just as in [5], 1776 texture images of 128classes were used in this work.×•128 pixel size from this database divided into 1115...viviviD=!I(i)......···..."X(k)=⎡⎢⎢⎢⎢⎣k1vik2vi.........···...kRvi⎤⎥⎥⎥⎥⎦⃗fk=DZT(ZZT+λI)−1(a)(b)123x2x1x3xRQWM11M13M12M1QInputHidden  Layer Output•••×Outex [46]: just as in [5], the original 68 images 746sub-images 128538 from TC_Outex_00013 were divided into 20128 pixel size without overlapping. Thus, the database used in this work has 1360 textures.××128 pixel size.USPTex [4]: this database has 2292 samples divided into 191 classes, 12 images per class, and each image has128Vistex: the database Vision Texture is provided by the Vision and Modeling Group - MIT Media Lab [55].Just as in [5], the original 54 images 512128 pixel size withoutoverlapping. Thus, the database used in this work has 864 images.512 were split into 16 sub-images 128××The proposed method is applied to the aforementioned databases and the accuracy is compared to other methodsof the literature. They are: Grey-Level Co-occurrence Matrix (GLCM) [30, 31], Gray Level Difference Matrix(GLDM) [36, 63], Fourier [1], Gabor Filters [33, 42], Fractal [3], Fractal Fourier [22], Local Binary Patterns (LBP) [47],Local Binary Patterns Variance (LBPV) [29], Complete Local Binary Pattern (CLBP) [28], Local Phase Quantization(LPQ) [48], Local Configuration Pattern (LCP) [27], Local Frequency Descriptor (LFD) [40], Binarized StatisticalImage Features (BSIF) [35], Local Oriented Statistics Information Booster (LOSIB) [25], Adaptive Hybrid Pattern(AHP) [67], Complex Network Texture Descriptors (CNTD) [5] and ELM signature [20].5 Results and Discussion5.1 Parameter Evaluation∈ {Figure 5.1 shows the accuracies achieved on the four databases with the feature vector (cid:126)Υ(Q)R using R = 4. In thisexperiment, we used different values of Q04, 09, 14, 19, 29, 39, which were selected because they produce a}number of features that is multiple of five for each feature vector considered. As can be seen in the figure, the successrates increase as we increase the value of Q. This increase is followed by an increase in the number of features used.The best accuracies are obtained using Q = 14 on the Vistex database and Q = 19 on the other databases. These valuesof Q produce feature vectors of size 45 (Q = 14) and 60 (Q = 19). Furthermore, the success rate stabilizes when weuse values of Q larger than Q = 14 on the Vistex database and values larger than Q = 19 on the other databases.Table 1 shows the accuracies obtained on the four databases using the feature vector (cid:126)Θ(R)Q1,Q2,...,Qm with R = 4.The results show that as the values of Q and its combinations increase (i.e. the number of features increases), thesuccess rates increase as well. However, very large feature vectors do not assure the highest performance, once thesuccess rates tend to stabilize at a determined value. For instance, if we compare the vector (cid:126)Θ(04)19,29,39, which has270 features, with the vector (cid:126)Θ(04)04,09,14, which has 90 attributes, the former has a lower performance in all thedatabases. This suggests that the proposed signature reaches its limit in terms of discrimination. Thus, we consideredthe vectors (cid:126)Θ(04)04,09,14 and (cid:126)Θ(04)04,14,19, since they presented a good trade-off between high accuracy and a smallnumber of features.Figure 3: Accuracies of the feature vector (cid:126)Υ(Q)R using different values of Q on the four databases.We also evaluated the feature vector (cid:126)Θ(R)Q1,Q2,...,Qm for different values of R. Figure 4 shows the accuracies yieldedconsidering the combinations Q =. The value of maximum radius R is associated{to the zone of connection between the pixels (i.e., vertices). Thus, lower values of R represent the closest pixels and,04, 14, 19}04, 09, 14}and Q ={6491419242934398486889092949698Accuracy (%)VistexOutexBrodatzUSPTexTable 1: Accuracies of the feature vector (cid:126)Θ(R)Q1,Q2,...,Qm using different values of Q and their combinations for themaximum radius R = 4.}Q1, Q2, ..., Qm{{04, 09}{04, 14}{04, 19}{04, 29}{04, 39}{09, 14}{09, 19}{09, 29}{09, 39}{14, 19}{14, 29}{14, 39}{19, 29}{19, 39}{29, 39}{04, 09, 14}{04, 09, 19}{04, 09, 29}{04, 09, 39}{04, 14, 19}{04, 14, 29}{04, 14, 39}{04, 19, 29}{04, 19, 39}{04, 29, 39}{09, 14, 19}{09, 14, 29}{09, 14, 39}{09, 19, 29}{09, 19, 39}{09, 29, 39}{14, 19, 29}{14, 19, 39}{14, 29, 39}{19, 29, 39}No of features Outex USPTex Brodatz Vistex97.2298.5097.9297.8098.1597.9298.1597.8097.8098.2697.9298.1597.4598.0397.9298.6198.2697.8097.8098.3898.2698.8498.0398.3898.3898.5098.1598.7398.2697.9297.9298.5098.6198.5097.9293.0293.8694.8894.4895.2793.5294.2694.1494.8894.5494.4895.3394.7695.2795.0595.0595.1694.8895.7295.2195.0595.8995.2795.6195.7294.9995.1095.6194.9395.0594.9995.0595.4495.5095.5094.0794.9895.4694.7695.4294.7294.4294.2494.8194.9494.9095.2994.5995.0394.3795.4695.5595.5195.9095.9495.5995.9095.9495.9095.0395.2495.4295.6895.2495.3894.5995.3395.7295.0394.6388.6088.8288.9788.3887.5788.0988.6087.5086.7689.0488.0987.6587.5087.2885.8889.3489.7188.6887.9489.4188.6888.5389.1288.6887.9489.5688.7588.0188.7587.9488.0188.7588.3888.0988.01456075105135759012015010513516515018021090105135165120150180165195225135165195180210240195225255270as R increases, the reach of connection increases as well. The results show that the lowest values of R provide betteraccuracies when compared to the highest values. This demonstrates that local patterns are more important than globalpatterns to discriminate the textures.Furthermore, we analyzed the combination of vectors (cid:126)Θ(R)Q1,Q2,...,Qm (showed in Table 1) with different values ofmaximum radius R, resulting in the vector (cid:126)Ψ(R1, R2)Q1,Q2,...,Qm. To compute this vector, we used the combinationsof Q that provided the best results in Table 1: Q =. In this experiment, we computedthe vector (cid:126)Ψ(R1, R2)Q1,Q2,...,Qm for two values of R (i.e, up to two combinations of (cid:126)Θ(R)Q1,Q2,...,Qm ) due to thelarge number of features generated.Table 2 shows the results of the vectors (cid:126)Ψ(R1, R2)Q1,Q2,...,Qm using the combination Q =. The highestaccuracy was provided by the vector (cid:126)Ψ(04, 06)04,09,14. The results of the vectors (cid:126)Ψ(R1, R2)Q1,Q2,...,Qm built withthe combination Q =are shown in Table 3. In this experiment, the best results were obtained by usingthe vector (cid:126)Ψ(04, 10)04,14,19. Tables 2 and 3 also show that by combining the vector (cid:126)Θ(R)Q1,Q2,...,Qm with differentvalues of R, the accuracy increases approximately 1% on the databases. However, in the two cases, the combinationsof high values of R provide inferior results. Even though the best results of the two Tables ((cid:126)Ψ(04, 06)04,09,14 and(cid:126)Ψ(04, 10)04,14,19) are similar, notice that the vector (cid:126)Ψ(04, 10)04,14,19 has a number of features larger than the vector(cid:126)Ψ(04, 06)04,09,14.04, 14, 19}04, 09, 14}4, 14, 19}{4, 9, 14{and Q ={{}7(a) Q = {04, 09, 14}(b) Q = {04, 14, 19}Figure 4: Accuracies using the feature vector (cid:126)Θ(R)Q1,Q2,...,Qm for the two better set of Q with different values ofmaximum radius R.Table 2: Accuracies using different sets of radii R and Q =04, 09, 14}.{R1, R2}{{04, 06}{04, 08}{04, 10}{04, 12}{06, 08}{06, 10}{06, 12}{08, 10}{08, 12}{10, 12}No of features Outex USPTex Brodatz Vistex98.7398.2698.8498.2698.6198.5098.3898.4998.1498.3891.5491.4791.4791.6991.5490.7490.4490.5890.2990.5996.6496.2596.5596.2596.4796.2596.3896.0395.7295.4696.1195,8895.8395.7295.7795.8395.8395.1595.2194.93180180180180180180180180180180Table 3: Accuracies using different sets of radii R and Q =4, 14, 19{.}R1, R2}{{04, 06}{04, 08}{04, 10}{04, 12}{06, 08}{06, 10}{06, 12}{08, 10}{08, 12}{10, 12}No of features Outex USPTex Brodatz Vistex99.1998.9599.1998.6198.6198.5098.2698.3798.0398.0396.7396.6896.9596.6496.2996.6096.5595.9495.9496.0390.0791.1791.3291.7690.1489.6390.2990.0091.3290.5195.8396.0596.0696.1196.3995.9596.0695.7295.7795.052402402402402402402402402402405.2 Comparison with other methodsTo evaluate the results obtained by our proposed method, we performed comparisons with methods present in theliterature. The experimental setup used was the same for all the methods (LDA with leave-one-out), except forCLBP, which used the classifier 1-Nearest Neighborhood (1-NN) with distance chi-square, according to the originalpaper. For our method, we adopted the two texture signatures that obtained the best results in the previous analysis:(cid:126)Ψ(04, 06)04,09,14 and (cid:126)Ψ(04, 10)04,14,19.Table 4 presents the results obtained by all the methods in the four image databases evaluated. The results show thatour proposed method obtained the best results when compared to the other methods using both signatures. Also, it84681012888990919293949596979899Accuracy (%)VistexOutexBrodatzUSPTex4681012888990919293949596979899Accuracy (%)VistexOutexBrodatzUSPTexTable 4: Comparison of accuracies of different texture analysis methods in four texture databases. A subset of thecompared results is present in [20].MethodsGLCM [30]GLDM [63]Gabor Filters [42]Fourier [63]Fractal [3]Fractal Fourier [22]LBP [47]LBPV [29]CLBP [28]AHP [67]BSIF [35]LCP [27]LFD [40]LPQ [48]ELM Signature [20]CNTD [5](cid:126)Θ(04)04,09,14(cid:126)Ψ(04, 06)04,09,14(cid:126)Ψ(04, 10)04,14,19No of features Outex USPTex Brodatz Vistex92.2497.1193.2979.5191.6779.7597.9288.6598.0398.3888.6694.4494.6892.4898.1598.0398.6198.7399.1983.6492.0689.2267.5078.2759.4785.4354.9791.1494.8577.6691.1483.5585.1295.1191.7195.4696.6496.9580.7386.7681.9181.9180.5168.3881.1075.6685.8088.3177.4386.2582.5779.4189.7186.7689.3491.5491.3290.4394.4389.8675.9087.1671.9693.6486.2695.3294.8891.4493.4790.9992.5195.2795.2795.0596.1196.062460486369682565556481202568127625618010890180240is important to stress that our method reached higher accuracies than the ELM signature and CNDT method (whichis also based on complex networks). This suggests that our method obtained superior performance because it hassimultaneously the main characteristics of both compared methods. In other words, the ELM signature uses only pixelintensities to train the neural network, without any valuable information from complex network modeling, and theCNTD method models images as complex networks and computes only traditional measures, without using a neuralnetwork to extract the deep characteristics from these complex networks.Even though our proposed method has signatures with a larger number of descriptors when compared to some methodsof the literature, it is important to emphasize that, if we consider only the vector (cid:126)Θ(R)Q1,Q2,...,Qm, the results are stillcompetitive. For instance, the vector (cid:126)Θ(04)4,9,14, which has only 90 features, provides superior performance on theVistex and USPTex databases. In the remainder databases, the results are very close to the highest accuracies (only0.37% smaller than the result of ELM signature on the Outex database and 0.27% smaller than the accuracy of theCLBP on the Brodatz database)6 ConclusionThis paper presented an innovative approach of texture feature extraction based on the fusion of complex network andrandomized neural network. In the proposed method, a new approach to model the image as a CN that uses only aparameter is presented. We also proposed a new way of characterizing the CN based on the idea of using the outputweights of a randomized neural network trained with topological properties of the CN. The obtained classificationresults on four databases outperformed other texture literature methods. Also, the proposed approach has an excellenttrade-off between performance and size of the feature vectors. This demonstrates that the proposed approach ishighly discriminative using the three feature vectors considered. In this way, this paper shows that the fusion ofcomplex network and randomized neural network is a research field with great potential as a feasible texture analysismethodology.AcknowledgmentsLucas Correia Ribas gratefully acknowledges the financial support grant #2016/23763-8, São Paulo Research Foundation(FAPESP). Jarbas Joaci de Mesquita Sá Junior thanks CNPq (National Council for Scientific and TechnologicalDevelopment, Brazil) (Grant: 302183/2017-5) for the financial support of this work. Leonardo Felipe dos SantosScabini acknowledges support from CNPq (Grant #134558/2016-2). Odemir M. Bruno thanks the financial support ofCNPq (Grant # 307797/2014-7) and FAPESP (Grant #s 14/08026-1 and 16/18809-9).9References[1] AZENCOTT, R., WANG, J.-P., AND YOUNES, L. Texture classification using windowed Fourier filters. IEEETransactions on Pattern Analysis and Machine Intelligence 19, 2 (1997), 148–153.[2] BACKES, A. R., AND BRUNO, O. M. A new approach to estimate fractal dimension of texture images. InInternational Conference on Image and Signal Processing (2008), Springer, pp. 136–143.[3] BACKES, A. R., CASANOVA, D., AND BRUNO, O. M. Plant leaf identification based on volumetric fractaldimension. International Journal of Pattern Recognition and Artificial Intelligence 23, 06 (2009), 1145–1160.[4] BACKES, A. R., CASANOVA, D., AND BRUNO, O. M. Color texture analysis based on fractal descriptors.Pattern Recognition 45, 5 (2012), 1984–1992.[5] BACKES, A. R., CASANOVA, D., AND BRUNO, O. M. Texture analysis and classification: A complex network-based approach. Information Sciences 219 (2013), 168–180.[6] BARABÁSI, A. L. Network science. Cambridge university press, 2016.[7] BARABÁSI, A.-L., AND ALBERT, R. Emergence of scaling in random networks. science 286, 5439 (1999),509–512.[8] BHARATI, M. H., LIU, J., AND MACGREGOR, J. F.Image texture analysis: methods and comparisons.Chemometrics and Intelligent Laboratory Systems 72, 1 (2004), 57 – 71.[9] BOCCALETTI, S., LATORA, V., MORENO, Y., CHAVEZ, M., AND HWANG, D.-U. Complex networks: Structureand dynamics. Physics reports 424, 4 (2006), 175–308.[10] BRAHNAM, S., JAIN, L. C., NANNI, L., LUMINI, A., ET AL. Local binary patterns: new variants andapplications. Springer, 2016.[11] BRODATZ, P. Textures: A photographic album for artists and designers. Dover Publications, New York, 1966.[12] BRUNO, O. M., DE OLIVEIRA PLOTZE, R., FALVO, M., AND DE CASTRO, M. Fractal dimension applied toplant identification. INFORMATION SCIENCES 178, 12 (2008), 2722–2733.[13] CALVETTI, D., MORIGI, S., REICHEL, L., AND SGALLARI, F. Tikhonov regularization and the L-curve forlarge discrete ill-posed problems. Journal of Computational and Applied Mathematics 123, 1 (2000), 423 – 446.[14] CHICKLORE, S., GOH, V., SIDDIQUE, M., ROY, A., MARSDEN, P. K., AND COOK, G. J. Quantifying tumourheterogeneity in 18f-fdg pet/ct imaging by texture analysis. European journal of nuclear medicine and molecularimaging 40, 1 (2013), 133–140.[15] COSTA, L. D. F., RODRIGUES, F. A., TRAVIESO, G., AND VILLAS BOAS, P. R. Characterization of complexnetworks: A survey of measurements. Advances in Physics 56, 1 (2007), 167–242.[16] COVER, T. M. Geometrical and statistical properties of systems of linear inequalities with applications in patternrecognition. IEEE Transactions on Electronic Computers EC-14, 3 (1965), 326–334.[17] CSURKA, G., DANCE, C., FAN, L., WILLAMOWSKI, J., AND BRAY, C. Visual categorization with bags ofkeypoints. In ECCV International Workshop on Statistical Learning in Computer Vision (2004), pp. 1–22.[18] DA FONTOURA COSTA, L., JR., O. N. O., TRAVIESO, G., RODRIGUES, F. A., BOAS, P. R. V., ANTIQUEIRA,L., VIANA, M. P., AND ROCHA, L. E. C. Analyzing and modeling real-world phenomena with complexnetworks: a survey of applications. Advances in Physics 60, 3 (2011), 329–412.[19] DA SILVA, N. R., VAN DER WEEËN, P., DE BAETS, B., AND BRUNO, O. M.Improved texture imageclassification through the use of a corrosion-inspired cellular automaton. Neurocomputing 149 (2015), 1560–1572.[20] DE MESQUITA SÁ JUNIOR, J. J., AND BACKES, A. R. Elm based signature for texture classification. PatternRecognition 51 (2016), 395 – 401.[21] DE VES, E., ACEVEDO, D., RUEDIN, A., AND BENAVENT, X. A statistical model for magnitudes and angles ofwavelet frame coefficients and its application to texture retrieval. Pattern Recognition 47, 9 (2014), 2925 – 2939.[22] FLORINDO, J. B., AND BRUNO, O. M. Fractal descriptors based on Fourier spectrum applied to texture analysis.Physica A: statistical Mechanics and its Applications 391, 20 (2012), 4909–4922.[23] FLORINDO, J. B., CASANOVA, D., AND BRUNO, O. M. Fractal measures of complex networks applied totexture analysis. In Journal of Physics: Conference Series (2013), vol. 410, IOP Publishing, p. 012091.[24] FUKUNAGA, K. Introduction to Statistical Pattern Recognition, 2nd ed. Academic Press, 1990.10[25] GARCÍA-OLALLA, O., ALEGRE, E., FERNÁNDEZ-ROBLES, L., AND GONZÁLEZ-CASTRO, V. Local orientedIn Pattern Recognition (ICPR), 2014 22ndstatistics information booster (losib) for texture classification.International Conference on (2014), IEEE, pp. 1114–1119.[26] GONÇALVES, W. N., DA SILVA, N. R., DA FONTOURA COSTA, L., AND BRUNO, O. M. Texture recognitionbased on diffusion in networks. Information Sciences 364 (2016), 51–71.[27] GUO, Y., ZHAO, G., AND PIETIKÄINEN, M. Texture classification using a linear configuration model baseddescriptor. In BMVC (2011), Citeseer, pp. 1–10.[28] GUO, Z., ZHANG, L., AND ZHANG, D. A completed modeling of local binary pattern operator for textureclassification. IEEE Transactions on Image Processing 19, 6 (2010), 1657–1663.[29] GUO, Z., ZHANG, L., AND ZHANG, D. Rotation invariant texture classification using lbp variance (lbpv) withglobal matching. Pattern recognition 43, 3 (2010), 706–719.[30] HARALICK, R. M. Statistical and structural approaches to texture. Proceedings of the IEEE 67, 5 (1979),786–804.[31] HARALICK, R. M., SHANMUGAM, K., AND DINSTEIN, I. H. Textural features for image classification. IEEETransactions on systems, man, and cybernetics, 6 (1973), 610–621.[32] HUANG, G.-B., ZHU, Q.-Y., AND SIEW, C.-K. Extreme learning machine: theory and applications. Neurocom-puting 70, 1 (2006), 489–501.[33] IDRISSA, M., AND ACHEROY, M. Texture classification using gabor filters. Pattern Recognition Letters 23, 9(2002), 1095–1102.[34] JAIN, A. K., AND FARROKHNIA, F. Unsupervised texture segmentation using Gabor filters. In Systems, Man andCybernetics, 1990. Conference Proceedings., IEEE International Conference on (1990), IEEE, pp. 14–19.[35] KANNALA, J., AND RAHTU, E. Bsif: Binarized statistical image features. In Pattern Recognition (ICPR), 201221st International Conference on (2012), IEEE, pp. 1363–1366.[36] KIM, J. K., AND PARK, H. W. Statistical textural features for detection of microcalcifications in digitizedmammograms. IEEE transactions on medical imaging 18, 3 (1999), 231–238.[37] LAM, W.-K., AND LI, C.-K. Rotated texture classification by improved iterative morphological decomposition.IEE Proceedings-Vision, Image and Signal Processing 144, 3 (1997), 171–179.[38] LEHMER, D. H. Mathematical methods in large scale computing units. Annals Comp. Laboratory HarvardUniversity 26 (1951), 141–146.[39] LIU, L., CHEN, J., FIEGUTH, P., ZHAO, G., CHELLAPPA, R., AND PIETIKAINEN, M. A survey of recentadvances in texture representation. arXiv preprint arXiv:1801.10324 (2018).[40] MAANI, R., KALRA, S., AND YANG, Y.-H. Noise robust rotation invariant features for texture classification.Pattern Recognition 46, 8 (2013), 2103–2116.[41] MALAMAS, E. N., PETRAKIS, E. G., ZERVAKIS, M., PETIT, L., AND LEGAT, J.-D. A survey on industrialvision systems, applications and tools. Image and vision computing 21, 2 (2003), 171–188.[42] MANJUNATH, B. S., AND MA, W.-Y. Texture features for browsing and retrieval of image data.IEEETransactions on pattern analysis and machine intelligence 18, 8 (1996), 837–842.[43] MIRANDA, G. H. B., MACHICAO, J., AND BRUNO, O. M. Exploring spatio-temporal dynamics of cellularautomata for pattern recognition in networks. Scientific Reports 6 (Nov 2016), 37329.[44] MOORE, E. H. On the reciprocal of the general algebraic matrix. Bulletin of the American Mathematical Society26 (1920), 394–395.[45] NANNI, L., LUMINI, A., AND BRAHNAM, S. Survey on LBP based texture descriptors for image classification.Expert Systems with Applications 39, 3 (2012), 3634–3641.[46] OJALA, T., MÄENPÄÄ, T., PIETIKÄINEN, M., VIERTOLA, J., KYLLÖNEN, J., AND HUOVINEN, S. Outex:New framework for empirical evaluation of texture analysis algorithms. In International Conference on PatternRecognition (2002), pp. 701–706.[47] OJALA, T., PIETIKAINEN, M., AND MAENPAA, T. Multiresolution gray-scale and rotation invariant textureclassification with local binary patterns. IEEE Transactions on pattern analysis and machine intelligence 24, 7(2002), 971–987.[48] OJANSIVU, V., AND HEIKKILÄ, J. Blur insensitive texture classification using local phase quantization. InInternational conference on image and signal processing (2008), Springer, pp. 236–243.11[49] PALM, C. Color texture classification by integrative co-occurrence matrices. Pattern recognition 37, 5 (2004),965–976.[50] PANJWANI, D. K., AND HEALEY, G. Markov random field models for unsupervised segmentation of texturedcolor images. IEEE Transactions on pattern analysis and machine intelligence 17, 10 (1995), 939–954.[51] PAO, Y.-H., PARK, G.-H., AND SOBAJIC, D. J. Learning and generalization characteristics of the random vectorfunctional-link net. Neurocomputing 6, 2 (1994), 163–180.[52] PAO, Y.-H., AND TAKEFUJI, Y. Functional-link net computing: theory, system architecture, and functionalities.Computer 25, 5 (1992), 76–79.[53] PARK, S. K., AND MILLER, K. W. Random number generators: good ones are hard to find. Communications ofthe ACM 31, 10 (1988), 1192–1201.[54] PENROSE, R. A generalized inverse for matrices. Mathematical Proceedings of the Cambridge PhilosophicalSociety 51, 3 (1955), 406—-413.[55] PICARD, R., GRACZYK, C., MANN, S., WACHMAN, J., PICARD, L., AND CAMPBELL, L. Vision texturedatabase. Media Laboratory, MIT, Cambridge, Massachusetts, 1995.[56] RIBAS, L. C., GONÇALVES, D. N., ORUÊ, J. P. M., AND GONÇALVES, W. N. Fractal dimension of maximumresponse filters applied to texture analysis. Pattern Recognition Letters 65 (2015), 116–123.[57] SCABINI, L. F., CONDORI, R. H., GONÇALVES, W. N., AND BRUNO, O. M. Multilayer complex networkdescriptors for color-texture characterization. Information Sciences 491 (2019), 30 – 47.[58] SCABINI, L. F., GONÇALVES, W. N., AND CASTRO JR, A. A. Texture analysis by bag-of-visual-words ofcomplex networks. In Iberoamerican Congress on Pattern Recognition (2015), Springer International Publishing,pp. 485–492.[59] SCHMIDT, W. F., KRAAIJVELD, M. A., AND DUIN, R. P. W. Feedforward neural networks with randomweights. In Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B:Pattern Recognition Methodology and Systems (1992), pp. 1–4.[60] TIKHONOV, A. N. On the solution of ill-posed problems and the method of regularization. Dokl. Akad. NaukUSSR 151, 3 (1963), 501—-504.[61] WATTS, D. J., AND STROGATZ, S. H. Collective dynamics of ‘small-world’networks. nature 393, 6684 (1998),440–442.[62] WENK, H. R. Preferred Orientation in Deformed Metal and Rocks: An Introduction to Modern Texture Analysis.Elsevier, 2013.[63] WESZKA, J. S., DYER, C. R., AND ROSENFELD, A. A comparative study of texture measures for terrainclassification. IEEE transactions on Systems, Man, and Cybernetics, 4 (1976), 269–285.[64] XU, D., CHEN, X., XIE, Y., YANG, C., AND GUI, W. Complex networks-based texture extraction andclassification method for mineral flotation froth images. Minerals Engineering 83 (2015), 105–116.[65] ZHANG, J., AND TAN, T. Brief review of invariant texture analysis methods. Pattern recognition 35, 3 (2002),735–747.[66] ZHANG, J., AND TAN, T. Brief review of invariant texture analysis methods. Pattern recognition 35, 3 (2002),735–747.[67] ZHU, Z., YOU, X., CHEN, C. P., TAO, D., OU, W., JIANG, X., AND ZOU, J. An adaptive hybrid pattern fornoise-robust texture analysis. Pattern Recognition 48, 8 (2015), 2592–2608.[68] ZIMER, A. M., RIOS, E. C., MENDES, P. D. C. D., GONÇALVES, W. N., BRUNO, O. M., PEREIRA, E. C.,AND MASCARO, L. H. Investigation of aisi 1040 steel corrosion in h2s solution containing chloride ions bydigital image processing coupled with electrochemical techniques. Corrosion Science 53, 10 (2011), 3193–3201.12