Artificial Intelligence 172 (2008) 483–513www.elsevier.com/locate/artintQuantifying the uncertainty of a belief net response:Bayesian error-bars for belief net inferenceTim Van Allen a, Ajit Singh b, Russell Greiner c,∗, Peter Hooper da Apollo Data Technologies, 12729 N.E. 20th Suite 7, Bellevue, WA 98005, USAb Center for Automated Learning and Discovery, Carnegie Mellon University, Pittsburgh, PA 15213, USAc Department of Computing Science, University of Alberta, Edmonton, Alberta T6G 2E8, Canadad Department of Mathematical and Statistical Sciences, University of Alberta, Edmonton, Alberta T6G 2G1, CanadaReceived 22 June 2006; received in revised form 8 September 2007; accepted 10 September 2007Available online 26 September 2007AbstractA Bayesian belief network models a joint distribution over variables using a DAG to represent variable dependencies and net-work parameters to represent the conditional probability of each variable given an assignment to its immediate parents. Existingalgorithms assume each network parameter is fixed. From a Bayesian perspective, however, these network parameters can be ran-dom variables that reflect uncertainty in parameter estimates, arising because the parameters are learned from data, or because theyare elicited from uncertain experts.Belief networks are commonly used to compute responses to queries—i.e., return a number for P(H = h | E = e). Parameteruncertainty induces uncertainty in query responses, which are thus themselves random variables. This paper investigates this queryresponse distribution, and shows how to accurately model this distribution for any query and any network structure. In particular,we prove that the query response is asymptotically Gaussian and provide its mean value and asymptotic variance. Moreover, wepresent an algorithm for computing these quantities that has the same worst-case complexity as inference in general, and alsodescribe straight-line code when the query includes all n variables. We provide empirical evidence that (1) our approximation ofthe variance is very accurate, and (2) a Beta distribution with these moments provides a very accurate model of the observed queryresponse distribution. We also show how to use this to produce accurate error bars around these responses—i.e., to determine thatthe response to P(H = h | E = e) is x ± y with confidence 1 − δ.© 2007 Elsevier B.V. All rights reserved.Keywords: Bayesian belief network; Variance; Bucket elimination; Credible interval; Error bars1. IntroductionBayesian belief nets (BNs), which provide a succinct model of a joint probability distribution, are used in anever increasing range of applications [13]. They are typically built by first finding an appropriate structure (either byinterviewing an expert, or by selecting a good model from training data), and then using a training sample to estimate* Corresponding author.E-mail addresses: tim@apollodatatech.com (T. Van Allen), ajit+@cs.cmu.edu (A. Singh), greiner@cs.ualberta.ca (R. Greiner),hooper@stat.ualberta.ca (P. Hooper).0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2007.09.004484T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513the parameters [22]. The resulting belief net is then used to answer queries—e.g., compute the conditional probabilityP(Cancer=true | Smoke=true, Gender=male). These responses clearly depend on the training sample usedto instantiate the parameters, in that different training samples will produce different parameters, which will lead todifferent responses.This paper investigates how variability within a sample induces variance in a query response, and presents a tech-nique for estimating the posterior distribution of the query responses produced by a belief net. Stated informally, ourgoal is an algorithm that takes• a belief net structure that we assume is correct (i.e., an I -map of the true distribution D [34])• a prior distribution over the network parameters Θ• a data sample S generated from D• a query of the form “What is q(Θ) = P(H = h | E = e, Θ)?”and returns both the expected value and the approximate variance of the query response q(Θ), based on the posteriordistribution of parameters given the sample. By using these moments and an appropriate distributional form, weapproximate the density of q(Θ), from which we can produce explicit [L, U ] ⊆ [0, 1] bounds such that 100(1 − δ)%of the posterior density is within the interval. In other words, the algorithm returns both a point-estimate of the answerand error bars around it.(cid:2)There are many other ways to use this variance around a query response. (1) In general, a good classifier shouldminimize Mean Squared Error, which can be expressed as “Bias2 + Variance” [36]. The results in this paper providea way to compute variance, which we then used as part of the Bias2 + Variance measure to estimate the quality ofdifferent belief net structures, when seeking the best classifier. Empirical evidence [18] suggests that this measure is,in fact, one of the most effective discriminative model selection criteria. (2) The maximum likelihood approach tocombining the responses of various independent belief net classifiers Pj involves weighting their respective (mean)probabilities by 1/variance; i.e., P ∗(hi|e) ∝j Pj (hi | e)/varj (hi|e). We [32] show that this works very well inpractice. (3) We could use query variance to detect outliers, as it could help differentiate sampling variation from trueoutliers [33]. (4) In classification, high probability is frequently taken as a proxy for confidence. Error bars provide amore statistically rigorous measure of confidence. For example, imagine we have determined that action1 (e.g., “applytreatmentX”) is optimal if P(+c | e) > 0.5, as this condition means action1 maximizes expected utility (MEU) [27].In the “traditional view”, we would be more comfortable taking action1 for larger values of P(+c | e)—e.g., moreconfident given e(cid:5) if P(+c | e(cid:5)) = 0.7, versus e(cid:5)(cid:5) when P(+c | e(cid:5)(cid:5)) = 0.6. Now imagine we know that P(+c | e(cid:5)) =0.7 ± 0.5 and P(+c | e(cid:5)(cid:5)) = 0.6 ± 0.001. In either case, the MEU response is still to take action1, as the expected utilitydepends only on the expected value of the response, but not on other characteristics of the posterior distribution.However, we should be more confident in taking this action in the second situation, e(cid:5)(cid:5), as there is less “probabilitymass” on the other side of the 0.5 decision boundary. This could also be useful when making decisions in safety-criticalscenarios, as this analysis can help to quantify the chance of a bad outcome, after taking the appropriate action—“gooddecision, bad outcome”. (5) Finally, if an expert is available to also provide the query response, error bars can be usedto validate a given belief net structure. For example, if an expert claims that P(B = 1 | D = 0, C = 1) = 0.5 but ourtechnique asserts that this response is in [0.26, 0.34] with 99% confidence, then we may question the validity of thenetwork. However, if our technique instead asserts that this response is in [0.17, 0.58] with 99% confidence, we maynot need to question the network structure.This paper provides a way to quantify this variance of a response. In particular, we show how to approximatethe posterior distribution of the query response, from which credible intervals (“Bayesian error-bars”) can be readilycomputed. The overall process is shown in Fig. 1. We begin with a network structure, with a prior on its parameters.The COMPUTEPOSTERIOR routine uses a data sample to perform a Bayesian update of this prior, yielding a posteriordistribution over the network parameters. Next, the MEANVAR algorithm calculates the mean and approximate vari-ance of the query response. Finally, the COMPUTEERRORBAR routine uses these moments to produce a model of theposterior distribution of the query response, from which it computes a 1 − δ credible interval. The COMPUTEPOSTE-RIOR and COMPUTEERRORBAR subroutines are well understood; our main contribution is defining and implementingMEANVAR.Section 2 provides the required background: defining belief nets and describing the Bayesian framework we use.This section also describes the COMPUTEPOSTERIOR and COMPUTEERRORBARS steps. Section 3 presents the the-T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513485Fig. 1. Overview of the overall process for computing error bar around the query response.oretical justification for MEANVAR. Given a set of explicitly specified assumptions, we prove that the query responsedistribution is asymptotically univariate Gaussian, and provide a closed-form formula for its asymptotic variance.1This theorem is the basis for our approximation of the posterior distribution.Section 4 presents an algorithm, MEANVAR, for computing these variances: In particular, it first describes the+ algorithm (an extension of the well-known Bucket Elimination algorithm [14]) that computes both theBUCKELIMresponse and also the derivative of the response wrt each of the CPtable entries; MEANVAR uses these derivatives forcomputing our approximation to the variance. This section provides theorems that state that the algorithm is correct,and that its asymptotic computational cost is the same as inference. Appendix A analyzes the special case wherethe query P(H = h | E = e) is “complete”, in that E specifies a value for every variable except H. It also provides astraightforward linear-time algorithm for this case.While our variance approximation is asymptotically accurate, it is not clear whether our error bar algorithm willwork well in practice, especially for a small sample, as our approximation is only first-order, and COMPUTEERROR-BAR inherits the assumption that the posterior query response will be Gaussian. We therefore perform experiments,based on Monte Carlo simulations, over a range of belief net structures and queries. The results, described in Section 5,suggest that (1) our approximation to the variance is acceptable, but (2) the resulting distribution is not well-modeledunder the Gaussian assumption. This negative result is not surprising: The response is a probability and so must be inthe [0, 1] interval; however a Gaussian distribution can have significant mass outside [0, 1]. We therefore consideredapproximating the query response with a Beta distribution, and found that this produces more accurate error bars.Section 6 surveys relevant work, to place our results in context. Appendix B provides proofs for the claims madein this paper. Given the abundance of notation, we provide a list of symbols in Fig. 3. Further details, along withcomplete results, inputs, experimental parameters, and raw data, are available in the webpage [20].2. Preliminaries2.1. Belief netsWe assume there is a fixed underlying distribution over n discrete random variables {X1, . . . , Xn}, which we denoteas the underlying distribution or the event distribution.We encode the event distribution as a belief net2 (cid:6)V, A, Θ(cid:7) that consists of a directed acyclic graph (DAG) whosenodes V represent variables and whose directed arcs A represent dependencies between variables. The (cid:6)V, A(cid:7) networkstructure encodes the independency relationships between variables in the underlying distribution—i.e., a node isindependent of its non-descendants, given an assignment to its immediate parents. Each node C ∈ V is also associated1 Essentially, “asymptotic” refers to a sufficiently large sample; see Theorem 2.2 Also known as Bayesian network, Bayesian belief network, probability net.486T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513Fig. 2. Diamond network: A simple example of a belief network, with CPtable distributions.with a conditional probability table, called its “CPtable”, that specifies Θc|f = P(C = c | F = f ) for each value c ∈ Cand each assignment f to its set of parents F [34]. That is, a belief network factors the underlying distribution intothe product of these conditional probabilities. While we will typically view each CPtable as a table, it is useful inSection 4 to view it as a function that maps each assignment of the variables {C} ∪ F to the associated probabilityvalue P(C = c | F = f ) ∈ [0, 1].Fig. 2 provides an example of a belief network. As a function, the CPtable for B is denoted fB (a, b); in therow view, as ΘB=1|A=0 = 0.412 and ΘB=0|A=0 = 0.588. As ΘB|A=0 is a distribution, the row entries must sum to(cid:2)b ΘB=b|A=0 = 1, and so ΘB=0|A=0 is implicitly defined by ΘB=1|A=0. We let Θ = {Θc|f} denote the set of allCPtable rows.In addition to providing a compact representation of a joint distribution, belief nets are used to effectively computemarginal and conditional probabilities. A query is a question of the form “What is P(H = h | E = e)?” and the answer(a real number in [0, 1]) is known as a query response.3 In this paper we assume the network structure is fixed, and sofor a particular query the answer depends only on the network parameters Θ; cf., Eq. (16) in Section 5.1, associatedwith the Diamond network on Fig. 2. To emphasize this relationship, we will use qh|e(Θ) = q(Θ) to denote theresponse to the query P(h | e, Θ), as a function of the parameters Θ.2.2. COMPUTEPOSTERIOR: Learning network parametersWe view Θ as a random vector, and follow the Bayesian view of parameter learning by placing a prior on Θ, andthen integrating the data to yield the posterior distribution of Θ; see Fig. 4. We will assume the rows are independent(see Definition 1 (part 2) in Section 3), in that each row ΘC|f is independent of every other row. Hence, the priorover Θ can be decomposed into priors on each row. Following standard practice [22], we will assume that each rowis Dirichlet distributed: ΘC|f ∼ Dir(mC=c1|f, . . . , mC=cr |f) where each mC=ci |f > 0.4 An alternative notation for thesame Dirichlet distribution isΘC|f = (cid:6)ΘC=c1|f, . . . , ΘC=cr |f(cid:7) ∼ Dir(mC|F=f; ˆΘC=c1|f, . . . , ˆΘC=cr |f)wheremC|f =(cid:3)imC=ci |fandˆΘC=ci |f =mC=ci |fmC|f(1)(2)mC|f is called the effective sample size of the distribution and each ˆΘC=c|f = E[ΘC=c|f] is the expected value ofΘC=c|f [9].3 We allow for the conditioning event, E = e, to involve no variables to allow unconditional queries of the form P(H = h). Also, while ournotation “H = h” suggests that our approach works only when there is a single query variable, note that everything holds when dealing with a setof query variables—i.e., P(H1 = h1, . . . , Hr = hr | E = e).4 A Dirichlet distribution with only two parameters is also known as a Beta distribution, which we will denote as Be(a, b).T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513487Top level algorithmsBUCKELIMBUCKELIM+Bucket elimination algorithm for computing the expected value of a query response; Section 4.1Variant of the bucket elimination algorithm that computes the expected value of a query response and thederivatives {q(c|f)h|e ( ˆΘ)}c,f; Section 4.2COMPUTEPOSTERIOR Computes the posterior distribution over the network parameters; Section 2.2COMPUTEERRORBARS Computes the actual (Bayesian) error bars, based on the distribution; Section 2.3MEANVARComputes the mean and variance of the response; Section 4DnkCC = c(C, f)Θ = {ΘC|f}C,fΘC|f = ΘC|F=fΘc|f = ΘC=c|F=fˆΘ = E[Θ]mC|F=fqh | e(Θ) = q(Θ)q(c|f)(Θ) = ∂q(Θ)∂Θc|fσ 2= σ 2h|e= ˜σ 2˜σ 2h|evh|e(C|f)h|e(Θ)h|e(Θ)bXi , b∅fi,j (·)Join(F )Elim(X, f )maxIndex(f )yBelief netsUnderlying distribution from which the data was drawnNumber of nodes/variables in the belief netTotal number of CPtable rows (over entire network)Capital letters denote nodes in the belief net (or equivalently the variables they represent). Bold denotesa set of variables—e.g., FLowercase letters denote an assignment to a variable. If the assignment is to a single binary variable, then{+c, −c} denotes the two valuesRefers to the CPtable row of variable C, corresponding to parental assignment F = fSet of all network parameters, over the entire networkCPtable row ΘC|f = (cid:6)Θc1|f, . . . , Θc(cid:4)|f(cid:7)Single network parameter, corresponding to P(C = c | F = f )Expected values of network parametersEffective sample size, associated with the CPtable row ΘC|F=f (Eq. (2))Query response as function of parameters, for fixed implicit belief net structure, and fixed implicit queryP(H = h | E = e, Θ)Derivative of query response with respect to formal parameter Θc|f (Eq. (6))Variance of the query responseApproximate variance of the query response (Eq. (7))Contribution to the variance of query P(h | e) due to CPtable row ΘC|F=f (Eq. (8))BUCKELIM and BUCKELIM+(Section 4)“Buckets” for holding tables, including the CPtablesFunction belonging to bucket bXiJoin of the functions f ∈ FMarginalizes the variable X from table fLargest index of the variables within Scheme(f ) (Eq. (11))Query response y = q(Θ) = P(H = h | E = e, Θ)Fig. 3. Notation.Consider the density of ΘC|A=1 in Fig. 2. Assume the prior density is Dir(1, 1) and we have a complete dataset—i.e., every instance in the set specifies the value of all variables. Only instances with A = 1 can contribute toΘC|A=1’s posterior. Moreover, the values of C are multinomially distributed within the selected sample. If there are34 instances where A = 1, of which 8 have C = 1 and 26 have C = 0, then the posterior distribution of ΘC|A=1 isDir(1 + 8, 1 + 26) = Dir(9, 27), which corresponds to the first row of C’s CPtable shown in Fig. 2. This is because aBayesian update of a Dirichlet prior with multinomial data yields a Dirichlet posterior—i.e., the Dirichlet distributionis the conjugate prior for multinomial distributions [43].Bayesian parameter learning in belief networks consists of updating each row in this fashion. In the absence ofprior knowledge a uniform prior, Dir(1, . . . , 1), over each row is often used. (Note this distribution is “flat”; thatis, every assignment is equally likely.) Frequently, the posterior Dirichlet row distributions are replaced by theirexpectations. In the above example, ΘC|A=1 ∼ Dir(9, 27) would be reduced to ˆΘC|A=1 = (cid:6)0.25, 0.75(cid:7). Replacingthe posterior row distributions with their expectations effectively ignores parameter uncertainty. When the networkparameters are fixed real numbers, the query response will be a real number. However, uncertain network parameters488T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513Fig. 4. Example of COMPUTEPOSTERIOR.will induce a distribution over the query response; that is, if Θ is a random variable, then q(Θ) is also a randomvariable. Furthermore, Cooper and Herskovits [9] have shown that, under our assumptions (Definition 1),(cid:4)E(cid:5)q(Θ)(cid:7)(cid:6)E[Θ]= q= q( ˆΘ)(3)That is, the posterior expectation of q(Θ) is simply the query response on the network with mean parameters. Recallthat ˆΘ = (cid:6) ˆΘ1, . . . , ˆΘr (cid:7) is encoded in the posterior row distributions, Dir(mC|f; ˆΘ1, . . . , ˆΘr ) in Eq. (1).There are a number of algorithms for computing the query response when the network parameters are fixed values.Section 4 will describe the one we will use below, Bucket Elimination. Such algorithms calculate the expected valueof the response qh|e( ˆΘ) but do not provide the variance:σ 2h|e= E(cid:4)(cid:6)(cid:7)qh|e(Θ) − qh|e( ˆΘ)2(cid:5)Theorem 2 provides an asymptotic form for this variance, and Section 4 describes an algorithm, MEANVAR, that caneffectively compute this quantity in general.T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–5134892.3. COMPUTEERRORBARS: Error bars for query responseCOMPUTEERRORBARS uses the mean and variance of a query response to estimate the posterior distribution ofq(Θ), which is then used to construct approximate Bayesian credible regions (error bars). This depends on the actualform of the distribution; here we consider two classes, Normal and Beta. In the latter case, we need to find the Betaparameters α and β that correspond to a given mean μ and variance σ 2, namelyα = μμ(1 − μ) − σ 2σ 2and β = (1 − μ)μ(1 − μ) − σ 2σ 2(4)(If either α or β is non-positive, then the associated density function is not well-defined. Also, if either parameter isless than 1, associated density has mode(s) at 0 and/or 1.)For any distribution D and any δ ∈ (0, 1), a 1 − δ credible region for the query response is a set ω(D) suchthat P(q(Θ) ∈ ω(D)) = 1 − δ. While there are infinitely many credible intervals on a continuous distribution, ourCOMPUTEERRORBARS system chooses contiguous intervals of the form [L, U ] such that the mass below the lowerbound L equals the mass above the upper bound U . Letting CDF−1(·; qh|e(Θ)) denote the inverse cumulative densityfunction of our model of qh|e(Θ), then(cid:8)(cid:8)(cid:9)(cid:9)L = CDF−1; qh|e(Θ)U = CDF−1δ21 − δ2; qh|e(Θ)Note that nothing in our framework precludes the use of other criteria for interval selection (e.g., shortest contiguous1 − δ interval).3. The posterior distribution of a responseThe MEANVAR subroutine requires an effective way to compute the variance of the response ˜σ 2h|eh|e(Θ).Our main theorem provides a closed-form expression for (an asymptotic approximation of) this quantity, given thefollowing assumptions:= ˜σ 2Definition 1. The independent Dirichlet property with respect to a DAG (cid:6)V, A(cid:7) holds under the following condi-tions:(1) True Structure: (cid:6)V, A(cid:7) is an I-map of the underlying joint distribution [34]—i.e., every independence claimimplied by the graph holds in the underlying distribution.(2) Parameter Independence: The distribution of a variable conditioned on one assignment to its parents is indepen-dent of the distribution conditioned on any other assignment to its parents. That is, if f1 (cid:12)= f2, then ΘC|f1 andΘC|f2 are independent (local parameter independence). Furthermore, the local conditional probability tables areindependent of one another (global parameter independence); see [39].(3) Dirichlet Assumption: The distribution of a variable given an assignment to its parents is Dirichlet distributed:ΘC|F=f ∼ Dir(mC=c1|f, . . . , mC=cr |f).(4) Non-degenerate Posterior Means: The expected posterior parameters ˆΘc|f are strictly between 0 and 1.5Note these assumptions conform with those made in maximum a posteriori learning of belief network parame-ters [22].As we are assuming that each network parameter ΘC=c|F=f = Θc|f is a random variable, the query response is afunction of the network parameters, and is thus itself a random variable. We will continue to use ˆΘ = { ˆΘc|f} = E[Θ]to refer the expected value of the parameters, under the posterior distribution, conditioned implicitly on observed dataand/or expert opinion.5 We discuss this constraint below—both after the statement of Theorem 2 and also following its proof in Appendix B.490T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513As mentioned above, we denote the query response as q(Θ) to emphasize that it is simply a function from aninstantiation of the network parameters to a real number in [0, 1]. For example, the query q+a|+b(Θ) = P(+a | + b, Θ)from Fig. 26 can be viewed asq+a|+b(Θ) = q+a|+b(Θ+a|{}, Θ−a|{}, Θ+b|+a, . . .)=Θ+b|+a × Θ+a|{}Θ+b|+a × Θ+a|{} + Θ+b|−a × Θ−a|{}(5)For each network parameter Θc|f ∈ Θ, we can consider the partial derivative of the query with respect to that parame-ter:q(c|f)(θ ) = ∂q(Θ)∂Θc|f(cid:10)(cid:10)(cid:10)(cid:10)θ(6)which is functional in θ and so can be evaluated at any specific instantiation of the network parameters. Using theq+a|+b(Θ) function from Eq. (5), observe thatq(+b|+a)(Θ) = Θ+a|{} × (Θ+b|+aΘ+a|{} + Θ+b|−aΘ−a|{}) − (Θ+b|+aΘ+a|{})(Θ+b|+aΘ+a|{} + Θ+b|−aΘ−a|{})2We can now state our main technical result:Theorem 2. Given the “independent Dirichlet property wrt a given DAG” assumption, the posterior mean of qh | e(Θ)is E[qh | e(Θ)] = qh | e( ˆΘ). Now consider an asymptotic framework where min{mC|f} → ∞ while the posterior meansˆΘc|f remain fixed at values strictly between 0 and 1, and let11 + mC|fvh|e(C|f)(cid:3)˜σ 2h|e=(C,F=f)(cid:8)(cid:3)vh|e(C|f) =(cid:9)q(c|F=f)h|e( ˆΘ)2 ˆΘc|F=f−q(c|F=f)h|e( ˆΘ) ˆΘc|F=f(cid:9)2(cid:8)(cid:3)c∈Cc∈CUnder this framework, the standardized random variableqh | e(Θ) − qh | e( ˆΘ)˜σh|econverges in distribution to the standard Normal distribution N (0, 1).(7)(8)(9)Given the form of Eq. (9), we see that ˜σ 2that q( ˆΘ) = E[q(Θ)] is the mean.)h|e (Eq. (7)) corresponds to the asymptotic variance. (Recall from Eq. (3)Each summation in Eq. (8) is over values of a particular variable C—so if this variable is binary, each sum willinvolve one term for +c and another for −c.The summation in Eq. (7) is over all CPtable rows; hence for the Diamond network in Fig. 2, this would involve9 terms, corresponding to 9 different vh|e(C|f) values: { vh|e(A|{}), vh|e(B| + a), vh|e(B| − a), . . . , vh|e(D| − b, −c)}.Moreover, this vh|e(C|f) is 0 whenever q(c|f)h|e ( ˆΘ) = 0 for each c ∈ C, which happens whenever this CPtable row isnot involved in computing qh|e(Θ) = P(h | e, Θ)—e.g., when this C is d-separated from the query (i.e., H⊥C, F|E) orwhen C is a barren node (i.e., neither it nor any of its descendants are instantiated in this query [31]); see Appendix A.As an extreme case, imagine the query corresponded to a single CPtable entry; e.g., in Fig. 2, the query was q+b|+a =P(+b | + a). Here only v+b|+a(B| + a) is relevant; the value of v+b|+a(C|f) is 0 for the terms corresponding to theother 8 CPtable rows (C, f). As q(+b|+a)+b|+a ( ˆΘ) = 0 for this query,+b|+a ( ˆΘ) = 1 and q(−b|+a)6 This corresponds to Q2 from Eq. (16).T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513491v+b|+a(B| +a) =(cid:9)q(b|+a)+b|+a ( ˆΘ)2 ˆΘb|+a−(cid:8) (cid:3)b∈B(cid:8) (cid:3)b∈B(cid:9)2q(b|+a)+b|+a ( ˆΘ) ˆΘb|+a= (12 · ˆΘ+b|+a + 02 · ˆΘ−b|+a) − (1 · ˆΘ+b|+a + 0 · ˆΘ−b|+a)2= ( ˆΘ+b|+a) − ( ˆΘ+b|+a)2 = ˆΘ+b|+a × ˆΘ−b|+awhich means the variance associated for this q+b|+a query is˜σ 2+b|+a=11 + mB|+a[ ˆΘ+b|+a × ˆΘ−b|+a]which as expected is exactly the variance associated with a single Dirichlet distributed variable Dir(mB|+a; ˆΘ+b|+a,ˆΘ−b|+a)—i.e., here Eq. (7) is exact and not just an approximation. Hooper [24] generalizes this to other situationswhere Eq. (7) is exact. (Appendix A provides another class of situations where it is easy to compute ˜σ 2h|e.)Eqs. (7) and (8) show that ˜σ 2h|e adds up the influence of each Θc|f on the query P(h | e), which is based on theh|e ( ˆΘ), weighted by (1 + mC|f)−1. So while mC|f is the same for each query, this variance will be differentderivative q(c|f)for different queries, as these derivatives will differ.While Appendix B provides the full proof, we note here that both the asymptotic normality and the derivation ofapproximate variance (Eq. (7)) employ a first-order Taylor expansion of the function q(Θ) about the posterior meanˆΘ. The conditions on ˆΘc|f and mC|f ensure that this first-order approximation is asymptotically valid. Comment#1after the proof in Appendix B discusses possible violations of these assumptions.We can also use Eq. (7) to understand how to deal with “fixed” CPtable rows—i.e., specific Θc|f entries thatcorrespond to definitions, and so can be viewed as constants rather than variables. Here, this corresponds to us-vh|e(C|f) ≈ 0 to the varianceing an effectively infinite mC|f value, which means this row will contributeapproximation—i.e., Eq. (7)’s summation can simply ignore these CPtable rows. Here, we can of course allow aCPtable entry to be deterministic—i.e., 0 or 1.11+mC|fSection 4 presents an algorithm that can compute both the mean and variance of the query response inO(n · exp(w)), where n is the number of variables in the network and w is the induced tree width [14]. This isthe same worst-case complexity as inference alone.3.1. The Beta approximation to the posteriorTheorem 2 suggests using a Normal distribution to approximate the response posterior distribution; however, theresponse is confined to [0, 1], whereas a Normal distribution has positive density on the entire real line. When thevariance is large, the Normal approximation may deviate considerably from the true posterior. Furthermore, we expectthe response distribution may often be skewed when its mean is near 0 or 1. It may therefore make sense to use a Betadistribution instead, as it is confined to [0, 1], can model skewed distributions, and when standardized, it converges inFig. 5. MEANVAR Process (including BUCKELIM+).492T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513distribution to a standard Normal random variable [3]. Eq. (4) shows how to map the computed mean μ and variancequantities σ 2 to the Beta distribution’s α and β parameters.4. Computing the variance: BUCKELIM+ and MEANVAREqs. (7) and (8) within Theorem 2 approximate the variance of the response. This section sketches the MEANVARalgorithm (Fig. 5) for computing this approximate variance. In particular, these equations involve the derivative of+ algorithm, appearing in Section 4.2, computes thesethe response with respect to each parameter. The BUCKELIM+values. That section also presents theorems showing the correctness and efficiency of this algorithm. As BUCKELIMis basically an extension of the Bucket Elimination algorithm, we first review that algorithm in Section 4.1. Finally,+Section 4.3 presents some remaining issues; in particular, it explicitly describes how the results of the BUCKELIMalgorithm can be used to compute the variance.There are many ways to optimize our overall MEANVAR system. First, note that MEANVAR uses BUCKELIMwhich in turn is based on BUCKELIM. As BUCKELIM is a standard, well-studied algorithm, there are now a numberof known effective optimizations, including graph reduction, heuristics for finding a good ordering, etc.; each of+ and hence to MEANVAR. Second, we can also find otherthese translates directly to an improvement to BUCKELIMinsights by squinting at Eq. (8). In particular, Appendix A provides straight-line code that applies to the special casewhen the query is complete (i.e., when the evidence includes all variables except the query variable).+4.1. Bucket elimination algorithm, BUCKELIMThe bucket elimination algorithm BUCKELIM [14] is an elegant framework for belief net inference in general,which uses non-serial dynamic programming to iteratively eliminate variables by marginalization.Notation. We must first introduce the notation of a table, which is a function mapping a set of named variables to (cid:17).For example,f (A, B) =a1100b1010f (A = a, B = b)0.300.700.910.09Notice this table corresponds to the ΘB|A function shown in Fig. 2. A table’s input variables are called its scheme;here Scheme(f ) = {A, B}.We next define two operations on tables, Join(·) and Elim(·, ·). The join operation combines a set of tables into af ∈F Scheme(f ) is the union of the variables of the tables, andnew table: Letting h = Join(F), then Scheme(h) =h’s values are(cid:11)(cid:12)h(x) =f (x|Scheme(f ))f ∈Fwhere “x|S” denotes the projection of an assignment x ∈ X onto a subset of its variables S ⊆ X; e.g.,(cid:6)A = 1, B = 0, C = 0(cid:7)|{A,C} = (cid:6)A = 1, C = 0(cid:7). Thus, the value of h on x is the product of the f ∈ F s evaluatedon the projections of x onto the scheme for that f . We will often simplify our notation by using Join(f, g) as anabbreviation for Join({f, g}), etc. Note Join({f }) = f and Join({}) is undefined. To make this Join more concrete,consider the tableg(A, C) =a1100c1010g(A = a, C = c)0.220.780.990.01T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513493Then h = Join(f, g) ish(A, B, C) =a11110000b11001100c10101010h(A = a, B = b, C = c)0.3 × 0.220.3 × 0.780.7 × 0.220.7 × 0.780.91 × 0.990.91 × 0.010.09 × 0.990.09 × 0.01This operation is similar to the relational join used in database theory [28].The elimination operation reduces a table f by marginalizing over one of the variables in f ’s scheme, producing afunction f−X = Elim(X, f ) using the variables Scheme(f ) − {X}. Assuming Scheme(f ) = {X, Y1, . . . , Yk},f−X(y1, . . . , yk) =(cid:3)f (x, y1, . . . , yk)xHence, using the f defined above,Elim(A, f ) = f−A(B) =b10f−A(B = b)0.3 + 0.910.7 + 0.09We can eliminate a set of variables, by repeating this marginalization process iteratively, for each variable; here,we would write Elim({Xi, . . . , Xj }, f ). (Observe we obtain the same result independent of the order in which weeliminate variables {Xi, . . . , Xj }.) Notice Elim({}, f ) ≡ f .BUCKELIM Algorithm. The bucket elimination algorithm BUCKELIM takes as input a belief net, encoded as its set ofassociated CPtables F over the variables V, a partial assignment to a subset of the variables E = e, and an ordering ofthe variables—e.g., π0 = (cid:6)A, B, C, D(cid:7). Below, we will assume the variables are numbered in order: (cid:6)X1, X2, . . . , Xn(cid:7).BUCKELIM then computes the value:P(E = e) = y =f (x|Scheme(f ))(10)(cid:3)(cid:12){x: x|E=e}f ∈Fwhere the sum is over all assignments to V that include the partial assignment E = e. Of course, we could compute yby simply joining all the functions in F and eliminating the variables in V not in E. However, BUCKELIM achievesbetter efficiency than this brute force approach by exploiting the fact that each function depends only on the variablesin its scheme.To do this, BUCKELIM builds and uses a set of buckets {bXi}i associated with each variable Xi ∈ V, and also oneadditional bucket b∅. Each bucket will hold a set of functions. The buckets are ordered according to π , with b∅ at thebeginning. BUCKELIM initially loads the input functions from F (that is, the belief net’s CPtables) into the buckets,as follows: Recalling that each table f uses a set of variables Scheme(f ) = {Xi}i , let(cid:13)(cid:14)maxIndex(f ) = argmaxXi ∈ Scheme(f )i(11)be the largest index of f ’s variables, based on the π order. We then assign f to the maxIndex(f ) bucket. (IfScheme(f ) = ∅, we define maxIndex(f ) = ∅.) Hence, using π0, we would assign ΘA|{} = ΘA to bucket bA, ΘB|Ato bucket bB , ΘC|A to bucket bC and ΘD|B,C to bucket bD. (Here, there happened to be a one-to-one mapping be-tween CPtable functions and buckets; that is not true in general.7)7 For example, in the ordering (cid:6)D, C, B, A(cid:7), the bD and bC buckets would both be empty, and bB bucket would include the function ΘD|B,Cand the bA bucket would include the three functions ΘA|{}, ΘB|A and ΘC|A. The rest of the text will consider only the π0 ordering.494Step0123b∅–––f∅,1() = gA()T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513bAfA,1(a) = ΘA=a|{}bBfB,1(b, a) = ΘB=b|A=abCfC,1(c, a) = ΘC=c|A=afC,2(b, c) = ΘD=1|B=b,C=cbD–fB,1(b, a) = ΘB=b|A=afB,2(a, b) = gC (a, b)fA,1(a) = ΘA=a|{}fA,1(a) = gB (a)fA,2(a) = gB (a)Fig. 6. Trace of BUCKELIM algorithm, on P(D = 1).BUCKELIM uses these buckets to answer queries. It traverses these buckets, in reverse order (here, this meansprocessing bD first, then bC , bB , bA and b∅). To process each bucket bXj : If the bucket is empty, BUCKELIM willskip it. Otherwise, BUCKELIM joins the functions in the bucket, then eliminates the variable Xj , producinggj = Elim(cid:7)(cid:6)Xj , Join(bXj ).(12)We then store gj in the bucket bXi , where i = maxIndex(gj ). That is, if Scheme(gj ) = ∅, we store gj in the b∅ bucket.Otherwise, Scheme(gj ) has a variable with the highest index, say Xi . (Note that i < j , as j had been the largest indexof each of the functions in bXj , and it is now eliminated.) BUCKELIM then stores gj in the bXi bucket, and continuesdown the ordering, to process bXj −1 . At the end, it processes the b∅ bucket—which involves Join(·)-ing constantfunctions, which corresponds to a simple scalar multiplication. BUCKELIM returns the resulting scalar.The algorithm has one final complication: The process suggested above would compute P({}), which is 1. In generalwe want to compute an expression of the form P(E = e). To do this, we first initialize the relevant CPtable functionsto correspond to those instantiations, and use only those “restricted” functions.Example. To illustrate this, consider the P(D = 1) query, using the π0 ordering. Observe first that we do not need allof D’s CPtable entries, but only fD=1,B,C(b, c) = ΘD=1|b,c(b, c). As this is a function of B and C, but not D—thatis maxIndex(fD=1,B,C) = C—we store this function in bucket bC . Hence, when BUCKELIM starts, its 5 buckets areconfigured as shown in Step#0 of Fig. 6.BUCKELIM then processes the buckets in reverse order. As bD is empty, BUCKELIM’s first non-trivial operationdeals with bC ; here it joins the two functions in that bucket, and eliminates the variable C; this produces8gC(a, b) = Elim(cid:3)(cid:7)(cid:6)C, Join(fC,1, fC,2)=(cid:3)cΘC=c|A=a ΘD=1|B=b,C=c=P(D = 1, C = c | B = b, A = a) = P(D = 1 | B = b, A = a)cAs this function has the scheme Scheme(gC) = {A, B}, it is stored in the bucket bB , where it is called fB,2; see theStep#1 entry in Fig. 6. BUCKELIM then sums out bB , computinggB (a) = Elim(cid:7)(cid:6)B, Join(fB,1, fB,2)=(cid:3)bΘB=b|A=a P(D = 1 | B = b, A = a)P(D = 1, B = b | A = a) = P(D = 1 | A = a)(cid:3)=b8 In some cases, the result of the computation has an easy interpretation; such as P(D = 1 | B = b, A = a) shown here. This is not always thecase.T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513495As this function involves only the variable A, it is stored in bA and called fA,2. It next sums out bA to produce aconstantgA() = Elim(cid:7)(cid:6)A, Join(fA,1, fA,2)=(cid:3)P(A = a) P(D = 1 | A = a) = P(D = 1)which is stored in b∅; see Step#2 and Step#3 of Fig. 6. BUCKELIM then returns f∅,1(), as the correct response to thisquery.a4.2. Extension to compute derivatives, BUCKELIM+BUCKELIM+ takes the same inputs as BUCKELIM: a belief net (represented by a set of CPtable functions F overthe variables V), an ordering of the variables π , and a partial assignment to the variables E = e. It returns both theexpected response (the y = P(E = e) = EΘ [P (E = e|Θ)] shown in Eq. (10)) and also, for each CPtable fi ≡ Θi ,the partial derivative ∂y= q(i)(Θ) = ∂q(Θ)/∂Θi . For notation, we will continue to name the functions appearing in∂fibucket bXi as {fi,j }j = {fi,1, fi,2, . . .}; e.g., the functions within bA are {fA,1, fA,2}. Notice every such function iseither an original CPtable (or perhaps a subset of such a table; e.g., fC,2 = ΘD=1|B,C ) or is a function produced byfirst joining a set of previously produced functions in a common bucket, then eliminating a variable from the resultingfunction—e.g.,(cid:7)(cid:6)fX,2(·) = gY (·) = ElimY, Join(fY,1, fY,2, fY,3)(13)We will continue to let gi(·) refer to the “output” of bucket bXi .BUCKELIM+ uses this information to compute the partial derivative ∂q(Θ)/∂fi,j of each function appearing ineach bucket (which includes each Θi CPtable function). It does this using the chain rule: e.g., using Eq. (13),∂q(Θ)∂fY,1= ∂q(Θ)∂fX,2× ∂fX,2∂fY,1Moreover, given the form of Eq. (13), we see that ∂fX,2corresponds roughly to Join(fY,2, fY,3), after marginalizing∂fY,1out some variables; see Eq. (14) below. The only remaining trick is to process the buckets in the forward direction, tomake sure we have computed ∂q(Θ)∂fX,2The overall algorithm is then. . .when we are asked to compute ∂q(Θ)∂fY,1.Step A Run BUCKELIM on its inputs to compute y. The intermediate results (the functions gi(·) created as the outputof each bucket) are stored for use in Step B.Step B Compute the derivative function for each bucket function fij as follows.(1) Compute ∂y∂g∅ for the output of b∅: g∅ = Elim({}, Join(b∅)) =(cid:15)i f∅,i (Eq. (12)). This is trivial, as g∅ is a constanty = g∅; thus ∂y∂g∅= 1.(2) Iterate over the remaining buckets in the order reversed from BUCKELIM—i.e., in the original order π , startingwith b∅.Let bXi be the current bucket, whose output is gi = Elim(Xi, Join(bXi )). Now recall that maxIndex(gi) < i, as gi wasformed by joining functions from the Xi bucket (whose variables had indices at most i), and then eliminating Xi . Aswe are processing buckets in order, BUCKELIMbefore reaching bXi .+ will have computed ∂y∂giFor each fij ∈ bXi , compute ∂y∂fij(cid:8)(cid:16)(cid:17)using:(cid:9)J = Join∂y∂fij= Elim∪ {h ∈ bXi : h (cid:12)= fij }∂y∂gi(cid:6)Scheme(J ) − Scheme(fij ), J(cid:7)(14)(The first argument of Elim is Scheme(J ) − Scheme(fij ) = {X ∈ Scheme(J ) : X /∈ Scheme(fij )}.) That is, we join∂yand all the functions of the bucket other than fij , then eliminate the variables that do not appear in Scheme(fij ) to∂gi496T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513whose scheme Scheme( ∂yproduce a function ∂y∂fij∂fijof y with respect to fij . (Note that q(C|f)(·) = ∂y∂fC,1) equals Scheme(fij ), and whose values are the partial derivativesin general.)Example. We already saw that, in b∅, ∂y∂g∅= 1, which means ∂y∂f∅,1(cid:8)(cid:9)(cid:9)(cid:8)= 1. In bA, as f∅,1 was formed from the fA,i ’s,q(A|{})(a) = ∂y∂fA,1= ∂y∂f∅,1= Elim{}, Join× ∂f∅,1∂fA,1= 1 ×(cid:12)j (cid:12)=1∂y∂f∅,1,∂f∅,1∂fA,1fA,j = fA,2 = P(D = 1 | A = a)∂yThat is, as fA,1 = ΘA|{},∂ΘA|{}P(A = a) and fA,1(a) = P(A = a). Notice Scheme( ∂y∂fA,1= P(D = 1 | A = a). This makes sense, as y = P(D = 1) =) = {A} = Scheme(fA,1). Similarly(cid:2)a P(D = 1 | A = a) ×∂y∂fA,2= fA,1(a) = P(A = a)+ continues working forward in the order—next processing the functions within bB : Here, as fB,1 wasBUCKELIMused to produce fA,2,q(B|A)(a, b) = ∂y∂fB,1(cid:8)= Elim{},∂y∂fA,2× ∂fA,2∂fB,1(cid:9)= P(A = a) ×(cid:12)fB,jj (cid:12)=1= P(A = a) × fB,2 = P(A = a) × P(D = 1 | B = b, A = a)= Elim({}, P(A = a)×P(B = b | A = a)) = P(A = a, B = b). Finally, as fC,1 and fC,2 were used to produceand ∂y∂fB,2fB,2,q(C|A)(a, c) = ∂y∂fC,1(cid:3)(cid:8)= ElimB,∂y∂fB,2× ∂fB,2∂fC,1(cid:9)(cid:3)=bP(A = a, B = b) ×(cid:12)j (cid:12)=1fC,j=P(A = a, B = b) × fC,2b(cid:3)=bP(A = a, B = b) × P(D = 1 | B = b, C = c)and q(D=1|B,C)(b, c) = ∂y∂fC,2= Elim(A,∂y∂fB,2× ∂fB,2∂fC,1) =(cid:2)a P(A = a, B = b) × P(C = c | A = a).Appendix B provides the proofs for following two theorems, which respectively prove that this algorithm is correct,and bound its computational complexity.Theorem 3. BUCKELIM∂y/∂ΘC|F.+ correctly computes the partial derivatives of the response wrt each CPtable qC|F(Θ) =Theorem 4. The worst case complexity of BUCKELIM(fixed) size of largest domain over the variables, and w is the induced tree width, given the ordering.+ is O(n · r w) time, where n is the number of nodes, r is the4.3. How MEANVAR uses BUCKELIM+The basic BUCKELIM algorithm deals only with the unconditional probabilities. To compute conditional probabil-, these inference algorithms willi P(H = hi, E = e); finallyities requires a summation and a division: That is, for P(H = h | E = e) = P(H=h,E=e)first compute P(H = hi, E = e) for each i, then sum these values to compute P(E = e) =it returns P(H = h | E = e) = P(H = h, E = e)/P(E = e).P(E=e)(cid:2)T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513497Our MEANVAR will therefore first call BUCKELIMter computing the response, BUCKELIMe)/∂Θi ) for all of the parameters Θi ’s, evaluated at ˆΘ. MEANVAR then uses the quotient rule for derivatives,+ on P(H = h, E = e), then on P(E = e). In each case, af-+ will also compute the derivatives ∂P(H = h, E = e)/∂Θi (resp., ∂P(E =∂P(H = h | E = e)∂Θi=1P(E = e)∂P(H = h, E = e)∂Θi− q(Θ)∂P(E = e)∂Θi(cid:18)(cid:19)to compute the actual derivatives we need. Given the complexity results in Theorem 4, it is trivial to observe thatMEANVAR is also O(n · exp (w)) time.5. ExperimentsTheorem 2 provides an approximation of the variance of a query response, ˜σ 2h|e that is asymptotically accurate.When network parameters are estimated from small or even moderately sized data sets, there is no guarantee on theaccuracy of ˜σ 2h|e. However, Section 5.2 shows that the approximation works well in practice.Given the true mean and approximate variance of a query response, we considered two models for the distributionof q(Θ): Normal and Beta, based respectively on asymptotic behavior, and observations of issues like support. Sec-tion 5.3 compares the fit of each model to samples from q(Θ). An important application of the query response modelis the estimation of error-bars, which is explored in Section 5.4. Finally, some researchers have suggested that the vari-ance is well approximated as a simple binomial expression. Section 5.5 presents empirical evidence to demonstratethat this not a good approximation.5.1. Experimental setupTo explore these claims, we require the ability to control the parameter uncertainty of Θ and the ability to samplefrom q(Θ). Our experiments will use common benchmark networks, whose parameters are usually given as fixedquantities, e.g., ΘB|a+ = (0.3, 0.7). We require parameter uncertainty in the form of Dirichlet row distribution, e.g.,ΘB|a+ ∼ Beta(3, 7). To resolve this problem we treat the network with fixed parameters as the underlying distri-bution, from which we produce a sample D by drawing m tuples from the network. Using the same structure asthe initial network, we learn new parameters using this D. Assuming a uniform Dirichlet prior over the parametersInput: Belief Net B (structure + parameters)Set of queries { qi (Θ) = P(hi | ei , Θ) }m = “effective size” used to compute parametersr = number of “trials”—each involves one specific set of parameter values1. Generate posterior over parametersFor k = 1..md(k) := RandomInstanceFrom(B)Compute posterior Θ|D from initial distribution Θc|f ∼ Dir(1, 1, . . . , 1) and data D = {d(k)}2. Compute replicate parameters, and query responsesFor j = 1..rDraw Θ(j ) from Θ|D posterior distributionFor each query i:Qji}, {Qj2For each query i, based on {Qji3. Actual Analysis of {Qj1:= qi (Θ(j ))}, . . .}j samples. . .a. Compute and compare ai to eib. Perform relevant statistical tests—Normal vs Beta(Section 5.2)Produce Quantile-Quantile plots(Section 5.3)c. Perform coverage experiments “error bars”d. Compare to binomial distribution(Section 5.5)(Section 5.4)Fig. 7. Experimental setup.498T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513Θi ∼ Dir(1, . . . , 1), the data is integrated in to produce posterior parameters Θ|D, as discussed above; see COM-PUTEPOSTERIOR in Section 2.2.Given such a belief net and posterior distribution, we use a Monte Carlo strategy to sample from q(Θ). We generatej =1, where each replicate is formed by replacing each Dirichlet row distributionr replicates from Θ, denoted {Θ (j )}rby a sample from it. That is, each replicate instantiates a belief net with fixed values for parameters Θ.To illustrate, imagine we started with the Diamond network, whose parameters were each initially uniform—i.e.,each Θi ∼ Beta(1, 1)—then used a m = 100 data samples to produce the posterior Θ|D distribution shown in Fig. 2,with ΘA|{} ∼ Beta(35, 67) and ΘB|+a ∼ Beta(7, 29). We then drew samples of Θ|D of the form(cid:20)(cid:20)(cid:20)(cid:20)Θ (1)+aΘ (2)+aΘ (3)+a= 0.35, Θ (1)= 0.31, Θ (2)= 0.33, Θ (3)...+b|+a+b|+a+b|+a= 0.18,= 0.21,= 0.17,...(cid:21)(cid:21)(cid:21)(cid:21). . .. . .. . .. . .(15)Notice this is not the same as drawing replicates of values for the domain variables A, B, . . . . For each instantiationof the parameters Θ (j ) and fixed query qi(.), we calculate Qj= qi(Θ (j )) using an exact algorithm for belief netiinference.Our experiments are based on r = 1000 samples. This allows us to empirically evaluate the quality of the approx-imations implicit in Theorem 2, which deals with the distribution over parameters Θ (Eq. (15)), not over the basictuples. We then performed various tests on these {Qj}j values; see next subsections. Fig. 7 summarizes these steps.iWe use four network structures in our experiments: Diamond, Alarm, Insurance and Hailfinder. Diamond is thefour variable network illustrated in Fig. 2, that allows for a variety of inferential patterns. We considered the followingqueries:Q1 : P(+a) = Θ+a|{}Q2 : P(+a | +b) =Θ+b|+a×Θ+a|{}Θ+b|+a ×Θ+a|{}+Θ+b|−a ×Θ−a|{}Q3 : P(+a | +b, +c) =Q4 : P(+b, +c | +a) = Θ+b|+a × Θ+c|+aΘ+b|+a ×Θ+c|+a ×Θ+a|{}Θ+b|+a ×Θ+c|+a ×Θ+a|{}+Θ+b|−a ×Θ+c|−a ×Θ−a|{}Q5 : P(+a | +d) =Q6 : P(+d | +a) =(cid:2)(cid:2)b,c Θ+d|B=b,C=c×ΘB=b|+a×ΘC=c|+a ×Θ+a|{}a,b,c Θ+d|B=b,C=c×ΘB=b|A=a ×ΘC=c|A=a ×ΘA=a|{}b,c Θ+d|B=b,C=c × ΘB=b|+a × ΘC=c|+a(cid:2)(16)The Alarm network, a 37 variable network described in [21], was designed by medical experts for monitoring intensivecare patients. Alarm has become a standard for evaluating belief net learning and inference algorithms. Here andbelow, we then used m = 150 samples to produce Dirichlet parameters. We generated 100 queries by choosing asingle query variable and three to five evidence assignments, using [21] to determine which variables could be queryvariables. (Here, and below, the value of each query and evidence variable is assigned uniformly at random.) Themean response of these queries tends to cover the [0, 1] interval well, including many queries whose mean responsewas near 0 or 1.The Insurance network [6] models car insurance risk, using 27 variables. We generated 100 queries by randomlysampling one query variable and either zero, one, or two evidence variables. To ensure that the mean response of thequeries covered the [0,1] interval, we used a rejection sampling procedure: We divided [0,1] into 5 ranges [0,0.2),[0.2,0.4), . . . , [0.8,1.0], and generated queries until an equal number of responses fell into each bin.9The Hailfinder network [1] is a 56 variable network for forecasting summer hailstorms. Again we generated 100queries using the same rejection sampling procedure. (More details about these networks, queries and parameters, aswell as more extensive experimental results, are available in [20].)9 This was required because we found that the naive approach of uniformly sampling queries with 1 query variable and 0–2 evidence variablesproduced queries whose responses were tightly clustered around 0.2–0.3.T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513499(a) Diamond m = 25(b) Alarm m = 150(c) Insurance m = 150(d) Hailfinder m = 150Fig. 8. Each (cid:6)x, y(cid:7) point represents a query whose x-value is the approximate variance of a query response computed using MEANVAR, ai , andwhose y-value is the sample variance of the response based on r = 1000 instances drawn from the true response distribution, ei from Eq. (17).Subfigures: (a) six queries on Diamond; (b) one hundred queries on Alarm; (c) one hundred queries on Insurance; (d) one hundred queries onHailfinder.5.2. Accuracy of variance approximationThe simplest measure of the accuracy of ˜σ 2h|e is how close it is to the true variance of a query response. Even thoughwe do not know the true variance of a query response, using r = 1000 samples from the query response distribution,{Qi}ri=1, provides a good estimate. This is especially true since q(Θ) is unidimensional and often unimodal. Thatis, over a wide range of queries qi , we want to see how our analytic approximation ai (i.e., the ˜σ 2h|e from Eq. (7) inTheorem 2) compares to the large-sample estimate.10ei = 1rr(cid:3)(cid:6)Qjij =1(cid:7)− E[Qi]2(17)As we know that our approximate variance is asymptotically correct as m → ∞, we are most interested in seeingits behavior for relatively small values of m. Fig. 8 shows our results on the four networks. We used m = 150 forthe three larger networks, but m = 25 for Diamond. (Using m = 150 here would reduce the variance in the queryresponses to nearly zero.) The fact that essentially all of the values are extremely near the “y = x” diagonal line showsa very tight fit over almost all of the queries. We quantify this below. While the absolute scale of the variances appears10 In our experiments, we use a nonparametric bootstrap to improve on the large-sample estimate.500T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513Fig. 9. Mean scaled percentage error vs. effective sample size m.small, note that the variance is usually less than 112 , which is the variance of the uniform distribution on [0,1]. Ofcourse, query distributions typically have much smaller variances. E.g., if a query has mean 0.4 and variance 0.01,then it will usually fall within two standard deviations of its mean—i.e., between 0.2 and 0.6.11While all of the deviances (between ai and ei ) are relatively small, many of the larger deviance value are associatedwith the queries with the largest variance values. This is not surprising, and follows from the fact that we are usingthe Delta method, which is based on a first-order Taylor expansion. We elaborate in Comment#2 after the proof ofTheorem 2 (in Appendix B).We use Mean Scaled Percentage Error(cid:3)MSPE = 100n|ai − ei|eii(i.e., the average percentage that ai deviates from ei over n queries) to evaluate the quality of our approximation.Fig. 9 shows that our MEANVAR approximations are accurate across a range of sample sizes. For example, even withonly m = 25 samples, our worst average over any structure was under 14%, and after 200 examples, the worst wasaround 7%. The rate of convergence of ˜σ 2h|e differs with each query; see Comment#1 after the proof of Theorem 2, inAppendix B.5.3. Normal vs Beta distributionSome tasks, such as the ones mentioned in [18] and [32], require only (an approximation of) variance; the previoussection showed that our analytic approximation of variance fits the empirical evidence very closely. However, formany other tasks (such as computing error bars), we also need to know the form of the underlying distribution. Whilethis parametric form is unknown in general [24], we can still determine whether we would get appropriate answers ifwe used some plausible form. Section 3.1 argued for fitting the mean and approximate variance to either a Normal orBeta distribution. Here we compare samples from the true distribution, {Qi}ri=1, against each model.Fig. 10 presents two histograms of these {Qji}j values for two different queries Qi , with expected means near0.5 and 0.05 respectively. On each, we have superimposed the pdf (probability distribution function) for the best fitNormal and best fit Beta distributions. The left figure shows that both distributions can have good fits, when the meanis far from 0 and from 1, and the distribution is fairly symmetric. The right figure shows that even the best-fittingNormal distribution has problems when the distribution is skewed.11 Each point in Fig. 8 is based on a single Θ vector produced from a single data set. We also considered other Θ parameter values, by estimatingnew parameters from other data sets generated from the underlying distribution—i.e., re-running process in Fig. 7, starting from Step 1. We foundthat the quality of the variance approximation is similar.T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513501(a) Low skew(b) High skewFig. 10. Two query response distributions from Alarm: Each is a histogram of the {Qi }rmodels. When the skew is low, the models tend to be similar, but when the skew is high, the Beta model is a much better fit.i=1 instances, overlayed with the fitted Beta and NormalFig. 11. Quantile–quantile plots for nine query instances (Alarm network), each comparing query sample distribution against our approximation ofthe q(Θ) distribution using calculated mean and variance. The left-hand side assumes q(Θ) is Gaussian; the right-hand side assumes q(Θ) is Betadistributed. The μ value is the expected query response.A “quantile–quantile plot” provides a way to visualize this fit, by plotting sample quantiles against the theoreticalquantiles of the fitted (Normal or Beta) distributions; see [4]. Here, a straight diagonal line indicates that, for eachk = 1..r, the kth instance (of r) appears where the distribution predicts k/r of the data should be. Fig. 11 shows ninesuch quantile–quantile plots, for the queries numbered {15, 25, . . . , 95}, where the numbers are based on the query’smean response.12 We see that many of the queries are better approximated by the Beta than the Normal, especiallywhen the true distribution is skewed (i.e., the ones with mean response μ near 0 or near 1).One way to quantify the difference between models is to compare the (log)likelihood of the query in-i ) (resp.,= qi(Θ j ) computed based on the Normal (resp., Beta) parametersj log PNormal(μi ,σ 2stances Qji(cid:2)i )(Qj12 These are only a representative sample of the 600+ quantile–quantile plots for Alarm. All of the plots are available in [20].502T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513Table 1Fail-Beta is the fraction of queries that did not fit the Beta hypothesis at 0.05 significance; Fail-Normal is the fraction of queries that did not fit the Normal hypothesis at 0.05 significanceDomainDiamond (m = 25)Alarm (m = 300)Insurance (m = 300)Hailfinder (m = 300)Fail-Beta0/616/10013/10010/100Fail-Normal1/650/10036/10031/100(cid:2)j log PBeta(ai ,bi )(Qji )). For Diamond, constructed using m = 25 samples, the likelihood of the Beta model is higherthan Normal model for each of the six queries. For Alarm (m = 300), the likelihood of the Beta model is higher thanthe Normal model for 92 of the 100 queries. For Insurance (m = 300), the Beta model is better for 89 of the 100queries. For Hailfinder (m = 300), the Beta model is better for 89 of the 100 queries.The fact that Beta has higher likelihood is even more impressive, given that this log-likelihood measure unnaturallyfavors the Normal model, especially when the response is near 0 or near 1: As Θ → 0 (or Θ → 1), the log-likelihoodof the Beta distribution usually tends to −∞, but this does not occur with the Normal distribution. This is becausethe log-likelihood for the N (μ, σ 2) model is a constant plus a sum of terms −(1/2) × [(x − μ)/σ ]2 while the log-likelihood for Be(a, b) is a constant plus a sum of terms (a − 1) × log(x) + (b − 1) × log(1 − x), which can blow up(large negative) if a > 1 and x is near 0, or if b > 1 and x is near 1. Hence, the Beta distribution is strongly affectedby values near the boundary of the unit interval, while the Normal log likelihood is not.Another approach is to use a goodness-of-fit test where, formally, the null hypothesis is that the samples {Qj }rj =1were drawn from the Normal model (resp., Beta model) and the alternate is the negation of the null hypothesis. Suchtests do not allow us to state that the data was drawn from the null model, instead the focus “is on the measure ofagreement of the data with the null hypothesis” [16]. Here, we are asking how close the observed distribution is to thebest-fit Normal (resp., the best-fit Beta) distribution.These goodness-of-fit procedures suggest that the Normal model is problematic. Using an Anderson–Darling nor-mality test [2], for example, 80 of the 100 Alarm queries showed evidence of non-normality at a significance level of0.05. Due to a lack of critical values for the Beta distribution, we resort to a lower power Kolmogorov–Smirnov testto compare model [11]. At the same 0.05 significance level, we get the results in Table 1. In all four domains, the Betadistribution tends to fit queries better than the Normal distribution.5.4. Accuracy of Bayesian error barsAnother way to compare the Normal and Beta models is based on their respective performance on some task. Ifa model is accurate then functionals of that model, such as credible regions of the posterior response, should also beaccurate.As mentioned above, a (1 − δ) credible set is a region ω ⊂ (cid:17) such that P(q(Θ) ∈ ω(D)) = (1 − δ). If our modelof the posterior response distribution is good, then the expected fraction of query samples Qj in ω should be (1 − δ).Here, for each query q, we first used MEANVAR to compute the mean and variance of the response, then usedthese values to compute a 90% credible region for the Normal distribution [LN , UN ], and for the Beta distribution[LB , UB ]. We then drew r = 1000 instances from q(Θ), using the procedure described in Section 5.1, and recordedwhat fraction of instances appeared in [LN , UN ] (resp., [LB , UB ]). If the distributional assumption is correct, weexpect this ratio to be close to 0.9. The results are presented in Fig. 12.While both approximations had problematic cases, the Beta distribution produces more accurate intervals for mostqueries. The problems with the Normal model are conspicuous when the mean query response is near 0 or 1, whichis when q(Θ) exhibits significant skew. Repeating this experiment with an 80% credible interval produces similarresults.1313 We considered yet other intervals. However, larger intervals (e.g., 95% or even 99%) are contrary to the goal of verifying whether the Normalor Beta distributions are good models of the posterior, as there are more distributions that will satisfy a given 0.99 interval by chance than there aredistributions that will satisfy a 0.90 interval by chance. By contrast, a really small interval (like 50% or 10%) would reflect only the model qualityimmediately around E[q(Θ)].T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513503(a) Alarm (Normal)(b) Insurance (Normal)(c) Hailfinder (Normal)(d) Alarm (Beta)(e) Insurance (Beta)(f) Hailfinder (Beta)Fig. 12. Query sample coverage of computed 90% Bayesian credible intervals. (a)–(c) assumes q(Θ) is Normally distributed; (d)–(f) assumes q(Θ)is Beta distributed. Each point represents a query, sorted in order of expected response. The horizontal dotted line represents the desired result: 90%coverage.(a) Normal approximation: ˜σ 2h|e:BS(b) Beta approximation: ˜σ 2h|e:BSFig. 13. Query sample coverage of computed 90% Bayesian credible intervals. These plots were generated using the same queries and samples asFigs. 12(a) and 12(d), except ˜σ 2h|e:BS (Eq. (18)) was used instead of ˜σ 2h|e (Eq. (7)).5.5. Binomial varianceSeveral researchers have suggested (personal communication) that the variance of the response may be muchwhere m is the total number of tuples, or= q( ˆΘ) (1−q( ˆΘ))msimpler, perhaps as trivial as the binomial variance ˜σ 2perhapsh|e:BS504T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513˜σ 2h|e:BS= q( ˆΘ) (1 − q( ˆΘ))m + k + 1(18)in the Bayesian framework when we assume uniform priors and the query variable has k values. To investigate thisclaim, we repeated the coverage test on Alarm, but used this ˜σ 2h|e (Eq. (7)). Fig. 13(a)(resp., Fig. 13(b)) shows the results, assuming the distribution is Normal (resp., Beta). Clearly this approximationperforms poorly in both instances. This is expected: Eq. (18) matches Eq. (7) essentially only if the query variable isnot connected to any other nodes:h|e:BS approximation in place of ˜σ 2Observation 5. If the query variable H is k-ary, is given uniform priors, and is not connected to any other nodes, thenfor any evidence E = e, ˜σ 2h|e= q( ˆΘ) (1 − q( ˆΘ))/(1 + k + m).In general, when there are connections, this variance will be larger, as the sum will include other positive quantities.5.6. Timing informationBoth BUCKELIM and BUCKELIM+ have O(n · exp(w)) time complexity, where w is the induced treewidth of+ depends on thethe ordering. While BUCKELIM+ must compute derivatives withaverage size of the functions after running BUCKELIM. This is because BUCKELIMrespect to all of the functions in the buckets after BUCKELIM. For example, on the 100 Alarm queries, the averagecost of our MEANVAR algorithm was 3.3 times that of BUCKELIM alone.+ is obviously slower, the additional cost of running BUCKELIM6. Related workOur results provide a way to compute the variance of a belief net’s response to a query, based on the posteriordistribution over Θ, which is determined by the data sample. This is done using the “Delta method”, which basicallypropagates the variance of each parameter, based on the partial derivatives.Kleiter [26] similarly uses this method within a stochastic simulation technique to approximate the mean andvariance of a query. His methodology appears to be more general than ours, allowing incomplete data, but it is alsomore computationally intensive. His system calculates an approximate variance for a complete query at each iterationof the simulation using an expression derived by the Delta method. While his derivation is somewhat similar to theone given in our Appendix B, we obtain a different expression for the approximate variance. We have been unableto verify whether the two expressions are equivalent. Kleiter also presents an exact result for a network with twonodes, A → B. He shows that, given complete data and an appropriate prior distribution, the query P(A | B) has aBeta distribution. In recent work, Hooper [24] generalizes this result using ideas related to likelihood equivalence ofthe BDe metric [23]; i.e., given a BDe prior and complete data, a query has a Beta distribution if can be representedas a CPtable parameter for an equivalent DAG structure inducing the same dependence model. In addition, our paper+, as welldiffers from Kleiter’s by also providing an effective way to compute the variance information BUCKELIMas empirical evidence that our approach typically works effectively, despite its approximations.Several other researchers also consider the posterior distribution over CPtables, but for different purposes. Forexample, Cooper and Herskovits [9] use it to compute the expected response to a query (see Eq. (3)). We extendtheir foundational result by computing the variance of the response as well. Similarly, while many BN-learningalgorithms compute the posterior distribution over CPtables [22], most seek a single set of parameters that maximizesthe likelihood, which again is different from our task.Others, including [42], use this response variance for various tasks, including a bias-variance analysis for variousprobabilistic classifiers (similar to [18]). These earlier systems, however, will estimate this quantity empirically—sometimes based on replicates of the Θ parameters, but more often based on a set of training instances from the basedomain D (each used to instantiate the parameters, and then to compute the response to a fixed question, etc.). Weprovide empirical evidence that our analytic approximations are as accurate, but much faster to compute: Most oftheir empirical estimates require hundreds or thousands of replicates, which means they require a computation that ishundreds or thousands of times more than just computing a single response. By contrast, we noted in Section 5.6 thatour approach can require a total computation only a small factor slower than just computing the response itself. Forcomplete queries (Appendix A), the additional cost needed to compute the variance is negligible.T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513505Our main theoretical result (Theorem 2) uses the sensitivity of the query to each CPtable entry. Many other projectsconsider such sensitivity analyses, providing mechanisms for propagating ranges of CPtable values to produce a rangein the response; cf., [8,10,12,29,30]. Those papers do not consider variance, but typically assume that parameters areintervals to be propagated throughout the network. While they require the user to explicitly specify the range of a localCPtable entry, our work uses a data sample as the source of these “intervals”, in that we consider intervals based onthe posterior distribution of the CPtable parameters {Θc|f}, which in turn is based on the observed data. This assumesthe CPtable parameters are independent Dirichlet rows, which is in accord with common models of learning beliefnetwork parameters.While our system must propagate all of the “ranges”, most of the other systems only propagate a single range,based on the one associated derivative. One exception is the Darwiche [12] system, which can simultaneously pro-duce all of the derivatives. This system compiles a belief network into a polynomial representation, which can thenbe symbolically differentiated. There appears to be a close similarity between his approach and our (independently+ algorithm (Section 4.2), as his polynomial can be viewed as a symbolic representation of thedeveloped) BUCKELIMvariable elimination process, and his differentiation proceeds from the leaves of the tree-representation of the polyno-+ traces over time. However, Darwiche doesmial to its root, tracing out the same pattern in space that our BUCKELIMnot consider our error-bar application, and so does not include the additional optimizations we could incorporate.Excluding the [12] result, none of the other projects provides an efficient way to compute those partial derivatively.Also, some of those other papers focus on properties of this derivative—e.g., when it is 0 for some specific CPtableentry. Note this information can be derived from Eq. (8). Finally, while some other results deal only with singlyconnected networks [10], our analysis holds for arbitrary structures.Our analysis also connects to work on abstractions, which also involves determining how influential a CPtable entryis, with respect to a query, towards deciding whether to include a specific node or arc [17]. Their goal is typicallycomputational efficiency in computing that response, either exactly or approximately. By contrast, our focus is incomputing the error-bars around the response, independent of the time required to determine that results.Lastly, our experiments require instances drawn from q(Θ), which can themselves be used to estimate approximatecredible intervals. There are applications of sampling methods to other problems in belief nets, which can possiblybe confused with our concerns. For example, in stochastic sampling inference algorithms, confidence intervals onthe posterior [7] refer to the distribution induced by sampling. However, the underlying network has no parameteruncertainty in those applications.7. ConclusionFurther extensionsOur current system has been implemented, and our data indicates that reasonably accurate error-bars can be pro-duced. There are several possible extensions. One class of extensions involves discharging the assumptions listed inDefinition 1. It may be possible to deal with situations where a single correct structure is not provided. Instead, wemay be given a distribution over structures or be forced to learn both structure and parameters from a data sample.(However, our empirical evidence on tasks that use these approximations [18,32] suggest that Eq. (7) works fairly welleven if the model structure is wrong.) We assume that the network parameters, Θ, can be decomposed into Dirichletrow distributions. It would be useful to apply a similar analysis to alternate CPtable encodings, such as Noisy-OR [34]or CP-Trees [5].The normative process, depicted in Fig. 1, starts with completely specified training instances. It is not clear how touse an incomplete training sample. First, the posterior distribution over Θ is not guaranteed to be Dirichlet. Second, itis not clear how to compute quantities like effective sample size. The naïve EM algorithm for learning parameters [22]will produce an effective sample size that is too large. Note, however, that our technique is independent of the sourceof the Θ distributions; i.e., they apply whenever we are given a product of Dirichlet distributions, however they areobtained.While our system takes advantage of optimizations available to Bucket Elimination algorithms (e.g., optimizedvariable orderings), the bucket data structures must be initialized for each query. It would be desirable to use amultiple query algorithm for inference and calculation of the partial derivatives, as this would amortize the costof initializing tables with evidence, and would also us to reuse intermediate computations shared by different queries.506T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513Query-DAGs [15], given its similarity to Bucket Elimination, is an obvious candidate; another option is JunctionTrees [25].Appendix A provided a very simple algorithm for computing our variance approximation ˜σ 2h|e for the special classof “complete data” queries, and then showed that we could use this to quickly deal with any query to a Naïve Bayesstructure (provided only the query node is the root). It would be useful to develop other specialized algorithms (thatsimilarly bypass BUCKELIM+) for other specific network topologies.ContributionsMany real-world systems work by reasoning probabilistically, based on a given belief net model. When beliefnet parameters are based on a finite random sample, these parameters can be viewed as random variables, whoseuncertainty induces uncertainty in the response to a given query. This paper addresses the challenge of computing thisposterior distribution of a belief net’s response to a query.We define the task, then prove that, given standard assumptions about the parameters, this response distributionis asymptotically Normal, and provide its mean and asymptotic variance (Theorem 2), from which we can estimatethe associated credible regions (“Bayesian error bars”) around the expected response. We also connect this task tothe well-understood problem of learning belief networks parameters from complete data, and provide an algorithmMEANVAR for computing this asymptotic variance for any query on any belief net. We prove both that MEANVAR iscorrect, and that it has the same asymptotic complexity as belief net inference. This procedure, in effect, propagates theuncertainty of each parameter based on its partial derivative. The subroutine used to compute all of these derivatives,+, is of independent interest as these derivatives have many uses outside the scope of this paper [15,BUCKELIM38]. We also provide a much simpler “straight-line” algorithm for efficiently computing the variance in two commonsituations: “complete data” queries, and Naïve Bayes inference.Our underlying theoretical claims, that the distribution is Normal and has variance ˜σ 2h|e (Eq. (7)), are only asymp-totic; they say nothing about the properties of this distribution given only a finite sample. We therefore ran a bodyof empirical tests to investigate the performance. Our results show that our approximate variance ˜σ 2h|e is extremelyclose to correct, given even small sample sizes. However, the associated distribution is often not close to Normal. Ourexperiments did show that this distribution is often well-approximated by the Beta distribution, and in particular, theassociated error-bars are typically fairly accurate.In earlier works, we have already identified two tasks that can use these variance quantities: discriminative modelselection, and as a way to combine responses from different Bayesian classifiers. These existing tasks are enabledby our (relatively) efficient methods for approximating variance. We eagerly anticipate the emergence of many otherapplications.AcknowledgementsWe are grateful for the many comments and suggestions received from Adnan Darwiche and the three anonymousreviewers. All authors gratefully acknowledge the generous support of NSERC. T.v.A. and A.S. also acknowledgesupport from iCORE, A.S. acknowledges further support in part by the National Science Foundation (grant IIS-0325581), and R.G. from the Alberta Ingenuity Center for Machine Learning. Most of this work was done whileT.v.A and A.S. were students at the University of Alberta.Appendix A. “Complete query” caseThis appendix considers the challenge of computing the variance of the response to a query P(H = h | E = e) insimple case where the query is “complete”, in that the evidence E includes every variable except the query H. Herewe show straight-line code that can compute the variance (as well as expected value) of the response.Using the identity [12,19]h|e (Θ) = ∂P(h | e)q(c|f)∂Θc|f= 1Θc|f(cid:4)(cid:5)PΘ (c, f, h | e) − PΘ (h | e) PΘ(c, f | e)T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513507(where each term PΘ (·) refers to the value computed by the belief net whose parameters are instantiated by Θ), wecan obtain the following derivative-free form of Eq. (8):vh|e(C|f) =(cid:22) (cid:2)(cid:5)(cid:4)P(c, f, h | e) − P(h | e)P(c, f | e)1c∈Cc|f(cid:5)(cid:4)P(f, h | e) − P(h | e) P(f | e)−2(cid:23)2where here (and below) we use P(·) for P ˆΘ (·), where ˆΘ is the posterior mean. We can trivial read off that vh|e(C|f)(Eq. (A.1)) will be 0 whenever H⊥C, F|E.The following theorem shows that most of these vh|e(C|f) terms are 0, and the remaining ones are trivial to compute.Theorem 6. Given the “complete query” P(H = +h | E = e), where E includes every non-H variable, the only non-0v+h|e(C|F = f) terms are . . .• associated with the query variable H: Assuming H’s parents are {M1, . . . , Mk}, and the evidence E = e includesthe +m = [+m1, . . . , +mk] assignment to these variables,v+h|e(H| + m) = P(+h | e)2(cid:18)(cid:3)P(h | e)2ˆΘh|+m+[1 − 2P(+h | e)]ˆΘ+h|+m(cid:19)(A.2)h∈H(For every other −m (cid:12)= +m assignment to the parents, v+h|e(H| − m) = 0.)• associated with H’s children: For each child S of H, with parent set {H, U1, . . . , Ur }, where the evidence E = eincludes +uuu = [+u1, . . . , +ur ] and S = +s,v+h|e(S| + h, +uuu) = P(+h | e)2(cid:5)(cid:4)21 − P(+h | e)(cid:18)1ˆΘ+s|+h,+uuu(cid:19)− 1and for each h(cid:5) (cid:12)= +hv+h|e(S|h(cid:5), +uuu) = P(+h | e)2 P(h(cid:5) | e)2(cid:18)1ˆΘ+s|h(cid:5),+uuu(cid:19)− 1and v+h|e(S|h, −uuu) = 0 for every other parental assignment [h, −uuu] where −uuu (cid:12)= +uuu.(A.1)(A.3)(A.4)Connecting this with the general graph in Fig. A.1, we need only deal with the CPtable rows associated with H andits children, the Si ’s. Notice the only values that could influence the variance correspond to these variables and theirparents, which is exactly the Markov blanket around H (which are the double-circled, but not triple-circled, nodes inFig. A.1). This makes sense, as these are the only values that influence the response itself. Note this means Theorem 6does not require that the evidence E include every non-query variable; for this theorem to apply, we need only requirethat E include all the variables in the Markov blanket around H.Fig. A.1. Example of a Belief Net structure, for computing P(H = h | E = e)—used to illustrate Theorem 6.508T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513Here, if H involves |H| = r classes, and it has (cid:4) children, then we can compute the variance by adding up only1 + (cid:4) × r quantities. Moreover, each of these computations is easy, as each ˆΘ·|· is simply a quick look-up, and theP(h | e) terms are trivial given that the evidence E includes the Markov blanket around H. (Moreover, this quantityhas probably already been computed, as it was needed to compute the response—note the basic inference algorithmtypically has already computed P(hi, e) for each hi ∈ H.)We close this section with two further simplifications. First, when dealing with binary classes H = {+h, −h},Eq. (A.2) reduces tov+h|e(H| + m) = P(+h | e)2 P(−h | e)2(cid:18)1ˆΘ+h|+m+1ˆΘ−h|+m(cid:19)and both Eqs. (A.3) and (A.4) reduce tov+h|e(S|h, +uuu) = P(+h | e)2 P(−h | e)2(cid:18)1ˆΘ+s|h,+uuu(cid:19)− 1Hence, using the notation defined above, the total variance here is simply(cid:18)(cid:3)(cid:3)(cid:8)(cid:9)(cid:19)11 + mH|+f+h|e(Θ) = P(+h | e)2 P(−h | e)2˜σ 21ˆΘh|+fSecond, recall from above that we can ignore “barren nodes”—aka “uninstantiated descendants”. For example,imagine that the evidence E = e does not include a value for one of the query variable’s children nor does not it includevalues for any of that child’s descendants—e.g., in Fig. A.1, imagine the evidence E does not include values for S1nor T1 nor T2. We can then ignore this branch. (This follows from the observation that the values of these variables donot contribute to the response [31] and so their associated parameters will also be irrelevant to the variance.)11 + mS|h,+uuu1ˆΘ+s|h,+uuu− 1+ShNote this means we can always quickly compute this variance approximation for a Naïve Bayes structure, whichwill require computing only 1 + (cid:4)(cid:5) × r quantities, where (cid:4)(cid:5) (cid:2) (cid:4) is the number of H’s children that are instantiated.Appendix B. ProofsProof of Theorem 2. The following is a more detailed version of the proof in [40], employing the Delta method.Consider the first-order Taylor expansionq(Θ) = q( ˆΘ) + L + Rwhere the linear term isL = [Θ − ˆΘ]T · q(cid:5)( ˆΘ)and the remainder term is[Θ − ˆΘ]T · qR = 12(cid:5)(cid:5)( ˜Θ) · [Θ − ˆΘ]1mC|fwith ˜Θ = ˆΘ + a(Θ − ˆΘ) for some a ∈ (0, 1). Here q(cid:5)( ˆΘ) is the vector of first partial derivatives evaluated at ˆΘ (whichis composed of the subvectors, q(C|f)h|e ( ˆΘ)), q(cid:5)(cid:5)( ˆΘ) is the matrix of second partial derivatives evaluated at ˆΘ, and thesuperscript T denotes transposition. Under our asymptotic framework, the variances of the components Θc|f of Θ are→ 0, so Θ converges in probability to ˆΘ. Since the components ˆΘc|f of ˆΘ are strictly between 0 and 1,of orderit follows that components of the matrix q(cid:5)(cid:5)(Θ) remain uniformly bounded for all Θ within an open neighborhood ofˆΘ. Consequently, the remainder term R converges to zero at a rate faster than the linear term L, so R is asymptoticallynegligible compared with L. We approximate q(Θ) − q( ˆΘ) by L, and we define ˜σ 2h|e to be the variance of the L term.Now (q(Θ) − q( ˆΘ))/ ˜σh|e and L/ ˜σh|e have the same asymptotic distribution. We claim that this distribution is thestandard Normal distribution. First note that L/ ˜σh|e has mean 0 and variance 1. Asymptotic normality follows from thefact that CPtable rows Θ TC|f are independent and each row (after suitable standardization) is asymptotically multivariate(cid:24)mC|f + 1 (ΘC|f − ˆΘC|f) converges in distribution to a multivariate Normal distribution withNormal. More precisely,ˆΘ TC|f, where Diag( ˆΘC|f) is a diagonal matrix with themean vector (0, . . . , 0) and covariance matrix Diag( ˆΘC|f) − ˆΘC|fT. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513509(cid:3)components of ˆΘC|f along the diagonal; see [3]. We can thus express L/ ˜σh|e as a linear combination of independent,asymptotically Normal terms. In this expression, we can standardize each of the independent random variables tohave unit variance. We thus write L/ ˜σh|e =r ar Zr , where r indexes all CPtable rows in the network, the Zr areindependent random variables with mean 0 and variance 1, and each Zr is asymptotically normal. The coefficientsar do not necessarily remain fixed under our asymptotic framework, but the coefficients must satisfy the constraint(cid:2)= 1 since L/ ˜σh|e has variance one. It then follows that L/ ˜σh|e is asymptotically standard normal.a2rThe proof is completed by deriving an expression for ˜σ 2h|e, which we defined as the variance of the linear term L.(cid:2)Since CPtable rows are independent, this variance can be decomposed as˜σ 2h|e=h|e ( ˆΘ)T · Cov(ΘC|f) · q(C|f)q(C|f)h|e ( ˆΘ)(B.1)C,fwhere the summation ranges over all CPtable rows and the covariance matrix for ΘC|f isCov(ΘC|f) =1mC|f + 1(Diag( ˆΘC|f) − ˆΘC|fˆΘ TC|f)It is then straightforward to obtain Eqs. (7) and (8). (cid:2)Comment#1: Assumptions underlying Theorem 2. The asymptotic framework in Theorem 2 involves only theposterior distribution, but not its derivation from sample data. The following argument shows that the assumptionsare supported by large-sample theory, provided the Dirichlet prior distribution is appropriate. Suppose the CPtableparameters are in fact generated by the assumed prior. It then follows, with probability one, that each Θc|f is strictlybetween zero and one. Now consider the behavior of the posterior distribution as the number of complete trainingcases becomes arbitrarily large. In the frequentist perspective, conditioning on the parameters and not the sampledata, the effective sample sizes mC|f and posterior means ˆΘc|f are random variables, each mC|f becomes arbitrarilylarge (with probability one), and each ˆΘc|f converges in probability to Θc|f. The asymptotic framework in Theorem 2is similar to this large-sample framework, differing primarily in the assumption (appropriate in a Bayesian context)that the posterior means are fixed.The text in Section 3 already argued that one can deal with deterministic links, within an asymptotic framework.(Here, we basically ignore these CPtable rows.) What happens if instead a Dirichlet prior is incorrectly adopted here?Large-sample theory shows that ˆΘ converges toward the boundary. The variance approximation (Eq. (7)) may still bevalid, but this is not guaranteed since higher-order terms in the expansion could be non-negligible. It is also possiblethat Eq. (7) is valid but asymptotic normality fails. For a simple example of this last scenario, suppose the query is asingle belief net parameter that is degenerate: q(Θ) = Θc|f = 0. Suppose the prior distribution for Θc|f is Be(a, b).(Use a = b = 1 for a flat prior.) There is zero probability that (C, F) = (c, f) so the posterior distribution will beBe(a, b + m) where m is the number of samples with F = f. The posterior variance is given by Eq. (7), an exact resulthere and not an approximation. A simple calculation shows that, as m → ∞, the posterior distribution of q(Θ)/ ˜σh|e√a, and variance 1. If a = 1, then this Gamma distributionis the Gamma distribution with shape parameter a, meanis the exponential distribution.1 < σ 2Comment#2: Why | ˜σ 2 − σ 2| can be large when σ 2 is large. Consider two queries q1(Θ) and q2(Θ) with corre-sponding variances σ 22 . The larger variance associated with q2 may arise from a combination of two factors.First, the components of Θ relevant to q2 may be more variable than components relevant to q1—i.e., there may besome Θi terms where ∂q2(Θ)is large, which have large (co)variances. The linear approximation describes local be-∂Θihavior of the function (here qi(·)) near its mean, so increased variation may produce a larger remainder term. Second,the first derivatives of q2 may tend to be larger than those of q1. In practice, larger linear terms are often associatedwith larger quadratic terms; e.g., statistical variable selection methods for polynomial models will seldom delete alinear term while retaining a corresponding quadratic term [35]. Larger quadratic terms produce a larger remainderterm. Both arguments support the claim that approximations to variance produced by the first-order Delta method canbe less accurate when the variance is larger. (We note, again, that in practice, even the largest of these errors is stillquite small.)510T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513Proof of Theorem 3. The proof is by induction on the bucket ordering. The basis is the first bucket b∅; it is trivial tocompute the derivative function of its output g∅ here, as it is the constant 1. To prove the inductive step, assume wehave computed the derivatives of all of functions in the first J buckets (i.e., we know ∂yfor all i ∈ {∅, 1, . . . , J − 1}∂fi,kand all k) and that Scheme( ∂y) = Scheme(fi,k), and now we now want to compute the derivatives for functions∂fi,kfJ,k in bucket bJ . Recall that BUCKELIM used these values in forming fI,1 = Elim(XJ , Join(fJ,1, . . . , fJ,k)) whichappears in the earlier bI bucket.⇐(cid:22)(cid:22)⇒Ordering for Step A (≡ BUCKELIM)Ordering for Step B(cid:22)⇒b∅. . .bIfI,1(XI , . . .)bJ. . .. . . fJ,1(XJ , . . .)fJ,2(XJ , . . .)...fJ,k(XJ , . . .)⇐(cid:22). . .To simplify notation, we will simply write f (x) for f (x|Scheme(f )).Now observe∂y∂fJ,(cid:4)(cid:8)(cid:8)= ElimZ, Join(cid:9)(cid:9)∂y∂fI,1,∂fI,1∂fJ,(cid:4)(B.2)(B.3)where Z are the variables that are not in fJ,(cid:4); see Eq. (B.5) below. (The Join here is just a trivial application of thechain rule—also known as “backpropagation” [37].) As I < J , by the inductive assumption we can assume that wehave already computed the first term ∂y) = Scheme(fI,1). Moreover, as∂fI,1fI,1(x) = Elim(XJ , Join(fJ,1, · · · , fJ,k)) =(cid:7), and that its arguments are Scheme( ∂y∂fI,1k fJ,k(x), the second term within the Join of Eq. (B.3),xJ(cid:12)(cid:2)(cid:15)(x1..I, xJ ) = Join(cid:6){fJ,k | k (cid:12)= (cid:4)}=∂fI,1∂fJ,(cid:4)fJ,k(x1..I, xJ )k(cid:12)=(cid:4)(B.4)is just the product of the other functions in the J th bucket [41], where x1..I corresponds to the variables within fI,1., will involve the variables in fI,1 as well as XJ . It is joined with ∂y∂fI,1should depend on onlyThe result of this computation, ∂fI,1∂fJ,(cid:4)(Eq. (B.3)), to produce a function with arguments in Scheme(fI,1) ∪ {XJ }. AsScheme(fJ,(cid:4)), the only remaining step is to marginalize out the extra variables∂y∂fJ,(cid:4)Z = Scheme(fI,1) ∪ {XJ } − Scheme(fJ,(cid:4));(B.5)this is done in second part of Eq. (14). This also fulfills the second part of the inductive step, as it insures thatScheme( ∂y∂J,(cid:4) ) = Scheme(fJ,(cid:4)). (cid:2)+ needs to consider at most 2n functions, as Step A (which is BUCKELIM) startsProof of Theorem 4. BUCKELIMwith at most one function (CPtable) for each of the n variables, then adds at most one new function each time avariable is eliminated.We therefore need only show that the cost of computing the derivative of each of these functions is O(r w), wherer is the largest domain of any of the variables, and w is the tree-width. We will consider the general case, using thenotation from the Eq. (B.2), where fI,1 = Elim(XJ , Join(bXJ )).Combining Eqs. (B.3) and (B.4), we obtain(cid:8)(cid:8)= ElimZ, Join∂y∂fJ,(cid:4)∂y∂fI,1, {fJ,k | k (cid:12)= (cid:4)}(cid:9)(cid:9)(B.6)Now consider the variables involved in the joinScheme= Scheme(fI,1)(cid:8)(cid:9)∂y∂fI,1Scheme(fJ,k) ⊂ Scheme(fI,1) ∪ {XJ } ∀kT. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513511which means this entire computation will involve no more than 1 + |Scheme(fI,1)| variables, which is at most 1 + w,as |Scheme(fI,1)| is at most the tree-width w.Therefore joining the required tables in Eq. (B.6) requires O(r w+1) = O(r w) time and space (as r is a constant).Of course, after the join computation is done, the algorithm will then marginalize out the variables in Z (Eq. (B.5)) toproduce a function (for the derivative) that has, at most, w variables. The cost of marginalization is at most the cost ofthe join, and so the overall complexity of computing a single derivative is O(r w). (cid:2)Proof of Observation 5. As noted above, we need only consider the CPtables of nodes that are not d-separatedfrom the query node H, which means we need consider only this single H node. The variance ˜σ 2h|e is therefore the= P(h) (1 − P(h))/(1 + mH). (Recallvariance associated with this single Dirichlet-distributed variable, which is σ 2HP(h) = q( ˆΘ).) As H has no parents, each of the n training instances contributes to one of its Dirichlet parameters.As H is k-ary and we start with uniform priors, this means mH = k + n. (Note we could also derive this value usingEqs. (A.2) and (A.1).) (cid:2)Proof of Theorem 6. As we sweep over the vh|e(C|f) terms, we need to consider three cases, depending on the queryvariable H: (i) H is C, (ii) H appears in F, or (iii) neither.Case (iii): If H (cid:12)= C and H /∈ F, then vh|e(C|F = f) = 0.14Here, as the evidence variables E includes every variable except H, it includes both C and all of F. To simplifynotation, write E = {. . . , +c, +f, . . .} to indicate the specific assignments used, where +f = [+f1, . . . , +fj ] refersto E’s assignment to all of C’s parents.Notice vh|e(C| − f) = 0 whenever −f (cid:12)= +f:(cid:6)c, F = −f , h | . . . , F = +f , . . .P(c, −f, h | e) = P(cid:7)= 0Similarly P(c, −f | e) = 0, P(−f, h | e) = 0 and P(−f | e) = 0.We therefore consider only F = +f. Here P(+f, h | e) = P(+f, h | . . . , +f, . . .) = P(h | e) and P(+f | e) = 1, whichwe use to reduce the bottom line of Eq. (A.1):(cid:5)(cid:4)P(+f, h | e) − P(h | e) P(+f | e)2 =(cid:4)P(h | e) − P(h | e) × 1(cid:5)2 = 0Now consider the top part of Eq. (A.1), and observe we need only consider the value of +c value of C withinthe summation. (For each −c (cid:12)= +c, P(−c, +f, h | e) = P(−c, +f, h | . . . , +c, . . .) = 0.) As above, we have thatP(c, +f, h | e) = P(c, h | e) and P(c, +f | e) = P(c | e). Hence,(cid:3)c∈C1ˆΘc|+f= 1ˆΘ+c|+f(cid:5)(cid:4)2P(c, +f, h | e) − P(h | e) P(c, +f | e)(cid:5)(cid:4)P(+c, +f, h | e) − P(h | e) P(+c, +f | e)2= 1ˆΘ+c|+f(cid:4)P(h | e) − P(h | e) × 1(cid:5)2 = 0Case (i): If H = C, where H’s parents are M = {Mi}, thenv+h|e(H| + m) = P(+h | e)2(cid:18)(cid:3)h∈HP(h | e)2ˆΘh|+m+[1 − 2P(+h | e)]ˆΘ+h|+m(cid:19)and v+h|e(H| − m) = 0for −m (cid:12)= +m.As argued above, this v+h|e(H|m) value is 0 when M = −m (cid:12)= +m (the value used within e). When M = +m, wecan again omit the M = m from the various equations; again, this means the second line of Eq. (A.1) is 0. Hence, asthe value of the query variable h = +c,14 Note this follows from the observation that this condition means H⊥C, F|E. We include this case as it provides useful analysis and notation.512T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513v+h|e(H| + m) ==(cid:5)(cid:4)P(h, +m, +h | e) − P(+h | e) P(h, +m | e)2(cid:3)h∈H1ˆΘh|+m(cid:5)(cid:4)1P(+h | e) − P(+h | e) P(+h | e)ˆΘ+h|+m(cid:3)2(cid:5)(cid:4)P(h, +h | e) − P(+h | e) P(h | e)21ˆΘh|+m(cid:5)(cid:4)P(+h | e)[1 − P(+h | e)]+h(cid:12)=+h1ˆΘ+h|+m== P(+h | e)2= P(+h | e)2(cid:18)[1 − P(+h | e)]2ˆΘ+h|+mP(h | e)2ˆΘh|+m+h∈H(cid:18)(cid:3)+(cid:3)h(cid:12)=+hP(h | e)2ˆΘh|+m(cid:19)[1 − 2P(+h | e)]ˆΘ+h|+mh(cid:12)=+h(cid:3)2 +(cid:5)(cid:4)2P(+h | e) P(h | e)1ˆΘh|+m(cid:19)Case (ii): If H ∈ F, then write C’s parents as F = {H} ∪ UUU, and assume the evidence E = e contains both UUU = +uuuand C = +c. Using the same arguments as above, we know that vh|e(C|h, −uuu) = 0 for any −uuu (cid:12)= +uuu. We thereforeconsider only UUU = +uuu, and can then reduce Eq. (A.1) as . . .(cid:3)⎤⎡v+h|e(C|h, +uuu) ==⎢⎣⎡⎢⎣c∈C(cid:4)(cid:5)P(c, h, +uuu, +h | e) − P(+h | e)P(c, h, +uuu | e)1ˆΘc|h,+uuu(cid:5)(cid:4)2P(h, +uuu, +h | e) − P(+h | e) P(h, +uuu | e)−⎤2⎥⎦(cid:5)(cid:4)2P(h, +h | e) − P(+h | e) P(h | e)1ˆΘ+c|h,+uuu(cid:5)(cid:4)2P(h, +h | e) − P(+h | e) P(h | e)−⎥⎦(This uses the fact that each term in the summation with c (cid:12)= +c is 0.)If H = +h, this isv+h|e(C| + h, +uuu) =⎡⎢⎣(cid:4)(cid:5)P(+h | e) − P(+h | e)P(+h | e)1ˆΘ+c|+h,+uuu(cid:5)(cid:4)P(+h | e) − P(+h | e) P(+h | e)−2⎤⎥⎦= [P(+h | e)(1 − P(+h | e))]2but if h(cid:5) (cid:12)= +h,v+h|e(C|h(cid:5), +uuu) =(cid:5)(cid:4)P(+h | e)P(h(cid:5) | e)2(cid:18)1ˆΘ+c|h(cid:5),+uuu(cid:19)− 1References(cid:18)1ˆΘ+c|+h,+uuu2(cid:19)− 1(cid:2)[1] B. Abramson, J. Brown, W. Edwards, A. Murphy, R.L. Winkler, Hailfinder: A Bayesian system for forecasting severe weather, Intl. J. Fore-casting 12 (1) (March 1996) 57–71.[2] T.W. Anderson, D.A. Darling, A test of goodness of fit, J. Amer. Statist. Assoc. 49 (268) (December 1954) 765–769.[3] Y. Akimoto, A note on uniform asymptotic normality of Dirichlet distribution, Math. Japon. 44 (1) (December 1996) 25–30.[4] A.C. Atkinson, Plots, Transformations, and Regression, Oxford, 1985.[5] C. Boutilier, N. Friedman, M. Goldszmidt, D. Koller, Context-specific independence in Bayesian networks, in: Twelfth Annual Conference onUncertainty in Artificial Intelligence (UAI-96), 1996.[6] J. Binder, D. Koller, S.J. Russell, K. Kanazawa, Adaptive probabilistic networks with hidden variables, Machine Learning 29 (2–3) (1997)213–244.T. Van Allen et al. / Artificial Intelligence 172 (2008) 483–513513[7] J. Cheng, M.J. Druzdzel, Confidence inference in Bayesian networks, in: Seventeenth Conference on Uncertainty in Artificial Intelligence(UAI-2001), Morgan Kaufmann Publishers, 2001, pp. 75–82.[8] E.F. Castillo, J.M. Gutiérrez, A.S. Hadi, Sensitivity analysis in discrete Bayesian networks, IEEE Trans. Man Cybernet. Syst. 27 (1997)412–424.[9] G. Cooper, E. Herskovits, A Bayesian method for the induction of probabilistic networks from data, Machine Learning 9 (1992) 309–347.[10] P. Che, R.E. Neapolitan, J. Kenevan, M. Evens, An implementation of a method for computing the uncertainty in inferred probabilities in beliefnetworks, in: 9th Annual Conference on Uncertainty in Artificial Intelligence (UAI-93), Morgan Kaufmann Publishers, 1993, pp. 292–300.[11] D.A. Darling, The Kolmogorov–Smirnov, Cramer–von Mises tests, Ann. Math. Stat. 28 (1957) 823–838.[12] A. Darwiche, A differential approach to inference in Bayesian networks, in: 16th Annual Conference on Uncertainty in Artificial Intelligence(UAI-93), 2000.[13] March 1995, Special issue of “Communications of the ACM”, on Bayesian Networks.[14] R. Dechter, Bucket elimination: A unifying framework for probabilistic inference, in: Learning and Inference in Graphical Models, 1998.[15] A. Darwiche, G.M. Provan, Query DAGs: A practical paradigm for implementing belief network inference, in: Twelfth Conference on Uncer-tainty in Artificial Intelligence (UAI’96), 1996.[16] R.B. D’Agostino, M.A. Stephens, Goodness-of-Fit Techniques, Statistics, Textbooks and Monographs, vol. 68, Marcel Dekker Inc., 1986.[17] R. Greiner, C. Darken, I. Santoso, Efficient reasoning, Computing Surveys, 2001.[18] Y. Guo, R. Greiner, Discriminative model selection for belief net structures, in: Twentieth National Conference on Artificial Intelligence(AAAI-05), Pittsburgh, July 2005, pp. 770–776.[19] R. Greiner, A. Grove, D. Schuurmans, Learning Bayesian nets that perform well, in: 13th Annual Conference on Uncertainty in ArtificialIntelligence (UAI-97), 1997.[20] http://www.cs.ualberta.ca/~greiner/RESEARCH/BNvar.[21] E.H. Herskovits, C.F. Cooper, Algorithms for Bayesian belief-network precomputation, in: Methods of Information in Medicine, 1991,pp. 362–370.[22] D.E. Heckerman, A tutorial on learning with Bayesian networks, in: M.I. Jordan (Ed.), Learning in Graphical Models, 1998.[23] D. Heckerman, D. Geiger, D.M. Chickering, Learning Bayesian networks: The combination of knowledge and statistical data, MachineLearning 20 (1995).[24] P. Hooper, Exact distribution theory for belief net responses, Technical report, University of Alberta, 2007, http://www.stat.ualberta.ca/~hooper/research/papers+talks/exactbeta.pdf.[25] F. Jensen, F. Jensen, Optimal junction trees, in: Tenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-94), Morgan Kauf-mann, San Francisco, CA, 1994.[26] G. Kleiter, Propagating imprecise probabilities in Bayesian networks, Artificial Intelligence 88 (1996).[27] R.L. Keeney, H. Raiffa, Decision with Multiple Objectives: Preferences and Value Tradeoffs, John Wiley & Sons, New York, 1976.[28] H. Korth, A. Silberschatz, S. Sudarshan, Database System Concepts, McGraw Hill, 1998.[29] U. Kjaerulff, L.C. Van der Gaag, Making sensitivity analysis computationally efficient, in: 16th Conference on Uncertainty in ArtificialIntelligence (UAI-00), 2000.[30] K.B. Laskey, Sensitivity analysis for probability assessments in Bayesian networks, IEEE Trans. Man Cybernet. Syst. 25 (6) (1995) 901–909.[31] Y. Lin, M.J. Druzdzel, Computational advantages of relevance reasoning in Bayesian beliefs networks, in: Uncertainty in Artificial Intelligence(UAI-97), 1997, pp. 342–350.[32] C. Lee, R. Greiner, S. Wang, Using variance estimates to combine Bayesian classifiers, in: International Conference on Machine Learning(ICML’06), Pittsburgh, June 2006.[33] A.W. Moore, Private communication, May 2003.[34] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, Morgan Kaufmann, 1988.[35] J.L. Peixoto, Hierarchical variable selection in polynomial regression models, American Statistician 41 (1987).[36] B. Ripley, Pattern Recognition and Neural Networks, Cambridge University Press, Cambridge UK, 1996.[37] D.E. Rumelhart, J.L. McClelland, the PDP Research Group (Eds.), Parallel Distributed Processing: Explorations in the Microstructure ofCognition, vol. 1: Foundations, The MIT Press, Cambridge, 1986.[38] A.P. Singh, What to do when you don’t have much data: Issues in small sample parameter learning in Bayesian networks, Master’s thesis,University of Alberta, 2004.[39] D.J. Spiegelhalter, S.L. Lauritzen, Sequential updating of conditional probabilities on directed graphical structures, Networks (1990) 579–605.[40] T. Van Allen, R. Greiner, P. Hooper, Bayesian error-bars for belief net inference, in: 17th Conference on Uncertainty in Artificial Intelligence(UAI-01), Aug 2001.[41] T. Van Allen, Handling uncertainty when you’re handling uncertainty: Model selection and error bars for belief networks, Master’s thesis,Department of Computing Science, University of Alberta, 2000.[42] G.I. Webb, P. Conilione, Estimating bias and variance from data, Technical report, 2005.[43] S. Wilks, Mathematical Statistics, John Wiley & Sons, 1962.