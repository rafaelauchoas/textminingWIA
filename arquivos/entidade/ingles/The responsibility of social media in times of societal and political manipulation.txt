Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   European Journal of Operational Research 291 (2021) 906–917 Contents lists available at ScienceDirect European Journal of Operational Research journal homepage: www.elsevier.com/locate/ejor The responsibility of social media in times of societal and political manipulation Ulrike Reisach Department of Information Management, Prof. Dr. Ulrike Reisach, Neu-Ulm University of Applied Sciences, Wiley-Street 1, D-89231 Neu-Ulm, Germany a r t i c l e i n f o a b s t r a c t Article history: Received 17 December 2019 Accepted 16 September 2020 Available online 22 September 2020 Keywords: Ethics in OR Decision-making Artificial intelligence Behavioural OR Education The way electorates were influenced to vote for the Brexit referendum, and in presidential elections both in Brazil and the USA, has accelerated a debate about whether and how machine learning techniques can influence citizens’ decisions. The access to balanced information is endangered if digital political ma- nipulation can influence voters. The techniques of profiling and targeting on social media platforms can be used for advertising as well as for propaganda: Through tracking of a person’s online behaviour, al- gorithms of social media platforms can create profiles of users. These can be used for the provision of recommendations or pieces of information to specific target groups. As a result, propaganda and dis- information can influence the opinions and (election) decisions of voters much more powerfully than previously. In order to counter disinformation and societal polarization, the paper proposes a responsibility-based approach for social media platforms in diverse political contexts. Based on the implementation require- ments of the “Ethics Guidelines for Trustworthy Artificial Intelligence” of the European Commission, the eth- ical principles will be operationalized, as far as they are directly relevant for the safeguarding of demo- cratic societies. The resulting suggestions show how the social media platform providers can minimize risks for societies through responsible action in the fields of human rights, education and transparency of algorithmic decisions. © 2020 The Author. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) 1. Introduction and research methodology 1.1. Aims and research question During the Corona-crisis in 2020, the degree of disinformation has reached a level which could endanger the proper functioning of democratic decision-making. Crises have always been a time of rising emotions and anxiety. These seem to culminate on social media platforms, where citizens and self-proclaimed experts give unsubstantiated advice dealing with Covid-19, or try to identify as- sumingly guilty parties and fabricate conspiracy theories, through amalgamating facts and false interpretations. The more exciting, the even more weird ideas that are shared, including vaccine anx- iety, and doubtful, or even potentially lethal health recipes. In the United States, these developments have fuelled the long- standing debate on whether misrepresentations and recommen- dations still fall under the freedom of speech, or should be ac- companied, e.g. with a fact check advice, or should be filtered out and deleted. The European Commission (EC) aims at combat- ing disinformation and appeals to the social media platforms to install a transparent and consistent moderation of disinformation ( EC, 2020a , 2020b ). Bell’s observation ( 2018 ) that “… techniques for fabricating, editing, and reframing news in harmful ways develop faster than they can be detected and countered …” describes the current situ- ation (p. 5). C. West Churchman asks “which end results are good in an objective sense?” ( Churchman, 1970 ). This question is ap- plies to the current issues of social media platforms: The paper asks whether and how social media platforms can (practically) and should (ethically), deal with risks of societal and political manipu- lation. The EC’s Action Plan on Disinformation ( 2019 ) defines disinfor- mation as is verifiably false or misleading information created, pre- sented and disseminated for economic gain, or to intentionally de- ceive the public. The reasons given for their action are: (a) the potential for far-reaching consequences such as public harm, (b) threats to democratic political and policy-making processes, (c) the risk of endangering the protection of EU citizens’ health, E-mail address: ulrike.reisach@hnu.de security and their environment. https://doi.org/10.1016/j.ejor.2020.09.020 0377-2217/© 2020 The Author. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ) U. Reisach European Journal of Operational Research 291 (2021) 906–917 This research focuses on the threats to democratic decision- making processes. Based on the developments in 2019 and 2020, the EC (2020a) expresses concerns which relate to the main rea- sons for the research: “Disinformation erodes trust in institutions and in digital and traditional media and harms our democracies by hampering the ability of citizens to take informed decisions. It can polarise debates, create or deepen tensions in society and un- dermine electoral systems, and have a wider impact on European security. It impairs freedom of opinion and expression, a funda- mental right enshrined in the Charter of Fundamental Rights of the European Union.” This paper analyses how disinformation endan- gers democratic decision-making and how social media platforms could contribute to tackling those challenges. 1.2. Research methodology Reflecting on the reasons and impacts of disinformation on so- cial media, a hermeneutic process of understanding and interpret- ing the ethical and societal consequences has been chosen. What can be expected is a set of ethically grounded suggestions for ac- tions which could be discussed and implemented by the platform owners. Hermeneutics confronts “quantitative methodologies with qualitative questions” ( van Dijck, 2014 : p. 206). The tools for this process are rooted in the humanities and in social science. Critical reflectivity ( Gregory, 20 0 0 ) is one such method that could be ap- plied to a topic that raises societal and political questions. The re- sults cannot resolve the existing problems of political manipulation completely, due to societal complexity ( DeTombe, 2002 ), associated with almost ubiquitous social media. The philosopher Wilhelm Dilthey (1833–1911) established hermeneutics in the humanities as interpretative sciences in their own right. He studied the rela- tionships between personal experience, its realization in creative expression, and the reflective understanding of this experience; and, finally, the logical development from these to the understand- ing of social groups and historical processes ( Dilthey, 2013 ). Using the concept of hermeneutics and reflective understand- ing, the operation and impacts of social media with regard to the current societal trends are discussed in Part 2. Examples for ma- nipulation are given in Part 3. Concepts of digital media ethics and responsibility are presented in Part 4, and compared with the tra- ditional media’s accountability approaches. Ethics codes for AI are shown and suggestions for responsible action of social media are provided. Based on those, a reflective and strategic corporate re- sponsibility of digital media is introduced as a new concept. Part 5 gives some reflections and limitations. Finally, the conclusion in part 6 offers a global outlook on societal responsibilities. 2. The role of social media platforms and their impacts on societies To explain why societal and political manipulation is a spe- cial issue for social media, the goals, functioning and legal sta- tus of the respective corporate actors need to be clarified. Social media platforms such as Facebook, Twitter, Instagram, YouTube and TikTok facilitate an interactive one-to-few or many-to-many- communication in an international scale. “In the web 2.0 era, an infinite “crowd” of users can anonymously and with almost no cost “voice” criticism and protest, via Twitter and Facebook” ( Fengler, 2012 , p. 184). 2.1. The business model of social media platforms In their official prospectus for the computerized US stock mar- ket NASDAQ, Facebook claimed their vision was “t o make the world more open and connected ” ( Facebook, 2012 ). Nevertheless, their business model is commercial. When going public they disclaim that their goal is profit and shareholder value: “Advertisers can en- gage with users … on Facebook or subsets of our users based on in- formation they have chosen to share with us such as their age, lo- cation, gender, or interests. We offer advertisers a unique combina- tion of reach, relevance, social context, and engagement to enhance the value of their ads.” ( Facebook, 2012 ). Users do not pay but give their data to Facebook who then sells them to their customers, the advertisers. Dwyer and Martin (2017) explain that data cap- ture, data-mining and behavioural advertising are typical activi- ties of social media businesses. The consequence for democracies is that market-driven media tend to favour advertisers and con- sumers over citizens ( Bardoel & d’Haenens, 2004 ). On such plat- forms, traffic and user time (and their data) are important param- eters. Originally social media had positive connotations for creative engagement, political participation and cross-promotion ( Dwyer & Martin, 2017 ). Through media users’ participation, “citizens jour- nalism” should provide manifold perspectives and help to avoid content filtering through governments or editors of traditional me- dia. Inspired by technical advancements, the internet followed the vision of freedom – freedom of thought, freedom of opinion, free- dom of speech and expression, and freedom of information. Digital media allow individuals and smaller groups to become visible and audible. Social media were supposed to allow “the small citizen to have a loud voice”, and bring their opinions to a large audience or group of followers. This opportunity is a blessing and risk, as Evan Williams, founder of Twitter, said in 2017: “I thought once every- body could speak freely and exchange information and ideas, the world is automatically going to be a better place … I was wrong about that” (quoted by Streitfeld, 2017 ). According to Beckers and Harder (2016) who analysed content in Dutch and Flemish news websites on Twitter, those are regularly used as a representa- tion of public opinion which contain, as they say, “strong—mostly negative—emotions”. Social media platforms are subject to US (business) culture, law and convictions: The USA’s “First Amendment ” of 1791 protects the freedom of speech, even in cases where other people are insulted ( Downing, 1999 ). Usually, freedom is limited through the freedom of fellow citizens and their rights, such as dignity. Print media have editors who usually think about the content, and their responsibil- ity and reputation. Most of the social media platforms act under California law, and prefer not to be classified as publishers or me- dia companies. This way they avoid the legal responsibilities that being publishers entail, and avoid being subject to the same legal restrictions and public service obligations as traditional media are ( Frau-Meigs, 2017 ). “… Facebook and other social platforms have long sought safe harbour under Section 230 of the Communica- tions Decency Act and have held firm that it is their legal right to be shielded from liability for the content their users post to their platforms.” ( Bell, 2018 , p. 53). This regulation of the US Code for Telecommunication dates back to the year 1934 and still pro- tects online intermediaries that host or republish speech against a range of laws that might otherwise be used to hold them legally responsible for what others say and do ( Legal Information Institute of Cornell Law School, 2018 ). In the current setting, neither the social media platforms nor their users are subject to journalistic accountability standards. Nev- ertheless, social media platforms should be guided by what serves the good of the society. Risky developments such as political ma- nipulation call for considerations regarding education, responsibil- ity and eventually, as Dwyer and Martin (2017) suggest, monitor- ing diversity and pluralism. Lipschultz (2017) says: “Social media communication raises important ethical issues because it can be perceived as anonymous – crossing borders and cultures world- wide.” (p. 7) Zweig, Deussen and Kraft (2017) therefore call for an 907 U. Reisach European Journal of Operational Research 291 (2021) 906–917 assessment, regulation and control of social media similar to those applying to print media and TV (p. 323). How any kind of regula- tion should be operationalized and implemented depends on the respective society exercising its authority and right to ensure the good use of social media, especially during election time. Possibili- ties and forms of an ethical governance are discussed in chapter 4 of this paper. it trans-border and trans-media, and therefore viral.” ( Frau-Meigs, 2017 ). Viral news is defined as networked news stories which of- ten carry social significance and unexpectedness and spread online mostly through social media in a much faster and wider manner than other news stories ( Al-Rawi, 2019 ). Political profiling may cause the following types of issues for democracies: 2.2. How AI is used for tracking and profiling Artificial Intelligence (AI) facilitates real-time tracking and pre- dictive analysis through transforming social action into online quantified data ( Mayer-Schoenberger & Cukier, 2013 ). Big Data vol- umes allow algorithms to analyse and cluster personal profiles of users much faster and more specifically than before. Through the almost global connectivity of users on social media platforms, ma- chine algorithms can use data for dynamical learning, classifying and predicting the audiences’ online behaviour within and across borders. This technique has been developed to increase the ef- ficiency of marketing: it allows selection of the consumers who are most likely to purchase the respective products or services ( Domingos & Richardson, 2001 , and Ballings & Van den Poel, 2015 ). Advertisers customize their services to address targeted audiences. They try to keep users on a platform as long as possible. This al- lows them to learn more about their interests and to trigger more sales through adverts which are presumably, as the algorithm sug- gests, based on the users’ interests and previous choices. According to Usher (2017) and Wall (2017) , corporate media companies ap- propriate such kind of content because they operate within an in- ternet system dominated by similar commercial values and profit motives. The same type of targeting and prediction is used for manipu- lative pieces of information or recommendations which reach vot- ers who have been categorized as “potentially prone” to absorb this kind of information. Personalities can be classified and pref- erences can be predicted through their data such as friends’ lists, content they read or watch, and online behaviour such as research- ing, shopping and writing. The volume of such data has increased through cross-domain tracking ( Ammicht-Quinn et al., 2018 , p. 7). This is done by collecting data from the activities of their plat- form users on other webpages. This way, social media platforms can track which newspapers’ apps or webpages and articles users read, which political parties’ or candidates’ profiles they visit, how long they remain there. Facebook, for example, collects data from persons who do not even have a Facebook account. This activity is called “cross-domain tracking”. Under US law, this type of data col- lection is no mayor issue: Any action against data collection needs evidence that the data has been abused ( Legal Information Insti- tute of Cornell Law School, 2019 ). While the collection of data is legal in the USA, and thus encourages many data driven business and AI models, this is not the case in Europe. 2.3. How social media may be used for influencing political opinions and election decisions (1) Political ads are highly optimized and targeted to specific in- dividuals. Individual users often do not know how, through whom, and why they are being influenced. (2) “Virality” of online content means an exponential dissemina- tion of information. It is created by “retweeting, favouriting, and replying” and has become an indicator of online mes- sage effectiveness ( Alhabash & McAlister, 2015 ). This effec- tiveness is favourable for advertisers, but also for manipula- tors who use triggers to gain attention. Novelty, aggressive claims, exciting news and assertations are forwarded to like- minded group members. Vosoughi, Roy and Aral (2018) anal- ysed the diffusion of news on Twitter and classified true and false information by using six independent fact-checking organizations. They found out that falsehood diffused sig- nificantly farther, faster, deeper, and more broadly than the truth in all categories. (3) Because certain topics have been classified as “relevant” for the users, they get “more of the same”, meaning the users’ assumptions and prejudices are echoed and reinforced. Par- ticipation in different networks leads to a segmentation and polarization of different groups in a society ( Collier, 2015 ) and diminishes the likelihood of political consensus or com- promise. Additionally, extreme opinions can fortify false be- liefs, and so can false accusations and conspiracy theories that relate to political candidates, or societal and political topics and strategies (Sunstein, 1999). (4) Disinformation is usually not contextualized and their ori- gin often remains opaque. Content cannot always be veri- fied/falsified by fact-checking, and filtering algorithms strug- gle with coded terminology and irony. (5) The anonymity of sources, paired with privacy protection, makes it impossible for police to detect and sue persons who upload illegal and illegitimate content on social me- dia. Since alarming news is shared more frequently, journal- ists and politicians are experiencing defamations or threats, which diminish their preparedness for working, or and standing as a candidate, in difficult times. (6) A communicative “race to the bottom” decreases respect both for human dignity, and for a fact-orientated and bal- anced societal dialogue. Fake news and digital agitation di- minish the common ground on which societies are built, and could erode trust in the democratic process of societal dis- course and co-operation. Albright (2018, p. 55) has asserted: “It is not just the Cam- bridge Analytica scandal, Facebook, Google, and Twitter each fell prey to election-related meddling by outside actors. And most (sic) every platform is struggling with manipulation, extremist content, and uncivil behaviour.” Among the different types of fake news ( Tandoc, Lim & Ling, 2018 ), fabrication, deception, manipulation, and propaganda are those which are frequently used for influenc- ing political opinions. Frau-Meigs (2017) defines disinformation as a toxic, but generally discernible mixture of truth and lies and sees fake news in the category of disinformation. Their “… malicious intent is unprecedented, because information technology makes This list is not exhaustive. It contains the most important issues, and shows how impacts reinforce each other. “Hate speech, fraud- ulent material, deliberate propaganda, and misinformation all grew largely unchecked in an environment where platforms did not po- lice the content they hosted with enough rigour.” ( Bell, 2018 , p. 5). After the Brexit vote, the European Data Protection Supervisor (2018) emphasized: “The principle of electoral transparency is not met if the voters have no freedom to seek, receive and impart in- formation about the process and the candidates, including about the source and spending of financial support received by a candi- date or a party.”908 U. Reisach European Journal of Operational Research 291 (2021) 906–917 3. How societies can be manipulated: examples 3.1. Social media: advertising and propaganda use the same tools People share content in disruptive phases when they are not sure what to believe or decide. They often cannot differentiate be- tween fact-based information and rumours/opinions. Humans are not rational and often react intuitively ( Akerlof & Shiller, 2010 ), therefore negative campaigning creates more reactions and per- sonal, and emotional content wins over complexity. The current problem is the co-incidence of availability of digital media as “burning lenses” for an already rough democracy: conflicts are de- bated in shrill tones, opposing views clash with each other, while a more balanced majority remains silent. People and journalists rely on online information aggregators, which create a self-fulfilling prophecy with a popular piece of information becoming even more popular ( Hong & Kim, 2018 ). Even mainstream TV and print media unwillingly contribute to the bias through their seemingly neutral framing, e.g. through repeating a statement by a person without mentioning that it is unsubstantiated. 3.2. Psychological nudges Profiling can detect the person’s wishes or fears and may use psychological nudges ( Thaler & Sunstein, 2009 ) and group pressure to trigger emotions. Three examples (a)–(c) illustrate this: (a) During the US elections 2016, several people absorbed some “false information” that Pope Francis would endorse Trump. The idea originated from a satirical website ( Schaedel, 2016 ), and was then spread by partisan groups to gain voters who had previously shown interest in the Republican Party and/or Trump and/or the Church. Allcott and Gentzkow (2017) analysed the effects and found that fake news had been shared through social media, but that the impact on the voters’ election behaviour is more complex: people are more likely to believe stories that favour their preferred can- didate, and absorb news through manifold channels. Since social media news gets quoted in newspapers, TV and of- fline talks, the effect might have been larger was measurable through web data. (b) Influences of viral disinformation are reported from Brazil, Pakistan and Mexico ( Shaban, 2019 ). In the “Global South”, WhatsApp plays a huge role in political campaigning ( Rennó, 2018 ). This messenger App is frequently used in Africa, South America, and Southeast Asia because it is free of charge and seemingly “personal”. For rural areas without WiFi but a telephone network, WhatsApp offers a chance to communicate, and for (political) advertisers is a formidable tool, because voters can be reached through their telephone numbers. In lesser developed countries, telephone numbers can be purchased and sold, by telecommunication providers or by advertisers, event managers, local associations, groups, influencers and private persons. Telecoms offer their services for free (called “zero-rating”) if users exclusively use Face- book or WhatsApp ( Rennó, 2018 ). This facilitates the tar- geted dissemination of “state-sponsored WhatsApp content”( Sircar, 2018 ) as well as propaganda in large groups. The cir- cumstances in Africa, Brazil and India do not raise hope for a well-informed usage and electoral decision-making. (c) Cadwalladr (2019) explains a similar effect, showing the false information that citizens of Wales received before the Brexit referendum: It claimed that the EU would open the doors for more than 70 Million Turkish people – which frightened citizens who were classified as potential recip- ients for this kind of fake news. Even recipients who had not fully believed this bogus claim could have been emo- tionally affected and/or got a small psychological nudge. Polonski (2018) researched the role of social media during the Brexit campaign in the UK and found that the “Leave Camp” was much more active on all social media platforms than the “Remain camp”. Through their emotionally charged messages, their hashtags were shared by many users and may have attracted undecided voters. 3.3. Politics 2.0 Echo chambers have been extensively discussed and blamed for societal divide, political polarization and an increase of extrem- ism and conspiracy theories, e.g. through the works of Sunstein (2007) and Pariser (2011) . During election campaigns, many differ- ent claims and statements contradict each other, and several users welcome simplification to reduce complexity. Since adoption be- haviour is common in situations of uncertainty ( Fang, Hu, Li & Tsai, 2013 , p. 2), social media and opinions of like-minded persons have strong influences. The distinction between legitimate self- advertising, exaggeration of problems, or of the opponent’s deficits, and the reality is difficult. An expressive revolution, an increased subjectivity, groups striving for their unique identity by differen- tiating themselves from the mainstream and through spreading their opinions, finds formidable tools in the online world. They gain more followers, and most of them remain aligned with their groups, even though they know that some of their assumptions are wrong ( Mercier & Sperber, 2017 ). This also applies to voting behaviour: a vote can be an expression of identity rather than be based on rational self-interest ( Hamlin & Jennings, 2011 ). “Social bots” are agents steered by unidentifiable developers who engage in the process of disinformation, often in order to make (advertising) money ( Graber & Lindemann, 2018 ). Faking a human identity through common names and friendly pictures, they are programmed to send fake news, comments, pictures or retweets related to certain key words or hashtags. Most of them simply copy or repeat existing messages, that meet their objectives and some try to involve or provoke others in discussions ( Stecher, 2017 ). They account for 9–15% of all “users” ( Varol, Ferrara, Davis, Menczer & Flammini, 2017 ) and often are not recognized as such by human users ( Graber & Lindemann, 2018 ). Misinformation, social programming, and identity cloning are some of their main activities ( Jones & Flaxman 2015 ). The virality of attention-catchers is supported by the advertising mechanism: Having many readers or followers means they can offer lots of potential customers to advertisers. Consequently, extraordinary content (including defamation, obvious lies, and in worst cases, violence) enhances the opportunity to place adverts. Coordinated through a “bot- net”, disinformation can effectively be multiplied and used as a powerful political tool. 3.4. Narratives and manipulated videos Narratives play the role of an enhancer because they are under- stood, believed and remembered better that any factual analysis ( Zak, 2014 ). This creates room for political influencing, as Collier (2015) explains in the cases of Russia regarding the Ukraine and several others in Europe and Africa. People tend to believe in the evidence of pictures and videos; but pictures can be manipulated, and so can videos. In 2018, Stan- ford researchers presented an AI based on a neural network with a novel space-time architecture, which creates stunningly realis- tic videos by using only one input video ( Kim et al., 2018 ). Those are colloquially called “deep fake”, because it is hard for ordinary people to recognize that this is not the real person speaking. The researchers hope that their demonstrations will inspire people to 909 U. Reisach European Journal of Operational Research 291 (2021) 906–917 think more critically about the video content they consume ev- ery day, especially if there is no proof of origin (ibid.). But cur- rent practice shows that speech and video manipulations are used to defame politicians and reach millions of people. For example, Nancy Pelosi, the Democrat leader of the US House of Represen- tatives ( Harwell, 2019 ), and other politicians and journalists suf- fer from embarrassments, where voice distortions and deep fake videos are used as a digital weapon to fight against political op- ponents. Although algorithms could find out whether a video is a fake or not, the societal and political risks remain. Synthetically generated videos fool people, groups or institutions with lesser knowledge and/or lesser money and access to machines which could reveal the deception. If one side has more money, then they can pay for the better deep fake. In dictatorships, this tool can be used for any kind of cyber-disinformation war and a visual re- writing of history, while people might still believe in what they see with their own eyes. 4. Concepts of digital media ethics and responsibility In digital media ethics, the term responsibility is used when referring to individuals and groups who are free in their deci- sion making. Ethics is based on well-founded standards of right and wrong that prescribe what humans ought to do, usually in terms of rights, obligations, benefits to society, fairness, or specific virtues ( Velasquez, Andre, Shanks, S.J. & Meyer, 2010 ). Ethics can be applied to analyse the aims, procedures and outcomes of di- rect and indirect human actions. In a famous speech “Policy as a vocation”, the sociologist Weber (2014) differentiates between the ethics of conviction, based on (teleological) values and principles, and the ethics of responsibility, which considers the impact of a (political) decision, and is therefore consequentialist. Philosophers have discussed the importance of these two contradictory views. Many agree that for political decisions the consequentialist view is more appropriate ( de Villiers, 2018 ). Weber’s ethics of respon- sibility applies well to social media: Here corporate responsibility means considering the impact of their corporate decisions on all societies in which they operate. Immanuel Kant’s Categorical Im- perative underlines the importance of the universal view with a universal law: “So act that the rule on which you act would admit of being adopted as a law by all rational beings” ( Kant, 1993 ). This universal principle is applicable to many fields, including social media. This means that “freedom requires responsibility” turns into “freedom of speech requires responsibility”, and this call applies to “… a variety of political regimes, public uses, media cultures and traditions of political engagement in various places” ( Zelizer, 2010 , p. 69). The concept of accountability is applied to traditional mass me- dia organizations and their journalists such as newspapers, TV, ra- dio broadcasting and the respective professionals. It can be inter- preted as “… responsiveness to the views of all with a legitimate interest in what is published, whether as individuals affected or on behalf of the society. It includes a willingness to explain, de- fend, and justify actions (and general tendencies) of publication or omission.” McQuail (2003, p. 204) . In organizations, governance comprises the rules and processes and the practice of leadership and decision making, including operationalizing norms like open- ness, participation, accountability, effectiveness and coherence ( EU 2001a , p. 8). As businesses, social media platforms work according such kind of rules in their internal processes, while trying to de- fend their role of “being just a platform”, a tool for the freedom of expression ( Zuckerberg, 2019 ). Since they never took on the role of being publishers, they are lacking …• the regulations and media accountability which apply to tra- ditional media, • the (virtue) ethics which Plaisance (2016, p. 465) recom- mends for professional media workers, • the “polis” ( Plaisance, 2016 , p. 466), meaning the context and environment which is necessary to lead an informed and open dialogue between different opinions and perspec- tives, • the media governance which would mean a more complex guidance according to agreed principles and based on a net- work of various influences, claims and demands ( McQuail, 2003, p. 17 ) from various stakeholders and their divergent interests. Most of the manipulative sources which are the subject of this paper are not issued by journalists but by anonymous persons or organizations which deliberately spread partisan content. This is why the well-known concept of media accountability with its rules and sanctions ( Fengler, 2012 ) does not well apply to social media platforms. It also does not apply to users who post their opinions on social media. These are no professional journalists, but individ- uals who are neither trained nor interested in explaining or jus- tifying their online posts. According to Barbie Zelizer (2010) , the notion of media accountability is outdated, and not appropriate for an emerging environment of prosumer news, continuous global news flows, and messy news. But who is responsible for the publication of content on a plat- form that claims to be only a carrier? (a) The user who voices an unsubstantiated opinion or belief, (b) Users who forward and recommend the statement, (c) The platform which provides the channel for this type of publishing, (d) The platform’s paying customers, the advertisers who are using the data of all those who read, forward, like or com- ment the incident. Like everyone who speaks to a larger audience, the users of so- cial media platforms and the forwarders of messages carry a re- sponsibility. They can report disinformation to the platform or to independent fact-check organizations. But sometimes users make up fake news themselves to win attention, as several youtubers or instagramers did during the corona crisis. Others forward thrilling news for similar reasons, or because the disinformation is not rec- ognized as such, or the message is perceived as fun or easy expla- nation of a subject which otherwise is quite complex. Amidst the dense flow of information, the platforms have a hard time identi- fying which pieces of information are harmful, dangerous or ma- nipulative. And whatever content it is, the broader it is spread, the better for advertising. The role of advertisers became visible in June 2020, when more than 10 0 0 companies boycotted Face- book through pausing their advertising. They were responding to civil-rights groups’ call for a boycott over what they saw as “a lack of progress in preventing hate speech and misinformation” ( Ives, 2020 ). Facebook announced it would expand its policies around hate speech and prohibit a wider category of hateful language in ads on the site. But even if a post violates their rules, it will remain, if it is from an important political figure, because it is deemed “newsworthy” ( Isaak & Frenkel, 2020 ). Several companies were not satisfied with this procedure (ibid.) and seem to look for alternative ways of online marketing. Others hope that Face- book makes progress through conversations with marketers and civil rights organizations about how, together, they can be a “force for good" ( Vranica & Seetharaman, 2020 ). A variety of actors have caused a problem of distributed agency and morality, due to non-individual responsibilities and au- tonomous agents such as AI ( Floridi, 2013 ; Taddeo & Floridi, 2018 ). 910 U. Reisach European Journal of Operational Research 291 (2021) 906–917 To overcome such kinds of issues, the Future of Life’s Asilomar AI Principles (2017) , one of the first prominent ethics codes for AI, de- clared: “Designers and builders of advanced AI systems are stake- holders in the moral implications of their use, misuse, and actions, with a responsibility and opportunity to shape those implications”. Corporate responsibilities can be applied similarly to legal respon- sibilities of institutions: In order to avoid internal shortcomings, the leading persons need to design an internal governance and training to make sure that subordinates follow the guidelines. The owners and designers of the business models are thus responsible because they decide about the aims, strategies, methods and tools for designers and builders of advanced AI systems. Thierry Breton, European Commissioner for the Internal Market, underlined that the CEO and board of directors, together with the team of devel- opers and managers of the media platforms and with their share- holders and customers (advertisers), cannot deny their overall re- sponsibility ( Centre on Regulation in Europe, 2020 ). Breton’s view takes the perspective of the European market: It does not have similarly huge social networking companies and follows the eth- ical and (potentially future) legal norms of Europe. The European interest is “… to ensure transparency of political and issue-based advertising and to tackle fake accounts and malicious use of bots”( European Commission, 2019 ). A further goal is achieving progress, after representatives of online platforms, leading social networks, advertisers and advertising industry had voluntarily agreed to a first self-regulatory Code of Practice which addresses the spread of online disinformation and fake news in 2018 ( European Commis- sion 2018a ; European Commission 2018b ). In their summary of “Media responsibility and accountabil- ity: New conceptualizations and practices” Bardoel and d’Haenens (2004, p. 22) conclude: “Given the structural changes in the me- dia context such as commercialization, increasing competition, de- creasing public spirit and more self-conscious and demanding citi- zens, a shift in media ethics and accountability systems from the level of the individual professional to media organizations and in- stitutions seems imperative.” The concept of Corporate Responsibil- ity is therefore transferred to social media platforms: Only those platforms have the means to detect signals generated by inau- thentic content and toxic behaviour, they may use AI to com- bat abuse and dis/misinformation. Albright (2018, pp. 55–56) sees platforms in a position to measure news exposure and audience impact, as well as the means to trace manipulation effort s back to specific technologies, actors, and sources. Those capabilities re- flect power and enable the social media platforms to be responsi- ble, much more than publishers of newspapers, or as governments and non-governmental fact-checker organizations could do. If they wanted, social media platforms had several means to act: they could flag disputed content, impose blanket algorithmic penalties (i.e., downranking), and block the redistribution of content sus- pected of fraudulent, spam-like activity, e.g. through the removal of a post’s re-share function ( Albright, 2018 , p. 56). Doing this would mean effort s and cost s and probably also evoke discussions, but those discussions would be valuable to restore an unbiased elec- toral information. 4.1. Global responsibility standards and strategic corporate responsibility The big social media platforms such as Facebook, YouTube, In- stagram, Twitter, Pinterest and TikTok have expanded their services to many countries all over the world. As a consequence, cross- border information and disinformation flows arrive in places with different cultural, political and legal systems. Their governments and courts may have different views regarding online comments and targeted political manipulation. Different perceptions and reg- ulations on social media may cause legal issues and demands for filtering. The European Commission (2011) published a definition of CSR as “… a concept whereby companies integrate social and environmen- tal concerns in their business operations and in their interactions with their stakeholders on a voluntary basis ”. Strategic CSR addresses so- cietal challenges and creates added value for business and society at the same time ( Schmidpeter, 2013 ). CSR is used as a tool for managerial reflection and better decision making, and, at the same time, it serves the enlightened self-interest of the firms ( Masoud, 2017 , p. 2 and 5). Those self-interests include, among others, the stability and prosperity of the conditions around them ( European Commission, 2001b ). The definitions and understandings of Corporate Social Respon- sibility (CSR), and/or Societal Responsibility (which is close to Cor- porate Citizenship) mirror the differences in the respective histori- cal (cultural, political), socio-economic and legal contexts, and they reveal the powers of the respective actors, such as companies, em- ployees or governments ( Berthoin, Oppen & Sobczak, 2009 ). Simi- lar differentiations were discussed by Gerner (2019) in his analysis of corporate sustainability and by Fifka (2011) in his comparison of Corporate Citizenship in the USA and Germany. Since the term Corporate Citizenship might be mainly associated with a specific nation, this paper uses the term Societal Responsibility in order to make clear that global networks and businesses have a cross- border responsibility for the respective societies. In a nutshell, CSR indicates the positive impacts of businesses on their stakeholders ( Turker, 2008 ). The United Nations (20 0 0) Global Compact Principles, and OECD (2011) Guidelines for Multinational Enterprises were intro- duced in order to foster responsibility for the companies’ interna- tional business. Those guidelines are not binding, since the United Nations has no means of enforcement. Nevertheless, global players need to take responsibilities for people and the societal and politi- cal environment in the countries where they are operating. They do so because their customers and the local and home country governments expect it and to avoid losses in brand image and rep- utation ( Šontait ˙e-Petkevi ˇcien ˙e, 2015 ). For social media platforms, such a “shame” factor is factually not relevant: the more people are already using a platform, the more attractive it becomes for new users. Billions of users of the social media platforms remain loyal to the platforms although they disagree with some of their decisions. Users can hardly choose alternative platform providers as long as there is no data-portability which would allow them to take along all their contacts, pictures and conversations. This is why the European Commission (2019) tries to break up oligopoly structures of social media platforms and fosters data-portability. Nevertheless, the European Market is big enough to be consid- ered, as the GDPR and the preparedness of several platforms to sign the Code of Practice on Disinformation ( European Commis- sion, 2018c ) shows. Inside and outside the companies, the societal and political consequences of disinformation are leading to a de- bate about the usage of one-to-many channels as political propa- ganda. A further subject of debate are the side-effects of insisting on the freedom of speech, whether it supports the truth or not. The US-elections in 2020, with the Corona crisis and the “Black lives matter” campaign, had further impact on the social media platform’s decisions: Twitter has suggested fact-checks for ques- tionable statements and has banned political ads ( The Economist, 2020 ). 4.2. Reflective CSR for social media platforms? For social media, a reflective CSR concept is a novelty. Typically, social media see their platform as the place for others to show their contents, opinions and activities. 911 U. Reisach European Journal of Operational Research 291 (2021) 906–917 For example, Facebook publishes no explicit CSR report, but reports about initiatives of others on their “CSRglobally” page ( Facebook, 2019 ). Additionally, environmental friendliness is men- tioned as well as their “fair pay” and diversity policy. WhatsApp and Instagram, the other two platforms of the Facebook group, also show only the initiatives of their users. A critical reflection of their own actions, and of the (side) effects of the usage of their plat- forms is not apparent. “Twitter for good” claims “We reflect the power of Twitter through civic engagement, volunteerism, and charitable partner- ships.” (Twitter 2019). On its website they list the following points: Internet Safety and Education, Freedom of Expression and Civil Liberties, Universal Access and Adoption, Equality, Emergency Re- sponse and Disaster Recovery, Community Engagement, Employee Engagement. Some of those are wishes, some values, others action examples. Even though the above-mentioned social media platforms are global players and rank among the biggest and most profitable companies in the world, there are no global responsibility stan- dards for this kind of companies. This vacuum is surprising, con- sidering the cases of (unintended) problems, as explained in Part 3. Potential reasons for the lack of explicit responsible actions are: • Their self-definition as a “facilitator” for presumably good purposes, • The strong focus on shareholder value, and • A lack of long-term societal focus on potential risks which do not yet directly affect the company and their revenue un- til such risks become a public issue in the press and media. Anticipating, recognizing and openly discussing the positive and negative societal impacts of content and processes, requires people, budgets and a long-term strategy. If shareholders do not appreciate these, because of quarterly profit goals and quarterly reporting, the companies’ directors probably have a hard time addressing long- term responsibility issues. Different opinions regarding the impacts for their own and other societies, or certain political orientations, might additionally influence the willingness of board members to decide between societal harmony and profit maximization. But corporate foresight is a responsibility for powerful and globally active companies. If business revenues are earned through the provision of services in foreign countries, the responsibilities of social media companies need to be extended to those societies. Responsibility comprises foresight and creative imagination, to an- ticipate long-term societal impacts, contextual changes and stake- holders’ reactions. An analysis and reflection ( Forrester, 1961 ) of the local circumstances is essential, especially before and during the implementation process, and then regularly to observe the de- velopments in social media usage. This is the basis for an adaption of their systems and societal engagement. Since global regulation and enforcement is unlikely to come soon, social media companies remain the ones who can act re- sponsibly in different countries. Closer contact between social me- dia directors and local stakeholders, such as local media, local scientists, users, non-governmental and governmental institutions would be helpful for mutual learning and trustful co-operation. Re- flecting the complexity of the societal interactions, the only way to achieve an agreement or compromise is through interactive dis- course ( Habermas, 2009 ), an open dialogue between the platform providers, their users and the communities and regulators. It re- mains to be seen whether this may result in global or region- ally diversified processes of improvement. Responsible design, us- age and communication of social media benefits and risks could contribute positively to digital and technical literacy, as well as civilized media usage and public debate. This could be seen as a valuable contribution to tackling global issues such as injustice, in- equality, poverty and societal destabilisation. Cultural, historical and political framework conditions need to be considered, including education levels and political instabili- ties. Anticipation and consideration of potential consequences, es- pecially for developing countries are essential. In order to find out how their platforms might be (mis)used, social media platforms could engage a council of independent local and international sci- entist to monitor and discuss the situation and (ethical) usage in different societies. Local or international non-governmental organi- zations caring for development and (digital) ethics could also be involved. This way, social media companies could open up and learn from a dialogue with stakeholders that have a different per- spective. Scenarios for potential (mis)use of the new platform need to be discussed, in order to avoid any harm caused by rumours or agitation, such as in India or Myanmar in 2018 ( Hogan & Safi, 2018 ). Therefore, supporting digital literacy and awareness for po- tential risks could be one of the responsibilities in lesser developed countries. 4.3. Ethics codes for social media platforms Ethics for digital media are on the point of intersection of sev- eral academic disciplines such as applied ethics and philosophy, so- cial science, especially communication and media science, journal- ism, sociology and psychology, business science, computer science, mathematics, statistics, data science and Artificial Intelligence (AI). The respective initiatives and norms will be explained in this para- graph with a focus on ethics codes to avoid political manipulation. Many countries have introduced regulations in order to protect users from content such as violence, terrorism, and child abuse. Social media companies need to remove harmful content within a short time. Since their AI is useful but limited in identifying poten- tially harmful content, social media companies have employees all over the world monitoring and removing illegal content. YouTube has 10,0 0 0 and Facebook more than 35,0 0 0 content moderators ( Reality Check team 2020 ). Nevertheless, political disinformation seems to be very difficult to identify and to remove, as investiga- tive journalists found out when following traces of radical right- wing groups on Facebook ( Basl, Riedel, Pittelkow & Altland, 2020 ). Code-words and seemingly friendly terminologies, combined with irony make it difficult for AI to recognize the real meaning of a text. But if humans such as journalists help with decoding, social media platforms can improve the effectivity of their filtering AI for hate speech. In early 2018, a group of leading AI researchers launched a pa- per on “The potentially malicious use of AI”. Beyond severe risks such as remote control across frontiers, they also mentioned the risk of more sophisticated social engineering ( Brundage et al., 2018 , p. 24): They forecasted the next step of manipulation: “Victims’ on- line information is used to automatically generate custom malicious websites/emails/links they would be likely to click on, sent from ad- dresses that impersonate their real contacts, using a writing style that mimics those contacts. As AI develops further, convincing chatbots may elicit human trust by engaging people in longer dialogues, and perhaps eventually masquerade visually as another person in a video chat. ” This paper was an appeal to designers, companies and gov- ernments to think more deeply on what to allow, what to regulate, where to negotiate and how to protect humans. The EU, based on the fundamental right of data protection, was first to implement data protection regulations ( EU, 1995 ). They were replaced by a much more detailed European General Data Protection Regulation (GDPR) which was issued in 2016 ( EU, 2016 ) and applies since 25 may 2018. This aligned the previously differ- ent implementation modes in the European member states, and applies to every company that collects and uses personal data in the EU and abroad, if European citizens are influenced. The mostly US-based social media companies adopted their terms of use and 912 U. Reisach European Journal of Operational Research 291 (2021) 906–917 Table 1 Principles and guidelines for AI elaborated by international organizations. Published in 08 Jan. 2017 Feb. 2018 (V1) and 2019 (V2) 14 Nov. 2019 03 May 2019 22 May 2019 Organization Future of Life Institute (FLI) The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems Institute of Electrical and Electronics Engineers (IEEE) European Commission (EC) Organization for Economic Cooperation and Development (OECD) World Economic Forum (WEF) Name of the Principles and/or Guidelines Asilomar AI Principles Global Initiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems Ethics Guidelines for Trustworthy AI Principles on AI AI Governance: A Holistic Approach to Implement Ethics into AI prolonged their customer information, asking their users to agree with the existing scope of data collection and their usage for ad- vertising; otherwise their services wouldn’t work properly. Euro- pean users and the GDPR sought an active management of the data, profile content and usage. But this is still an aspiration in most social media. Meanwhile, the California Consumer Privacy Act released a similar legislation on data protection which became ef- fective on January 1, 2020 ( California Legislative Information, 2018 ). Several principles and guidelines for AI were elaborated by international and trans-national organizations between 2017 and 2019. The most prominent are listed below along with the date of their publication. Further ethics codes originate from national associations, e.g. the Chinese Academy of Engineering’s “Beijing AI Principles”( Beijing Academy of Artificial Intelligence, 2019) , and from Infor- mation and Communication Technology (ICT) corporations them- selves. A comparison has been provided and updated by Linking AI principles.org ( Zeng, Lu & Huangfu, 2019a and Zeng, 2019b ). The timeline shows that the recent guidelines are more holistic and try to give suggestions for practical implementation. The “Ethics Guide- lines for Trustworthy AI” of The High Level Task Force (2019) of the European Commission is one of the most comprehensive and de- tailed guidelines for a responsible operational use. (1) They address four ethical principles : Respect for human autonomy, Prevention of harm, Fairness and Explicability. (2) They suggest to implement seven key requirements that AI systems should meet: Human agency and oversight, Technical robustness and safety, Privacy and data governance, Transparency, Diversity, Non-discrimination and fairness, Societal and environmental wellbeing, Accountability. (3) They operationalise key requirements through an assessment list, tailored to the particular system’s application. In all of the 41 pages of the guidelines, social media platforms are not directly mentioned. However, the principles and guidelines offer a good template for developing their specific Corporate Re- sponsibility. The question whether a sectoral approach is needed, given the context-specificity of AI systems, is kept open by the EC’s High-Level Task Force, as well as the question of future policymak- ing or regulation. In the EC’s Task Force and in the working groups of all the other initiatives, independent experts met representatives of social media, both contributed their experience and views. This mixture might explain the prudent and careful approach of giving suggestions and leaving the real decisions to the companies and governments. 4.4. Societal responsibility of social media platforms Social media platforms are commercial businesses. Since social media plays a leading role in influencing societies, their directors and shareholders need to become aware that good corporate gov- ernance is necessary to help in tackling the ethical and societal issues caused by the development and use of their platform. Research on the effects of digitalization seems to prove that pre-existing inequalities are being increased ( Cohen, 2018 , and Didier et al., 2015 ). One of the reasons is money: high quality sources are costly, and who doesn’t pay gets free, but often lesser quality or partisan information. Engagement does therefore not mean showing pictures and videos of social engagements of the users, but the companies’ decision makers’ acting and aligning their business models to the challenges given in the respective societies. Sometimes it helps to change the settings: One of the responsible actions was implementing the “forwarded message” notification in WhatsApp. It informs the user that a message is forwarded and is not an original text of a friend. After broad debates in India after mob-lynchings, WhatsApp reduced the forwarding of information to a maximum of five recipients in order to limit the virality of toxic rumours ( Gopal Jayal, 2018 ). A further limitation of the group size in WhatsApp groups (256 max., as of Rennó, 2018 ) could be a further step – but this also limits the revenues earned through ad- vertising. Responsibility and adaption to societal needs contradict the short-term profit interests of the platforms along their interna- tional expansion. Digital media platform services address many different societies worldwide, which makes their responsibilities manifold and inter- woven. Diverging ethical and legal standards as well as education and wealth levels influence the opportunities and risks regarding the provision and usage of online media. With regard to political risk minimizing, responsible societal action of social media com- panies could comprise the following fields, listed in order of their importance: (1) Human rights and fairness, applied globally (a) Human agency : “AI systems should support human au- tonomy and decision-making … This requires that AI sys- tems should both act as enablers to a democratic, flour- ishing and equitable society by supporting the user’s agency and foster fundamental rights, and allow for hu- man oversight” ( European Commission 2019 ). (b) Privacy and data governance/Access to data : Personal data, opinions and information paths need to be protected in order to safeguard the freedom and confidentiality of the citizens’ political opinions. (c) Societal and environmental well-being / Society and Democ- racy : “The use of AI systems should be given careful consideration particularly in situations relating to the democratic process, including not only political decision- making but also electoral contexts” ( European Commis- sion 2019 ). For social media platforms this also means independence from political advertising to avoid their platforms becoming multipliers of partisan propaganda wherever they operate. (d) Stakeholder participation and social dialogue : Companies should engage in an open discussion with the involve- ment of social partners and stakeholders, including the 913 U. Reisach European Journal of Operational Research 291 (2021) 906–917 general public ( European Commission 2019 ) in all the countries where they are active. (e) Actions against harmful content: Companies should de- velop strategies to avoid becoming multipliers of violence or hate and actively engage in the protection of persons and groups who need support. Clear criteria, AI ( Arnold & Scheutz, 2018 ), and teams of independent experts can help to identify potentially harmful content and ban it from platforms. By anomaly detection, AI can be used to detect the start of viral processes. If those turn out to be toxic, such as the video of the massacre in Christ Church in March 2019, social media platforms could “freeze” sus- picious activities until further investigation is made and a release has been given by a council of independent advi- sors. This type of action can be institutionalized, similar to stock markets which close dealing if there are strange movements in the market. (2) Education in the usage of digital technologies and media (a) Tech and media literacy ( Entsminger, Esposito, Tse & Goh, 2018 ) can support citizens’ short- and long-term abil- ity to spot untrustworthy news sources and information ( Jacob, Sapienza & Lister, 2019 ). For example, courses on specific fields of AI could be provided in schools, pub- lic learning centres, senior citizens’ homes. This helps to integrate different parts of the population and gain ex- perience and a better understanding of the benefits and risks of online media and AI. (b) Educational support for vulnerable populations with low or inadequate education levels or resources , e.g. through pro- viding teaching programs for digital literacy (in general, not merely as advertising for the usage of their own app). Users need to be shown how to identify illegal and ille- gitimate content, and to avoid creating, using or forward- ing such content. To learn about the quality of sources could be one of the main teaching aims in digital liter- acy. (c) Education and awareness to foster an ethical mind-set : “A prerequisite for educating the public is to ensure the proper skills and training of ethicists in this space”( European Commission, 2019 ). (3) Taxes as a fair contribution to the local societies Taxes are not mentioned in the principles and guidelines, probably because corporate representatives have a strong saying in the respective task forces. But taxes are needed to provide high quality ICT infrastructure throughout a country, and are necessary to run states and to implement and exe- cute good governance. Taxes are also needed to increase the citizens’ education level which is crucial to be able to dif- ferentiate between high-quality sources and disinformation. Knuutinen (2014) points out that strongly increasing prof- its of companies for many years, combined with decreasing taxes for states, can undermine the foundations of democra- cies. Taxes are part of each company’s societal responsibil- ity, are a matter of fairness and expected by stakeholders, for social media companies as well as for any other type of business. If companies choose low tax havens or loopholes to evade payment of adequate taxes in the countries where they earn revenues, they are failing in their ethical duties and should not be proud of any donations which they might later give. (4) Transparency of algorithmic decisions Since profiling is supported by AI, the process of classify- ing human online behaviour needs to be made transparent. Algorithm audits ( Zweig et al., 2017 ) and some external accreditation for critical algorithms, could be helpful to assess the status quo and progress of responsibility of the respective companies. Possibly start with the ones that have a broad societal impact and engage independent certified accreditation bodies. Algorithms could be analysed through laboratory testing and reverse engineering ( Diakopoulos, 2015 ), which means reconstructing the algorithm to identify the functional principles. Additionally, more cooperation with independent researchers would be helpful in order to research the dissemination of misinformation ( Lazer et al., 2018 ; Torabi Asr & Taboada, 2019 ). Regarding algorithm transparency, three points need to be considered: • The details of the algorithms are usually business secrets, because of curious competitors or hack- ers. But a certified auditor or accreditation agency could be entitled to check, similarly to how they check the company’s financial data or processes. Even though algorithms-in-action are usually inscrutable, a blueprint-algorithm can be comprehensive ( Kavanagh, McGarraghy & Séamas, 2015 ). • The generalized depictions of the technology, and the claims about how they work or are used, may differ a lot from how they are embedded in specific social or organizational contexts ( Kavanagh et al., 2015 ). Re- sponsibility also requires a continuous monitoring and tracing through empirical studies to ensure the relia- bility. This should happen with the companies’ own algorithms, and with the services connected to affil- iated customers. In the end, a regular and credibly performed external evaluation could avoid legal initia- tives in this field. 5. Reflection and limitations Media platforms hold unique power in societies. The tempta- tion is strong for political and economic elites in developing or immature democracies to capture and pervert the media to pro- mote their own interests ( Rothman, 2015 ). Any type of regulation has two sides: it might prevent harm, but decreases the degree of freedom. The debate on “freedom versus regulation” has a long history. and Immanuel Kant tried to establish an ethical concept in order that decision-making be in the hands of the free, but re- sponsible individual. Freedom and responsibility are thus seen as a pair, and corporate responsibility is a complementary concept to entrepreneurial freedom. If social media companies choose to act responsibly, they have the scope for their own thematic focusses and claims. They can decide whether to do a bottom-up (recommended) or top-down approach, whether they integrate employees and other stakehold- ers (from different nations) or whether they “think for them”. An interactive approach would allow them to learn from the stake- holders’ expectations, to be more innovative, through a diversity of experiences and thoughts. All actors in this debate need situation- centred critical (self-) reflectivity ( Mead, 1910 ), empathy, care and compassion for others in order to responsibly balance divergent in- terests. As a method, multi-criteria decision making (MCDM) could facilitate decision making related to complex problems ( Brugha, 2004 ). Responsibly governed, social media platforms can be a tool for better informed decisions and for leveraging their societies up- wards, socially and economically. As Bell (2018, p. 5) says: “It’s becoming obvious that platforms cannot scale advertising revenue in tandem with the robust intervention needed to ameliorate the type of material they publish.” Therefore, the likelihood that so- cial media platforms take this responsibility is high – especially if more competition and data-portability would create more choices for users who then might chose the more responsible offer. If 914 U. Reisach European Journal of Operational Research 291 (2021) 906–917 companies find their own ways of dealing constructively with the ethical challenges of their operations, they could find multiple and adaptive ways of improvement, rather than being bound by maybe rigid or impractical regulations. Since social media platforms have different principles ( Zeng et al., 2019a ), challenges and user and stakeholder groups, the ways to act responsibly will be different and the companies could be in competition with each other in order to achieve the highest degree of responsible action, credibility and reputation. Data porta- bility is a necessary prerequisite to revitalize competition, other- wise it is almost impossible to change a social media provider without losing contact data, conversations, photos and videos in a long timeline of usage. This creates a so called “lock-in” situa- tion for users ( Zweig et al., 2017 ). The network-effect, reinforced by the acquisitions of WhatsApp and Instagram, strengthen the cur- rent “the winner takes it all” position of Facebook. A change in the current oligopolistic structure requires competition, anti-monopoly laws and their ideas of entangling conglomerates and/or creat- ing data-portability between different providers ( Entsminger et al., 2018 ). The EC (2020b) is following this path along their new Eu- ropean Data Strategy but it remains open when and how they can achieve their goal. The EU is not the only region which requires changes: The different political systems and interests are leading to a trend to install specific regulation which ends up in a split internet (“splinternet”). Whether and how far such new “border- lines” between regional cyber systems systematically follow certain societal/political values and norms, and what a splinternet practi- cally means for global communication respectively social media, is a subject for future research. Ethical norms could be implemented into technology or innova- tion assessment practices which see ethics and multi-stakeholder involvement as a “design” factor of technology. Following the EU’s goals, programmes and sample cases, von Schomberg (2013) de- fines “Responsible Research and Innovation as a transparent, interac- tive process by which societal actors and innovators become mutually responsive to each other with a view to the (ethical) acceptability, sus- tainability and societal desirability of the innovation process and its marketable products in order to allow a proper embedding of scientific and technological advances in our society ”. How fast this will hap- pen, whether companies or regulators will take the first steps, and whether different approaches will be chosen for different cultures, remains to be seen. More efforts in developing and discussing re- alistic scenarios for a desirable future of our societies would be helpful ( Cath, Wachter, Mittelstadt, Taddeo & Floridi, 2018 ). 6. Conclusion Analysing, clustering, tracking, profiling, prediction and recom- mendations are powerful tools for supporting decision making. Profiling and targeted information created by machine learning al- gorithms have been growing exponentially, and so has their signif- icance for citizen’s political decision making. In digital media, AI can potentially be (mis-)used as an instrument of power through information filtering and targeted (mis-)information and/or manip- ulation. This can happen in different formats such as text, audio and video, and be multiplied by social bots and viral effects. Since the US-American social media platforms tend to understand them- selves as tools for others, they show a few CR activities, but do not assume comprehensive societal responsibility. This would be necessary to contribute positively to a more balanced communica- tion amongst competing parties and interest groups in societies, to safeguard trust in their democratic and legal system and to help stabilize fragile democracies. Suggestions for responsible actions have been derived from the European concept of societal cohesion and the Guidelines for Trustworthy AI. They extend CSR to a responsibility for all societies in which social media services are used and create revenues for the companies. The complexity and diversity of local framework con- ditions and societies in terms of wealth, education, values, and po- litical systems turned out to be a challenge for social media plat- form providers. They can master those challenges through trans- parency and opening up for dialogue and cooperation with local experts and stakeholder groups. Ensuring benefits and preventing harm will depend on defining the common interests regarding dig- ital media platforms and on forecasting/anticipating potential ben- efits and risks. The transparency of processes and interdisciplinary thinking are as necessary as situation centred consciousness. This would help guiding and bundling technical, ethical, and societal re- search effort s and help hedging the risks and harvest the benefits of social media and their AI for generations to come. References Akerlof, G. A., & Shiller, R. J. (2010). Animal Spirits: How Human Psychology Drives the Economy and Why It Matters for Global Capitalism . Princeton, NJ: Princeton University Press https://doi.org/10.1002/hrm.20337 . Albright, J. (2018). Emerging issues and platform reckonings. 2018 N. Rashidian, P. Brown, E. Hansen, E. Bell, J. Albright, & A. Hartstone (Eds.), Friend and foe: The platform press at the heart of journalism 14 June 2018. https://www.cjr.org/ tow _ center _ reports/the-platform-press-at-the-heart-of-journalism.php . Alhabash, S., & McAlister, A. R. (2015). Redefining virality in less broad strokes: Predicting viral behavioral intentions from motivations and uses of Facebook and Twitter. New Media & Society, 17 (8), 1317–1339. https://doi.org/10.1177/ 14614 4 4814523726 . Allcott, H., & Gentzkow, M. (2017). Social media and fake news in the 2016 election. Journal of Economic Perspectives, 31 (2), 211–236 American Economic Association, Spring 2017 . https://doi.org/10.1257/jep.31.2.211 . Al-Rawi, A. (2019). Viral news on social media. Digital Journalism, 7 (1), 63–79. https: //doi.org/10.1080/21670811.2017.1387062 . Ammicht-Quinn, R., Baur, A., Bile, T., Bremert, B., Büttner, B., Grigorjew, O. et al. (2018). White paper tracking. Beschreibung und bewertung neuer methoden. (White paper tracking. Description and assessment of new methods). Forum Pri- vatheit und Selbstbestimmtes Leben in der Digitalen Welt (Forum for privacy and self-determined life in the digital world). https://www.forum-privatheit. de/wp-content/uploads/Forum-Privatheit-Whitepaper-Tracking.pdf Accessed 30 November 2019. Arnold, T., & Scheutz, M. (2018). The “big red button” is too late: An alternative model for the ethical evaluation of AI systems. Ethics and Information Technol- ogy , 59–69. https://doi.org/10.1007/s10676- 018- 9447- 7 . Ballings, M., & Van den Poel, D CRM in social media: Predicting increases in Face- book usage frequency. EJOR 244(1), 248–260. doi: 10.1016/j.ejor.2015.01.001 . Bardoel, J., & D’Haenens, L. (2004). Media responsibility and accountability: New conceptualizations and practices. Communications, 29 , 5–25. https://doi.org/10. 1515/comm.20 04.0 07 . Basl, C., Riedel, K., Pittelkow, S., & Altland, N. (2020). Rechte Facebook-Gruppen: Der hass ist intelligenter geworden (Right-wing Facebook-groups: The hate has be- come more intelligent). In: Tagesschau.de investigativ, 24 June 2020. https:// www.tagesschau.de/investigativ/ndr-wdr/facebook-hassrede-105.html Accessed June 30, 2020. Beckers, K., & Harder, R. A. (2016). Twitter just exploded. Digital Journalism, 4 (7), 910–920. https://doi.org/10.1080/21670811.2016.1161493 . Beijing Academy of Artificial Intelligence (2019). Beijing AI Principles (2019). Bei- jing 28 May, 2019. https://www.baai.ac.cn/news/beijing- ai- principles- en.html Accessed 30 June 2020. Bell, E. (2018). Executive summary. 2018 N. Rashidian, P. Brown, E. Hansen, E. Bell, J. Albright, & A. Hartstone (Eds.), Friend and foe: The platform press at the heart of journalism, 14 june 2018 https://www.cjr.org/tow _ center _ reports/ the- platform- press- at- the- heart- of- journalism.php . Berthoin, A . A ., Oppen, M., & Sobczak, A . (2009). (Re) discovering the social re- sponsibility of business in Germany. Journal of Business Ethics, 89 (3), 285–301. https://doi.org/10.1007/s10551-010-0390-8 . Brugha, C. (2004). Structure of multi-criteria decision making. Journal of the Oper- ational Research Society, 55 (11), 1156–1168. https://doi.org/10.1057/palgrave.jors. 2601777 . Brundage, M., Shahar, A., Jack, C., Toner, H., Eckersley, P., Garfinkel, B., et al. (2018). The malicious use of artificial intelligence: Forecasting, prevention, and mitigation Oxford, February 2018 https://www.fhi.ox.ac.uk/publications/ malicious- use- artificial- intelligence- forecasting- prevention- mitigation- brundage- m- avin- s- clark- j- et- al- 2018/ . Cadwalladr, C. (2019). How did social media manipulate our votes and our elec- tions?. In TED Radio Hour July 12, 2019, 9:46 AM ET https://www.npr.org/ transcripts/740771021?storyId=740771021&t=1574694646363 . California Legislative Information (2018). SB 1121, Dodd. California Consumer Privacy Act of 2018. https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill _ id= 201720180SB1121 Accessed 30 November 2019. Cath, C., Wachter, S., Mittelstadt, B., Taddeo, M., & Floridi, L. (2018). Artificial intel- ligence and the good society”: The US, EU, and UK approach. Science and Engi- neering Ethics, 24 (2018), 505. https://doi.org/10.1007/s11948- 017- 9901- 7 . 915 U. Reisach European Journal of Operational Research 291 (2021) 906–917 Centre on Regulation in Europe (2020). Towards a post COVID-19 digital deal between tech and governments? A conversation between Mark Zucker- berg and Thierry Breton, Brussels 18 May 2020. https://cerre.eu/events/ covid19- digital- deal- regulation- tech- governments- zuckerberg- breton Accessed 30 June 2020. Churchman, C. W. (1970). Operations research as a profession. Management Science, 17 (2), B37–B53 INFORMS Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA . Cohen, J. (2018). Digitally connected living and quality of life: An analysis of the Gauteng City-Region, South Africa. Electronic Journal of Information Systems in Developing Countries . https://doi.org/10.1002/isd2.12010 . Collier, P. (2015). The cultural foundations of economic failure: A conceptual toolkit. Journal of Economic Behavior & Organization, 126, Part B , 5–24 VolumeJune 2016. https://doi.org/10.1016/j.jebo.2015.10.017 . de Villiers, E. (2018). Revisiting Max Weber’s ethics of responsibility. Tuebingen: Mohr-Siebeck . DeTombe, D. J. (2002). Complex societal problems in operational research. Eu- ropean Journal of Operation Research, 140 , 232–249. https://doi.org/10.1016/ S0377-2217(02)0 0 066-8 . Diakopoulos, N. (2015). Algorithmic accountability: Journalistic investigation of computational power structures. Digital Journalism, 3 (3), 398–415. https://doi. org/10.1080/21670811.2014.976411 . Didier, C., Duan, W., Dupuy, J.-. P., Guston, D. H., Liu, Y., López Cerezo, J. A. L., et al. (2015). Acknowledging AI’s dark side. Science (New York, NY), 349 (6252), 1064 04 Sep 2015. https://doi.org/10.1126/science.349.6252.1064-c . Dilthey, W. (2013). Einleitung in die Geisteswissenschaften ( Introduction to the human sciences ) . CreateSpace Independent Publishing first published 1883. Reprint 2013 . Domingos, P. , & Richardson, M. (2001). Mining the network value of customers. In Proceedings of the seventh ACM SIGKDD international conference on knowledge dis- covery and data mining (pp. 57–66). ACM . Downing, J. D. H. (1999). ‘Hate speech’ and ‘First Amendment absolutism’ discourses in the US. Discourse & Society, 10 (2), 175–189. 1999Sage Publications Ltd https: //www.jstor.org/stable/42888248 . Dwyer, T., & Martin, F. (2017). Sharing News Online. Digital Journalism, 5 (8), 1080–1100. https://doi.org/10.1080/21670811.2017.1338527 . Entsminger, J., Esposito, M., Tse, T., & Goh, D. (2018). What govern- ments need to understand about ethical AI. European Business Re- view , 71–75. Sept./Oct 2018 http://www.europeanbusinessreview.com/ what- governments- need- to- understand- about- ethical- ai/ . European Commission (2001a). European Governance A White Paper. Brussels 25 July 2001. https://ec.europa.eu/commission/presscorner/detail/en/DOC _ 01 _ 10 Accessed 30 May 2020. European Commission. (2001b). Green paper-promoting a European framework for corporate social responsibility. COM, 2001 , 366. https://ec.europa.eu/ commission/presscorner/detail/en/DOC _ 01 _ 9 . European Commission (2011). A renewed EU strategy 2011-14 for Corporate Social Responsibility. Brussels, 25.10.2011 COM (2011) 681 final. https: //www.europarl.europa.eu/meetdocs/2009 _ 2014/documents/com/com _ com (2011)0681 _ /com _ com(2011)0681 _ en.pdf . Accessed 30 June 2020. European Commission (2018a). A Europe that protects: Countering illegal content online. https://ec.europa.eu/digital- single- market/en/news/europe- protects-countering-illegal-content-online Accessed 30 November 2019. European Commission (2018b). Countering illegal hate speech online. Commission initiative shows continued improvement, further platforms join. http://europa. eu/rapid/press-release _ IP-18-261 _ en.htm Accessed 20 February 2019. European Commission (2018c). Shaping Europe’s digital future - Code of Practice on Disinformation. Brussels, News article 26 September 2018. https://ec.europa.eu/ digital- single- market/en/news/code- practice- disinformation . Accessed 30 June 2020. European Commission (2019). Media Convergence and social media (Unit I.4): strat- egy – shaping Europe’s digital future – Policy – tackling online disinformation. https://ec.europa.eu/digital- single- market/en/tackling- online- disinformation Accessed 30 June 2020. European Commission (2020a). Digitale Wirtschaft – Kommission strebt mehr Wet- tbewerb im Internet an (Digital Economy – Commission strives for more com- petition in the internet). Vertretung in Germany. In: Representation in Ger- many: EU-News no.11/2020, 1-2. https://ec.europa.eu/germany/sites/germany/ files/docs/eu _ nachrichten _ 11 _ 2020web.pdf Accessed 30 June 2020. European Commission. (2020b). European data strategy. Strategy priorities 2019–2024: A Europe fit for the digital age. Making the EU a role model for a society em- powered by data Brussels, 19 February 2020 https://ec.europa.eu/info/strategy/ priorities- 2019- 2024/europe- fit- digital- age/european- data- strategy _ en . European Data Protection Supervisor (2018). Opinion 3/2018: EDPS opinion on online manipulation and personal data . https://edps.europa.eu/sites/edp/files/ publication/18- 03- 19 _ online _ manipulation _ en.pdf Accessed 30 November 2019. European Union. (1995). Directive 95/46/EC of the European Parliament and of the Council of 24 October 1995 on the protection of individuals with regard to the processing of personal data and on the free movement of such data. Official Journal L, 281 . 23/11/1995 P 0031 – 0050 https://eur-lex.europa.eu/ legal-content/EN/TXT/HTML/?uri=CELEX:31995L0046&from=DE . European Union (2016). Regulation (EU) 2016/679 of the European Parliament and of the Council on the protection of natural persons with regard to the pro- cessing of personal data and on the free movement of such data, and repeal- ing Directive 95/46/EC (General Data Protection Regulation). Brussels 27 April 2016. https://eur- lex.europa.eu/legal- content/EN/TXT/?uri=celex%3A32016R0679 Accessed 30 November 2019. Facebook (2012). IPO-Prospectus Filed Pursuant to Rule 424(b)(4), Registration No. 333-179287 NASDAQ 18 May 2012. https://www.nasdaq.com/markets/ipos/filing. ashx?filingid=8629552 Accessed 30 June 2020. Facebook (2019). CSR Globally. https://www.facebook.com/CSRglobally/ Accessed 30 November 2019. Fang, X., Hu, P. J., Li, Z., & Tsai, W. , F. (2013). Predicting adoption probabilities in social networks.. Information Systems Research, 24 (1). http://arxiv.org/pdf/1309. 6369v1 . Fengler, S. (2012). From media self-regulation to ’crowd-criticism’: Media account- ability in the digital age. Central European Journal of Communication . https: //pdfs.semanticscholar.org/0273/a277df6d0b25a74ca67a189f352441863de9.pdf . Fifka, M. (2011). Corporate citizenship in Deutschland und den USA ( Corporate citizen- ship in Germany and the USA ) . Wiesbaden: Springer . Floridi, L. (2013). Distributed morality in an information society. Science and Engi- neering Ethics 19-727-743. https://doi.org/10.1007/s11948- 012- 9413- 4 . Forrester, J. W. (1961). Industrial dynamics . Cambridge, MA: MIT Press . Frau-Meigs, D. (2017). Developing a critical mind against fake news. The https://en.unesco.org/courier/ UNESCO Courier . July-Sept.2017, Paris july-september-2017/developing-critical-mind-against-fake-news . Future of Life Institute (2017). Asilomar AI principles. https://futureoflife.org/ ai-principles/ Accessed 30 November 2019. Gerner, M. (2019). Assessing and managing sustainability in international perspec- tive: Corporate sustainability across cultures – Towards a strategic framework implementation approach. International Journal of Corporate Social Responsibility . vol. 4 issue 1. https://econpapers.repec.org/article/sprijocsr/v _ 3a4 _ 3ay _ 3a2019 _ 3ai _ 3a1 _ 3ad _ 3a10.1186 _ 5fs40991- 019- 0043- x.htm . Accessed 30 June 2020. Gopal Jayal, N. (2018). India: Don’t blame WhatsApp for the lynch mobs. The in- terpreter . Sydney: Lowy Institute 2. Aug. 2018 https://www.lowyinstitute.org/ the-interpreter/india-dont-blame-whatsapp-lynch-mobs . Graber, R., & Lindemann, T. (2018). Neue Propaganda im Internet. Social bots und das Prinzip sozialer Propaganda (New propaganda in the Internet. Social bots and the principle of social propaganda). In K. Sachs-Hombach, & B. Zywietz (Eds.), Fake news, hashtags & social bots. Aktivismus- und Propagandaforschung (New methods of populist propaganda) (pp. 51–68). Wiesbaden: Springer VS. https://link.springer.com/chapter/10.1007%2F978- 3- 658- 22118- 8 _ 3 . Gregory, W. J. (20 0 0). Transforming Self and Society: A “Critical Appreciation”Model. Systemic Practice and Action Research, 13 , 475–501. https://doi.org/10. 1023/A:1009541430809 . Habermas, J. (2009). Diskursethik ( Discourse ethics ). Philosophische Texte Band 3, Studienausgabe . Frankfurt am Main: Suhrkamp Publishing https://www.suhrkamp.de/buecher/diskursethik _ philosophische _ texte _ band _ -juergen _ habermas _ 58528.html . Hamlin, A., & Jennings, C. (2011). Expressive political behaviour: Foundations, scope and implications. British Journal of Political Science, 41/3 , 645–670. https://doi. org/10.1017/S0 0 071234110 0 0 020 . Harwell, D. (2019). Faked Pelosi videos, slowed to make her appear drunk, spread across social media. The Washington Post, Technology Section May 24, 2019, Washington, https://www.washingtonpost.com/technology/2019/05/23/ faked- pelosi- videos- slowed- make- her- appear- drunk- spread- across- social- media/ , Accessed 30 June 2020 . Hogan, L., & Safi, M. (2018). Revealed: Facebook hate speech exploded in Myanmar during Rohingya crisis. The Guardian 3 April 2018, https: //www.theguardian.com/world/2018/apr/03/revealed- facebook- hate- speech - exploded- in- myanmar- during- rohingya- crisis , Accessed 30 June 2020 . Hong, S., & Kim, N. (2018). Will the internet promote democracy? Search engines, concentration of online news readership, and e-democracy. Journal of Infor- mation Technology & Politics, 15 (4) 2018. https://doi.org/10.1080/19331681.2018. 1534703 . Isaak, M., & Frenkel, S. (2020). Facebook adds labels for some posts as advertisers pull back. New York Times 26 June 2020, https://www.nytimes.com/2020/06/26/ technology/facebook- labels- advertisers.html , Accessed 30 June 2020 . Ives, N. (2020). Where advertisers boycotting facebook are spending their money instead. Wall Street Journal . June 29, 2020 https://www.wsj.com/articles/ where-advertisers-boycotting-facebook-are-spending-their-money-instead- 11593467895 . Jacob, R. B., Sapienza, E., & Lister, S. (2019). UNDP’s engagement with the media for governance, sustainable development and peace . Oslo: United Nations Develop- ment Programme 25 January 2019 https://www.undp.org/content/dam/undp/ library/Democratic%20Governance/OGC/UNDP%20Engagement%20with%20the% 20Media.pdf . Jones, M. D. , & Flaxman, L. (2015). Mind wars . A History of Mind Control, Surveillance, and Social Engineering by the Government, Media, and Secret Societies . Wayne, NJ: New Page Books, Career Press . Kant, I. (1993). Grounding for the Metaphysics of Morals. In J. W. Ellington (Ed.), Translator (3rd ed.). Indianapolis: Hackett Publishing Company. https://www. hackettpublishing.com/grounding- for- the- metaphysics- of- morals . Kavanagh, D., McGarraghy, S., & Séamas, K. (2015). Ethnography in and around an algorithm. Paper submitted for inclusion in EGOS. SWG Creativity, Reflexivity and Responsibility in Organizational Ethnography . https://researchrepository.ucd. ie/handle/10197/7348 . Kim, H., Garrido, P., Tewari, A., Xu, W., Thies, J., Nießner, M., et al. (2018). Deep video portraits . siggraph vancouver 2018 . Stanford University, Zollhoefer. M https:// web.stanford.edu/ ∼zollhoef/papers/SG2018 _ DeepVideo/page.html . Knuutinen, R. (2014). Corporate social responsibility, taxation and aggressive tax planning. Nordic Tax Journal, 2014 (1), 36–75. https://doi.org/10.1515/ ntaxj- 2014- 0 0 03 . 916 U. Reisach European Journal of Operational Research 291 (2021) 906–917 control. Science (New York, NY), 361 (6404) IssueAAAS. https://doi.org/10.1126/ science.aat5991 . Tandoc, E. C., Lim, Z. W., & Ling, R. (2018). Defining “fake news”. Digital Journalism, 6 (2), 137–153. https://doi.org/10.1080/21670811.2017.1360143 . Thaler, R. , & Sunstein, C. R. (2009). Nudge. Improving decisions about health, wealth and happiness . London: Penguin Books . The Economist (2020). Speech online: The moderator’s dilemma. San Fran- cisco, 04 June 2020. https://www.economist.com/international/2020/06/04/ donald- trump- has- reignited- a- debate- about- regulating- speech- online Ac- cessed 30 June 2020. The High-Level Expert Group on Artificial Intelligence. (2019). Ethics guidelines for trustworthy AI . Brussels: European Commission 8 April 2019 https://ec.europa. eu/futurium/en/ai- alliance- consultation/guidelines#Top . Torabi Asr, F., & Taboada, M. (2019). Big data and quality data for fake news and misinformation detection. Big Data & Society . https://doi.org/10.1177/ 2053951719843310 . Turker, D. (2008). Measuring corporate social responsibility: A scale development study. Journal of Business Ethics, 85 , 411–427 20 09Springer 20 08. https://doi.org/ 10.1007/s10551- 008- 9780- 6 . United Nations. (20 0 0). Global compact. The ten principles of the UN global compact https://www.unglobalcompact.org/what- is- gc/mission/principles . Usher, N. (2017). The appropriation/amplification model of citizen journalism. Jour- nalism Practice, 11 2-3, 247-265. https://doi.org/10.1080/17512786.2016.1223552 . van Dijck, J. (2014). Datafication, dataism and dataveillance. Big Data between scientific paradigm and ideology. Surveillance & Society, 12 (2), 197–208. http://www.academia.edu/26648535/Datafication _ dataism _ and _ dataveillance _ Big _ Data _ between _ scientific _ paradigm _ and _ ideology . Varol, O., Ferrara, E., Davis, C.A., .Menczer, F., & Flammini, A. (2017). Online human- bot interactions: Detection, estimation, and characterization. https://arxiv.org/ pdf/1703.03107 Accessed 30 November 2019. Velasquez, M., Andre, C., Shanks, T., S.J., & Meyer, M. (2010). What is ethics? Ethics IIE V1 N1 (Fall 1987). Revised in 2010. https://www.scu.edu/ethics/ ethics-resources/ethical- decision- making/what- is- ethics/ Accessed 30 Novem- ber 2019. von Schomberg, R. (2013). A vision of responsible innovation. In R. Owen, M. Heintz, & J. Bessant (Eds.), Responsible innovation . London: John Wi- ley forthcoming http://www.pacitaproject.eu/wp-content/uploads/2014/04/ von- Schomberg- RRI- owenbookChapter.pdf . Vosoughi, S., Roy, D., & Aral, S. (2018). The spread of true and false news online. Science (New York, NY), 359 (6380), 1146–1151. https://doi.org/10.1126/science. aap9559 . Vranica, S., & Seetharaman, D. (2020). Facebook tightens controls on speech as ad boycott grows. Wallstreet Journal June 26, 2020, https://www.wsj.com/articles/unilever-to-halt-u-s-ads-on-facebook-and- twitter-for-rest-of-2020-11593187230 , Accessed 30 June 2020 . Wall, M. (2017). Mapping citizen and participatory journalism. Journalism Practice, 11 (2–3), 134–141. https://doi.org/10.1080/17512786.2016.1245890 . Weber, M. (2014). Politics as a vocation, translation of his speech “Politik als Beruf” in 1919. In D. Owen, & T. B. Strong (Eds.), The Vocation lectures, 2/14/04 edition, translated by R. Livingstone . Indianapolis: Hackett Publishing Company. https://books.google.de/books?id= _ _ sYjFoyjtwC&dq=D.+Owen+%26+T.B.+Strong+ (2014)+The+Vocation+lectures+Weber,+M.+politics+as+a+vocation&lr=&source= gbs _ navlinks _ s . Zak, P. (2014). Why your brain loves good storytelling. Harvard Business Review , (October). https://hbr.org/2014/10/why- your- brain- loves- good- storytelling . Ac- cessed 30 November 2019 . Zelizer, B. (2010). Journalism, memory, and the voice of the visual. In B. Zelizer (Ed.), About to die: How news images move the public (pp. 1–27). Oxford: Ox- ford University Press. https://repository.upenn.edu/cgi/viewcontent.cgi?article= 1684&context=asc _ papers . Zeng, Y. (2019b). Linking AI Principles (LAIP), research center for brain-inspired intelligence: institute of automation, Chinese Academy of sciences, innova- tion academy of artificial intelligence, school of artificial intelligence, Uni- versity of Chinese academy of sciences, China-UK Research Centre for AI Ethics and Governance, Research Center for AI Ethics and Sustainable Devel- opment, Beijing Academy of Artificial Intelligence, Beijing 2018–2020. http:// linking- ai- principles.org/ Accessed 29 August 2020. Zeng, Y., Lu, E., & Huangfu, C. ( 2019a ). Linking artificial intelligence principles. https: //arXiv:1812.04814 Accessed 29 August 2020. Zuckerberg, M. (2019). Mark Zuckerberg speech at Georgetown University, 22.10.2019, New York. Facebook Newsroom https://newsroom.fb.com/news/2019/ 10/mark- zuckerberg- stands- for- voice- and- free- expression/ , Accessed 30 June 2020 . Zweig, K., Deussen, O., & Krafft, T. (2017). Algorithmen und Meinungs- 40 , 318–326. https://doi.org/10.1007/ Spektrum, bildung. Informatik s00287- 017- 1050- 5 . Lazer, D. M. J., Baum, M. A., Benkler, Y., Berinsky, A. J., Greenhill, K. M., Menczer, F., et al. (2018). The science of fake news: Addressing fake news requires a multidisciplinary effort. Science (New York, NY), 359 (6380), 1094–1096. https: //doi.org/10.1126/science.aao2998 . Legal Information Institute of Cornell Law School (2018). 47U.S. Code § 230 – Pro- tection for private blocking and screening of offensive material. https://www. law.cornell.edu/uscode/text/47/230 Accessed 30 June 2020. Legal Information Institute of Cornell Law School (2019). Federal rules of evidence, rule 101. Scope; definitions, (4) data compilation. https://www.law.cornell.edu/ rules/fre Accessed 30 June 2020. Lipschultz, J. H. (2017). Social Media Communication: Concepts, Practices, Data, Law and Ethics (2nd edition). New York: Routledge. https://doi.org/10.4324/ 9781315388144 . Masoud, N. (2017). How to win the battle of ideas in corporate social responsibility: The international pyramid model of CSR. International Journal of Corporate Social Responsibility, 2 , 4. https://doi.org/10.1186/s40991- 017- 0015- y . Mayer-Schonberger, V., & Cukier, K. (2013). Big data: A revolution that will trans- form how we live, work and think. Canada: Eamon Dolan/Houghton Mifflin Har- court https://books.google.de/books/about/Media _ Accountability _ and _ Freedom _ of _ Publ.html?id=zjIDEsN6NnUC&redir _ esc=y . McQuail, D. (2003). Media Accountability and Freedom of Publication . Oxford: Oxford University Press. https://books.google.de/books/about/Media _ Accountability _ and _ Freedom _ of _ Publ.html?id=zjIDEsN6NnUC&redir _ esc=y . Mead, G. H. (1910). Social consciousness and the consciousness of meaning. Psycho- logical Bulletin, 7 (12), 397–405. https://doi.org/10.1037/h0074293 . Mercier, H. , & Sperber, H. (2017). The enigma of reason . Boston: Harvard University Press . Organization of Economic Cooperation and Development. (2011). Guidelines for multinational enterprises Paris http://mneguidelines.oecd.org/guidelines/ . Pariser, E. (2011). The filter bubble: What the internet is hiding from you . London: Penguin . Plaisance, P. L. (2016). Media ethics theorizing, re -oriented: A shift in focus for individual-level analyses. Journal of Communication, 66 (3), 454–474. https:// onlinelibrary.wiley.com/doi/epdf/10.1111/jcom.12230 . Polonski, V. (2018). Impact of social media on the outcome of the EU referendum . Political studies Association, Loughborough University, Centre of the Study of Journalism, Culture and Community, Bournemouth University https://www. referendumanalysis.eu/eu-referendum-analysis-2016/section-7-social-media/ impact- of- social- media- on- the- outcome- of- the- eu- referendum/ . Reality Check team. (2020). Social media: How do other governments regulate it? BBC . 12 February 2020 https://www.bbc.com/news/technology-47135058 . Rennó, R. (2018). WhatsApp: The widespread use of WhatsApp in political cam- paigning in the global south. Our data, our selves, Tacticaltech.org., https:// ourdataourselves.tacticaltech.org/posts/whatsapp/ Accessed 30 November 2019. Rothman, P. (2015). The politics of media development: The importance of engaging government and civil society . Washington, DC: Centre for International Media As- sistance at the National Endowment for Democracy https://www.cima.ned.org/ wp- content/uploads/2015/08/CIMA- The- Politics- of- Media- Development.pdf . Schaedel, S. (2016). Did the pope endorse trump? Ask factcheck, a project of the Annenberg public policy center https://www.factcheck.org/2016/10/ did- the- pope- endorse- trump/ Posted on October 24, 2016 . Schmidpeter, R. (2013). Corporate social responsibility: A New management paradigm? In John O. Okpara, & Samuel O. Idowu (Eds.), Corporate social re- sponsibility. Challenges, opportunities and strategies for the 21st century leaders Heidelberg: Springer. https://doi.org/10.1007/978- 3- 642- 40975- 2 _ 10 . Shaban, H. (2019). WhatsApp trying to curb viral misinformation with mes- saging limit. The Washington Post January 22, 2019. In: NDTV World- News, https://www.ndtv.com/world- news/whatsapp- trying- to- curb- on- viral- misinformation- with- messaging- limit- 1981684 , Accessed 30 June 2020 . Sircar, S. (2018). India tops the world... in forwarding messages! Umm, why though? Bloomberg/Quint July 25, 2018, https://www.thequint.com/voices/ opinion/reasons-why-indians-forward-many-whatsapp-messages , Accessed 30 June 2020 . Šontait ˙e-Petkevi ˇcien ˙e, M. (2015). CSR reasons, practices and impact to corporate reputation. Procedia – Social and Behavioral Sciences, 213 , 503–508 Elsevier, De- cember 2015. https://doi.org/10.1016/j.sbspro.2015.11.441 . Stecher, N. (2017). Social bots – Mensch oder maschine? Wie erkennt man den unterschied. (Social bots – Human or machine? How to recognize the difference). Initiative D21 e.V., Berlin Juli 2017. https://initiatived21.de/ artikel- was- sind- social- bots/ Accessed 30 November 2019. Streitfeld, D. (2017). ‘The internet is broken’: @ev is trying to salvage it. The New York Times 20. May 2017, https://www.nytimes.com/2017/05/20/technology/ evan- williams- medium- twitter- internet.html , Accessed 30 June 2020 . Sunstein, C. R. (2007). Republic.com 2.0 . Princeton University Press https://www. jstor.org/stable/j.ctt7tbsw . Taddeo, M., & Floridi, L. (2018). How AI can be a force for the good. An ethical framework will help to harness the potential of AI while keeping humans in 917 