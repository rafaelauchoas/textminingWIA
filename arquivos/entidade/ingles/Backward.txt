Artificial Intelligence 170 (2006) 953–982www.elsevier.com/locate/artintBackward-chaining evolutionary algorithmsRiccardo Poli ∗, William B. LangdonDepartment of Computer Science, University of Essex, UKReceived 12 August 2005; received in revised form 11 April 2006; accepted 24 April 2006Available online 9 June 2006AbstractStarting from some simple observations on a popular selection method in Evolutionary Algorithms (EAs)—tournamentselection—we highlight a previously-unknown source of inefficiency. This leads us to rethink the order in which operations areperformed within EAs, and to suggest an algorithm—the EA with efficient macro-selection—that avoids the inefficiencies associ-ated with tournament selection. This algorithm has the same expected behaviour as the standard EA but yields considerable savingsin terms of fitness evaluations. Since fitness evaluation typically dominates the resources needed to solve any non-trivial problem,these savings translate into a reduction in computer time. Noting the connection between the algorithm and rule-based systems,we then further modify the order of operations in the EA, effectively turning the evolutionary search into an inference process op-erating in backward-chaining mode. The resulting backward-chaining EA creates and evaluates individuals recursively, backwardfrom the last generation to the first, using depth-first search and backtracking. It is even more powerful than the EA with efficientmacro-selection in that it shares all its benefits, but it also provably finds fitter solutions sooner, i.e., it is a faster algorithm. Thesealgorithms can be applied to any form of population based search, any representation, fitness function, crossover and mutation,provided they use tournament selection. We analyse their behaviour and benefits both theoretically, using Markov chain theory andspace/time complexity analysis, and empirically, by performing a variety of experiments with standard and back-ward chainingversions of genetic algorithms and genetic programming.© 2006 Elsevier B.V. All rights reserved.Keywords: Evolutionary computation; Genetic algorithm; Genetic programming; Efficient search; Backward chaining; Tournament selection1. IntroductionEvolutionary Algorithms (EAs) (see Algorithm 1) are a simple and, today, very popular form of search and opti-misation technique [1,2,6,13,21,22]. Their invention dates back many decades [11,14,18,34,40] (and see also [10]).EAs share several ingredients with mainstream AI search techniques. For example, EAs can be seen as special kindsof generate-and-test algorithms, as parallel forms of beam search, etc. (see [26] for a discussion on similarities anddifferences between EAs and other search algorithms). However, their development has been largely in parallel andindependent from AI search.Despite the simplicity of EAs, sound theoretical models of EAs and precise mathematical results have been scarceand hard to obtain, often emerging many years after the proposal of the original algorithm [7,15,16,19,24,25,28–31,* Corresponding author.E-mail addresses: rpoli@essex.ac.uk (R. Poli), wlangdon@essex.ac.uk (W.B. Langdon).0004-3702/$ – see front matter © 2006 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.04.003954R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982Select sub-population for reproductionRecombine the genes of selected parents1: Initialise population2: Evaluate population3: loop4:5:6: Mutate the offspring stochastically7:8:9: end loopEvaluate the fitness of the new populationIf stopping criterion is satisfied then exit loopAlgorithm 1. Generic evolutionary algorithm.36–39,42–44,47]. An important reason for this delay is that each algorithm, representation, set of genetic operatorsand, in some cases, fitness function requires a different theoretical model. In addition, the randomness, non-linearitiesand immense number of degrees of freedom present in a typical EA make life very hard for theoreticians.One line of theoretical research where differences in representations have not been an obstacle is the analysis ofselection algorithms (step 4 in Algorithm 1). This is because selection requires only knowledge of the fitness (orphenotype) of the individuals in the population, and so the same form of selection can be applied irrespective of therepresentation of an individual (or genotype).Different selection methods have been analysed mathematically in depth in the last decade or so. The main empha-sis of previous research has been the takeover time [12], i.e., the time required by selection to fill up the populationwith copies of the best individual in the initial generation, and the evaluation of the changes produced by selection onthe fitness distribution of the population [4,5,23]. In this second line of research, the behaviour of selection algorithmsis characterised using the loss of diversity, i.e., the proportion of individuals in a population that are not selected.These theoretical studies are very comprehensive and appeared to have completely characterised selection, funda-mentally making it a largely understood process. However, starting from some simple observations on the samplingbehaviour of perhaps the most popular selection method, tournament selection, in this paper we show that there is apossible source of inefficiency in EAs. This phenomenon, which had not been analysed in previous research, has verydeep implications, its analysis effectively leading to a completely new class of EAs which is more powerful and closerin spirit to classical AI techniques than traditional EAs.The paper is organised as follows. In Section 2 we describe tournament selection, we briefly review previousrelevant theoretical results, and then go on to describe, in Section 3, the sampling inefficiency in this form of selection.In order to remove the predicted sampling inefficiency of tournament selection, in Section 4, we rethink the orderin which operations are performed within EAs. This reveals that, embedded in EAs, is a graph-structure induced bytournament selection which connects individual samples of the search space across time. (See Fig. 1 in Section 4.)We are then able to suggest an algorithm, the EA with efficient macro-selection, that exploits this graph to remove theinefficiencies associated with tournament selection. The algorithm has the same expected behaviour as the standardEA, while providing considerable savings in terms of fitness evaluations. Furthermore, it is totally general, i.e., it canbe applied to any representation and fitness function, and can be used with any crossover and mutation.In Section 5, we note an unexpected connection between the operations of the EA with efficient macro-selectionand rule-based systems, which leads us to further modify the order of operations in the EA effectively turning theevolutionary search into an inference process operating in backward-chaining mode. The resulting algorithm, whichwe call a backward-chaining EA, creates and evaluates individuals recursively. It starts at the last generation and, usingdepth-first search and backtracking, works backwards to the first. This algorithm is even more powerful than the EAwith efficient macro-selection in that it shares all its benefits, but it provably finds fitter solutions sooner, i.e., it is afaster algorithm.We analyse theoretically the behaviour of the EA with efficient macro-selection and the backward chaining EAalgorithms in Section 6. In particular, in Section 6.1 we start analysing the sampling behaviour of tournament se-lection, focusing on its effects over one time step (a generation) of an EA. We do this by noting and exploiting thesimilarity between sampling and the coupon collection problem. We extend the one-generation analysis to full runs inSection 6.2 by inventing, and then modelling mathematically using Markov chain theory, a more complex version ofthe problem—the iterated coupon collection problem—which exactly mimics tournament selection over multiple gen-erations. This allows us to fully and exactly evaluate the effects of the sampling inefficiency of tournament selectionover entire runs and indicates that extent of the savings that could be achieved.R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982955We discuss the details of the practical implementation of a backward-chaining EA in Section 7 while we comparethe time and space complexity of our implementation with those for a standard EA in Section 8. In Section 9 weprovide experimental results with a Genetic Algorithm (GA) and a Genetic Programming (GP) implementation ofbackward chaining EA. We discuss our findings in Section 10 and provide our conclusions in Section 11.2. Tournament selectionTournament selection is one of the most popular forms of selection in EAs. In its simplest form, a group of nindividuals is chosen randomly uniformly from the current population, and the one with the best fitness is selected(e.g., see [2]). The parameter n is called the tournament size and can be used to vary the selection pressure exerted bythis method (the higher n the higher the pressure to select above average quality individuals).In a population of size M, the takeover time is defined as the number of generations required for selection (whenno other operator is present) to obtain a population containing M − 1 copies of the best individual in the initialgeneration [12]. In [12] the takeover time for tournament selection was estimated using the asymptotic expressiont∗ = 1ln n(cid:2)ln(M) + ln(cid:4)(cid:5)(cid:3)ln(M)where the approximation improves as the population size M → ∞.The loss of (fitness) diversity is the proportion of individuals of a population that is not selected during the selectionphase. Assuming every member of the population has a unique fitness, the loss of diversity pd for tournament selectionwas estimated in [4,5] aspd = n− 1n−1 − n− nn−1 ,and later calculated exactly in [23] as(cid:7)pd = 1MM(cid:6)k=11 − kn − (k − 1)nM n(cid:8)M.The quantities t ∗ and pd give an idea of the intensity with which a selection scheme acts on the population as afunction of the tournament size n and population size M.The only two other pieces of research we are aware of that are relevant in the context of our work are [41,45].In [41] a particular version of tournament selection that guarantees that all individuals in a run are sampled is proposedand it is shown, in some cases, to improve the problem-solving ability of a GA. Similar results have been recentlyreported in [45] which, following an early version of our work [32], proposes a different tournament strategy, whichalso guarantees that all individuals are sampled. While these two lines of work concentrate on modifying tournamentselection, we focus on understanding and exploiting the sampling behaviour of standard tournament selection.3. Sampling behaviour of tournament selectionLet us denote with S the number of selection steps required in each generation (it will immediately become apparentwhat is meant by this). If one assumed that only selection is used or that we use selection to form a mating pool,1 thecreation of a new generation would require exactly S = M selection steps. These are exactly the conditions assumedin [41,45]. However, we do not make this assumption. Instead, we consider the currently much more common casewhere each genetic operator directly invokes the selection procedure to provide a sufficient number of parents for itsapplication (e.g., twice in case of crossover). So, there are situations where more than M selection steps are required toform a new generation. Consider a generational selecto-recombinative algorithm, where crossover is performed withprobability pc and reproduction is performed with probability 1 − pc. (Mutation can be included by making randomchanges to children after they have been created by either crossover and/or reproduction. In all cases the number1 The mating pool is an intermediate population which gets created by using only selection and from which other operations, such as reproductionand crossing over, draw individuals uniformly at random.956R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982of selection steps is unchanged.) The number S of selection steps required to form a new generation is a stochasticvariable with meanE[S] = M(1 − pc) + ρMpc = M1 + (ρ − 1)pc,(cid:2)(cid:5)where ρ = 1 for a crossover operator which returns two offspring after each application, and ρ = 2 if only oneoffspring is returned. The two-offspring version of crossover requires fewer tournaments, and, since ρ = 1, the numberof selection steps required to form a new generation with this operator is not stochastic and we have simply S = M.For brevity in the following we will use the definition α = [1 + (ρ − 1)pc].2Because in each tournament we need n individuals and we perform S = αM selection steps, tournament selectionrequires drawing nαM individuals uniformly at random (with resampling) from the current population. An interestingside effect is, particularly for small tournaments, that not all individuals in a particular generation are necessarilysampled within the nαM draws. For example, let us imagine running an EA starting from a random populationcontaining four individuals, which we will denote as 1, 2, 3 and 4. Let us assume that we are creating the nextgeneration using tournament selection with tournament size n = 2 and mutation only. Then, the creation of the firstindividual will require randomly picking two individuals from the current population (say individuals 1 and 4) andselecting the best for mutation. We repeat the processes to create the second, third and fourth new individuals. Note itis entirely possible that individual 3 was never involved in any of the tournaments.It is absolutely crucial, at this stage, to stress the difference between not sampling and not selecting an individual ina particular generation. Not selecting refers to an individual which was involved in one or more tournaments, but didnot win any, and this is exactly what previous work on loss of diversity has concentrated on. Not sampling refers to anindividual which did not participate in any tournament at all, simply because it was not sampled during the creationof the required S = αM tournament sets. It is individuals such as this that are the focus of this paper. Therefore,the results in this paper are orthogonal to those appeared in the work mentioned in Section 2 and are not limited byuniqueness assumptions.Continuing with our argument, in general, how many individuals should we expect not to take part in any ofS = αM tournaments? As will be shown in Section 6.1, an answer comes straight from the literature on the couponcollector problem. However, before we explain the connection in more detail, we may want to reflect briefly on whythis effect is important.In general those individuals that do not get sampled by the selection process have no influence whatsoever onfuture generations. However, these individuals use up resources, e.g., memory, but also, and more importantly, CPUtime for their creation and evaluation. For instance, individual 3 in the previous example was randomly generated andhad its fitness evaluated in preparation for selection, but neither its fitness nor its genetic make up could have anyinfluence on future generations. So, one might ask, why did we generate such an individual in the first place? Andwhat about generations following the first two? It is entirely possible that an individual in generation two got createdand evaluated, but was then neglected by tournament selection, so it had no effect whatsoever on generations 3, 4, etc.Did we really need to generate and evaluate such an individual? If not, what about the parents of such an individual:did we need them? What sort of saving could we obtain by not creating unnecessary individuals in a run?In Section 6 we will provide theoretical answers to all the questions above and more. In particular, we will showthat in some conditions, savings of 20% fitness evaluations or, in fact, even more are easily achievable. Before wedo this, however, we want to reconsider the way EAs are run and see whether there are ways in which we couldexploit the inefficiencies of tournament selection. Amazingly, we will find that not only there are efficient algorithmsfor achieving this, but also that this can be done without altering in any way the expected behaviour of evolutionaryalgorithms.4. Running EAs efficientlyNormally, in each generation of an EA with tournament selection we iterate the following phases (see Algorithm 2):2 In the following we will ignore the (potential) stochasticity of S. This is justifiable for various reasons: a) it simplifies the analysis (but withoutsignificant loss in terms of accuracy of the results obtained, as empirically verified), b) when ρ = 1 (two-offspring crossover or mutation onlyalgorithm) there is no stochasticity (and so the analysis is exact), c) even with ρ = 2 (one-offspring crossover) it is possible to slightly modify theEA in such a way that there is no stochasticity.R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–9829571: Randomly initialise individuals in population pop, calculate correspondingfitness values, and store them in vector fitop = choose genetic operatorfor arg from 1 to arity(op) dofor ind from 1 to M do2: for gen from 1 to G do3:4:5:6:7:8:9:10:11:12:13:14: end forpool = choose n random individuals drawing from popw[arg] = select winner from pool based on fitnesses in fitend fornewpop[ind] = result of running operator op with arguments w[1], . . .newfit[ind] = fitness of newpop[ind]end forpop = newpopfit = newfitAlgorithm 2. Standard generational EA with tournament selection. M is the population size,n is the tournament size, and G is maximum number of generations.(a) the choice of genetic operator to use to create a new individual (step 4 in Algorithm 2),(b) the creation of a random pool of individuals for the application of tournament selection (step 6),(c) the identification of the winner of the tournament (parent) based on fitness (step 7),(d) the execution of the chosen genetic operator (step 9),(e) the evaluation of the fitness of the resulting offspring (step 10).Naturally, phases (b) and (c) are iterated as many times as the arity of the genetic operator chosen in phase (a), andthe whole process needs to be repeated as many times as there are individuals in the new population.The genetic makeup of the individuals involved in these operations is of interest only in phase (d) (we need to knowthe parents in order to produce offspring) and phases (c) and (e) (we must know the genetic makeup of individualsin order to evaluate their fitness). However, phases (a) and (b) (steps 4 and 6 in Algorithm 2) do not require anyknowledge about the actual individuals involved in the creation of a new individual. In most implementations thesephases are just performed by properly manipulating numbers drawn from a pseudo-random number generator.So, there is really no reason why we could not first iterate phases (a) and (b) as many times as needed to create afull new generation (of course, memorising all the decisions taken), and then iterate phases (c)–(e). This idea was firstused in [46] for the purposed on speeding up GP fitness evaluation.3In fact, we could go even further. In many practical applications of EAs, people fix a maximum number of gener-ations they are prepared to run their algorithm for.4 Let this number be G. So, at the cost of some memory space, asshown in Algorithm 3, we could iterate phases (a) and (b) not just for one generation but for a whole run from the firstgeneration to generation G (steps 2–9) and then iterate phases (c)–(e) (steps 10–18) as required (that is, either untilgeneration G or until any other stopping criterion is satisfied). We call this algorithm an EA with macro-selection, forobvious reasons.Because the decisions as to which operator to adopt to create a new individual and which elements of the popu-lation to use for a tournament are random, statistically speaking this version of the algorithm is exactly the same asthe original. In fact, if the same seed is used for the random number generator in both algorithms, they are indistin-guishable! However, unlike the standard EA, the EA with macro-selection can easily be modified to avoid wasting thecomputation involved in generating and evaluating the individuals “neglected” by tournament selection.3 The main idea in [46] was to estimate the fitness of the individuals involved in the tournaments by evaluating them on a randomly chosen subsetof the training set available. On the basis of this estimate, for most tournaments it was often possible to determine with a small error probabilitywhich individual would win if all the examples where used. These tournaments could therefore be decided quickly, while only in a subset oftournaments individuals ended up being evaluated using the whole training set. This is what produced the speed up.4 This is a limit that is virtually always present, even if another stopping criterion, e.g., based on fitness, is present.958R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–9821: Randomly initialise individuals in population pop[0], calculate corresponding fitness values,and store them in vector fit[0]op[gen][ind] = choose genetic operatorfor arg from 1 to arity(op[gen][ind]) dopool[gen][ind][arg] = choose n random individuals drawing from pop[gen-1]for ind from 1 to M doend forend for2: for gen from 1 to G do3:4:5:6:7:8:9: end for10: for gen from 1 to G do11:12:13:14:15:16:17:18: end forend forfor ind from 1 to M dofor arg from 1 to arity(op[gen][ind]) dow[arg] = select winner from pool[gen][ind][arg] based on fitnesses in fit[gen-1]end forpop[gen][ind] = result of running operator op[gen][ind] with arguments w[1], . . .fit[gen][ind] = fitness of pop[gen][ind]Algorithm 3. EA with macro-selection.Fig. 1. Example of graph structure induced by tournament selection. Shaded nodes are the possible ancestors of the first individual in the lastgeneration. Note how some nodes are not directly or indirectly connected to the nodes in the last generation.To see how this is possible we should note that the iteration of phases (a) and (b) over multiple generations (steps 2–9 in Algorithm 3) induces a graph structure containing (G + 1)M nodes. Nodes represent all the individuals duringa run (more precisely the elements of the pop array). Edges (which are stored in the pool array) connect each indi-vidual to the individuals which were involved in the tournaments necessary to select the parents of such an individual.We will call these individuals the possible ancestors of the individual (note that the possible ancestors of an individualare a superset of the actual ancestors, i.e., the parents, the parents of the parents, etc. of the individual in question).Let us consider an example where we have a population of M = 6 individuals which we run for G = 3 generationsusing binary tournaments (n = 2), crossover rate pc = 1/3 and each crossover produces one child. Mutation andreproduction are performed with a rate (1 − pc) = 2/3. The graph induced by tournament selection might look likethe one in Fig. 1.R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982959for ind from 1 to M doop[gen][ind] = choose genetic operatorfor arg from 1 to arity(op[gen][ind]) do1: for gen from 1 to G do2:3:4:5:6:7:8: end for9: Analyse connected components in pool array and calculate neglected array10: Randomly initialise individuals in population pop[0] except thoseend forend forpool[gen][ind][arg] = choose n random individuals drawing from pop[gen-1]marked in neglected[0], calculate fitness values, and store them in vector fit[0]for ind from 1 to M do11: for gen from 1 to G do12:13:14:15:16:17:18:19:20:21: end forend ifend forif not(neglected[gen][ind]) dofor arg from 1 to arity(op[gen][ind]) dow[arg] = select winner from pool[gen][ind] [arg] based on fitnesses in fit[gen-1]end forpop[gen][ind] = result of running operator op[gen][ind] with arguments w[1], . . .fit[gen][ind] = fitness of pop[gen][ind]Algorithm 4. EA with efficient macro-selection.We must emphasise that, although the graph-structure connecting the individuals in the population across timeinduced by tournament selection is particularly evident in (and is, thereby, revealed by) the EA with macro-selection,it is, nonetheless, present and deeply embedded in every EA using this form of selection.So, how is this going to help us avoid generating and evaluating the individuals “neglected” by tournament selec-tion? Simple. After macro-selection (the iteration of phases (a) and (b) up until generation G) is completed we analysethe information in the graph structure induced by tournament selection, we identify which individuals are unnecessary,we mark them, and we avoid calculating and evaluating them when iterating phases (c)–(e). Clearly we want to markthose population members that were not involved in any tournament in each generation. However, if we are interestedin calculating and evaluating all the individuals in the population at generation G, maximum efficiency is achievedby considering only the individuals which are directly or indirectly connected with the M individuals in generationG—a problem we can easily solve with a trivial connected-component algorithm. The modified algorithm is shown inAlgorithm 4. We call this an EA with efficient macro-selection (EA-EMS). Note that to maximise efficiency, unusually,initialisation is not the first phase of the EA effectively coming after the macro-selection and connected-componentdetection phases.Let us have a brief look at the differences between our new EA-EMS and a standard EA. Irrespective of theproblem being solved and the parameter settings used, the behaviours of the standard algorithm and the efficientversion proposed above will have to be on average identical. So, what are the differences between the two EAs?Obviously, the standard algorithm requires more fitness evaluations and creations of individuals while the EA-EMSrequires more bookkeeping and use of memory. Also, clearly, in any particular run, the plots of average fitness andmaximum fitness in each generation may differ (since in EA-EMS not all individuals are considered in calculatingthese statistics). However, when averaged over multiple runs the average fitness plots would have to coincide.A more important difference comes from the fact that most practitioners keep track of the best individual seen sofar in a run of an EA and designate that as the result of the run. In EA-EMS we can either return the best individual ingeneration G or the best individual seen in a run, out of those that have been sampled by tournament selection. Becausethe EA-EMS algorithm does not create and evaluate individuals that did not get sampled, the end-of-run results maydiffer in the EA and EA-EMS algorithms. Of course, quite often the best individual seen in a run is actually a memberof the population at the last generation. So, if one creates and evaluates all individuals in generation G (which leads toonly a minor inefficiency in the EA with efficient macro-selection), most of the time the two algorithms will behaveidentically from this point of view too.960R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982The EA-EMS offers us a way to fully exploit the sampling behaviour of tournament selection. This might appearto be the best we can get. However, the recursive nature of connected-component detection and the similarity betweenthe mechanics of EAs and that of rule-based systems suggest a way to make further substantial improvements, as willbe discussed in the next section.5. Backward-chaining EAs and rule-based systemsAt a sufficiently high level of abstraction, there are surprising similarities between EAs and Rule-Based Systems(RBSs) operating in forward-chaining mode (e.g., see [35,48]). In RBSs, we start with a working memory containingsome premises, we apply a set of IF-THEN inference rules which modify the working memory by adding or removingfacts and we iterate this process until a certain condition is satisfied (e.g., a fact which we consider to be a conclusionis asserted). The working memory in a RBS has a similar rôle to that of the population in an EA, and the facts in theworking memory are effectively like the individuals in an population. Because the rules in the knowledge base of aRBS effectively manipulate the facts in the working memory, they share some similarity with the genetic operators inan EA which create new members of the population.Running EAs from generation 0, to generation 1, to generation 2, and so on is the norm: the clock ticks forwardin nature, and this is certainly what has been done for decades in the field of evolutionary computation. The looseanalogy between RBSs and EAs mentioned above is not, in itself, terribly useful, except for one thing: it suggests thepossibility of running an EA in backward chaining mode, like one can do with a RBS, thereby radically subvertingthe natural order of operations in the EA and the “time = generation number” EA canon.Broadly speaking, when a RBS is run in backward chaining, the system focuses on one particular conclusion thatit attempts to prove and operates as follows: a) it looks for all the rules which have such a conclusion as a consequent(i.e. a term following the “THEN” part of a rule), b) it analyses the antecedent (the “IF” part) of each such rule, c) ifthe antecedent is a fact (in other words, it is already in the working memory) then the original conclusion is provenand can be placed in the working memory. Otherwise the system saves the state of the inference and recursivelyrestarts the process with the antecedent as a new conclusion to prove. If there is no rule which has the conclusion as aconsequent, the recursion is stopped and another way of proving it is attempted. If a rule has more than one condition(which is quite common), the system attempts to prove the truth of all the conditions, one at a time. It will assert theconclusion of the rule only if all conditions are satisfied. When backward chaining, the RBS only considers rules thatcan contribute to determining the truth or falsity of the target conclusion. This can lead to major efficiency gains.So, how would we run an EA in backward-chaining mode? Let us suppose we are interested in knowing the makeupof the population at generation G and let us start by focusing on the first individual in the population. Let r be suchan individual. Effectively r plays the rôle of a conclusion we want to prove. In order to generate r we only need toknow what operator to apply to produce it and what parents to use. In turn, in order to know which parents to use, weneed to perform tournaments to select them.5 In each such tournaments we will need to know the makeup of n (thetournament size) individuals from the previous generation (which, of course, at this stage we may still not know). Letus call I = {s1, s2, . . .} the set of the individuals that we need to know in generation G − 1 in order to determine r.Clearly, s1, s2, . . . are like the premises in a rule which, if applied, would allow us to work out r (this would requireevaluating the fitness of each element of I , deciding the winners of the tournament(s) and applying the chosen geneticoperator to generate r). Normally we will not know the makeup of these individuals. However, we can recursivelyconsider each si as a subgoal. So, we determine which operator should be used to compute s1, we determine which setof individuals at generation G − 2 is needed to do so, and we continue with the recursion. When we emerge from it,we repeat the process for s2, etc. The recursion can terminate in one of two ways: a) we reach generation 0, in whichcase we can directly instantiate the individual in question by invoking the initialisation procedure for the particular EAwe are considering, or b) the individual for which we need to know the genetic makeup has already been constructedand evaluated. Clearly the individuals in generation 0 have a rôle similar to that of the initial contents of the workingmemory in a RBS. Once we have finished with r we repeat the process with all the other individuals of interest in thepopulation at generation G, one by one. The process is summarised in Algorithm 5. We will call an EA running inthis mode a Backward-Chaining EA (BC-EA).5 Decisions regarding operator choice and tournaments are trivial and can be made on the spot by drawing random numbers or can be all madein advance as in the EA-EMS.R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–9829611: Let r be an individual in the population at generation G2: Choose an operator to apply to generate r3: Do tournaments to select the parents:{s1, s2, . . .} = individuals in generation G − 1 involved in the tournaments4: Do recursion using each unknown si as a subgoal. Recursion terminatesat generation 0 or when the individual is known (i.e. has been evaluated before).5: Repeat for all individuals of interest in generation G.Algorithm 5. Backward-chaining EA.Clearly, at its top level, the BC-EA is a recursive depth-first traversal of the graph induced by tournament selection(see Fig. 1 and Section 4). While we traverse the graph (more precisely, when we re-emerge from each recursion), weare in a position to know the genetic makeup of the nodes encountered and so we can invoke the fitness evaluationprocedure for them. Thus, we can label each node with the genetic makeup and fitness of the individual representedby such a node. Recursion stops when we reach a node without incoming links (a generation-0 individual, which getsimmediately labelled randomly and evaluated) or when we reach a node that has been previously labelled.Statistically a BC-EA is fully equivalent to the EA-EMS, and so it presents the same level of equivalence to anordinary EA. In particular, if the same seed is used for the random number generators and all decisions regardingoperators and tournaments are performed in a batch before the graph traversal, the Gth generations of a BC-EA andan EA are indistinguishable.So, if there are no differences why bother with a BC-EA instead of using a simpler “forward-chaining” versionof the algorithm? One important difference between the two modes of operation is the order in which individualsin the population are evaluated. To illustrate this let us re-consider the example in Fig. 1 and let us suppose that, inthe first instance, we are interested in knowing the first individual in the last generation. (The possible ancestors ofthis individual are shown as shaded nodes in Fig. 1.) Furthermore, for brevity, let us denote the nodes in row i (forindividual) and column g (for generation) in the graph with the notation rig. In a forward chaining EA, even if weknew which individuals are unnecessary to define our target individual r13 (individuals r50, r31, r61, r22, etc.), wewould evaluate individuals column by column from the left to the right. (E.g. EA-EMS evaluates r10, r20, r30, r40,r60, r11, r21, r41, r51, r12, r32, and finally r13.) That is, generation 0 individuals are computed before generation 1individuals, which in turn are computed before generation 2 individuals, and so on. A BC-EA would instead evaluatenodes in a different order. For example, it might do it according to the sequence: r10, r30, r40, r11, r20, r21, r12, r60,r41,r51, r32, and finally r13. So, the algorithm would move back and forth evaluating nodes at different generations.That is “time (cid:5)= generation number” in the BC-EA.Why is this important? Typically, in an EA both the average fitness of the population and the maximum fitness ineach generation grow as the generation number grows. In our forward chaining EA the first 3 individuals evaluatedhave an expected average fitness equal to the average fitness of the individuals at generation 0, and the same is truefor the BC-EA. However, unlike for the forward-chaining EA, the fourth individual created and evaluated by BC-EAbelongs to generation 1, so its fitness is expected to be higher than that of the previous individuals. Individuals 5 and 6have same expected fitness in the two algorithms. However, the seventh individual drawn by BC-EA is a generation 2individual, while the forward EA draws a generation 1 individual. So, again the BC-EA is expected to produce a higherfitness sample than the other EA. Of course, this process is not going to continue indefinitely, and at some point theindividuals evaluated by BC-EA start being on average inferior. This is unavoidable since the sets of individualssampled by the two algorithms are identical.This behaviour is general. In virtually all problems of practical interest, fitness tends to increase generation aftergeneration. So a BC-EA will find fitter individuals faster than an EA-EMS in the first part of a run and slower inthe second part. So, if one restricts oneself to that first phase, the BC-EA is not just more efficient than an ordinaryEA because it avoids evaluating individuals neglected by tournament selection but also because it tends to find bettersolutions faster. I.e. BC-EA is also a more effective search algorithm. How can we make sure we work in the regionwhere the BC-EA is superior to the corresponding EA-EMS? Simple: like any ordinary EA, in a BC-EA one doesnot need to continue evolution until all the individuals in generation G are known and evaluated; we can stop thealgorithm whenever the best fitness seen so far reaches a suitably high value. In this way we can avoid at least a partof the phase where BC-EA is slower than the EA-EMS.962R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982It is worth noting that this “faster convergence” behaviour is present in a BC-EA irrespective of the value of thetournament size, although, of course, the benefits of using BC-EAs depend on it.6. TheoryIn this section we want to model mathematically the sampling behaviour of tournament selection and understandwhat savings we can achieve using the EA-EMS and the BC-EA. We will start by drawing an analogy betweentournament selection and the coupon collection problem.6.1. Coupon collection and tournament selectionIn the coupon collector problem, every time a collector buys a certain product, a coupon is given to him or her. Thecoupon is equally likely to be any one of N types. In order to win a prize, the collector must have at least one couponof each type. The question is: how many products will the collector have to buy before he can expect to have a full setof coupons? The answer [9] can be derived by considering that the probability of obtaining a first coupon in one trialis 1 (so the expected waiting time is just 1 trial), the probability of obtaining a second coupon (distinct from the firstone) is N −1N −1 ), the probability of obtaining a third coupon (distinct from the firsttwo) is N −2N −2 ), and so on. So, the expected number of trials to obtain a full set ofcoupons isN (so the expected waiting time is NN (so the expected waiting time is NEN = 1 + NN − 1+ NN − 2+ · · · + N = N log N + O(N).It is well known that the N log N limit is sharp. If X is the number of purchases before one of each type of coupon iscollected, for any constant cPr{X > N log N + cN } = 1 − e−e−c.limN→∞E.g., for c = 3, in the limit where there are many types of coupons, the probability it takes more than N log N + 3Ntrials to purchase at least one of each type is less than 5%.How is the process of tournament selection related to the coupon collection problem? We can imagine that theM individuals in the current population are N = M distinct coupons and that tournament selection will draw (withreplacement) nαM times from this pool of coupons. Because of the sharpness of the coupon-collector limit mentionedabove, if nα > log M + c for some suitable positive constant c, then we should expect tournament selection to sampleall individuals in the population most of the time. However, for sufficiently small tournament sizes or for sufficientlylarge populations the probability that there will be individuals not sampled by selection becomes significant.So, how many different coupons (individuals) should we expect to have sampled at the end of the nαM trials? Inthe coupon collection problem, the expected number of trials necessary to obtain a set of x distinct coupons is [9]Ex = 1 + NN − 1+ NN − 2+ · · · +NN − x + 1= N logNN − x+ O(N).By setting Ex = nαM, N = M and ignoring terms of order O(N), from this we obtain an estimate for the number ofdistinct individuals sampled by selection−nα).x ≈ M(1 − e(1)This indicates that the expected proportion of individuals not sampled in the current population varies approximatelylike a negative exponential of the tournament size.This approximation is quite accurate. However, we can calculate the expected number of individuals neglectedafter performing nαM trials directly. We first calculate the probability that one individual is not involved in one trialas 1 − 1/M. Then the expected number of individuals not involved in any tournaments is simply(cid:7)(cid:8)−nαMM(1 − 1/M)nαM = MMM − 1which also varies like a negative exponential of the tournament size.,R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982963Fig. 2. Proportion of individuals not sampled in one generation by tournament selection for different tournament sizes, n, and population sizes Massuming only two-offspring crossover and/or mutation are used.As shown in Fig. 2 for α = 1 (two-offspring crossover or no crossover), typically for n = 2 over 13% of thepopulation is neglected, for n = 3 this drops to 5%, for n = 4 this is 2%, and becomes negligible for bigger valuesof n.This simple analysis suggests that saving computational resources by avoiding the creation and evaluation of in-dividuals which will not be sampled by the tournament selection process is possible only for relatively low selectionpressures. However, tournament sizes in the range 2–5 are quite common in practice, particularly when attackinghard, multi-modal problems which require extensive exploration of the search space before zooming the search ontoany particular region. Furthermore, in the next section, where we look at the behaviour of tournament selection overmultiple generations, we will show that much bigger savings than those suggested above can be achieved.6.2. Iterated coupon collector problemLet us consider a new game, that we will call the iterated coupon collection problem, where the coupon set changesat regular intervals, but the number of coupons available, N , remains constant. Initially the collector is given a (pos-sibly incomplete) set of m0 old coupons. Each old coupon allows the collector to draw n new coupons. So, he canperform a total of nm0 trials, which will produce a set of m1 distinct coupons from the new set. The coupon set nowchanges, and the player performs nm1 trials to gather as many new distinct coupons as possible. And so on. Interestingquestions here are: what happens to mt as t grows? Will it reach a limit? Will it oscillate? In which way will the valuesof n, m0 and N influence its behaviour?Before we answer these questions let us motivate our analysis a little. How is this new problem related to EAs andtournament selection? The connection is simple (we will assume α = 1, e.g. two offspring crossover and mutation, forthe sake of clarity). Suppose we are interested in computing and evaluating m0 individuals in a particular generation,G, of a run. These are like the initial set of old coupons given to the player. Clearly, in order to create such individuals,we will need to know who their parent(s) were. This will require running m0 tournaments to select such parents. Ineach tournament we randomly pick n individuals from generation G − 1 (each distinct individual in that generationis equivalent to a coupon in the new coupon set). After, nm0 such trials we will be in a position to determine whichindividuals in generation G − 1 will contribute to future generations, we can count them and denote this number964R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982with m1.6 So, again, we can concentrate on these individuals only. They are equivalent of the new set of coupons thecollector has gathered. We can now perform nm1 trials to determine which individuals in generation G − 2 (the newcoupon set) will contribute to future generations, we can count them and denote this number with m2 and so on untilwe reach the initial random generation. There the game stops.The graph induced by tournament selection is a stochastic variable. Every time we run an EA, we instantiatesuch a variable. So, in terms of the graph structure associated to tournament selection, the process described abovecorresponds to the instantiation of one such structure and mt corresponds to the number of possible ancestors (nodes)of the m0 individuals of interest, in the (G − t)th vertical layer of the graph. So, effectively, the iterated couponcollector problem is a model for the sampling behaviour of tournament selection over multiple generations in agenerational EA.Knowing the sequence mt for a particular EA would tell us how much we could save by not creating and evaluatingindividuals which will not be sampled by selection. Naturally, we will not have an oracle to help us choose G and togive us m0. For now, while we concentrate on understanding more about the iterated coupon collector problem, wecould think of G as the number of generations we are prepared to run our EA for, and we might imagine that m0 = M(the whole population).In the classical coupon collection problem, the shopper will typically perform as many trials as necessary to gathera full collection of coupons. As we have seen before, however, it is quite easy to estimate how many distinct couponsone should expect at the end of any given fixed number of trials. Because the iterated coupon collection game startswith a known number of trials, we can calculate the expected value of m1. However, we cannot directly apply thetheory in the previous section to gather information about m2. This is because m1 is a stochastic variable, so in orderto estimate m2 we would need to know the probability distribution of m1 not just its expected value.Exact probabilistic modelling can be obtained by considering the coupon collection game as a Markov chain, wherethe state of the chain is the number of distinct coupons collected. The transition matrix for the chain can easily beconstructed by noticing that the chain can be in state k (i.e., the collector has k distinct coupons) after the next couponis purchased only if either it was already in state k and the new coupon is a duplicate (which happens with probabilityk−1N ) or it was in state k − 1 and the next coupon is different from all those currently held (which, of course, happenswith probability N −k+1). So, the number of distinct individuals in the previous generation sampled when randomlypicking individuals for tournament selection can be described by applying the following Markov transition matrix anumber of times:⎛⎞NA = 1M⎜⎜⎜⎜⎜⎜⎝010M0 M − 10...00...0002000M − 2 3......00. . .0. . .0. . .0. . .0.... . .. . . M⎟⎟⎟⎟⎟⎟⎠.The process alwayse0 = ( 10samples from the population) is given by At e0, which is simply the first column of the matrix At .from state 0. This can be represented using the state probability vector0 )T.7 So, the probability distribution over the states after t coupon purchases (or random0 . . .starts0Suppose we are only interested in m0 individuals in the last generation G. The number of tournaments used toconstruct the last generation will be nαm0. Therefore the probability distribution of the number of distinct individualswe need to know from generation G − 1, m1, is given by Anαm0 e0. Notice that this also gives us the probabilitydistribution over the number of draws, nαm1, we will need to make from generation G − 2 in order to fully determinethe m1 individuals we want to know at generation G − 1.6 Note we work back from the last generation, index0, generation G − 1, index1, generation G − 2, index2, etc. Also, because at this stage weare only interested in knowing the number of individuals playing an active rôle in generation G − 1, there is no need to determine the winners ofthe tournaments. We just need to know who was involved in which tournament. So, we do not even need to evaluate fitness, and, therefore, we donot need to know the genetic makeup of any individual.7 Each element of a state probability vector represents the probability of a system being in the corresponding state. Since a system must alwaysbe in some state, the elements of the vector must add up to 1. In the coupon collection problem initially only state 0 is possible. So, only the firstelement of e0 is non-zero.R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982965For example, if the population size is M = 3, the number of states in the Markov chain is M + 1 = 4, the tournamentsize is n = 2, we use a two-offspring version of crossover (α = 1) and we are interested in m0 = 1 individuals (sonαm0 = 2), then the probability distribution of m1 is represented by the following probability vectorA2e0 =(cid:8)2(cid:7)13⎛⎜⎝0300012000210003⎞⎟⎠2 ⎛⎜⎝⎞⎟⎠ =⎛⎜⎝(cid:7)(cid:8)1910000 0 0 03 1 0 06 6 4 00 2 5 9⎞⎛⎞⎛⎞⎟⎠⎜⎝⎟⎠ = 13⎜⎝⎟⎠ .10000120If we were interested in m0 = 2 individuals at generation G, the probability distribution over the number m1 of uniqueindividuals sampled would be A4e0 = ( 0.0 0.0370 0.5185 0.4444 )T. Finally, if we were interested in the wholepopulation (m0 = 3) the distribution would be A6e0 = ( 0 0.0041 0.2551 0.7407 )T, which reveals that, in theseconditions, even when building a whole generation there are still more than 1 in 4 chances of not sampling the wholepopulation at the previous generation. Of course, if m0 = 0, the probability vector for m1 is e0, i.e., m1 = 0, and, moregenerally, mt = 0 for 0 < t < G.Although this example is trivial, it reveals that for any given m0 we can compute a distribution over m1. That is,we can define a new Markov chain to model the iterated coupon collector problem. In this chain a state is exactlythe same as in the coupon-collector chain (i.e., the number of distinct coupons collected), except that now a time stepcorresponds to a complete set of draws from the new coupon set rather than just the draw of one coupon. Since thenumber of states is unchanged, the transition matrix B for this new chain is the same size as A, i.e. (M + 1) × (M + 1).Column i of B is the probability distribution for m0 = i, i.e. Anαie0. That is(cid:15)(cid:15)AMnαe0(cid:15)(cid:15)A2nαe0(cid:15)(cid:15)Anαe0(cid:15)(cid:15). . .(cid:3)e0B =(cid:4).For instance, we have just calculated these for the case M = 3, n = 2 and α = 1 so⎛⎜⎝B =00010 0.3333 0.0370 0.00410 0.6667 0.5185 0.25510.4444 0.740700⎞⎟⎠ .The important thing is that, now that the transition matrix is defined, the chain can be iterated to compute the proba-bility distributions of m2, m3 and so on, as far back as necessary to reach generation 0.In general B is block diagonal of the form(cid:7)(cid:8),1 0T0 CB =where 0 is a column vector containing M zeros and C is a M × M stochastic matrix. Clearly B is not ergodic (fromstate 0 we cannot reach any state other than 0 itself), so we cannot expect a unique limit distribution for mt . However,because B is block diagonal, we haveBx =(cid:7)(cid:8).1 0T0 CxSo, if we ensured that the probability of the chain initially being in state 0 is 0 (that is Pr{m0 = 0} = 0), the chaincould never visit such a state at any future time. Because of this property, and because, objectively, state 0 is totallyuninteresting (of course we already know that if we are interested in no individual at generation G, we do not need toknow any individual at previous generations!) we can declare such a state of the iterated coupon-collection chain asinvalid, and reduce the state set to {1, 2, . . . , M}. In this situation C is the state transition matrix for the chain, and tounderstand the sampling behaviour of tournament selection over multiple generations we just need to concentrate onthe properties of C.The transition matrix C is ergodic if nα > 1 as can be easily seen by the following argument. If nα > 1 then eachold coupon gives us the right to draw more than one new coupon in the iterated coupon-collection problem. So, ifthe state of the chain is k (k < M), it is always possible to reach state k + 1 in one stage of the game with non-zeroprobability. From there it is then, of course, possible to reach state k + 2 and so on up to M. So, from any lowerstate it is always possible to reach any higher state in repeated iterations of the game. But, of course, the converse isalways true: irrespective of the value of nα there is always a chance of getting fewer coupons than we had before in an966R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982iteration of the game, due to resampling. So, from any higher state we can also reach any lower state (in fact, unlikethe reverse, we can achieve this in just one iteration of the game).Since, α (cid:2) 1 and n > 1 for any practical applications, the condition nα > 1 is virtually always satisfied and C isergodic. Then, the Perron–Frobenius theorem guarantees that the probability over the states of the chain convergestowards a limit distribution which is independent from the initial conditions (see [7,8,25,33,36] for other applicationsof this result to EAs). This distribution is given by the (normalised) eigenvector corresponding to the largest eigenvalueof C (λ1 = 1), while the speed at which the chain converges towards such a distribution is determined by the magnitudeof the second largest eigenvalue λ2 (the relaxation time of an ergodic Markov chain is 1/(1 − |λ2|)). Naturally, thisinfinite-time limit behaviour of the chain is particularly important if G is sufficiently big that mt settles into the limitdistribution well before we iterate back to generation 0. Otherwise the transient behaviour is what one needs to focuson. Both are provided by the theory.Because the transition matrices we are talking about are relatively small (the matrix C is M ×M), they are amenableto numerical manipulation. We can, for example, find the eigenvalues and eigenvectors of C for quite respectablepopulation sizes, certainly well in the range of those used in many applications of EAs, thereby determining the limitdistribution and the speed at which this is approached.If p(t) is a probability vector representing the probability distribution over mt , then the expected value of mt isE[mt ] = ( 1 2 . . . M ) · p(t) = ( 12 . . . M ) · Ct p(0).(2)Typically m0 will be fixed by the user, i.e. the probability distribution p(0) will be a delta function centred at this onevalue chosen by the user. Thus p(0) = em0 (where el is a base vector containing all zeros except for element l whichis 1).If p∗ denotes the limit distribution for p(t), then for large enough G, the average number γ of individuals (ingenerations 0 through to G − 1) that have no effect whatsoever on a designated set of m0 individuals of interest at. . . M ) · p∗. This is the average saving that could be achieved bygeneration G is approximately γ = M − ( 1 2not creating and evaluating unnecessary individuals using the EA-EMS and the BC-EA.Notice that the ergodicity of the selection process means that over many generations the fraction of individuals wecan avoid creating does not depend much on m0. That is, for large enough G, whether m0 is one or as large as M willmake little difference to the saving. So we might want to know the entire makeup of generation G and still have asaving of approximately γ G creations and evaluations of individuals.6.3. Approximate model of transient behaviourThe model presented in the previous section is comprehensive and accurate, but for many practical purposes anapproximate but simpler model would be desirable. We develop such a model in this section. In particular we willfocus on modelling the transient behaviour of the number of individuals sampled by tournament selection over multiplegenerations, mt .When the number of individuals we want to know the fitness of at the end of our EA run, m0, is sufficiently smallerthan the population size M, we expect the number of individuals sampled by selection mt to grow exponentially as welook back towards the start of the run. The reasons for this are quite simple: when only a few samples are drawn froma population, resampling is very unlikely, and, so, the ancestors of the individual of interest will tend to form a treefor at least some generations before the last. The branching factor of the tree is nα. So, for small enough t, generationG − t will include (nα)t ancestors of each individual of interest in generation G. Naturally, this exponential growthcontinues only until resampling starts to become significant, i.e., until m0(nα)t becomes comparable with the expectednumber of individuals processed in the limit distribution p∗.For small populations or high selective pressures the transient is short. However, there are cases where the transientlasts for many generations. For example, in a population of M = 100 000 individuals and α = 1 (i.e., in the case of onlymutation and/or two-offspring crossover), the transient lasts for almost 20 generations (although it is exponential onlyfor around 16 or 17). This population size may appear very big and these generation numbers may appear quite small.However, big populations and short runs are actually typical of at least one class of EAs, genetic programming [3,18,19], where populations of several millions of individuals are not uncommon when solving complex importantproblems [17]. So, it is worth evaluating the impact of the transient on the total number of fitness evaluations.R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982967Let us assume G − te is the last generation in which the transient is effectively exponential. We can obtain anapproximation of te by assuming G − te is the generation at which the exponential m0(nα)te hits the population sizelimit M, wherebyte ≈ log(M/m0)log(nα).(3)So, for example, if n = 2 and α = 1, a population of size M and m0 = 1, we have an exponential transient of te ≈log2 M generations.The number of individuals evaluated during the last te generations of a run is the sum of a geometric series, i.e.,F B = m0(nα)te+1 − 1nα − 1.By substituting Eq. (3) into this expression we obtain(cid:8)(cid:7)(cid:7)(cid:7)(cid:8)(cid:8)F B = m01nα − 1nαMm0− 1.(4)This leads us to a simple upper bound for the number of individuals required in the exponential transient: F B <( nαnα−1 )M.Eq. 4 allows us to compute the maximum efficiency gain obtainable. Let us assume we run our BC-EA for tegenerations and we are interested in computing m0 individuals at generation te. To compute the same individuals,a standard EA would need to construct and evaluate of the order ofF F = M × te ≈ M log(M/m0)log(nα)individuals. So, the speedup achievable exploiting the full transient isspeedup = F FF B≈(cid:3)m0M log(M/m0)log(nα)(cid:4)(cid:3)1nα−1nα Mm0(cid:4) ≈ log(M/m0)nαlog(nα)nα−1(cid:3)(cid:4) .− 1√Clearly, maximum speedup can be obtained for m0 = 1 and nα = 2, in which case we have a speedup factor ofM. This can be big for very large populations. However, in practice it appears to be hard toapproximately log2achieve speed-ups of much more than around 10 or so, since the speedup factor is a logarithmic function of thepopulation size.7. BC-EA implementationEncouraged by the theoretical evidence provided in the previous section which indicate that substantial savingscan be achieved, following the ideas in Section 5, we have designed and implemented two backward-chaining EAs inJava. One is a simple GA, which we will refer to as BC-GA; the other is a GP implementation, which we will callBC-GP. The objective was to evaluate whether the BC-EA approach indeed brings significant efficiency gains, andwhether BC-EAs compare well with equivalent standard (forward) versions in terms of ability to solve problems.Algorithm 6 provides a pseudo-code description of the key components of our implementation. (All other com-ponents are exactly as in an ordinary EA and so are omitted for brevity.) The main thing to notice is that we use a“lazy-evaluation” type of approach. We do not create the full graph structure induced by tournament selection. In-stead we statically create the nodes in the graph (and store them using two-dimensional arrays) but graph edges aredynamically generated and stored on the stack as we do recursion. This is achieved by choosing the genetic operatorand invoking the tournament selection procedure only when needed in order to construct an individual, rather than atthe beginning of a run and for all individuals and generations.Also, note that our implementation is rather simplistic, in that it requires the pre-allocation of three (G + 1) × Marrays:968R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982procedure run(G,M)1: Create arrays Known, Population and Fitness2: for all individuals I of interest in generation G do3:4: end for5: return all I of interestevolve_back(I,G)returnif random_float() < crossover_rate thenPopulation[gen][indiv] = random individualprocedure evolve_back(indiv,gen)1: if Known[indiv][gen] then2:3: end if4: if gen = 0 then5:6: else7:8:9:10:11:12:13:14:15: end if16: Fitness[gen][indiv] = fit_func(Population[gen][indiv])17: Known[gen][indiv] = true18: returnparent1 = tournament(gen-1)parent2 = tournament(gen-1)Population[gen][indiv] = crossover(parent1,parent2)parent = tournament(gen-1)Population[gen][indiv] = mutation(parent)end ifelseprocedure tournament(gen)1: fbest = 02: best = undefined3: for tournament_size times do4:5:6:7:8:end if9:10: end for11: return Population[gen][best]candidate = random integer 1. . .Mevolve_back( candidate, gen )if Fitness[gen][candidate] > fbest thenfbest = Fitness[gen][candidate]best = candidateAlgorithm 6. Backward-chaining EA with one-offspring crossover.Population is an array containing the individuals in the population at each generation.8Fitness is an array of single precision floating point numbers. This is used to store the fitness of the individuals inPopulation.Known is an array of bits. This is initialised to 0. A bit set to 1 indicates that the corresponding individual in Popu-lation has been computed and its fitness has been calculated.Using arrays as our main data structures is appropriate given the scientific objectives of the implementation. However itis also wasteful since, in BC-EAs, only those entries of the arrays corresponding to individuals sampled by tournamentselection are used. In a “production” implementation one could use more sophisticated and efficient data structures(such as hash tables) and save some memory.8 If the representation is of fixed size, individuals are directly stored in Population. With variable-size representations, however, this is anarray of pointers to other dynamically-allocated data structures representing the individuals.R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982969if myrand < crossover_rate/2 or sibling_pool[gen] = empty thenreturnPopulation[gen][indiv] = random individualmyrand = random_float()if myrand < crossover_rate thenprocedure evolve_back(indiv,gen)1: if Known[indiv][gen] then2:3: end if4: if gen = 0 then5:6: else7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22: end if23: Fitness[gen][indiv] = fit_func(Population[gen][indiv])24: Known[gen][indiv] = true25: returnparent1 = tournament(gen-1)parent2 = tournament(gen-1)offsprings = crossover(parent1,parent2)Population[gen][indiv] = offspring[1]sibling_pool[gen].add(offspring[2])parent = tournament(gen-1)Population[gen][indiv] = mutation(parent)end ifend ifelseelsePopulation[gen][indiv] = sibling_pool[gen].remove_random_indiv()Algorithm 7. Backward-chaining EA with two-offspring crossover.As we mentioned in Section 3, crossover operators that return two offspring require on average half the number ofselection steps than crossovers returning one offspring. Therefore, one child crossover operators are less efficient in aBC-EA. In order to take full advantage of two-offspring crossovers we need to modify the algorithm slightly. The ma-jor change is to add an expandable array, sibling_pool, which temporarily stores the second offspring generatedby each crossover operation. Other minor changes are required to the evolve_back routine (see Algorithm 7).8. Space and time complexity of BC-EABC-EAs are based on changing the order of various operations in an EA. This requires memorising choices andindividuals over multiple generations. Let us evaluate the space complexity of our BC-EA and compare it to the spacecomplexity of standard EAs.98.1. Fixed-size representationsWe consider EAs where the representation of each individual requires a fixed amount of memory: b bytes. Thespace complexity of a forward generational EA isCF = 2 × (b + 4) × Mwhere we assumed that we store both the current and the new generation, and that fitness values are stored in a vectorof floats (4 byte each). So, for b (cid:8) 1, CF ≈ 2bM. In BC-EA we need to store one array of individuals (each being ofsize b), one of floats, and one bit array, all of size (G + 1) × M. So the space complexity isCB = (G + 1) × M ×(cid:7)(cid:8).b + 4 + 189 Our calculations will ignore the small amount of memory required in the stack during recursion. Also, in the case of BC-EAs with two-offspringcrossover we will ignore the memory required for the expandable array sibling_pool since this typically contains only a few individuals.970R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982For b (cid:8) 1, CB ≈ (G + 1)bM. So, the difference in space complexity between the two algorithms is(cid:6)C = CB − CF ≈ (G − 1) × M × bwhich indicates that in most conditions the use of BC-EA carries a significant memory overhead. However, this doesnot prevent the use of BC-EAs. For example, if the representation of an individual requires b = 100 bytes, and we runa population of M = 1 000 individuals for 100 generations, BC-EA requires only around 10 MB of memory to run.Let us now consider the time complexity of the BC-EA. In fixed-size representations, often the time required byeach fitness evaluation is approximately constant. Since the time spent doing fitness evaluation almost invariablydominates that required by all other phases of an EA, the time complexity of a standard EA is proportional to thenumber of fitness function callsF F = (G + 1)M.Likewise, for the BC-EA we have a time complexity proportional toF B = EB ,where EB is the number of individuals actually created and evaluated during the run. Naturally, with low selectivepressure and large populations F B < F F and so, the BC-EA runs faster than a corresponding EA.8.2. Variable-size representationsWe divide the calculation into two parts:C = Cfixed + Cvariable,where Cfixed represents the amount of memory (in bytes) required to store the data structures necessary to run the EAexcluding the individuals themselves, while Cvariable represents the memory used by the individuals. For variable-sizerepresentations, such as GP trees, this can vary as a function of the random seed used, the generation number andother parameters and details of a run.As far as the fixed complexity is concerned, in a forward generational EA systemCFfixed= 2 × M × (4 + 4) = 16M.As before, the factor of 2 arises since, in our generational approach, we store both the current and the new generation.This requires 2 vectors of pointers (4 byte each) to the population members and two vectors of fitness values (floats,4 byte each), where the vectors are of size M. In BC-GP, instead, we need(cid:7)4 + 4 + 18= (G + 1) × M ×≈ 8(G + 1)MCBfixed(cid:8)since we need to store one array of pointers (4 bytes each), one of floats, and one bit array, all of size (G + 1) × M.Variable space complexity is harder to compute. For a forward variable-size representation EA this isCF≈ 2 × M × SFmax,variablemax is the maximum (over all generations) of the average size of the individuals in each generation of a run.where SFIn a BC-EACBvariable= EB × SBavg,where SBavg is the average size of the individuals during a BC-EA run (i.e., it is the individual size averaged over allindividuals created in a run). (Remember EB is the number of individuals actually created and evaluated during therun.) So, the difference in space complexity between the two algorithms is(cid:3)− 2 × M × SF8(G + 1) − 16(cid:6)C = CB − CF ≈ M(cid:4)+ EB × SBavgmax,which, again, indicates that in most conditions the BC-EA carries a significant memory overhead. However, again, thisdoes not prevent the use of BC-EA. Consider, for example, a BC-GP system with a population of 100 000 individualsrun for 50 generations, where the average program size (throughout a run) is 100 bytes. In the worst possible caseR. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982971(where all programs are constructed and evaluated and, so, EB = (G + 1)M) we need just over 500 MB of memory—which is readily available in most modern personal computers.The memory overhead of BC-EA, (cid:6)C, is a function of the average average-individual-size SBavg and the maximummax. We know that statistically BC-EA and EA behave the same, so we expect SFmaxmax. We cannot, however, say much more about (cid:6)C in general since the size of individuals often= SBmaxaverage-individual-size SFavg < SFand so SBvaries widely.As in fixed-size representations, time complexity is dominated by fitness evaluation. Naturally, the number offitness function calls is the same as in the fixed-size case (see Section 8.1). However, very often, in variable-sizerepresentations, the execution time of the fitness function varies with the size of the individual being evaluated. So,to say something more precise we need to know how size varies and how evaluation time depends on the size of therepresentation. To illustrate how this analysis can be done, in the next section we consider the case of GP.8.2.1. Space and time complexity in BC-GPThe variability in individual size is particularly marked in GP due to a common phenomenon known as bloat.Bloat is a progressive growth in program size not accompanied by a corresponding improvement in fitness [20].This can become very marked in the late phases of a run. If bloat happens in a particular problem, then programsin both standard GP and BC-GP will increase in size. However, since with BC-GP we may choose to evaluate onlya few individuals in the last generations of a run (i.e. m0 (cid:9) M), and since bloat is typically most marked in thesegenerations, SBavg. That is, with bloat the programs created in a BC-GP system may beon average smaller than those created by forward GP. So, we may have SBmax.avgavg can be much smaller than SF(cid:9) SFThese effects partly mitigate the memory overhead, (cid:6)C, of BC-GP. Also because BC-GP tends to evaluates smallerprograms than GP bloat has an interesting impact on run time too. To see this we need to assess the computationalcomplexity T required to run GP and BC-GP. T is effectively dominated by the cost of running the fitness function.The cost of fitness evaluation depends on various factors, but it is typically approximately proportional to the numberof primitives in the program to be evaluated (i.e., executed) and the number of examples in the training set (or fitnesscases in GP’s jargon), K. So, if we express T in number of primitives executed, for standard GP we haveT F = F F × K × SFavg= (G + 1) × M × K × SFavgand for BC-GPT B = F B × K × SBavg= EB × K × SBavg.So, the saving provided by BC-GP is(cid:6)T = T F − T B = K ×(cid:3)(G + 1) × M × SFavg− EB × SBavg(cid:4).That is, for a bloating population the parsimony of BC-GP in terms of fitness evaluations is compounded with itsparsimony in terms of program sizes to produce even more impressive savings.8.3. Wall-clock execution-timeThe number of fitness evaluations is the standard measure for efficiency for evolutionary algorithms. This is rea-sonable since, as already mentioned, for any non-trivial problem, the cost of fitness evaluation is the overwhelmingcomponent in the computation load of most EAs. This is particularly true for GP.When comparing different algorithms it is often assumed that fitness evaluation will require approximately thesame computation in all of the algorithms tested. However, as we observed above, this is not true when evaluationtime depends on particular features (e.g., size) of the structures being evaluated, like in GP. In the case of GP, forexample, we saw that it is more appropriate to use the number of primitives executed as a measure of efficiency.However, in practice there are situations where execution time may be effected by other factors, such as the totalamount of memory used by an algorithm. In particular, if an algorithm uses an amount of memory that exceeds thephysical memory of the computer and/or if there are other programs competing for memory running at the same time,paging and disk access for memory may become important factors in determining the performance of the algorithm.Naturally, these factors are difficult to include in an analysis. However, because BC-EAs use more memory thanthe corresponding forward versions, it is clear there must be settings where paging and disk access will slow down972R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982BC-EAs. This may make these algorithms actually less efficient than corresponding forward EAs, even though, intheory, they should be faster. Fortunately, as we will show in Section 9.4, even in very demanding conditions we neverobserved this happen, despite our using a fairly ordinary PC for our experiments.9. Experimental resultsWe performed experiments with a BC-GA and a BC-GP with both one-offspring and two-offspring crossover,comparing them to standard GA and GP versions.9.1. Test problemsIn the case of the BC-GA we run experiments with the counting ones problem. This is a simple linear problem,widely used for benchmarking purposes, requiring a binary representation, where the fitness of an individual is thenumber of ones in the bit string representing such an individual.In the case of BC-GP we considered a larger variety of problems with complexity ranging from very simple tovery hard. In all problems the objective was to induce a continuous target function from examples. Problems of thistype are known as symbolic regression problems in the GP literature (e.g., see [3,18]), since GP is asked to finda function which fits certain data-points, rather than finding coefficients for a pre-fixed function, as in a standardregression task. The target functions were a univariate quartic polynomial, a multivariate quadratic polynomial anda multivariate cubic polynomial. The quartic polynomial is f (x) = x4 + x2 + x3 + x. For this easy problem weused 20 fitness cases of the form (x, f (x)) obtained by choosing x uniformly at random in the interval [−1, +1].One multivariate polynomial, Poly-4, is f (x1, x2, x3, x4) = x1x2 + x3x4 + x1x4. This is a much harder problem thanthe previous one, but it is still solvable. For this problem 50 fitness cases of the form (x1, x2, x3, x4, f (x1, . . . , x4))were used. They were generated by randomly setting xi ∈ [−1, +1]. The second multivariate polynomial, Poly-10,is f (x1, . . . , x10) = x1x2 + x3x4 + x5x6 + x1x7x9 + x3x6x10. Also for this problem we used 50 fitness cases of theform (x1, . . . , x10, f (x1, . . . , x10)), which, again, were obtained by randomly setting xi ∈ [−1, +1]. This problem isextremely hard.9.2. GA vs. BC-GALet us start by corroborating experimentally the equivalence between GA and BC-GA and the expected fasterconvergence behaviour of BC-EA. To assess this we performed 100 independent runs of both a forward GA and ourBC-GA applied to a 100-bit counting ones problem. In these runs the maximum number of generations G was set to99 (i.e., we did 100 generations). The population size M was 100. Only tournament selection and mutation (mutationrate pm = 0.01) were used (so α = 1). In the BC-GA we computed all the individuals in the last generation (G). Thatis, m0 = M. To make a comparison between the algorithms possible, unless otherwise stated, we computed statisticsevery M fitness evaluations. We treated this interval as a generation even though the fitness evaluations may be spreadover several generations.Fig. 3 shows the fitness vs. generation plots for GA and BC-GA when the tournament size n is 2. It is clear fromthe figure that BC-GA performs about 20% fewer fitness evaluations than the standard EA, reaching, however, thesame average and maximum fitness values. As predicted in the previous sections this significant computational savingcomes without altering in any substantial way the behaviour of the EA.Fig. 4 shows the fitness vs. generation plots for the two algorithms when the tournament size n is 3. With thistournament size, there is still a saving of about 6% which is definitely worth having, but clearly for even higherselection pressures the disadvantages of using a BC-GA in terms of memory use and bookkeeping become quicklypreponderant.An important question about BC-EA is how the expected number of fitness evaluations changes as a function ofM, n, m0 and G. In order to assess the impact of both the transient and the limit-distribution behaviour of BC-EA, weperformed a series of experiments where the task was to evaluate just one individual at generation G (that is m0 = 1).In these experiments, we set G = 49 (i.e., we performed exactly 50 generations, 0 through to 49). This was bigenough that all transients had finished well before generation 0, thereby revealing also the limit-distribution samplingbehaviour. In the experiments we used populations of size M = 10, M = 1 000 and M = 100 000. So, forward runsR. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982973Fig. 3. Comparison between BC-GA and standard GA when a tournament size of 2 is used. Means over 100 independent runs. Note, to make thecomparison possible, in the case of the BC-GA each unit on the abscissa axis (“Generations”) corresponds M = 100 fitness evaluations.Fig. 4. Comparison between BC-GA and standard GA when a tournament size of 3 is used. Means over 100 independent runs.required exactly F F = 500, F F = 50 000 and F F = 5 000 000 fitness evaluations to complete. For each setting wedid 100 independent runs.Fig. 5 shows the average proportion of individuals evaluated by BC-EA in each generation when mutation only isused (α = 1) for tournaments sizes n = 2 and n = 3 as a function of the population size M, while Fig. 6 shows theaverage proportion of individuals evaluated by a BC-EA with one-offspring crossover with pc = 0.5 (α = 1.5) for thesame tournaments sizes.974R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982Fig. 5. Average proportion of individuals evaluated by BC-EA when mutation only is used (α = 1) for tournaments sizes n = 2 and n = 3. Meansover 100 independent runs.Fig. 6. Average proportion of individuals evaluated by BC-EA when one-offspring crossover with pc = 0.5 is used (α = 1.5) for tournaments sizesn = 2 and n = 3. Means over 100 independent runs.From these figures we can see that, as expected, the limit-distribution saving is largely independent from the size ofthe population. E.g., for n = 2, after the transient about 80% of the population is a possible ancestor of the individualof interest in generation G if mutation only is used, while this goes up to 94% when α = 1.5. For EAs where long runsare used, these percentages provide an approximate estimation of the total proportion of fitness evaluations requiredby a backward chaining version of the algorithm w.r.t. the standard algorithm.R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982975Fig. 7. Logarithmic plot of the average proportion of individuals evaluated by BC-EA when mutation only is used (α = 1) for tournaments sizesn = 2–5 and a population size M = 100 000.Table 1Mean number of fitness evaluations recorded during 50 generations in the experiments shown in Figs. 5 and 6 as a percentage of the fitnessevaluations required by a forward EA (reported in the last column for reference)M101 000100 000BC-EA + mutationn = 276.8%64.0%53.4%n = 390.8%81.9%74.1%BC-EA+one-offspring crossovern = 3n = 290.4%81.8%73.8%95.2%89.2%83.2%Forward EA50050 0005 000 000Figs. 5 and 6 also show that during most of the transient the number of individuals sampled by tournament selection,mt , grows very quickly (backward from generation 49). As clearly shown in Fig. 7, the growth is exponential aspredicted in Section 6.3. The rapid growth lasts for te ≈ 18 generations for n = 2, for te ≈ 12 generations for n = 3,for te ≈ 9 generations for n = 4, and for te ≈ 8 generations for n = 5 which confirms the accuracy of the approximationin Eq. (3) (that predicts te values of approximately 17, 10, 8 and 7, respectively).Even when runs last for more than te generations, the effects of the exponential transient are marked. To illustratethis, Table 1 reports the mean total number of fitness evaluations recorded in the experiments shown in Figs. 5 and 6as a percentage of the standard EA fitness evaluations, F F = (G + 1) × M. Taking, for example, the case of n = 2and no mutation, where the limit distribution effort would be around 80%, we can see that efforts of as low as 53.4%of those required by a forward EA are achieved.109.3. GP vs. BC-GPThe function set for GP included the functions +, −, × and the “protected” division DIV where DIV(x, y) = x/yunless |y| <= 0.001, in which case DIV(x, y) = x to avoid run-time errors. The terminal set included the indepen-dent variables in the problem (x for Quartic, x1, x2, x3, x4 for Poly-4 and x1, x2, . . . , x10 for Poly-10). Fitness wascalculated as the negation of the sum of the absolute errors between the output produced by a program and the desired10 The similarity between the numbers in the third and fourth columns of the table are not a mistake: both mutation with tournament size n = 3and one-offspring crossover with tournament size n = 2 and pc = 0.5 require the same average number of selection steps, 3M.976R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982Fig. 8. Quartic polynomial regression problem. Normal GP contrasted with chance of success with BC-GP (population size 100, average over 1 000runs).output on each of the fitness cases. A problem was considered to be solved if a program with an error of less than10−5 summed across all fitness cases was found. We used binary tournaments (n = 2) for parent selection. The initialpopulation was created using the “grow” method [18] with maximum depth of 6 levels (the root node being at level 0).We used 80% two-offspring sub-tree crossover (with uniform random selection of crossover points) and 20% pointmutation with a 2% chance of mutation per tree node. The population size M was 100, 1 000, 10 000 and 100 000.For the purpose of comparing the problem solving ability of GP and BC-GP, we gave both algorithms the samenumber of fitness evaluations. The maximum number of fitness evaluations was 30M, which for standard GP corre-sponds to 30 generations. For different experiments, depending on statistical requirements, we performed 100, 1 000or even 5 000 independent runs of both backward and forward GP. In the BC-GP we computed 80% of the finalgeneration (i.e. m0 = 0.8M) since this is approximately the steady state value of mt for n = 2 and t → ∞.Figs. 8 and 9 compare the success probabilities of BC-GP and GP for the quartic polynomial for population sizes100 and 1 000. The error bars indicate standard error (based on the binomial distribution). As expected BC-GP doesbetter and the difference is statistically significant except for the final generations. With a population of 1 000 (Fig. 9)or bigger (data not reported), BC-GP is also always statistically better than or equal to standard GP. Naturally, with bigpopulations both forward and backward GP almost always solve the quartic polynomial. Nevertheless BC-GP reaches100% faster.The four-variate polynomial, Poly-4, is much harder than Quartic and requires large populations to be solvable inmost runs. Fig. 10 shows the fraction of successful runs with a population of 1 000. Fig. 11 plots similar data but fora population of 10 000. The difference between BC-GP and forward GP is statistically significant for all populationsizes used.Poly-10 is very hard. We tried 1 000 runs with populations of 100, 1 000 and 10 000, and 100 runs with 100 000individuals. Neither standard GP nor BC-GP found a solution in any of their runs. As illustrated in Fig. 12 for the caseM = 10 000, BC-GP on average finds better programs for the same number of fitness evaluations.In Figs. 8–12 we have compared forward GP and BC-GP when both algorithms are given the same number offitness evaluations. Instead, in Table 2, we show a comparison when they are run for the same number of generations(G = 30). Like for the BC-GA, thanks to the savings obtained by avoiding the creation and evaluation of individualsnot sampled by selection (and their unnecessary ancestors), by the end of the runs, BC-GP evolved solutions ofsimilar fitness (which again confirms the statistical equivalence of EAs and BC-EAs), but took around 20% fewerfitness evaluations. Similar savings are obtained at all population sizes.R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982977Fig. 9. Quartic polynomial regression problem. As Fig. 8 but with population of 1 000.Fig. 10. Fraction of successful runs (out of 5 000 runs) on the Poly-4 problem for forward GP and BC-GP (30 generations) with populations of1 000.All the tests reported in this section have been performed also for the case of tournament size n = 3. We don’treport on these for brevity. In all cases BC-GP was superior to GP, but, naturally, by a smaller margin.9.4. Wall-clock execution-time comparisonTo evaluate whether paging and disk access for memory had an impact in our experiments, we considered the mostdemanding of our representations—variable-size GP trees—and ran a series of experiments measuring wall-clock978R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982Fig. 11. Fraction of successful runs (out of 1 000 runs) on the Poly-4 problem for forward GP and BC-GP with populations of 10 000.Fig. 12. Error summed over 50 test cases for Poly-10 regression problem (means of 1 000 runs, with populations of 10 000).Table 2Normal GP v. Backward chaining on Quartic, Poly 4 and Poly 10. Population 10 000. Generations 30. Means of 1 000 runsProblemQuarticPoly-4Poly-10ForwardBest Fit0.000.1211.12Evals300 000300 000300 000Succ Prob100.0%96.3%0.0%BackwardBest Fit0.000.1611.29Evals240 321240 315240 299Succ Prob100.0%96.0%0.0%Saving19.9%19.9%19.9%R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982979Table 3Wall-clock per-primitive execution-time comparison between GP and BC-GPGPBC-GPPopulation size1000.782 µs0.819 µs1 0000.596 µs0.598 µs10 0000.592 µs0.591 µs100 0000.599 µs0.595 µsexecution times for both forward and backward GP and different population sizes applied to the Quartic polynomialproblem. Each run lasted for 50 generations. All other GP parameters were as in Section 9.3. The results are shownin Table 3. Results are the average execution time per primitive (in microseconds). Averages were computed over 10independent runs by dividing the total execution time of the runs by the number of primitives executed in the runs.For a fairer comparison we report execution time per primitive instead of total execution time since BC-GP runs usedaround 20% fewer fitness evaluations that GP runs. Runs were performed on a 3 GHz Linux PC with 2 GB of memory.As one can clearly see in Table 3, even with very large populations, there were no significant differences in per-primitive execution times between GP and BC-GP. Also, with the exception of populations of 100 individuals, therewere no significant differences in execution times as the population size was varied. The higher execution time in pop-ulations of 100 individuals is an artifact due to the code for collecting statistics requiring a non-negligible proportionof the computation time at such small population sizes.10. DiscussionIn this paper we have focused on a source of inefficiency in the sampling behaviour of tournament selection: thecreation and evaluation of individuals that cannot influence future generations. We have proposed general methodsto remove this source of inefficiency and speed up EAs based on tournament selection. One of these methods, thebackward chaining EA, provides the additional benefit of converging faster than a standard (forward) algorithm due toits constructing and evaluating individuals belonging to later generations sooner. We have analysed these algorithmsboth theoretically (Section 6) and experimentally (Section 9), strongly corroborating the feasibility of this approach.The implementation of a backward chaining EA is not very complex and the added book keeping required isquite limited. However, there is no doubt that BC-EAs require more memory than their forward counterparts. If one,however, is prepared to accept this overhead and adopt the ideas behind BC-EA, the computational savings can bevery big. These are achievable not only when we exploit the transient behaviour of the algorithm, but also in thelimit-distribution behaviour, as will be illustrated below.Maximum savings are achieved when nα is minimum. The smallest value α can take is 1 and with standardtournament selection the minimum for n is 2. So, we already know that the best we can do is saving around 20%fitness evaluations. However, a form of tournament selection exists (e.g., see [12,22]) that we can modify to obtaineven more spectacular savings.In this form of tournament selection, one picks up two individuals at random and then chooses the one with thehigher fitness with probability p, the other with probability 1 − p. For p = 1 this form of selection is equivalentto standard tournament selection with n = 2, while it is a form of random selection for p = 0.5. By acting on pit is possible to vary the selection pressure of the method continuously between these two extremes. An alternativedescription of the method is that we choose the higher fitness individual with probability q and randomly between thetwo with probability 1 − q (naturally p = q + (1 − q)/2 = (1 + q)/2). In this case q can be varied in the interval[0, 1].This second version of the algorithm can be modified for our purposes. Instead of first choosing a pair of individualsand then deciding whether we select the best or we pick one at random, we, equivalently, first decide which selectionstrategy we are going to use, and then, based on this, we randomly draw individuals from the population. If we decideto go for the best in the tournament, then we must draw two individuals from the population. However, if we decideto choose randomly between the two members of the tournament, then we can just draw one random individual fromthe population (instead of drawing two individuals and then randomly discarding one).With this method, the expected number of individuals drawn in each tournament is n = 2 × q + 1 × (1 − q) =q + 1 (cid:3) 2. So, clearly the smaller q the bigger the saving we should expect in a BC-EA. Just to get a feel for the980R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982order of magnitude of these savings, let us assume α = 1 and let us use Eq. (1) (Section 6.1) to estimate the expectedproportion of individuals not sampled. This is approximately e−(1+q). So, for very low selection pressures saving ofover 35% fitness evaluations are possible.Naturally, much more substantial savings can be obtained when exploiting the transient behaviour of BC-EAs. InSection 6.3 we showed that when running a BC-EA with m0 = 1, we can make our EA run up to 10 times faster.However, the reader will probably wonder about the usefulness of evaluating just one individual in the last generation.Normally we would want to have the whole generation. However, we need to remember that the individual provided bya BC-EA (with m0 = 1) at generation G is effectively a random sample drawn from the population at that generation.Although we expect one individual to be insufficient, one important question is whether we really need to have thewhole of generation G in order to solve a problem, particularly considering that in many EAs there is substantial loss ofgenotypic diversity in the population in the late phases of a run. In [27] we have experimented with an implementationof BC-GP with one-offspring crossover showing that even when run with m0 = 1 BC-GP can solve problems. So,in at least some cases, we do not need the whole population. To get a more complete and satisfactory answer, futurework on BC-EAs will need to include a thorough investigation of the best way to choose m0 and G.11. ConclusionsIn this paper, we have analysed the sampling behaviour of tournament selection over multiple generations and usedthis analysis to come up with and demonstrate more efficient implementations of evolutionary algorithms (EAs) thatare much more rooted in classical AI than any other previous class of EAs. In particular we have proposed a newway of running EAs, the backward chaining-EA (BC-EA), which offers a combination of fast convergence, increasedefficiency in terms of fitness evaluations, complete statistical equivalence to a standard EA and broad applicability.Because of these interesting properties we think the class of BC-EAs is an area worthy of further investigation.To reiterate, the BC-EA algorithm is not hard to implement, as we have discussed in Section 7. Also, BC-EA tendsto find better individuals faster irrespective of the tournament sizes. However, if one wants to use tournaments withmore than three individuals and to compute a large proportion of the final generation, the computational saving pro-vided by BC-EA may be too limited to be worth the implementation effort and the memory overhead. In applicationswhich require computing only a small number of individuals in a given generation of interest and where a very largepopulation is used, then BC-EA can be fruitfully applied even for large tournament size. For example, with BC-EA,tournament size 7, and a population of a million individuals—which is not unusual in some EAs such as GP—onecould calculate 1 individual at generation 7, 7 individuals at generation 6, 49 individuals at generation 5, etc. at a costinferior to that required to initialise the population in a forward EA. The information gained in this way about futuregenerations could prove very important, for example, in deciding whether to continue a run or not. This informationis certainly not available in a traditional EA.In future research we intend to test the new algorithm on real-world problems, and explore possible ways of furtherimproving the allocation of trials and decision making in BC-EAs, for example, by replacing our current depth-first-search strategy with an informed search algorithm, such as, perhaps, A∗. Applying a backward chaining approach toother forms of local selection, beyond tournament selection, is another promising area for future research.AcknowledgementsWe would like to thank Chris Stephens, Darrell Whitley, Kumara Sastry and Bob McKay for their useful com-ments. The reviewers and the co-editor-in-chief in charge of this manuscript are also warmly thanked for their help inimproving it.References[1] T. Baeck, D.B. Fogel, Z. Michalewicz (Eds.), Oxford Univ. Press, Oxford, 1997.[2] T. Bäck, D.B. Fogel, T. Michalewicz (Eds.), Evolutionary Computation 1: Basic Algorithms and Operators, Institute of Physics Publishing,2000.[3] W. Banzhaf, P. Nordin, R.E. Keller, F.D. Francone, Genetic Programming—An Introduction; On the Automatic Evolution of ComputerPrograms and its Applications, Morgan Kaufmann, January 1998, dpunkt.verlag.R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982981[4] T. Blickle, L. Thiele, A mathematical analysis of tournament selection, in: L.J. Eshelman (Ed.), Proceedings of the Sixth International Con-ference on Genetic Algorithms (ICGA’95), San Francisco, CA, Morgan Kaufmann, 1995, pp. 9–16.[5] T. Blickle, L. Thiele, A comparison of selection schemes used in evolutionary algorithms, Evolutionary Computation 4 (4) (1997) 361–394.[6] L. Davis (Ed.), Handbook of Genetic Algorithms, Van Nostrand Reinhold, New York, 1991.[7] T.E. Davis, J.C. Principe, A Markov chain framework for the simple genetic algorithm, Evolutionary Computation 1 (3) (1993) 269–288.[8] K.A. De Jong, W.M. Spears, D.F. Gordon, Using Markov chains to analyze GAFOs, in: L.D. Whitley, M.D. Vose (Eds.), Proceedings of theThird Workshop on Foundations of Genetic Algorithms, San Francisco, CA, July 31–August 2 1995, Morgan Kaufmann, 1995, pp. 115–138.[9] W. Feller, An Introduction to Probability Theory and Its Applications, vol. 2, John Wiley, 1971.[10] D.B. Fogel (Ed.), Evolutionary Computation. The Fossil Record. Selected Readings on the History of Evolutionary Computation, IEEE Press,1998.[11] L.J. Fogel, A.J. Owens, M.J. Walsh, Artificial Intelligence through Simulated Evolution, Wiley, New York, 1966.[12] D.E. Goldberg, K. Deb, A comparative analysis of selection schemes used in genetic algorithms, in: G.J.E. Rawlins (Ed.), Foundations ofGenetic Algorithms, Morgan Kaufmann, 1991.[13] D.E. Goldberg, Genetic Algorithms in Search, Optimization, and Machine Learning, Addison-Wesley, Reading MA, 1989.[14] J. Holland, Adaptation in Natural and Artificial Systems, University of Michigan Press, Ann Arbor, MI, 1975.[15] J. He, X. Yao, Drift analysis and average time complexity of evolutionary algorithms, Artificial Intelligence 127 (1) (2001) 57–85.[16] J. He, X. Yao, Towards an analytic framework for analysing the computation time of evolutionary algorithms, Artificial Intelligence 145 (1–2)(2003) 59–97.[17] J.R. Koza, M.A. Keane, M.J. Streeter, W. Mydlowec, J. Yu, G. Lanza, Genetic Programming IV: Routine Human-Competitive MachineIntelligence, Kluwer Academic, 2003.[18] J.R. Koza, A genetic approach to the truck backer upper problem and the inter-twined spiral problem, in: Proceedings of IJCNN InternationalJoint Conference on Neural Networks, vol. IV, IEEE Press, 1992, pp. 310–318.[19] W.B. Langdon, R. Poli, Foundations of Genetic Programming, Springer-Verlag, Berlin, 2002.[20] W.B. Langdon, T. Soule, R. Poli, J.A. Foster, The evolution of size and shape, in: L. Spector, W.B. Langdon, U.-M. O’Reilly, P.J. Angeline(Eds.), Advances in Genetic Programming 3, MIT Press, Cambridge, MA, June 1999, pp. 163–190. Chapter 8.[21] Z. Michalewicz, Genetic Algorithms + Data Structures = Evolution Programs, second ed., Springer-Verlag, Berlin, 1994.[22] M. Mitchell, An Introduction to Genetic Algorithms, MIT Press, Cambridge, MA, 1996.[23] T. Motoki, Calculating the expected loss of diversity of selection schemes, Evolutionary Computation 10 (4) (2002) 397–422.[24] N.F. McPhee, R. Poli, J.E. Rowe, A schema theory analysis of mutation size biases in genetic programming with linear representations, in:Proceedings of the 2001 Congress on Evolutionary Computation CEC2001, COEX, World Trade Center, 159 Samseong-dong, Gangnam-gu,Seoul, Korea, 27–30 May 2001, IEEE Press, 2001, pp. 1078–1085.[25] A.E. Nix, M.D. Vose, Modeling genetic algorithms with Markov chains, Annals of Mathematics and Artificial Intelligence 5 (1992) 79–88.[26] R. Poli, B. Logan, The evolutionary computation cookbook: Recipes for designing new algorithms, in: Proceedings of the Second OnlineWorkshop on Evolutionary Computation, Nagoya, Japan, March 1996.[27] R. Poli, W.B. Langdon, Backward-chaining genetic programming, in: H.-G. Beyer, U.-M. O’Reilly, D.V. Arnold, W. Banzhaf, C. Blum, E.W.Bonabeau, E. Cantu-Paz, D. Dasgupta, K. Deb, J.A. Foster, E.D. de Jong, H. Lipson, X. Llora, S. Mancoridis, M. Pelikan, G.R. Raidl, T. Soule,A.M. Tyrrell, J.-P. Watson, E. Zitzler (Eds.), GECCO 2005: Proceedings of the 2005 Conference on Genetic and Evolutionary Computation,vol. 2, Washington, DC, 25–29 June 2005, ACM Press, 2005, pp. 1777–1778.[28] R. Poli, N.F. McPhee, General schema theory for genetic programming with subtree-swapping crossover: Part I, Evolutionary Computa-tion 11 (1) (March 2003) 53–66.[29] R. Poli, N.F. McPhee, General schema theory for genetic programming with subtree-swapping crossover: Part II, Evolutionary Computa-tion 11 (2) (June 2003) 169–206.[30] R. Poli, N.F. McPhee, J.E. Rowe, Exact schema theory and Markov chain models for genetic programming and variable-length geneticalgorithms with homologous crossover, Genetic Programming and Evolvable Machines 5 (1) (March 2004) 31–70.[31] R. Poli, Exact schema theory for genetic programming and variable-length genetic algorithms with one-point crossover, Genetic Programmingand Evolvable Machines 2 (2) (June 2001) 123–163.[32] R. Poli, Tournament selection, iterated coupon-collection problem, and backward-chaining evolutionary algorithms, in: Proceedings of theFoundations of Genetic Algorithms Workshop (FOGA 8), 4th January 2005.[33] R. Poli, J.E. Rowe, N.F. McPhee, Markov chain models for GP and variable-length GAs with homologous crossover, in: Proceedings of theGenetic and Evolutionary Computation Conference (GECCO-2001), San Francisco, CA, 7–11 July 2001, Morgan Kaufmann, 2001.[34] I. Rechenberg, Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution, Frommann–Holzboog,Stuttgart, 1973.[35] S.J. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, second ed., Prentice Hall, Englewood Cliffs, NJ, 2003.[36] G. Rudolph, Convergence analysis of canonical genetic algorithm, IEEE Transactions on Neural Networks 5 (1) (1994) 96–101.[37] G. Rudolph, Genetic algorithms, in: T. Baeck, D.B. Fogel, Z. Michalewicz (Eds.), Handbook of Evolutionary Computation, Oxford UniversityPress, Oxford, 1997, pp. B2.4-20–27.[38] G. Rudolph, Models of stochastic convergence, in: T. Baeck, D.B. Fogel, Z. Michalewicz (Eds.), Handbook of Evolutionary Computation,Oxford University Press, Oxford, 1997, pp. B2.3-1–3.[39] G. Rudolph, Stochastic processes, in: T. Baeck, D.B. Fogel, Z. Michalewicz (Eds.), Handbook of Evolutionary Computation, Oxford Univer-sity Press, Oxford, 1997, pp. B2.2-1–8.[40] H.-P. Schwefel, Numerical Optimization of Computer Models, Wiley, Chichester, 1981.[41] K. Sastry, D.E. Goldberg, Modeling tournament selection with replacement using apparent added noise, in: Proceedings of ANNIE 2001,vol. 11, 2001, pp. 129–134.982R. Poli, W.B. Langdon / Artificial Intelligence 170 (2006) 953–982[42] C.R. Stephens, Some exact results from a coarse grained formulation of genetic dynamics, in: L. Spector, E.D. Goodman, A. Wu, W.B.Langdon, H.-M. Voigt, M. Gen, S. Sen, M. Dorigo, S. Pezeshk, M.H. Garzon, E. Burke (Eds.), Proceedings of the Genetic and EvolutionaryComputation Conference (GECCO-2001), San Francisco, CA, 7–11 July 2001, Morgan Kaufmann, 2001, pp. 631–638.[43] C.R. Stephens, H. Waelbroeck, Effective degrees of freedom in genetic algorithms and the block hypothesis, in: T. Bäck (Ed.), Proceedings ofthe Seventh International Conference on Genetic Algorithms (ICGA97), East Lansing, Morgan Kaufmann, 1997, pp. 34–40.[44] C.R. Stephens, H. Waelbroeck, Schemata evolution and building blocks, Evolutionary Computation 7 (2) (1999) 109–124.[45] A. Sokolov, D. Whitley, Unbiased tournament selection, in: H.-G. Beyer, U.-M. O’Reilly, D.V. Arnold, W. Banzhaf, C. Blum, E.W. Bonabeau,E. Cantu-Paz, D. Dasgupta, K. Deb, J.A. Foster, E.D. de Jong, H. Lipson, X. Llora, S. Mancoridis, M. Pelikan, G.R. Raidl, T. Soule, A.M.Tyrrell, J.-P. Watson, E. Zitzler (Eds.), GECCO 2005 Proceedings of the 2005 Conference on Genetic and Evolutionary Computation, vol. 2,Washington, DC, 25–29 June, ACM Press, 2005, pp. 1131–1138.[46] A. Teller, D. Andre, Automatically choosing the number of fitness cases: The rational allocation of trials, in: J.R. Koza, K. Deb, M. Dorigo,D.B. Fogel, M. Garzon, H. Iba, R.L. Riolo (Eds.), Genetic Programming 1997: Proceedings of the Second Annual Conference, StanfordUniversity, CA, 13–16 July 1997, Morgan Kaufmann, 1997, pp. 321–328.[47] M.D. Vose, The Simple Genetic Algorithm: Foundations and Theory, MIT Press, Cambridge, MA, 1999.[48] P.H. Winston, Artificial Intelligence, third ed., Addison-Wesley, Reading, MA, 1992.