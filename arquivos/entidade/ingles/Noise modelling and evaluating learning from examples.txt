Artificial Intelligence 82 (1996) 157-179 Artificial Intelligence Noise modelling and evaluating learning from examples Ray J. Hickey* Faculty of Informatics, University of Ulster at Coleraine, Ulster, Co. Londonderry BT52 lSA, N. Ireland, UK Received July 1993; revised July 1994 Abstract The means of evaluating, using artificial data, algorithms, such as ID3, which learn to as the method of artificial universes. from examples is enhanced and referred is treated as a dependent variable with description attributes concepts The central notions are that of a class model and its associated representations class attribute the independent using irrelevant attribute a small universe which is then altered data generated noise has a detrimental is discussed and modelled The notion of an the construction of to increase noise. Learning curves for ID3 used on from trials. These show that increasing from these universes are estimated effect on learning. variables. The nature of noise in the model is also considered. The ideas are illustrated in which a as that of majorisation. information-theoretic ideas especially functioning through 1. Introduction Supervised learning of concepts major interest. Amongst are: induction of decision [S]); instance-based Niblett artificial neural networks Goldberg and Holland Iba and Thompson the many approaches from classified examples remains a problem of that have been taken to this task rules (Clark and [9]); (see Booker, [6] and Langley, (see Aha [2] and Cost and Salzberg [26]); genetic classifiers [24]), or high-level trees (Quinlan learning (Rumelhart [4]); B y a esian classifiers (see Cheeseman [17]). the examples presented Typically to the algorithm are representative in some sense of a set of possible examples and often constitute a very small subset. Each language (often example consists of a description in an appropriate just a simple attribute-value the assigned class or concept offered by the teacher. The learning task is therefore one of induction of a general concept description from the particular cases provided. together with representation formalism) * Telephone: +44-(0)1265-44141. E-mail: rj.hickey@ulst.ac.uk 0004-3702/96/$15.00 SSDZ 0004-3702(94)00094-S 0 1996 Elsevier Science B.V. All rights reserved 158 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 In real-world settings the task is complicated by the presence of noise of various forms such as errors attribute values or in classification by the teacher. This not only makes the work of the learning algorithm more difficult but also complicates Performance of an algorithm or comparison of several algorithms the evaluation of its performance. in recording is usually assessed by one of the following means: (1) Empirical analysis using real data. From a database of examples (such as one of the standard (Murphy learning. Further classification performance and Aha sets kept in the “Machine Learning Repository” [21])) subsets are drawn at random and used for subsets are then drawn and used for evaluation of the of the learned description. (2) Empirical analysis using artificial data. In order the effects of to simulate to a given prescription and then aspects noise, data is generated according of the example description or the class are altered using a mechanism involving known probabilities. Irrelevant attributes may be introduced. (3) Average-case analysis. Examples to a known probabilistic prescription. The expected value behaviour of the learning algorithm is then derived. are generated according (4) PAC analysis. The probably correct approximately of basis for assessing performance. A by an algorithm [31] provides a theoretical is said to be PAC-learnable Valiant if with probability concept 1 - 6 the learned concept description has a probability 1 - E of classifying the analysis is probabilistic, almost correctly on subsequent no underlying distribution is assumed and it is therefore “worst case” over all possible distributions. trials. Although for examples (PAC) theory (2) provides greater opportunity [16] argue that method than method the most well-known artificial data sets containing In the first three methods a learning curve showing performance (typically classification accuracy) against number of training examples can be derived. For methods (1) and (2) this curve is estimated from data over many trials. Kibler and for systematic Langley investigation (1) particularly with regard to the controlled adminis- tration of noise; in method (1) the naturally occurring noise cannot be quantified satisfactorily. Amongst an element of noise is the LED domain of Breiman et al. [5] where components of an LED display for digits are inverted with a small probability. Aha [l] has introduced a variation of the empirical approach by taking a database and altering it in a random (called a case) which retains In method from the essential characteristics of the original. (3) the theoretical the underlying derived Pazzani and Sarrett [22] or Langley et al. [17]. Unfortunately feasible only for simple algorithms. Analysis of more sophisticated such as ID3 appear learning curve can, mathematics permitting, be see this approach seems algorithms [26]) would setup (into which noise may be introduced); to produce a variant for experimentation [24,25]) or backpropagation (Rumelhart (Quinlan fashion to be too difficult. (l)-(3), Unlike methods performance of an algorithm the PAC approach offers very little insight into the and is overly pessimistic in typical circumstances R.J. Hickey I Artijicial Intelligence 82 (1996) 157-l 79 1.59 about its capabilities. Pazzani and Sarrett [22] show learning curves derived from the average case and PAC approaches. For noise modelling in the PAC approach [27]. see Valiant [32], Angluin and Laird [3] or Sakakibara 1.1. Modelling noise and extending the use of artificial data: the method of artificial universes Although, as indicated above, the introduction of noise in a systematic way can the modelling has often (2), (3) and (4), in practice in methods be undertaken been ad hoc and lacking in any underlying theory. The resulting artificial domains are then not sufficiently realistic with regard to complexity of noise. Also it may not be clear how much noise overall has been introduced. The purpose of this work (2) by introducing to measure and modify with ease the amount of noise in an artificial system. Amongst the benefits of this approach are: is to enhance theory of noise which makes the capabilities of method it possible a unified (1) a simple means of producing data sets which possess the required amount tasks for a wide range of algo- of noise and which provide challenging rithms; (2) greater clarity concerning the relative importance of different sources of noise; (3) greater insight into the nature of noise and information and how these may be explicated. The approach involves specifying a complete probabilistic model for attributes used in the example description and the class. This will be referred an artificial universe. The class model of the universe will declare between descriptions and class distributions-as The class model can be represented table to a comparatively relationships distinct from individual classes. forms from an exhaustive small set of very general rules. An artificial universe can be used to generate examples for use by any of the types of learning algorithm mentioned above and, after learning has taken place, can provide a true indication of the performance of a learned concept description. Since approximated from a series of trials involving generated examples. is a random variable in many different value behaviour its expected can be latter the the to as Noise will be modelled using the class distributions amount of noise will be manipulated principally using the majorisation which is concerned with the relative degree of inequality amongst elements in this case a vector of probabilities constituting a distribution. real vector, in the class model. The relation in a Although what (2) above is merely an extension or elaboration is proposed here it will be referred of method to as the method of artificial universes. Specifying a complete probability model for the generation of data is hardly a new idea. Many of the artificial domains in the literature such as the LED domain [5] generators offering a limited noise modelling are such models. General-purpose see, for example Lounis and Bisson [18]. In capability have also been developed; a simple yet general means of essence the contribution is to provide here 160 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 prescribing a particular amount of noise and in a way which is largely independent of its physical source. The ideas in this paper were outlined by Hickey method was illustrated sequent experimentation trates on the underlying [14] is performed but with a much larger number of trials. the construction using ID3 on generated theory although a similar set of experiments through [14] where the use of the of a small universe and sub- examples. The paper concen- to those in 1.2. Plan of the paper The definition of an artificial universe and its class model are given in Section 2 of the class model is also in Section 3. Two and a running example is introduced. The representation discussed. The modelling of different types of irrelevant attribute role of majorisation (pure noise and redundant) in explicating noise and its relationship types of noise is addressed are defined. The to other in Section 4. This is applied to produce ideas are reviewed information-theoretic information statistics for a universe. In Section 5 the means of assessing the performance of a deterministic classifier, results obtained from using is discussed. Experimental learning, through from universes with varying degrees of noise are reported. acquired ID3 on data generated 2. Artificial universes (referred set together with An object or situation to be classified is described using attributes to attributes) which will be labelled a, b, c, . . . . as the description or condition (usually finite) or continuous. The description These attributes may be discrete the attribute description schema. A vector of values (a,, b,, . . .) where there is one value from is called a description vector. More generally a each attribute partially is called a condition or complex. The instantiated to an object or situation will be labelled classl, classes that may be assigned class2, . . . and will be regarded as values of an attribute called class (the class attribute). that attributes may description vector in the schema the values is called take An artificial universe together with the notion of a class model and its representation are then defined as follows: Definition 2.1. An (artificial) universe consists of a description attribute and the joint distribution of the class and description attributes. schema, a class Definition 2.2. The function which maps a description vector to the distribution of the class attribute the class model of the universe. conditional on that vector is called R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 161 Definition 2.3. Any statement specifying the class model is said to be a representu- tion of the class model. The class model is analogous to a statistical model, such as is used in regression schema the independent variables. Likewise the class distributions play the role to note that it is that are for is the correct one. This in the next section when the physical sources of analysis: class is treated as the dependent variable with the description providing that error distributions play in a statistical model. It is important the observed schema vectors and class and their probabilistic being modelled. The artificial universe makes no statement about whether, example, point will be further elaborated noise are discussed. A representation the class associated with a particular description is usually a set of rules of the form: relationship if (complex) then (class distribution) where the complexes and model completely define the universe. joint distribution form a partition of the set of description vectors. The class to the description are sufficient attributes of To illustrate these ideas a small universe, called universe 1, will be built. This is a variation on the universe defined in [14]: it has the same description schema and the number of classes has joint distribution of the description attributes however been increased from three to eight. The schema involving description attributes a, b, c, d, and e is: a: b: c: d: e: al, a2 b,, b,, b,, b, Cl, c27 C3 d,, d,, d, e1, e2 class : 1,2,3,4,5,6,7, g The rule set specifying do not appear. The joint distribution of the description attributes Table 2. Here dependent obtained as: the class model is shown in Table 1. Note that d and e in d is is then on a and c. The probability (a, b), c and e are defined of any description occurring to be mutually independent; is provided P(a = vi, b = u2, c = u3, d = u4, e = u5) =P(u=u,,b=u,)~P(c=u,)~P(d=u,~a=u,,c=u,)~P(e=u,). (1) right-hand The description vector. side of (1) is an example of a generating expression for a The rule set in Table 1 offers a fairly compressed general complexes. Clearly discrete it is possible if all attributes in the description to tabulate the mapping between representation involving schema are finite individual description 162 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 Table 1 A representation of the class model for universe I Rule number 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1.5 16 Description Class distribution and b=b, andc=c, and c=c) and b=b, a=a, a = a2 and b = b, and c = c1 a = a2 and b = b, and c = c2 a=az b = b, and c = c, b = b, and c = cz b=b, a = a, and b = b, a = a2 and b = b, and c = c, a = az and b = b, and c = c2 a = a, and b = b, and c = c3 b = b, and c = c, a=a, a = a, and b = b, and c = c2 a = a, and b = b, and c = c) a = a, and b = b, and c = c, and c=cz and b=b, (0.5,0.5,0,0,0,0,0,0) (0.55,0,0,0.45,0,0,0,0) (0,0,0.6,0,0,0,0,0.4) (O,O, O,O, O,O, 0.4,0.6) (O,O, 0,0.3,0,0,0.7,0) (O,O, o,o, 0,0.55,0.45,0) (O,O, 0.4,0,0.6,0,0,0) (0.3,0.7,0,0,0,0,0,0) (O,O, 0,0.6,0,0.4,0,0) (O,O, 0.6,0,0,0.4,0,0) (O,O, O,O,O, l,O, 0) (O,O, 0,0.35,0,0.65,0,0) (O,O, o,o, o.s,o, 0,0.5) (0.4,0,0,0.6,0,0,0,0) (0,0.6,0,0,0,0.4,0,0) (O,O, o,o, 0,0.5,0.5,0) vectors and their class distributions; representation. this will be referred to as the enumerated One of the motivations of of the complications dependent on one another to artificial data modelling mutual for this work is to simulate in an artificial setting some the attributes in some way is one aspect of this. In other approaches real world. Making the description independence is often assumed. for the joint distribution of the description attributes in universe 1; (a, b), c and e are Table 2 Probabilities mutually independent; a d is dependent on (a, c) b b, 0.05 0.15 C2 0.05 a, a2 C Cl 0.6 Conditional values ofaandc a, and (c, or c,) a, and c1 a2 and c, a2 and c2 a, and c. b, 0.02 0.03 C3 0.35 d 0.4 0.2 0.1 0.25 0.3 b, 0.4 0.25 e e, 0.3 0.2 0.5 0.2 0.7 0.3 b, 0 0.1 e2 0.7 0.4 0.3 0.7 0.05 0.4 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 163 Once a universe is specified as above, examples can be generated from it as follows: (1) Generate a description vector from the joint distribution of the description attributes using the generating expression. (2) Look up the generated vector in the class model to find the class distribution. a class from the class distribution. (3) Generate (4) The description vector and the class thus obtained constitute It may appear the example. that the use of a rule set to define the class model is somehow learning algorithms which induce high-level rules. This is not the It is just a convenient way to produce a specification for a universe. rather the representation, biased towards case however. Moreover the choice of one representation use of general rules versus the enumerated for the generation of examples. for example is of no consequence than another, The marginal, i.e. unconditional, distribution of class (referred to as the default from the universe specification. For universe 1 this distribution) is: can be obtained (0.1215,0.1510,0.0400,0.2115,0.0445,0.2675,0.1270,0.0370) . (2) The majority class, i.e. most probable, default class. Here in the default distribution it is class 6 with a 26.75% chance of occurring. is called the 2.1. Sub-universes are removed attributes in the example description There are universes within universes. These will be called sub-universes. several sub-universe with a reduced description marginal sub-universe w.r.t. conditional attributes sub-inverse instantiation partially with their values being restricted sub-universes uninstantiated. conditional of one or more attributes, If in a to as the schema. This will be referred the remaining description attributes. There are also to a given complex having one or more a can be used on the complex. Such a complex may involve partial In such a case the in the schema for the sub-universe to those specified in the instantiation. attributes are included e.g. b = b, or b,. relative These latter attributes to formulate this results instantiated Class models and generating expressions can be constructed for sub-universes from the specification of the universe itself. 3. Noise and the class model In learning the relationship from examples, noise is anything which obscures between description and class. There are three major physical sources of noise: (1) insufficiency of the description (2) corruption of attribute values in the example description, (3) erroneous classification of training examples. schema, 164 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 distributions appearing in the class model this can be If there are non-degenerate to the presence of one or more of these noise sources. The user of the attributed to the origin of noise. artificial universe method can adopt one of two attitudes it, i.e. say nothing about it, and just investigate how the Firstly she can ignore noise affects learning, perhaps varying the degree of noise in the manner to be described below. Secondly she can declare that the noise originates from some of to specify how much came from the sources above. It would be difficult, though, each source without building a more elaborate model. For the most part, the origin of noise does not affect the analysis of learning and the first however, approach the sources separately is often all that is required. To see this consider and how they might be catered for within the class model. The class distributions in the in the class model describe association between observed description vector and the class it is assigned and can insufficiency of the description schema (what the statisticians call residual error). the first of these, the uncertainty to model be taken therefore the These distributions can also be used to model--or rather eliminate the separate need for-attribute distinction can be made between actual attributes which are observed. Usually, as, for example, domain an actual attribute true attribute according distribution) which, description vector. noise. To explicitly allow for corruption of attribute values a and in the LED takes values from the same set as its corresponding (called the corruption true to a given probability distribution in the most general case, (which are not observed) true attributes is conditional upon the The definition of a universe can be augmented the joint distribution together with the class attribute. From this can of both true and actual attributes be obtained a true and an actual class model (class distributions conditional on the true and actual attributes respectively). to provide that If it is assumed It may be argued is being tested (on fresh unclassified examples) that the mechanism which corrupts example descriptions is at for learning and also when the work both when examples are being obtained learned concept description then the true class model has no role to play: the actual class model can be used to generate examples and to assess the learned concept. the purpose of introducing for learning. the overall effect on noise resulting It does not follow, paradoxical true and actual is that it allows the affect of attribute noise to be determined. Schaffer in the LED domain and the probability of corruption it is difficult to though, In general, from a particular set of corrupting though it may seem, that corrupting to true class model has distributions which are the actual class model has attributes [28], for example, investigates state distributions. the attribute distributions necessarily define a universe non-degenerate each of its distributions degenerate (so that class is uncertain) whereas at a single class! In fact it is possible the consequences increases noise. in which increases separate the in example classification do, however, require more elaborate model- Errors ling. Here (and perhaps other sources too), so that some of the classes present in the examples are actually is used the wrong class. After learning, however, the acquired concept description teacher miss-classification the class distributions reflect R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 165 is not present. to make classifications. Thus the classification error mechanism those used to Evaluation of learning would use the true class distributions-not required: an actual universe generate the examples. Two universes are therefore It appears from the literature (used for example generation) and a true universe. that there is little awareness of this aspect of classification noise since the data for testing is usually generated according to the same prescription as that for learning. 3.1. Irrelevant attributes In addition to noise schema, i.e. those that contribute cases to distinguish here, namely there are often “irrelevant” in the little or nothing to classification. There are two those of redundancy attributes present and pure noise. Definition 3.1. An attribute having the property the class model those attributes which do appear. in which it does not appear that there is a representation relative is said to be redundant of to Definition 3.2. An attribute, a, which satisfies P(class = c ( S, A) = P(class = c 1 S) for all c, S and A, where S is a subset of the vector description space of all the attributes excluding a and A is a subset of the values taken by a, is description called a pure noise attribute. A pure noise attribute (about class); all other attributes are said to be informative. A universe at least one attribute be uninformative. is said to be uninformative in which it is said to is said to be informative, otherwise is informative redundancy of an attribute may be relative subset of for example, when it is functionally dependent on those in the subset. the omission of an attribute The attributes, Pure noise implies redundancy but not vice versa: from a representation will be seen below. of the universe does not guarantee that it is pure noise as to a particular In universe 1, d is redundant (it is not instantiated given representation distribution of the class model) but for the condition b = b, and c = c1 is: in any of the rules in the it is not pure noise. The class (0.113,0.263,0,0.375,0,0.250,0,0), indicating the class distribution changes to that class 4 is the most likely. If, however, d = d, is also instantiated (0.212,0.494,0,0.176,0,0.118,0,0) , which makes class 2 the most likely. Some authors, to mean redundancy useful Only a pure noise attribute circumstances information as defined here. Since redundant for example Pazzani and Sarrett [22], use the term “irrelevant” attributes may contain is unfortunate. that there are no about class the use of the term “irrelevant” irrelevant in the sense is truly in which knowledge of its value can influence classification. 166 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 The definition dence of class and attributes-abbreviated Pearl definition, however, whereas independence attribute assertion of conditional essentially conditional The implementation for pure noise appears similar to that of conditional the pure noise attribute in this discussion indepen- given all the other description (see In the latter the conditioning description vector must be fully instantiated that is not the case for pure noise. Thus pure noise implies conditional [23] for a general discussion of conditional to “conditional independence). independence” as was made for that attribute does not appear, for universe 1, is equivalent but not conversely. The specification of a class model in which an to the is independence independence). of a pure noise attribute can be achieved with the further it be that In universe 1, e to a for each one for a description requirement (in addition of the vector of all the other description attributes.’ independent is pure noise. Additional pure noise attributes can be added very easily universe already defined simply by specifying a marginal distribution and adding a corresponding vector. term to the generating expression the concept description) (so that redundancy to omission from By taking S to be the empty set in Definition 3.2 it follows that class and a pure noise attribute are independent. Clearly a universe is uninformative if and only if all the class distributions in the class model are identical. In this case they are all equal to the default distribution in for class. In any representation sub-inverses. The notion of informa- the rules define uninformative from that of noise in the universe class tiveness of an attribute there will be occasions when distributions. knowledge of its value will be of some use to classification regardless of how much noise is in the universe although, of course, the extent of the latter limits just how informative of the class model, conditional an attribute can be. is quite separate If an attribute the complexes is informative that appear then It is also possible for a universe with no noise in the class distributions several pure noise attributes. The assessment of how attributes universe is further explicated using information-theoretic are will be pursued further informative concepts. in the next section when noise to have individual in the 4. Assessing the degree of noise in the class distributions The question remains as to how to manipulate the class distribution required amount of noise. In statistical modelling of residual error, to employ normal distributions with the variance parameter practice to achieve a it is common indicating the ’ Let a’ denote the vector of all description attributes excluding a. The assertion: “independence of a independence of cla.ss and a given a’ implies that a is pure noise” and a’ together with the conditional follows easily from elementary probability for pure noise is also readily seen to be equivalent to the independence of (clam,a’) and a. theory. This sufficient condition R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 167 the degree of noise using only extent of noise. For class distributions where there are at most a small number of is a means discrete (often nominal) classes this is not appropriate. What is needed of explicating in the class in universe 1 the class distribution and not the classes themselves. For example, for the first rule for the class model (Table 1) is (0.5, 0.5, 0, 0, 0, 0, 0, distribution than that of the next rule which is 0). Is there more noise in this distribution (0.55, 0, 0, 0.45, 0, 0, 0, O)? Such a means of comparison is provided by the majorisation relation applied to probability vectors. Majorisation underpins much of the theory of measurement the probabilities and uncertainty. of information 4.1. Majorisation and noise Measures of uncertainty such as Shannon’s entropy function entropy(P) = - 2 pi In pi , i=l where P=(p,, possible classes, provide an assessment of noise based on probabilities only. , . . , p,) and C:=, pi = 1 is the probability distribution on the n Entropy, though, is only one of many possible measures of uncertainty. The it unique in well-known additivity of information property of entropy which renders up to positive multiples communication by the Gini index of diversity: (Shannon and Weaver [30]) and which has meaning is not relevant here. Another popular measure is provided theory gini(P)=l-C n i=l p?. Information information be identified with uncertainty. and uncertainty the lesser the uncertainty, are usually regarded as dual, the and will be treated as such here. Noise will i.e. the greater Uncertainty measures are usually defined to be strictly Schur-concave (see Hickey [ll, 121) that is they respect the pre-ordering afforded by the majorisation relation between probability distributions. as developed in the mathematical Majorisation theory of inequalities provides a the same total of their amongst vectors of real numbers having (but not necessarily having the same number of elements) interpreted in this vector are less equal than those in that vector”. It is a rather than a partial ordering because it lacks antisymmetry: vectors of elements majorise each other but are not pre-ordering elements as: “the elements pre-ordering which differ only in permutation identical. necessarily Applied to discrete probability strict majorisation distributions, distribution whose probabilities the more informative. Here probability distributions only. For the general the background distributions indeed, the notion “less noisy than”, relative explicates are less equal is the less noisy or, equivalently, frequency i.e. the or, to majorisation will be outlined for theory of majorisation see [19]. 168 R. .I. Hickey I Artificial Intelligence 82 (1996) 157-179 some Majorisation re-distributing (larger) probability can be defined using the notion of an equalising transfer which to another the there are to pi ith and jth involves having a smaller probability two probabilities i and j such that pi >pj such probabilities unchanged and the latter being replaced by pi and pi where then some of the excess in p, over pi is transferred is created with all except (which may be zero) in such a way as to render involved more equal. Formally, that a new distribution, P’, from one event if in P = (pl, . . . , p,), the p; = cpi + (1 - c)p, ) p; = (1 - c)p, + cpj (3) for some c, 0 CC c 1. This leads to a definition of majorisation: into P by a finite number of equalising that Q can be Definition 4.1. If discrete distributions P and Q are such transformed then Q is said to majorise P (or P de-majorises Q) and this is written P < Q. If P < Q and Q is not a permutation of P then Q is said to strictly majorise P (or P strictly de-majorises Q) and this is written P < Q. transfers It follows that majorisation to swapping the ith and jth probabilities. Thus if Q is a permutation of P then P =S Q in (3) can be reformulated and Q =S P. The expression is transitive. The case c = 0 corresponds as P=QSy (4) where S is the doubly stochastic matrix with ‘kk = 1, k#i, j, sii = c ) sji = 1 - c ) sij = 1 - c ) sjj = c ) and all other elements zero. It is shown in [19] that P =S Q if and only if (4) holds for some (more complex) doubly stochastic matrix S. If P i Q then P will be regarded as more uncertain or noisier or less informative of noise, low noise is synonymous interpretation than Q. With the majorisation with high concentration noise corresponds of probability on a small number of events whereas high to a spread of probability across a large number of events. The first two class distributions in universe 1 are related by majorisation: (0.5,0.5,0,0,0,0,0,0) < (0.55,0,0,0.45,0,0,0,0) since a single transfer of 0.05 from 0.55 to 0.45 produces a permutation of the left-hand side. A more useful reformulation of the definition of majorisation terms of partial sums of the sorted probabilities (P[l], * * . ) pm,) denote then P 4 Q holds if and only if the decreasing rearrangement in each vector can be given in (see [19]). Let of P, i.e. P,~] 3 * * * 3pPrn1, q[l] + ’ * ’ + q[k] >!+I] + ’ ’ ’ +P,k, (5) for all k, 1 s k s n. This is the most suitable form for computation. R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 169 The following useful properties of majorisation (P < Q) are immediate conse- quences of (5): (1) max(P) < max(Q) where max is the maximum function. (2) The size of the support of P, i.e. the number of non-zero probabilities, must be at least as great as that of Q. (3) The smallest non-zero probability in P must be at least as large as that in Q. distributions It can also be seen from (5) that majorisation (0.3,0.7) is defined as “modulo zero are equally noisy. probabilities”: in this application, noise will usually be modelled over a fixed number of (Since, the uniform this point classes distribution . . _ , 1, n) is the most noisy and is majorised by every other distribution on n classes. At the other end of the scale any degenerate distribution is least noisy in the sense that it majorises every other distribution. little consequence.) Amongst and (0.3,0.7,0,0,0) IZ classes is of (l/n, If classes in a universe are combined, in the class the original since combining probabilities amounts to the reverse the resulting class distribution i.e. an inequality transfer. A concrete example of the use of majorisation transfer, is provided by the commonly through inversion of classes with a known used device of introducing noise probability. This is the mechanism in the classification noise process of Angluin and Laird 131. In a two-class problem suppose Q = ( ql, q2) is a class distribution. Suppose to Q it is then inverted with probability CY. This results in a new class distribution P = ( pl, p2) where that although a class is generated according further model majorises of an equality Pl = (I- “hi?, + aq2 > P2 = ‘y41 + (I- ahI2 3 (6) is just (3). Thus P < Q. More generally, which the inversion probability, LY, can be split equally amongst the remaining IZ - 1 classes. (1 - (Y) and off- This generalizes diagonal elements al(n - 1) and so again P < Q. to (4) where S has diagonal elements in an n-class problem, (6) 4.2. Increasing noise in universe 1 To illustrate the majority the use of majorisation, universe 1 will be made noisier (thereby creating universe 2) by de-majorising each class distribution. This will be achieved by leaving and the remaining probability more evenly over the other classes instead of spreading on a single class as is the case in universe 1. This is the it being concentrated strategy of the description joint distribution attributes will not be altered. in each distribution that was employed class probability in [14]. The unchanged that only the distribution to re-distribute The new class distributions are shown in Table 3 alongside those for universe 1. is no residual Notice probability of the majority class probability has been spread fairly evenly across most of the other classes. for rule 11 is unchanged-there In the other cases the complement here. 170 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 Table 3 Class distributions for universes 1 and 2 Rule number Universe 1 Universe 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (0.5,0.5,0,0,0,0,0,0) (0.55,0,0,0.45,0,0,0,0) (0,0,0.6,0,0,0,0,0.4) (0,0,0,0,0,0,0.4,0.6) (0,0,0,0.3,0,0,0.7,0) (0,0,0,0,0,0.55,0.45,0) (0,0,0.4,0,0.6,0,0,0) (0.3,0.7,0,0,0,0,0,0) (0,0,0,0.6,0,0.4,0,0) (0,0,0.6,0,0,0.4,0,0) (O,O,O,O,O, l,O,O) (0,0,0,0.35,0,0.65,0,0) (0,0,0,0,0.5,0,0,0.5) (0.4,0,0,0.6,0,0,0,0) (0,0.6,0,0,0,0.4,0,0) (0,0,0,0,0,0.5,0.5,0) (0.5,0.1,0.1,0,0,0.1,0.1,0.1) (0.55,0.05,0.1,0.1,0.05,0,0.1,0.05) (0.1,0.1,0.6,0,0.05,0.05,0.05,0.05) (0.1,0.1,0.05,0.05,0,0.05,0.05,0.6) (0.05,0.05,0.05,0.05,0,0.05,0.7,0.05) (0,0.05,0.05,0.05,0.05,0.55,0.15,0.1) (0.1,0.05,0.05,0.1,0.6,0,0.05,0.05) (0.1,0.7,0.05,0.05,0,0.05,0.05,0) (0.1,0.05,0.05,0.6,0,0.1, (0.05,0.05,0.6,0.05,0.1,0.05,0.05,0.05) (O,O,O,O,O,LO,O) (0.05,0.05,0,0.05,0,0.65,0.1,0.1) (0.1,0.1,0.05,0.05,0.5,0.1,0.05,0.05) (0.1,0.05,0.05,0.6,0.05,0.05,0.05,0.05) (0.05,0.6,0.05,0.05,0.05,0.05,0.05,0.1) (0.1,0.1,0.1,0.1,0,0.05,0.5,0.05) 0.05,0.05) Universe 2 has a different default class distribution. It is (0.1364,0.1699,0.0594,0.1378,0.0531,0.2124,0.1534,0.0778), (7) whereas that for universe 1, given in (2), is: (0.1215,0.1510,0.0400,0.2115,0.0445,0.2675,0.1270,0.0370). The default class is still class 6 but its probability has dropped by over 5%. It is not generally that increasing noise will reduce the probability of the default class. In the next section, universe 2 will be altered slightly to produce an increase in default class probability over that of universe 1. the case, though, 4.3. Measures of information and uncertainty As a step towards defining these measures a function which is monotone w.r.t. majorisation is needed: Definition 4.2. A continuous discrete probability distributions Q. If W’)<4(&) (strictly) Schur-convex is &&r-convex h w enever P < Q then 4 real-valued function, then - 4 is (strictly) S&w-concave. 4, on the space of finite if 4(P) =Z 4(Q) whenever P =S is is strictly Schur-convex. If 4 Schur-convex with symmetry arbitrary Schur-convex formation measures as it guarantees familiar from entropy, function, functions are necessarily symmetric. Ordinary convexity implies Schur-convexity (see [19]). Although not essential convexity has an important meaning (and is in fact equivalent to) the property, is always at least as that expected conditional information together in an in- for R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 171 great as that of an unconditional variables X and Y this property is: distribution. Expressed in terms of random 4(X I Y) 3 4Jm . (8) If 4 in (8) is strictly convex independent. Accordingly, then equality occurs the following definition if and only if X and Y are is made: Definition 4.3. A real-valued probability distributions and strictly convex (concave). to be weak. continuous is a measure of information If the function function, 4, on the space of discrete if it is symmetric is said lacks strictness, the measure (uncertainty) Entropy and the Gini index are both symmetric and strictly concave and are is a weak error or is a weak measure of uncer- therefore measures of uncertainty. The maximum information measure: miss-classification tainty. function, max(P), is not strict. The commonly-used function, error(P) = 1 - max(P) its convexity The development given above can be extended carries over idea of majorisation that strict convexity was desirable the where observation also made by Breiman et al. [5] in the context of selection measures CART to continuous distributions in a natural way (see 1131). The in a measure of information was for the learning algorithm. (in fact majorisation in terms of this property All information measures render the same ordering of informativeness between can be [19]). On the other hand if two distributions are two distributions which are related by majorisation defined not related by majorisation between dis- measures which will order tributions as assessed by a real-valued measure. The latter inequality, without majorisation holding, could be just an artefact of the measure used and not indicative of any material difference that it is generally better in information. the itself rather than, say, by increasing entropy. amount of noise using majorisation then them differently. Thus majorisation to find two information is a stronger condition it is always possible It is for this reason than “has greater to manipulate information” 4.4. Information in a universe Measures of information are useful, however, for providing overall summaries of the information content of a universe, that is, information about class. Definition 4.4. The rule information measure universe. The defauZt information the measure. The information gain is: is its expectation over the class distributions in the class model of the is that of the default distribution with respect to in a universe with respect to an information 172 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 1 rule information - default information 1 . A dual definition holds for uncertainty measures. By virtue of the strict in Definition 4.3 the information gain is zero if and only if the universe convexity is uninformative. The rule information assesses the contribution class. The benefit of the attributes as provided by the information in a universes universe all class distributions are permutations of one distribution, P, but not all identical. The uninformative universe with all class distributions equal to P has the same rule information but zero information gain. in terms of their noise content using rule information. Suppose is, however, gain. Care must be of the attributes to identifying relative to the default information taken when comparing When the information measure is the max function, the default and rule informations have interpretations as classification rates: Definition 4.5. When informations UCR) respectively. are called the information measure the default and rule is max, the default and universe classification rates (DCR and Increasing the noise in a class distribution the rule can never increase formation measure is weak). information gain, though, may increase or decrease. information In particular in any representation (and will decrease of the universe in- the UCR can only decrease. The it unless the is substantial Table 4 shows information statistics, using entropy and max, for universes 1 and the identification of class as 2. It is clear that the description attributes there about class provided by each attribute on its own. For both universes, b is the and max. No single most attribute, however, can classify satisfactorily on its own. Because universe 2 was information over the default. Also shown is the information as judged by both entropy informative facilitate attribute Table 4 Information shown in brackets statistics relating to entropy and max for universes 1 and 2; ranks of each attribute are Information Universe 1 Default Rules Gain Attributes: a b c d e Entropy 1.8742 0.5976 1.2766 1.6728 1.3936 1.6420 1.8526 1.8742 (3) (1) (2) (4) (5) Max 0.2675 0.6540 0.3865 0.3622 0.4250 0.3395 0.2675 0.2675 (2) (1) (3) (4) (5) Universe 2 Entropy 1.9847 1.1849 0.7998 1.8853 1.6761 1.8921 1.9728 1.9847 (2) (1) (3) (4) (5) Max 0.2124 0.6540 0.4416 0.2922 0.3649 0.2316 0.2151 0.2124 (2) (1) (3) (4) (5) R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 173 constructed distribution unaltered, from universe 1 by leaving the majority probability in each class its UCR of 65.4% is the same as that for universe 1. [15] has observed Even a small universe such as this can provide a substantial algorithms. Holte “Machine Learning Repository” very informative with the others contributing universes in which several attributes are needed for effective classification. task for learning that many of the real data sets in the [21] appear to possess a single attribute which is little. With the method of artificial simple to build a small manageable universe and yet it is comparatively 5. Evaluating learning The result of concept learning from examples will usually be a mechanism for i.e. a means of deciding, given any description vector, what the classification, class is. Some learning algorithms may produce a non-deterministic corresponding i.e. offer a choice of possible classes with an indication of the classification, to each. For example a trained neural network degree of uncertainty tree may have relative may give the strengths of its output units or a decision for observed classes in its leaves. Some rule induction algorithms such frequencies as CN2 (Clark and Boswell [7]) ’ d m uce overlapping rules. The user of such a system can decide on a rule for selection of a particular class thus creating a deterministic classifier. It will be assumed here that classifiers produce a definite class on each occasion. attached Definition 5.1. A (determintiitic) classifier is a mapping from the set of description vectors to the set of classes. A statement of this mapping is called a representation of the classifier. The universe itself provides a best classifier, i.e. one which has the optimal probability of correct classification. Definition 5.2. A best classifier associated with a universe maps each description vector to a majority class in its associated class distribution in the class model, i.e. in this distribution. A default classifier for a one which has maximum probability universe class in the default dis- tribution. assigns each description vector a majority Because of ties in the class distribution, the best and the default classifiers may not be unique. Concept learning can be characterised examples. With some forms of learning from examples, networks than explicit. To obtain description vectors must be supplied For small universes with discrete attributes as estimation of the best classifier from though, such as neural is implicit rather the classifier all possible to it and the resulting classifications noted. this does not present a problem. For learning, an extensional the classifier obtained and instance-based form of 174 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 if continuous large cases or very this would present attributes difficulties. One possibility here is to estimate the classifier using a second stage of learning. A large sample of description vectors are supplied to the classifier for (noiseless) example set is then passed to a learning classification. The resulting algorithm which generates to the true classifier. rules. The rule set obtained is an approximation involved are Evaluating an extensionally provides a set of deterministic available classifier is straightforward. The classifier rules of the form if (complex) then (class) from which, using calculated. the universe specification, the classification rate can be Definition 5.3. The probability classifies a randomly selected condition value of the actual classification probability rate (ACR). from its is called the actual classification probability of the rule. The expected in a classifier is its actual clussiJication the universe which satisfies induced classification rule correctly that an example The ACR of a classifier can never exceed universe. which is the rate for the default classifier. The UCR is the classification the best classifier. the UCR (Definition 4.5) of the if the classifier is sufficiently bad, be less than the DCR, rate for It can, though, An ACR associated with a classifier learned from data randomly generated relationship is a random variable. The expected ACR over example sets of a from a universe particular size is a useful indicator of the effectiveness of the learning algorithm. set is the The learning curve. The expected ACR for an example set size, while difficult to from a number of trials of compute learning. for algorithms such as ID3, can be estimated this expected ACR and size of example between 5.1. Experiments with 103 on universes 1 and 2 To examine the effect of increased noise on learning with ID3, a number of trials were performed using universes 1 and 2. Each trial consisted of generating the an example the majority class in each leaf of the deterministic in the tree universe, set of a particular classifier obtained by adopting size, inducing a tree with ID3, producing (a mild form of pruning) the ACR of the classifier. the probabilities and computing, from To estimate expected ACRs and the learning curve, a number of trials were carried out for a range of example set sizes from 5 to 5000. Because of the high from small sizes, a large number of replications were variability carried out for these (4000 for size 5) with the number of replications decreasing as the size increased (down to 25 for a size of 5000). in ACR obtained The results of these experiments are shown in Table 5 and the learning curves R.J. Hickey I Artificial Intelligence 82 (1996) 157-l 79 175 Table 5 Estimates of expected ACR, expressed as a percentage, sets generated by universes 1 and 2; estimated standard errors are also given as percentages for rules sets induced by ID3 from example Size No. of trials Universe 1 Universe 2 Est. expected ACR (Est. standard error) DCR = 26.75%, UCR = 65.4% Est. expected ACR (Est. standard error) DCR = 21.24%, UCR = 65.4% 5 8 10 12 15 20 30 50 75 100 200 300 500 750 1000 1250 1500 2000 3000 5000 4000 4000 2000 2000 2000 1000 1000 500 500 200 100 100 100 100 100 100 50 50 50 25 29.2 34.8 37.5 39.8 42.1 45.1 48.2 51.0 52.4 53.6 56.0 57.3 59.1 60.7 61.6 62.2 62.4 63.1 63.9 64.5 (0.12) (0.13) (0.18) (0.17) (0.16) (0.20) (0.16) (0.16) (0.12) (0.16) (0.19) (0.20) (0.13) (0.09) (0.10) (0.09) (0.11) (0.09) (0.08) (0.06) 23.5 27.8 30.0 31.7 33.6 35.7 39.1 42.5 44.4 46.6 51.2 53.8 51.7 60.4 61.6 62.5 63.1 63.9 64.6 64.9 (0.11) (0.13) (0.18) (0.18) (0.18) (0.24) (0.21) (0.24) (0.18) (0.25) (0.29) (0.23) (0.14) (0.11) (0.08) (0.07) (0.07) (0.06) (0.04) (0.04) to size 100 are displayed above in Fig. 1. The expected ACR for universe 1 is up consistently the increased noise in universe 2 presents considerable difficulties for ID3. For larger example set sizes, universe 2 appears to be slightly better with the differences being statistically significant for sizes of 1500 and above. The explanation that for universe 2 until about size 750 showing for this that 70 60 50 40 g Q g g 30 &j 20 IO 0 -- 0 20 40 60 80 Ex set size - Universe 1 Uniwrse 2 - ~--- UCR (64.5%) i 100 Fig. 1. ID3 learning curves for example sets of up to size 100 from universes 1 and 2 176 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 Table 6 Class distributions Rule number for universe 3 Class distribution 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 (0.5,0.1,0.05,0,0,0.2,0.05,0.1) (0.55,0.05,0.1,0.05,0.05,0.15,0,0.05) (O.OS,O.OS, 0.6,0,0.05,0.15,0.05,0.05) (0.05,0.05,0.05,0.05,0,0.15,0.05,0.6) (0.05,0.05,0.05,0.05,0,0.05,0.7,0.05) (0,0.05,0.05,0.05,0.05,0.55,0.15,0.1) (0.05,0.05,0.05,0,0.6,0.15,0.05,0.05) (0.05,0.7,0.05,0.05,0,0.1,0.05,0) (0.05,0.05,0.05,0.6,0,0.15,0.05,0.05) (0.05,0.05,0.6,0.05,0.05,0.1,0.05,0.05) (O,O, o,o, 0, l,O, 0) (0.05,0.05,0,0.05,0,0.65,0.1,0.1) (0.1,0.05,0.05,0.05,0.5,0.15,0.05,0.05) (0.05,0.05,0.05,0.6,0.05,0.1,0.05,0.05) (0.05,0.6,0.05,0.05,0.05,0.1,0.05,0.05) (0.05,0.1,0.05,0.05,0,0.2,0.5,0.05) may be that the single minority class in universe 1 is emerging on occasions as the in universe 2 with its that is unlikely majority small residual probabilities. in the data-something to happen It might be argued that the better expected ACRs obtained for universe 1 for smaller sample sizes are due to a superior default classification rate (DCR): from Table 4 the DCR for universe 1 is 26.75% whereas for universe 2 it is 21.24% so that universe 1 has a considerable head start on universe 2. To investigate this, universe 2 was modified slightly to produce a DCR close to that of universe 1. This was achieved by transferring class (class 6) in the class distribution of most rules. the description statistics for universe 3 are shown in Table 7. The default distribution of the attributes, define universe 3 are shown in Table 6. The information for universe The new distributions which, a small amount of probability joint distributions to the majority together with Table 7 Information shown in brackets statistics for universe 3; ranks of each attribute are Information Default Rules Gain Attributes: a b : e Entropy 1.9453 1.1692 0.7761 1.8391 1.6217 1.8412 1.9326 1.9453 (2) (1) (3) (4) (5) Max 0.2705 0.6540 0.3835 0.3293 0.3825 0.2736 0.2705 0.2705 (2) (I) (3) (4) (5) R.J. Hickey I Arti&ial Intelligence 82 (1996) 157-179 177 Table 8 Estimates of expected ACR, expressed as a percentage, sets generated by universes 1 and 3; estimated standard errors are also given as percentages for rules sets induced by ID3 from example Size No. of trials Universe 1 Universe 3 Est. expected ACR (Est. standard error) DCR = 26.75%, UCR = 65.4% Est. expected ACR (Est. standard error) DCR = 27.05%, UCR = 65.4% 4000 4000 2000 2000 2000 1000 1000 500 500 200 100 100 100 100 100 100 50 50 50 25 29.2 34.8 37.5 39.8 42.1 45.1 48.2 51.0 52.4 53.6 56.0 57.3 59.1 60.7 61.6 62.2 62.4 63.1 63.9 64.5 (0.12) (0.13) (0.18) (0.17) (0.16) (0.20) (0.16) (0.16) (0.12) (0.16) (0.19) (0.20) (0.13) (0.09) (0.10) (0.09) (0.11) (0.09) (0.08) (0.06) 5 8 10 12 15 20 30 50 75 100 200 300 500 750 1000 1250 1500 2000 3000 5000 3 is 24.8 29.2 31.3 33.0 35.0 36.7 40.3 43.5 45.7 47.4 51.8 55.3 58.0 60.5 62.0 62.7 63.3 64.0 64.6 65.0 (0.12) (0.13) (0.18) (0.18) (0.17) (0.23) (0.19) (0.22) (0.19) (0.22) (0.24) (0.16) (0.14) (0.10) (0.08) (0.07) (0.08) (0.07) (0.03) (0.02) (0.1129,0.1666,0.0551,0.1245,0.0525,0.2705,0.1419,0.0760) those for universe 1 for comparison). and thus the DCR is 27.05%, slightly above that for universe 1 which is 26.75%. Trials similar to those described above for universes 1 and 2 were carried out to the ID3 learning curve for universe 3. The results are shown in Table 8 estimate It can be seen that the pattern for (alongside universe 3 follows that for universe 2: ACRs are well below those for universe 1 for sample sizes up to 750 and then universe 3 creeps ahead by a small but statistically significant amount. Thus the results for universe 3 are comparable with those from universe 2 to which it is similar in all regards except for DCR. to be caused by the The lower ACRs for universes 2 and 3, therefore, de-majorising of the class distributions. appear 6. Conclusion The method of artificial universes has the class model as its central notion. By focusing on the class distributions of the model as the means of describing noise 178 R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 tasks of varying degrees of difficulty together with the inclusion of redundant and irrelevant attributes, setting up learning Smallish universes with 5 to 10 attributes can provide quite challenging remain comprehensible relation affords a simple practical way of manipulating noise The majorisation and uncertainty measures gives a levels and through its link with information indicators. unified account of several seemingly different that, at least for ID3, results facilitated. tasks yet to the experimenter. from experimental the process of increasing is greatly class distributions leads to poorer learning curves. It would appear noise by de-majorising References [l] D.W. Aha, Generalising Proceedings Ninth International Conference Mateo, CA, 1992) l-10. from case studies: a case study, in: D. Sleeman and P. Edwards, eds., (Morgan Kaufmann, San on Machine Learning [2] D.W. Aha, D. Kibler and M. Albert, Instance-based learning algorithms, Mach. Learning 6 (1991) 37-66. [3] D. Angluin and P. Laird, Learning from noisy examples, Mach. Learning 2 (1988) 343-370. [4] L.B. Booker, D.A. Goldberg and J.H. Holland, Classifier systems and genetic algorithms, Artif. Intell. 40 (1989) 235-282. [5] L. Breiman, J. Friedman, R. Olshen and C. Stone, Classification and Regression Trees (Wadsworth, Belmont, CA, 1984). [6] P. Cheeseman, classification (Morgan Kaufmann, San Mateo, CA, 1988) 54-64. J. Kelly, M. Self, J. Stutz, W. Taylor and D. Freeman, AUTOCLASS: a Bayesian on Machine Learning International Conference in: Proceedings system, Fifth [7] P. Clark and R. Boswell, Rule induction with CN2: some recent improvements, in: Y. Kodratoff, ed., EWSL-91 (Springer-Verlag, Berlin, 1991) 151-163. [8] P. Clark and T. Niblett, The CN2 induction algorithm, Mach. Learning 3 (1989) 261-283. [9] S. Cost and S. Salzberg, A weighted nearest neighbour algorithm for learning with symbolic features, Mach. Learning 10 (1993) 57-78. [lo] D. Haussler, Quantifying inductive bias: AI learning algorithms and Valiant’s learning frame- work, Artif. Intell. 36 (1986) 177-221. [ll] R.J. Hickey, A note on the measurement of randomness, [12] R.J. Hickey, Majorisation, randomness and some discrete distributions, J. Appl. Prob. 19 (1982) 229-232. J. Appl. Prob. 20 (1983) 897-902. [13] R.J. Hickey, Continuous majorisation and randomness, [14] R.J. Hickey, Artificial universes: learn from examples, Conference on Machine Learning to evaluating algorithms which in: D. Sleeman and P. Edwards, eds., Proceedings Ninth International towards a systematic approach (Morgan Kaufman, San Mateo, CA, 1992) 196-205. J. Appl. Prob. 21 (1984) 924-929. [15] R.C. Holte,Very simple classification rules perform well on most commonly used datasets, Mach. Learning 11 (1993) 63-91. [16] D. Kibler and P. Langley, Machine Proceedings Third European Working Session on Learning 81-92. learning as an experimental science, in: D. Sleeman, ed., (Pitman, Glasgow, Scotland, 1988) [17] P. Langley, W. Iba and K. Thompson, An analysis of Bayesian classifiers, in: Proceedings AAAI-92, San Jose, CA (MIT Press, Cambridge, MA, 1992) 223-228. [18] H Lounis and G.M. Bisson, Evaluation of learning systems: an artificial databased approach, in: Y. Kodratoff, ed., EWSL-91 (Springer-Verlag, Berlin, 1991) 463-481. [19] A.W. Marshall and I. Olkin, Inequalities: The Theory of Majorisation and its Applications (Academic Press, New York, 1979). R.J. Hickey I Artificial Intelligence 82 (1996) 157-179 179 [20] J. Mingers, An empirical comparison of pruning methods for decision tree induction, Mach. Learning 4 (1989) 227-243. [21] P.M. Murphy and D.W. Aha, UC1 repository of machine learning databases, Maintained at the Department of Information and Computer Science, University of California, Irvine, CA (1992). learning [22] M.J. Pazzini and W. Sarrett, A framework for average case analysis of conjunctive algorithms, Mach. Learning 9 (1992) 349-372. [23] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Znference (Morgan Kaufmann, San Mateo, CA, 1988). [24] J.R. Quinlan, Induction of decision trees, Mach. Learning 1 (1986) 81-106. [25] J.R. Quinlan, Simplifying decision trees, Znt. J. Man-Mach. Stud. 27 (1987) 221-234. internal representations [26] D.E. Rumelhart, G.E. Hinton and R.J. Williams, Learning by error in: D.E. Rumelhart, J.L. McClelland and the PDP Research Group, eds., Parallel propagation, Distributed Processing 1 (MIT Press, Cambridge, MA, 1986) 318-362. [27] Y. Sakakibara, Noise-tolerant occam algorithms and their applications to learning decision trees, Mach. Learning 11 (1993) 37-62. [28] C. Schaffer, Sparse data and the effect of overfitting avoidance in decision tree induction, in: Proceedings AAAZ-92, San Jose, CA (MIT Press, Cambridge, MA, 1992) 147-152. [29] C. Schaffer, Overfitting avoidance as bias, Mach. Learning 10 (1993) 153-178. [30] C.E. Shannon and W. Weaver, Mathematics of Communication Theory (University of Illinois Press, Urbana, IL, 1964). [31] L.G. Valiant, A theory of the learnable, Commun. ACM 27 (1984) 1134-1142. [32] L.G. Valiant, Learning conjunctions of disjunctions, in: Proceedings ZJCAZ-85, Los Angeles, CA (Morgan Kaufmann, San Mateo, CA, 1985) 560-566. 