Artificial Intelligence 171 (2007) 434–439www.elsevier.com/locate/artintNo regrets about no-regretYu-Han ChangIntelligent Systems Division, USC Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA 90292, USAReceived 16 May 2006; received in revised form 26 October 2006; accepted 13 December 2006Available online 13 February 2007AbstractNo-regret is described as one framework that game theorists and computer scientists have converged upon for designing andevaluating multi-agent learning algorithms. However, Shoham, Powers, and Grenager also point out that the framework has seriousdeficiencies, such as behaving sub-optimally against certain reactive opponents. But all is not lost. With some simple modifications,regret-minimizing algorithms can perform in many of the ways we wish multi-agent learning algorithms to perform, providingsafety and adaptability against reactive opponents. We argue that the research community should have no regrets about no-regretmethods.© 2007 Elsevier B.V. All rights reserved.Keywords: Multi-agent learning; Regret-minimization; Game theory1. IntroductionTraditional no-regret algorithms sometimes perform sub-optimally because the regret criterion only compares thealgorithm’s performance to the possible alternate outcomes in the individual stage games of a repeated game, assumingthe opponent’s action stays fixed. For example, a typical no-regret agent playing Prisoner’s Dilemma would end upalways defecting, since this minimizes the algorithm’s regret relative to the other possible action of cooperating, giventhe observed sequence of opponent actions. However, it is relatively simple to extend standard no-regret approachesto handle and avoid these suboptimal cases. In fact, I would argue that such modified no-regret algorithms hold muchpromise for the future direction of multi-agent learning.Regret minimization methods, sometimes also referred to as experts algorithms or hedging algorithms, provide theclearest method for evaluating agent performance in general multi-agent settings. Since we would often like to assumethat the opponent in multi-agent learning problems is unknown, it is usually difficult to evaluate agent performance,which depends on the type of opponent the agent ends up playing against. Regret-minimization approaches circumventthis problem by defining performance in terms of a comparison class of possible strategies that the agent itself iscapable of executing. Thus, no assumptions need to be made about the opponent’s strategy. Furthermore, as Shohamet al. have stated, many regret-minimizing algorithms can also guarantee safety in addition to universal consistency.These benefits can be extended to the case where the agent faces reactive opponents as well. Instead of consideringsingle actions at each time step, our modified regret framework considers multi-period strategies by dividing up theE-mail address: ychang@ISI.EDU.0004-3702/$ – see front matter © 2007 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2006.12.007Y.-H. Chang / Artificial Intelligence 171 (2007) 434–439435sequence of games into intervals. The regret criterion can now take into account reactive strategies such as “Tit-for-Tat”. Two issues arise from this setup: 1) an action’s potential reward can no longer be observed unless that action isactually played, since the reward depends on the opponent’s current reactive strategy, which cannot be observed, and2) computational complexity grows exponentially as we consider longer intervals and a larger set of more complexstrategies. The first issue can be resolved by using a no-regret algorithm such as Auer, Cesa-Bianchi, Freund, andSchapire’s EXP3 algorithm [1], which extends Freund and Schapire’s multiplicative weight algorithm [6] to the caseof partial information. We propose to alleviate the second issue by choosing the set of possible strategies carefully andby incorporating learning algorithms as experts in a modified version of the EXP3 algorithm [4].2. Mathematical backgroundFor most of this article, we will focus our attention on repeated games, though the techniques described can po-tentially be extended to stochastic games. The repeated game setting already captures much of the complexity of themulti-agent learning problem, where we need to focus on our ability to learn about and react to the opponent. In astochastic game, we also need to learn about and adapt to the external environment. Here we will focus on modelingthe states of the opponent and set aside the problem of additionally modeling the states of the external environment atthe same time.During each stage game of the repeated game, each player simultaneously chooses to play a particular actionai ∈ Ai and receives reward based on the joint action taken. We use the terms policy and strategy interchangeably.While Nash equilibrium is the accepted solution concept for a single stage game, in repeated game and stochastic gamesettings, modern game theory often takes a more general view of optimality, a view that has also gained acceptancein the machine learning community [3,5]. The key difference is the treatment of the history of actions taken in thegame. Here we define a behavioral strategy β : H → Ai , where H =t H t and H t is the set of all possible histories oflength t. Histories are observations of joint actions, ht = (ai, a−i, ht−1). For simplicity, we will assume A = A1 = A2.(cid:2)Definition 1. A τ -length behavioral strategy βτ : H τ → A is a mapping from the set of all possible histories H τ toactions a ∈ A. Let Bτ be the set of all possible τ -length behavioral strategies βτ .We note that |Bτ | = |A||A|2τ. In the case where we take H t = H , we could even consider learning algorithmsthemselves to be a possible “behavioral strategy” for playing a repeated game.This definition of our strategy space is clearly more powerful, and allows us to define a much larger set of potentialequilibria. However, when the opponent is not rational, it is no longer advantageous to find and play an equilibriumstrategy. In fact, given an arbitrary opponent, the Nash equilibrium strategy may return a lower payoff than some otheraction. Indeed, the payoff may be worse than the original Nash equilibrium value. Thus, we turn to regret minimizationalgorithms.2.1. Regret-minimizationIn repeated games, the standard regret minimization framework enables us to perform almost as well as the bestaction, if that single best action were played in every time period. We will frequently refer to the EXP3 algorithm (andits variants) explored by Auer et al. [1] as an example of this type of algorithm. In the original formulation of EXP3,we choose single actions to play, but we do not get to observe the rewards we would have received if we had chosendifferent actions.We need to be a bit more precise about the definition of regret here. Auer et al. consider an adversarial settingwhere the reward function is actually a sequence of reward functions that change at each time period. We denote thecumulative reward for executing an algorithm H for T time periods by RH , and this is compared with the cumulativereward the agent could have received had it chosen to execute a fixed action a for all T time periods, Rmax = maxa Ra.Since the algorithm H is randomized, we will be discussing expected regret Rmax − E[RH ]. The authors show thatT K ln K, where K is the number of actionthe performance of EXP3 exhibits an expected regret bound of 2choices, and T is the number of rounds we play the game. In situations where the rewards for all possible actions areobserved at each period, this upper bound on the expected regret can be reduced to O(T ln K ).e − 1√√√436Y.-H. Chang / Artificial Intelligence 171 (2007) 434–439Generally speaking, these regret-minimizing algorithms hedge between possible actions by keeping a weight foreach action that is updated according to the action’s historical performance. The probability of playing an action isthen its fraction of the total weights mixed with the uniform distribution. Intuitively, better experts perform better, getassigned higher weight, and are played more often. Sometimes these algorithms are called experts algorithms, since wecan think of the actions as being recommended by a set of experts. This set is also referred to as our comparison class.This comparison class provides a clear means of evaluating our algorithm’s performance by pegging this evaluationmetric on a set of strategies and assumptions that we know how to execute.It is important to note that most of these existing methods only compare our performance against strategies that arebest responses to what are often called oblivious or myopic opponents. That is, the opponent does not learn or react toour actions, instead playing a pre-selected, but possibly arbitrary, fixed string of actions. Under most circumstances,however, we might expect an intelligent opponent to change their strategy as they observe our own sequence of plays.For example, consider the game of repeated Prisoner’s Dilemma. If we follow the oblivious opponent assumption,then the best choice of action on hindsight would always be to defect, since we’re assuming that the oblivious opponentwill not change his next action in response to our defection. This approach would assign low scores (high regret) tocooperative strategies, and thus miss out on the chance to earn higher rewards by cooperating with opponents such asa Tit-for-Tat opponent, which cooperates with us as long as we also cooperate. These opponents can be called reactiveopponents.Our extension of the regret framework deals with reactive opponents by expanding our comparison class of strate-gies to include reactive, or behavioral, strategies. Now, instead of only comparing against the best action from thestage game assuming that the opponent’s action stays fixed, we can also compare our performance against strategiessuch as “Tit-for-Tat” with the assumption that the opponent chooses its actions in response to our strategy. To makethis precise, we divide the sequence of games into phases of length λ, and within each phase, we play the same be-havioral strategy for λ stage games. Thus, instead of choosing actions from A during each stage game, we are insteadchoosing a behavioral strategy from Bλ to play during each phase. Now, if we choose the strategy “Tit-for-Tat”, wewill be able to observe that it produces higher cumulative rewards over λ stage games than a “Always Defect” strategyover the same length phase.In related work, Mannor and Shimkin [8] propose a similar “super-game” framework for extending the regret-minimization framework to handle stochastic games. In this super-game framework, the phases allow the agent toobserve the external environment’s reaction to its strategy. Our basic extensions to EXP3 follow the same lines, butdeal specifically with modeling a reactive opponent rather than a Markov external environment. We then proposefurther extensions to deal with the overwhelming computational complexity of this setup. Mannor and Shimkin focuson an alternate method based on the empirical Bayes envelope to provide a more efficient algorithm with slightlyweaker guarantees than in their super-game setup. de Farias and Meggido [5] also explore the problem of optimizingbehavior against a reactive opponent, but they use a different performance metric rather than regret.3. Extending the experts frameworkWe now describe in detail the basic extensions to the experts framework that allow us to capture the added power ofbehavioral strategies. Instead of choosing actions from A, we choose behavioral strategies from Bτ . Bτ also replaces Aas our comparison class, essentially forcing us to compare our performance against more complex and possibly betterperforming strategies. While executing βτ ∈ Bτ for a phase consisting of λ stage games, the agent receives rewardat each time step, but does not observe the rewards it would have received had it played any of its other possiblestrategies. This is reasonable since the opponent may adapt differently as a particular strategy is played, causing adifferent cumulative outcome over λ time periods. Thus, the opponent could be an arbitrary black-box opponent orperhaps a fixed finite automaton.For example, we might consider an opponent whose action choices only depend on the previous τ -length historyof joint actions. Thus, we can construct a Markov model M of our opponent using the set of all possible τ -lengthhistories as the state space. If the optimal policy β in this MDP is ergodic, we can use the (cid:5)-return mixing timeνβ,M [7] of the induced Markov chain as our choice of λ, since this would give us a good idea of the average rewardspossible with this policy in the long run. We will usually assume that we are given ν = maxβ∈Bτ νβ,M , or we simplychoose a sufficiently large fixed λ.Y.-H. Chang / Artificial Intelligence 171 (2007) 434–439437Thus, if we are executing a policy β learned on a particular opponent model M, then we must run the policy forat least νβ,M time periods to properly estimate the benefit of using that policy. Setting an expert’s commitment timehorizon λ = ν ensures that the expert receives good estimates of a policy’s value during each λ-length phase, whichwe denote by its total reward during that phase, rβ (t, t + λ − 1). Over T time periods, the cumulative reward is thenRβ =rβ ((i − 1)λ + 1, iλ).(cid:3)(cid:5)T /λ(cid:6)i=1Alternatively, we may be given a fixed phase length λ, and we would like to be able to evaluate all possiblestrategies in order to choose the optimal strategy. This would entail enumerating all possible behavioral strategiesover λ periods. A hedging algorithm H that switches between this set of Bλ strategies and picks a βi at each phase iwould get a cumulative reward denotedRH =(cid:5)T /λ(cid:6)(cid:4)i=1(cid:5)(i − 1)λ + 1, iλ(cid:6).rβiThe expected regret of following algorithm H is thus maxβ∈Bλ Rβ − E[RH ]. However, there are still |A||A|2λpossiblestrategies to evaluate. Not only would this take a long time to try each possible strategy, but the regret bounds alsobecome exceedingly weak. The expected regret after T time periods is:√(cid:7)2e − 1|A|λ+|A|2λ/2T λ ln |A|.Clearly this amounts to a computationally infeasible approach to this problem. In traditional MDP solution tech-niques, we are saved by the Markov property of the state space, which reduces the number of strategies we need toevaluate by allowing us to re-use information learned at each state. Without any assumptions about the opponent’sbehavior, as in the classic regret minimization framework, we cannot get such benefits.4. Learning algorithms as expertsHowever, we might imagine that not all policies are useful or fruitful ones to explore, given our choice of a fixedcommitment length of λ. In fact, in most cases, we probably have some rough idea about the types of opponent modelsthat may be appropriate for a given domain. For example, in our Prisoner’s Dilemma example, we might expect thatour opponent is either a Tit-for-Tat player, an Always-Defect or Always-Cooperate player.Given particular assumptions about possible opponent models, we may then be able to use a learning algorithm toestimate the model parameters based on observed history. These learning algorithms can be viewed as experts that rec-ommend a particular behavioral strategy β to be played during each game phase. The behavioral strategies discussedabove are simply experts that recommend the same strategy βi to be played during every phase. The traditional staticexperts would recommend that the same action be played at every stage game during every game phase.Definition 2. Given a set of experts E, the regret we obtain by following an algorithm H is defined as maxe∈E Re −E[RH ], where each expert e outputs a recommended behavioral strategy β ∈ Bλ at each phase of the repeated game.For example, we might use a learning expert if we believe that the opponent may be Markov in the τ -length historyof joint actions, as discussed in the previous section. We can construct a Markov model of the opponent and use anefficient learning algorithm (such as E3 from Kearns and Singh [7]) to learn the (cid:5)-optimal policy in time polynomialin the number of states, |A|2τ . We would be able to efficiently learn the best response strategy to opponents such asthe one shown in Fig. 1, in this case as long as we choose τ (cid:2) 4. In contrast, the hedging algorithm needs to evaluateeach of the exponentially large number of possible policies, namely |A||A|2τpossible policies.Fig. 1. A possible opponent model with five states. Each state corresponds to the number of consecutive “Cooperate” (C) actions we have justplayed. “D” stands for the “Defect”.438Y.-H. Chang / Artificial Intelligence 171 (2007) 434–439Of course, by using a small number of learning experts rather than the entire set of behavioral strategies, we can nolonger guarantee regret minimization over all possible policies. As we will discuss in the following section, we canchoose a subset of fixed policies against which we can compare the performance of any learning algorithms we decideto use, and we can guarantee no-regret relative to this subset of fixed policies, as well as relative to the recommendedstrings of actions produced by the learning algorithms.In some ways, using learning algorithms as experts simply off-loads the exploration from the experts framework toeach individual learning algorithm. The computational savings occur because each learning algorithm makes particularassumptions about the structure of the world and of the opponent, thus enabling each expert to learn more efficientlythan hedging between all possible strategies.5. The hedged learnerSince the chosen learning algorithms will sometimes fail to output good policies, we propose to incorporate themas experts inside a hedging algorithm that hedges between a set of experts that includes the learners. This allowsthe hedging algorithm to switch to using the other experts if a particular learning algorithm fails. It might fail due toincorrect opponent assumptions, such as in the previous section’s example if we chose τ < 4, or the learning algorithmmay simply be ill-suited for the particular domain.Here we outline one method for adding learning experts into a regret-minimization algorithm such as Auer et al.’sEXP3 alongside other static experts. It is straightforward to extend these results to other variants of EXP3 such asEXP3.P.1, which guarantees similar bounds that hold uniformly over time and with probability one. We are given Kstatic experts, each of which plays a single pure action a at every stage game. We need to add L reactive experts, whichmay either be learning algorithms or behavioral strategies. For evaluation, the K static experts have νk = 1, k ∈ K,since they assume that the opponent’s action string is independent of our agent’s action choices. For all l ∈ L, weassume we are given νl > 1 based on the policies these experts can produce and their assumptions regarding theopponent model. Since this may not be the true mixing time, we will often call these νl the commitment lengthrequired by expert l. When it is clear from context, we will often write K and L as the number of experts in the setsK and L, respectively.Hierarchical hedging: Let H0 denote the top-level hedging algorithm. Construct a second-level hedging algorithmH1 composed of all the original K static strategies. Use H1 and the learning algorithms as the L + 1 experts thatH0 hedges between.In contrast, a naive approach for adding learning experts might be to run each of the K + L experts for the sameamount of time, i.e. the longest commitment phase required by any of the static experts or learning algorithms, λmax =maxi∈K∪L νi . This naive approach suffers from two main drawbacks, both stemming from the same issue. Because thenaive approach follows all experts for λmax periods, it follows the static experts for longer than necessary. Intuitively,this slows down the algorithm’s adaptation rate. Furthermore, we also lose out on much of the safety benefit that comesfrom hedging between the pure actions. Whereas a hedging algorithm over the set of pure action static experts withλ = 1 is able to guarantee that we attain at least the safety (minimax) value of the stage game, this is no longer true withthe naive approach and λ > 1 since we have not included all possible λmax-length behavioral experts. This removesuncertainty regarding the next action, and thus many opponent strategies would be able to exploit this determinism. Forexample, consider an opponent that runs a behavioral strategy that simply plays the best response to the last period’sobserved action. Each of the K static experts may then incur high loss when it is run for λmax periods. HierarchicalHedging addresses these issues.Proposition 3. Suppose we have a set K of static experts, and a set L of reactive experts with commitment lengths λi ,maxi λi > |K|. We can devise an algorithm with regret bound:(cid:7)√(cid:5)√2e − 1T K ln K +λmaxT (L + 1) ln(L + 1)(cid:6).Proposition 4. The Hierarchical Hedging algorithm (HH) will attain an asymptotic average reward at least close tothe safety value, or minimax value v, of the stage game: For any (cid:5) > 0, lim infT →∞ RHH/T − v (cid:2) −(cid:5) almost surely.Y.-H. Chang / Artificial Intelligence 171 (2007) 434–439439Hierarchical hedging thus provides one method for devising computationally tractable regret-minimizing algo-rithms that are able to perform well against a large set reactive opponents, achieve no-regret against arbitrary oppo-nents, and guarantee safety.6. Issues to overcome and conclusionAs satisfying as the no-regret framework may seem it be, it isn’t a panacea for the complex problems we needto deal with in multi-agent learning problems. As opponent strategies grow more complex, our recognition processand possible strategic responses grow more complex as well. Computational time grows accordingly. We will need topursue and develop further techniques to reduce this computational load. Work in this direction includes constructingefficient learners that depend only on the VC dimension of the concept class rather than the number of possibleexperts in this class [9]. We can also try to initially use small opponent models and only enlarge these models usingstate-splitting techniques when it becomes necessary [2].However, even given these challenges, one benefit of the regret-minimization approach is that it evaluates ourperformance relative to a comparison class of strategies that we know we can execute. Thus, given some fixed compu-tational constraints, we can use no-regret approaches to achieve agent performance that is comparable (or better than)the best strategy among a set of strategies that we know we can compute within our constraints. This capability holdsno matter what type of opponent the agent ends up facing in the game.We’ve seen that regret-minimization approaches provide us with a number of benefits when applied to the multi-agent learning problem. Specifically, these techniques are likely to be most useful in the fifth area of research thatShoham et al. describe, namely prescriptive, non-cooperative problems. These techniques allow us to create algorithmsthat guarantee safety and exhibit no-regret behavior against arbitrary opponents. Moreover, the extensions we’vedescribed in this article allow the inclusion of behavioral strategies and even learning algorithms in our comparisonclass, thus ensuring that our agent performs well against not only static opponents, but also reactive opponents as well.No-regret algorithms can be said to only perform as well as the comparison class by which they are measured. Byopening the door to these larger comparison classes, we can now develop regret-minimizing algorithms that not onlyexhibit no regret but also truly perform well in a wide variety of settings.References[1] P. Auer, N. Cesa-Bianchi, Y. Freund, R.E. Schapire, Gambling in a rigged casino: the adversarial multi-armed bandit problem, in: Proceedingsof the 36th Symposium on Foundations of Computer Science, 1995.[2] Y. Chang, P.R. Cohen, C.T. Morrison, W. Kerr, R.S. Amant, The Jean system, in: Proceedings of the International Conference on Developmentand Learning, 2006.[3] Y. Chang, L.P. Kaelbling, Playing is believing: The role of beliefs in multi-agent learning, in: Advances in Neural Information ProcessingSystems, 2001.[4] Y. Chang, L.P. Kaelbling, Hedged learning: Regret-minimization with learning experts, in: International Conference on Machine Learning,2005.[5] D.P. de Farias, N. Meggido, How to combine expert (or novice) advice when actions impact the environment, in: Advances in Neural InformationProcessing Systems, 2004.[6] Y. Freund, R.E. Schapire, Adaptive game playing using multiplicative weights, Games and Economic Behavior 29 (1999) 79–103.[7] M. Kearns, S. Singh, Near-optimal reinforcement learning in polynomial time, in: Proceedings of the International Conference on MachineLearning, 1998.[8] S. Mannor, N. Shimkin, The empirical Bayes envelope approach to regret minimization, Technion Technical Report EE-No1261, 2000.[9] A. Strehl, C. Mesterharm, M.L. Littman, H. Hirsh, Experience-efficient learning in associative bandit problems, in: Proceedings of the Interna-tional Conference on Machine Learning, 2006.