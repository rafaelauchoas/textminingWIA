Artificial Intelligence 193 (2012) 129–148Contents lists available at SciVerse ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintA defeasible reasoning model of inductive concept learning fromexamples and communicationSantiago Ontañón a,c,∗, Pilar Dellunde a,b, Lluís Godo a, Enric Plaza aa IIIA-CSIC, Artificial Intelligence Research Institute, Spanish Council for Scientific Research, Campus UAB, 08193 Bellaterra, Catalonia, Spainb Universitat Autònoma de Barcelona, 08193 Bellaterra, Catalonia, Spainc Computer Science Department, Drexel University, Philadelphia, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 27 September 2011Received in revised form 14 June 2012Accepted 29 August 2012Available online 30 August 2012Keywords:InductionLogicArgumentationMachine learningConcept learning1. IntroductionThis paper introduces a logical model of inductive generalization, and specifically of themachine learning task of inductive concept learning (ICL). We argue that some inductiveprocesses, like ICL, can be seen as a form of defeasible reasoning. We define a consequencerelation characterizing which hypotheses can be induced from given sets of examples, andstudy its properties, showing they correspond to a rather well-behaved non-monotoniclogic. We will also show that with the addition of a preference relation on inductivetheories we can characterize the inductive bias of ICL algorithms. The second part of thepaper shows how this logical characterization of inductive generalization can be integratedwith another form of non-monotonic reasoning (argumentation), to define a model ofmultiagent ICL. This integration allows two or more agents to learn, in a consistent way,both from induction and from arguments used in the communication between them. Weshow that the inductive theories achieved by multiagent induction plus argumentation aresound, i.e. they are precisely the same as the inductive theories built by a single agent withall data.© 2012 Elsevier B.V. All rights reserved.Inductive generalization is the basis for machine learning methods which learn general hypotheses from examples. How-ever, with the exception of a few isolated proposals [1–4], there has been little effort towards specific logical models ofinductive generalization. The lack of a formal logical model of induction may have hindered the development of approachesthat combine induction with other forms of logical reasoning.In this paper we do not tackle induction in its more general definition, but limit ourselves to inductive generalization,and specifically, to the common task of inductive concept learning (ICL), which is the most well studied induction problemin machine learning. We will argue that inductive generalization is a form of defeasible reasoning, and define an inductiveconsequence relation (denoted by |∼) characterizing which hypotheses can be induced from given sets of examples, andshow its logical properties.Relationships between inductive reasoning and non-monotonic reasoning have already been established by Flach in [1,5],where he presents a logical analysis of induction and considers several postulates for a general inductive consequence* Corresponding author at: Computer Science Department, Drexel University, Philadelphia, USA.E-mail addresses: santi@iiia.csic.es, santi@cs.drexel.edu (S. Ontañón), pilar@iiia.csic.es, pilar.dellunde@uab.cat (P. Dellunde), godo@iiia.csic.es (L. Godo),enric@iiia.csic.es (E. Plaza).0004-3702/$ – see front matter © 2012 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2012.08.006130S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148relation along with representation theorems in terms of preferential models, in the tradition of non-monotonic reasoning1[7]. However, while the work of Flach aims at defining general rationality postulates for induction in general, our focus ison characterizing a particular form of induction (ICL), which allows us to develop a more concrete model (see Appendix Bfor an in-depth comparison of our proposal with Flach’s). Moreover, Flach presents a logical characterization of inductionfocusing on hypothesis generation rather than hypothesis selection, i.e. intending to model which are the valid hypotheses onecan induce from a set of examples, but not which of those hypotheses is the best one. In this paper, within the framework ofICL, we go one step further and propose that hypothesis selection can also be logically characterized by means of a preferencerelation on inductive theories (suitable sets of hypotheses), and propose some preference relations which capture the typicalbiases used in ICL algorithms (like parsimony or margin maximization).There are two main implications of defining a logical model of inductive generalization. First, it allows for a betterunderstanding of ICL algorithms, and second, it facilitates the integration of inductive reasoning with other forms of logicalreasoning, as we will show by integrating ICL with computational argumentation to define a model of multiagent ICL.This paper extends the preliminary work in [8], modeling inductive generalization as a non-monotonic logic, extending theproperties satisfied, and using preference relations to model bias in ICL.The second part of this paper presents an integration of two non-monotonic forms of reasoning: induction and argu-mentation. This integration shows the advantage of having a logical model of induction. For instance, a multiagent inductionsystem such as [9] already introduced the idea of integrating inductive learning and argumentation in an implemented sys-tems, but lacked any formal grounding for such an integration. In particular, in this paper we present a model of multiagentICL obtained by directly integrating our inductive consequence relation with computational argumentation. In this approach,argumentation is used to model the communication between agents, and ICL models their internal learning processes.The remainder of this paper is organized as follows. Section 2 introduces the problem of inductive concept learning astypically framed in the machine learning literature. Then, Section 3 introduces a logical model of induction and proposesan inductive consequence relation, while Section 4 deals with preferences over inductive theories. In Section 5 we recallbasic notions of computational argumentation and we introduce the notion of argumentation-consistent induction. Next,Sections 6 and 7 define a model of multiagent ICL by integrating our logical model of ICL with computational argumen-tation. The paper closes discussing some related work and with the conclusions. We have also included two appendices:Appendix A contains a generalization of Theorem 1 to n agents, and Appendix B provides more details on the comparisonof our inductive consequence relation with Flach’s previous work.2. BackgroundInductive concept learning (ICL) [10] using inductive techniques is not defined formally in the literature of machinelearning; rather it is usually defined as a task, as follows:Given:Find1. A space X of instances2. A space of hypotheses or generalizations H , modeled as a set of mappings h : X → {0, 1}3. A target concept c, modeled as a partially known mapping c : X → {0, 1}4. A set D of training examples (for which c is known), where a training example is a pair (cid:5)xi, c(xi)(cid:6)A hypothesis h ∈ H such that h(x) = c(x) for each instance x in the set of training examples DThis strictly Boolean definition is usually weakened to allow the equality h(x) = c(x) not being true for all examples inD but just for a percentage, and the difference is called the error of the learnt hypothesis.Another definition of inductive concept learning is that used in Inductive Logic Programing (ILP) [11], where the back-ground knowledge, in addition to the examples, has to be taken into account. Nevertheless, ILP also defines ICL as a task tobe achieved by an algorithm, as follows:Given:Find+−and negative Eexamples of a predicate p1. A set of positive E2. A set of Horn rules (background knowledge) B3. A space of hypotheses H (a sublanguage of Horn logic language)A hypothesis h ∈ H such that• ∀e ∈ E• ∀e ∈ E+ : B ∧ h |(cid:10) e (h is complete)− : B ∧ h (cid:11)|(cid:10) e (h is consistent)These definitions, although widespread, are unsatisfactory and leave several issues without a precise characterization. Forexample, the space of hypotheses H is usually expressed only by conjunctive formulas. However, most concepts need morethan one conjunctive formula (more than one generalization) but this is “left outside” of the definition and is explained as1 A similar work for abductive reasoning is that of Pino-Pérez and Uzcátegui [6].S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148131part of the strategy of an inductive algorithm. For instance, the set-covering strategy [12] consists of finding one definition(cid:12)that covers only part of the positive examples in D, proceeding then to eliminate the covered examples to obtain a new Dthat will be used in the next step. Another example is that, typically, smaller hypotheses are preferred to longer hypotheses;but again, that is left out of the definition.In this paper our goal is not to provide a definition of the task of inductive concept learning, but to provide a logicalcharacterization of the inductive inference processes required for performing such task.3. Inductive generalization for concept learningInductive generalization can be seen as having two main components: hypothesis generation and hypothesis selection[1]. We will model the former using an inductive consequence relation, that defines which statements are valid inductive con-sequences of given a set of examples, and the later using a preference relation, which determines which of those statementsare “better” than others. This section formally defines our inductive consequence relation.3.1. An inductive consequence relationIn order to present our model of inductive concept learning, let us start by describing our language. There are three basicelements in our language: examples, hypotheses (or generalizations) and background knowledge. We will use fragmentsof first-order logic as the representation language for these elements. Given that we focus on inductive concept learning,hypotheses will basically be classification rules (i.e. rules which classify an example as either belonging to the target conceptor not). Therefore, in the rest of this paper, the hypotheses induced from examples will be called rules.We will use a distinguished unary predicate C to denote the target concept. Thus, we will write C(a) when the exam-ple identified by the constant a belongs to the target concept, and ¬C(a) otherwise. Our formulas will be of two kinds:examples, and rules.• Examples will be conjunctions of the form ϕ(a) ∧ C(a), where a is a constant, ϕ(x) is an arbitrary formula with x beingits only free variable. A positive example of C will be of the form ϕ(a) ∧ C(a); a negative example of C will be of the formϕ(a) ∧ ¬C(a).• Rules will be universally quantified formulas of the form (∀x)(ϕ(x) → C(x)), where ϕ(x) is an arbitrary formula with xbeing its only free variable.The set of examples will be noted by Le and the set of rules by Lr , and the set of all formulas of our language will beL = Le ∪ Lr . In what follows, we will use the symbol (cid:14) to denote derivation in classical first-order logic. By backgroundknowledge we will refer to a finite set of formulas K ⊂ Lr .Let us assume that the similarity type of our first-order language is finite (that is, we have a finite number of constants,predicates and function symbols). We fix a finite number of variables and we assume that all the variables contained inthe formulas (either in examples or in rules) are among these. Without loss of generality we can also assume that ineach formula (either in examples or in rules) there are not different quantifier blocks with the same variable. Moreover,we can assume also that, the variable x does not occur quantified in φ(x). For instance, we will not allow formulas likeφ(x) := (∀ y)R yx ∧ (∃ y)(∀x)T xy. Under these assumptions, using the fact that every first-order formula is logically equivalentto a prenex formula with the same free variables, it is not difficult to check that there are only a finite number of (exampleand rule) formulas modulo logical equivalence (see for instance [13, Chap. 2]). Therefore, we will assume that both Le andLr are finite.The previously defined notation allows us to define an inductive consequence relation between examples and rules. Forsimplicity we will write α → β as a shorthand for the formula (∀x)(α(x) → β(x)).Definition 1 (Covering). Given background knowledge K , we say that a rule α → C covers an example ϕ(a) ∧ C(a) (or ϕ(a) ∧¬C(a)) when ϕ(a) ∧ K (cid:14) α(a).Definition 2 (Inductive consequence). Given background knowledge K , a set of examples Δ ⊆ Le and a rule r = α → C , theinductive consequence Δ |∼K α → C holds iff:(1) (Explanation) r covers at least one positive example of C in Δ,(2) (Consistency) r does not cover any negative example of C in Δ.Notice that if we have two conflicting examples in Δ of the form ϕ(a) ∧ C(a) and ψ(b) ∧ ¬C(b), and ϕ(a) is a less specificdescription than ψ(a) (i.e. K (cid:14) ψ(a) → ϕ(a)) then no rule α → C covering the example ϕ(a) ∧ C(a) can be inductivelyderived from Δ. The next definition identifies when a set of examples is free of these kind of conflicts.Definition 3 (Consistent set of examples). A set of examples Δ is said to be consistent with respect to a concept C and backgroundknowledge K when: if ϕ(a) ∧ C(a) and ψ(b) ∧ ¬C(b) belong to Δ, then both K (cid:2) ϕ → ψ and K (cid:2) ψ → ϕ.132S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148Definition 4 (Inducible rules). Given a consistent set of examples Δ and background knowledge K , the set of all rules thatcan be induced from Δ and K is IRK (Δ) = {(ϕ → C) ∈ Lr | Δ |∼K ϕ → C}.Notice that if Δ contains examples for a given concept C and also examples of ¬C , the set IRK (Δ) will contain bothrules that conclude C and rules that conclude ¬C . In general, IRK (Δ) contains rules that conclude every concept for whichthere are examples in Δ. Also, notice that since Lr is finite, IRK (Δ) must also be finite. Next we show some interestingproperties of the inductive consequence |∼K .Some formalizations of defeasible reasoning as non-monotonic logics, such as [14] and [7], consider Reflexivity, Left LogicalEquivalence and Right Weakening the basic properties without which a system should not be considered a logical system,while others, such as [15], consider Reflexivity and Cut to be the basic properties of a logical system. Since our consequencerelation is defined between two different sets of formulas (examples and rules), most of these properties do not directlyapply to our setting. Nevertheless, it is interesting to check whether the principles underlying these properties hold for ourconsequence relation.Intuitively speaking, the Left Logical Equivalence property expresses the requirement that logically equivalent formulashave exactly the same consequences. In our framework, in order to evaluate this principle, we need to define first thenotion of equivalent sets of examples.Δ+ =, whereDefinition 5 (Equivalent sets of examples). Given background knowledge K , and two sets of examples Δ = Δ+ ∪ Δ−Γ = Γ + ∪ Γ −(cid:3)(cid:2)ϕ0(a0) ∧ C(a0), . . . , ϕk(ak) ∧ C(ak)(cid:3)(cid:2)ϕk+1(ak+1) ∧ ¬C(ak+1), . . . , ϕn(an) ∧ ¬C(an)(cid:3)(cid:2)φ0(b0) ∧ C(b0), . . . , φl(bl) ∧ C(bl)(cid:3)φl+1(bl+1) ∧ ¬C(bl+1), . . . , φm(bm) ∧ ¬C(bm)+ =− =− =ΔΓΓ(cid:2)andwe say that Δ is equivalent to Γ modulo K , (Δ ≡K Γ ), iff1. For every i (cid:2) k, there is j (cid:2) l such that K (cid:14) ϕi → φ j ;2. For every j (cid:2) l, there is i (cid:2) k such that K (cid:14) φi → ϕ j ;3. For every i > k, there is j > l such that K (cid:14) ϕi → φ j ;4. For every j > l, there is i > k such that K (cid:14) φi → ϕ j .Now we can show that, after suitable reformulations, Left Logical Equivalence and the rest of above mentioned propertiesare satisfied.Proposition 1. The inductive consequence relation |∼K satisfies the following properties:1. Reflexivity: Assume that Δ is consistent w.r.t. C and K . If ϕ(a) ∧ C(a) ∈ Δ, then Δ |∼K ϕ → C .2. Left Logical Equivalence: If Δ |∼K α → C and Δ ≡K Δ(cid:12)3. Right Logical Equivalence: If K (cid:14) β ↔ α and Δ |∼K α → C , then Δ |∼K β → C .4. Cut: If Δ ∪ {ϕ(a) ∧ C(a), φ(b) ∧ C(b)} |∼K α → C and K (cid:14) ϕ → φ then Δ ∪ {ϕ(a) ∧ C(a)} |∼K α → C .5. Cautious Monotonicity: If Δ |∼K α → C and Δ |∼K β → C , for every new constant b, Δ ∪ {α(b) ∧ C(b)} |∼K β → C .6. Cautious Right Weakening: If K (cid:14) α → β and Δ |∼K β → C , and α → C covers some positive example in Δ, then Δ |∼K α → C ., then Δ(cid:12) |∼K α → C .Proof.1. Since ϕ(a) ∧ C(a) ∈ Δ and we obviously have ϕ(a) ∧ K (cid:14) ϕ(a), explanation trivially holds. Now assume ψ(a) ∧ ¬C(a) ∈ Δ.Then, since Δ is consistent w.r.t. C and K , ψ(a) ∧ K (cid:2) ϕ(a), hence consistency also holds.2. By definition of covering, if a rule α → C covers a positive example of Δ, say ϕ(a) ∧ C(a), it covers any other examplesuch that K (cid:14) ϕ → φ. By definition of equivalent sets of examples (modulo K ), at least one of such. An analogous argument holds for the negative examples.3. By definition of covering, two logically equivalent rules (modulo K ) cover exactly the same positive and negative exam-4. The reason is that, if the rule α → C covers the positive example φ(b) ∧ C(b), since K (cid:14) ϕ → φ, then α → C also coversφ(b) ∧ C(b) ∈ Δ(cid:12)examples belongs to Δ(cid:12)ples.the positive example ϕ(a) ∧ C(a).5. By Definition 2 adding a positive example for an induced rule maintains the validity of that rule.6. By Definition 2 the rule α → C clearly satisfies the explanation property. Moreover, α → C satisfies also the consistencyproperty: otherwise, since K (cid:14) ϕ → φ, the rule β → C will cover a negative example, contrary to our assumption. (cid:2)S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148133The first property, Reflexivity, transforms (or lifts) every example e ∈ Δ into a rule re where constants have been substi-tuted by variables. This lifting is usually called in ICL literature the “single representation trick,” by which an example in thelanguage of instances (here Le ) is transformed into an expression in the language of hypotheses (here Lr ).The Right Logical Equivalence property expresses that one may replace logically equivalent formulas by one another onthe right of the |∼K . The Cut property expresses the fact that one may, in his way towards a plausible conclusion, first adda hypothesis to the facts he knows to be true and prove the plausibility of his conclusion from this enlarged set of factsand then infer inductively this added hypothesis from the facts. Notice that the validity of Cut does not imply monotonicity.Nevertheless, we have seen that a form of Cautious Monotonicity holds for our relation.Observe also that the inductive consequence relation |∼K does not satisfy Right Weakening: If K (cid:14) α → β and Δ |∼K β →C , then Δ |∼K α → C . The reason is that, since α is more specific than β, the rule α → C may cover no positive example.Right Weakening expresses the fact that one must be ready to accept as plausible consequences all that is logically impliedby what one thinks are plausible consequences. We have proposed instead a Cautious Right Weakening property as the onethat is relevant in our model.Let us now analyze some additional properties, which are specially relevant for inductive concept learning.Proposition 2. The inductive consequence relation |∼K satisfies the following properties:1. If Δ |∼K α → C and K (cid:14) α → ϕ then Δ (cid:11)|∼K ϕ → ¬C .2. If Δ |∼K α → C and K (cid:14) ϕ → α then Δ (cid:11)|∼K ϕ → ¬C .3. Falsity Preserving: let r = α → C be such that it covers a negative example from Δ, hence r /∈ IRK (Δ); then r /∈ IRK (Δ ∪ Δ(cid:12)) forany further set of examples Δ(cid:12).4. Positive Monotonicity: Δ |∼K α → C implies Δ ∪ {ϕ(a) ∧ C(a)} |∼K α → C .5. Negative Monotonicity: if ϕ(a) ∧ ¬C(a) ∈ Δ, Δ |∼K α → C implies Δ \ {ϕ(a) ∧ ¬C(a)} |∼K α → C .6. Positive Non-monotonicity: if ϕ(a) ∧ C(a) ∈ Δ, Δ |∼K α → C does not imply Δ \ {ϕ(a) ∧ C(a)} |∼K α → C .7. Negative Non-monotonicity: Δ |∼K α → C does not imply Δ ∪ {ϕ(a) ∧ ¬C(a)} |∼K α → C , but it implies Δ ∪ {ϕ(a) ∧ ¬C(a)} (cid:11)|∼Kα → ¬C .8. Generalization: if Δ = {ϕ(a) ∧ C(a)} and Δ |∼K α → C then K (cid:14) ϕ → α.9. If Δ1 ∪ Δ2 |∼K α → C then either Δ1 |∼K α → C or Δ2 |∼K α → C , that is, IRK (Δ1 ∪ Δ2) ⊆ IRK (Δ1) ∪ IRK (Δ2).Proof.1. Let us assume that K (cid:14) α → ϕ and Δ |∼K ϕ → ¬C . Then, by Consistency, for all ψ(a) ∧ C(a) ∈ Δ we have ψ(a) ∧ K (cid:2)ϕ(a), and hence ψ(a) ∧ K (cid:2) α(a) as well. Then, clearly Δ (cid:11)|∼K α → C .2. Let us assume now that K (cid:14) ϕ → α and Δ |∼K ϕ → ¬C . Then, by Explanation, there exists ψ(a) ∧ ¬C(a) ∈ Δ such thatψ(a) ∧ K (cid:14) ϕ(a). But then we have ψ(a) ∧ K (cid:14) α(a) as well, so again Δ (cid:11)|∼K α → C .3. Notice that if r covers a negative example of Δ, that particular example will remain in Δ ∪ Δ(cid:12)4. This property is stronger than Cautious Monotonicity, and follows by the same argument.5. It is direct consequence that if α → C follows from Δ, it cannot cover any negative example.6. Removing a positive example invalidates an inductive inference when that example is the only one covered the rule.7. Δ |∼K α → C does not imply Δ ∪ {ϕ(a) ∧ ¬C(a)} |∼K α → C because nothing prevents ϕ(a) ∧ K (cid:14) α(a) to hold. The fact.that Δ ∪ {ϕ(a) ∧ ¬C(a)} (cid:11)|∼K α → ¬C is there a consequence of Properties 3 and 1.8. If Δ consists of only one positive example ϕ(a) ∧ C(a), the only way for α to cover ϕ(a) is that α (classically) followsfrom ϕ.9. Let r ∈ IRK (Δ1 ∪ Δ2) (see Definition 4). It means that r at least covers a positive example eno negative example of Δ1 ∪ Δ2, so it covers no negative example of both Δ1 and Δ2. Now, if e+ ∈ Δ2, then r ∈ IRK (Δ2), hence in any case r ∈ IRK (Δ1) ∪ IRK (Δ2). (cid:2)r ∈ IRK (Δ1); otherwise, if e+ ∈ Δ1 ∪ Δ2 and covers+ ∈ Δ1 then clearlyLet us now examine the intuitive interpretation of the properties in Proposition 2 from the point of view of ICL; for thispurpose we will reformulate some notions into the vocabulary commonly used in ICL.Properties 1 and 2 state that by generalizing (resp. specializing) an induced rule will never conclude the negation of thetarget concept. Property 3 states the well-known fact that induction is falsity preserving, i.e. once we know some inducedrule is not valid, it will never be valid again by adding more examples to Δ. Property 4 states that adding a positivedoes not invalidate any existing induced rule, i.e. IRK (Δ) does not decrease; notice that it can increase sinceexample eIRK (Δ ∪ {ethat were not in IRK (Δ). Property 5 states that no negative examplecan be covered if α → C follows from Δ. Property 6 states that when we remove the only positive example covered by therule, we invalidate the inductive inference.++}) might have induced rules that explain emight invalidate existing induced rules in IRK (Δ), i.e. IRK (Δ ∪Property 7 states that adding a negative example e−}) ⊆ IRK (Δ). This is related to Property 3, since once a negative example defeats an induced rule r, we know r will never{ebe valid regardless of how many examples are added to Δ. Property 8 states a generalization property, in the case whereΔ consists of only one positive example. Property 9 states that the rules that can be induced from the union of two sets ofexamples are a subset of the union of the rules that can be induced from each of the sets.+−134S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148Actually, a few of the mentioned properties in Propositions 1 and 2 suffice to fully characterize the inductive consequencerelation |∼K , as we will show presently. For the sake of simplicity, we will assume that we don’t have any backgroundknowledge K .Proposition 3 (Characterization). Let |≈ be a relation between consistent sets of examples for a concept C and rules satisfying thefollowing properties:(P1) Reflexivity: if ϕ(a) ∧ C(a) ∈ Δ then Δ |≈ ϕ → C .(P2) Generalization: if Δ = {ϕ(a) ∧ C(a)} and Δ |≈ α → C then (cid:14) ϕ → α.(P3) Negative Monotonicity: if Δ |≈ α → C and ϕ(a) ∧ ¬C(a) ∈ Δ, then Δ \ {ϕ(a) ∧ ¬C(a)} |≈ α → C .(P4) If Δ1 ∪ Δ2 |≈ α → C then either Δ1 |≈ α → C or Δ2 |≈ α → C .(P5) If Δ |≈ α → C and (cid:14) α → ϕ then Δ |(cid:11)≈ ϕ → ¬C .Then, it holds that Δ |≈ α → C iff α → C covers at least one positive example of C and does not cover any negative example of C in Δ,as required by Definition 2.Proof. In what follows, given a consistent set of examples Δ and a concept C , we will denote by Δ+examples for C in Δ, and by Δ−some positive example in Δ and (ii) α → C does not cover any negative example.its subset of positiveits set of negative examples. Assume Δ |≈ α → C , we have to prove that (i) α → C covers(i) Using (P3) one can remove all negative examples from Δ but still preserving the consequence, i.e. we have Δ+ |≈ α → C .such that {ϕ(a) ∧Now, using (P4), we conclude that there must exist at least one positive example ϕ(a) ∧ C(a) ∈ Δ+C(a)} |≈ α → C . Finally, by (P2), one has that (cid:14) ϕ → α.(ii) By contraposition. Assume α → C covers a negative example ψ(b) ∧ ¬C(b) ∈ Δ−, and hence (cid:14) ψ → α. By (P1), we haveΔ |≈ ψ → ¬C , and since (cid:14) ψ → α, by (P5) we also have Δ |(cid:11)≈ α → C , contradiction. (cid:2)3.2. Inductive theoriesThe notions of inductive consequence and inducible rules allow us to define the idea of an inductive theory for a givenconcept as a set of inducible rules which, together with the background knowledge, explain all the positive examples.Definition 6 (Inductive theory). An inductive theory T for a concept C , w.r.t. Δ and K , is a subset T ⊆ IRK (Δ) such that all(cid:12) ⊂ Tthe rules in T conclude C , and for all ϕ(a) ∧ C(a) ∈ Δ, it holds that T ∪ K ∪ {ϕ(a)} (cid:14) C(a). T is minimal if there is no Tthat Tis an inductive theory for C .(cid:12)Since rules concluding C in IRK (Δ) do not cover any negative example of C , if T is an inductive theory for C w.r.t. Δand K , and ψ(a) ∧ ¬C(a) ∈ Δ for some constant a, then it holds that T ∪ K ∪ {ψ(a)} (cid:2) C(a). Observe that, in the case thatΔ is a consistent set of examples, the existence of inductive theories is guaranteed due to the reflexivity property: the setof all rules obtained lifting examples is an inductive theory. Notice also that the notion of inductive theory is relevant forICL because an inductive machine learning algorithm has as output a specific inductive theory.3.3. ExemplificationThe Zoology data set is a standard machine learning dataset containing 101 instances of animals associated with ananimal family (fish, insect, mammal, etc.). The goal is to learn general descriptions of each of the families by induction. Forexemplification purposes, we will use mammal as our target concept, represented by m. The Zoology dataset, as availablefrom the UCI machine learning repository, has no background knowledge so K = ∅. Let us now consider three animals inZoology (an aardvark, an antelope and a bass):e1 := hair(a1) ∧ milk(a1) ∧ predator(a1) ∧ toothed(a1)∧ backbone(a1) ∧ breathes(a1) ∧ fourlegged(a1)∧ catsize(a1) ∧ m(a1)= ϕ1(a1) ∧ m(a1)e2 := hair(a2) ∧ milk(a2) ∧ toothed(a2) ∧ backbone(a2)∧ breathes(a2) ∧ fourlegged(a2) ∧ tail(a2)∧ catsize(a2) ∧ m(a2)= ϕ2(a2) ∧ m(a2)S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148135e3 := eggs(a3) ∧ aquatic(a3) ∧ predator(a3) ∧ fins(a3)∧ backbone(a3) ∧ toothed(a3) ∧ tail(a3) ∧ ¬m(a3)= ϕ3(a3) ∧ ¬m(a3)Given Δ = {ϕ1(a1) ∧ m(a1), ϕ2(a2) ∧ m(a2), ϕ3(a3) ∧ ¬m(a3)}, to illustrate |∼K , we consider several hypotheses:(cid:4)r1 := (∀x)(cid:4)r2 := (∀x)(cid:4)r3 := (∀x)(cid:4)r4 := (∀x)(cid:5)hair(x) ∧ milk(x) → m(x)(cid:5)toothed(x) ∧ backbone(x) → m(x)(cid:5)tail(x) ∧ domestic(x) → m(x)(cid:5)fourlegged(x) → m(x)• Δ |∼K r1, because both ϕ1(a1) (cid:14) hair(a1) ∧ milk(a1) and ϕ2(a2) (cid:14) hair(a2) ∧ milk(a2) (thus satisfying the explanationcondition) and ϕ3(a3) (cid:2) hair(a3) ∧ milk(a3) (thus satisfying the consistency condition).• Δ (cid:11)|∼K r2, because ϕ1(a1) (cid:14) toothed(a1) ∧ backbone(a1), hence it satisfies explanation, but ϕ3(a3) (cid:14) toothed(a3) ∧backbone(a3), and thus it’s not consistent.• Δ (cid:11)|∼K r3, because ϕ1(a1) (cid:2) tail(a1) ∧ domestic(a1) and ϕ2(a2) (cid:2) tail(a2) ∧ domestic(a2), i.e. it does not satisfy the expla-nation condition. So, even if r3 satisfies the consistency condition, it does not explain any example.• Δ |∼K r4, because both ϕ1(a1) (cid:14) fourlegged(a1) and ϕ2(a2) (cid:14) fourlegged(a2) (thus satisfying the explanation condition)and ϕ3(a3) (cid:2) fourlegged(a3) (thus satisfying the consistency condition).In this example, the sets T 1 = {r1} ⊆ IRK (Δ), T 2 = {r4} ⊆ IRK (Δ) and T 3 = {r1, r4} ⊆ IRK (Δ) are inductive theories of mw.r.t. Δ. Clearly, only T 3 is not minimal.4. Preference over inductive consequencesAlthough many rules can be inductive consequences of a given set of examples, ICL algorithms have a set of preferencesand inductive biases that make them prefer some rules over the rest, or some inductive theories over the rest. For example,rules that cover more positive examples are preferred over rules that cover less examples, shorter rules are preferred overlonger rules, and hypotheses with larger margins are preferred over those with smaller margins [16]. In our model ofinductive generalization we incorporate these criteria by means of a preference relation.Depending on the bias we want to model, the preference relation might be defined over rules or over inductive theories.In either case, since preference might depend on both the set of examples Δ and background knowledge K , we will noteour preference relation by (cid:3)Δ,K .When the preference is expressed over rules, we write r1 (cid:3)Δ,K r2 when r1 is at least as preferred as r2 (r1 >Δ,K r2 whenr1 is strictly preferred to r2). In general, this preference relation is only assumed to be a partial preorder in the set IRK (Δ).Definition 7 (Preferred rules). The set of preferred rules IRinducible rules that are maximally preferred.(cid:2)K (Δ) = {r ∈ IRK (Δ) | (cid:3)r(cid:12) ∈ IRK (Δ): r(cid:12) >Δ,K r} is the subset ofWhen the preference is expressed over inductive theories, we write T 1 (cid:3)Δ,K T 2 when T 1 is at least as preferred as T 2.Again, in general, this preference relation is assumed to be only a partial preorder on the set of possible inductive theories.Given that ICL algorithms ultimately return inductive theories, if the preference is expressed over rules, we are interestedin having a preference over inductive theories, which can be defined as follows.Definition 8 (Preference over inductive theories). Given a preference (cid:3)Δ,K over rules, an inductive theory T is preferred over(cid:12)(cid:12) ∈ T(cid:12) ∈ Tanother theory Tsuch that r, and for each r ∈ T there is no r, if there exist r ∈ T , r, denoted T (cid:3)Δ,K Tsuch that r (cid:3)Δ,K r(cid:12)(cid:12) >Δ,K r.(cid:12)(cid:12)(cid:12)Having a preference relation (cid:3)Δ,K on inductive theories allows us to define the following concepts of preferred and idealinductive theories.Definition 9 (Preferred inductive theory). We say that an inductive theory T is (maximally) preferred with respect to (cid:3)Δ,K if(cid:12) >Δ,K T .there is no other inductive theory T(cid:12) ⊆ IRK (Δ) such that TDefinition 10 (Ideal inductive theory). We say that an inductive theory T is ideal with respect to (cid:3)Δ,K if it is both maximallypreferred w.r.t. (cid:3)Δ,K and minimal.136S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148We remark that in the previous definition the term “ideal theory” neither carries any implicit meaning of being a “best”theory according to some unspecified criterion nor any other mathematical or algebraic meaning, it is just a shorthand todenote an inductive theory that is minimal and maximally preferred (according to a given preference relation).Next, we present two examples of how some typical biases of ICL techniques can be expressed using our framework.4.1. ParsimonyMost ICL algorithms have a bias towards finding shorter hypotheses (i.e. Parsimony or Occam’s Razor), which typicallytranslates to more general hypotheses. We can formalize both notions using two preference relations.Given a function size(T ), which returns the number of symbols required to express the inductive theory T in a givenlogical language, we can define the preference T 1 (cid:3)Δ,K T 2 ⇔ size(T 1) (cid:2) size(T 2), which effectively captures the bias towardsshorter hypotheses.A bias towards more general hypotheses is easier to express as a preference relation between rules. We can define thepreference relation α → C (cid:3)Δ,K β → C iff β ∧ K (cid:14) α, i.e. the rule α → C is preferred to β → C if it is more general. Then,using Definition 8, a preference over inductive theories can be established, as well as preferred (Definition 9) and ideal(Definition 10) inductive theories.4.2. Margin maximizationIn machine learning, margin is commonly defined as the distance from the examples to the decision boundary [16].A classifier which maximizes the margin has the decision boundary far away from every example; this ensures that smallvariations in the training set do not result in misclassifications. Margin maximization is usually employed in machinelearning and pattern recognition techniques where instances are represented in metric spaces. We will show now thatan analogous principle can be applied for logic-based instance representation.First, in order to use the notion of margin, we need to define some measure of distance, or similarity, between examples.To formalize this notion, we assume for simplicity that all predicates in the language are unary and that examples ϕ(a) ∧C(a) are such that ϕ(a) is a conjunction of literals, i.e. ϕ(a) is of the form p1(a) ∧ · · · ∧ pk(a) ∧ ¬pk+1(a) ∧ · · · ∧ ¬pn(a). Inthat case, the only variable in a predicate stands for an example identifier, and hence for our purposes here we can actuallyconsider these unary predicates as atomic propositions. Simplifying, we will denote by ϕ the propositionalized version ofϕ(a), i.e. ϕ = p1 ∧ · · · ∧ pk ∧ ¬pk+1 ∧ · · · ∧ ¬pn. This is indeed the case in the example described in Section 5. We willfurther assume the set P of unary predicates (now propositional variables) we work with is finite.Let Ω = {w : P → {0, 1}} be the space of possible worlds. Given a propositional formula ϕ, we will denote by [ϕ] the setof possible worlds satisfying the formula ϕ (according to classical propositional logic). We assume there is a distance func-tion δ : Ω × Ω → R+(cid:12)) = 0means that w = wto some degree. A usual choice for δ, among others(see e.g. [17,18]), is the Hamming distance, that counts the number of elements of P over which two worlds differ.(cid:12)) evaluates how far or different two worlds w and w(cid:12)) < 1 means that w resembles to w. The intuition is that δ(w, w, 0 < δ(w, ware: δ(w, w(cid:12)(cid:12)(cid:12)Given such a distance function δ on the set of possible worlds Ω , the distance between two propositional formulasbuilt form P can be measured by the distance between the corresponding sets of possible worlds, using the well-knownHausdorff distance derived from δ: δH (ϕ, ψ) = max(Iδ(ϕ | ψ), Iδ(ψ | ϕ)), where(cid:5)(cid:4)Iδ(ϕ | ψ) = maxw∈[ψ]minw(cid:12)∈[ϕ]δw, w(cid:12)Now, given a set of examples Δ, a distance δ and a threshold τ ∈ R+, we can consider an expanded set of examples Δ∗τ(cid:12))(cid:12)) ∧ C(a(cid:12)) ∧ ¬C(bτ ) (hence, in particular, Δ∗where for each ϕ(a) ∧ C(a) ∈ Δ (resp. ϕ(b) ∧ ¬C(b) ∈ Δ) we include all those additional fictitious examples ψ(a(resp. ψ(b(cid:12))), such that the distance between ψ and ϕ is at most τ , i.e. such that δH (ϕ, ψ) (cid:2) τ .Given an inductive theory T ⊆ IRK (Δ), we say that T is valid to the level τ whenever T is also an inductive theory ofIRK (Δ∗τ must be consistent). We assign a preference degree τ to an inductive theory T , notedPref (T ) = τ , when τ is the maximum for which T is still an inductive theory of IRK (Δ∗τ ) (i.e. Pref (T ) is the maximum(cid:12)).degree to which T is valid). This induces a natural preference over inductive theories: T (cid:3)Δ,K TMoreover, according to Definition 9, a preferred inductive theory T is one such that there is no other inductive theory(cid:12) ⊆ IRK (Δ) such that TTNotice that, in the present setting, a preferred inductive theory T maximizes the margin according to the distance δH .τ is expanding as much as possible around all positive examples ϕ(a) ∧ C(a) andAs shown in Fig. 1, the reason is that Δ∗negative examples ϕ(b) ∧ ¬C(b) without T covering any fictitious example of the opposite sign.(cid:12) ⇔ Pref (T ) (cid:3) Pref (T(cid:12) >Δ,K T .4.3. ExemplificationLet us now illustrate the use of preferences by continuing the exemplification started in Section 3.3.Let us consider again the inductive theories used before: T 1 = {r1} ⊆ IRK (Δ), T 2 = {r4} ⊆ IRK (Δ) and T 3 = {r1, r4} ⊆IRK (Δ), and consider a new inductive theory T 4 = {r1, r5, r6}, where:S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148137Fig. 1. Margin based on similarity measure δH .(cid:4)r5 := (∀x)(cid:4)r6 := (∀x)(cid:5)hair(x) ∧ fourlegged(x) → m(x)(cid:5)milk(x) ∧ fourlegged(x) → m(x)Given a function size(·), counting the symbols in a formula (ignoring parenthesis), the size of an inductive theory is sim-ply the sum of the sizes of its rules. Thus we have: size(r1) = 10, size(r4) = 7, size(r5) = 10, size(r6) = 10, and therefore:size(T 1) = 10, size(T 2) = 7, size(T 3) = 17, and size(T 4) = 30. Using the parsimony preference we have: T 2 (cid:3)Δ,K T 1 (cid:3)Δ,KT 3 (cid:3)Δ,K T 4. In fact, there is no other inductive theory with size smaller than 7, and thus T 2 is a preferred inductive theory.Since T 2 is also minimal, it is actually an ideal inductive theory.Notice, however, that if we were to use margin maximization as the preference criterion, with the Hamming distance,T 2 would not be preferred, since it is only valid to the level 0. In fact, the margin preference degrees of these inductivetheories are Pref (T 1) = 0, Pref (T 2) = 0, Pref (T 3) = 0, Pref (T 4) = 1 and, thus, T 4 would be preferred to the others.5. Induction and argumentationOne of the main advantages of having a logical model of induction is that it allows an easy integration of inductivereasoning with other forms of logical reasoning. In order to illustrate its benefits, this section presents a model of mul-tiagent ICL obtained by directly integrating our inductive consequence relation with computational argumentation. In thisintegration, argumentation is used to model the communication between agents, and ICL models their internal learningprocesses.For the sake of simplicity of presentation, we will consider a multiagent system scenario with two agents Ag1 and Ag2having a same target concept C . However, as shown in Appendix A, our main theoretical result applies to the more generalcase of an arbitrary number of agents. We make the following assumptions:1. The background knowledge K of both agents is the same.22. The set of rules Lr and the set of examples Le are defined as before (see Section 3).3. Each agent has a set of examples Δ1, Δ2 ⊆ Le such that Δ1 ∪ Δ2 is consistent.The goal of each agent Agiis to induce an inductive theory T i such that T i ⊆ IR(Δ1 ∪ Δ2) and that constitutes aninductive theory w.r.t. Δ1 ∪ Δ2. We will call this problem multiagent ICL.For this purpose, a naïve approach would be to have both agents sharing their complete sets of examples; however, thatmight not be always feasible for a number of reasons, like cost or privacy. In this section, we will show that by sharing someof the rules they have induced from examples (rather than all of their examples), two agents can also solve the multiagentICL problem. Let us start presenting our computational argumentation framework.5.1. Computational argumentationWe will follow Dung’s abstract argumentation formalization [19] and define an argumentation framework as a pair A =(Γ, (cid:4)), where Γ is a finite set of arguments, and (cid:4) is an attack relation.(cid:12), we write r (cid:4) r(cid:12)to represent that r attacks r(cid:12). Moreover, if bothr (cid:4) r(cid:12)and r(cid:12) (cid:4) r we sayGiven two arguments, r and rthat r blocks r(cid:12).As in any argumentation system, the goal is to determine whether a given argument is defeated or not according to agiven semantics. In our case we will adopt the semantics based on dialectical trees [20,21] explained below:2 For simplicity, since both agents share K and C , in the rest of this paper we will write IR(Δ) rather than IRK (Δ), and just sayinductive theory, insteadof saying inductive theory of C .138S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148Definition 11 (Argumentation line). Given an argumentation framework A = (Γ, (cid:4)) and r0 ∈ Γ , an argumentation line in Arooted in r0 is a sequence: such that:1. ri+1 (cid:4) ri (for i (cid:2) k),2. if ri+1 (cid:4) ri and ri blocks ri−1 then ri (cid:11)(cid:4) ri+1.The argument rk is called the leaf node of λ.Additionally, for the purposes of ICL, we will assume that the attack relation has no cycles (which is the case for thedefinition of attack we will introduce later in this paper, Definition 12), and hence there are no repeated arguments in anargumentation line. Consequently, argumentation lines are always finite by construction. The set Λ(r0) of maximal argu-mentation lines rooted in r0 are those that are not subsequences of other argumentation lines rooted in r0. Clearly, Λ(r0)can be arranged in the form of a tree, where all paths from the root to the leaf nodes exactly correspond to all the possiblemaximal argumentation lines rooted in r0 that can be constructed in the given argumentation framework. In order to decidewhether r0 is defeated in A, the nodes of this tree are marked U (undefeated) or D (defeated) according to the following(cautious) rules:1. Every leaf node is marked U.2. Each inner node is marked U iff all of its children are marked D, otherwise it is marked D.Therefore, the arguments in the argumentation framework A will be either undefeated or defeated according to theirmarking, as follows:Undefeated: U(A) = {r ∈ Γ | r is marked U in the tree Λ(r)}Defeated: D(A) = {r ∈ Γ | r is marked D in the tree Λ(r)}.5.2. Argumentation-based inductionInduction and argumentation can be integrated through the notion of argumentation-consistent induction. While inductionwas defined with respect to a set of observations Δ, argumentation-consistent induction will be defined with respect to a setof observations Δ, and a set of arguments Θ . The essential idea is that we consider arguments to be rules, i.e. Θ ⊆ Lr (anexample can also be used as an argument through its corresponding lifted rule, see the reflexivity property in Proposition 1).Therefore, in the rest of this paper, we will use the terms “rule” and “argument” interchangeably.Given that arguments will be rules, we can now define the attack relation (cid:4) between rules as follows.Definition 12 (Attack). Given two rules r, r(cid:12) ∈ Γ , an attack relation r (cid:4) r(cid:12)holds whenever:1. r = (∀x)(α(x) → (cid:15)(x)),2. r3. K (cid:14) (∀x)(α(x) → β(x)),(cid:12) = (∀x)(β(x) → ¬(cid:15)(x)), andwhere ¬(cid:15) = ¬C when (cid:15) = C and ¬(cid:15) = C when (cid:15) = ¬C .Argumentation-consistent induction consists of inducing rules that agree with both Δ (i.e. not covering negative exam-ples present in Δ) and Θ (i.e. not being defeated by the arguments in Θ ).Definition 13 (Argumentation-consistent inducible rule). A rule r ∈ IR(Δ) is argumentation-consistent with respect to a set ofarguments Θ if r ∈ U(A), where A = (Θ ∪ IR(Δ), (cid:4)).The set of all the argumentation-consistent rules induced is AIR(Δ, Θ) = IR(Δ) ∩ U(A).Now we can define argumentation-consistent inductive theories.Definition 14 (Argumentation-consistent inductive theory). An argumentation-consistent inductive theory T , with respect to Δand a set of arguments Θ , is an inductive theory of Δ such that T ⊆ AIR(Δ, Θ).In the multiagent context starting in the next section, the goal of an agent is to build an argumentation-consistentinductive theory, where such theory will be composed by rules that have not been defeated by a set of arguments Θcoming from another agent.S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148139Fig. 2. Achieving multiagent induction by combining inductive reasoning and computational argumentation.6. Argumentation-consistent induction in multiagent systemsLet us now show how the notion of argumentation-consistent induction can be used to model induction in a scenariowith two agents. The main idea is that agents induce rules from the examples they know, and then they share them withthe other agent. Rules received from the other agent are added into the own agent’s argumentation framework to updateher argumentation-consistent induced rules. Thus, in addition to the set of examples Δi , each agent Agi has an individualargumentation framework Ai , containing both (1) the set of inducible rules IR(Δ) inducted by Agi and (2) the set ofarguments Θ received from another agent.Let us now prove that two agents communicating their induced rules and performing argumentation using the kind ofattack in Definition 12 would obtain the exact same set of inducible rules as a single agent knowing the examples knownto both agents.Since the attack relation between rules is always the same, in the following we will simply write U(Γ ) instead of U(A)to denote the set of undefeated rules of the argumentation system A = (Γ, (cid:4)).Theorem 1 (Argumentation-consistent Induction). U(IR(Δ1) ∪ IR(Δ2)) = IR(Δ1 ∪ Δ2).Proof. Notice that by definition U(IR(Δ)) = IR(Δ); consequently, we have AIR(Δ, IR(Δ)) = IR(Δ).First, we prove that IR(Δ1 ∪ Δ2) ⊆ U(IR(Δ1) ∪ IR(Δ2)). Let r ∈ IR(Δ1 ∪ Δ2) then r = α → C covers a positive example ofΔ1 ∪ Δ2 and does not cover any negative example of Δ1 ∪ Δ2. W.l.o.g., assume the covered positive example is from Δ1.(cid:12) (cid:4) r, i.e. such that K (cid:14) β → α. It isThen r ∈ IR(Δ1). Suppose there exists a rule rclear that rcovers it,r must cover δ−(cid:12) = β → ¬C ∈ IR(Δ1) ∪ IR(Δ2) such that r(cid:12)(cid:12) /∈ IR(Δ1), hence assume that ras well, contradiction.covers a negative example δ− ∈ Δ2, but if r(cid:12) ∈ IR(Δ2). This means rSecond, we prove that IR(Δ1 ∪ Δ2) ⊇ U(IR(Δ1) ∪ IR(Δ2)). Let r ∈ U(IR(Δ1) ∪ IR(Δ2)). W.l.o.g., assume r ∈ IR(Δ1). Thenr = α → C covers a positive example of Δ1 and does not cover any negative example of Δ1. Assume also, looking for acontradiction, that r /∈ IR(Δ1 ∪ Δ2). Since we have assumed that r ∈ IR(Δ1), this means that r covers a negative exampleof Δ2. This negative example can be specialized to a rule ris thespecialization of an example in Δ2 and Δ1 ∪ Δ2 is consistent, the rule ris undefeated. Consequently, r /∈ U(IR(Δ1) ∪ IR(Δ2)),which contradicts our original assumption. Therefore we can conclude IR(Δ1 ∪ Δ2) ⊇ U(IR(Δ1) ∪ IR(Δ2)). (cid:2)(cid:12) = β → ¬C ∈ IR(Δ2) such that K (cid:14) β → α. Since r(cid:12)(cid:12)(cid:12)The previous theorem shows that, given two agents, Ag1 and Ag2, each one with sets of examples Δ1 and Δ2 re-spectively, they can induce the same set of rules either by sharing their induced rules IR(Δ1) and IR(Δ2) and then usingargumentation, or by exchanging all of their examples and then using induction. This equivalence is illustrated in Fig. 2, thatshows two equivalent approaches to obtain an inductive theory w.r.t. Δ1 ∪ Δ2: centralized induction (on the left hand side),and argumentation-consistent induction (on the right hand side). In Appendix A of this paper, we show how this resultapplies to the more general case of an arbitrary number of agents.Clearly, sharing the complete IR(Δi)’s is not a practical solution either, since a) they can be very large, and b) given thereflexivity property, IR(Δi) contains Δi . Nevertheless, Theorem 1 shows that theoretically, the problem of multiagent ICLcan be modeled using individual induction plus argumentation. In fact, if the purpose is finding inductive theories, not allarguments in the IR(Δi)’s need to be exchanged. Section 7 presents a dialogue game that finds an inductive theory w.r.t.Δ1 ∪ Δ2 using the same theoretical idea as used in Theorem 1, but focusing on exchanging a smaller subset of rules.However, let us first illustrate the concepts of argumentation-consistent induction described in this section with anexemplification.6.1. ExemplificationConsider two agents, Ag1 and Ag2, knowing a set of examples Δ1 = {e1, e2, e4} and Δ2 = {e5, e6, e7}. Here, e1, e2 and e3are the three examples used in Section 3.3, and the new four examples (e4 is a sealion, e5 is a seasnake, e6 is a platypus,and e7 is a chicken) are defined as:140S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148Fig. 3. Two agents, Ag1 and Ag2, knowing different sets of examples, and some sample rules that can be induced from them.e4 := hair(a4) ∧ milk(a4) ∧ aquatic(a4) ∧ predator(a4) ∧ toothed(a4)∧ backbone(a4) ∧ breathes(a4) ∧ fins(a4) ∧ twolegged(a1)∧ tail(a4) ∧ catsize(a4) ∧ m(a4)= ϕ4(a4) ∧ m(a4)e5 := aquatic(a5) ∧ predator(a5) ∧ toothed(a5) ∧ backbone(a5)∧ venomous(a5) ∧ fins(a5)∧ tail(a5) ∧ ¬m(a5)= ϕ5(a5) ∧ ¬m(a5)e6 := hair(a6) ∧ eggs(a6) ∧ milk(a6) ∧ aquatic(a6) ∧ predator(a6)∧ backbone(a6) ∧ breathes(a6) ∧ fourlegged(a6) ∧ tail(a6)∧ catsize(a6) ∧ m(a6)= ϕ6(a6) ∧ m(a6)e7 := feathers(a7) ∧ eggs(a7) ∧ airborne(a7) ∧ backbone(a7)∧ breathes(a7) ∧ twolegged(a7) ∧ tail(a7) ∧ ¬m(a7)= ϕ7(a7) ∧ ¬m(a7)Thus, Δ1 contains three positive examples (e1, e2 and e4) and no negative example, and Δ2 contains two negativeexamples (e5 and e7) and one positive example (e6). Let us now consider some of the rules that the agents can induce fromthose examples. For instance, two of the rules that Ag1 can induce are r1, r3 ∈ IR(Δ1) below:(cid:4)r1 := (∀x)(cid:4)r3 := (∀x)(cid:5)backbone(x) → m(x)(cid:5)backbone(x) ∧ toothed(x) ∧ twolegged(x) → m(x)Agent Ag2 can induce the rule r2 ∈ IR(Δ2):(cid:4)r2 := (∀x)(cid:5)backbone(x) ∧ toothed(x) → ¬m(x)When the two agents perform induction in isolation, no issues are found with those three rules, as shown in Fig. 3. However,let us consider now the situation where agent Ag1 communicates r1 and r3 to Ag2, and Ag2 communicates r2 to Ag1. Inthis situation, according to Definition 12, the following attacks hold: r2 (cid:4) r1 and r3 (cid:4) r2.Let us first consider Ag1, who, in addition to its inducible rules IR(Δ1), now has access to the set of rules Θ2→1 = {r2}.Now, to perform argumentation-consistent induction, Ag1 assesses which are the rules that are both inducible from Δ1and consistent with Θ2→1. For that purpose, Ag1 constructs the argumentation framework A1 = (IR(Δ1) ∪ {r2}, (cid:4)). It iseasy to verify that, since r2 is attacked by r3, and r3 is not attacked by any other rule, r2 is defeated. Thus, both r1 andr3 are argumentation-consistent inductions and belong to AIR(Δ1, Θ2→1). Therefore, knowing r2 does not change the set ofinducible rules of Ag1, even if r2 attacks r1 (see Fig. 4).Now, considering agent Ag2, in addition to its inducible rules IR(Δ2), now Ag2 has access to the set of rules Θ1→2 ={r1, r3}. Similarly as before, to perform argumentation-consistent induction, Ag2 assesses which are the rules that are bothinducible from Δ2 and consistent with Θ1→2. Ag2 constructs the argumentation framework A2 = (IR(Δ2) ∪ {r1, r3}, (cid:4)). Inthis case, the rule r2, induced by Ag2 is defeated (because it is attacked by r3, which is not attacked by any other rule), andthus r2 /∈ AIR(Δ2, Θ1→2). Thus, in this case, knowing r1 and r3 changes the set of inducible rules of Ag2.S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148141Fig. 4. The same two agents from Fig. 3, after they communicate some rules.7. Reaching inductive theories in multiagent concept learningWhile Theorem 1 shows that it is possible to solve the problem of multiagent ICL using individual induction plus ar-gumentation, this section shows that when agents want to just agree on a single inductive theory, it is not necessary, ingeneral, to exchange all of their induced rules. This section presents a dialogue game [22] through which two agents cansolve the multiagent ICL problem by communication, specifically by exchanging some of the rules they induced from exam-ples. To define the dialogue game, we need to define an interaction protocol, including the types of messages that agentsare allowed to use, and the conditions under which types of messages can be exchanged. The dialogue game is defined fortwo agents Ag1 and Ag2, each of which has an individual set of examples Δ1, Δ2, and consists of a series of rounds. Ateach round t of the dialogue game, each agent Agi holds a current inductive theory, T ti , that is revised after each round.When the game terminates, both agents reach a common inductive theory with respect to Δ1 ∪ Δ2.During the dialogue game, agents communicate to each other rules induced from their examples. Through this ruleexchange, an agent Agi may attack the inductive theory T tj of the other agent Ag j if it is not consistent with Δi .At the end of each round t, an agent Agi knows the following six pieces of information, namely (Δi, T tj→i, Ati ), where:Θti , T tj, Θti→ j,1. Δi is the set of examples known to Agi .2. T t3. T t4. Θt5. Θt6. Atii is the current inductive theory w.r.t. Δi that agent Agi is holding.j is the current inductive theory w.r.t. Δ j that the other agent is holding.i→ j is the set of arguments (rules) that agent Agi has sent to Ag j up to the round t. Notice that Θtj→i is the set of arguments (rules) that agent Agi has received from Ag j up to the round t.= (IR(Δi) ∪ Θtj→i, (cid:4)) is the argumentation framework for Agi ; notice that the set of arguments is composed of the⊆ IR(Δi).i→ jrules inducible by Agi plus the arguments sent by the other agent Ag j .Let us now provide some auxiliary definitions, before we introduce the dialogue game interaction protocol.Definition 15 (Defeaters of a rule). Given an argumentation framework A = (Γ, (cid:4)), and a defeated argument r ∈ D(A), theset of defeaters of r is:Defeaters(r, A) =(cid:2)r(cid:12) ∈ Γ(cid:6)(cid:6) r(cid:12) (cid:4) r and r(cid:3)(cid:12) ∈ U(A)That is to say, the set of undefeated arguments that attack r.Definition 16 (Defeated arguments from communication). Given the set of arguments Θtset of those received arguments that are defeated according to Agi is Dt= D(Atj→ii ) ∩ Θtj→i .j→i communicated by Ag j to Agi , theUsing the previous definitions, we can now present the dialogue game through which two agents Ag1 and Ag2 can findan inductive theory w.r.t. Δ1 ∪ Δ2.Before the first round, at t = 0, the two agents are assumed to hold initial inductive theories T 01⊆ IR(Δ2)w.r.t. Δ1 and Δ2 respectively. Moreover, we assume each agent has communicated its own inductive theory to the otheragent, and thus:⊆ IR(Δ1) and T 02Θ 01→2= T 01and Θ 02→1= T 02Consequently, the initial argumentation systems of the agents are set to:=A01(cid:4)IR(Δ1) ∪ Θ 02→1, (cid:4)(cid:5)and A02=(cid:4)IR(Δ2) ∪ Θ 01→2, (cid:4)(cid:5)142S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148Then, at each round t, starting at t = 1, each agent Agi computes the new values of the tuple (T tvalues at the previous round (T t−1j and Θtother agent.). Notice that Δi does not change and T ti→ j, At−1, Θt−1iii→ j, Ati , Θti ), based on thej→i are computed by theActually, each round t (cid:3) 1 of the protocol is divided in five simple steps: generate attacks, send attacks, update inductivetheories, send updated inductive theories, and update state. The process ends when no agent generates new attacks. In moredetail, a round t of the protocol is as follows:1. Generate Attacks: Agi generates a set of attacks Rt(cid:12) ∈ Defeaters(r, At−1)j→i i.e. Agi selects one attack for each argument r sent by the other agent that is defeated according toi by selecting a single argument (whichever) rifor each r ∈ Dt−1Agi .2. Send Attacks: Each agent Agi sends Rti to the other agent.= Rtj= ∅, then the process terminates, since this means that the current theories held by each agent (T t−1If RtiT t−1) are acceptable for the other agent (no attack can be found). Otherwise the protocol proceeds to the next step.iandj3. Update Inductive Theories: Each agent Agi generates a new argumentation-consistent⊆inductive theory T ti= (IR(Δi) ∪ Θt−1j, (cid:4))—i.e. the new theory T tj→iitaking into account the attacks received, and replaces the rules that werei , where Bt−1)) ⊆ T t∪ Rti∪ Rtj) such that (T t−1AIR(Δi, Θt−1j→icontains all the undefeated rules from T t−1defeated in T t−1by new rules.∩ U(Bt−1iiii4. Send Updated Inductive Theories: Each agent Agi sends T t5. Update State: the set of arguments received by each agent is increased accordingly:i to the other agent.2→11→2= Θt−11→2= Θt−12→1∪ Rt1∪ Rt2∪ T t• Θt1• Θt∪ T t2both agents update their argumentation frameworks:• At1• At2And new round t + 1 starts by going back to the first step.= (IR(Δ1) ∪ Θt= (IR(Δ2) ∪ Θt2→1, (cid:4))1→2, (cid:4))∪ T t2.When the process terminates, both agents have a common and agreed argumentation-consistent inductive theory, namely∗ = T tT1The reason is that, when the process terminates,if the set Δ1 ∪ Δ2 is consistent, then each agent Agi has ani w.r.t. Δi that is also consistent with the examples in Δ j . Nevertheless, T targumentation-consistent inductive theory T timight not be an inductive theory w.r.t. Δ j , since there might be examples in Δ j not covered by T ti . However, their union∗ = T t2 is an inductive theory w.r.t. the examples in Δ1 ∪ Δ2 and, since both agents know T t∪ T t1 and T t2, both agentsT1∗as a common and agreed argumentation-consistent inductive theory w.r.t. Δ1 ∪ Δ2, as the following theoremcan have Tproves.Theorem 2. If the set Δ1 ∪ Δ2 is consistent, the previous process always ends in a finite number of rounds t, and when it ends, T t1is an inductive theory w.r.t. Δ1 ∪ Δ2.∪ T t21→2= Θt−1Proof. First, let us prove that the final theories (T tthe termination condition (Θt1→2 and ΘtT ti found by agent Agi at the final round t has no counterexamples in either Δ1 nor in Δ2.2) are consistent with Δ1 ∪ Δ2. For this purpose we will show that2→1) implies that the argumentation-consistent inductive theoryLet us assume that there is an example ak ∈ Δ1 which is a counterexample of a rule r ∈ T t2. Because of the reflexivityproperty, there is a rule rk ∈ IR(Δ1) which corresponds to that example. Since Δ1 ∪ Δ2 is consistent, there is no counterex-ample of rk, and thus rk is undefeated. Since rk (cid:4) r by assumption, r would have been defeated, and therefore rule r couldnot be part of any argumentation-consistent inductive theory generated by Ag2. The same reasoning can prove that thereare no counterexamples of T t1 and T t2→1= Θt−1Since T t1 and T 2 are inductive theories w.r.t. Δ1 and Δ2 respectively, it follows from the above that T t2 is an inductivetheory w.r.t. Δ1 ∪ Δ2 because it has no counterexamples in Δ1 ∪ Δ2, and every example in Δ1 ∪ Δ2 is explained at least byone rule in T t1∪ T t1 in Δ1 ∪ Δ2.1 or in T t2.Finally, the process has to terminate in a finite number of steps, since, by assumption, IR(Δ1) and IR(Δ2) are finite sets,⊆ IR(Δi), there is2→1 grow at least with one new argument at each round; however, since Θtand the sets Θtonly a finite number of new arguments that can be added to Θt1→2 and Θti→ j before the termination condition holds. (cid:2)i→ jThus, we have shown that the inductive theories achieved by argumentation-consistent induction are sound. Theorem 1has shown that the set of inductive theories that can be reached through sharing examples is the same as the set ofinductive theories that can be reached by sharing induced rules and then performing argumentation-consistent induction.Furthermore, Theorem 2 shows that it is possible to reach one of those inductive theories by using a simple dialogue gameS. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148143that does not require in general the exchange of all the induced rules made by an agent. As a consequence, centralizing allexamples into a single inductive process is no longer imperative, at least in ICL, since induction followed by argumentationis a viable option.The process to find a multiagent inductive theory can be seen as composed of three mechanisms: induction, argumen-tation and belief revision. Agents use induction to generate general rules from concrete examples, they use argumentationto decide which of the rules sent by another agent cannot be defeated, and finally they perform belief revision when theychange their inductive theories in light of the arguments sent by another agent. The belief revision process is embodied inthe way the set of undefeated rules U(Ati ) changes from round to round, which also determines how an agent’s inductivetheory changes in light of the arguments shared by the other agent.A particular implementation of this integration model is the A-MAIL framework [9], where two agents perform inductionon separate example sets and engage in argumentation until they reach individual inductive theories that are consistent withtheir example sets. The A-MAIL framework offers a particular realization of three mechanisms of induction, argumentationand belief revision. The need of having an argumentation-consistent inductive process is met by ABUI (Argumentation-basedBottom-Up Induction), a new inductive method that finds inductive rules consistent with the set of undefeated rules at anystep of the argumentation process.7.1. ExemplificationLet us assume we have two agents, Ag1 and Ag2 and let Δ1 = {e1, e2, e3} (containing the three examples used inSection 3.3, an aardvark, an antelope and a bass), and Δ2 = {e4, e6, e7} (containing some of the examples used in Section 6.1,a sealion, a platypus, and a chicken). Now, the two agents want to find a common inductive theory of the concept mammal,represented by the unary predicate m. Let us explain the process.Before the protocol starts, at t = 0, each agent has individually found an inductive theory:(cid:2)(cid:2)(cid:4)(∀x)(cid:4)(∀x)==T 01T 02breathes(x) → m(x)(cid:5)(cid:3)aquatic(x) → m(x)(cid:5)(cid:3),andIntuitively, since all the positive examples of mammal known to Ag1 are land animals, and all the negative ones are not,Ag1 has induced that breathing is enough to characterize a mammal. A similar situation has occurred with Ag2, who hasfind by induction that being aquatic is enough to characterize a mammal, since it happens that the only two examples ofmammals Ag2 knows are aquatic.Moreover, at t = 0, each agent has communicated to the other agent their individually found inductive theories and buildtheir initial argumentation systems, and thus:Θ 0A011→2== T 01 ,(cid:4)IR(Δ1) ∪ Θ 02→1(cid:5)2→1, (cid:4)and Θ 0= T 02and A02=(cid:4)IR(Δ2) ∪ Θ 01→2, (cid:4)(cid:5)The protocol then proceeds as follows.Round t = 1.1. Agents proceed by generating attacks against the rules they have received they believe are defeated.• Since the rule (∀x)(aquatic(x) → m(x)) generated by Ag2 is defeated according to Ag1, Ag1 selects one attack to• Since the rule (∀x)(breathes(x) → m(x)) generated by Ag1 is defeated according to Ag2, Ag2 selects one attack todefeat it: R11defeat it: R12= {(∀x)(aquatic(x) ∧ ¬hair(x) → ¬m(x))};= {(∀x)(breathes(x) ∧ feathers(x) → ¬m(x))}.2. These attacks are sent to each other.3. Agents update their theories:• Due to the attacks received, Ag1 updates its inductive theory by removing all the defeated arguments, and replacingthem by new undefeated arguments, and generates: T 11= {(∀x)(hair(x) → m(x))}.• Analogously, Ag2 updates its inductive theory by removing all the defeated arguments, and replacing them by newundefeated arguments, and generates: T 12= {(∀x)(milk(x) → m(x))}.4. These theories are sent to each other.5. Agents update their states:∪ T 1= Θ 01 ; Θ 12→1, (cid:4)), A2∪ R11→21= (IR(Δ1) ∪ Θ 1• Θ 1• A111→22→11= Θ 0∪ R12→12= (IR(Δ2) ∪ Θ 1∪ T 12 ;1→2, (cid:4)).Round t = 2.1. Agents should try now to generate attacks, but since the arguments sent in the previous round R12 are un-1 respectively, no new attacks can be generated and the protocol1 and R1defeated in the argumentation systems A1ends.2 and A1144S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148As a result, both agents have reached inductive theories T 11 and T 12 that are consistent with the whole set of examplesof both agents Δ1 ∪ Δ2 (i.e. each theory has any counterexample neither in Δ1 nor in Δ2). Theorem 2 guarantees that∗ = T 11T∪ T 12=(cid:2)(cid:4)(∀x)(cid:5)hair(x) → m(x)(cid:4), (∀x)milk(x) → m(x)(cid:5)(cid:3)is a common and agreed argumentation-consistent inductive theory. Notice that this result is reached without exchangingany example, and exchanging a small amount of inducible rules.8. Related workPeter Flach [1] introduced a logical analysis of induction, focusing on hypothesis generation. In Flach’s analysis, induc-tion was studied on the meta-level of consequence relations and focused on different properties that may be desirable fordifferent kinds of induction. In this paper we cover both hypothesis generation and hypothesis selection, but we focus in alimited form of induction, namely inductive concept learning, extensively studied in machine learning. A direct differencebetween Flach’s work and the research presented in this paper is that we impose strong syntactical constraints on ourinductive consequence relation (from sets of examples to rules), in order to focus on the specific machine learning problemof inductive concept learning, whereas the work of Peter Flach, no restrictions were applied, in order to study the sound-ness and completeness of sets of meta-level properties of inductive consequence relations. Appendix B offers an in-depthcomparison of some properties of our consequence relation with some of Flach’s meta-level properties.A refinement of Flach’s consistency-based confirmation using Hempel’s direct confirmation was studied in [4]. The au-thors proposed that inductive generalization can be modeled as a deductive process given a completion technique, whichcaptures inductive assumptions, such as “every unknown individual is similar to the known ones.” The difference with ourwork is that, albeit restricted to the particular task of ICL, we propose a specific non-monotonic logic consequence relation,instead of resorting to a completion technique.Related to the work of Flach is that of DelGrande [3], where he studied the algebra of hypotheses that can be formedby induction from sets of examples. In the same way as Flach, DelGrande limited his study to hypothesis generation, andconsidered that his model is a restriction with respect to the general problem of induction, where induction as such playsthe limited role of proposing an initial set of hypotheses, which is later refined using deductive techniques.Also related is the work of Datteri et al. [2], where induction (in machine learning) was understood as a deductiveprocess; Dateri et al. modeled a typical process of a machine learning inductive algorithm in several steps, and provided alogical model for each step (that they call “deductive”). The final argument was that machine learning inductive algorithmsare then “inductionless,” as every step in the process is a logical inference. Our approach, a non-monotonic logical model ofthe whole process of an inductive algorithm, clarifies the nature of inductive concept learning: it is a form of defeasible (i.e.non-deductive) reasoning, similar (albeit not identical) to other forms of defeasible reasoning modeled by non-monotoniclogic.Concerning the integration of inductive reasoning with other forms of logical reasoning, Michalski [23], in his Inferen-tial Theory of Learning, started a unified characterization of all forms of inference (deduction, analogy, induction, etc.) anddefined knowledge transmutation operators. However, those operators were only illustrated with examples, and never com-pletely formalized. In this paper, we have taken on a smaller task: instead of trying to formalize all types of inference,we have focused on a very specific form of inference (inductive generalization), and, in this way, we have managed tocompletely characterize it in the form of a consequence relation.Our approach to model multiagent induction is related to that of merging argumentation systems, which has beenstudied by Coste-Marquis et al. [24], where a group of agents, each one having a different argumentation framework (withpotentially inconsistent attack relations) want to merge them. Coste-Marquis et al. proposed to do so by sharing all thearguments and then letting each agent construct a partial argumentation system where one argument attacks another whenthe majority of agents in the group that know both arguments consider there is an attack. After that, agents can merge theiropinions on which arguments are defeated. Notice, however, that in our setting, since we are not dealing with an abstractargumentation framework and our arguments are actually logical formulas, all agents agree on the attack relation, and thus,we don’t require such merging procedure.Arguments and argumentation have been used in a few approaches of machine learning. For instance, arguments areused in the argument-based machine learning framework [25]; this approach did not employ an argumentation process,instead it assumed that arguments are given as part of the input of the inductive process, and are exploited by the inductivealgorithm.Argumentation has been used in the context of multiagent learning in [26]; however, this approach used argumentationand machine learning as black-boxes that are not integrated, while our logical model of inductive generalization allows for adeep integration of inductive reasoning and argumentation. Amgoud and Serrurier [27] proposed the use of argumentationas a framework to formalize the classification process, and in particular binary classification in the context of conceptlearning. The main difference between the work of Amgoud and Serrurier and ours is that they focus on classification, i.e.given an unclassified example, a set of examples and a set of hypotheses, find the classification of the new instance togetherwith an explanation of why such classification is provided. Argumentation, in their framework, is used to determine whichpossible classifications (understood as arguments coming from examples or hypotheses) are acceptable, given all the otherS. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148145hypotheses and examples, and thus determine a classification for the new example. They also considered a preferencerelation on the set of hypothesis for guiding the search in the hypothesis space and to define the attack relation betweenthem. In contrast, in our work, we are interested on a logical modeling of the concept learning process itself: the processthrough which hypotheses (rules) are generated from a given set of examples. We also use a preference relation, but weused it to rank the induced rules and the set of inductive theories, rather than to define the attack relation. In our proposal,argumentation is only used as a communication framework when multiple agents are involved in the learning process.Our previous work focused first on case-based learning from argumentation-based communication processes [28], wherearguments in the form of both rules and cases were interchanged, but no inductive theory was reached: the agents usedcase-based learning plus argumentation to classify unknown examples. Later, as mentioned before, the A-MAIL frameworkwas the first realization of an argumentation-based approach to multiagent induction [9]. The main difference between [9]and the work presented in this paper is that A-MAIL was a particular implementation that was experimentally validated towork, in the sense that agents achieved mutually consistent inductive definitions of a concept by exchanging arguments andattacks.3 However, there was no formal proof, in [9], that achieving mutually consistent inductive definitions was alwayspossible, as we have done in this paper. On the other hand, in this paper we focus on providing theoretical results thatexplain why an approach like A-MAIL may achieve coordinated induction using argumentation.9. Conclusions and future workThis paper presents two main contributions, one being an inductive consequence relation in the framework of non-monotonic reasoning for inductive concept learning, and the other argumentation-consistent induction, integrating learningfrom examples by inductive generalization with learning from argumentation-based communication.The standard model of non-monotonic reasoning could not be directly applied to our inductive consequence relation. Weneeded to relax and reinterpret some of the properties of this model, taking into account that our inductive consequencerelation is defined between two different sets of formulas (examples and rules). Specifically, Cautious Monotonicity andCautious Right Weakening properties maintain the spirit of the standard model properties by reinterpreting them into acontext in which we have two separate sets of formulas.Furthermore, Proposition 2 presented six additional properties that characterize our inductive consequence relationwhich, as we have shown, are the properties specific to, and anticipated for, inductive concept learning.The notion of inductive theory, introduced here, is a formalization of the intuitive notion of the output resulting from anICL algorithm: a set of formulas that, as a whole, cover and explain all positive examples of the target concept. This notionallows us to deal with hypothesis selection modeled as preferences over inductive theories, modeling well establishedinductive biases such as parsimony and error margin maximization.Moreover, the notion of inductive theory has allowed us,in the second part of this paper, to integrate the non-monotonic reasoning process of inductive generalization with another non-monotonic reasoning process, namely argumen-tation. Argumentation-consistent induction is the key notion in articulating inductive generalization with argumentation:the rules derived by induction are required to be acceptable inside the argumentation framework. Conceptually, the rulesinduced by an agent are learnt not only from examples but from the arguments that are the result of communicating withanother agent.Finally, argumentation-consistent induction allowed us to prove that a group of agents communicating their inducedrules and performing argumentation would obtain the exact same set of inducible rules as a single agent knowing theexamples known to all agents. Thus, learning directly from examples is equivalent (modulo inductive theory equivalence)to learning from communication from another agent that also learns from examples. In other words, for two agents ormore, first communicating all their examples and then learning by induction is equivalent to first learning by inductionindividually and then communicating the generalizations they have learnt using argumentation.In this paper we have centered our analysis on a setting where we assume no noise in the examples, and where wedo not allow induced rules to have any counterexamples. ICL techniques usually accept generalizations that are not 100%consistent with the set of examples. Our future work will focus on moving from a purely Boolean approach to a graded (orweighted) approach, where generalizations that are not 100% consistent with the examples can have a degree of acceptabil-ity. This broader framework would be closer to implemented systems such as A-MAIL [9] that accept induced rules with lessthan 100% consistency as long as they are above a given confidence threshold.AcknowledgementsWe are grateful to Peter Flach for helpful comments and suggestions on an earlier version of this manuscript and toFrancesc Esteva for valuable discussions on an earlier manuscript. Research partially funded by the projects Agreement Tech-nologies (CSD2007-0022), ARINF (TIN2009-14704-C03-03), Next-CBR (TIN2009-13692-C03-01), LoMoReVI (FFI2008-03126-E/FILO), and by the grants 2009-SGR-1433 and 2009-SGR-1434 of the Generalitat de Catalunya.3 Specifically, in [9] we focused on developing and evaluating an inductive algorithm that take into account argument attacks; this algorithm, called ABUIfor argumentation-based bottom-up induction, performs a bottom up search in the space of generalizations to find an induced rule from examples suchthat is not defeated by the set of known arguments attacking previously induced rules.146S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148Appendix A. Argumentation-consistent induction for n agentsThe main theoretical result of this paper concerning inductive concept learning in multiagent systems is captured inTheorem 1. Such result states that learning directly from examples is equivalent to learning from communication fromanother agent that also learns from examples. In this appendix, we generalize this result for multiagent systems with morethan two agents.Theorem 3 (Argumentation-consistent induction for n agents). U((cid:7)i=1...n IR(Δi)) = IR((cid:7)i=1...n Δi).Proof. Notice that by definition U(IR(Δ)) = IR(Δ); consequently, we have AIR(Δ, IR(Δ)) = IR(Δ).(cid:7)i=1...n IR(Δi)). Let r = α → C be such that r ∈ IR(First, we prove that IR(i=1...n Δi) ⊆ U((cid:7)(cid:7)(cid:7)(cid:7)(cid:7)i=1...n Δi), then r covers ai=1...n Δi . W.l.o.g., assume the covered positive(cid:12) (cid:4) r, i.e. suchcovers ai=1...n IR(Δi) such that r(cid:12)(cid:12) ∈ IR(Δ j) for some Δ j , such that j (cid:11)= k. This means r(cid:12) = β → ¬C ∈i=1...n Δi and does not cover any negative example ofpositive example ofexample is from Δk. Then r ∈ IR(Δk). Suppose there exists a rule rthat K (cid:14) β → α. It is clear that rnegative example δ− ∈ Δ j , but if r(cid:7)Second, we prove that IR((cid:12) /∈ IR(Δk), hence assume rcovers it, r must cover δ−(cid:12)(cid:7)as well, contradiction.i=1...n IR(Δi)). Let r = α → C be such that r ∈ U(i=1...n Δi) ⊇ U((cid:7)i=1...n IR(Δi)). W.l.o.g.,assume r ∈ IR(Δk). Then r covers a positive example of Δk and does not cover any negative example of Δk. Assumei=1...n Δi). Since we have assumed that r ∈ IR(Δk), this means that r cov-also, looking for a contradiction, that r /∈ IR((cid:12) = β → ¬C ∈ IR(Δ j) such thaters a negative example of some Δ j . This negative example can be specialized to a rule rK (cid:14) β → α. Since ris undefeated. Con-(cid:7)sequently, r /∈ U(i=1...n Δi) ⊇U(i=1...n IR(Δi)), which contradicts our original assumption. Therefore we can conclude IR(is the specialization of an example in Δ j andi=1...n Δi is consistent, the rule r(cid:7)(cid:7)(cid:7)(cid:7)(cid:12)(cid:12)i=1...n IR(Δi)). (cid:2)Appendix B. Flach’s general approach to inductive consequence relationsIn their seminal paper [7] Kraus, Lehmann and Magidor (KLM) study “general patterns of non-monotonic reasoning andtry to isolate properties that could help us map the field of non-monotonic reasoning by reference to positive properties.”Following Gabbay [14], KLM focus their study at the level of consequence relations and choose a Gentzen-style notationof axiom schemata and inference rules to express structural properties of a consequence relation that could adequatelyrepresent a non-monotonic logic.Based on the KLM framework, Flach [1,5] studies the process of inductive hypothesis formation from two perspectives:finding general rules that explain given specific evidence (explanatory induction), and finding general rules that are confirmedby the evidence (confirmatory induction). Both forms of hypothesis formation are axiomatized also at the level of consequencerelations, providing a set of rationality postulates for various forms of induction.For Flach, an inductive consequence relation |∼ is a set of pairs of formulas, α |∼ β meaning that “β is a possibleinductive hypothesis given evidence α.” Inductive consequence relations are intended to model the behavior of inductiveagents. Flach does not fix a particular definition of |∼, he studies rationality postulates limiting different possible definitions.He starts with a set of general principles for induction and then presents specific sets of principles for each type of induction(explanatory and confirmatory).Since our consequence relation |∼K is defined between two different sets of formulas (examples and rules), most of theseproperties do not directly apply to our setting. Nevertheless, it is interesting to check whether the Flach’s general principles(listed below) underlying these properties hold for |∼K .1. Verification (a predicted observation verifies the hypothesis)(cid:14) α ∧ β → γ , α |∼ βα ∧ γ |∼ β2. Falsification (an observation, the negation of which was predicted, falsifies the hypothesis)(cid:14) α ∧ β → γ , α |∼ βα ∧ ¬γ (cid:11)|∼ β3. Left Logical Equivalence (the logical form of the evidence is immaterial)(cid:14) α ↔ β, α |∼ γβ |∼ γ4. Right Logical Equivalence (the logical form of the hypothesis is immaterial)(cid:14) β ↔ γ , α |∼ βα |∼ γS. Ontañón et al. / Artificial Intelligence 193 (2012) 129–1481475. Left Reflexivity (evidence allowing some hypothesis is admissible)α |∼ βα |∼ α6. Right Reflexivity (any hypothesis allowed by some evidence is admissible)α |∼ ββ |∼ β7. Right Extension (any hypothesis can be extended with a prediction)(cid:14) α ∧ β → γ , α |∼ βα |∼ β ∧ γIn order to check the validity of these general principles in our ICL framework, we need first to set out how to interpretFlach’s consequence relation |∼ in terms of our inductive consequence relation |∼K , taking into account our restrictedlanguage of rules and examples. Indeed, in an expression α |∼ β we interpret the evidence α as a set of (both positive andnegative) examples Δ for a concept C , and the hypothesis β as a rule (∀x)(ϕ(x) → C(x)).In this setting, we provide the following justifications and propose an adapted form of these principles to our framework:1. Verification: interpreting a predicted observation as a new positive example γ (a) ∧ C(a) already covered by an inducedrule β → C from a set of examples Δ, the principle holds by Property 3 of Proposition 2 (Positive monotonicity).K (cid:14) γ → β, Δ |∼K β → CΔ ∪ {γ (a) ∧ C(a)} |∼K β → C2. Falsification: with the same interpretation as in the previous item, a new negative example γ (a) ∧ ¬C(a) is not coveredby an induced rule β → C from Δ when γ (a) ∧ C(a) was already covered by β → C . That is,K (cid:14) γ → β, Δ |∼K β → CΔ ∪ {γ (a) ∧ ¬C(a)} (cid:11)|∼K β → CThis follows by the very definition of the inductive consequence relation |∼K .3. Left Logical Equivalence: if Δ |∼K α → C and Δ ≡K Δ(cid:12), then Δ(cid:12) |∼K α → C . This directly follows from Property 2 inProposition 1.Δ |∼K α → C, Δ ≡K Δ(cid:12)Δ(cid:12) |∼K α → C4. Right Logical Equivalence: if K (cid:14) β ↔ α and Δ |∼K α → C , then Δ |∼K β → C . This directly follows from Property 3 inProposition 1.K (cid:14) β ↔ α, Δ |∼K α → CΔ |∼K β → C5. Left Reflexivity: if Δ |∼K β → C for some rule β → C , this means that Δ is consistent, and hence, for every α(a) ∧ C(a) ∈Δ, we have {α(a) ∧ C(a)} |∼K α → C . This follows from Property 1 of Proposition 1.Δ |∼K β → C, α(a) ∧ C(a) ∈ Δ{α(a) ∧ C(a)} |∼K α → C6. Right Reflexivity: if Δ |∼K β → C for some set of examples Δ, for every example β(a) ∧ C(a), we have {β(a) ∧ C(a)} |∼Kβ → C . This follows from Property 1 of Proposition 1.Δ |∼K β → C{β(a) ∧ C(a)} |∼K β → C7. Right Extension: if Δ |∼K β → C , by definition of covering, there must exist a positive example α(a) ∧ C(a) ∈ Δ suchthat (cid:14) α → β. Assuming (cid:14) α ∧ β → γ , we have that (cid:14) α → β ∧ γ . Since Δ is assumed to be consistent, β ∧ γ cannotcover any negative example, and consequently Δ |∼K β ∧ γ → C .Δ |∼K β → C, {α(a) ∧ C(a)} |∼K β → C, ∧ (cid:14) α ∧ β → γΔ |∼K β ∧ γ → C148S. Ontañón et al. / Artificial Intelligence 193 (2012) 129–148References[1] P.A. Flach, Logical characterisations of inductive learning, in: Handbook of Defeasible Reasoning and Uncertainty Management Systems, vol. 4, AbductiveReasoning and Learning, Kluwer Academic Publishers, Norwell, MA, USA, 2000, pp. 155–196.[2] E. Datteri, H. Hosni, G. Tamburrini, An inductionless, default based account of machine learning, in: L. Magnani (Ed.), Model-Based Reasoning in Scienceand Engineering, College Publications, 2006, pp. 379–399.[3] J.P. Delgrande, A formal approach to learning from examples, International Journal of Man–Machine Studies 26 (1987) 123–141.[4] N. Lachiche, P. Marquis, A model for generalization based on confirmatory induction, in: Proceedings of the 9th European Conference on MachineLearning (ECML), 1997, pp. 154–161.[5] P.A. Flach, Rationality postulates for induction, in: Yoav Shoham (Ed.), Proc. 6th Int. Conf. on Theoretical Aspects of Rationality and Knowledge, MorganKaufmann, 1996, pp. 267–281.[6] R. Pino-Pérez, C. Uzcátegui, Jumping to explanations versus jumping to conclusions, Artificial Intelligence 111 (1–2) (1999) 131–169, http://dx.doi.org/10.1016/S0004-3702(99)00038-7.[7] S. Kraus, D. Lehmann, M. Magidor, Nonmonotonic reasoning, preferential models and cumulative logics, Artificial Intelligence 44 (1–2) (1990) 167–207.[8] S. Ontañón, P. Dellunde, L. Godo, E. Plaza, Towards a logical model of induction from examples and communication, in: Proceedings of the 13thInternational Conference of the Catalan Association for Artificial Intelligence (CCIA), IOS Press, 2010, pp. 259–268.[9] S. Ontañón, E. Plaza, Multiagent inductive learning: an argumentation-based approach, in: Proceedings of the Twenty Seventh International Conferenceon Machine Learning (ICML), Omnipress, 2010, pp. 839–846.[10] T. Mitchell, Machine Learning, McGraw-Hill, 1997.[11] N. Lavraˇc, S. Džeroski, Inductive Logic Programming, Techniques and Applications, Ellis Horwood, 1994.[12] J.R. Quinlan, Learning logical definitions from relations, Machine Learning 5 (1990) 239–266.[13] H. Enderton, A Mathematical Introduction to Logic, second edition, Harcourt/Academic Press, 2001.[14] D. Gabbay, Theoretical foundations for non-monotonic reasoning in expert systems, in: Logics and Models of Concurrent Systems, Springer-Verlag, NewYork, NY, USA, 1985, pp. 439–457.[15] A. Avron, Simple consequence relations, Information and Computation 92 (1) (1991) 105–140.[16] V. Vapnik, Estimation of Dependences Based on Empirical Data, Information Science and Statistics, Springer, 2006.[17] C. Lafage, J. Lang, Propositional distances and preference representation, in: S. Benferhat, P. Besnard (Eds.), Proceedings of ECSQARU 2001, in: LNAI,vol. 2143, Springer, 2001, pp. 48–59.[18] S.-H. Nienhuys-Cheng, Distance between herbrand interpretations: A measure for approximations to a target concept, in: Proceedings of the 7thInternational Workshop on Inductive Logic Programming, Springer-Verlag, London, UK, 1997, pp. 213–226.[19] P.M. Dung, On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games, ArtificialIntelligence 77 (2) (1995) 321–357.[20] C. Chesñevar, G. Simari, A lattice-based approach to computing warranted beliefs in skeptical argumentation frameworks, in: Proceedings of theTwentieth International Joint Conference on Artificial Intelligence (IJCAI), 2007, pp. 280–285.[21] N. Rotstein, M. Moguillansky, G. Simari, Dialectical abstract argumentation: a characterization of the marking criterion, in: Proceedings of the TwentyFirst International Joint Conference on Artificial Intelligence (IJCAI), 2009, pp. 898–903.[22] H. Prakken, Coherence and flexibility in dialogue games for argumentation, Journal of Logic and Computation 15 (2005) 1009–1040.[23] R. Michalski, Inferential theory of learning as a conceptual basis for multistrategy learning, Machine Learning 11 (2–3) (1993) 111–152.[24] S. Coste-Marquis, C. Devred, S. Konieczny, M.-C. Lagasquie-Schiex, P. Marquis, On the merging of Dung’s argumentation systems, Artificial Intelli-gence 171 (2007) 730–753.[25] M. Možina, J. Zabkar, I. Bratko, Argument based machine learning, Artificial Intelligence 171 (10–15) (2007) 922–937.[26] M. Wardeh, T.J.M. Bench-Capon, F. Coenen, Padua: a protocol for argumentation dialogue using association rules, Artificial Intelligence in Law 17 (3)(2009) 183–215.[27] L. Amgoud, M. Serrurier, Arguing and explaining classifications, in: Proceedings of the Sixth International Conference on Agents and Multiagent Systems(AAMAS), ACM, New York, NY, USA, 2007, pp. 1–7.[28] S. Ontañón, E. Plaza, Learning and joint deliberation through argumentation in multiagent systems, in: Proceedings of the Sixth International Confer-ence on Agents and Multiagent Systems (AAMAS), 2007, pp. 971–978.