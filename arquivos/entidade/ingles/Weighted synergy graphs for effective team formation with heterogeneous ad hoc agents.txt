Artificial Intelligence 208 (2014) 41–65Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintWeighted synergy graphs for effective team formation withheterogeneous ad hoc agentsSomchaya Liemhetcharat∗, Manuela VelosoSchool of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 12 September 2012Received in revised form 12 December 2013Accepted 17 December 2013Available online 19 December 2013Keywords:Multi-agentMulti-robotHeterogeneousCapabilitySynergyAd hocTeam formation1. IntroductionPrevious approaches to select agents to form a team rely on single-agent capabilities,and team performance is treated as a sum of such known capabilities. Motivatedby complex team formation situations, we address the problem where both single-agent capabilities may not be known upfront, e.g., as in ad hoc teams, and whereteam performance goes beyond single-agent capabilities and depends on the specificsynergy among agents. We formally introduce a novel weighted synergy graph model tocapture new interactions among agents. Agents are represented as vertices in the graph,and their capabilities are represented as Normally-distributed variables. The edges of theweighted graph represent how well the agents work together,i.e., their synergy in ateam. We contribute a learning algorithm that learns the weighted synergy graph usingobservations of performance of teams of only two and three agents. Further, we contributetwo team formation algorithms, one that finds the optimal team in exponential time,and one that approximates the optimal team in polynomial time. We extensively evaluateour learning algorithm, and demonstrate the expressiveness of the weighted synergy graphin a variety of problems. We show our approach in a rich ad hoc team formation problemcapturing a rescue domain, namely the RoboCup Rescue domain, where simulated robotsrescue civilians and put out fires in a simulated urban disaster. We show that the weightedsynergy graph outperforms a competing algorithm, thus illustrating the efficacy of ourmodel and algorithms.© 2013 Elsevier B.V. All rights reserved.Heterogeneous agents have varying capabilities that affect their task performance. We research on teams of such hetero-geneous agents and how the performance of a team at a task relates to the composition of the team. Team performancehas previously been computed as the sum of individual agent capabilities, e.g., the amount of resources an agent pos-sesses [38,6]. In this work, we are interested in a model of team performance that goes beyond the sum of single-agentcapabilities. We understand that there is synergy among the agents in the team, where team performance at a particu-lar task depends not only on the individual agents’ capabilities, but also on the composition of the team itself. Specificagents may have or acquire a high task-based relationship that allows them to perform better as a team than other agentswith equivalent individual capabilities but a low task-based relationship. There are many illustrations of such synergy inreal human teams, basically for any task. An example is an all-star sports team comprised of top players from around the* Corresponding author. Somchaya Liemhetcharat has graduated from Carnegie Mellon University and is now at the Institute for Infocomm Research (I2R),Singapore. Tel.: +65 6408 2000, fax: +65 6776 1378.E-mail addresses: som@ri.cmu.edu (S. Liemhetcharat), veloso@cs.cmu.edu (M. Veloso).0004-3702/$ – see front matter © 2013 Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.artint.2013.12.00242S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65world, hence individual agents with high capabilities, who may have a lower synergy as a team and perform worse than awell-trained team of individuals with lower capabilities but much higher synergy.To model task-based relationships, we introduce a connected weighted graph structure, where the vertices representthe agents, and the edges represent the task-based relationships. In such graphs, we define the level of synergy of a setof agents, as a function of the shortest path between agents. We further devise a non-binary metric of team performancebased on a Gaussian model of the individual agent capabilities. Such probabilistic variables allow us to capture the inherentvariability in team performance in a dynamic world. We show that our formulation of team performance captures manyinteresting characteristics, such as the effects of including new agents into the team.Most existing team formation approaches assume that the agent capabilities are known a priori (e.g., [48]). We aremotivated by research in ad hoc agents, that learn to collaborate with previously unknown teammates [40]. An ad hoc teamis one where the agents in the team have not collaborated with each other. Assuming an ad hoc team, we address the teamsynergy learning question as: given a set of agents with unknown capabilities, how do we model and learn the capabilitiesand synergy of the agents through observations, in order to form an effective team, i.e., a subset of the agents? A solutionto this problem will enable ad hoc teams to be applied to a variety of problems in the real world, where effective teamsneed to be composed from agents who may not have previously worked together.A motivating scenario is the urban search-and-rescue (USAR) domain. Many USAR robots have been developed by dif-ferent research groups, with a variety of hardware capabilities. When a disaster occurs, researchers from around the worldarrive with their USAR robots. Due to safety and space constraints, only a subset of these robots may be able to be deployedto the site. Since many of these researchers have not collaborated in the past, selecting an effective team is an ad hocproblem, where the agent capabilities and synergy are initially unknown. Some of these robots may have been designedto work well with other robots developed by the same group, and in some cases, robots from different sources may havesynergy in a team, e.g., a robot that clears rubble quickly so that another robot can search. Thus, it is necessary to modeland learn the synergy of these robots and select the best team of robots to be deployed.We contribute a learning algorithm that uses only observations of the performance of teams of two and three agents,in order to learn the agent capabilities and weighted graph structure of the weighted synergy graph. The learning algorithmiterates through weighted graph structures, and computes the agent capabilities using the observations. We also contributetwo team formation algorithms that uses the learned weighted synergy graph to find an effective team that solves the task.Our approach does not make many assumptions about the agents, only that observations of their performance is available,and as such our approach is applicable to many multi-agent domains.We perform extensive experiments to demonstrate that our learning algorithm effectively learns the structure of repre-sentative graph types and agent capabilities. We compare the weighted synergy graph to the unweighted synergy graph thatwe previously introduced [26], and demonstrate that the weighted synergy graph is more expressive and hence applicable tomore domains. We apply the weighted synergy graph model to the RoboCup Rescue domain (that simulates rescue robots ina USAR scenario), and show that the learned weighted synergy graph is used to form a near-optimal team, and outperformsIQ-ASyMTRe [48], a competing algorithm.In summary, the contributions of this work are:1. A novel model of multi-agent team performance, the weighted synergy graph model, where agents are vertices in a con-nected weighted graph, edges represent how well agents work together, and agent capabilities are Normally-distributedvariables;2. The definition of the synergy of a multi-robot team as a function of the weighted synergy graph model;3. A team formation algorithm that forms the optimal team in exponential time;4. A team formation algorithm that approximates the optimal team in polynomial time;5. A learning algorithm that learns a weighted synergy graph using only observations of agent teams comprising two andthree agents;6. Extensive experiments that evaluate our model and algorithms using synthetic data;7. Application of the weighted synergy graph model to the RoboCup Rescue domain.The article is organized as follows: Section 2 discusses related research in multi-robot task allocation, coalition formation,team formation, and ad hoc teams, and how it compares to our work. In Section 3, we formally define the weighted synergygraph model and our team formation algorithms. Section 4 contributes the synergy graph learning algorithm, while Section 5presents extensive learning experiments. Section 6 compares the expressiveness of the weighted and unweighted synergygraph models. Section 7 details the experiments in the RoboCup Rescue domain, and Section 8 draws conclusions.2. Related workThis section presents a review of related work, discussing the relevant domains of task allocation, coalition formation,ad hoc coordination and team formation.S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65432.1. Multi-robot task allocationMulti-robot task allocation (MRTA) is focused on the problem of allocating a set of tasks to a group of robots so as tomaximize a utility function. The MRTA problem is categorized along three axes: single-task robots (ST) versus multi-taskrobots (MT), single-robot tasks (SR) versus multi-robot tasks (MR), and instantaneous assignment (IA) versus time-extendedassignment (TA) [17].Multi-robot teams have been used for many tasks, such as in the RoboCup Rescue Simulation League [20]. A city issimulated to have had a disaster, necessitating rescue robots to help to mitigate the crisis. Civilians are located aroundthe city that have to be rescued and/or directed to safe zones, and fires that break out in the city have to be put out.The scenario is a MRTA problem (specifically ST-MR-TA), and the goal is to assign the rescue agents to different tasks in thecity effectively. The tasks appear over time, and require varying durations to be completed. The rescue robots are allocatedto tasks, and re-assigned new tasks as the old tasks are completed. There are a variety of methods to assign tasks to robots,such as using a biologically-inspired approach [12], using coalition formation with spatial and temporal constraints [35],modeling the problem as a generalized allocation problem with task interdependencies [13], and developing predictivemodels [21]. We use the RoboCup Rescue simulator and existing algorithms designed for it as a test-bed for our weightedsynergy graph model. We view the RoboCup Rescue problem as a team formation problem, where existing MRTA algorithmsare selected before the simulation starts (each MRTA algorithm controls a subset of the rescue robots during the simulation),and the goal is to form the optimal combination of MRTA algorithms to achieve maximum utility.Representing heterogeneous capabilitiesThe capabilities of heterogeneous robots have been represented in different ways. One technique defines utility as thedifference between the quality of the task execution and the expected resource cost [17]. The utility of a robot performinga task is quantified, and is general and does not impose any restrictions on the utility function. However, this generality isa double-edged sword and does not impose any structure on the tasks that would allow them to be grouped.Another technique is the resource model, where a set of resources is first defined, e.g., robot arm, camera, sonar [6].Then, each robot is defined as a vector that contains the number of each resource a robot has, e.g., 2 arms, 1 camera and 0sonar. In this model, resources are either completely interchangeable (i.e., a robot arm on one robot is equivalent to a robotarm on another) [38,44,39], or with a level of quality [6]. In these resource models, tasks are defined as a list of requiredresources, e.g., a task requires 2 robot arms, 3 cameras, and 1 sonar, and in the latter model, a minimum quality of eachresource is also defined.The service model of capabilities [38,46] is similar to the resource model, with the following distinctions: a robot is eitherable or unable to performance a service, i.e., there is no quantity or quality of a single service a robot possesses. Tasks aredefined as a list of the number of each type of service required (similar to the resource model), but in the task allocationprocess, each robot can only perform a single service, instead of providing all the services it is capable of. The authors statethat while resources can be exchanged in multi-agent systems, resources on multi-robot systems are usually physical andcannot be exchanged. Having a minimum number of resources does not necessarily imply task completion because otherconstraints such as proximity have to be represented and met. The resource model can still be applied with the addition ofconstraints, but the service model abstracts and represents these constraints succinctly.Robot capabilities are also described by schemas, i.e., inputs and outputs of information types, and robots are defined assets of schemas: the sensors of the robots provide outputs without any inputs, and other schemas are perceptual (e.g., vision,localization), motor (e.g., controlling actuators), and communication [33,43]. The task is defined as a set of desired outputs,and a team of robots is capable of completing the task if a joint plan exists that produces the outputs by chaining theschemas in the robots.We have previously introduced the concept of mutual state capabilities, where a robot’s capability in the task dependson its teammate and their joint state [25]. Such a model of capabilities captures the notion of synergy among robots, whichwe further elaborate in this work.Evaluating task allocationsThere are two main methods to quantify the performance of an allocation of tasks to robots. The first method, task-basedperformance, defines a utility gained by completing each task. The second method, team-based performance, defines thequality of performance of each robot performing the task allocated to it.Task-based performance is used in market-based techniques [8,10] and other approaches [7]. Each task is associated witha reward, which is computed based on domain-specific factors, such as the amount of unexplored space [49] or number ofitems picked up [9]. In market-based techniques, the cost of performing the task is used to form the bid, and an auctioneerassigns tasks based on robots’ bids [43]. The final performance of the task allocation is then computed based on the profit,i.e., the difference between the utility gained from the tasks and the costs incurred [9]. In some approaches, only the sumof utilities gained is used to calculate performance, and the costs are only used for the allocation process [45,43], and inothers the goal is to complete all the subtasks while minimizing the total cost [31]. The benefit of task-based performanceis that the utility gained from completing a task is independent of how the task is completed — after the allocation, eachtask is either completed or not completed, and there is no measure of how well a particular task was done, other than thecosts incurred by the robots assigned to it.44S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65In team-based performance, the quality of a completed task varies depending on the robots allocated to it, e.g., R 1 maycomplete task T 1 with a lower quality than if R2 completed T 1. The performance of the task allocation is then the differencebetween the quality of completed tasks and the costs incurred by the robots. With this formulation, the ST-SR-IA problemcan be posed as an optimal assignment problem and solved in polynomial time, or with a market-based approach [17].When each task requires more than one robot to complete, it becomes a ST-MR-IA problem, and has many similaritieswith the coalition formation problem, which we describe next. Team-based performance measures how well a task wascompleted, so the composition of a team has a larger impact beyond the costs.2.2. Coalition formationCoalition formation involves the partitioning of a set of agents A into disjoint subsets so as to maximize the overallutility. In characteristic function games, the value of each possible subset is given by a characteristic function, and thegoal is to find the optimal coalition structure to as to maximize the sum of the characteristic function of each coalition.The number of coalition structures is O (| A|| A|) and ω(| A|| A|/2), so enumerating possible coalition structures to find theoptimal is intractable [37]. Coalition formation is applicable to MRTA [38] in domains such as deciding flight targets forUAVs [16].In characteristic function games, the value of a coalition depends only on the agents within it. There has been re-cent work in coalition formation with externalities, where a coalition’s value depends on the structure of other coalitions.A logic-based representation is used for coalition formation with externalities, that is fully expressive and at least as con-cise as regular coalition formation representations [32]. Positive and negative externalities are considered separately, where+P Fsub means weakly sub-additive, so merging two coalitions decreases their joint value (or keeps it constant) while increas-−ing the values of other coalitions in the structure (or keeps them constant), and P Fsup means weakly super-additive, wheremerging a coalition increases its value and decreases the values of other coalitions (or keeps values constant) [34].Mixed externalities are also considered, where both positive and negative effects can occur [2]. Agents are defined with aset of types, and agents of some types as competitors, and the value of a coalition structure improves if they are in singletoncoalitions. Conversely, agents of the remaining types are collaborators, and the value improves if all of them are in a singlelarge coalition. Using this formulation, the authors use a branch and bound algorithm to find the best coalition structurewith guaranteed worst-case bounds.Externalities in coalitions is an interesting area because it considers how the values of coalitions are computed. In regularcoalition formation, the characteristic function defines the values of coalitions, and the function is assumed to be generaland unknown, so little work has been done to analyze possible structures in the characteristic function.2.3. Ad hoc teamsThe ad hoc problem was recently introduced, and the goal is “to create an autonomous agent that is able to efficientlyand robustly collaborate with previously unknown teammates on tasks to which they are all individually capable of con-tributing as team members” [40]. An example of an ad hoc problem is with two robots — one that is pre-programmed tofollow a certain policy, and another that has to adapt its behavior so that the pair is jointly optimal without explicit com-munication. The pre-programmed robot and the ad hoc robot can be viewed as the teacher and learner in a multi-armedbandit problem [42,3]. Similarly, a two-player game-theoretic setting is used to study how an ad hoc agent can vary itsactions so as to maximize the payoff with a best-response teammate that has varying amounts of memory of previous in-teractions [41]. The work has been extended to situations where a single ad hoc agent leads multiple teammates in selectingthe optimal joint action [1].Role assignment in an ad hoc team is considered, where an ad hoc agent has to select a role, such that it maximizesthe team’s overall utility based on its observations of its teammates [15]. An ad hoc agent in the pursuit domain has tovary its behavior to better suit the team’s objective of capturing the target, by modeling its teammates and choosing abest response [4]. In the case where the system state and joint action is fully observable, but the model of teammates isunknown, biased adaptive play can be used by an ad hoc agent to optimize the joint action of the team [47].Locker-room agreements, i.e., policies agreed upon by the team prior to evaluation, can be used to coordinate robotswithout additional communication. Robots can estimate the state of a teammate in order to decide which robot shouldapproach the ball in the robot soccer domain, and while the robots agree on the policy > 90% of the time, communicationis necessary for situations where errors in state estimation may cause policy fluctuations in the team [18].We are interested in the ad hoc team formation problem, where the agents in the team have not collaborated with eachother. The capabilities of the agents and how well they coordinate at the task are initially unknown, and the goal is to forman effective ad hoc team by selecting relevant agents to form the team.2.4. Team formationTeam formation is focused on the problem of selecting the best subset of agents that can complete a task. In the ASyMTRemodel, each robot is defined with schemas and the team is selected by composing feasible teams through planning, and thenselecting the optimal team using a heuristic [33]. The algorithm has been extended to form teams that complete multipleS. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6545tasks, using both team formation and a market-based task allocation algorithm [43]. IQ-ASyMTRe is a recent extension toASyMTRe that handles information quality [48], and we compare our proposed model to IQ-ASyMTRe.Graphs can be used to represent relationships among agents, where a subgraph of connected agents are selected tocomplete a task [14]. Similarly, by using social networks of agents, where agents have different skills, and edge weightsrepresent communication costs, the optimal team to complete the task has to cover all the required skills [22,23], or tradeoff between skills and connectivity [11]. The edges in a social network graph can also be used as constraints, where an agentis assigned a task, and must find teammates that are directly connected to it [7], or form a connected sub-network [5].We recently introduced the synergy graph model, where agents are vertices in an unweighted connected graph, and agentcapabilities are modeled as Normally-distributed variables [26]. In this work, we formally define the weighted synergy graphmodel, where weighted edges are used in the graph. We show that our model is more expressive while having the sameruntime cost, and thus it can be applied to more domains. In addition, we perform extensive experiments to demonstratethe efficacy of our learning algorithm, and also apply the weighted synergy graph model to the RoboCup Rescue domain.2.5. How our work fitsOur goal is to form effective ad hoc teams, through observations of the agents’ performance in the task and modelingthe synergistic effects among agents in the team. We build upon the related research, and focus on the following:• First, we model team-based performance based on the agent’s individual capabilities, and the synergistic effects amongmembers of the team, and not a sum of single-agent capabilities. Compared to the MRTA problem, we are interested informing a single team for a task, and not the allocation of multiple agents to multiple tasks.• Second, coalition formation focuses on how to partition a set of agents to maximize a value function, while we areinterested in modeling the value function, based on observations of the agents at the task.• Third, research in the ad hoc domain has so far focused on how a single ad hoc agent can adapt to its teammates inorder to improve task performance. We are interested in forming an effective ad hoc multi-agent team.3. Team formation at a taskIn this section, we formally define the team formation problem and our weighted synergy graph model that modelsthe synergistic effects of agents working together in a team, and contribute our team formation algorithms that use theweighted synergy graph model to form an effective team for the task.3.1. Formally defining the team formation problemWe begin with the definition of the set of agents and the definition of a team:Definition 3.1. The set of agents is A = {a1, . . . , aN }, where each an ∈ A is an agent.Definition 3.2. A team is any subset A ⊆ A.There is a task to be performed that can be accomplished with any number of agents with varying performance,and hence any subset of A is a valid team. The performance of a team is the utility attained by that team when per-forming the task, and is domain-dependent. In a dynamic world, the performance of teams of agents is non-deterministic,so multiple observations of the same team at the task may result in different values:Definition 3.3. The performance of a team A ⊆ A is P A and is non-deterministic.Definition 3.4. An observation o A is a real value corresponding to an observed utility attained by the team A ⊆ A, i.e., o Ais a sample of P A .For example, suppose that a disaster has occurred in an urban area (such as a city), and that multiple urban search-and-rescue (USAR) personnel have arrived on the scene to offer their aid. A is the set of all USAR personnel, and the taskis saving lives and minimizing damage to the city. Suppose A0 ⊆ A is a USAR team that performs the task. The perfor-= 3.4. However, due to the dynamic nature of the USAR taskmance of the team is measured and forms the observation o A0(for example, wind causing fires to spread), the observed performance of A0 may be different if the task was repeated, i.e.,o A0 would be a different number each time A0 performed the task.Since the performance of a team is non-deterministic, we define the δ-optimal team:Definition 3.5. The δ-optimal team is the team Aat least u with probability δ, and the probability of any other team A doing so is at most δ:⊆ A such that there exists some utility u where A∗δ∗δ obtains a utility of46S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65P(P A∗δ(cid:2) u) = δand P(P A (cid:2) u) (cid:3) δ ∀ A ⊆ A∗δThe goal is to find the δ-optimal team of agents A⊆ A, and we assume that δ is given as part of the domain in-formation. The δ-optimality measure was designed in order to rank non-deterministic performance; when performanceis deterministic (or only the mean is considered), comparing the performance of teams is done with the (cid:2) operator.In δ-optimality, δ determines whether a risky team or risk-averse team is preferred. For example, when δ = 12 , only themean performance is considered, and the δ-optimal team is equivalent to the optimal team with non-deterministic per-formance. When δ < 12 , a high-risk, high-reward team is preferred, i.e., one that has a low probability of attaining a highperformance. Conversely, when δ > 12 , a low-risk, low-reward team is preferred, i.e., one that has a high probability ofattaining a low performance.3.2. Modeling task-based relationshipsIn order to find the δ-optimal team A∗δ , we want to create a model of how well agents work together at the task. In thesocial networks domain, social graphs are used for team formation, where an edge between a pair of agents indicates thatthe agents have a social relationship, and the weight of the edge indicates the communication cost between them [22,11].We are interested in forming teams that perform well in a task, and hence we model the task-based relationships amongthe agents as a task-based graph, where agents are vertices in the graph and edges represent the task-based relationships.One possible approach to the task-based graph is to use a fully-connected graph, where the weight of an edge indicatesthe cost of 2 agents working together. Fig. 1a shows an example of a fully-connected task-based graph with 4 agents.The agent a1 works better with a2 than a3, as indicated by the lower edge weight of 1 between a1 and a2, compared withan edge weight of 4 between a1 and a3.A fully-connected task-based graph has a major drawback — the task-based relationships between pairs of agents arecompletely independent, since every pair of agents is connected by an edge whose weight can be arbitrary. Using the notionthat edge weights represent the cost of agents working together, we introduce the concept of transitivity in task-basedrelationships. For example, if agent a1 works very well with a2, and a2 works very well with a3, then a1 will work wellwith a3. The transitivity occurs because for a1 to work very well with a2, there should be some underlying coordinationstrategy, and similarly between a2 and a3. Assuming that the agents use the same algorithms regardless of partners (i.e., theydo not switch strategies), then a1 and a3 will be able to work well together since there is some overlap in their coordinationstrategies with a2, albeit with higher cost. We assume that agents are always able to coordinate (e.g., all agents use the samecommunication protocol), or performance is based on their joint actions (similar to game theory), so any pair of agents hassome task-based relationship.To capture this notion of task-based transitivity, we use a connected graph where the shortest distance between agentsindicates the cost of them working together. Fig. 1b shows a connected task-based graph, by modifying the graph in Fig. 1asuch that edges {a1, a3}, {a1, a4}, and {a2, a4} have been removed. However, the shortest distance between agents are equalin both Fig. 1a and Fig. 1b, and thus both graphs express the same task-based relationships.While the shortest distance between agents in the task-based graph represents the cost of agents working together,we want to explicitly model the task-based relationship. Thus, we introduce a compatibility function φ : R+ → R+, whereφ(d(ai, a j)) returns the task-based compatibility between agents ai and a j , and d(ai, a j) is the shortest distance betweenthem in the task-based graph. φ is a monotonically-decreasing function, so larger distances correspond to lower compatibil-ity. Like the value function V , φ is domain-specific, and two intuitive examples of φ are:φfraction(d) = 1dφdecay(d) = exp(cid:3)(cid:2)− d ln 2hwhere φfraction is a fraction function, and φdecay is an exponential decay function with half-life h.In [26], we assumed that the edges in the task-based graph were unweighted (having a weight of 1), and that thecompatibility function would be adjusted to capture the task-based relationship among agents. However, there are manyFig. 1. a) A fully-connected task-based graph with 4 agents, where the edge weights represent the cost of agents working together to perform the task.b) A connected task-based graph with the same 4 agents, where some edges have been removed while still preserving the pairwise distances betweenagents.S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6547Fig. 2. a) A weighted task-based graph with 3 agents. b) An unweighted task-based graph with the 3 agents (all edges have a weight of 1). If the compati-bility function in (a) is φ(d) = 1d , the task-based relationships of the agents cannot be represented with an unweighted graph and an adjusted compatibilityfunction.Fig. 3. Weighted task-based graphs with 3 agents, where the agents’ heterogeneous capabilities are attached to each vertex and are represented as a) values;b) Normally-distributed variables.combinations of weighted edges such that an adjusted compatibility function cannot capture the same relationship withunweighted edges.d , the compatibility of {a1, a2}, {a1, a3}, and {a2, a3} are then 1Fig. 2a shows one such example of a task-based graph that cannot be represented with unweighted edges. Supposeφ(d) = 15 respectively. Suppose we use an unweightedtask-based graph and find an adjusted compatibility function φ(cid:7)and d arethe shortest distance functions in the unweighted and weighted task-based graphs respectively. In an unweighted task-basedgraph, the longest possible distance between any pair agents is |A| − 1, if all the agents formed a chain. Thus, with 3agents, the greatest distance is 2. Since the compatibility function is monotonically decreasing, φ(cid:7)(2) = 18 , which is the(cid:7)(a1, a2) =lowest compatibility among a1, a2, and a3, and implies that d(cid:7)(a2, a3) = 1 (Fig. 2b). As such, no compatibility function φ(cid:7)d5 whichis impossible.(cid:7)(a1, a3) = 2. However, this in turn implies that dcan be defined, since φ(cid:7)(1) has to be equal to 1(cid:7)(ai, a j)) = φ(d(ai, a j)), where d8 and 1, such that φ(cid:7)(d3 and 13 , 1(cid:7)Thus, in this work, we use weighted edges in the task-based graph to represent how well agents perform as a team.3.3. Representing agent capabilitiesThe task-based graph and compatibility function described above model how well agents work together at a task. How-ever, heterogeneous agents have different capabilities that affect their performance at a task. The performance of a team ofagents then depends on the capabilities of the agents and their task-based relationship.One method to represent heterogeneous agent capabilities is to assign a value μi for each agent ai , where μi correspondsto the agent ai ’s mean capability at the task. In this work, we view capability as a measure of an agent’s contribution tothe team performance at a task, and not a binary (capable/incapable). As such, the mean capability refers to the averageperformance the agent contributes to the task.Fig. 3a shows an example of 3 agents a1, a2, a3, where a1 works equally well with agents a2 and a3, i.e., d(a1, a2) =d(a1, d3). Even though a1 works equally well with them, a2 is has a higher mean capability than a3. As such, the meanperformance of team {a1, a2} is greater than {a1, a3}.While values model the agents’ mean capabilities at the task, it does not capture variability. Since the agents act in adynamic, non-deterministic world, their performance varies over multiple instances. Thus, instead of a single values, we usea Normally-distributed variable to represent an agent’s capability, i.e., each agent ai is associated with a variable Ci , which isthe agent ai ’s non-deterministic capability at the task. We use a Normal distribution because it is unimodal, correspondingto the agent’s performance with a peak value, and variability as its deviation. Also, Normal distributions are widely used fortheir mathematical properties, which we exploit later.By using a Normal variable instead of a single value, we can now model how consistent an agent is at the task. Fig. 3bshows a modification of Fig. 3a, where a2 has a higher variance for its performance compared to a3. As such, dependingon δ, the team {a1, a3} may outperform {a1, a2}.48S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–653.4. Defining the weighted synergy graphWe have detailed how task-based relationships are represented with the compatibility function φ, which uses the dis-tance between agent vertices in a graph, and how agent capabilities are represented as Normally-distributed variables.In this section, we formally define the weighted synergy graph and how it is used to compute the performance of a teamof agents at a task.Definition 3.6. The weighted synergy graph is a tuple (G, C), where:• G = (V , E) is a connected weighted graph,• V = A, i.e., the set of vertices corresponds to the set of agents,• ei, j = (ai, a j, w i, j) ∈ E is an edge between agents ai , a j with weight w i, j ∈ R+• C = {C1, . . . , C N }, where Ci ∼ N (μi, σ 2i ) is agent ai ’s capability at the task.,Weighted synergy graphs are connected, so at least one path exists between any pair of vertices. The distance d(ai, a j)between any two agents ai , a j is defined to be the shortest distance between them in the graph.Using the weighted synergy graph, we quantify the performance of a pair of agents:Definition 3.7. The pairwise synergy S2(ai, a j) between two agents ai , a j in a weighted synergy graph is Ci, j = φ(d(ai, a j)) ·(Ci + C j), where d(ai, a j) is the shortest distance between ai and a j in the weighted synergy graph, and φ : R+ → R+isthe compatibility function.We assume that Ci and C j are independent for all i, j, and so the summation Ci + C j in the pairwise synergy function canbe performed easily. This assumption is reasonable as the effect of agents working in a team is captured by the compatibilityfunction and their distance in the weighted synergy graph, so the variables representing their individual capabilities (Ci, C j )are independent.The pairwise synergy function between any two agents always exists, since we assume that the weighted graph is con-nected (there is always a shortest distance in the graph between any pair of agents), and that the task can be accomplishedwith any number of agents (Section 3.1).We use the graph structure to model the synergy among agents, specifically using the edges (that connect two agents)to compute the shortest distance between pairs of agents, and hence the pairwise synergy is the building block for thesynergy of a team of agents. Using the pairwise synergy function, we now define the performance of a team of agents:Definition 3.8. The synergy S( A) of a set of agents A ⊆ A in a weighted synergy graph is the average of the pairwisesynergy of its components, i.e.,(cid:6)(cid:5) ·S2(ai, a j).1(cid:4)| A|2{ai ,a j }∈ AUsing the definitions above, the synergy of a team A is a Normally-distributed random variable C A ∼ N (μ A, σ 2A ).In particular:μ A = 1(cid:4)| A|(cid:5)2σ 2A= 1(cid:4)| A|(cid:5)22(cid:7)(cid:5)(cid:4)d(ai, a j)φ· (μi + μ j)ai ,a j ∈ A(cid:7)ai ,a j ∈ A(cid:5)(cid:4)d(ai, a j)φ2 ·(cid:4)(cid:5)σ 2i+ σ 2j(3.1)(3.2)The synergy of a team A ⊆ A depends critically on the capabilities of the agents in A, and the pairwise distancesbetween them. We illustrate how synergy changes based on the team composition below.3.4.1. A weighted synergy graph exampleWhile the synergy function S is linear and takes the average of pairwise synergy, many interesting agent relationshipsare modeled. Fig. 4 shows an example of a weighted synergy graph with five agents in a rescue task.a1, a2 and a3 are personnel in the ambulance, namely the agent that carries the stretcher, the agent that performs CPR,and the agent that drives the ambulance respectively. Since these three agents have trained as a team, their task-based rela-tionship is very high, and so the distance between them in the weighted synergy graph is very low (a distance of 1 betweenany pair of the three). As heterogeneous agents, their individual capabilities are different. For example, the capability of a2,the CPR agent, has a high mean (that reflects the high payoff CPR provides to the task) and correspondingly high variance(that reflects that CPR does not always succeed). As each agent is individually capable of saving lives (i.e., the driver agentand stretcher agent can also perform first aid), the task can be completed with any subset of agents.a4 is an intern at the hospital, and a5 is a surgeon there. The surgeon’s capability is both better and more consistentthan the CPR agent’s: C5 ∼ N (35.7, 3.3) versus C2 ∼ N (20.3, 10.9), reflecting the surgeon’s skills. The surgeon has a highS. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6549Fig. 4. An example of a weighted synergy graph modeling capabilities and the task-based relationships of a group of agents in a rescue task.Fig. 5. Three equivalent weighted synergy graphs, i.e., the shortest distance between pairs of agents is equivalent in the three graphs.task-based relationship with the CPR agent (reflected by a distance of 2 in the graph) since both agents frequently worktogether, but is not as high as within the personnel in the ambulance (i.e., a1, a2 and a3) that have distances of 1 withone another. The intern, a4, has the lowest mean capability and second-highest variance, reflecting its poor skill at the task.Further, the intern has only worked with the surgeon (on operations in the hospital) and the ambulance driver (to coordinatethe arrival of patients). As such, the intern has edges with weights of 3 with both the surgeon a5 and the driver a3. Sincethe intern has no direct edges with the other agents (e.g., the CPR agent a2), the task-based relationship between them willbe transitive. For example, the task-based relationship of a2 and a4 uses a distance of 4 (1 from the edge e{a2,a3} and 3 fromthe edge e{a3,a4}). The transitivity reflects that the intern works less well with the CPR agent than the driver agent, by thedistance of edge e{a2,a3}.Using Definitions 3.7 and 3.8, and assuming that the compatibility function φ(d) = 1d , we can compute the synergy of theagents. The team {a1, a2, a3} (the agents in the ambulance) has a synergy of C{a1,a2,a3} ∼ N (17.8, 2.8). Since the surgeona5 is more capable than a2 (the CPR agent), one might consider replacing a2 with a5. However, the synergy would beC{a1,a3,a5} ∼ N (12.1, 0.3), which has a lower mean than the team {a1, a2, a3}. The lower mean is due to the task-basedrelationships among the agents — the other agents in the ambulance (the stretcher agent and the driver agent) work welland perform better with the CPR agent than with the surgeon.When these four agents are in a team, their synergy is C{a1,a2,a3,a5} ∼ N (17.8, 0.8), which shows that the additionof the surgeon agent (instead of replacing the CPR agent) potentially benefits the team by lowering the variance of theirperformance. However, adding agents does not always improve the team performance — the team consisting of all the agentshas a synergy C{a1,a2,a3,a4,a5} ∼ N (13.0, 0.3), since the intern agent has a low capability and a poor task-based relationshipwith the other agents, as reflected by its high edge weights to other agents.Thus, the weighted synergy graph captures interesting and complex relationships among agents, where the compositionof the team makes a significant difference in the overall task performance.3.4.2. Equivalence in weighted synergy graphsFig. 5 shows three examples of weighted synergy graphs with 3 agents, such that d(a1, a2) = 1, d(a1, a3) = 2,and d(a2, a3) = 3. In Fig. 5a, only 2 edges are present while 3 edges are present in Figs. 5b and 5c. In particular, the edgee2,3 of Fig. 5c is not used, since a shorter path exists between a2 and a3 using edges e1,2 and e1,3. Thus, in general, morethan one graph structure can be used to define the distance relationship among agents.Definition 3.9. Weighted synergy graphs S = (G, C) and S(cid:7) = (G(cid:7), C(cid:7)) are equivalent if:(cid:7),• V = V(cid:7)• C = C• d(ai, a j) in graph G = d(ai, a j) in graph G,(cid:7) ∀ai, a j ∈ A.Since the shortest distance between all pairs of agents are identical, their compatibility (as computed by φ) is identical,and hence the synergy of a team of agents are equal given equivalent weighted synergy graphs. In this work, we do notdistinguish between equivalent weighted synergy graphs.50S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–653.5. Assumptions of the weighted synergy graph modelThe weighted synergy graph model defined above captures the task-based relationships among agents and is used tocompute the synergy of agent teams using the distances in the weighted graph and the agent capabilities. We now list theassumptions of the model, and a short discussion of the rationales behind the assumptions and how the assumptions canbe relaxed:1. Team performance is Normally-distributed;2. The synergy of a team is a function of the pairwise synergies;3. Task-based relationships are transitive and modeled via shortest distance in the graph.Regarding Assumption 1, Section 3.3 explains the rationale of using Normal distributions to model team performance.If the actual performance was from some other distribution, the weighted synergy graph model can be modified to use thatdistribution or any arbitrary distribution, although the computation of pairwise synergy S2 and synergy S could be morecomplex (by adding arbitrary distributions together).Assumption 2 comes about from the graph-based nature of the weighted synergy graph model. Since edges involvetwo agents, we derive the synergy function S from the pairwise synergy S2. Further, the task can be accomplished by anynumber of agents, so pairwise synergy between any two agents always exists. If the assumption is relaxed, the synergygraph can be extended to use hyperedges (edges between more than two agents), and S can be updated to reflect thechange.The weighted synergy graph model assumes transitivity in the task-based relationship (Assumption 3). Our work focuseson tasks where this assumption holds — we believe that this is the case for many tasks involving software agents (for ex-ample in the RoboCup Rescue domain, which we elaborate later). In cases where the assumption does not hold, the modelcan be updated to use other measures other than the shortest distance between agents, and is the focus of our future work.3.6. Solving the team formation problem∗δWe defer how a weighted synergy graph is learned from observations of performance to the next section. In this section,∗δ such thatwe explain how to use a weighted synergy graph to form the δ-optimal team for the task, i.e., the team AP(P A(cid:2) u) = δ and P(P A (cid:2) u) (cid:3) δ ∀ A ⊆ A.Using the weighted synergy graph model and the synergy equations, we can compute the synergy of a team of agentsA ⊆ A. However, the synergy computed is a Normally-distributed variable, and we need to rank such variables to chooseone possible team over another.To do so, we use an evaluation function (the V function introduced by us in [24] that we rename to Evaluate here)that converts a Normally-distributed variable into a real number using a risk factor ρ ∈ (0, 1):Evaluate(X, ρ) = μ X + σ X · Φwhere X ∼ N (μ X , σ 2(3.3)X ) and Φ−1 is the inverse of the cumulative distribution function of the standard Normal distribution.In particular, for any Normally-distributed variable X , Evaluate( X, 1 − δ) returns a value v X such that P ( X (cid:2) v X ) = δ.−1(ρ)Theorem 3.10. Let A, AIf v A (cid:2) v A(cid:7) , then P (C A(cid:7) (cid:2) v A) (cid:3) δ.(cid:7) ⊆ A, and C A = S( A), C A(cid:7) = S( A(cid:7)). Let Evaluate(C A, 1 − δ) = v A and Evaluate(C A(cid:7) , 1 − δ) = v A(cid:7) .Proof. P (C A(cid:7) (cid:2) v(cid:7)A) = δ and v A (cid:2) v(cid:7)A⇒ P (C A(cid:7) (cid:2) v A) (cid:3) P (C A(cid:7) (cid:2) v A(cid:7) ) ⇒ P (C A(cid:7) (cid:2) v A) (cid:3) δ. (cid:2)Corollary 3.11. A∗δ= argmax A⊆AEvaluate(S( A), 1 − δ).Proof. Let Aδ = argmax A⊆AEvaluate(S( A), 1 − δ). Let Evaluate(S( Aδ), 1 − δ) = v. ∀ A ⊆ A, P (S( A) (cid:2) v) (cid:3) δ (fromTheorem 3.10) ∴ A= Aδ . (cid:2)∗δEvaluate returns a real number that we use to rank possible teams and find the δ-optimal team A∗δ , since the teamthat returns the highest value from Evaluate corresponds to the δ-optimal team. We now contribute two team forma-tion algorithms: FindδOptimalTeam, a branch-and-bound algorithm that finds the δ-optimal team in exponential time,and ApproxδOptimalTeam, that approximates that δ-optimal team in polynomial time. Both algorithms assume that| is known and given as a parameter to the algorithm. If n is unknown, then the algorithms are run iteratively forn = | An = 1, . . . , N and the best-performing team is selected.∗δ3.6.1. Finding the δ-optimal teamOur firstteam.Algorithm 1 shows the pseudo-code of the algorithm. The inputs to the algorithm are n (the size of the desired team),team formation algorithm, FindδOptimalTeam, uses branch-and-bound to find the δ-optimalS. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6551Algorithm 1: Find the δ-optimal team of size nelseend ifreturn ( Abest, v best)v A ← Evaluate(S( A), 1 − δ)if v A (cid:2) v max thenreturn ( A, v A )FindδOptimalTeam(n, δ, S, A, Abest, v best)1: if | A| = n then2:3:4:5:6:7:8: end if9: n ← maxai ∈ A (i)10: for i = n + 1 to N do11:12:13:14:15:16:17: end for18: return ( Abest, v best)end ifAnext ← A ∪ {ai }(nextmin, nextmax) ← CalculateBounds(n, δ, S, Anext)if nextmax > v best thenv best ← max (v best, nextmin)( Abest, v best) ← FindδOptimalTeam(n, δ, S, Anext, Abest, v best)δ (for δ-optimality), S (the weighted synergy graph), A (the team being considered), Abest (the best team found so far),and v best (the value of the best team found so far). | A| (cid:3) n during the execution of the algorithm, and contains the fixedmembers of the team, e.g., if n = 5 and A = {a1, a2}, then FindδOptimalTeam returns the optimal team of 5 agents giventhat {a1, a2} are in the team. The initial call to FindδOptimalTeam sets A = Abest = ∅ and v best = −∞.Lines 1–8 describe the base case when the team is fully-formed — the value of the team is computed with Evaluate,and the team is returned if its value is greater than the current best team. Otherwise, branching and bounding is performed.The branching occurs as agents are added to A and the algorithm is recursively called (lines 11 and 15). The bounds of theteam are computed with CalculateBounds (described below), and the team is pruned if the maximum of the bound(nextmax) is lower than the current best value v best (line 13).To compute the bounds of a team given a synergy graph, CalculateBounds uses the following heuristic. The mini-mum and maximum pairwise distance between vertices in the weighted synergy graph (excluding pairs of the selected teamAnext) are computed, as well as the maximum and minimum agent capabilities (excluding the agents in Anext). The maxi-mum bound is computed by using the synergy function S and assuming that all distances with undetermined agents are theminimum distance, and agent capabilities are maximum. Similarly, the minimum bound is computed using the maximumdistances and minimum agent capabilities.Theorem 3.12. Finding the δ-optimal team is NP-hard.Proof. We reduce the max-clique problem, which is NP-complete [19], to the weighted synergy graph team formationproblem.Suppose that G = (V , E) is an unweighted graph, and the goal is to determine if a clique of at least size n exists, i.e.,a subgraph of n vertices that is fully connected.We define a connected weighted graph G(cid:7)weight 2 such that the graph Gis connected. The number and identity of these edges are unimportant.Using the graph GWe define the compatibility function as: φ(d) =, we create a weighted synergy graph S = (G(cid:8)(cid:7)1 if d(cid:3)10 otherwise(cid:7), C) such that all Ci ∼ N (1, 1) ∈ C ., and define δ = 12 , i.e., only the means of the capabilities(cid:7) = (V , E(cid:7)) where ∀e = (v, v(cid:7)) ∈ E, ∃e(cid:7) = (v, v(cid:7), 1) ∈ E(cid:7). Further, we add edges ofmatter.S( A) = 1(cid:4)| A|(cid:5)2= 1(cid:4)| A|(cid:5)2= 1(cid:4)| A|(cid:5)2= 1(cid:4)| A|(cid:5)2= 1(cid:4)| A|2(cid:7)(cid:5)(cid:4)a, a(cid:7)S2(from Eq. (3.8))a,a(cid:7)∈ A(cid:7)a,a(cid:7)∈ A(cid:7)a,a(cid:7)∈ A(cid:4)d(cid:4)a, a(cid:7)φ(cid:5)(cid:5)(Ca + Ca(cid:7) )(from Eq. (3.7))(cid:5)(cid:5)(cid:4)d(cid:4)a, a(cid:7)2φ(cid:2)since δ = 12and Ca = Ca(cid:7) ∼ N (1, 1)(cid:3)(cid:7)(cid:4)2since φ(d) = 1 if d (cid:3) 1 and 0 otherwise(cid:5)a,a(cid:7)∈ A s.t. d(a,a(cid:7))=1(cid:9)(cid:10)(cid:5)(cid:4)(cid:9)a, ae(cid:7) =, 1(cid:7)(cid:5) 2(cid:7)∈ Es.t. a, a(cid:7) ∈ A(cid:11)(cid:9)(cid:9)52S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65Algorithm 2: Approximate the δ-optimal team of size nAneighbor ← NeighborTeam( Abest)v neighbor ← Evaluate(S( Aneighbor), 1 − δ)if accept(v best, v neighbor) thenApproxδOptimalTeam(n, δ, S)1: Abest ← RandomTeam(S, n)2: v best ← Evaluate(S( Abest), 1 − δ)3: repeat4:5:6:7:8:9:end if10: until done()11: return AbestAbest ← Aneighborv best ← v neighborHence, a clique in G of size n corresponds to a having a synergy of 1(cid:4)(cid:7)in Gthat is fully connected with edges of weight 1.(cid:4)n(cid:5) 22n2(cid:5)= 2, since a clique in G corresponds to a cliqueFurther, a clique has maximum synergy (compared to other subsets of size n) since it has the maximum number of edgeswith weight 1.Thus, determining if the δ-optimal team of size n has synergy 2 is at least as hard as determining if a clique of n exists,and so finding the δ-optimal team is NP-hard. (cid:2)Finding the δ-optimal team is NP-hard, and branch-and-bound can fully explore the space in the worst case. As such,the runtime of FindδOptimalTeam is O (Nn). If n is unknown, then the algorithm is run for increasing n for a totalruntime of O (N N ).3.6.2. Approximating the δ-optimal teamFinding the δ-optimal team takes exponential time in the worst case, and in many situations, a near-optimal team issufficient to solve the problem. Algorithm 2 shows the pseudo-code for ApproxδOptimalTeam, that approximates theδ-optimal team of size n, given δ and a weighted synergy graph S.Algorithm 2 first begins by generating a random team of size n, which is performed by randomly selecting n agentsfrom A. The value of the team is then computed with the Evaluate function. The random team and its value thus formsthe initial guess of the algorithm.Next, the algorithm begins its approximation loop. Lines 3–10 of Algorithm 2 show a general approximation algorithm(with functions accept and done) to illustrate that our algorithm is compatible with most approximation algorithms.In this work, we use simulated annealing but other approximation algorithms (such as hill-climbing) are suitable as well.In simulated annealing, the done function would check if the desired number of iterations has been run, and the acceptfunction would accept a neighbor based on the temperature schedule (computed from the current iteration number) andthe difference in Evaluate scores of the current best guess and its neighbor.NeighborTeam( Abest) randomly swaps one selected agent a ∈ Abest with an unselected one a(cid:7) ∈ A \ Abest. In this way,neighbor teams are generated from the current best estimate Abest so as to effectively explore the space of possible teamsof size n. The value of the neighbor team v neighbor is computed with Evaluate, and the team is accepted or rejected basedon the criteria of the approximation algorithm (e.g., simulated annealing uses a temperature schedule).Thus, Algorithm 2 finds an approximation to the δ-optimal team given its size n. The algorithm runs in O (n2) (the syn-ergy function S takes O (n2) and simulated annealing runs a constant number of iterations) if n is known. Otherwise,the algorithm is run iteratively for increasing n and has total runtime of O (N 3). In comparison, a brute-force algorithmwould take O () if n is known, and O (2N ) otherwise.(cid:4)(cid:5)Nn3.6.3. Comparing the team formation algorithmsTo evaluate both team formation algorithms, and compare their performance (amount of the search space explored,and value of the formed team), we generated random weighted synergy graphs. We varied the number of agents in thegraph from 10 to 15, and randomly created 1000 connected weighted graph structures where each edge weight varied from1 to 5. For each weighted graph structure generated, the agent capabilities Ci ∼ N (μi, σ 2i ) were also randomly generated,such that μi ∈ (50, 150) and σ 2(cid:16), so that the search space is as∈ (0, 1002). The size of the desired team was set to (cid:15) Ni2large as possible.FindδOptimalTeam always finds the optimal team, so we were interested in evaluating how many times Calcu-lateBounds and Evaluate were called. For ApproxδOptimalTeam, we ran simulated annealing for 1000 iterations(so 1000 calls to Evaluate were made), and we were interested in evaluating the quality of the team formed.Table 1 shows the results of our comparisons. The effectiveness of the formed team is expressed as a value in [0, 1],where 0 means the worst possible team (with the minimum value), and 1 means the optimal team (with the maximumvalue):S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6553Table 1The number of evaluations done by FindδOptimalTeam and ApproxδOptimalTeam to compute and approximate the δ-optimal team respectively,and the quality of the team found (where 0 means the worst team and 1 is the optimal team).# agentsFindδOptimalTeamApproxδOptimalTeam101112131415# evaluations340 ± 69545 ± 1251216 ± 2541993 ± 4494439 ± 9327307 ± 1694Team effectiveness# evaluations111111100010001000100010001000Effectiveness( A) =Evaluate(S( A), 1 − δ) − Evaluate(S( Amin∗δ ), 1 − δ) − Evaluate(S( AminEvaluate(S( Aδδ), 1 − δ)), 1 − δ)Team effectiveness0.996 ± 0.0210.992 ± 0.0340.997 ± 0.0200.996 ± 0.0210.998 ± 0.0100.998 ± 0.013(3.4)FindδOptimalTeam finds the optimal team but evaluates a large number of agent teams — the number of calls toCalculateBounds and Evaluate are greater than the size of the search space. In comparison, running ApproxδOp-timalTeam for a fixed number of iterations (1000) finds the δ-optimal team or a team very close to optimal most ofthe time. When there are 12 or more agents, ApproxδOptimalTeam performs competitively with FindδOptimalTeamwhile evaluating a smaller amount of teams (only 1000). We believe the high performance of FindδOptimalTeam isbecause the neighbor generation allows for good exploration of the space — a team with a high score will remain witha high score when a single member is swapped. Further, the agent capabilities were uniformly sampled within a rangeand distances were randomly generated; ApproxδOptimalTeam may not perform as well if there are outliers in theteam (e.g., an agent with extremely high capabilities and low pairwise distances). Comparatively, in FindδOptimalTeam,the bounds of performance are computed when some agents in the team is fixed; the bounds are large when few agentsare fixed and only become narrow as most agents are fixed. As such, pruning can only occur towards the bottom of thebranch-and-bound search tree and so a large search space is required. Thus, we use ApproxδOptimalTeam for the laterparts of this work.4. Learning the weighted synergy graphA(cid:12)We have formally defined the weighted synergy graph and our team formation algorithm to approximate the δ-optimalteam. However, the team formation algorithm assumes the existence of a weighted synergy graph. In this section, we con-tribute our learning algorithm that learns a weighted synergy graph using only observations of the performance of teams ofagents in A.A∈A s.t. | A|=2Let A2 ⊂ 2such that A2 ={ A}. Similarly, let A3 ⊂ 2{ A}. Hence,A2 and A3 are the sets of all pairs and triples of agents respectively. Our learning algorithm uses the observations of theagents in A2,3 = A2 ∪ A3. Specifically, let O be the set of observations, where ∀ A ∈ A2,3, ∃o A,1, . . . , o A,M such that eacho A,m is an observation of the performance of the team A at the task. Since any subset of agents will attain a performancevalue at the task, and the synergy function S is computed from the pairwise synergy function S2, the observation set Ois sufficient for learning. In particular, A2 provides information about the shortest distance between pairs of agents andthe agents’ capabilities using the pairwise synergy function S2 (Definition 3.7). However, there are multiple solutions forany pairwise synergy (increasing capabilities versus decreasing distances), and A3 provides information about the overallstructure of the graph using the synergy function S (Definition 3.8), and provides additional constraints to the learningproblem.such that A3 =A∈A s.t. | A|=3(cid:12)AAlgorithm 3 shows the pseudo-code of our learning algorithm. The algorithm first generates a random weighted graphstructure with N = |A| vertices, and the function LearnCapabilities estimates the capabilities of the agents using theweighted graph structure and observation set O . We elaborate on LearnCapabilities later in this section. The weightedgraph structure and estimated capabilities then form an initial guess S of the weighted synergy graph, and the log-likelihoodof the observations in O given S is computed with LogLikelihood.4.1. Learning the weighted synergy graph structureThe learning algorithm iteratively improves the learned weighted synergy graph: using the current estimate of theweighted synergy graph, the function NeighborStructure generates a neighbor weighted graph structure by using thecurrent graph structure and performing one of four possible actions:1. Increase the weight of a random edge by 12. Decrease the weight of a random edge by 13. Add an edge of random weight between two vertices4. Remove a random edge that does not disconnect the graph54S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65Algorithm 3: Learn a weighted synergy graph(cid:7)) ← NeighborStructure(G)LearnWeightedSynergyGraph( O )1: G = (V , E) ← RandomStructure(A)2: C ← LearnCapabilities(G, O )3: S ← (G, C)4: score ← LogLikelihood(S, O )5: repeat6:7:8:9:10:11:12:13:14: until done()15: S ← PruneEdges(S)16: return S(cid:7) = (V , EG(cid:7) ← LearnCapabilities(GC(cid:7) ← (GSscoreif accept(score, score(cid:7) ← LogLikelihood(S(cid:7)) then(cid:7)S ← Sscore ← score(cid:7), O )end if(cid:7), C(cid:7), O )(cid:7))(cid:7)Fig. 6. The four possible actions used to generate neighbor weighted graph structures for the learning algorithm.Fig. 6 illustrates these four actions. While the edge weights in the weighted synergy graph definition are real numbers,NeighborStructure only generates structures with integer weights. We use integer weights in the learning algorithm,as they provide a close approximation while still allowing discrete steps in neighbor generation. Higher accuracy can beachieved by increasing/decreasing the edge weights with smaller step size values. However, decreasing the step size in-creases the space of possible weighted graphs, so it is a trade-off that has to be managed.4.2. Learning the agent capabilitiesLearnCapabilities uses the generated weighted synergy graph structure and the observation set O to learn thecapabilities of the agents. Algorithm 4 shows how the capabilities are learned.First, distributions are estimated from the observation set O . For every team A ∈ A2,3, there are m observationso A,1, . . . , o A,m of the team’s performance. An unbiased estimator then estimates the distribution N (μ, σ 2) that representsA’s performance and forms D where:∀ A ∈ A2,3,(cid:4)∃A, D A ∼ N(cid:4)μ A, σ 2A(cid:5)(cid:5)∈ D(4.1)Next, the matrices M1, M2, B1, B2 are initialized as zeros. The matrices are used to form the matrix equations M1 X1 = B1]T are the means and variances of the agent capabilities1 , . . . , σ 2and M2 X2 = B2, where X1 = [μ1, . . . , μN ]T and X2 = [σ 2respectively.NS. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6555Algorithm 4: Learn capabilities from weighted synergy graph structure)) ∈ D doLearnCapabilities(G, O )1: D ← EstimateDistributions(O )2: M1 ← [0]|D|×N3: M2 ← [0]|D|×N4: B 1 ← [0]|D|×15: B 2 ← [0]|D|×16: α ← 07: for all ( A2 = {ai , a j }, N (μ A2 , σ 2A28:9:10:11:12:13:14:15:16: end for17: for all ( A3 = {ai , a j , ak}, N (μ A3 , σ 2A318:19:20:21:α ← α + 1di, j ← distance(ai , a j, G)M1(α, i) ← φ(di, j )M1(α, j) ← φ(di, j )M2(α, i) ← φ(di, j )2M2(α, j) ← φ(di, j )2B 1(α) ← μ A2B 2(α) ← σ 2A2α ← α + 1di, j ← distance(ai , a j , G)di,k ← distance(ai , ak, G)d j,k ← distance(a j , ak, G)M1(α, i) ← 1M1(α, j) ← 1M1(α, k) ← 1M2(α, i) ← 1M2(α, j) ← 1M2(α, k) ← 1B 1(α) ← μ A3B 2(α) ← σ 2A33 (φ(di, j ) + φ(di,k))3 (φ(di, j ) + φ(d j,k))3 (φ(di,k) + φ(d j,k))9 (φ(di, j )2 + φ(di,k)2)9 (φ(di, j )2 + φ(d j,k)2)9 (φ(di,k)2 + φ(d j,k)2)22:23:24:25:26:27:28:29:30: end for31: (μ1, . . . , μN ) ← LeastSquares(M1, B 1)N ) ← LeastSquares(M2, B 2)32: (σ 21 , . . . , σ 233: C ← (C1 ∼ N (μ1, σ 234: return C1 ), . . . , C N ∼ N (μN , σ 2N )))) ∈ D doFig. 7. The capabilities of agents are learned from the observation set and a weighted synergy graph structure.Lines 7–30 of Algorithm 4 show how the matrices are filled in, where the counter α keeps track of the row number of thematrices. The distance between agents are computed using the weighted graph structure G, and used by the compatibilityfunction φ. Lines 10–13 and 22–27 use the synergy equations in Definitions 3.7 and 3.8 to compute the contribution of eachagent.56S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65For example, Fig. 7 shows an example weighted synergy graph structure, and the process of Algorithm 4. We will usethe example weighted synergy graph and the team {a1, a2} to explain how the matrices are set. Suppose the compatibility2 (C1 + C2). Furthermore, the observationsfunction is φ(d) = 1involving {a1, a2} in the observation set are used to estimate the distribution N (μ{a1,a2}, σ 2{a1,a2}), and two equations aregenerated:d . Using Definition 3.7, the team {a1, a2} forms the equation 112122(μ1 + μ2) = μ{a1,a2}(cid:5)(cid:4)σ 21+ σ 22= σ 2{a1,a2}These two equations are separately formed and evaluated because the distributions Ci ∈ C are independent. Since thedistance between a1 and a2 is 2, di, j = 2 in Algorithm 4 (Line 9), and φ(di, j) = 12 , the values of columns 1 and 2 of M1 areset to 122 using the second equation.The values of the corresponding rows of B 1 and B2 are set to be μ{a1,a2} and σ 2{a1,a2} respectively (Lines 14–15). A similarprocess occurs for observations involving three agents using the synergy function S.2 (Lines 10–11). Similarly, Lines 12–13 sets columns 1 and 2 of M2 to be φ(di, j)2 = 1Once M1, M2, B1, and B2 have been filled in by iterating through all teams with two and three agents and filling in thematrices, a least-squares solver is used to solve for the means and variances of the agent capabilities. These then form theagent capabilities C = {C1, . . . , C N } that are returned by the function.Algorithm 4 exploits the fact that the agent capabilities are Normally-distributed and mutually independent. If the ca-pabilities were from a different distribution, then a different technique, such as solving log-likelihood expressions, may berequired.4.3. Calculating the log-likelihood and accepting neighbor candidatesThe weighted synergy graph structure and learned capabilities are combined to form a weighted synergy graph. The func-tion LogLikelihood computes the sum of log-likelihood of every observation o ∈ O given a synergy graph S. Everyobservation is o = ( A, p) where A ⊆ A is an agent team, and p ∈ R is the observed performance of the team. Using the syn-ergy graph S and the synergy function S, the distribution of A’s performance (N (μ A, σ 2A )) is computed using Definition 3.8.)). The sum ofThe log-likelihood of o is defined as the log-likelihood of p in this distribution, i.e., log(exp( (p−μ A )21√σ A2π2σ 2Alog-likelihood of the observations computed by LogLikelihood gives a measure of how closely a synergy graph matchesthe observations, where a higher sum log-likelihood indicates a better match.The score of the current best weighted synergy graph is compared to the score of the neighbor weighted synergy graph,and accepted by on the approximation algorithm used. For example, when simulated annealing is used, the difference inthe scores is compared to the temperature schedule.Hence, our learning algorithm iteratively improves the weighted synergy graph structure and learns the agent capabilitiesthat best match the observations given.5. Evaluating the learning algorithmIn this section, we describe the extensive experiments that show the efficacy of our learning algorithm. We first gen-erate weighted synergy graphs that are hidden from the learning algorithm. The observation set O used for training, i.e.,the performance of teams of two and three agents, is extracted from the hidden model, and then used to learn a newweighted synergy graph, which we compare against the hidden one. For each team of two or three agents, 30 observationsare generated, following the distribution of their performance in the hidden model.In order to quantitatively measure how well the learning algorithm performs, we used the log-likelihood of data giventhe weighted synergy graph. During training, the log-likelihood of the training examples is used to determine whether or notto accept a neighbor weighted synergy graph. In addition, a set of test data is generated, that consists of the performance ofteams of four or more agents. Thus, the test data contains information that is never seen or used by the learning algorithm.We measured the log-likelihood of the test data given the learned weighted synergy graph in each iteration of simulatedannealing. However, because each trial uses a randomly generated hidden graph, log-likelihood values vary from trial totrial. Thus, we scaled the log-likelihood numbers to be between 0 and 1, where 0 is the log-likelihood of the initial guess,and 1 means that the log-likelihood of the learned weighted synergy graph matches that of the hidden one used to generatethe data.5.1. Learning representative graph structuresIn our first set of experiments, the hidden weighted synergy graphs were of three representative structure types — chain,loop, and star. The learning algorithm does not know the structure type of the hidden weighted synergy graph, and insteadstarts with a random graph structure. Fig. 8 shows the hidden, initial and final weighted synergy graphs for each of theS. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6557Fig. 8. Results of the learning algorithm using representative graph structure types. The agent capabilities are not shown, and the vertices are laid out forvisual purposes. a) Examples of weighted synergy graphs with 5 agents generated to form representative structure types. b) The initial randomly-generatedweighted synergy graph of the learning algorithm. c) Learned weighted synergy graphs corresponding to the weighted synergy graphs in (a), after 1000iterations of simulated annealing.Fig. 9. Performance of the learning algorithm with different weighted synergy graph structure types, averaged over 100 trials.representative structure types. When the hidden graphs are generated, the edge weights are random integers in the range[1, 10]. The limits were chosen to provide reasonable bounds for the learning algorithm’s exploration of structures, whileallowing for a significant difference in compatibility among agents.It is very interesting that the structure learned closely matches the hidden graph, even though the learning algorithmhas no prior regarding such structures. The algorithm randomly generates a graph by creating edges between all possiblepairs of vertices with 50% probability. Fig. 9 shows the learning curves of simulated annealing for 1000 iterations with 10agents and averaged over 100 trials per structure type. While the scaled log-likelihood of star rises very quickly compared toloop and chain, the latter two outperform star after 600 iterations. The final score of star, loop, and chain are 0.93, 0.95 and0.96 respectively, which shows that the learned weighted synergy graphs closely matches the hidden ones. The experimentswere run in Java using MATLAB as the least-squares solver, on an Intel Quad-Core 2.4 GHz machine. On average, the learningalgorithm took 20.6 ± 0.3 seconds to perform 1000 iterations of simulated annealing to learn the weighted synergy graphwith 10 agents, and was consistent across the structure types.5.2. Learning random weighted synergy graphsIn the second set of experiments, we randomly generated weighted synergy graphs, to further explore the space ofpossible graph structures. We varied the number of agents from 10 to 15, and varied the agent capabilities C with ascale-factor γ . Each agent capability Ci ∼ N (μi, σ 2∈ (0, γ ). Thus,i ) was randomly generated such that μi ∈ ( γ2 ) and σ 22 , 3γi58S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65Fig. 10. Performance of the learning algorithm on random graph structures with varying number of agents, averaged over 100 trials.Table 2Scaled log-likelihood of the learned weighted synergy graphs, varying the number of agents, and ρ, the scale-factor of agent capabilities.# agentsScale-factor of agent capabilities (γ )1011121314151.00.960.960.930.930.900.892.50.950.950.930.930.930.905.00.970.950.940.930.930.917.50.970.950.930.920.920.9110.00.960.950.940.930.910.91Fig. 11. Learning curves of two compatibility functions — φdecay and φfraction, averaged over 100 trials.a higher value of γ creates a larger range of possible agent capabilities, and we wanted to investigate if it would have anyeffect on the learning algorithm.Fig. 10 shows the performance of the learning algorithm with a varying number of agents, and γ = 10, averaged over 100trials per number of agents. The shape of the learning curve is consistent, and the performance of the learning algorithmdecreases slightly as the number of agents increases. Table 2 shows the final score of the learned weighted synergy graphas the number of agents and scale-factor γ varies. In all cases, the score is 0.89 or higher, which indicates that the learnedweighted synergy graph closely matches the hidden one. The scale-factor γ has little to no effect of the final learningoutcome, while an increase in the number of agents decreases the score slightly.In the experiments above, we used the compatibility function φfraction(d) = 1d . Other compatibility functions are possible,such as φdecay(d) = exp(− d ln 23 ), which is an exponential decay function. Fig. 11 shows the learning curves of the learningalgorithm with the two compatibility functions. Although φfraction increases more slowly than φdecay, the final score of thealgorithm is similar after all 1000 iterations, which indicates that while the compatibility function has an effect on thelearning rate, the final outcome is similar.S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6559Fig. 12. Effectiveness of teams found in the learned weighted synergy graph, and learned unweighted synergy graphs, using simulated annealing with 1000iterations. The compatibility function was φfraction(d) = 1d .6. Evaluating the weighted synergy graph modelIn the previous section, we evaluated our learning algorithm and showed that it learns representative weighted graphstructures and attains high log-likelihood when learning from randomly generated weighted synergy graphs. In this section,we evaluate the expressiveness of the weighted synergy graph model compared to the unweighted synergy graph.6.1. Experimental setupIn these experiments, we assume that the agent capabilities C are known, and the goal is to learn the structure of thesynergy graph. Algorithm 3 learns the structure of the weighted synergy graph, except that LearnCapabilities is notcalled since the agent capabilities are known. To learn the structure of the unweighted synergy graph, the learning algorithmin [26] is used, which is similar to Algorithm 3, except that the neighbor graph function only adds and removes randomedges (since all edges are unweighted).The process of this set of experiments is similar to the previous section, but we only learn the structure of the weightedand unweighted synergy graphs, and not the agent capabilities (since they are known). We varied the number of agentsN from 5 to 15. In each trial, a random weighted synergy graph was created where the agent capabilities were generatedwith γ = 10. The observation set comprising the performance of teams with 2 and 3 agents are extracted from the hiddensynergy graph. In addition, the performance of teams with 4 and more agents are extracted from the hidden synergy graphand form the test set. From the same observation set, a weighted synergy graph and an unweighted synergy graph arelearned. We calculate the log-likelihood of the learned weighted synergy graph and unweighted synergy graph using the(cid:16)).test data. In addition, with each of the learned synergy graphs, we approximate the δ-optimal team of size n = max(4, (cid:15) N2∀i ∈ [1, N]), while having aWe chose such a value of n so as to maximize the number of possible teams (i.e.,minimum size of 4 since the performance of teams with sizes 2 and 3 are used in learning the synergy graphs. From theteams found, we computed their effectiveness:N(cid:15) N2(cid:2)Ni(cid:5)(cid:4)(cid:4)(cid:5)(cid:16)Effectiveness( A) =Evaluate(S( A), 1 − δ) − Evaluate(S( Amin∗δ ), 1 − δ) − Evaluate(S( AminEvaluate(S( Aδδ), 1 − δ)), 1 − δ)= argmax AEvaluate(S( A), 1 − δ) is the δ-optimal team, and Aminδis the worst-performing team given δ, i.e.,∗δwhere AAminδ= argmin AEvaluate(S( A), 1 − δ).Thus, the effectiveness of a team is a value from 0 to 1, where 1 is the δ-optimal team, and 0 is the worst-possible team.We use the effectiveness to measure the performance of the learned synergy graph, because the goal of the synergy graphis to form an effective team, and the performance is scaled since the actual performance of teams varies from trial to trialdue to the randomized generation of the hidden synergy graph.6.2. Comparison resultsIn our first set of experiments, we used the fraction compatibility function, i.e., φfraction(d) = 1d , and set δ = 0.5. For eachvalue of N, we performed 100 trials, where a different hidden synergy graph was generated in each trial. The weightedand unweighted synergy graphs were then learned using 1000 iterations of simulated annealing for each trial using theobservation set extracted from the hidden model. Fig. 12 shows the effectiveness of the teams found by the weighted andunweighted synergy graphs. As the number of agents N increases, the effectiveness of the teams found by both synergy60S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65Fig. 13. Average effectiveness of teams found in the learned weighted and unweighted synergy graphs, using 1000 and 2000 iterations of simulatedannealing to learn the synergy graph structure, using φfraction(d) = 1d .Fig. 14. Effectiveness of teams found in the learned weighted and unweighted synergy graphs with φdecay(d) = exp(− d ln 22annealing.), and 1000 iterations of simulatedgraph types decrease, reflecting the increase in difficulty in learning the graph and finding the δ-optimal team. However,across all values of N, the team found by the learned weighted synergy graph outperforms the team found by the learnedunweighted synergy graph. We performed a paired Student’s T-test (one-tailed) on the 1100 total trials (11 values of N with100 trials per N), and the results were statistically significant to a value of p = 9.8 × 10−258.We repeated the experiments with φfraction(d) = 1d , but increased the number of iterations of simulated annealing to2000, to investigate if a higher number of iterations would improve the overall performance of the learned synergy graphs.Fig. 13 shows the average effectiveness of the teams found by the weighted and unweighted synergy graphs with both1000 and 2000 iterations of simulated annealing. The learned weighted synergy graph performs better with 2000 iterations−20). However, a greater number of iterationscompared to 1000 iterations (statistically significant to a value of p = 8.7 × 10of simulated annealing does not affect the performance of the learned unweighted synergy graph (p = 0.14). Thus, a greaternumber of iterations of simulated annealing allows the weighted synergy graph learning algorithm to converge on a closermatch to the hidden synergy graph, while the “best” unweighted synergy graph is already found within 1000 iterations andhence increasing the number of iterations has little effect.Thirdly, we used the compatibility function φdecay(d) = exp(− d ln 2h) with the half-life h = 2. In this way, the compatibilityfunction φdecay decreases at a slower pace than φfraction initially, but has much smaller values once d is large. As before,we varied N and ran 100 trials per value of N. We ran 1000 iterations of simulated annealing for both learning algorithms,and Fig. 14 shows the effectiveness of the teams found by the learned synergy graphs. While the effectiveness of the learnedweighted synergy graph decreases more rapidly as N increases compared to φfraction, the learned weighted synergy graphoutperforms the learned unweighted synergy graph, with p = 3.6 × 10−160.Thus, these experiments show that the weighted synergy graph is more expressive than the unweighted synergy graph.The results are statistically significant across a large range of agent sizes, with two compatibility functions, and with differ-ent number of iterations of simulated annealing.S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6561Fig. 15. Screenshot of the RoboCup Rescue simulator showing the initial positions of the simulated robots. Green, red, blue, and white circles are civilians,fire engines, police cars, and ambulances respectively. (For interpretation of the references to color in this figure, the reader is referred to the web versionof this article.)7. Using weighted synergy graphs with the RoboCup Rescue simulatorThe previous sections showed the efficacy of our learning algorithm, and the expressiveness of the weighted synergygraph model. In this section, we evaluate using the weighted synergy graph model to capture the team performance ofsimulated robots in a realistic rescue scenario.7.1. The RoboCup Rescue simulatorThe RoboCup Rescue Simulation League provides an open-source simulator for agent development [20]. The simulatoruses a map of a city, such as Berlin, Istanbul, Paris etc. (Fig. 15), and simulates the event that a natural disaster has occurred.Civilians are randomly positioned in the city with varying amounts of health, some of whom may be buried under rubble.Fires break out in random parts of the city, and roads throughout the city are blocked by fallen debris, making themimpassable for humans and vehicles.Three types of rescue robots are deployed: ambulances, fire engines and police cars. The ambulances help to rescuecivilians, while fire engines put out fires and police cars clear the road obstructions. The simulator runs for a fixed numberof timesteps, and the score is a weighted sum based on the number of civilians and rescue robots alive and the proportionof buildings in the city that are not burnt.The RoboCup Rescue domain is a multi-robot task allocation problem in the ST-MR-TA category [17]. Different approacheshave been used to solve this problem, such as treating it as a generalized allocation problem [13], using a biologically-inspired approach [12], and extending coalition formation to handle spatial and temporal constraints [35].7.2. Experimental setupAs part of the RoboCup Rescue competition, participants from various universities around the world develop algorithmsto control all the rescue robots (i.e., the non-civilians). We use the RoboCup Rescue simulator as an ad hoc multi-robotscenario, where combinations of pre-existing algorithms are used to compose an effective team. The rescue robots havea standardized communication protocol defined in the RoboCup Rescue simulator, which allows different RoboCup partici-pants’ algorithms to be run simultaneously, each controlling a subset of the rescue robots. In particular, we are interested inmodeling the performance of ad hoc combinations of these algorithms. For example, when running two algorithms simulta-neously, each algorithm controls half of the rescue robots. We wanted to compare the effectiveness of the weighted synergygraph model at modeling the interactions and forming an effective team, versus the unweighted synergy graph [26] as wellas IQ-ASymTRe [48].We used the Istanbul1 map from RoboCup 2011, and the source code of 6 RoboCup participants (Poseidon, RoboAKUT,Ri-one, RMAS_ArtSapience, SBCe_Saviour, and SEU_RedSun) [36]. The source code of 8 RoboCup participants were availablefor download, but only 6 ran out of the box without much modification. In the Istanbul1 map, there are 46 rescue robots tobe controlled, and each of the 6 RoboCup algorithms are designed to control all 46 robots to perform the task. We treatedthe 6 RoboCup algorithms as 6 separate agents, such that any subset of these 6 agents can be used to control the 46 robots.62S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65We distributed the 46 rescue robots among the selected agents randomly, such that each agent controlled an approximatelyequal number of rescue robots. For example, if two agents (RoboCup algorithms) were picked, then each algorithm wouldcontrol 23 robots (assigned randomly), and the score of the two agents would be the score returned by the RoboCup Rescuesimulator at the end of its simulation, which is based on the number of civilians and rescue robots alive and the health ofthe buildings in the city. The RoboCup Rescue simulator models randomness and noise in the simulation, and we used thedefault settings of the simulator in our experiments, with the Istanbul1 map from RoboCup 2011.6We varied the number of selected agents N from 2 to 5. For each value of N, there areNcombinations of agents,and we ran 30 simulations for each combination. For example, when N = 3, there are 20 combinations of agent triples(e.g., Poseidon, RoboAKUT, and Ri-one), and we ran 30 simulations for each triple. Each simulation had a different allocationof rescue robots to agents, and hence each simulation resulted in a different score given by the simulator. An observationconsists of the agents (e.g., Poseidon, RoboAKUT, and Ri-one) and a single score they attained (e.g., 14.8), and there are 30observations per agent team combination.(cid:4)(cid:5)(cid:4)We used the scores of simulation runs where N = 2 and N = 3 as the observation set for training, with 1050 = 30((cid:5)6) total observations. To evaluate the learned models, the algorithms formed teams of size 4 and 5. Since the score of a3team changes depending on the robot allocation, we used the average score attained in the 30 simulations as our measure,hence corresponding to δ = 0.5 in our problem definition. We did not form any team of size 6, because only one such teamexists (using all the agents). We did not include data from N = 1 for training or testing, since the simulator is deterministic(given a fixed initial random seed) so there would not be variance in the agents’ performance over 30 simulations.+62(cid:4)(cid:5)7.3. Modeling the agent interactionshFor the weighted and unweighted synergy graph models, we used the decay compatibility function, i.e., φdecay(d) =exp(− d ln 2), where h = 2, and ran 1000 iterations of simulated annealing. We chose the decay compatibility function as thecompatibility decreases more gradually than φfraction, and used 1000 iterations of simulated annealing as it had good resultsin the previous sections. In addition, since the learned synergy graph depends on a random process of changing edges,we performed 10 trials to learn the synergy graph from the RoboCup Rescue data.IQ-ASyMTRe [48] calculates the expected cost of a coalition A and task t as:(cid:6)cost( A, t) = (cid:13)cost( A, t)/F (Q A, Yt)(7.1)where (cid:13)cost( A, t) is the summation of costs of all activated schemas in A, Q A is the coalition quality of A, and Yt is the tasktype of t.Since we have a single task, and all coalitions (combinations of agents) can complete the task, we set F (Q A, Yt) = 1 forall A. As such, cost( A, t) = (cid:13)cost( A, t). Since the internal schemas of the participants’ algorithms are unknown, we treat eachagent as a single schema, and estimate its cost as the average of the score of agent combinations involving it:(cid:13)cost(a) =A s.t. a∈ A score( A)| A s.t. a ∈ A|(7.2)where a is an agent (i.e., one of the 6 algorithms), A is a team of 2 to 5 agents, and score( A) is the score obtained in theRoboCup Rescue simulator using the agents in A to control the rescue robots.From the cost of each agent, we then define the cost of a coalition in IQ-ASymTRe as:(cid:13)cost( A, t) =(cid:7)a∈ A(cid:13)cost(a)(7.3)To form a team using IQ-ASyMTRe, we iterate through all possible combinations of agents given the desired team size,and pick the team with the highest cost. Typically, the team with the lowest cost is picked in IQ-ASyMTRe, but because weused the score as the measure of cost (there is no actual metric for cost of the algorithms), it is desirable to pick the teamwith the highest score.7.4. Ad hoc team formation resultsTable 3 shows the scores of the teams formed with the weighted synergy graph, unweighted synergy graph, and IQ-ASyMTRe. The score corresponds to the final value returned by the RoboCup simulator at the end of the simulation.The weighted and unweighted synergy graph models perform similarly, showing that while the weighted synergy graphmodel is more expressive (shown in the previous section), the interactions of the agents in the RoboCup Rescue domaincan be modeled with an unweighted graph. We believe that both synergy graph models performed identically due to thesmall number of agents — if there were a larger number of agents then the interactions would be more complex and theweighted synergy graph would outperform the unweighted one. Both synergy graph models find the optimal 4-agent team,and find the optimal 5-agent team 80% of the time. In comparison, IQ-ASyMTRe finds a good but non-optimal 4-agent team,and finds a 5-agent team that is close to the worst possible combination. The results are statistically significant to a valueS. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6563Table 3Scores of combinations of algorithms in the RoboCup Rescue simulator,weighted synergy graph model and other algorithms.formed by theAlgorithmWeighted Synergy GraphUnweighted Synergy GraphIQ-ASyMTReBest Possible TeamWorst Possible Team4 agents14.3 ± 014.3 ± 012.314.37.15 agents12.7 ± 0.712.7 ± 0.78.413.08.1−137 for 4 agents, and p = 3.7 × 10−9 for 5 agents (single-tailed paired T-test) between the synergy graphof p = 5.4 × 10model and IQ-ASyMTRe. The p-values for the weighted and unweighted models versus IQ-ASMTRe are identical since bothsynergy graph models attained the same results.Thus, the synergy graph model outperforms IQ-ASyMTRe. The results are compelling in that only observations of 2 and3 agents were used for training, but teams of 4 and 5 agents were formed. Further, no assumptions of the agents wereused and they were treated as black-boxes in the synergy graph model. Thus, the efficacy of the synergy graph model,the learning and team formation algorithms have been demonstrated on the RoboCup Rescue domain and can be extendedto many other domains.8. ConclusionsWe contributed a representation of team performance that goes beyond the sum of single-agent capabilities; we formallyintroduced the weighted synergy graph model, where the inherent differences in the agents’ capabilities are modeled withNormally-distributed variables, and the task-based relationship is modeled as a connected weighted graph. Weighted syn-ergy graphs are applicable to many multi-agent domains, and we presented an example of a rescue task, and showed howdifferent agents and their task-based relationships are modeled effectively.To apply weighted synergy graphs on problem domains, we contributed an algorithm that learns a weighted synergygraph from observations, and two team formation algorithms that use a learned weighted synergy graph to form an effectivemulti-agent team. These algorithms enable the weighted synergy graph model to be applied to a variety of problems,since the only input are the training observations. The learning algorithm learns the structure of the weighted graph byiteratively improving the graph structure, and learns the agents’ capabilities by solving a system of equations derived fromthe observations and the graph structure. Our first team formation algorithm uses branch-and-bound to find the optimalteam, and our second team formation algorithm explores possible teams and approximates the optimal team. Although theapproximation algorithm does not have formal guarantees on the formed team, we found that the team that is formed isclose to optimal while only exploring a small subset of the space. The optimal team formation algorithm searches a largeamount of space in order to guarantee optimality, so we believe the approximation algorithm is more applicable due toscalability issues with the optimal team formation algorithm in real problems.We extensively evaluated our learning algorithm. First, we created random weighted synergy graphs with representativegraph structure types, and demonstrated that our learning algorithm learned weighted synergy graphs with structuressimilar to the types. Second, we generated random weighted synergy graphs that were hidden from the learning algorithm,and showed that the log-likelihood of the learned weighted synergy graph closely matches that of the hidden ones. Thus,the agent capabilities and graph structures were effectively learned using our learning algorithm. Although we evaluatedthe learning algorithm with data derived from weighted synergy graphs, the learning algorithm is applicable to data fromany source and will learn the closest weighted synergy graph that matches the data. The learning algorithm does nothave guaranteed bounds on the learned result but from our experiments, we believe that a small number of iterations ofsimulated annealing (compared to the entire space of weighted graphs) is sufficient to learn a model that matches theobserved data well.We applied the weighted synergy graph model to a problem domain: urban search-and-rescue. Using the RoboCupRescue simulator, we made minor modifications to six algorithms written by RoboCup participants, such that combinationsof these algorithms can be run in parallel to perform the RoboCup Rescue task. We treated each of these six algorithmsas separate agents, and modeled their capabilities and task-based relationships when combined to form an ad hoc team.We showed that the weighted synergy graph model forms a near-optimal team, outperforming the teams selected withIQ-ASyMTRe. Hence, the weighted synergy graph model effectively learned and modeled the synergy of the RoboCup Rescuealgorithms.The weighted synergy graph effectively models agent capabilities and their task-based relationships. Our algorithms onlyrequire the assumption of retrieving observations of the agents’ performance at the task, which makes them applicable tomany multi-agent and multi-robot domains. In particular, as long as problem domains can be treated as a black-box wherethe input is a multi-agent team, and the output is an observed performance, the weighted synergy graph model is appli-cable. We have applied the weighted synergy graph model to multi-robot problems such as foraging [27] and configuringrobots from modules [30], and we continue to apply our model to other domains. Modeling and learning the synergy ofmulti-agent teams is a novel contribution, and we are actively pursuing this area of research. We have successfully applied64S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–65weighted synergy graphs on actual robots performing role assignment [27] and forming teams that are robust to fail-ures [28], and we have introduced an algorithm to learn the synergy of a new teammate [29]. We are using synergy graphson other interesting problems, such as modeling task-based relationships that are non-transitive, and modeling agents thatlearn to coordinate better over time.AcknowledgementsThis work was partially supported by the Air Force Research Laboratory under grant number FA87501020165, by theOffice of Naval Research under grant number N00014-09-1-1031, and the Agency for Science, Technology, and Research(A*STAR), Singapore. The views and conclusions contained in this document are those of the authors and should not be in-terpreted as representing the official policies, either expressed or implied, of any sponsoring institution, the U.S. governmentor any other entity.References[1] N. Agmon, P. Stone, Leading multiple ad hoc teammates in joint action settings, in: AAAI Workshop: Interactive Decision Theory and Game Theory,2011.[2] B. Banerjee, L. Kraemer, Coalition structure generation in multi-agent systems with mixed externalities, in: Proceedings of the International Conferenceon Autonomous Agents and Multiagent Systems, 2010, pp. 175–182.[3] S. Barrett, P. Stone, Ad hoc teamwork modeled with multi-armed bandits: An extension to discounted infinite rewards, in: Proc. Int. Conf. AutonomousAgents and Multiagent Systems – Adaptive Learning Agents Workshop, 2011.[4] S. Barrett, P. Stone, S. Kraus, Empirical evaluation of ad hoc teamwork in the pursuit domain, in: Proceedings of the International Conference onAutonomous Agents and Multiagent Systems, 2011, pp. 567–574.[5] B. Bulka, M. Gaston, M. desJardins, Local strategy learning in networked multi-agent team formation, J. Autonom. Agents Multi-Agent Syst. 15 (2007)29–45.[6] J. Chen, D. Sun, Resource constrained multirobot task allocation based on leader-follower coalition methodology, J. Robot. Res. 30 (12) (2011)1423–1434.[7] M. de Weerdt, Y. Zhang, T. Klos, Distributed task allocation in social networks, in: Proceedings of the International Conference on Autonomous Agentsand Multiagent Systems, 2007, pp. 500–507.[8] M.B. Dias, TraderBots: A new paradigm for robust and efficient multirobot coordination in dynamic environments, PhD thesis, The Robotics Institute,Carnegie Mellon University, 2004.[9] M.B. Dias, A. Stentz, A free market architecture for distributed control of a multirobot system, in: Proceedings of the International Conference onIntelligent Autonomous Systems, 2000, pp. 115–122.[10] M.B. Dias, A. Stentz, Multi-robot exploration controlled by a market economy, in: Proceedings of the IEEE/RSJ International Conference on IntelligentRobots and Systems, 2002, pp. 2714–2720.[11] C. Dorn, S. Dustdar, Composing near-optimal expert teams: A trade-off between skills and connectivity, in: Proceedings of the International Conferenceon Cooperative Information Systems, 2010, pp. 472–489.[12] Fernando dos Santos, Ana L.C. Bazzan, Towards efficient multiagent task allocation in the robocup rescue: A biologically-inspired approach, J. Autonom.Agents Multi-Agent Syst. 22 (2011) 465–486.[13] P. Ferreira, F. Dos Santos, A.L. Bazzan, D. Epstein, S.J. Waskow, RoboCup Rescue as multiagent task allocation among teams: experiments with taskinterdependencies, J. Autonom. Agents Multi-Agent Syst. 20 (2010) 421–443.[14] M. Gaston, M. desJardins, Agent-organized networks for dynamic team formation, in: Proceedings of the International Conference on AutonomousAgents and Multiagent Systems, 2005, pp. 230–237.[15] K. Genter, N. Agmon, P. Stone, Role-based ad hoc teamwork, in: Proceedings of the Plan, Activity, and Intent Recognition Workshop at the Twenty-FifthConference on Artificial Intelligence (PAIR-11), 2011.[16] J.M. George, J. Pinto, P.B. Sujit, J.B. Sousa, Multiple UAV coalition formation strategies (extended abstract), in: Proceedings of the International Confer-ence on Autonomous Agents and Multiagent Systems, 2010, pp. 1503–1504.[17] B.P. Gerkey, M.J. Mataric, A formal analysis and taxonomy of task allocation in multi-robot systems, J. Robot. Res. 23 (9) (2004) 939–954.[18] M. Isik, F. Stulp, G. Mayer, H. Utz, Coordination without negotiation in teams of heterogeneous robots, in: Proceedings of the RoboCup InternationalSymposium, 2006, pp. 355–362.[19] R.M. Karp, Reducibility among combinatorial problems, in: Complexity of Computer Computations, 1972, pp. 85–103.[20] H. Kitano, S. Tadokoro, I. Noda, H. Matsubara, T. Takahashi, A. Shinjou, S. Shimada, RoboCup Rescue: Search and rescue in large-scale disasters as adomain for autonomous agents research, in: Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics, 1999, pp. 739–743.[21] A. Kleiner, M. Brenner, T. Bräuer, C. Dornhege, M. Göbelbecker, M. Luber, J. Prediger, J. Stückler, B. Nebel, Successful search and rescue in simulateddisaster areas, in: RoboCup 2005: Robot Soccer World Cup IX, vol. 4020, 2006, pp. 323–334.[22] T. Lappas, K. Liu, E. Terzi, Finding a team of experts in social networks, in: Proceedings of the International Conference on Knowledge Discovery andData Mining, 2009, pp. 467–476.[23] C. Li, M. Shan, Team formation for generalized tasks in expertise social networks, in: Proceedings of the International Conference on Social Computing,2010, pp. 9–16.[24] S. Liemhetcharat, M. Veloso, Mutual state capability-based role assignment model (extended abstract), in: Proceedings of the International Conferenceon Autonomous Agents and Multiagent Systems, 2010, pp. 1509–1510.[25] S. Liemhetcharat, M. Veloso, Modeling mutual capabilities in heterogeneous teams for role assignment, in: Proceedings of the IEEE/RSJ InternationalConference on Intelligent Robots and Systems, 2011, pp. 3638–3644.[26] S. Liemhetcharat, M. Veloso, Modeling and learning synergy for team formation with heterogeneous agents, in: Proceedings of the International Con-ference on Autonomous Agents and Multiagent Systems, 2012, pp. 365–375 (nominated for Best Student Paper Award).[27] S. Liemhetcharat, M. Veloso, Weighted synergy graphs for role assignment in ad hoc heterogeneous robot teams, in: Proceedings of the IEEE/RSJInternational Conference on Intelligent Robots and Systems, 2012, pp. 5247–5254.[28] S. Liemhetcharat, M. Veloso, Forming an effective multi-robot team robust to failures, in: Proceedings of the IEEE/RSJ International Conference onIntelligent Robots and Systems, 2013, pp. 5240–5245.[29] S. Liemhetcharat, M. Veloso, Learning the synergy of a new teammate, in: Proceedings of the IEEE/RSJ International Conference on Intelligent Robotsand Systems, 2013, pp. 5246–5251.S. Liemhetcharat, M. Veloso / Artificial Intelligence 208 (2014) 41–6565[30] S. Liemhetcharat, M. Veloso, Synergy graphs for configuring robot team members, in: Proceedings of the International Conference on AutonomousAgents and Multiagent Systems, 2013, pp. 111–118.[31] C. Lim, R. Mamat, T. Braunl, Market-based approach for multi-team robot cooperation, in: Proceedings of the International Conference on AutonomousRobots and Agents, 2009, pp. 62–67.[32] T. Michalak, D. Marciniak, M. Szamotulski, T. Rahwan, M. Wooldridge, P. McBurney, N. Jennings, A logic-based representation for coalitional games withexternalities, in: Proceedings of the International Conference on Autonomous Agents and Multiagent Systems, 2010, pp. 125–132.[33] L. Parker, F. Tang, Building multirobot coalitions through automated task solution synthesis, Proc. IEEE 94 (7) (2006) 1289–1305.[34] T. Rahwan, T. Michalak, N. Jennings, M. Wooldridge, P. McBurney, Coalition formation with spatial and temporal constraints, in: Proceedings of theInternational Joint Conference on Artificial Intelligence, 2009, pp. 257–263.[35] S. Ramchurn, M. Polukarov, A. Farinelli, C. Truong, Coalition formation with spatial and temporal constraints, in: Proceedings of the InternationalConference on Autonomous Agents and Multiagent Systems, 2010, pp. 1181–1188.[36] RoboCupRescue, RoboCup Rescue Simulation League, http://roborescue.sourceforge.net/, 2011 (online).[37] T. Sandholm, K. Larson, M. Andersson, O. Shehory, F. Tohme, Coalition structure generation with worst case guarantees, J. Artif. Intell. 111 (1999)209–238.[38] T. Service, J. Adams, Coalition formation for task allocation: theory and algorithms, J. Autonom. Agents Multi-Agent Syst. 22 (2011) 225–248.[39] O. Shehory, S. Kraus, Task allocation via agent coalition formation, J. Artif. Intell. 101 (1–2) (1998) 165–200.[40] P. Stone, G. Kaminka, S. Kraus, J. Rosenschein, Ad hoc autonomous agent teams: Collaboration without pre-coordination, in: Proceedings of the Inter-national Conference on Artificial Intelligence, 2010.[41] P. Stone, G. Kaminka, J. Rosenschein, Leading a best-response teammate in an ad hoc team, in: Agent-Mediated Electronic Commerce: Designing TradingStrategies and Mechanisms for Electronic Markets, 2009, pp. 132–146.[42] P. Stone, S. Kraus, To teach or not to teach? Decision making under uncertainty in ad hoc teams, in: Proceedings of the International Conference onAutonomous Agents and Multiagent Systems, 2010, pp. 117–124.[43] F. Tang, L. Parker, A complete methodology for generating multi-robot task solutions using ASyMTRe-D and market-based task allocation, in: Proceed-ings of the IEEE International Conference on Robotics and Automation, 2007, pp. 3351–3358.[44] P. Tosic, G. Agha, Maximal clique based distributed coalition formation for task allocation in large-scale multi-agent systems, in: Proceedings of theInternational Workshop on Massively Multi-Agent Systems, 2004, pp. 104–120.[45] L. Vig, J. Adams, Market-based multi-robot coalition formation, in: Proceedings of the International Symposium on Distributed Autonomous RoboticsSystems, 2006, pp. 227–236.[46] L. Vig, J. Adams, Coalition formation: From software agents to robots, J. Intell. Robot. Syst. 50 (2007) 85–118.[47] F. Wu, S. Zilberstein, X. Chen, Online planning for ad hoc autonomous agent teams, in: Proceedings of the International Joint Conference on ArtificialIntelligence, 2011, pp. 439–445.[48] Y. Zhang, L. Parker, Task allocation with executable coalitions in multirobot tasks, in: Proceedings of the IEEE International Conference on Robotics andAutomation, 2012.[49] R. Zlot, A. Stentz, M.B. Dias, S. Thayer, Multi-robot exploration controlled by a market economy, in: Proceedings of the IEEE International Conferenceon Robotics and Automation, 2002, pp. 3016–3023.