llOPEN ACCESSPerspectiveAddressing bias in big data and AIfor health care: A call for open scienceNatalia Norori,1,2 Qiyang Hu,1 Florence Marcelle Aellen,1 Francesca Dalia Faraci,3 and Athina Tzovara1,4,5,*1Institute of Computer Science, University of Bern, Neubr€uckstrasse 10 3012 Bern, Switzerland2Population Health Sciences, Bristol Medical School, University of Bristol, Bristol BS8 1UD, UK3Institute of Digital Technologies for Personalized Healthcare (MeDiTech), Department of Innovative Technologies, University of AppliedSciences and Arts of Southern Switzerland, 6962 Lugano, Switzerland4Sleep Wake Epilepsy Center | NeuroTec, Department of Neurology, Inselspital, Bern University Hospital, University of Bern, 3010 Bern,Switzerland5Helen Wills Neuroscience Institute, University of California Berkeley, Berkeley, CA 94720, USA*Correspondence: athina.tz@gmail.comhttps://doi.org/10.1016/j.patter.2021.100347THE BIGGER PICTURE Bias in the medical field can be dissected along three directions: data-driven, algo-rithmic, and human. Bias in AI algorithms for health care can have catastrophic consequences by propa-gating deeply rooted societal biases. This can result in misdiagnosing certain patient groups, like genderand ethnic minorities, that have a history of being underrepresented in existing datasets, further amplifyinginequalities.Open science practices can assist in moving toward fairness in AI for health care. These include (1) partici-pant-centered development of AI algorithms and participatory science; (2) responsible data sharing and in-clusive data standards to support interoperability; and (3) code sharing, including sharing of AI algorithmsthat can synthesize underrepresented data to address bias. Future research needs to focus on developingstandards for AI in health care that enable transparency and data sharing, while at the same time preservingpatients’ privacy.Concept: Basic principles of a newdata science output observed and reportedSUMMARYArtificial intelligence (AI) has an astonishing potential in assisting clinical decision making and revolutionizingthe field of health care. A major open challenge that AI will need to address before its integration in the clinicalroutine is that of algorithmic bias. Most AI algorithms need big datasets to learn from, but several groups ofthe human population have a long history of being absent or misrepresented in existing biomedical datasets.If the training data is misrepresentative of the population variability, AI is prone to reinforcing bias, which canlead to fatal outcomes, misdiagnoses, and lack of generalization. Here, we describe the challenges inrendering AI algorithms fairer, and we propose concrete steps for addressing bias using tools from the fieldof open science.INTRODUCTIONDespite the astonishing potential of artificial intelligence (AI) inhealth care, its regular use in the clinical routine comes withseveral ethical and societal challenges. As a notable example,one of the most frequent medical therapies is oxygen administra-tion, whose levels in the blood are measured through a pulseoximeter.1 The pulse oximeter measures oxygen saturation bysending infrared light through the skin. Measurements of thepulse oximeter are known to be affected by the patient’s skin co-lor, as the device systematically overestimates oxygen satura-tion levels in nonwhite patients.2 As a result, Black patients arethree times more likely to suffer from an occult hypoxemia thatremains undetected by pulse oximeters compared with whitepatients.1 As highlighted by this example, disparities in healthcare may start at the level of clinical measurements, which canultimately shape erroneous medical decisions for entire patientgroups, and can be amplified with the development of AItechnologies.AI promises to provide data-driven approaches to supportclinical decision making and public health policymaking, gradu-ally benefiting the health of society. Deep neural networks havegenerated substantial advances in medical imaging and preci-sion medicine. In contrast to more ‘‘traditional’’ machine learningPatterns 2, October 8, 2021 ª 2021 The Author(s). 1This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).llOPEN ACCESSapproaches, deep neural networks rely on propagating an inputsignal through multiple layers of transformations.3 This results inthe extraction of more complex patterns of information from theinput signal than simpler techniques are typically able to reveal.As the amount of data in the biomedical field constantly in-creases, the use of deep learning has also seen a vast increase,as deep neural networks are particularly powerful in extractinginformation from large datasets.4In one of many examples,in the field of dermatology,convolutional neural networks (CNNs) are able to classify imagesof skin lesions as accurately as trained dermatologists.5 Notably,CNNs have even been found to be superior to dermatologists inmelanoma image classification.6 In cardiology, machine learninghas been proposed for developing risk assessments and per-forming predictions of cardiovascular events.7 In sleep medicine,deep learning can automate sleep scoring, a tedious task that isotherwise manually performed.8 Similar applications also havebeen reported in the fields of neurology, radiology, and pathol-ogy.9,10 Apart from an important role in diagnostics, AI also hasapplications in drug discovery and development, where it couldbe used to identify drug-drug interactions and to developpersonalized treatments.11 AI systems could also help reducehealth care costs, predict patients’ no show, or shorten hospitalwaiting times by searching millions of medical records.12 Ourgoal with this article is to focus on the question of AI and fairnessin relation to bias in health care, and examine how open sciencetools can help address it. We start with an overview of knownsources and examples of bias in the medical field. We then focuson data bias, and outline the main open challenges that need tobe addressed from an algorithmic, medical, and societal point ofview. Last, we offer recommendations for future directions, high-lighting the role of open science in addressing bias in AI.AI AND BIAS IN MEDICINEBias can be defined statistically and socially. Statistically, biasrefers to cases in which the distribution of a given dataset isnot reflecting the true distribution of the population. Statisticalbias can cause an algorithm to produce an output that differsfrom the true estimate.13 Social bias, by contrast, refers to ineq-uities that may result in suboptimal outcomes for given groups ofthe human population.12 The medical field is no stranger to bias,2 Patterns 2, October 8, 2021PerspectiveFigure 1. Illustration of different sources ofbias in training machine learning algorithmswhich oftentimes is hard to quantify anddetect (see Figure 1 for an overview). Todate, there have been numerous reportsof algorithms that discriminate againstvulnerable groups in the same fields inwhich AI has shown promising results.In one of many examples, CNNs thatprovide high accuracy in skin lesion classi-fication6 are often trained with images ofskin lesion samples of white patients, usingdatasets in which the estimated proportionof Black patients is approximately 5% to10%.14 As a result, when tested with images of Black patients,the networks have approximately half the diagnostic accuracycompared with what their creators originally claimed.14 Blackpatients, whose lesions may have different characteristics fromwhite patients, may thus be less likely to be accurately diag-nosed by automated algorithms. This omission should not betaken lightly, as Black patients have the highest mortality ratefor melanoma, with an estimated 5-year survival rate of only70%, versus 94% for white patients. Misdiagnoses and socio-economic barriers hindering access to health care may causeskin cancer at a more advanced stage in Black patients, hinder-ing treatment.15than others.Racial bias in health care results in some groups of patientsgetting better medicalIn anothertreatmentexample, AI algorithms used health costs as a proxy for healthneeds and falsely concluded that Black patients are healthierthan equally sick white patients, as less money was spent onthem.16 As a result, these algorithms gave higher priority to whitepatients when treating life-threatening conditions, such as dia-betes and kidney disease, even though Black patients havehigher severity indexes.16The Coronavirus Disease 2019 (COVID-19) has proven howbiased AI systems amplify existing inequalities, placing vulner-able populations at a higher risk of severe illness and death.17AI is used in the fight against COVID-19, but because of thetime pressure to develop concrete solutions againstthepandemic, AI might be likely to reinforce COVID-19–induced in-equalities at scale, because its performance in vulnerable popu-lations may not have been thoroughly tested.18Algorithmic bias is not exclusive to race. Gender inequalitiesalso can be exacerbated by imbalanced algorithms. Forexample, in cardiology, a heart attack is overwhelmingly mis-diagnosed in women.19 Nevertheless, prediction models for car-diovascular disease that claim to predict heart attacks 5 yearsbefore they happen20 are trained in predominantly maledatasets. As cardiovascular disease has different patterns ofexpression in men versus women,21 an algorithm that hasbeen trained predominantly with data samples of men may notbe as accurate in diagnosing women.Another interesting area for AI in medicine is the quest for auto-mated sleep scoring algorithms. Since the 1960s, many algorithmshave been developed, reaching very good perfomances,22 butPerspectivewhen used in a clinical routine they fail miserably. Being trained onyoung healthy individuals, automated algorithms are often unableto decrypt sleep disorders in older patients. Thanks to bigger andmore heterogeneous datasets, automated sleep staging hasimproved, but we are still far from acceptable performance whenvalidation is done on new datasets or unseen sleep disorders. Insleep scoring, training of AI algorithms is done utilizing visualscoring labels as gold standards. Cognitive biases can lead topoor inter- (70%–80%) and intra- (90%) scoring agreement.23This intrinsic limit could be overcome by AI, as its use neutralizesexternal sources of variance like human expert variability, offeringa uniform standardized solution.8Bias is concerning in areas where the lack of variability intraining data is harder to identify at an early stage, such asdrug development and clinical trials. In the case of clinical trials,the majority of participants are male, of a limited age group, andfrom similar ethnic backgrounds.24 Preclinical studies are alsoaffected by gender bias, as they typically include either a vastmajority, or exclusively male animals,25 which resulted in theNIH issuing guidelines to balance the ratio of male/female ani-mals.26 Gender biases during the preclinical stages of drugdevelopment could alter how women react to newly developeddrugs.27 The results of drug behavior, side effects, and effective-ness from such early studies may in turn be transferred into thedatasets that are then used to train AI algorithms.Data limitations are a criticalissue that can result in bias(Figure 1), but the lack of diversity in clinical datasets is not theonly source of bias. Researchers and clinicians can also imputeunconscious judgments and biases into their research (Figure 1),thus deploying AI algorithms that are biased by design. If ethicalissues are not addressed before further implementation of algo-rithms in the clinical practice, AI might fail to deliver benefits to allpatients, increasing health inequities.MOVING TOWARD FAIRNESS IN AI: CURRENTCHALLENGESBiasSources of bias in AI may be present in most, if not all, stages ofthe algorithmic development process. Algorithmic bias canemerge due to the use of imbalanced or misrepresentativetraining data, the implementation of data collection systemsinfluenced by human subjectivity, lack of proper regulation inthe design process, and replication of human prejudices thatcauses algorithms to mirror historical inequalities.13Vulnerable groups have a long history of being absent or misrep-resented in existing datasets. When AI algorithms are trained withdatasets in which vulnerable groups are not well represented, theirpredictive value may be limited. Algorithms may be able to detectpatterns specific to the majority groups that they were trained with,but they may have poor performance in recognizing patterns thatare present in patient groups that were never seen during training.As an example, skin cancer has a strong genetic component.15 If adiagnostic algorithm is only trained with genetic data of white pa-tients, it may fail to generalize to patients of other ethnicities.More generally, if AI is used as a diagnostic or therapeuticapproach in patients who are invisible in the datasets that AIalgorithms are trained with, these may fail to diagnose or treatentire patient groups, such as ethnic and gender minorities, im-llOPEN ACCESSmigrants, children, the elderly, and people with disabilities.These failures for certain population groups can be hard torecognize during the early training and testing phases of AIdeployment, unless they are specifically sought after.Sources of biasData-driven bias. Most fields of human research are heavilybiased toward participants with a Western, Educated, Industrial-ized, Rich, Democratic—WEIRD—profile,28 and are not repre-sentative of the human population as a whole. As several ofthe available datasets that are used to train AI algorithms werecollected in the context of scientific studies, they in turn arebiased.reflectOftentimes, the quantification of certain forms of bias in agiven dataset is relatively straightforward, as the data samplescarry features thatthe characteristics of bias. Forexample, bias due to ethnicity could be inferred from a datasetof skin samples, or bias due to gender or ancestry can be in-ferred from genetic data. However, in many cases it is impos-sible to quantify biases in the composition of a dataset. Forinstance, biases due to socioeconomic status or sexual orien-tation are often impossible to infer in a biomedical dataset un-less this information has been explicitly collected and includedas metadata.Although variables and metadata that do not directly apply to agiven research question may seem irrelevant for quantifying bias,there is strong evidence that suggests the contrary. For instance,several neuroscience studies have shown that socioeconomicvariables are associated with detectable differences in brainstructure29 and functions.30 To be able to assess the influenceof socioeconomic variables in neurological data, future studieswill need to start collecting homogenized metadata correspond-ing to factors that may induce bias.As an example of data-driven bias and data gaps, polygenicrisk scores use data from genome-wide association studies(GWAS) to calculate a person’s inherited susceptibility for a dis-ease. Although polygenic risk scores have a great potential aspredictive biomarkers, 81% of GWAS studies are conducted inindividuals of European ancestry.31 This affects the generaliz-ability of polygenic risk scores across different populations andcan result in biased predictions and further inequities in healthoutcomes.Algorithmic bias. When an algorithm is trained on biased data,it is likely to reinforce patterns from the dominant category of thedata it was trained with. In the simplest case, when an algorithmis trained to classify a dataset consisting of 80% healthy and20% diseased images,just by predicting every sample ashealthy, the algorithm will achieve a performance of 80% accu-racy. Alternative metrics should therefore be used that are at-tuned to class imbalance, such as the F1 score. This can bedefined as follows:F1 =TPTP + FP + FN2where TP = true positive, FP = false positive, and FN = falsenegative. In the above-mentioned example of an imbalancedataset, this would result in a score of 0. Therefore, the F1 scorecould be a more reliable and intuitive metric in the case of imbal-ance datasets.Patterns 2, October 8, 2021 3llOPEN ACCESSHaving objective ways to estimate chance levels is crucial toavoid misinterpretation of findings. Permuting the labels of theavailable samples and retraining an algorithm to give ‘‘random’’predictions can provide an empirical estimation of chancelevels.32 This should be combined with performance metricsthat are not affected by imbalanced datasets,33 or with classifi-cation techniques that include weight factors that take into ac-count in the algorithms’ optimization step the fact that someclasses are imbalanced.33Moreover, algorithms that mitigate bias can be used wheneverpossible. To increase algorithmic fairness, protected attributes,such as gender or ethnicity, can be included during training inorder to ensure that algorithmic predictions are statistically inde-pendent from these attributes.34 Alternatively, loss functions canbe defined per protected group, and may be forced to remainbelow a certain level for all defined groups, so that no singlegroup is systematically misclassified.34 Similar approacheshave been introduced in different AI frameworks, such as in thelearning,35 and are summarized in open-case of adversarialsource toolkits that can be used to mitigate algorithmic bias.36Human bias. As AI algorithms are designed by humans, theymay often reflect human biases. Algorithms are often designedto tackle what their developers consider the most urgent prob-lems to solve, which are not necessarily the same challengesfaced by the individuals that are concerned by those algorithms.Lack of diversity in engineering and biomedical teams can repli-cate unconscious bias and power imbalances.37Human bias in AI can be one of the hardest ones to detect andmitigate, as it can result from long-held societal prejudices thatmay be subtle at the level of society, and amplified by AI andlarge datasets. The medical field has several examples whereracial, gender, or age disparities are affecting clinical decisionmaking, quality of treatment, and outcome prognosis.It is well documented that Black patients have lower survivalrates compared to white patients for different cancer types.38Although mortality rates from cardiovascular disease havemajorly decreased over the past 10 years, Black patients hadhigher mortality rates in 2017 compared with the mortality ratethat white patients had back in 2007.39 Similar trends are seenfor patients suffering from depression, with ethnic minoritiesexperiencing more severe symptoms and receiving medicationless often than white patients.40Apart from race, also gender results in bias and unequal treat-ments. Historically, women have been regarded as the ‘‘smaller’’version of men, and medication dosages were adjusted for patientsize, without taking into account sex differences.41 In health care,sex differences can be substantial and include differences ingene expression,42 or in the prevalence, age, onset, symptom-atology, morbidity, and mortality oflife-threatening diseases,such as coronary heart disease,43 stroke, and different types ofcancer.44 Compared with men, women are more likely to have theirpain levels underestimated by clinicians.45 Moreover, nonhetero-sexual women exhibit higher risk factors for certain forms of cancer,cardiovascular disease, or mental health, despite generally highersocioeconomic status than heterosexual women.46 Individualswho are lesbian, gay, bisexual, transgender, transsexual, two-spirit, queer, or questioning (LGBTQ+) are particularly affected byinequalities in health care, which arise due to particular needs fortreatment47 and due to bias in health care practitioners.484 Patterns 2, October 8, 2021PerspectiveData gapsOver the past decade, governments, funders, and institutionshave worked together to promote open data sharing. As a result,the world has access to public datasets to train AI algorithms,but most of them are not diverse, disaggregated, and interoper-able.49 Data repositories have substantially increased the num-ber of open datasets available to train and develop algorithms,but vulnerable populations remain underrepresented in healthcare data. This lack of diversity restricts the utility and generaliz-ability of the datasets and the AI algorithms trained with them. Inaddition,lack of consistency and coherency, differences informatting, and limited data disaggregation prevent open data-sets from being intermixed and used to power large, complexsystems.Developing inclusive technologies relies on counting peoplein, but gaps in data tend to leave certain groups unnoticed.When minority groups are invisible in datasets used to deployAI algorithms, their needs and phenotypes may become invis-ible. As an example, commercial, and also open genomic data-bases, like the Personal Genome Project, contain data that arein their vast majority of European origin.49 The lack of geneticdata for large parts of the human population might hinder thedevelopment of biomarkers and treatments for conditions witha heavy genetic component.To characterize datasets, it is important to collect comprehen-sive metadata. As an example, despite numerous initiatives toinclude sexual orientation and gender identity in electronic healthrecords, to date this information is largely missing.50 In the vastmajority of medical records it is thus impossible to identifyLGBTQ+ individuals, who experience health disparities, andmay have unique health care needs. Moreover,informationrelated to the researchers or clinicians who collected certainmetadata is also often missing, and its importance is ignored. La-bels associated with medical data, disease rating scales, anddiagnosis may be imbued with cognitive biases of the healthcare personnel who collected this information.51 Personalitytraits like tolerance to risk, or overconfidence may result in diag-nostic, therapeutic, or management errors, and may impactpatient outcomes.44Data standards and interoperabilityStandardization makes data interoperable and impactful. Whendata are not openly available and are published in inconsistentand incompatible formats, it becomes difficult to exchange,analyze, and interpret them.Inconsistency in data sharing,variability in data quality, and different levels of data usabilitydetermine whether or not researchers get access to high qualitytraining datasets for fair AI.52Importantly, several of the underlying datasets that power AIalgorithms were not built for this purpose. As a result, the datastandards (or lack of) applied to these datasets limit the potentialof the algorithms that are trained with them. This is a major lim-itation in many of the datasets published in data science web-sites, such as Kaggle, where binary gender fields, incompletegender disaggregation, and incompatible formats make it diffi-cult to not only build inclusive AI, but also to test for biases.Apart from formatting, data standardization can encouragepatient groups to capture their characteristics in a way thatfacilitates readability and interoperability. For data standards toPerspectivereflect those who are ultimately impacted by their adoption,broad and active participation from members of different sectorsand communities is required during all steps of their designprocess.To become interoperable, datasets need to track and measureinclusivity, have the possibility to exchange samples, and haveclear structures that are capable to support multiple systems.Creating data standards is a complex process, but also amandatory point of passage for training fair AI algorithms. Inthe fast-paced field of AI, it sometimes might be better to adoptexisting standards instead of creating completely new ones.MOVING TOWARD FAIRNESS IN AI: A CALL FOR OPENSCIENCEThe fair implementation of AI in health care requires integratingprinciples of inclusivity, openness, and trust in biomedical data-sets by design. The idea of openly sharing multiple facets of theresearch process, including data, methods, and results underterms that allow reuse, redistribution, and reproduction of allfindings gave birth to open science, a practice that is stronglysupported by several institutions and funding agencies.Although open science is a wide term that encompassesseveral practices, recent attempts have framed open sciencewithin a framework of inclusivity, such that no science can beopen unless it is inclusive by design. This inclusivity aspect,llOPEN ACCESSFigure 2. Illustration of open science toolsthat can help address bias in AIalong with the well-established advan-tages of increasing scientific rigor, trust,and use of resources, make open sciencesuitable for increasing algorithmic fairnessin the biomedical field. Openly sharing theentire research process along with its re-sults incentivizes participation and helpsremove key barriers that prevent membersof vulnerable or underrepresented groupsfrom being part of the global scientificcommunity. This can be achieved acrossseveral axes (see Figure 2 for a summaryillustration).Sharing data increases inclusivityDuring the recent years, the value of openlysharing health care data has become moreevident than ever, with researchers, gov-ernments, and nongovernmental organiza-tions worldwide implementing open datasharing practices to quantify and respondto health emergencies.53 The millions ofdata points being shared have acceleratedthe development of cutting-edge AI tech-nology for epidemiologic, diagnostic, andtherapeutic interventions. Although healthcare data are necessary to advance medi-they contain sensitive informationcine,that needs to be safeguarded for privacy reasons.54 It is notenough to simply code the name and surname of a patient toensure anonymity. In one of many examples, electroencephalog-raphy signals, which are typically considered anonymous, canbe used as a biometric identifier.55 Therefore, new anonymiza-tion processes must be conceived. Responsible data sharingframeworks designed with openness at their core that also pro-tect the individual’s rights to privacy are needed for health caredata. One example of such a framework is the federated learningsystems, which enable the training of AI algorithms at a locallevel, allowing individuals to maintain control and anonymity oftheir data.56Inclusive health data standards to supportinteroperabilityData standards lead to efficient data infrastructure and supportinteroperability. Shared formatting, language, and data identi-fiers make information scalable, while comprehensive metadatadescriptions enhance the discoverability of communities andconcepts.57Fair biomedical data standards cannot be developed inisolation, and require constant feedback from patient or com-munity representatives.58 However, creating standards fromthe ground up is a complex process; therefore, the adoptionof existing health data standards is advised. A number ofexisting health data standards are recommended by health au-thorities, such as Health Level 7, the International OrganizationPatterns 2, October 8, 2021 5llOPEN ACCESSfor Standardization (ISO), and OpenEHR, among others.58 Inaddition, the Open Standards for Data Guidebook provides ageneral introduction to open data standards for data, makingit easier to find and adopt existing standards and, when neces-sary, create new ones.Generating synthetic data to combat biasOftentimes, despite the best intentions, it is impossible tohave unbiased datasets. Questions of privacy, anonymity,and trust may obstruct the participation of underrepresentedgroups in data sharing initiatives. To overcome this limitation,the field of machine learning has severaltools, such asGenerative Adversarial Networks,59 that can be used to artifi-cially generate synthetic data and augment underrepresentedclasses, such as skin lesion images.60 This can allow neuralnetworks to be trained with more samples of data thatmay be underrepresented, such as data of ethnic minorities.Future studies can evaluate the efficacy of this approachin decreasing the rates of misclassified samples ofunderrepresented groups.Sharing inclusive AI algorithmsSharing data is not always feasible or desirable due to questionsof privacy and security. Thus, sharing code and retraining exist-ing algorithms with data collected at a local level, for example inhospitals across the globe, can circumvent the lack of diversity inexisting openly shared datasets.Opening up the source code of AI algorithms can acceleratealgorithmic development by allowing scientists and engineersto extend, reuse, and validate shared code. Open-source prac-tices facilitate collaboration, making code and algorithms acces-including members of sensitive groups.61sible to anyone,Openly sharing AI algorithms in a comprehensive way contrib-utes to computational transparency and interoperability.Sharing code can empower individuals to evaluate the perfor-mance of novel AI algorithms on different datasets. This canallow researchers from all around the globe to test whether agiven algorithm, developed, for example, in Europe predomi-nantly with data of white patients, generalizes to data of patientsin Asia or Latin America. Sharing code can enable local researchcommunities to validate and fine-tune existing neural networksfor the needs of their local patient groups, resulting in a distrib-uted model for training future AI algorithms.Evaluating algorithmic efficiency and fairnessField-testing can give researchers the opportunity to assess theperformance of algorithms in different population groups andclinical settings.62 Given the ethical implications of AI in medi-cine, AI algorithms should be evaluated as rigorously as otherhealth care interventions, like clinical trials.63 Open science prac-tices that encourage transparency, like preregistration for AIstudies, need to become the norm before these can be used todiagnose or treat a specific patient group. Moreover, transparentguidelines like the Good Evaluations and Practices for HealthInformatics, can guide users through a multistep process tocontrol for issues that may arise during different stages ofalgorithmic design and implementation.64The limitations of AI algorithms that can be identified throughsuch investigations should be transparently communicated toclinicians and policymakers. This can ensure that AI algorithmscan be applied to the populations they have been tested on.6 Patterns 2, October 8, 2021PerspectiveCommon metrics for AI reliabilityAnother important issue is related to the inconsistency and limitsof the metrics adopted for assessing AI reliability. The adoptionof common standardized metrics should be strongly favored,and the clinical perspective should be considered in algorithmicapplicability and interpretability. Whenever possible, the metricsshould not only focus on the numerical accuracy, but alsoinclude quality of care and patient outcomes.65Explainable AI modelsA direction that AI algorithms will need to consider is that ofexplainable AI. Several powerful AI algorithms are employing aso-called ‘‘black box’’ approach, where it is difficult or evenimpossible to understand how the obtained results have beenachieved.66 Explainable AI by contrast includes interpretable AImodels, where the strengths and weaknesses of a decision-making process are transparent.67 AI applications often haveto deal with a trade-off between model performance and inter-pretability. On the one hand, simple models, such as linearclassifiers or decision trees, are generally interpretable but often-times lead to suboptimal performance. On the other hand, morecomplex models like deep neural networks provide high classifi-cation performance, but identifying the features that drive anaccurate classification can be cumbersome and oftentimesimpossible.Feature interpretability, together with a strong performance areprioritized in explainable AI models. In explainable AI, the featuresthat a model is using to make a decision need to be traceable andunderstandable by a human. As an example, transparent tech-niques like decision trees, relying on interpretable features, canprovide a ‘‘white-box’’ approach for diagnosis.68The field of computer vision has dedicated a substantial effortin obtaining interpretable features and understanding the pro-cess of classification.69 For example, the kernels or intermediatefeatures of a trained neural network may shed light on the learnedstructure in different layers of the network, giving rise to methodslike class activation mapping (CAM).67 Other methods aregradient based, like saliency maps,68 and calculate the contribu-tion of each input pixel to the overall classification performance.The combination of these two approaches has given rise to Grad-CAM,70 which allows the identification of regions of interest in theinput data that mostly influenced the network’s decision. Theseapproaches can be integrated in the future with existing algo-rithms and datasets, so that features driving a network’s decisioncan be potentially shared together with the data used to train thenetwork in order to increase transparency.Participant-centered development of AI algorithmsAn important component of open science that can be a strongasset in the fight against bias in AI applications is participatoryscience. Participatory science involves scientists and nonscien-tists working together toward the creation of scientific knowl-edge.71 Participatory science can be used in the developmentof novel AI algorithms to actively include individuals who are con-cerned with the applications of a given algorithm, like specificpatient groups. When members of underrepresented groupsare actively engaged in science, they can contribute to the iden-tification of bias against their communities, and with solutions toincrease their representations in the datasets used to develop AIalgorithms.61PerspectiveIncluding communities (such as indigenous peoples, peoplewith disabilities, the LGBQ + community, immigrants, etc.) inthe design of data collection and AI deployment can ensurethat the outcomes that can be achieved from the design of AImodels directly benefit them. Moreover, the active engagementof patient groups in AI deployment might reduce the propagationof biases and misconceptions, and can help scientists evaluatewhether their research questions are equally relevant to patientsand groups that are traditionally underrepresented in science.As a notable example, the Open Artificial Pancreas (OpenAPS) isa community-led initiative that designs openly accessible technol-ogy for automatically adjusting insulin intake in patients with type 1diabetes, in order to keep blood glucose in a safe range.72 Open-APS has resulted in patient-led data commons and in the genera-tion of rich clinical datasets that may be used for patient-ledresearch, and have already resulted in several research studies.61Participant-centered algorithms and datasets can be facili-tated by community-based platforms specifically designed toenable collection of personal data and give individuals the pos-sibility to design novel study questions or algorithms thatconcern themselves and their communities.61 Open Humans isan example of such a platform that allows participants to sharetheir personal data, design their own research questions, andalso design and share their own algorithms. Open Humans takesa participant-centered approach to data sharing, in order tosolve some of the challenges associated with data ethics, pri-vacy, and patient involvement.61CONCLUSIONSHealth care is being transformed by the growing number of datasources that are constantly shared, collected, and implementedinto AI systems. Using AI for public good can help tackle some ofthe world’s most pressing issues, including providing humanitar-ian assistance and supporting emergency response. Oneexample of this is the United Kingdom’s National Health ServiceCovid-19 contact-tracing app, which helped prevent between100,000 and 900,000 Covid-19 infections from October toDecember 2020.73 Organizations like Omdena and the AlanTuring Institute are pioneers in developing ethical AI solutionsin a humanitarian context. From predicting climate risks, toincreasing transparency, and responding to epidemics, these or-ganizations have proven that when AI is inclusive and fair, it canbe used in solving the world’s most pressing issues.In order for new technologies to be inclusive, they need to beaccurate and representative of the needs of diverse populations.Algorithmic and human bias, along with information gaps andlack of data standards, common metrics, and interoperableframeworks pose the biggest threats to move toward fair AI. Im-plementing the principles of open science into AI design andevaluation tools could help strengthen collaborations betweenthe AI and medical fields, and open up the space for diversevoices to participate in AI deployment for medicine.ACKNOWLEDGMENTSllOPEN ACCESSAUTHOR CONTRIBUTIONSN.N. and A.T. conceptualized the article. All authors wrote the original draft andreviewed and edited the article. A.T. provided funding acquisition.DECLARATION OF INTERESTSThe authors declare no competing interests.INCLUSION AND DIVERSITYOur group of authors represents diverse scientific backgrounds including thefields of healthcare, math and physics, computer science, neuroscience, andbiomedical engineering.REFERENCES1. Sjoding, M.W., Dickson, R.P., Iwashyna, T.J., Gay, S.E., and Valley, T.S.(2020). Racial bias in pulse oximetry measurement. N. Engl. J. Med. 383,2477–2478.2. Bickler, P.E., Feiner, J.R., and Severinghaus, J.W. (2005). Effects of skinpigmentation on pulse oximeter accuracy at low saturation. Anesthesi-ology 102, 715–719.3. Goodfellow,(MIT Press).I., Bengio, Y., and Courville, A.(2016). Deep Learning4. Wainberg, M., Merico, D., Delong, A., and Frey, B.J. (2018). Deep learningin biomedicine. Nat. Biotechnol. 36, 829–838.5. Chan, S., Reddy, V., Myers, B., Thibodeaux, Q., Brownstone, N., and Liao,W. (2020). Machine learning in dermatology: current applications, oppor-tunities, and limitations. Dermatol. Ther. 10, 365–386.6. Brinker, T.J., Hekler, A., Enk, A.H., Berking, C., Haferkamp, S., Hauschild,A., Weichenthal, M., Klode, J., Schadendorf, D., Holland-Letz, T., et al.(2019). Deep neural networks are superior to dermatologists in melanomaimage classification. Eur. J. Cancer 119, 11–17.7. Cuocolo, R., Perillo, T., De Rosa, E., Ugga, L., and Petretta, M. (2019). Cur-rent applications of big data and machine learning in cardiology. J. Geriatr.Cardiol. JGC 16, 601–607.8. Fiorillo, L., Puiatti, A., Papandrea, M., Ratti, P.L., Favaro, P., Roth, C., Bar-giotas, P., Bassetti, C.L., and Faraci, F.D.(2019). Automated sleepscoring: a review of the latest approaches. Sleep Med. Rev. 48, 101204.9. Vishnu, V.Y., and Vinny, P.W. (2019). The neurologist and artificial intelli-gence: titans at crossroads. Ann. Indian Acad. Neurol. 22, 264–266.10. Parwani, A.V. (2019). Next generation diagnostic pathology: use of digitalpathology and artificial intelligence tools to augment a pathological diag-nosis. Diagn. Pathol. 14, 138.11. Re´ da, C., Kaufmann, E., and Delahaye-Duriez, A. (2020). Machine learningapplications in drug development. Comput. Struct. Biotechnol. J. 18,241–252.12. Davenport, T., and Kalakota, R. (2019). The potential for artificial intelli-gence in healthcare. Future Healthc. J. 6, 94–98.13. Parikh, R.B., Teeple, S., and Navathe, A.S. (2019). Addressing bias in arti-ficial intelligence in health care. JAMA 322, 2377.14. Kamulegeya, L.H., et al. (2019). Using Artificial Intelligence on Derma-tology Conditions in Uganda: A Case for Diversity in Training Data Setsfor Machine Learning. http://biorxiv.org/lookup/doi/10.1101/826057.15. Gupta, A.K., Bharadwaj, M., and Mehrotra, R. (2016). Skin cancer con-cerns in people of color: risk factors and prevention. Asian Pac. J. CancerPrev. APJCP 17, 5257–5264.This work is supported by Mozilla Foundation, the Swiss National ScienceFoundation (#320030_188737), and the Interfaculty Research Cooperation‘‘Decoding Sleep: From Neurons to Health & Mind’’ of the University of Bern.16. Obermeyer, Z., Powers, B., Vogeli, C., and Mullainathan, S. (2019). Dis-secting racial bias in an algorithm used to manage the health of popula-tions. Science 366, 447–453.Patterns 2, October 8, 2021 7llOPEN ACCESSPerspective17. Chowkwanyun, M., and Reed, A.L. (2020). Racial health disparities andcovid-19 - caution and context. N. Engl. J. Med. 383, 201–203.18. Leslie, D., Mazumder, A., Peppin, A., Wolters, M.K., and Hagerty, A.(2021). Does ‘‘AI’’ stand for augmenting inequality in the era of covid-19healthcare? BMJ, n304. https://doi.org/10.1136/bmj.n304.19. Maserejian, N.N., Link, C.L., Lutfey, K.L., Marceau, L.D., and McKinlay,J.B. (2002). Disparities in physicians’ interpretations of heart diseasesymptoms by patient gender: results of a video vignette factorial experi-ment. J. Womens Health 18, 1661–1667.20. Oikonomou, E.K., Williams, M.C., Kotanidis, C.P., Desai, M.Y., Marwan,M.,, Antonopoulos, A.S., Thomas, K.E., Thomas, S., Akoumianakis, I.,Fan, L.M., et al. (2019). A novel machine learning-derived radiotranscrip-tomic signature of perivascular fat improves cardiac risk prediction usingcoronary CT angiography. Eur. Heart J. 40, 3529–3543.21. Fauci, A.S., Longo, D.L., Kasper, D.L., Hauser, S.L., Jameson, J.L., andLoscalzo, J. (2008). In Harrison’s Principles of Internal Medicine, A.S.Fauci, et al., eds. (McGraw-Hill Medical), pp. 1649–1650.22. Penzel, T., and Conradt, R. (2000). Computer based sleep recording andanalysis. Sleep Med. Rev. 4, 131–148.23. Danker-Hopfe, H., Anderer, P., Zeitlhofer, J., Boeck, M., Dorn, H., Gruber,G., Heller, E., Loretz, E., Moser, D., Parapatics, S., et al. (2009). Interraterreliability for sleep scoring according to the Rechtschaffen & Kales and thenew AASM standard. J. Sleep Res. 18, 74–84.24. Oh, S.S., Galanter, J., Thakur, N., Pino-Yanes, M., Barcelo, N.E., White,M.J., de Bruin, D.M., Greenblatt, R.M., Bibbins-Domingo, K., Wu,A.H.B., et al. (2015). Diversity in clinical and biomedical research: a prom-ise yet to be fulfilled. Plos Med. 12, e1001918.25. Yoon, D.Y., Mansukhani, N.A., Stubbs, V.C., Heleowski, I.B., Woodruff,T.K., and Kibbe, M.R. (2014). Sex bias exists in basic science and transla-tional surgical research. Surgery 156, 508–516.38. Esnaola, N.F., and Ford, M.E. (2012). Racial differences and disparities incancer care and outcomes: where’s the rub? Surg. Oncol. Clin. N. Am. 21,417–437.39. Churchwell, K., Elkind, M.S.V., Benjamin, R.M., Carson, A.P., Chang, E.K.,Lawrence, W., Mills, A., Odom, T.M., Rodriguez, C.J., et al. (2020). Call toaction: structural racism as a fundamental driver of health disparities: apresidential advisory from the American Heart Association. Circulation+142, e454–e468.40. Vyas, C.M., Donneyong, M., Mischouion, D., Chang, G., Gibson, H., Cook,N.R., Manson, J.E., Reynolds, C.F., 3rd, and Okereke, O.I. (2020). Associ-ation of race and ethnicity with late-life depression severity, symptomburden, and care. JAMA Netw. Open 3, e201606.41. Karlsson Lind, L., von Euler, M., Korkmaz, S., and Schenck-Gustafsson, K.(2017). Sex differences in drugs: the development of a comprehensiveknowledge base to improve gender awareness prescribing. Biol. SexDiffer. 8, 32.42. Kassam, I., Wu, Y., Yang, J., Visscher, P.M., and McRae, A.F. (2019). Tis-sue-specific sex differences in human gene expression. Hum. Mol. Genet.28, 2976–2986.43. Maas, A.H.E.M., and Appelman, Y.E.A. (2010). Gender differences in cor-onary heart disease. Neth. Heart J. Mon. J. Neth. Soc. Cardiol. Neth. HeartFound. 18, 598–602.44. Dorak, M.T., and Karpuzoglu, E. (2012). Gender differences in cancer sus-ceptibility: an inadequately addressed issue. Front. Genet. 3, 268.45. Calderone, K.L. (1990). The influence of gender on the frequency of painand sedative medication administered to postoperative patients. SexRoles 23, 713–725.46. Valanis, B.G., Bowen, D.J., Bassford, T., Whitlock, E., Charney, P., andCarter, R.A. (2000). Sexual orientation and health: comparisons in thewomen’s health initiative sample. Arch. Fam. Med. 9, 843–853.47. The Lancet. (2016). Meeting the unique health-care needs of LGBTQ peo-26. Clayton, J.A., and Collins, F.S. (2014). Policy: NIH to balance sex in cellple. Lancet 387, 95.and animal studies. Nature 509, 282–283.27. Chandak, P., and Tatonetti, N.P. (2020). Using machine learning to identifyadverse drug effects posing increased risk to women. Patterns 1, 100108.28. Henrich, J., Heine, S.J., and Norenzayan, A. (2010). The weirdest people inthe world? Behav. Brain Sci. 33, 61–83.29. Ellwood-Lowe, M.E., Humphreys, K.L., Ordax, S.J., Camacho, M.C., Sac-chet, M.D., and Gotlib, I.H. (2018). Time-varying effects of income on hip-pocampal volume trajectories in adolescent girls. Dev. Cogn. Neurosci.30, 41–50.30. Hackman, D.A., Farah, M.J., and Meaney, M.J. (2010). Socioeconomicstatus and the brain: mechanistic insights from human and animalresearch. Nat. Rev. Neurosci. 11, 651–659.31. Popejoy, A.B., and Fullerton, S.M. (2016). Genomics is failing on diversity.Nature 538, 161–164.32. Neto, E.C. (2018). Using permutations to detect, quantify and correct forconfounding in machine learning predictions. ArXiv.33. He, H., and Garcia, E.A. (2009). Learning from imbalanced data. IEEETrans. Knowl. Data Eng. 21, 1263–1284.34. Agarwal, A., Dudı´k, M., and Wu, Z.S. (2019). Fair regression: quantitativedefinitions and reduction-based algorithms. ArXiv.35. Zhang, B.H., Lemoine, B., and Mitchell, M. (2018). Mitigating unwantedbiases with adversarial learning. ArXiv.48. Morris, M., Cooper, R.L., Ramesh, A., Tabatabai, M., Arcury, T.A., Shinn,M., Im, W., Juarez, P., and Matthew-Juarez, P. (2019). Training to reduceLGBTQ-related bias among medical, nursing, and dental students andproviders: a systematic review. BMC Med. Educ. 19, 325.49. Greshake Tzovaras, B., and Tzovara, A. (2019). The personal data is polit-ical. In The Ethics of Medical Data Donation, vol. 137, J. Krutzinna and L.Floridi, eds. (Springer International Publishing), pp. 133–140.50. Grasso, C., McDowell, M.J., Goldhammer, H., and Keuroghlian, A.S.(2019). Planning and implementing sexual orientation and gender identitydata collection in electronic health records. J. Am. Med. Inform. Assoc.JAMIA 26, 66–70.51. Saposnik, G., Redelmeier, D., Ruff, C.C., and Tobler, P.N. (2016). Cogni-tive biases associated with medical decisions: a systematic review.BMC Med. Inform. Decis. Mak. 16, 138.52. Panch, T., Mattie, H., and Celi, L.A. (2019). The ‘‘inconvenient truth’’ aboutAI in healthcare. NPJ Digit. Med. 2, 77.53. Huston, P., Edge, V.L., and Bernier, E. (2019). Reaping the benefits ofOpen Data in public health. Can. Commun. Dis. Rep. Releve Mal. Transm.Au Can. 45, 252–256.54. Abouelmehdi, K., Beni-Hessane, A., and Khaloufi, H. (2018). Big health-care data: preserving security and privacy. J. Big Data 5, 1.55. Jalaly Bidgoly, A., Jalaly Bidgoly, H., and Arezoumand, Z. (2020). A surveyon methods and challenges in EEG based authentication. Comput. Secur.93, 101788.36. Bellamy, R.K.E., Dey, K., Hind, M., Hoffman, S.C., Houde, S., Kannan, K.,Lohla, P., Martino, J., Mehta, S., Mojsilovic, A., et al. (2019). AI Fairness360: an extensible toolkit for detecting and mitigating algorithmic bias.IBM J. Res. Dev. 63, 4:1–4:15.56. Rieke, N., Hancox, J., Li, W., Milletari, F., Roth, H.R., Albarqouni, S., Ba-kas, S., Galtier, M.N., Landman, B.A., Maier-Hein, K., Ourselin, S., et al.(2020). The future of digital health with federated learning. NPJ Digit.Med. 3, 119.37. Smith-Doerr, L., Alegria, S.N., and Sacco, T. (2017). How diversity mattersin the US science and engineering workforce: a critical review consideringintegration in teams, fields, and organizational contexts. Engag. Sci. Tech-nol. Soc. 3, 139.57. Bolam, M.R., Corbett, L.E., Ellero, N.P., Kenfield, K.S., Mitchell, E.T., Opa-sik, S.A., and Ryszka, D. (2018). Current work in diversity, inclusion andaccessibility by metadata communities: a working report from the ALA/ALCTS metadata standards committee. Tech. Serv. Q. 35, 367–376.8 Patterns 2, October 8, 2021PerspectivellOPEN ACCESS58. Schulz, S., Stegwee, R., and Chronaki, C. (2019). Standards in healthcaredata. In Fundamentals of Clinical Data Science, P. Kubben, M. Dumontier,and A. Dekker, eds. (Springer International Publishing), pp. 19–36. https://doi.org/10.1007/978-3-319-99713-1_3.59. Goodfellow, I.J., et al. (2014). Generative adversarial networks. ArXiv.60. Mikolajczyk, A., and Grochowski, M.(2018). Data augmentation forimproving deep learning in image classification problem. In 2018 Interna-tional Interdisciplinary PhD Workshop (IIPhDW) 117–122 (IEEE). https://doi.org/10.1109/IIPHDW.2018.8388338.61. Greshake Tzovaras, B., et al. (2019). Open Humans: a platform for partic-ipant-centered research and personal data exploration. GigaScience 8,giz076.62. Magrabi, F., Ammenwerth, E., McNair, J.B., De Keizer, N.F., Hypponen,H., Nykanen, P., Rigby, M., Scott, P.J., Vehko, T., Shyui-Lee Wong, Z.,et al. (2019). Artificial intelligence in clinical decision support: challengesfor evaluating AI and practicalimplications: a position paper from theIMIA technology assessment & quality development in health informaticsworking group and the EFMI working group for assessment of health infor-mation systems. Yearb. Med. Inform. 28, 128–134.66. Roscher, R., Bohn, B., Duarte, M.F., and Garcke, J. (2020). Explainablemachine learning for scientific insights and discoveries. IEEE Access 8,42200–42216.67. Price, W.N. (2018). Big data and black-box medical algorithms. Sci. Transl.Med. 10, eaao5333.68. Rieg, T., Frick, J., Baumgartl, H., and Buettner, R. (2020). Demonstration ofthe potential of white-box machine learning approaches to gain insightsfrom cardiovascular disease electrocardiograms. PLoS One 15,e0243615.69. Montavon, G., Samek, W., and M€uller, K.-R. (2018). Methods for interpret-ing and understanding deep neural networks. Digit. Signal. Process.73, 1–15.70. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Ba-tra, D. (2020). Grad-CAM: visual explanations from deep networks viagradient-based localization. Int. J. Comput. Vis. 128, 336–359.71. Cornwall, A., and Jewkes, R. (1995). What is participatory research? Soc.63. Topol, E.J. (2020). Welcoming new guidelines for AI clinical research. Nat.Sci. Med. 41, 1667–1676.Med. 26, 1318–1320.64. Nyk€anen, P., Brender, J., Talmon, J., de Keizer, N., Rigby, M., Beuscart-Zephir, M.C., and Ammenwerth, E. (2011). Guideline for good evaluationpractice in health informatics (GEP-HI). Int. J. Med. Inf. 80, 815–827.72. Lewis, D., Leibrand, S., and #OpenAPS Community. (2016). Real-worlduse of open source artificial Pancreas systems. J. Diabetes Sci. Technol.10, 1411.65. Kelly, C.J., Karthikesalingam, A., Suleyman, M., Corrado, G., and King, D.(2019). Key challenges for delivering clinical impact with artificial intelli-gence. BMC Med. 17, 195.73. Wymant, C., Ferretti, L., Tsallis, D., Charalambides, M., Abeler-Dorner, L.,Bonsall, D., Hinch, R., Kendell, M., Milsom, L., Ayres, M., et al. (2021). Theepidemiological impact of the NHS COVID-19 app. Nature 594, 408–412.Patterns 2, October 8, 2021 9