8102voN9]GL.sc[1v06730.1181:viXraEA-LSTM: Evolutionary Attention-based LSTMfor Time Series PredictionYouru Li†,‡, Zhenfeng Zhu†,‡, Deqiang Kong≡, Hua HanΞ, Yao Zhao†,‡,† Institute of Information Science, Beijing Jiaotong University, Beijing, China‡ Beijing Key Laboratory of Advanced Information Scienceand Network Technology, Beijing, China≡ Microsoft Multimedia, Beijing, ChinaΞ National Laboratory of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences (CAS), Beijing, China†,‡ {liyouru,zhfzhu,xumx0721,yzhao}@bjtu.edu.cn,≡ kodeqian@microsoft.com Ξ hua.han@ia.ac.cnAbstractTime series prediction with deep learning meth-ods, especially long short-term memory neural net-works (LSTMs), have scored significant achieve-ments in recent years. Despite the fact that theLSTMs can help to capture long-term dependen-cies, its ability to pay different degree of attentionon sub-window feature within multiple time-stepsis insufficient. To address this issue, an evolu-tionary attention-based LSTM training with com-petitive random search is proposed for multivari-ate time series prediction. By transferring sharedparameters, an evolutionary attention learning ap-proach is introduced to the LSTMs model. Thus,like that for biological evolution, the pattern forimportance-based attention sampling can be con-firmed during temporal relationship mining. To re-frain from being trapped into partial optimizationlike traditional gradient-based methods, an evolu-tionary computation inspired competitive randomsearch method is proposed, which can well config-ure the parameters in the attention layer. Exper-imental results have illustrated that the proposedmodel can achieve competetive prediction perfor-mance compared with other baseline methods.1 IntroductionA time series is a series of data points indexed in time or-der. Effective prediction of time series can make better useof existing information for analysis and decision-making. Itswide range of applications includes but not limited to clini-cal medicine [Liu et al., 2018], financial forecasting [Cao etal., 2015], traffic flow prediction [Hulot et al., 2018], humanaction prediction[Du et al., 2015] and other fields. Differentfrom other prediction modeling tasks, time series adds thecomplexity of sequence dependence among the input vari-It is crucial to build a suitable predictive model forables.the real data so as to make good use of the complex sequencedependencies.The research on the time series prediction began withthe introduction of regression equations [Yule, 1927] inthe prediction of the number of sunspots over a year forthe data analysis.The auto-regressive moving averagemodel (ARMA) and auto-regressive integrated moving av-erage model (ARIMA) [Box and Pierce, 1968] indicate thatthe time series prediction modeling based on the regressionmethod gradually becomes mature. Therefore, such mod-els also become the most basic and important ones in timeseries prediction. Due to the high complexity, irregularity,randomness and non-linearity of real data, it is very diffi-cult for the methods above to achieve high-accuracy pre-diction through complex models. With machine learningmethods, people build nonlinear prediction model based ona large number of historical time data. The fact is that wecan obtain more accurate prediction results than traditionalstatistic-based models through repeated iterations of trainingand learning to approximate the real model. Typical methodssuch as support vector regression or classification [Drucker etal., 1996] based on kernel method and artificial neural net-works (ANN) [Davoian and Lippe, 2007] with the strongnonlinear function approximation ability and tree-based en-semble learning method, for instance, gradient boosting re-gression or decision tree (GBRT, GBDT) [Li and Bai, 2016;Ke et al., 2017]. However, methods mentioned above beginto expose their own defects in dealing with the sequence de-pendence among input variables in time series prediction.The most commonly used and effective tool for time se-quence model is recurrent neural network, or RNN [Rumel-hart et al., 1986]. In normal neural networks, calculation re-sults are mutually dependent, yet those of hidden layers in   Figure 1: Graphical illustration of training evolutionary attention-based LSTM with competitive random search. This figure is composed oftwo parts. The left part displays the process of competitive random search, and the right part the structure of evolutionary attention-basedLSTM. On the right, each sample: Xt = (x1t ) in the training set X = (X1, X2, ..., XT ) multiplies attention weight Wi, thelearning result of the left part, producing ˜Xt = (x1i ), and ˜Xt are respectively sent to LSTMs for training. Finally,i , x2the error between the predication result ˜yT and the real value yT is obtained in the validation set. The left part consists of a loop where theinitial optimization sipace W = (W1, W2, ..., WN ) is established in “a”, and the subspace Wi is encoded into W B = (W B2 , ..., W BN )through binary code and sent to “b”. Meanwhile, Wi are respectively transferred to the right network and the corresponding loss evaluation isgained in accordance with the prediction error of the network. Then, the champion subspace set ˜W is selected according to the loss situationof ˜W B in “c”, and its subset combination is traversed repeatedly. Finally, the optimization space is reestablished in the light of operations inthe red dotted box and W , the new-generation optimization space, is produced.t , ..., xLt W 1i , ..., xL1 , W Bt W Lt W 2t , x2RNN are highly relevant to the current input as well as thoseproduced last time in hidden layers. However, with longerdriving sequence, problems such as vanishing gradient oftenappear in the training of RNN with commonly-used activa-tion functions, e.g., tanh or sigmoid functions which limit theprediction accuracy of this model. The long short-term mem-ory units (LSTMs) was proposed [Hochreiter and Schmid-huber, 1997] based on the original RNN which mediates thebalance between memorizing and forgetting by adding somemultiple threshold gates. LSTMs and the gated recurrent unit(GRU) [Cho et al., 2014a] address the limited ability to dealwith the long-term dependencies. These methods have led tothe successful application for many sequence learning prob-lems like machine translation [Cho et al., 2014b]. There-fore, the LSTMs is generally regarded as one of the state-of-the-art methods to deal with the time series prediction prob-lem. Learning from cognitive neuroscience, some researchersintroduce attention mechanisms to the encoding-decodingframework [Bahdanau et al., 2014] to better select from in-put series and encode the information in long-term memoryto improve information processing ability. Recently, atten-tion mechanisms have been widely used and performed wellin many different types of deep learning tasks, such as imagecaptioning [Lu et al., 2017], visual question answering [Yu etal., 2017] and speech recognition [Kim et al., 2017]. Addi-tionally, in recent years, some related work [Qin et al., 2017;Liang et al., 2018] on time series prediction is improvedusually by introducing attention layers into the encoding-decoding framework.Time series prediction is usually performed through slid-ing time-window feature and make prediction depends on theorder of events. Firstly, we establish a multi-variate temporalprediction model based on LSTMs. Then, inspired by howhuman brain process input information with attention mech-anism, we add an attention layer into the LSTMs. The intro-duced attention mechanism can quantitatively attach weightto period with diverse importance in the sliding time windowso as to avoid being attention-distracted which is the primarily(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:4)(cid:410)(cid:410)(cid:286)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)(cid:3)(cid:367)(cid:258)(cid:455)(cid:286)(cid:396)(cid:62)(cid:94)(cid:100)(cid:68)(cid:3)(cid:367)(cid:258)(cid:455)(cid:286)(cid:396)(cid:400)(cid:856)(cid:856)(cid:856)(cid:24)(cid:286)(cid:272)(cid:381)(cid:282)(cid:286)(cid:856)(cid:856)(cid:856)(cid:18)(cid:381)(cid:373)(cid:393)(cid:286)(cid:410)(cid:349)(cid:410)(cid:349)(cid:381)(cid:374)(cid:90)(cid:286)(cid:271)(cid:437)(cid:349)(cid:367)(cid:282)(cid:349)(cid:374)(cid:336)(cid:62)(cid:381)(cid:400)(cid:400)(cid:3)(cid:296)(cid:286)(cid:286)(cid:282)(cid:271)(cid:258)(cid:272)(cid:364)(cid:3)(cid:87)(cid:258)(cid:396)(cid:258)(cid:373)(cid:286)(cid:410)(cid:286)(cid:396)(cid:3)(cid:410)(cid:396)(cid:258)(cid:374)(cid:400)(cid:296)(cid:286)(cid:396)(cid:349)(cid:374)(cid:336)(cid:3)(cid:90)(cid:286)(cid:271)(cid:437)(cid:349)(cid:367)(cid:282)(cid:349)(cid:374)(cid:336)(cid:3)(cid:282)(cid:286)(cid:410)(cid:258)(cid:349)(cid:367)(cid:400)(cid:3)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:28)(cid:374)(cid:272)(cid:381)(cid:282)(cid:286)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:62)(cid:94)(cid:100)(cid:68)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:94)(cid:367)(cid:349)(cid:282)(cid:349)(cid:374)(cid:336)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:1004)(cid:856)(cid:1007)(cid:1004)(cid:856)(cid:1006)(cid:1004)(cid:856)(cid:1011)(cid:1004)(cid:856)(cid:1012)(cid:1004)(cid:856)(cid:1005)(cid:1004)(cid:856)(cid:1013)(cid:1004)(cid:856)(cid:1010)(cid:1004)(cid:856)(cid:1007)(cid:1004)(cid:856)(cid:1008)(cid:1004)(cid:856)(cid:1009)(cid:1004)(cid:856)(cid:1007)(cid:1005)(cid:856)(cid:1004)(cid:11)(cid:11)(cid:11)(cid:15)(cid:12)(cid:12)(cid:15)(cid:12)(cid:55)(cid:76)(cid:55)(cid:47)(cid:92)(cid:41)(cid:58)(cid:92)(cid:52)(cid:4)(cid:37)(cid:76)(cid:58)(cid:37)(cid:77)(cid:58)(cid:37)(cid:78)(cid:58)(cid:37)(cid:78)(cid:58)(cid:20)(cid:58)(cid:76)(cid:58)(cid:49)(cid:58)(cid:20)(cid:76)(cid:58)(cid:21)(cid:47)(cid:76)(cid:58)(cid:16)(cid:20)(cid:47)(cid:76)(cid:58)(cid:16)(cid:47)(cid:76)(cid:58)(cid:20)(cid:21)(cid:94)(cid:15)(cid:15)(cid:17)(cid:17)(cid:17)(cid:15)(cid:96)(cid:47)(cid:87)(cid:87)(cid:87)(cid:87)(cid:59)(cid:91)(cid:91)(cid:91)(cid:32)(cid:20)(cid:76)(cid:86)(cid:21)(cid:76)(cid:86)(cid:20)(cid:77)(cid:86)(cid:21)(cid:77)(cid:86)(cid:20)(cid:47)(cid:76)(cid:86)(cid:16)(cid:20)(cid:47)(cid:77)(cid:86)(cid:16)(cid:47)(cid:77)(cid:86)(cid:47)(cid:76)(cid:86)(cid:11)(cid:12)(cid:47)(cid:60)(cid:20)(cid:19)(cid:19)(cid:20)(cid:20)(cid:20)(cid:20)(cid:47)(cid:77)(cid:86)(cid:16)(cid:20)(cid:47)(cid:76)(cid:86)(cid:16)(cid:19)(cid:19)(cid:19)(cid:20)(cid:20)(cid:20)(cid:20)(cid:47)(cid:78)(cid:86)(cid:16)(cid:19)(cid:19)(cid:20)(cid:20)(cid:20)(cid:20)(cid:20)(cid:76)(cid:86)(cid:20)(cid:47)(cid:78)(cid:86)(cid:16)(cid:21)(cid:76)(cid:86)(cid:47)(cid:76)(cid:86)(cid:11)(cid:12)(cid:42)(cid:60)(cid:11)(cid:12)(cid:48)(cid:60)(cid:41)(cid:20)(cid:37)(cid:58)(cid:37)(cid:76)(cid:58)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:37)(cid:49)(cid:58)(cid:20)(cid:20)(cid:21)(cid:21)(cid:94)(cid:15)(cid:15)(cid:17)(cid:17)(cid:17)(cid:15)(cid:96)(cid:47)(cid:47)(cid:87)(cid:87)(cid:76)(cid:87)(cid:76)(cid:87)(cid:76)(cid:59)(cid:91)(cid:58)(cid:91)(cid:58)(cid:91)(cid:58)(cid:32)(cid:4)(cid:60)(cid:60)(cid:60)(cid:59)(cid:92)(cid:258)(cid:271)(cid:272)(cid:282)(cid:58)(cid:4)(cid:20)(cid:21)(cid:11)(cid:15)(cid:15)(cid:17)(cid:17)(cid:17)(cid:15)(cid:12)(cid:55)(cid:55)(cid:92)(cid:92)(cid:92)(cid:92)(cid:32)(cid:21)(cid:76)(cid:58)(cid:22)(cid:76)(cid:58)(cid:20)(cid:21)(cid:11)(cid:15)(cid:15)(cid:17)(cid:17)(cid:17)(cid:15)(cid:12)(cid:55)(cid:55)(cid:92)(cid:92)(cid:92)(cid:92)(cid:32)(cid:4)(cid:4)(cid:4)(cid:4)insufficient nature in traditional LSTMs. Specifically, insteadof gradient-based methods, a competitive random search isemployed to train the attention mechanism with referenceto evolutionary computation and genetic algorithm[Holland,1973]. When approximating the optimum solution, the ran-dom searching operators adopted can divert searching direc-tion to the largest extent so as to avoid being trapped by lo-cal optimum solution[Zhang et al., 2017; Conti et al., 2017;Lehman et al., 2017]. As a result, compared with the tradi-tional gradient-based one, competitive random search boastsstronger global searching ability when solving parameters inattention layer. So we can take advantage of this method tofurther improve the prediction accuracy of the LSTMs. Todemonstrate the preformance, we conduct some experimentson real time series prediction datasets in both regression andclassification tasks to compare it with some other baselinemethods. The results show that the peoposed method can pro-duce higher prediction accuracy than other baseline methods.2 PreliminariesIn this section, formulation and description of the prob-lem will be displayed. Time series prediction which canbe divided into regression or classification problems usu-ally uses a historical sequence of values as the input data.Given sliding-window feature matrix of training series X =(X1, X2, ..., XT ) and Xt = (x1t ), where Xt ∈ X.Meanwhile, we define the length of time-step as L. Typically,historical values y = (y1, y2, ..., yT −1) are also given. As forclassification problems, the historical values y are discrete.t , ..., xLt , x2Generally, we learn a nonlinear mapping function by usingthe history-driven sequence feature X and its correspondingtarget value y to obtain the predicted value ˜yT with the fol-lowing formulation:˜yT = f (X, y)(1)where mapping f (·) is the nonliner mapping function we aimto learn.3 MethodologyIn this section, we will introduce the evolutionary attentionbased-LSTM and the competitive random search and presenthow to train this model in detail. In this part, we first givethe overview of the model we proposed. Then, we will de-tail the evolutionary attention-based LSTM. Furthermore, wepresent the competitive random search and a collaborativetraining mechanism to train the model. A graphical illustra-tion is shown in Figure 1.3.1 OverviewThe idea of an evolutionary attention-based LSTM is to intro-duce a layer of attention to the basic LSTMs network. Thisenables the LSTMs networks not only to handle the long-term dependencies of drive sequences over historical timesteps, but also an importance-based sampling. To avoid beingtrapped, we learn the attention weights by a competitive ran-dom search referring to evolutionary computation. To trainthe model, a collaborative meachaism is proposed. Attentionweight that is learnt from the competitive random search istransferred to evolutionary attention-based LSTM networksfor time series prediction. Meanwhile, predicted errors, asthe feedback, are sent to direct the searching process.3.2 Temporal Modeling with EA-LSTMTraditional methods generally model the time series predic-tion problem with hand-crafted features and make the predic-tion by well-designed regressors. Recurrent neural network(RNN) is chosen because of its capability to model long-termhistorical information of temporal sequences. Despite of somany basic LSTMs variants for capturing long-time depen-dencies proposed recently, a large-scale analysis shows thatnone of them can improve the performance in this issue sig-nificantly [Hochreiter and Schmidhuber, 1997]. Therefore,we solve the problem of long-term dependence by replacingthe simple RNN unit with the LSTMs neuron structure in therecurrent neural network. The LSTMs is a special kind ofRNN. With its gated structure, including the forget gate, theinput gate and the output gate, LSTMs can memorize whatshould be memorized and forget what should be forgot. Espe-cially, the forget gate is the first operator in LSTMs to decidewhat information in last time-step should be dropped with asigmoid function. It is a key operator in its gated structure.Firstly, we define the attention weights as:W = (W 1, W 2, ..., W L)(2)with these attention weights, we can take importance-basedsampling for input data with˜Xt = (x1t W 1, x2t W 2, ..., xLt W L)(3)Then, ˜X = ( ˜X1, ˜X2, ..., ˜XT ) is fed into LSTM networks.Furthermore, we can learn the nonliner mapping function bythese formulations [Graves, 2012] of the calculating processin LSTMs cells as follows:it = σ(Wxi ˜Xt + Whiht−1 + Wcict−1 + bi)f t = σ(Wxf ˜Xt + Whf ht−1 + Wcf ct−1 + bf )ct = f tct−1 + it tanh(Wxc ˜Xt + Whcht−1 + bc)ot = σ(Wxo ˜Xt + Whoht−1 + Wcoct−1 + bo)ht = ot tanh(ct)(4)(5)(6)(7)(8)where σ(·) represents the activation function of sigmoid andW matrices with double subscript the connection weights be-tween the two cells. In addition, it represents input gate state,f t forget gate state, ct cell state, ot output gate and ht thehidden layer output in current time-step. Finally, we can takethe last element of output vector ht−1 as the predicted value.It can be represented as:˜yt = ht−1the final output value can be contacted to a vector:˜yT = (˜y1, ˜y2, ..., ˜yT )(9)(10)3.3 Competitive Random SearchAlgorithm 1 Competitive Random SearchBased on genetic algorithm, competitive random search(CRS) is proposed to generate the optimum parameter combi-nations in the attention layer of LSTM network. The detailedprocess of the CRS is elaborated in Figure 1. The CRS con-sists of four parts which are introduced as follows.1 , W B2 , ..., W BIn Figure 1, attention weights set W = (W1, W2, ..., WN )is given in “a”. While being translated into W B =(W BN ) through binary code and sent into “b”,the subset Wi which denotes attention weights are transferredinto networks in the right part and produce a correspondingloss value according to predicted error in the networks. Then,the champion attention weights subset ˜W is selected accord-ing to the loss of W B = (W BN ) in “c”, andits subset combination is traversed repeatedly. Finally, as isshown in the red dotted box, a new attention wights is rebuiltand W Bk , the new-generation optimization subspace, is pro-duced.2 , ..., W B1 , W BRandom operators are introduced to illustrate how opti-mization space is rebuilt in “d”. In the red dotted box in Fig-ure 1, if the selected champion combination is W Bi and W Bjwhere each individual is composed of binary strings, they willbe evenly divided into L segments in line with L, the time stepdefined in section 2.1. Then, the corresponding W Bi can bei , S2expressed by W Bi is a segmentof W Bi , ..., SLi . Two important operators are described as follows:• Randomly select. Firstly, we define this opreator asΛ(·). Its function is to randomly select subsegments ineach champion combination. For instance, in Figure 1,the subsegment L − 1 of the two subspaces are selected.It should be noted that the number of selected subsectionis not fixed.i ) where S1i = (S1• Recombine. This opreator can be expressed as Γ(·). Itis defined to recombine the genes in the selected sub-segment. The process interchanges the two subsegmentsexpressed by binary codes with the length of 6 and fromdifferent subspaces in either even or odd index. sL−1and sL−1after Γ(·). It should alsojbe noted that the figure only displays interchange in theeven index, but the index where actual interchanges hap-pen is decided by the random judgment of Γ(·).will generate sL−1kiAfter the abovementioned two steps, gene mutation, a link inbiological evolution, is imitated. The operator M (·) is set toreverse the genotype of the newly generated sL−1in a ran-dom index. For instance, 0 is reversed to 1. Finally, sL−1kreplaces the corresponding sL−1k whichis inserted into W . When rebuilding optimization space, wewill repeatedly traverse subspace ˜W until the size of W hasreached the default value N . The key factor in the CRS is theerror feedback introduced from the right network in Figure 1.The CRS is demonstrated in the optimizing issue as follows:i , forming W Bin W Bkimin L( ˜yT (Θ(F, W )), yT )(11)where F are entire parameters in LSTM networks and Θ(·)is the parameter space needed when obtaining the predictedInput:N : size of attention weights set, T : epochs , ˜W : cham-pion attention weights subset with the size of ˜N , L =(L1, L2, ..., LN ): loss of each Wi ∈ WOutput:W : attention weights setW ←(W1, W2, ..., WN )˜W ←Ranking(Wi|Li, ˜N )W ← ∅while length(W )<N doW ← ˜Wfor (Wi, Wj) ∈ ˜W doelse1: while t < T doif t = 0 then2:3:4:5:6:7:8:9:10:11:12:13:14:15: end whileend whileend ifend forW ← WkWk ← M (Γ(Λ(Wi, Wj)))value ˜yT . The most important operator is the rebuilding pro-cess which can determines the performance significantly bycontrolling the direction of the random searching. Algoithm1 outlines the competitive random search.3.4 Parameters TransferringTo train our model, we proposed a collaborative mecha-nism which combines EA-LSTM with the competitive ran-dom search. The idea of collaborative training is to sharethe parameters and loss feedback between the two compo-nents of the model. We use mini-batch stochastic gradientdecent (SGD) together with Adam optimizer [Kingma andBa, 2014] to train EA-LSTM. Except for attention layer, theother parameter in LSTMs can be learned by standard backpropagation through time algorithm with mean squared errorand cross entropy loss as the objective function. Meanwhile,the attention weights outputted by competitive random searchwill be fed into attention layer before the LSTM network be-gin to be trained. In addition, the current prediction loss of theLSTMs in validation set will be used to rank the optimizationspace.4 ExperimentsIn this section, the description of datasets used in our researchis given firstly. Then we will introduce the parameter settingsand show the training result of EA-LSTM. Furthermore, wecompare the model we proposed with some baseline modelse.g., SVR, GBRT, RNN, GRU and LSTM. In addition, severalattention-based methods also as the competitors to verify theperformance of our proposed model.4.1 Datasets Description and SetupTo compare the performance of different models with variedtypes of time series prediction problem, datasets used in ourTable 1: Statistic of Two Datasets for Regression TasksDatasetBeijing PM2.5SML 2010Sensors Train & Vaild81635,0403,600Test8,760537experiments are described as follows:• Beijing PM2.5 Data1 (PM2.5). This dataset [Liang etal., 2015] contains the PM2.5 data of US Embassy inBeijing with an hour sampling rate between January 1st,2010 and December 31st, 2014. Meanwhile, meteoro-logical data from Beijing Capital International AirportIts sensor data e.g., current time,are also included.PM2.5 concentration, dew point, temperature, pressure,wind direction, wind speed, hours of snow, hours of rain.The PM2.5 concentration is the target value to predict inthe experiments.• SML20102 (SML). It is a uci open dataset [Zamora-Mart´ınez et al., 2014] used for indoor temperature pre-diction. This dataset is collected from a monitor sys-tem mounted in a domotic house.It corresponds toapproximately 40 days of monitoring data. The datawas sampled every minute, computing and uploading itsmoothed with 15 minute means. The sensor data we useincludes current time, weather forecast temperature, car-bon dioxide, relative humidity, lighting, rain, sun dusk,wind, sun light in west facade, sun light in east facade,sun light in south facade, sun irradiance, Enthalpic mo-tor 1 and 2, Enthalpic motor turbo, outdoor temperature,outdoor relative humidity, and day of the week. Theroom temperature is the target value to predict in ourexperiments.• MSR Action3D Dataset3(MSR). MSR Action3Ddataset contains twenty actions: high arm wave, hori-zontal arm wave, hammer, hand catch, forward punch,high throw, draw x, draw tick, draw circle, hand clap,two hand wave, side-boxing, bend, forward kick, sidekick, jogging, tennis swing, tennis serve, golf swing,pick up and throw. There are 10 subjects, each subjectperforms each action 2 or 3 times. There are 567 depthmap sequences in total. The resolution is 320x240. Thedata was recorded with a depth sensor similar to theKinect device.The setting for PM2.5 and SML dataset is given in Table1. In addition, as for MSR dataset, we follow the standardsetting provided in [Du et al., 2015] and calculate the averageaccuracy for comparison.4.2 Parameter Settings and SensitivityThere are three parameters in the basic LSTM model, i.e.,the number of time steps L and the size of hidden unitsfor each layers in LSTM m (we set the same hidden units1http://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data2http://archive.ics.uci.edu/ml/datasets/SML20103http://research.microsoft.com/en-us/um/people/zliu/actionrecorsrc/.for each layer in LSTM) and the batchsize b in trainingprocess. We carefully tuned the parameters L (time-steps),m (hidden units number) and b (batchsize) for our basicmodel. To approximate the best performance of the model,we conducted a grid search over L ∈ {3, 6, 12, 18, 24},m ∈ {16, 32, 64, 128, 256}, b ∈ {64, 128, 256, 512, 1024}in the Beijing PM2.5 dataset and L ∈ {6, 12, 18, 24, 36} ,m ∈ {16, 32, 64, 128, 256}, b ∈ {64, 128, 256, 512, 1024}in the SML2010 dataset and m ∈ {16, 32, 64, 128}, b ∈{8, 16, 32, 64} in the MSR Action3D dataset. It should benoted that in MSR dataset we set the number of frames ineach sample as the L. When one parameter vaies, the othersare fixed. Finally, we achieve the hyperparameters with thebest performance over the validation set which are used to fixthe basic model structure. The box line diagram plotted inFigure 2-3 is used to show the sensitivity of the parameterson two dataset used for regression tasks.The root means squared error for the time series task withone box-whisker (showing middle value, 25% and 75% quan-tiles, minimum, maximum and outliers) for five testing resultsof the basic model we proposed. After grid searching, wedefine the hyperparameters used in EA-LSTM with the bestones. The hyperparameters of LSTM in differents datasetsare given in Table 2.Furthermore, there are four hyperparameters in compet-itive random search, i.e., the size of attention weights setN , the encoding length for each attention weights, the sizeof champion attention weights subset ˜W and the number ofepochs T . To balance the solving efficiency, we defined sizeof optimization apace as 36, encoding length for each sub-space as 6 which varies from 0.016 to 1.000, the size of ˜W as6, and the number of epochs T as 20.Table 2: Hyperparameters of LSTM in Each DatasetDatasetBeijing PM2.5SML 2010MSR Action3DTime-Steps Units Batchsize128128128256128161824134.3 Evaluation MetricsTo evaluate the performance, we take the Root Mean SquaredErrors (RMSE) [Plutowski et al., 1996] and Mean AbsoluteErrors (MAE) as the evaluation metrics. They are calculatedby the following.RM SE =(cid:118)(cid:117)(cid:117)(cid:116)1NN(cid:88)(˜yit − yit)2i=1M AE =1NN(cid:88)i=1|˜yit − yit|(12)(13)where ˜yiof testing samples.t is prediction, yit is real value and N is the numberFigure 2: Parameter Sensitivity in Beijing PM2.5 DatasetFigure 3: Parameter Sensitivity in SML2010 Dataset4.4 Training Attention LayerWe trained the EA-LSTM with Competitive random searchfor 20 epochs. Training processes are visualized in Figure 4.In addition, the points drawn in Figure 4 indicate the error andaccuracy corresponding to the weights selected in championsubspace ˜W . We can find that training with the CRS, atten-tion weights in optimization space continuously improve theperformance and not be trapped. Meanwhile, to better under-stand importance-based sampling of input series within timesteps, the most suitable attention weights are visualized byheat map and showed in Figure5. In Figure 5, varied scaleof attention distribution of input driving series within mul-tiple time steps over each datasets are showed as well. Bysolving attention weights which can better suits for the char-acteristics across different tasks, we improve the performanceof the LSTMs and get better prediction results. In addition,we can also find that the proposed method effectively utilizelocal information within one sampling window according tovaried scale of attention distribution in Figure 5. It is crucialto make a soft feature selection in multiple time steps timeseries prediction.4.5 Performance ComparisonTo evaluate the performance of the EA-LSTM training withcompetitive random search in time series prediction, we setcontrast experiments with some other baseline methods, in-cluding traditional machine learning methods and deep learn-ing methods. In experiments, the SVR, GBRT, RNN, LSTMsand GRU as competitors are carefully tuned respectively. Inaddition, all the baseline methods we compared were trainedand tested for five times and final prediction results showedin Table 3 were averaged to reduce random errors. We cansee that the proposed method effectively improved the per-formance against to its baseline counterparts in both publicopen benchmarking datasets uaually uesd for time series pre-diction.Furthermore, we also compared the proposed method withDA-RNN [Qin et al., 2017] in our public testing dataset:SML 2010. DA-RNN, which is similar to the traditionalattention-based model,is a time series predictive modeltrained by solving the network parameters together withattention-layer parameters. As a matter of fact, this modelobtained the state-of-the-art performance by constructing amore complex attention mechanism. With the dataset iden-tically classified into sets for training, validating, and test-ing, the experimental results show that the EA-LSTM can geta higher predicted precision. We can also see that there isthe feasibility to enhance attention-based model by improvingtraining method for attention layer not only by introducing amore complex attention mechanism.In addition, we compared the proposed method with thesame method whose optimization method is replaced withgradient descent which named ”Attention-LSTM” to clearlyhighlight the benefit of using the competitive random search,24183126Time Steps1.11.21.31.41.51.61.71.81.9RMSEBeijing PM2.5 Dataset128326425616Hidden Units0.751.001.251.501.752.002.252.50RMSEBeijing PM2.5 Dataset128256512641024Batchsize0.70.80.91.01.11.21.31.41.5RMSEBeijing PM2.5 Dataset241812366Time Steps0.0200.0250.0300.0350.0400.0450.050RMSESML2010 Dataset128326425616Hidden Units0.0200.0250.0300.0350.0400.0450.0500.055RMSESML2010 Dataset128256512641024Batchsize0.020.030.040.050.060.070.08RMSESML2010 DatasetFigure 4: Training Process of Competitive Random Search. The points drawn in figure indicates the error calculation results of each weightin champion subspace ˜W for each epoch. There are six champion weights with blue spot and the best one with red spot for each epoch in thefigure. It should be noted that points in the third subfigure shows the accuracy curve of EA-LSTM.Table 3: Performance of Different Baseline Methods Compared in Two Datasets for Regression TasksModelBeijing PM2.5SML2010DatasetsSVRGBRTRNNGRULSTMAttention-LSTMDA-RNN [Qin et al., 2017]EA-LSTMMAE2.67790.99090.86460.67330.61680.2324——0.1902RMSE2.86231.05760.96210.74330.70260.3619——0.2755MAE0.05580.02530.02610.02310.01780.01900.01500.0103RMSE0.06520.03270.03670.02880.02340.02250.01970.0154instead of gradient descent. Specifically, an input-attentionlayer whose weights are learned together with other parame-ters is introduced to LSTM networks. The experimental re-sults clearly highlight the benefit of using the evolutionarycomputation inspired competitive random search to refrainfrom being trapped into partial optimization effectively, in-stead of gradient descent.Table 4: Experimental Results on The MSR Action3D Dataset.Methods[Gowayyed et al., 2013][Vemulapalli et al., 2014]HBRNN [Du et al., 2015]LSTMAttention-LSTMEA-LSTMAccuracy/%91.2692.4694.4990.6792.5895.20More general, we also add a comparison between our pro-posed model and some baseline ones by human action recog-nition experiments, a typical time series prediction task fortesting the ability to take temporal modeling of differentmethods. The experimental results testify that the proposedmethod also delivers robust performance even in classifica-tion prediction tasks.5 ConclusionThis paper proposed an evolutionary attention-based LSTMmodel (EA-LSTM) which is trained with competitive randomsearch for time series prediction. The parameters of attentionlayer used for importance-based sampling in the proposedEA-LSTM networks can be confirmed during temporal rela-tionship mining. Thus, this network is able to properly settlethe local feature relationship within time-steps. As a hardoptimization issue to approximate the best attention weightsfor real input driving series data, the CRS method we pro-posed can avoid being trapped during the parameters solving.Experiments show the EA-LSTM can make competitive pre-diction performance compared with the state-of-the-art meth-ods. These results demonstrate that training evolutionaryattention-based LSTM with competitive random search cannot only help to capture long-term dependencies in time se-ries prediction, but also effectively utilize local informationwithin one sampling window according to varied scale of at-tention distribution. Besides, taking genetic algorithm as anexample, this paper introduces evolutionary computation tosubstructure training in deep neural networks, which achievedgood performance in experiments. For future work, more2.55.07.510.012.515.017.520.0Epochs012345RMSEBeijing PM2.5 Dataset2.55.07.510.012.515.017.520.0Epochs0.0000.0250.0500.0750.1000.1250.1500.1750.200RMSESML2010 Dataset2.55.07.510.012.515.017.520.0Epochs80.082.585.087.590.092.595.097.5100.0Accuracy/%MSR Action3D Dataset[Conti et al., 2017] E. Conti, V. Madhavan, F. Petroski Such,J. Lehman, K. O. Stanley, and J. Clune.Improving Ex-ploration in Evolution Strategies for Deep ReinforcementLearning via a Population of Novelty-Seeking Agents.arXiv:1712.06560, 2017.[Davoian and Lippe, 2007] Kristina Davoian and Wolfram-Manfred Lippe. Time series prediction with parallel evo-lutionary artificial neural networks. In ICDM 2007, pages10–15, 2007.[Drucker et al., 1996] Harris Drucker, Christopher J. C.Burges, Linda Kaufman, Alexander J. Smola, andVladimir Vapnik. Support vector regression machines. InNIPS, pages 155–161, 1996.[Du et al., 2015] Yong Du, Wei Wang, and Liang Wang. Hi-erarchical recurrent neural network for skeleton based ac-tion recognition. In CVPR 2015, pages 1110–1118, 2015.[Gowayyed et al., 2013] Mohammad Abdelaziz Gowayyed,Marwan Torki, Mohamed Elsayed Hussein, and Motaz El-Saban. Histogram of oriented displacements (HOD): de-scribing trajectories of human joints for action recognition.In IJCAI 2013, pages 1351–1357, 2013.[Graves, 2012] Alex Graves. Supervised Sequence Labellingwith Recurrent Neural Networks, volume 385 of Studies inComputational Intelligence. Springer, 2012.[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter andJ¨urgen Schmidhuber. Long short-term memory. NeuralComputation, 9(8):1735–1780, 1997.[Holland, 1973] John H. Holland. Genetic algorithms andthe optimal allocation of trials. SIAM J. Comput., 2(2):88–105, 1973.[Hulot et al., 2018] Pierre Hulot, Daniel Aloise, and San-jay Dominik Jena. Towards station-level demand predic-tion for effective rebalancing in bike-sharing systems. InProceedings of the 24th ACM SIGKDD International Con-ference on Knowledge Discovery & Data Mining, 2018,pages 378–386, 2018.[Ke et al., 2017] Guolin Ke, Qi Meng, Thomas Finley,Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, andTie-Yan Liu. Lightgbm: A highly efficient gradient boost-ing decision tree. In NIPS 2017, pages 3149–3157, 2017.[Kim et al., 2017] Suyoun Kim, Takaaki Hori, and ShinjiWatanabe.Joint ctc-attention based end-to-end speechrecognition using multi-task learning. In ICASSP, pages4835–4839, 2017.[Kingma and Ba, 2014] Diederik P. Kingma and JimmyAdam: A method for stochastic optimization.Ba.arXiv:1412.6980, 2014.[Lehman et al., 2017] Joel Lehman, Jay Chen, Jeff Clune,and Kenneth O. Stanley.Safe mutations for deepand recurrent neural networks through output gradients.arXiv:1712.06563, 2017.[Li and Bai, 2016] Xia Li and Ruibin Bai. Freight vehicletravel time prediction using gradient boosting regressiontree. In ICMLA, pages 1010–1015, 2016.Figure 5: Plot of the attention distribution for each time-step ofthe features extracted by sliding-time-window in three experimen-tal datasets, in which the coordinates represent the time steps of theinput driving series.studies inspired by biological rules will be employed to im-prove the perfromance of neural networks which are hard totrain.6 AckowledgmentsThis work was jointly sponsored by the National Key Re-search and Development of China (No.2016YFB0800404)and the National Natural Science Foundation of China(No.61572068, No.61532005) and the Fundamental Re-search Fundsthe Central Universities of Chinafor(No.2018YJS032).References[Bahdanau et al., 2014] Dzmitry Bahdanau, KyunghyunCho, and Yoshua Bengio. Neural machine translation byjointly learning to align and translate. Computer Science,2014.[Box and Pierce, 1968] G. E. P. Box and Davida. Pierce.Distribution of residual autocorrelations in autoregressive-Pub-integrated moving average time series models.lications ofthe American Statistical Association,65(332):1509–1526, 1968.[Cao et al., 2015] Wei Cao, Liang Hu, and Longbing Cao.Deep modeling complex couplings within financial mar-kets. In AAAI 2015, pages 2518–2524, 2015.[Cho et al., 2014a] Kyunghyun Cho, Bart van Merrienboer,Dzmitry Bahdanau, and Yoshua Bengio. On the proper-ties of neural machine translation: Encoder-decoder ap-proaches. In EMNLP, pages 103–111, 2014.[Cho et al., 2014b] Kyunghyun Cho, Bart van Merrienboer,C¸ aglar G¨ulc¸ehre, Dzmitry Bahdanau, Fethi Bougares,Holger Schwenk, and Yoshua Bengio. Learning phraserepresentations using RNN encoder-decoder for statisticalmachine translation. In EMNLP, pages 1724–1734, 2014.[Liang et al., 2015] Xuan Liang, Tao Zou, Bin Guo, Shuo Li,Haozhe Zhang, Shuyi Zhang, Hui Huang, and Song XiChen. Assessing beijing’s pm2. 5 pollution: severity,weather impact, apec and winter heating. In Proc. R. Soc.A, volume 471, page 20150257, 2015.[Liang et al., 2018] Yuxuan Liang, Songyu Ke,JunboZhang, Xiuwen Yi, and Yu Zheng. Geoman: Multi-levelattention networks for geo-sensory time series prediction.In IJCAI 2018, pages 3428–3434, 2018.[Liu et al., 2018] Luchen Liu, Jianhao Shen, Ming Zhang,Zichang Wang, and Jian Tang. Learning the joint rep-resentation of heterogeneous temporal events for clinicalendpoint prediction. In AAAI, 2018, 2018.[Lu et al., 2017] Jiasen Lu, Caiming Xiong, Devi Parikh,and Richard Socher. Knowing when to look: Adaptiveattention via a visual sentinel for image captioning.InCVPR, pages 3242–3250, 2017.[Plutowski et al., 1996] Mark Plutowski, Garrison W. Cot-trell, and Halbert White. Experience with selecting ex-emplars from clean data. Neural Networks, 9(2):273–294,1996.[Qin et al., 2017] Yao Qin, Dongjin Song, Haifeng Chen,Wei Cheng, Guofei Jiang, and Garrison W. Cottrell. Adual-stage attention-based recurrent neural network fortime series prediction. In IJCAI, pages 2627–2633, 2017.[Rumelhart et al., 1986] David E. Rumelhart, Geoffrey E.Hinton, and Ronald J. Williams. Learning representationsby back-propagating errors. Nature, 323(6088):533–536,1986.[Vemulapalli et al., 2014] Raviteja Vemulapalli, Felipe Ar-rate, and Rama Chellappa. Human action recognition byrepresenting 3d skeletons as points in a lie group. In CVPR2014, pages 588–595, 2014.[Yu et al., 2017] Zhou Yu,Jianping Fan, andDacheng Tao. Multi-modal factorized bilinear poolingwith co-attention learning for visual question answering.In ICCV, pages 1839–1848, 2017.Jun Yu,[Yule, 1927] G. Udny Yule. On a method of investigatingperiodicities in disturbed series, with special reference towolfer’s sunspot numbers. Philosophical Transactions ofthe Royal Society of London, 226(226):267–298, 1927.[Zamora-Mart´ınez et al., 2014] FZamora-Mart´ınez,P Romeu, P Botella-Rocamora, and J Pardo.On-line learning of indoor temperature forecasting modelsEnergy and Buildings,towards energy efficiency.83:162–172, 2014.[Zhang et al., 2017] X. Zhang, J. Clune, and K. O. Stanley.On the Relationship Between the OpenAI Evolution Strat-egy and Stochastic Gradient Descent. arXiv:1712.06564,2017.