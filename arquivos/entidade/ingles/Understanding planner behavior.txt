Artificial Intelligence 76 (1995) 125-166 Artificial Intelligence Understanding planner behavior Adele E. Howe a**, Paul R. Cohen b~l a Computer Science Department, Colorado State Universiry, Fort Collins, CO 80523, USA b Computer Science Department, University of Massachusetts, Amherst, MA 01003, USA Received June 1993; revised March 1994 Abstract As planners and their environments become increasingly complex, planner behavior becomes increasingly difficult to understand. We often do not understand what causes them to fail, so that we can debug their failures, and we may not understand what allows them to succeed, so that we can design the next generation. This paper describes a partially automated methodology for understanding planner behavior over long periods of time. The methodology, calkd Dependency Interpretation, uses statistical dependency detection to identify interesting patterns of behavior in execution traces and interprets the patterns using a weak model of the planner’s interaction to explain how the patterns might be caused by the planner. Dependency with its environment Interpretation has been applied to identify possible causes of plan failures in the Phoenix planner. By analyzing four sets of execution traces gathered from about 400 runs of the Phoenix planner, we showed that the statistical dependencies describe patterns of behavior that are sensitive to the version of the planner and to increasing temporal separation between events, and that dependency detection degrades predictably as the number of available execution traces decreases and as noise is introduced in the execution traces. Dependency Interpretation is appropriate when a complete and correct model of the planner and environment is not available, but execution traces are available. 1. Introduction AI planners have long been introspective. They sit thinking about whether their actions will interact or their plans are efficient, and then they issue a plan. Some planners execute their plans, sometimes before planning is complete. The introspective nature of planners can make it difficult for us to understand why they act as they do, especially when * Corresponding author. Emaik howe@cs.colostakedu. 1 E-mail: cohen@cs.umass.edu. 0004-3702/95/.$09.50 @ 1995 Elsevier Science B.V. All rights reserved SSDIOOO4-3702(94)00083-2 126 A.E. Howe, i?R. CohedArtijicial Intelligence 76 (1995) 125-166 regularities the regularities. the planner and even more so when includes many interacting and acting are interleaved, their behavior, we frequently have trouble connecting components, planning to a dynamic environment. Although we know how our planners are designed, and we can record design combine represent that skips a staggering amount of detail and permits us to predict and explain of modifications Our approach traces. To understand planner behavior, we must traces in a way the effects to produce complex execution the internal workings of a planner and its external execution the two. A planner’s and actions and events in the environment to the planner, to understanding its tasks, and its environment. is to find statistical planner behavior is responsive in actions among actions-unexpectedly arise for many follow another in a strict order. Some dependencies, traces and then use a weak model of the planner to explain frequent or infrequent co- reasons, most very mundane. For example, trace because a frequently- in an execution execution First, we find dependencies occurrences. Dependencies one action might frequently however, alert includes both actions used subplan suggests us to problems; for example, trace, that the planner has become the trace, mundane or otherwise, we find out which plans were being executed during and we search the plans for structures of actions, such as a strict linear ordering between actions. When we have found all such structures, we of the dependencies. All these steps are use them to help us debug completely the failure-recovery a long sequence of obstacle-avoidance trapped. To explain dependencies to index a library of explanations that often result in co-occurrences automated or semi-automated, and they have been applied in an execution component of the Phoenix planner to find statistical dependencies relationships, to identify in execution It is straightforward slightly more difficult However, most dependencies is to show how weak knowledge interpret dependencies. Our approach fragments of execution traces in terms of detailed histories of internal and external states, but, rather, to explain in statistical plans. signify nothing of interest. The contribution such as overlap, between dependencies. of this paper can be used to about a planner and its environment in terms of general structures is not to explain particular over many planning traces, and only tendencies, episodes, [ 121. I. 1. Related research have taken the other approach Much of the research effort into understanding in particular, understanding why plans planner behavior has focused on plan fail so that the plan and the planner failures, can be debugged. Researchers two general positions on plan and planner debugging. One is that bugs or failures can be anticipated by looking at the structure is to uncover pathologies by simulating plan execution or of plans, actually executing plans. Sussman’s HACKER is the earliest example of the first approach [ 191: critics and detect look at the structure of a plan at each level of its development potential problems Instead of simulating would increasingly execution that to identify and satisfy among actions; again, without actually simulating plan lead to bugs. For several years, researchers built planners to discover bugs, HACKER could recognize structures (and opportunities), which inform the next level of plan development. complex constraints execution [ 181. A.E. Howe, P.R. Cohen/Art@cial Intelligence 76 (1995) 125-166 127 [ 161 debugs plans by simulating them with a causal model. GORDIUS Many subsequent efforts relied on simulation or execution, however. Hammond’s [ lo] simulates execution to produce an execution trace of the plan that includes CHEF relationships between plan actions and resulting states. CHEF chains backward through the execution trace to determine the steps that caused a failure, classifies the failure cause based on the explanation of what happened and indexes into a set of general repair strategies to fix the plan to avoid the observed failure. Like CHEF, Simmons’ traces GOFUXUS plan assumptions by regressing desired outcomes (values of states) through a causal dependency structure (generated through simulation of the plan) and identifying mis- matches. The system then repairs the faulty assumptions with one of a set of general repair strategies. Both Hammond’s and Simmons’ approaches assume that the simulator has a correct model of the domain; the approaches differ in the kinds of flaws they detect and the strategies they apply to repair the faults. Related efforts, all of which rely on causal models of the planner, include Hudlicka and Lesser’s work on diagnosing failures during execution [ 141, and Birnbaum et al.? proposal to enhance model-based diagnosis of plans [ 31. Most of the research addresses debugging plans, rather than debugging the planner. While the two are related (after all, explaining why a plan would fail is a step toward explaining why a planner should not favor such a plan), little has been done on planner debugging. The most notable exception is Hammond’s CHEF, which learned from its plan failures. CHEF would remember repaired plans and the bugs found in them so that subsequent planning could use the newly modified plans and account for the bugs found previously. Others have exploited the idea of model-based explanation of failures, bugs and errors that arise in execution traces to learn new plans (e.g., [ 251) . Our position is an amalgam of the structural and simulation/execution approaches focused on debugging the planner. We think it is too difficult to debug a planner by identifying an individual bug and explaining how it arose in terms of a history of states, variable bindings, environmental events and other details. Instead, we rely on statistical dependencies to point us to pieces of plans, and we look for structural features of those pieces that might explain the dependencies. After we have made some modifications, we test whether our explanations were correct by executing plans and seeing whether particular dependencies disappear or are reduced. Our approach has some parallels in software testing and program analysis. For example, Bates and Wileden [l] developed a language for describing salient abstractions of a system’s behavior; a module monitors the system’s behavior and extracts event traces based on the desired abstractions. Gupta describes a knowledge-based system for selectively collecting and analyzing traces of interprocess messages [ 91. In both of these systems, a human programmer must examine the resulting traces and localized failures to determine why the software failed and to debug it. As in our statistical approach, a technique for hardware fault diagnosis, called correspondence analysis, classifies failure modes into “causes” by analyzing contingency tables of system test results [ 151. As in our knowledge-based approach, the DAACS (Dump Analysis And Consulting System) takes a snapshot of a particular type of fatal program error (the contents of a minidump) and matches information from the dump to a belief network of canonical diagnoses [ 41. The result is a set of hypotheses about the source of the failure. 128 A.E. Howe, RR. Cohen/Artij?cial Intelligence 76 (1995) 125-166 Most research in AI debugging explains particular failures in order to debug a plan; most research from software testing addresses finding patterns of behavior in order to debug a program. Our approach combines the two to explain patterns of behavior over time in order to debug a program, the planner. In particular, we use a statistical technique (called dependency detection) to identify patterns of behavior followed by a knowledge- based interpretation phase to construct explanations of how the planner produced those behaviors. The next two sections will describe the statistics and their interpretation. 2. Dependency detection In this section we will describe how to find statistical dependencies in execution traces. A dependency is an unexpectedly frequent or infrequent co-occurrence. For example, let A, B and C be actions in a plan, and consider the execution trace BABCCCBBCBABABCABAC. One thing you will notice is that A almost never occurs without B following it, imme- diately (only the last occurrence of A is followed by C). We could represent this with a contingency table where the first action in the subsequence identifies the row and the second action identifies the column. B 4 3 7 E5 A X Totals E 1 10 11 Totals 5 13 18 The table shows that the subsequence AB occurred four times and the subsequence occurred just once. In addition, of A followed by something other than B (denoted fi) the table shows 3 occurrences of D and 10 occurrences of m. Apparently, the odds of seeing B as the second element in a two-element subsequence depends on whether the first element is A. If the first element is not A, then the odds of seeing B are 3:10, whereas the odds are 4:l if the first element is A. In other words, the presence of A appears to make B more likely. This impression might be erroneous: the execution trace is short, the numbers in the contingency table are small, and the apparent relationship between A and B might be no more than an accidental pattern in a random sequence of letters. Statistical tests of contingency tables tell us the probability that apparent relationships, such as the dependency between A and B, are due to chance. (We will not describe the underlying probabilistic justification for these tests, here, but see [7, Chapter 21.) The most common test of contingency tables is the chi-square test [ 171, but we will as described later. The test statistic use the closely-related G test because it is additive, for a contingency table is: G = 2 C fij In cells (1) A.E. Howe, P.R. CohedArtiJicial Intelligence 76 (1995) 125-166 129 where fij is the number of occurrences, or frequency, in the cell iJ and fij is the expected frequency for cell ij. Expected frequencies can be arrived at two ways: they can be specified extrinsically (as we might specify that the expected frequency of males to females in a sample is one to one) or they can be calculated from the contingency table under the assumption that the row and column variables are independent. In the first case, the G test is a test of goodness of fit to an extrinsic frequency distribution; in the second, it is a test of independence. Dependency detection is based on tests of independence. and 1 l/(7+1 Expected frequencies for tests of independence are derived from row and column sums. In the table, above, the total numbers of occurrences of B and B are 7 and 11, respectively, so our best estimate of the population probabilities of B and B are 7/(7+11) l), respectively. By the same logic, our best estimates of the population probabilities of A and A are 5/ (5+13) and 13/ (5+ 13), respectively. Now, if the occurrence of B is independent of the precursor A, then the probability of the sequence AB is just the product of the probabilities of A and B. That is, assuming independence, Pr(AB) = Pr(A) x Pr(B) = (5/18) x (7/18). The expected frequency of AB is the probability of AB times 18, the number of items in our contingency table. This product, (5/18) x (7/18) x 18 simplifies to (5 x 7)/18. In general, the expected frequency for a cell in row i and column j, is fij = fi. f.j/ f l .9 where fi. and f .j are the totals for row i and column j, respectively, and f.. is the sum of all the cells. But remember: this formula gives the expected frequency for cells under the assumption that the column factor is independent of the row factor. Equation ( 1) simply sums the deviations between the expected frequencies and the actual frequencies in a contingency table. The larger the value of G, the less we be- lieve the independence assumption. Substituting expected and actual frequencies for our contingency table into Eq. ( 1) , above, we obtain: Gz2b1n($) +lln($) +3ln($) +lOln($)] =5.007. When this value is referred to a chi-square distribution with one degree of freedom, we find that the probability of attaining a result greater than or equal to G, under the assumption that the occurrence of B is independent of the occurrence of A, is no more than 0.0265. Thus, if we claim that B depends on A, then the probability of being wrong is less than three in 100. The G test for independence tells us whether the ratio of B to B is significantly different in one row of the contingency table than in another. Thus, it is sometimes called a heterogeneity test because it tells us whether the ratios of cell frequencies in each row are heterogenenous. In our example, heterogeneity is significant. Although the dependency between A and B is probably not spurious, it is not the only dependency in the execution trace, and is perhaps not as easy to explain as other that might subsume it. Note, for example that the longer subsequence dependencies B*B occurs four times, three of which have A in place of the wildcard *. * Perhaps the *We have used the symbol * to designate a single wildcard symbol. 130 A.E. Howe, PR. Cohen/Arttjkial Intelligence 76 (1995) 125-166 10 603 90 10 603 90 10 603 90 Fig. 1. An example of a dependency between A and B, where the dependency does not depend on the previous action. trace is telling us that B tends to be followed after an intervening execution itself, and, by chance, A was the intervening is whether B*B subsumes AB, that is, whether AB occurs primarily of BAB, or whether, is one of the relationships we must consider when we look at longer sequences of actions. large number of sequences if the execution For example, BAB, then we must consider is a dependency action by action three times out of four. The question as a subsequence trace includes an unexpectedly three possibilities: between A and B, and the dependency instead, AI3 has some independent existence. Subsumption does not depend ( 1) There on the preceding action, B. (2) There is a dependency between B and B separated by one action, and the dependency does not depend on the intervening action, A. (3) There is a dependency An example of the first kind between B, A and B the four is shown in Fig. 1. The lower two-action level represents resent the upper With a tree like this we can see whether a dependency pends on a precursor. the odds of seeing B after x are 1:30, and action precedes expectedly CAB. levels of the tree rep- and sequences we considered three possible precursors, A, B and C, of these sequences. between A and B itself de- In Fig. 1, at least, the odds of seeing B after A are 1:6, whereas ratios do not depend on which these subsequences. In other words, Fig. 1 shows an un- of A and B that is not subsumed by AAB, BAB or earlier: AB, AB, AB, B, high co-occurrence two-action the -- Fig. 2 shows an example of the second kind, an A*B dependency that we substitute that does not depend In this case, the odds of the odds of seeing x*B are 1:30, and these odds do not for the wildcard. on the intervening seeing A*B are 1:6, whereas depend on whether action the intervening action is A, B or C. Sometimes, dependencies there is a strong propensity a strong propensity the overall contingency of B following A is 0.5 and the probability can cancel each other out. Fig. 3 shows a case in which sequence AAB, and for B to follow AA in the three-action for B to not follow A in the sequences BAB and CAB. However, between A and B: the probability of B following x is also 0.5. table shows no dependency A. E. Howe, P.R. CohedArtijicial Intelligence 76 (1995) 125-l 66 131 B B 10 60 10 Ml0 60 3 903 SO3 So Fig. 2. An example of a dependency between A and B separated by one action, where the dependency does not depend on the intervening action. 24 6 132 132 7 1966 66 5 11 66 66 Fig. 3. Although the contingency table shows no dependency between A and B, B shows a strong propensity to follow A in the sequence AAB, but the opposite effect holds for the sequences BAB and CAB. 26 162 2 246 2 9 4 13 2 9 3 12 Fig. 4. The AB dependency appears only when the previous action is A. 132 A.E. Howe, RR. Cohen/Artificial Intelligence 76 (1995) 125-166 Lastly, Fig. 4 shows a case in which the AB dependency shows up only when the first actions of a three-action sequence are AA, and this dependency dominates the data. There is no evidence in Fig. 4 for a general two-action AB dependency, because B does not appear to depend on A in three-action sequences BAB and CAB. With G tests, we can differentiate the cases illustrated in Figs. 1, 2, 3 and 4. To do so, we must be able to test both two-item dependencies (e.g., *AB) and three-item dependencies (e.g., AAB). When it doesn’t matter which item we substitute for *, only two-item dependencies will be significant. When dependencies cancel each other, only three-item dependencies will be significant. Usually, though, two- and three-item dependencies will all be significant. Let us show how to build contingency tables to test all these cases. We begin with the contingency table in Fig. 4, which we reproduce here as Table 1. Table 1 The Fig. 4 contingency table Ii mi Totals 39 450 Because Eq. ( 1) is a sum of differences between expected and actual frequencies, we can partition it into components. For instance, we can calculate one G statistic for the *A row of Table 1, and another G statistic for the *x row, and then add these statistics together to get G for the whole table. Noting that the row sums are 210, 279, and the column sums are 39, 450, respectively, we can calculate expected frequencies as above. The G statistic for the first row is: The G statistic for the second row is calculated the same way: The sum of these statistics is 20.28, which is exactly what we get when we apply Eq. ( 1) to run a test of independence on the whole contingency table: Gp = 2 A.E. Howe, P.R. Cohen/Artificial Intelligence 76 (1995) 125-166 133 There’s nothing magical about this u&&i&y property: it is just a consequence of Eq. ( 1) being a sum. However, decomposing G tells us that G,A = 9.40 and G,, = 10.88, that is, each row contributes roughly equally to the total G value of 20.28. Said dif- ferently, the frequencies 30 and 180 deviate from their expectations only a little less than do the frequencies 9 and 270. For convenience, Gp will denote the G statistic for Table 1; the P stands for “pooled” or “packed”, in contrast to other G statistics we’ll calculate later. Comparing Gp = 20.28 to a chi-square distribution with one degree of freedom, we find it is highly significant. This means the occurrence of B or B depends on whether it is preceded by *A or *x, but it says little about possible effects of the first item in the precursor, denoted *. For instance, we have not tested the possibility, suggested by the tree in Fig. 4, that the dependency between B and *A reflects the strong propensity of B to follow AA (in 162 of 188 cases), whereas the tendency of g to follow BA or CA appears much weaker, due to relatively few cases. To test three-item dependencies, we must build slightly different contingency tables. summarize the frequencies of AAB and m, Note that a row in a contingency table might itself summarize another contingency table, the structure of which is lost when it is summarized. In Fig. 4, for example, the frequencies of *Al3 and *ti BAB and BAi%, and CAB and CAB. We can “unpack” the 30 occurrences of *AB into 26 occurrences of AAB and two occurrences each of BAB and CAB. Similarly, we can unpack the 180 occurrences of *@. We could also unpack the second row of Table l-the it isn’t necessary for our immediate purposes. Unpacking the first row will suffice, and it yields Table 2. and the 270 occurrences of *B-but nine occurrences of *B Table 2 The “unpacked” contingency table B 26 2 2 9 AA BA CA *x Totals 39 ix 162 9 9 270 450 The total G statistic for Table 2 will necessarily exceed Gp = 20.28, calculated above. Remember that G measures heterogeneity, the degree to which the ratios of frequencies in rows differ. In general, heterogeneity for a table increases when a row is unpacked, because the row is replaced by two or more new rows that are apt to be heterogeneous. Only when the new rows contain frequencies in exactly the proportions of the unpacked row, will heterogeneity remain constant (we will see an example shortly). The G statistic for Table 2, denoted G~J for “unpacked”, is easily computed withEq. (1): 134 A.E. Howe, RR. Cohen/Artificial Intelligence 76 (1995) 125-166 Gu=2 As expected, Gu > Gp, but not by much. We will let Gu = G~J - Gp = 0.287 denote the increase in heterogeneity due to unpacking. Because Gu is so small, we know that ratios the new rows in Table 2, AA, BA and CA, differ little amongst themselves-the of their frequencies are similar. In fact, the ratio in row AA is 26:162, or roughly 1:6; whereas the ratios for rows BA and CA are 1:4.5. The raw row frequencies, however, are very different: Row AA boasts 188 occurrences of B or h, whereas the other rows contain just 11 occurrences each. The contributions of each row to Gu reflect these differences: We have derived three results: First, a strong dependency exists between the precursors *A and *A, and the subsequent occurrence of B or B (Gp = 20.28). Second, the heterogeneity introduced by unpacking *A into AA, BA and CA is negligible (Gu = 0.287). Third, the precursor AA contributes more to Gu than BA or CA does. How might we interpret these results? We can see in Table 1 that B is more likely to follow *A than *A: the probability of observing B is l/6 following *A and l/30 following *A. We don’t know whether ( 1) the presence or absence of A in the precursor is responsible, or (2) the presence of absence of a particular item that occurs in place of * is respon- sible, or (3) an interaction between a particular substitution for * and the subsequent item in the precursor is responsible. In fact, our results support the third interpretation: B follows *A more frequently than *A, so we know that the presence or absence of A as the second item in the precursor increases the likelihood of observing B. But this is not the whole story, Unpacking the A.E. Howe, RR. Cohen/Artificial Intelligence 76 (1995) 125-166 135 to its propensity the ratios of frequencies in the AA, BA *A row of Table 1 affects heterogeneity little; to each other, they must also be similar and CA rows are similar, and if they are similar in the *A row. In other words, the propensity of B, say, to to the ratio of frequencies follow *A is similar that the to follow AA, BA or CA. This suggests first item in a two-item precursor has little influence on the occurrence of B when the second the fact that 188 of the 210 cases in the *A row of Table 1 are actually AA precursors, while BA and CA contribute very little. Thus, we conclude B; while likely) but weaker. The conclusion AA influences B. that the precursor AA strongly affects the occurrence of (observing B is more that *A influences B is too general; we see now that the effects of the precursors BA and CA are consistent in the precursor is A. However, this ignores item We are ready to quantify these conclusions. Keep in mind that G statistics are additive. As a result, we can construct a summary of our results like this: Statistic G GAA %A &A G*X GJ GH Value 20.28 7.327 1.18 1.18 10.88 20.567 0.287 Significance significant significant not significant not significant significant significant not significant to estimate cell frequencies, The first result, Gp = 20.28, tells us *A influences B, and to “make up” the row sum. Similarly, sums are fixed because we use them results were significant, we would conclude if none of the more that *, the item that precedes A, specific test, on a 2 x 2 table, has one degree of freedom. The row and doesn’t matter. This as is the column if a row contains C columns, only C - 1 are free to total sum for the table. Thus, vary; a column of R the last being constrained rows has R - 1 degrees of freedom. Thus an R x C table has (R - 1) x (C - 1) degrees of freedom. The result table, Gu = 20.567, has three for the 4 x 2 unpacked degrees of freedom and is also significant. It tells us that the ratios of row frequen- different. Because G is additive, we can see which cies of the four rows rows AA for the heterogeneity. Clearly, and the unpacked when we ask of each row whether pectations, grees of freedom, because cies) constrains G,, columns. their ex- test on a row has C - 1 de- frequen- tests of GAA, G~A, GCA, and two row *A contribute most. The rows BA and CA are insignificant: each have one degree of freedom because our contingency its cell frequencies is no. Each as measured by G, the answer is used to estimate expected in Table 2 are significantly in Table 2 are responsible one of the frequencies the row sum (which tables have only in a row. The are different from We get very different results when we calculate G statistics in Fig. 1. For example, here are the statistics for other execution for traces besides Fig. 1: the one summarized 136 A.E. Howe, I?R. Cohen/Art@ial Intelligence 76 (1995) 125-166 Statistic GP GAA GBA GCA G*x Value 9.40 3.133 3.133 3.133 10.88 Significance significant not significant not significant not significant significant Gv GH 9.4 0 significant not significant The interpretation of these results is that there is a significant dependency between A and B in two-action subsequences, and this effect is identical (& = 0) for all substitutions-A, B and C-for *. In fact, none of the individual three-action dependen- cies is significant; for example, the ratio of the occurrences of B to B in the sequences AAB and ti is not significantly different than the expected population ratio ( GAA = 3.133). In short, when the second action of a precursor is A, the first action does not influence the occurrence of B. It turns out that the situation in Fig. 2 yields identical G statistics and has a very similar interpretation to the case in Fig. 1. The only difference is that we look for differences due to the precursors AA, AB and AC instead of the precursors AA, BA, CA. However, the ratio of B to B is not influenced by whether the first two actions are AA, AB or AC; in all cases, the ratio is 1:6. Thus, & = 0 and (;p = Er = 9.4. We conclude that the data show a dependency between A and B, even when another action intervenes, and the intervening action does not influence this dependency. Finally, consider the situation in which dependencies cancel each other out, as shown in Fig. 3. The G statistics for this case are: Statistic GP GAA GBA &A GGi GJ GH Value 0 11.565 5.754 2.306 0 19.625 19.625 Significance not significant significant significant not significant not significant significant significant When the precursor is AA, then the ratio of B to B in the subsequent action is significantly different from its expectation ( GAA = 11.565). Similarly, if the precursor is BA, then the ratio of B to B is also significantly different from its expectation (GBA = 5.754). If the precursor is CA, no effect is evident (GCA = 2.306). Despite strong effects of two precursors, no effect is evident when all the data are pooled (Gp = 0). This is because there are really two three-item dependencies that cancel each other out. The first entails that AA is likely to be followed by B; the second says BA A.E. Howe. P.R. Cohen/Artificial Intelligence 76 (1995) 125-166 137 is likely to be followed by B. These opposing tendencies yield a highly heterogeneous table when the *A row is unpacked (Gn = 19.625), but of course they are invisible in the *A row itself (G*A = 0). We note in passing that the test of Gn has two degrees of freedom because three rows result from unpacking the *A row but their sums are constrained by the *A row sum. An alternative formulation would be to run an ordinary test of independence on a table with rows AA, BA and CA: this will produce Gn, and clearly has two degrees of freedom. In sum, for any subsequence XYZ of three successive actions in an execution trace, we can differentiate three cases: Z depends on Y, irrespective of action X (Fig. 1) ; Z depends on X, irrespective of action Y (Fig. 2) ; and the dependency between Z and Y itself depends on X (Figs. 3 and 4). 3. Interpretation Dependencies, by themselves, raise more questions than they answer. They might suggest relationships that we did not expect or they might only confirm what we knew before or be spurious. Interpretation distinguishes these situations and explains the relationships, suggested by the dependency, in terms of the planner and its environment. Interpretation has two parts: identifying what plans are involved in the dependencies and constructing explanations of how the plans might have produced the dependencies. In contrast to detecting dependencies, both parts rely on knowledge about the planner and its environment. 3. I, Identifying suggestive plan structures For the first part of interpretation, we search for the means of producing the dependen- cy-how one event or action can influence the occurrence of another. The purpose of this step is to identify which planning structures, knowledge and mechanisms influence particular events (or suggest the means for a dependency). If one event depends on another, there must be a medium through which they interact; we search for it in the description of the plans. We can also detect unusually low co-occurrence of events, but at present, we do not try to explain any lack of desired co-occurrence. Determining what might produce the dependencies requires knowledge about the planner and its environment. Yet, this conflicts with the goal of minimizing our reliance on a model of the planner and the environment. Our solution is to supplement the available execution information and knowledge structures available within the planner with a weak model of the planner and its interaction with the environment. The process of identifying suggestive plan structures starts by selecting one depen- dency for attention. We select one dependency for pragmatic reasons. First, at present, the search for suggestive structures is supported by a set of Lisp functions, one for each type of suggestive structure; it would be tedious to run the functions for every dependency. Second, even if this step and the next were fully automated, the designer would have to wade through a lot of information about the structures and explanations. Focusing attention on a single dependency reduces the possible deluge of information. 138 A.E. Howe, PR. Cohen/Artificial Intelligence 76 (1995) 125-166 One must be careful in selecting a dependency for interpretation. Spurious dependen- those that have a low the dependency cies can be most easily avoided by selecting strong dependencies: probability of being due to chance and that are based on many whether that the dependency their tenacity tions) and their utility the observed outcomes). and significant. Dependencies to changes they appear impervious the desirability (i.e., whether is a judgment is unexpected is of interest (e.g., call; only the designer can decide can also be ranked by condi- of the observed behavior or the value of in environmental instances. Determining Having for attention, selected dency with actions the actions the plans and then determine how the precursor might have influenced event. each depen- a dependency in plans, and find structural descriptions of the interaction between in in the dependency. Roughly speaking, we first situate the dependency the dependent to associate it remains Associating dependencies with actions in the plans the dependency First, we need to locate to localize what produces are composed of plan actions and environment behavior. Dependencies mine the role of the planner in producing to the planner. Plan actions need only be associated with the plans appear. Environment plan actions to on-going plans. Consequently, specify all the ways that the events in the dependency plan actions the events. To deter- the dependencies, we need to relate both parts they can typically, information in which events are detected, directly or indirectly, by plan actions; that are detected, and all the ways the initiate sensor activation or processing or at least relate sensory in the dependency to sets of actions each dependency can be mapped appear The mapping from to be searched the dependency for possible space includes both the predecessor (Action A) from a dependency represent a hierarchical no longer concentrate precursor and the antecedent of the dependency. on the dependency, from (solid relationship). lines the dependency indicate in plans. to the plans demarcates interactions. For example, Fig. 5 shows a plan the portion of the plan that (Action P) and temporal precedence, dotted the antecedent lines process, we the From this point in the interpretation but rather on the parts of plans between the actions in the dependency that were involved short- and long-term memory the planner’s short-term memory Finding suggestive structures identified actions), we still do not know what they have Having them de- (call pendency to do with each other. We could search to find out. The problem with searching and his- tories of environmental for all the runs of the planner, irrelevant. The problem with searching of plans, is that plans are described with the wrong amount actions and strategies of detail for inferring behavior over time-too much procedural detail about how actions are instantiated in plans and too little detail about how plan fragments and actions can be included events) the detail would be overwhelming long-term memory (e.g., all the plan expansions, variable bindings, if it could somehow be recorded (e.g., the planner’s in different plans. is that, even representation for planning) and largely A.E. Howe, P.R. Cohen/Art.$cial Intelligence 76 (1995) 125-166 139 Fig. 5. A dependency has been mapped to two actions in a plan. The shaded region indicates the portion of the plan possibly relating to the dependency. and actions We supplement The suggestive plan structures the available declarative plan representation with structural knowl- to form executable combines plan fragments are formed between in plans and describe shared commitments about edge about how the planner plans and what relationships combinations. nate actions expectations gest means by which events and the planners such structures beyond efforts of separate plans or agents, and constrain decision making efficiency). the world. They are called suggestive the plan fragments as a result of the that coordi- idioms to a course of action or shared they sug- In many cases, plans the structures because interact. (so promoting plan reuse), help coordinate in plan languages because they support generalizing (hopefully improving are language-based specific variable actions might are included instantiations the structure structures To identify relationships for particular dependencies, we search (as in Fig. 5) for structural In this case, the temporal structure the predecessor; is guaranteed plan by dependencies the suggestive to follow another the two actions. These structures are of three basic types: temporal, control and is that the antecedent necessarily is called Sequential Actions, in which in some plan. One cannot easily distinguish suggestive parts of plans demarcated between data interactions. follows one action control and data relationships action determines whether a later action will be included hierarchical the expansion of the parent determines which actions are included at the lower this suggestive to the plan. A data interaction For example, actions; this structure and another uses it. is one in which some the in plans constitute a control structure because level; is called Selection Constructs, in which one action adds another is one in which plan actions share information. some plan actions may set values of plan variables used by other plan is called Shared Variables, in which one action sets some variable in this figure. A control structure in the plan. For example, between actions relationship structure structure While most planners construct plans with these three types of structural relationships, the set of suggestive ways the planner of suggestive structures structures depends information incorporates fundamentally about the environment on the plan language and the into its plans. The set for one planner will be described in Section 4.2. 140 A.E. Howe, PR. CohedArtijcial Intelligence 76 (1995) 125-166 3.2. Explaining how suggestive structures produce dependencies The second part of interpretation tures might have produced stories of what might have happened: how suggestive structures might combine dependencies. They do not precisely determine are hypotheses about the source of the observed dependency. the observed dependencies. The explanations struc- to brief to cause the cause of the dependency, but rather of how the suggestive is to find explanations amount The explanations are intended to explain caused the antecedent. This may seem a subtle distinction, are co-occurrences, not causal they share the same cause or because emphasize ways that the events can co-occur, the dependencies, not explain how but the it is not. relationships. Two events may co-occur together by chance. The than how one can rather they occurred precursor Dependencies because explanations cause the other. of a dependency An example of an explanation between parts of different plans is Resource Contention, which describes what happens when several plans vie for the same resource with only one or none able to acquire or access it. Resource contention is common when multiple agents or plans compete for the same limited resources and can occur in plans without forced temporal sequencing or controlled An example of an explanation Overcommitment, which describes why a plan may be inappropriate, be retracted, when the environment suggest overcommitment environment about the environment. are strong temporal sequences, data dependencies and plan actions, and control decisions based on possibly old information is and perhaps need to that the in particular ways. Suggestive structures between for a dependency between actions and the environment resource management. changes The result of the interpretation phase is a set of hypotheses about how the planner the observed dependencies. Given a set of suggestive structures and it is not try to describe phase will be limited by its knowledge of or may find too many; this interpretation to find the “correct” one. Just as when human programmers this phase may not find any explanations might have produced explanations, guaranteed the behavior of a program, the planner. 4. One application: Failure Recovery Analysis We have applied dependency interpretation (i.e., and planner the Phoenix have led to a specialized This section describes how the target behavior vironment the planner) Recovery Analysis and explain cases words, to assist in debugging recovery is the process by which the planner. system which form of dependency (i.e., plan failure) to understand why plans fail in Phoenix. and the target en- and called Failure is to identify in other exacerbate or cause failure; the environment interpretation includes (or FRA). The purpose of Failure Recovery Analysis in which plans may influence, time failure. Most planners that operate recovery. Even reactive systems the planner recognizes in dynamic environments include reactions and repairs a plan include for responding Failure or execution some form of failure to less desirable situations. A.E. Howe, P.R. Cohen/Artificial Intelligence 76 (1995) 125-166 141 1. Run Planner Execution Traces F->R->F->R->F 5. Modify Planner Explanations Dependencies F-F,R-F,FR-F Suggestive Structures for Failure Recovery Analysis: the designer iteratively debugs the planner by testing the fig. 6. The cycle planner, interactions. We chose to focus on failure recovery as the aspect of the planner and environment to record and analyze for several reasons. First, failure recovery influences which failures occur. Minor changes in the design of failure recovery produce significant changes in the number and types of failures (as we discovered in a series of experiments with Phoenix [ 131). Second, failure recovery uses plans in ways not explicitly foreseen by its designers, but not forbidden or prevented by them either. Failure recovery repairs plans by adding or replacing portions of them. As a result, the plan may include plan fragments that are juxtaposed in orders and contexts not envisioned by the designers. Consequently, failure recovery may test the limits of the planner. FRA proceeds as an iterative process in which the designer tests the planner, interprets the dependencies, and modifies the planner based on the dependency interpretation (as in Fig. 6). The process continues until the designer is satisfied with the resulting planner. The designer starts by running the planner in its environment. The Phoenix system automates this first step; an automated experiment controller varies the environment within pre-defined ranges and collects execution traces of which failures occurred and how they were repaired. An example execution trace is: &j -+ R, -+ F+ + Rp + Fn,, + R_yp + Fip + R, + Fnrs where F’s are failure types and R’s are recovery methods. The subscripts indicate individuals from a set, so F,,, means failure of type ner. Second, the execution traces are searched for dependencies between recovery efforts and failures. These dependencies tell the designer how the recovery actions influence the next failure 3 that occurs and how one failure influences the next. For FRA, dependencies consist of combinations of failure types and the recovery actions that repaired them. We 3 The next failure refers to the failure that appears next in the execution removes time stamps, the temporal separation is unknown; the execution it could be immediate or many hours later. trace. Because trace 142 A.E. Howe, l?R. Cohen/Art$cial Intelligence 76 (1995) 125-166 of three types: F-F detect dependencies R-F (recovery action followed by the next failure type) and FR-F (failure recovery method described by the equations list of execution type followed by the next failure type), type and the (as that accept a in Section 2) is implemented traces as input and return a list of dependencies it followed by the next failure). Dependency detection as Lisp functions that repaired as output. (failure selects one of the dependencies for further attention and tries the designer how the planner’s Third, to determine The dependencies suggestive failure. To find suggestive structures, for each of the structures. that involve are mapped structures actions might have caused the observed dependency. to actions in plans and then the plans are searched to be susceptible the actions and that are known the designer runs a set of Lisp functions for to that check Fourth, the designer matches the suggestive and modifications. At present, possible explanations signer must look through a set of explanations The explanations according and modifications are all described structures to a set of for the dependency the de- this step is not automated; that are indexed by suggestive structures. terms and are listed in general and modifications. indicate which explanations the designer of the planner chooses and modifies the most to what structures At the end of the cycle, on his or her understanding suspected around, modification and the incidence of failures changed achieved flaw. The cycle begins again with the designer running the designer can search for more flaws to fix and can determine whether the desired effect, that is, the observed dependency disappeared the planner likely explanation to remove based the the planner. Next time the for the better. than a fully automated In contrast to other approaches rather designer, and ultimately attention for generality. Because explanations, planners, but it cannot guarantee of failure. how to fix the planner it uses a weak model, to debugging, FRA is a procedure applied by a human to focus system. The designer decides where trades power structures and for many that it will find all bugs or even find the actual cause in its suggestive the bug. FRA to repair inherent it can localize a broad range of bugs and should be appropriate 4.1. Phoenix: the target planner and its environment [ 81 serves as the laboratory in Fig. 7, Phoenix provides The Phoenix system4 interpretation. As shown an agent architecture with a set of plan knowledge bases for each type of agent, and an experimental and collecting data. Its environment for this application of dependency in Yellowstone National Park. for automatically the simulated environment, experiments controlling interface is forest fire fighting The goal of forest fire fighting in irregular spread moisture content, wind speed and direction, and natural boundaries and large roads). Fires are contained by removing fires as efficiently as possible. Forest fires rates, as a function of ground cover, elevation, (e.g., bodies of water them fuel from their paths, causing shapes, at variable is to contain 4 “Phoenix” refers to the entire system: simulator and all the components to distinguish the planner. Whenever possible, we use “Phoenix planner” the system. that comprise the planner the agents, including from the other parts of A.E. Howe, P.R. Cohen/Artificial Intelligence 76 (1995) 125-166 143 Simulation Controller V Forest Fire Simulator 4-+ Fig. 7. Diagram of the separate processes that comprise the Phoenix system. The arcs between the processes indicate a transfer of data between processes. fireline. One agent, the fireboss, coordinates to burn out, called building field agents, bulldozers, whenever possible. Other agents, watchtowers, gasoline carriers, and helicopters, the activities of the fireboss and bulldozers gasoline the fire with fireline, exploiting natural boundaries support and delivering to refuel agents by gathering in the field. the activities of to surround information Fig, 8 shows the interface to the simulator. The map in the upper part of the display depicts Yellowstone National Park north of Yellowstone Lake. Features such as wind speed and direction are shown in the window features types are shown as light lines or grey shaded areas. Four such as rivers, roads and terrain fireline around a fire near the center of the figure. A watchtower bulldozers are building is visible at the top near the center. in the upper left, and geographic reflexes and a planner. Sensors perceive the environment. change All Phoenix agents have the same agent architecture, which consists of sensors, ef- local to the planner level of compe- can a particular the cognitive the state of the environment reflexes and component Together, layer provides than faster actions and avoids detrimental control that occur system. Each fectors, the agent, and effecters form a two-layer tence. Reflexes address changes respond, and the planner coordinates The planner plans by skeletal (a structure its plan conditions (e.g., as plans are executed or as information environment). of the environment. and monitoring for developing library plans for partially detailed, uninstantiated interactions. to the timeline searches to the task and as needed the state of the Plans are further expanded and instantiated refinement; when new tasks are added in progress), the planner becomes available about plans appropriate plan By almost any measure, the Phoenix environment is challenging for AI planners. As a result, Phoenix plans fail for lots of reasons. The environment so if the planner fail. Phoenix will plans can fail because plans also fail because situations. bases a plan on slow or no change in their abilities agents are limited they are based on obsolete or uncertain include bugs and have not been they can change unpredictably, in the environment, to sense the plan so Phoenix in all possible the environment; information. tested 144 A.E. Howe, PR. Cohen/Artificial Intelligence 76 (1995) 125-166 Fig. 8. View from Phoenix simulator of bulldozers fighting a fire 4.2. Phoenix-specific knowledge for FRA FRA uses three kinds of domain-specific knowledge: types of failures and recovery actions, suggestive structures, and explanations. At present, the Phoenix planner recog- nizes 11 types of failures and applies eight recovery actions to repair those failures. The failures types are listed in Table 3; the recovery actions in Table 4. While the failure types are obviously specific to the Phoenix planner, the recovery actions, sugges- tive structures and explanations are designed to generalize beyond Phoenix. We believe that many of them would apply to and characterize other planners as aptly as they do Phoenix, and we are in the process of acquiring other planners and analyzing them for whether these structures apply. Following Sussman’s lead on plans [ 191, we envision defining canonical bugs and fixes for many classes of planners. Suggestive structures Suggestive structures are determined by the plan language. The Phoenix plan language is fairly impoverished in its representation of goals and effects of actions; it is largely a procedural language in which most of the reasoning is opaque. The suggestive structures for the Phoenix planner exploit what information is available in the structure of the plans. The following is a listing of the suggestive structures acquired so far for the Phoenix planner. A.E. Howe, PR. Cohen/Artificial Intelligence 76 (1995) 125-166 145 Table 3 Failure pes for the Phoenix planner CCP ccv CFP FNE NER NRS PRJ PTR RU IP ZBT Can’t Calculate Path: The planner cannot find a safe path between two points on the map. Can’t Calculate Variable: This failure type is a catchall for being unable to assign a value to some variable. Can’t Find Plan: Planning operates by searching a plan library for one plan that fits the requirements of its context in the plan and is appropriate to the current state of the environment. This failure type is detected when no plan meets these criteria. Fire Not Encircled: Each fire-fighting plan includes, as its last step, a check that the fire really has been contained; this failure is detected when the fire is not found to be fully encircled by fireline. Not Enough Resources: Fire-fighting plans estimate the resources required to contain particular fires; this failure is detected when the fire is too large for available resources. No Remaining Segments: This failure indicates a mismatch in allocation of work to individual agents. Can’t Calculate Projection: This failure is detected when the fire size cannot be projected for some future time, due to bugs in code or anomalies in available information. Can’t Calculate Path to Road: Normally, paths are calculated between two points; one variant on path calculation is to calculate a path from a point to the nearest road. Resource Unavailable: Resources can be assumed to be available during initial planning, but actually are unavailable when the plan later attempts to allocate them. Insufficient Progress: Envelopes detect differences between observed and expected progress in plans; this failure is detected when the observed progress is inadequate. Zero Build Time: In examining a particular fire, the planner may find that the fire is already contained and thus requires no fireline. Table 4 Recovery actions for the Phoenix planner WATA Wait and try the failed action again. RV RA SA SP AR RP RT Re-calculate one variable used in the failed action. Re-calculate all variables used in the failed action. Substitute a similar plan step for the failed action. Substitute one projection action for another which has failed. Assign additional resources to a plan. Abort the current plan and re-plan at the parent level (i.e., the level in the plan immediately above this one). Abort current plan and re-plan at the top level (i.e., redo the entire plan). to all uses, mismatch Shared Variables: One action sets some variable and another uses it. Shared vari- in value not getting to failure for a variety of reasons: changes ables are vulnerable propagated in assumptions between how the value is set and used, and confusion Shared Resources: Two actions allocate and use the same resource. Separate plans such may access as bulldozers (global instance frames). The Phoenix planner does not arbitrate resource use beyond and fuel carriers and also to some the same resources. Resources about the units of a variable. types of data structures first come first served. refer both to physical and shared resources variables 146 A.E. Howe, P.R. Cohen/Artificial Intelligence 76 (1995) 125-166 l Sequential Actions: One action orderings Sequential the next (barring is inexorable extremely weak as information determining whether one action being completely unrelated. is guaranteed to follow another are vulnerable in that the progression of replanning). the intercession about flaws in the planner; typically precedes another in some plan. to is for to from one action This structure it is merely useful in a plan as opposed in plans l Unordered Actions: Two actions are unordered by the plan. Actions without or- to be the actions are but if for some reason they are not dering constraints opportunistically independent (e.g., mistake on designer’s part or modification independent, interleaved by the planner at run time. Typically, then the resulting order may produce their effects should be unrelated), by failure recovery) that the designer interactions. the actions (meaning indicate expects a do-list the Phoenix plan underlying languages, iteration constructs. The assumption l Iteration Constructs: Multiple actions are added to the plan by the same decision language supports action. Like most programming several of these constructs several the language is that we wish to take similar action over and over; for example, construct, which adds to the plan some number of the same includes action, each with a different variable binding. Because each of these is the same action, if one action they share many of the same assumptions is vulnerable, l Repeated Actions: One action in many diflerent plans and gets used in times in one or many diflerent contexts. Some planning more plans. For example, path calculation time some agent is moved from one place to another. These oft repeated actions can be difficult it may be difficult to program to predict in advance all the possible situations (and so tend to be programmed in which they may be used. actions are executed every actions are repeated many the rest may be as well. the environment; is included incorrectly) because about l Environment References: One action senses the result onto another; or two actions share assumptions about the state of the environment. and calculate variables or make planning Some plan actions sense the environment If the decision or calculation decisions based on the condition of the environment. assumes constancy of environmental then those calculations or decisions may be vulnerable the environment and passes conditions, to failure. Explanations Dependencies may result from one action causing a later failure or from incidental relations the structural list this cataloging planners. in plans. The catalog of explanations influences on failure is not intended the ways to be complete, that failure dependencies for the Phoenix planner identifies some of in Phoenix planner. As with the suggestive structures, for in Phoenix and other to serve as a starting point can be produced but rather l Overly Constrained Environment Assumptions: A sequence of plan actions assumes in those for a in the in the environment may account from changes stability types of failures: conditions and thus is vulnerable constancy of environmental conditions. Actions higher frequency of certain environment. those resulting that assume to changes A.E. Howe, l?R. CohedArtiJScial Intelligence 76 (199.5) 12.5-166 147 A suggestive structure that suggests this explanation some planner modifications actions to coordinate to update actions is environment that might remove references. the depen- the model of the environment more to stay that require the environment this explanation, Given dency are: add monitoring often, or reorder actions the same. l Implicit AssumptionxTwo actions make different assumptions about the value of a plan variable to the extent that the later action’s requirements for successful are explicit, as in plan variables, execution are violated. Some plan expectations the values of the plan underlying and some are implicit, variables. as in the assumptions Some suggestive structures that suggest combined with shared variables, unordered tions, or iteration constructs to the planner plan description incompatible to make the assumptions that might remove actions are not used in the same plans. combined with shared variables. Some modifications the dependency explicit or change are: add new variables to the the plans so that the this explanation actions actions combined with repeated ac- are: sequential l Resource Contention:Several plans may vie for the same resource with only one or treats on-going plans as independent, which minimizes none able to acquire or access it. Resource contention agents or plans vie for the same limited planner actions and allows for reuse of plans, but makes resource contention more likely. is parallel actions combined that might remove is common when multiple resources. For the most part, the Phoenix the search for inter- A suggestive structure this explanation to the planner that suggests resources. Some modifications are: institute a reservation policy reserve with shared dependency on resource availability, tervals so that plans cannot be interleaved between sequence the contending immediately plans. (e.g., when a decision the resources), the is made based in- resource decisions and use, or create protection the amount of coordination and flexibility by moderating the plan may be overly constrained l 0vercommitment:A set of actions may be determined so far in advance that the trade- into such that it may discover about the state of the world or may fail early on, but be so structured planner cannot change the sequence to accommodate change. Plan designers off efficiency plans. Sometimes information that the information Some suggestive actions combined with shared variables or parallel actions combined with shared resources. Some planner and recovery modifications are: explicit so that the knowledge gleaned by one can be make the shared assumptions in the planning used in decision making by the other or change one subplan means (i.e., do not is made for the later plan that a different choice try to do the same thing if it failed before). cannot structures that might remove so that a failure this explanation the dependency later decisions. that suggest are: parallel influence built l Temporal Sequencing in Plans: When actions are strictly ordered in plans, failure that detects that is detected early in the plan cannot follow the plan is somehow restarted or several plans occurrences may match the order. A failure cannot occur if the action it does not get executed. A failure one detected by later actions unless of the same type are interleaved. 148 A.E. Howe, I?R. Cohen/Art@cial Intelligence 76 (1995) 125-166 A suggestive this explanation, replace structure that suggests this explanation a planner modification that might remove is sequential actions. Given is to the dependency the temporal structure with more opportunistic control. l High Base Frequency: Planning actions that have a high base frequency have more in for failures detected by these fre- due to strict actions are repeated many or dependencies opportunity to detect failure. Some planning a plan. This can lead to a high base frequency quent actions and may lead to spurious dependencies ordering effects. A suggestive ner modification that might remove of the action for different situations. the dependency that suggests structure is repeated actions. A plan- is to create different versions this explanation times l Band-Aid Solutions:A recovery action may repair the immediate failure, but that failure may be symptomatic of a deeper problem, which leads to subsequent fail- ures. In this sense, that some other symptom will be detected. the next failure because it inevitable it causes it makes Some suggestive structures that suggest this explanation are: R-F dependency combined with unordered actions and shared variables, F,R-F, combined with sequential references. FR-F dependency Some planner and recovery modifications are: limit the application of the suspect recovery action, force a replan earlier, add new the plan structure so that related recovery methods failures can be identified. actions and environment the failure, or change that might remove the dependency dependency, to repair or l Downstream Failures: Recovery actions may disrupt the flow of control between plan actions so that they cause later failures. The recovery method may alter to fail. of subsequent plan actions, conditions Recovery may cause later failures by making a poor repair or by not updating all plan variables or expectations thus causing them that are related structures to the repair. that suggest Some suggestive this explanation actions, shared variables and a local repair method are: R-F dependency combined with sequential one of WATA, RV, RAV, or SA) or FR-F dependency actions and environment might remove action, narrow the detrimental relationship the dependency the scope of the changes made by failure recovery side effects or modify the repair and the subsequent the plan description combined with sequential that recovery to not produce the to make explicit of the suspect references. Some planner and recovery modifications the application are: limit between failure. (R is l Stealing: A recovery action that significantly mod$es the expectations, constraints or assumptions of the plan may preclude a host of related failures, thereby making unrelated failures more likely to occur next. When some failures are prevented or avoided by relaxing constraints the internal model to match the environment, to these plan modifications may appear more frequent. In effect, the recovery method steals failures from the normal flow causing them to be replaced by others. Some suggestive structures that suggest this explanation in the plan or updating failures not related dependency where R is either a replan or a substitution FR-F should not be modified to prevent it because, are: R-F dependency or action. The planner for the most part, this is a good effect. A.E. Howe, RR. Cohen/Art$cial Intelligence 76 (1995) 125-166 149 5. Empirical evaluation of the Phoenix planner can it help programmers is useful, we need to assess how much information the Phoenix planner the technique it and how much effort is required To determine whether FRA is feasible, we need to know whether the procedure works learn something that they did not know before applying FRA? To determine at all; in particular, about whether applying demonstrate the usefulness of FRA by determining what information of execution the amount of effort expended sensitivity of the underlying is gained by In this section, we that FRA is feasible by applying FRA to the Phoenix planner, and we assess can be gained from different sets the relationship between and the traces for the Phoenix planner and by estimating (i.e., how many execution to gain that information. traces are collected) statistics. 5.1. Demonstrating FRA in Phoenix the feasibility To demonstrate of the FRA procedure, we applied it to help debug traces from 94 the Phoenix planner fought (we will refer to this set of traces as the “Base Case”). The fires were set at eight hour intervals and the the current version of the Phoenix planner. First, we collected execution experiment three fires over the course of about 60 simulation execution wind speed and direction was allowed to change roughly every hour. included 968 failures) in which hours (which trials (i.e., From failure recovery to analyze 15 F-F dependencies, (Fip) that is expensive [ RSp, F,] . We selected this dependency because in Fig. 9). The dependency Third, we ran Lisp functions Second, we ran Lisp functions that had been previously added to improve the execution efforts and failures. We found 46 dependencies: and 16 FR-F dependencies. traces for statistical depen- 15 R-F dencies between that set, we dependencies, the fail- selected one for interpretation: to repair and the precursor ure is a frequently occurring includes a failure recovery method recovery performance but seemed to be interacting detrimentally with other parts of the plan. This [R, followed by Fip] ) was observed 52 times in the set of execution relationship that it was a common pattern of behavior traces, which suggests to map the dependency (as includes a recovery action as precursor, R,, and a transforms a failed indirect attack plan (abbreviated PiO) shown failure as a successor, Fip. R, type of fireline projection calculation into a repaired plan PA by substituting actions action for calculating and model- projections: multiple-fked-shell ( Ap_,,fs), tight-shell (A,.,), based ( Ap_mb). RSp replaces one of these with another. Failure FQ, is detected when plan and not enough monitoring time remains (a structure [ 11 I > called indirect-attack-envelope ( Aen”). for comparing The three projection calculation in three the same suggestive indirect attack plans. All three indirect attack plans include different structures: Shared Variable and Sequential Ordering. All projection calculation actions set the variable attack-projection which is used by the envelope action. The envelope action always follows the plan. Fip is detected by an envelope action actions and the envelope action appear together for the failed one. The Phoenix plan to suggestive plan structures the fire has been insufficient to complete expected in the indirect attack plans. that progress against to actual progress for the planner. three different the projection a different calculation indicates includes library action 150 A.E. Howe, RR. Cohen/Art$cial Intelligence 76 (1995) 125-166 Dependency: SharedVariable Fig. 9. Mapping a dependency to two suggestive smxtures. Fourth, we looked up explanations the observed dependency of how the suggestive structures might have pro- to repair the flaw. structures of suggestive found for the dependency and identified possible modifications two different explanations: Xmplicit Assumptions duced Combinations structures ing) underlie The Shared Variable can cause a failure sets the variable differently plicit assumptions be properly monitored or may violate monitoring Alternatively, only a symptom of a deeper failure raging out of control or the available lead to many explanations. The two suggestive [ Rsp, fip] (Shared Variable and Sequential Order- and Band-aid Solutions. action calculation im- to about acceptable progress. the recovery action R, could lead to FQ, if the recovery action is repairing the fire may be for the task. if the substituted projection than was expected by the envelope action (thus, violating (i.e., resources may really be inadequate ; the projection may not be specified well enough is a band-aid, not a cure); of the envelope) assumptions structures implement two suggestive solution are to limit repairs for a band-aid in the FRA procedure, the planner. With FRA, to select one and The results of the analysis explanations are one dependency, for the dependency. At this stage and two the possible the designer can look designer decides what to do to change the repair. The up stereotypical repairs, but still needs stereotypical the application of the recovery action R, or to add a new recovery action for the failure F&e In this case, the recovery failures, action Rsp had been added to improve Fpv and Fner, removing levels. Alternatively, adding a new recovery method might produce just as many new dependencies. For these reasons, we rejected The stereotypical the most constructive use explicit by defining additional variables. We checked involving type in the dependency) the failure that they both use, attack-projection. In particular, of the variable’s code (the code and envelope code (the code that detects the variable they make about these modifications as unlikely repairs for implicit assumptions were more promising. the precursor of the dependency) it would set performance recovery performance for the assumptions in two expensive back to previous the assumptions to be to make repair seemed to be helpful. the projection The projection code calculates some point in the future. The three versions of the projection an estimate or projection of the extent of the fire at code differ calculation A.E. Howe, P.R. Cohen/Art@cial Intelligence 76 (1995) 125-166 151 available for projections in how they search of the resources The estimates of resource performance into a summary expectations of progress to fight the fire (i.e., and in how they estimate the combined the likely performance fireline). time building together include many factors that are bundled to construct the summary variable variable. The envelope code uses for the plan. the code, it became obvious actions differed not only that the performance estimates set by the in how they were estimated but also in what fireline, rate of travel to the fire, startup that the envelope assumed (e.g., rate of building and refueling overhead). Because the rate of building fireline, the conditions the different projection actions. To accommodate actions were restructured the envelope action then combines to set separate variables the separate variables for signaling these for each to define By examining three projection capabilities were included times for new instructions, the summaries failures effectively varied among differences, of the capabilities; expected progress. reflected only the projection observed than intended just described, As part of the changes in minutes, but treated as seconds, A,,., code (one of the three projection some of the Fpe’s previously actions being even more optimistic estimated being one sixtieth of its desired value). Because were variables it is unlikely FRA it was buggy-that Consequently, we fixed these bugs because indicated needed local to the projection that FRA could have found a few bugs were detected and fixed in the actions used). The bugs almost certainly caused in the data and had led to some projection (e.g., one of the activities had been to one parameter of the estimate in these bugs code and were not recorded as part of the plan, in the use of the parameters. code was buggy and suggested one way in which variables were not being set and used consistently. this was the aspect of the planner that the projection the projection the parameters to be fixed. these bugs that FRA indicated involved leading the modifications were effective if the Rsp-F@ dependency is not traces and the overall rate of failures decreases. To test the in 87 trials in which three fires were set over traces for dependencies. The dependencies included 12 F-F dependencies, traces for these 87 trials the execution We can tell whether in new execution from the execution detected modifications, we ran the modified planner 12 hour intervals and analyzed detected 12 R-F dependencies detected per hour5 hour yields a significant The modifications in the new execution result, z = -13.11, and zero FR-F dependencies. The Rsp-l$, dependency was not traces, and the rate of failures went from 0.414 failures in mean failures per to 0.333 failures per hour (a z test on the differences p < 0.0001). in different type Fp+ accounted to the planner also resulted type. Failure and only 0.3% in the new execution type that is also detected during projection and FCC,, decreased from 25% increased incidences of each failure in the Base Case experiment (a failure from 16% to 12%, increased traces significantly. Table 5 lists the incidence of failures for the Base Case and for for 20.8% of the failures traces. Similarly, calculation) from 15% to 12%. Yet, the overall percentage the counts of failures F,, and Fprr in the execution the incidence of F,,, to 42%, and decreased of I$ 5 Failures per hour was used as a measure because it was the most general measure of progress across all the activities of the agents. 152 A.E. Howe. P.R. Cohen/Artificial Intelligence 76 (1995) 125-166 Table 5 Failure counts for the Base Case and the modified planner execution traces Failure type Base Case Modified planner CCP ccv CFP FNE 1P NER NRS PFR PRJ mR RDU ZBT Number 232 5 143 I 381 246 3 0 321 76 122 4 1540 % 0.151 0.003 0.093 0.005 0.247 0.160 0.002 0 0.208 0.049 0.079 0.003 1.000 Number 142 0 92 5 415 122 15 2 1 147 129 7 1137 % 0.125 0 0.081 0.004 0.418 0.107 0.013 0.002 0.001 0.129 0.113 0.006 1 .Ooo in counts for different types calculation. An optimistic the bugs in the code rather than implementing the traces from the modified planner. Most of these changes of failures were probably due to removing the changes to the projection some failure types is that by removing that can only be detected early in fire fighting plans), failures more data, experiment were enhanced a failure this optimistic to record a measure of progress in (i.e., failures the plans are getting further and in the plan are becoming more obvious. Without explanation If the in the fire fighting plan when the cause of some early plan failures then we might detect a general that are detected at later points trend toward more progress. to know whether for the increase it is difficult is detected, explanation is correct. the procedure fewer failures suggestive structures, explanations system. Based on the dependencies, that were found while following This example shows that a designer can apply FRA to help identify and repair flaws and in a planning modifications for FFL4, we were directed to examine and modify one portion of the code for the Phoenix planner. The modified that planner detected the one was to be avoided by the modifications. Because we fixed bugs other than is due identified by FRA, to the it is virtually change motivated finding some bugs. FRA pointed us to a source of impossible there, whether or not it matched problems, the explanation it led us to examine a relationship that the improvement by applying FRA. For a system of any complexity, to analyze code without and we thought the help of FRA. FRA was useful because that had been assumed previously to fix every problem and did not exhibit it is impossible than previously the dependency selected with to be correct. to conclude found trials as before. These new execution As another example, we performed another cycle of FXA. First, we collected execu- the same tion traces from 102 experiment conditions time intervened between thors and the Explorer used for the experiment moved from University of Massachusetts to Colorado State University. The dependency (about but the failure 2/3 overlap), the previous example and this one during which one of the au- traces were collected because considerable sets in the new traces are similar included 1043 failures) under to 0.261 failures/hour is significantly (which lower (went rate A.E. Howe, P.R. Cohen/Art@cial Intelligence 76 (1995) 125-166 1.53 from 0.333 failures/hour), installing the system in Colorado. due probably to some general fixes made in the course of for a new plan to fight the fire. F,,, and selected one for attention. We found 23 by [ Rn, F,,,]. We selected the dependency amount of data (all cells in contingency the dependencies by the value in their upper left repair cost of the top five in the ranking. to repair. Third, we found suggestive in action which aborts the plan and expensive is a replanning (the bulldozers) resources found were: Shared Variables is detected during projection are inadequate (setting and the assessment of available the state Fourth, we found explanations: Overly the variables the weather and the fire), we the of the two explanations: make explicit (both examine (e.g., decision and coordinate those assumptions and Implicit Assumptions. Because Second, we searched and from for dependencies that set selected only the most expensive the fire. The suggestive for this dependency. Re those based on a reasonable that the available structures dependencies considering table must be at least 5), ranking cell, and then picking So, [ Rn, Fner] is both common structures progress and searches when the planner discovers to contain then using the variable describing bulldozers), of the fire and the current weather conditions). Constrained Environment Assumptions involved were inclined environment with the projection to select a plan when replanning environment the recommendations of the replanning to combine assumptions to hold during replanning. referred likely to the state of the environment code. Our intuition was that because the fire was first sighted as had been used in replanning, decision might be out of sync with the possibly more demanding the same criteria had been used the state of the Sequential Structure, and Environment References the fire, also both reference in extremely they differed related changes their reliance on available two are already used only Based on this analysis, we made a set of closely limited in their flexibility under varying environment to the Phoenix the criteria used to select plans, and we assigned variables based of the plan selection. Phoenix can select from five general (easy) condi- three had been chosen to use resources were available) was selected when resources were tight, the most (estimated many aspects of the state of the world) was selected when condi- from average, and otherwise, one was selected at random. The planner. We changed on some of the assumptions fire fighting plans. Of those, situations. Although tions and randomly. The selection whatever accurate tions differed considerably characteristics and analysis of the code. Additionally, available was made explicit at plan selection plan. observation, about the number of bulldozers the remaining so that the most stingy resources, criteria were modified time as an assignment of bulldozers to the of the three plans were assessed through pilot experiments, the assumption (able We ran and the percentage trials. We analyzed the same conditions the results and found as before and collected execution from 82 ex- had periment disappeared to 1%. However, to 0.346 failures per hour. We have several choices at this point: We could back out this change and try a different solution; we could as- sume that the change needs to be better tuned and so run more pilot experiments to test set for ones that may have the selection criteria, or we could examine is left to the programmer, been that the [ Rn, Fner] dependency from 9% of the failures introduced by this change and fix those. The decision of F,,,, had reduced the dependency rate increased the failure traces 154 A.E. Howe, RR. Cohen/Artificial Intelligence 76 (1995) 125-166 but in all three cases, dependency detection can be used to assess the results and IXA can be used to suggest additional changes. 5.2. Utility of dependencies We assess the utility of PRA by estimating the information gained relative to the effort required. We focus on dependency detection because it is the core technique of IRA; dependencies are the information that drives the rest of the procedure and the amount of effort required is dominated by collecting execution traces. We wished to determine the quality of the information: do dependencies capture weak or strong relationships and do dependencies summarize abstractly the interaction of the planner and its environment. We estimate the information gained by describing how many and of what strength dependencies were found in a series of four experiments with Phoenix and by showing how the dependencies change as the planner is modified and as the time between the precursor and the antecedent is increased. In other words, we show that dependencies characterize strong effects and that the dependencies reflect what the planner does and how much time passes between the planner’s actions and the dependent event. We estimate the effort required to detect dependencies by estimating the sensitivity of dependency detection to the amount of data available and the noise in the data and by summarizing the amount of computation required. Our measurements are based on a set of four sets of execution traces of the Phoenix planner, which were gathered during a series of experiments. In the first three experi- ments, the failure recovery portion of the Phoenix planner was incrementally modified. The third experiment is the Base Case used in Section 5.1, the fourth experiment in- volved the modified planner described in the same section. 5.2.1. Information gained As the planner and its environment changes (or is changed), the type and strength of dependencies changes as well. Prior to gathering the two sets of execution traces described in Section 5.1, we gathered execution traces from two previous versions of the planner (these two versions had successively simpler versions of failure recovery) and analyzed them for dependencies. Amount of information If dependency detection discovers too many dependencies, then the designer will be swamped with information that by its volume becomes useless. If most of the dependencies are strong (meaning the probability that they are due to noise or chance is low), then the designer can have more confidence in dependencies as an accurate characterization of a relationship between precursor and failure type. Different dependencies were detected in the execution traces for each of the four versions of the planner. In terms of evaluating whether the designer is likely to be it appears that, while the dependencies detected in each swamped in dependencies, experiment’s execution traces were different, the number of dependencies detected was not overwhelming. The most dependencies detected in any one experiment was 46 for the Base Case used in Section 5.1; the fewest dependencies detected was 24. The total A.E. Howe, RR. Cohen/Artificial Intelligence 76 (1995) 125-166 155 COI Jnt 60 60 40 20 c.05 <.I0 C.20 >.20 Probability, given G for contingency table Fig. 10. Histogram of p values for all precursor and failure combinations (F-F, R-F and FR-F) in four data sets. p values are the probabilities that the G value for a contingency table was due to noise or chance. for all four sets of execution traces was 125. To get a sense for how many dependencies are likely to be detected, Fig. 10 shows the distribution of p values for four data sets. We say that we detect a dependency between a precursor and a failure type if the p (probability that G was due to chance) is less than (Y, where (Y = 0.05. As one would expect, the overwhelming majority of p values are > LY. In the figure, each column is the count of precursor-failure combinations with p between the upper limit of the previous column and the limit listed at the bottom of the column; so < 1.0 means the count of 0.05 < p < 1.0. The strength of a dependency can be measured in terms of the probability that the ratios observed for the dependency arose due to chance or noise. The majority of the dependencies detected in the four sets of execution traces were well below the (Y threshold. In fact, the histogram in Fig. 11 shows that over half of the dependencies had p < 0.01, indicating that the dependencies that were detected were highly significant. Sensitivity of the dependencies to planner version To determine whether the dependencies reflect the changes being made to the planner, we tested the overlap in the dependencies detected in the four sets of execution traces. Additionally, we tested the temporal persistence of dependencies by analyzing the exe- cution traces for dependencies between a precursor and a failure that occurs much later in the execution trace. The incidence of failure types changed across the four experiments. Consequently, we expect the dependencies detected in the execution traces for each experiment to change as well. We tested this expectation by comparing the sets of dependencies detected across combinations of the experiments and counting the overlap. The results are summarized in Table 6. The planner used in experiment four is a variant of the planner used in experiment three, which is a variant of what was used in experiment two, which is a variant of what was used in experiment one. Thus, one would expect adjacent experiments (i.e., one and two, two and three, three and four) to share relatively many dependencies, 156 A.E. Howe, RR. CohedArtijkial Intelligence 76 (1995) 125-166 Fig. 11. Histogram of p values for dependencies. The dependencies were detected in execution traces from four experiments. Probability, given G for contingency table Table 6 Number of deoendencies shared bv combinations of exneriments l&2 2&3 3&4 l&3 2&4 l&4 1,2&3 2,3&4 1,2,3&4 R-F F-F FR-F 1 9 1 Totals 11 1 5 1 I 4 4 0 8 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 2 0 1 0 1 and non-adjacent experiments to share relatively few dependencies. Data from different experiments should share dependencies because the dependencies capture the structure of some interactions within the planner and between the planner and the environment; thus, the more that the planner changes, the more the dependencies should change. In fact, the execution traces for the four experiments exhibit just this phenomenon: experiments one and two had the most dependencies in common; the full set (one, two, three and four) had the fewest in common. Sensitivity of the dependencies to temporal separation Dependencies represent downstream influences of precursors on later failures. A RsP- fiP dependency indicates the RsP influences or makes more likely the occurrence of Z$, as the next failure. Detecting “downstream influences” implies that we expect the effects of precursors (i.e., failure types and recovery methods) to have some persistence, to last long after the actions that detected the failures or repaired them have finished. In evaluating whether dependencies capture those downstream influences, we need to determine for how long (i.e., how many steps in the execution traces) the precursors influence failures downstream. FRA detects dependencies between a precursor and the failure that immediately fol- lows it. The algorithm was designed to detect dependencies between the precursor and A.E. Howe, F!R. Cohen/Artificial Intelligence 76 (1995) 125-166 157 limited the number of the failure immediately following it for pragmatic reasons-it patterns that needed to be tested by the detection code. Dependency detection runs G tests on all possible patterns of precursors and failures. Thus, the number of patterns increases combinatorially as we add earlier failures and recovery methods to the precur- sors, making it computationally expensive to consider longer and earlier precursors. Practically, we need to know whether we are missing some downstream influences when we limit the patterns to include only the previous failure. We can divide this concern about missing information into two parts: Should precursors include more than just a single pair of a failure and the recovery method that repaired it? Do precursors influence failures after the next failure? To answer the first question, we modified dependency detection to test much longer patterns, patterns of more than one failure and more than one recovery method, and we looked at whether dependencies are detected at all as the precursors become longer and more complicated. To answer the second question, we modified dependency detection to test dependencies between a failure and a much later failure, for example, two, three, or four failures later. As with the first question, we looked at whether dependencies still are detected as the temporal separation increases between the precursor and the failure. FRA tests for dependencies of types F-F, R-F and FR-F for all failures and recovery methods that appear in the execution traces. For example, the execution traces for experiment 1 contained 10 different failure types and six different recovery methods, so there were 600 possible patterns of FR-F or 600 different patterns of length two to be tested. Given the number of different failure types in the execution traces and the number of different recovery methods, we can calculate the number of possible patterns for any length precursor. For example, a precursor of length three (up to RFR-F) produces 4620 possible patterns: 3600 RFR-F, 600 RF*-F, 360 R *R-F, and 60 R *w-F. The * stands for a wild card value, meaning it does not matter what value is in that position so long as something in the execution trace appears between the other values. R **-F means a particular recovery method R followed by any failure, then by any recovery method and finally by a particular failure F. Testing all possible patterns up to FRFR-F would require 59,280 tests for the execu- tion traces for the first experiment. The execution traces for all four experiments together included only 3900 failures (collected over 15,000 simulation hours), which suggests that even if computation time were not a concern, the execution traces could not even contain all the patterns. So, we need to determine whether it is even worth testing for the longer patterns in the execution traces. We did so by modifying the dependency detection code to test any length precursor and examining the rates of detection. Table 7 shows the results of testing for dependencies with precursor up to length four (i.e., FRFR-F) in the execution traces for experiment 1. Clearly, the number of possible combinations of failure types and recovery methods quickly outpaces the size of the execution traces, but as the length of the precursor increases, the percentage of dependencies detected for the patterns found in the execution traces decreases (23% for R-F to 9% for FRFR-F) . Table 7 suggests that, as one might expect, the influence of the precursors decays over time. A smaller percentage of long precursors are detected as dependencies than (the number of significant combinations of short precursors. For example, G/Found is divided by the number of different combinations found in the execution traces) 158 A.E. Howe, F(R. Cohen/Artificial Intelligence 76 (1995) 125-166 Table 7 Dependency search space for precursors up to FRFR Combinations Found G test G/Combinations G/Found R-F FR-F RFR-F FRFR- F Totals 60 700 4620 53900 59280 40 160 531 1737 2468 9 36 50 162 257 0.150 0.050 0.011 0.003 - 0.23 0.23 0.09 0.09 - Table 8 Dependency search space for temporally separated failures. Execution traces are from the first experiment Combinations Non-empty G test G/Comb. G/Non-empty F-F F*-F F **-F F r**-F 760 760 760 760 225 220 207 199 43 20 33 32 0.06 0.03 0.04 0.04 0.19 0.09 0.16 0.16 of the values for F and R) decreases 0.23 for the short precursor R and decreases G/Combinations (the number of significant combinations the dependencies The decrease are not significantly did still detect 162 dependencies 9 dependencies lower, we could still be missing dependencies account in percentages of length limiting found from 0.15 to 0.003, to 0.09 for the longest precursor FRFR; divided by the number of combinations that indicating long combinations. tested to just two, we found. Yet, the program than the higher the numbers are relatively if we do not test for longer dependencies. four, a count considerably for a small proportion of all the possible suggests that by limiting the proportion of dependencies the length for precursors of length 2. So while Alternatively, we can evaluate whether the influence of precursors diminishes over time just failures, gathered over longer temporal separations. failure Fy. Table 8 shows the results that as the distance traces) was analyzed for dependencies of failure F, followed by some number of failures for temporal separations to the failure of significant by looking at simpler precursors, The same data set (the earliest set of execution F,F*-F;,, meaning pairs of a particular followed by a particular of up to four failures. increases, dependencies decrease is that we are likely longer dependencies It appears the number of patterns decreases slightly over longer decreases intervals. to be missing dependencies (19% for F-F in the proportion of significant dependencies from the precursor slightly and the proportion and 16% for F *w-F). is so small, by not considering Because the the only conclusion dependencies over to finding intervals. The future work section describes some proposed approaches 52.2. Effort required: sensitivity of dependency detection to size of data set test, which means The G test is a statistical that it is sensitive as part of dependency detection, in the execution traces. For example, to the amount of data the amount of the execution available. For the G tests performed data refers to the number of patterns trace, A.E. Howe, RR. Cohen/Artificial Intelligence 76 (1995) 125-166 159 includes five FR-F patterns. The contingency table for the pattern FpqRsp-Fip con- structed from this execution trace is shown in Table 9. The total number of patterns is five. The ratio for the pattern FpjRsp-Fi,, in the execution trace is 1:l (i.e., the ratio of the count of the precursor followed by the target failure to the count of the precursor not followed by the target failure) and the ratio for any other precursor ( Fp,.jRsp) being followed by the failure (Fip) is 1:2. The difference between those two ratios is not sufficient to detect a dependence between the precursor and the failure; a G test on this contingency table yields G = 0.236, p < 0.627. If we had 20 times more patterns in the execution traces (i.e., 100 patterns in the traces) with the same ratios (i.e., 20:20 and 20:40), a G test would be significant, G = 4.711, p < 0.03, and a dependency would be detected. Table 9 Contingency table for the dependency Rsp+,, The total number of patterns and the ratios of the patterns in a contingency table influences the results of the G test. Imagine that you have a significant contingency table (p < cy) , there are two ways to change the table that may change the result (i.e., whether p is still less than a): vary the total count of the table (i.e., n) and vary the ratios (top row to bottom row). The first change addresses the sensitivity of the test to the size of the execution traces; the second addresses the sensitivity to noise: how much of a difference is required to detect a dependency no longer? The two changes are not independent, but to understand the influence of each, we examine them separately. The sensitivity of the test to the size of the execution traces is examined analytically, and the sensitivity to noise is examined empirically in this section, To determine the effect of the sample size on detecting dependencies, we will examine the equation underlying the G test used to detect dependencies, the heterogeneity test. The nature of the G test is that the G values for subsets of the sample can be added together to get a G value for the superset (this was the property exploited in pruning). If the ratios remain the same but the total number of counts in the contingency table double, then the G value for the contingency table doubles as well. For example, the G value for the contingency table in Table 9 is 0.236; the G value for the contingency table with 20 times more patterns is 4.711 or roughly (as close as one gets with round off error) 20 times 0.236. To explain how this arises, consider an alternative, but mathematically equivalent form of the equation for heterogeneity: G=2 [aIn +bln (5) - (oih)ln(a+h)] , 160 A.E. Howe, RR. Cohen/Art@cial Intelligence 76 (1995) 125-166 where a is the count from the upper left of the contingency table, b is the count from the upper right of the contingency table, c is the count from the lower left of the contingency table, d is the count from the lower right of the contingency table, ~?f = c/(c + d) and j+ = d( c + d) . Assuming we hold constant the ratios (a:b and c:d) but vary the total count (a + b + c + d), we can replace b with x * a (making a ratio of u:n * a) and c with y * d (making a ratio of c:y * c) and simplify the equation to: G=2a[(l +x)ln(l +y) +xln; - (1 +x)ln(l +x)1. Y (2) which illustrates that if we hold the ratios constant (i.e., x and y) and vary only a to increase the total, then the value of G increases linearly with increases in a. This explanation is valid only for the simple version of G (heterogeneity) used to detect the dependencies in the first place. Linearity is desirable because the effect of more data is predictable and because it tends to reduce the likelihood of surprising results if a few more trials are collected. We know that as more execution traces are collected, the more likely we are to detect depen- dencies, but that the new dependencies detected are likely to have been borderline before the addition of data. The bottom line is that the G test can find strong dependencies given execution traces with few patterns, but given more patterns it will also find rare dependencies. So if a user of FRA is interested in detecting any dependencies, then a few execution traces will be adequate to do so; if the user wishes to find rare or obscure dependencies, then it will be necessary to gather more execution traces. The level of effort expended in gathering execution traces depends on what kinds of dependencies one wishes to find. The preceding analysis tells us how G changes if we expend the effort to gather more execution traces, but the analysis assumes that the ratios in the contingency table are constant. Consequently, we cannot say much about the effect of noise given only a few patterns in execution traces. To rephrase the concern, one of the problems with fewer patterns in execution traces is that any particular failure may be a random event. Formal analysis of this concern is difficult. Instead, we tested the effect of noise empirically by inserting random events into the execution traces and testing whether the dependen- cies found in the original traces remained. The counts are shown in Table 10. About 65% of the dependencies remain after introducing noise. This means that 35% of the dependencies detected would disappear if a few patterns more or less were included in the execution traces. As one would expect, most of the dependencies that were vul- nerable to the tweaking were based on execution traces that included few instances of the precursor/failure pattern: 23 out of 44 or 52% of the dependencies that disappeared were based on contingency tables in which f < 5. The implication of the sensitivity of dependency detection to noise in the execution traces is that rare patterns (i.e., patterns based on few instances in the execution traces) are especially sensitive to noise and SO should be viewed skeptically. When evaluating the cost of executing dependency detection, we need to consider two factors. First, dependency detection is simple and fully automated; the calculations for each test are fast and can be run in batch mode. The computation time required to collect the dependencies in the four sets of execution traces was far shorter than the A.E. Howe, I?R. Cohen/Ar@cial Intelligence 76 (1995) 125-166 161 Table 10 Dependencies remaining after tweaking contingency tables. Contingency tables were tested for sensitivity to minor changes in ratios. The table includes the number of dependencies remaining after tweaking over the total number of deoendencies found in the execution traces from the four exoeriments. Experiment 1 Experiment 2 Experiment 3 Experiment 4 R-F F-F FR-F Total o/4 9113 517 14124 418 15119 314 22131 10/15 II15 11/16 28146 II12 10/12 o/o 17124 time required to gather the execution traces in the first place (less than ten minutes for dependency detection, but about two weeks for the data collection). Second, the complexity of the dependency detection algorithm is mitigated by a practical reduction in the number of patterns to be considered at each step. The only patterns considered are those that appeared in the execution traces; for the four experiments, the number of patterns observed was only four-fifths of those possible. Of those that remained, the first step in dependency detection reduced the set by two thirds, and the second step by another half. From a practical standpoint, the cost of executing dependency detection is fairly low. 6. Future work We have described a technique for identifying and explaining dependencies between a planner and its behavior and have applied it to explain sources of failure in the Phoenix planner. Obviously, this is just the beginning. We envision three directions for future research. First, we intend to expand the definition of dependencies to encompass longer time intervals and longer combinations of actions and events. Second, we will examine how dependencies characterize environments and explore whether they can be used as “markers” for identifying similarities between apparently different environments. Third, we will apply dependency interpretation to planners other than Phoenix and behaviors other than plan failures. 6.1. Detect longer dependencies Dependency detection is based on the assumption that the most recent precursors are most likely to influence which failure occurs next, or to phrase it differently, that a precursor’s influence does not persist beyond the next failure. Empirical evidence suggests that we probably are missing dependencies by not extending the temporal extent of precursors. Unfortunately, the combinatorial nature of dependency detection appears to preclude identifying arbitrarily long sequences of significant precursors. Unless we gather incredibly long execution traces, we quickly run out of instances of each pattern of precursor and failure. Additionally, considering more patterns means consuming more computation time. 162 A.E. Howe, RR. CohedArtiJScial Intelligence 76 (1995) 125-166 However, the combinatorics of dependency detection are based on searching for a&i- trarily long sequences. We can manage the complexity by focusing the search for long sequences; rather than searching the execution traces for any pattern of precursor and failure, we search for particularly interesting ones. Sets of longer dependencies can be accumulated either by controlling the collection of data to selectively test for particular dependencies (i.e., experiment design) or by heuristically controlling the construction and comparison of dependencies (i.e., supplementing the G test). A new experiment design would selectively eliminate recovery methods or plan actions to test whether each precipitates or avoids particular failures [6]. For the analysis, we would remove an action or recovery method from consideration, which results in execution traces free from interactions with the missing action. Consequently, rather than examining all possible chains of which some method is a member, dependency detection would involve comparing dependency sets generated from execution traces with and without each action. By comparing the dependency sets generated in this way, one can infer which dependencies were due to interactions with the missing methods. For example, if an action, say R,, is removed and the frequency of Fip relative to other failures decreases, then the analysis should determine how the Rsp might have produced the additional Fip failures: Was Rsp itself producing the failures? Does R, in conjunction with other recovery methods, (e.g., R,R,-Fip dependencies) account for the surplus Fip failures? Does R, influence Fip over longer intervals? Briefly, with the new experiment design, we can determine which of the possible explanations account for the additional Fip failures by comparing the counts of particular failures, with and without the action, for whether changes in the counts differ uniformly and depend on the previous failure. If the counts differ uniformly, then we can conjecture that the difference is due solely to the action that was removed; otherwise, we check further for how the failure counts compared to the counts when other actions were removed, looking for cases where actions behaved similarly (if action X and action Y interact, then removing either one should produce similar results). Longer combinations require two changes to the application of the G test: determining pools and partitions for longer precursors and comparing the results of separate G tests. The pool and partitions represent different hypotheses about what is producing the observed dependency; for example, does F, influence FY or does F, in conjunction with particular recovery methods, say R, and Rb, influence FY? If we consider all possible pools and their partitions, then we are faced with a combinatorial algorithm. If we consider pools and partitions based on whether one or the other was significant in other applications of the G test, then we may use results on smaller chains to determine which larger chains to explore. Our intuition suggests that if a set of partitions is found to account for little of the variance in a pool, then it is probably not worth looking at longer precursors that include the partitions. For example, if Rsp-Fip is favored over F,R,- Fip based on a G test, then longer precursors, such as F”R,F.xR,-Fip, are unlikely to account for much variance either. To further develop this approach, we need to run the G test on pools of pairs (e.g., FR-F) and partitions of triples (e.g., RFR-F ad FFR-F) (and probably longer pools and partitions) and test whether the intuition is supported by the data. Then, we need to derive heuristics for selecting pools and partitions based on the results of previous G tests. A.E. Howe, RR. Cohen/Artificial Intelligence 76 (1995) 125-166 163 Another approach to finding longer dependencies is to rephrase the problem from finding all possible long dependencies to finding many highly significant dependencies. In this case, we use local search techniques to explore the large space of possible long dependencies. By using local search, we will find the strongest of the related dependen- cies (and so need not do the pruning step), can easily tune the amount of time spent searching by modifying the number of random starts, and can define search operators that match our intuition about what constitutes a neighborhood of dependencies. Working out the logistics for detecting longer dependencies is the first step toward broadening the set of events included in execution traces. Execution traces for FFL4 include only failures and recovery methods; yet, many other events and actions influence the types of failures that occur. Instrumentation is available in the Phoenix system to collect other influences (e.g., changes in the weather, initiating new plans, and observations of new fires), but currently FRA cannot analyze such execution traces. Enhancing dependency detection to consider longer chains means that more events can be added to the execution traces without being inundated with possible dependencies. 6.2. Construct equivalence classes of environments Execution traces from the four experiments yielded different dependencies. The de- pendencies were not only different, but the degree of difference appears to depend on how much failure recovery and the planner differed from one experiment to the next. Based on analyzing the execution traces for the Phoenix planner, similar versions, i.e., versions that differed by a “single” change in the implementation, of failure recovery result in similar sets of dependencies. Comparisons of dependencies detected from execution traces of pilot studies sug- gest that dependencies are also sensitive to the degree of similarity in the environment. Perhaps, some dependencies function as markers for particular characteristics of environ- ments. For example, severely resource-constrained environments may be characterized by many repetitions of resource contention failures, producing the observed dependency that one resource contention failure leads to another. Additionally, failure recovery meth- ods that perform short-term load balancing to reduce the contention for one constrained resource may simply cause a different type of resource contention failure, producing a dependency between short-term load balancing recovery methods and particular types of resource contention failures. One would expect these dependencies to appear in any type of resource-constrained environment, however superficially different, whether it is a transportation planner, an air traffic control system, or a forest fire fighter dispatcher. Looking for such markers requires a lingua franca so that dependency sets for differ- ent environments can be compared. One option is to describe a hierarchy of failures and methods from general classes, such as “resource contention”, to domain specific instances (e.g., “bulldozer unavailable” in Phoenix). The benefit of constructing equivalence classes of environments is that we can predict more easily how a given planner will perform in any environment in the equivalence class. From a design standpoint, we can design planners for new environments by borrowing heavily from previous designs known to work well in similar environments. From a scientific standpoint, we will be able to tell when differences in environments 164 A.E. Howe, P.R. Cohen/Art@cial Intelligence 76 (1995) 125-166 are superficial, allowing us to compare planners with knowledge bases designed for different environments. 6.3. Apply dependency interpretation to other systems Applying Dependency Interpretation to other systems means enhancing the other systems to collect execution traces and acquiring supportive knowledge specific to the other systems. For planners embedded in simulated environments, collecting execution traces should be supported by the simulator already; for planners that are not in simulated environments, the planner itself may need to be augmented to collect the execution traces. Phoenix-specific knowledge is applied at each step of FRA; for a new system, we need to acquire two types of knowledge: l environment event types and plan actions for the planner, l suggestive plan structures and explanations for the new planner. The first should be easy because most planners have pre-defined methods for recogniz- ing salient environment events. The second is the more difficult. The intuition behind suggestive structures is that people who program particular systems have structures that they check first when trying to track down bugs or understand why a program behaves as it does. For example, with resource contention failures, one would probably look first at the structures involved in managing the resources: pairings of reserve/release actions, actions that consume resources and decisions about resources and the preconditions on those actions. We have recorded some of the structures that are most suspect for the Phoenix planner and believe that many of these structures apply equally well to other planners. We intend to test this conjecture. 7. Conclusion Part of the challenge of designing new planners is understanding when and why they work. The lessons of previous systems are the basis of future designs. Unfortunately, as our planners and their host environments become increasingly complex, they become increasingly difficult to understand. This paper describes our first experiments with a method for understanding the be- havior of planners. The approach combines a domain-independent, syntactic technique for summarizing behavior and identifying interesting patterns, with a domain-dependent, semantic technique for interpreting those patterns. The strength of the approach lies in the application of statistical techniques to reduce the tremendous amounts of execu- tion information down to salient patterns and in the reliance on weak domain/planner knowledge for interpretation. Statistical dependency detection prunes an overwhelming and largely uninteresting space of behavioral data, and dependency interpretation ex- ploits knowledge of the interactions of small parts of the planner to explain behavior of the system as a whole. The future of planning depends on our ability to explain the behavior of increasingly complex systems. For us, this has meant changing our focus from explaining individual decisions in particular planning episodes to explaining statistical anomalies across many A.E. Howe, l?R. Cohen/Artificial Intelligence 76 (1995) 125-166 165 episodes. Also, our explanations are not in terms of highly specific state information, but, rather, in terms of general plan structures and programming idioms. Consequently, we cannot debug the Phoenix planner given only the execution trace of a single planning episode, nor can we pinpoint the source of a bug to a particular line of code. Nor can anyone else, however. We think it unlikely that anyone will write a program to automatically localize bugs in arbitrary complex systems, given execution traces that may or may not be buggy. Dependency detection and interpretation, on the other hand, find and explain statistical anomalies that might indicate bugs or planners that are particularly well designed for their environments, well enough for a programmer to easily find previously unsuspected bugs and a designer to extrapolate a good design to a new environment. Acknowledgments This research was supported by ARPA-AFOSR contracts F49620-89-C-00113 and F30602-93-C-0100, a grant from the Office of Naval Research under the University Re- search Initiative NOOO14-86-K-076, the National Science Foundation under an Issues in Real-Time Computing grant, CDA-8922572, and National Science Foundation Research Initiation Award #RIA IRI-9308573. The US Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright no- tation hereon. This research was conducted as part of the first author’s Ph.D. Thesis research at the University of Massachusetts. We wish to thank Cynthia Loiselle for carefully reading and translating portions of the document to LaTex and David Hart and David Westbrook for helping run experiments with Phoenix. We also wish to thank the reviewers for their careful review and insightful comments. References [ 1 ] PC. Bates and J.C. Wileden, High-level debugging of distributed systems: the behavioral abstraction approach, Department of Computer and Information Science 83-29, University of Massachusetts, Amherst, MA (1983). [2] S.W. Bennett, Learning approximate plans for use in the real world, in: A. Segre, ed., Proceedings of rhe Sixth Zntemational Workshop on Machine Learning, Ithaca, NY (Morgan Kaufmann, San Mateo, CA, 1989) 224-228. [3] L. Bimbaum, G. Collins, M. Freed and B. Krnlwich, Model-based diagnosis of planning failures, in: Proceedings AAAI-90, Boston, MA ( 1990) 318-323. [4] L.J. Bumell and SE. Talbot, Incorporating probabilistic reasoning in a reactive program debugging system, in: Proceedings Ninth Conference on Arti$cial Intelligencefor Applicaiions, Orlando, FL ( 1993) 321-327. [5] S.A. Chien, Learning by analyzing fortuitous occurences, in: A. Segre, ed., Proceedings of rhe Sixrh International Workshop on Machine Learning, Ithaca, NY (Morgan Kaufmann, San Mateo, CA, 1989) 249-251. [6] P.R. Cohen, An experiment to test additivity of effects, Technical Report Memo No. 20, University of Massachusetts, Experimental Knowedge Systems Laboratory, Amherst, MA ( 1991). 171 P.R. Cohen, Empirical Methodsfor Arhjicial Infelligence (MIT Press, Cambridge, MA, 1993). [8] P.R. Cohen, M. Greenberg, D.M. Hart and A.E. Howe, Trial by fire: understanding the design requirements for agents in complex environments, AI Mag. 10 (3) ( 1989) 32-48. 166 A.E. Howe, PR. Cohen/Art@cial Intelligence 76 (1995) 125-166 [91 N.K. Gupta and R.E. Seviora, An expert system approach to real time system debugging, in: Proceedings IEEE Computer Society Conference on AI Applications, Denver, CO ( 1984) 336-343. 1101 K.J. Hammond, Explaining and repairing plans that fail, in: Proceedings IJCAI-87, Milan, Italy (1987) 109-l 14; also: Art& Intell. 45 (1990) 173-228. [ 111 D.M. Hart, P.R. Cohen and SD. Anderson, Envelopes as a vehicle for improving the efficiency of plan execution, in: K.P Sycara, ed., Proceedings Workshop on Innovative Approaches to Planning, Scheduling and Control (Morgan Kaufmann, San Mateo, CA, 1990) 71-76 [ 121 A.E. Howe, Accepting the inevitable: the role of failure recovery in the design of planners, Ph.D. Thesis, University of Massachusetts, Department of Computer Science, Amherst, MA (1993). [ 131 A.E. Howe and P.R. Cohen, Failure recovery: a model and experiments, in: Proceedings AAAI-91, Anaheim, CA (1991) 801-808. [ 141 E. Hudlicka and V. Lesser, Modeling and diagnosing problem-solving system behavior, IEEE Trans. Syst. Man Cybern. 17 (3) (1987) 407-419. 1151 L.F. Pau, Failure Diagnosis and Pet$ormance Monitoring, Control and Systems Theory 11 (Marcel Dekker, New York, 198 1). [ 161 R.G. Simmons, A theory of debugging plans and interpretations, in: Proceedings AAAI-88, Minneapolis, MN ( 1988) 94-99. [ 171 R.R. Sokal and F.J. Rohlf, Biometry: The Principles and Practice of Statistics in Biological Research (Freeman, New York, 2nd ed., 1981). [ 181 M. Stefik, Planning with constraints (MOLGEN: Part l), ArtiJ Intell. 16 (1981) 111-139. [ 191 G.A. Sussman, A computational model of skill acquisition, Technical Report Memo no. AI-TR-297, MIT AI Lab, Cambridge, MA (1973). 