Artificial Intelligence 174 (2010) 670–684Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintMonte Carlo tree search in KriegspielPaolo Ciancarini∗, Gian Piero FaviniDipartimento di Scienze dell’Informazione, University of Bologna, Italya r t i c l ei n f oa b s t r a c tArticle history:Received 20 September 2009Received in revised form 4 April 2010Accepted 4 April 2010Available online 9 April 2010Keywords:GamesChessKriegspielIncomplete informationMonte Carlo tree search1. IntroductionPartial information games are excellent examples of decision making under uncertainty. Inparticular, some games have such an immense state space and high degree of uncertaintythat traditional algorithms and methods struggle to play them effectively. Monte Carlo treesearch (MCTS) has brought significant improvements to the level of computer programsin games such as Go, and it has been used to play partial information games as well.However, there are certain games with particularly large trees and reduced informationin which a naive MCTS approach is insufficient: in particular, this is the case of gameswith long matches, dynamic information, and complex victory conditions. In this paper weexplore the application of MCTS to a wargame-like board game, Kriegspiel. We describe andstudy three MCTS-based methods, starting from a very simple implementation and movingto more refined versions for playing the game with little specific knowledge. We comparethese MCTS-based programs to the strongest known minimax-based Kriegspiel program,obtaining significantly better experimental results with less domain-specific knowledge.© 2010 Elsevier B.V. All rights reserved.Partial information games provide a good model and testbed for many real-world situations involving decision mak-ing under uncertainty. They can be very difficult for a computer program to play well. These games typically require acombination of complex tasks such as heuristic search, belief state reconstruction, and opponent modeling.Moreover, some games are particularly challenging because at any time the number of possible, indistinguishable statesfar exceeds the storage and computational abilities of present-day computers. In this paper, the focus is on one such game,Kriegspiel or invisible chess. The game is interesting for at least three reasons. Firstly, its rules are identical to those of Chess,a very well-known game; however, the players’ perception of the board is different, only being able to see their own pieces.Secondly, it is a game with a huge number of states and limited means of acquiring information. Finally, the nature ofuncertainty is entirely dynamic. These issues put Kriegspiel in a category different from other partial information gamessuch as Stratego or Phantom Go (the partial information variant of Go [1]), wherein a newly discovered piece of informationremains valid for the rest of the game. Information in Kriegspiel is scarce, precious, and ages fast.In fact, even if it is an old game, well known to game theorists and even discussed by von Neumann and Morgensternin [2] under the name of blind chess, the first attempt to build an effective Kriegspiel playing program came only in 2005and was based on Monte Carlo sampling [3]. It was, however, defeated by our first program, described in [4] and based ona form of minimax on a game tree of data structures called metapositions. These had been first defined in [5] for a partialinformation variant of Shogi, that is Japanese Chess. Our program was better than other competing programs, but was notgood enough to compete with the best human players.* Corresponding author.E-mail address: cianca@cs.unibo.it (P. Ciancarini).0004-3702/$ – see front matter © 2010 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2010.04.017P. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684671Fig. 1. The four phases of Monte Carlo tree search: selection, expansion, simulation and backpropagation.In this paper we present and study different ways of applying Monte Carlo tree search to Kriegspiel. Monte Carlo treesearch has been imposing itself over the past years as a major tool for games in which traditional minimax techniquesdo not yield good results due to the size of the state space and the difficulty of crafting an adequate evaluation function.The game of Go is the primary example, albeit not the only one, of a tough environment for minimax where Monte Carlotree search was able to improve the level of computer programs considerably [6,7]. Since Kriegspiel shares the two traits ofbeing a large game and a difficult one to express with an evaluation function (unlike its complete information counterpart),it is only natural to test a similar approach.The paper is organized as follows. Section 2 contains a high-level introduction to Monte Carlo tree search (MCTS), withan emphasis on its successful application to Phantom Go. In Section 3, we introduce the game of Kriegspiel, its rules,and what makes it similar, yet very different, to Phantom Go. Section 4 contains the most significant research results onKriegspiel, especially those related to previous Monte Carlo methods. We give a high-level view of three MCTS approachesin Section 5, showing how they are similar and where they differ; the corresponding programs are then described in greaterdetail separately. Section 6 contains some experimental tests comparing the strength and the performance of the variousprograms. Finally, we give our conclusions and some future research directions in Section 7.2. Monte Carlo tree searchMonte Carlo tree search (MCTS) is an evolution of some simpler and older methods based on Monte Carlo sampling.While the core concept is still the same – a program plays a large number of random simulated games and picks the movethat seems to yield the highest victory ratio – the purpose of MCTS is to make the computation converge to the right valuemuch more quickly than pure Monte Carlo. This is accomplished by guiding the simulations with a game tree that grows toaccommodate new nodes over time; more promising nodes are, in theory, reached first and visited more often than nodesthat are likely to be unattractive.MCTS is an iterative method that performs the same four steps until its available time runs out. These steps are summa-rized in Fig. 1.• Selection. The algorithm selects a leaf node from the tree based on the number of visits and their average value.• Expansion. The algorithm optionally adds new nodes to the tree.• Simulation. The algorithm somehow simulates the rest of the game one or more times, and returns the value of thefinal state (or their average, if simulated multiple times).• Backpropagation. The value is propagated to the node’s ancestors up to the root, and new average values are computedfor these nodes.After performing these phases as many times as time allows, the program chooses the root’s child that has received themost visits and plays the corresponding move. This may not necessarily coincide with the node with the highest meanvalue. A discussion about why the mean operator alone does not make a good choice is contained in [8].672P. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684MCTS should be thought of as a method rather than a specific algorithm, in that it does not dictate hard policies forany of the four phases. It does not truly specify how a leaf should be selected, when a node should be expanded, howsimulations should be conducted or how their values should be propagated upwards. In practice, however, game-playingprograms tend to use variations of the same algorithms for several of the above steps.Selection as a task is similar in spirit to the n-bandit problem since the program needs to strike a balance betweenexploration (devoting some time to new nodes) and exploitation (directing the simulations towards nodes that have shownpromise so far). For example, programs can make use of the UCT algorithm (Upper Confidence bound applied to Trees) firstgiven in [6]. This algorithm chooses at each step the child node maximizing the quantity(cid:2)U i = v i + cln Nni,where v i is the value of node i, N is the number of times the parent node was visited, ni is the number of times node iwas visited, and c is a constant that favors exploitation if low, and exploration if high.Expansion varies dramatically depending on the game being considered, its state space and branching factor. In gen-eral, most programs will expand a node after it has been visited a sufficient number of times. Simulation also dependswildly on the type of game. There is a large literature dealing with MCTS simulation strategies for the game of Go alone.Backpropagation offers the problem of which backup operator to use when calculating the value of a node.2.1. MCTS and partial information in Phantom GoMonte Carlo tree search has been used successfully in large, complex partial information games, most notably PhantomGo. This game is the partial information version of the classic game of Go: the player has no direct knowledge of hisopponent’s stones, but can infer their existence if he tries to put his own stone on an intersection and discovers he isunable to. In that case, he can try another move instead. [1] describes an MCTS algorithm for playing the game, obtaininga good playing strength on a 9 × 9 board. A thorough comparison of several Monte Carlo approaches to Phantom Go, withor without tree search, has recently been given in [9]. We are especially interested in Phantom Go because its state spaceand branching factor are much larger than most other (already complex) partial information games such as poker, for whichgood Monte Carlo strategies exist; see, for example, [10].MCTS algorithms for Phantom Go are relatively straightforward in that they mostly reuse knowledge and methods fromtheir Go counterparts: in fact, they mostly differ from Go programs because in the simulation phase the starting board isgenerated with a new random setup for the opponent’s stones every time instead of always being the same. It is legitimateto wonder whether this approach can be easily converted to other games with an equally huge state space, or PhantomGo is a special case, descending from a game that is particularly suited to MCTS. In the next section we discuss Kriegspiel,which is to chess what Phantom Go is to Go, and compare the two games for similarities and differences.3. KriegspielKriegspiel, named after the ‘war game’ used by the Prussian army to train its officers, is a chess variant invented at theend of the XIX century to transform standard chess into a wargame. It has been studied and played by game theorists of thecaliber of John von Neumann and Lloyd Shapley. It is played on three different chessboards, one for either player and onefor the referee. They are positioned in such a way that the referee sees all the boards while the players can see only theirown. From the referee’s point of view, a game of Kriegspiel is a game of Chess. The players, however, can only see theirown pieces while the opponent’s are in the dark, as if hidden by a “fog of war”. On his turn, a player selects a move andcommunicates it to the referee, so that there is no direct communication between the two opponents. If a move is illegal,the referee will reject the move and ask the player to choose a different one. If it is legal, the referee will instead informboth players as to the consequences of that move, if any. This information depends on the Kriegspiel variant being played;see [11] to find out more about the game. On the Internet Chess Club, which hosts the largest community of players of thisgame, the referee’s messages are the following.• When the move is legal and it is not a check or a capture, the referee will give no information, saying “White moved”or “Black moved”. We will call this “silent referee” because no information is given to the players when a move isaccepted.• When a chessman is captured: in this case the referee will say whether the captured chessman is a pawn or a pieceand where it was captured, but in the latter case he will not say what kind of piece.• When the king of the player to move is in check: in this case the referee will disclose the direction (or directions) ofcheck among the following: rank, file, short diagonal, long diagonal, knight.• In order to speed up the game, when the player to move has one or more capturing moves using his pawns, the refereewill announce that (“pawn tries”) but will not tell which pawn can perform the capture.• When the game is over, the referee announces the checkmate or drawn game for any standard condition (e.g. stalemateor not enough material or position repetition for three times, etc.).P. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684673These rules are also used for the international Computer Olympiad, where a Kriegspiel tournament has been played in2006 and 2009.On a superficial level, Kriegspiel and Phantom Go are quite similar. Both keep the same rules as their complete infor-mation versions, only adding a layer of uncertainty in the form of the fog of war managed by a referee. The transcript of aKriegspiel game is a legal chess game, just like the transcript of a Phantom Go game is a legal Go game. Both involve moveattempts as their core mechanics; illegal attempts provide information on the state of the game. In both games, a playercan purposely try a move just for the purpose of information gathering. On the other hand, there are differences worthmentioning between the two games. We list some of the most significant ones.• The nature of Kriegspiel uncertainty is strongly dynamic: while Go stones are, if not immutable, at least largely staticand once discovered permanently decrease uncertainty by a large factor, information in Kriegspiel ages and quickly be-comes old. One needs to consider whether uncertainty means the same thing in the two games, and whether Kriegspielis a harsher battlefield in this respect.• There are several dozen combinations of messages that the Kriegspiel referee can return, compared to just two inPhantom Go. This makes their full representation in the game tree very difficult.• In Phantom Go there always exists a sequence of illegal moves that will reveal the full state of the game and removeuncertainty altogether; no such thing exists in Kriegspiel, where no sequence of moves can ever reveal the referee’schessboard except near the end of the game.• Uncertainty grows faster in Phantom Go, but also decreases automatically in the endgame. By contrast, Kriegspiel un-certainty only decreases permanently when a piece is captured, which is rarely guaranteed to happen.• In Phantom Go, the player’s ability to reduce uncertainty increases as the game progresses since there are more enemystones, but the utility of this additional information often decreases because less and less can be done about it. It isexactly the opposite in Kriegspiel: much like in Battleship, since there are fewer enemies on the board and fewer alliesto hit them with, the player has a harder time making progress, but any information can give him a major advantage.• There are differences carried over from their complete information counterparts, most notably the victory conditions.Kriegspiel is about causing an event that can happen suddenly and at almost any time, whereas Go games are concernedwith the accumulation of score. From the point of view of Monte Carlo methods, score-based games tend to be morefavorable than condition-based games, if the condition is difficult to observe in a random game. Even with considerablematerial advantage, it is relatively rare to force a checkmate with random moves.Hence, there are mixed results from comparing the two games; at the very least, they represent two different kindsof uncertainty, that could be best described as static vs. dynamic uncertainty. We wish to investigate the effectiveness ofMonte Carlo methods – and especially MCTS – in the context of dynamic uncertainty.4. Related worksResearch on Kriegspiel can be classified as follows. The earliest papers of the 1970s dealt with protocols for implementinga Kriegspiel referee and are not of interest here. Research in the following decades focused on solving specific subsets of theKriegspiel game, most notably a few simple endgames. The first serious programs for playing an entire game of Kriegspielcame out around 2005. We discuss four of these, with a special emphasis on two: a Monte Carlo one and a minimax-basedone.4.1. Early resultsBecause the information shared by the players is very limited, the information set for Kriegspiel is huge. If one considersthe number of distinct belief states in a game of Kriegspiel, the KRK (king and rook versus king) ending alone has a numberof possible states close to the whole game of checkers. However, many of these states are in practice equivalent since thereis no strategy that allows to distinguish them in a normal game. This complexity is the primary reason why, for a longtime, research only focused on algorithms for specific endings, such as KRK, KQK or KPK. Ferguson showed game-theoreticalgorithms for solving KBNK and KBBK under specific starting conditions; see, for example, [12]. Automatic search throughfour Kriegspiel endgames was first tackled in [13].State reconstruction routines are the main object of [14]; [15] focuses on efficient belief state management in order torecognize and find Kriegspiel checkmates. The focus of both papers is on search and problem-solving rather than actuallyplaying the game.4.2. The first Monte Carlo programDue to the complexity of the domain, computer programs capable of playing a full Kriegspiel game have only emerged inrecent years. The first Monte Carlo (though not MCTS) approach to Kriegspiel is due to Parker, Nau and Subrahmanian [3].Their program plays by using and maintaining a state pool that is sampled and evaluated with a chess function. In the paper,the authors call the information set associated with a given situation a belief state, the set containing all the possible game674P. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684states compatible with the information the player has gathered so far. They apply a statistical sampling technique, which hasproven successful in several partial information games such as bridge and poker, and adapt it to Kriegspiel. The techniqueconsists of generating a set of sample states (i.e. chessboards, a subset of the information set/belief state), compatible withthe referee’s messages and analyzing them with well-known complete information algorithms and evaluation functions.This approach feeds the randomly sampled boards to the popular open source GNUChess engine and chooses the move thatobtains the highest average score in each sample. The choice of using a chess engine is both the method’s greatest strength,as it saves the trouble of defining Kriegspiel domain knowledge, and its most important flaw, as positions are evaluatedaccording to chess standards, with the assumption that each player can see the whole board.Obviously, in the case of Kriegspiel, generating good samples is far harder than anything in Bridge or Poker. Not onlyis the state space immensely larger, but also the duration of the game is longer, with many more choices to be taken andbranches to be explored. For the same reasons, evaluating a move is computationally more expensive than a position inBridge, and the program needs to run the evaluation function on each sample; as a consequence, fewer samples can beanalyzed even though the size of the state space would command many more.The authors in [3] describe four sampling algorithms, three of which they have implemented (the fourth, AOS, gener-ating samples compatible with all observations, would equate to generating the whole information set, and is thereforeintractable).• LOS (Last Observation Sampling). Generates up to a certain quantity of samples compatible with the last observationonly (it has no memory of what happened before the last move).• AOSP (All Observation Sampling with Pool). The algorithm updates and maintains a pool of samples (chessboards),numbering about a few tens of thousands, all of which are guaranteed to be compatible with all the observations sofar.• HS (Hybrid Sampling). This works much like AOSP, except that it may also introduce last-observation samples undercertain conditions.They conducted experiments with timed versions of the three algorithms, basically generating samples and evaluatingthem until a timer runs out, for instance after 30 seconds. They conclude that LOS behaves better than random play, AOSPis better than LOS, and HS is better than AOSP.It may surprise that HS, introducing a component of the less refined LOS, behaves better than the pure AOSP, but itis in fact to be expected. The size of the AOSP pool is minuscule compared with the information set for the largest partof the game. No matter how smart the generation algorithm may be or how much it strives to maintain diversity, it isimpossible to convey the full possibilities of a midgame information set (a fact we also confirm with the present research).The individual samples will begin to acquire too much weight, and the algorithm will begin to evaluate a component ofnoise. The situation worsens as the pool, which is already biased, is used to evolve the pool itself. Invariably, many possiblestates will be forgotten. In this context, LOS actually helps because it introduces fresh states, some of which may not in factbe possible, but prevents the pool from stagnating.4.3. A minimax-based programThe first program based on minimax-like search, as well as the strongest before the present research, is described in ourprevious paper [4]; here we summarize its workings. The program builds an approximation of the game’s information setbased on the concept of metapositions as a tool for merging an immense amount of game states into a single, small andmanageable data structure.The term metaposition was first introduced by Sakuta [16], where it was applied to some endgame scenarios for theShogi equivalent of Kriegspiel. The primary goal of representing an extensive form game tree with metapositions is totransform an imperfect information game into one of perfect information, which offers several important advantages andsimplifications, including the applicability of the Minimax theorem. A metaposition merges different, but equally likelymoves, into one state. A Kriegspiel metaposition can contain a huge number of states, and is obviously represented with anapproximated version. We developed a specific function for evaluating a metaposition as a whole, ignoring the individualstates that make it up. Then, we built a modified minimax algorithm for searching through metapositions. This leads to agood level of play (which in computer Kriegspiel translates to the level of an average human player), at the price of requiringa custom evaluation function for Kriegspiel and a good deal of domain knowledge. The program was called Darkboard andwon the first computer Kriegspiel tournament held at the 11th Computer Olympiad in Turin, Italy, in 2006. It also playedon the Internet Chess Club where it got a max Elo rating of 1870, but averaging around 1600 points.An improved version of Darkboard with a better evaluation function is used as a benchmark for the Monte Carlo algo-rithms in this paper.4.4. Other programsRecently, there have been two attempts at modeling an opponent in Kriegspiel with Markov decision processes in thelimited case of a 4 × 4 chessboard in [17]. The authors then shifted to a Monte Carlo approach (though not MCTS) withP. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684675Fig. 2. Comparison of three simulation methods. Approach A is standard Monte Carlo tree search, approach B simulates referee messages only and fork-move runs, approach C immediately computes the value of a node in approach B for k = 1.Fig. 3. Three-tiered game tree representation in our algorithms.particle filtering techniques in [18]; this newer method allows the program to play on a normal 8 × 8 board. The latter workhas some similarities, at least in spirit, with the modeling techniques presented in this paper; however, it is still similar to[3] in that the particle filtering creates plausible chess positions which are evaluated by an engine like GNUChess.5. Three MCTS approachesIn this section, we provide three Monte Carlo tree search methods for playing Kriegspiel, which we label A, B and C.These approaches are summarized in Fig. 2 and can be briefly described as follows. Approach A is a MCTS algorithm thatstays as faithful as possible to previous literature, in particular to existing Phantom Go methods. In this algorithm, a possiblegame state is generated randomly with each simulation, moves are random as well and games are simulated to their naturalend. Approach B is an evolution of MCTS in which the program does not try to generate the opponent’s board; instead, onlythe referee’s messages are simulated. In other words, games are simulated from a player’s partial point of view instead ofthe referee’s omniscient one. Approach C is a simplification of approach B in which the algorithm can explore more nodesby cutting the simulation after just one move.These three programs share major portions of code and implementation, in particular making use of the same represen-tation for the game tree, shown in Fig. 3. As there are thousands of possible opponent moves depending on the unknownlayout of the board, we resort to a three-level game tree for each two plies of the game, two of which represent ref-eree messages rather than moves. The first two layers could be merged together (program moves and their outcomes), butremain separate for computational ease in move selection.676P. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684Fig. 4. Database data for handle “paoloc” playing as White, t = 10, p = knight, both as absolute probabilities and delta values from move 9.Initially, we investigated an approach that was as close as possible to the MCTS techniques developed for Go and itsimperfect information variant, taking into account the important differences between these games and Kriegspiel. We de-veloped the other two methods after performing several unsuccessful tests in which approach A could not be distinguishedfrom a player moving randomly.The three approaches all use some profiling data taken from a database of about 12 000 games played by human playerson the Internet Chess Club. Because information is scarce, some kind of opponent modeling is an important component of aKriegspiel program. Our algorithms make use of information from the game database in order to build an opponent’s model,either for a specific adversary or for an unknown one: the unknown opponent is considered to be an averaged version ofall the players in the database. We will therefore suppose that we have access to two 8 × 8 matrices D w (p, t) and Db(p, t)estimating the probability distribution for piece p at time t when our opponent is playing as White and Black, respectively.These matrices are available for all t up to a certain time when they are deemed too noisy to be of any practical value. Ofcourse, their values can be smoothed by averaging them over several moves or even over neighboring squares, especiallylater in the game.These matrices can contain truly vital information, as shown in Fig. 4. Ten moves (twenty plies) into the game, thelocations of this player’s knights can be inferred with high probability. This is no coincidence: in the almost total absenceof information most players will use the same tested strategies over and over again, making them easier to predict. Thesematrices are used in different ways by our algorithms: approach A uses absolute probabilities (the unmodified values of D wand Db) in order to reconstruct realistic boards for MCTS sampling purposes, whereas approaches B and C exploit gradientvalues, that is, the values of D(p, t + 1) − D(p, t) in order to evolve their abstract model from one move to the next.5.1. Approach APseudocode for approach A is shown in Fig. 5. Our approach A implements the four steps of MCTS as follows.• Selection is implemented with UCT for the program’s own moves, as seen in the pseudocode: the opponent plays thesame pseudorandom moves as in the Simulation step. Choosing different values for the exploration constant c did nothave any impact on performance. [9] showed that there are two main methods for guessing the opponent’s unknownstones in Phantom Go: late random opponent-move guessing and early probabilistic opponent-move guessing. In theformer, some stones are added as the opponent plays them and the rest are filled just before the simulation step; inthe latter, stones are added after the first move based on their frequency of play during the first move. It is noted thatearly guessing outperforms late guessing.The concept of move is very different in Kriegspiel, so we would not be able to easily build and use frequency statisticsin the same way. Nevertheless, recognizing the power of early guessing, we fill the entire board before we even startSelection (note, however, that the tree does not contain boards, but only referee’s messages which are used to traverseit; the tree never deals with specific boards). We used the probability distributions D w and Db discussed in the previoussection. They were collected from a database of online games to estimate density for each piece type at any given pointin time. The matrices, including completely a priori knowledge, are not the only information used by the algorithm;several heuristics helped to construct the random boards, such as remembering how many pieces are left in play, andP. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684677function approach_A(Node root) {while (availableTime) {Board b = generateRandomBoard(root);Node n = root;Move move;while (!isLeaf(n)) {if (programTurn(n)) {n = uctSelection(n,legalMoves(b),refereeMessage(b,move));move = n.move;}else {move = getPseudoRandomMove(b);n = getChild(n,refereeMessage(b,move));}playMove(b,move);}n = expand(n);double outcome = simulation(b);backpropagate(outcome,n);}return mostVisitedChild(root);}Fig. 5. Pseudocode for approach A.how many pawns can be on each file. The generator also strived to make positions that did not contradict the lastreferee’s message, as Last Observation Sampling was reported to yield the best results in [3] when applied to the sametask.• We implement Expansion by adding a new random node with each iteration. We considered this random choice tobe a reasonable solution; we judged a custom Expansion heuristic to be remarkably difficult to devise in Kriegspiel.Choosing a new node for each simulation also allows to easily compare this approach to an evaluation function-basedone exploring the same amount of nodes.• Simulation raises a number of questions in a game of partial information, such as whether and how to generate themissing information, and how to handle subsequent moves. Existing research is of limited help, since to the best of ourknowledge this is the first time MCTS is applied to a game in which knowledge barely survives the next move or two.Go is relatively straightforward in that one can play a random move anywhere except in one’s own eyes. It is also easierto estimate the length of a simulated Go game, which is generally related to the number of intersections left on theboard. Kriegspiel simulations are necessarily heavier to compute due to the rules of the game. Even generating the listof moves is a nontrivial task that requires special care.Our simulated players play pseudorandom moves until they draw by the fifty move rule or they reach a standardendgame position with a clear winner (such as king and rook versus king), in which case the game is adjudicated. Inorder to make the simulation more realistic, both players almost always try to capture back or exploit a pawn try whenpossible – this is basic and almost universal human behavior when playing the game, and is also shared by all ourprograms. In this sense the simulated moves are not random, but only pseudorandom.• We implemented standard Backpropagation, using the average node value as backup operator.As mentioned, approach A failed, performing little better than a random player and losing badly and on every timesetting to a more traditional program based on minimax search. Program A’s victory ratio was below 2%, and its victorieswere essentially random and unintentional mid-game checkmates. Investigating the reasons of the failure showed threemain ones, in addition to the obvious slowness of the search.First, the positions for the opponent’s pieces as generated by the program are not realistic. The generation algorithm usesprobability distributions for pieces, pawns and king that are updated after each referee message. While the probabilities arequite accurate, this does not account for the high correlation between different pieces, that is, pieces protecting other pieces.Kriegspiel players generally protect their pieces quite heavily, in order to maximize their chances of successfully repellingan attack. As a result, the program tends to underestimate the protection level of the opponent’s pieces. Secondly, becausemoves are chosen randomly, it also underestimates the opponent’s ability to coordinate an attack and hardly pays attentionto its own defense.Lastly, but perhaps most importantly, there is the subtler issue of progress, defined as decreasing uncertainty. Gameswhere Monte Carlo tree search has been tested most thoroughly have a built-in notion of progress. In Go, adding a stonechanges the board permanently, permanently decreasing the possible moves in the future. The same happens in Scrabblewhen a word is added to the board. Kriegspiel, on the other hand, like most wargames, has no such notion; if the playersdo nothing significant, nothing happens. In fact, it can be argued that many states have similar values and a program failingto find a good long-term plan will either rush a very risky plan or just choose to minimize the risk by moving the same678P. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684function approach_B(Node root, int k) {while (availableTime) {Node n = root;Move move;while (!isLeaf(n)) {if (programTurn(n)) {n = uctSelection(n);Message msg = probabilisticMessage(n);n = getChild(n,msg);}else {Message msg = probabilisticMessage(n);n = getChild(n,msg);}}n = expand(n);double outcome = simulation(n,k);backpropagate(outcome,n);}return mostVisitedChild(root);}Fig. 6. Pseudocode for approach B.piece back and forth. When a Monte Carlo method does not perform enough simulations to find a stable maximum, it cando either.It is unlikely for a mere implementation of MCTS techniques as seen in Go or Phantom Go to work effectively inKriegspiel, at least under the resource constraints of current computer systems. In order for it to work, we would haveto change the rules of the game or the players would need to receive more information on the state of the board.5.2. Approach BAn effective Monte Carlo tree search Kriegspiel program needs to converge to a better value, not to mention more quickly,than the implementation presented in approach A. Reducing the major amount of noise in the simulation step is also ofparamount importance. As seen, performing Monte Carlo search on individual states, as standard MCTS would dictate, leadsto highly unstable results. A possible solution could lay in running simulations but not on individual game states – rather,on their perception from the computer player’s point of view. This would save us the trouble, both computational andalgorithmic, of generating plausible game states that reward intelligent play in simulations.The core spirit of Monte Carlo methods is preserved by running the simulations as usual, but instead of running themas chess games with complete information, they would be run as Kriegspiel games with partial information. As an aside,simulating an abstract model of the game instead of the game itself has already been done in the context of Monte Carloprograms; for example, [19] does so with a real-time strategy game, for which a detailed simulation over continuous timewould be impossible. What the authors do instead is simulating high-level system responses to high-level decisions andstrategies, and this is conceptually close to our own goal.We therefore define our program B, whose pseudocode is listed in Fig. 6. This approach removes the randomness involvedin generating single states and instead only simulates the referee messages, without worrying about the enemy layout thatgenerated them. A reduced version of the abstract model used in approach A estimates the likelihood of a given refereemessage in response to a certain move.Our model is very utilitarian. For example, there is a chance of the enemy retaliating on a capture one or more timesand a chance of a move being illegal. At core, this is based on three 8 × 8 piece probability matrices Pk (king), Pw (pawn)and Pc (other chessman). P i j contains the probability of a piece of the given type being on square (i, j), with rank 0 beingWhite’s first rank and the opponent being Black. We do not distinguish between different pieces such as queens and rooksas most Kriegspiel rulesets do not give a player enough information to do so. In approach A, the same matrices are usedto generate random chessboards, but here they serve their purpose directly in probabilisticMessage: they determinethe probabilities with which referee’s messages are picked in response to a move (UCT still selects the move).We make two sets of assumptions. The first set models the rules of chess to predict the outcomes of the program’sown moves from the probability matrices for the opponent’s pieces. It also updates the probabilities with the knowl-edge gained from the referee’s responses to the program’s moves. The second set provides our opponent model, updatingthe opponent’s probabilities when it is his turn to move and deciding the outcomes of his moves. In other words, thefirst set of assumptions is nothing more than probability theory applied to chess; the second set is, in fact, an opponentmodel.P. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684679The first set is as follows:• The probability for the opponent to control a square (i, j) is equal to a sum of componentsProbcontrol(i, j) =Pkxy + Pwi−1, j+1 + Pwi+1, j+1 + c1(cid:3)dist(x, y,i, j)=1(cid:3)c2Pcxy,x, ymeaning the sum of probabilities for the king in the surrounding squares, a pawn in the compatible diagonal squares,and all squares on the same rank, file and diagonals multiplied by suitable coefficients. Here, c1 = 3/7 since at mostthree out of seven pieces in the starting set other than the king and pawns are able to attack along any given direction:queen and rooks for ranks and files, and queen and bishops for the diagonals. The only exception is the knight check,which only two pieces can perform. c2 is calculated dynamically so that enemy pieces covering each other are accountedfor; basically, c2 decreases as the distance to (i, j) increases. Probcontrol can be greater than 1; in fact, it should be readas the expectation for the number of enemy pieces controlling the square.• The probability for a move to be legal is equal to the probability of all squares on the piece’s path Pt from (i1, j1) to(i2, j2) (except the destination square itself unless it is a straight pawn move) being empty, minus a pin probability. Werecall that a piece is pinned if moving it would leave the king in check. That is,(cid:4)Problegal(Pt) =(1 − Pki j − Pwi j − Pci j) − Probpin,(i, j)∈Ptwhere⎧⎨⎩Probpin =0Probcontrol(i1 j1)Probcontrol(i2 j2)if the piece is not protecting the king,if the piece is protecting the king,if the piece is the king.This, while approximated, accounts for a number of cases, including pieces being pinned by unknown enemy piecesand the king being unable to move to a threatened square.• The probabilities of capturing a piece or pawn on (i2, j2) are equal to Pci2 j2 and Pwi2 j2 , respectively.• The probability of the program causing a check is equal to a sum of Pki j over the squares threatened by the move, againwith a damping coefficient c2 designed to reduce the impact of far away squares.• When a square is found to be empty by moving through it or due to a lack of pawn tries, the probabilities for theenemy pieces on that square are set to 0. Conversely, when a square is known to be occupied (usually because of acapture), the sum of the probability matrices for that squares is brought to 1. In both cases, the matrices are normalizedafterwards so their total sum over the board does not change.The second set contains the following assumptions suggested by human play observed on the Internet Chess Club:• When the program captures something, there is a very high chance of the capturing piece being, in turn, captured bythe opponent. This reflects the fact that most pieces are always protected. Long chains of blind sacrifices are commonin Kriegspiel: for the second and subsequent captures, the program uses Probcontrol to determine whether there isretaliation.• When a check message is heard there is a chance, assumed to be constant in our model, that the checking piece iscaptured. Human players often try to capture the offending piece as their first reaction to a check. In particular, a playerhas nothing to lose from probing the check’s direction with his king.• When the opponent moves, there is a fixed chance to suffer a capture. The victim is chosen at random, with theprobability of capture being directly proportional to Probcontrol so that more exposed pieces are captured with higherprobability.• All pieces stand a more or less equal chance of being moved by the opponent; if the program knows that the opponenthas k1 pawns and k2 pieces left, the probabilities of the king, a pawn, or a piece being moved are, respectively,P king =1k1 + k2 + 1,P pawn =k1k1 + k2 + 1,P piece =k2k1 + k2 + 1.• The enemy king’s movement is modeled as a random walk over a graph corresponding to the set of permissible squares.Pki j(t + 1) = (1 − P king)Pki j(t) + P king(cid:3)(cid:3)i−1(cid:2)x(cid:2)i+1j−1(cid:2) y(cid:2) j+1f king(x, y, t)Pkxy(t),where fis a suitable function that scales and centers the probability delta values gathered from the game databasediscussed in Section 5, so that their sum is 1. This function makes use of D w or Db, depending on whether the opponentis White or Black. The rationale behind using delta values from the previous move instead of directly comparing thevalues of D is that delta values represent trends rather than snapshots, and seem to be more likely to carry over evenduring atypical games.680P. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684Fig. 7. Density spreading routine in approaches B and C (second diagonal sweep not shown).• Pawns are modeled separately as one-way Markov chains.• A generic piece other than a pawn or king is the most complex to model. The computational burden of calculating acustom transition matrix for each chessboard (as its values would change depending on board layout) and discoveringwhich squares can affect which would be too high: MCTS relies on speed and number of simulations. Instead, the boardis scanned along several directions, as shown in Fig. 7. Whenever a group of two or more empty squares is found, theprogram runs a fast random walk update over those squares, still using function f and the database data as long as it isavailable. If the database is not active or the game has reached a point where it is no longer useful, all squares becomeequally attractive.Pci j(t + 1) = (1 − P piece)Pci j(t) + c1 P piecec2 P i j(t),(cid:3)with c1 and c2 indicated as different constants for exposition’s sake. c1 is again a piece probability factor, as not allpieces can move along a given direction. It is also a generic adjustment factor; it indicates the probability of finding apiece that can move as desired and is willing to. In the current implementation, c2 = 1k−1 , where k is the number ofsquares in the sequence.While this algorithm can help improve performance and run more simulations than approach A can in the same amountof time, the real advantage is that the opponent no longer plays randomly in the simulations. Instead, it plays accordingto some average, realistic expectations while the actual moves are never disclosed. This is very close to the way a humanKriegspiel player plans his moves.A second point of interest about method B is that it does not play full games as that proved to be too detrimental toperformance in approach A. Instead, it simulates a portion of the game that is at most k moves long (k is passed as aparameter). The algorithm also accounts for quiescence, and allows simulations to run past the limit of k moves after itsstarting point in the event of a string of captures. The first move is considered to be the one leading to the tree nodewhere simulation begins; as such, when k = 1, there is basically no exploration past the current node except for quiescence.Intuitively, a low value of k gives the program less foresight but increases the number of simulations and as such its shortterm accuracy; a high value of k should do the opposite. At the end of the simulated snippet, the resulting chessboardis evaluated using the only real notion of Kriegspiel theory in this method; that basically reduces to counting how manypieces the player has left, minus the number of enemy pieces left.5.3. Approach CThe third and final approach, called C and shown in Fig. 8, is approach B taken to the extreme for k = 1; it was developedafter noticing the success of that value of k in the first tests. There is a tendency already noticed in Kriegspiel literature,first in [3] and then in [18], for myopic searches to outperform their far-sighted counterparts. If anything, using k = 1 offersa tremendous performance boost, as each node needs only be sampled once. Since the percentages for each referee messageare known in the model, it is easy to calculate the results for each and obtain a weighed average value. As seen in thepseudocode, the function getOutcomeProbabilities interrogates the referee simulator on the probabilities of a given outcometaking place from the penultimate to the latest explored node. Each outcome has a progress value identical to approach B’sand equal to the number of allied pieces on the board.Approach C makes the bold assumption that the value estimated with approach B’s abstract model for k = 1 is the truth,or at least as close to the truth as one can get. Because simulations are assumed to instantly converge through the weighedaverage, the backup operator is also changed from the average to the maximum node value. Of course, this is the fastestsimulation strategy, blurring the line between simulation and a UCT-driven evaluation function (or, more accurately, a costfunction in a pathfinding algorithm), and it can be very discontinuous from one node to the next. If approach C is successful,it means that information in Kriegspiel is so scarce and of such a transient nature that the benefits of global explorationby simulating longer games are quite limited compared to the loss of accuracy in the short run. This emphasizes selectionstrategies over simulation strategies. Another way to think of approach C is as if simulations happened entirely on the treeP. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684681function approach_C(Node root) {while (availableTime) {Node n = root;Move move;while (!isLeaf(n)) {if (programTurn(n)) {n = uctSelection(n);Message msg = probabilisticMessage(n);n = getChild(n,msg);}else {Message msg = probabilisticMessage(n);n = getChild(n,msg);}}if (n.explored) n = expand(n);double outcomeValues[ ], probabilities[ ], value;getOutcomeProbabilities(n,outcomeValues,probabilities);for (int a=0; a<outcomeValues.length; a++)value += outcomes[a] * probabilities[a];n.explored = true;backpropagate(outcome,n);}return mostVisitedChild(root);}Fig. 8. Pseudocode for approach C.Fig. 9. Example of a C simulation step (simplified).itself rather than in separate trials, at the rate of one simulation per node. This is based on the assumption that good nodesare more likely to have good children, and the best node usually lies at the end of a series of good or decent nodes.Fig. 9 shows an example of how approach C might simulate Rg1 on the sample board. The actual program wouldconsider more referee messages than those listed. The nodes contain the material balance at that time: it is positive whenthe player has captured more pieces than he lost. The weighed average of the leaf nodes amounts to 0.047, which is thenbackpropagated. The program handles quiescence moves, as seen in the Ng1 retaliatory move if the rook is captured.6. Tests6.1. Tests versus a minimax programWe tested our approaches, with the exception of A which is not strong enough to be interesting, against an improvedversion of our program Darkboard described in [4], which we call “minimax program”. Tests versus humans on the InternetChess Club showed that Darkboard’s playing strength is reasonable by human standards, ranking at club level above average(around 1600 Elo points). The program used in our tests is even slightly stronger than the aforementioned one, since itperforms a series of hard-coded checks that prevent the program from making obvious blunders. It should be noted thatour MCTS programs do not include these checks.The evaluation function of the minimax program is rather complex and domain-specific, consisting of several componentsincluding material, positional and information bonuses. By contrast, our MCTS programs know very little about Kriegspiel:both B and C only know that the more pieces they have, the better. They know nothing about protection, promoting pawns,securing the center or gathering information trying moves which are likely to be illegal.682P. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684Fig. 10. Comparison of MCTS approaches B and C with a fixed-depth minimax program at different time settings and simulation depths, with error intervals.The experimental setup is as follows, and the results of the tests are summarized in Fig. 10. We played 16 instances ofB and 4 of C against the minimax program (A was judged too weak to test). The instances of B are the combinations offour time settings (1, 2, 4 and 8 seconds per move) with four values of k: 1, 3, 7 and 10. The instances of C correspond tothe time settings alone as k is by definition 1 in this approach. The minimax program always plays under the same settingsfound to be optimal for its algorithm. It explores approximately 10 000 metapositions per move; the presence of a pruningalgorithm ensures a search depth of 3 to 5 moves depending on the situation. The minimax program requires between 1and 2 seconds to run under these settings; the program hits a performance plateau after this point, preventing it fromfurther increases in strength. In other words, this was the strongest version of Darkboard we could ready for use.The test took place on an eight-core Mac Pro machine, with two sets of tests running simultaneously over three weeks.The MCTS programs can take advantage of multiple processors thanks to a form of root parallelism, whereas the minimaxprogram does not. We ran a variable number of games for each tested instance, ranging from a minimum of 1500 for the8-second tests to a maximum of 4000. The brackets in the figure represent a 95% confidence interval for the data, expressedas Elo ratings. Programs that perform worse than the minimax player are below 0 in the graph whereas the better programsare above 0. We recall that in the Elo rating system a difference of 200 points corresponds to an expected result of about0.75 (with 1 being a win and 0.5 being a draw), and a difference of 400 points has an expected result of about 0.9.The MCTS programs do not have particular optimizations and their parameters have not been thoroughly fine-tuned.The programs are all identical outside the simulation task, with the single exception of the UCT exploration parameter c.Approach C uses a lower value of c leaning towards exploitation more on the basis that each node is only evaluated once.However, this different value of c has only a small beneficial value on the program: most of the advantage of the weighedaverage method lies in its speed, which allows it to visit many more nodes than the corresponding B program for k = 1.The speedup factor ranges from 10 to 20 on average, everything else being equal.Experimental findings generally confirm our expectation, that is, the lower values of k should be more effective underfaster time settings, and the higher values of k should eventually gain the upper hand as the program is given more timeto reason. When k is low, the program can execute more simulations which make it more accurate in the short term, thusreducing the number of tactical blunders. On the other hand, given enough time the broader horizon of a higher k findsmore strategic possibilities and longer plans through simulation that the lower k cannot see until they are encounteredthrough selection.At 1 second per move, k = 1 has a large advantage over the other B programs. Doubling the time reduces the gapamong all programs, and at 4 and 8 seconds per move the longer simulations have a small but significant edge, actuallyoutperforming the minimax program by a slight margin. The only disappointment came from the k = 3 programs, whichdid not really shine under any time setting. It is possible that three moves is just not enough to consistently generate goodplans out of random tries. Since Kriegspiel plans can be interleaved with basically useless moves that more or less maintainthe status quo on the board, a ten-move sequence can contain a good three-move sequence with higher likelihood.Given the simplicity of the approach and the lack of specialized knowledge compared to the minimax program’s trainedparameters and pruning techniques, B programs are quite remarkable, though not as much as the performance of C typeprograms. These can defeat the benchmark program consistently, ranking over 100 Elo points above it and winning aboutP. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684683three times more games than they lose to it. Since approach C has basically no lookahead past the node being explored,we can infer that UCT selection is the major responsible for its performance, favoring the paths of least danger and highestreward under similar time settings to the minimax program’s. The UCT exploration–exploitation method beats the hardpruning algorithms used by the minimax program, showing that in such a game as Kriegspiel totally pruning a node canoften remove an interesting, underestimated line of play: there are relatively few bad nodes that can be safely ignored. Itappears more profitable to allocate different amounts of time and resources to different moves, like in Monte Carlo treesearch and the related n-armed bandit problem.6.2. Tests versus humansWe collected more experimental evidence that approach C is effective by letting it play against humans over the ICC,where it reached an average of 1750 Elo points after playing 7121 games. The matches are significant as they all tookplace under the same time settings, 3 minutes without increment. These time settings are considered quite generous forKriegspiel games on the ICC, meaning that the program does not win by just making its opponents time out. Out of allgames, 2777 were played against a set of 20 players recognized as the strongest in the community. Program C’s averagescore against these players is 0.26, whereas the minimax program’s average score against the same players was 0.17. Thisis a further confirmation that there is a gap of about 100 Elo points between the old program and the new, with 200 morepoints to be gained before the computer can challenge a top human on equal terms.Last but not least, approach C confirmed its playing strength by winning the gold medal with a perfect score in theKriegspiel tournament held at the 14th Computer Olympiad in Pamplona, Spain, in May 2009.7. Conclusions and future workThere are two main conclusions to be drawn from our experiments. First, they show that a Monte Carlo tree searchalgorithm can converge to good results in a reasonable amount of time even in a very difficult environment like Kriegspiel,whose lengthy simulations might at first appear to be a significant disadvantage of the method. However, the applicationof a Monte Carlo method must be carefully designed. In fact, we were unable to achieve a good level of play by generatingrandom states and running simulations on them. Instead, we had to abstract the game with a model in which single statesare not important, and only their perception matters.The second conclusion is that approach C, in which simulations only last one move plus quiescence, performed the best,defeating a minimax-based program and faring well against human players. This seemingly counterintuitive result can beexplained with the accuracy of algorithm C in simulating short-term tactical opportunities. The difference between B and Cdecreases as the program is allotted more simulation time, but never totally disappears. This may hint at inaccuracies in theassumptions behind the referee’s simulated messages. The simulated games in B would therefore suffer more from theseweaknesses.Our program as a whole can still be improved by a large factor. It is possible to refine and compare the simulationassumptions for the referee starting from the current ones, for example through genetic algorithms or other adaptive meth-ods. It is also possible to consider a hybrid approach between B and C; this method would simulate the first move like Cand then launch longer simulations for the next moves. A similar method combining heuristics and simulation for Go isdescribed in [20].Many more improvements are possible. In the game of Go, Monte Carlo tree search is more and more often combinedwith game-specific heuristics that help the program in the Selection and Simulation tasks. Since Monte Carlo methods arerelatively weaker when they are short on time, these algorithms drive exploration through young nodes when there is littlesampling data available on them. Examples of such algorithms are the two progressive strategies described in [21]. SinceKriegspiel is often objective-driven when played by humans, objective-based heuristics are the most likely candidates tomake good progressive strategies, and research is already underway in that direction. More sophisticated opponent modelingtechniques also seem necessary to conquer the strongest opponents.AcknowledgementsWe thank the anonymous reviewers for their comments and suggestions which improved this paper.References[1] T. Cazenave, A Phantom Go program, in: J. van den Herik (Ed.), Proc. Advances in Computer Games, vol. 11, Taipei, Taiwan (revised papers), in: LectureNotes in Computer Science, vol. 4250, Springer, 2005, pp. 120–126.[2] J. von Neumann, O. Morgenstern, Theory of Games and Economic Behavior, Princeton University Press, 1944.[3] A. Parker, D. Nau, V. Subrahmanian, Game-tree search with combinatorially large belief states, in: Proc. 19th Int. Joint Conf. on Artificial Intelligence(IJCAI05), Edinburgh, Scotland, 2005, pp. 254–259.[4] P. Ciancarini, G. Favini, Representing Kriegspiel states with metapositions, in: Proc. 20th Int. Joint Conference on Artificial Intelligence (IJCAI-07),Hyderabad, India, 2007, pp. 2450–2455.[5] M. Sakuta, H. Iida, Solving kriegspiel-like problems: Exploiting a transposition table, ICCA Journal 23 (4) (2000) 218–229.684P. Ciancarini, G.P. Favini / Artificial Intelligence 174 (2010) 670–684[6] L. Kocsis, C. Szepesvari, Bandit based Monte-Carlo planning, in: Proc. European Conference on Machine Learning (ECML), in: Lecture Notes in ComputerScience, vol. 4212, Springer, 2006, pp. 282–293.[7] S. Gelly, Y. Wang, Exploration exploitation in Go: UCT for Monte-Carlo Go, in: NIPS: Proc. 20th Annual Conference on Neural Information ProcessingSystems, 2006.[8] R. Coulom, Efficient selectivity and backup operators in Monte-Carlo tree search, in: Computers and Games, in: Lecture Notes in Computer Science,vol. 4630, Springer, 2007, pp. 72–83.[9] J. Borsboom, J. Saito, G. Chaslot, J. Uiterwijk, A comparison of Monte-Carlo methods for Phantom Go, in: Proc. 19th Belgian–Dutch Conference onArtificial Intelligence – BNAIC, Utrecht, The Netherlands, 2007.[10] D. Billings, A. Davidson, J. Schaeffer, D. Szafron, The challenge of poker, Artificial Intelligence 134 (2002) 201–240.[11] C. Wetherell, T. Buckholtz, K. Booth, A director for Kriegspiel, a variant of chess, The Computer Journal 15 (1) (1972) 66–70.[12] T. Ferguson, Mate with bishop and knight in Kriegspiel, Theoretical Computer Science 96 (1992) 389–403.[13] A. Bolognesi, P. Ciancarini, Computer programming of Kriegspiel endings: The case of KR vs K, in: J. van den Herik, H. Iida, E. Heinz (Eds.), Advancesin Computer Games, vol. 10, Kluwer, Graz, Austria, 2003, pp. 325–342.[14] M. Nance, A. Vogel, E. Amir, Reasoning about partially observed actions, in: Proc. 21st National Conf. on Artificial Intelligence (AAAI ’06), Boston, USA,2006, pp. 888–893.[15] S. Russell, J. Wolfe, Efficient belief-state AND–OR search, with application to Kriegspiel, in: Proc. 19th Int. Joint Conf. on Artificial Intelligence (IJCAI05),Edinburgh, Scotland, 2005, pp. 278–285.[16] M. Sakuta, Deterministic solving of problems with uncertainty, PhD thesis, Shizuoka University, Japan, 2001.[17] A. Del Giudice, P. Gmytrasiewicz, J. Bryan, Towards strategic Kriegspiel play with opponent modeling, in: AAMAS 09: Proc. 8th Int. Conf. on AutonomousAgents and Multiagent Systems, Budapest, Hungary, International Foundation for Autonomous Agents and Multiagent Systems, 2009, pp. 265–1266.[18] J. Bryan, P. Gmytrasiewicz, A. Del Giudice, Particle filtering approximation of Kriegspiel play with opponent modeling, in: AAMAS 09 Workshop onMulti-agent Sequential Decision-Making in Uncertain Domains, 2009.[19] M. Chung, M. Buro, J. Schaeffer, Monte Carlo planning in RTS games, in: Proc. IEEE Symposium on Computational Intelligence and Games, 2005.[20] S. Gelly, D. Silver, Combining online and offline knowledge in UCT, in: ICML 07: Proc. 24th Int. Conf. on Machine Learning, ACM Press, 2007, pp. 273–280.[21] G. Chaslot, M. Winands, J. van der Herik, J. Uiterwijk, B. Bouzy, Progressive strategies for Monte-Carlo tree search, New Mathematics and NaturalComputation 4 (3) (2008) 343–352.