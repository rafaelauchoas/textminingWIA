Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.   Contents lists available at ScienceDirect Computers in Biology and Medicine journal homepage: www.elsevier.com/locate/compbiomed COVID-19 detection in cough, breath and speech using deep transfer learning and bottleneck features Madhurananda Pahar a, *, Marisa Klopper b, Robin Warren b, Thomas Niesler a, ** a Department of Electrical and Electronic Engineering, Stellenbosch University, South Africa b SAMRC Centre for Tuberculosis Research, DSI-NRF Centre of Excellence for Biomedical Tuberculosis Research, Division of Molecular Biology and Human Genetics, Faculty of Medicine and Health Sciences, Stellenbosch University, South Africa  A R T I C L E I N F O  A B S T R A C T  Keywords: COVID-19 Cough Breath Speech Transfer learning Bottleneck features We present an experimental investigation into the effectiveness of transfer learning and bottleneck feature extraction in detecting COVID-19 from audio recordings of cough, breath and speech. This type of screening is non-contact, does not require specialist medical expertise or laboratory facilities and can be deployed on inex-pensive consumer hardware such as a smartphone. We use datasets that contain cough, sneeze, speech and other noises, but do not contain COVID-19 labels, to pre-train three deep neural networks: a CNN, an LSTM and a Resnet50. These pre-trained networks are subsequently either fine-tuned using smaller datasets of coughing with COVID-19 labels in the process of transfer learning, or are used as bottleneck feature extractors. Results show that a Resnet50 classifier trained by this transfer learning process delivers optimal or near-optimal performance across all datasets achieving areas under the receiver operating characteristic (ROC AUC) of 0.98, 0.94 and 0.92 respectively for all three sound classes: coughs, breaths and speech. This indicates that coughs carry the strongest COVID-19 signature, followed by breath and speech. Our results also show that applying transfer learning and extracting bottleneck features using the larger datasets without COVID-19 labels led not only to improved performance, but also to a marked reduction in the standard deviation of the classifier AUCs measured over the outer folds during nested cross-validation, indicating better generalisation. We conclude that deep transfer learning and bottleneck feature extraction can improve COVID-19 cough, breath and speech audio classification, yielding automatic COVID-19 detection with a better and more consistent overall performance.  1. Introduction COVID-19 (COrona VIrus Disease of 2019) was declared a global pandemic on February 11, 2020 by the World Health Organisation (WHO). Caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), this disease affects the respiratory system and includes symptoms like fatigue, dry cough, shortness of breath, joint pain, muscle pain, gastrointestinal symptoms and loss of smell or taste [1,2]. Due to its effect on the vascular endothelium, the acute respiratory distress syndrome can originate from either the gas or vascular side of the alveolus which becomes visible in a chest x-ray or computed tomogra-phy (CT) scan for COVID-19 patients [3,4]. Among the patients infected with SARS-CoV-2, between 5% and 20% are admitted to an intensive care unit (ICU) and their mortality rate varies between 26% and 62% [5]. Medical lab tests are available to diagnose COVID-19 by analysing exhaled breaths [6]. This technique was reported to achieve an accuracy of 93% when considering a group of 28 COVID-19 positive and 12 COVID-19 negative patients [7]. Related work using a group of 25 COVID-19 positive and 65 negative patients achieved an area under the ROC curve (AUC) of 0.87 [8]. Previously, machine learning algorithms have been applied to detect COVID-19 using image analysis. For example, COVID-19 was detected from CT images using a Resnet50 architecture with 96.23% accuracy in Ref. [9]. The same architecture also detected pneumonia due to COVID-19 with an accuracy of 96.7% [10] and COVID-19 from x-ray images with an accuracy of 96.30% [11]. * Corresponding author. ** Corresponding author. E-mail addresses: mpahar@sun.ac.za (M. Pahar), marisat@sun.ac.za (M. Klopper), rw1@sun.ac.za (R. Warren), trn@sun.ac.za (T. Niesler). https://doi.org/10.1016/j.compbiomed.2021.105153 Received 17 November 2021; Received in revised form 24 November 2021; Accepted 14 December 2021  ComputersinBiologyandMedicine141(2022)105153Availableonline17December20210010-4825/¬©2021ElsevierLtd.Allrightsreserved.M. Pahar et al.                                                                                                                 The automatic analysis of cough audio for COVID-19 detection has also received recent attention. Coughing is a predominant symptom of many lung ailments and its effect on the respiratory system varies [12, 13]. Lung disease can cause the glottis to behave differently and the airway to be either restricted or obstructed and this can influence the acoustics of the vocal audio such as cough, breath and speech [14,15]. This raises the prospect of identifying the coughing audio associated with a particular respiratory disease such as COVID-19 [16,17]. Re-searchers have found that a simple binary machine learning classifier can distinguish between healthy and COVID-19 respiratory audio, such as coughs gathered from crowdsourced data, with an AUC above 0.8 [18]. Improved performance was achieved using a convolutional neural network (CNN) for cough and breath audio, achieving an AUC of 0.846 [19]. In our previous work, we have also found that automatic COVID-19 detection is possible on the basis of the acoustic cough signal [20]. Here we extend this work firstly by considering whether breath and speech audio can also be used effectively for COVID-19 detection. Secondly, since the COVID-19 datasets at our disposal are comparatively small, we apply transfer learning and extract bottleneck features to take advantage of other datasets that do not include COVID-19 labels. To do this, we use publicly available as well as our own datasets that do not include COVID-19 labels to pre-train three deep neural network (DNN) archi-tectures: a CNN, a long short-term memory (LSTM) and a 50-layer residual-based architecture (Resnet50), which uses convolutional layers with skip connections. For subsequent COVID-19 classifier eval-uation, we used the Coswara dataset [21], the Interspeech Computa-tional Paralinguistics ChallengE (ComParE) dataset [22] and the Sarcos dataset [20], all of which do contain COVID-19 labels. We report further evidence of accurate discrimination using all three audio classes and conclude that vocal audio including coughing, breathing and speech are all affected by the condition of the lungs to an extent that they carry acoustic information that can be used by existing machine learning classifiers to detect signatures of COVID-19. We are also able to show that the variability in performance of the classifiers, as measured over the independent outer folds of nested cross-validation, is strongly reduced by the pre-training, despite the absence of COVID-19 labels in the pre-training data. We can therefore conclude that the application of transfer learning enables the COVID-19 classifiers to perform both more accurately and with greater greater consistency. This is key to the viability of the practical implementation of cough audio screening, where test data can be expected to be variable, depending for example on the location and method of data capture. Sections 2 and Section 3 summarise the datasets used for experi-mentation and the primary feature extraction process. Section 4 de-scribes the transfer learning process and Section 5 explains the bottleneck feature extraction process. Section 6 presents the experi-mental setup, including the cross-validated hyperparameter optimisa-tion and classifier evaluation process. Experimental results are presented in Section 7 and discussed in Section 8. Finally, Section 9 summarises and concludes this study. 2. Data 2.1. Datasets without COVID-19 labels for pre-training classification. The last two (Google Audio Set & Freesound and Lib-rispeech) were compiled from publicly available data. Since all five datasets were compiled before the start of the COVID-19 pandemic, they are unlikely to contain data from COVID-19 positive subjects. All data-sets used for pre-training include manual annotations but exclude COVID-19 labels. 2.1.1. TASK dataset This corpus consists of spontaneous coughing audio collected at a small tuberculosis (TB) clinic near Cape Town, South Africa [23]. The dataset contains 6000 recorded coughs by patients undergoing TB treatment and 11 393 non-cough sounds such as laughter, doors opening and objects moving. This data was intended for the development of cough detection algorithms and the recordings were made in a multi-bed ward environment using a smartphone with an attached external microphone. The annotations consist of the time locations and labels of sounds, including coughs. 2.1.2. Brooklyn dataset This dataset contains recordings of 746 voluntary coughs by 38 subjects compiled for the development of TB cough audio classification systems [24]. Audio recording took place in a controlled indoor booth, using a R√òDE M3 microphone and an audio field recorder. The anno-tations include the start and end times of each cough. 2.1.3. Wallacedene dataset This dataset consists of recordings of 1358 voluntary coughs by 51 patients, also compiled for the development of TB cough audio classi-fication [25]. In this case, audio recording took place in an outdoor booth located at a busy primary healthcare clinic. Recording was per-formed using a R√òDE M1 microphone and an audio field recorder. This therefore a poorer data has more environmental noise and signal-to-noise ratio than the Brooklyn dataset. As for the Brooklyn dataset, annotations include the start and end times of each cough. 2.1.4. Google Audio Set & Freesound The Google Audio Set dataset contains excerpts from 1.8 million Youtube videos that have been manually labelled according to an ontology of 632 audio event categories [26]. The Freesound audio database is a collection of tagged sounds uploaded by contributors from around the world [27]. In both datasets, the audio recordings were contributed by many different individuals under widely varying recording conditions and noise levels. From these two datasets, we have compiled a collection of recordings that include 3098 coughing sounds, 1013 sneezing sounds, 2326 speech excerpts and 1027 other non-vocal sounds such as engine noise, running water and restaurant chatter. Previously, this dataset was used for the development of cough detection algorithms [28]. Annotations consist of the time locations and labels of the particular sounds. 2.1.5. LibriSpeech As a source of speech audio data, we have selected utterances by 28 male and 28 female speakers from the freely available LibriSpeech corpus [29]. These recordings contain very little noise. The large size of the corpus allowed easy gender balancing. Audio data with COVID-19 labels remain scarce, which limits clas-sifier training. We have therefore made use of five datasets without COVID-19 labels for pre-training. These datasets contain recordings of coughing, sneezing, speech and non-vocal audio. The first three datasets (TASK, Brooklyn and Wallacedene) were compiled by ourselves as part of research projects concerning cough monitoring and cough 2.1.6. Summary of data used for pre-training In total, the data described above includes 11 202 cough sounds (2.45 h of audio), 2.91 h of speech from both male and female partici-pants, 1013 sneezing sounds (13.34 min of audio) and 2.98 h of other non-vocal audio. Hence sneezing is under-represented as a class in the pre-training data. Since such an imbalance can detrimentally affect the ComputersinBiologyandMedicine141(2022)1051532M. Pahar et al.                                                                                                                 performance of neural networks [30,31], we have applied the synthetic minority over-sampling technique (SMOTE) [32]. SMOTE oversamples the minor class by creating additional synthetic samples rather than, for example, random oversampling. We have in the past successfully applied SMOTE to address training set class imbalances in cough detection [23] and cough classification [20] based on audio recordings. In total, therefore, a dataset containing 10.29 h of audio recordings annotated with four class labels (cough, speech, sneeze, noise) was available to pre-train the neural architectures. The composition of this dataset is summarised in Table 1. All recordings used for pre-training were downsampled to 16 kHz. in our previous work exclusively for independent validation [20]. In this study, however, it has also been used to fine-tune and evaluate pre-trained DNN classifiers by means of transfer learning and the extraction of bottleneck features. 2.2.4. Summary of data used for classification Table 2 shows that the COVID-19 positive class is under-represented in all datasets available for classification. To address this, we again apply SMOTE during training. We also note that the Coswara dataset contains the largest number of subjects, followed by ComParE and finally Sarcos. As for pre-training, all recordings were downsampled to 16 kHz. 2.2. Datasets with COVID-19 labels for classification 3. Primary feature extraction Three datasets of coughing audio with COVID-19 labels were avail-able for experimentation. 2.2.1. Coswara dataset This dataset is specifically developed with the testing of classification algorithms for COVID-19 detection in mind. Data collection is web- based, and participants contribute by using their smartphones to re-cord their coughing, breathing and speech. Audio recordings were collected of both shallow and deep breaths as well as speech uttered at a normal and fast pace. However, since the deep breaths consistently outperformed the shallow breaths in our initial experiments, the latter will not be presented in our experiments. At the time of writing, the data included contributions from participants located on five different con-tinents [20,21,33]. Figs. 1 and 2 show examples of Coswara breaths and speech respectively, collected from both COVID-19 positive and COVID-19 negative subjects. It is evident that breaths have more higher- frequency content than speech and interesting to note that COVID-19 breaths are, on average, 30% shorter than non-COVID-19 breaths (Table 2). All audio recordings were pre-processed to remove periods of silence to within a margin of 50 ms using a simple energy detector. 2.2.2. ComParE dataset This dataset was provided as a part of the 2021 Interspeech Computational Paralinguistics ChallengE (ComParE) [22]. The ComParE dataset contains recordings of both coughs and speech, where the latter is the utterance ‚ÄòI hope my data can help to manage the virus pandemic‚Äô in the speaker‚Äôs language of choice. 2.2.3. Sarcos dataset This dataset was collected in South Africa as part of this research and currently contains recordings of coughing by 18 COVID-19 positive and 26 COVID-19 negative subjects. Audio was pre-processed in the same way as the Coswara data. Since this dataset is very small, we have used it From the time-domain audio signals, we have extracted mel- frequency cepstral coefficients (MFCCs) and linearly-spaced log filter-bank energies, along with their respective velocity and acceleration coefficients. We have also extracted the signal zero-crossing rate (ZCR) [34] and kurtosis [34], which are indicative respectively of time-domain signal variability and tailedness, i.e. the prevalence of higher amplitudes. MFCCs have been very effective in speech processing [35], but also in discriminating dry and wet coughs [36], and recently in characterising COVID-19 audio [37]. Linearly-spaced log filterbank energies have proved useful in several biomedical applications, including cough audio classification [24,25,38]. Features are extracted from overlapping frames, where the frame overlap Œ¥ is computed to ensure that the audio signal is always divided into exactly ùíÆ frames, as illustrated in Fig. 3. This ensures that the entire audio event is always represented by a fixed number of frames, which allows a fixed input dimension to be maintained for classification while preserving the general overall temporal structure of the sound. Such fixed two-dimensional feature dimensions are particularly useful for the training of DNN classifiers, and have performed well in our previous experiments [20]. The frame length (‚Ñ± ), number of frames (ùíÆ), number of lower order MFCCs (‚Ñ≥) and number of linearly spaced filters (‚Ñ¨) are regarded as feature extraction hyperparameters, listed in Table 3. The table shows that in our experiments each audio signal is divided into between 70 and 200 frames, each of which consists of between 512 and 4096 samples, corresponding to between 32 msec and 256 msec of audio. The number of extracted MFCCs (‚Ñ≥) lies between 13 and 65, and the number of linearly-spaced filterbanks (‚Ñ¨) between 40 and 200. This allows the spectral information included in each feature to be varied. The input feature matrix to the classifiers has the dimension of (3‚Ñ≥ + 2, ùíÆ) for ‚Ñ≥ MFCCs along with their ‚Ñ≥ velocity and ‚Ñ≥ accel-eration coefficients, as shown in Fig. 3. Similarly, for linearly spaced filters, the dimension of the feature matrix is (3‚Ñ¨ + 2, ùíÆ). Table 1 Summary of the Datasets used in Pre-training. Classifiers are pre-trained on 10.29 h audio recordings annotated with four class labels: cough, sneeze, speech and noise. The datasets do not include any COVID-19 labels. Type Cough Sneeze Speech Noise Dataset Sampling Rate No of Events Total audio Average length Standard deviation TASK dataset Brooklyn dataset Wallacedene dataset Google Audio Set & Freesound Total (Cough) Google Audio Set & Freesound Google Audio Set & Freesound + SMOTE Total (Sneeze) Google Audio Set & Freesound LibriSpeech Total (Speech) TASK dataset Google Audio Set & Freesound Total (Noise) 44.1 kHz 44.1 kHz 44.1 kHz 16 kHz ‚Äî 16 kHz 16 kHz ‚Äî 16 kHz 16 kHz ‚Äî 44.1 kHz 16 kHz ‚Äî 6000 746 1358 3098 11 202 1013 9750 10 763 2326 56 2382 12 714 1027 13 741 91 min 6.29 min 17.42 min 32.01 min 2.45 h 13.34 min 2.14 h 2.14 h 22.48 min 2.54 h 2.91 h 2.79 h 11.13 min 2.79 h 0.91 s 0.51 s 0.77 s 0.62 s 0.79 s 0.79 s 0.79 s 0.79 s 0.58 s 2.72 min 4.39 s 0.79 s 0.65 s 0.79 s 0.25 s 0.21 s 0.31 s 0.23 s 0.23 s 0.21 s 0.23 s 0.23 s 0.14 s 0.91 min 0.42 s 0.23 s 0.26 s 0.23 s ComputersinBiologyandMedicine141(2022)1051533M. Pahar et al.                                                                                                                 Fig. 1. Pre-processed breath signals from both COVID-19 positive and COVID-19 negative subjects in the Coswara dataset. Breaths corresponding to inhalation are marked by arrows, and are followed by an exhalation. Fig. 2. Pre-processed speech (counting from 1 to 20 at a normal pace) from both COVID-19 positive and COVID-19 negative subjects in the Coswara dataset. In contrast to breath (Fig. 1), the spectral energy in this speech is concentrated below 1 kHz. Table 2 Summary of the datasets used for COVID-19 classification. Cough, breath and speech signals were extracted from the Coswara, ComParE and Sarcos datasets. COVID-19 positive subjects are under-represented in all three. Type Dataset Sampling Rate Label Subjects Total audio Average per subject Standard deviation Cough Coswara 44.1 kHz ComParE 16 kHz Sarcos 44.1 kHz Breath Coswara 44.1 kHz Speech Coswara (normal) 44.1 kHz Coswara (fast) 44.1 kHz ComParE 16 kHz COVID-19 Positive Healthy Total COVID-19 Positive Healthy Total COVID-19 Positive COVID-19 Negative Total COVID-19 Positive Healthy Total COVID-19 Positive Healthy Total COVID-19 Positive Healthy Total COVID-19 Positive Healthy Total 92 1079 1171 119 398 517 18 26 44 88 1062 1150 88 1077 1165 85 1074 1159 214 396 610 4.24 min 0.98 h 1.05 h 13.43 min 40.89 min 54.32 min 0.87 min 1.57 min 2.45 min 8.58 min 2.77 h 2.92 h 12.42 min 2.99 h 3.19 h 7.62 min 1.91 h 2.03 h 44.02 min 1.46 h 2.19 h 2.77 s 3.26 s 3.22 s 6.77 s 6.16 s 6.31 s 2.91 s 3.63 s 3.34 s 5.85 s 9.39 s 9.126 s 8.47 s 9.99 s 9.88 s 5.38 s 6.39 s 6.31 s 12.34 s 13.25 s 12.93 s 1.62 s 1.66 s 1.67 s 2.11 s 2.26 s 2.24 s 2.23 s 2.75 s 2.53 s 5.05 s 5.23 s 5.29 s 4.27 s 3.09 s 3.22 s 2.76 s 1.77 s 1.88 s 5.35 s 4.67 s 4.93 s ComputersinBiologyandMedicine141(2022)1051534M. Pahar et al.                                                                                                                 Fig. 3. Feature extraction process for a breath audio. The frame overlap Œ¥ is calculated to ensure that the entire recording is divided into ùíÆ segments. For ‚Ñ≥MFCCs, for example, this results in a feature matrix with dimensions (3‚Ñ≥ + 2, ùíÆ). Table 3 Primary feature (PF) extraction hyperparameters. We have used between 13 and 65 MFCCs and between 40 and 200 linearly spaced filters to extract log energies. Hyperparameters Description Range MFCCs (‚Ñ≥) lower order MFCCs to keep Linearly spaced filters used to extract log energies 13 √ó k, where k = 1, 2, 3, 4, 5 40 to 200 in steps of 20 (‚Ñ¨) Frame length (‚Ñ± ) Segments (ùíÆ) into which audio is segmented number of frames extracted from audio 2k where k = 9, 10, 11, 12 10 √ó k, where k = 7, 10, 12, 15, 20 We will refer to the features described in this section as primary features (PF) to distinguish them from the bottleneck features (BNF) described in Section 5. 4. Transfer learning architecture Since the audio datasets with COVID-19 labels described in Section 2.2 are small, they may lead to overfitting when training deep archi-tectures. Nevertheless, in previous work we have found that deep ar-chitectures perform better than shallow classifiers when using these as training sets [20]. In this work, we consider whether the classification performance of such DNNs can be improved by applying transfer learning. To achieve this, we use the datasets described in Section 2.1 con-taining 10.29 h of audio, labelled with four classes: cough, sneeze, speech and noise, but that do not include COVID-19 labels (Table 1 in Section 2.1). This data is used to pre-train three deep neural Table 4 Hyperparameters of the pre-trained networks: Feature extraction hyper-parameters were adopted from the optimal values in previous related work [20], while classifier hyperparameters were optimised on the pre-training data using cross-validation. FEATURE EXTRACTION HYPERPARAMETERS Hyperparameters ‚Ñ≥‚Ñ±ùíÆMFCCs Frame length Segments Values 39 210 = 1024 150 CLASSIFIER HYPERPARAMETERS Hyperparameters Classifier Values Convolutional filters Kernel size Dropout rate Dense layer (for pre-training) Dense layer (for fine-tuning) LSTM units Learning rate Batch Size Epochs CNN CNN CNN, LSTM CNN, LSTM, Resnet50 CNN, LSTM, Resnet50 LSTM LSTM CNN, LSTM, Resnet50 CNN, LSTM, Resnet50 256 & 128 & 64 2 0.2 512 & 64 & 4 32 & 2 512 & 256 & 128 (cid:0) 3 = 0.001 1027 = 128 70 architectures: a CNN, an LSTM and a Resnet50. The feature extraction hyperparameters: ‚Ñ≥ = 39, ‚Ñ± = 210 and ùíÆ = 150 delivered good per-formance in our previous work [20] and thus have also been used here (Table 4). The CNN consists of three convolutional layers, with 256, 128 and 64 (2 √ó 2) kernels respectively, each followed by (2,2) max-pooling. The LSTM consists of three layers with 512, 256 and 128 LSTM units respectively, each including dropout with a rate of 0.2. A standard Resnet50, as described in Table 1 of [39], has been implemented with 512-dimensional dense layers. During pre-training, all three networks (CNN, LSTM and Resnet50) are terminated by three dense layers with dimensionalities 512, 64 and finally 4 to correspond to the four classes mentioned in Table 1. Relu activation functions were used throughout, except in the four- dimensional output layer which was softmax. All the above architec-tural hyperparameters were chosen by optimising the four-class classi-fiers during cross-validation (Table 4). After pre-training on the datasets described in Section 2.1, the 64 and 4-dimensional dense layers terminating the network were discarded from the CNN, the LSTM and the Resnet50. This left three trained deep neural networks, each accepting the same input dimensions and each with a 512-dimensional relu output layer. The parameters of these three pre-trained networks were then fixed for the remaining experiments. In order to obtain COVID-19 classifiers by transfer learning, two dense layers are added after the 512-dimensional output layer of each of the three pre-trained deep networks. The final layer is a two- dimensional softmax, to indicate COVID-19 positive and negative clas-ses respectively. The dimensionality of the penultimate layer was also considered to be a hyperparameter and was optimised during nested k- fold cross-validation. Its optimal value was found to be 32 for all three architectures. The transfer learning process for a CNN architecture is illustrated in Fig. 4. 5. Bottleneck features The 512-dimensional output of the three pre-trained networks described in the previous section has a much lower dimensionality than the (3‚Ñ≥ + 2, ùíÆ) i.e. (3 √ó 39 + 2) √ó 150 = 17 850 dimensional input matrix consisting of primary features (Table 4). Therefore, the output of this layer can be viewed as a bottleneck feature vector [40‚Äì42]. In addition to fine-tuning, where we add terminating dense layers to the three pre-trained networks and optimise these for the binary COVID-19 detection task as shown in Fig. 4, we have trained logistic regression (LR), support vector machine (SVM), k-nearest neighbour (KNN) and multilayer perceptron (MLP) classifiers using these bottleneck features as inputs. Bottleneck features computed by the CNN, the LSTM or the Resnet50 were chosen based on the one which performed better in the corresponding transfer learning experiments. Since the Resnet50 ach-ieved higher development set AUCs than the CNN and the LSTM during transfer learning, it was used to extract bottleneck features on which the LR, SVM, KNN and MLP classifiers were trained. ComputersinBiologyandMedicine141(2022)1051535M. Pahar et al.                                                                                                                 Fig. 4. CNN Transfer Learning Architecture. Cross-validation on the pre-training data determined the optimal CNN architecture to have three convolutional layers with 256, 128 and 64 (2 √ó 2) kernels respectively, each followed by (2,2) max-pooling. The convolutional layers were followed by two dense layers with 512 and 64 relu units each, and the network was terminated by a 4-dimensional softmax. To apply transfer learning, the final two layers were removed and replaced with a new dense layer and a terminating 2-dimensional softmax to account for COVID-19 positive and negative classes. Only this newly added portion of the network was trained for classification on the data with COVID-19 labels. In addition, the outputs of the third-last layer (512-dimensional dense relu) from the pre-trained network were used as bottleneck features. 6. Experimental method 6.1. Hyperparameter optimisation We have evaluated the effectiveness of transfer learning (Section 4) and bottleneck feature extraction (Section 5) using CNN, LSTM and Resnet50 architectures in improving the performance of COVID-19 classification based on cough, breath and speech audio signals. In order to place these results in context, we provide two baselines. 1. As a first baseline, we train the three deep architectures (CNN, LSTM and Resnet50) directly on the primary features extracted from data containing COVID-19 labels (as described in Section 2.2) and hence skip the pre-training. Some of these baseline results were developed in our previous work [20]. 2. As a second baseline, we train shallow classifiers (LR, SVM, KNN and MLP) on the primary input features (as described in Section 3), also extracted from the data containing COVID-19 labels (described in Section 2.2). Hyperparameters for three pre-trained networks have already been described in Section 4 and are listed in Table 4. The remaining hyper-parameters are those of the baseline deep classifiers (CNN, LSTM and Resnet50 without pre-training), the four shallow classifiers (LR, SVM, KNN and MLP), and the dimensionality of the penultimate layer for the deep architectures during transfer learning. With the exception of Resnet50, all these hyperparameters optimi-sation and performance evaluation has been performed within the inner loops of nested k-fold cross-validation scheme [43]. Due to the excessive computational requirements of optimising Resnet50 metaparameters within the same cross-validation framework, we have used the standard 50 skip layers in all experiments [39]. Classifier hyperparameters and the values considered during optimisation are listed in Table 5. A five-fold split, similar to that employed in Ref. [20], was used for the nested cross-validation. The performance of these baseline systems will be compared against: 6.2. Classifier evaluation 1. Deep architectures (CNN, LSTM and Resnet50) trained by the transfer learning process. The respective deep architectures are pre- trained (as described in Section 4), after which the final two layers are fine-tuned on the data containing COVID-19 labels (as described in Section 2.2). 2. Shallow architectures (LR, SVM, KNN and MLP) trained on the bottleneck features extracted from the pre-trained networks. Table 5 Classifier hyperparameters, optimised using leave-p-out nested cross- validation. Hyperparameters Classifier Range Regularisation Strength (Œ±1) l1 penalty (Œ±2) l2 penalty (Œ±3) Kernel Coefficient (Œ±4) No. of neighbours (Œ±5) Leaf size (Œ±6) No. of neurons (Œ±7) No. of convolutional filters (Œ≤1) Kernel size (Œ≤2) Dropout rate (Œ≤3) Dense layer size (Œ≤4) LSTM units (Œ≤5) Learning rate (Œ≤6) Batch Size (Œ≤7) Epochs (Œ≤8) LR, SVM LR LR, MLP SVM KNN KNN MLP CNN CNN CNN, LSTM CNN, LSTM LSTM LSTM, MLP CNN, LSTM CNN, LSTM 10i where, i = (cid:0) 7, (cid:0) 6, ‚Ä¶, 6, 7 0 to 1 in steps of 0.05 0 to 1 in steps of 0.05 10i where, i = (cid:0) 7, (cid:0) 6, ‚Ä¶, 6, 7 10 to 100 in steps of 10 5 to 30 in steps of 5 10 to 100 in steps of 10 3 √ó 2k where k = 3, 4, 5 2 and 3 0.1 to 0.5 in steps of 0.2 2k where k = 4, 5 2k where k = 6, 7, 8 10k where, k = (cid:0) 2, (cid:0) 3, (cid:0) 4 2k where k = 6, 7, 8 10 to 250 in steps of 20 Receiver operating characteristic (ROC) curves were calculated within both the inner and outer loops of the nested cross-validation scheme described in the previous section. The inner-loop ROC values were used for the hyperparameter optimisation, while the average of the outer-loop ROC values indicates final classifier performance on the in-dependent held-out test sets. The AUC score indicates how well the classifier performs over a range of decision thresholds [44]. The threshold that achieves an equal error rate (Œ≥EE) was computed from these curves. We note the mean per-frame probability that an event such as a cough is from a COVID-19 positive subject by ÃÇP: ÃÇP =‚àëKi=1 P(Y = 1|Xi, Œ∏)ùíÆ(1) where ùíÆ indicates the number of frames in an event and P(Y = 1|Xi, Œ∏) is the output of the classifier for feature vector Xi and parameters Œ∏ for the ith frame. Now we define the indicator variable C as: {10C =if ÃÇP ‚â• Œ≥EEotherwise(2) We then define two COVID-19 index scores CI1 and CI2 in Equations (3) and (4) respectively, with N1 the number of events from the subject in the recording and N2 the total number of frames of the events gath-ered from the subject. Here, N2 = ùíÆ √ó N1. ComputersinBiologyandMedicine141(2022)1051536M. Pahar et al.                                                                                                                 Table 6 COVID-19 cough classification performance. For the Coswara, Sarcos and ComParE datasets the highest AUCs of 0.982, 0.961 and 0.944 respectively were achieved by a Resnet50 trained by transfer learning in the first two cases and a KNN classifier using 12 primary features determined by sequential forward selection (SFS) in the third. When Sarcos is used exclusively as a validation set for a classifier trained on the Coswara data, an AUC of 0.954 is achieved. Dataset ID Classifier Best Feature Hyperparameters Best Classifier Hyperparameters (Optimised inside nested cross-validation) Performance Spec Sens Acc AUC œÉAUC Default Resnet50 (Table 1 in Ref. [39]) Table 4 ‚Äù Œ±3 = 0.35, Œ±7 = 50 Œ±1 = 104, Œ±4 = 101 Œ±5 = 20, Œ±6 = 15 (cid:0) 1, Œ±2 = 0.5, Œ±3 = 0.5 Œ±1 = 10Default Resnet50 (Table 1 in Ref. [39]) Table 4 in [20] ‚Äù Default Resnet50 (Table 1 in Ref. [39]) Table 4 ‚Äù Œ±3 = 0.75, Œ±7 = 70 (cid:0) 2, Œ±4 = 104 Œ±1 = 10Œ±5 = 40, Œ±6 = 20 (cid:0) 3, Œ±2 = 0.4, Œ±3 = 0.6 Œ±1 = 10Default Resnet50 (Table 1 in Ref. [39]) Table 5 in [20] ‚Äù Default Resnet50 (Table 1 in Ref. [39]) Table 4 ‚Äù Œ±3 = 0.25, Œ±7 = 20 (cid:0) 3, Œ±4 = 102 Œ±1 = 10Œ±5 = 70, Œ±6 = 20 Œ±1 = 104, Œ±2 = 0.3, Œ±3 = 0.7 Œ±5 = 60, Œ±6 = 25 Œ±5 = 60, Œ±6 = 25 Œ±3 = 0.65, Œ±7 = 40 97% 92% 93% 92% 89% 88% 84% 98% 99% 97% 92% 92% 89% 88% 88% 85% 83% 92% 73% 96% 89% 88% 86% 85% 85% 85% 84% 84% 98% 98% 95% 96% 93% 90% 86% 93% 90% 91% 96% 92% 91% 90% 89% 87% 86% 96% 75% 91% 93% 92% 90% 90% 90% 86% 86% 90% 97% 95% 94% 94% 91% 89% 85% 95% 95% 94% 94% 92% 90% 89% 89% 86% 85% 94% 74% 93% 91% 90% 88% 88% 88% 86% 85% 92% 0.982 0.972 0.964 0.963 0.942 0.917 0.898 0.976 0.953 0.942 0.961 0.943 0.917 0.913 0.904 0.883 0.867 0.954 0.779 0.938 0.934 0.916 0.898 0.912 0.903 0.882 0.863 0.944 78% 80% 80% 0.855 76% 80% 78% 0.839 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 2 √ó 103 √ó 103 √ó 104 √ó 103 √ó 107 √ó 108 √ó 1018 √ó 10(cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 39 √ó 1043 √ó 103 √ó 103 √ó 104 √ó 107 √ó 106 √ó 108 √ó 109 √ó 10‚Äì ‚Äì ‚Äì (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 4 √ó 104 √ó 104 √ó 105 √ó 106 √ó 108 √ó 108 √ó 109 √ó 1013 √ó 1014 √ó 10(cid:0) 3 (cid:0) 3 (cid:0) 3 Œ±1 = 10Œ±1 = 10(cid:0) 1 (cid:0) 4, Œ±4 = 10(cid:0) 2, Œ±2 = 0.6, Œ±3 = 0.4 75% 78% 77% 0.814 69% 73% 71% 0.789 12 √ó 1013 √ó 10(cid:0) 3 (cid:0) 3 Table 4 ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù Table 4 in [20] ‚Äù ‚Äù Table 4 ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù Table 5 in [20] ‚Äù Table 4 ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù ‚Ñ¨ = 60, ‚Ñ± = 211, ùíÆ = 70 ‚Ñ¨ = 60, ‚Ñ± = 211, ùíÆ = 70 ‚Ñ≥ = 13, ‚Ñ± = 210, ùíÆ =100 ‚Ñ¨ = 80, ‚Ñ± = 29, ùíÆ = 70 ‚Ñ¨ = 140, ‚Ñ± = 211, ùíÆ =70 Coswara Sarcos Sarcos (val only) ComParE C1 C2 C3 C4 C5 C6 C7 C8 Resnet50 + TL CNN + TL LSTM + TL MLP + BNF SVM + BNF KNN + BNF LR + BNF Resnet50 + PF [20] CNN + PF [20] LSTM + PF [20] Resnet50 + TL LSTM + TL CNN + TL C9 C10 C11 C12 C13 C14 MLP + BNF SVM + BNF C15 KNN + BNF C16 LR + BNF C17 Resnet50 + TL C18 LSTM + PF [20] C19 LSTM + PF + SFS C20 [20] Resnet50 + TL LSTM + TL CNN + TL C21 C22 C23 C24 MLP + BNF SVM + BNF C25 KNN + BNF C26 LR + BNF C27 KNN + PF + SFS C28 KNN + PF C29 C30 MLP + PF C31 C32 SVM + PF LR + PF CI1 =‚àëN1i=1 CN1CI2 =‚àëN2i=1 P(Y = 1|Xi)N2Hence Equation (1) computes a per-event average probability while Equation (4) computes a per-frame average probability. The use of one of Equations (3) and (4) was considered an additional hyperparameter during cross-validation, and it was found that taking the maximum value of the index scores consistently led to the best performance. The average specificity, sensitivity and accuracy, as well as the AUC together with its standard deviation (œÉAUC) are shown in Tables 6‚Äì8 for cough, breath and speech events respectively. These values have all been calculated over the outer folds during nested cross-validation. Hyper-parameters producing the highest AUC over the inner loops have been noted as the ‚Äòbest classifier hyperparameter‚Äô. 7. Experimental results COVID-19 classification performance based on cough, breath and speech is presented in Tables 6‚Äì8 respectively. These tables include the (3) (4) performance of baseline deep classifiers without pre-training, deep classifiers trained by transfer learning (TL), shallow classifiers using bottleneck features (BNF) and baseline shallow classifiers trained directly on the primary features (PF). The best performing classifiers appear first for each dataset and the baseline results are shown towards the end. Each system is identified by an ‚ÄòID‚Äô. 7.1. Coughs We have found in our previous work [20] that, when training a Resnet50 on only the Coswara dataset, an AUC of 0.976 (œÉAUC = 0.018) can be achieved for the binary classification problem of distinguishing COVID-19 coughs from healthy coughs. These results are reproduced as baseline systems C8, C9 and C10 in Table 6. The improved results ach-ieved by transfer learning are indicated by systems C1 to C7 in the same table. Specifically, system C1 shows that, by applying transfer learning as described in Section 4, the same Resnet50 architecture can achieve an AUC of 0.982 (œÉAUC = 0.002). The entries for systems C2 and C3 show that pre-training also improves the AUCs achieved by the deep CNN and LSTM classifiers from 0.953 (system C9) to 0.972 (system C2) and from 0.942 (system C10) to 0.964 (system C3) respectively. Of particular note ComputersinBiologyandMedicine141(2022)1051537M. Pahar et al.                                                                                                                 Table 7 COVID-19 breath classifier performance: For breaths, the best performance was achieved by an SVM using bottleneck features (AUC = 0.942). The Resnet50 classifier trained by transfer learning achieves a similar AUC of 0.934. Dataset ID Classifier Best Feature Hyperparameters Best Classifier Hyperparameters (Optimised inside nested cross-validation) Performance Spec Sens Acc AUC Coswara B1 B2 B3 B4 B5 B6 B7 B8 B9 Resnet50 +TL LSTM + TL CNN + TL SVM + BNF MLP + BNF KNN + BNF LR + BNF Resnet50 +PF LSTM + PF Table 4 Default Resnet50 (Table 1 in Ref. [39]) 87% 93% 90% 0.934 ‚Äù Table 4 ‚Äù ‚Äù Œ±1 = 102, Œ±4 = 10‚Äù Œ±3 = 0.45, Œ±7 = 50 ‚Äù Œ±5 = 70, Œ±6 = 10 ‚Äù Œ±1 = 10‚Äù ‚Ñ≥ = 39,‚Ñ± = 210,ùíÆ = 150  Default Resnet50 (Table 1 in Ref. [39]) (cid:0) 4, Œ±2 = 0.8, Œ±3 = 0.2 (cid:0) 2 86% 85% 88% 87% 87% 86% 92% 90% 89% 94% 93% 93% 90% 90% 88% 87% 91% 90% 90% 88% 91% 0.927 0.914 0.942 0.923 0.922 0.891 0.923 ‚Ñ≥ = 26,‚Ñ± = 211,ùíÆ = 120 Œ≤3 = 0.1, Œ≤4 = 32, Œ≤5 = 128, Œ≤6 = 0.001, Œ≤7 = 256, Œ≤8 = 170 90% 86% 88% 0.917 B10 CNN + PF ‚Ñ≥ = 52,‚Ñ± = 210,ùíÆ = 100 Œ≤1 = 48, Œ≤2 = 2, Œ≤3 = 0.3, Œ≤4 = 32, Œ≤7 = 256, Œ≤8 = 210 87% 85% 86% 0.898 œÉAUC 3 √ó 10(cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 3 √ó 103 √ó 104 √ó 106 √ó 109 √ó 108 √ó 1034 √ó(cid:0) 3 1041 √ó(cid:0) 3 1042 √ó(cid:0) 3 10Table 8 COVID-19 speech classifier performance: For the Coswara (fast and normal speech) and the ComParE speech the highest AUCs were 0.893, 0.861 and 0.923 respectively and achieved by a Resnet50 trained by transfer learning in the first two cases and an SVM using with bottleneck features in the third case. Dataset ID Classifier Best Feature Hyperparameters Best Classifier Hyperparameters (Optimised inside nested cross-validation) Performance Spec Sens Acc AUC 90% 88% 88% 83% 83% 80% 79% 84% 85% 82% 81% 85% 85% 85% 83% 80% 87% 85% 85% 84% 84% 83% 81% 82% 0.893 0.877 0.875 0.871 0.867 0.868 0.852 0.864 Coswara normal speech S1 S2 S3 S4 S5 S6 S7 S8 S9 Resnet50 + TL LSTM + TL CNN + TL MLP + BNF SVM + BNF KNN + BNF LR + BNF Resnet50 + PF ‚Ñ≥ = 26,‚Ñ± = 210,ùíÆ = 120  Default Resnet50 (Table 1 in Ref. [39]) Default Resnet50 (Table 1 in Ref. [39]) Table 4 ‚Äù Œ±3 = 0.25, Œ±7 = 60 (cid:0) 6, Œ±4 = 105 Œ±1 = 10Œ±5 = 50, Œ±6 = 10 Œ±1 = 102, Œ±2 = 0.6, Œ±3 = 0.4 Table 4 ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù LSTM + PF ‚Ñ≥ = 26,‚Ñ± = 211,ùíÆ = 150 Œ≤3 = 0.1, Œ≤4 = 32, Œ≤5 = 128, Œ≤6 = 0.001, Œ≤7 = 256, Œ≤8 = 170 84% 78% 81% 0.844 S10 CNN + PF ‚Ñ≥ = 39,‚Ñ± = 210,ùíÆ = 120 Œ≤1 = 48, Œ≤2 = 2, Œ≤3 = 0.3, Œ≤4 = 32, Œ≤7 = 256, Œ≤8 = 210 82% 78% 80% 0.832 Coswara fast speech Resnet50 + TL LSTM + TL CNN + TL S11 S12 S13 S14 MLP + BNF SVM + BNF S15 KNN + BNF S16 LR + BNF S17 Table 4 ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù Default Resnet50 (Table 1 in Ref. [39]) Table 4 ‚Äù Œ±3 = 0.55, Œ±7 = 70 (cid:0) 2 Œ±1 = 104, Œ±4 = 10Œ±5 = 60, Œ±6 = 15 Œ±1 = 10(cid:0) 3, Œ±2 = 0.4, Œ±3 = 0.6 84% 83% 82% 78% 78% 77% 77% 78% 78% 76% 83% 83% 83% 82% 81% 81% 79% 81% 81% 81% 80% 0.861 0.860 0.851 0.858 0.856 0.854 0.841 S18 LSTM + PF ‚Ñ≥ = 26,‚Ñ± = 211,ùíÆ = 120 Œ≤3 = 0.1, Œ≤4 = 32, Œ≤5 = 128, Œ≤6 = 0.001, Œ≤7 = 256, Œ≤8 = 170 84% 80% 82% 0.856 S19 Resnet50 + PF ‚Ñ≥ = 39,‚Ñ± = 210,ùíÆ = 150  Default Resnet50 (Table 1 in Ref. [39]) 82% 78% 80% 0.822 S20 CNN + PF ‚Ñ≥ = 52,‚Ñ± = 210,ùíÆ = 100 Œ≤1 = 48, Œ≤2 = 2, Œ≤3 = 0.3, Œ≤4 = 32, Œ≤7 = 256, Œ≤8 = 210 79% 77% 78% 0.810 ComParE Resnet50 + TL S21 LSTM + TL S22 CNN + TL S23 SVM + BNF S24 S25 MLP + BNF KNN + BNF S26 LR + BNF S27 S28 MLP + PF +SFS S29 MLP + PF Table 4 ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù ‚Äù ‚Ñ≥ = 26,‚Ñ± = 211,ùíÆ = 150 (cid:0) 1, Œ±4 = 103 Default Resnet50 (Table 1 in Ref. [39]) Table 4 ‚Äù Œ±1 = 10Œ±3 = 0.3, Œ±7 = 60 Œ±5 = 20, Œ±6 = 15 Œ±1 = 102, Œ±2 = 0.45, Œ±3 = 0.7 Œ±3 = 0.35, Œ±7 = 70 ‚Ñ≥ = 26,‚Ñ± = 211,ùíÆ = 150 Œ±3 = 0.35, Œ±7 = 70 S30 KNN + PF ‚Ñ¨ = 100,‚Ñ± = 210,ùíÆ = 120 Œ±5 = 70, Œ±6 = 15 S31 SVM + PF ‚Ñ¨ = 80, ‚Ñ± = 211, ùíÆ = 120 Œ±1 = 10(cid:0) 2, Œ±4 = 10(cid:0) 3 S32 LR + PF ‚Ñ¨ = 60, ‚Ñ± = 210, ùíÆ = 100 Œ±1 = 104, Œ±2 = 0.35, Œ±3 = 0.65 84% 82% 80% 84% 80% 80% 81% 82% 90% 88% 88% 88% 88% 86% 85% 88% 87% 85% 84% 86% 84% 83% 83% 85% 0.914 0.897 0.892 0.923 0.905 0.891 0.890 0.912 81% 85% 83% 0.893 80% 84% 82% 0.847 79% 81% 80% 0.836 69% 72% 71% 0.776 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 (cid:0) 3 œÉAUC 3 √ó 104 √ó 104 √ó 108 √ó 107 √ó 106 √ó 107 √ó 1051 √ó(cid:0) 3 1051 √ó(cid:0) 3 1052 √ó(cid:0) 3 102 √ó 103 √ó 103 √ó 107 √ó 108 √ó 108 √ó 1011 √ó(cid:0) 3 1047 √ó(cid:0) 3 1045 √ó(cid:0) 3 1041 √ó(cid:0) 3 104 √ó 105 √ó 105 √ó 104 √ó 106 √ó 107 √ó 107 √ó 1011 √ó(cid:0) 3 1014 √ó(cid:0) 3 1016 √ó(cid:0) 3 1015 √ó(cid:0) 3 1018 √ó(cid:0) 3 10ComputersinBiologyandMedicine141(2022)1051538M. Pahar et al.                                                                                                                 Fig. 5. COVID-19 cough classification: A Resnet50 classifier with transfer learning achieved the highest AUC in classifying COVID-19 coughs for the Coswara and Sarcos datasets (0.982 and 0.961 respectively). For the ComParE dataset, AUCs of 0.944 and 0.934 were achieved by a KNN classifier using 12 features identified by SFS and by a Resnet50 classifier trained by transfer learning respectively. in all these cases is the substantial decrease in the standard deviation of the AUC (œÉAUC) observed during cross-validation when implementing transfer learning. This indicates that pre-training leads to classifiers with more consistent performance on the unseen test data. The Sarcos dataset is much smaller than the Coswara dataset and too small to train a deep classifier directly. For this reason, it was used only as an independent validation dataset for classifiers trained on the Cos-wara data in our previous work [20]. It can however be used to fine-tune pre-trained classifiers during transfer learning, and the resulting per-formance is reflected by systems C11 to C17 in Table 6. Previously an AUC of 0.938 (system C20) was achieved when using Sarcos as an in-dependent validation data and applying sequential forward selection (SFS) [20]. Here, we find that transfer learning applied to the Resnet50 model results in an AUC of 0.961 (system C11) and a lower standard deviation (œÉAUC = 0.003). As an additional experiment, we apply the Resnet50 classifier trained by transfer learning using the Coswara data to the Sarcos data, thus again using the latter exclusively as an inde-pendent validation set. The resulting performance is indicated by system C18, while the previous baselines are repeated as systems C19 and C20 [20]. System C18 achieves an AUC of 0.954, which is only slightly below the 0.961, achieved by system C11 where the pre-trained model used the Sarcos data for fine-tuning, and slightly higher than the AUC of 0.938 achieved by system C20 which is the baseline LSTM trained on Coswara without transfer learning but employing SFS [45]. This supports our earlier observation that transfer learning appears to lead to more robust classifiers that can generalise to other datasets. Due to the extreme computational load, we have not yet been able to evaluate SFS within the transfer learning framework. For the ComParE dataset, we have included shallow classifiers trained directly on the primary input features (KNN + PF, MLP + PF, SVM + PF and LR + PF). These are the baseline systems C29 to C32 in Table 6. The best-performing shallow classifier is C29, where a KNN used 60 linearly spaced filterbank log energies as features. System C28 is the result of applying SFS to system C29. In this case, SFS identifies the top 12 features based on the development sets used during nested cross- validation, and results in the best-performing shallow system with an Fig. 6. COVID-19 breath classification: An SVM classifier using bottleneck features (BNF) achieved the highest AUC of 0.942 when classifying COVID-19 breath. The Resnet50 with and without the transfer learning has achieved AUCs of 0.934 and 0.923 respectively, with higher œÉAUC for the latter (Table 7). ComputersinBiologyandMedicine141(2022)1051539M. Pahar et al.                                                                                                                 Fig. 7. COVID-19 speech classification: An SVM classifier using bottleneck features (BNF) achieved the highest AUC of 0.923 when classifying COVID-19 speech in ComParE dataset. A Resnet50 trained by transfer learning achieves a slightly lower AUC of 0.914. Speech (normal and fast) in the Coswara dataset can be used to classify COVID-19 with AUCs of 0.893 and 0.861 respectively using a Resnet50 trained by transfer learning. AUC of 0.944. This represents a substantial improvement over the AUC of 0.855 achieved by the same system without SFS (system C29). Sys-tems C21 to C27 in Table 6 are obtained by transfer learning using the ComParE dataset. These show improved performance over the shallow classifiers without SFS. In particular, after transfer learning, the Resnet50 achieves almost the same AUC as the best ComParE system (system C28) with a lower œÉAUC. When considering the performance of the shallow classifiers trained on the bottleneck features across all three datasets in Table 6, we see that a consistent improvement over the use of primary features with the same classifiers is observed. The ROC curves for the best-performing COVID- 19 cough classifiers are shown in Fig. 5. 7.2. Breath Table 7 demonstrates that COVID-19 classification is also possible on the basis of breath signals. The baseline systems B8, B9 and B10 are trained directly on the primary features, without pre-training. By comparing these baselines with B1, B2 and B3, we see that transfer learning leads to a small improvement in AUC for all three deep archi-tectures. Furthermore, systems B4 to B7 show that comparable perfor-mance can be achieved by shallow classifiers using the bottleneck features. The best overall performance (AUC = 0.942) was achieved by an SVM classifier trained on the bottleneck features (system B4). How-ever, the Resnet50 trained by transfer learning (system B1) performed almost equally well (AUC = 0.934). The ROC curves for the best- performing COVID-19 breath classifiers are shown in Fig. 6. As it was observed for coughs, the standard deviation of the AUC (œÉAUC) is consistently lower for the pre-trained networks. 7.3. Speech Although not as informative as cough or breath audio, COVID-19 classification can also be achieved on the basis of speech audio re-cordings. For Coswara, the best classification performance (AUC =0.893) was achieved by a Resnet50 after applying transfer learning (system S1). For the ComParE data, the top performer (AUC = 0.923) was an SVM trained on the bottleneck features (system S24). However, the Resnet50 trained by transfer learning performed almost equally well, with an AUC of 0.914 (system S21). Furthermore, while good performance was also achieved when using the deep architectures without applying the transfer learning process (systems S8‚ÄìS10, S18‚ÄìS20 and S28‚ÄìS32), this again was at the cost of a substantially higher standard deviation œÉAUC. Finally, for the Coswara data, perfor-mance was generally better when speech was uttered at a normal pace rather than a fast pace. The ROC curves for the best-performing COVID- 19 speech classifiers are shown in Fig. 7. 8. Discussion Previous studies have shown that it is possible to distinguish between the coughing sounds made by COVID-19 positive and COVID-19 nega-tive subjects by means of automatic classification and machine learning. However, the fairly small size of datasets with COVID-19 labels limits the effectiveness of these techniques. The results of the experiments we have presented in this study show that larger datasets of other vocal and respiratory audio that do not include COVID-19 labels can be leveraged to improve classification performance by applying transfer learning [46]. Specifically, we have shown that the accuracy of COVID-19 clas-sification based on coughs can be improved by transfer learning for two datasets (Coswara and Sarcos) while almost optimal performance is achieved on a third dataset (ComParE). A similar trend is seen when performing COVID-19 classification based on breath and speech audio. However, these two types of audio appear to contain less distinguishing information, since the achieved classification performance is a little lower than it is for cough. Our best cough classification system has an area under the ROC curve (AUC) of 0.982, despite being trained on what remains a fairly small COVID-19 dataset with 1171 participants (92 COVID-19 positive and 1079 negative). Other research reports a similar AUC but using a much larger dataset with 8380 participants (2339 positive and 6041 negative) [47]. While our experiments also show that shallow classifiers, when used in conjunction with feature selection, can in some cases match or surpass the performance of the deeper archi-tectures; a pre-trained Resnet50 architecture provides consistent optimal or near-optimal performance across all three types of audio signals and datasets. Due to the very high computational cost involved, ComputersinBiologyandMedicine141(2022)10515310M. Pahar et al.                                                                                                                 we have not yet applied such feature selection to the deep architectures themselves, and this remains part of our ongoing work. Another important observation that we can make for all three types of audio signals is that transfer learning strongly reduces the variance in the AUC (œÉAUC) exhibited by the deep classifiers during cross-validation (Tables 6‚Äì8). This suggests that transfer learning leads to more consis-tent classifiers that are less prone to over-fitting and better able to generalise to unseen test data. This is important because robustness to variable testing conditions is essential in implementing COVID-19 classification as a method of screening. An informal listening assessment of the Coswara and the ComParE data indicates that the former has greater variance and more noise than the latter. Our experimental results presented in Tables 6‚Äì8 found that, for speech classification on noisy data, fine-tuning a pre-trained net-works demonstrates better performance, while for cleaner data, extracting bottleneck features and then applying a shallow classifier exhibits better performance. It is interesting to note that MFCCs are always the features of choice for this noisier dataset, while the log en-ergies of linear filters are often preferred for the less noisy data. Although all other classifiers have shown the best performance when using these log-filterbank energy features, MLP classifiers performed best when using MFCCs and were best at classifying COVID-19 speech. A similar conclusion was drawn in Ref. [24], where coughs were recorded in a controlled environment with little environmental noise. A larger number of frames in the feature matrix also generally leads to better performance as it allows the classifier to find more detailed temporal patterns in the audio signal. Finally, we note that, for the shallow classifiers, hyperparameter optimisation selected a higher number of MFCCs and also a more densely populated filterbank than what is required to match the reso-lution of the human auditory system. This agrees with an observation already made in our previous work that the information used by the classifiers to detect COVID-19 signature is at least to some extent not perceivable by the human ear. 9. Conclusions In this study, we have demonstrated that transfer learning can be used to improve the performance and robustness of the DNN classifiers for COVID-19 detection in vocal audio such as cough, breath and speech. We have used a 10.29 h audio data corpus, which does not have any COVID-19 labels, to pre-train a CNN, an LSTM and a Resnet50. This data contains four classes: cough, sneeze, speech and noise. In addition, we have used the same architectures to extract bottleneck features by removing the final layers from the pre-trained models. Three smaller datasets containing cough, breath and speech audio with COVID-19 la-bels were then used to fine-tune the pre-trained COVID-19 audio clas-sifiers using nested k-fold cross-validation. Our results show that a pre-trained Resnet50 classifier that is either fine-tuned or used as a bottleneck extractor delivers optimal or near- optimal performance across all datasets and all three audio classes. The results show that transfer learning using the larger dataset without COVID-19 labels led not only to improved performance, but also to a much smaller standard deviation of the classifier AUC, indicating better generalisation to unseen test data. The use of bottleneck features, which are extracted by the pre-trained deep models and therefore also a way of incorporating out-of-domain data, also provided a reduction in this standard deviation and near-optimal performance. Furthermore, we see that cough audio carries the strongest COVID-19 signatures, followed by breath and speech. The best-performing COVID-19 classifier achieved an area under the ROC curve (AUC) of 0.982 for cough, followed by an AUC of 0.942 for breath and 0.923 for speech. We conclude that successful classification is possible for all three classes of audio considered. However, deep transfer learning improves COVID-19 detection on the basis of cough, breath and speech signals, yielding automatic classifiers with higher accuracies and greater robustness. This is significant since such COVID-19 screening is inex-pensive, easily deployable, non-contact and does not require medical expertise or laboratory facilities. Therefore it has the potential to decrease the load on the health care systems. As a part of ongoing work, we are considering the application of feature selection in the deep architectures, the fusion of classifiers using various audio classes like cough, breath and speech, as well as the optimisation and adaptation necessary to allow deployment on a smartphone or similar mobile platform. Declaration of competing interest We confirm that there is no conflict of interest statement to be declared. Acknowledgements This research was supported by the South African Medical Research Council (SAMRC) through its Division of Research Capacity Develop-ment under the Research Capacity Development Initiative as well as the COVID-19 IMU EMC allocation from funding received from the South African National Treasury. Support was also received from the European Union through the EDCTP2 programme (TMA2017CDF-1885). The content and findings reported are the sole deduction, view and re-sponsibility of the researcher and do not reflect the official position and sentiments of the SAMRC or the EDCTP. We would also like to thank South African Centre for High Perfor-mance Computing (CHPC) for providing computational resources on their Lengau cluster to support this research, and gratefully acknowl-edge the support of Telkom South Africa. We also especially thank Igor Miranda, Corwynne Leng, Renier Botha, Jordan Govendar and Rafeeq du Toit for their support in data collection and annotation. References [1] A. Carf√¨, R. Bernabei, F. Landi, et al., Persistent symptoms in patients after acute COVID-19, JAMA 324 (6) (2020) 603‚Äì605. [2] D. Wang, B. Hu, C. Hu, F. Zhu, X. Liu, J. Zhang, B. Wang, H. Xiang, Z. Cheng, Y. Xiong, et al., Clinical characteristics of 138 hospitalized patients with 2019 novel coronavirus‚Äìinfected pneumonia in Wuhan, China, JAMA 323 (11) (2020) 1061‚Äì1069. [3] J.J. Marini, L. Gattinoni, Management of COVID-19 respiratory distress, JAMA 323 (22) (2020) 2329‚Äì2330. [4] D. Aguiar, J.A. Lobrinus, M. Schibler, T. Fracasso, C. Lardi, Inside the lungs of COVID-19 disease, Int. J. Leg. Med. 134 (2020) 1271‚Äì1274. [5] D.R. Ziehr, J. Alladina, C.R. Petri, J.H. Maley, A. Moskowitz, B.D. Medoff, K. A. Hibbert, B.T. Thompson, C.C. Hardin, Respiratory pathophysiology of mechanically ventilated patients with COVID-19: a cohort study, Am. J. Respir. Crit. Care Med. 201 (12) (2020) 1560‚Äì1564. [6] C. E. Davis, M. Schivo, N. J. Kenyon, A breath of fresh air‚Äìthe potential for COVID- 19 breath diagnostics, EBioMedicine 63. [7] S. Grassin-Delyle, C. Roquencourt, P. Moine, G. Saffroy, S. Carn, N. Heming, J. Fleuriet, H. Salvator, E. Naline, L.-J. Couderc, et al., Metabolomics of exhaled breath in critically ill COVID-19 patients: a pilot study, EBioMedicine 63 (2021) 103154. [8] D.M. Ruszkiewicz, D. Sanders, R. O‚ÄôBrien, F. Hempel, M.J. Reed, A.C. Riepe, K. Bailie, E. Brodrick, K. Darnley, R. Ellerkmann, et al., Diagnosis of COVID-19 by analysis of breath with gas chromatography-ion mobility spectrometry-a feasibility study, EClinicalMedicine 29 (2020) 100609. [9] S. Walvekar, D. Shinde, et al., Detection of COVID-19 from CT images using Resnet50, in: 2nd International Conference on Communication & Information Processing (ICCIP) 2020, 2020. ComputersinBiologyandMedicine141(2022)10515311M. Pahar et al.                                                                                                                 [10] H. Sotoudeh, M. Tabatabaei, B. Tasorian, K. Tavakol, E. Sotoudeh, A.L. Moini, [34] R. Bachu, S. Kopparthi, B. Adapa, B.D. Barkana, Voiced/unvoiced decision for Artificial intelligence empowers radiologists to differentiate pneumonia induced by COVID-19 versus influenza viruses, Acta Inf. Med. 28 (3) (2020) 190. speech signals based on zero-crossing rate and energy, in: Advanced Techniques in Computing Sciences and Software Engineering, Springer, 2010, pp. 279‚Äì282. [11] M. Yildirim, A. Cinar, A deep learning based hybrid approach for COVID-19 disease detections, Trait. Du. Signal 37 (3) (2020) 461‚Äì468. [12] T. Higenbottam, Chronic cough and the cough reflex in common lung diseases, Pulm. Pharmacol. Therapeut. 15 (3) (2002) 241‚Äì247. [35] M. Pahar, L.S. Smith, Coding and decoding speech using a biologically inspired coding system, in: 2020 IEEE Symposium Series on Computational Intelligence (SSCI), IEEE, 2020, pp. 3025‚Äì3032, https://doi.org/10.1109/ SSCI47803.2020.9308328. [13] A. Chang, G. Redding, M. Everard, Chronic wet cough: protracted bronchitis, [36] H. Chatrzarrin, A. Arcelus, R. Goubran, F. Knoefel, Feature extraction for the chronic suppurative lung disease and bronchiectasis, Pediatr. Pulmonol. 43 (6) (2008) 519‚Äì531. differentiation of dry and wet cough sounds, in: IEEE International Symposium on Medical Measurements and Applications, IEEE, 2011, pp. 162‚Äì166. [14] K.F. Chung, I.D. Pavord, Prevalence, pathogenesis, and causes of chronic cough, [37] M.B. Alsabek, I. Shahin, A. Hassan, Studying the similarity of COVID-19 sounds Lancet 371 (9621) (2008) 1364‚Äì1374. [15] J. Knocikova, J. Korpas, M. Vrabec, M. Javorka, Wavelet analysis of voluntary cough sound in patients with respiratory diseases, J. Physiol. Pharmacol. 59 (Suppl 6) (2008) 331‚Äì340. [16] A. Imran, I. Posokhova, H.N. Qureshi, U. Masood, M.S. Riaz, K. Ali, C.N. John, M. I. Hussain, M. Nabeel, AI4COVID-19: AI enabled preliminary diagnosis for COVID- 19 from cough samples via an app, Inform. Med. Unlocked 20 (2020) 100378. [17] J. Laguarta, F. Hueto, B. Subirana, COVID-19 artificial intelligence diagnosis using only cough recordings, IEEE Open J. Eng. Med. Biol. 1 (2020) 275‚Äì281, https:// doi.org/10.1109/OJEMB.2020.3026928. [18] C. Brown, J. Chauhan, A. Grammenos, J. Han, A. Hasthanasombat, D. Spathis, T. Xia, P. Cicuta, C. Mascolo, Exploring automatic diagnosis of COVID-19 from crowdsourced respiratory sound data, in: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 3474‚Äì3484. [19] H. Coppock, A. Gaskell, P. Tzirakis, A. Baird, L. Jones, B. Schuller, End-to-end convolutional neural network enables COVID-19 detection from breath and cough audio: a pilot study, BMJ Innovations 7 (2).. [20] M. Pahar, M. Klopper, R. Warren, T. Niesler, COVID-19 cough classification using machine learning and global smartphone recordings, Comput. Biol. Med. 135 (2021) 104572, https://doi.org/10.1016/j.compbiomed.2021.104572. [21] N. Sharma, P. Krishnan, R. Kumar, S. Ramoji, S.R. Chetupalli, N. R, P.K. Ghosh, S. Ganapathy, Coswara‚ÄìA database of breathing, cough, and voice sounds for COVID-19 diagnosis, Proc. Interspeech 2020 (2020) 4811‚Äì4815, https://doi.org/ 10.21437/Interspeech.2020-2768. [22] B.W. Schuller, A. Batliner, C. Bergler, C. Mascolo, J. Han, I. Lefter, H. Kaya, S. Amiriparian, A. Baird, L. Stappen, S. Ottl, M. Gerczuk, P. Tzirakis, C. Brown, J. Chauhan, A. Grammenos, A. Hasthanasombat, D. Spathis, T. Xia, P. Cicuta, M. R. Leon, J.J. Zwerts, J. Treep, C. Kaandorp, The INTERSPEECH 2021 computational Paralinguistics challenge: COVID -19 Cough, COVID -19 Speech, escalation & primates, in: Proceedings INTERSPEECH 2021, 22nd Annual Conference of the International Speech Communication Association, ISCA, Brno, Czechia, 2021 to appear. [23] M. Pahar, I. Miranda, A. Diacon, T. Niesler, Deep neural network based cough detection using bed-mounted accelerometer measurements, in: ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP), 2021, pp. 8002‚Äì8006, https://doi.org/10.1109/ ICASSP39728.2021.9414744. [24] G. Botha, G. Theron, R. Warren, M. Klopper, K. Dheda, P. Van Helden, T. Niesler, Detection of tuberculosis by automatic cough sound analysis, Physiol. Meas. 39 (4) (2018), 045005. [25] M. Pahar, M. Klopper, B. Reeve, R. Warren, G. Theron, T. Niesler, Automatic cough classification for tuberculosis screening in a real-world environment, Physiol. Meas. 42 (2021) 105014, https://doi.org/10.1088/1361-6579/ac2fb8. [26] J.F. Gemmeke, D.P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R.C. Moore, M. Plakal, M. Ritter, Audio set: an ontology and human-labeled dataset for audio events, in: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2017, pp. 776‚Äì780. [27] F. Font, G. Roma, X. Serra, Freesound technical demo, in: Proceedings of the 21st ACM International Conference on Multimedia, 2013, pp. 411‚Äì412. [28] I.D. Miranda, A.H. Diacon, T.R. Niesler, A comparative study of features for acoustic cough detection using deep architectures, in: 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), IEEE, 2019, pp. 2601‚Äì2605. [29] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, Librispeech: an ASR corpus based on public domain audio books, in: 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2015, pp. 5206‚Äì5210, https://doi.org/10.1109/ICASSP.2015.7178964. [30] J. Van Hulse, T.M. Khoshgoftaar, A. Napolitano, Experimental perspectives on learning from imbalanced data, in: Proceedings of the 24th International Conference on Machine Learning, 2007, pp. 935‚Äì942. [31] B. Krawczyk, Learning from imbalanced data: open challenges and future directions, Prog. Artificial Intell. 5 (4) (2016) 221‚Äì232. [32] N.V. Chawla, K.W. Bowyer, L.O. Hall, W.P. Kegelmeyer, SMOTE: synthetic minority over-sampling technique, J. Artif. Intell. Res. 16 (2002) 321‚Äì357. [33] A. Muguli, L. Pinto, N. Sharma, P. Krishnan, P. K. Ghosh, R. Kumar, S. Ramoji, S. Bhat, S. R. Chetupalli, S. Ganapathy, et al., DiCOVA Challenge: Dataset, Task, and Baseline System for COVID-19 Diagnosis Using Acoustics, arXiv preprint arXiv: 2103.09148.. based on correlation analysis of MFCC, in: 2020 International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI), IEEE, 2020, pp. 1‚Äì5. [38] S. Aydƒ±n, H.M. SaraoÀòglu, S. Kara, Log energy entropy-based EEG classification with multilayer neural networks in seizure, Ann. Biomed. Eng. 37 (12) (2009) 2626. [39] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770‚Äì778. [40] A. Silnova, P. Matejka, O. Glembek, O. Plchot, O. Novotn·ª≥, F. Grezl, P. Schwarz, L. Burget, J. Cernock·ª≥, BUT/Phonexia Bottleneck Feature Extractor, Odyssey, 2018, pp. 283‚Äì287. [41] Y. Song, I. McLoughLin, L. Dai, Deep bottleneck feature for image classification, in: Proceedings of the 5th ACM on International Conference on Multimedia Retrieval, 2015, pp. 491‚Äì494. [42] Q.B. Nguyen, J. Gehring, K. Kilgour, A. Waibel, Optimizing deep bottleneck feature extraction, in: The 2013 RIVF International Conference on Computing & Communication Technologies-Research, Innovation, and Vision for Future (RIVF), IEEE, 2013, pp. 152‚Äì156. [43] S. Liu, Leave-p-Out cross-validation test for uncertain verhulst-pearl model with imprecise observations, IEEE Access 7 (2019) 131705‚Äì131709. [44] T. Fawcett, An introduction to ROC analysis, Pattern Recogn. Lett. 27 (8) (2006) 861‚Äì874. [45] P.A. Devijver, J. Kittler, Pattern Recognition: A Statistical Approach, Prentice Hall, 1982. [46] D. Grant, I. McLane, J. West, Rapid and scalable COVID-19 screening using speech, breath, and cough recordings, in: 2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI), IEEE, 2021, pp. 1‚Äì6, https://doi.org/ 10.1109/BHI50953.2021.9508482. [47] J. Andreu-Perez, H. P¬¥erez-Espinosa, E. Timonet, M. Kiani, M.I. Giron-Perez, A. B. Benitez-Trinidad, D. Jarchi, A. Rosales, N. Gkatzoulis, O.F. Reyes-Galaviz, et al., A generic deep learning based cough analysis system from clinically validated samples for point-of-need Covid-19 test and severity levels, IEEE Trans. Serv. Comput. (2021), https://doi.org/10.1109/TSC.2021.3061402, 1‚Äì1. Madhurananda Pahar received his BSc in Mathematics from the University of Calcutta, India, and his MSc in Computing for Financial Markets followed by his PhD in Computational Neuroscience from the University of Stirling, Scotland. Currently, he is working as a post-doctoral fellow at Stellen-bosch University, South Africa. His research interests are in machine learning and signal processing for audio signals and smart sensors in bio-medicine. Currently, he is involved in the application of deep learning to the detection and classification of tuberculosis and COVID-19 coughs in real-world environ-ments as well as the monitoring of patient behaviour using smart sensors such as an accelerometer. Marisa Klopper is a researcher at the Division of Molecular Biology and Human Genetics of Stellenbosch University, South Africa. She holds a PhD in Molecular Biology from Stellenbosch University and her research interest is in TB and drug-resistant TB diagnosis, epidemiology and physiology. She has been involved in cough classification for the last 6 years, with application to TB and more recently COVID-19. ComputersinBiologyandMedicine141(2022)10515312M. Pahar et al.                                                                                                                 Robin Warren is the Unit Director of the South African Med-ical Research Council‚Äôs Centre for Tuberculosis Research and Distinguished Professor at Stellenbosch University. He has a B2 rating by the National Research Council (NRF) and is a core member of the DSI-NRF Centre of Excellence for Biomedical Tuberculosis Research and head the TB Genomics research thrust. He has published over 320 papers in the field of TB and have an average H-index (Scopus, Web of Science and Google Scholar) of 65. Thomas Niesler obtained the B.Eng (1991) and M.Eng (1993) degrees in Electronic Engineering from the University of Stel-lenbosch, South Africa and a Ph.D. from the University of Cambridge, England, in 1998. He joined the Department of Engineering, University of Cambridge, as a lecturer in 1998 and subsequently the Department of Electrical and Electronic Engineering, University of Stellenbosch, in 2000, where he has been Professor since 2012. His research interests lie in the areas of signal processing, pattern recognition and machine learning. ComputersinBiologyandMedicine141(2022)10515313