Artificial Intelligence 175 (2011) 1757–1789Contents lists available at ScienceDirectArtificial Intelligencewww.elsevier.com/locate/artintDecentralized MDPs with sparse interactionsFrancisco S. Melo a,∗, Manuela Veloso ba GAIPS – INESC-ID, TagusPark, Edifício IST, 2780-990 Porto Salvo, Portugalb Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213, USAa r t i c l ei n f oa b s t r a c tArticle history:Received 26 April 2010Received in revised form 29 April 2011Accepted 7 May 2011Available online 10 May 2011Keywords:Multiagent coordinationSparse interactionDecentralized Markov decision processesCreating coordinated multiagent policies in environments with uncertainty is a challengingproblem, which can be greatly simplified if the coordination needs are known to belimited to specific parts of the state space. In this work, we explore how such localinteractions can simplify coordination in multiagent systems. We focus on problems inwhich the interaction between the agents is sparse and contribute a new decision-theoreticmodel for decentralized sparse-interaction multiagent systems, Dec-SIMDPs, that explicitlydistinguishes the situations in which the agents in the team must coordinate from thosein which they can act independently. We relate our new model to other existing modelssuch as MMDPs and Dec-MDPs. We then propose a solution method that takes advantageof the particular structure of Dec-SIMDPs and provide theoretical error bounds on thequality of the obtained solution. Finally, we show a reinforcement learning algorithm inwhich independent agents learn both individual policies and when and how to coordinate.We illustrate the application of the algorithms throughout the paper in several multiagentnavigation scenarios.© 2011 Elsevier B.V. All rights reserved.1. IntroductionDecision-theoretic models, such as Dec-MDPs and Dec-POMDPs, provide a rich framework to tackle decentralizeddecision-making problems. However, using these models to create coordinated multiagent policies in environments withuncertainty is a challenging problem, even more so if the decision-makers must tackle issues of partial observability. Assuch, solving finite-horizon Dec-POMDPs is a NEXP-complete problem and thus computationally too demanding to solveexcept for the simplest scenarios.Recent years have witnessed a profusion of work on Dec-POMDP-related models that aim at capturing some of thefundamental features of this class of problems, such as partial observability, without incurring the associated computationalcost. In this paper, we contribute to this area of research, and introduce a new model for cooperative multiagent decision-making in the presence of partial observability. Our model is motivated by the observation that, in many real-world scenarios,the tasks of the different agents in a multiagent system are not coupled at every decision step but only in relativelyinfrequent situations. We refer to such problems as having sparse interactions.Multi-robot systems provide our primary motivation, as the interaction among different robots is naturally limited byeach robot’s physical boundaries, such as workspace or communication range, and limited perception capabilities. Therefore,when programming a multi-robot system to perform some task, one natural approach is to subdivide this task into smallertasks that each robot can then execute autonomously or as part of a smaller group. As an example, consider the scenarioin Fig. 1. In this scenario, three robots must navigate to their goal locations, marked with dashed lines. While Robot 3 can* Corresponding author.E-mail address: fmelo@inesc-id.pt (F.S. Melo).0004-3702/$ – see front matter © 2011 Elsevier B.V. All rights reserved.doi:10.1016/j.artint.2011.05.0011758F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789Fig. 1. Example of a simple navigation task.navigate to its goal, disregarding the remaining robots, Robots 1 and 2 need to coordinate so as not to cross the narrowdoorway simultaneously. However, this coordination needs only occur around the doorway.Other examples include problems of sequential resource allocation, in which groups of agents must interact only tothe extent that they need to share some common resource. In this context, several methods have been proposed thatleverage sparse interactions by decomposing the global problem into several smaller local problems that can be solvedmore efficiently, and then combining the obtained solutions [1,2]. Such approaches, however, are not particularly concernedwith partial observability issues. Additional examples include problems of task allocation, where the different agents ina multiagent system are assigned to different subtasks, and the interactions between the agents when performing suchsubtasks is localized to small regions of the joint state space [3]. Such problems include emergency response scenarios,where emergency teams are assigned different subtasks (for example, assisting different victims) and interact only in specificlocalized situations.Several approaches have exploited simplified models of interaction in multiagent settings. For example, learning tasksinvolving multiple agents can be partitioned in a state-wise manner, allowing different agents to independently learn theresulting “smaller tasks” [4]. Similarly, a hierarchical learning algorithm can be used that considers only interactions betweenthe different agents at a higher control level, while allowing the agents to learn lower level tasks independently [5]. Otherworks use coordination graphs to compactly represent dependences between the actions of different agents, thus capturingthe local interaction between them [6,7]. Local interactions have also been exploited to minimize communication duringpolicy execution [8] and in the game-theoretic literature to attain compact game representations. Examples include graphicalgames [9] and action-graph games [10].In this article we consider Dec-MDPs with sparse interactions, henceforth Dec-SIMDPs. Dec-SIMDPs leverage the indepen-dence between agents to decouple the decision process in significant portions of the joint state space. In those situationsin which the agents interact—the interaction areas—Dec-SIMDPs rely on communication to bring down the computationalcomplexity of the joint decision process. Dec-SIMDPs balance the independence assumptions with observability: in anygiven state, the agents are either independent or can share state information (e.g., by communicating).1 A related modelhas recently been proposed under the designation of distributed POMDPs with coordination locales [13]. We postpone untilSection 6 a more detailed discussion of this and other related models.The contributions of this article are threefold. We provide a precise formalization of the Dec-SIMDP model and discussthe relation with well-established decision-theoretic models such as Dec-MDPs, MMDPs and MDPs. We then contribute twonew algorithms that exhibit significant computational savings when compared to existing algorithms for Dec-SIMDPs, andillustrate their application in several simple navigation tasks. Finally, we investigate the influence of interaction areas in theperformance of a multiagent system and contribute a new learning algorithm that allows each agent in one such system toindividually learn those interaction areas.2. Decision-theoretic models for multiagent systemsWe now review several standard decision-theoretic models that are relevant for our work [14–16]. We start with singleagent models, namely Markov decision processes (MDPs) and their partially observable counterparts (POMDPs) before mov-ing to multiagent models such as multiagent MDPs (MMDPs) and their partially observable counterparts (Dec-POMDPs). Weestablish the notation we use and review some fundamental concepts of later relevance.To fully specify the different models in this section, we should explicitly include the initial state x0 or a distributionthereof. However, in order to avoid cluttering the notation, we omit the explicit reference to this initial state, with theunderstanding that one such state is implicit.1 Both independence assumptions and communication can significantly bring down the computational complexity in Dec-POMDP related models [11,12].F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–178917592.1. Markov decision processesA Markov decision process (MDP) describes a sequential decision problem in which a single agent must choose an action atevery time step to maximize some reward-based optimization criterion. We use MDPs in the Dec-SIMDP model proposed inSection 3 to describe an agent in those situations where its actions are independent of other agents—i.e., in those situationswhere the agent can be modeled individually.Formally, an MDP is a tuple M = (X, A, P, r, γ ), where X represents the finite state space, A represents the finiteaction space, P(x, a, y) represents the transition probability from state x to state y when action a is taken, and r(x, a)represents the expected reward for taking action a in state x. The scalar γ is a discount factor.A Markov policy is a mapping π : X × A → [0, 1] such that, for all x ∈ X ,(cid:2)a∈Aπ (x, a) = 1.Solving an MDP consists of determining a policy π so as to maximize, for all x ∈ X ,V π (x) = EπX(t), A(t)(cid:3)(cid:4)γ tr∞(cid:2)t=0(cid:7)(cid:5) (cid:6)(cid:6) X(0) = x,where X(t) denotes the state at time step t, A(t) denotes the action taken at that time instant such that(cid:8)(cid:6)(cid:6) H(t) = h(cid:9)(cid:8)(cid:6)(cid:9)(cid:6) X(t) = xP= PA(t) = a= π (x, a),where H(t) = { X(0), A(0), . . . , X(t − 1), A(t − 1), X(t)} is the random variable corresponding to the history of the MDP up totime t, and h denotes a particular realization of H(t) such that X(t) = x. We write A(t) ∼ π to denote the above dependenceof A(t) on the policy π . We define the Q -function associated with a policy π asA(t) = a(cid:3)Q π (x, a) = Eπ(cid:4)γ tr∞(cid:2)t=0X(t), A(t)(cid:5) (cid:6)(cid:6) X(0) = x, A(0) = a(cid:7),where, again, A(t) ∼ π for all t > 0.For any finite MDP, there is at least one optimal policy π ∗, such thatV π ∗(x) (cid:2) V π (x)for every policy π and every state x ∈ X . The value function corresponding to π ∗verifies the Bellman optimality equation,(cid:10)r(x, a) + γ(x) = maxa∈AP(x, a, y)V(cid:2)( y)(cid:11)V∗∗.y∈XThe associated Q -function in turn verifies∗Q(x, a) = r(x, a) + γ(cid:2)y∈XP(x, a, y) maxu∈A∗Q( y, u).is denoted by V∗(short for V π ∗) and(1)(2)The optimal policy can be recovered directly from Qin state x ∈ X only if a ∈ arg maxu∈A Q∗corresponding optimal Q -function, Q.by assigning an action a ∈ A a positive probability of being selected∗(x, u). As such, the solution for any given MDP can be obtained by computing the∗Given a function q defined over X × A, the Bellman operator H is defined as(Hq)(x, a) = r(x, a) + γ(cid:2)y∈XP(x, a, y) maxu∈Aq( y, u).(3)The function Qestimate Q (0), a dynamic programming (DP) method known as value iteration.in (2) is the fixed-point of H and thus can be computed, e.g., by iteratively applying H to some initial∗2.1.1. Q-learning∗It is possible to use the operator H in (3) and use a standard fixed-point iteration to compute Qunknown, Qconsisting of state–action–reward–next state experienced in the environment, and is defined by the update rulecan be estimated using the Q -learning algorithm [17]. Q -learning allows Q∗∗. If P and/or r areto be estimated from transitionsQ k+1(x, a) = (1 − αk)Q k(x, a) + αk,(4)(cid:12)r(x, a) + γ maxu∈A(cid:13)Q k( y, u)1760F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789∗(x, a), y is a sample from the distribution P(x, a, ·) and {αk} is a step-size sequence.where Q k(x, a) is the kth estimate of QThe sample y and the value of r(x, a) can be obtained from a generative model or from the actual system, not requiringthe knowledge of either P or r. Under suitable conditions, the estimates Q k converge to Qwith probability 1 [18]. Wehenceforth write QLUpdate(Q ; x, a, r, y, Q(cid:7)) to compactly denote the general Q -learning update operation in (4), i.e.,∗(cid:4)Q updt(x, a) = QLUpdateQ ; x, a, r, y, Q(cid:5)(cid:7)(cid:3) (1 − αk)Q (x, a) + αk(cid:12)r + γ maxu∈A(cid:13)( y, u),(cid:7)Qwhere Q and Q(cid:7)are general Q -functions.2.2. Partially observable Markov decision processesPartially observable MDPs describe problems essentially similar to MDPs, in which an agent must choose an action atevery time step to maximize a reward-based criterion. However, at each time step t, the agent in a POMDP cannot directlyobserve the state of the process, X(t), but accesses only an indirect observation thereof, Z (t). POMDPs will be useful whenplanning for Dec-SIMDPs in Section 4. Taking advantage of the particular structure of Dec-SIMDPs, we reduce the globalplanning problem to the problem of planning in a collection of individual single-agent POMDPs.Formally, a POMDP is a tuple (X , A, Z, P, O, r, γ ), where X is the finite state space, A is the finite action space and Zis the finite observation space. As before, P(x, a, y) represents the transition probability from state x to state y when actiona is taken, and now O(x, a, z) represents the probability of observing z ∈ Z given that the state is x and action a was taken,i.e.,O(x, a, z) = P(cid:8)Z (t + 1) = z(cid:6)(cid:6) X(t + 1) = x, A(t) = a(cid:9).Finally, r(x, a) again represents the expected reward for taking action a in state x and γ is a discount factor.A non-Markov policy is a mapping π : H → [0, 1] such that, for all finite histories h ∈ H,(cid:2)a∈Aπ (h, a) = 1,where h = {a(0), z(1), a(1), . . . , a(t − 1), z(t)} is a finite history, i.e., a finite sequence of action–observation pairs. As before,the history observed up to time t is a random variable, denoted as H(t), and we denote by H the set of all possible finitehistories for the POMDP. Solving a POMDP consists of determining a policy π so as to maximize, for all |X |-dimensionalprobability vectors b,(cid:3)(cid:7)∞(cid:2)(cid:4)V π (b) = Eπγ trX(t), A(t)(cid:5) (cid:6)(cid:6) X(0) ∼ b,t=0where X(0) ∼ b denotes the fact that X(0) is distributed according to b. Similarly, we define the Q -function associated witha policy π asQ π (b, a) = Eπ(cid:3)∞(cid:2)t=0(cid:4)γ trX(t), A(t)(cid:5) (cid:6)(cid:6) X(0) ∼ b, A(0) = a(cid:7).We refer to the probability vector b in the expressions above as the initial belief state or just the initial belief. It translatesthe “belief ” that the agent has at time t = 0 regarding its state at that time. From the initial belief and given the history upto time t, H(t), we can construct a sequence {b(t)} of probability vectors recursively as(cid:4)(cid:5)b(t), A(t), Z (t + 1)b y(t + 1) = Bel(cid:5)y, A(t), Z (t + 1)x, A(t), ybx(t)P(cid:3) η(cid:2)O(cid:5)(cid:4)(cid:4),where Bel is the belief update operator, bx(t) denotes the x component of b(t) and η is a normalization factor. We refer tothe vector b(t) as the belief at time t. It corresponds to a distribution over the unknown state at time t, such thatx(cid:8)bx(t) = PX(t) = x(cid:6)(cid:9)(cid:6) H(t).Given the POMDP model parameters P and O, every finite history h ∈ H can be mapped to a belief b. Moreover, forany given policy, two histories leading to the same belief will yield the same future total expected discounted reward. Assuch, beliefs can be used as compact representations of histories, and we can define policies in terms of beliefs instead ofhistories [19]. In fact, it is possible to reinterpret a POMDP as an infinite MDP in which the state space corresponds to theset of all possible beliefs. Therefore, for any finite POMDP, there is at least one optimal policy π ∗, such thatV π ∗(b) (cid:2) V π (b)F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891761for any π and every initial belief b. The corresponding value function is denoted by Voptimality equation,∗V(b) = maxa∈A(cid:2)bxx(cid:10)r(x, a) + γP(x, a, y)O( y, a, z)V∗(cid:4)(cid:5)Bel(b, a, z)(cid:2)z, y(cid:11),∗and also verifies the Bellmanwhere x, y take values in X , z takes values in Z and Bel(b, a, z) corresponds to the updated belief after taking action a andobserving z. The associated Q -function in turn verifies∗Q(b, a) =(cid:2)bxx(cid:10)r(x, a) + γ(cid:2)z, yP(x, a, y)O( y, a, z) maxu∈AQ(cid:4)∗(cid:11)(cid:5)Bel(b, a, z), u.In spite of their representative power, POMDPs have been shown to be undecidable in the worst case for infinite horizonsettings such as those considered in this paper [20]. As such, exact solutions can be computed only in very specific instances,and most approaches in the literature resort to approximate or heuristic methods [21,22].We now describe an MDP-based heuristic solution method for POMDPs that we use later in the paper. This method isknown as Q MDP as it makes use of the optimal Q -function for the underlying MDP as an estimate for the optimal Q -function for the POMDP [23]. Since the optimal solution for the underlying MDP can be efficiently computed, this methodis very simple and fast to implement and attains good performance in many practical situations [21,23].Let M = (X , A, Z, P, O, r, γ ) be a POMDP with finite state, action and observation spaces. Associated with this POMDP,¯M. Q MDP uses an estimate for thebe the optimal Q -function forthere is an underlying MDP ¯M = (X, A, P, r, γ ). Let ¯Qoptimal Q -function for the POMDP given by∗(cid:2)ˆQ (b, a) =∗¯Qbx(x, a).xWhen using Q MDP, the agent acts under the implicit assumption that state uncertainty only affects the immediate decision,i.e., after one decision the agent will act on the underlying MDP. Since such assumption seldom holds, Q MDP sometimesexhibits poor performance and several works have proposed further improvements on Q MDP to address this issue [21,24],but we do not pursue such discussion here.2.3. Multiagent MDPsMultiagent Markov decision processes (MMDPs) generalize MDPs to multiagent cooperative scenarios. MMDPs describesequential decision tasks in which multiple agents must each choose an individual action at every time-step that jointlymaximize some common reward-based optimization criterion. Formally, an MMDP is a tuple M = (N , X, (Ak), P, r, γ ),where N is the number of agents, X represents the finite state space, Ak is the finite individual action space of agent k.As in MDPs, P(x, a, y) represents the transition probability from state x to state y when the joint action a = (a1, . . . , aN ) istaken; r(x, a) represents the expected reward received by all agents for taking the joint action a in state x. In an MMDP allagents receive the same reward, which implies that MMDPs represent fully cooperative multiagent tasks.A joint action a is a tuple a = (a1, . . . , aN ) and we denote by A = ×NAk the set of all possible joint actions—the jointk=1action space. For k = 1, . . . , N, we writeA−k = A1 × · · · × Ak−1 × Ak+1 × · · · × ANto denote the set of joint actions of all agents other than agent k. We write a−k to denote a general element of A−k andrefer to any such action as a reduced joint action or simply a reduced action. We write a = (a−k, ak) to denote the fact thatthe joint action a is composed by the reduced action a−k and the individual action ak for agent k.We also assume that the state space X can be factored among the N agents as X = X0 × X1 × · · · × XN , where X0denotes an agent-independent component of the state. As such, each element x ∈ X is a tuple x = (x0, . . . , xN ), with xk ∈ Xk,k = 0, . . . , N. For any x ∈ X , we refer to the pair (x0, xk) as the local state of agent k, and generally denote it as ¯xk.An individual Markov policy for agent k is a mapping πk : X × Ak → [0, 1] such that, for all x ∈ X ,(cid:2)πk(x, ak) = 1.ak∈AkSimilarly, a joint policy is a mapping π : X × A → [0, 1] that we take as the combination of N individual policies, i.e.,π (x, a) =N(cid:14)k=1πk(x, ak),where a = (a1, . . . , aN ); π−k denotes a reduced policy and π = (π−k, πk) denotes the fact that the joint policy π is composedby the reduced policy π−k and the individual policy πk for agent k.1762F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789In an MMDP, the purpose of all agents is to determine a joint policy π so as to maximize, for all x ∈ X ,(cid:3)∞(cid:2)(cid:4)γ trX(t), A(t)(cid:7)(cid:5) (cid:6)(cid:6) X(0) = x,V π (x) = Eπt=0where X(t) denotes the state at time t and A(t) denotes the joint action taken at time t. The Q -function associated with ajoint policy π is defined from V π as its single-agent counterpart.For the purposes of planning, i.e., computing the optimal policy, an MMDP is indistinguishable from an ordinary MDP. Infact, when considering a centralized controller, an MMDP reduces to an MDP. It is only at execution time that an MMDPdiffers from an MDP, since the process of decision making is not centralized. However, when we move to partially observablesettings, decentralized execution raises severe difficulties even during planning.2.4. Decentralized POMDPsDecentralized POMDPs (Dec-POMDPs) are partially observable generalizations of MMDPs. As in MMDPs, the agents in aDec-POMDP must each choose an individual action at every time step that jointly maximize some common reward-basedoptimization criterion. Unlike MMDPs, however, the agents can only access the global state of the process by means of localindirect observations. Formally, a Dec-POMDP can be represented a tuple(cid:4)M =N, (Xk), (Ak), (Zk), P, (Ok), r, γ(cid:5),where N is the number of agents, X = ×NAk is the set of joint actions, each Zkrepresents the set of possible local observations for agent k, P(x, a, y) represents the transition probabilities from jointstate x to joint state y when the joint action a is taken, each Ok(x, a, zk) represents the probability of agent k making thelocal observation zk when the joint state is x and the last action taken was a, and r(x, a) represents the expected rewardreceived by all agents for taking the joint action a in joint state x. The scalar γ is a discount factor.Xk is the joint state space, A = ×NIn this partially observable multiagent setting, an individual non-Markov policy for agent k is a mapping πk : Hk → [0, 1]k=1k=0such that, for all hk ∈ Hk,(cid:2)π (hk, ak) = 1,ak∈Akwhere hk = {ak(0), zk(1), . . . , ak(t − 1), zk(t)} is an individual history for agent k,i.e., a sequence of individual action–observation pairs. Hk is the set of all possible finite histories for agent k and we denote by Hk(t) the random variablethat represents the history of agent k at time t.Like in POMDPs, in a Dec-POMDP each agent has only partial perception of the global state. Therefore, from the agent’sperspective, its local observations are non-Markovian—the current local observation and action are not sufficient to uniquelydetermine its next observation. In the general multiagent setting, however, there is no compact representation of historiesthat plays the role of beliefs in POMDPs, implying that the passage from MMDP to its partially observable counterpart isfundamentally different from the same passage in single-agent scenarios.2 However, if communication between the agentsis instantaneous, free and error-free, then a Dec-POMDP reduces to a “large” POMDP, and partial observability is no longeran issue.Solving a Dec-POMDP consists in determining a joint policy π that maximizes the total sum of discounted rewards.In order to write this in terms of a function, we consider a distinguished initial state, x0 ∈ X , that is assumed commonknowledge among all agents. We thus want to maximize(cid:3)∞(cid:2)V π = Eπ(cid:4)γ trX(t), A(t)(cid:5) (cid:6)(cid:6) X(0) = x0.(cid:7)t=0Dec-POMDPs constitute one of the most representative models in the decision-theoretic literature [26]. In the remainderof this section we discuss several specializations of this model that are relevant for this work.2.4.1. Decentralized MDPsa state x ∈ X such thatA decentralized MDP (Dec-MDP) is a particular case of a Dec-POMDP in which, for every joint observation z ∈ Z , there is(cid:8)PX(t) = x(cid:6)(cid:6) Z (t) = z(cid:9)= 1.2 This fact can also be observed by considering the worst-case computational complexity of each of the different models. In finite horizon settings,POMDPs are PSPACE-complete, versus the P-completeness of fully observable MDPs [25]. In finite-horizon multiagent settings, however, Dec-MDPs areNEXP-complete [16] even in the 2-agent case, versus the P-completeness of MMDPs.F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891763Intuitively, the agents in a Dec-MDP have joint full observability: if all agents share their observations, they can recover thestate of the Dec-MDP unambiguously. This is in contrast with MMDPs where each agent, individually, has full observability.In fact, MMDPs can be seen as a particular subclass of Dec-MDPs in which the individual observations of each agent allowit to recover the state of the Dec-MDP/MMDP unambiguously. Formally, for every individual observation zk ∈ Zk, there is astate x ∈ X such that(cid:8)PX(t) = x(cid:6)(cid:6) Zk(t) = zk(cid:9)= 1.(5)Throughout this work, we focus on Dec-MDPs and specializations thereof. Moreover, as with MMDPs, we assume that thestate can be factored and the agents have local full observability, meaning that each agent can infer from its local observationsthe corresponding local state unambiguously. Formally local full observability can be translated into the following condition:for every local observation zk ∈ Zk there is a local state ¯xk ∈ X0 × Xk such that(cid:8)P¯Xk(t) = ¯xk(cid:6)(cid:6) Zk(t) = zk(cid:9)= 1.Although more general Dec-MDP models are possible [16], we adhere to this simplified version, as this is sufficient for ourpurposes. For future reference, we define the setX−k = X0 × · · · × Xk−1 × Xk+1 × · · · × XNcorresponding to the local state of all agents other than agent k, and denote by x−k a general element of X−k. As withactions, we write x = (x−k, xk) to denote the fact that the kth component of x takes the value xk.2.4.2. Transition, observation and reward independenceTransition-independent Dec-MDPs constitute a particular subclass of Dec-MDPs in which, for all (x, a) ∈ X × A,(cid:8)(cid:8)PPX0(t + 1) = y0Xk(t + 1) = yk(cid:6)(cid:6) X(t) = x, A(t) = a(cid:6)(cid:9)(cid:6) X(t) = x, A(t) = a(cid:9)(cid:8)= P(cid:8)= PX0(t + 1) = y0Xk(t + 1) = yk(cid:6)(cid:9)(cid:6) X0(t) = x0,(cid:6)(cid:6) ¯Xk(t) = ¯xk, Ak(t) = ak(cid:9).(6a)(6b)(7)The transition probabilities can thus be factorized asP(x, a, y) = P0(x0, y0)N(cid:14)k=1Pk(¯xk, ak, yk),whereP0(x0, y0) = PPk(¯xk, ak, yk) = P(cid:8)(cid:8)X0(t + 1) = y0Xk(t + 1) = yk(cid:6)(cid:6) X0(t) = x0(cid:9),(cid:6)(cid:6) ¯Xk(t) = ¯xk, Ak(t) = ak(cid:9).This particular class of Dec-MDPs was introduced in [27] and seeks to exploit a particular form of independence to bringdown the computational complexity required to solve such models. In this class of problems, the local state of each agentconstitutes a sufficient statistic for its history, and the optimal policy for each agent can thus be computed in terms of thisindividual state [12]. This particular class of Dec-MDPs has been shown to be NP-complete in finite-horizon settings, versusthe NEXP-completeness of general finite-horizon Dec-MDPs [12].Similarly, reward-independent Dec-MDPs correspond to a subclass of Dec-MDPs in which, for all x, a,r(x, a) = f(cid:4)rk(¯xk, ak), k = 1, . . . , N(cid:5),(8)i.e., the global reward function r can be obtained from local reward functions rk, k = 1, . . . , N. To ensure consistency of thedecision process, we also require that(cid:4)(cid:5)r−k(x−k, a−k), rk(¯xk, ak)f(cid:4)(cid:5)r−k(x−k, a−k), rk(¯xk, uk)(cid:2) fif and only ifrk(¯xk, ak) (cid:2) rk(¯xk, uk).One typical example isr(x, a) =N(cid:2)k=1rk(¯xk, ak),(9)where x = (x0, . . . , xN ) and a = (a1, . . . , aN ). Interestingly, it was recently shown [28,11] that reward independent Dec-MDPsretain NEXP-complete complexity. However, when associated with transition independence, reward independence impliesthat a Dec-MDP can be decomposed into N independent MDPs, each of which can be solved separately. The complexity ofthis class of problems thus reduces to that of standard MDPs (P-complete).1764F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789Fig. 2. Scenario in Example 1. Two robots must coordinate around a narrow doorway.3. Decentralized sparse-interaction MDPs (Dec-SIMDPs)We now depart from the transition-independent Dec-MDP and introduce a new model for multiagent decision problemsthat is, at the same time, more general and more specific than transition independent Dec-MDPs. Our goal is to exploitsparse interactions among the different agents in a Dec-MDP and so we are interested in Dec-MDPs in which there is somelevel of both transition and reward dependency, but this dependency is limited to specific regions of the state space.To motivate our proposed model, we start with a simple example that is used throughout the section to illustrate theseveral concepts introduced.Example 1. Consider the scenario depicted in Fig. 2, corresponding to an environment consisting of two rooms of 10 cellseach connected by a narrow doorway D. In this scenario, two mobile robots acting as a team, Robot 1 and Robot 2, musteach navigate from their depicted initial positions to the corresponding goal positions, marked in Fig. 2 with the labelsGoal 1 and Goal 2, respectively. Each robot has available 4 actions, N, S, E, and W that move to the adjacent cell in thecorresponding direction with probability of 0.8, and fail with probability 0.2. In case of failure, the robot remains in thesame cell. Whenever a robot reaches the corresponding goal position, the team is granted a reward of +1. Conversely, ifboth robots stand simultaneously in the narrow doorway, corresponding to the cell marked as “D”, the robots bump intoeach other and the team is granted a reward of −20. Also, in this situation, the robots get in the way of one another, andthe probability of an individual action failing is 0.4. Each robot is able to perceive unambiguously its own position in theenvironment. Also, when both robots simultaneously stand in the shaded area in Fig. 2, they are able to see each other andcan thus perceive their joint state unambiguously.This problem can be modeled as a Dec-POMDP(cid:4)MH =2, (Xk), (Ak), (Zk), P, (Ok), r, γ(cid:5),where• Xk = {1, . . . , 20, D}, for k = 1, 2;• Ak = {N, S, E, W }, k = 1, 2;• Zk = Xk ∪ X I , k = 1, 2, where X I = {6, 15, D} × {6, 15, D};• the transition and observation functions can be derived from the description above;• the reward function r is given by⎧⎪⎨r(x, a) =⎪⎩if x = (20, 9),if x1 = 20 or x2 = 9,21−20 if x = (D, D),0otherwise.The reward function of this example is purposefully designed to include a region in the state space of negative reward wherethe two robots must coordinate. Outside this interaction area (say, the shaded region in Fig. 2), each robot can pursue itsown goal, disregarding the existence of other robot.The Dec-SIMDP model that we introduce explicitly addresses situations in which the interaction between the agents ina Dec-MDP is localized, much as in Example 1. In order to define our model that leverages such local interactions, we mustfirst formalize concepts such as agent dependence and agent interaction. These definitions refine the notions of transitionand reward independence described in Section 2.4.Let K = {k1, . . . , km} be a subset of agents in a Dec-MDP. We denote byXK = X0 × Xk1× · · · × XkmF.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891765the joint state space of all agents in K . Extending the notation introduced in Section 2, we write X−K to denote the jointstate space of the agents not in K . We write xK to denote a general element of XK and x−K to denote a general elementof X−K . We write x = (x−K , xK ) to distinguish the components of x corresponding to agents in K and those correspondingto agents not in K .We assume that the reward function in our Dec-MDP to be decomposed asr(x, a) =N(cid:2)k=1rk(¯xk, ak) +M R(cid:2)i=1r Ii (xK i , aK i ),(10)where each rk corresponds to an individual component of the reward function that depends only on agent k and thereare M R sets, K i , i = 1, . . . , M R , and M R reward components, r Ii (the interaction components), each depending on all theagents in K i and only on these.Similarly, we assume that the transition probabilities for our Dec-MDP can be decomposed asP(x, a, y) = P0(x0, y0)N(cid:14)(cid:4)k=11 − IK i (k)IX IKi(cid:5)(xK i )Pk(¯xk, ak, yk) + P0(x0, y0)M P(cid:14)i=1PK i (xK i , aK i , y K i )IX IKi(xK i ),(11)where each X IK ifactorized as in (7). We write IU (x) to denote the indicator function for set U .includes those states in XK i where the transition probabilities for each of the agents k ∈ K i cannot beExpressions (10) and (11) explicitly distinguish the components of the reward and transition functions that can befactorized from those that cannot. The decompositions in (10) and (11) imply no loss of generality, since any reward rcan be trivially written in that form by setting M R = 1, rk ≡ 0, K1 = {1, . . . , N}, and r I= r, and the same occurs with the1= P. However, our interest is not in the general case, buttransition probabilities, by setting M P = 1, K1 = {1, . . . , N} and PI1—i.e., the subset of X × A in which these quantities are non-zero—arei and P0when the supports ofsmall when compared with X × A.M Pi=1 PK iM Ri=1 r IIX IKi(cid:19)(cid:20)Definition 3.1 (Agent independence). Given a Dec-MDP M, an agent k0 is independent of agent k1 in a state x ∈ X if thefollowing conditions both hold at state x:• It is possible to decompose the global reward function r(x, a) as in (10) in such a way that no agent set K i containsboth k0 and k1.• It is possible to decompose the transition probabilities P(x, a, y) as in (11) in such a way that no agent set K i containsboth k0 and k1.When any of the above conditions does not hold, agent k0 is said to depend on agent k1 in x. Similarly, agent k0 is inde-pendent of a set of agents K = {k1, . . . , km} at state x if the above conditions hold for all k ∈ K in state x, and dependentotherwise.An agent k0 depends on another agent k1 in a particular state if either the reward function or the transition probabilitiesor both cannot be decomposed so as to decouple the influence of each agent’s individual state and action. However, thedependence relation is not symmetrical: even if agent k0 depends on agent k1, the reverse need not hold. In fact, it ispossible that the two agents are reward independent and that the transition probabilities from state xk0 depend on theindividual state/action of agent k1 without the reverse holding.We illustrate the notion of agent independence using Example 1.Example 1 (Cont.). 1 In the Dec-MDP MH , as previously introduced, it is possible to write the reward function r asr(x, a) = r1(x1, a1) + r2(x2, a2) + r I (x, a),where r1(x1, a1) = I20(x1), r2(x2, a2) = I9(x2) and r I (x, a) = I(D,D)(x). We write Ix(·) to denote the indicator function forthe set {x}. In state (D, D), Robot 1 and Robot 2 both depend on the other, as seen in the transition probabilities. In theremaining joint states in X , Robot 1 and Robot 2 are independent.Definition 3.2 (Agent interaction). In a Dec-MDP M, a set of agents K interact at state x ∈ X if the following conditions allhold• For all k0 ∈ K , if agent k0 depends on some agent k1 in state x, then k1 ∈ K .• For all k1 ∈ K , if there is an agent k0 that depends on agent k1 in state x, then k0 ∈ K .• There is no strict subset K(cid:7) ⊂ K such that the above conditions hold for K.(cid:7)If the agents in a set K interact in a state x, then we refer to xK as an interaction state for the agents in K .1766F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789The concept of interaction introduced above captures a local dependence between a set of agents in a Dec-MDP. If xKis an interaction state for the agents in K , this does not mean that each agent k in K depends on all other agents in thatstate x. Instead, it means that there is at least one agent in K that either depends on k or k depends on it. Furthermore,the definition of agent interaction is transitive: if agents k0 and k1 interact at some state x and agents k1 and k2 interact atthat same state x, then agents k0 and k2 interact at x.We again resort to Example 1 to illustrate the concept of agent interaction.Example 1 (Cont.). Robot 1 and Robot 2 interact at state (D, D). In fact, as seen above, not only does Robot 1 depend onRobot 2, but the converse also holds. Also, there is no strict subset of {1, 2} such that the conditions of Definition 3.2 hold.Definition 3.3 (Interaction area). Consider a general N-agent Dec-MDP M = (N, (Xk), (Ak), (Zk), P, (Ok), r, γ ). We define aninteraction area X I as verifying:(i) X I ⊂ XK , for some set of agents K ;(ii) there is at least one state x(iii) for any x ∈ X I , aK ∈ AK and y /∈ X I ,∗∗ ∈ X I such that x(cid:8)PX K (t + 1) = y(cid:6)(cid:6) X K (t) = x, A K (t) = aKis an interaction state for the agents in K ;(cid:9)= P0(x0, y0)(cid:14)k∈KP(xk, ak, yk);(12)(iv) the set X I is connected.3An agent k is involved in an interaction at time t if there is at least one interaction area X I involving a set of agents K , suchthat k ∈ K and X K (t) ∈ X I . Otherwise, we say agent k is involved in no interaction.Condition (i) states that each interaction area involves a subset K of the agents in a Dec-MDP. Condition (ii) ensures thatin every interaction area there is at least one interaction state involving all agents in K , to minimize the number of agentsinvolved in each interaction. Condition (iii) defines interaction areas as regions of the state space “around” an interactionstate. Finally, condition (iv) states that interaction areas are sets of adjacent states in terms of the transition function.Unlike interaction states, interaction areas are not disjoint: an agent can be simultaneously involved in an interaction at twodifferent interaction areas.Example 1 (Cont.). The joint state (D, D) is an interaction state, corresponding to the situation in which both Robot 1 andRobot 2 are in the narrow doorway. An interaction area should include the state (D, D) and at least those states immediatelyadjacent to it. In fact, in order for the two robots to avoid bumping into each other in the narrow doorway, they mustcoordinate before actually reaching the doorway, and hence the inclusion of the neighboring states in the interaction areaassociated with this interaction state. In this particular scenario, this corresponds to the shaded area in Fig. 2. For illustrationpurposes, we consider X I = {6, 15, D} × {6, 15, D}, although other (larger) sets are also possible.As required in (i), X I ⊂ X1 × X2. Also, as required in (ii), (D, D) ∈ X I . Condition (iii) requires that the transition proba-bilities from states in X I to states not in X I can be factored as in (7). In our scenario, such factorization is not possible onlyin state (D, D), but it is also not possible to transition from this state to a state outside X I . This means that condition (iii)also holds. Finally, X I is connected and condition (iv) also holds.We exploit the fact that an agent not involved in any interaction should be unaffected by partial joint state observability,in the sense that its optimal action is the same independently of the state of the other agents. The purpose of defining theinteraction areas in a Dec-MDP is precisely to single out those situations in which the actions of one agent depend on otheragents. As such, when in an interaction area, the agent should use state information from the other agents in the interactionarea to choose its actions. Therefore, we consider scenarios in which all agents in a particular interaction area X I ⊂ XK attime t have full access to the state X K (t). We henceforth refer to such a Dec-MDP as having observable interactions.Definition 3.4 (Observable interactions). A Dec-MDP has observable interactions, if for any interaction area X I involving a set Kof agents, for each k ∈ K there is a set of local observations Z Ik⊂ Zk such that for every x ∈ X I(cid:8)(cid:9)PZk(t) ∈ Z Ik(cid:6)(cid:6) X K (t) ∈ X Iand, for every zk ∈ Z Ik there is a local state xK ∈ X I , such that(cid:6)(cid:6) Zk(t) = zkX K (t) = xK= 1.= 1P(cid:9)(cid:8)3 A set U ⊂ X is connected if, for any pair of states x, y ∈ U , there is a sequence of actions that, with positive probability, yields a state-trajectory{x(0), . . . , x(T )} such that x(t) ∈ U , t = 0, . . . , T , and either x(0) = x and x(T ) = y or vice versa.F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891767Our focus on Dec-MDPs with observable interactions translates a property often observed in real-world scenarios: wheninvolved in an interaction, agents are often able to observe or communicate information that is relevant for coordination.Interaction areas encapsulate the need for information sharing in a general multiagent decision problem.Informally, we say that a Dec-MDP M = (N, (Xk), (Ak), (Zk), P, (Ok), r, γ ) has sparse interactions if all agents are in-⊂ XK i for some set of agents K i , and such that}, with X IMi|. We refer to a Dec-MDP with sparse, observable interactions as a decentralized sparse-interaction MDP (Dec-dependent except in a set of M interaction areas, {X I|X IiSIMDP).1 , . . . , X I| (cid:11) |XK iDefinition 3.5 (Dec-SIMDP). Let M = (N, (Xk), (Ak), (Zk), P, (Ok), r, γ ) be a Dec-MDP with sparse observable interactions,encapsulated in a set of M interaction areas, {X I}, as described above. We represent such a decentralized sparse-interaction MDP (Dec-SIMDP) as a tuple1 , . . . , X IM(cid:4)Γ ={Mk, k = 1, . . . , N},(cid:21)(cid:4)where(cid:5), i = 1, . . . , M(cid:22)(cid:5),X Ii , MIi• each Mk is an MDP Mk = (X0 × Xk, Ak, Pk, rk, γ ) that individually models agent k in the absence of other agents,where rk is the component of the joint reward function associated with agent k in the decomposition in (9);• each MIiis an MMDP that captures a local interaction between K i agents in the states in X Ii and is given by MIi=(K i, XK i , (Ak), PIi , r Ii , γ ), with X Ii⊂ XK i .Each MMDP MIsuperset of an interaction area as defined above.i describes the interaction between a subset K i of the N agents, and the corresponding state space XK i is aA Dec-SIMDP is an alternative way of representing a Dec-MDP with observable interactions. For all agents outside in-teraction areas, the joint transition probabilities and reward function for a Dec-SIMDP can be factorized as in (7) and (9),and it is possible to model these agents using individual MDPs. In the states of each interaction area in a Dec-SIMDP, andonly in these, the agents involved in the interaction are able to communicate freely. In these areas, the agents can thus usecommunication to overcome local state perception and can be modeled using a local MMDP, observing each other’s stateand deciding jointly on their action. Outside these areas, the agents have only a local perception of the state and, therefore,choose the actions independently of the other agents. A simple, albeit inaccurate, way of thinking of a Dec-SIMDP is as aDec-MDP in which each agent has access, at each time step, to all state-information required to predict its next local stateand reward. In fact, a Dec-SIMDP may include states—namely in interaction areas—in which agents are able to perceive jointstate information, not strictly required to predict their next local state and reward. Hence the inaccuracy of the description.For illustration purposes, we revisit Example 1, using a Dec-SIMDP model to describe the corresponding navigationproblem.Example 1 (Cont.). The problem depicted in Fig. 2 can be represented as a Dec-SIMDP(cid:4)Γ ={M1, M2},(cid:4)X I , MI(cid:5)(cid:5),where• M1 is an MDP M1 = (X1, A1, P1, r1, γ ), describing the task of Robot 1 (navigating to Goal 1) in the absence of Robot 2.X1 and A1 are the same as in the definition of the original Dec-MDP (see page 1764), P1 and r1 arise from thedecomposition of r and P described in (10) and (11);• M2 is an MDP M2 = (X2, A2, P2, r2, γ ), describing the task of Robot 2 (navigating to Goal 2) in the absence of Robot 1;• X I is the interaction area already defined;• MI is an MMDP MI = (2, X , A, P, r I , γ ), that captures the interaction between Robot 1 and Robot 2 in X I .In this example, there is a unique MMDP MI since there is a single interaction area X I . Also, since there are only tworobots/agents in the environment, the MMDP MI is a fully observable version of the original Dec-MDP M with a simplifiedreward function. In more general scenarios, there will be an MMDP corresponding to each interaction area that includes onlythe agents involved in that interaction.Finally, we explore the relation between the Dec-SIMDP model and the MDP and MMDP models. As expected, in theabsence of any interaction areas, the Dec-SIMDP reduces to a set of independent MDPs that can be solved separately.A Dec-SIMDP with no interaction areas thus captures the situation in which the agents are completely independent. Inthose situations in which all agents interact in all states, as assumed in the general Dec-MDP model, the whole state spaceis an interaction area and, as such, our assumption of observable interactions renders our model equivalent to an MMDP.Nevertheless, the appeal of the Dec-SIMDP model is that many practical situations do not fall in either of the two extreme1768F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789cases i.e., independent MDPs vs. fully observable MMDP. It is in these situations that the Dec-SIMDP model may bring anadvantage over more general but potentially intractable models.We first assume interaction areas to be known in advance (i.e., they are provided as part of the model specification)and present a planning algorithm that leverages the particular structure of Dec-SIMDPs in Section 4. In Section 5, we thendiscuss how these areas can be determined and an algorithm that allows each agent to learn these interaction areas.4. Planning in Dec-SIMDPsWe address the problem of planning in Dec-SIMDPs, i.e., estimating the optimal policy for each agent in a Dec-SIMDPwhen the model is fully specified, including the interaction areas.We start by introducing a general heuristic approach that relies on the solution for an associated POMDP, leading to twogeneral algorithms, MPSI and LAPSI. We then introduce the concept of generalized α-vectors for Dec-SIMDPs and describeinstances of both MPSI and LAPSI that use generalized α-vectors, discussing the main properties of these methods. Thecomplete proofs of all results in this section are in Appendix A.4.1. Heuristic planning in Dec-SIMDPsWe start by considering a Dec-SIMDP in which all except one of the agents have full state observability. We refer tothis agent as agent k and further suppose that the remaining agents (those with full state observability) follow some fixedknown policy, π−k. Agent k can be modeled as a POMDP and the other agents can be collectively regarded as part of theenvironment. In this particular situation, any POMDP solution method can be used to compute the policy for agent k.Our heuristic departs from this simplified setting and computes a policy for each agent k as if all other agents had fullobservability and followed some fixed known policy π−k. This hypothesized policy π−k allows each agent k to approximatelytrack the other agents and choose its actions accordingly. The closer π−k is to the actual policy of the other agents, the betteragent k is able to track them and select its individual actions. In fact, even if the ability of an agent to track the other agentsnecessarily depends on the stochasticity of the environment,4 in our scenarios the determinant factor in the agents’ abilityto maintain a reliable (albeit flat) belief on the state of the other agents depends on how accurate the transition model forthe other agents is, which critically depends on how close π−k is to their actual policy.Algorithm 1 summarize this general algorithm:Algorithm 1 Heuristic planning algorithm for Dec-SIMDPsRequire: Dec-SIMDP model Γ ;1: for all agents k = 1, . . . , N do2:3:4:5: end forBuild hypothetical policy ˆπ−k ;Using Γ and ˆπ−k , build POMDP model for agent k;Use preferred POMDP solution technique to compute πk ;The idea behind Algorithm 1 can be used in general Dec-POMDPs. However, as the hypothesized policy π−k seldomcorresponds to the actual policy followed by the other agents, this method does not allow each agent k to properly track theother agents and decide accordingly, even if the environment is close to deterministic. Hence this approach is inadequate forgeneral Dec-POMDPs, where we expect it to lead to poor results. The particular structure of Dec-SIMDPs, however, rendersthis approach more appealing because (i) outside of interaction areas, the individual policy of agent k ideally exhibits littledependence on the state/policy of the other agents. As such, poor tracking in these areas has little impact on the policy ofagent k; and (ii) inside interaction areas, local full observability allows agent k to perfectly track the other agents involvedin the interaction and choose its actions accordingly.We present two algorithms, Myopic Planning for Sparse Interactions (MPSI) and Look-Ahead Planning for Sparse Interac-tions (LAPSI), that are particular instances of Algorithm 1 but consider different hypothetical policies for the other agents.In MPSI, agent k considers each of the other agents as completely self-centered and oblivious to the interactions. Agent kj (cid:12)= k, acts according to a policy π j —the optimal policy for the corresponding MDP M j in thethus acts as if each agent j,Dec-SIMDP. In environments with almost no interaction, the MPSI heuristic provides a good approximation to the policy ofthe other agents outside the interaction areas.In LAPSI, agent k considers that all other agents jointly adopt the optimal policy for the underlying MMDP.5 LAPSI isthe counterpart to MPSI, as it provides a good approximation to the policy of the other agents in scenarios where theinteractions are not so sparse.4 For example, in POMDPs with very stochastic transitions (meaning that transitions out of one state may lead to many states), the beliefs used fordecision-making are often flat, where many states have non-negligible probability.5 The MMDP associated with the Dec-SIMDP is the MMDP obtained by endowing all agents with global full-state observability.F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891769Using the corresponding hypothesized policies for the remaining agents, MPSI and LAPSI can now leverage any POMDPsolution methods to obtain a policy for each agent k. We now introduce the concept of generalized α-vectors, that we use toconstruct particular instances of both MPSI and LAPSI.4.2. Generalized α-vectors for Dec-SIMDPsMPSI and LAPSI are described in terms of general POMDP solvers and can be used to compute an individual policyfor each agent in a Dec-SIMDP. We now propose particular instances of both MPSI and LAPSI that exploit the structureof the Dec-SIMDP model. We depart from the single-agent POMDP model constructed using the proposed approach (seeAlgorithm 1) and introduce generalized α-vectors, a simplification of the α-vectors used in POMDP solvers [29]. Using thegeneralized α-vectors, we derive a variation of the Q MDP heuristic [23] and provide bounds for the performance of thismethod when applied to Dec-SIMDPs. We focus on 2-agent scenarios, to avoid unnecessarily complicating the presentation,remarking that the development presented extends easily to more than two agents only at the cost of more cumbersomeexpressions.Using the POMDP model obtained by adopting the introduced heuristic, agent k computes an individual policy πk thatmaps beliefs to actions. However, agent k has full local state observability, implying that the kth component of the state isunambiguously determined. Furthermore, given our assumption of observable interactions (Definition 3.4), at each time steponly those state components corresponding to agents not interacting with agent k are unobservable. By definition, thesestate-components do not depend on the state-action of agent k and depend only on π−k.Recovering the POMDP model for agent k (Section 2),∗Q(b, a) =(cid:2)bxx(cid:10)r(x, a) + γ(cid:2)z, yP(x, a, y)O( y, a, z) maxu(cid:11)(cid:5)Bel(b, a, z), u.(cid:4)∗QSince agent k has full local observability, the belief b concerns only the state component of the other agent,∗Q(¯xk, b−k, a) =(cid:2)x−kbx−k(cid:10)r(x, a) + γ(cid:2)z, yP(x, a, y)O( y, a, z) maxu(cid:4)∗Q¯yk, Bel(b, a, z), u(cid:5)(cid:11).The other agent is assumed to follow a fixed policy that depends only on the current state, so we can eliminate the explicitdependence on its action,∗Q(¯xk, b−k, ak) =(cid:2)x−kbx−kwhere(cid:10)rπ−k (x, ak) + γ(cid:2)z, yPπ−k (x, ak, y)O( y, a, z) maxukQ(cid:4)∗¯yk, Bel(b, ak, z), uk(cid:5)(cid:11),rπ−k (x, ak) =(cid:2)π−k(x, a−k)r(cid:4)(cid:5)x, (a−k, ak),a−kPπ−k (x, ak, y) =(cid:2)a−k(cid:4)π−k(x, a−k)P(cid:5)x, (a−k, ak), y.Every time step t that agent k is in an interaction area, implying that so is the other agent, agent k can unambiguouslyperceive their joint state and hence Zk(t) = X(t). In all remaining time steps, Zk(t) = ¯Xk(t), meaning that the agent observesonly its local state. Denoting by XI the set of all joint states in any interaction area, i.e., XI =(cid:23)Mi=1X Ii ,∗Q(¯xk, b−k, ak) =(cid:2)(cid:10)rπ−k (x, ak) + γbx−k(cid:2)y∈XIPπ−k (x, ak, y) maxuk∗Q( ¯yk, y−k, uk)x−k+ γ(cid:2)y /∈XIPπ−k (x, ak, y) maxukQ(cid:4)∗¯yk, Bel(b, ak, ¯yk), uk(cid:11)(cid:5).(13)We now focus explicitly on the updated belief Bel(·) and the optimal Q -function for the states in the interaction areas.The general belief-update expression is (Section 2.2)(cid:4)(cid:5)b, ak, Zk(t + 1)= ηBel y(cid:2)x(cid:4)(cid:5)bx(t)Px, A(t), yO(cid:4)(cid:5)y, A(t), Zk(t + 1).In the current setting, the belief concerns only the distribution over states of the other agent. As such, if Xk(t) = xk, then(cid:4)(cid:5)b, ak, Zk(t + 1)= ηBel y−k(cid:4)(cid:5)bx−k (t)Pπ−kx, Ak(t), y−kO(cid:4)(cid:5)y−k, Ak(t), Zk(t + 1),(cid:2)x−k1770F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789where x = (x−k, ¯xk),Pπ−k (x, ak, y−k) =(cid:2)a−kπ−k(x, a−k)P−k(cid:4)x−k, (a−k, a), y−k(cid:5)and, as in Section 2.4, P−k denotes the transition probabilities corresponding to all except the kth components of the state.If the agents are not in an interaction area, the transitions of the other agent do not depend on the actions of agent k andhence(cid:4)(cid:5)b, ak, Zk(t + 1)= ηBel y−kbx−k (t)Pπ−k (x, y−k)O(cid:4)(cid:5)y−k, Ak(t), Zk(t + 1).(cid:2)If X(t + 1) is in an interaction area, then agent k can observe the state of the other agent and, as such,x−kBel y−k (b, ak, y) = I X−k(t+1)( y−k),where Ix denotes the indicator function for the singleton set {x}. For the general situation in which X(t + 1) /∈ XI ,(cid:5)(cid:2)(cid:4)( y),(14)Bel y−k (b, ak, ¯yk) = ηbx−k (t)Pπ−kx, Ak(t), y−kx−kIX cIwhere, for a general set U ⊂ X , IU is the indicator function for the set U and U c denotes the complement of U in X .We now consider the expression for Q∗(¯xk, b−k, ak) when the agents are in an interaction area. In this case, b−k = ex−kfor some x−k, where ex is the probability vector where each component y is given by Ix( y). This implies that∗Q(¯xk, x−k, ak) = rπ−k (x, ak) + γPπ−k (x, ak, y) maxuk∗Q( ¯yk, y−k, uk)(cid:2)y∈XI+ γ(cid:2)y /∈XIPπ−k (x, ak, y) maxukQ(cid:4)∗¯yk, Bel(b, ak, ¯yk), uk(cid:5).(15)Noting the similarity between the right-hand side of (15) and the term in square brackets in the right-hand side of (13), wedefine a generalized α-vector for agent k, αk, recursively as follows:αk(x, ak) = rπ−k (x, ak) + γ(cid:2)Pπ−k (x, ak, y) maxuk(cid:2)αk( y, uk)IXI ( y)+ γ(cid:2)ykyP(¯xk, ak, ¯yk) maxuky−kPπ−k (x−k, y−k)αk( y, uk)IX cI( y).(16)Theorem 4.1. Given a two-agent Dec-SIMDP Γ , the generalized α-vectors associated with agent k when the other agent follows afixed and known policy π−k are well defined, i.e., they always exist and are unique.Proof. (sketch) The complete proof of the theorem is in Appendix A. We introduce a dynamic-programming operator Tksuch thatαk = Tkαkand show this operator to be a contraction in the sup-norm. The statement in the theorem follows from Banach’s fixed-pointtheorem. (cid:2)The operator Tk is used to iteratively compute the generalized α-vectors, in a way very similar to the value-iterationalgorithm used to compute the optimal Q -function for an MDP. The next result establishes that the generalized α-vectorsassociated with a Dec-SIMDP can be computed efficiently.Theorem 4.2. The generalized α-vectors for a 2-agent Dec-SIMDP Γ verifying the conditions of Theorem 4.1 can be computed inpolynomial time.Proof. (sketch) The result follows from noting that the generalized α-vectors can be computed by solving an associatedMDP. (cid:2)Given that an MDP is a particular case of a Dec-SIMDP in which there is a single agent, for such a Dec-SIMDP thegeneralized α-vectors correspond exactly to the optimal Q -values. It then follows from Theorem 4.2 that computing thegeneralized α-vectors for a Dec-SIMDP is P-complete.F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891771All results extend to scenarios with more than two agents. For example, the definition in (16) takes the more generalformαk(x, ak) = rπ−k (x, ak) + γ(cid:2)(cid:2)Pπ−k (xO , ak, y O ) maxukPπ−k (x−O , y−O )αk( y, uk),y−Owhere y = ( y O , y−O ), for y ∈ X . The components y O correspond to the observable components of y—those that belong toagents involved in an interaction with agent k—and y−O correspond to the remaining components.y O4.3. Generalized α-vectors in LAPSI and MPSIWe now use the generalized α-vectors to compute estimates ˆQ (b, ak) of the optimal Q -function for agent k,ˆQ (¯xk, b−k, ak) =(cid:2)bx−k αk(x, ak).x−k(17)This approximation shares several features with the Q MDP heuristic (Section 2.2), since we compute the generalized α-vectors for a Dec-SIMDP by solving an associated MDP. One would thus expect our methods to suffer in states of greatuncertainty, much like Q MDP does. In these states, action selection may be conservative but our assumed sparse interactionswill hopefully minimize the effects of this situation. Our experimental results indicate that this may indeed be the case.We now derive error bounds for the approximation in (17) that depend only on the dispersion of the maximum valuesof the generalized α-vectors outside the interaction areas. This result can be extended to general POMDPs, providing errorbounds for the Q MDP heuristic that depend only on the optimal Q -function for the underlying MDP.Given a generalized α-vector αk(x, ak), let xO denote the observable components of x—those corresponding to agentsinteracting with k at ¯xk—and x−O the remaining components. We define the dispersion of a set of α-vectors αk(x, ak), withx ∈ X and ak ∈ Ak, as(cid:6)(cid:6)(cid:6)(cid:6)maxuk(cid:3)k = maxxO(cid:2)(cid:4)αkx−O(xO , x−O ), uk(cid:2)(cid:5)−x−O(cid:4)αkmaxuk(cid:5)(xO , x−O ), uk(cid:6)(cid:6)(cid:6)(cid:6).(18)The dispersion of a set of α-vectors measures how the maximum value of αk taken over all actions differs from thecorresponding average in the non-observable components of the state. The dispersion quantifies how the lack of knowledgeof agent k on the state of the other agents can impact the action choice of agent k in terms of value.Theorem 4.3. Let M = ({Mk, k = 1, . . . , N}, {(X Iobtained from the approximation (17) when the policy π−k for the other agents is fixed and known. Then,i ), i = 1, . . . , M}) be a Dec-SIMDP and let πk denote the policy for agent ki , MI(cid:24)(cid:24)V∗ − V πk(cid:24)(cid:24)∞(cid:4) 2γ 2(1 − γ )2(cid:3)k,where (cid:3)k represents the dispersion of the α-vectors associated with π−k.Proof. See Appendix A. (cid:2)(19)Theorem 4.3 translates well-known bounds for approximate policies to our particular setting. As expected, the boundin (19) is proportional to the total dispersion of the generalized α-vectors. Also, the bound in (19) is zero if either• The whole state space is an interaction area, i.e., XI = X . In this case, we recover the MMDP version of the problem• There are no interaction states, i.e., XI = ∅. In this case, the generalized α-vectors for agent k do not depend on the(Section 3).other agents, implying that (cid:3)k = 0.Finally, the above bounds assume that the policy hypothesized for the other agents corresponds to their actual policy, whichis not the case in either MPSI and LAPSI. In particular, defining the errors in rπ−k and Pπ−k , arising from the imperfectknowledge of π−k, asεr = (cid:14)rπ−k− r ˆπ−k(cid:14)∞,εP = (cid:14)Pπ−k− P ˆπ−k(cid:14)∞,we finally get(cid:24)(cid:24)V∗ − V πk(cid:24)(cid:24)∞(cid:4) 2γ 2(1 − γ )2(cid:3)k + εr + γ εP1 − γ,(20)1772F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789where the first term corresponds to the approximation error of the POMDP policy, and the second term corresponds to theapproximation error of the policy of the other agents.The error bounds in (20) are generally loose—as known similar bounds for MDPs are also loose. However, in our view, themost interesting aspect of the proposed bounds arises from the definition and identification of the dispersion of generalizedα-vectors, on which those bounds critically depend.Using the generalized α-vectors in LAPSI and MPSI is now straightforward. Both methods use the estimate in (17) tochoose the action for agent k. The difference between the two methods lies on the policy π−k hypothesized for the otheragents, needed to both track the belief b−k and to compute the α-vectors. In MPSI, π−k is taken as the reduced policyj (cid:12)= k, the optimal policy for the corresponding MDP M j in the Dec-SIMDP. Inobtained from the individual policies π j ,LAPSI, π−k is obtained from the optimal policy for the underlying MMDP by ignoring component k.For illustration purposes, we apply both of these methods to several problems of different dimension, using (17) asour estimate for the POMDP optimal Q -function. As discussed above, the estimate in (17) corresponds to a variation of theQ MDP heuristic. Our results indicate that, even using such a sub-optimal POMDP solver, LAPSI is able to attain a performanceclose to optimal in all test scenarios while incurring a computational cost similar to that of solving the underlying MMDP.MPSI, while computationally more efficient, seems to lead to agents that excessively avoid interactions. We also compareour algorithms with a previous algorithm for Dec-SIMDPs, the IDMG algorithm [30]. Our results indicate that LAPSI is ableto attain similar performance to that of IDMG while providing significant computational savings. We also show that theIDMG algorithm is, by design, unable to consider future interactions when planning outside the interaction areas, whichleads to poor performance. Such limitation is not present in LAPSI.4.4. ResultsTo gain a better understanding of the applicability and general properties of our methods, we compared the performanceof both MPSI and LAPSI to those of individual agents that plan disregarding other agents in the environment and of theoptimal fully observable MMDP policy. We also tested the performance of the IDMG algorithm, previously introduced asan heuristic planning algorithm for Dec-SIMDPs [30]. It was empirically shown that this algorithm attains near-optimalperformance on several scenarios, and we use the results obtained with this approach as a benchmark against which wecompare the performance of LAPSI and MPSI. We briefly describe the IDMG algorithm and outline the main differencesbetween this approach and those presented in this article.4.4.1. The IDMG algorithmLet Γ = ({Mk, k = 1, . . . , N}, {(X Ii , MIi ), i = 1, . . . , M}) denote an N agent Dec-SIMDP. Let Qfunction associated with each of the MDPs Mk in Γ . Similarly, let Q Ithe MMDP MIifollows:in Γ . IDMG proposes to use these two sets of functions, {Q∗k denote the optimal Q -i denote the joint optimal Q -function associated withi , i = 1, . . . , M}, as∗k , k = 1, . . . , N} and {Q I• At every time step t for which an agent k is involved in no interaction, it follows the greedy policy with respect to Qi.e.,πk(xk) ∈ arg maxak∈AkQ∗k (xk, ak).• Otherwise, let O denote the set of all agents interacting with k at time step t. For each k(cid:7) ∈ O , and all x ∈ X , let∗k ,¯Q k(cid:7) (x, a) = Q∗k(cid:7) (¯xk(cid:7) , ak(cid:7) ) +M(cid:2)i (xK i , aK i )IXKiQ I(xK i ).i=1¯Q k(cid:7) (x, ·) combines QFor each x ∈ X ,state x.Since these functions are different for each agent kand constructs a matrix game(cid:4)Γ X(t) =(cid:5)|O |, (Ak(cid:7) ), (qk(cid:7) ),∗k(cid:7) (x, ·) with the joint Q -functions associated with all interactions of agent k(cid:7)in(cid:7) ∈ O , at each time-step t IDMG adopts a game-theoretic approachinvolving all agents in O . Each payoff function qk(cid:7) is given byqk(cid:7) (a O ) = rk(cid:7) (¯xk(cid:7) , ak(cid:7) ) +M(cid:2)i=1i (xK i , aK i )IXKir I(xK i ) + γ(cid:2)y O ∈XOP(xO , a O , y O )Nashk(cid:7)(cid:4)¯Q k(cid:7) ( y O , ·), k(cid:7) ∈ O(cid:5),where X(t) = x and Nashk(cid:7) is a max-like operator denoting the Nash value for agent kwith respect to the game definedby the matrices ¯Q k(cid:7) in state y O . Finally, at every time step t for which agent k is involved in some interaction, it choosesits action according to the Nash equilibrium for the game Γ X(t).(cid:7)F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891773Table 1Distinction between different methods used for comparison in terms of planning and prescribed policies.PolicyIDMGMPSILAPSIIndiv.Opt.PlanningEquilibriumMDP (hyp.)MMDP (hyp.)–MMDPIn int. areasEquilibriumPOMDPPOMDPMDPMMDPOutside int. areasMDPPOMDPPOMDPMDPMMDPThere are two important distinctions between the IDMG algorithm and LAPSI/MPSI. Outside interaction areas, IDMG isoblivious to the existence of other agents in the environment, i.e., it never considers the effect that future interactions mayhave in its performance. This, as will soon be apparent, leads to situations in which IDMG may perform arbitrarily poor.A second significant difference concerns the use of a game-theoretic solution to resolve the interactions that requires thecomputation of multiple equilibria. Computing equilibria is known to be a computationally demanding problem, which leadto a very significant computational advantage to LAPSI/MPSI, as will also become apparent from the results.Table 1 summarizes the main distinctions between the different methods used for comparison. For each method, thesecond column indicates which policy is hypothesized for the other agents during planning. The third column correspondsto the policy prescribed by that method in interaction areas, and the last column corresponds to the policy prescribedoutside interaction areas. During planning, IDMG assumes the other agents adopt an equilibrium solution in interactionareas and prescribes this same policy in those areas. Outside interaction areas, IDMG follows an individual MDP policythat disregards the other agents. During planning, LAPSI and MPSI assume the other agents to follow the joint MMDP andindividual MDP policies, respectively. Both methods then construct a POMDP for planning, whose policy is then followedinside and outside interaction areas. Individual agents do not consider other agents during planning and always follow anindividual MDP policy. The optimal agents assume that the other agents follow the joint MMDP policy during planning andthen adopt that same policy.4.4.2. Experimental setupFig. 3 depicts the different scenarios used to test our algorithm. The reason for using navigation scenarios is that theDec-SIMDP model appears particularly appealing for modeling multi-robot problems. Furthermore, in this class of problems,the results can easily be visualized and interpreted. In each of the test scenarios, each robot in a set of two/four robots mustreach one specific state. In the smaller environments (Fig. 3a through 3d), the goal state is marked with a boxed number,corresponding to the number of the robot. The cells with a simple number correspond to the initial states for the robots.In the larger environments (Fig. 3e through 3j), the goal for each robot is marked with a cross and the robots each departfrom the other robot’s goal state, in an attempt to increase the possibility of interaction.Each robot has 4 possible actions that move the robot in the corresponding direction with probability 0.8 and fail withprobability 0.2, leaving the state of the robot unchanged. The shaded regions correspond to interaction areas, inside of whichthe darker cells correspond to interaction states. When two or more robots stand in one of these cells simultaneously, theyboth get a penalty of −20. Upon reaching the corresponding goal, each agent receives a reward of +1. In all experimentswe used γ = 0.95.Table 2 summarizes the dimension of the joint state space for the corresponding Dec-MDP. For comparison purposes,Table 2 also includes the number of interaction states and the number of states in the individual MDPs that give an idea ofthe number of states in the corresponding Dec-SIMDP model.4.4.3. Description and discussion of resultsFor each of the different scenarios in Fig. 3, we run the algorithms in Table 1 and test the obtained policies for 1000independent Monte Carlo trials. Each trial lasts for 250 time steps.Except for the individual agents, all methods are able to completely avoid miscoordinations in all tested scenarios. Table 3summarizes the average number of miscoordinations for the individual agents. The results in Table 3 provide a hint on thecoordination needs for the different environments. For example, environments such as cit or suny require no coordinationat all, while Map 4 (Fig. 3d) requires significant coordination. Tables 4 and 5 present comparative results in terms of totaldiscounted reward and steps-to-goal. Performance results for the IDMG algorithm in Map 4 (Fig. 3d) are not available, sincethe complexity of this particular instance prevented the method from providing solutions in practical computational time.The LAPSI algorithm performs very close to the optimal MMDP policy in all environments, in spite of the significant dif-ference in terms of state information available to both methods. Also, in most scenarios, LAPSI and IDMG perform similarly,both in terms of total discounted reward and in terms of steps-to-goal. The only exceptions are Map 2, where LAPSI out-performs IDMG, and ISR, where IDMG outperforms LAPSI. Interestingly, however, the difference in terms of time-to-goal inthe ISR environment is not significant. Our results agree with previous ones that showed that IDMG attains close-to-optimalperformance in most scenarios considered here [30].Interestingly, in the ISR scenario alone, the individual agents are actually able to outperform the Dec-SIMDP planningmethods. Observing the results in terms of steps-to-goal, we conclude that in this particular scenario the Dec-SIMDP plan-ning methods take longer to reach the goal, an indication that they may be attempting to avoid miscoordinations at a cost of1774F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789Fig. 3. Environments used in the experiments.Table 2Dimension of the different test scenarios. The number of joint states in a scenario involving N agents in an environment described bya map with M cells is given by M N .Environment# Joint states# States in inter. areasMap 1Map 2Map 3Map 4citcmuisrmitpentagonsuny441129640065,536490017,6891849240127045476936181188153714203192113287# MDP states21 (×2)36 (×2)20 (×2)16 (×4)70 (×2)133 (×2)43 (×2)49 (×2)52 (×2)74 (×2)F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891775Table 3Number of miscoordinations for the individualagents in each of the test scenarios. The results areaveraged over 1000 independent Monte Carlo tri-als. For each environment, bold entries correspondto optimal values (differences from optimal are notstatistically significant).EnvironmentMap 1Map 2Map 3Map 4citcmuisrmitpentagonsunyIndiv.0.4090.2390.2801.9430.0000.2490.0040.0100.1200.000Table 4Total discounted reward for each of the four different algorithms in each of the test scenarios. The results are averaged over 1000 independent MonteCarlo trials. For each environment, bold entries correspond to optimal values (differences are not statistically significant). Italic entries correspond to valueswhose differences are not statistically significant.EnvironmentMap 1Map 2Map 3Map 4citcmuisrmitpentagonsunyIDMG12.03510.67213.722–11.1782.83914.1686.66316.03111.161MPSI11.13010.15913.24915.38411.1052.68813.9376.64115.16211.130LAPSI11.99210.94713.70115.56411.1262.82413.9976.64815.97611.139Indiv.5.9197.4239.378−4.74111.1200.98214.3016.62814.16711.144Opt.12.05911.10813.83716.44711.1282.84014.4076.70516.01611.149Table 5Steps-to-goal for each of the four different algorithms in each of the test scenarios. The results are averaged over 1000 independent Monte Carlo trials.For each environment, bold entries correspond to optimal values (differences are not statistically significant). Italic entries correspond to values whosedifferences are not statistically significant.EnvironmentMap 1Map 2Map 3Map 4citcmuisrmitpentagonsunyIDMG11.02113.3688.450−12.42239.3387.99322.5785.34812.448MPSI12.75214.4339.2826.08812.55249.3418.98624.50719.68412.500LAPSI11.09112.8288.4776.07112.51439.4448.01222.6185.41612.487Indiv.9.96412.4947.5296.29212.52638.8807.50022.3295.00812.477Opt.10.97712.5468.2675.00112.51239.3407.44022.4445.36512.469reaching the goal later, this slightly impacting their performance. However, in most other scenarios, the performance of theindividual agents is significantly worse than that of all other methods. The difference is more noticeable in those scenarioswhere coordination is more critical.Another interesting observation is that MPSI typically performs worse than the other methods. Since an agent in MPSIconsiders the other agents to disregard the consequences of miscoordinations (each is focused only on its individual goal),it is expected that the agent following MPSI is more “cautious” and hence the observed longer time to the goal. To seewhy this is so, consider for example the environment in Map 1, in which the two agents cannot simultaneously cross thenarrow passage. If the agents find each other in opposite sides of the passage, one of them must make way to the other. InMPSI, each agent k assumes that the other agent will not deviate and, as such, agent k will cautiously deviate. This meansthat, in this case, both agents will deviate. Both agents will eventually move across the narrow passage, but the cautiousdecision-process causes delays and impacts the total reward received, as seen in Table 5. In LAPSI the agents are implicitlyfollowing coordinated policies, so the delays observed in MPSI do not occur.In our results, the difference in performance between both LAPSI and IDMG and the optimal MMDP policy occurs bothin terms of total discounted reward and in terms of steps-to-goal. Given the discount factor γ , the latter in part explains1776F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789Fig. 4. Computation time for the different algorithms as a function of the problem dimension, corresponding to the number of joint states reported in thesecond column of Table 2.the former: if the agents take longer to reach their goal, the corresponding reward is further discounted. The above resultsthus indicate that both our algorithms and the IDMG algorithm require more time to reach the goal configuration than thatneeded by MMDP solution, and this time is spent in avoiding the penalties. Also, the choice of interaction areas greatlyinfluences the ability of the algorithms to avoid penalties without incurring any delays in reaching the goal.6Since IDMG requires the computation of several equilibria both in the off-line planning phase and in the on-line runningphase, the computational complexity of the IDMG algorithm may quickly become prohibitive, in scenarios with large action6 This was already reported in [30] concerning the IDMG algorithm.F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891777Fig. 5. Example scenario where avoiding the interaction may be beneficial.Table 6Estimated error bounds for the different environments.EnvironmentMap 1Map 2Map 3citcmuisrmitpentagonsunyMPSI7.2489.82613.01938.15398.05978.22954.70121.58654.856LAPSI3.6835.7176.43620.79244.58644.40632.70421.36928.723spaces and/or with many interaction areas. We compare the computational effort of our methods with that of IDMG, bothin terms of the average off-line computation time and the on-line computation time. Fig. 4 summarizes these results.Both MPSI and LAPSI are significantly more computationally efficient than the IDMG algorithm according to either of thetwo performance metrics. It is also interesting to note how the average computation times evolve with the dimension ofthe problem: while the computation time for both MPSI and LAPSI grows linearly with the dimension of the problem,the computation time of IDMG also depends on the number of interaction areas and hence the irregular growth patternobserved in Fig. 4.Finally, the IDMG method is, by construction, unable to consider future interactions when planning for the action in anon-interaction area. In this sense, the IDMG algorithm is “myopic” to such interactions and only handles these as it reachesan interaction area, which can have a negative impact on the performance of the method.Consider the scenario depicted in Fig. 5. Once again, the two robots must reach the marked states while avoiding simul-taneously crossing the narrow pathways. We model this problem using a Dec-SIMDP: the two sets of shaded cells representtwo interaction areas in which the robots only get a non-zero penalty by standing simultaneously in the darker state. In thisenvironment, and ignoring the interaction, Robot 1 can reach its goal by using either of the narrow pathways, since bothtrajectories have the same length. However, Robot 2 should use the upper pathway, since it is significantly faster than usingthe lower pathway.By using the IDMG algorithm, Robot 2 goes to the upper pathway while Robot 1 chooses randomly between the two,for example, the upper pathway. In this case, according to the IDMG algorithm, both robots reach the interaction areasimultaneously and Robot 1 must move out of the way for Robot 2 to go on. Therefore, Robot 1 takes 10 steps to reachGoal 1 while Robot 2 takes 8 steps to reach Goal 2, for an average of 9 steps to reach the goal. If, instead, Robot 1 takesthe lower pathway, the two robots reach their goal states in 8 steps each. The IDMG algorithm chooses between these twopossibilities randomly—or, at least, has no way to differentiate between the two. Therefore, IDMG agents take an averageof 8.5 steps to reach their goal. We ran 1000 independent trials using the IDMG algorithm in this scenario and, indeed,obtained an average of 8.485 steps-to-goal, with a standard deviation of 0.5.For comparison purposes, we also ran 1000 independent trials using the LAPSI algorithm in this same scenario. Out of1000 trials, Robot 1 always picked the lower pathway and the group took an average of 8 steps-to-goal with a variance of 0.This difference could be made arbitrarily large by increasing the “narrow doorway” to an arbitrary number of states, thuscausing an arbitrarily large delay.We conclude this section by providing Table 6 with the value of the bounds from Theorem 4.3 for some of the testenvironments. Although these bounds are generally loose, they still provide insights on some properties of our methods.The error bounds for MPSI are, in general, larger than those for LAPSI, since the policy estimate for MPSI is, in mostscenarios, more crude than that of LAPSI. Moreover, although the bounds exhibit some dependency on the dimension of theproblem—larger scenarios tend to have larger bounds,—this is not always the case. For example, while Map 3 is smaller thanMap 2, the corresponding bounds are actually larger. Also, ISR is much smaller than CMU but the corresponding bounds areapproximately similar.1778F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17895. Learning interaction areasIn the previous sections we introduced the Dec-SIMDP model and proposed the MPSI and the LAPSI planning algorithms,both able to leverage the sparse interactions in Dec-SIMDPs to overcome the computational burden associated with moregeneral decision-theoretic models. The Dec-SIMDP model relies on the concept of interaction areas that capture local de-pendences between groups of agents. These interaction areas include non-interaction states that nevertheless improve thecoordination performance of the agent group. We now introduce a learning algorithm that learns those interaction areas.We start by considering the simple case of a 2-agent transition-independent MMDP given by M = (2, X , (Ak), P, r, γ )where, as before, r is decomposable asr(x, a) = r1(x1, a1) + r2(x2, a2) + r I (x, a),where rk corresponds to the individual component of the reward function associated with agent k and r I is the joint com-ponent of the reward function. If r I ≡ 0, each agent can use standard Q -learning to learn its optimal individual policy π ∗k ,and the policy π ∗ = (π ∗2 ) is optimal for M. However, if r I (x, a) (cid:12)= 0 in some state x, the policy learned by each agentusing Q -learning and disregarding the existence of the other agent is generally suboptimal [31,32]. The optimal policy inthat case must take into account whatever the other agent is doing.1 , π ∗When moving to a Dec-MDP setting, the situation in which r I ≡ 0 poses no additional difficulties, as the problem can stillbe decomposed into independent single-agent MDPs. If the interaction between the two agents is sparse, we expect eachagent to coordinate only around those states x for which r I (x, a) (cid:12)= 0—the interaction areas. This coordination may requireeach agent to have access to state and action information about the other agent. Therefore, interaction areas should comprisethe states in which information on the joint state brings improvements in performance over local-state information.In general, the performance obtained using joint state information in all time steps is never worse than that obtainedusing this information only at certain situations. However, since we are interested in determining the interaction areas, weare only interested in states in which this information brings an improvement in performance. To this purpose, we addan artificial penalty every time the agent uses joint state information, to ensure that the latter is only used when actuallyuseful.We augment the individual action space of each agent with one pseudo-action, the Coordinate action, that incurs theaforementioned penalty and consists of two steps:1. An active perception step, in which the agent tries to determine the local state information of the other agent;2. A coordinating step, in which the agent makes use of the local state information from the other agent, if available, tochoose one of its primitive actions.The active perception step of the Coordinate action may not succeed; whether or not it actually succeeds is environmentdependent. Considering once again a multi-robot navigation scenario, the active perception step can consist for example ofthe use of an onboard camera to localize the other robot. In this case, the robot is able to localize the other robot onlywhen the latter is in its field-of-view. Another possibility consists of the use of explicit wireless communication, in whichone robot requires the other robot to share its location.Going back to our algorithm, each agent k uses standard Q -learning to estimate Qk (¯xk, ak) for all local states ¯xk andall ak ∈ Ak ∪ {Coordinate}. The role of the coordinating step is to use the local state information from the other agent,provided by the active perception step, to guide the actual choice of the actions in Ak. To this purpose, each agent k keepsan estimate of a second Q -function, the interaction Q -function Q Ik , defined in terms of the immediate reward of agent kk at different state-action pairsand the value of agent k’s policy. Since this policy is defined in terms of Qare independent, i.e.,∗k , the values of Q I∗k (x, a) = rk(x, ak) + γQ I(cid:2)¯yk∈Xkk(¯xk, ¯yk) maxPabk∈AkQ∗k ( ¯yk, bk).The above relation can be used in a Q -learning-like update to estimate the value Q Iak ∈ Ak in each joint state (x1, x2).k associated with each individual actionAlgorithm 2 summarizes our algorithm; π(cid:8) is the learning policy i.e., a policy that ensures sufficient exploration, suchas an ε-greedy policy, and by πg(Q , ·) the greedy policy with respect to function Q . The flag ActivePercept is true if the(cid:7)) denotes the general Q -learningactive perception step is successful and the general instruction QLUpdate(Q ; x, a, r, y, Qupdate defined in Section 2.1.The update of Q Ik uses the estimates of Qk . Hence, the values in Q I∗k andnot on Q Ik only determine the one step behavior of the Coordinate action, and therefore thereis not a direct dependency among entries in Q Ik corresponding to different states/actions (Fig. 6). Such independence isparticularly useful as it implies that, unlike algorithms that learn directly on the joint state-action space, our matrix Q Ik issparse—i.e., the number of non-zero elements is small—and thus requires a similar sample complexity as that necessary tolearn Q∗k , as the individual action at the next step depends on the values in Q∗k .F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891779else∗k and Q Ik ;ˆAk(t) = πg (Q IChoose Ak(t) using π(cid:8);if Ak(t) = COORDINATE thenif ActivePercept = TRUE thenAlgorithm 2 Learning algorithm for agent kRequire: Learning policy π(cid:8)1: Initialize Q2: Set t = 0;3: while (FOREVER) do4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20: end whileend ifSample Rk(t) and ¯Xk(t + 1);if ActivePercept = TRUE then; ¯Xk(t), Ak(t), Rk(t), ¯Xk(t + 1), Qend ifQLUpdate(Qt = t + 1;Sample Rk(t) and ¯Xk(t + 1);k , X(t));k , ¯Xk(t));QLUpdate(Q IkˆAk(t) = πg (Qend ifelse∗k∗∗k );; X(t), ˆAk(t), Rk(t), ¯Xk(t + 1), Q∗k );Fig. 6. Illustration of the dependence between the global Q -function, Q∗k , and the local Q -function, Q Ik .Also, the Coordinate action does not use any joint action information, only joint state information. This fact and theaforementioned independence of the components of Q Ik similar to ageneralized fictitious play process [33]. However, in scenarios where the action information from other agents is necessaryfor coordination, our algorithm may exhibit poor performance.k associated with different ¯xk makes the learning of Q ISo far we have described how to apply our learning algorithm to 2-agent MMDPs. In MMDPs with N > 2 agents, andsince the local state information from the other agents arises from an active perception step, each agent can only perceivethe local state information concerning one other agent. Therefore, in problems for which coordination requires state infor-mation concerning more than two agents, the performance of our algorithm is expected to decrease. Another implicationis that we can apply Algorithm 2 without modification, by disregarding the identity of the other agent and merely consid-ering that the local information obtained by active perception concerns some other agent. However, if we consider moregeneral active perception processes—i.e., the agent is actually able to perceive the state of all other agents—the algorithmcan be trivially modified to address coordination of any number of agents, with an obvious tradeoff in terms of the memoryrequirements of the algorithm.5.1. ResultsWe apply our learning algorithm to the test scenarios in Fig. 3, where we include an individual penalty of −0.1 in allscenarios every time an agent uses the Coordinate action. The excessive use of such Coordinate action impacts the sparsityof Q Ik , which is undesirable for large problems.We run our learning algorithm in each of the test environments. Table 7 summarizes the number of learning stepsallowed in each environments. During learning, exploration is ensured by combining a greedy policy with optimistic initialvalues [34]. We use an initial learning rate of α = 0.1. Every 2 × 104 time steps, this rate is updated as α ← 0.1 × α. Forcomparison purposes, we also run:• One instance of single-agent Q -learning for each robot, in the absence of other robots in the environment. The robotsthus learn how to reach their goal but not how to avoid miscoordination, since there are no other robots in the envi-ronment during the learning stage and hence the learner never actually experiences the miscoordination penalty.• One instance of single-agent Q -learning for each robot, in the presence of other robots in the environment. The robotsthus learn how to best reach their goal while avoiding interaction using only individual state information. Due to the1780F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789Table 7Number of time-steps used for learning in each of the testenvironments.EnvironmentLearning stepsMap 1Map 2Map 3citcmuisrmitpentagonsuny1041051045 × 1052 × 1055 × 1052 × 1053 × 1052 × 105Table 8Distinction between different learning approaches used for comparison. The second and third columns indicate whether learning andtest are conducted in the presence or absence of other agents. The last column indicates state observability.ApproachIndiv.Non-coop.Coop.Opt.LearningSingleMultipleMultipleMultipleTestMultipleMultipleMultipleMultipleState observabilityLocalLocalLocal + Active perc.GlobalTable 9Total discounted reward for each of the four different algorithms in each of the test scenarios. The results are averaged over 1000 independent Monte Carlotrials. For each environment, bold entries correspond to optimal values (differences are not statistically significant).EnvironmentMap 1Map 2Map 3Map 4citcmuisrmitpentagonsunyIndiv.2.8261.6075.470−3.1809.790−1.31910.6956.55414.67611.167Non-coop.3.0464.4877.0020.00011.0020.91014.2966.66612.91811.104Coop.6.3049.9577.1714.22911.1121.63014.2586.50216.55411.135Opt.12.05911.10813.83716.44711.1282.84014.4076.70516.01611.149general lack of knowledge about the global state, the agents in this second comparison set correspond to independentlearners in the sense of [31].Table 8 summarizes this information. The second and third columns show whether learning and testing are conducted inthe presence or absence of other agents (corresponding to the labels “Multiple” and “Single”, respectively). The last columnindicates whether the agents have access to only local state information, global state information or, in the case of ourmethod, local state information complemented by state information from the active perception step of the Coordinateaction.We evaluate each of the learned policies in the different environments, performing 1000 independent Monte Carlo trials.Each trial lasts for 250-time-steps, during which each learned policy (Indiv., Non-coop., and Coop.) is evaluated in theoriginal problem, i.e., with two/four robots moving simultaneously in the environment. For comparison, we also present theresults obtained with the optimal fully observable MMDP policy from Section 4. However, since the framework consideredhere is different from that considered in Section 4, the results from the optimal MMDP policy cannot be used for directcomparison, but only as a reference to better assess the performance of the learning methods. Table 9 presents the resultsin terms of total discounted reward.The results in Table 9 show that our algorithm generally outperforms the two other learning methods, matching theoptimal MMDP policy in some of the test scenarios. However, the behavior of the different algorithms varies in the differentscenarios. For example, individual agents never experience miscoordination penalties during learning. Therefore, they act asif such penalty does not exist, which impacts negatively the total discounted reward they receive as seen, for example, inMap 2. The non-cooperative agents experience miscoordination penalties during learning and are better able to avoid them,although their lack of joint state information prevents them from attaining optimal performance. In particular, in the Map 4environment, they adopt cautious policies that prefer avoiding penalties to attaining the agents’ goals. Such cautious policiesexplain the total discounted reward of 0.00 and the steps to goal.F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891781Table 10Steps-to-goal for each of the four different algorithms in each of the test scenarios. The results are averaged over 1000 independent Monte Carlo trials.For each environment, bold entries correspond to optimal values (differences are not statistically significant). Numbers in parentheses indicate that not allagents reached the goal.EnvironmentMap 1Map 2Map 3Map 4citcmuisrmitpentagonsunyIndiv.10.86212.9008.3038.24413.54240.1008.21624.1005.30012.441Non-coop.10.84112.8008.154> 250.0013.50039.8008.21124.6005.50012.558Coop.(10.821)17.500(8.467)(4.999)13.520(39.815)8.22625.3004.90012.492Opt.10.97712.5468.2675.00112.51239.3407.44022.4445.36512.539Table 11Number of crashes for the four different algorithms in each of the test scenarios. The results are averaged over 1000 independent Monte Carlo trials. Foreach environment, bold entries correspond to optimal values (differences are not statistically significant).EnvironmentMap 1Map 2Map 3Map 4citcmuisrmitpentagonsunyIndiv.0.4080.5000.3031.9110.0000.5000.0120.0000.1000.000Non-coop.0.3970.3000.2610.0000.0000.1000.0050.0000.2000.000Coop.0.0000.0000.0000.0000.0000.0000.0060.0000.0000.000Opt.0.0000.0000.0000.0000.0000.0000.0000.0000.0000.000Tables 10 and 11 show the number of steps-to-goal and the number of miscoordinations per trial for each of the testenvironments. Our algorithm exhibits nearly no miscoordinations in all environments, which means that the robots areable to coordinate in their choice of path. However, in some scenarios, this is attained at the cost of having only one ofthe agents reach the goal, corresponding to the values in parenthesis in Table 10. This indicates that, in these scenarios,coordination would require incurring in an excessive penalty arising from using the Coordinate action, and the agents optby sacrificing reaching one of the goals. This is an interesting aspect of our learning approach: the agents trade-off thecost of the Coordinate action by the usefulness of the information provided. This is in contrast with the individual agents:they always reach their goal in minimum time, but at a cost of severe miscoordination penalties that they never experienceduring learning, and hence their poor performance in terms of total discounted reward.Our results also confirm that interaction in the larger environments is much sparser than that in the smaller environ-ments. In some of the test scenarios (e.g., cit), the environment and the initial/goal positions for both robots are suchthat explicit coordination actions are not really necessary to avoid miscoordinations. Therefore, both the individualistic Q -learners and the non-cooperative Q -learners should be able to attain a good performance. In general, as seen in Tables 9through 11, there are some environments in which all methods attain similar performance, indicating that no coordinationactions are necessary, since both the Individual and Non-coordinated approaches are able to attain optimal performance.The fact that our algorithm attains a similar performance means that it learns that the Coordinate action is not necessary,otherwise the total discounted reward would be inferior to that of the other methods.We also analyze how the increasing penalties for miscoordination affect the performance of the robots in all scenarios.We run all three learning algorithms for different values of the miscoordination penalty and evaluated the obtained policies.As expected, as the penalty for miscoordination increases, the total discounted reward of those methods that are unableto avoid miscoordinations (Table 11) decreases accordingly, accentuating the differences already observed in Table 10. Thereliability of the performance of the individualistic and non-coordinated policies—translated in the variance of the observedperformance—is also severely affected as the miscoordination penalty increases.Finally, the non-cooperative Q -learning algorithm is able to surpass the individualistic Q -learning in several scenarios.This is particularly evident by observing the performance as the miscoordination penalty increases, and can be interpretedas the non-cooperative agents having to act with increasing caution due to the penalties experienced during learning.In conclusion, our proposed method is able learn to coordinate only when coordination is necessary to attain goodperformance. In several scenarios, our learning agents resort to coordination actions in those states where joint state infor-mation leads to improved performance, and these states corresponds precisely to the interaction areas defined in Section 3.In other scenarios, our agents are also able to learn to trade-off the benefits of coordination actions against the costs ofsuch actions.1782F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17896. Related workA wide range of models have been proposed to formalize decision-making problems in multiagent systems, includingstochastic games [35], multiagent MDPs [36], Dec-POMDPs [37,16], and I-POMDPs [38]. These models differ in scope, as-sumptions, and computational complexity [26]. For example, multiagent MDPs are can be solved in polynomial time, whilefinite-horizon Dec-MDP and Dec-POMDP models are NEXP-complete, even for the benign 2-agent scenarios, and complexityresults are even worse in non-cooperative settings.Efforts to handle the worst-case complexity of decentralized multiagent models led to approximate methods that trade-off optimality and computability (e.g., [39,40]), and reduced models that trade off representability and computability. Severalsuch models assume that the interaction among agents can be simplified in different ways. For example, in transition-independent Dec-MDPs, the transition probabilities for each agent depend solely on its own actions and local states.Transition independent Dec-MDPs are NP-complete and can be solved using the Coverage Set algorithm [41,27].Our Dec-SIMDP model can be viewed as an extension of transition-in dependent Dec-MDP, as it allows the transitionprobabilities for an agent to dependent on the actions of other agents in interaction areas, where joint state observabilityor free instantaneous communication is assumed. We expect the worst-case complexity of a Dec-SIMDP to be between thatof transition-independent Dec-MDPs and general Dec-MDPs.Local interactions have also been exploited in other multiagent scenarios. For example, several works propose theuse of hierarchical approaches that subdivide an overall task in a hierarchy of subtasks, each restricted to the statesand actions relevant to that particular subtask [5,42,4]. The subtasks are conducted individually by each agent and donot require the local state of different agents to be shared. Going up the hierarchy corresponds to moving from low-level “local” tasks to higher-level “global” tasks, in which coordination is necessary and must be accounted for explic-itly. Since execution at the highest level corresponds to several low-level time steps, communication needs are mini-mized.Coordination graphs [6,43] capture local dependences between the different agents in an MMDP, allowing the overall Q -function to be decomposed into local Q -functions that can be optimized individually. Coordination graphs have been usedfor efficient planning and learning in large multiagent MDPs [44]. Although the problems to which coordination graphshave been applied are significantly different from those we study, the interactions between agents captured by coordinationgraphs are related to our notion of interaction areas in our Dec-SIMPD model, and it would be interesting to investigate theuse of a graphical structure to compactly represent the dependencies among agents in a Dec-SIMDP.The coordination graph structure has also been learned from experience based on the concept of utile coordination [7].Although using a fundamentally different approach, this work relates to our learning of interaction areas in that bothmethods infer where joint state information can improve the performance of the agents. We differ in the fact that ouragents explicitly learn how to trade-off the benefits of querying the other agents’ states with the environment-imposedlimitations of this querying process and the associated cost. Our method captures the impact that communication costs mayhave both on the decision process and on the process of learning these inter-agent dependencies.Both hierarchical approaches and coordination graphs discussed above exploit local interactions among the agents andshould thus be able to accommodate some level of partial state observability.In the game-theoretic literature, local dependences between players in large games have also been explored. For example,graphical games [9] represent n-player matrix games as undirected graphs, where players correspond to nodes, and the payofffor each node depends only on the actions of its direct neighbors in the graph. Action-graph games further generalize theconcept of graphical games, exploring sparser dependences between players in a game [10]. Multiple algorithms have beenproposed to compute Nash equilibria in this class of games, mostly relying on “continuation methods” [45], where a knownsolution for a simple game is gradually perturbed toward a solution to the desired game. Continuation methods run in timeexponential in the in-degree of the action-graph, and not in its number of nodes. Therefore, games with many context-specific independences yield sparsely connected action-graph games, leading to exponential savings in computational time.In some classes of problems, additional structure of the corresponding action-graph games can lead even to more efficientcomputation of equilibria [46,47].The main concept of our Dec-SIMDP model was originally proposed under the designation of interaction-driven Markovgames [30], with the associated IDMG algorithm. Our contributions in this paper extend the original IDMG formulation inseveral aspects. We formalize the relation between Dec-MDPs and Dec-SIMDPs, introducing concepts such as interaction ar-eas and interaction states. We presented two general heuristic planning algorithms with corresponding convergence analysisand error bounds that overcome some limitations of the original IDMG algorithm: (i) outside interaction areas, both LAPSIand MPSI reason about future interactions, unlike IDMG which only considers interactions when they actually occur; (ii) ininteraction areas, LAPSI and MPSI also offer computational advantages over IDMG, since the latter requires the computationof several equilibria.Other closely related models to the Dec-SIMDP model are those that explore event-driven interactions [48–50], and dis-tributed POMDPs with coordination locales (DCPLs) [13]. Particularly in DCPLs, each agent is assumed independent of allother agents except on previously specified coordination locales. As in the IDMG algorithm, the proposed TREMOR algo-rithm for DCPLs models each agent k using a POMDP model that is solved to yield a policy πk for that agent. Coordinationlocales are handled by modifying the POMDP model for each agent taking the policies of the other agents into account.Instead, our approach assumes that interactions are fully observable, which can be used for coordination.F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891783More recently, an information-theoretic measure of inter-agent influence has been proposed under the designation ofinfluence gap [28], which indicates how much the actions of one agent determine the actions of the other agents in theoptimal policy. Such influence gap then attempts at quantifying the level of dependence between the different agents. Asexpected, larger influence gaps, corresponding to smaller inter-agent influence, typically translate into less computationalcomplexity. While the influence gap is a measure of global inter-agent influence, our interaction areas capture local interac-tions between the agents. Larger or numerous interaction areas typically lead to harder problems. As future work, it wouldbe interesting to relate the number or size of interaction areas in Dec-SIMDPs in terms of the proposed influence gap, usingthe latter to assess the potential usefulness of the Dec-SIMDP model in particular problems.7. Conclusion and future workIn this work, we analyzed how local interactions in a multiagent system can be used to simplify the process of decen-tralized decision making. We introduced Dec-SIMDPs, a new decision-theoretic model for decentralized multiagent systemswith sparse interaction that explicitly distinguishes the situations in which the agents in a team must coordinate fromthose in which they can act independently. We formalized the relation between Dec-MDPs and Dec-SIMDPs, establishingDec-SIMDPs as particular instances of Dec-MDPs. We presented MPSI and LAPSI, two general heuristic planning approachesthat reduce planning in a Dec-SIMDP to a sequence of planning problems in single-agent POMDPs. We analyzed particularinstances of MPSI and LAPSI and derived bounds for the quality of the obtained policies. Finally, we presented an RL algo-rithm that can be used to learn interaction areas in a Dec-SIMDP. We illustrated the application of all algorithms throughoutthe paper in several multiagent navigation scenarios.Both instances of MPSI and LAPSI used in the experimental results rely on having each agent track the other agents inthe environment using a belief vector that is then used to choose the actions. The difference between the two algorithmslies in the assumed policy for the other agents. MPSI assumes each of the other agents to be completely driven by itsindividual goals, thus discarding whatever interaction there may be. In the cases where these interactions are negative,MPSI agents then act more cautiously. In contrast, the LAPSI agent assumes that the other agents are team-players, in thatthey choose their actions for the common goal of the group. Hence, the LAPSI agent adopts a policy that is closer to theactual optimal fully-observable policy. The LAPSI algorithm successfully leverages the particular independences betweenthe different agents to attain efficient and yet near-optimal performance. In MPSI and LAPSI, these modeling strategies areused to abstract the decision process of each agent into a single-agent decision process—namely, a POMDP. Although weillustrated our methods using a Q MDP-like approach, the same principle can be used with any other POMDP solver.The differences between MPSI and LAPSI may provide additional information in defining the interaction areas. WhileMPSI relies on the optimal policies for the individual MDPs in the Dec-SIMDP model, LAPSI relies on the joint policy forthe underlying MMDP. Since outside interaction areas we expect the actions of the different agents to be approximatelyindependent, the interactions areas should be those in which the estimated policies using the individual MDPs and the jointMMDP disagree. This provides one recipe for choosing the interaction states as those in which individual state-informationis not sufficient to determine the best action.The results of learning algorithm open several interesting questions. One first issue concerns the dependence of theperformance of the algorithm on the cost of the Coordinate action. In fact, we observed that our agents are able to learna trade-off between the benefits arising from good coordination and the cost of that same coordination. Such trade-off issimilar to the problem of exchanging reward by information arising in POMDPs [22] and it would be interesting to analyzeboth how this trade-off is influenced by the cost of the Coordinate action and how such trade-off extends to situationswhere further partial state observability is considered.Another aspect open to future investigation regards the performance guarantees of the algorithm. As remarked inSection 5, the parallel learning process taking place when the agent executes the Coordinate action bears significant re-semblances to a generalized fictitious play. It would be interesting to analyze how the convergence guarantees of fictitiousplay can be translated to our particular learning algorithm. The scenarios used in our work, in which our learning algorithmexhibited interesting performance, were focused on localized coordination, based on the underlying assumption of the ad-vantages of our model and learning. As coordination increases to a more global level, the learning algorithm may need tobe adapted to attain the desired performance.In view of the completely independent way by which the agents learn, it remains to investigate if the learning algorithmwould be applicable to different reward functions for the different agents, or other scenarios in which the agents are notcompletely independent in terms of dynamics but exhibit some weak coupling—for example in the states where interactionoccurs. It would also be interesting to explore our ideas in more general models such as Dec-POMDPs, alleviating therequirement of full local state observability.AcknowledgementsThe authors would like to acknowledge the thorough comments of the anonymous reviewers that significantly con-tributed to improve the quality of the presentation, and the helpful discussions with Matthijs Spaan. This research waspartially sponsored by the project CMU-PT/SIA/0023/2009 under the Carnegie Mellon Portugal Program and its Informa-tion and Communications Technologies Institute, and the Portuguese Fundação para a Ciência e Tecnologia. The views andconclusions contained in this document are those of the authors only.1784F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789Appendix A. ProofsA.1. Proof of Theorem 4.1We show that the generalized α-vectors can be computed using a convergent dynamic-programming-like approach, byiterating over the recurrent expression in (16).In a Dec-SIMDP verifying the conditions of the theorem, a generalized α-vector αk is actually a |X | × |Ak| matrix withcomponent (x, ak) given by αk(x, ak). For a general matrix |X | × |Ak| matrix W , let Tk be the operator defined as(Tk W )(x, ak) = rπ−k (x, ak) + γPπ−k (x, ak, y) maxukW ( y, uk) + γ maxuk(cid:2)y∈XI(cid:2)Pπ−k (x, ak, y)W ( y, uk),y /∈XIwhere (Tk W )(x, ak) denotes the element (x, ak) of the matrix Tk W . The operator Tk is closely related with the Bellmanoperator H introduced in (3). We establish the assertion of the theorem by showing Tk to be a contraction in the supremumnorm. In fact, we have(cid:14)Tk W 1 − Tk W 2(cid:14)∞ = maxx,ak(cid:4) γ maxx,ak(cid:6)(cid:6)(cid:6)(Tk W 1)(x, ak) − (Tk W 2)(x, ak)(cid:6)(cid:2)yPπ−k (x, ak, y) maxuk(cid:6)(cid:6)(cid:6)W 1( y, uk) − W 2( y, uk)(cid:6),where the last inequality follows from Jensen’s inequality, implying that(cid:6)(cid:6)(cid:6)W 1(x, ak) − W 2(x, ak)(cid:6) = γ (cid:14)W 1 − W 2(cid:14)∞.(cid:14)Tk W 1 − Tk W 2(cid:14)∞ (cid:4) γ maxx,akWe have thus shown that Tk is a contraction in the supremum norm, which implies that• Tk has a unique fixed-point, corresponding to the generalized α-vectors;• Tk can be used to compute the generalized α-vectors in a dynamic-programming-like fashion, using the update ruleα(n+1)k(x, ak) =(cid:5)(cid:4)Tkα(n)k(x, ak),where α(n)k denotes the nth estimate of αk(x, ak). (cid:2)A.2. Proof of Theorem 4.2We show that the problem of computing the generalized α-vectors for a Dec-SIMDP verifying the conditions of thetheorem is equivalent in terms of complexity to that of solving an MDP whose dimension depends polynomially on thedimension of the original Dec-SIMDP. In particular, we show that computing the generalized α-vectors for such Dec-SIMDPis equivalent to computing the optimal Q -function for an MDP. Since MDPs are known to be P-complete [25], the desiredresult follows.We rewrite (16) asαk(x, ak) = rπ−k (x, ak) + γ(cid:2)y∈XIPπ−k (x, ak, y) maxuk(cid:2)αk( y, uk)+ γ(cid:2)y /∈XIPπ−k (x, ak, y) maxukηxakz /∈XIPπ−k (x, ak, z)αk(z, uk),(A.1)whereηxak=(cid:19)1y /∈XIPπ−k (x, ak, y).We now construct an MDP ˆM = ( ˆX , Ak, ˆP, ˆr), where ˆX = X ∪ X × Ak andˆP(ˆx, ak, ˆy) =⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩Pπ−k (ˆx, ak, ˆy)1/ηˆxak(cid:19)ηzukηzuk Pπ−k (z, uk, y)/η yak0y /∈XIPπ−k (z, uk, y)Pπ−k ( y, ak, ˆy)if ˆx ∈ X and ˆy ∈ XI ,if ˆx ∈ X and ˆy = (ˆx, ak),if ˆx = (z, uk) and ˆy ∈ XI ,if ˆx = (z, uk), ˆy = ( y, ak), and y /∈ XI ,otherwise.These probabilities are well defined since, for ˆx ∈ X ,(cid:2)y /∈XI(cid:26)ˆyand, for ˆx = (z, uk)(cid:2)F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891785(cid:2)ˆP(ˆx, ak, ˆy) =(cid:2)ˆy∈XIPπ−k (ˆx, ak, ˆy) + 1/ηˆxak=(cid:2)ˆy∈XIPπ−k (ˆx, ak, ˆy) +Pπ−k (ˆx, ak, y) = 1ˆP(ˆx, ak, ˆy) = ηzukˆy(cid:2)y /∈XI(cid:25) (cid:2)Pπ−k (z, uk, y)ˆy∈XIPπ−k ( y, ak, ˆy) + 1/η yak= ηzuk(cid:2)y /∈XIPπ−k (z, uk, y) = 1.The reward function for(cid:27)ˆM isrπ−k (ˆx, ak)(cid:19)ηzuky /∈XIˆr(ˆx, ak) =Pπ−k (z, uk, y)rπ−k ( y, ak)if ˆx ∈ X ,if ˆx = (z, uk).The optimal Q -function for this MDP verifies the recursive relation (2), namely:∗Q(ˆx, ak) = ˆr(ˆx, ak) + γˆP(ˆx, ak, ˆy) maxuk∗( ˆy, uk).Q(cid:2)ˆy∈ ˆXFor ˆx ∈ X , and replacing the definitions of ˆP and ˆr,∗Q(ˆx, ak) = rπ−k (ˆx, ak) + γ(cid:2)ˆy∈XIPπ−k (ˆx, ak, ˆy) maxuk∗Q( ˆy, uk) + γηˆxakQmaxuk∗(cid:4)(ˆx, ak), uk(cid:5).Similarly, for ˆx = (z, uk),∗Q(ˆx, ak) = ηzuk(cid:2)y /∈XIPπ−k (z, uk, y)(cid:10)rπ−k ( y, ak) + γ(cid:2)ˆy∈XIPπ−k ( y, ak, ˆy) maxuk∗( ˆy, uk)Q(cid:4)∗Q( y, ak), uk(cid:11)(cid:5)+ γη yak(cid:2)maxuk= ηzukPπ−k (z, uk, y)Q∗( y, ak).(A.2)(A.3)y /∈XIReplacing (A.2) in (A.3) yields∗Q(ˆx, ak) = rπ−k (ˆx, ak) + γ(cid:2)ˆy∈XIPπ−k (ˆx, ak, ˆy) maxuk∗Q( ˆy, uk) + γηˆxakmaxukηˆxak(cid:2)y /∈XIPπ−k (ˆx, ak, y)Q∗( y, uk),which is (A.1). As such, in computing the optimal Q -function for the MDP ˆM, we compute the generalized α-vectors forthe original Dec-SIMDP asαk(x, ak) = Q∗(x, ak).Since the dimension of the new MDP grows linearly with the dimension of the generalized α-vectors, and, hence, with thedimension of the corresponding Dec-SIMDP, the statement of the theorem follows. (cid:2)A.3. Proof of Theorem 4.3For a general MDP M = (X, A, P, r, γ ), if ˆπ is the greedy policy with respect to a function ˆQ , i.e., ifˆπ (x) = arg maxa∈AˆQ (x, a)for all x ∈ X , then(cid:24)(cid:24)∗ˆπ − V(cid:24)(cid:24)V(cid:4) 2γ(1 − γ )2∞BE( ˆQ ),(A.4)where BE( ˆQ ) is the Bellman error associated with the function ˆQ ,7(cid:2)(cid:6)(cid:6)(cid:6)(cid:6)r(x, a) + γBE( ˆQ ) = supx,a(cid:6)(cid:6)(cid:6)ˆQ ( y, u) − ˆQ (x, a)(cid:6).P(x, a, y) maxuy7 This fact follows from Proposition 6.1 of [18].1786F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789In our Dec-SIMDP setting, since we are assuming the policy π−k to be fixed and known, the decision process from agentk’s perspective is a standard POMDP. Since a POMDP can be recast as an equivalent belief MDP, it follows that the relation(A.4) also holds for POMDPs. Writing down the Bellman error for a general POMDP (X , A, Z, P, O, r, γ ) thus yieldsBE( ˆQ ) = supb,a(cid:10)r(x, a) + γ(cid:6)(cid:6)(cid:6)(cid:6)(cid:2)bxx(cid:2)z, yP(x, a, y)O( y, a, z) maxu(cid:5)(cid:4)ˆQ(cid:7)za, ub(cid:11)(cid:6)(cid:6)(cid:6)− ˆQ (b, ak)(cid:6).For simplicity of notation, we consider the Bellman error at (b, a) to beBE( ˆQ , b, a) =(cid:10)r(x, a) + γbx(cid:2)z, yP(x, a, y)O( y, a, z) maxu(cid:4)ˆQ(cid:7)za, ubFor the POMDP as perceived by agent k in our Dec-SIMDP setting,BE( ˆQ , b, ak) =(cid:10)rπ−k (x, ak) + γbx−k(cid:2)z, yPπ−k (x, ak, y)O( y, z) maxu(cid:11)(cid:5)(cid:6)(cid:6)(cid:6)− ˆQ (b, a)(cid:6).(cid:4)ˆQb(cid:7)zak, u(cid:11)(cid:5)(cid:6)(cid:6)(cid:6)− ˆQ (b, ak)(cid:6)which, replacing the definitions of ˆQ (b, ak) and the generalized α-vectors yields, leads toBE( ˆQ , b, ak) = γ(cid:2)xO , y O(cid:2)bxO Pπ−k (xO , ak, y O ) maxuk(cid:2)x−O , y−O(cid:2)bxO bx−O Pπ−k (xO , ak, y O ) maxuky−Ox, y Obx−O Pπ−k (x−O , y−O )αk( y, uk)(cid:6)(cid:6)(cid:6)Pπ−k (x−O , y−O )αk( y, uk)(cid:6),(cid:6)(cid:2)(cid:6)(cid:6)(cid:6)x(cid:6)(cid:2)(cid:6)(cid:6)(cid:6)x−k(cid:6)(cid:6)(cid:6)(cid:6)−using the notation introduced in Section 4.2. LettingΛk(x−O , y O , uk) =(cid:2)Pπ−k (x−O , y−O )αk( y, uk),y−OthenBE( ˆQ , b, ak) (cid:4) γ maxy O(cid:6)(cid:6)(cid:6)(cid:6)maxuk(cid:2)x−Obx−O Λ(x−O , y O , uk) −(cid:2)x−Obx−O maxuk(cid:6)(cid:6)(cid:6)(cid:6).Λ(x−O , y O , uk)In order to bound the right-hand side of the expression above, we need two auxiliary results that generalize some of thebounds in [51] to the case of functions defined over Rn and are of independent interest per se.Lemma 1. Let {xk, k = 1, . . . , M} be a set of points in Rn, for some (finite) n, and {βk, k = 1, . . . , M} a set of corresponding weights,verifying 0 (cid:4) βk (cid:4) 1, k = 1, . . . , M and(cid:19)k βk = 1. Let f : Rn → R be a convex function. Then, it holds that(cid:26)(cid:11)(cid:10)(cid:2)(cid:25)(cid:2)(cid:25)(cid:2)(cid:26)xk/M,(A.5)βkxk∗(cid:4) βf (xk) − M fkkk(cid:2)kβk f (xk) − fwhereβ∗ = maxkβk.Proof. The proof essentially follows that of Lemma 1 in [51]. Let k(cid:25)(cid:2)(cid:25)(cid:2)(cid:2)(cid:26)(cid:26)(cid:5)∗ − βkf (xk) + fβkxkk∗(cid:2) βM fxk/Mk(cid:4)βk(cid:12)=k∗∗be such that β∗ = βk∗ . Eq. (A.5) can be rewritten asβ∗ − βkMβ∗f (xk) + 1Mβ∗ f(cid:25)(cid:2)(cid:26)βkxk(cid:2) f(cid:25)(cid:2)(cid:26)xk/M.kkor, equivalently,(cid:2)Sincek(cid:12)=k∗(cid:2)k(cid:12)=k∗β∗ − βkMβ∗+ 1Mβ∗= 1,F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891787then(cid:2)k(cid:12)=k∗β∗ − βkMβ∗f (xk) + 1Mβ∗ f(cid:26)βkxk(cid:2) f(cid:25)(cid:2)k(cid:25) (cid:2)k(cid:12)=k∗β∗ − βkMβ∗ xk + 1Mβ∗(cid:2)k(cid:26)βkxk= f(cid:25)(cid:2)(cid:26)xk/M,kwhere the first inequality follows from Jensen’s inequality, implying (A.5). (cid:2)Corollary 1. Let {xi, i = 1, . . . , N} be a set of points in Rn, for some finite n, and {pi, i = 1, . . . , N} a corresponding sequence ofweights verifying 0 (cid:4) pi (cid:4) 1 andi pi = 1. Let U denote a closed convex set that can be represented as the convex hull of a set ofpoints {ak, k = 1, . . . , M} in Rn. Let f : U → R be a convex function. Then, it holds that(cid:19)(cid:2)ipi f (xi) − f(cid:25)(cid:2)(cid:26)pi xi(cid:4)i(cid:2)kf (ak) − M f(cid:25)(cid:2)(cid:26)ak/M.kProof. Each xi can be written as(A.6)xi =(cid:2)kλikak,with 0 (cid:4) λik (cid:4) 1 and(cid:19)k λik = 1, i = 1, . . . , n. Then,(cid:25)(cid:2)(cid:25)(cid:2)(cid:2)(cid:26)=pi fλikak(cid:2)ipi f (xi) − fpi xii(cid:26)− f(cid:25)(cid:2)(cid:2)(cid:26)piλikaki(cid:2)(cid:4)f (ak)k(cid:2)i(cid:25)(cid:2)k(cid:2)(cid:26)akpiλik.piλik − fkikiLetting βk =(cid:19)i piλik,(cid:2)ipi f (xi) − f(cid:25)(cid:2)(cid:26)pi xi(cid:4)iFinally, applying Lemma 1,(cid:25)(cid:2)(cid:2)pi f (xi) − f(cid:26)pi xi(cid:4)(cid:2)k(cid:2)βk f (ak) − f(cid:25)(cid:2)(cid:26)βkak.kβk f (ak) − f(cid:25)(cid:2)(cid:26)βkakii(cid:10)(cid:2)k∗(cid:4) βkk(cid:25)(cid:2)f (ak) − M f(cid:26)(cid:11)ak/M(cid:2)(cid:4)kf (ak) − M f(cid:25)(cid:2)k(cid:26)ak/M.k(cid:2)The two bounds differ as (A.5) depends on the function f and the set of points {xk, k = 1, . . . , M}, while (A.6) dependsonly on the function f and on the set U .∗For any given ukalpha-vectors αk( y, u∈ X−O and y∗O . Then, from Corollary 1,∗∈ XO , it holds that Λk(x−O , y∗O∗O , u∗k ) lies in the convex hull of the set of∗∈ Ak, x−O∗k ) where y O = y(cid:6)(cid:6)(cid:6)(cid:6)maxuk(cid:2)y−OBE( ˆQ , b, ak) (cid:4) γ maxy O(cid:4)αk( y O , y−O ), uk(cid:2)(cid:5)−y−O(cid:4)αkmaxuk( y O , y−O ), uk(cid:5)(cid:6)(cid:6)(cid:6)(cid:6),finally yielding(cid:24)(cid:24)V∗ − V πk(cid:24)(cid:24)∞(cid:4) 2γ 2(1 − γ )2maxy O(cid:6)(cid:6)(cid:6)(cid:6)maxuk(cid:4)αk(cid:2)y−O( y O , y−O ), uk(cid:2)(cid:5)−y−Omaxuk(cid:4)( y O , y−O ), uk(cid:5)αk(cid:6)(cid:6)(cid:6)(cid:6).(cid:2)1788F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–1789References[1] N. Meuleau, M. Hauskrecht, K. Kim, L. Peshkin, L. Kaelbling, T. Dean, C. Boutilier, Solving very large weakly coupled Markov decision processes, in: Proc.15th AAAI Conf. Artificial Intelligence, 1998, pp. 165–172.[2] S. Singh, D. Cohn, How to dynamically merge Markov decision processes, Advances in Neural Information Processing Systems 10 (1998) 1057–1063.[3] R. Nair, M. Tambe, Hybrid BDI-POMDP framework for multiagent teaming, Journal of Artificial Intelligence Research 23 (2005) 367–420.[4] P. Stone, M. Veloso, Team-partitioned, opaque-transition reinforcement learning, in: Proc. RoboCup-98, 1998, pp. 206–212.[5] M. Ghavamzadeh, S. Mahadevan, R. Makar, Hierarchical multiagent reinforcement learning, Journal of Autonomous Agents and Multiagent Sys-tems 13 (2) (2006) 197–229.[6] C. Guestrin, D. Koller, R. Parr, Multiagent planning with factored MDPs, Advances in Neural Information Processing Systems 14 (2001) 1523–1530.[7] J. Kok, P. Hoen, B. Bakker, N. Vlassis, Utile coordination: learning interdependencies among cooperative agents, in: IEEE Symp. Computational Intelli-gence and Games, 2005, pp. 61–68.[8] M. Roth, R. Simmons, M. Veloso, Exploiting factored representations for decentralized execution in multiagent teams, in: Proc. 6th Int. Conf. Au-tonomous Agents and Multiagent Systems, 2007, pp. 469–475.[9] M. Kearns, M. Littman, S. Singh, Graphical models for game theory, in: Proc. 17th Conf. Uncertainty in Artificial Intelligence, 2001, pp. 253–260.[10] A. Xin Jiang, K. Leyton-Brown, N. Bhat, Action-graph games, Tech. rep. TR-2008-13, Univ. British Columbia, 2008.[11] M. Allen, S. Zilberstein, Complexity of decentralized control: special cases, Advances in Neural Information Processing Systems 22 (2009) 19–27.[12] C. Goldman, S. Zilberstein, Decentralized control of cooperative systems: categorization and complexity analysis, Journal of Artificial Intelligence Re-search 22 (2004) 143–174.[13] P. Varakantham, J. Kwak, M. Taylor, J. Marecki, P. Scerri, M. Tambe, Exploiting coordination locales in distributed POMDPs via social model shaping, in:Proc. 19th Int. Conf. Automated Planning and Scheduling, 2009, pp. 313–320.[14] M. Puterman, Markov Decision Processes, Discrete Stochastic Dynamic Programming, John Wiley & Sons, Inc., 1994.[15] L. Kaelbling, M. Littman, A. Cassandra, Planning and acting in partially observable stochastic domains, Artificial Intelligence 101 (1998) 99–134.[16] D. Bernstein, R. Givan, N. Immerman, S. Zilberstein, The complexity of decentralized control of Markov decision processes, Mathematics of OperationsResearch 27 (4) (2002) 819–840.[17] C. Watkins, Learning from delayed rewards, Ph.D. thesis, King’s College, Cambridge Univ., 1989.[18] D. Bertsekas, J. Tsitsiklis, Neuro-Dynamic Programming, Athena Scientific, 1996.[19] R. Smallwood, E. Sondik, The optimal control of partially observable Markov processes over a finite horizon, Operations Research 21 (5) (1973) 1071–1088.[20] O. Madani, S. Hanks, A. Condon, On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems, in:Proc. 16th AAAI Conf. Artificial Intelligence, 1999, pp. 541–548.[21] A. Cassandra, Exact and approximate algorithms for partially observable Markov decision processes, Ph.D. thesis, Dept. Computer Sciences, Brown Univ.,1998.[22] D. Aberdeen, A (revised) survey of approximate methods for solving partially observable Markov decision processes, Tech. rep., National ICT Australia,2003.[23] M. Littman, A. Cassandra, L. Kaelbling, Learning policies for partially observable environments: scaling up, in: Proc. 12th Int. Conf. Machine Learning,1995, pp. 362–370.[24] F. Melo, M. Ribeiro, Transition entropy in partially observable Markov decision processes, in: Proc. 9th Int. Conf. Intelligent Autonomous Systems, 2006,pp. 282–289.[25] C. Papadimitriou, J. Tsitsiklis, The complexity of Markov decision processes, Mathematics of Operations Research 12 (3) (1987) 441–450.[26] S. Seuken, S. Zilberstein, Formal models and algorithms for decentralized decision-making under uncertainty, Journal of Autonomous Agents andMultiagent Systems 17 (2) (2008) 190–250.[27] R. Becker, S. Zilberstein, V. Lesser, C. Goldman, Solving transition independent decentralized Markov decision processes, Journal of Artificial IntelligenceResearch 22 (2004) 423–455.[28] M. Allen, Interactions in decentralized environments, Ph.D. thesis, Univ. Massachusetts, Amherst, 2009.[29] A. Cassandra, Optimal policies for partially observable Markov decision processes, Tech. rep. CS-94-14, Dept. Computer Sciences, Brown Univ., 1994.[30] M. Spaan, F. Melo, Interaction-driven Markov games for decentralized multiagent planning under uncertainty, in: Proc. 7th Int. Conf. AutonomousAgents and Multiagent Systems, 2008, pp. 525–532.[31] C. Claus, C. Boutilier, The dynamics of reinforcement learning in cooperative multiagent systems, in: Proc. 15th AAAI Conf. Artificial Intelligence, 1998,pp. 746–752.[32] M. Tan, Multi-agent reinforcement learning: independent vs. cooperative agents, in: Readings in Agents, Morgan Kaufman, 1997, pp. 487–494.[33] D. Leslie, E. Collins, Generalised weakened fictitious play, Games and Economic Behavior 56 (2006) 285–298.[34] R. Sutton, A. Barto, Reinforcement Learning: An Introduction, MIT Press, 1998.[35] L. Shapley, Stochastic games, Proceedings of the National Academy of Sciences 39 (1953) 1095–1100.[36] C. Boutilier, Planning, learning and coordination in multiagent decision processes, in: Theoretical Aspects of Rationality and Knowledge, 1996, pp. 195–210.[37] D. Pynadath, M. Tambe, The communicative multiagent team decision problem: analyzing teamworktheories and models, Journal of Artificial Intelli-gence Research 16 (2002) 389–423.[38] P. Gmytrasiewicz, P. Doshi, A framework for sequential planning in multiagent settings, Journal of Artificial Intelligence Research 24 (2005) 49–79.[39] R. Emery-Montemerlo, G. Gordon, J. Schneider, S. Thrun, Approximate solutions for partially observable stochastic games with common payoffs, in:Proc. 3rd Int. Conf. Autonomous Agents and Multiagent Systems, 2004, pp. 136–143.[40] M. Roth, Execution-time communication decisions for coordination of multiagent teams, Ph.D. thesis, Carnegie Mellon University, August 2007.[41] R. Becker, S. Zilberstein, V. Lesser, C. Goldman, Transition-independent decentralized Markov decision processes, in: Proc. 2nd Int. Conf. AutonomousAgents and Multiagent Systems, 2003, pp. 41–48.[42] R. Makar, S. Mahadevan, Hierarchical multiagent reinforcement learning, in: Proc. 5th Int. Conf. Autonomous Agents, 2001, pp. 246–253.[43] C. Guestrin, S. Venkataraman, D. Koller, Context-specific multiagent coordination and planning with factored MDPs, in: Proc. 18th AAAI Conf. ArtificialIntelligence, 2002, pp. 253–259.[44] J. Kok, N. Vlassis, Sparse cooperative Q -learning, in: Proc. 21st Int. Conf. Machine Learning, 2004, pp. 61–68.[45] N. Bhat, K. Leyton-Brown, Computing Nash equilibria of action-graph games, in: Proc. 20th Conf. Uncertainty in Artificial Intelligence, 2004, pp. 35–42.[46] A. Xin Jiang, K. Leyton-Brown, A polynomial-time algorithm for action-graph games, in: Proc. 21st AAAI Conf. Artificial Intelligence, 2006, pp. 679–684.[47] A. Xin Jiang, K. Leyton-Brown, Computing pure Nash equilibria in symmetric action-graph games, in: Proc. 22nd AAAI Conf. Artificial Intelligence, 2007,pp. 79–85.[48] R. Becker, V. Lesser, S. Zilberstein, Decentralized Markov decision processes with event-driven interactions, in: Proc. 3rd Int. Conf. Autonomous Agentsand Multiagent Systems, 2004, pp. 302–309.F.S. Melo, M. Veloso / Artificial Intelligence 175 (2011) 1757–17891789[49] R. Becker, V. Lesser, S. Zilberstein, Analyzing myopic approaches for multiagent communication, in: Proc. IEEE Int. Conf. Intelligent Agent Technology,2005, pp. 550–557.[50] R. Becker, A. Carlin, V. Lesser, S. Zilberstein, Analyzing myopic approaches for multiagent communications, Computational Intelligence 25 (1) (2009)31–50.[51] S. Simic, On a global upper bound for Jensen’s inequality, Journal of Mathematical Analysis and Applications 343 (2008) 414–419.