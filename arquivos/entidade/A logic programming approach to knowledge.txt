Artificial Intelligence 144 (2003) 157–211www.elsevier.com/locate/artintA logic programming approach to knowledge-stateplanning, II: The DLVK system ✩Thomas Eiter a, Wolfgang Faber a, Nicola Leone b,∗, Gerald Pfeifer a,Axel Polleres aa Institut für Informationssysteme, Technische Universität Wien, Favoritenstraße 9–11, A-1040 Wien, Austriab Department of Mathematics, University of Calabria, 87030 Rende (CS), ItalyReceived 10 January 2002AbstractIn Part I of this series of papers, we have proposed a new logic-based planning language, calledK. This language facilitates the description of transitions between states of knowledge and it is wellsuited for planning under incomplete knowledge. Nonetheless, K also supports the representation oftransitions between states of the world (i.e., states of complete knowledge) as a special case, provingto be very flexible. In the present Part II, we describe the DLVK planning system, which implements Kon top of the disjunctive logic programming system DLV. This novel planning system allows for solv-ing hard planning problems, including secure planning under incomplete initial states (often calledconformant planning in the literature), which cannot be solved at all by other logic-based planningsystems such as traditional satisfiability planners. We present a detailed comparison of the DLVK sys-tem to several state-of-the-art conformant planning systems, both at the level of system features andon benchmark problems. Our results indicate that, thanks to the power of knowledge-state problemencoding, the DLVK system is competitive even with special purpose conformant planning systems,and it often supplies a more natural and simple representation of the planning problems. 2002 Elsevier Science B.V. All rights reserved.Keywords: Deductive planning system; Disjunctive logic programming; Answer sets; Knowledge-states;Incomplete information; Conformant planning; Secure planning✩ A preliminary system description has appeared in: Thomas Eiter, Wolfgang Faber, and MirosławTruszczy´nski, editors, Proceedings ofthe 6th International Conference on Logic Programming andNonmonotonic Reasoning (LPNMR-01), Lecture Notes in Computer Science, Vol. 2173, Springer, 2001,pp. 429–433.* Corresponding author.E-mail addresses: eiter@kr.tuwien.ac.at (T. Eiter), faber@kr.tuwien.ac.at (W. Faber), leone@unical.it(N. Leone), pfeifer@dbai.tuwien.ac.at (G. Pfeifer), axel@kr.tuwien.ac.at (A. Polleres).0004-3702/02/$ – see front matter  2002 Elsevier Science B.V. All rights reserved.doi:10.1016/S0004-3702(02)00367-3158T. Eiter et al. / Artificial Intelligence 144 (2003) 157–2111. IntroductionThe need for modeling the behavior of robots in a formal way led to the definition oflogic-based languages for reasoning about actions and planning, such as [13,16–18,20,21,26,33,45]. These languages allow for specifying planning problems of the form “Find asequence of actions that leads from an initial state to a goal state”.A state is characterized by the truth values of a number of fluents, describing relevantproperties of the domain of discourse. An action is applicable only if some preconditions(formulas over the fluents) hold in the current state; executing this action changes thecurrent state by modifying the truth values of some fluents. Most of these languages arebased on extensions of classical logics and describe transitions between possible states ofthe world where every fluent necessarily is either true or false. However, robots usually donot have a complete view of the world. Even if their knowledge is incomplete (a numberof fluents may be unknown, e.g., whether a door in front of the robot is open), they musttake decisions, execute actions, and reason on the basis of their (incomplete) information athand. For example, if it is not known whether a door is open, the robot might do a sensingaction, or decide to push back.In [5,6], we have proposed a new language, K (where K should remind of states ofknowledge) for planning under incomplete knowledge. This language is very flexible, andis capable of modeling transitions between states of the world (i.e., states of completeknowledge) and reasoning about them as a particular case. Compared to similar planninglanguages, in particular Giunchiglia and Lifschitz’ action language C [17,26,29], K iscloser in spirit to answer set semantics [12] than to classical logics. It supports theexplicit use of default negation, and thus exploiting the power of answer sets to deal withincomplete knowledge. In [6] we have defined the syntax and semantics of K, discussedhow it can be used for knowledge representation, plus we have analyzed the computationalcomplexity of planning in K.In the present paper, which is Part II of this series of papers, we turn to theDLVK planning system, which implements K on top of the DLV answer set program-ming system [7,9]. DLVK is a powerful planning system, which is freely available at<URL:http://www.dbai.tuwien.ac.at/proj/dlv/> and ready-to-use for experiments. In com-parison to similar logic-based planning systems like CCALC [30,31], CPlan [10,15], orCMBP [4] DLVK has the following key features:• Explicit background knowledge: The planning domain has a background (representedby a stratified Datalog program) which describes static predicates.• Type declarations: The arguments of changeable predicates, called fluents, and actionatoms are typed.• Strong and weak negation: The DLVK system provides two kinds of negation familiarfrom answer set semantics, namely weak (or default) negation “not” and strong (orclassical) negation “¬”, also denoted by “–”. Weak negation allows for a simple andintuitive statement of inertia rules for fluents, or for the statement of default values forfluents in the domain.• Complete and incomplete states: By default, states in DLVK are consistent sets ofground literals, in which not every atom must appear, and thus represent statesT. Eiter et al. / Artificial Intelligence 144 (2003) 157–211159of knowledge. However, by suitable constructs, DLVK also allows for representingtransitions between possible states of the world (which can be seen as states ofcomplete knowledge).• Parallel/Sequential execution of actions: Simultaneous execution of actions is possi-ble, and in fact the default mode. All actions to be executed must qualify through anexecutability condition. Mutual exclusion of actions can be enforced in a sequentialplanning mode.• Secure (conformant) planning: DLVK is able to compute secure plans (often calledconformant plans in the literature [19,42]). Informally, a plan is secure, if it isapplicable starting at any legal initial state and enforces the goal, regardless of howthe state evolves. Using this feature, we can also model possible-worlds planning withan incomplete initial state, where the initial world is only partially known, and we arelooking for a plan reaching the desired goal from every possible world according tothe initial state.Main contributions. The main contributions of the present paper are the following:(1) We reduce planning in K to answer set programming by means of an efficienttransformation. Using this transformation, a planning problem in K is translated into an“equivalent” disjunctive logic program, which is then submitted to DLV for evaluation.The solutions of the original planning problem are obtained from the answer setsproduced by DLV, which correspond to the optimistic plans. The use of disjunctiverules in the transformation, which we use for natural problem modeling, can be easilyeliminated by using unstratified negation instead, and thus an adapted transformationcan be implemented on systems such as Smodels [35].(2) We discuss the issue of secure planning, alias conformant planning and its realizationin the DLVK system. Briefly, the system imposes a “security check” on optimistic plansin order to assess whether a plan is secure or not, which is transformed to a nestedcall to DLV itself. By the foundational results in [6], finding a secure plan is a (cid:2)P3 -hard1 problem, and such a two-step approach for secure planning (that is, first find anoptimistic plan and then check its security) is mandatory under polynomial reductionsto answer set programming, since DLV can only solve problems with complexity in (cid:2)P2with polynomial overhead.(3) We compare DLVK with the following state-of-the-art (conformant) planning systems:CCALC [30,31], CMBP [4], CPlan [10,15], GPT [3], and SGP [47].In particular, we first provide an overview of these systems comparing their mainfeatures. We then consider a number of benchmark problems, namely problems inthe blocksworld and “bomb in the toilet” domains, and discuss their encodings in thedifferent systems from the viewpoint of knowledge representation. Having conductedextensive experimentation, we report the execution times of the systems on a number of1 We use the common notion where (cid:2)Pnondeterministic Turing machine using an NP oracle, whereas (cid:2)Ppolynomially by a nondeterministic Turing machine using a (cid:2)P2 describes the class of problems solvable in polynomial time by a3 is the respectively problem class solvable2 oracle, and so on (cf. [36]).160T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211planning-problem instances and compare the performance of the systems. As it turnsout, thanks to the power of knowledge-state problem encodings DLVK can competeeven with special purpose conformant planning systems in the experiments, and itoften supplies a more elegant and succinct representation of the planning problems.This may be taken as promising evidence for the potential usefulness of knowledge-state problem encodings for conformant planning.To the best of our knowledge, DLVK is the first declarative logic-programming basedplanning system which allows solving (cid:2)P2 -hard planning problems like planning underincomplete initial states.The remainder of this paper is organized as follows: In the next section, we introducethe DLVK planning system at the user and system architecture levels. After that, we turnto the technical realization of DLVK, and discuss in Sections 3 and 4 the transformation ofDLVK planning problems to answer set programs, where the former section is devoted tooptimistic planning and the latter considers secure planning. After that, we compare theDLVK planning system to a number of other planning systems. Section 6 discusses furtherrelated work and presents an outlook to ongoing and future work.In order to alleviate reading, relevant definitions and notation from the foundationalPart I [6] are provided in Appendix A of the present paper.2. The planning system DLVKIn this section, we describe the DLVK planning system, which provides an imple-mentation of the language K as a front-end of the DLV system [7,9]. We first describehow planning problems are specified in DLVK, followed by the architecture of the sys-tem, and finally briefly the usage of DLVK. In order not to be abundant, we shall re-strict ourselves to a short exposition in which we focus on the essential facts. Fur-ther information can be found in the foundational paper [6] or on the DLVK web page<URL:http://www.dbai.tuwien.ac.at/proj/dlv/K/>.2.1. Planning problems in DLVKIn this section, we describe how planning problems can be represented as “programs”in the DLVK system. For this purpose, we shall consider an example in the well-knownblocksworld domain. DLVK programs are built using statements of the language K, plusfurther optional control statements. We shall not exhaustively repeat all details of K here,and in particular we shall not formally define the semantics of K. The details and the formaldefinition of the semantics of K, which we include in abbreviated form in Appendix A, canbe found in [6].A planning problem is a pair P = (cid:5)PD, q(cid:6) of a planning domain (informally, theworld of discourse) PD and a query q, which specifies the goal. A planning problem isT. Eiter et al. / Artificial Intelligence 144 (2003) 157–211161represented as a combination of a background knowledge Π, which is a stratified Datalogprogram (cf. Section 3.1), and a program of the following general form:fluents:actions:initially:always:goal:FDADIRCRqwhere the sections fluents through always are optional and may be omitted. Theyconsist of statements, described below, each of which is terminated by “.”. Togetherwith the background knowledge Π, they specify a K planning domain of form PD =(cid:5)Π, (cid:5)D, R(cid:6)(cid:6) (see Appendix A), where the declarations D are given by FD and AD andthe rules R by IR and CR .The statements in FD and AD are fluent and action declarations, respectively, whichtype the fluents and actions with respect to the (static) background predicates. They havethe formp(X1, . . . , Xn) requires t1, . . . , tm(1)where p is a fluent or action predicate of arity n (cid:1) 0, and the ti are classical literals, i.e.,an atom α or its negation ¬α (also denoted −α), over the predicates from the backgroundknowledge, such that every variable Xi occurs in t1, . . . , tm (as common, upper case lettersdenote variables). Only ground instances of fluents and actions which are “supported”by some ground instance of a declaration, i.e., the requires part is true, need to beconsidered.The initially-section specifies conditions that hold in an initial state (note that, ingeneral, the initial state may not be unique). They have the form of causal rules, which aredescribed next, without the after part.The always-section specifies the dynamics of the planning domain in terms ofcausation rules of the formcaused f if b1, . . . , bk, not bk+1, . . . , not blafter a1, . . . , am, not am+1, . . . , not an(2)where f is either a classical literal over a fluent or false (representing absurdity), thebi ’s are classical literals over fluents and background predicates, and the aj ’s are positiveaction atoms or classical literals over fluent and background predicates. Informally, therule (2) states that f is true in the new state reached by executing (simultaneously) someactions, provided that the condition of the after part is true with respect to the old stateand the actions executed on it, and the condition of the if part is true in the new state.Both the if- and after-parts are optional. Specifically, both can be omitted togetherwith the caused-keyword to represent simple facts.The always-section also contains executability conditions for actions, i.e., expressionsof the formexecutable a if b1, . . . , bk, not bk+1, . . . , not bl(3)162T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211Fig. 1. Sussman’s blocksworld planning problem.fluents :on(B, L) requires block(B), location(L).occupied(B) requires location(B).move(B, L) requires block(B), location(L).actions :initially : on(a, table). on(b, table). on(c, a).always :caused occupied(B) if on(B1, B), block(B).executable move(B, L) if B <> L.nonexecutable move(B, L) if occupied(B).nonexecutable move(B, L) if occupied(L).noConcurrency.caused on(B, L) after move(B, L).caused -on(B, L1) after move(B, L), on(B, L1), L <> L1.inertial on(B, L).on(c, b), on(b, a), on(a, table) ? (3)goal :Fig. 2. DLVK program for Sussman’s problem in the blocksworld domain PDbw .where a is an action atom and b1, . . . , bl are classical literals. Informally, such a conditionsays that a (well-typed) action is eligible for execution in a state, if b1, . . . , bk are knownto hold while bk+1, . . . , bl are not known to hold in that state.The goal-section, finally, specifies the goal to be reached, and has the formg1, . . . , gm, not gm+1, . . . , not gn ? (i)(4)where g1, . . . , gn are ground fluent literals, n (cid:1) m (cid:1) 0, and i (cid:1) 0 is the number of steps inwhich the plan must reach the goal.All rules in IR and CR have to satisfy the safety requirement for default negated typeliterals,2 i.e., each variable occurring in a default negated type literal has to occur in atleast one non-negated type literal or dynamic literal. Note that this safety restriction doesnot apply to action and fluent literals whose variables are already range restricted by therespective declarations.Example 2.1 (Sussman’s blocksworld planning problem). An example of a DLVK programis given in Fig. 2. It represents Sussman’s famous planning problem in the blocksworlddomain [44], depicted in Fig. 1, by which he showed anomalous behavior of STRIPSplanning.The blocksworld planning domain PDbw involves distinguishable blocks and a table.Blocks and the table can serve as locations on which other blocks can be put (a block2 These are literals corresponding to predicates defined in the background knowledge.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211163can hold at most one other block, while the table can hold arbitrarily many blocks).The background knowledge Πbw thus has predicates block and location defined asfollows:block(a). block(b). block(c).location(table).location(B) :- block(B).In the DLVK program, two fluents are declared for representing states: on(B,L), whichstates that some block B resides on some location L, and occupied(L), which is truefor a location L, if its capacity of holding blocks is exhausted. Furthermore, there is a singleaction predicate move(B,L), which represents moving a block B to some location L (andimplicitly removes that block from its previous location).With this fluent and action repertoire, we can describe the initial state and the causalrules as well as executability conditions guarding state transitions. As for the initial state,the configuration of blocks shown on the left in Fig. 1 is expressed by the three factson(a,table), on(b,table), and on(c,a). Note that only positive facts are statedfor on; nevertheless the initial state is unique because the fluent on is interpreted underthe closed world assumption (CWA) [40], i.e., if on(B,L) does not hold, we assume thatit is false.The values of the fluent occupied in the initial state are not specified explicitly, ratherthey are obtained from a general rule that applies to all states, and thus is part of thealways-section of the program (the first rule there). It says that a block B is occupied ifsomething (B1) is on it. Note that the rule does not apply to B = table, since the table issupposed to have unlimited capacity. Furthermore, B1 must be a block, by the declarationof the fluent on.Next we specify when an action move(B,L) is executable. The first condition statesthat this is possible if the block B and the target location L are distinct (a block cannot bemoved onto itself). The two negative conditions nonexecutable. . . state that the moveis not executable if either the block B or the target location L is occupied, respectively.These statements are shorthand macros for causation rules which interdict the execution ofan action (see Appendix A.3). Thus, the move is executable, if the positive condition holdsand both negative conditions fail.In the standard blocksworld setting, only one block can be moved at a time. Anothermacro, noConcurrency, enforces this. This macro is convenient for computingsequential plans, i.e., plans under mutual exclusion of parallel actions.The effects of a move action are defined by two dynamic rules. The first states that amoved block is on the target location after the move, and the second that a block is not onthe location from which it was moved, provided it was moved to a different location.The last statement in the always-section is an inertial statement for the fluent on,which is another macro (see Appendix A.3) informally expressing that the fluent shouldstay true, unless it explicitly becomes false in the new state.To solve Sussman’s problem, the query in the goal-section contains the configurationon the right side in Fig. 1, and furthermore, prescribes a plan length of 3 (which isfeasible).164T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211The semantics of planning domains is defined in terms of legal states and statetransitions. Informally, a state is any consistent collection of ground fluent literals whichrespect the typing information. It is a legal initial state, if it satisfies all rules in theinitially-section and the rules in the always-section with empty after part underanswer set semantics (cf. Section 3.1) if causal rules are read as logic programmingrules. A state transition is a triple t = (cid:5)s, A, s(cid:10)(cid:6) where s, s(cid:10) are states and A is a set oflegal action instances in PD, i.e., action instances that respect the typing information.Such t is legal, if the action set A is executable w.r.t. s, i.e., each action a in A is thehead of a clause (3) whose body is true, and s(cid:10) satisfies all causal rules (2) from thealways-section whose after part is true with respect to s and A under answer setsemantics.An optimistic plan for a goal g1, . . . , gm, not gm+1, . . . , not gn is now a sequenceof action sets (cid:5)A1, . . . , Ai(cid:6), i (cid:1) 0, such that a corresponding sequence T = (cid:5)(cid:5)s0, A1, s1(cid:6),(cid:5)s1, A2, s2(cid:6), . . . , (cid:5)si−1, Ai, si(cid:6)(cid:6) of legal state transitions (cid:5)sj −1, Aj , sj (cid:6) exists that leads froma legal initial state s0 to a state si which establishes the goal, i.e., {g1, . . . , gm} ⊆ si and{gm+1, . . . , gn} ∩ si = ∅. This sequence of legal state transitions is called trajectory, anda solution to a DLVK planning problem is an optimistic plan of length i specified in thegoal-section (cf. (4)).Example 2.2 (Sussman’s problem (continued)). A well-known solution to Sussman’sproblem consists of first moving block c onto the table, then moving b on top of a, andfinally moving c on top of b.In the DLVK setting, this amounts to the optimistic plan(cid:2){move(c, table)}, {move(b, a)}, {move(c, b)}(cid:1).We omit the description of the (unique) trajectory for this plan at this point; it will be givenin Section 2.3.2.1.1. Secure planningDLVK has a special statement “securePlan.” which may be specified before thegoal-section. It instructs the system to compute only secure plans, which are specialoptimistic plans. Note that securePlan is not a macro, and is, by complexity arguments,not expressible as a macro which can be expanded efficiently.Informally, an optimistic plan (cid:5)A1, . . . , An(cid:6) issecure, if it is applicable under anyevolution of the system: starting from any legal initial state s0, the first action set A1(i (cid:1) 1) can always be done (i.e., some legal transition (cid:5)s0, A1, s1(cid:6) exists), and for everysuch possible state s1, the next action set A2 can be done etc., and eventually, afterhaving performed all actions, the goal is always established. Secure plans are oftencalled conformant plans in the literature, and are considered in scenarios with incompleteinformation about initial states or nondeterministic action effects.Example 2.3 (Blocksworld with incomplete initial state). Let us consider a differentplanning problem in the blocksworld, illustrated in Fig. 3. Here, a further block d is present,whose exact location is unknown, but we know that it is not on top of c.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211165Fig. 3. A blocksworld planning problem with incomplete initial state.The background knowledge Πbw and the DLVK program for Sussman’s problem fromabove are modified as follows.For introducing block d to the planning domain, we add the fact block(d) to Πbwand the fact -on(d,c). to the initially-section of the DLVK program.Let us first consider the necessary extensions for handling cases in which the initial statedescription is incorrect (e.g., when completing the partial initial state description, incorrectinitial states can arise). The following conditions should hold for each block: (i) It is ontop of a unique location, (ii) it does not have more than one block on top of it, and (iii) it issupported by the table (i.e., it is either on the table or on a stack of blocks which is on thetable) [27].It is straightforward to incorporate conditions (i) and (ii) into the initially-section:initially : forbidden on(B, L), on(B, L1), L <> L1.forbidden on(B1, B), on(B2, B), block(B), B1 <> B2.Here, forbidden is a macro (cf. Section A.3) which amounts to a constraint.For condition (iii), we introduce a fluent supported, which should be true for anyblock in a legal initial state:fluents :supported(B) requires block(B).We then describe supported and include a constraint that each block must be supported.initially : caused supported(B) if on(B, table).caused supported(B) if on(B, B1), supported(B1).forbidden not supported(B).Now we modify the goal-section togoal :on(a, c), on(c, d), on(d, b), on(b, table) ? (4)and, finally, to obtain a plan that works under any possible location of block d in thebeginning, we use the total f macro of DLVK (defined in Appendix A.3), whichgenerates the two alternatives for the value of a fluent f:initially : total on(d, Y).In this way, all completions of on which satisfy the initial state constraints lead tolegal initial states; in fact, there are two such states, corresponding to on(d,b) andon(d,table).166T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211fluents :on(B, L) requires block(B), location(L).occupied(B) requires location(B).supported(B) requires block(B).actions :move(B, L) requires block(B), location(L).initially : on(a, table). on(b, table). on(c, a). -on(d, c).total on(d, Y).forbidden on(B, L), on(B, L1), L <> L1.forbidden on(B1, B), on(B2, B), block(B), B1 <> B2.caused supported(B) if on(B, table).caused supported(B) if on(B, B1), supported(B1).forbidden not supported(B).caused occupied(B) if on(B1, B), block(B).executable move(B, L) if B <> L.nonexecutable move(B, L) if occupied(B).nonexecutable move(B, L) if occupied(L).noConcurrency.caused on(B, L) after move(B, L).caused -on(B, L1) after move(B, L), on(B, L1), L <> L1.inertial on(B, L).on(a, c), on(c, d), on(d, b), on(b, table) ? (4)always :goal :Fig. 4. DLVK program for a variant of Sussman’s problem in an incomplete world.The rewritten DLVK program is depicted in Fig. 4, and using this program we are able tocompute the following solution, which is a secure plan:(cid:1){move(d, table)}, {move(d, b)}, {move(c, d)}, {move(a, c)}(cid:2)2.1.2. Knowledge-state vs. world-state planningKnowledge state planning in K offers some features which are not available inother planning languages. Recall that a knowledge state is a set of consistent fluentliterals, which describes the current knowledge about the planning world. The negationas failure construct allows for expressing defeasible rules and default conclusions, bywhich a more natural modeling of rational planning agents which have to deal withincomplete information becomes possible at a qualitative level. In fact, a knowledgestate describes more accurately the belief set of an agent about the world, which isformed by using strict and defeasible causal laws. This is in particular relevant if we areinterested in “reasonable” plans for achieving a goal. However, our framework is limitedto an elementary level, and does not directly allow for the representation of disjunctiveknowledge.A useful feature of knowledge-state planning is that it may allow for an elegant encodingof conformant planning problems with a world-state model in which the values of certainfluents remain open. In particular, this applies if world states are projected to subsets offluents of interest. This supports forgetting information and, to some extent, focusing byrestricting attention to those fluents whose value may have an influence on the evolutionof the world depending on the actions that are taken. The advantages of a knowledge-T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211167Fig. 5. DLVK system architecture.state encoding over a world-state encoding of the well-known “bomb in toilet” problem[34] are discussed in Appendix C. For further discussion of knowledge-state planning, see[6].2.2. System architectureThe architecture of the DLVK system is outlined in Fig. 5. It accepts files containingDLVK input and background knowledge stored as plain Datalog files. Then, by meansof suitable transformations from K to disjunctive logic programming that we willdescribe in Section 3, it uses the classic DLV core to solve the corresponding planningproblem.DLVK comes with two parsers: The first accepts DLVK files, that is, files with a filenameextension of .plan that constitute a DLVK program, while the second parser acceptsoptional background knowledge specified as stratified Datalog. Both parsers are able toread their input from an arbitrary number of files, and both convert this input to an internalrepresentation and store it in a common database.The actual DLVK front-end consists of four main modules, the Controller, the PlanGenerator, the Plan Checker, and the Plan Printer. The Controller manages the other threemodules; it performs user interactions (where appropriate), and controls the execution ofthe entire front-end.To that end, the Controller first invokes the Plan Generator, which translates theplanning problem at hand into a suitable program in the core language of DLV (disjunctivelogic programming under the answer set semantics as described in Section 3.1) accordingto the transformation lp(P) provided in Section 3.2. The Controller then invokes the DLVkernel to solve the corresponding problem. The resulting answer sets (if any) are fed backto the Controller, which extracts the solutions to the original planning problem from theseanswer sets, transforms them back to the original planning domain, and saves them intothe common database.The Controller then optionally (if the user specified the securePlan command orinvoked a secure check interactively) invokes the Plan Checker. Similarly to the PlanGenerator, the Checker uses the original problem description together with the optimisticplan computed by the Generator to generate a disjunctive logic program that solves theproblem of verifying whether this (optimistic) plan is in fact also a secure plan as intuitively168T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211introduced in Section 2.1.1 (details and the actual transformation employed by the PlanChecker will be provided in Section 4).The Plan Printer, finally, translates the solutions found by the Generator (and optionallyverified by the Checker) back into suitable output for the planning user and prints it.2.3. Using DLVKDLVK is a command-line oriented system, which is realized as front-end to the DLV logicprogramming system. It accepts two types of input files: (i) DLVK files, which carry thefilename extension .plan and contain DLVK code as described in Section 2.1; (ii) optionalbackground knowledge in the form of a stratified Datalog program, which is kept in fileswithout any filename extension.The planning front-end itself is invoked by means of the -FP family of command-lineoptions: -FP, -FPopt and -FPsec, followed by any number of DLVK files and filescontaining background knowledge.• -FP invokes the DLVK system in interactive mode, where an optimistic plan iscomputed and the user is then prompted whether to perform a security check for thatplan and whether to compute another (optimistic) plan, respectively.• -FPopt computes all optimistic plans in batch mode, without user intervention, while• -FPsec computes all secure plans (applying by default secure check SC1, as definedin Section 4) in batch mode.In all these cases, by means of the command-line option -n = x the number of planscomputed and printed can be limited to at most x; by default all possible plans arecomputed.Further DLVK command-line options which affectthe security checking will beintroduced at the end of Section 4.As an example, assume that the DLVK program for Sussman’s blocksworld planningproblem from Fig. 2 in Section 2.1 resides in a file blocksworld.plan, while thebackground knowledge about blocks and locations is saved in a file background.Invokingdlv -FP blocksworld.plan backgroundresults in the following output:DLV [build DEV/Dec 17 2001 gcc 2.95.3 (release)]STATE 0: occupied(a), on(a,table), on(b,table), on(c,a)ACTIONS: move(c,table)STATE 1: on(a,table), on(b,table), on(c,table), -on(c,a)ACTIONS: move(b,a)STATE 2: occupied(a), on(a,table), on(b,a), on(c,table),-on(b,table)ACTIONS: move(c,b)T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211169STATE 3: on(a,table), on(b,a), on(c,b), -on(c,table),PLAN:occupied(a), occupied(b)move(c,table); move(b,a); move(c,b)Check whether that plan is secure (y/n)? yThe plan is secure.Search for other plans (y/n)? yThis describes a successful trajectory (cid:5)(cid:5)s0, A1, s1(cid:6), (cid:5)s1, A2, s2(cid:6), (cid:5)s2, A3, s3(cid:6)(cid:6) wheres0, . . . , s3 correspond to the lines starting with STATE 0, . . . , STATE 3 in the outputabove, and A1, A2, and A3 correspond to the three ACTIONS lines; the entire plan is againprinted at the end.Now, let us consider the program from Fig. 4, that is, the variant of the Sussman problemwith an incomplete initial state. Let us assume that we have added the fact block(d)to the background knowledge and modified the file blocksworld.plan accordingly. Againinvoking DLVK as above will produce the following output:DLV [build DEV/Dec 17 2001 gcc 2.95.3 (release)]STATE 0: occupied(a), on(a,table), on(b,table), on(c,a),-on(d,c), supported(a), supported(b),supported(c), -on(d,a), on(d,table), -on(d,b),-on(d,d), supported(d)ACTIONS: move(c,table)STATE 1: on(a,table), on(b,table), on(c,table),on(d,table), -on(c,a)ACTIONS: move(d,b)STATE 2: on(a,table), on(b,table), on(c,table), on(d,b),-on(d,table), occupied(b)ACTIONS: move(c,d)STATE 3: on(a,table), on(b,table), on(c,d), on(d,b),-on(c,table), occupied(b), occupied(d)ACTIONS: move(a,c)STATE 4: on(a,c), on(b,table), on(c,d), on(d,b),-on(a,table), occupied(b), occupied(c),occupied(d)move(c,table); move(d,b); move(c,d); move(a,c)PLAN:Check whether that plan is secure (y/n)? yThe plan is NOT secure.Search for other plans (y/n)? y170T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211The first plan we arrive at is not secure, so we answer the question whether to search forother plans positively, and indeed find a secure plan (observe that initial states (STATE 0)are larger here because of the total statement for on(d,Y)):STATE 0: occupied(a), on(a,table), on(b,table), on(c,a),-on(d,c), supported(a), supported(b),supported(c), -on(d,a), on(d,table), -on(d,b),-on(d,d), supported(d)ACTIONS: move(d,c)STATE 1: on(a,table), on(b,table), on(c,a), on(d,c),-on(d,table), occupied(a), occupied(c)ACTIONS: move(d,b)STATE 2: on(a,table), on(b,table), on(c,a), on(d,b),-on(d,c), occupied(a), occupied(b)ACTIONS: move(c,d)STATE 3: on(a,table), on(b,table), on(c,d), on(d,b),-on(c,a), occupied(b), occupied(d)ACTIONS: move(a,c)STATE 4: on(a,c), on(b,table), on(c,d), on(d,b),-on(a,table), occupied(b), occupied(c),occupied(d)move(d,c); move(d,b); move(c,d); move(a,c)PLAN:Check whether that plan is secure (y/n)? yThe plan is secure.Search for other plans (y/n)?While looking for further secure plans, we encounter several optimistic plans, none ofwhich is secure, so we change our strategy and invoke DLVK with the -FPsec optioninstead of using the interactive mode enabled with -FP. This yield the following result:DLV [build DEV/Dec 17 2001 gcc 2.95.3 (release)]PLAN: move(d,c); move(d,b); move(c,d); move(a,c)PLAN: move(d,table); move(d,b); move(c,d); move(a,c)Indeed, while there are many optimistic plans, there is only just a single further secureplan in addition to the one we already found. Note that, as secure plans usually have manydifferent trajectories, DLVK only prints the plans themselves, omitting the information onstates.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–2111713. Transforming optimistic planning to answer set programmingIn this section, we discuss how planning problems in DLVK are transformed into answerset programs. We consider here optimistic planning, and deal with secure planing inthe next section. As a preliminary, we first recall some concepts of (disjunctive) logicprogramming.3.1. Disjunctive logic programmingWe consider extended disjunctive logic programs with two kinds of negation like in theK language, i.e., weak negation “not” and strong negation “¬”, as introduced in [12]over a function-free first-order language. Strings starting with uppercase (respectively,lowercase) letters denote variables (respectively, constants). A positive (respectivelynegative) classical literal l is either an atom a or a negated atom ¬a, respectively; itscomplement, denoted ¬l, is ¬a and a, respectively. A positive (respectively, negative)failure (NAF) literal # is of the form l or not l, where l is a classical literal. Unlessstated otherwise, by literal we mean a classical literal.A disjunctive rule (rule, for short) R is a formulaa1 v . . . v an :- b1, . . . , bk, not bk+1, . . . , not bm,(5)where all ai and bj are classical literals and n (cid:1) 0, m (cid:1) k (cid:1) 0. The part to theleft (respectively, right) of “:-” is the head (respectively, body) of R, where “:-”is omitted if m = 0. We let H (R) = {a1, . . . , an} be the set of head literals andB(R) = B+(R) ∪ B−(R) the set of body literals, where B+(R) = {b1, . . . , bk} andB−(R) = {bk+1, . . . , bm}. A constraint is a rule with empty head (n = 0).A disjunctive logic program (DLP) P (simply, program) is a finite set of rules. It ispositive, if it is not-free (i.e., ∀r ∈ P : B−(r) = ∅), and normal, if it is v-free (i.e.,∀R ∈ P : |H (R)| (cid:2) 1). A normal program is also called a Datalog program. As usual,a term (atom, rule etc.) is ground, if no variables appear in it. A ground program is alsocalled propositional.Answer sets of DLPs are defined as consistent answer sets for EDLPs as in [12,25].That is, for any program P , let UP be its Herbrand universe and BP be the Herbrand baseof P over UP (if no constant appears in P , an arbitrary constant is added to UP ). Letground(P ) =R∈P ground(R) denote the grounding of P , where ground(R) is the set ofall ground instances of R.(cid:3)Then, an interpretation is any set I ⊆ BP of ground literals. An answer set of a positiveground program P is any consistent interpretation I , i.e., I ∩ {¬l | l ∈ I } = ∅, such thatI is the least (w.r.t. set inclusion) set closed under the rules of P , i.e., B(R) ⊆ I impliesH (R) ∩ I (cid:20)= ∅ for every R ∈ P .3 An interpretation I is an answer set of an arbitrary groundprogram P , if it is an answer set of the Gelfond–Lifschitz reduct P I , i.e., the programobtained from P by deleting3 We only consider consistent answer sets, while in [12,25] also the (inconsistent) set BP may be an answerset. Technically, we assume that negative classical literals ¬a are viewed as new atoms –a, and constraints :-a, –aare implicitly added. This is the standard way how true negation is implemented in systems like DLV or Smodels.172T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211• all rules R ∈ P such that B−(R) ∩ I (cid:20)= ∅, and• all negative body literals from the remaining rules.The answer sets of a non-ground program P are those of its ground instantiationground(P ). We shall denote by AS(P ) the set of all answer sets of any program P .3.2. Transformation lp(P)The main building-block underlying the realization of the DLVK system is the translationof a DLVK planning problem P, given by a background knowledge Π and a DLVK programas in Section 2.1, into a logic program lp(P), whose answer sets represent the optimisticplans of P. For the sake of our translation, we extend fluent and action literals by atimestamp parameter T such that an answer set AS of the translated program lp(P)corresponds to a successful trajectory T = (cid:5)(cid:5)s0, A1, s1(cid:6), . . . , (cid:5)sn−1, An, sn(cid:6)(cid:6) of P in thefollowing sense:• The fluent literals in AS having timestamp 0 represent a (legal) initial state s0 of T .• The fluent literals in AS having timestamp i > 0 represent the state si obtained afterexecuting i many action sets (i.e., they represent the evolution after i steps).• The action literals in AS having timestamp i represent the actions in Ai+1 (i.e., thoseactions which are executed at step i + 1).Moreover, trajectories encoded in the answer sets of lp(P) are guaranteed to establish thegoal of the planning problem, and the underlying sequence of action sets is therefore anoptimistic plan.In the following, we incrementally describe a transformation from a planning problemP to a logic program lp(P). We will illustrate this transformation on the blocksworldplanning problem from Section 2.1.In what follows, let σ act, σ fl, and σ typ be the sets of action, fluent and type names,act, Lfl, and Ltyp be the set of all action, positive action, fluent,act (dynrespectively, and let Lact, L+and type literals, respectively. Furthermore, Lfl,typ = Lfl ∪ Ltyp and Ldyn = Lfl ∪ L+stands for dynamic literals).Step 0 (Macro expansion). In a preliminary step, replace all macros in the DLVK-programby their definitions (cf. Appendix A.3).Example. In the encoding of Sussman’s problem, among others the macrosalways : nonexecutable move(B,L) if occupied(B).inertial on(B,L).are replaced bycaused false after move(B,L), occupied(B).on(B,L) if not -on(B,L) after on(B,L).T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211173Step 1 (Background knowledge). The background knowledge Π is already given as a logicprogram; all the rules in Π can be directly included in lp(P), without further modification.Step 2 (Auxiliary predicates). To represent steps, we add the following facts to lp(P)time(0)., . . . , time(i).next(0, 1)., . . . , next(i − 1, i).where i is the plan length of the query q = G?(i) ∈ P at hand.The predicate time denotes all possible timestamps and the predicate next describesa successor relation over the timestamps in our program.Note that we refrain from using built-in predicates of a particular logic programmingengine here. In the DLVK implementation, all above auxiliary predicates are efficientlyhandled in a preprocessing step.Example. For Sussman’s problem, where q = on(c, b), on(b, a), on(a, table)? (3),we add the following facts:time(0).time(1).time(2).time(3).next(0,1).next(1,2).next(2,3).Step 3 (Causation rules). For each causation rule r:caused H if B after A in CR,we include a rule r (cid:10) into lp(P) as follows:(cid:4)(cid:10)h(r) =if H = false,if H = f (¯t), f ∈ σ fl,where T1 is a new variable. To the body of r (cid:10), we add the following literals:∅,f (¯t, T1),• each default type literal in r, i.e., (not)l ∈ A ∪ B where l ∈ Ltyp;• (not) b(¯t, T1), where (not) b(¯t) ∈ B and b(¯t) ∈ Lfl;• (not) b(¯t, T0), where (not) b(¯t) ∈ A and b(¯t) ∈ Ldyn, where T0 is a new variable;• for timing, we add– time(T1), if A is empty;– next(T0,T1), otherwise.• To respect typing declarations and to establish standard safety of r (cid:10), we add for anyaction/fluent literal in H and default negated fluents/actions literals typing informationfrom the corresponding action/fluent declarations. That is, if H = (¬)p(¯t) respectivelynot (¬)p(¯t) ∈ A ∪ B such that p(¯t) ∈ Ldyn,p(Y ) requires t1(Y1), . . . , tm(Ym)is an action/fluent declaration (standardized apart), and θ is a substitution such thatθ (Y ) = ¯t , then we add θ (t1(Y1)), . . . , θ (tm(Ym)) to the body for r (cid:10). If p has multipleaction/fluent declarations, each of them is considered separately, which gives rise tomultiple typed versions of r (cid:10).174T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211Example. In our encoding of Sussman’s problem, the statementalways : caused occupied(B) if on(B1, B), block(B).leads to the following rule in lp(P):occupied(B, T1):- on(B1, B, T1), block(B), location(B), time(T1).Here, the timing atom time(T1) is added, and the type information location(B) forthe fluent occupied(B) in the H -part of the statement.Step 4 (Executability conditions). For each executability condition e of the formexecutable a(¯t) if B in CR, we introduce a rule e(cid:10) in lp(P) as follows:(cid:10)h(e) = a(¯t, T0) v ¬a(¯t, T0),where T0 is a new variable. To the body of e(cid:10), we add the following literals:• Each default type literal in e, i.e., (not)l ∈ B where l ∈ Ltyp;• (not) b(¯t, T0), where (not) b(¯t) ∈ B and l ∈ Ldyn;• next(T0,T1) where T1 is a new variable;• for typing and safety, type information literals for a(¯t) and every default literalnot (¬)p((cid:22)t ) ∈ B such that p((cid:22)t ) ∈ Ldyn, similar as in Step 3 (which may lead tomultiple rules e(cid:10)).Example. In our running example, the conditionexecutable move(B,L) if B <> L.introduces in lp(P) the rulemove(B, L, T0) v − move(B, L, T0):- B <> L, block(B),location(L), next(T0, T1).Here, type information block(B), location(L) is added for move(B,L).Step 5 (Initial state constraints). Initial state constraints in IR(cid:10) are transformed like staticcausation rules r in Step 3 (i.e., A is empty), but we use the constant 0 instead of thevariable T1 and omit the literal time(0).Example. The facts in IR :initially: on(a,table). on(b,table). on(c,a).become:on(a,table,0). on(b,table,0). on(c,a,0).T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211175Step 6 (Goal query). Finally, the query q:goal : g1(t1), . . . , gm(tm), not gm+1(tm+1), . . . , not gn(tn) ? (i).is translated to:goal_reached:- g1(t1, i), . . . , gm(tm, i), not gm+1(tm+1, i), . . . , not gn(tn, i).:- not goal_reached.where goal_reached is a new predicate symbol.Example. q = on(c, b), on(b, a), on(a, table) ? (3), the goal for Sussman’s problem,leads to the following rules in lp(P):goal_reached:- on(c, b, 3), on(b, a, 3), on(a, table, 3).:- not goal_reached.The complete transformation of Sussman’s blocksworld problem, Pbw, after expansion ofall macros (see Appendix A.3), is shown in Fig. 6.As the reader can easily verify, the above transformation employs disjunction only inStep 4 for translating executability conditions. Furthermore, negated action atoms ¬a(t, T )occur only in the heads of the rules of lp(P). Thus, the program is head-cycle-free, whichis profitably exploited by the DLV engine underlying our implementation. The disjunction,which informally encodes a guess of whether the action a(t ) is executed or not at timeblock(a). block(b). block(c). location(table).location(B) :- block(B).time(0). time(1). time(2). time(3). next(0, 1). next(1, 2). next(2, 3).on(a, table, 0) :- block(a), location(table).on(b, table, 0) :- block(b), location(table).on(c, a, 0)move(B, L, T0) v -move(B, L, T0):- B <> L, block(B),:- block(c), location(a).location(L), next(T0, T1).:- move(B, L, T0), occupied(B, T0), next(T0, T1).:- move(B, L, T0), occupied(L, T0), next(T0, T1).occupied(B, T1) :- on(B1, B, T1), block(B), location(B), time(T1).on(B, L, T1):- on(B, L, T0), not -on(B, L, T1), block(B), location(L),next(T0, T1).on(B, L, T1)-on(B, L1, T1):- move(B, L, T0), block(B), location(L), next(T0, T1).:- move(B, L, T0), on(B, L1), L <> L1, block(B),location(L1), next(T0, T1).1, t(cid:10):- move(t1, t2, T0), move(t(cid:10)2, T0).t2, t(cid:10)2∈ {a, b, c, table}, (t1, t2) (cid:20)= (t(cid:10)for t1, t(cid:10)1∈ {a, b, c}1, t(cid:10)2)goal_reached :- on(c, b, 3), on(b, a, 3), on(a, table, 3).:- not goal_reached.Fig. 6. Transformation of Sussman’s planning problem Pbw from Section 2.1.176T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211T0, may equivalently be replaced by v-free guessing rules. The adapted transformation canthen be used on engines for computing answer sets of normal programs, such as Smodels.The following result formally states the desired correspondence between the solutionsof a DLVK planning problem P and the answer sets of the logic program lp(P) obtained byfollowing the procedure described above.Theorem 3.1 (Answer set correspondence). Let P be a planning problem, given by abackground knowledge Π and a DLVK-program, and let lp(P) the logic program generated= {a(t) |by Steps 0–6 above. Define, for any consistent set of literals S, the sets ASja(t, j − 1) ∈ S, a ∈ σ act} and sSj= {f (t) | f (t, j ) ∈ S, f (t) ∈ Lfl}, for all j (cid:1) 0. Then,(i) for each optimistic plan P = (cid:5)A1, . . . , Ai(cid:6) of P and witnessing trajectory T =(cid:5)(cid:5)s0, A1, s1(cid:6), (cid:5)s1, A2, s2(cid:6), . . . , (cid:5)si−1, Ai, si (cid:6)(cid:6), there exists some answer set S of lp(P)such that Aj = ASj for all j = 1, . . . , i and sj = sSj , for all j = 0, . . . , i;(ii) for each answer set S of lp(P), the sequence P = (cid:5)A1, . . . , Ai(cid:6) is a solution of P,i.e., an optimistic plan, witnessed by the trajectory T = (cid:5)(cid:5)s0, A1, s1(cid:6), (cid:5)s1, A2, s2(cid:6), . . . ,(cid:5)si−1, Ai, si(cid:6)(cid:6), where Aj = ASk for all j = 1, . . . , i and k = 0, . . . , i.j and sk = sSProof. The proof is based on the well-known notion of splitting of a logic programas defined in [28]. We define the splitting sequence U = (cid:5)UBG, U0, . . . , Ui , UG(cid:6) =(cid:5)BG, BG ∪ S0, . . . , BG ∪ S0 ∪ · · · ∪ Si, BG ∪ S0 ∪ · · · ∪ Si ∪ G(cid:6) of the program P (cid:10) =ground(lp(P)) as follows:• BG is the set of type literals and time and next literals occurring in P (cid:10);• Sj , 0 (cid:2) j (cid:2) i, is the set of literals in P (cid:10) of the form f (t, j ), where f ∈ σ fl, and of theform a(t, j − 1), where a ∈ σ act;• G = {goal_reached}.By the Splitting Sequence Theorem of [28], P (cid:10) (and thus lp(P)) has some (con-sistent) answer set S iff S = XBG ∪ X0 ∪ · · · ∪ Xi ∪ XG for some solution X =(cid:5)XBG, X0, . . . , Xi, XG(cid:6) of P (cid:10) w.r.t. U . We note the following facts.• PBG = bUBG(P (cid:10)) (as defined in [28], intuitively the program corresponding to UBG)consists of the background program and of the facts defining time and next.• P0 = eUBG(bU0(P (cid:10)) \ bUBG(P (cid:10)), XBG) (as defined in [28], intuitively the programcorresponding to U0) consists of rules and constraints which are translations of initialstate constraints and static rules (i.e., causation rules with empty after), in which theargument of time and the last argument of the head predicates has been instantiatedwith 0.• Pj = eUj−1(bUj (P (cid:10))\bUj−1 (P (cid:10)), XBG ∪X0 ∪· · ·∪Xj −1), for 1 (cid:2) j (cid:2) i (intuitively, theprogram corresponding to Uj ), consists of rules and constraints which are translationsof causation rules and executability conditions in the always-section, in which theargument of time and the second argument of next is instantiated with j (thus, theT. Eiter et al. / Artificial Intelligence 144 (2003) 157–211177last argument in head predicates of causation rules and executability conditions is jand j − 1, respectively).• PG = eUi (bUG(P (cid:10)) \ bUi (P (cid:10)), XBG ∪ X0 ∪ · · · ∪ Xi) (intuitively the programcorresponding to UG) consists of the rule and the constraint which were generatedby Step 6.• XBG ∪ X0 ∪ · · · ∪ Xi ∪ XG is a consistent set iff each of the sets XBG, X0, . . . , Xi , XGis consistent, since no literal in any of the sets BG, S0, . . . , Si , G exists such that itscomplement is contained in any other of these sets.We now prove (i) and (ii) of the theorem.(i) We show that for each optimistic plan a corresponding answer set S of lp(P) asdescribed exists. By the Splitting Sequence Theorem, we must prove that a respectivesolution X = (cid:5)XBG, X0, . . . , Xi, XG(cid:6) of lp(P) exists:XBG:X0:Xj :XG:As s0 is a legal initial state, the background knowledge has a consistent answerset. Thus, by definition of PBG, it clearly has a consistent answer set XBG.s0 in the witnessing trajectory must be a legal initial state, so s0 satisfies all rules inthe initially-section and the rules in the always-section with empty afterpart, under answer set semantics if causal rules are read as logic programmingrules. These rules are essentially identical (modulo the time literals and thetimestamp arguments) to P0, so X0 exists and s0 = sS,PFor 1 (cid:2) j (cid:2) i, (cid:5)sj −1, Aj , sj (cid:6) must be a legal transition. We proceed inductively.Aj has to be an executable action set w.r.t. sj −1, so each action a ∈ Aj must occurin the head of an executability condition whose body is true w.r.t. sj −1. There mustbe a corresponding clause in Pj constructed by Step 4 of the translation such that,the body is true w.r.t. Xj −1. If we choose Xj such that AS,P= Aj , then all theserules are satisfied. Each rule in Pj which has an action literal la in the head suchthat la is not in Aj either does not have a true body in Xj −1, or we include itsnegation ¬la into Xj .0.jFurthermore, sj satisfies all causal rules from the always-section whoseafter part is true w.r.t. sj −1 and Aj under answer set semantics. From thecorrespondence of causal rules from the always-section and rules in Pj , weand Aj = AS,Pmay thus conclude that Pj has an answer set Xj s.t. sj = sS,P,as seen above.si satisfies the goal of P. Let g1, . . . , gm, not gm+1, . . . , not gn?(i) be thegoal of P. Then {g1, . . . , gm} ⊆ si and {gm+1, . . . , gn} ∩ si = ∅ hold. Sincesi = sXi ,P, the body of the rule generated in Step 6 is true and therefore XG ={goal_reached} exists.jjiIn total, we have shown that for each optimistic plan of P a corresponding answer set Sof lp(P) exists, which contains literals representing a witnessing trajectory.(ii) We must prove that for each answer set S of lp(P), a corresponding optimistic planof P exists. By the Splitting Sequence Theorem, a solution X = (cid:5)XBG, X0, . . . , Xi, XG(cid:6)178T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211exists for lp(P). Since X0 is an answer set for the program corresponding to initial stateconstraints and static rules, a legal initial state s0 = sS,P0 must exist as well.For Xj , 1 (cid:2) j (cid:2) i, we proceed inductively. All rules corresponding to executabilityconditions w.r.t. time j − 1 must be satisfied, so for every literal la in Xj which correspondsto a positive action literal, a rule in Pj whose body is true w.r.t. Xj −1 and in which la is theonly true head literal, must exist. By construction, an executability condition whose bodyis true w.r.t. sj −1 and whose head is the corresponding action literal, must exist in P, so anexecutable action set Aj = AS,Pexists w.r.t. sj −1.All rules in Pj corresponding to causal rules from the always-section of P must besatisfied by Xj and Xj −1 (for literals translated from after-parts). So for each causalrule in P, either its after-part is false w.r.t. Aj and sj −1, or the causal rule is satisfied bythe state sj = sS,P.jFinally, since XG exists, goal_reached must be true. Hence, the body of the rulegenerated in Step 6 must be true, and therefore si must establish the goal of P.In total, we have shown that for each answer set S of lp(P) an optimistic plan of Pexists, such that the witnessing trajectory can be constructed from S as described. ✷j4. Secure planningThe translation in the previous section results in logic programs where the projectionsof the answer sets on the positive actions correspond to optimistic plans.As we have already mentioned above, the DLVK system also provides the functionalityof checking whether a given optimistic plan is secure for certain planning domains. Thus,a secure plan may be found in two steps as follows: (i) Find an optimistic plan P , and (ii)check whether P is secure. The test (ii) informally amounts to testing the following threeconditions:(1) the actions of P are executable in the respective stages of the execution;(2) at any stage, executing the respective actions of P always leads to some legal successorstate; and(3) the goal is true in every possible state reached if all steps of the plan are successfullyexecuted.In arbitrary planning domains, the security check is .P2 -complete [6],4 and thus, bywidely believed complexity hypotheses, it is not polynomially reducible to a SAT solver orother computational logic system with expressiveness bounded by NP or co-NP. However,as shown in [6], a polynomial reduction is possible for the class of proper propositionalplanning problems, where a planning problem P is proper, if the underlying planningdomain PD is proper, i.e., given any state s and any set of actions A, deciding whethersome legal state transition (cid:5)s, A, s(cid:10)(cid:6) exists is possible in polynomial time.4 I.e., co-(cid:2)P2 -complete (cf. [36]).T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211179In the DLVK system, we have focused on proper propositional planning domains, andwe have implemented security checking by a polynomial reduction to logic programs withcomplexity in co-NP.Note that for a proper planning domain PD, there is an algorithm APD which, givenan arbitrary state s and a set of actions A, decides in polynomial time whether some legaltransition (cid:5)s, A, s(cid:10)(cid:6) exists. The existence of such an algorithm APD , given PD, is difficultto decide, even in the propositional case, and APD is not efficiently constructible underwidely accepted complexity beliefs. We thus looked for suitable semantic properties ofplanning domains which can be ensured by syntactic conditions and enable a simple (oreven trivial) check for the existence of a legal transition (cid:5)s, A, s(cid:10)(cid:6), which uniformly worksfor a class of accepted planning domains.4.1. false-committed domains and security check SC1One such condition is when, informally, the existence of a legal transition (cid:5)s, A, s(cid:10)(cid:6)can only be blocked by a causal rule with head false or by an (implicit) consistencyconstraint :-f, -f. That is, if such constraints are disregarded, some legal transition(cid:5)s, A, s(cid:10)(cid:6) always exists, otherwise, if some constraint is violated in any such s(cid:10), then nolegal transition (cid:5)s, A, s(cid:10)(cid:6) exists. This condition can be ensured by a syntactic conditionwhich employs stratification on the causation rules.With this in mind, we develop a security check SC1, which, given an optimistic planP = (cid:5)A1, . . . , An(cid:6) of length n (cid:1) 0 for a planning problem P, rewrites the logic programlp(P) in Section 3 to a logic program Π1(P, P ) and returns “yes” if this program has noanswer set, and “no” otherwise.The modifications are as follows:• In order to check condition (1) mentioned at the beginning of this section, therules resulting from executability conditions are removed from lp(P). Instead, for anexecutability condition of the formexecutable a(X) if B1(Y1), . . . , Bm(Ym), not Bm+1(Ym+1), . . . , not Bn(Yn)in P, we generate the following rule for each a(c) ∈ Aj +1 (j = 0, . . . , n − 1) of P , whereσ is the substitution mapping the variables X to c:(cid:5)σa(X, j ) :- B1(Y1, j ), . . . , Bm(Ym, j ),not Bm+1(Ym+1, j ), . . . , not Bn(Yn, j ), next(j, j + 1)(cid:6).This enforces that whenever an action a(c) in the plan P is executable in the respectivestate j , then a(c, j ) will be derived by Π1(P, P ); no further actions will be derived. Toguarantee that the actions of the plan P are always executable, we add a rulenotex :- not a(c, j ).for each a(c) ∈ Aj +1. Here, notex is a new auxiliary predicate which intuitivelyexpresses that the plan P cannot be properly executed; its truth allows building a witnessfor the insecurity of P .180T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211• Concerning condition (2), in any situation where a causal rule with head false isviolated or a fluent inconsistency arises, an answer set witnessing the insecurity of Pshould be generated. To this end, the transformation is modified as follows:Each constraint :- Body, time(T1). in lp(P) derived from a causal rule of the formcaused false..., is rewritten tonotex :- Body, time(T1), T1 > 0.:- σ (Body).where σ is a substitution mapping T1 to 0. Observe that violation of constraints referringto an initial state does not generate a counterexample.Each constraint :- Body, next(T0, T1). in lp(P) which has been derived from a causalrule caused false..., is rewritten tonotex :- Body, next(T0, T1).And for each fluent f (X), the (implicit) consistency constraint (discussed in Footnote 2)is transformed to a rule for non-initial statesnotex :- f (X, T1), –f (X, T1), time(T1), T1 > 0.while those for the initial state remain unchanged::- f (X, 0), −f (X, 0).Constraint violations (explicit or implicit) in non-initial states therefore lead to a witnessinganswer set containing notex.• Finally, for condition (3), the goal constraint :- not goal_reached. is modifiedto:- goal_reached, not notex.We can read the rewritten goal constraint as follows: The constraint is satisfied, and thusthe plan P is not secure, if (i) either notex is true, which means that some action in Pcannot be executed or a constraint is violated when executing the actions in P , or (ii) ifgoal_reached is false, which means that after successfully executing all actions in P ,the goal is not established.Before we can state the informal conditions, under which the security check SC1 works,more precisely, we need some auxiliary concepts.Definition 4.1 (Constraint-free, constraint- & executability-condition-free shadow). Forany planning domain PD = (cid:5)Π, (cid:5)D, R(cid:6)(cid:6), let cfs(PD) denote the planning domain whichresults from PD by dropping all causal constraints with head false and interpretingnegative fluents as new (positive) fluents, and call it the constraint-free shadow of PD.Furthermore, let cefs(PD) denote the planning domain derived from cfs(PD) by omittingall executability conditions and adding executable a. for each legal action instance a,and call it the constraint- and executability–condition-free shadow of PD.Definition 4.2 (false-committed planning domains). We call a planning domain PDfalse-committed, if the following conditions hold:T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211181(i) If s is a legal state in PD and A is an action set which is executable in s w.r.t. PD, theneither (i.1) every legal transition (cid:5)s, A, s(cid:10)(cid:6) in cfs(PD) is also a legal transition in PD,or (i.2) no (cid:5)s, A, s(cid:10)(cid:10)(cid:6) is a legal transition in PD, for all states s(cid:10)(cid:10) in PD.(ii) For any state s and action set A in cefs(PD), there exists some legal transition (cid:5)s, A, s(cid:10)(cid:6)in cefs(PD).Example 4.1 (Blocksworld with incomplete initial state (continued)). Let us reconsider theblocksworld planning problem of Example 2.3. It is easily seen that our formulation ofthe respective planning domain, PDbwi , is false-committed. Indeed, it contains a singleoccurrence of default negation not , via the statement inertial on(B,L)., which isnot critical for the existence of a successor state in cefs(PD), so condition (ii) is guaranteed.As for condition (i), for each state s and action set A, which is executable in s, there is asingle legal transition (cid:5)s, A, s(cid:10)(cid:6) in cfs(PDbwi ), and thus one of the cases (i.1) and (i.2) mustapply.Consider first the optimistic plan P which is secure, as we have seen:(cid:2){move(d, table)}, {move(d, b)}, {move(c, d)}, {move(a, c)}.(cid:1)to build an answer set S of Π1(Pbwi, P ) fails: starting fromIndeed, an attemptany initial state, the actions in Ai are always executable and no constraint is vio-lated, thus notex cannot be included in S. To satisfy the rewritten goal constraint:- goal_reached,not notex. thus goal_reached must not be included inthe atoms on(a,c,2), on(c,d,2), on(d,b,2),S. However, as easily seen,on(b,table,2) must be included in S. This, however, means that goal_reachedhas to be included in S, which is a contradiction. Thus, no answer set S exists, whichmeans that the plan P is secure.Let us now modify the number of steps in the goal to i = 2, and consider the optimisticplan P(cid:1){move(c, d)}, {move(a, c)}(cid:2)In this case we can build an answer set of Π1(Pbwi, P ) starting from an initial state inwhich block d is on the table, by including at each stage the literals that are enforced.Then, both actions in the plan can be executed, and we end up in a state in which thegoal is not satisfied. Both goal_reached and notex cannot be derived, and thus theconstraint :-goal_reached, not notex. in Π1(Pbwi, P ) is satisfied, admitting ananswer set which witnesses the insecurity of P . Hence, the check outputs “no”, i.e., theplan is not secure.To show that the security check SC1 works properly for all false-committed planningdomains, we need the notions of soundness and completeness for security checks.Definition 4.3 (Security check). A security check for a class of planning domains PD isany algorithm which takes as input a planning problem P in a planning domain from theclass PD and an optimistic plan P for P, and outputs “yes” or “no”. A security check issound, if it reports “yes” only if P is a secure plan for P, and is complete if it reports “yes”in case P is a secure plan for P.182T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211In other words, for a sound security check only “yes” can be trusted, while for acomplete security check “no” can be trusted.Theorem 4.1. The security check SC1 is sound and complete for the class of false-committed planning domains.Proof. We outline the proof, but omit the details. Let P = (cid:5)A1, . . . , An(cid:6) be an optimisticplan for a planning problem P in a false-committed planning domain PD.(Soundness) Suppose that P is not secure. This means that an initial state s0 and atrajectory T = (cid:5)(cid:5)s0, A1, s1(cid:6), . . . , (cid:5)sj −1, Aj , sj (cid:6)(cid:6) in PD (where 0 (cid:2) j (cid:2) n) exist, such thatone of the conditions (1)–(3) for plan security stated at the beginning of this section isviolated. We then can build an answer set S of the program Π1(P, P ), in which, startingfrom s0, respective literals are included which correspond to the legal transitions in T as inlp(P). We consider the three cases:Suppose first that condition (1) is violated, i.e., some action a(c) in the action set Aj ofP is not executable. Then, no rule with head a(c, j ) fires, and thus we may add notexto S, as it can be derived from the rule notex :- not a(c, j ). By (ii) of false-committedness for PD, we can add literals for the stages j + 1, . . . , n modeling transitionsin cefs(PD) to S such that we obtain an answer set of Π1(P, P ).Suppose next that condition (2) is violated, i.e., no successor state exists. By (ii)of false-committedness for PD, we can add literals to S modeling a legal transition(cid:5)sj , Aj +1, sj +1(cid:6) in cfs(PD), and by (i) of false-committedness for PD, notex will bederived, as in PD some rule with head false fires or opposite fluent literals f, - -f arein S. Using (ii) again, we can add literals for the remaining stages j + 2, . . . , n modelingtransitions in cefs(PD), such that we obtain an answer set S of Π1(P, P ).Suppose finally that condition (3) is violated. That is, j = n and the goal is not satisfiedby sn. Then, the rule with head goal_reached is not applicable, the modified goalconstraint is satisfied, and an answer set S exists. Note that this also includes the casen = 0.In any of these three cases, an answer set S of Π1(P, P ) exists, and SC1(P, P ) outputs“no”.(Completeness) Suppose SC1(P, P ) outputs “no”, i.e., Π1(P, P ) has some answerset S. Then, either notex ∈ S or goal_reached /∈ S must hold. In the former case,notex must be derived either (a) from some rule r : notex :- not a(c, j )., or (b)from some rule notex :- ... time(j). corresponding to a rewritten constraintwith head false or a consistency constraint for strong negation. Let r be such thatj is minimal. Then S encodes with respect to the stages 0, . . . , j − 1 a trajectoryT = (cid:5)(cid:5)s0, A1, s1(cid:6), . . . , (cid:5)sj −1, Aj , sj (cid:6)(cid:6), such that Aj +1 is not executable in sj w.r.t. PD.In case (a), we immediately obtain that condition (1) of security is violated and hencethat P is not secure. In case (b), a trajectory T = (cid:5)(cid:5)s0, A1, s1(cid:6), . . . , (cid:5)sj −2, Aj −1, sj −1(cid:6)(cid:6)in PD exists such that executing Aj in sj −1 w.r.t. cfs(PD) as encoded in S leads to astate sj which violates some constraint of PD with head false or contains oppositeliterals. By item (i) of false-committedness for PD, we can conclude that no legaltransition (cid:5)sj −1, Aj , sj (cid:6) exists in PD, which violates condition (2) of security. On theother hand, if goal_reached /∈ S while notex /∈ S, then S encodes a trajectoryT. Eiter et al. / Artificial Intelligence 144 (2003) 157–211183T = (cid:5)(cid:5)s0, A1, s1(cid:6), . . . , (cid:5)sn−1, An, sn(cid:6)(cid:6) w.r.t. PD such that in the final state sn the goal isfalse, i.e., condition (3) of security is violated. That is, in all cases P is not secure. ✷Now that we have introduced the class of false-committed planning domains, welook for syntactic conditions on planning domains which can be efficiently checkedand guarantee false-committedness. One such condition can be obtained by imposingstratification on causation rules as follows: For any causation rule r of the form (A.2) letlp(r) be the corresponding logic programming rule f :-b1, . . . , bk, not bk+1, . . . , not blwhich emerges by skipping the after-part.Definition 4.4 (Stratified planning domain). A planning domain PD = (cid:5)Π, (cid:5)D, R(cid:6)(cid:6) isstratified, if the logic program ΠPD consisting of all rules lp(r), where r ∈ CR hash(r) (cid:20)= false and a nonempty if-part, is stratified in the usual sense (and stronglynegated atoms are treated as new atoms).For example, the blocksworld planning domain PDbwi described above is stratified.It is easy to see that stratified planning domains are false-committed. Indeed, sinceany stratified logic program is guaranteed to have an answer set, item (ii) of Definition 4.2holds. Furthermore, for each legal state s and action set A which is executable in s w.r.t.PD, there exists a single candidate state s(cid:10) for a legal transition (cid:5)s, A, s(cid:10)(cid:6) in cfs(PD), whichis computed by evaluating a subset of the rules of ΠPD ; this transition is not legal in PD ifs(cid:10) violates some constraint in CR with head false or introduces inconsistency. Note thatstratified planning domains PD are proper.Corollary 4.2. The security check SC1 is sound and complete for the class of stratifiedplanning domains.A possible extension of Corollary 4.2 allows for limited usage of unstratified causationrules. For example, pairsinertial f.inertial -f.of positive and negative inertia rules for the same ground fluent f, which amount to therules+rf−rf: caused f if not -f after f.: caused -f if not f after -f.violate stratification. Nevertheless, pairwise inertia for a fluent f can be allowed safely, ifeach of the two rules together with the remainder of the planning domain is stratified. Thatis, we check for stratification of the two subdomains that result from the planning domainPD by omitting the positive and negative inertia rules for f , denoted by PD−f and PD+f ,respectively. If both PD−f and PD+f are stratified, then SC1 is sound and complete for−PD. This holds because in any state s, only one of the rules rf can be active withrespect to s.+f and r184T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211We can further extend this to multiple pairs of ground inertia rules, where combinationsfor positive and negative inertia rules have to be checked. We go one step further and extendit to mux-stratified planning domains, which we define next.Two causation rules r0, r1 in PD are a mutually exclusive pair (mux-pair), if theirafter-parts are not simultaneously satisfiable in any state s and for any executable actionset A w.r.t. s in PD.Definition 4.5 (Mux-stratified planning domains). Let PD be a planning domain andE = {(ri,0, ri,1) | 1 (cid:2) i (cid:2) n}, n (cid:1) 0, a set of mux-pairs in PD. Then, PD is called mux-stratified w.r.t. E, if each planning domain PD(cid:10) that results from PD by removing one ofthe rules ri,0 and ri,1 for all i = 1, . . . , n is stratified.Notice that E does not necessarily contain all mux-pairs occurring in PD; we may evenchoose E = ∅, where mux-stratified coincides with stratified planning domain.Note that E induces a bipartite graph GE , whose vertices are the rules occurring in Eand whose edges are the pairs in E. The removal sets for building PD(cid:10) which need to beconsidered are given by the maximal independent sets of GE . There may be exponentiallymany such sets, and thus the cost for (simple) mux-stratification testing grows fast.We now establish the following result.Theorem 4.3. Every planning domain PD which is mux-stratified w.r.t. some set of mux-pairs E is false-committed.Proof. Consider any state s and executable action set A w.r.t. s in PD. Denote byactive(s, A, PD) the set of all ground rules in ground(ΠPD ) which correspond to instancesr (cid:10) of causation rules in PD such that the after-part of r (cid:10) is true w.r.t. s, A and the answerset M of the background knowledge.Then, we claim that active(s, A, PD) is stratified, i.e., its (ground) dependency graphdoes not contain a negative cycle. Indeed, towards a contradiction assume that the (ground)dependency graph of active(s, A, PD) contains a negative cycle C. Then, C involves onlyrules which correspond to instances of causation rules not occurring in E, and rules whichcorrespond to instances of causation rules from r1,i1, . . . , rn,in , where E = {(ri,0, ri,1) | 1 (cid:2)i (cid:2) n} and ij ∈ {0, 1}, for all j = 1, . . . , n. (A rule R is involved in all edges l1 → l2of the dependency graph, where l1 ∈ H (R) and l2 ∈ B(R).) This means that C is alsopresent in the ground dependency graph of ΠPD(cid:10) for some PD(cid:10) which results from PD byremoving the causation rules r1,1−1j , r1,1−2j , . . . , rn,1−nj . Consequently, the (non-ground)dependency graph of ΠPD(cid:10) contains a negative cycle. This, however, contradicts that PD ismux-stratified w.r.t. E; the claim is proved.Since the ground program active(s, A, PD) is stratified, it is easily seen that conditions(i) and (ii) of false-committedness hold for s. Since s was arbitrary, it follows that PDis false-committed. ✷By combining Theorems 4.1 and 4.3, we obtain the following corollary.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211185Corollary 4.4. The security check SC1 is sound and complete for the class of mux-stratifiedplanning domains, and in particular if E consists of opposite ground inertial-rules.The DLVK system provides limited support for testing mux-stratification, whichcurrently works for the set E consisting of all opposite ground inertia rules; an extensionto larger classes is planned for future DLVK releases. Notice, however, that decidingwhether a given pair (r0, r1) is a mux-pair in a given planning domain is intractable ingeneral.A generalization of the result in Corollary 4.4 to sets E of non-ground oppositeinertial rules fails. The reason is that in this case, multiple transition candidates (cid:5)s, A, s(cid:10)(cid:6)exist in cfs(PD) in general, which correspond to multiple answer sets of the programactive(s, A, PD). However, some of them might not be legal in PD, and condition (i)of false-committedness may be violated. Preliminary results suggest that under furtherrestrictions, like excluding constraints and causation rules with opposite unifiable heads,SC1 may be applied. We leave this for further work.4.2. Serial planning domains and security check SC2Besides SC1, the DLVK system provides an alternative security check SC2 for handlingother classes of proper planning domains, and the system design easily allows theincorporation of further security checks.The check SC2 is obtained by a slight modification of the program clauses inΠ1(PD, P ), resulting in a program Π2(PD, P ) as follows: the head notex of each rulewhich stems from a causal rule r such that h(r) = false and the if-part is not empty, isshifted to the negative body, i.e.,notex :- Body.is rewritten to:- Body, not notex.Informally, this shift means that the violation of a constraint on the successor state s(cid:10) istolerated, and we eliminate s(cid:10) as a counterexample to the security of the plan.We will see that this check works for the following class of planning domains.Definition 4.6 (Serial planning domains). A planning domain PD is serial, if it has thefollowing properties:(i) if s is a state in PD and A is executable in s w.r.t. PD, then some legal transition(cid:5)s, A, s(cid:10)(cid:6) is guaranteed to exist, and, moreover,(ii) for any state s and set of actions A in cefs(PD), some legal transition (cid:5)s, A, s(cid:10)(cid:6) existsw.r.t. cefs(PD).Obviously, serial planning domains PD are proper, as the check APD for telling whethera legal transition exists for s and executable A is trivial (just always return “yes”). Thefollowing can be observed:186T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211Theorem 4.5. The security check SC2 is sound and complete for serial planning domains.The proof of this result is similar to the proof of Theorem 4.1, and we thus omit it.A syntactical restriction guaranteeing seriality are stratified planning domains PD whichcontain no rules r such that h(r) = false and employ no strong negation. The serialproperty is preserved if we we also allow arbitrary totalization statements and limited useof strong negation, e.g., either all occurrences of a fluent are strongly negated or none is.Note that such planning domains are not false-committed in general.The security check SC2 also works for generalizations of serial planning domains.For example, we may safely add rules r of the form caused false after B.Furthermore, SC2 may also be profitably combined with SC1 in order to enlarge classesfor which security checking is supported.4.3. Incomplete security checkingWe may combine (fast) security checks which are sound and security checks which arecomplete to obtain checks which return the correct answer if possible, and leave the answeropen otherwise. This is similar to the use of incomplete constraint solvers in constraintprogramming, which return either “yes”, “no”, or “unknown” if queried about satisfiabilityof a constraint; the obvious requirement is that the answer returned does not contradict thecorrect result.Suppose that we have a suite of security checks SC1, . . . , SCn, where SC1, . . . , SCj , forsome j (cid:2) n, are known to be sound for a class of planning domains PD and SCk, . . . , SCn,for some k (cid:2) n, are known to be complete for PD. Then, we can combine them to thefollowing test T :(cid:7)T (P, P ) =“yes”,“no”,“unknown”, otherwise.if SCi(P, P ) = “yes”, for some i ∈ {1, . . . , j };if SCi(P, P ) = “no”, for some i ∈ {k, . . . , n};Observe that in the “yes” case of T , SCi(P, P ) = “yes” must hold for all i ∈ {k, . . . , n},and symmetrically in the “no” case that SCi(P, P ) = “no”, for all i ∈ {1, . . . , j }; this canbe used for checking integrity of the sound respectively complete security checks involved.Note that we can always use a dummy complete security check which reports “yes”on every input. By merging the “unknown” case into the “no” case, we thus can combinesound security checks SC1, . . . , SCj to another, more powerful sound security check SCfor the class PD. In particular, if SC1, . . . , SCj are known to exhaust all secure plans, thenSC is a sound and complete security check for PD.To account for the results in this section, in addition to the command-line options-FP, -FPopt, and -FPsec that we have seen in Section 2.3, DLVK provides threefurther options controlling the security checking: -FPcheck=n where n ∈ {1, 2} (whichcorrespond to SC1 and SC2 in the current implementation) selects a security check, while-FPsoundcheck=n and -FPcompletecheck=n where n ∈ {1, 2}, as above, can beused to specify a security check known to be sound and complete, respectively, for the inputdomain. The incorporation of further built-in security checks and support for user-definedsecurity checks is planned for the future.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–2111875. Comparison and experimentsIn the following, we will compare DLVK with several state-of-the-art conformantplanning systems, and report about experimental results about the performance of thesystem. The results presented here are mainly intended to give a momentary view on thestate of the current implementation of DLVK and its capabilities. To that end, we presentextensive benchmark results, and also compare the expressive power and flexibility of thevarious systems.5.1. Overview of compared systems5.1.1. CCALCThe Causal Calculator (CCALC) is a model checker for the languages of causal theories[30]. It translates programs in the action language C into the language of causal theorieswhich are in turn transformed into SAT problems using literal completion as described in[31]. This approach is based on Satisfiability Planning [22], where planning problems arereduced to SAT problems which are then solved by means of an efficient SAT solver likeSATO [48] or relsat [1].Though its inputlanguage allows nondeterminism in the initial state and alsonondeterministic action effects, CCALC as such is not capable of conformant planning andonly computes “optimistic plans” (according to DLVK terminology). Plan length is fixed,and both sequential and concurrent planning are supported.CCALC is written in Prolog. For our tests, we used version 1.90 of CCALC whichwe obtained from <URL:http://www.cs.utexas.edu/users/tag/cc/> and a trial version ofSICStus Prolog 3.8.6; we tested the system with SATO 3.2.1 and relsat 1.1.2. On theinstances SATO could solve it was significantly faster than relsat; relsat was used onlyfor the instances SATO could not solve in our experiments.5.1.2. CMBPThe Conformant Model Based Planner [4] is based on the model checking paradigmas well and relies on symbolic techniques such as BDDs. CMBP only allows sequentialplanning. Its input language is an extension of AR [16]. Unlike action languages such as Cor K, this language only supports propositional actions. Nondeterminism is allowed in theinitial state and for action effects. The length of computed plans is always minimal, but theuser has to declare an upper bound using command-line option -pl. If -pl is set equal tothe minimal plan length for the specific problem, this can be used to fix the plan length inadvance. We used this method to be comparable with DLVK which currently can only dealwith fixed plan length.For our tests, we used CMBP 1.0, available at <URL:http://sra.itc.it/people/roveri/cmbp/>.5.1.3. CPlanIntroduced in [10,15], CPlan is a conformant planner based on CCALC and the Caction language [17,26,29]. This language is similar to K in many respects, but close toclassical logic, while K is more “logic programming oriented” by the use default negation188T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211(see [6] for further discussion). CPlan uses CCALC only to generate a SAT instance andreplaces the optional SAT-solvers used by CCALC with an own procedure that extractsconformant plans from these SAT instances. CPlan implements full conformant planningand supports the computation of both minimal length plans as well as plans of fixed length,by incrementing plan length from a given lower bound until a plan is found or a given upperbound is reached. We set the upper and lower bound equal to the minimal plan length ofthe specific problems for our experiments to be comparable with DLVK. Sequential andconcurrent planning are possible; nondeterminism is allowed in the initial state as well asfor action effects.For our tests, we used CPlan 1.3.0, which is available at <URL:http://frege.mrg.dist.unige.it/∼otto/cplan.html>, together with CCALC 1.90 to produce the input for CPlan.5.1.4. GPTThe General Planning Tool [3] employs heuristic search techniques like A∗ to searchthe belief space. Its input language is a subset of PDDL. Nondeterminism is allowed inthe initial state as well as for action effects. GPT only supports sequential planning andcalculates plans of minimal length.We used version GPT 1.14 obtained from <URL:http://www.cs.ucla.edu/∼bonet/software/>.5.1.5. SGPIn addition to conformant planning, Sensory Graphplan (SGP, [47]) can also deal withsensing actions. SGP is an extension of the Graphplan algorithm [2]. Its input languageis an extension of PDDL [14]. Nondeterminism is allowed only in the initial state. Theprogram always calculates plans of minimal length.5 SGP does not support sequentialplanning, but computes concurrent plans automatically recognizing mutually exclusiveactions. That means, minimal length plans in terms of SGP are not plans with a minimalnumber of actions but with a minimal number of steps needed. At each step an arbitrarynumber of parallel actions are allowed, as long as the preconditions or effects are notmutually exclusive which is automatically detected by the algorithm.SGP is written in LISP and available at <URL:http://www.cs.washington.edu/ai/sgp.html>. For our tests, we used a trial version of Allegro Common Lisp 6.0.5.1.6. Specific system featuresWe would also like to point out further specific features of some of these special purposeplanning systems:• SGP automatically recognizes mutually exclusive actions in concurrent plans. It ispossible to encode concurrent plans in DLVK by explicitly describing the mutuallyexclusive actions, as done in our encodings of the “bomb in the toilet” benchmarkproblems for multiple toilets (see Section 5.2.2). However, the language K is more5 SGP comprises the functionality of another system by Smith and Weld called CGP (Conformant Graphplan,[42]), but is slower in general. As CGP is no longer maintained and not available online, we nevertheless decidedto choose SGP for our experiments.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211189Table 1Overview of system featuresInput languageSequential plansConcurrent plansOptimistic plansConformant plansKDLVKyesyesyesyesCCALCCCPlanCCMBPARSGPGPTPDDLPDDLyesyesyesnoyesyesnoyesyesnonoyesnoyesnoyesyesnoyesnonoyesyesnoMinimal plan lengthFixed plan lengtha An upper bound can be specified, but computed plans are always minimal.yesnoayesyesnoyesnoyescomplex than PDDL, which makes automatic recognition of possible conflicts ofactions much harder in our framework. On the other hand, our notions of executabilityand nonexecutability allow more flexible encodings of parallel actions than SGP.• GPT and SGP always compute minimal plans, which is not possible in the currentversion of DLVK.• CMBP and CPlan optionally compute minimal plans, where the user may specifyupper and/or lower bounds for the plan length.Table 1 provides a comparison of DLVK and all the systems introduced above. Notethat CCALC is not capable of conformant planning, and thus we cannot use it on therespective benchmark problems. On the other hand, CPlan showed slow performance onthe deterministic planning benchmarks that we considered. Therefore, we considered thesetwo systems in combination (CCALC for deterministic planning benchmarks and CPlanfor conformant planning benchmarks).5.2. Benchmark problems and encodings5.2.1. BlocksworldFor benchmarking we have chosen some blocks world instances to illustrate theperformance of DLVK on deterministic domains. Problems P1–P4 are due to [8], andproblem P5 is a slight modification of P4, which needs two moves more. The initialconfigurations and the respective goal configurations of P1–P5 together with the minimumnumber of moves (steps) needed to solve these problems are shown in Fig. 7.5.2.2. Bomb in the toiletTo show the capabilities of DLVK on planning under incomplete information, and inparticular conformant planning, we have chosen the well-known “bomb in the toilet”problem [34] and variations thereof, where we employ a naming convention due to [4].The respective planning domain comprises actions with nondeterministic effects, the initialstate is incomplete and, in more elaborated versions, several actions are available that canbe done in parallel.190T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211Fig. 7. Blocksworld planning instances.BT(p)—Bomb in the toilet with p packages. The basic scenario of the “bomb in thetoilet” problem is as follows. We have been alarmed that there is a bomb (exactly one) ina lavatory. There are p suspicious packages which could contain the bomb. There is onetoilet bowl, and it is possible to dunk a package into it. If the dunked package containedthe bomb, then the bomb is disarmed and a safe state is reached. The obvious goal is toreach a safe state via a secure plan.BTC(p)—Bomb in the toilet with certain clogging.In a slightly more elaborated version,dunking a package clogs the toilet, making further dunking impossible. The toilet can beunclogged by flushing it. The toilet is assumed to be unclogged initially. Note that thisdomain still comprises only deterministic action effects.BTUC(p)—Bomb in the toilet with uncertain clogging.In a further elaboration of thedomain, dunking a package has a nondeterministic effect on the status of toilet, which iseither clogged or not clogged afterwards.BMTC(p,t), BMTUC(p,t)—Bomb in the toilet with multiple toilets. Yet another elabora-tion is that several toilet bowls (t (cid:1) 1, rather than just one) are available in the lavatory.5.2.3. Encodings usedAs far as possible, we used the original encodings which come along with thedistributions of the respective systems.CCALC/CPlan. CCALC is not capable of conformant planning, while CPlan proved veryslow on deterministic domains. Thus, for the blocksworld problems P1–P5 we used the Cencoding provided by Esra Erdem with pure CCALC [8], while we used CPlan for the“bomb in the toilet” problems, with slight modifications of the C encodings provided withthe current CPlan distribution.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211191CMBP. For CMBP, we used the “bomb in the toilet” encodings which are included in thedistribution. BMTUC(p, t) is not included, but only a trivial modification of BMTC(p, t) isneeded to obtain an encoding for BMTUC. Because only propositional actions are allowedin the input language of CMBP, an encoding of blocks world where many different movesare possible is quite large. As no encoding is included in the examples, a (straightforward)encoding of P1 which we used for comparison can be found in Section B.1 of Appendix B.GPT. The distribution of GPT provides encodings for various “bomb in the toilet”problems; BMTUC(p, t) was not included, but the respective extension of BTUC(p) istrivial. For blocksworld, we used an adapted version of the SGP encoding, as the PDDLdialects of the two systems slightly differ. The encoding for P1 can be found in theappendix.SGP. For SGP we used the blocks world and bomb in toilet encodings coming with thedistribution. BTUC(p) and BMTUC(p, t) cannot be encoded in SGP which only allowsnondeterminism in the initial state.SGP generates concurrent plans, so we did not compare the sequential versions ofBT(p) and BMTC(p, t). Furthermore, for the blocks world problems, this means that theminimal plan lengths differ from the K encodings, and we provide them in an extra columnof Table 2. Note that the number of actions in the plans computed by SGP is not necessarilyminimal. For example, for P3 a plan with 4 steps and 9 moves exists whereas SGP finds aplan with 4 steps and 12 moves.DLVK. We have tested for the “bomb in the toilet” problems two different encodingsin DLVK, developed in [6]. The first one, labeled ws in the results, mimics world-stateplanning, in which the different completions of the states (“totalizations”) to world-statesare considered. The second one, labeled ks, uses the power of knowledge-state planningprovided by DLVK; it does not complete the states right away, but leaves the value ofunknown fluents open in accordance with the real knowledge of the planning agent aboutthe state of affairs. In both encodings, we first consider concurrent actions and then oneaction at time.Since the blocksworld problems are not conformant planning instances, we useoptimistic planning for them. For the knowledge-state encoding of the “bomb in the toilet”problems, the applicability of the security check SC1 is straightforward even for BTUC(p)and BMTUC(p, t), as the domains are mux-stratified w.r.t. the inertia rules for cloggedand -clogged, as these fluents do not occur in any bodies of other causation rules.Furthermore, thanks to the knowledge-state representation, the domains are deterministicand have unique initial states, so the security check is trivial and negligible for timing.The world-state encodings of BT(p), BTC(p), and BMTC(p, t) are stratified, so thesecurity check SC1 is guaranteed to be sound and complete for these problems byCorollary 4.2. In the case of BTUC(p) and BMTUC(p, t) in the world-state programs,the macro total violates stratification. However, both BTUC(p) and BMTUC(p, t) arefalse-committed domains, and thus the security check SC1 is sound and completefor these problems by Theorem 4.5. Indeed, the respective programs have no cyclewith an odd number of negative arcs in their dependency graphs (cf. BMTUC(p, t) in192T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211Appendix C.1; BTUC(p, t) and BMTUC(p, t) have the same dependency graph, sincethe only difference is that some fluents get an additional argument [6]), so by well-known results at least one answer set is guaranteed, and thus condition (ii) of false-committedness holds. Furthermore, the only constraints are those resulting from expandingthe nonexecutable statements. Since these constraints refer only to actions, either alls(cid:10) in a transition (cid:5)s, A, s(cid:10)(cid:6) satisfy them or no s(cid:10) does. Therefore, condition (i) of false-committedness is enforced as well. The world-state encodings of “bomb in the toilet” arenot deterministic, so the security check is responsible for a considerable portion of thetimings.5.3. Benchmark results and discussionIn this section, we compare the various systems in terms of representation capabilitiesand run-time benchmarks.5.3.1. Test environmentAll tests were performed on a Pentium III 733 MHz machine with 256 MB ofmain memory running SuSE Linux 6.4. The results for the blocks world problems aresummarized in Table 2. Tables 3–9 show the results for the various “bomb in the toilet”problems. The minimal plan length is reported in the second column of each table. Notethat for CCALC the results include 1.23 s startup time for SICStus Prolog, while for SGP0.27 s startup time is included. Run-times longer than 1200 CPU seconds were omitted,which is indicated by a dash in the tables.5.3.2. RepresentationFrom the viewpoint of expressiveness, the language K often allows a more compact andreadable encoding than AR or PDDL dialects: CMBP allows only propositional actions(see Appendix B.1 for a blocks world encoding in AR), whereas languages like C and Kallow a much more elegant encoding of complex actions. PDDL dialects as used by GPT orSGP, on the other hand, do not allow expressing ramifications which makes the encoding ofTable 2Experimental results for blocksworld problems P1–P5ProblemStepsBlocksKDLVCCALCCMBPGPTSGPsteps/actionsatime468911P1P2P3P4P5a As SGP supports only concurrent planning, the number of steps and number of actions for the solutions1.73 s2.18 s5.42 s15.83 s350.43 sb9.69 s43.85 s248.45 s––0.04 s0.11 s8.81 s8.91 s21.14 s1.13 s2.52 s–––0.18 s7.95 s–––3/45/74/12––4581111found are displayed in an extra column. Note that the number of actions is not necessarily minimal.b With CCALC and SATO no solution for P5 could be found, the timing for P5 was generated using relsat,which is significantly slower on the other problem instances.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211193Table 3Experimental results for BT(p) with concurrent dunksBT(p)StepsKDLVCPlanSGPBT(2)BT(3)BT(4)BT(5)BT(6)BT(7)BT(8)BT(9)BT(10)BT(11)BT(12)BT(13)BT(14)BT(15)BT(16)BT(17)BT(18)BT(19)BT(20)1111111111111111111wsks0.01 s0.02 s0.01 s0.02 s0.02 s0.02 s0.02 s0.01 s0.02 s0.02 s0.02 s0.03 s0.03 s0.03 s0.03 s0.03 s0.03 s0.03 s0.04 s0.01 s0.01 s0.01 s0.01 s0.01 s0.01 s0.01 s0.02 s0.02 s0.02 s0.02 s0.02 s0.01 s0.01 s0.01 s0.02 s0.02 s0.02 s0.02 s1.38 s1.38 s1.39 s1.42 s1.47 s1.56 s1.79 s2.29 s3.41 s6.04 s11.98 s25.28 s57.71 s127.75 s294.44 s678.19 s–––0.69 s0.80 s0.95 s1.21 s1.55 s2.00 s2.56 s3.32 s4.27 s5.34 s6.66 s8.16 s9.98 s12.11 s14.57 s17.43 s20.74 s24.47 s28.78 sTable 4Experimental results for BT(p) sequentialT(p)StepsKDLVCPlanCMBPGPTBT(2)BT(3)BT(4)BT(5)BT(6)BT(7)BT(8)BT(9)BT(10)BT(11)BT(12)BT(13)BT(14)BT(15)BT(16)BT(17)BT(18)BT(19)BT(20)234567891011121314151617181920ws0.02 s0.03 s0.11 s1.50 s28.78 s593.15 s–––––––––––––ks0.02 s0.02 s0.02 s0.03 s0.03 s0.03 s0.05 s0.06 s0.08 s0.10 s0.13 s0.16 s0.21 s0.28 s0.35 s0.47 s0.61 s0.78 s0.98 s1.37 s1.39 s1.39 s1.45 s1.81 s5.12 s65.85 s––––––––––––0.03 s0.04 s0.04 s0.04 s0.04 s0.05 s0.06 s0.07 s0.10 s0.19 s0.39 s0.82 s1.76 s4.00 s8.82 s19.03 s38.95 s91.89 s199.63 s0.56 s0.55 s0.61 s0.61 s0.63 s0.67 s0.68 s0.78 s0.95 s1.27 s2.12 s3.89 s8.87 s19.13 s42.17 s93.69 s208.00 s496.95 s546.43 s194T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211Table 5Experimental results for BTC(p)BTC(p)StepsKDLVCPlanCMBPGPTSGPBTC(2)BTC(3)BTC(4)BTC(5)BTC(6)BTC(7)BTC(8)BTC(9)BTC(10)BTC(11)BTC(12)BTC(13)BTC(14)BTC(15)BTC(16)BTC(17)BTC(18)BTC(19)BTC(20)3579111315171921232527293133353839ws0.02 s0.08 s1.56 s36.28 s–––––––––––––––ks0.01 s0.02 s0.02 s0.03 s0.04 s0.06 s0.08 s0.11 s0.14 s0.20 s0.26 s0.34 s0.45 s0.58 s0.74 s0.94 s1.17 s1.46 s1.80 s1.37 s1.39 s1.39 s2.36 s28.95 s178.97 s–––––––––––––0.04 s0.04 s0.05 s0.05 s0.06 s0.07 s0.12 s0.21 s0.39 s0.81 s1.72 s3.79 s8.82 s16.92 s42.92 s92.03 s197.85 s––0.59 s0.60 s0.60 s0.62 s0.66 s0.68 s0.74 s0.81 s1.04 s1.48 s2.51 s4.68 s10.84 s23.31 s51.40 s114.21 s273.25 s374.00 s–0.92 s3.30 s191.60 s––––––––––––––––Table 6Experimental results for BTUC(p)BTUC(p)StepsKDLVCPlanCMBPGPTBTUC(2)BTUC(3)BTUC(4)BTUC(5)BTUC(6)BTUC(7)BTUC(8)BTUC(9)BTUC(10)BTUC(11)BTUC(12)BTUC(13)BTUC(14)BTUC(15)BTUC(16)BTUC(17)BTUC(18)BTUC(19)BTUC(20)3579111315171921232527293133353839ws0.03 s0.61 s87.54 s––––––––––––––––ks0.02 s0.02 s0.03 s0.03 s0.04 s0.05 s0.08 s0.10 s0.14 s0.19 s0.25 s0.33 s0.43 s0.55 s0.71 s0.90 s1.15 s1.41 s1.74 s1.35 s1.45 s1.93 s2.48 s–51.72 s–––––––––––––0.03 s0.04 s0.04 s0.06 s0.06 s0.07 s0.12 s0.20 s0.39 s0.80 s1.72 s3.79 s8.81 s16.94 s42.93 s92.02 s197.84 s––0.59 s0.60 s0.61 s0.66 s0.65 s0.74 s0.75 s0.88 s1.18 s1.81 s3.18 s6.42 s14.43 s32.25 s71.10 s159.53 s368.12 s––T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211195Table 7Experimental results for BMTC(p) with concurrent dunksBMTC(p, t)StepsKDLVCPlanSGPBMTC(2, 2)BMTC(3, 2)BMTC(4, 2)BMTC(5, 2)BMTC(6, 2)BMTC(7, 2)BMTC(8, 2)BMTC(9, 2)BMTC(10, 2)BMTC(2, 3)BMTC(3, 3)BMTC(4, 3)BMTC(5, 3)BMTC(6, 3)BMTC(7, 3)BMTC(8, 3)BMTC(9, 3)BMTC(10, 3)BMTC(2, 4)BMTC(3, 4)BMTC(4, 4)BMTC(5, 4)BMTC(6, 4)BMTC(7, 4)BMTC(8, 4)BMTC(9, 4)BMTC(10, 4)133557799113335557111333355ws0.02 s0.04 s0.11 s2.79 s37.04 s––––0.02 s0.02 s0.08 s0.35 s17.81 s223.31 s–––0.02 s0.02 s0.03 s0.18 s5.29 s61.73 s668.74 s––ks0.01 s0.02 s0.03 s0.04 s0.07 s0.52 s10.66 s206.27 s–0.02 s0.02 s0.03 s0.03 s0.06 s0.13 s0.74 s5.90 s389.08 s0.02 s0.02 s0.02 s0.04 s0.05 s0.09 s0.41 s1.06 s12.14 s1.41 s1.50 s1.72 s3.37 s13.04 s71.50 s–––1.62 s2.31 s4.81 s13.55 s43.34 s210.71 s417.62 s––2.89 s9.19 s37.55 s158.74 s571.77 s––––0.95 s3.40 s7.17 s––––––1.15 s1.76 s15.01 s76.28 s592.41 s––––1.52 s2.34 s3.71 s372.74 s–––––action effects less readable and elaboration tolerant (see Appendix B.2 for a GPT encodingof blocksworld).Similar remarks apply also to the “bomb in the toilet” problems, where K allows forvery compact and at the same time intuitive encodings.5.3.3. PerformanceThe running times on blocksworld instances in Table 2 show that DLVK is significantlyfaster than the other systems if there are many action instances.Under the world-state encodings of the different “bomb in the toilet” instances, DLVK isnot competitive except for BT(p) with concurrent dunks, where plan length is always 1, andBMTC(p). This indicates that DLVK’s performance is quite sensitive to (increasing) planlength, especially for sequential planning. Still, DLVK outperforms SGP, a special purposeplanning system, on all comparable instances, and also CPlan (which is the system mostcomparable to DLVK in terms of expressiveness and similar in nature) seems to be withinreach.196T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211Table 8Experimental results for BMTC(p) sequentialBMTC(p, t)StepsKDLVCPlanCMBPGPTBMTC(2, 2)BMTC(3, 2)BMTC(4, 2)BMTC(5, 2)BMTC(6, 2)BMTC(7, 2)BMTC(8, 2)BMTC(9, 2)BMTC(10, 2)BMTC(2, 3)BMTC(3, 3)BMTC(4, 3)BMTC(5, 3)BMTC(6, 3)BMTC(7, 3)BMTC(8, 3)BMTC(9, 3)BMTC(10, 3)BMTC(2, 4)BMTC(3, 4)BMTC(4, 4)BMTC(5, 4)BMTC(6, 4)BMTC(7, 4)BMTC(8, 4)BMTC(9, 4)BMTC(10, 4)2468101214161823579111315172346810121416ws0.02 s0.07 s2.47 s208.52 s–––––0.02 s0.03 s1.84 s291.24 s–––––0.02 s0.41 s0.60 s149.65 s–––––ks0.02 s0.02 s0.04 s0.05 s0.07 s0.10 s0.13 s0.20 s0.28 s0.02 s0.02 s0.03 s0.06 s0.09 s0.25 s15.42 s––0.02 s0.02 s0.03 s0.06 s0.10 s0.15 s0.47 s67.07 s–1.41 s1.50 s1.64 s2.66 s32.77 s12.46 s–––1.50 s1.85 s2.86 s5.92 s14.50 s40.41 s–––2.02 s3.67 s9.03 s30.55 s–199.73 s–––0.04 s0.05 s0.06 s0.06 s0.09 s0.12 s0.23 s0.48 s0.96 s0.04 s0.04 s0.06 s0.09 s0.14 s0.30 s0.62 s1.44 s3.31 s0.04 s0.05 s0.07 s0.13 s0.23 s0.51 s1.13 s2.94 s6.38 s0.76 s0.78 s0.81 s0.82 s0.86 s0.96 s1.11 s1.48 s2.26 s0.76 s0.81 s0.84 s0.90 s0.99 s1.17 s1.66 s2.79 s5.64 s0.81 s0.83 s0.92 s1.01 s1.27 s1.85 s3.34 s7.18 s17.34 sUnder the knowledge-state encodings, DLVK outperforms its competitors in many of thechosen examples. The sensitivity to increasing plan length/search space can, however, alsopartly be observed here, where execution times seem to grow drastically from one instanceto the next. This can be partly explained by the general heuristics of the underlying DLVsystem, which might not scale up well in some cases. For instance, DLV as a general purposeproblem solver does not include special heuristics towards plan search. In particular,during the answer set generation process, no distinction is made between actions andfluents, which might be useful for planning tasks to control the generation of answer setsrespectively plans; this may be part of further investigations.5.3.4. Effect of concurrent actions and default negationOnce we also consider concurrent actions (which are not supported by GPT andCMBP), DLVK performs better than CPlan on some larger instances of BMTC(p, t) andBMTUC(p, t) (see Tables 7 and 9).Using the expressive power of default negation to express unknown fluents with theknowledge-state encodings of “bomb in the toilet” in K pays off well: DLVK outperforms allT. Eiter et al. / Artificial Intelligence 144 (2003) 157–211197Table 9Experimental results for BMTUC(p) with concurrent dunksBMTUC(p, t)StepsKDLVBMTUC(2, 2)BMTUC(3, 2)BMTUC(4, 2)BMTUC(5, 2)BMTUC(6, 2)BMTUC(7, 2)BMTUC(8, 2)BMTUC(9, 2)BMTUC(10, 2)BMTUC(2, 3)BMTUC(3, 3)BMTUC(4, 3)BMTUC(5, 3)BMTUC(6, 3)BMTUC(7, 3)BMTUC(8, 3)BMTUC(9, 3)BMTUC(10, 3)BMTUC(2, 4)BMTUC(3, 4)BMTUC(4, 4)BMTUC(5, 4)BMTUC(6, 4)BMTUC(7, 4)BMTUC(8, 4)BMTUC(9, 4)BMTUC(10, 4)133557799113335557111333355ws0.02 s0.11 s7.39 s––––––0.02 s0.02 s0.28 s34.09 s–––––0.02 s0.02 s0.03 s0.84 s748.90 s––––ks0.02 s0.03 s0.03 s0.04 s0.07 s0.80 s23.57 s818.23 s–0.02 s0.02 s0.03 s0.03 s0.05 s0.10 s0.74 s9.55 s693.99 s0.02 s0.02 s0.02 s0.04 s0.05 s0.08 s0.55 s0.98 s17.89 sCPlan1.40 s2.06 s3.54 s8.18 s787.58 s––––1.55 s10.27 s41.03 s181.45 s600.66 s––––2.54 s119.18 s582.84 s––––––other systems, including the special purpose conformant planners GPT and CMBP, excepton sequential BMTC(p, t) and BMTUC(p, t) with more than two toilets (see Tables 8 and10), where CMBP is fastest.5.3.5. Summary of experimental resultsOverall, the results indicate that DLVK is competitive with state of the art conformantplanners, especially when exploiting the K language features in terms of knowledge-stateproblem encodings. Recall, however, that some of the systems compute minimal plans,which is (currently) not supported by DLVK. The comparison of DLVK to CCALC/CPlanis particularly relevant, since these systems are closest in spirit to DLVK. As we cansee, the advanced features of knowledge-state encoding lead to significant performanceimprovements.198T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211Table 10Experimental results for BMTUC(p) sequentialBMTUC(p, t)StepsKDLVCPlanCMBPGPTBMTUC(2, 2)BMTUC(3, 2)BMTUC(4, 2)BMTUC(5, 2)BMTUC(6, 2)BMTUC(7, 2)BMTUC(8, 2)BMTUC(9, 2)BMTUC(10, 2)BMTUC(2, 3)BMTUC(3, 3)BMTUC(4, 3)BMTUC(5, 3)BMTUC(6, 3)BMTUC(7, 3)BMTUC(8, 3)BMTUC(9, 3)BMTUC(10, 3)BMTUC(2, 4)BMTUC(3, 4)BMTUC(4, 4)BMTUC(5, 4)BMTUC(6, 4)BMTUC(7, 4)BMTUC(8, 4)BMTUC(9, 4)BMTUC(10, 4)2468101214161823579111315172346810121416ws0.02 s0.52 s264.20 s––––––0.02 s0.04 s71.03 s––––––0.01 s0.78 s5.81 s––––––ks0.02 s0.02 s0.04 s0.05 s0.07 s0.10 s0.14 s0.21 s0.27 s0.02 s0.03 s0.04 s0.05 s0.08 s0.21 s13.39 s––0.02 s0.02 s0.04 s0.06 s0.09 s0.13 s0.42 s64.02 s–1.39 s1.96 s3.37 s361.64 s–––––1.49 s6.47 s22.07 s150.72 s–––––1.93 s41.70 s182.92 s837.33 s–––––0.04 s0.04 s0.05 s0.06 s0.08 s0.12 s0.23 s0.47 s0.96 s0.04 s0.05 s0.06 s0.09 s0.14 s0.29 s0.61 s1.45 s3.31 s0.04 s0.05 s0.07 s0.12 s0.23 s0.51 s1.13 s2.94 s6.37 s0.78 s0.80 s0.81 s0.85 s0.92 s1.04 s1.34 s2.00 s3.71 s0.79 s0.81 s0.86 s0.98 s1.19 s1.74 s3.15 s6.69 s15.57 s0.79 s0.86 s0.97 s1.33 s2.23 s4.79 s11.37 s28.07 s68.26 s6. Further related work and conclusionWe have discussed the relation of DLVK to a number of planning systems in Section 5already, and complement this by briefly addressing further approaches and systems here.6.1. Further related workThe idea to employ declarative logic programming systems for planning finds its rootsin the seminal paper Subrahmanian and Zaniolo [43], which carried out the idea ofsatisfiability planning [22] to the framework of declarative logic programming.Planning under incomplete knowledge has been widely investigated in the AI literature.Most works extend algorithms/systems for classical planning, rather than using deductiontechniques for solving planning tasks as proposed in this paper. The systems Buridan [23],UDTPOP [37], Conformant Graphplan [42], CNLP [38] and CASSANDRA [39] fall inthis class. In particular, Buridan, UDTPOP, and Conformant Graphplan can solve secureplanning (also called conformant planning) problems, like DLVK. On the other hand, theT. Eiter et al. / Artificial Intelligence 144 (2003) 157–211199systems CNLP and CASSANDRA deal with conditional planning (where the sequence ofactions to be executed depends on dynamic conditions).More recent works propose the use of automated reasoning techniques for planningunder incomplete knowledge. In [41] a technique for encoding conditional planningproblems in terms of 2-QBF formulas is proposed. The work in [11] proposes a techniquebased on regression for solving secure planning problems in the framework of the SituationCalculus, and presents a Prolog implementation of such a technique. In [31], sufficientsyntactic conditions ensuring security of every (optimistic) plan are singled out. Whilesharing their logic-based nature, our work presented in this paper differs considerably fromsuch proposals, since it is based on a different formalism.6.2. SummaryIn this paper, we have presented the DLVK planning system, which implements theK action and planning language, introduced and discussed in the companion paper [6],on top of the DLV logic programming system. In the course of this, we have shown atransformation of planning problems in K into logic programming. In particular, we havegiven such a transformation for optimistic planning, which is planning in the traditionalsense, and we have discussed how secure planning, i.e., conformant planning, can berealized for certain classes of planning problems via a transformation of security checkinginto logic programming. Our transformations use disjunctions in rule heads supported byDLV, but can be easily adapted to be disjunction-free, and thus become available for otherlogic programming systems such as Smodels [35]. Furthermore, we have compared oursystem on some standard benchmark problems to similar logic-based planning systems,namely CCALC [30,31], CPlan [10,15], CMBP [4]), GPT [3], and SGP [47]. We obtainedpromising performance results for secure planning exploiting the power of knowledge-stateproblem encodings, which are a distinguishing feature of the K planning language. As webelieve, the results of the present paper show that knowledge-state encoding of planningproblems has, besides it conceptual conciseness and natural appeal, potential also from acomputational perspective.6.3. Further and future workEnhancing and further improving the DLVK planning system is an ongoing effort. Thereare several issues which we address in our current and future research. One issue, discussedmore in detail in the companion paper [6], is the development of a methodology forprofitably using the knowledge-state planning approach.Another issue concerns improvements and enhanced capabilities for secure planning.We have performed further experiments with a different approach of conformant answerset planning presented in [24]. In contrast to the plan security checking described here, thatpaper sketches an integrated encoding of conformant planning domains. In that approach,all answer sets correspond to secure plans and no further checking is necessary. Theseresults seem to be very encouraging, but it is only possible to encode a rather restrictedclass of domains in DLV. In fact, since secure planning is (cid:2)P3 -complete [6], complexityarguments show that this method can not be efficiently extended to all planning domains.200T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211On the other hand, security checking for all planning domains is in the class .P2 , andthus can be polynomially encoded to DLV. However, such a transformation remains to bedesigned in full generality. Besides these issues, also extended handling of incompletesecurity checking, as described in this paper, is part of our research, and we consider furtherbuilt-in as well as support for user-defined security checks.Finally, the use of the DLV engine as a computational backbone suggests to use its ca-pabilities to enhance the DLVK planning system by further features. In particular, by theuse of weak constraints, it is possible to compute in DLV optimal answer sets of a logicprogram. This provides a computational basis for determining optimal plans of a planningproblem, which are plans that minimize a given objective function, such as cost of actions,or execution time. To our knowledge, current logic-based planning systems do not offercomprehensive such capabilities. Enhancing the K language and the DLVK system for opti-mal planning is on our agenda, and such features will be included in future DLVK releases.AcknowledgementsThis work has greatly benefited from interesting discussions with and comments ofMichael Gelfond, Vladimir Lifschitz, Riccardo Rosati, and Hudson Turner. Furthermore,we are grateful to Claudio Castellini, Alessandro Cimatti, Esra Erdem, Enrico Giunchiglia,David E. Smith, and Dan Weld for kindly supplying explanations, support, and commentson the systems that we used for comparison. Furthermore, we appreciate the reviewcomments which helped to improve this paper.This work was supported by FWF (Austrian Science Funds) under the projectsP14781-INF and Z29-INF and the European Commission under projects FET-2001-37004WASP and IST-2001-33570 INFOMIX.Appendix A. Definition of language KThis appendix contains, in shortened form, the definition of the language K; see [6] formore details and examples.A.1. Basic syntaxWe assume σ act, σ fl, and σ typ disjoint sets of action, fluent and type names, respectively,i.e., predicate symbols of arity (cid:1) 0, and disjoint sets σ con and σ var of constant andvariable symbols. Here, σ fl, σ act describe dynamic knowledge and σ typ describes staticbackground knowledge. An action (respectively fluent, type) atom is of form p(t1, . . . , tn),where p ∈ σ act (respectively, σ fl, σ typ) has arity n and t1, . . . , tn ∈ σ con ∪ σ var . An action(respectively, fluent, type) literal l is an action (respectively, fluent, type) atom a or itsnegation ¬a, where “¬” (alternatively, “–”) is the true negation symbol. We define ¬.l = aif l = ¬a and ¬.l = ¬a if l = a, where a is an atom. A set L of literals is consistent,if L ∩ ¬.L = ∅. Furthermore, L+ (respectively, L−) is the set of positive (respectively,negative) literals in L. The set of all action (respectively, fluent, type) literals is denotedT. Eiter et al. / Artificial Intelligence 144 (2003) 157–211201as Lact (respectively, Lfl, Ltyp). Furthermore, Lfl,typ = Lfl ∪ Ltyp, Ldyn = Lfl ∪ L+L = Lfl,typ ∪ L+act.act, andAll actions and fluents must be declared using statements as follows.Definition A.1 (Action, fluent declaration). An action (respectively, fluent) declaration, isof the form:p(X1, . . . , Xn) requires t1, . . . , tmact (respectively, p ∈ L+where p ∈ L+t1, . . . , tm ∈ Ltyp, m (cid:1) 0, and every Xi occurs in t1, . . . , tm.(A.1)fl ), X1, . . . , Xn ∈ σ var where n (cid:1) 0 is the arity of p,If m = 0, the keyword requires may be omitted. Causation rules specify dependen-cies of fluents on other fluents and actions.Definition A.2 (Causation rule). A causation rule (rule, for short) is an expression of theformcaused f if b1, . . . , bk, not bk+1, . . . , not blafter a1, . . . , am, not am+1, . . . , not an(A.2)where f ∈ Lfl ∪ {false}, b1, . . . , bl ∈ Lfl,typ, a1, . . . , anL, l (cid:1) k (cid:1) 0, and n (cid:1) m (cid:1) 0.Rules where n = 0 are static rules, all others dynamic rules. When l = 0 (respectively,n = 0), “if” (respectively, “after”) is omitted; if both l = n = 0,“caused” is optional.+(r) = {b1, . . . , bk},−(r) = {am+1, . . . , an}, and lit(r) =+(r)) accesses the stateWe access parts of a causation rule r by h(r) = {f }, post−(r) = {bk+1, . . . , bl}, prepost{f, b1, . . . , bl, a1, . . . , an}. Intuitively, prebefore (respectively, after) some action(s) happen.+(r) = {a1, . . . , am}, pre+(r) (respectively, postSpecial static rules may be specified for the initial states.Definition A.3 (Initial state constraint). An initial state constraint is a static rule of theform (A.2) preceded by “initially”.The language K allows conditional execution of actions, where several alternativeexecutability conditions may be specified.Definition A.4 (Executability condition). An executability condition e is an expression ofthe formexecutable a if b1, . . . , bk, not bk+1, . . . , not bl(A.3)where a ∈ L+act and b1, . . . , bl ∈ L, and l (cid:1) k (cid:1) 0.If l = 0 (i.e., executability is unconditional), “if” is skipped. The parts of e are+(e) = {b1, . . . , bk}, pre−(e) = {bk+1, . . . , bl}, and lit(e) =accessed by h(e) = {a}, pre−(e) refers to the state at which some action’s suitability{a, b1, . . . , bl}. Intuitively, preis evaluated. Here, the state after action execution is not involved. For convenience, wedefine post+(e) = post−(e) = ∅.202T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211All causal rules and executability conditions must satisfy the following condition, whichis similar to safety in logic programs [46]: Each variable in a default-negated type literalmust also occur in some literal which is not a default-negated type literal. No safety isrequested for variables appearing in other literals. The reason is that variables appearing influent and action literals are implicitly safe by the respective type declarations.Notation. For any causal rule, initial state constraint, and executability condition r andν ∈ {post, pre, b}, we define ν(r) = ν+(r) ∪ ν−(r), where bs(r) = posts(r) ∪ pres(r).A.1.1. Planning domains and planning problemsDefinition A.5 (Action description, planning domain). An action description (cid:5)D, R(cid:6)consists of a finite set D of action and fluent declarations and a finite set R of safe causationrules, safe initial state constraints, and safe executability conditions. A K planning domainis a pair PD = (cid:5)Π, AD(cid:6), where Π is a stratified Datalog program (the backgroundknowledge) which is safe (cf. [46]), and AD is an action description. We call PD positive,if no default negation occurs in AD.Definition A.6 (Planning problem). A planning problem P = (cid:5)PD, q(cid:6) is a pair of aplanning domain PD and a query q, i.e.,g1, . . . , gm, not gm+1, . . . , not gn ? (i)(A.4)where g1, . . . , gn ∈ Lfl are variable-free, n (cid:1) m (cid:1) 0, and i (cid:1) 0 denotes the plan length.A.2. SemanticsWe start with the preliminary definition of the typed instantiation of a planning domain.This is similar to the grounding of a logic program, with the difference being that onlycorrectly typed fluent and action literals are generated.Let PD = (cid:5)Π, (cid:5)D, R(cid:6)(cid:6) be a planning domain, and let M be the (unique) answer set of Π[12]. Then, θ (p(X1, . . . , Xn)) is a legal action (respectively, fluent) instance of an action(respectively, fluent) declaration d ∈ D of the form (A.1), if θ is a substitution defined overX1, . . . , Xn such that {θ (t1), . . . , θ (tm)} ⊆ M. By LPD we denote the set of all legal actionand fluent instances. The instantiation of a planning domain respecting type information isas follows.Definition A.7 (Typed instantiation). For any planning domain PD = (cid:5)Π, (cid:5)D, R(cid:6)(cid:6), itstyped instantiation is given by PD↓ = (cid:5)Π↓, (cid:5)D, R↓(cid:6)(cid:6), where Π↓ is the grounding of Π(over σ con) and R↓ = {θ (r) | r ∈ R, θ ∈ 9r }, where 9r is the set of all substitutions θ ofthe variables in r using σ con, such that lit(θ (r)) ∩ Ldyn ⊆ LPD ∪ (¬.LPD ∩ L−fl ).In other words, in PD↓ we replace Π and R by their ground versions, but keep ofthe latter only rules where the atoms of all fluent and action literals agree with theirdeclarations. We say that a PD = (cid:5)Π, (cid:5)D, R(cid:6)(cid:6) is ground, if Π and R are ground, andmoreover that it is well-typed, if PD and PD↓ coincide.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211203A.2.1. States and transitionsDefinition A.8 (State, state transition). A state w.r.t. a planning domain PD is anyconsistent set s ⊆ Lfl ∩ (lit(PD) ∪ lit(PD)−) of legal fluent instances and their negations.A state transition is any tuple t = (cid:5)s, A, s(cid:10)(cid:6) where s, s(cid:10) are states and A ⊆ Lact ∩ lit(PD) isa set of legal action instances in PD.Observe that a state does not necessarily contain either f or ¬f for each legal instancef of a fluent, and may even be empty (s = ∅). State transitions are not constrained; thiswill be done in the definition of legal state transitions below. We proceed in analogy tothe definition of answer sets in [12], considering first positive (i.e., involving a positiveplanning domain) and then general planning problems.In what follows, we assume that PD = (cid:5)Π, (cid:5)D, R(cid:6)(cid:6) is a well-typed ground planningdomain and that M is the unique answer set of Π. For any other PD, the respective conceptsare defined through its typed grounding PD↓.Definition A.9 (Legal initial state). A state s0 is a legal initial state for a positive PD, if s0is the least set (w.r.t. ⊆) such that post(c) ⊆ s0 ∪ M implies h(c) ⊆ s0, for all initial stateconstraints and static rules c ∈ R.For a positive PD and a state s, a set A ⊆ L+act is called executable action set w.r.t.s, if for each a ∈ A there exists an executability condition e ∈ R such that h(e) = {a},pre(e) ∩ Lfl,typ ⊆ s ∪ M, and pre(e) ∩ L+⊆ A. Note that this definition allows foractmodeling dependent actions, i.e., actions which depend on the execution of other actions.Definition A.10 (Legal state transition). Given a positive PD, a state transition t =(cid:5)s, A, s(cid:10)(cid:6) is called legal, if A is an executable action set w.r.t. s and s(cid:10) is the minimalconsistent set that satisfies all causation rules w.r.t. s ∪ A ∪ M. That is, for every causationrule r ∈ R, if (i) post(r) ⊆ s(cid:10) ∪ M, (ii) pre(r) ∩ Lfl,typ ⊆ s ∪ M, and (iii) pre(r) ∩ Lact ⊆ Aall hold, then h(r) (cid:20)= {false} and h(r) ⊆ s(cid:10).This is now extended to general a well-typed ground PD containing default negationusing a Gelfond–Lifschitz type reduction to a positive planning domain [12].Definition A.11 (Reduction). Let PD be a ground and well-typed planning domain, and lett = (cid:5)s, A, s(cid:10)(cid:6) be a state transition. Then, the reduction PDt = (cid:5)Π, (cid:5)D, Rt (cid:6)(cid:6) of PD by t isthe planning domain where Rt is obtained from R by deleting(1) each r ∈ R, where either post(2) all default literals not L (L ∈ L) from the remaining r ∈ R.−(r) ∩ (s(cid:10) ∪ M) (cid:20)= ∅ or pre−(r) ∩ (s ∪ A ∪ M) (cid:20)= ∅, andNote that PDt is positive and ground. We extend further definitions as follows.Definition A.12 (Legal initial state, executable action set, legal state transition). For anyplanning domain PD, a state s0 is a legal initial state, if s0 is a legal initial state for204T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211PD(cid:5)∅,∅,s0(cid:6); a set A is an executable action set w.r.t. a state s, if A is executable w.r.t. sin PD(cid:5)s,A,∅(cid:6); and, a state transition t = (cid:5)s, A, s(cid:10)(cid:6) is legal, if it is legal in PDt .A.2.2. PlansDefinition A.13 (Trajectory). A sequence of state transitions T = (cid:5)(cid:5)s0, A1, s1(cid:6), (cid:5)s1, A2, s2(cid:6),. . . , (cid:5)sn−1, An, sn(cid:6)(cid:6), n (cid:1) 0, is a trajectory for PD, if s0 is a legal initial state of PD and all(cid:5)si−1, Ai, si(cid:6), 1 (cid:2) i (cid:2) n, are legal state transitions of PD.If n = 0, then T = (cid:5) (cid:6) is empty and has s0 associated explicitly.Definition A.14 (Optimistic plan). A sequence of action sets (cid:5)A1, . . . , Ai(cid:6), i (cid:1) 0, is anoptimistic plan for a planning problem P = (cid:5)PD, q(cid:6), if a trajectory T = (cid:5)(cid:5)s0, A1, s1(cid:6),(cid:5)s1, A2, s2(cid:6), . . . , (cid:5)si−1, Ai, si (cid:6)(cid:6) exists in PD which establishes the goal, i.e., {g1, . . . , gm} ⊆si and {gm+1, . . . , gn} ∩ si = ∅.Optimistic plans amount to “plans”, “valid plans”, etc. as defined in the literature. Theterm “optimistic” should stress the credulous view in this definition, with respect to incom-plete fluent information and nondeterministic action effects. In such cases, the executionof an optimistic plan P might fail to reach the goal. We thus resort to secure plans.Definition A.15 (Secure plans (alias conformant plans)). An optimistic plan (cid:5)A1, . . . , An(cid:6)is a secure plan, if for every legal initial state s0 and trajectory T = (cid:5)(cid:5)s0, A1, s1(cid:6), . . . , (cid:5)sj −1,Aj , sj (cid:6)(cid:6) such that 0 (cid:2) j (cid:2) n, it holds that (i) if j = n then T establishes the goal, and (ii) ifj < n, then Aj +1 is executable in sj w.r.t. PD, i.e., some legal transition (cid:5)sj , Aj +1, sj +1(cid:6)exists.Note that plans admit in general the concurrent execution of actions. We call a plan(cid:5)A1, . . . , An(cid:6) sequential (or non-concurrent), if |Aj | (cid:2) 1, for all 1 (cid:2) j (cid:2) n.A.3. MacrosK includes several macros as shorthands for frequently used concepts. Let a ∈ L+actdenote an action atom, f ∈ Lfl a fluent literal, B a (possibly empty) sequence b1, . . . , bk,not bk+1, . . . , not bl where each bi ∈ Lfl,typ, i = 1, . . . , l, and A a (possibly empty)sequence a1, . . . , am, not am+1, . . . , not an where each aj ∈ L, j = 1, . . . , n.Inertia. To allow for an easy representation of fluent inertia, K providesinertial f if B after A.⇔ caused f if not ¬.f, B after f, A.Defaults. A default value of a fluent can be expressed by the shortcutdefault f. ⇔ caused f if not ¬.f.It is in effect unless some other causation rule provides evidence to the opposite value.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211205Totality. For reasoning under incomplete, but total knowledge K provides (f positive):total f if B after A.⇔ caused f if not -f, B after A.caused -f if not f, B after A.State integrity. For integrity constraints that refer to the preceding state, K providesforbidden B after A. ⇔ caused false if B after A.Nonexecutability. For specifying that some action is not executable, K providesnonexecutable a if B. ⇔ caused false after a, B.By this definition, nonexecutable overrides executable in case of conflicts.Non-concurrent plans. To exclude simultaneous execution of actions, K providesnoConcurrency. ⇔ caused false after a1, a2.where a1 and a2 range over all possible actions such that a1, a2 ∈ LPD ∩ Lact anda1 (cid:20)= a2.In all macros, “if B” (respectively, “after A”) can be omitted, if B (respectively, A)is empty.Appendix B. Problem encodings for other systemsB.1. Blocksworld problem P1 for CMBPDOMAIN blocks_P1ACTIONSact : { move_1_4, move_1_3, move_1_2, move_1_0, move_2_4, move_2_3,move_2_1, move_2_0, move_3_4, move_3_2, move_3_1, move_3_0,move_4_3, move_4_2, move_4_1, move_4_0 };FLUENTSon_1 : 0..4; on_2 : 0..4; on_3 : 0..4; on_4 : 0..4;blocked_1 : boolean; blocked_2 : boolean;blocked_3 : boolean; blocked_4 : boolean;INERTIAL on_1, blocked_1, on_2, blocked_2, on_3, blocked_3, on_4, blocked_4;CAUSES act = move_1_4 FALSE IF blocked_1 | blocked_4;CAUSES act = move_1_3 FALSE IF blocked_1 | blocked_3;CAUSES act = move_1_2 FALSE IF blocked_1 | blocked_2;CAUSES act = move_1_0 FALSE IF blocked_1;CAUSES act = move_1_4 on_1 = 4 & blocked_4 IF 1;CAUSES act = move_1_4 !blocked_2 IF on_1 = 2;CAUSES act = move_1_4 !blocked_3 IF on_1 = 3;CAUSES act = move_1_3 on_1 = 3 & blocked_3 IF 1;CAUSES act = move_1_3 !blocked_2 IF on_1 = 2;CAUSES act = move_1_3 !blocked_4 IF on_1 = 4;206T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211CAUSES act = move_1_2 on_1 = 2 & blocked_2 IF 1;CAUSES act = move_1_2 !blocked_3 IF on_1 = 3;CAUSES act = move_1_2 !blocked_4 IF on_1 = 4;CAUSES act = move_1_0 on_1 = 0 IF 1;CAUSES act = move_1_0 !blocked_2 IF on_1 = 2;CAUSES act = move_1_0 !blocked_3 IF on_1 = 3;CAUSES act = move_1_0 !blocked_4 IF on_1 = 4;CAUSES act = move_2_4 FALSE IF blocked_2 | blocked_4;CAUSES act = move_2_3 FALSE IF blocked_2 | blocked_3;CAUSES act = move_2_1 FALSE IF blocked_2 | blocked_1;CAUSES act = move_2_0 FALSE IF blocked_2;CAUSES act = move_2_4 on_2 = 4 & blocked_4 IF 1;CAUSES act = move_2_4 !blocked_1 IF on_2 = 1;CAUSES act = move_2_4 !blocked_3 IF on_2 = 3;CAUSES act = move_2_3 on_2 = 3 & blocked_3 IF 1;CAUSES act = move_2_3 !blocked_1 IF on_2 = 1;CAUSES act = move_2_3 !blocked_4 IF on_2 = 4;CAUSES act = move_2_1 on_2 = 1 & blocked_1 IF 1;CAUSES act = move_2_1 !blocked_3 IF on_2 = 3;CAUSES act = move_2_1 !blocked_4 IF on_2 = 4;CAUSES act = move_2_0 on_2 = 0 IF 1;CAUSES act = move_2_0 !blocked_1 IF on_2 = 1;CAUSES act = move_2_0 !blocked_3 IF on_2 = 3;CAUSES act = move_2_0 !blocked_4 IF on_2 = 4;CAUSES act = move_3_4 FALSE IF blocked_3 | blocked_4;CAUSES act = move_3_2 FALSE IF blocked_3 | blocked_2;CAUSES act = move_3_1 FALSE IF blocked_3 | blocked_1;CAUSES act = move_3_0 FALSE IF blocked_3;CAUSES act = move_3_4 on_3 = 4 & blocked_4 IF 1;CAUSES act = move_3_4 !blocked_1 IF on_3 = 1;CAUSES act = move_3_4 !blocked_2 IF on_3 = 2;CAUSES act = move_3_2 on_3 = 2 & blocked_2 IF 1;CAUSES act = move_3_2 !blocked_1 IF on_3 = 1;CAUSES act = move_3_2 !blocked_4 IF on_3 = 4;CAUSES act = move_3_1 on_3 = 1 & blocked_1 IF 1;CAUSES act = move_3_1 !blocked_2 IF on_3 = 2;CAUSES act = move_3_1 !blocked_4 IF on_3 = 4;CAUSES act = move_3_0 on_3 = 0 IF 1;CAUSES act = move_3_0 !blocked_1 IF on_3 = 1;CAUSES act = move_3_0 !blocked_2 IF on_3 = 2;CAUSES act = move_3_0 !blocked_4 IF on_3 = 4;CAUSES act = move_4_3 FALSE IF blocked_4 | blocked_3;CAUSES act = move_4_2 FALSE IF blocked_4 | blocked_2;CAUSES act = move_4_1 FALSE IF blocked_4 | blocked_1;CAUSES act = move_4_0 FALSE IF blocked_4;CAUSES act = move_4_3 on_4 = 3 & blocked_3 IF 1;CAUSES act = move_4_3 !blocked_1 IF on_4 = 1;CAUSES act = move_4_3 !blocked_2 IF on_4 = 2;CAUSES act = move_4_2 on_4 = 2 & blocked_2 IF 1;CAUSES act = move_4_2 !blocked_1 IF on_4 = 1;CAUSES act = move_4_2 !blocked_3 IF on_4 = 3;T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211207CAUSES act = move_4_1 on_4 = 1 & blocked_1 IF 1;CAUSES act = move_4_1 !blocked_2 IF on_4 = 2;CAUSES act = move_4_1 !blocked_3 IF on_4 = 3;CAUSES act = move_4_0 on_4 = 0 IF 1;CAUSES act = move_4_0 !blocked_1 IF on_4 = 1;CAUSES act = move_4_0 !blocked_2 IF on_4 = 2;CAUSES act = move_4_0 !blocked_3 IF on_4 = 3;INITIALLY on_1 = 0 & on_2 = 0 & on_3 = 0 & on_4 = 3 &blocked_3 & !blocked_1 & !blocked_2 & !blocked_4;CONFORMANT on_1 = 0 & on_2 = 1 & on_3 = 2 & on_4 = 3;B.2. Blocks world problem P1 for GPT(define (domain bw)(:model SEARCH)(:types BLOCK)(:functions (on BLOCK BLOCK )(clear BLOCK :boolean))(:objects table - BLOCK)(:action puton:parameters ?X - BLOCK ?Y - BLOCK ?Z - BLOCK:precondition (:and (= (on ?X) ?Z):effect(= (clear ?X) true)(:or (= (clear ?Y) true) (= ?Y table))(:not (= ?Y ?Z))(:not (= ?X ?Z))(:not (= ?X table)))(:set (on ?X) ?Y)(:set (clear ?Z) true)(:set (clear ?Y) false)))(define (problem p1)(:domain bw)(:objects b0 b1 b2 b3 - BLOCK)(:init(:set (on b0) table)(:set (on b1) table)(:set (on b2) table)(:set (on b3) b2)(:set (clear b0) true)(:set (clear b1) true)(:set (clear b2) false)(:set (clear b3) true)(:set (clear table) false))(:goal (:and (= (on b3) b2)(= (on b2) b1)(= (on b1) b0)(= (on b0) table))))208T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211Appendix C. DLVK encodings of BMTUC(p, t)C.1. World-state encodingBackground knowledge:package(1). package(2). ... package(p).toilet(1). toilet(2). ... toilet(t).DLVK program:fluents :actions :always :clogged(T) requires toilet(T).armed(P) requires package(P).unsafe.dunk(P, T) requires package(P), toilet(T).flush(T) requires toilet(T).inertial armed(P).inertial clogged(T).caused -clogged(T) after flush(T).caused -armed(P) after dunk(P, T).total clogged(T) after dunk(P, T).caused unsafe if armed(P).executable flush(T).executable dunk(P, T) if not clogged(T).nonexecutable dunk(P, T) if flush(T).nonexecutable dunk(P, T) if dunk(P1, T), P <> P1.nonexecutable dunk(P, T) if dunk(P, T1), T <> T1.initially : total armed(P).forbidden armed(P), armed(P1), P <> P1.forbidden not unsafe.In this encoding, weak negation of the fluent clogged is a CWA representation of thenegated fluent -clogged, which relieves us from storing negative information explicitly.The possible world-states are encoded (1) via the total-statement for the fluentarmed in the initially section, which generates all possible initial states, and (2)via the total-statement for the fluent clogged in the always section, which specifiesthe effect of dunking a package.C.2. Knowledge-state encodingBackground knowledge:package(1). package(2). ... package(p).toilet(1). toilet(2). ... toilet(t).DLVK program:fluents :actions :always :T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211209clogged(T) requires toilet(T).armed(P) requires package(P).dunked(T) requires toilet(T).unsafe.dunk(P, T) requires package(P), toilet(T).flush(T) requires toilet(T).inertial -armed(P).inertial clogged(T) if not dunked(T).inertial -clogged(T) if not dunked(T).caused dunked(T) after dunk(P, T).caused -clogged(T) after flush(T).caused -armed(P) after dunk(P, T).caused unsafe if not -armed(P).executable flush(T).executable dunk(P, T) if -clogged(T).nonexecutable dunk(P, T) if flush(T).nonexecutable dunk(P, T) if dunk(P1, T), P <> P1.nonexecutable dunk(P, T) if dunk(P, T1), T <> T1.initially : -clogged(T).In this encoding, the fluents armed and clogged are treated as three-valued. Insteadof encoding all possible initial world states by cases, we have a single initial state in whichwe only know that all toilets are not clogged, while the values of the fluents armed areopen. We may gain, on the one hand, knowledge on fluent armed by executing an actionflush, while on the other hand, we may lose (“forget”) information on fluent clogged,if we know that something has been dunked into the respective toilet (for his projection,we use the auxiliary fluent dunked).An advantage of this encoding is that optimistic and secure plans coincide on thisencoding, since nondeterministic effects of action dunk are treated by “forgetting” thevalue of the respective fluent clogged. We point out that the “bomb in toilet problem” isper se computationally easy; so it seems that encodings based on world-states artificiallybloat this problem, because of their lack of a natural statement about fluents being unknownin some state. For further discussion, we refer to [6].References[1] R. Bayardo, R. Schrag, Using CSP look-back techniques to solve real-world SAT instances, in: Proc. AAAI-97, Providence, RI, 1997, pp. 203–208.[2] A.L. Blum, M.L. Furst, Fast planning through planning graph analysis, Artificial Intelligence 90 (1997)281–300.[3] B. Bonet, H. Geffner, Planning with incomplete information as heuristic search in belief space, in: S. Chien,S. Kambhampati, C.A. Knoblock (Eds.), Proc. AIPS-00, Breckenridge, CO, 2000, pp. 52–61.210T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211[4] A. Cimatti, M. Roveri, Conformant planning via symbolic model checking, J. Artificial Intelligence Res. 13(2000) 305–338.[5] T. Eiter, W. Faber, N. Leone, G. Pfeifer, A. Polleres, Planning under incomplete knowledge, in: J. Lloyd,V. Dahl, U. Furbach, M. Kerber, K.-K. Lau, C. Palamidessi, L.M. Pereira, Y. Sagiv, P.J. Stuckey (Eds.),CL2000, in: Lecture Notes in AI (LNAI), Vol. 1861, Springer, London, 2000, pp. 807–821.[6] T. Eiter, W. Faber, N. Leone, G. Pfeifer, A. Polleres, A logic programming approach to knowledge-stateplanning: Semantics and complexity, Technical Report INFSYS RR-1843-01-11, TU Wien, 2001.[7] T. Eiter, N. Leone, C. Mateis, G. Pfeifer, F. Scarcello, The KR system dlv: Progress report, comparisonsand benchmarks, in: A.G. Cohn, L. Schubert, S.C. Shapiro (Eds.), Proc. KR-98, Morgan Kaufmann, SanMateo, CA, 1998, pp. 406–417.[8] E. Erdem, Applications of logic programming to planning: Computational experiments, unpublished draft,1999, http://www.cs.utexas.edu/users/esra/papers.html.[9] W. Faber, N. Leone, G. Pfeifer, Pushing goal derivation in DLP computations, in: M. Gelfond, N. Leone, G.Pfeifer (Eds.), Proc. LPNMR-99, El Paso, TX, in: Lecture Notes in AI (LNAI), Vol. 1730, 1999, pp. 177–191.[10] P. Ferraris, E. Giunchiglia, Planning as satisfiability in nondeterministic domains, in: Proc. AAAI-00, Austin,TX, AAAI Press/MIT Press, Cambridge, MA, 2000, pp. 748–753.[11] A. Finzi, F. Pirri, R. Reiter, Open world planning in the situation calculus, in: Proc. AAAI-00, Austin, TX,AAAI Press/MIT Press, Cambridge, MA, 2000, pp. 754–760.[12] M. Gelfond, V. Lifschitz, Classical negation in logic programs and disjunctive databases, New GenerationComput. 9 (1991) 365–385.[13] M. Gelfond, V. Lifschitz, Representing action and change by logic programs, J. Logic Programming 17(1993) 301–321.[14] M. Ghallab, A. Howe, C. Knoblock, D. McDermott, A. Ram, M. Veloso, D. Weld, D. Wilkins, PDDL—ThePlanning Domain Definition language, Tech. Report, Yale Center for Computational Vision and Control,1998, available at http://www.cs.yale.edu/pub/mcdermott/software/pddl.tar.gz.[15] E. Giunchiglia, Planning as satisfiability with expressive action languages: Concurrency, constraints andnondeterminism, in: A.G. Cohn, F. Giunchiglia, B. Selman (Eds.), Proc. KR-2000, Morgan Kaufmann, SanMateo, CA, 2000, pp. 657–666.[16] E. Giunchiglia, G.N. Kartha, V. Lifschitz, Representing action: Indeterminacy and ramifications, ArtificialIntelligence 95 (1997) 409–443.[17] E. Giunchiglia, V. Lifschitz, An action language based on causal explanation: Preliminary report, in: Proc.AAAI-98, Madison, WI, 1998, pp. 623–630.[18] E. Giunchiglia, V. Lifschitz, Action languages, temporal action logics and the situation calculus, in: WorkingNotes of the IJCAI’99 Workshop on Nonmonotonic Reasoning, Action, and Change, Stockholm, Sweden,1999.[19] R. Goldman, M. Boddy, Expressive planning and explicit knowledge, in: Proc. AIPS-96, AAAI Press, 1996,pp. 110–117.[20] L. Iocchi, D. Nardi, R. Rosati, Planning with sensing, concurrency, and exogenous events: Logicalframework and implementation, in: A.G. Cohn, F. Giunchiglia, B. Selman (Eds.), Proc. KR-2000, MorganKaufmann, San Mateo, CA, 2000, pp. 678–689.[21] G.N. Kartha, V. Lifschitz, Actions with indirect effects (preliminary report), in: Proceedings of the FourthInternational Conference on Principles of Knowledge Representation and Reasoning (KR-94), Bonn,Germany, 1994, pp. 341–350.[22] H. Kautz, B. Selman, Planning as satisfiability, in: Proc. ECAI-92, Vienna, Austria, 1992, pp. 359–363.[23] N. Kushmerick, S. Hanks, D.S. Weld, An algorithm for probabilistic planning, Artificial Intelligence 76(1–2) (1995) 239–286.[24] N. Leone, R. Rosati, F. Scarcello, Enhancing answer set planning, in: A. Cimatti, H. Geffner, E. Giunchiglia,J. Rintanen (Eds.), IJCAI-01 Workshop on Planning under Uncertainty and Incomplete Information, Seattle,WA, 2001, pp. 33–42.[25] V. Lifschitz, Foundations of logic programming, in: G. Brewka (Ed.), Principles of Knowledge Representa-tion, CSLI Publications, Stanford, 1996, pp. 69–127.T. Eiter et al. / Artificial Intelligence 144 (2003) 157–211211[26] V. Lifschitz, Action languages, answer sets and planning, in: K. Apt, V.W. Marek, M. Truszczy´nski,D.S. Warren (Eds.), The Logic Programming Paradigm—A 25-Year Perspective, Springer, Berlin, 1999,pp. 357–373.[27] V. Lifschitz, Answer set planning, in: D.D. Schreye (Ed.), Proc. ICLP-99, MIT Press, Las Cruces, NM,1999, pp. 23–37.[28] V. Lifschitz, H. Turner, Splitting a logic program, in: P. Van Hentenryck (Ed.), Proc. ICLP-94, MIT Press,Cambridge, MA, 1994, pp. 23–37.[29] V. Lifschitz, H. Turner, Representing transition systems by logic programs, in: M. Gelfond, N. Leone, G.Pfeifer (Eds.), Proc. LPNMR-99, El Paso, TX, in: Lecture Notes in AI (LNAI), Vol. 1730, Springer, Berlin,1999, pp. 92–106.[30] N. McCain, H. Turner, Causal theories of actions and change, in: Proc. AAAI-97, Providence, RI, 1997,pp. 460–465.[31] N. McCain, H. Turner, Satisfiability planning with causal theories, in: A.G. Cohn, L. Schubert, S.C. Shapiro(Eds.), Proc. KR-98, Morgan Kaufmann, San Mateo, CA, 1998, pp. 212–223.[32] J. McCarthy, Formalization of Common Sense. Papers by John McCarthy edited by V. Lifschitz, Ablex,Norwood, NJ, 1990.[33] J. McCarthy, P.J. Hayes, Some philosophical problems from the standpoint of artificial intelligence, in:B. Meltzer, D. Michie (Eds.), Machine Intelligence 4, Edinburgh University Press, Edinburgh, 1969,pp. 463–502, reprinted in [32].[34] D. McDermott, A critique of pure reason, Comput. Intelligence 3 (1987) 151–237, cited in [4].[35] I. Niemelä, Logic programming with stable model semantics as constraint programming paradigm, Ann.Math. Artificial Intelligence 25 (3–4) (1999) 241–273.[36] C.H. Papadimitriou, Computational Complexity, Addison-Wesley, Reading, MA, 1994.[37] M.A. Peot, Decision-theoretic planning, Ph.D. thesis, Stanford University, Stanford, CA, 1998.[38] M.A. Peot, D.E. Smith, Conditional nonlinear planning, in: Proceedings of the First International Conferenceon Artificial Intelligence Planning Systems, AAAI Press, 1992, pp. 189–197.[39] L. Pryor, G. Collins, Planning for contingencies: A decision-based approach, J. Artificial Intelligence Res. 4(1996) 287–339.[40] R. Reiter, On closed world data bases, in: H. Gallaire, J. Minker (Eds.), Logic and Data Bases, Plenum Press,New York, 1978, pp. 55–76.[41] J. Rintanen, Constructing conditional plans by a theorem-prover, J. Artificial Intelligence Res. 10 (1999)323–352.[42] D.E. Smith, D.S. Weld, Conformant Graphplan, in: Proc. AAAI-98, Madison, WI, AAAI Press/MIT Press,Cambridge, MA, 1998, pp. 889–896.[43] V. Subrahmanian, C. Zaniolo, Relating stable models and AI planning domains, in: L. Sterling (Ed.),Proceedings of the 12th International Conference on Logic Programming, Tokyo, Japan, MIT Press,Cambridge, MA, 1995, pp. 233–247.[44] G.J. Sussman, The virtuous nature of bugs, in: J. Allen, J. Hendler, A. Tate (Eds.), Readings in Planning,Morgan Kaufmann, San Mateo, CA, 1990, Chapter 3, pp. 111–117, originally written 1974.[45] H. Turner, Representing actions in logic programs and default theories: A situation calculus approach,J. Logic Programming 31 (1–3) (1997) 245–298.[46] J.D. Ullman, Principles of Database and Knowledge Base Systems, Vol. 1, Computer Science Press, 1989.[47] D.S. Weld, C.R. Anderson, D.E. Smith, Extending Graphplan to handle uncertainty & sensing actions, in:Proc. AAAI-98, Madison, WI, AAAI Press/MIT Press, Cambridge, MA, 1998, pp. 897–904.[48] H. Zhang, SATO: An efficient propositional prover, in: Proceedings of the International Conference onAutomated Deduction (CADE-1997), 1997, pp. 272–275.